[
  {
    "discussion_id": "2182202906",
    "pr_number": 19108,
    "pr_file": "crates/ty_ide/src/semantic_tokens.rs",
    "created_at": "2025-07-03T08:36:52+00:00",
    "commented_code": "+use crate::Db;\n+use ruff_db::files::File;\n+use ruff_db::parsed::parsed_module;\n+use ruff_python_ast as ast;\n+use ruff_python_ast::visitor::source_order::{SourceOrderVisitor, walk_expr, walk_stmt};\n+use ruff_python_ast::{Expr, Stmt, TypeParam, TypeParams};\n+use ruff_text_size::{Ranged, TextRange};\n+use ty_python_semantic::{HasType, SemanticModel, types::Type};\n+\n+// This module walks the AST and collects a set of \"semantic tokens\" for a file\n+// or a range within a file. Each semantic token provides a \"token type\" and zero\n+// or more \"modifiers\". This information can be used by an editor to provide\n+// color coding based on semantic meaning.\n+\n+// Current limitations and areas for future improvement:\n+\n+// TODO: Need to handle semantic tokens within quoted annotations.\n+\n+// TODO: Need to properly handle Annotated expressions. All type arguments other\n+// than the first should be treated as value expressions, not as type expressions.\n+\n+// TODO: An identifier that resolves to a parameter when used within a function\n+// should be classified as a parameter, selfParameter, or clsParameter token.\n+\n+// TODO: Properties (or perhaps more generally, descriptor objects?) should be\n+// classified as property tokens rather than just variables.\n+\n+// TODO: Special forms like Protocol and TypedDict should probably be classified\n+// as class tokens, but they are currently classified as variables.\n+\n+// TODO: Type aliases (including those defined with the Python 3.12 \"type\" statement)\n+// do not currently have a dedicated semantic token type, but they maybe should.\n+\n+// TODO: Additional token modifiers might be added (e.g. for static methods,\n+// abstract methods and classes).\n+\n+/// Semantic token types supported by the language server.\n+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]\n+#[repr(u32)]\n+pub enum SemanticTokenType {\n+    Namespace = 0,\n+    Class = 1,\n+    Parameter = 2,\n+    SelfParameter = 3,\n+    ClsParameter = 4,\n+    Variable = 5,\n+    Property = 6,\n+    Function = 7,\n+    Method = 8,\n+    Keyword = 9,\n+    String = 10,\n+    Number = 11,\n+    Decorator = 12,\n+    BuiltinConstant = 13,\n+    TypeParameter = 14,\n+}\n+\n+impl SemanticTokenType {\n+    /// Returns all supported token types for LSP capabilities.\n+    pub fn all() -> Vec<&'static str> {\n+        vec![\n+            \"namespace\",\n+            \"class\",\n+            \"parameter\",\n+            \"selfParameter\",\n+            \"clsParameter\",\n+            \"variable\",\n+            \"property\",\n+            \"function\",\n+            \"method\",\n+            \"keyword\",\n+            \"string\",\n+            \"number\",\n+            \"decorator\",\n+            \"builtinConstant\",\n+            \"typeParameter\",\n+        ]\n+    }\n+}\n+\n+/// Semantic token modifiers.\n+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]\n+#[repr(u32)]\n+pub enum SemanticTokenModifier {\n+    Definition = 0,\n+    Readonly = 1,\n+    Async = 2,\n+}\n+\n+impl SemanticTokenModifier {\n+    /// Returns all supported token modifiers for LSP capabilities.\n+    pub fn all() -> Vec<&'static str> {\n+        vec![\"definition\", \"readonly\", \"async\"]\n+    }\n+}\n+\n+/// A semantic token with its position and classification.\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct SemanticToken {\n+    pub range: TextRange,\n+    pub token_type: SemanticTokenType,\n+    pub modifiers: Vec<SemanticTokenModifier>,\n+}\n+\n+/// The result of semantic tokenization.\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct SemanticTokens {\n+    pub tokens: Vec<SemanticToken>,\n+}\n+\n+/// Generates semantic tokens for a Python file within the specified range.\n+/// Pass a full file range to get tokens for the entire file.\n+pub fn semantic_tokens(db: &dyn Db, file: File, range: TextRange) -> Option<SemanticTokens> {\n+    let parsed = parsed_module(db, file).load(db);\n+    let semantic_model = SemanticModel::new(db, file);\n+\n+    let mut visitor = SemanticTokenVisitor::new(db, &semantic_model, range);\n+    visitor.visit_body(parsed.suite());\n+\n+    Some(SemanticTokens {\n+        tokens: visitor.tokens,\n+    })\n+}\n+\n+/// AST visitor that collects semantic tokens.\n+struct SemanticTokenVisitor<'db> {\n+    #[allow(dead_code)]\n+    db: &'db dyn Db,\n+    #[allow(dead_code)]\n+    semantic_model: &'db SemanticModel<'db>,\n+    tokens: Vec<SemanticToken>,\n+    in_class_scope: bool,\n+    in_type_annotation: bool,\n+    range_filter: TextRange,\n+}\n+\n+impl<'db> SemanticTokenVisitor<'db> {\n+    fn new(\n+        db: &'db dyn Db,\n+        semantic_model: &'db SemanticModel<'db>,\n+        range_filter: TextRange,\n+    ) -> Self {\n+        Self {\n+            db,\n+            semantic_model,\n+            tokens: Vec::new(),\n+            in_class_scope: false,\n+            in_type_annotation: false,\n+            range_filter,\n+        }\n+    }\n+\n+    fn add_token(\n+        &mut self,\n+        range: TextRange,\n+        token_type: SemanticTokenType,\n+        modifiers: Vec<SemanticTokenModifier>,\n+    ) {\n+        // Only emit tokens that intersect with the range filter\n+        if range.intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        self.tokens.push(SemanticToken {\n+            range,\n+            token_type,\n+            modifiers,\n+        });\n+    }\n+\n+    fn is_constant_name(name: &str) -> bool {\n+        name.chars().all(|c| c.is_uppercase() || c == '_') && name.len() > 1\n+    }\n+\n+    fn classify_name(\n+        &self,\n+        name: &ast::ExprName,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        // Try to get the inferred type of this name expression using semantic analysis\n+        let ty = name.inferred_type(self.semantic_model);\n+        let name_str = name.id.as_str();\n+        self.classify_from_type_and_name_str(ty, name_str)\n+    }\n+\n+    fn classify_from_type_and_name_str(\n+        &self,\n+        ty: Type,\n+        name_str: &str,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        let mut modifiers = vec![];\n+\n+        // In type annotation contexts, names that refer to nominal instances or protocol instances\n+        // should be classified as Class tokens (e.g., \"int\" in \"x: int\" should be a Class token)\n+        if self.in_type_annotation {\n+            match ty {\n+                Type::NominalInstance(_) | Type::ProtocolInstance(_) => {\n+                    return (SemanticTokenType::Class, modifiers);\n+                }\n+                _ => {\n+                    // Continue with normal classification for other types in annotations\n+                }\n+            }\n+        }\n+\n+        match ty {\n+            Type::ClassLiteral(_) => (SemanticTokenType::Class, modifiers),\n+            Type::TypeVar(_) => (SemanticTokenType::TypeParameter, modifiers),\n+            _ if ty.is_function_literal() => {\n+                // Check if this is a method based on current scope\n+                if self.in_class_scope {\n+                    (SemanticTokenType::Method, modifiers)\n+                } else {\n+                    (SemanticTokenType::Function, modifiers)\n+                }\n+            }\n+            _ if ty.is_bound_method() => (SemanticTokenType::Method, modifiers),\n+            _ if ty.into_module_literal().is_some() => (SemanticTokenType::Namespace, modifiers),\n+            _ => {\n+                // Check for constant naming convention\n+                if Self::is_constant_name(name_str) {\n+                    modifiers.push(SemanticTokenModifier::Readonly);\n+                }\n+                // For other types (variables, modules, etc.), assume variable\n+                (SemanticTokenType::Variable, modifiers)\n+            }\n+        }\n+    }\n+\n+    fn classify_from_type_for_attribute(\n+        ty: Type,\n+        attr_name: &ast::Identifier,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        let attr_name_str = attr_name.id.as_str();\n+        let mut modifiers = vec![];\n+\n+        // Classify based on the inferred type of the attribute\n+        if ty.is_class_literal() {\n+            (SemanticTokenType::Class, modifiers)\n+        } else if ty.is_function_literal() {\n+            // This is a function accessed as an attribute, likely a method\n+            (SemanticTokenType::Method, modifiers)\n+        } else if ty.is_bound_method() {\n+            // Method bound to an instance\n+            (SemanticTokenType::Method, modifiers)\n+        } else if ty.into_module_literal().is_some() {\n+            // Module accessed as an attribute (e.g., from os import path)\n+            (SemanticTokenType::Namespace, modifiers)\n+        } else if ty.is_property_instance() {\n+            // Actual Python property\n+            (SemanticTokenType::Property, modifiers)\n+        } else {\n+            // Check for constant naming convention\n+            if Self::is_constant_name(attr_name_str) {\n+                modifiers.push(SemanticTokenModifier::Readonly);\n+            }\n+\n+            // For other types (variables, constants, etc.), classify as variable\n+            (SemanticTokenType::Variable, modifiers)\n+        }\n+    }\n+\n+    fn classify_parameter(\n+        &self,\n+        _param: &ast::Parameter,\n+        is_first: bool,\n+        func: &ast::StmtFunctionDef,\n+    ) -> SemanticTokenType {\n+        if is_first && self.in_class_scope {\n+            // Check if this is a classmethod (has @classmethod decorator)\n+            let is_classmethod =\n+                func.decorator_list\n+                    .iter()\n+                    .any(|decorator| match &decorator.expression {\n+                        ast::Expr::Name(name) => name.id.as_str() == \"classmethod\",\n+                        ast::Expr::Attribute(attr) => attr.attr.id.as_str() == \"classmethod\",\n+                        _ => false,\n+                    });\n+\n+            // Check if this is a staticmethod (has @staticmethod decorator)\n+            let is_staticmethod =\n+                func.decorator_list\n+                    .iter()\n+                    .any(|decorator| match &decorator.expression {\n+                        ast::Expr::Name(name) => name.id.as_str() == \"staticmethod\",\n+                        ast::Expr::Attribute(attr) => attr.attr.id.as_str() == \"staticmethod\",\n+                        _ => false,\n+                    });\n+\n+            if is_staticmethod {\n+                // Static methods don't have self/cls parameters\n+                SemanticTokenType::Parameter\n+            } else if is_classmethod {\n+                // First parameter of a classmethod is cls parameter\n+                SemanticTokenType::ClsParameter\n+            } else {\n+                // First parameter of an instance method is self parameter\n+                SemanticTokenType::SelfParameter\n+            }\n+        } else {\n+            SemanticTokenType::Parameter\n+        }\n+    }\n+\n+    fn add_dotted_name_tokens(&mut self, name: &ast::Identifier, token_type: SemanticTokenType) {\n+        let name_str = name.id.as_str();\n+        let name_range = name.range();\n+        let name_start = name_range.start();\n+\n+        // Split the dotted name and calculate positions for each part\n+        let mut current_offset = 0usize;\n+        for part in name_str.split('.') {\n+            if !part.is_empty() {\n+                let part_start =\n+                    name_start + ruff_text_size::TextSize::try_from(current_offset).unwrap();\n+                let part_end = part_start + ruff_text_size::TextSize::try_from(part.len()).unwrap();\n+                let part_range = ruff_text_size::TextRange::new(part_start, part_end);\n+\n+                self.add_token(part_range, token_type, vec![]);\n+            }\n+            // Move past this part and the dot\n+            current_offset += part.len() + 1; // +1 for the dot\n+        }\n+    }\n+\n+    fn classify_from_alias_type(\n+        &self,\n+        ty: Type,\n+        local_name: &ast::Identifier,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        self.classify_from_type_and_name_str(ty, local_name.id.as_str())\n+    }\n+\n+    fn visit_type_annotation(&mut self, annotation: &ast::Expr) {\n+        let prev_in_type_annotation = self.in_type_annotation;\n+        self.in_type_annotation = true;\n+        self.visit_expr(annotation);\n+        self.in_type_annotation = prev_in_type_annotation;\n+    }\n+\n+    fn visit_type_params(&mut self, type_params: &TypeParams) {\n+        for type_param in &type_params.type_params {\n+            self.visit_type_param(type_param);\n+        }\n+    }\n+\n+    fn visit_type_param(&mut self, type_param: &TypeParam) {\n+        // Emit token for the type parameter name\n+        let name_range = type_param.name().range();\n+        self.add_token(\n+            name_range,\n+            SemanticTokenType::TypeParameter,\n+            vec![SemanticTokenModifier::Definition],\n+        );\n+\n+        // Visit bound expression (for TypeVar)\n+        match type_param {\n+            TypeParam::TypeVar(type_var) => {\n+                if let Some(bound) = &type_var.bound {\n+                    self.visit_type_annotation(bound);\n+                }\n+                if let Some(default) = &type_var.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+            TypeParam::ParamSpec(param_spec) => {\n+                if let Some(default) = &param_spec.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+            TypeParam::TypeVarTuple(type_var_tuple) => {\n+                if let Some(default) = &type_var_tuple.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Visit decorators, handling simple name decorators vs complex expressions\n+    fn visit_decorators(&mut self, decorators: &[ast::Decorator]) {\n+        for decorator in decorators {\n+            match &decorator.expression {\n+                ast::Expr::Name(name) => {\n+                    // Simple decorator like @staticmethod - use Decorator token type\n+                    self.add_token(name.range(), SemanticTokenType::Decorator, vec![]);\n+                }\n+                _ => {\n+                    // Complex decorator like @app.route(\"/path\") - use normal expression rules\n+                    self.visit_expr(&decorator.expression);\n+                }\n+            }\n+        }\n+    }\n+}\n+\n+#[allow(clippy::elidable_lifetime_names)]\n+impl<'db> SourceOrderVisitor<'_> for SemanticTokenVisitor<'db> {\n+    fn visit_stmt(&mut self, stmt: &Stmt) {\n+        // If we have a range filter and this statement doesn't intersect, skip it\n+        // as an optimization\n+        if stmt.range().intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        match stmt {\n+            ast::Stmt::FunctionDef(func) => {\n+                // Function name\n+                self.add_token(\n+                    func.name.range(),\n+                    if self.in_class_scope {\n+                        SemanticTokenType::Method\n+                    } else {\n+                        SemanticTokenType::Function\n+                    },\n+                    if func.is_async {\n+                        vec![\n+                            SemanticTokenModifier::Definition,\n+                            SemanticTokenModifier::Async,\n+                        ]\n+                    } else {\n+                        vec![SemanticTokenModifier::Definition]\n+                    },\n+                );\n+\n+                // Type parameters (Python 3.12+ syntax)\n+                if let Some(type_params) = &func.type_params {\n+                    self.visit_type_params(type_params);\n+                }\n+\n+                // Parameters\n+                for (i, param) in func.parameters.args.iter().enumerate() {\n+                    let token_type = self.classify_parameter(&param.parameter, i == 0, func);\n+                    self.add_token(param.parameter.name.range(), token_type, vec![]);\n+\n+                    // Handle parameter type annotations\n+                    if let Some(annotation) = &param.parameter.annotation {\n+                        self.visit_type_annotation(annotation);\n+                    }\n+                }\n+\n+                // Handle return type annotation\n+                if let Some(returns) = &func.returns {\n+                    self.visit_type_annotation(returns);\n+                }\n+\n+                // Visit decorator expressions\n+                self.visit_decorators(&func.decorator_list);\n+\n+                // Clear the in_class_scope flag so inner functions\n+                // are not treated as methods\n+                let prev_in_class = self.in_class_scope;\n+                self.in_class_scope = false;\n+                walk_stmt(self, stmt);\n+                self.in_class_scope = prev_in_class;\n+            }\n+            ast::Stmt::ClassDef(class) => {\n+                // Class name\n+                self.add_token(\n+                    class.name.range(),\n+                    SemanticTokenType::Class,\n+                    vec![SemanticTokenModifier::Definition],\n+                );\n+\n+                // Type parameters (Python 3.12+ syntax)\n+                if let Some(type_params) = &class.type_params {\n+                    self.visit_type_params(type_params);\n+                }\n+\n+                // Handle base classes and type annotations in inheritance\n+                if let Some(arguments) = &class.arguments {\n+                    // Visit base class arguments\n+                    for arg in &arguments.args {\n+                        self.visit_expr(arg);\n+                    }\n+                    // Visit keyword arguments (for metaclass, etc.)\n+                    for keyword in &arguments.keywords {\n+                        self.visit_expr(&keyword.value);\n+                    }\n+                }\n+\n+                // Visit decorator expressions\n+                self.visit_decorators(&class.decorator_list);\n+\n+                let prev_in_class = self.in_class_scope;\n+                self.in_class_scope = true;\n+                walk_stmt(self, stmt);\n+                self.in_class_scope = prev_in_class;\n+            }\n+            ast::Stmt::AnnAssign(assign) => {\n+                // Handle annotated assignments (e.g., x: int = 5)\n+                if let ast::Expr::Name(name) = assign.target.as_ref() {\n+                    let (token_type, modifiers) = self.classify_name(name);\n+                    self.add_token(name.range(), token_type, modifiers);\n+                }\n+\n+                // Handle the type annotation\n+                self.visit_type_annotation(&assign.annotation);\n+\n+                // Handle the value if present\n+                if let Some(value) = &assign.value {\n+                    self.visit_expr(value);\n+                }\n+            }\n+            ast::Stmt::Import(import) => {\n+                for alias in &import.names {\n+                    if let Some(asname) = &alias.asname {\n+                        self.add_token(asname.range(), SemanticTokenType::Namespace, vec![]);\n+                    } else {\n+                        // Create separate tokens for each part of a dotted module name\n+                        self.add_dotted_name_tokens(&alias.name, SemanticTokenType::Namespace);\n+                    }\n+                }\n+                walk_stmt(self, stmt);\n+            }\n+            ast::Stmt::ImportFrom(import) => {\n+                if let Some(module) = &import.module {\n+                    // Create separate tokens for each part of a dotted module name\n+                    self.add_dotted_name_tokens(module, SemanticTokenType::Namespace);\n+                }\n+                for alias in &import.names {\n+                    if let Some(asname) = &alias.asname {\n+                        // For aliased imports (from X import Y as Z), classify Z based on what Y is\n+                        let ty = alias.inferred_type(self.semantic_model);\n+                        let (token_type, modifiers) = self.classify_from_alias_type(ty, asname);\n+                        self.add_token(asname.range(), token_type, modifiers);\n+                    } else {\n+                        // For direct imports (from X import Y), use semantic classification\n+                        let ty = alias.inferred_type(self.semantic_model);\n+                        let (token_type, modifiers) =\n+                            self.classify_from_alias_type(ty, &alias.name);\n+                        self.add_token(alias.name.range(), token_type, modifiers);\n+                    }\n+                }\n+                walk_stmt(self, stmt);\n+            }\n+            _ => {\n+                // For all other statement types, let the default visitor handle them\n+                walk_stmt(self, stmt);\n+            }\n+        }\n+    }\n+\n+    fn visit_expr(&mut self, expr: &Expr) {\n+        // If we have a range filter and this statement doesn't intersect, skip it\n+        // as an optimization\n+        if expr.range().intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        match expr {\n+            ast::Expr::Name(name) => {\n+                let (token_type, modifiers) = self.classify_name(name);\n+                self.add_token(name.range(), token_type, modifiers);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::Attribute(attr) => {\n+                // Use semantic analysis to determine the correct token type for the attribute\n+                let ty = expr.inferred_type(self.semantic_model);\n+                let (token_type, modifiers) =\n+                    Self::classify_from_type_for_attribute(ty, &attr.attr);\n+\n+                self.add_token(attr.attr.range(), token_type, modifiers);\n+                // Continue visiting the base expression\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::Call(call) => {\n+                // The function being called\n+                if let ast::Expr::Name(name) = call.func.as_ref() {\n+                    self.add_token(name.range(), SemanticTokenType::Function, vec![]);\n+                }\n+                // Continue visiting arguments\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::StringLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::String, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::NumberLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::Number, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::BooleanLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::BuiltinConstant, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::NoneLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::BuiltinConstant, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            _ => {\n+                // For all other expression types, let the default visitor handle them\n+                walk_expr(self, expr);\n+            }\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+    use crate::tests::cursor_test;\n+    use ruff_db::source::source_text;\n+    use ruff_text_size::{TextLen, TextRange};\n+\n+    /// Helper function to get full file range for testing\n+    fn full_file_range(db: &dyn Db, file: File) -> TextRange {\n+        let source = source_text(db, file);\n+        TextRange::new(0.into(), source.text_len())\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_basic() {\n+        let test = cursor_test(\"def foo(): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+        assert!(!tokens.tokens.is_empty());\n+\n+        // Should have at least a function name token\n+        let function_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Function))\n+            .collect();\n+        assert!(!function_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_class() {\n+        let test = cursor_test(\"class MyClass: pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a class token\n+        let class_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Class))\n+            .collect();\n+        assert!(!class_tokens.is_empty());\n+    }",
    "repo_full_name": "astral-sh/ruff",
    "discussion_comments": [
      {
        "comment_id": "2182202906",
        "repo_full_name": "astral-sh/ruff",
        "pr_number": 19108,
        "pr_file": "crates/ty_ide/src/semantic_tokens.rs",
        "discussion_id": "2182202906",
        "commented_code": "@@ -0,0 +1,2531 @@\n+use crate::Db;\n+use ruff_db::files::File;\n+use ruff_db::parsed::parsed_module;\n+use ruff_python_ast as ast;\n+use ruff_python_ast::visitor::source_order::{SourceOrderVisitor, walk_expr, walk_stmt};\n+use ruff_python_ast::{Expr, Stmt, TypeParam, TypeParams};\n+use ruff_text_size::{Ranged, TextRange};\n+use ty_python_semantic::{HasType, SemanticModel, types::Type};\n+\n+// This module walks the AST and collects a set of \"semantic tokens\" for a file\n+// or a range within a file. Each semantic token provides a \"token type\" and zero\n+// or more \"modifiers\". This information can be used by an editor to provide\n+// color coding based on semantic meaning.\n+\n+// Current limitations and areas for future improvement:\n+\n+// TODO: Need to handle semantic tokens within quoted annotations.\n+\n+// TODO: Need to properly handle Annotated expressions. All type arguments other\n+// than the first should be treated as value expressions, not as type expressions.\n+\n+// TODO: An identifier that resolves to a parameter when used within a function\n+// should be classified as a parameter, selfParameter, or clsParameter token.\n+\n+// TODO: Properties (or perhaps more generally, descriptor objects?) should be\n+// classified as property tokens rather than just variables.\n+\n+// TODO: Special forms like Protocol and TypedDict should probably be classified\n+// as class tokens, but they are currently classified as variables.\n+\n+// TODO: Type aliases (including those defined with the Python 3.12 \"type\" statement)\n+// do not currently have a dedicated semantic token type, but they maybe should.\n+\n+// TODO: Additional token modifiers might be added (e.g. for static methods,\n+// abstract methods and classes).\n+\n+/// Semantic token types supported by the language server.\n+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]\n+#[repr(u32)]\n+pub enum SemanticTokenType {\n+    Namespace = 0,\n+    Class = 1,\n+    Parameter = 2,\n+    SelfParameter = 3,\n+    ClsParameter = 4,\n+    Variable = 5,\n+    Property = 6,\n+    Function = 7,\n+    Method = 8,\n+    Keyword = 9,\n+    String = 10,\n+    Number = 11,\n+    Decorator = 12,\n+    BuiltinConstant = 13,\n+    TypeParameter = 14,\n+}\n+\n+impl SemanticTokenType {\n+    /// Returns all supported token types for LSP capabilities.\n+    pub fn all() -> Vec<&'static str> {\n+        vec![\n+            \"namespace\",\n+            \"class\",\n+            \"parameter\",\n+            \"selfParameter\",\n+            \"clsParameter\",\n+            \"variable\",\n+            \"property\",\n+            \"function\",\n+            \"method\",\n+            \"keyword\",\n+            \"string\",\n+            \"number\",\n+            \"decorator\",\n+            \"builtinConstant\",\n+            \"typeParameter\",\n+        ]\n+    }\n+}\n+\n+/// Semantic token modifiers.\n+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]\n+#[repr(u32)]\n+pub enum SemanticTokenModifier {\n+    Definition = 0,\n+    Readonly = 1,\n+    Async = 2,\n+}\n+\n+impl SemanticTokenModifier {\n+    /// Returns all supported token modifiers for LSP capabilities.\n+    pub fn all() -> Vec<&'static str> {\n+        vec![\"definition\", \"readonly\", \"async\"]\n+    }\n+}\n+\n+/// A semantic token with its position and classification.\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct SemanticToken {\n+    pub range: TextRange,\n+    pub token_type: SemanticTokenType,\n+    pub modifiers: Vec<SemanticTokenModifier>,\n+}\n+\n+/// The result of semantic tokenization.\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct SemanticTokens {\n+    pub tokens: Vec<SemanticToken>,\n+}\n+\n+/// Generates semantic tokens for a Python file within the specified range.\n+/// Pass a full file range to get tokens for the entire file.\n+pub fn semantic_tokens(db: &dyn Db, file: File, range: TextRange) -> Option<SemanticTokens> {\n+    let parsed = parsed_module(db, file).load(db);\n+    let semantic_model = SemanticModel::new(db, file);\n+\n+    let mut visitor = SemanticTokenVisitor::new(db, &semantic_model, range);\n+    visitor.visit_body(parsed.suite());\n+\n+    Some(SemanticTokens {\n+        tokens: visitor.tokens,\n+    })\n+}\n+\n+/// AST visitor that collects semantic tokens.\n+struct SemanticTokenVisitor<'db> {\n+    #[allow(dead_code)]\n+    db: &'db dyn Db,\n+    #[allow(dead_code)]\n+    semantic_model: &'db SemanticModel<'db>,\n+    tokens: Vec<SemanticToken>,\n+    in_class_scope: bool,\n+    in_type_annotation: bool,\n+    range_filter: TextRange,\n+}\n+\n+impl<'db> SemanticTokenVisitor<'db> {\n+    fn new(\n+        db: &'db dyn Db,\n+        semantic_model: &'db SemanticModel<'db>,\n+        range_filter: TextRange,\n+    ) -> Self {\n+        Self {\n+            db,\n+            semantic_model,\n+            tokens: Vec::new(),\n+            in_class_scope: false,\n+            in_type_annotation: false,\n+            range_filter,\n+        }\n+    }\n+\n+    fn add_token(\n+        &mut self,\n+        range: TextRange,\n+        token_type: SemanticTokenType,\n+        modifiers: Vec<SemanticTokenModifier>,\n+    ) {\n+        // Only emit tokens that intersect with the range filter\n+        if range.intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        self.tokens.push(SemanticToken {\n+            range,\n+            token_type,\n+            modifiers,\n+        });\n+    }\n+\n+    fn is_constant_name(name: &str) -> bool {\n+        name.chars().all(|c| c.is_uppercase() || c == '_') && name.len() > 1\n+    }\n+\n+    fn classify_name(\n+        &self,\n+        name: &ast::ExprName,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        // Try to get the inferred type of this name expression using semantic analysis\n+        let ty = name.inferred_type(self.semantic_model);\n+        let name_str = name.id.as_str();\n+        self.classify_from_type_and_name_str(ty, name_str)\n+    }\n+\n+    fn classify_from_type_and_name_str(\n+        &self,\n+        ty: Type,\n+        name_str: &str,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        let mut modifiers = vec![];\n+\n+        // In type annotation contexts, names that refer to nominal instances or protocol instances\n+        // should be classified as Class tokens (e.g., \"int\" in \"x: int\" should be a Class token)\n+        if self.in_type_annotation {\n+            match ty {\n+                Type::NominalInstance(_) | Type::ProtocolInstance(_) => {\n+                    return (SemanticTokenType::Class, modifiers);\n+                }\n+                _ => {\n+                    // Continue with normal classification for other types in annotations\n+                }\n+            }\n+        }\n+\n+        match ty {\n+            Type::ClassLiteral(_) => (SemanticTokenType::Class, modifiers),\n+            Type::TypeVar(_) => (SemanticTokenType::TypeParameter, modifiers),\n+            _ if ty.is_function_literal() => {\n+                // Check if this is a method based on current scope\n+                if self.in_class_scope {\n+                    (SemanticTokenType::Method, modifiers)\n+                } else {\n+                    (SemanticTokenType::Function, modifiers)\n+                }\n+            }\n+            _ if ty.is_bound_method() => (SemanticTokenType::Method, modifiers),\n+            _ if ty.into_module_literal().is_some() => (SemanticTokenType::Namespace, modifiers),\n+            _ => {\n+                // Check for constant naming convention\n+                if Self::is_constant_name(name_str) {\n+                    modifiers.push(SemanticTokenModifier::Readonly);\n+                }\n+                // For other types (variables, modules, etc.), assume variable\n+                (SemanticTokenType::Variable, modifiers)\n+            }\n+        }\n+    }\n+\n+    fn classify_from_type_for_attribute(\n+        ty: Type,\n+        attr_name: &ast::Identifier,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        let attr_name_str = attr_name.id.as_str();\n+        let mut modifiers = vec![];\n+\n+        // Classify based on the inferred type of the attribute\n+        if ty.is_class_literal() {\n+            (SemanticTokenType::Class, modifiers)\n+        } else if ty.is_function_literal() {\n+            // This is a function accessed as an attribute, likely a method\n+            (SemanticTokenType::Method, modifiers)\n+        } else if ty.is_bound_method() {\n+            // Method bound to an instance\n+            (SemanticTokenType::Method, modifiers)\n+        } else if ty.into_module_literal().is_some() {\n+            // Module accessed as an attribute (e.g., from os import path)\n+            (SemanticTokenType::Namespace, modifiers)\n+        } else if ty.is_property_instance() {\n+            // Actual Python property\n+            (SemanticTokenType::Property, modifiers)\n+        } else {\n+            // Check for constant naming convention\n+            if Self::is_constant_name(attr_name_str) {\n+                modifiers.push(SemanticTokenModifier::Readonly);\n+            }\n+\n+            // For other types (variables, constants, etc.), classify as variable\n+            (SemanticTokenType::Variable, modifiers)\n+        }\n+    }\n+\n+    fn classify_parameter(\n+        &self,\n+        _param: &ast::Parameter,\n+        is_first: bool,\n+        func: &ast::StmtFunctionDef,\n+    ) -> SemanticTokenType {\n+        if is_first && self.in_class_scope {\n+            // Check if this is a classmethod (has @classmethod decorator)\n+            let is_classmethod =\n+                func.decorator_list\n+                    .iter()\n+                    .any(|decorator| match &decorator.expression {\n+                        ast::Expr::Name(name) => name.id.as_str() == \"classmethod\",\n+                        ast::Expr::Attribute(attr) => attr.attr.id.as_str() == \"classmethod\",\n+                        _ => false,\n+                    });\n+\n+            // Check if this is a staticmethod (has @staticmethod decorator)\n+            let is_staticmethod =\n+                func.decorator_list\n+                    .iter()\n+                    .any(|decorator| match &decorator.expression {\n+                        ast::Expr::Name(name) => name.id.as_str() == \"staticmethod\",\n+                        ast::Expr::Attribute(attr) => attr.attr.id.as_str() == \"staticmethod\",\n+                        _ => false,\n+                    });\n+\n+            if is_staticmethod {\n+                // Static methods don't have self/cls parameters\n+                SemanticTokenType::Parameter\n+            } else if is_classmethod {\n+                // First parameter of a classmethod is cls parameter\n+                SemanticTokenType::ClsParameter\n+            } else {\n+                // First parameter of an instance method is self parameter\n+                SemanticTokenType::SelfParameter\n+            }\n+        } else {\n+            SemanticTokenType::Parameter\n+        }\n+    }\n+\n+    fn add_dotted_name_tokens(&mut self, name: &ast::Identifier, token_type: SemanticTokenType) {\n+        let name_str = name.id.as_str();\n+        let name_range = name.range();\n+        let name_start = name_range.start();\n+\n+        // Split the dotted name and calculate positions for each part\n+        let mut current_offset = 0usize;\n+        for part in name_str.split('.') {\n+            if !part.is_empty() {\n+                let part_start =\n+                    name_start + ruff_text_size::TextSize::try_from(current_offset).unwrap();\n+                let part_end = part_start + ruff_text_size::TextSize::try_from(part.len()).unwrap();\n+                let part_range = ruff_text_size::TextRange::new(part_start, part_end);\n+\n+                self.add_token(part_range, token_type, vec![]);\n+            }\n+            // Move past this part and the dot\n+            current_offset += part.len() + 1; // +1 for the dot\n+        }\n+    }\n+\n+    fn classify_from_alias_type(\n+        &self,\n+        ty: Type,\n+        local_name: &ast::Identifier,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        self.classify_from_type_and_name_str(ty, local_name.id.as_str())\n+    }\n+\n+    fn visit_type_annotation(&mut self, annotation: &ast::Expr) {\n+        let prev_in_type_annotation = self.in_type_annotation;\n+        self.in_type_annotation = true;\n+        self.visit_expr(annotation);\n+        self.in_type_annotation = prev_in_type_annotation;\n+    }\n+\n+    fn visit_type_params(&mut self, type_params: &TypeParams) {\n+        for type_param in &type_params.type_params {\n+            self.visit_type_param(type_param);\n+        }\n+    }\n+\n+    fn visit_type_param(&mut self, type_param: &TypeParam) {\n+        // Emit token for the type parameter name\n+        let name_range = type_param.name().range();\n+        self.add_token(\n+            name_range,\n+            SemanticTokenType::TypeParameter,\n+            vec![SemanticTokenModifier::Definition],\n+        );\n+\n+        // Visit bound expression (for TypeVar)\n+        match type_param {\n+            TypeParam::TypeVar(type_var) => {\n+                if let Some(bound) = &type_var.bound {\n+                    self.visit_type_annotation(bound);\n+                }\n+                if let Some(default) = &type_var.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+            TypeParam::ParamSpec(param_spec) => {\n+                if let Some(default) = &param_spec.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+            TypeParam::TypeVarTuple(type_var_tuple) => {\n+                if let Some(default) = &type_var_tuple.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Visit decorators, handling simple name decorators vs complex expressions\n+    fn visit_decorators(&mut self, decorators: &[ast::Decorator]) {\n+        for decorator in decorators {\n+            match &decorator.expression {\n+                ast::Expr::Name(name) => {\n+                    // Simple decorator like @staticmethod - use Decorator token type\n+                    self.add_token(name.range(), SemanticTokenType::Decorator, vec![]);\n+                }\n+                _ => {\n+                    // Complex decorator like @app.route(\"/path\") - use normal expression rules\n+                    self.visit_expr(&decorator.expression);\n+                }\n+            }\n+        }\n+    }\n+}\n+\n+#[allow(clippy::elidable_lifetime_names)]\n+impl<'db> SourceOrderVisitor<'_> for SemanticTokenVisitor<'db> {\n+    fn visit_stmt(&mut self, stmt: &Stmt) {\n+        // If we have a range filter and this statement doesn't intersect, skip it\n+        // as an optimization\n+        if stmt.range().intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        match stmt {\n+            ast::Stmt::FunctionDef(func) => {\n+                // Function name\n+                self.add_token(\n+                    func.name.range(),\n+                    if self.in_class_scope {\n+                        SemanticTokenType::Method\n+                    } else {\n+                        SemanticTokenType::Function\n+                    },\n+                    if func.is_async {\n+                        vec![\n+                            SemanticTokenModifier::Definition,\n+                            SemanticTokenModifier::Async,\n+                        ]\n+                    } else {\n+                        vec![SemanticTokenModifier::Definition]\n+                    },\n+                );\n+\n+                // Type parameters (Python 3.12+ syntax)\n+                if let Some(type_params) = &func.type_params {\n+                    self.visit_type_params(type_params);\n+                }\n+\n+                // Parameters\n+                for (i, param) in func.parameters.args.iter().enumerate() {\n+                    let token_type = self.classify_parameter(&param.parameter, i == 0, func);\n+                    self.add_token(param.parameter.name.range(), token_type, vec![]);\n+\n+                    // Handle parameter type annotations\n+                    if let Some(annotation) = &param.parameter.annotation {\n+                        self.visit_type_annotation(annotation);\n+                    }\n+                }\n+\n+                // Handle return type annotation\n+                if let Some(returns) = &func.returns {\n+                    self.visit_type_annotation(returns);\n+                }\n+\n+                // Visit decorator expressions\n+                self.visit_decorators(&func.decorator_list);\n+\n+                // Clear the in_class_scope flag so inner functions\n+                // are not treated as methods\n+                let prev_in_class = self.in_class_scope;\n+                self.in_class_scope = false;\n+                walk_stmt(self, stmt);\n+                self.in_class_scope = prev_in_class;\n+            }\n+            ast::Stmt::ClassDef(class) => {\n+                // Class name\n+                self.add_token(\n+                    class.name.range(),\n+                    SemanticTokenType::Class,\n+                    vec![SemanticTokenModifier::Definition],\n+                );\n+\n+                // Type parameters (Python 3.12+ syntax)\n+                if let Some(type_params) = &class.type_params {\n+                    self.visit_type_params(type_params);\n+                }\n+\n+                // Handle base classes and type annotations in inheritance\n+                if let Some(arguments) = &class.arguments {\n+                    // Visit base class arguments\n+                    for arg in &arguments.args {\n+                        self.visit_expr(arg);\n+                    }\n+                    // Visit keyword arguments (for metaclass, etc.)\n+                    for keyword in &arguments.keywords {\n+                        self.visit_expr(&keyword.value);\n+                    }\n+                }\n+\n+                // Visit decorator expressions\n+                self.visit_decorators(&class.decorator_list);\n+\n+                let prev_in_class = self.in_class_scope;\n+                self.in_class_scope = true;\n+                walk_stmt(self, stmt);\n+                self.in_class_scope = prev_in_class;\n+            }\n+            ast::Stmt::AnnAssign(assign) => {\n+                // Handle annotated assignments (e.g., x: int = 5)\n+                if let ast::Expr::Name(name) = assign.target.as_ref() {\n+                    let (token_type, modifiers) = self.classify_name(name);\n+                    self.add_token(name.range(), token_type, modifiers);\n+                }\n+\n+                // Handle the type annotation\n+                self.visit_type_annotation(&assign.annotation);\n+\n+                // Handle the value if present\n+                if let Some(value) = &assign.value {\n+                    self.visit_expr(value);\n+                }\n+            }\n+            ast::Stmt::Import(import) => {\n+                for alias in &import.names {\n+                    if let Some(asname) = &alias.asname {\n+                        self.add_token(asname.range(), SemanticTokenType::Namespace, vec![]);\n+                    } else {\n+                        // Create separate tokens for each part of a dotted module name\n+                        self.add_dotted_name_tokens(&alias.name, SemanticTokenType::Namespace);\n+                    }\n+                }\n+                walk_stmt(self, stmt);\n+            }\n+            ast::Stmt::ImportFrom(import) => {\n+                if let Some(module) = &import.module {\n+                    // Create separate tokens for each part of a dotted module name\n+                    self.add_dotted_name_tokens(module, SemanticTokenType::Namespace);\n+                }\n+                for alias in &import.names {\n+                    if let Some(asname) = &alias.asname {\n+                        // For aliased imports (from X import Y as Z), classify Z based on what Y is\n+                        let ty = alias.inferred_type(self.semantic_model);\n+                        let (token_type, modifiers) = self.classify_from_alias_type(ty, asname);\n+                        self.add_token(asname.range(), token_type, modifiers);\n+                    } else {\n+                        // For direct imports (from X import Y), use semantic classification\n+                        let ty = alias.inferred_type(self.semantic_model);\n+                        let (token_type, modifiers) =\n+                            self.classify_from_alias_type(ty, &alias.name);\n+                        self.add_token(alias.name.range(), token_type, modifiers);\n+                    }\n+                }\n+                walk_stmt(self, stmt);\n+            }\n+            _ => {\n+                // For all other statement types, let the default visitor handle them\n+                walk_stmt(self, stmt);\n+            }\n+        }\n+    }\n+\n+    fn visit_expr(&mut self, expr: &Expr) {\n+        // If we have a range filter and this statement doesn't intersect, skip it\n+        // as an optimization\n+        if expr.range().intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        match expr {\n+            ast::Expr::Name(name) => {\n+                let (token_type, modifiers) = self.classify_name(name);\n+                self.add_token(name.range(), token_type, modifiers);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::Attribute(attr) => {\n+                // Use semantic analysis to determine the correct token type for the attribute\n+                let ty = expr.inferred_type(self.semantic_model);\n+                let (token_type, modifiers) =\n+                    Self::classify_from_type_for_attribute(ty, &attr.attr);\n+\n+                self.add_token(attr.attr.range(), token_type, modifiers);\n+                // Continue visiting the base expression\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::Call(call) => {\n+                // The function being called\n+                if let ast::Expr::Name(name) = call.func.as_ref() {\n+                    self.add_token(name.range(), SemanticTokenType::Function, vec![]);\n+                }\n+                // Continue visiting arguments\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::StringLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::String, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::NumberLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::Number, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::BooleanLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::BuiltinConstant, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::NoneLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::BuiltinConstant, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            _ => {\n+                // For all other expression types, let the default visitor handle them\n+                walk_expr(self, expr);\n+            }\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+    use crate::tests::cursor_test;\n+    use ruff_db::source::source_text;\n+    use ruff_text_size::{TextLen, TextRange};\n+\n+    /// Helper function to get full file range for testing\n+    fn full_file_range(db: &dyn Db, file: File) -> TextRange {\n+        let source = source_text(db, file);\n+        TextRange::new(0.into(), source.text_len())\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_basic() {\n+        let test = cursor_test(\"def foo(): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+        assert!(!tokens.tokens.is_empty());\n+\n+        // Should have at least a function name token\n+        let function_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Function))\n+            .collect();\n+        assert!(!function_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_class() {\n+        let test = cursor_test(\"class MyClass: pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a class token\n+        let class_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Class))\n+            .collect();\n+        assert!(!class_tokens.is_empty());\n+    }",
        "comment_created_at": "2025-07-03T08:36:52+00:00",
        "comment_author": "dhruvmanila",
        "comment_body": "Should we instead assert the number of tokens because we know how many it's going to be for each test cases? Using the count also has the benefit that when it fails we would know what was the expected value and how many did we get when using `assert_eq!(class_tokens.len(), expected)`\r\n\r\nThis applies to other test cases as well.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2182223504",
    "pr_number": 19108,
    "pr_file": "crates/ty_ide/src/semantic_tokens.rs",
    "created_at": "2025-07-03T08:46:35+00:00",
    "commented_code": "+use crate::Db;\n+use ruff_db::files::File;\n+use ruff_db::parsed::parsed_module;\n+use ruff_python_ast as ast;\n+use ruff_python_ast::visitor::source_order::{SourceOrderVisitor, walk_expr, walk_stmt};\n+use ruff_python_ast::{Expr, Stmt, TypeParam, TypeParams};\n+use ruff_text_size::{Ranged, TextRange};\n+use ty_python_semantic::{HasType, SemanticModel, types::Type};\n+\n+// This module walks the AST and collects a set of \"semantic tokens\" for a file\n+// or a range within a file. Each semantic token provides a \"token type\" and zero\n+// or more \"modifiers\". This information can be used by an editor to provide\n+// color coding based on semantic meaning.\n+\n+// Current limitations and areas for future improvement:\n+\n+// TODO: Need to handle semantic tokens within quoted annotations.\n+\n+// TODO: Need to properly handle Annotated expressions. All type arguments other\n+// than the first should be treated as value expressions, not as type expressions.\n+\n+// TODO: An identifier that resolves to a parameter when used within a function\n+// should be classified as a parameter, selfParameter, or clsParameter token.\n+\n+// TODO: Properties (or perhaps more generally, descriptor objects?) should be\n+// classified as property tokens rather than just variables.\n+\n+// TODO: Special forms like Protocol and TypedDict should probably be classified\n+// as class tokens, but they are currently classified as variables.\n+\n+// TODO: Type aliases (including those defined with the Python 3.12 \"type\" statement)\n+// do not currently have a dedicated semantic token type, but they maybe should.\n+\n+// TODO: Additional token modifiers might be added (e.g. for static methods,\n+// abstract methods and classes).\n+\n+/// Semantic token types supported by the language server.\n+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]\n+#[repr(u32)]\n+pub enum SemanticTokenType {\n+    Namespace = 0,\n+    Class = 1,\n+    Parameter = 2,\n+    SelfParameter = 3,\n+    ClsParameter = 4,\n+    Variable = 5,\n+    Property = 6,\n+    Function = 7,\n+    Method = 8,\n+    Keyword = 9,\n+    String = 10,\n+    Number = 11,\n+    Decorator = 12,\n+    BuiltinConstant = 13,\n+    TypeParameter = 14,\n+}\n+\n+impl SemanticTokenType {\n+    /// Returns all supported token types for LSP capabilities.\n+    pub fn all() -> Vec<&'static str> {\n+        vec![\n+            \"namespace\",\n+            \"class\",\n+            \"parameter\",\n+            \"selfParameter\",\n+            \"clsParameter\",\n+            \"variable\",\n+            \"property\",\n+            \"function\",\n+            \"method\",\n+            \"keyword\",\n+            \"string\",\n+            \"number\",\n+            \"decorator\",\n+            \"builtinConstant\",\n+            \"typeParameter\",\n+        ]\n+    }\n+}\n+\n+/// Semantic token modifiers.\n+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]\n+#[repr(u32)]\n+pub enum SemanticTokenModifier {\n+    Definition = 0,\n+    Readonly = 1,\n+    Async = 2,\n+}\n+\n+impl SemanticTokenModifier {\n+    /// Returns all supported token modifiers for LSP capabilities.\n+    pub fn all() -> Vec<&'static str> {\n+        vec![\"definition\", \"readonly\", \"async\"]\n+    }\n+}\n+\n+/// A semantic token with its position and classification.\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct SemanticToken {\n+    pub range: TextRange,\n+    pub token_type: SemanticTokenType,\n+    pub modifiers: Vec<SemanticTokenModifier>,\n+}\n+\n+/// The result of semantic tokenization.\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct SemanticTokens {\n+    pub tokens: Vec<SemanticToken>,\n+}\n+\n+/// Generates semantic tokens for a Python file within the specified range.\n+/// Pass a full file range to get tokens for the entire file.\n+pub fn semantic_tokens(db: &dyn Db, file: File, range: TextRange) -> Option<SemanticTokens> {\n+    let parsed = parsed_module(db, file).load(db);\n+    let semantic_model = SemanticModel::new(db, file);\n+\n+    let mut visitor = SemanticTokenVisitor::new(db, &semantic_model, range);\n+    visitor.visit_body(parsed.suite());\n+\n+    Some(SemanticTokens {\n+        tokens: visitor.tokens,\n+    })\n+}\n+\n+/// AST visitor that collects semantic tokens.\n+struct SemanticTokenVisitor<'db> {\n+    #[allow(dead_code)]\n+    db: &'db dyn Db,\n+    #[allow(dead_code)]\n+    semantic_model: &'db SemanticModel<'db>,\n+    tokens: Vec<SemanticToken>,\n+    in_class_scope: bool,\n+    in_type_annotation: bool,\n+    range_filter: TextRange,\n+}\n+\n+impl<'db> SemanticTokenVisitor<'db> {\n+    fn new(\n+        db: &'db dyn Db,\n+        semantic_model: &'db SemanticModel<'db>,\n+        range_filter: TextRange,\n+    ) -> Self {\n+        Self {\n+            db,\n+            semantic_model,\n+            tokens: Vec::new(),\n+            in_class_scope: false,\n+            in_type_annotation: false,\n+            range_filter,\n+        }\n+    }\n+\n+    fn add_token(\n+        &mut self,\n+        range: TextRange,\n+        token_type: SemanticTokenType,\n+        modifiers: Vec<SemanticTokenModifier>,\n+    ) {\n+        // Only emit tokens that intersect with the range filter\n+        if range.intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        self.tokens.push(SemanticToken {\n+            range,\n+            token_type,\n+            modifiers,\n+        });\n+    }\n+\n+    fn is_constant_name(name: &str) -> bool {\n+        name.chars().all(|c| c.is_uppercase() || c == '_') && name.len() > 1\n+    }\n+\n+    fn classify_name(\n+        &self,\n+        name: &ast::ExprName,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        // Try to get the inferred type of this name expression using semantic analysis\n+        let ty = name.inferred_type(self.semantic_model);\n+        let name_str = name.id.as_str();\n+        self.classify_from_type_and_name_str(ty, name_str)\n+    }\n+\n+    fn classify_from_type_and_name_str(\n+        &self,\n+        ty: Type,\n+        name_str: &str,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        let mut modifiers = vec![];\n+\n+        // In type annotation contexts, names that refer to nominal instances or protocol instances\n+        // should be classified as Class tokens (e.g., \"int\" in \"x: int\" should be a Class token)\n+        if self.in_type_annotation {\n+            match ty {\n+                Type::NominalInstance(_) | Type::ProtocolInstance(_) => {\n+                    return (SemanticTokenType::Class, modifiers);\n+                }\n+                _ => {\n+                    // Continue with normal classification for other types in annotations\n+                }\n+            }\n+        }\n+\n+        match ty {\n+            Type::ClassLiteral(_) => (SemanticTokenType::Class, modifiers),\n+            Type::TypeVar(_) => (SemanticTokenType::TypeParameter, modifiers),\n+            _ if ty.is_function_literal() => {\n+                // Check if this is a method based on current scope\n+                if self.in_class_scope {\n+                    (SemanticTokenType::Method, modifiers)\n+                } else {\n+                    (SemanticTokenType::Function, modifiers)\n+                }\n+            }\n+            _ if ty.is_bound_method() => (SemanticTokenType::Method, modifiers),\n+            _ if ty.into_module_literal().is_some() => (SemanticTokenType::Namespace, modifiers),\n+            _ => {\n+                // Check for constant naming convention\n+                if Self::is_constant_name(name_str) {\n+                    modifiers.push(SemanticTokenModifier::Readonly);\n+                }\n+                // For other types (variables, modules, etc.), assume variable\n+                (SemanticTokenType::Variable, modifiers)\n+            }\n+        }\n+    }\n+\n+    fn classify_from_type_for_attribute(\n+        ty: Type,\n+        attr_name: &ast::Identifier,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        let attr_name_str = attr_name.id.as_str();\n+        let mut modifiers = vec![];\n+\n+        // Classify based on the inferred type of the attribute\n+        if ty.is_class_literal() {\n+            (SemanticTokenType::Class, modifiers)\n+        } else if ty.is_function_literal() {\n+            // This is a function accessed as an attribute, likely a method\n+            (SemanticTokenType::Method, modifiers)\n+        } else if ty.is_bound_method() {\n+            // Method bound to an instance\n+            (SemanticTokenType::Method, modifiers)\n+        } else if ty.into_module_literal().is_some() {\n+            // Module accessed as an attribute (e.g., from os import path)\n+            (SemanticTokenType::Namespace, modifiers)\n+        } else if ty.is_property_instance() {\n+            // Actual Python property\n+            (SemanticTokenType::Property, modifiers)\n+        } else {\n+            // Check for constant naming convention\n+            if Self::is_constant_name(attr_name_str) {\n+                modifiers.push(SemanticTokenModifier::Readonly);\n+            }\n+\n+            // For other types (variables, constants, etc.), classify as variable\n+            (SemanticTokenType::Variable, modifiers)\n+        }\n+    }\n+\n+    fn classify_parameter(\n+        &self,\n+        _param: &ast::Parameter,\n+        is_first: bool,\n+        func: &ast::StmtFunctionDef,\n+    ) -> SemanticTokenType {\n+        if is_first && self.in_class_scope {\n+            // Check if this is a classmethod (has @classmethod decorator)\n+            let is_classmethod =\n+                func.decorator_list\n+                    .iter()\n+                    .any(|decorator| match &decorator.expression {\n+                        ast::Expr::Name(name) => name.id.as_str() == \"classmethod\",\n+                        ast::Expr::Attribute(attr) => attr.attr.id.as_str() == \"classmethod\",\n+                        _ => false,\n+                    });\n+\n+            // Check if this is a staticmethod (has @staticmethod decorator)\n+            let is_staticmethod =\n+                func.decorator_list\n+                    .iter()\n+                    .any(|decorator| match &decorator.expression {\n+                        ast::Expr::Name(name) => name.id.as_str() == \"staticmethod\",\n+                        ast::Expr::Attribute(attr) => attr.attr.id.as_str() == \"staticmethod\",\n+                        _ => false,\n+                    });\n+\n+            if is_staticmethod {\n+                // Static methods don't have self/cls parameters\n+                SemanticTokenType::Parameter\n+            } else if is_classmethod {\n+                // First parameter of a classmethod is cls parameter\n+                SemanticTokenType::ClsParameter\n+            } else {\n+                // First parameter of an instance method is self parameter\n+                SemanticTokenType::SelfParameter\n+            }\n+        } else {\n+            SemanticTokenType::Parameter\n+        }\n+    }\n+\n+    fn add_dotted_name_tokens(&mut self, name: &ast::Identifier, token_type: SemanticTokenType) {\n+        let name_str = name.id.as_str();\n+        let name_range = name.range();\n+        let name_start = name_range.start();\n+\n+        // Split the dotted name and calculate positions for each part\n+        let mut current_offset = 0usize;\n+        for part in name_str.split('.') {\n+            if !part.is_empty() {\n+                let part_start =\n+                    name_start + ruff_text_size::TextSize::try_from(current_offset).unwrap();\n+                let part_end = part_start + ruff_text_size::TextSize::try_from(part.len()).unwrap();\n+                let part_range = ruff_text_size::TextRange::new(part_start, part_end);\n+\n+                self.add_token(part_range, token_type, vec![]);\n+            }\n+            // Move past this part and the dot\n+            current_offset += part.len() + 1; // +1 for the dot\n+        }\n+    }\n+\n+    fn classify_from_alias_type(\n+        &self,\n+        ty: Type,\n+        local_name: &ast::Identifier,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        self.classify_from_type_and_name_str(ty, local_name.id.as_str())\n+    }\n+\n+    fn visit_type_annotation(&mut self, annotation: &ast::Expr) {\n+        let prev_in_type_annotation = self.in_type_annotation;\n+        self.in_type_annotation = true;\n+        self.visit_expr(annotation);\n+        self.in_type_annotation = prev_in_type_annotation;\n+    }\n+\n+    fn visit_type_params(&mut self, type_params: &TypeParams) {\n+        for type_param in &type_params.type_params {\n+            self.visit_type_param(type_param);\n+        }\n+    }\n+\n+    fn visit_type_param(&mut self, type_param: &TypeParam) {\n+        // Emit token for the type parameter name\n+        let name_range = type_param.name().range();\n+        self.add_token(\n+            name_range,\n+            SemanticTokenType::TypeParameter,\n+            vec![SemanticTokenModifier::Definition],\n+        );\n+\n+        // Visit bound expression (for TypeVar)\n+        match type_param {\n+            TypeParam::TypeVar(type_var) => {\n+                if let Some(bound) = &type_var.bound {\n+                    self.visit_type_annotation(bound);\n+                }\n+                if let Some(default) = &type_var.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+            TypeParam::ParamSpec(param_spec) => {\n+                if let Some(default) = &param_spec.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+            TypeParam::TypeVarTuple(type_var_tuple) => {\n+                if let Some(default) = &type_var_tuple.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Visit decorators, handling simple name decorators vs complex expressions\n+    fn visit_decorators(&mut self, decorators: &[ast::Decorator]) {\n+        for decorator in decorators {\n+            match &decorator.expression {\n+                ast::Expr::Name(name) => {\n+                    // Simple decorator like @staticmethod - use Decorator token type\n+                    self.add_token(name.range(), SemanticTokenType::Decorator, vec![]);\n+                }\n+                _ => {\n+                    // Complex decorator like @app.route(\"/path\") - use normal expression rules\n+                    self.visit_expr(&decorator.expression);\n+                }\n+            }\n+        }\n+    }\n+}\n+\n+#[allow(clippy::elidable_lifetime_names)]\n+impl<'db> SourceOrderVisitor<'_> for SemanticTokenVisitor<'db> {\n+    fn visit_stmt(&mut self, stmt: &Stmt) {\n+        // If we have a range filter and this statement doesn't intersect, skip it\n+        // as an optimization\n+        if stmt.range().intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        match stmt {\n+            ast::Stmt::FunctionDef(func) => {\n+                // Function name\n+                self.add_token(\n+                    func.name.range(),\n+                    if self.in_class_scope {\n+                        SemanticTokenType::Method\n+                    } else {\n+                        SemanticTokenType::Function\n+                    },\n+                    if func.is_async {\n+                        vec![\n+                            SemanticTokenModifier::Definition,\n+                            SemanticTokenModifier::Async,\n+                        ]\n+                    } else {\n+                        vec![SemanticTokenModifier::Definition]\n+                    },\n+                );\n+\n+                // Type parameters (Python 3.12+ syntax)\n+                if let Some(type_params) = &func.type_params {\n+                    self.visit_type_params(type_params);\n+                }\n+\n+                // Parameters\n+                for (i, param) in func.parameters.args.iter().enumerate() {\n+                    let token_type = self.classify_parameter(&param.parameter, i == 0, func);\n+                    self.add_token(param.parameter.name.range(), token_type, vec![]);\n+\n+                    // Handle parameter type annotations\n+                    if let Some(annotation) = &param.parameter.annotation {\n+                        self.visit_type_annotation(annotation);\n+                    }\n+                }\n+\n+                // Handle return type annotation\n+                if let Some(returns) = &func.returns {\n+                    self.visit_type_annotation(returns);\n+                }\n+\n+                // Visit decorator expressions\n+                self.visit_decorators(&func.decorator_list);\n+\n+                // Clear the in_class_scope flag so inner functions\n+                // are not treated as methods\n+                let prev_in_class = self.in_class_scope;\n+                self.in_class_scope = false;\n+                walk_stmt(self, stmt);\n+                self.in_class_scope = prev_in_class;\n+            }\n+            ast::Stmt::ClassDef(class) => {\n+                // Class name\n+                self.add_token(\n+                    class.name.range(),\n+                    SemanticTokenType::Class,\n+                    vec![SemanticTokenModifier::Definition],\n+                );\n+\n+                // Type parameters (Python 3.12+ syntax)\n+                if let Some(type_params) = &class.type_params {\n+                    self.visit_type_params(type_params);\n+                }\n+\n+                // Handle base classes and type annotations in inheritance\n+                if let Some(arguments) = &class.arguments {\n+                    // Visit base class arguments\n+                    for arg in &arguments.args {\n+                        self.visit_expr(arg);\n+                    }\n+                    // Visit keyword arguments (for metaclass, etc.)\n+                    for keyword in &arguments.keywords {\n+                        self.visit_expr(&keyword.value);\n+                    }\n+                }\n+\n+                // Visit decorator expressions\n+                self.visit_decorators(&class.decorator_list);\n+\n+                let prev_in_class = self.in_class_scope;\n+                self.in_class_scope = true;\n+                walk_stmt(self, stmt);\n+                self.in_class_scope = prev_in_class;\n+            }\n+            ast::Stmt::AnnAssign(assign) => {\n+                // Handle annotated assignments (e.g., x: int = 5)\n+                if let ast::Expr::Name(name) = assign.target.as_ref() {\n+                    let (token_type, modifiers) = self.classify_name(name);\n+                    self.add_token(name.range(), token_type, modifiers);\n+                }\n+\n+                // Handle the type annotation\n+                self.visit_type_annotation(&assign.annotation);\n+\n+                // Handle the value if present\n+                if let Some(value) = &assign.value {\n+                    self.visit_expr(value);\n+                }\n+            }\n+            ast::Stmt::Import(import) => {\n+                for alias in &import.names {\n+                    if let Some(asname) = &alias.asname {\n+                        self.add_token(asname.range(), SemanticTokenType::Namespace, vec![]);\n+                    } else {\n+                        // Create separate tokens for each part of a dotted module name\n+                        self.add_dotted_name_tokens(&alias.name, SemanticTokenType::Namespace);\n+                    }\n+                }\n+                walk_stmt(self, stmt);\n+            }\n+            ast::Stmt::ImportFrom(import) => {\n+                if let Some(module) = &import.module {\n+                    // Create separate tokens for each part of a dotted module name\n+                    self.add_dotted_name_tokens(module, SemanticTokenType::Namespace);\n+                }\n+                for alias in &import.names {\n+                    if let Some(asname) = &alias.asname {\n+                        // For aliased imports (from X import Y as Z), classify Z based on what Y is\n+                        let ty = alias.inferred_type(self.semantic_model);\n+                        let (token_type, modifiers) = self.classify_from_alias_type(ty, asname);\n+                        self.add_token(asname.range(), token_type, modifiers);\n+                    } else {\n+                        // For direct imports (from X import Y), use semantic classification\n+                        let ty = alias.inferred_type(self.semantic_model);\n+                        let (token_type, modifiers) =\n+                            self.classify_from_alias_type(ty, &alias.name);\n+                        self.add_token(alias.name.range(), token_type, modifiers);\n+                    }\n+                }\n+                walk_stmt(self, stmt);\n+            }\n+            _ => {\n+                // For all other statement types, let the default visitor handle them\n+                walk_stmt(self, stmt);\n+            }\n+        }\n+    }\n+\n+    fn visit_expr(&mut self, expr: &Expr) {\n+        // If we have a range filter and this statement doesn't intersect, skip it\n+        // as an optimization\n+        if expr.range().intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        match expr {\n+            ast::Expr::Name(name) => {\n+                let (token_type, modifiers) = self.classify_name(name);\n+                self.add_token(name.range(), token_type, modifiers);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::Attribute(attr) => {\n+                // Use semantic analysis to determine the correct token type for the attribute\n+                let ty = expr.inferred_type(self.semantic_model);\n+                let (token_type, modifiers) =\n+                    Self::classify_from_type_for_attribute(ty, &attr.attr);\n+\n+                self.add_token(attr.attr.range(), token_type, modifiers);\n+                // Continue visiting the base expression\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::Call(call) => {\n+                // The function being called\n+                if let ast::Expr::Name(name) = call.func.as_ref() {\n+                    self.add_token(name.range(), SemanticTokenType::Function, vec![]);\n+                }\n+                // Continue visiting arguments\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::StringLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::String, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::NumberLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::Number, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::BooleanLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::BuiltinConstant, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::NoneLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::BuiltinConstant, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            _ => {\n+                // For all other expression types, let the default visitor handle them\n+                walk_expr(self, expr);\n+            }\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+    use crate::tests::cursor_test;\n+    use ruff_db::source::source_text;\n+    use ruff_text_size::{TextLen, TextRange};\n+\n+    /// Helper function to get full file range for testing\n+    fn full_file_range(db: &dyn Db, file: File) -> TextRange {\n+        let source = source_text(db, file);\n+        TextRange::new(0.into(), source.text_len())\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_basic() {\n+        let test = cursor_test(\"def foo(): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+        assert!(!tokens.tokens.is_empty());\n+\n+        // Should have at least a function name token\n+        let function_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Function))\n+            .collect();\n+        assert!(!function_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_class() {\n+        let test = cursor_test(\"class MyClass: pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a class token\n+        let class_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Class))\n+            .collect();\n+        assert!(!class_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_variables() {\n+        let test = cursor_test(\"x = 42\ny = 'hello'<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have variable tokens\n+        let variable_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Variable))\n+            .collect();\n+        assert!(variable_tokens.len() >= 2);\n+\n+        // Should have number and string tokens\n+        let number_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Number))\n+            .collect();\n+        assert!(!number_tokens.is_empty());\n+\n+        let string_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::String))\n+            .collect();\n+        assert!(!string_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_self_parameter() {\n+        let test = cursor_test(\"class MyClass:\n    def method(self, x): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a self parameter token\n+        let self_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::SelfParameter))\n+            .collect();\n+        assert!(!self_tokens.is_empty());\n+\n+        // Should have a regular parameter token for x\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(!param_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_cls_parameter() {\n+        let test =\n+            cursor_test(\"class MyClass:\n    @classmethod\n    def method(cls, x): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a cls parameter token\n+        let cls_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::ClsParameter))\n+            .collect();\n+        assert!(!cls_tokens.is_empty());\n+\n+        // Should have a regular parameter token for x\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(!param_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_staticmethod_parameter() {\n+        let test =\n+            cursor_test(\"class MyClass:\n    @staticmethod\n    def method(x, y): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have only regular parameter tokens (no self/cls)\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(param_tokens.len() >= 2);\n+\n+        // Should not have self or cls parameter tokens\n+        let self_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::SelfParameter))\n+            .collect();\n+        assert!(self_tokens.is_empty());\n+\n+        let cls_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::ClsParameter))\n+            .collect();\n+        assert!(cls_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_custom_self_cls_names() {\n+        let test = cursor_test(\n+            \"class MyClass:\n    def method(instance, x): pass\n    @classmethod\n    def other(klass, y): pass<CURSOR>\",\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a self parameter token for \"instance\"\n+        let self_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::SelfParameter))\n+            .collect();\n+        assert!(!self_tokens.is_empty());\n+\n+        // Should have a cls parameter token for \"klass\"\n+        let cls_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::ClsParameter))\n+            .collect();\n+        assert!(!cls_tokens.is_empty());\n+\n+        // Should have regular parameter tokens for x and y\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(param_tokens.len() >= 2);\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_modifiers() {\n+        let test = cursor_test(\n+            \"class MyClass:\n    CONSTANT = 42\n    async def method(self): pass<CURSOR>\",\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a class token with Definition modifier\n+        let class_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Class))\n+            .collect();\n+        assert!(!class_tokens.is_empty());\n+        assert!(\n+            class_tokens[0]\n+                .modifiers\n+                .contains(&SemanticTokenModifier::Definition)\n+        );\n+\n+        // Should have a constant with Readonly modifier\n+        let constant_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                matches!(\n+                    t.token_type,\n+                    SemanticTokenType::Property | SemanticTokenType::Variable\n+                ) && t.modifiers.contains(&SemanticTokenModifier::Readonly)\n+            })\n+            .collect();\n+        assert!(!constant_tokens.is_empty());\n+\n+        // Should have an async method with Async modifier\n+        let async_method_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                matches!(t.token_type, SemanticTokenType::Method)\n+                    && t.modifiers.contains(&SemanticTokenModifier::Async)\n+            })\n+            .collect();\n+        assert!(!async_method_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_classification_vs_heuristic() {\n+        let test = cursor_test(\n+            r#\"\n+import sys\n+class MyClass:\n+    pass\n+\n+def my_function():\n+    return 42\n+\n+x = MyClass()\n+y = my_function()\n+z = sys.version<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have module tokens for imports\n+        let module_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Namespace))\n+            .collect();\n+        assert!(!module_tokens.is_empty());\n+\n+        // Should have class tokens\n+        let class_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Class))\n+            .collect();\n+        assert!(!class_tokens.is_empty());\n+\n+        // Should have function tokens\n+        let function_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Function))\n+            .collect();\n+        assert!(!function_tokens.is_empty());\n+\n+        // Should have variable tokens for assignments\n+        let variable_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Variable))\n+            .collect();\n+        assert!(variable_tokens.len() >= 3); // x, y, z\n+    }\n+\n+    #[test]\n+    fn test_builtin_constants() {\n+        let test = cursor_test(\"x = True\ny = False\nz = None<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have builtin constant tokens for True, False, and None\n+        let builtin_constant_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::BuiltinConstant))\n+            .collect();\n+        assert_eq!(builtin_constant_tokens.len(), 3);\n+\n+        // Should have variable tokens for x, y, z\n+        let variable_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Variable))\n+            .collect();\n+        assert_eq!(variable_tokens.len(), 3);\n+    }",
    "repo_full_name": "astral-sh/ruff",
    "discussion_comments": [
      {
        "comment_id": "2182223504",
        "repo_full_name": "astral-sh/ruff",
        "pr_number": 19108,
        "pr_file": "crates/ty_ide/src/semantic_tokens.rs",
        "discussion_id": "2182223504",
        "commented_code": "@@ -0,0 +1,2531 @@\n+use crate::Db;\n+use ruff_db::files::File;\n+use ruff_db::parsed::parsed_module;\n+use ruff_python_ast as ast;\n+use ruff_python_ast::visitor::source_order::{SourceOrderVisitor, walk_expr, walk_stmt};\n+use ruff_python_ast::{Expr, Stmt, TypeParam, TypeParams};\n+use ruff_text_size::{Ranged, TextRange};\n+use ty_python_semantic::{HasType, SemanticModel, types::Type};\n+\n+// This module walks the AST and collects a set of \"semantic tokens\" for a file\n+// or a range within a file. Each semantic token provides a \"token type\" and zero\n+// or more \"modifiers\". This information can be used by an editor to provide\n+// color coding based on semantic meaning.\n+\n+// Current limitations and areas for future improvement:\n+\n+// TODO: Need to handle semantic tokens within quoted annotations.\n+\n+// TODO: Need to properly handle Annotated expressions. All type arguments other\n+// than the first should be treated as value expressions, not as type expressions.\n+\n+// TODO: An identifier that resolves to a parameter when used within a function\n+// should be classified as a parameter, selfParameter, or clsParameter token.\n+\n+// TODO: Properties (or perhaps more generally, descriptor objects?) should be\n+// classified as property tokens rather than just variables.\n+\n+// TODO: Special forms like Protocol and TypedDict should probably be classified\n+// as class tokens, but they are currently classified as variables.\n+\n+// TODO: Type aliases (including those defined with the Python 3.12 \"type\" statement)\n+// do not currently have a dedicated semantic token type, but they maybe should.\n+\n+// TODO: Additional token modifiers might be added (e.g. for static methods,\n+// abstract methods and classes).\n+\n+/// Semantic token types supported by the language server.\n+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]\n+#[repr(u32)]\n+pub enum SemanticTokenType {\n+    Namespace = 0,\n+    Class = 1,\n+    Parameter = 2,\n+    SelfParameter = 3,\n+    ClsParameter = 4,\n+    Variable = 5,\n+    Property = 6,\n+    Function = 7,\n+    Method = 8,\n+    Keyword = 9,\n+    String = 10,\n+    Number = 11,\n+    Decorator = 12,\n+    BuiltinConstant = 13,\n+    TypeParameter = 14,\n+}\n+\n+impl SemanticTokenType {\n+    /// Returns all supported token types for LSP capabilities.\n+    pub fn all() -> Vec<&'static str> {\n+        vec![\n+            \"namespace\",\n+            \"class\",\n+            \"parameter\",\n+            \"selfParameter\",\n+            \"clsParameter\",\n+            \"variable\",\n+            \"property\",\n+            \"function\",\n+            \"method\",\n+            \"keyword\",\n+            \"string\",\n+            \"number\",\n+            \"decorator\",\n+            \"builtinConstant\",\n+            \"typeParameter\",\n+        ]\n+    }\n+}\n+\n+/// Semantic token modifiers.\n+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]\n+#[repr(u32)]\n+pub enum SemanticTokenModifier {\n+    Definition = 0,\n+    Readonly = 1,\n+    Async = 2,\n+}\n+\n+impl SemanticTokenModifier {\n+    /// Returns all supported token modifiers for LSP capabilities.\n+    pub fn all() -> Vec<&'static str> {\n+        vec![\"definition\", \"readonly\", \"async\"]\n+    }\n+}\n+\n+/// A semantic token with its position and classification.\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct SemanticToken {\n+    pub range: TextRange,\n+    pub token_type: SemanticTokenType,\n+    pub modifiers: Vec<SemanticTokenModifier>,\n+}\n+\n+/// The result of semantic tokenization.\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct SemanticTokens {\n+    pub tokens: Vec<SemanticToken>,\n+}\n+\n+/// Generates semantic tokens for a Python file within the specified range.\n+/// Pass a full file range to get tokens for the entire file.\n+pub fn semantic_tokens(db: &dyn Db, file: File, range: TextRange) -> Option<SemanticTokens> {\n+    let parsed = parsed_module(db, file).load(db);\n+    let semantic_model = SemanticModel::new(db, file);\n+\n+    let mut visitor = SemanticTokenVisitor::new(db, &semantic_model, range);\n+    visitor.visit_body(parsed.suite());\n+\n+    Some(SemanticTokens {\n+        tokens: visitor.tokens,\n+    })\n+}\n+\n+/// AST visitor that collects semantic tokens.\n+struct SemanticTokenVisitor<'db> {\n+    #[allow(dead_code)]\n+    db: &'db dyn Db,\n+    #[allow(dead_code)]\n+    semantic_model: &'db SemanticModel<'db>,\n+    tokens: Vec<SemanticToken>,\n+    in_class_scope: bool,\n+    in_type_annotation: bool,\n+    range_filter: TextRange,\n+}\n+\n+impl<'db> SemanticTokenVisitor<'db> {\n+    fn new(\n+        db: &'db dyn Db,\n+        semantic_model: &'db SemanticModel<'db>,\n+        range_filter: TextRange,\n+    ) -> Self {\n+        Self {\n+            db,\n+            semantic_model,\n+            tokens: Vec::new(),\n+            in_class_scope: false,\n+            in_type_annotation: false,\n+            range_filter,\n+        }\n+    }\n+\n+    fn add_token(\n+        &mut self,\n+        range: TextRange,\n+        token_type: SemanticTokenType,\n+        modifiers: Vec<SemanticTokenModifier>,\n+    ) {\n+        // Only emit tokens that intersect with the range filter\n+        if range.intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        self.tokens.push(SemanticToken {\n+            range,\n+            token_type,\n+            modifiers,\n+        });\n+    }\n+\n+    fn is_constant_name(name: &str) -> bool {\n+        name.chars().all(|c| c.is_uppercase() || c == '_') && name.len() > 1\n+    }\n+\n+    fn classify_name(\n+        &self,\n+        name: &ast::ExprName,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        // Try to get the inferred type of this name expression using semantic analysis\n+        let ty = name.inferred_type(self.semantic_model);\n+        let name_str = name.id.as_str();\n+        self.classify_from_type_and_name_str(ty, name_str)\n+    }\n+\n+    fn classify_from_type_and_name_str(\n+        &self,\n+        ty: Type,\n+        name_str: &str,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        let mut modifiers = vec![];\n+\n+        // In type annotation contexts, names that refer to nominal instances or protocol instances\n+        // should be classified as Class tokens (e.g., \"int\" in \"x: int\" should be a Class token)\n+        if self.in_type_annotation {\n+            match ty {\n+                Type::NominalInstance(_) | Type::ProtocolInstance(_) => {\n+                    return (SemanticTokenType::Class, modifiers);\n+                }\n+                _ => {\n+                    // Continue with normal classification for other types in annotations\n+                }\n+            }\n+        }\n+\n+        match ty {\n+            Type::ClassLiteral(_) => (SemanticTokenType::Class, modifiers),\n+            Type::TypeVar(_) => (SemanticTokenType::TypeParameter, modifiers),\n+            _ if ty.is_function_literal() => {\n+                // Check if this is a method based on current scope\n+                if self.in_class_scope {\n+                    (SemanticTokenType::Method, modifiers)\n+                } else {\n+                    (SemanticTokenType::Function, modifiers)\n+                }\n+            }\n+            _ if ty.is_bound_method() => (SemanticTokenType::Method, modifiers),\n+            _ if ty.into_module_literal().is_some() => (SemanticTokenType::Namespace, modifiers),\n+            _ => {\n+                // Check for constant naming convention\n+                if Self::is_constant_name(name_str) {\n+                    modifiers.push(SemanticTokenModifier::Readonly);\n+                }\n+                // For other types (variables, modules, etc.), assume variable\n+                (SemanticTokenType::Variable, modifiers)\n+            }\n+        }\n+    }\n+\n+    fn classify_from_type_for_attribute(\n+        ty: Type,\n+        attr_name: &ast::Identifier,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        let attr_name_str = attr_name.id.as_str();\n+        let mut modifiers = vec![];\n+\n+        // Classify based on the inferred type of the attribute\n+        if ty.is_class_literal() {\n+            (SemanticTokenType::Class, modifiers)\n+        } else if ty.is_function_literal() {\n+            // This is a function accessed as an attribute, likely a method\n+            (SemanticTokenType::Method, modifiers)\n+        } else if ty.is_bound_method() {\n+            // Method bound to an instance\n+            (SemanticTokenType::Method, modifiers)\n+        } else if ty.into_module_literal().is_some() {\n+            // Module accessed as an attribute (e.g., from os import path)\n+            (SemanticTokenType::Namespace, modifiers)\n+        } else if ty.is_property_instance() {\n+            // Actual Python property\n+            (SemanticTokenType::Property, modifiers)\n+        } else {\n+            // Check for constant naming convention\n+            if Self::is_constant_name(attr_name_str) {\n+                modifiers.push(SemanticTokenModifier::Readonly);\n+            }\n+\n+            // For other types (variables, constants, etc.), classify as variable\n+            (SemanticTokenType::Variable, modifiers)\n+        }\n+    }\n+\n+    fn classify_parameter(\n+        &self,\n+        _param: &ast::Parameter,\n+        is_first: bool,\n+        func: &ast::StmtFunctionDef,\n+    ) -> SemanticTokenType {\n+        if is_first && self.in_class_scope {\n+            // Check if this is a classmethod (has @classmethod decorator)\n+            let is_classmethod =\n+                func.decorator_list\n+                    .iter()\n+                    .any(|decorator| match &decorator.expression {\n+                        ast::Expr::Name(name) => name.id.as_str() == \"classmethod\",\n+                        ast::Expr::Attribute(attr) => attr.attr.id.as_str() == \"classmethod\",\n+                        _ => false,\n+                    });\n+\n+            // Check if this is a staticmethod (has @staticmethod decorator)\n+            let is_staticmethod =\n+                func.decorator_list\n+                    .iter()\n+                    .any(|decorator| match &decorator.expression {\n+                        ast::Expr::Name(name) => name.id.as_str() == \"staticmethod\",\n+                        ast::Expr::Attribute(attr) => attr.attr.id.as_str() == \"staticmethod\",\n+                        _ => false,\n+                    });\n+\n+            if is_staticmethod {\n+                // Static methods don't have self/cls parameters\n+                SemanticTokenType::Parameter\n+            } else if is_classmethod {\n+                // First parameter of a classmethod is cls parameter\n+                SemanticTokenType::ClsParameter\n+            } else {\n+                // First parameter of an instance method is self parameter\n+                SemanticTokenType::SelfParameter\n+            }\n+        } else {\n+            SemanticTokenType::Parameter\n+        }\n+    }\n+\n+    fn add_dotted_name_tokens(&mut self, name: &ast::Identifier, token_type: SemanticTokenType) {\n+        let name_str = name.id.as_str();\n+        let name_range = name.range();\n+        let name_start = name_range.start();\n+\n+        // Split the dotted name and calculate positions for each part\n+        let mut current_offset = 0usize;\n+        for part in name_str.split('.') {\n+            if !part.is_empty() {\n+                let part_start =\n+                    name_start + ruff_text_size::TextSize::try_from(current_offset).unwrap();\n+                let part_end = part_start + ruff_text_size::TextSize::try_from(part.len()).unwrap();\n+                let part_range = ruff_text_size::TextRange::new(part_start, part_end);\n+\n+                self.add_token(part_range, token_type, vec![]);\n+            }\n+            // Move past this part and the dot\n+            current_offset += part.len() + 1; // +1 for the dot\n+        }\n+    }\n+\n+    fn classify_from_alias_type(\n+        &self,\n+        ty: Type,\n+        local_name: &ast::Identifier,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        self.classify_from_type_and_name_str(ty, local_name.id.as_str())\n+    }\n+\n+    fn visit_type_annotation(&mut self, annotation: &ast::Expr) {\n+        let prev_in_type_annotation = self.in_type_annotation;\n+        self.in_type_annotation = true;\n+        self.visit_expr(annotation);\n+        self.in_type_annotation = prev_in_type_annotation;\n+    }\n+\n+    fn visit_type_params(&mut self, type_params: &TypeParams) {\n+        for type_param in &type_params.type_params {\n+            self.visit_type_param(type_param);\n+        }\n+    }\n+\n+    fn visit_type_param(&mut self, type_param: &TypeParam) {\n+        // Emit token for the type parameter name\n+        let name_range = type_param.name().range();\n+        self.add_token(\n+            name_range,\n+            SemanticTokenType::TypeParameter,\n+            vec![SemanticTokenModifier::Definition],\n+        );\n+\n+        // Visit bound expression (for TypeVar)\n+        match type_param {\n+            TypeParam::TypeVar(type_var) => {\n+                if let Some(bound) = &type_var.bound {\n+                    self.visit_type_annotation(bound);\n+                }\n+                if let Some(default) = &type_var.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+            TypeParam::ParamSpec(param_spec) => {\n+                if let Some(default) = &param_spec.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+            TypeParam::TypeVarTuple(type_var_tuple) => {\n+                if let Some(default) = &type_var_tuple.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Visit decorators, handling simple name decorators vs complex expressions\n+    fn visit_decorators(&mut self, decorators: &[ast::Decorator]) {\n+        for decorator in decorators {\n+            match &decorator.expression {\n+                ast::Expr::Name(name) => {\n+                    // Simple decorator like @staticmethod - use Decorator token type\n+                    self.add_token(name.range(), SemanticTokenType::Decorator, vec![]);\n+                }\n+                _ => {\n+                    // Complex decorator like @app.route(\"/path\") - use normal expression rules\n+                    self.visit_expr(&decorator.expression);\n+                }\n+            }\n+        }\n+    }\n+}\n+\n+#[allow(clippy::elidable_lifetime_names)]\n+impl<'db> SourceOrderVisitor<'_> for SemanticTokenVisitor<'db> {\n+    fn visit_stmt(&mut self, stmt: &Stmt) {\n+        // If we have a range filter and this statement doesn't intersect, skip it\n+        // as an optimization\n+        if stmt.range().intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        match stmt {\n+            ast::Stmt::FunctionDef(func) => {\n+                // Function name\n+                self.add_token(\n+                    func.name.range(),\n+                    if self.in_class_scope {\n+                        SemanticTokenType::Method\n+                    } else {\n+                        SemanticTokenType::Function\n+                    },\n+                    if func.is_async {\n+                        vec![\n+                            SemanticTokenModifier::Definition,\n+                            SemanticTokenModifier::Async,\n+                        ]\n+                    } else {\n+                        vec![SemanticTokenModifier::Definition]\n+                    },\n+                );\n+\n+                // Type parameters (Python 3.12+ syntax)\n+                if let Some(type_params) = &func.type_params {\n+                    self.visit_type_params(type_params);\n+                }\n+\n+                // Parameters\n+                for (i, param) in func.parameters.args.iter().enumerate() {\n+                    let token_type = self.classify_parameter(&param.parameter, i == 0, func);\n+                    self.add_token(param.parameter.name.range(), token_type, vec![]);\n+\n+                    // Handle parameter type annotations\n+                    if let Some(annotation) = &param.parameter.annotation {\n+                        self.visit_type_annotation(annotation);\n+                    }\n+                }\n+\n+                // Handle return type annotation\n+                if let Some(returns) = &func.returns {\n+                    self.visit_type_annotation(returns);\n+                }\n+\n+                // Visit decorator expressions\n+                self.visit_decorators(&func.decorator_list);\n+\n+                // Clear the in_class_scope flag so inner functions\n+                // are not treated as methods\n+                let prev_in_class = self.in_class_scope;\n+                self.in_class_scope = false;\n+                walk_stmt(self, stmt);\n+                self.in_class_scope = prev_in_class;\n+            }\n+            ast::Stmt::ClassDef(class) => {\n+                // Class name\n+                self.add_token(\n+                    class.name.range(),\n+                    SemanticTokenType::Class,\n+                    vec![SemanticTokenModifier::Definition],\n+                );\n+\n+                // Type parameters (Python 3.12+ syntax)\n+                if let Some(type_params) = &class.type_params {\n+                    self.visit_type_params(type_params);\n+                }\n+\n+                // Handle base classes and type annotations in inheritance\n+                if let Some(arguments) = &class.arguments {\n+                    // Visit base class arguments\n+                    for arg in &arguments.args {\n+                        self.visit_expr(arg);\n+                    }\n+                    // Visit keyword arguments (for metaclass, etc.)\n+                    for keyword in &arguments.keywords {\n+                        self.visit_expr(&keyword.value);\n+                    }\n+                }\n+\n+                // Visit decorator expressions\n+                self.visit_decorators(&class.decorator_list);\n+\n+                let prev_in_class = self.in_class_scope;\n+                self.in_class_scope = true;\n+                walk_stmt(self, stmt);\n+                self.in_class_scope = prev_in_class;\n+            }\n+            ast::Stmt::AnnAssign(assign) => {\n+                // Handle annotated assignments (e.g., x: int = 5)\n+                if let ast::Expr::Name(name) = assign.target.as_ref() {\n+                    let (token_type, modifiers) = self.classify_name(name);\n+                    self.add_token(name.range(), token_type, modifiers);\n+                }\n+\n+                // Handle the type annotation\n+                self.visit_type_annotation(&assign.annotation);\n+\n+                // Handle the value if present\n+                if let Some(value) = &assign.value {\n+                    self.visit_expr(value);\n+                }\n+            }\n+            ast::Stmt::Import(import) => {\n+                for alias in &import.names {\n+                    if let Some(asname) = &alias.asname {\n+                        self.add_token(asname.range(), SemanticTokenType::Namespace, vec![]);\n+                    } else {\n+                        // Create separate tokens for each part of a dotted module name\n+                        self.add_dotted_name_tokens(&alias.name, SemanticTokenType::Namespace);\n+                    }\n+                }\n+                walk_stmt(self, stmt);\n+            }\n+            ast::Stmt::ImportFrom(import) => {\n+                if let Some(module) = &import.module {\n+                    // Create separate tokens for each part of a dotted module name\n+                    self.add_dotted_name_tokens(module, SemanticTokenType::Namespace);\n+                }\n+                for alias in &import.names {\n+                    if let Some(asname) = &alias.asname {\n+                        // For aliased imports (from X import Y as Z), classify Z based on what Y is\n+                        let ty = alias.inferred_type(self.semantic_model);\n+                        let (token_type, modifiers) = self.classify_from_alias_type(ty, asname);\n+                        self.add_token(asname.range(), token_type, modifiers);\n+                    } else {\n+                        // For direct imports (from X import Y), use semantic classification\n+                        let ty = alias.inferred_type(self.semantic_model);\n+                        let (token_type, modifiers) =\n+                            self.classify_from_alias_type(ty, &alias.name);\n+                        self.add_token(alias.name.range(), token_type, modifiers);\n+                    }\n+                }\n+                walk_stmt(self, stmt);\n+            }\n+            _ => {\n+                // For all other statement types, let the default visitor handle them\n+                walk_stmt(self, stmt);\n+            }\n+        }\n+    }\n+\n+    fn visit_expr(&mut self, expr: &Expr) {\n+        // If we have a range filter and this statement doesn't intersect, skip it\n+        // as an optimization\n+        if expr.range().intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        match expr {\n+            ast::Expr::Name(name) => {\n+                let (token_type, modifiers) = self.classify_name(name);\n+                self.add_token(name.range(), token_type, modifiers);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::Attribute(attr) => {\n+                // Use semantic analysis to determine the correct token type for the attribute\n+                let ty = expr.inferred_type(self.semantic_model);\n+                let (token_type, modifiers) =\n+                    Self::classify_from_type_for_attribute(ty, &attr.attr);\n+\n+                self.add_token(attr.attr.range(), token_type, modifiers);\n+                // Continue visiting the base expression\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::Call(call) => {\n+                // The function being called\n+                if let ast::Expr::Name(name) = call.func.as_ref() {\n+                    self.add_token(name.range(), SemanticTokenType::Function, vec![]);\n+                }\n+                // Continue visiting arguments\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::StringLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::String, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::NumberLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::Number, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::BooleanLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::BuiltinConstant, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::NoneLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::BuiltinConstant, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            _ => {\n+                // For all other expression types, let the default visitor handle them\n+                walk_expr(self, expr);\n+            }\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+    use crate::tests::cursor_test;\n+    use ruff_db::source::source_text;\n+    use ruff_text_size::{TextLen, TextRange};\n+\n+    /// Helper function to get full file range for testing\n+    fn full_file_range(db: &dyn Db, file: File) -> TextRange {\n+        let source = source_text(db, file);\n+        TextRange::new(0.into(), source.text_len())\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_basic() {\n+        let test = cursor_test(\"def foo(): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+        assert!(!tokens.tokens.is_empty());\n+\n+        // Should have at least a function name token\n+        let function_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Function))\n+            .collect();\n+        assert!(!function_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_class() {\n+        let test = cursor_test(\"class MyClass: pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a class token\n+        let class_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Class))\n+            .collect();\n+        assert!(!class_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_variables() {\n+        let test = cursor_test(\"x = 42\\ny = 'hello'<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have variable tokens\n+        let variable_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Variable))\n+            .collect();\n+        assert!(variable_tokens.len() >= 2);\n+\n+        // Should have number and string tokens\n+        let number_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Number))\n+            .collect();\n+        assert!(!number_tokens.is_empty());\n+\n+        let string_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::String))\n+            .collect();\n+        assert!(!string_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_self_parameter() {\n+        let test = cursor_test(\"class MyClass:\\n    def method(self, x): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a self parameter token\n+        let self_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::SelfParameter))\n+            .collect();\n+        assert!(!self_tokens.is_empty());\n+\n+        // Should have a regular parameter token for x\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(!param_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_cls_parameter() {\n+        let test =\n+            cursor_test(\"class MyClass:\\n    @classmethod\\n    def method(cls, x): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a cls parameter token\n+        let cls_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::ClsParameter))\n+            .collect();\n+        assert!(!cls_tokens.is_empty());\n+\n+        // Should have a regular parameter token for x\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(!param_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_staticmethod_parameter() {\n+        let test =\n+            cursor_test(\"class MyClass:\\n    @staticmethod\\n    def method(x, y): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have only regular parameter tokens (no self/cls)\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(param_tokens.len() >= 2);\n+\n+        // Should not have self or cls parameter tokens\n+        let self_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::SelfParameter))\n+            .collect();\n+        assert!(self_tokens.is_empty());\n+\n+        let cls_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::ClsParameter))\n+            .collect();\n+        assert!(cls_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_custom_self_cls_names() {\n+        let test = cursor_test(\n+            \"class MyClass:\\n    def method(instance, x): pass\\n    @classmethod\\n    def other(klass, y): pass<CURSOR>\",\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a self parameter token for \"instance\"\n+        let self_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::SelfParameter))\n+            .collect();\n+        assert!(!self_tokens.is_empty());\n+\n+        // Should have a cls parameter token for \"klass\"\n+        let cls_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::ClsParameter))\n+            .collect();\n+        assert!(!cls_tokens.is_empty());\n+\n+        // Should have regular parameter tokens for x and y\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(param_tokens.len() >= 2);\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_modifiers() {\n+        let test = cursor_test(\n+            \"class MyClass:\\n    CONSTANT = 42\\n    async def method(self): pass<CURSOR>\",\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a class token with Definition modifier\n+        let class_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Class))\n+            .collect();\n+        assert!(!class_tokens.is_empty());\n+        assert!(\n+            class_tokens[0]\n+                .modifiers\n+                .contains(&SemanticTokenModifier::Definition)\n+        );\n+\n+        // Should have a constant with Readonly modifier\n+        let constant_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                matches!(\n+                    t.token_type,\n+                    SemanticTokenType::Property | SemanticTokenType::Variable\n+                ) && t.modifiers.contains(&SemanticTokenModifier::Readonly)\n+            })\n+            .collect();\n+        assert!(!constant_tokens.is_empty());\n+\n+        // Should have an async method with Async modifier\n+        let async_method_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                matches!(t.token_type, SemanticTokenType::Method)\n+                    && t.modifiers.contains(&SemanticTokenModifier::Async)\n+            })\n+            .collect();\n+        assert!(!async_method_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_classification_vs_heuristic() {\n+        let test = cursor_test(\n+            r#\"\n+import sys\n+class MyClass:\n+    pass\n+\n+def my_function():\n+    return 42\n+\n+x = MyClass()\n+y = my_function()\n+z = sys.version<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have module tokens for imports\n+        let module_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Namespace))\n+            .collect();\n+        assert!(!module_tokens.is_empty());\n+\n+        // Should have class tokens\n+        let class_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Class))\n+            .collect();\n+        assert!(!class_tokens.is_empty());\n+\n+        // Should have function tokens\n+        let function_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Function))\n+            .collect();\n+        assert!(!function_tokens.is_empty());\n+\n+        // Should have variable tokens for assignments\n+        let variable_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Variable))\n+            .collect();\n+        assert!(variable_tokens.len() >= 3); // x, y, z\n+    }\n+\n+    #[test]\n+    fn test_builtin_constants() {\n+        let test = cursor_test(\"x = True\\ny = False\\nz = None<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have builtin constant tokens for True, False, and None\n+        let builtin_constant_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::BuiltinConstant))\n+            .collect();\n+        assert_eq!(builtin_constant_tokens.len(), 3);\n+\n+        // Should have variable tokens for x, y, z\n+        let variable_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Variable))\n+            .collect();\n+        assert_eq!(variable_tokens.len(), 3);\n+    }",
        "comment_created_at": "2025-07-03T08:46:35+00:00",
        "comment_author": "dhruvmanila",
        "comment_body": "I think we can extract these assertions in a separate function that would take in a parameter that says how many of the semantic tokens are expected and the function would then assert that only those tokens are present with the exact same quantity. For example, for this test case the call could look like:\r\n\r\n```rs\r\nassert_token_count([\r\n    (SemanticTokenType::BuiltinConstant, 3),\r\n    (SemanticTokenType::Variable, 3),\r\n]);\r\n```\r\n\r\nAnd, same for other test cases.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2182236628",
    "pr_number": 19108,
    "pr_file": "crates/ty_ide/src/semantic_tokens.rs",
    "created_at": "2025-07-03T08:52:47+00:00",
    "commented_code": "+use crate::Db;\n+use ruff_db::files::File;\n+use ruff_db::parsed::parsed_module;\n+use ruff_python_ast as ast;\n+use ruff_python_ast::visitor::source_order::{SourceOrderVisitor, walk_expr, walk_stmt};\n+use ruff_python_ast::{Expr, Stmt, TypeParam, TypeParams};\n+use ruff_text_size::{Ranged, TextRange};\n+use ty_python_semantic::{HasType, SemanticModel, types::Type};\n+\n+// This module walks the AST and collects a set of \"semantic tokens\" for a file\n+// or a range within a file. Each semantic token provides a \"token type\" and zero\n+// or more \"modifiers\". This information can be used by an editor to provide\n+// color coding based on semantic meaning.\n+\n+// Current limitations and areas for future improvement:\n+\n+// TODO: Need to handle semantic tokens within quoted annotations.\n+\n+// TODO: Need to properly handle Annotated expressions. All type arguments other\n+// than the first should be treated as value expressions, not as type expressions.\n+\n+// TODO: An identifier that resolves to a parameter when used within a function\n+// should be classified as a parameter, selfParameter, or clsParameter token.\n+\n+// TODO: Properties (or perhaps more generally, descriptor objects?) should be\n+// classified as property tokens rather than just variables.\n+\n+// TODO: Special forms like Protocol and TypedDict should probably be classified\n+// as class tokens, but they are currently classified as variables.\n+\n+// TODO: Type aliases (including those defined with the Python 3.12 \"type\" statement)\n+// do not currently have a dedicated semantic token type, but they maybe should.\n+\n+// TODO: Additional token modifiers might be added (e.g. for static methods,\n+// abstract methods and classes).\n+\n+/// Semantic token types supported by the language server.\n+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]\n+#[repr(u32)]\n+pub enum SemanticTokenType {\n+    Namespace = 0,\n+    Class = 1,\n+    Parameter = 2,\n+    SelfParameter = 3,\n+    ClsParameter = 4,\n+    Variable = 5,\n+    Property = 6,\n+    Function = 7,\n+    Method = 8,\n+    Keyword = 9,\n+    String = 10,\n+    Number = 11,\n+    Decorator = 12,\n+    BuiltinConstant = 13,\n+    TypeParameter = 14,\n+}\n+\n+impl SemanticTokenType {\n+    /// Returns all supported token types for LSP capabilities.\n+    pub fn all() -> Vec<&'static str> {\n+        vec![\n+            \"namespace\",\n+            \"class\",\n+            \"parameter\",\n+            \"selfParameter\",\n+            \"clsParameter\",\n+            \"variable\",\n+            \"property\",\n+            \"function\",\n+            \"method\",\n+            \"keyword\",\n+            \"string\",\n+            \"number\",\n+            \"decorator\",\n+            \"builtinConstant\",\n+            \"typeParameter\",\n+        ]\n+    }\n+}\n+\n+/// Semantic token modifiers.\n+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]\n+#[repr(u32)]\n+pub enum SemanticTokenModifier {\n+    Definition = 0,\n+    Readonly = 1,\n+    Async = 2,\n+}\n+\n+impl SemanticTokenModifier {\n+    /// Returns all supported token modifiers for LSP capabilities.\n+    pub fn all() -> Vec<&'static str> {\n+        vec![\"definition\", \"readonly\", \"async\"]\n+    }\n+}\n+\n+/// A semantic token with its position and classification.\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct SemanticToken {\n+    pub range: TextRange,\n+    pub token_type: SemanticTokenType,\n+    pub modifiers: Vec<SemanticTokenModifier>,\n+}\n+\n+/// The result of semantic tokenization.\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct SemanticTokens {\n+    pub tokens: Vec<SemanticToken>,\n+}\n+\n+/// Generates semantic tokens for a Python file within the specified range.\n+/// Pass a full file range to get tokens for the entire file.\n+pub fn semantic_tokens(db: &dyn Db, file: File, range: TextRange) -> Option<SemanticTokens> {\n+    let parsed = parsed_module(db, file).load(db);\n+    let semantic_model = SemanticModel::new(db, file);\n+\n+    let mut visitor = SemanticTokenVisitor::new(db, &semantic_model, range);\n+    visitor.visit_body(parsed.suite());\n+\n+    Some(SemanticTokens {\n+        tokens: visitor.tokens,\n+    })\n+}\n+\n+/// AST visitor that collects semantic tokens.\n+struct SemanticTokenVisitor<'db> {\n+    #[allow(dead_code)]\n+    db: &'db dyn Db,\n+    #[allow(dead_code)]\n+    semantic_model: &'db SemanticModel<'db>,\n+    tokens: Vec<SemanticToken>,\n+    in_class_scope: bool,\n+    in_type_annotation: bool,\n+    range_filter: TextRange,\n+}\n+\n+impl<'db> SemanticTokenVisitor<'db> {\n+    fn new(\n+        db: &'db dyn Db,\n+        semantic_model: &'db SemanticModel<'db>,\n+        range_filter: TextRange,\n+    ) -> Self {\n+        Self {\n+            db,\n+            semantic_model,\n+            tokens: Vec::new(),\n+            in_class_scope: false,\n+            in_type_annotation: false,\n+            range_filter,\n+        }\n+    }\n+\n+    fn add_token(\n+        &mut self,\n+        range: TextRange,\n+        token_type: SemanticTokenType,\n+        modifiers: Vec<SemanticTokenModifier>,\n+    ) {\n+        // Only emit tokens that intersect with the range filter\n+        if range.intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        self.tokens.push(SemanticToken {\n+            range,\n+            token_type,\n+            modifiers,\n+        });\n+    }\n+\n+    fn is_constant_name(name: &str) -> bool {\n+        name.chars().all(|c| c.is_uppercase() || c == '_') && name.len() > 1\n+    }\n+\n+    fn classify_name(\n+        &self,\n+        name: &ast::ExprName,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        // Try to get the inferred type of this name expression using semantic analysis\n+        let ty = name.inferred_type(self.semantic_model);\n+        let name_str = name.id.as_str();\n+        self.classify_from_type_and_name_str(ty, name_str)\n+    }\n+\n+    fn classify_from_type_and_name_str(\n+        &self,\n+        ty: Type,\n+        name_str: &str,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        let mut modifiers = vec![];\n+\n+        // In type annotation contexts, names that refer to nominal instances or protocol instances\n+        // should be classified as Class tokens (e.g., \"int\" in \"x: int\" should be a Class token)\n+        if self.in_type_annotation {\n+            match ty {\n+                Type::NominalInstance(_) | Type::ProtocolInstance(_) => {\n+                    return (SemanticTokenType::Class, modifiers);\n+                }\n+                _ => {\n+                    // Continue with normal classification for other types in annotations\n+                }\n+            }\n+        }\n+\n+        match ty {\n+            Type::ClassLiteral(_) => (SemanticTokenType::Class, modifiers),\n+            Type::TypeVar(_) => (SemanticTokenType::TypeParameter, modifiers),\n+            _ if ty.is_function_literal() => {\n+                // Check if this is a method based on current scope\n+                if self.in_class_scope {\n+                    (SemanticTokenType::Method, modifiers)\n+                } else {\n+                    (SemanticTokenType::Function, modifiers)\n+                }\n+            }\n+            _ if ty.is_bound_method() => (SemanticTokenType::Method, modifiers),\n+            _ if ty.into_module_literal().is_some() => (SemanticTokenType::Namespace, modifiers),\n+            _ => {\n+                // Check for constant naming convention\n+                if Self::is_constant_name(name_str) {\n+                    modifiers.push(SemanticTokenModifier::Readonly);\n+                }\n+                // For other types (variables, modules, etc.), assume variable\n+                (SemanticTokenType::Variable, modifiers)\n+            }\n+        }\n+    }\n+\n+    fn classify_from_type_for_attribute(\n+        ty: Type,\n+        attr_name: &ast::Identifier,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        let attr_name_str = attr_name.id.as_str();\n+        let mut modifiers = vec![];\n+\n+        // Classify based on the inferred type of the attribute\n+        if ty.is_class_literal() {\n+            (SemanticTokenType::Class, modifiers)\n+        } else if ty.is_function_literal() {\n+            // This is a function accessed as an attribute, likely a method\n+            (SemanticTokenType::Method, modifiers)\n+        } else if ty.is_bound_method() {\n+            // Method bound to an instance\n+            (SemanticTokenType::Method, modifiers)\n+        } else if ty.into_module_literal().is_some() {\n+            // Module accessed as an attribute (e.g., from os import path)\n+            (SemanticTokenType::Namespace, modifiers)\n+        } else if ty.is_property_instance() {\n+            // Actual Python property\n+            (SemanticTokenType::Property, modifiers)\n+        } else {\n+            // Check for constant naming convention\n+            if Self::is_constant_name(attr_name_str) {\n+                modifiers.push(SemanticTokenModifier::Readonly);\n+            }\n+\n+            // For other types (variables, constants, etc.), classify as variable\n+            (SemanticTokenType::Variable, modifiers)\n+        }\n+    }\n+\n+    fn classify_parameter(\n+        &self,\n+        _param: &ast::Parameter,\n+        is_first: bool,\n+        func: &ast::StmtFunctionDef,\n+    ) -> SemanticTokenType {\n+        if is_first && self.in_class_scope {\n+            // Check if this is a classmethod (has @classmethod decorator)\n+            let is_classmethod =\n+                func.decorator_list\n+                    .iter()\n+                    .any(|decorator| match &decorator.expression {\n+                        ast::Expr::Name(name) => name.id.as_str() == \"classmethod\",\n+                        ast::Expr::Attribute(attr) => attr.attr.id.as_str() == \"classmethod\",\n+                        _ => false,\n+                    });\n+\n+            // Check if this is a staticmethod (has @staticmethod decorator)\n+            let is_staticmethod =\n+                func.decorator_list\n+                    .iter()\n+                    .any(|decorator| match &decorator.expression {\n+                        ast::Expr::Name(name) => name.id.as_str() == \"staticmethod\",\n+                        ast::Expr::Attribute(attr) => attr.attr.id.as_str() == \"staticmethod\",\n+                        _ => false,\n+                    });\n+\n+            if is_staticmethod {\n+                // Static methods don't have self/cls parameters\n+                SemanticTokenType::Parameter\n+            } else if is_classmethod {\n+                // First parameter of a classmethod is cls parameter\n+                SemanticTokenType::ClsParameter\n+            } else {\n+                // First parameter of an instance method is self parameter\n+                SemanticTokenType::SelfParameter\n+            }\n+        } else {\n+            SemanticTokenType::Parameter\n+        }\n+    }\n+\n+    fn add_dotted_name_tokens(&mut self, name: &ast::Identifier, token_type: SemanticTokenType) {\n+        let name_str = name.id.as_str();\n+        let name_range = name.range();\n+        let name_start = name_range.start();\n+\n+        // Split the dotted name and calculate positions for each part\n+        let mut current_offset = 0usize;\n+        for part in name_str.split('.') {\n+            if !part.is_empty() {\n+                let part_start =\n+                    name_start + ruff_text_size::TextSize::try_from(current_offset).unwrap();\n+                let part_end = part_start + ruff_text_size::TextSize::try_from(part.len()).unwrap();\n+                let part_range = ruff_text_size::TextRange::new(part_start, part_end);\n+\n+                self.add_token(part_range, token_type, vec![]);\n+            }\n+            // Move past this part and the dot\n+            current_offset += part.len() + 1; // +1 for the dot\n+        }\n+    }\n+\n+    fn classify_from_alias_type(\n+        &self,\n+        ty: Type,\n+        local_name: &ast::Identifier,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        self.classify_from_type_and_name_str(ty, local_name.id.as_str())\n+    }\n+\n+    fn visit_type_annotation(&mut self, annotation: &ast::Expr) {\n+        let prev_in_type_annotation = self.in_type_annotation;\n+        self.in_type_annotation = true;\n+        self.visit_expr(annotation);\n+        self.in_type_annotation = prev_in_type_annotation;\n+    }\n+\n+    fn visit_type_params(&mut self, type_params: &TypeParams) {\n+        for type_param in &type_params.type_params {\n+            self.visit_type_param(type_param);\n+        }\n+    }\n+\n+    fn visit_type_param(&mut self, type_param: &TypeParam) {\n+        // Emit token for the type parameter name\n+        let name_range = type_param.name().range();\n+        self.add_token(\n+            name_range,\n+            SemanticTokenType::TypeParameter,\n+            vec![SemanticTokenModifier::Definition],\n+        );\n+\n+        // Visit bound expression (for TypeVar)\n+        match type_param {\n+            TypeParam::TypeVar(type_var) => {\n+                if let Some(bound) = &type_var.bound {\n+                    self.visit_type_annotation(bound);\n+                }\n+                if let Some(default) = &type_var.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+            TypeParam::ParamSpec(param_spec) => {\n+                if let Some(default) = &param_spec.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+            TypeParam::TypeVarTuple(type_var_tuple) => {\n+                if let Some(default) = &type_var_tuple.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Visit decorators, handling simple name decorators vs complex expressions\n+    fn visit_decorators(&mut self, decorators: &[ast::Decorator]) {\n+        for decorator in decorators {\n+            match &decorator.expression {\n+                ast::Expr::Name(name) => {\n+                    // Simple decorator like @staticmethod - use Decorator token type\n+                    self.add_token(name.range(), SemanticTokenType::Decorator, vec![]);\n+                }\n+                _ => {\n+                    // Complex decorator like @app.route(\"/path\") - use normal expression rules\n+                    self.visit_expr(&decorator.expression);\n+                }\n+            }\n+        }\n+    }\n+}\n+\n+#[allow(clippy::elidable_lifetime_names)]\n+impl<'db> SourceOrderVisitor<'_> for SemanticTokenVisitor<'db> {\n+    fn visit_stmt(&mut self, stmt: &Stmt) {\n+        // If we have a range filter and this statement doesn't intersect, skip it\n+        // as an optimization\n+        if stmt.range().intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        match stmt {\n+            ast::Stmt::FunctionDef(func) => {\n+                // Function name\n+                self.add_token(\n+                    func.name.range(),\n+                    if self.in_class_scope {\n+                        SemanticTokenType::Method\n+                    } else {\n+                        SemanticTokenType::Function\n+                    },\n+                    if func.is_async {\n+                        vec![\n+                            SemanticTokenModifier::Definition,\n+                            SemanticTokenModifier::Async,\n+                        ]\n+                    } else {\n+                        vec![SemanticTokenModifier::Definition]\n+                    },\n+                );\n+\n+                // Type parameters (Python 3.12+ syntax)\n+                if let Some(type_params) = &func.type_params {\n+                    self.visit_type_params(type_params);\n+                }\n+\n+                // Parameters\n+                for (i, param) in func.parameters.args.iter().enumerate() {\n+                    let token_type = self.classify_parameter(&param.parameter, i == 0, func);\n+                    self.add_token(param.parameter.name.range(), token_type, vec![]);\n+\n+                    // Handle parameter type annotations\n+                    if let Some(annotation) = &param.parameter.annotation {\n+                        self.visit_type_annotation(annotation);\n+                    }\n+                }\n+\n+                // Handle return type annotation\n+                if let Some(returns) = &func.returns {\n+                    self.visit_type_annotation(returns);\n+                }\n+\n+                // Visit decorator expressions\n+                self.visit_decorators(&func.decorator_list);\n+\n+                // Clear the in_class_scope flag so inner functions\n+                // are not treated as methods\n+                let prev_in_class = self.in_class_scope;\n+                self.in_class_scope = false;\n+                walk_stmt(self, stmt);\n+                self.in_class_scope = prev_in_class;\n+            }\n+            ast::Stmt::ClassDef(class) => {\n+                // Class name\n+                self.add_token(\n+                    class.name.range(),\n+                    SemanticTokenType::Class,\n+                    vec![SemanticTokenModifier::Definition],\n+                );\n+\n+                // Type parameters (Python 3.12+ syntax)\n+                if let Some(type_params) = &class.type_params {\n+                    self.visit_type_params(type_params);\n+                }\n+\n+                // Handle base classes and type annotations in inheritance\n+                if let Some(arguments) = &class.arguments {\n+                    // Visit base class arguments\n+                    for arg in &arguments.args {\n+                        self.visit_expr(arg);\n+                    }\n+                    // Visit keyword arguments (for metaclass, etc.)\n+                    for keyword in &arguments.keywords {\n+                        self.visit_expr(&keyword.value);\n+                    }\n+                }\n+\n+                // Visit decorator expressions\n+                self.visit_decorators(&class.decorator_list);\n+\n+                let prev_in_class = self.in_class_scope;\n+                self.in_class_scope = true;\n+                walk_stmt(self, stmt);\n+                self.in_class_scope = prev_in_class;\n+            }\n+            ast::Stmt::AnnAssign(assign) => {\n+                // Handle annotated assignments (e.g., x: int = 5)\n+                if let ast::Expr::Name(name) = assign.target.as_ref() {\n+                    let (token_type, modifiers) = self.classify_name(name);\n+                    self.add_token(name.range(), token_type, modifiers);\n+                }\n+\n+                // Handle the type annotation\n+                self.visit_type_annotation(&assign.annotation);\n+\n+                // Handle the value if present\n+                if let Some(value) = &assign.value {\n+                    self.visit_expr(value);\n+                }\n+            }\n+            ast::Stmt::Import(import) => {\n+                for alias in &import.names {\n+                    if let Some(asname) = &alias.asname {\n+                        self.add_token(asname.range(), SemanticTokenType::Namespace, vec![]);\n+                    } else {\n+                        // Create separate tokens for each part of a dotted module name\n+                        self.add_dotted_name_tokens(&alias.name, SemanticTokenType::Namespace);\n+                    }\n+                }\n+                walk_stmt(self, stmt);\n+            }\n+            ast::Stmt::ImportFrom(import) => {\n+                if let Some(module) = &import.module {\n+                    // Create separate tokens for each part of a dotted module name\n+                    self.add_dotted_name_tokens(module, SemanticTokenType::Namespace);\n+                }\n+                for alias in &import.names {\n+                    if let Some(asname) = &alias.asname {\n+                        // For aliased imports (from X import Y as Z), classify Z based on what Y is\n+                        let ty = alias.inferred_type(self.semantic_model);\n+                        let (token_type, modifiers) = self.classify_from_alias_type(ty, asname);\n+                        self.add_token(asname.range(), token_type, modifiers);\n+                    } else {\n+                        // For direct imports (from X import Y), use semantic classification\n+                        let ty = alias.inferred_type(self.semantic_model);\n+                        let (token_type, modifiers) =\n+                            self.classify_from_alias_type(ty, &alias.name);\n+                        self.add_token(alias.name.range(), token_type, modifiers);\n+                    }\n+                }\n+                walk_stmt(self, stmt);\n+            }\n+            _ => {\n+                // For all other statement types, let the default visitor handle them\n+                walk_stmt(self, stmt);\n+            }\n+        }\n+    }\n+\n+    fn visit_expr(&mut self, expr: &Expr) {\n+        // If we have a range filter and this statement doesn't intersect, skip it\n+        // as an optimization\n+        if expr.range().intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        match expr {\n+            ast::Expr::Name(name) => {\n+                let (token_type, modifiers) = self.classify_name(name);\n+                self.add_token(name.range(), token_type, modifiers);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::Attribute(attr) => {\n+                // Use semantic analysis to determine the correct token type for the attribute\n+                let ty = expr.inferred_type(self.semantic_model);\n+                let (token_type, modifiers) =\n+                    Self::classify_from_type_for_attribute(ty, &attr.attr);\n+\n+                self.add_token(attr.attr.range(), token_type, modifiers);\n+                // Continue visiting the base expression\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::Call(call) => {\n+                // The function being called\n+                if let ast::Expr::Name(name) = call.func.as_ref() {\n+                    self.add_token(name.range(), SemanticTokenType::Function, vec![]);\n+                }\n+                // Continue visiting arguments\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::StringLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::String, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::NumberLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::Number, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::BooleanLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::BuiltinConstant, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::NoneLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::BuiltinConstant, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            _ => {\n+                // For all other expression types, let the default visitor handle them\n+                walk_expr(self, expr);\n+            }\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+    use crate::tests::cursor_test;\n+    use ruff_db::source::source_text;\n+    use ruff_text_size::{TextLen, TextRange};\n+\n+    /// Helper function to get full file range for testing\n+    fn full_file_range(db: &dyn Db, file: File) -> TextRange {\n+        let source = source_text(db, file);\n+        TextRange::new(0.into(), source.text_len())\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_basic() {\n+        let test = cursor_test(\"def foo(): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+        assert!(!tokens.tokens.is_empty());\n+\n+        // Should have at least a function name token\n+        let function_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Function))\n+            .collect();\n+        assert!(!function_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_class() {\n+        let test = cursor_test(\"class MyClass: pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a class token\n+        let class_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Class))\n+            .collect();\n+        assert!(!class_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_variables() {\n+        let test = cursor_test(\"x = 42\ny = 'hello'<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have variable tokens\n+        let variable_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Variable))\n+            .collect();\n+        assert!(variable_tokens.len() >= 2);\n+\n+        // Should have number and string tokens\n+        let number_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Number))\n+            .collect();\n+        assert!(!number_tokens.is_empty());\n+\n+        let string_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::String))\n+            .collect();\n+        assert!(!string_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_self_parameter() {\n+        let test = cursor_test(\"class MyClass:\n    def method(self, x): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a self parameter token\n+        let self_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::SelfParameter))\n+            .collect();\n+        assert!(!self_tokens.is_empty());\n+\n+        // Should have a regular parameter token for x\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(!param_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_cls_parameter() {\n+        let test =\n+            cursor_test(\"class MyClass:\n    @classmethod\n    def method(cls, x): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a cls parameter token\n+        let cls_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::ClsParameter))\n+            .collect();\n+        assert!(!cls_tokens.is_empty());\n+\n+        // Should have a regular parameter token for x\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(!param_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_staticmethod_parameter() {\n+        let test =\n+            cursor_test(\"class MyClass:\n    @staticmethod\n    def method(x, y): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have only regular parameter tokens (no self/cls)\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(param_tokens.len() >= 2);\n+\n+        // Should not have self or cls parameter tokens\n+        let self_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::SelfParameter))\n+            .collect();\n+        assert!(self_tokens.is_empty());\n+\n+        let cls_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::ClsParameter))\n+            .collect();\n+        assert!(cls_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_custom_self_cls_names() {\n+        let test = cursor_test(\n+            \"class MyClass:\n    def method(instance, x): pass\n    @classmethod\n    def other(klass, y): pass<CURSOR>\",\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a self parameter token for \"instance\"\n+        let self_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::SelfParameter))\n+            .collect();\n+        assert!(!self_tokens.is_empty());\n+\n+        // Should have a cls parameter token for \"klass\"\n+        let cls_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::ClsParameter))\n+            .collect();\n+        assert!(!cls_tokens.is_empty());\n+\n+        // Should have regular parameter tokens for x and y\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(param_tokens.len() >= 2);\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_modifiers() {\n+        let test = cursor_test(\n+            \"class MyClass:\n    CONSTANT = 42\n    async def method(self): pass<CURSOR>\",\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a class token with Definition modifier\n+        let class_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Class))\n+            .collect();\n+        assert!(!class_tokens.is_empty());\n+        assert!(\n+            class_tokens[0]\n+                .modifiers\n+                .contains(&SemanticTokenModifier::Definition)\n+        );\n+\n+        // Should have a constant with Readonly modifier\n+        let constant_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                matches!(\n+                    t.token_type,\n+                    SemanticTokenType::Property | SemanticTokenType::Variable\n+                ) && t.modifiers.contains(&SemanticTokenModifier::Readonly)\n+            })\n+            .collect();\n+        assert!(!constant_tokens.is_empty());\n+\n+        // Should have an async method with Async modifier\n+        let async_method_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                matches!(t.token_type, SemanticTokenType::Method)\n+                    && t.modifiers.contains(&SemanticTokenModifier::Async)\n+            })\n+            .collect();\n+        assert!(!async_method_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_classification_vs_heuristic() {\n+        let test = cursor_test(\n+            r#\"\n+import sys\n+class MyClass:\n+    pass\n+\n+def my_function():\n+    return 42\n+\n+x = MyClass()\n+y = my_function()\n+z = sys.version<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have module tokens for imports\n+        let module_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Namespace))\n+            .collect();\n+        assert!(!module_tokens.is_empty());\n+\n+        // Should have class tokens\n+        let class_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Class))\n+            .collect();\n+        assert!(!class_tokens.is_empty());\n+\n+        // Should have function tokens\n+        let function_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Function))\n+            .collect();\n+        assert!(!function_tokens.is_empty());\n+\n+        // Should have variable tokens for assignments\n+        let variable_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Variable))\n+            .collect();\n+        assert!(variable_tokens.len() >= 3); // x, y, z\n+    }\n+\n+    #[test]\n+    fn test_builtin_constants() {\n+        let test = cursor_test(\"x = True\ny = False\nz = None<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have builtin constant tokens for True, False, and None\n+        let builtin_constant_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::BuiltinConstant))\n+            .collect();\n+        assert_eq!(builtin_constant_tokens.len(), 3);\n+\n+        // Should have variable tokens for x, y, z\n+        let variable_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Variable))\n+            .collect();\n+        assert_eq!(variable_tokens.len(), 3);\n+    }\n+\n+    #[test]\n+    fn test_builtin_constants_in_expressions() {\n+        let test = cursor_test(\n+            r#\"\n+def check(value):\n+    if value is None:\n+        return False\n+    return True\n+\n+result = check(None)<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have builtin constant tokens for None, False, True, and None again\n+        let builtin_constant_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::BuiltinConstant))\n+            .collect();\n+        assert_eq!(builtin_constant_tokens.len(), 4); // None, False, True, None\n+\n+        // Should have function tokens\n+        let function_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Function))\n+            .collect();\n+        assert!(!function_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_range() {\n+        let test = cursor_test(\n+            r#\"\n+def function1():\n+    x = 42\n+    return x\n+\n+def function2():\n+    y = \"hello\"\n+    z = True\n+    return y + z<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+        let full_tokens = result.unwrap();\n+\n+        // Get the range that covers only the second function\n+        let source = ruff_db::source::source_text(&test.db, test.cursor.file);\n+        let lines: Vec<&str> = source.split('\n').collect();\n+\n+        // Find the start of function2 (around line 5, offset roughly starts at second function)\n+        let mut function2_start = 0;\n+        for (i, line) in lines.iter().enumerate() {\n+            if line.contains(\"def function2\") {\n+                // Get the character offset for the start of this line\n+                function2_start = lines[..i].iter().map(|l| l.len() + 1).sum(); // +1 for newline\n+                break;\n+            }\n+        }\n+\n+        let range = ruff_text_size::TextRange::new(\n+            ruff_text_size::TextSize::try_from(function2_start).unwrap(),\n+            ruff_text_size::TextSize::try_from(source.len()).unwrap(),\n+        );\n+\n+        let range_result = semantic_tokens(&test.db, test.cursor.file, range);\n+        assert!(range_result.is_some());\n+        let range_tokens = range_result.unwrap();\n+\n+        // Range-based tokens should have fewer tokens than full scan\n+        // (should exclude tokens from function1)\n+        assert!(range_tokens.tokens.len() < full_tokens.tokens.len());\n+\n+        // Should still have tokens for function2, y, z, \"hello\", True\n+        let function_tokens: Vec<_> = range_tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Function))\n+            .collect();\n+        assert!(!function_tokens.is_empty()); // function2\n+\n+        let variable_tokens: Vec<_> = range_tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Variable))\n+            .collect();\n+        assert!(variable_tokens.len() >= 2); // y, z\n+\n+        let string_tokens: Vec<_> = range_tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::String))\n+            .collect();\n+        assert!(!string_tokens.is_empty()); // \"hello\"\n+\n+        let builtin_tokens: Vec<_> = range_tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::BuiltinConstant))\n+            .collect();\n+        assert!(!builtin_tokens.is_empty()); // True\n+\n+        // Verify that no tokens from range_tokens have ranges outside the requested range\n+        for token in &range_tokens.tokens {\n+            assert!(\n+                range.contains_range(token.range),\n+                \"Token at {:?} is outside requested range {:?}\",\n+                token.range,\n+                range\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    fn test_dotted_module_names() {\n+        let test = cursor_test(\n+            r#\"\n+import os.path\n+import sys.version_info\n+from urllib.parse import urlparse\n+from collections.abc import Mapping<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have module tokens for each part of dotted names\n+        let module_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Namespace))\n+            .collect();\n+\n+        // Should have tokens for: os, path, sys, version_info, urllib, parse, collections, abc\n+        // That's 8 separate module tokens\n+        assert!(module_tokens.len() >= 8);\n+\n+        // Should have tokens for imported names with correct classifications\n+        let source = ruff_db::source::source_text(&test.db, test.cursor.file);\n+\n+        // urlparse should be classified based on its actual semantic type (likely Function)\n+        let urlparse_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"urlparse\"\n+            })\n+            .collect();\n+        assert_eq!(urlparse_tokens.len(), 1, \"Expected 1 token for urlparse\");\n+        // urlparse is a function, so it should be classified as Function\n+        assert!(\n+            matches!(\n+                urlparse_tokens[0].token_type,\n+                SemanticTokenType::Function | SemanticTokenType::Variable\n+            ),\n+            \"urlparse should be classified as Function or Variable\"\n+        );\n+\n+        // Mapping should be classified as a class (ABC/Protocol)\n+        let mapping_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"Mapping\" && matches!(t.token_type, SemanticTokenType::Class)\n+            })\n+            .collect();\n+        assert_eq!(\n+            mapping_tokens.len(),\n+            1,\n+            \"Expected 1 class token for Mapping\"\n+        );\n+        let mapping_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"Mapping\" && matches!(t.token_type, SemanticTokenType::Class)\n+            })\n+            .collect();\n+        assert_eq!(\n+            mapping_tokens.len(),\n+            1,\n+            \"Expected 1 class token for Mapping\"\n+        );\n+\n+        // Verify that none of the module tokens contain periods\n+        // by checking that each token's text length matches what we expect\n+        let source = ruff_db::source::source_text(&test.db, test.cursor.file);\n+        for token in &module_tokens {\n+            let token_text = &source[token.range];\n+            assert!(\n+                !token_text.contains('.'),\n+                \"Module token should not contain periods: '{token_text}'\"\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    fn test_module_type_classification() {\n+        let test = cursor_test(\n+            r#\"\n+import os\n+import sys\n+from collections import defaultdict\n+\n+# os and sys should be classified as namespace/module types\n+x = os\n+y = sys<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Find tokens for imported modules\n+        let source = ruff_db::source::source_text(&test.db, test.cursor.file);\n+        let module_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                matches!(t.token_type, SemanticTokenType::Namespace)\n+                    && (token_text == \"os\" || token_text == \"sys\")\n+            })\n+            .collect();\n+\n+        // Should have 4 namespace tokens: os, sys (in imports), os, sys (in assignments)\n+        assert_eq!(\n+            module_tokens.len(),\n+            4,\n+            \"Expected 4 namespace tokens for module references, got {}\",\n+            module_tokens.len()\n+        );\n+\n+        // Verify that variables assigned to modules are also classified as namespace\n+        let xy_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                (token_text == \"x\" || token_text == \"y\")\n+                    && matches!(t.token_type, SemanticTokenType::Namespace)\n+            })\n+            .collect();\n+\n+        // Should have 2 namespace tokens for x and y (since they hold module values)\n+        assert_eq!(\n+            xy_tokens.len(),\n+            2,\n+            \"Expected 2 namespace tokens for x and y, got {}\",\n+            xy_tokens.len()\n+        );\n+\n+        // Verify that defaultdict is classified based on its semantic type (likely Class since it's a constructor)\n+        let defaultdict_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"defaultdict\"\n+            })\n+            .collect();\n+\n+        assert_eq!(\n+            defaultdict_tokens.len(),\n+            1,\n+            \"Expected 1 token for defaultdict, got {}\",\n+            defaultdict_tokens.len()\n+        );\n+        // defaultdict is actually a class constructor, so it should be classified as Class\n+        assert!(\n+            matches!(\n+                defaultdict_tokens[0].token_type,\n+                SemanticTokenType::Class | SemanticTokenType::Variable\n+            ),\n+            \"defaultdict should be classified as Class or Variable based on semantic analysis\"\n+        );\n+    }\n+\n+    #[test]\n+    fn test_import_classification() {\n+        let test = cursor_test(\n+            r#\"\n+from os import path\n+from collections import defaultdict, OrderedDict, Counter\n+from typing import List, Dict, Optional\n+from mymodule import CONSTANT, my_function, MyClass<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+        let source = ruff_db::source::source_text(&test.db, test.cursor.file);\n+\n+        // path should be classified as namespace (since os.path is actually a module)\n+        let path_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"path\" && matches!(t.token_type, SemanticTokenType::Namespace)\n+            })\n+            .collect();\n+        assert_eq!(\n+            path_tokens.len(),\n+            1,\n+            \"Expected 1 namespace token for path (os.path is a module)\"\n+        );\n+\n+        // defaultdict should be classified based on its actual semantic type (likely Function)\n+        let defaultdict_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"defaultdict\"\n+            })\n+            .collect();\n+\n+        if defaultdict_tokens.is_empty() {\n+            panic!(\"No tokens found for 'defaultdict'\");\n+        } else {\n+            // defaultdict is actually a class constructor, so it might be classified as Class\n+            let token_type = &defaultdict_tokens[0].token_type;\n+            assert!(\n+                matches!(\n+                    token_type,\n+                    SemanticTokenType::Variable\n+                        | SemanticTokenType::Class\n+                        | SemanticTokenType::Function\n+                ),\n+                \"defaultdict should be classified as Variable, Class, or Function, got {token_type:?}\"\n+            );\n+        }\n+\n+        // The remaining tests are more flexible since semantic analysis might not have complete info\n+        // for all imports, especially from unresolved modules\n+\n+        // Just verify that we have tokens for the expected imported names\n+        let expected_names = vec![\n+            \"OrderedDict\",\n+            \"Counter\",\n+            \"List\",\n+            \"Dict\",\n+            \"Optional\",\n+            \"CONSTANT\",\n+            \"my_function\",\n+            \"MyClass\",\n+        ];\n+        for name in expected_names {\n+            let name_tokens: Vec<_> = tokens\n+                .tokens\n+                .iter()\n+                .filter(|t| {\n+                    let token_text = &source[t.range];\n+                    token_text == name\n+                })\n+                .collect();\n+            assert!(\n+                !name_tokens.is_empty(),\n+                \"Expected at least 1 token for {name}\"\n+            );\n+        }\n+\n+        // CONSTANT should have readonly modifier if it's classified as Variable\n+        let constant_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"CONSTANT\"\n+            })\n+            .collect();\n+        if !constant_tokens.is_empty()\n+            && matches!(constant_tokens[0].token_type, SemanticTokenType::Variable)\n+        {\n+            assert!(\n+                constant_tokens[0]\n+                    .modifiers\n+                    .contains(&SemanticTokenModifier::Readonly),\n+                \"CONSTANT should have readonly modifier when classified as Variable\"\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    fn test_attribute_classification() {\n+        let test = cursor_test(\n+            r#\"\n+import os\n+import sys\n+from collections import defaultdict\n+from typing import List\n+\n+class MyClass:\n+    CONSTANT = 42\n+    \n+    def method(self):\n+        return \"hello\"\n+    \n+    @property\n+    def prop(self):\n+        return self.CONSTANT\n+\n+obj = MyClass()\n+\n+# Test various attribute accesses\n+x = os.path              # path should be namespace (module)\n+y = obj.method           # method should be method (bound method)\n+z = obj.CONSTANT         # CONSTANT should be variable with readonly modifier\n+w = obj.prop             # prop should be property\n+v = MyClass.method       # method should be method (function)\n+u = List.__name__        # __name__ should be variable<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+        let source = ruff_db::source::source_text(&test.db, test.cursor.file);\n+\n+        // Find all tokens and create a map for easier testing\n+        let mut token_map = std::collections::HashMap::new();\n+        for token in &tokens.tokens {\n+            let token_text = &source[token.range];\n+            token_map\n+                .entry(token_text.to_string())\n+                .or_insert_with(Vec::new)\n+                .push(token);\n+        }\n+\n+        // Test path attribute (should be namespace since os.path is a module)\n+        let path_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"path\" && matches!(t.token_type, SemanticTokenType::Namespace)\n+            })\n+            .collect();\n+        assert!(\n+            !path_tokens.is_empty(),\n+            \"Expected at least 1 namespace token for 'path' attribute\"\n+        );\n+\n+        // Test method attribute (should be method - bound method)\n+        let method_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"method\" && matches!(t.token_type, SemanticTokenType::Method)\n+            })\n+            .collect();\n+        assert!(\n+            !method_tokens.is_empty(),\n+            \"Expected at least 1 method token for 'method' attribute\"\n+        );\n+\n+        // Test CONSTANT attribute (should be variable with readonly modifier)\n+        let constant_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"CONSTANT\"\n+                    && matches!(\n+                        t.token_type,\n+                        SemanticTokenType::Variable | SemanticTokenType::Property\n+                    )\n+                    && t.modifiers.contains(&SemanticTokenModifier::Readonly)\n+            })\n+            .collect();\n+        assert!(\n+            !constant_tokens.is_empty(),\n+            \"Expected at least 1 variable/property token with readonly modifier for 'CONSTANT' attribute\"\n+        );\n+\n+        // Test property attribute (should be property)\n+        // Note: This might not work perfectly if the semantic analyzer doesn't have full property info\n+        // but we should have at least a variable token for it\n+        let prop_or_var_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"prop\"\n+                    && matches!(\n+                        t.token_type,\n+                        SemanticTokenType::Property | SemanticTokenType::Variable\n+                    )\n+            })\n+            .collect();\n+        assert!(\n+            !prop_or_var_tokens.is_empty(),\n+            \"Expected at least 1 property/variable token for 'prop' attribute\"\n+        );\n+\n+        // Test __name__ attribute (should be variable since it's a string attribute)\n+        let name_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"__name__\" && matches!(t.token_type, SemanticTokenType::Variable)\n+            })\n+            .collect();\n+        assert!(\n+            !name_tokens.is_empty(),\n+            \"Expected at least 1 variable token for '__name__' attribute\"\n+        );\n+    }\n+\n+    #[test]\n+    fn test_attribute_fallback_classification() {\n+        let test = cursor_test(\n+            r#\"\n+class MyClass:\n+    some_attr = \"value\"\n+    \n+obj = MyClass()\n+# Test attribute that might not have detailed semantic info\n+x = obj.some_attr        # Should fall back to variable, not property\n+y = obj.unknown_attr     # Should fall back to variable<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+        let source = ruff_db::source::source_text(&test.db, test.cursor.file);\n+\n+        // Test that attributes with unknown/basic types fall back to variable, not property\n+        let attr_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                (token_text == \"some_attr\" || token_text == \"unknown_attr\")\n+                    && matches!(\n+                        t.token_type,\n+                        SemanticTokenType::Variable | SemanticTokenType::Property\n+                    )\n+            })\n+            .collect();\n+\n+        // We should have tokens for both attributes\n+        assert!(\n+            attr_tokens.len() >= 2,\n+            \"Expected at least 2 tokens for attribute expressions\"\n+        );",
    "repo_full_name": "astral-sh/ruff",
    "discussion_comments": [
      {
        "comment_id": "2182236628",
        "repo_full_name": "astral-sh/ruff",
        "pr_number": 19108,
        "pr_file": "crates/ty_ide/src/semantic_tokens.rs",
        "discussion_id": "2182236628",
        "commented_code": "@@ -0,0 +1,2531 @@\n+use crate::Db;\n+use ruff_db::files::File;\n+use ruff_db::parsed::parsed_module;\n+use ruff_python_ast as ast;\n+use ruff_python_ast::visitor::source_order::{SourceOrderVisitor, walk_expr, walk_stmt};\n+use ruff_python_ast::{Expr, Stmt, TypeParam, TypeParams};\n+use ruff_text_size::{Ranged, TextRange};\n+use ty_python_semantic::{HasType, SemanticModel, types::Type};\n+\n+// This module walks the AST and collects a set of \"semantic tokens\" for a file\n+// or a range within a file. Each semantic token provides a \"token type\" and zero\n+// or more \"modifiers\". This information can be used by an editor to provide\n+// color coding based on semantic meaning.\n+\n+// Current limitations and areas for future improvement:\n+\n+// TODO: Need to handle semantic tokens within quoted annotations.\n+\n+// TODO: Need to properly handle Annotated expressions. All type arguments other\n+// than the first should be treated as value expressions, not as type expressions.\n+\n+// TODO: An identifier that resolves to a parameter when used within a function\n+// should be classified as a parameter, selfParameter, or clsParameter token.\n+\n+// TODO: Properties (or perhaps more generally, descriptor objects?) should be\n+// classified as property tokens rather than just variables.\n+\n+// TODO: Special forms like Protocol and TypedDict should probably be classified\n+// as class tokens, but they are currently classified as variables.\n+\n+// TODO: Type aliases (including those defined with the Python 3.12 \"type\" statement)\n+// do not currently have a dedicated semantic token type, but they maybe should.\n+\n+// TODO: Additional token modifiers might be added (e.g. for static methods,\n+// abstract methods and classes).\n+\n+/// Semantic token types supported by the language server.\n+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]\n+#[repr(u32)]\n+pub enum SemanticTokenType {\n+    Namespace = 0,\n+    Class = 1,\n+    Parameter = 2,\n+    SelfParameter = 3,\n+    ClsParameter = 4,\n+    Variable = 5,\n+    Property = 6,\n+    Function = 7,\n+    Method = 8,\n+    Keyword = 9,\n+    String = 10,\n+    Number = 11,\n+    Decorator = 12,\n+    BuiltinConstant = 13,\n+    TypeParameter = 14,\n+}\n+\n+impl SemanticTokenType {\n+    /// Returns all supported token types for LSP capabilities.\n+    pub fn all() -> Vec<&'static str> {\n+        vec![\n+            \"namespace\",\n+            \"class\",\n+            \"parameter\",\n+            \"selfParameter\",\n+            \"clsParameter\",\n+            \"variable\",\n+            \"property\",\n+            \"function\",\n+            \"method\",\n+            \"keyword\",\n+            \"string\",\n+            \"number\",\n+            \"decorator\",\n+            \"builtinConstant\",\n+            \"typeParameter\",\n+        ]\n+    }\n+}\n+\n+/// Semantic token modifiers.\n+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]\n+#[repr(u32)]\n+pub enum SemanticTokenModifier {\n+    Definition = 0,\n+    Readonly = 1,\n+    Async = 2,\n+}\n+\n+impl SemanticTokenModifier {\n+    /// Returns all supported token modifiers for LSP capabilities.\n+    pub fn all() -> Vec<&'static str> {\n+        vec![\"definition\", \"readonly\", \"async\"]\n+    }\n+}\n+\n+/// A semantic token with its position and classification.\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct SemanticToken {\n+    pub range: TextRange,\n+    pub token_type: SemanticTokenType,\n+    pub modifiers: Vec<SemanticTokenModifier>,\n+}\n+\n+/// The result of semantic tokenization.\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct SemanticTokens {\n+    pub tokens: Vec<SemanticToken>,\n+}\n+\n+/// Generates semantic tokens for a Python file within the specified range.\n+/// Pass a full file range to get tokens for the entire file.\n+pub fn semantic_tokens(db: &dyn Db, file: File, range: TextRange) -> Option<SemanticTokens> {\n+    let parsed = parsed_module(db, file).load(db);\n+    let semantic_model = SemanticModel::new(db, file);\n+\n+    let mut visitor = SemanticTokenVisitor::new(db, &semantic_model, range);\n+    visitor.visit_body(parsed.suite());\n+\n+    Some(SemanticTokens {\n+        tokens: visitor.tokens,\n+    })\n+}\n+\n+/// AST visitor that collects semantic tokens.\n+struct SemanticTokenVisitor<'db> {\n+    #[allow(dead_code)]\n+    db: &'db dyn Db,\n+    #[allow(dead_code)]\n+    semantic_model: &'db SemanticModel<'db>,\n+    tokens: Vec<SemanticToken>,\n+    in_class_scope: bool,\n+    in_type_annotation: bool,\n+    range_filter: TextRange,\n+}\n+\n+impl<'db> SemanticTokenVisitor<'db> {\n+    fn new(\n+        db: &'db dyn Db,\n+        semantic_model: &'db SemanticModel<'db>,\n+        range_filter: TextRange,\n+    ) -> Self {\n+        Self {\n+            db,\n+            semantic_model,\n+            tokens: Vec::new(),\n+            in_class_scope: false,\n+            in_type_annotation: false,\n+            range_filter,\n+        }\n+    }\n+\n+    fn add_token(\n+        &mut self,\n+        range: TextRange,\n+        token_type: SemanticTokenType,\n+        modifiers: Vec<SemanticTokenModifier>,\n+    ) {\n+        // Only emit tokens that intersect with the range filter\n+        if range.intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        self.tokens.push(SemanticToken {\n+            range,\n+            token_type,\n+            modifiers,\n+        });\n+    }\n+\n+    fn is_constant_name(name: &str) -> bool {\n+        name.chars().all(|c| c.is_uppercase() || c == '_') && name.len() > 1\n+    }\n+\n+    fn classify_name(\n+        &self,\n+        name: &ast::ExprName,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        // Try to get the inferred type of this name expression using semantic analysis\n+        let ty = name.inferred_type(self.semantic_model);\n+        let name_str = name.id.as_str();\n+        self.classify_from_type_and_name_str(ty, name_str)\n+    }\n+\n+    fn classify_from_type_and_name_str(\n+        &self,\n+        ty: Type,\n+        name_str: &str,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        let mut modifiers = vec![];\n+\n+        // In type annotation contexts, names that refer to nominal instances or protocol instances\n+        // should be classified as Class tokens (e.g., \"int\" in \"x: int\" should be a Class token)\n+        if self.in_type_annotation {\n+            match ty {\n+                Type::NominalInstance(_) | Type::ProtocolInstance(_) => {\n+                    return (SemanticTokenType::Class, modifiers);\n+                }\n+                _ => {\n+                    // Continue with normal classification for other types in annotations\n+                }\n+            }\n+        }\n+\n+        match ty {\n+            Type::ClassLiteral(_) => (SemanticTokenType::Class, modifiers),\n+            Type::TypeVar(_) => (SemanticTokenType::TypeParameter, modifiers),\n+            _ if ty.is_function_literal() => {\n+                // Check if this is a method based on current scope\n+                if self.in_class_scope {\n+                    (SemanticTokenType::Method, modifiers)\n+                } else {\n+                    (SemanticTokenType::Function, modifiers)\n+                }\n+            }\n+            _ if ty.is_bound_method() => (SemanticTokenType::Method, modifiers),\n+            _ if ty.into_module_literal().is_some() => (SemanticTokenType::Namespace, modifiers),\n+            _ => {\n+                // Check for constant naming convention\n+                if Self::is_constant_name(name_str) {\n+                    modifiers.push(SemanticTokenModifier::Readonly);\n+                }\n+                // For other types (variables, modules, etc.), assume variable\n+                (SemanticTokenType::Variable, modifiers)\n+            }\n+        }\n+    }\n+\n+    fn classify_from_type_for_attribute(\n+        ty: Type,\n+        attr_name: &ast::Identifier,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        let attr_name_str = attr_name.id.as_str();\n+        let mut modifiers = vec![];\n+\n+        // Classify based on the inferred type of the attribute\n+        if ty.is_class_literal() {\n+            (SemanticTokenType::Class, modifiers)\n+        } else if ty.is_function_literal() {\n+            // This is a function accessed as an attribute, likely a method\n+            (SemanticTokenType::Method, modifiers)\n+        } else if ty.is_bound_method() {\n+            // Method bound to an instance\n+            (SemanticTokenType::Method, modifiers)\n+        } else if ty.into_module_literal().is_some() {\n+            // Module accessed as an attribute (e.g., from os import path)\n+            (SemanticTokenType::Namespace, modifiers)\n+        } else if ty.is_property_instance() {\n+            // Actual Python property\n+            (SemanticTokenType::Property, modifiers)\n+        } else {\n+            // Check for constant naming convention\n+            if Self::is_constant_name(attr_name_str) {\n+                modifiers.push(SemanticTokenModifier::Readonly);\n+            }\n+\n+            // For other types (variables, constants, etc.), classify as variable\n+            (SemanticTokenType::Variable, modifiers)\n+        }\n+    }\n+\n+    fn classify_parameter(\n+        &self,\n+        _param: &ast::Parameter,\n+        is_first: bool,\n+        func: &ast::StmtFunctionDef,\n+    ) -> SemanticTokenType {\n+        if is_first && self.in_class_scope {\n+            // Check if this is a classmethod (has @classmethod decorator)\n+            let is_classmethod =\n+                func.decorator_list\n+                    .iter()\n+                    .any(|decorator| match &decorator.expression {\n+                        ast::Expr::Name(name) => name.id.as_str() == \"classmethod\",\n+                        ast::Expr::Attribute(attr) => attr.attr.id.as_str() == \"classmethod\",\n+                        _ => false,\n+                    });\n+\n+            // Check if this is a staticmethod (has @staticmethod decorator)\n+            let is_staticmethod =\n+                func.decorator_list\n+                    .iter()\n+                    .any(|decorator| match &decorator.expression {\n+                        ast::Expr::Name(name) => name.id.as_str() == \"staticmethod\",\n+                        ast::Expr::Attribute(attr) => attr.attr.id.as_str() == \"staticmethod\",\n+                        _ => false,\n+                    });\n+\n+            if is_staticmethod {\n+                // Static methods don't have self/cls parameters\n+                SemanticTokenType::Parameter\n+            } else if is_classmethod {\n+                // First parameter of a classmethod is cls parameter\n+                SemanticTokenType::ClsParameter\n+            } else {\n+                // First parameter of an instance method is self parameter\n+                SemanticTokenType::SelfParameter\n+            }\n+        } else {\n+            SemanticTokenType::Parameter\n+        }\n+    }\n+\n+    fn add_dotted_name_tokens(&mut self, name: &ast::Identifier, token_type: SemanticTokenType) {\n+        let name_str = name.id.as_str();\n+        let name_range = name.range();\n+        let name_start = name_range.start();\n+\n+        // Split the dotted name and calculate positions for each part\n+        let mut current_offset = 0usize;\n+        for part in name_str.split('.') {\n+            if !part.is_empty() {\n+                let part_start =\n+                    name_start + ruff_text_size::TextSize::try_from(current_offset).unwrap();\n+                let part_end = part_start + ruff_text_size::TextSize::try_from(part.len()).unwrap();\n+                let part_range = ruff_text_size::TextRange::new(part_start, part_end);\n+\n+                self.add_token(part_range, token_type, vec![]);\n+            }\n+            // Move past this part and the dot\n+            current_offset += part.len() + 1; // +1 for the dot\n+        }\n+    }\n+\n+    fn classify_from_alias_type(\n+        &self,\n+        ty: Type,\n+        local_name: &ast::Identifier,\n+    ) -> (SemanticTokenType, Vec<SemanticTokenModifier>) {\n+        self.classify_from_type_and_name_str(ty, local_name.id.as_str())\n+    }\n+\n+    fn visit_type_annotation(&mut self, annotation: &ast::Expr) {\n+        let prev_in_type_annotation = self.in_type_annotation;\n+        self.in_type_annotation = true;\n+        self.visit_expr(annotation);\n+        self.in_type_annotation = prev_in_type_annotation;\n+    }\n+\n+    fn visit_type_params(&mut self, type_params: &TypeParams) {\n+        for type_param in &type_params.type_params {\n+            self.visit_type_param(type_param);\n+        }\n+    }\n+\n+    fn visit_type_param(&mut self, type_param: &TypeParam) {\n+        // Emit token for the type parameter name\n+        let name_range = type_param.name().range();\n+        self.add_token(\n+            name_range,\n+            SemanticTokenType::TypeParameter,\n+            vec![SemanticTokenModifier::Definition],\n+        );\n+\n+        // Visit bound expression (for TypeVar)\n+        match type_param {\n+            TypeParam::TypeVar(type_var) => {\n+                if let Some(bound) = &type_var.bound {\n+                    self.visit_type_annotation(bound);\n+                }\n+                if let Some(default) = &type_var.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+            TypeParam::ParamSpec(param_spec) => {\n+                if let Some(default) = &param_spec.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+            TypeParam::TypeVarTuple(type_var_tuple) => {\n+                if let Some(default) = &type_var_tuple.default {\n+                    self.visit_type_annotation(default);\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Visit decorators, handling simple name decorators vs complex expressions\n+    fn visit_decorators(&mut self, decorators: &[ast::Decorator]) {\n+        for decorator in decorators {\n+            match &decorator.expression {\n+                ast::Expr::Name(name) => {\n+                    // Simple decorator like @staticmethod - use Decorator token type\n+                    self.add_token(name.range(), SemanticTokenType::Decorator, vec![]);\n+                }\n+                _ => {\n+                    // Complex decorator like @app.route(\"/path\") - use normal expression rules\n+                    self.visit_expr(&decorator.expression);\n+                }\n+            }\n+        }\n+    }\n+}\n+\n+#[allow(clippy::elidable_lifetime_names)]\n+impl<'db> SourceOrderVisitor<'_> for SemanticTokenVisitor<'db> {\n+    fn visit_stmt(&mut self, stmt: &Stmt) {\n+        // If we have a range filter and this statement doesn't intersect, skip it\n+        // as an optimization\n+        if stmt.range().intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        match stmt {\n+            ast::Stmt::FunctionDef(func) => {\n+                // Function name\n+                self.add_token(\n+                    func.name.range(),\n+                    if self.in_class_scope {\n+                        SemanticTokenType::Method\n+                    } else {\n+                        SemanticTokenType::Function\n+                    },\n+                    if func.is_async {\n+                        vec![\n+                            SemanticTokenModifier::Definition,\n+                            SemanticTokenModifier::Async,\n+                        ]\n+                    } else {\n+                        vec![SemanticTokenModifier::Definition]\n+                    },\n+                );\n+\n+                // Type parameters (Python 3.12+ syntax)\n+                if let Some(type_params) = &func.type_params {\n+                    self.visit_type_params(type_params);\n+                }\n+\n+                // Parameters\n+                for (i, param) in func.parameters.args.iter().enumerate() {\n+                    let token_type = self.classify_parameter(&param.parameter, i == 0, func);\n+                    self.add_token(param.parameter.name.range(), token_type, vec![]);\n+\n+                    // Handle parameter type annotations\n+                    if let Some(annotation) = &param.parameter.annotation {\n+                        self.visit_type_annotation(annotation);\n+                    }\n+                }\n+\n+                // Handle return type annotation\n+                if let Some(returns) = &func.returns {\n+                    self.visit_type_annotation(returns);\n+                }\n+\n+                // Visit decorator expressions\n+                self.visit_decorators(&func.decorator_list);\n+\n+                // Clear the in_class_scope flag so inner functions\n+                // are not treated as methods\n+                let prev_in_class = self.in_class_scope;\n+                self.in_class_scope = false;\n+                walk_stmt(self, stmt);\n+                self.in_class_scope = prev_in_class;\n+            }\n+            ast::Stmt::ClassDef(class) => {\n+                // Class name\n+                self.add_token(\n+                    class.name.range(),\n+                    SemanticTokenType::Class,\n+                    vec![SemanticTokenModifier::Definition],\n+                );\n+\n+                // Type parameters (Python 3.12+ syntax)\n+                if let Some(type_params) = &class.type_params {\n+                    self.visit_type_params(type_params);\n+                }\n+\n+                // Handle base classes and type annotations in inheritance\n+                if let Some(arguments) = &class.arguments {\n+                    // Visit base class arguments\n+                    for arg in &arguments.args {\n+                        self.visit_expr(arg);\n+                    }\n+                    // Visit keyword arguments (for metaclass, etc.)\n+                    for keyword in &arguments.keywords {\n+                        self.visit_expr(&keyword.value);\n+                    }\n+                }\n+\n+                // Visit decorator expressions\n+                self.visit_decorators(&class.decorator_list);\n+\n+                let prev_in_class = self.in_class_scope;\n+                self.in_class_scope = true;\n+                walk_stmt(self, stmt);\n+                self.in_class_scope = prev_in_class;\n+            }\n+            ast::Stmt::AnnAssign(assign) => {\n+                // Handle annotated assignments (e.g., x: int = 5)\n+                if let ast::Expr::Name(name) = assign.target.as_ref() {\n+                    let (token_type, modifiers) = self.classify_name(name);\n+                    self.add_token(name.range(), token_type, modifiers);\n+                }\n+\n+                // Handle the type annotation\n+                self.visit_type_annotation(&assign.annotation);\n+\n+                // Handle the value if present\n+                if let Some(value) = &assign.value {\n+                    self.visit_expr(value);\n+                }\n+            }\n+            ast::Stmt::Import(import) => {\n+                for alias in &import.names {\n+                    if let Some(asname) = &alias.asname {\n+                        self.add_token(asname.range(), SemanticTokenType::Namespace, vec![]);\n+                    } else {\n+                        // Create separate tokens for each part of a dotted module name\n+                        self.add_dotted_name_tokens(&alias.name, SemanticTokenType::Namespace);\n+                    }\n+                }\n+                walk_stmt(self, stmt);\n+            }\n+            ast::Stmt::ImportFrom(import) => {\n+                if let Some(module) = &import.module {\n+                    // Create separate tokens for each part of a dotted module name\n+                    self.add_dotted_name_tokens(module, SemanticTokenType::Namespace);\n+                }\n+                for alias in &import.names {\n+                    if let Some(asname) = &alias.asname {\n+                        // For aliased imports (from X import Y as Z), classify Z based on what Y is\n+                        let ty = alias.inferred_type(self.semantic_model);\n+                        let (token_type, modifiers) = self.classify_from_alias_type(ty, asname);\n+                        self.add_token(asname.range(), token_type, modifiers);\n+                    } else {\n+                        // For direct imports (from X import Y), use semantic classification\n+                        let ty = alias.inferred_type(self.semantic_model);\n+                        let (token_type, modifiers) =\n+                            self.classify_from_alias_type(ty, &alias.name);\n+                        self.add_token(alias.name.range(), token_type, modifiers);\n+                    }\n+                }\n+                walk_stmt(self, stmt);\n+            }\n+            _ => {\n+                // For all other statement types, let the default visitor handle them\n+                walk_stmt(self, stmt);\n+            }\n+        }\n+    }\n+\n+    fn visit_expr(&mut self, expr: &Expr) {\n+        // If we have a range filter and this statement doesn't intersect, skip it\n+        // as an optimization\n+        if expr.range().intersect(self.range_filter).is_none() {\n+            return;\n+        }\n+\n+        match expr {\n+            ast::Expr::Name(name) => {\n+                let (token_type, modifiers) = self.classify_name(name);\n+                self.add_token(name.range(), token_type, modifiers);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::Attribute(attr) => {\n+                // Use semantic analysis to determine the correct token type for the attribute\n+                let ty = expr.inferred_type(self.semantic_model);\n+                let (token_type, modifiers) =\n+                    Self::classify_from_type_for_attribute(ty, &attr.attr);\n+\n+                self.add_token(attr.attr.range(), token_type, modifiers);\n+                // Continue visiting the base expression\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::Call(call) => {\n+                // The function being called\n+                if let ast::Expr::Name(name) = call.func.as_ref() {\n+                    self.add_token(name.range(), SemanticTokenType::Function, vec![]);\n+                }\n+                // Continue visiting arguments\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::StringLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::String, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::NumberLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::Number, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::BooleanLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::BuiltinConstant, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            ast::Expr::NoneLiteral(_) => {\n+                self.add_token(expr.range(), SemanticTokenType::BuiltinConstant, vec![]);\n+                walk_expr(self, expr);\n+            }\n+            _ => {\n+                // For all other expression types, let the default visitor handle them\n+                walk_expr(self, expr);\n+            }\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+    use crate::tests::cursor_test;\n+    use ruff_db::source::source_text;\n+    use ruff_text_size::{TextLen, TextRange};\n+\n+    /// Helper function to get full file range for testing\n+    fn full_file_range(db: &dyn Db, file: File) -> TextRange {\n+        let source = source_text(db, file);\n+        TextRange::new(0.into(), source.text_len())\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_basic() {\n+        let test = cursor_test(\"def foo(): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+        assert!(!tokens.tokens.is_empty());\n+\n+        // Should have at least a function name token\n+        let function_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Function))\n+            .collect();\n+        assert!(!function_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_class() {\n+        let test = cursor_test(\"class MyClass: pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a class token\n+        let class_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Class))\n+            .collect();\n+        assert!(!class_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_variables() {\n+        let test = cursor_test(\"x = 42\\ny = 'hello'<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have variable tokens\n+        let variable_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Variable))\n+            .collect();\n+        assert!(variable_tokens.len() >= 2);\n+\n+        // Should have number and string tokens\n+        let number_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Number))\n+            .collect();\n+        assert!(!number_tokens.is_empty());\n+\n+        let string_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::String))\n+            .collect();\n+        assert!(!string_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_self_parameter() {\n+        let test = cursor_test(\"class MyClass:\\n    def method(self, x): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a self parameter token\n+        let self_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::SelfParameter))\n+            .collect();\n+        assert!(!self_tokens.is_empty());\n+\n+        // Should have a regular parameter token for x\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(!param_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_cls_parameter() {\n+        let test =\n+            cursor_test(\"class MyClass:\\n    @classmethod\\n    def method(cls, x): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a cls parameter token\n+        let cls_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::ClsParameter))\n+            .collect();\n+        assert!(!cls_tokens.is_empty());\n+\n+        // Should have a regular parameter token for x\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(!param_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_staticmethod_parameter() {\n+        let test =\n+            cursor_test(\"class MyClass:\\n    @staticmethod\\n    def method(x, y): pass<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have only regular parameter tokens (no self/cls)\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(param_tokens.len() >= 2);\n+\n+        // Should not have self or cls parameter tokens\n+        let self_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::SelfParameter))\n+            .collect();\n+        assert!(self_tokens.is_empty());\n+\n+        let cls_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::ClsParameter))\n+            .collect();\n+        assert!(cls_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_custom_self_cls_names() {\n+        let test = cursor_test(\n+            \"class MyClass:\\n    def method(instance, x): pass\\n    @classmethod\\n    def other(klass, y): pass<CURSOR>\",\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a self parameter token for \"instance\"\n+        let self_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::SelfParameter))\n+            .collect();\n+        assert!(!self_tokens.is_empty());\n+\n+        // Should have a cls parameter token for \"klass\"\n+        let cls_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::ClsParameter))\n+            .collect();\n+        assert!(!cls_tokens.is_empty());\n+\n+        // Should have regular parameter tokens for x and y\n+        let param_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Parameter))\n+            .collect();\n+        assert!(param_tokens.len() >= 2);\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_modifiers() {\n+        let test = cursor_test(\n+            \"class MyClass:\\n    CONSTANT = 42\\n    async def method(self): pass<CURSOR>\",\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have a class token with Definition modifier\n+        let class_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Class))\n+            .collect();\n+        assert!(!class_tokens.is_empty());\n+        assert!(\n+            class_tokens[0]\n+                .modifiers\n+                .contains(&SemanticTokenModifier::Definition)\n+        );\n+\n+        // Should have a constant with Readonly modifier\n+        let constant_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                matches!(\n+                    t.token_type,\n+                    SemanticTokenType::Property | SemanticTokenType::Variable\n+                ) && t.modifiers.contains(&SemanticTokenModifier::Readonly)\n+            })\n+            .collect();\n+        assert!(!constant_tokens.is_empty());\n+\n+        // Should have an async method with Async modifier\n+        let async_method_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                matches!(t.token_type, SemanticTokenType::Method)\n+                    && t.modifiers.contains(&SemanticTokenModifier::Async)\n+            })\n+            .collect();\n+        assert!(!async_method_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_classification_vs_heuristic() {\n+        let test = cursor_test(\n+            r#\"\n+import sys\n+class MyClass:\n+    pass\n+\n+def my_function():\n+    return 42\n+\n+x = MyClass()\n+y = my_function()\n+z = sys.version<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have module tokens for imports\n+        let module_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Namespace))\n+            .collect();\n+        assert!(!module_tokens.is_empty());\n+\n+        // Should have class tokens\n+        let class_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Class))\n+            .collect();\n+        assert!(!class_tokens.is_empty());\n+\n+        // Should have function tokens\n+        let function_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Function))\n+            .collect();\n+        assert!(!function_tokens.is_empty());\n+\n+        // Should have variable tokens for assignments\n+        let variable_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Variable))\n+            .collect();\n+        assert!(variable_tokens.len() >= 3); // x, y, z\n+    }\n+\n+    #[test]\n+    fn test_builtin_constants() {\n+        let test = cursor_test(\"x = True\\ny = False\\nz = None<CURSOR>\");\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have builtin constant tokens for True, False, and None\n+        let builtin_constant_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::BuiltinConstant))\n+            .collect();\n+        assert_eq!(builtin_constant_tokens.len(), 3);\n+\n+        // Should have variable tokens for x, y, z\n+        let variable_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Variable))\n+            .collect();\n+        assert_eq!(variable_tokens.len(), 3);\n+    }\n+\n+    #[test]\n+    fn test_builtin_constants_in_expressions() {\n+        let test = cursor_test(\n+            r#\"\n+def check(value):\n+    if value is None:\n+        return False\n+    return True\n+\n+result = check(None)<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have builtin constant tokens for None, False, True, and None again\n+        let builtin_constant_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::BuiltinConstant))\n+            .collect();\n+        assert_eq!(builtin_constant_tokens.len(), 4); // None, False, True, None\n+\n+        // Should have function tokens\n+        let function_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Function))\n+            .collect();\n+        assert!(!function_tokens.is_empty());\n+    }\n+\n+    #[test]\n+    fn test_semantic_tokens_range() {\n+        let test = cursor_test(\n+            r#\"\n+def function1():\n+    x = 42\n+    return x\n+\n+def function2():\n+    y = \"hello\"\n+    z = True\n+    return y + z<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+        let full_tokens = result.unwrap();\n+\n+        // Get the range that covers only the second function\n+        let source = ruff_db::source::source_text(&test.db, test.cursor.file);\n+        let lines: Vec<&str> = source.split('\\n').collect();\n+\n+        // Find the start of function2 (around line 5, offset roughly starts at second function)\n+        let mut function2_start = 0;\n+        for (i, line) in lines.iter().enumerate() {\n+            if line.contains(\"def function2\") {\n+                // Get the character offset for the start of this line\n+                function2_start = lines[..i].iter().map(|l| l.len() + 1).sum(); // +1 for newline\n+                break;\n+            }\n+        }\n+\n+        let range = ruff_text_size::TextRange::new(\n+            ruff_text_size::TextSize::try_from(function2_start).unwrap(),\n+            ruff_text_size::TextSize::try_from(source.len()).unwrap(),\n+        );\n+\n+        let range_result = semantic_tokens(&test.db, test.cursor.file, range);\n+        assert!(range_result.is_some());\n+        let range_tokens = range_result.unwrap();\n+\n+        // Range-based tokens should have fewer tokens than full scan\n+        // (should exclude tokens from function1)\n+        assert!(range_tokens.tokens.len() < full_tokens.tokens.len());\n+\n+        // Should still have tokens for function2, y, z, \"hello\", True\n+        let function_tokens: Vec<_> = range_tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Function))\n+            .collect();\n+        assert!(!function_tokens.is_empty()); // function2\n+\n+        let variable_tokens: Vec<_> = range_tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Variable))\n+            .collect();\n+        assert!(variable_tokens.len() >= 2); // y, z\n+\n+        let string_tokens: Vec<_> = range_tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::String))\n+            .collect();\n+        assert!(!string_tokens.is_empty()); // \"hello\"\n+\n+        let builtin_tokens: Vec<_> = range_tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::BuiltinConstant))\n+            .collect();\n+        assert!(!builtin_tokens.is_empty()); // True\n+\n+        // Verify that no tokens from range_tokens have ranges outside the requested range\n+        for token in &range_tokens.tokens {\n+            assert!(\n+                range.contains_range(token.range),\n+                \"Token at {:?} is outside requested range {:?}\",\n+                token.range,\n+                range\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    fn test_dotted_module_names() {\n+        let test = cursor_test(\n+            r#\"\n+import os.path\n+import sys.version_info\n+from urllib.parse import urlparse\n+from collections.abc import Mapping<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Should have module tokens for each part of dotted names\n+        let module_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| matches!(t.token_type, SemanticTokenType::Namespace))\n+            .collect();\n+\n+        // Should have tokens for: os, path, sys, version_info, urllib, parse, collections, abc\n+        // That's 8 separate module tokens\n+        assert!(module_tokens.len() >= 8);\n+\n+        // Should have tokens for imported names with correct classifications\n+        let source = ruff_db::source::source_text(&test.db, test.cursor.file);\n+\n+        // urlparse should be classified based on its actual semantic type (likely Function)\n+        let urlparse_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"urlparse\"\n+            })\n+            .collect();\n+        assert_eq!(urlparse_tokens.len(), 1, \"Expected 1 token for urlparse\");\n+        // urlparse is a function, so it should be classified as Function\n+        assert!(\n+            matches!(\n+                urlparse_tokens[0].token_type,\n+                SemanticTokenType::Function | SemanticTokenType::Variable\n+            ),\n+            \"urlparse should be classified as Function or Variable\"\n+        );\n+\n+        // Mapping should be classified as a class (ABC/Protocol)\n+        let mapping_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"Mapping\" && matches!(t.token_type, SemanticTokenType::Class)\n+            })\n+            .collect();\n+        assert_eq!(\n+            mapping_tokens.len(),\n+            1,\n+            \"Expected 1 class token for Mapping\"\n+        );\n+        let mapping_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"Mapping\" && matches!(t.token_type, SemanticTokenType::Class)\n+            })\n+            .collect();\n+        assert_eq!(\n+            mapping_tokens.len(),\n+            1,\n+            \"Expected 1 class token for Mapping\"\n+        );\n+\n+        // Verify that none of the module tokens contain periods\n+        // by checking that each token's text length matches what we expect\n+        let source = ruff_db::source::source_text(&test.db, test.cursor.file);\n+        for token in &module_tokens {\n+            let token_text = &source[token.range];\n+            assert!(\n+                !token_text.contains('.'),\n+                \"Module token should not contain periods: '{token_text}'\"\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    fn test_module_type_classification() {\n+        let test = cursor_test(\n+            r#\"\n+import os\n+import sys\n+from collections import defaultdict\n+\n+# os and sys should be classified as namespace/module types\n+x = os\n+y = sys<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+\n+        // Find tokens for imported modules\n+        let source = ruff_db::source::source_text(&test.db, test.cursor.file);\n+        let module_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                matches!(t.token_type, SemanticTokenType::Namespace)\n+                    && (token_text == \"os\" || token_text == \"sys\")\n+            })\n+            .collect();\n+\n+        // Should have 4 namespace tokens: os, sys (in imports), os, sys (in assignments)\n+        assert_eq!(\n+            module_tokens.len(),\n+            4,\n+            \"Expected 4 namespace tokens for module references, got {}\",\n+            module_tokens.len()\n+        );\n+\n+        // Verify that variables assigned to modules are also classified as namespace\n+        let xy_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                (token_text == \"x\" || token_text == \"y\")\n+                    && matches!(t.token_type, SemanticTokenType::Namespace)\n+            })\n+            .collect();\n+\n+        // Should have 2 namespace tokens for x and y (since they hold module values)\n+        assert_eq!(\n+            xy_tokens.len(),\n+            2,\n+            \"Expected 2 namespace tokens for x and y, got {}\",\n+            xy_tokens.len()\n+        );\n+\n+        // Verify that defaultdict is classified based on its semantic type (likely Class since it's a constructor)\n+        let defaultdict_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"defaultdict\"\n+            })\n+            .collect();\n+\n+        assert_eq!(\n+            defaultdict_tokens.len(),\n+            1,\n+            \"Expected 1 token for defaultdict, got {}\",\n+            defaultdict_tokens.len()\n+        );\n+        // defaultdict is actually a class constructor, so it should be classified as Class\n+        assert!(\n+            matches!(\n+                defaultdict_tokens[0].token_type,\n+                SemanticTokenType::Class | SemanticTokenType::Variable\n+            ),\n+            \"defaultdict should be classified as Class or Variable based on semantic analysis\"\n+        );\n+    }\n+\n+    #[test]\n+    fn test_import_classification() {\n+        let test = cursor_test(\n+            r#\"\n+from os import path\n+from collections import defaultdict, OrderedDict, Counter\n+from typing import List, Dict, Optional\n+from mymodule import CONSTANT, my_function, MyClass<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+        let source = ruff_db::source::source_text(&test.db, test.cursor.file);\n+\n+        // path should be classified as namespace (since os.path is actually a module)\n+        let path_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"path\" && matches!(t.token_type, SemanticTokenType::Namespace)\n+            })\n+            .collect();\n+        assert_eq!(\n+            path_tokens.len(),\n+            1,\n+            \"Expected 1 namespace token for path (os.path is a module)\"\n+        );\n+\n+        // defaultdict should be classified based on its actual semantic type (likely Function)\n+        let defaultdict_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"defaultdict\"\n+            })\n+            .collect();\n+\n+        if defaultdict_tokens.is_empty() {\n+            panic!(\"No tokens found for 'defaultdict'\");\n+        } else {\n+            // defaultdict is actually a class constructor, so it might be classified as Class\n+            let token_type = &defaultdict_tokens[0].token_type;\n+            assert!(\n+                matches!(\n+                    token_type,\n+                    SemanticTokenType::Variable\n+                        | SemanticTokenType::Class\n+                        | SemanticTokenType::Function\n+                ),\n+                \"defaultdict should be classified as Variable, Class, or Function, got {token_type:?}\"\n+            );\n+        }\n+\n+        // The remaining tests are more flexible since semantic analysis might not have complete info\n+        // for all imports, especially from unresolved modules\n+\n+        // Just verify that we have tokens for the expected imported names\n+        let expected_names = vec![\n+            \"OrderedDict\",\n+            \"Counter\",\n+            \"List\",\n+            \"Dict\",\n+            \"Optional\",\n+            \"CONSTANT\",\n+            \"my_function\",\n+            \"MyClass\",\n+        ];\n+        for name in expected_names {\n+            let name_tokens: Vec<_> = tokens\n+                .tokens\n+                .iter()\n+                .filter(|t| {\n+                    let token_text = &source[t.range];\n+                    token_text == name\n+                })\n+                .collect();\n+            assert!(\n+                !name_tokens.is_empty(),\n+                \"Expected at least 1 token for {name}\"\n+            );\n+        }\n+\n+        // CONSTANT should have readonly modifier if it's classified as Variable\n+        let constant_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"CONSTANT\"\n+            })\n+            .collect();\n+        if !constant_tokens.is_empty()\n+            && matches!(constant_tokens[0].token_type, SemanticTokenType::Variable)\n+        {\n+            assert!(\n+                constant_tokens[0]\n+                    .modifiers\n+                    .contains(&SemanticTokenModifier::Readonly),\n+                \"CONSTANT should have readonly modifier when classified as Variable\"\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    fn test_attribute_classification() {\n+        let test = cursor_test(\n+            r#\"\n+import os\n+import sys\n+from collections import defaultdict\n+from typing import List\n+\n+class MyClass:\n+    CONSTANT = 42\n+    \n+    def method(self):\n+        return \"hello\"\n+    \n+    @property\n+    def prop(self):\n+        return self.CONSTANT\n+\n+obj = MyClass()\n+\n+# Test various attribute accesses\n+x = os.path              # path should be namespace (module)\n+y = obj.method           # method should be method (bound method)\n+z = obj.CONSTANT         # CONSTANT should be variable with readonly modifier\n+w = obj.prop             # prop should be property\n+v = MyClass.method       # method should be method (function)\n+u = List.__name__        # __name__ should be variable<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+        let source = ruff_db::source::source_text(&test.db, test.cursor.file);\n+\n+        // Find all tokens and create a map for easier testing\n+        let mut token_map = std::collections::HashMap::new();\n+        for token in &tokens.tokens {\n+            let token_text = &source[token.range];\n+            token_map\n+                .entry(token_text.to_string())\n+                .or_insert_with(Vec::new)\n+                .push(token);\n+        }\n+\n+        // Test path attribute (should be namespace since os.path is a module)\n+        let path_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"path\" && matches!(t.token_type, SemanticTokenType::Namespace)\n+            })\n+            .collect();\n+        assert!(\n+            !path_tokens.is_empty(),\n+            \"Expected at least 1 namespace token for 'path' attribute\"\n+        );\n+\n+        // Test method attribute (should be method - bound method)\n+        let method_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"method\" && matches!(t.token_type, SemanticTokenType::Method)\n+            })\n+            .collect();\n+        assert!(\n+            !method_tokens.is_empty(),\n+            \"Expected at least 1 method token for 'method' attribute\"\n+        );\n+\n+        // Test CONSTANT attribute (should be variable with readonly modifier)\n+        let constant_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"CONSTANT\"\n+                    && matches!(\n+                        t.token_type,\n+                        SemanticTokenType::Variable | SemanticTokenType::Property\n+                    )\n+                    && t.modifiers.contains(&SemanticTokenModifier::Readonly)\n+            })\n+            .collect();\n+        assert!(\n+            !constant_tokens.is_empty(),\n+            \"Expected at least 1 variable/property token with readonly modifier for 'CONSTANT' attribute\"\n+        );\n+\n+        // Test property attribute (should be property)\n+        // Note: This might not work perfectly if the semantic analyzer doesn't have full property info\n+        // but we should have at least a variable token for it\n+        let prop_or_var_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"prop\"\n+                    && matches!(\n+                        t.token_type,\n+                        SemanticTokenType::Property | SemanticTokenType::Variable\n+                    )\n+            })\n+            .collect();\n+        assert!(\n+            !prop_or_var_tokens.is_empty(),\n+            \"Expected at least 1 property/variable token for 'prop' attribute\"\n+        );\n+\n+        // Test __name__ attribute (should be variable since it's a string attribute)\n+        let name_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                token_text == \"__name__\" && matches!(t.token_type, SemanticTokenType::Variable)\n+            })\n+            .collect();\n+        assert!(\n+            !name_tokens.is_empty(),\n+            \"Expected at least 1 variable token for '__name__' attribute\"\n+        );\n+    }\n+\n+    #[test]\n+    fn test_attribute_fallback_classification() {\n+        let test = cursor_test(\n+            r#\"\n+class MyClass:\n+    some_attr = \"value\"\n+    \n+obj = MyClass()\n+# Test attribute that might not have detailed semantic info\n+x = obj.some_attr        # Should fall back to variable, not property\n+y = obj.unknown_attr     # Should fall back to variable<CURSOR>\n+\"#,\n+        );\n+\n+        let result = semantic_tokens(\n+            &test.db,\n+            test.cursor.file,\n+            full_file_range(&test.db, test.cursor.file),\n+        );\n+        assert!(result.is_some());\n+\n+        let tokens = result.unwrap();\n+        let source = ruff_db::source::source_text(&test.db, test.cursor.file);\n+\n+        // Test that attributes with unknown/basic types fall back to variable, not property\n+        let attr_tokens: Vec<_> = tokens\n+            .tokens\n+            .iter()\n+            .filter(|t| {\n+                let token_text = &source[t.range];\n+                (token_text == \"some_attr\" || token_text == \"unknown_attr\")\n+                    && matches!(\n+                        t.token_type,\n+                        SemanticTokenType::Variable | SemanticTokenType::Property\n+                    )\n+            })\n+            .collect();\n+\n+        // We should have tokens for both attributes\n+        assert!(\n+            attr_tokens.len() >= 2,\n+            \"Expected at least 2 tokens for attribute expressions\"\n+        );",
        "comment_created_at": "2025-07-03T08:52:47+00:00",
        "comment_author": "dhruvmanila",
        "comment_body": "Do you see any benefit of using `>=` instead of `assert_eq`? It might be useful to assert for equality to avoid any future change to go unnoticed.",
        "pr_file_module": null
      }
    ]
  }
]