[
  {
    "discussion_id": "2182272287",
    "pr_number": 19108,
    "pr_file": "crates/ty_server/src/server/api/requests/semantic_tokens.rs",
    "created_at": "2025-07-03T09:10:27+00:00",
    "commented_code": "+use std::borrow::Cow;\n+\n+use crate::DocumentSnapshot;\n+use crate::document::PositionExt;\n+use crate::server::api::traits::{BackgroundDocumentRequestHandler, RequestHandler};\n+use crate::session::client::Client;\n+use lsp_types::{\n+    SemanticToken, SemanticTokens, SemanticTokensParams, SemanticTokensRangeParams,\n+    SemanticTokensRangeResult, SemanticTokensResult, Url,\n+};\n+use ruff_db::source::{line_index, source_text};\n+use ruff_text_size::{TextLen, TextRange};\n+use ty_ide::semantic_tokens;\n+use ty_project::ProjectDatabase;\n+\n+/// Common logic for generating semantic tokens, either for full document or a specific range.\n+/// If no range is provided, the entire file is processed.\n+fn generate_semantic_tokens(\n+    db: &ProjectDatabase,\n+    file: ruff_db::files::File,\n+    range: Option<TextRange>,\n+) -> Option<Vec<SemanticToken>> {\n+    let source = source_text(db, file);\n+    let line_index = line_index(db, file);\n+\n+    let requested_range = range.unwrap_or_else(|| TextRange::new(0.into(), source.text_len()));\n+    let semantic_token_data = semantic_tokens(db, file, requested_range);\n+\n+    let semantic_token_data = semantic_token_data?;\n+\n+    // Convert semantic tokens to LSP format with delta encoding\n+    // Sort tokens by position to ensure proper delta encoding\n+    // This prevents integer underflow when computing deltas for out-of-order tokens\n+    let mut sorted_tokens = semantic_token_data.tokens;\n+    sorted_tokens.sort_by_key(|token| token.range.start());\n+\n+    // Convert semantic tokens to LSP format\n+    let mut lsp_tokens = Vec::new();\n+    let mut prev_line = 0u32;\n+    let mut prev_start = 0u32;\n+\n+    for token in sorted_tokens {\n+        let start_position = line_index.line_column(token.range.start(), &source);\n+        let line = u32::try_from(start_position.line.to_zero_indexed()).unwrap_or(u32::MAX);\n+        let character = u32::try_from(start_position.column.to_zero_indexed()).unwrap_or(u32::MAX);\n+        let length = token.range.len().to_u32();\n+        let token_type = token.token_type as u32;\n+        let token_modifiers = token\n+            .modifiers\n+            .iter()\n+            .fold(0u32, |acc, modifier| acc | (1 << (*modifier as u32)));\n+\n+        // LSP semantic tokens are encoded as deltas\n+        let delta_line = line - prev_line;\n+        let delta_start = if delta_line == 0 {\n+            character - prev_start\n+        } else {\n+            character\n+        };\n+\n+        lsp_tokens.push(SemanticToken {\n+            delta_line,\n+            delta_start,\n+            length,\n+            token_type,\n+            token_modifiers_bitset: token_modifiers,\n+        });\n+\n+        prev_line = line;\n+        prev_start = character;\n+    }\n+\n+    Some(lsp_tokens)\n+}\n+\n+pub(crate) struct SemanticTokensRequestHandler;\n+\n+impl RequestHandler for SemanticTokensRequestHandler {\n+    type RequestType = lsp_types::request::SemanticTokensFullRequest;\n+}\n+\n+impl BackgroundDocumentRequestHandler for SemanticTokensRequestHandler {\n+    fn document_url(params: &SemanticTokensParams) -> Cow<Url> {\n+        Cow::Borrowed(&params.text_document.uri)\n+    }\n+\n+    fn run_with_snapshot(\n+        db: &ProjectDatabase,\n+        snapshot: DocumentSnapshot,\n+        _client: &Client,\n+        params: SemanticTokensParams,\n+    ) -> crate::server::Result<Option<SemanticTokensResult>> {\n+        if snapshot.client_settings().is_language_services_disabled() {\n+            return Ok(None);\n+        }\n+\n+        let Some(file) = snapshot.file(db) else {\n+            tracing::debug!(\"Failed to resolve file for {:?}\", params);\n+            return Ok(None);\n+        };\n+\n+        let Some(lsp_tokens) = generate_semantic_tokens(db, file, None) else {\n+            return Ok(None);\n+        };\n+\n+        Ok(Some(SemanticTokensResult::Tokens(SemanticTokens {\n+            result_id: None,\n+            data: lsp_tokens,\n+        })))\n+    }\n+}\n+\n+pub(crate) struct SemanticTokensRangeRequestHandler;\n+\n+impl RequestHandler for SemanticTokensRangeRequestHandler {\n+    type RequestType = lsp_types::request::SemanticTokensRangeRequest;\n+}\n+\n+impl BackgroundDocumentRequestHandler for SemanticTokensRangeRequestHandler {\n+    fn document_url(params: &SemanticTokensRangeParams) -> Cow<Url> {\n+        Cow::Borrowed(&params.text_document.uri)\n+    }\n+\n+    fn run_with_snapshot(\n+        db: &ProjectDatabase,\n+        snapshot: DocumentSnapshot,\n+        _client: &Client,\n+        params: SemanticTokensRangeParams,\n+    ) -> crate::server::Result<Option<SemanticTokensRangeResult>> {\n+        if snapshot.client_settings().is_language_services_disabled() {\n+            return Ok(None);\n+        }\n+\n+        let Some(file) = snapshot.file(db) else {\n+            tracing::debug!(\"Failed to resolve file for {:?}\", params);\n+            return Ok(None);\n+        };\n+\n+        let source = source_text(db, file);\n+        let line_index = line_index(db, file);\n+\n+        // Convert LSP range to text offsets\n+        let start_offset =\n+            params\n+                .range\n+                .start\n+                .to_text_size(&source, &line_index, snapshot.encoding());\n+\n+        let end_offset = params\n+            .range\n+            .end\n+            .to_text_size(&source, &line_index, snapshot.encoding());\n+\n+        let requested_range = ruff_text_size::TextRange::new(start_offset, end_offset);",
    "repo_full_name": "astral-sh/ruff",
    "discussion_comments": [
      {
        "comment_id": "2182272287",
        "repo_full_name": "astral-sh/ruff",
        "pr_number": 19108,
        "pr_file": "crates/ty_server/src/server/api/requests/semantic_tokens.rs",
        "discussion_id": "2182272287",
        "commented_code": "@@ -0,0 +1,165 @@\n+use std::borrow::Cow;\n+\n+use crate::DocumentSnapshot;\n+use crate::document::PositionExt;\n+use crate::server::api::traits::{BackgroundDocumentRequestHandler, RequestHandler};\n+use crate::session::client::Client;\n+use lsp_types::{\n+    SemanticToken, SemanticTokens, SemanticTokensParams, SemanticTokensRangeParams,\n+    SemanticTokensRangeResult, SemanticTokensResult, Url,\n+};\n+use ruff_db::source::{line_index, source_text};\n+use ruff_text_size::{TextLen, TextRange};\n+use ty_ide::semantic_tokens;\n+use ty_project::ProjectDatabase;\n+\n+/// Common logic for generating semantic tokens, either for full document or a specific range.\n+/// If no range is provided, the entire file is processed.\n+fn generate_semantic_tokens(\n+    db: &ProjectDatabase,\n+    file: ruff_db::files::File,\n+    range: Option<TextRange>,\n+) -> Option<Vec<SemanticToken>> {\n+    let source = source_text(db, file);\n+    let line_index = line_index(db, file);\n+\n+    let requested_range = range.unwrap_or_else(|| TextRange::new(0.into(), source.text_len()));\n+    let semantic_token_data = semantic_tokens(db, file, requested_range);\n+\n+    let semantic_token_data = semantic_token_data?;\n+\n+    // Convert semantic tokens to LSP format with delta encoding\n+    // Sort tokens by position to ensure proper delta encoding\n+    // This prevents integer underflow when computing deltas for out-of-order tokens\n+    let mut sorted_tokens = semantic_token_data.tokens;\n+    sorted_tokens.sort_by_key(|token| token.range.start());\n+\n+    // Convert semantic tokens to LSP format\n+    let mut lsp_tokens = Vec::new();\n+    let mut prev_line = 0u32;\n+    let mut prev_start = 0u32;\n+\n+    for token in sorted_tokens {\n+        let start_position = line_index.line_column(token.range.start(), &source);\n+        let line = u32::try_from(start_position.line.to_zero_indexed()).unwrap_or(u32::MAX);\n+        let character = u32::try_from(start_position.column.to_zero_indexed()).unwrap_or(u32::MAX);\n+        let length = token.range.len().to_u32();\n+        let token_type = token.token_type as u32;\n+        let token_modifiers = token\n+            .modifiers\n+            .iter()\n+            .fold(0u32, |acc, modifier| acc | (1 << (*modifier as u32)));\n+\n+        // LSP semantic tokens are encoded as deltas\n+        let delta_line = line - prev_line;\n+        let delta_start = if delta_line == 0 {\n+            character - prev_start\n+        } else {\n+            character\n+        };\n+\n+        lsp_tokens.push(SemanticToken {\n+            delta_line,\n+            delta_start,\n+            length,\n+            token_type,\n+            token_modifiers_bitset: token_modifiers,\n+        });\n+\n+        prev_line = line;\n+        prev_start = character;\n+    }\n+\n+    Some(lsp_tokens)\n+}\n+\n+pub(crate) struct SemanticTokensRequestHandler;\n+\n+impl RequestHandler for SemanticTokensRequestHandler {\n+    type RequestType = lsp_types::request::SemanticTokensFullRequest;\n+}\n+\n+impl BackgroundDocumentRequestHandler for SemanticTokensRequestHandler {\n+    fn document_url(params: &SemanticTokensParams) -> Cow<Url> {\n+        Cow::Borrowed(&params.text_document.uri)\n+    }\n+\n+    fn run_with_snapshot(\n+        db: &ProjectDatabase,\n+        snapshot: DocumentSnapshot,\n+        _client: &Client,\n+        params: SemanticTokensParams,\n+    ) -> crate::server::Result<Option<SemanticTokensResult>> {\n+        if snapshot.client_settings().is_language_services_disabled() {\n+            return Ok(None);\n+        }\n+\n+        let Some(file) = snapshot.file(db) else {\n+            tracing::debug!(\"Failed to resolve file for {:?}\", params);\n+            return Ok(None);\n+        };\n+\n+        let Some(lsp_tokens) = generate_semantic_tokens(db, file, None) else {\n+            return Ok(None);\n+        };\n+\n+        Ok(Some(SemanticTokensResult::Tokens(SemanticTokens {\n+            result_id: None,\n+            data: lsp_tokens,\n+        })))\n+    }\n+}\n+\n+pub(crate) struct SemanticTokensRangeRequestHandler;\n+\n+impl RequestHandler for SemanticTokensRangeRequestHandler {\n+    type RequestType = lsp_types::request::SemanticTokensRangeRequest;\n+}\n+\n+impl BackgroundDocumentRequestHandler for SemanticTokensRangeRequestHandler {\n+    fn document_url(params: &SemanticTokensRangeParams) -> Cow<Url> {\n+        Cow::Borrowed(&params.text_document.uri)\n+    }\n+\n+    fn run_with_snapshot(\n+        db: &ProjectDatabase,\n+        snapshot: DocumentSnapshot,\n+        _client: &Client,\n+        params: SemanticTokensRangeParams,\n+    ) -> crate::server::Result<Option<SemanticTokensRangeResult>> {\n+        if snapshot.client_settings().is_language_services_disabled() {\n+            return Ok(None);\n+        }\n+\n+        let Some(file) = snapshot.file(db) else {\n+            tracing::debug!(\"Failed to resolve file for {:?}\", params);\n+            return Ok(None);\n+        };\n+\n+        let source = source_text(db, file);\n+        let line_index = line_index(db, file);\n+\n+        // Convert LSP range to text offsets\n+        let start_offset =\n+            params\n+                .range\n+                .start\n+                .to_text_size(&source, &line_index, snapshot.encoding());\n+\n+        let end_offset = params\n+            .range\n+            .end\n+            .to_text_size(&source, &line_index, snapshot.encoding());\n+\n+        let requested_range = ruff_text_size::TextRange::new(start_offset, end_offset);",
        "comment_created_at": "2025-07-03T09:10:27+00:00",
        "comment_author": "dhruvmanila",
        "comment_body": "We can directly convert the range here using https://github.com/astral-sh/ruff/blob/28ab61d8850af9abe7f5b3f0406008b8abde4f8c/crates/ty_server/src/document/range.rs#L89-L101:\r\n\r\n```suggestion\r\n        let requested_range = params.range.to_text_range(...);\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2182288549",
    "pr_number": 19108,
    "pr_file": "crates/ty_server/src/server/api/requests/semantic_tokens.rs",
    "created_at": "2025-07-03T09:16:55+00:00",
    "commented_code": "+use std::borrow::Cow;\n+\n+use crate::DocumentSnapshot;\n+use crate::document::PositionExt;\n+use crate::server::api::traits::{BackgroundDocumentRequestHandler, RequestHandler};\n+use crate::session::client::Client;\n+use lsp_types::{\n+    SemanticToken, SemanticTokens, SemanticTokensParams, SemanticTokensRangeParams,\n+    SemanticTokensRangeResult, SemanticTokensResult, Url,\n+};\n+use ruff_db::source::{line_index, source_text};\n+use ruff_text_size::{TextLen, TextRange};\n+use ty_ide::semantic_tokens;\n+use ty_project::ProjectDatabase;\n+\n+/// Common logic for generating semantic tokens, either for full document or a specific range.\n+/// If no range is provided, the entire file is processed.\n+fn generate_semantic_tokens(\n+    db: &ProjectDatabase,\n+    file: ruff_db::files::File,\n+    range: Option<TextRange>,\n+) -> Option<Vec<SemanticToken>> {\n+    let source = source_text(db, file);\n+    let line_index = line_index(db, file);\n+\n+    let requested_range = range.unwrap_or_else(|| TextRange::new(0.into(), source.text_len()));\n+    let semantic_token_data = semantic_tokens(db, file, requested_range);\n+\n+    let semantic_token_data = semantic_token_data?;\n+\n+    // Convert semantic tokens to LSP format with delta encoding\n+    // Sort tokens by position to ensure proper delta encoding\n+    // This prevents integer underflow when computing deltas for out-of-order tokens\n+    let mut sorted_tokens = semantic_token_data.tokens;\n+    sorted_tokens.sort_by_key(|token| token.range.start());\n+\n+    // Convert semantic tokens to LSP format\n+    let mut lsp_tokens = Vec::new();\n+    let mut prev_line = 0u32;\n+    let mut prev_start = 0u32;\n+\n+    for token in sorted_tokens {\n+        let start_position = line_index.line_column(token.range.start(), &source);\n+        let line = u32::try_from(start_position.line.to_zero_indexed()).unwrap_or(u32::MAX);\n+        let character = u32::try_from(start_position.column.to_zero_indexed()).unwrap_or(u32::MAX);",
    "repo_full_name": "astral-sh/ruff",
    "discussion_comments": [
      {
        "comment_id": "2182288549",
        "repo_full_name": "astral-sh/ruff",
        "pr_number": 19108,
        "pr_file": "crates/ty_server/src/server/api/requests/semantic_tokens.rs",
        "discussion_id": "2182288549",
        "commented_code": "@@ -0,0 +1,165 @@\n+use std::borrow::Cow;\n+\n+use crate::DocumentSnapshot;\n+use crate::document::PositionExt;\n+use crate::server::api::traits::{BackgroundDocumentRequestHandler, RequestHandler};\n+use crate::session::client::Client;\n+use lsp_types::{\n+    SemanticToken, SemanticTokens, SemanticTokensParams, SemanticTokensRangeParams,\n+    SemanticTokensRangeResult, SemanticTokensResult, Url,\n+};\n+use ruff_db::source::{line_index, source_text};\n+use ruff_text_size::{TextLen, TextRange};\n+use ty_ide::semantic_tokens;\n+use ty_project::ProjectDatabase;\n+\n+/// Common logic for generating semantic tokens, either for full document or a specific range.\n+/// If no range is provided, the entire file is processed.\n+fn generate_semantic_tokens(\n+    db: &ProjectDatabase,\n+    file: ruff_db::files::File,\n+    range: Option<TextRange>,\n+) -> Option<Vec<SemanticToken>> {\n+    let source = source_text(db, file);\n+    let line_index = line_index(db, file);\n+\n+    let requested_range = range.unwrap_or_else(|| TextRange::new(0.into(), source.text_len()));\n+    let semantic_token_data = semantic_tokens(db, file, requested_range);\n+\n+    let semantic_token_data = semantic_token_data?;\n+\n+    // Convert semantic tokens to LSP format with delta encoding\n+    // Sort tokens by position to ensure proper delta encoding\n+    // This prevents integer underflow when computing deltas for out-of-order tokens\n+    let mut sorted_tokens = semantic_token_data.tokens;\n+    sorted_tokens.sort_by_key(|token| token.range.start());\n+\n+    // Convert semantic tokens to LSP format\n+    let mut lsp_tokens = Vec::new();\n+    let mut prev_line = 0u32;\n+    let mut prev_start = 0u32;\n+\n+    for token in sorted_tokens {\n+        let start_position = line_index.line_column(token.range.start(), &source);\n+        let line = u32::try_from(start_position.line.to_zero_indexed()).unwrap_or(u32::MAX);\n+        let character = u32::try_from(start_position.column.to_zero_indexed()).unwrap_or(u32::MAX);",
        "comment_created_at": "2025-07-03T09:16:55+00:00",
        "comment_author": "dhruvmanila",
        "comment_body": "The spec says:\r\n\r\n> The `deltaStart` and the `length` values must be encoded using the encoding the client and server agrees on during the `initialize` request (see also [TextDocuments](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocuments)).\r\n\r\nSo, I think we would need to convert them to the LSP values and use them instead.\r\n\r\nFor that, we've \r\n\r\nhttps://github.com/astral-sh/ruff/blob/28ab61d8850af9abe7f5b3f0406008b8abde4f8c/crates/ty_server/src/document/range.rs#L42-L52\r\n\r\nSo, I think this should be:\r\n\r\n```suggestion\r\n        let start_position = token.start().to_position(...);\r\n```\r\n\r\nAnd, then you can access the `line` and `character` values directly like `start_position.line` and `start_position.character`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2171977539",
    "pr_number": 18982,
    "pr_file": "crates/ty_python_semantic/src/semantic_model.rs",
    "created_at": "2025-06-27T12:46:54+00:00",
    "commented_code": "return vec![];\n             }\n         };\n-        let Some(module) = resolve_module(self.db, &module_name) else {\n+        self.module_completions(&module_name)\n+    }\n+\n+    /// Returns completions for symbols available in the given module as if\n+    /// it were imported by this model's `File`.\n+    fn module_completions(&self, module_name: &ModuleName) -> Vec<Completion> {\n+        let Some(module) = resolve_module(self.db, module_name) else {\n             tracing::debug!(\"Could not resolve module from `{module_name:?}`\");\n             return vec![];\n         };\n         let ty = Type::module_literal(self.db, self.file, &module);\n-        crate::types::all_members(self.db, ty).into_iter().collect()\n+        crate::types::all_members(self.db, ty)\n+            .into_iter()\n+            .map(|name| Completion {\n+                name,\n+                builtin: module_name.as_str() == \"builtins\",",
    "repo_full_name": "astral-sh/ruff",
    "discussion_comments": [
      {
        "comment_id": "2171977539",
        "repo_full_name": "astral-sh/ruff",
        "pr_number": 18982,
        "pr_file": "crates/ty_python_semantic/src/semantic_model.rs",
        "discussion_id": "2171977539",
        "commented_code": "@@ -58,26 +58,44 @@ impl<'db> SemanticModel<'db> {\n                 return vec![];\n             }\n         };\n-        let Some(module) = resolve_module(self.db, &module_name) else {\n+        self.module_completions(&module_name)\n+    }\n+\n+    /// Returns completions for symbols available in the given module as if\n+    /// it were imported by this model's `File`.\n+    fn module_completions(&self, module_name: &ModuleName) -> Vec<Completion> {\n+        let Some(module) = resolve_module(self.db, module_name) else {\n             tracing::debug!(\"Could not resolve module from `{module_name:?}`\");\n             return vec![];\n         };\n         let ty = Type::module_literal(self.db, self.file, &module);\n-        crate::types::all_members(self.db, ty).into_iter().collect()\n+        crate::types::all_members(self.db, ty)\n+            .into_iter()\n+            .map(|name| Completion {\n+                name,\n+                builtin: module_name.as_str() == \"builtins\",",
        "comment_created_at": "2025-06-27T12:46:54+00:00",
        "comment_author": "AlexWaygood",
        "comment_body": "nit: we usually use this API for checking whether a module is a specific module from a given search path with a given name\r\n\r\n```suggestion\r\n                builtin: module.is_known(KnownModule::Builtins),\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2172015534",
        "repo_full_name": "astral-sh/ruff",
        "pr_number": 18982,
        "pr_file": "crates/ty_python_semantic/src/semantic_model.rs",
        "discussion_id": "2171977539",
        "commented_code": "@@ -58,26 +58,44 @@ impl<'db> SemanticModel<'db> {\n                 return vec![];\n             }\n         };\n-        let Some(module) = resolve_module(self.db, &module_name) else {\n+        self.module_completions(&module_name)\n+    }\n+\n+    /// Returns completions for symbols available in the given module as if\n+    /// it were imported by this model's `File`.\n+    fn module_completions(&self, module_name: &ModuleName) -> Vec<Completion> {\n+        let Some(module) = resolve_module(self.db, module_name) else {\n             tracing::debug!(\"Could not resolve module from `{module_name:?}`\");\n             return vec![];\n         };\n         let ty = Type::module_literal(self.db, self.file, &module);\n-        crate::types::all_members(self.db, ty).into_iter().collect()\n+        crate::types::all_members(self.db, ty)\n+            .into_iter()\n+            .map(|name| Completion {\n+                name,\n+                builtin: module_name.as_str() == \"builtins\",",
        "comment_created_at": "2025-06-27T13:08:59+00:00",
        "comment_author": "BurntSushi",
        "comment_body": "Oh nice!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2164590499",
    "pr_number": 18763,
    "pr_file": "crates/ruff_linter/src/rules/flake8_use_pathlib/rules/os_path_getsize.rs",
    "created_at": "2025-06-24T17:55:02+00:00",
    "commented_code": "let mut diagnostic = checker.report_diagnostic(OsPathGetsize, range);\n \n-    diagnostic.try_set_fix(|| {\n-        let (import_edit, binding) = checker.importer().get_or_import_symbol(\n-            &ImportRequest::import(\"pathlib\", \"Path\"),\n-            call.start(),\n-            checker.semantic(),\n-        )?;\n+    if is_fix_os_path_getsize_enabled(checker.settings()) {\n+        diagnostic.try_set_fix(|| {\n+            let (import_edit, binding) = checker.importer().get_or_import_symbol(\n+                &ImportRequest::import(\"pathlib\", \"Path\"),\n+                call.start(),\n+                checker.semantic(),\n+            )?;\n \n-        let replacement = format!(\"{binding}({arg_code}).stat().st_size\");\n+            let replacement = if is_path_call(checker, arg) {\n+                format!(\"{arg_code}.stat().st_size\")\n+            } else {\n+                format!(\"{binding}({arg_code}).stat().st_size\")\n+            };\n \n-        Ok(\n-            Fix::safe_edits(Edit::range_replacement(replacement, range), [import_edit])\n-                .with_applicability(applicability),\n-        )\n-    });\n+            Ok(\n+                Fix::safe_edits(Edit::range_replacement(replacement, range), [import_edit])\n+                    .with_applicability(applicability),\n+            )\n+        });\n+    }\n+}\n+\n+fn is_path_call(checker: &Checker, expr: &Expr) -> bool {\n+    match expr {\n+        Expr::Call(expr_call) => match checker.semantic().resolve_qualified_name(&expr_call.func) {\n+            Some(name) => name.segments() == [\"pathlib\", \"Path\"],\n+            None => false,\n+        },\n+        _ => false,\n+    }\n }",
    "repo_full_name": "astral-sh/ruff",
    "discussion_comments": [
      {
        "comment_id": "2164590499",
        "repo_full_name": "astral-sh/ruff",
        "pr_number": 18763,
        "pr_file": "crates/ruff_linter/src/rules/flake8_use_pathlib/rules/os_path_getsize.rs",
        "discussion_id": "2164590499",
        "commented_code": "@@ -94,18 +95,34 @@ pub(crate) fn os_path_getsize(checker: &Checker, call: &ExprCall) {\n \n     let mut diagnostic = checker.report_diagnostic(OsPathGetsize, range);\n \n-    diagnostic.try_set_fix(|| {\n-        let (import_edit, binding) = checker.importer().get_or_import_symbol(\n-            &ImportRequest::import(\"pathlib\", \"Path\"),\n-            call.start(),\n-            checker.semantic(),\n-        )?;\n+    if is_fix_os_path_getsize_enabled(checker.settings()) {\n+        diagnostic.try_set_fix(|| {\n+            let (import_edit, binding) = checker.importer().get_or_import_symbol(\n+                &ImportRequest::import(\"pathlib\", \"Path\"),\n+                call.start(),\n+                checker.semantic(),\n+            )?;\n \n-        let replacement = format!(\"{binding}({arg_code}).stat().st_size\");\n+            let replacement = if is_path_call(checker, arg) {\n+                format!(\"{arg_code}.stat().st_size\")\n+            } else {\n+                format!(\"{binding}({arg_code}).stat().st_size\")\n+            };\n \n-        Ok(\n-            Fix::safe_edits(Edit::range_replacement(replacement, range), [import_edit])\n-                .with_applicability(applicability),\n-        )\n-    });\n+            Ok(\n+                Fix::safe_edits(Edit::range_replacement(replacement, range), [import_edit])\n+                    .with_applicability(applicability),\n+            )\n+        });\n+    }\n+}\n+\n+fn is_path_call(checker: &Checker, expr: &Expr) -> bool {\n+    match expr {\n+        Expr::Call(expr_call) => match checker.semantic().resolve_qualified_name(&expr_call.func) {\n+            Some(name) => name.segments() == [\"pathlib\", \"Path\"],\n+            None => false,\n+        },\n+        _ => false,\n+    }\n }",
        "comment_created_at": "2025-06-24T17:55:02+00:00",
        "comment_author": "ntBre",
        "comment_body": "```suggestion\r\nfn is_path_call(checker: &Checker, expr: &Expr) -> bool {\r\n    expr.as_call_expr().is_some_and(|expr_call| {\r\n        checker\r\n            .semantic()\r\n            .resolve_qualified_name(&expr_call.func)\r\n            .is_some_and(|name| matches!(name.segments(), [\"pathlib\", \"Path\"]))\r\n    })\r\n}\r\n```",
        "pr_file_module": null
      }
    ]
  }
]