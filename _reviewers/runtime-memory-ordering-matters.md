---
title: Memory ordering matters
description: 'When working with shared data in multithreaded environments, memory
  ordering is critical to prevent race conditions and ensure thread safety:


  1. **Avoid transient incorrect states** - Don''t write an incorrect value before
  replacing it with the correct one, as other threads may see the intermediate state:'
repository: dotnet/runtime
label: Concurrency
language: C++
comments_count: 3
repository_stars: 16578
---

When working with shared data in multithreaded environments, memory ordering is critical to prevent race conditions and ensure thread safety:

1. **Avoid transient incorrect states** - Don't write an incorrect value before replacing it with the correct one, as other threads may see the intermediate state:

```cpp
// INCORRECT: Creates a race window where incorrect value is visible
*ppvRetAddrLocation = (void*)pfnHijackFunction;
#if defined(TARGET_ARM64)
*ppvRetAddrLocation = PacSignPtr(*ppvRetAddrLocation);
#endif

// CORRECT: Prepare final value before publishing
void* pvHijackAddr = (void*)pfnHijackFunction;
#if defined(TARGET_ARM64)
pvHijackAddr = PacSignPtr(pvHijackAddr);
#endif
*ppvRetAddrLocation = pvHijackAddr;
```

2. **Use atomic operations correctly** - Pay careful attention to argument order in functions like `InterlockedCompareExchange(destination, exchange, comparand)`:

```cpp
// INCORRECT: Arguments in wrong order
if (versionStart != InterlockedCompareExchange(&s_stackWalkNativeToILCacheVersion, versionStart, versionStart | 1))

// CORRECT: Proper argument order
if (versionStart != InterlockedCompareExchange(&s_stackWalkNativeToILCacheVersion, versionStart | 1, versionStart))
```

3. **Use memory barriers when publishing shared data** - Ensure supporting data is visible before publishing pointers:

```cpp
// INCORRECT: May be reordered
s_stackWalkCacheSize = cacheSize;
s_stackWalkCache = newCache; // Other threads might see the pointer before size is set

// CORRECT: Memory barrier ensures proper ordering
VolatileStore(&s_stackWalkCacheSize, cacheSize);
s_stackWalkCache = newCache;
```

Proper memory ordering is essential for preventing subtle multithreading bugs that may only appear under high load or on specific hardware architectures.


[
  {
    "discussion_id": "2092090898",
    "pr_number": 110472,
    "pr_file": "src/coreclr/nativeaot/Runtime/thread.cpp",
    "created_at": "2025-05-15T23:20:47+00:00",
    "commented_code": "#endif\n\n        *ppvRetAddrLocation = (void*)pfnHijackFunction;\n#if defined(TARGET_ARM64)\n        *ppvRetAddrLocation = PacSignPtr(*ppvRetAddrLocation);\n#endif // TARGET_ARM64",
    "repo_full_name": "dotnet/runtime",
    "discussion_comments": [
      {
        "comment_id": "2092090898",
        "repo_full_name": "dotnet/runtime",
        "pr_number": 110472,
        "pr_file": "src/coreclr/nativeaot/Runtime/thread.cpp",
        "discussion_id": "2092090898",
        "commented_code": "@@ -817,6 +826,9 @@ void Thread::HijackReturnAddressWorker(StackFrameIterator* frameIterator, Hijack\n #endif\n \n         *ppvRetAddrLocation = (void*)pfnHijackFunction;\n+#if defined(TARGET_ARM64)\n+        *ppvRetAddrLocation = PacSignPtr(*ppvRetAddrLocation);\n+#endif // TARGET_ARM64",
        "comment_created_at": "2025-05-15T23:20:47+00:00",
        "comment_author": "jkotas",
        "comment_body": "```suggestion\r\n        void* pvHIjackAddr = (void*)pfnHijackFunction;\r\n#if defined(TARGET_ARM64)\r\n        pvHIjackAddr = PacSignPtr(pvHIjackAddr);\r\n#endif // TARGET_ARM64\r\n        *ppvRetAddrLocation = pvHIjackAddr;\r\n```\r\nWe should avoid writing the wrong value first, and then overwriting it with the correct value. It can cause interesting race conditions.",
        "pr_file_module": null
      },
      {
        "comment_id": "2093116465",
        "repo_full_name": "dotnet/runtime",
        "pr_number": 110472,
        "pr_file": "src/coreclr/nativeaot/Runtime/thread.cpp",
        "discussion_id": "2092090898",
        "commented_code": "@@ -817,6 +826,9 @@ void Thread::HijackReturnAddressWorker(StackFrameIterator* frameIterator, Hijack\n #endif\n \n         *ppvRetAddrLocation = (void*)pfnHijackFunction;\n+#if defined(TARGET_ARM64)\n+        *ppvRetAddrLocation = PacSignPtr(*ppvRetAddrLocation);\n+#endif // TARGET_ARM64",
        "comment_created_at": "2025-05-16T14:02:19+00:00",
        "comment_author": "SwapnilGaikwad",
        "comment_body": "Done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2178499791",
    "pr_number": 117218,
    "pr_file": "src/coreclr/vm/debugdebugger.cpp",
    "created_at": "2025-07-01T20:56:22+00:00",
    "commented_code": "}\n\n#ifndef DACCESS_COMPILE\n// This is an implementation of a cache of the Native->IL offset mappings used by managed stack traces. It exists for the following reasons:\n// 1. When a large server experiences a large number of exceptions due to some other system failing, it can cause a tremendous number of stack traces to be generated, if customers are attempting to log.\n// 2. The native->IL offset mapping is somewhat expensive to compute, and it is not necessary to compute it repeatedly for the same IP.\n// 3. Often when these mappings are needed, the system is under stress, and throwing on MANY different threads with similar callstacks, so the cost of having locking around the cache may be significant.\n//\n// The cache is implemented as a simple hash table, where the key is the IP + fAdjustOffset\n// flag, and the value is the IL offset. We use a version number to indicate when the cache\n// is being updated, and to indicate that a found value is valid, and we use a simple linear\n// probing algorithm to find the entry in the cache.\n//\n// The replacement policy is randomized, and there are s_stackWalkCacheWalk(8) possible buckets to check before giving up.\n//\n// Since the cache entries are greater than a single pointer, we use a simple version locking scheme to protect readers.\n\nstruct StackWalkNativeToILCacheEntry\n{\n    void* ip = NULL; // The IP of the native code\n    uint32_t ilOffset = 0; // The IL offset, with the adjust offset flag set if the native offset was adjusted by STACKWALK_CONTROLPC_ADJUST_OFFSET\n};\n\nstatic LONG s_stackWalkNativeToILCacheVersion = 0;\nstatic DWORD s_stackWalkCacheSize = 0; // This is the total size of the cache (We use a pointer+4 bytes for each entry, so on 64bit platforms 12KB of memory)\nconst DWORD s_stackWalkCacheWalk = 8; // Walk up to this many entries in the cache before giving up\nconst DWORD s_stackWalkCacheAdjustOffsetFlag = 0x80000000; // 2^31, put into the IL offset portion of the cache entry to check if the native offset was adjusted by STACKWALK_CONTROLPC_ADJUST_OFFSET\nstatic StackWalkNativeToILCacheEntry* s_stackWalkCache = NULL;\n\nbool CheckNativeToILCacheCore(void* ip, bool fAdjustOffset, uint32_t* pILOffset)\n{\n    // Check the cache for the IP\n    int hashCode = MixPointerIntoHash(ip);\n    StackWalkNativeToILCacheEntry* cacheTable = VolatileLoad(&s_stackWalkCache);\n    \n    if (cacheTable == NULL)\n    {\n        // Cache is not initialized\n        return false;\n    }\n    DWORD cacheSize = VolatileLoadWithoutBarrier(&s_stackWalkCacheSize);\n    int index = hashCode % cacheSize;\n\n    DWORD count = 0;\n    do\n    {\n        if (VolatileLoadWithoutBarrier(&cacheTable[index].ip) == ip)\n        {\n            // Cache hit\n            uint32_t dwILOffset = VolatileLoad(&cacheTable[index].ilOffset); // It is IMPORTANT that this load have a barrier after it, so that the version check in the containing funciton is safe.\n            if (fAdjustOffset != ((dwILOffset & s_stackWalkCacheAdjustOffsetFlag) == s_stackWalkCacheAdjustOffsetFlag))\n            {\n                continue; // The cache entry did not match on the adjust offset flag, so move to the next entry.\n            }\n\n            dwILOffset &= ~s_stackWalkCacheAdjustOffsetFlag; // Clear the adjust offset flag\n            *pILOffset = dwILOffset;\n            return true;\n        }\n    } while (index = (index + 1) % cacheSize, count++ < s_stackWalkCacheWalk);\n\n    return false; // Not found in cache\n}\n\nbool CheckNativeToILCache(void* ip, bool fAdjustOffset, uint32_t* pILOffset)\n{\n    LIMITED_METHOD_CONTRACT;\n\n    LONG versionStart = VolatileLoad(&s_stackWalkNativeToILCacheVersion);\n\n    if ((versionStart & 1) == 1)\n    {\n        // Cache is being updated, so we cannot use it\n        return false;\n    }\n\n    if (CheckNativeToILCacheCore(ip, fAdjustOffset, pILOffset))\n    {\n        // When looking in the cache, the last load from the cache is a VolatileLoad, which allows a load here to check the version in the cache\n        LONG versionEnd = VolatileLoadWithoutBarrier(&s_stackWalkNativeToILCacheVersion);\n        if (versionEnd == versionStart)\n        {\n            // Cache was not updated while we were checking it, so we can use it\n            return true;\n        }\n    }\n\n    return false;\n}\n\nvoid InsertIntoNativeToILCache(void* ip, bool fAdjustOffset, uint32_t dwILOffset)\n{\n    CONTRACTL\n    {\n        THROWS;\n    }\n    CONTRACTL_END;\n\n    uint32_t dwILOffsetCheck;\n    if (CheckNativeToILCache(ip, fAdjustOffset, &dwILOffsetCheck))\n    {\n        // The entry already exists, so we don't need to insert it again\n        _ASSERTE(dwILOffsetCheck == dwILOffset);\n        return;\n    }\n\n    // Insert the IP and IL offset into the cache\n    \n    LONG versionStart = VolatileLoadWithoutBarrier(&s_stackWalkNativeToILCacheVersion);\n    if ((versionStart & 1) == 1)\n    {\n        // Cache is being updated by someone else, so we can't modify it\n        return;\n    }\n\n    if (versionStart != InterlockedCompareExchange(&s_stackWalkNativeToILCacheVersion, versionStart, versionStart | 1))",
    "repo_full_name": "dotnet/runtime",
    "discussion_comments": [
      {
        "comment_id": "2178499791",
        "repo_full_name": "dotnet/runtime",
        "pr_number": 117218,
        "pr_file": "src/coreclr/vm/debugdebugger.cpp",
        "discussion_id": "2178499791",
        "commented_code": "@@ -1013,14 +1013,176 @@ void DebugStackTrace::Element::InitPass1(\n }\n \n #ifndef DACCESS_COMPILE\n+// This is an implementation of a cache of the Native->IL offset mappings used by managed stack traces. It exists for the following reasons:\n+// 1. When a large server experiences a large number of exceptions due to some other system failing, it can cause a tremendous number of stack traces to be generated, if customers are attempting to log.\n+// 2. The native->IL offset mapping is somewhat expensive to compute, and it is not necessary to compute it repeatedly for the same IP.\n+// 3. Often when these mappings are needed, the system is under stress, and throwing on MANY different threads with similar callstacks, so the cost of having locking around the cache may be significant.\n+//\n+// The cache is implemented as a simple hash table, where the key is the IP + fAdjustOffset\n+// flag, and the value is the IL offset. We use a version number to indicate when the cache\n+// is being updated, and to indicate that a found value is valid, and we use a simple linear\n+// probing algorithm to find the entry in the cache.\n+//\n+// The replacement policy is randomized, and there are s_stackWalkCacheWalk(8) possible buckets to check before giving up.\n+//\n+// Since the cache entries are greater than a single pointer, we use a simple version locking scheme to protect readers.\n+\n+struct StackWalkNativeToILCacheEntry\n+{\n+    void* ip = NULL; // The IP of the native code\n+    uint32_t ilOffset = 0; // The IL offset, with the adjust offset flag set if the native offset was adjusted by STACKWALK_CONTROLPC_ADJUST_OFFSET\n+};\n+\n+static LONG s_stackWalkNativeToILCacheVersion = 0;\n+static DWORD s_stackWalkCacheSize = 0; // This is the total size of the cache (We use a pointer+4 bytes for each entry, so on 64bit platforms 12KB of memory)\n+const DWORD s_stackWalkCacheWalk = 8; // Walk up to this many entries in the cache before giving up\n+const DWORD s_stackWalkCacheAdjustOffsetFlag = 0x80000000; // 2^31, put into the IL offset portion of the cache entry to check if the native offset was adjusted by STACKWALK_CONTROLPC_ADJUST_OFFSET\n+static StackWalkNativeToILCacheEntry* s_stackWalkCache = NULL;\n+\n+bool CheckNativeToILCacheCore(void* ip, bool fAdjustOffset, uint32_t* pILOffset)\n+{\n+    // Check the cache for the IP\n+    int hashCode = MixPointerIntoHash(ip);\n+    StackWalkNativeToILCacheEntry* cacheTable = VolatileLoad(&s_stackWalkCache);\n+    \n+    if (cacheTable == NULL)\n+    {\n+        // Cache is not initialized\n+        return false;\n+    }\n+    DWORD cacheSize = VolatileLoadWithoutBarrier(&s_stackWalkCacheSize);\n+    int index = hashCode % cacheSize;\n+\n+    DWORD count = 0;\n+    do\n+    {\n+        if (VolatileLoadWithoutBarrier(&cacheTable[index].ip) == ip)\n+        {\n+            // Cache hit\n+            uint32_t dwILOffset = VolatileLoad(&cacheTable[index].ilOffset); // It is IMPORTANT that this load have a barrier after it, so that the version check in the containing funciton is safe.\n+            if (fAdjustOffset != ((dwILOffset & s_stackWalkCacheAdjustOffsetFlag) == s_stackWalkCacheAdjustOffsetFlag))\n+            {\n+                continue; // The cache entry did not match on the adjust offset flag, so move to the next entry.\n+            }\n+\n+            dwILOffset &= ~s_stackWalkCacheAdjustOffsetFlag; // Clear the adjust offset flag\n+            *pILOffset = dwILOffset;\n+            return true;\n+        }\n+    } while (index = (index + 1) % cacheSize, count++ < s_stackWalkCacheWalk);\n+\n+    return false; // Not found in cache\n+}\n+\n+bool CheckNativeToILCache(void* ip, bool fAdjustOffset, uint32_t* pILOffset)\n+{\n+    LIMITED_METHOD_CONTRACT;\n+\n+    LONG versionStart = VolatileLoad(&s_stackWalkNativeToILCacheVersion);\n+\n+    if ((versionStart & 1) == 1)\n+    {\n+        // Cache is being updated, so we cannot use it\n+        return false;\n+    }\n+\n+    if (CheckNativeToILCacheCore(ip, fAdjustOffset, pILOffset))\n+    {\n+        // When looking in the cache, the last load from the cache is a VolatileLoad, which allows a load here to check the version in the cache\n+        LONG versionEnd = VolatileLoadWithoutBarrier(&s_stackWalkNativeToILCacheVersion);\n+        if (versionEnd == versionStart)\n+        {\n+            // Cache was not updated while we were checking it, so we can use it\n+            return true;\n+        }\n+    }\n+\n+    return false;\n+}\n+\n+void InsertIntoNativeToILCache(void* ip, bool fAdjustOffset, uint32_t dwILOffset)\n+{\n+    CONTRACTL\n+    {\n+        THROWS;\n+    }\n+    CONTRACTL_END;\n+\n+    uint32_t dwILOffsetCheck;\n+    if (CheckNativeToILCache(ip, fAdjustOffset, &dwILOffsetCheck))\n+    {\n+        // The entry already exists, so we don't need to insert it again\n+        _ASSERTE(dwILOffsetCheck == dwILOffset);\n+        return;\n+    }\n+\n+    // Insert the IP and IL offset into the cache\n+    \n+    LONG versionStart = VolatileLoadWithoutBarrier(&s_stackWalkNativeToILCacheVersion);\n+    if ((versionStart & 1) == 1)\n+    {\n+        // Cache is being updated by someone else, so we can't modify it\n+        return;\n+    }\n+\n+    if (versionStart != InterlockedCompareExchange(&s_stackWalkNativeToILCacheVersion, versionStart, versionStart | 1))",
        "comment_created_at": "2025-07-01T20:56:22+00:00",
        "comment_author": "Copilot",
        "comment_body": "The arguments to InterlockedCompareExchange are reversed; it should be `InterlockedCompareExchange(&s_stackWalkNativeToILCacheVersion, versionStart | 1, versionStart)` to correctly acquire the update lock.\n```suggestion\n    if (versionStart != InterlockedCompareExchange(&s_stackWalkNativeToILCacheVersion, versionStart | 1, versionStart))\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2178499798",
    "pr_number": 117218,
    "pr_file": "src/coreclr/vm/debugdebugger.cpp",
    "created_at": "2025-07-01T20:56:22+00:00",
    "commented_code": "}\n\n#ifndef DACCESS_COMPILE\n// This is an implementation of a cache of the Native->IL offset mappings used by managed stack traces. It exists for the following reasons:\n// 1. When a large server experiences a large number of exceptions due to some other system failing, it can cause a tremendous number of stack traces to be generated, if customers are attempting to log.\n// 2. The native->IL offset mapping is somewhat expensive to compute, and it is not necessary to compute it repeatedly for the same IP.\n// 3. Often when these mappings are needed, the system is under stress, and throwing on MANY different threads with similar callstacks, so the cost of having locking around the cache may be significant.\n//\n// The cache is implemented as a simple hash table, where the key is the IP + fAdjustOffset\n// flag, and the value is the IL offset. We use a version number to indicate when the cache\n// is being updated, and to indicate that a found value is valid, and we use a simple linear\n// probing algorithm to find the entry in the cache.\n//\n// The replacement policy is randomized, and there are s_stackWalkCacheWalk(8) possible buckets to check before giving up.\n//\n// Since the cache entries are greater than a single pointer, we use a simple version locking scheme to protect readers.\n\nstruct StackWalkNativeToILCacheEntry\n{\n    void* ip = NULL; // The IP of the native code\n    uint32_t ilOffset = 0; // The IL offset, with the adjust offset flag set if the native offset was adjusted by STACKWALK_CONTROLPC_ADJUST_OFFSET\n};\n\nstatic LONG s_stackWalkNativeToILCacheVersion = 0;\nstatic DWORD s_stackWalkCacheSize = 0; // This is the total size of the cache (We use a pointer+4 bytes for each entry, so on 64bit platforms 12KB of memory)\nconst DWORD s_stackWalkCacheWalk = 8; // Walk up to this many entries in the cache before giving up\nconst DWORD s_stackWalkCacheAdjustOffsetFlag = 0x80000000; // 2^31, put into the IL offset portion of the cache entry to check if the native offset was adjusted by STACKWALK_CONTROLPC_ADJUST_OFFSET\nstatic StackWalkNativeToILCacheEntry* s_stackWalkCache = NULL;\n\nbool CheckNativeToILCacheCore(void* ip, bool fAdjustOffset, uint32_t* pILOffset)\n{\n    // Check the cache for the IP\n    int hashCode = MixPointerIntoHash(ip);\n    StackWalkNativeToILCacheEntry* cacheTable = VolatileLoad(&s_stackWalkCache);\n    \n    if (cacheTable == NULL)\n    {\n        // Cache is not initialized\n        return false;\n    }\n    DWORD cacheSize = VolatileLoadWithoutBarrier(&s_stackWalkCacheSize);\n    int index = hashCode % cacheSize;\n\n    DWORD count = 0;\n    do\n    {\n        if (VolatileLoadWithoutBarrier(&cacheTable[index].ip) == ip)\n        {\n            // Cache hit\n            uint32_t dwILOffset = VolatileLoad(&cacheTable[index].ilOffset); // It is IMPORTANT that this load have a barrier after it, so that the version check in the containing funciton is safe.\n            if (fAdjustOffset != ((dwILOffset & s_stackWalkCacheAdjustOffsetFlag) == s_stackWalkCacheAdjustOffsetFlag))\n            {\n                continue; // The cache entry did not match on the adjust offset flag, so move to the next entry.\n            }\n\n            dwILOffset &= ~s_stackWalkCacheAdjustOffsetFlag; // Clear the adjust offset flag\n            *pILOffset = dwILOffset;\n            return true;\n        }\n    } while (index = (index + 1) % cacheSize, count++ < s_stackWalkCacheWalk);\n\n    return false; // Not found in cache\n}\n\nbool CheckNativeToILCache(void* ip, bool fAdjustOffset, uint32_t* pILOffset)\n{\n    LIMITED_METHOD_CONTRACT;\n\n    LONG versionStart = VolatileLoad(&s_stackWalkNativeToILCacheVersion);\n\n    if ((versionStart & 1) == 1)\n    {\n        // Cache is being updated, so we cannot use it\n        return false;\n    }\n\n    if (CheckNativeToILCacheCore(ip, fAdjustOffset, pILOffset))\n    {\n        // When looking in the cache, the last load from the cache is a VolatileLoad, which allows a load here to check the version in the cache\n        LONG versionEnd = VolatileLoadWithoutBarrier(&s_stackWalkNativeToILCacheVersion);\n        if (versionEnd == versionStart)\n        {\n            // Cache was not updated while we were checking it, so we can use it\n            return true;\n        }\n    }\n\n    return false;\n}\n\nvoid InsertIntoNativeToILCache(void* ip, bool fAdjustOffset, uint32_t dwILOffset)\n{\n    CONTRACTL\n    {\n        THROWS;\n    }\n    CONTRACTL_END;\n\n    uint32_t dwILOffsetCheck;\n    if (CheckNativeToILCache(ip, fAdjustOffset, &dwILOffsetCheck))\n    {\n        // The entry already exists, so we don't need to insert it again\n        _ASSERTE(dwILOffsetCheck == dwILOffset);\n        return;\n    }\n\n    // Insert the IP and IL offset into the cache\n    \n    LONG versionStart = VolatileLoadWithoutBarrier(&s_stackWalkNativeToILCacheVersion);\n    if ((versionStart & 1) == 1)\n    {\n        // Cache is being updated by someone else, so we can't modify it\n        return;\n    }\n\n    if (versionStart != InterlockedCompareExchange(&s_stackWalkNativeToILCacheVersion, versionStart, versionStart | 1))\n    {\n        // Someone else updated the cache version while we were attempting to take the lock, so abort updating the cache\n        return;\n    }\n    // Now we have the lock, so we can safely update the cache\n\n    if (s_stackWalkCache == NULL)\n    {\n        // Initialize the cache if it is not already initialized\n        DWORD cacheSize = CLRConfig::GetConfigValue(CLRConfig::UNSUPPORTED_NativeToILOffsetCacheSize);\n        s_stackWalkCacheSize = cacheSize;",
    "repo_full_name": "dotnet/runtime",
    "discussion_comments": [
      {
        "comment_id": "2178499798",
        "repo_full_name": "dotnet/runtime",
        "pr_number": 117218,
        "pr_file": "src/coreclr/vm/debugdebugger.cpp",
        "discussion_id": "2178499798",
        "commented_code": "@@ -1013,14 +1013,176 @@ void DebugStackTrace::Element::InitPass1(\n }\n \n #ifndef DACCESS_COMPILE\n+// This is an implementation of a cache of the Native->IL offset mappings used by managed stack traces. It exists for the following reasons:\n+// 1. When a large server experiences a large number of exceptions due to some other system failing, it can cause a tremendous number of stack traces to be generated, if customers are attempting to log.\n+// 2. The native->IL offset mapping is somewhat expensive to compute, and it is not necessary to compute it repeatedly for the same IP.\n+// 3. Often when these mappings are needed, the system is under stress, and throwing on MANY different threads with similar callstacks, so the cost of having locking around the cache may be significant.\n+//\n+// The cache is implemented as a simple hash table, where the key is the IP + fAdjustOffset\n+// flag, and the value is the IL offset. We use a version number to indicate when the cache\n+// is being updated, and to indicate that a found value is valid, and we use a simple linear\n+// probing algorithm to find the entry in the cache.\n+//\n+// The replacement policy is randomized, and there are s_stackWalkCacheWalk(8) possible buckets to check before giving up.\n+//\n+// Since the cache entries are greater than a single pointer, we use a simple version locking scheme to protect readers.\n+\n+struct StackWalkNativeToILCacheEntry\n+{\n+    void* ip = NULL; // The IP of the native code\n+    uint32_t ilOffset = 0; // The IL offset, with the adjust offset flag set if the native offset was adjusted by STACKWALK_CONTROLPC_ADJUST_OFFSET\n+};\n+\n+static LONG s_stackWalkNativeToILCacheVersion = 0;\n+static DWORD s_stackWalkCacheSize = 0; // This is the total size of the cache (We use a pointer+4 bytes for each entry, so on 64bit platforms 12KB of memory)\n+const DWORD s_stackWalkCacheWalk = 8; // Walk up to this many entries in the cache before giving up\n+const DWORD s_stackWalkCacheAdjustOffsetFlag = 0x80000000; // 2^31, put into the IL offset portion of the cache entry to check if the native offset was adjusted by STACKWALK_CONTROLPC_ADJUST_OFFSET\n+static StackWalkNativeToILCacheEntry* s_stackWalkCache = NULL;\n+\n+bool CheckNativeToILCacheCore(void* ip, bool fAdjustOffset, uint32_t* pILOffset)\n+{\n+    // Check the cache for the IP\n+    int hashCode = MixPointerIntoHash(ip);\n+    StackWalkNativeToILCacheEntry* cacheTable = VolatileLoad(&s_stackWalkCache);\n+    \n+    if (cacheTable == NULL)\n+    {\n+        // Cache is not initialized\n+        return false;\n+    }\n+    DWORD cacheSize = VolatileLoadWithoutBarrier(&s_stackWalkCacheSize);\n+    int index = hashCode % cacheSize;\n+\n+    DWORD count = 0;\n+    do\n+    {\n+        if (VolatileLoadWithoutBarrier(&cacheTable[index].ip) == ip)\n+        {\n+            // Cache hit\n+            uint32_t dwILOffset = VolatileLoad(&cacheTable[index].ilOffset); // It is IMPORTANT that this load have a barrier after it, so that the version check in the containing funciton is safe.\n+            if (fAdjustOffset != ((dwILOffset & s_stackWalkCacheAdjustOffsetFlag) == s_stackWalkCacheAdjustOffsetFlag))\n+            {\n+                continue; // The cache entry did not match on the adjust offset flag, so move to the next entry.\n+            }\n+\n+            dwILOffset &= ~s_stackWalkCacheAdjustOffsetFlag; // Clear the adjust offset flag\n+            *pILOffset = dwILOffset;\n+            return true;\n+        }\n+    } while (index = (index + 1) % cacheSize, count++ < s_stackWalkCacheWalk);\n+\n+    return false; // Not found in cache\n+}\n+\n+bool CheckNativeToILCache(void* ip, bool fAdjustOffset, uint32_t* pILOffset)\n+{\n+    LIMITED_METHOD_CONTRACT;\n+\n+    LONG versionStart = VolatileLoad(&s_stackWalkNativeToILCacheVersion);\n+\n+    if ((versionStart & 1) == 1)\n+    {\n+        // Cache is being updated, so we cannot use it\n+        return false;\n+    }\n+\n+    if (CheckNativeToILCacheCore(ip, fAdjustOffset, pILOffset))\n+    {\n+        // When looking in the cache, the last load from the cache is a VolatileLoad, which allows a load here to check the version in the cache\n+        LONG versionEnd = VolatileLoadWithoutBarrier(&s_stackWalkNativeToILCacheVersion);\n+        if (versionEnd == versionStart)\n+        {\n+            // Cache was not updated while we were checking it, so we can use it\n+            return true;\n+        }\n+    }\n+\n+    return false;\n+}\n+\n+void InsertIntoNativeToILCache(void* ip, bool fAdjustOffset, uint32_t dwILOffset)\n+{\n+    CONTRACTL\n+    {\n+        THROWS;\n+    }\n+    CONTRACTL_END;\n+\n+    uint32_t dwILOffsetCheck;\n+    if (CheckNativeToILCache(ip, fAdjustOffset, &dwILOffsetCheck))\n+    {\n+        // The entry already exists, so we don't need to insert it again\n+        _ASSERTE(dwILOffsetCheck == dwILOffset);\n+        return;\n+    }\n+\n+    // Insert the IP and IL offset into the cache\n+    \n+    LONG versionStart = VolatileLoadWithoutBarrier(&s_stackWalkNativeToILCacheVersion);\n+    if ((versionStart & 1) == 1)\n+    {\n+        // Cache is being updated by someone else, so we can't modify it\n+        return;\n+    }\n+\n+    if (versionStart != InterlockedCompareExchange(&s_stackWalkNativeToILCacheVersion, versionStart, versionStart | 1))\n+    {\n+        // Someone else updated the cache version while we were attempting to take the lock, so abort updating the cache\n+        return;\n+    }\n+    // Now we have the lock, so we can safely update the cache\n+\n+    if (s_stackWalkCache == NULL)\n+    {\n+        // Initialize the cache if it is not already initialized\n+        DWORD cacheSize = CLRConfig::GetConfigValue(CLRConfig::UNSUPPORTED_NativeToILOffsetCacheSize);\n+        s_stackWalkCacheSize = cacheSize;",
        "comment_created_at": "2025-07-01T20:56:22+00:00",
        "comment_author": "Copilot",
        "comment_body": "Writing to `s_stackWalkCacheSize` without a memory barrier may be reordered around the store to `s_stackWalkCache`; use `VolatileStore` or an atomic write to ensure sizes are visible before the pointer is published.\n```suggestion\n        VolatileStore(&s_stackWalkCacheSize, cacheSize);\n```",
        "pr_file_module": null
      }
    ]
  }
]
