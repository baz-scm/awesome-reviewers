[
  {
    "discussion_id": "1898400754",
    "pr_number": 134663,
    "pr_file": "library/std/src/sync/nonpoison/mutex.rs",
    "created_at": "2024-12-27T09:44:38+00:00",
    "commented_code": "+#[cfg(all(test, not(any(target_os = \"emscripten\", target_os = \"wasi\"))))]\n+mod tests;\n+\n+use crate::cell::UnsafeCell;\n+use crate::fmt;\n+use crate::marker::PhantomData;\n+use crate::mem::ManuallyDrop;\n+use crate::ops::{Deref, DerefMut};\n+use crate::ptr::NonNull;\n+use crate::sys::sync as sys;\n+\n+/// A mutual exclusion primitive useful for protecting shared data.\n+///\n+/// For more information about mutexes, check out the documentation for the\n+/// poisoning variant of this lock found at [std::sync::poison::Mutex](std::sync::Mutex).\n+///\n+/// # Example\n+///\n+/// ```\n+/// use std::sync::{Arc, nonpoison::Mutex};\n+/// use std::thread;\n+/// use std::sync::mpsc::channel;\n+///\n+/// const N: usize = 10;\n+///\n+/// // Spawn a few threads to increment a shared variable (non-atomically), and\n+/// // let the main thread know once all increments are done.\n+/// //\n+/// // Here we're using an Arc to share memory among threads, and the data inside\n+/// // the Arc is protected with a mutex.\n+/// let data = Arc::new(Mutex::new(0));\n+///\n+/// let (tx, rx) = channel();\n+/// for _ in 0..N {\n+///     let (data, tx) = (Arc::clone(&data), tx.clone());\n+///     thread::spawn(move || {\n+///         // The shared state can only be accessed once the lock is held.\n+///         // Our non-atomic increment is safe because we're the only thread\n+///         // which can access the shared state when the lock is held.\n+///         let mut data = data.lock();\n+///         *data += 1;\n+///         if *data == N {\n+///             tx.send(()).unwrap();\n+///         }\n+///         // the lock is unlocked here when `data` goes out of scope.\n+///     });\n+/// }\n+///\n+/// rx.recv().unwrap();\n+/// ```\n+///",
    "repo_full_name": "rust-lang/rust",
    "discussion_comments": [
      {
        "comment_id": "1898400754",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 134663,
        "pr_file": "library/std/src/sync/nonpoison/mutex.rs",
        "discussion_id": "1898400754",
        "commented_code": "@@ -0,0 +1,492 @@\n+#[cfg(all(test, not(any(target_os = \"emscripten\", target_os = \"wasi\"))))]\n+mod tests;\n+\n+use crate::cell::UnsafeCell;\n+use crate::fmt;\n+use crate::marker::PhantomData;\n+use crate::mem::ManuallyDrop;\n+use crate::ops::{Deref, DerefMut};\n+use crate::ptr::NonNull;\n+use crate::sys::sync as sys;\n+\n+/// A mutual exclusion primitive useful for protecting shared data.\n+///\n+/// For more information about mutexes, check out the documentation for the\n+/// poisoning variant of this lock found at [std::sync::poison::Mutex](std::sync::Mutex).\n+///\n+/// # Example\n+///\n+/// ```\n+/// use std::sync::{Arc, nonpoison::Mutex};\n+/// use std::thread;\n+/// use std::sync::mpsc::channel;\n+///\n+/// const N: usize = 10;\n+///\n+/// // Spawn a few threads to increment a shared variable (non-atomically), and\n+/// // let the main thread know once all increments are done.\n+/// //\n+/// // Here we're using an Arc to share memory among threads, and the data inside\n+/// // the Arc is protected with a mutex.\n+/// let data = Arc::new(Mutex::new(0));\n+///\n+/// let (tx, rx) = channel();\n+/// for _ in 0..N {\n+///     let (data, tx) = (Arc::clone(&data), tx.clone());\n+///     thread::spawn(move || {\n+///         // The shared state can only be accessed once the lock is held.\n+///         // Our non-atomic increment is safe because we're the only thread\n+///         // which can access the shared state when the lock is held.\n+///         let mut data = data.lock();\n+///         *data += 1;\n+///         if *data == N {\n+///             tx.send(()).unwrap();\n+///         }\n+///         // the lock is unlocked here when `data` goes out of scope.\n+///     });\n+/// }\n+///\n+/// rx.recv().unwrap();\n+/// ```\n+///",
        "comment_created_at": "2024-12-27T09:44:38+00:00",
        "comment_author": "tgross35",
        "comment_body": "Could you add an example to show the nonpoisoning behavior? Possibly:\r\n\r\n```rust\r\nuse std::thread;\r\nuse std::sync::{Arc, nonpoison::Mutex};\r\n\r\nlet mutex = Arc::new(Mutex::new(0u32));\r\nlet mut handles = Vec::new();\r\n\r\nfor n in 0..10 {\r\n    let m = Arc::clone(&mutex);\r\n    let handle = thread::spawn(move || {\r\n        let mut guard = m.lock();\r\n        *guard += 1;\r\n        panic!(\"panic from thread {n} {guard}\")\r\n    });\r\n    handles.push(handle);\r\n}\r\n\r\nfor h in handles {\r\n    h.join();\r\n}\r\n\r\nprintln!(\"Finished, locked {} times\", mutex.lock());\r\n```\r\n\r\nIt would be good to have something similar in `tests.rs`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1898401898",
    "pr_number": 134663,
    "pr_file": "library/std/src/sync/nonpoison/mutex.rs",
    "created_at": "2024-12-27T09:46:39+00:00",
    "commented_code": "+#[cfg(all(test, not(any(target_os = \"emscripten\", target_os = \"wasi\"))))]\n+mod tests;\n+\n+use crate::cell::UnsafeCell;\n+use crate::fmt;\n+use crate::marker::PhantomData;\n+use crate::mem::ManuallyDrop;\n+use crate::ops::{Deref, DerefMut};\n+use crate::ptr::NonNull;\n+use crate::sys::sync as sys;\n+\n+/// A mutual exclusion primitive useful for protecting shared data.\n+///\n+/// For more information about mutexes, check out the documentation for the\n+/// poisoning variant of this lock found at [std::sync::poison::Mutex](std::sync::Mutex).\n+///\n+/// # Example\n+///\n+/// ```\n+/// use std::sync::{Arc, nonpoison::Mutex};\n+/// use std::thread;\n+/// use std::sync::mpsc::channel;\n+///\n+/// const N: usize = 10;\n+///\n+/// // Spawn a few threads to increment a shared variable (non-atomically), and\n+/// // let the main thread know once all increments are done.\n+/// //\n+/// // Here we're using an Arc to share memory among threads, and the data inside\n+/// // the Arc is protected with a mutex.\n+/// let data = Arc::new(Mutex::new(0));\n+///\n+/// let (tx, rx) = channel();\n+/// for _ in 0..N {\n+///     let (data, tx) = (Arc::clone(&data), tx.clone());\n+///     thread::spawn(move || {\n+///         // The shared state can only be accessed once the lock is held.\n+///         // Our non-atomic increment is safe because we're the only thread\n+///         // which can access the shared state when the lock is held.\n+///         let mut data = data.lock();\n+///         *data += 1;\n+///         if *data == N {\n+///             tx.send(()).unwrap();\n+///         }\n+///         // the lock is unlocked here when `data` goes out of scope.\n+///     });\n+/// }\n+///\n+/// rx.recv().unwrap();\n+/// ```\n+///\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+#[cfg_attr(not(test), rustc_diagnostic_item = \"NonPoisonMutex\")]\n+pub struct Mutex<T: ?Sized> {\n+    inner: sys::Mutex,\n+    data: UnsafeCell<T>,\n+}\n+\n+// these are the only places where `T: Send` matters; all other\n+// functionality works fine on a single thread.\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+unsafe impl<T: ?Sized + Send> Send for Mutex<T> {}\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+unsafe impl<T: ?Sized + Send> Sync for Mutex<T> {}\n+\n+/// An RAII implementation of a \"scoped lock\" of a mutex. When this structure is\n+/// dropped (falls out of scope), the lock will be unlocked.\n+///\n+/// The data protected by the mutex can be accessed through this guard via its\n+/// [`Deref`] and [`DerefMut`] implementations.\n+///\n+/// This structure is created by the [`lock`] and [`try_lock`] methods on\n+/// [`Mutex`].\n+///\n+/// [`lock`]: Mutex::lock\n+/// [`try_lock`]: Mutex::try_lock\n+#[must_use = \"if unused the Mutex will immediately unlock\"]\n+#[must_not_suspend = \"holding a MutexGuard across suspend \\\n+                      points can cause deadlocks, delays, \\\n+                      and cause Futures to not implement `Send`\"]\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+#[clippy::has_significant_drop]\n+#[cfg_attr(not(test), rustc_diagnostic_item = \"NonPoisonMutexGuard\")]\n+pub struct MutexGuard<'a, T: ?Sized + 'a> {\n+    lock: &'a Mutex<T>,\n+}\n+\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+impl<T: ?Sized> !Send for MutexGuard<'_, T> {}\n+#[stable(feature = \"mutexguard\", since = \"1.19.0\")]\n+unsafe impl<T: ?Sized + Sync> Sync for MutexGuard<'_, T> {}\n+\n+/// An RAII mutex guard returned by `MutexGuard::map`, which can point to a\n+/// subfield of the protected data. When this structure is dropped (falls out\n+/// of scope), the lock will be unlocked.\n+///\n+/// The main difference between `MappedMutexGuard` and [`MutexGuard`] is that the\n+/// former cannot be used with [`Condvar`], since that\n+/// could introduce soundness issues if the locked object is modified by another\n+/// thread while the `Mutex` is unlocked.\n+///\n+/// The data protected by the mutex can be accessed through this guard via its\n+/// [`Deref`] and [`DerefMut`] implementations.\n+///\n+/// This structure is created by the [`map`] and [`try_map`] methods on\n+/// [`MutexGuard`].\n+///\n+/// [`map`]: MutexGuard::map\n+/// [`try_map`]: MutexGuard::try_map\n+/// [`Condvar`]: crate::sync::Condvar\n+#[must_use = \"if unused the Mutex will immediately unlock\"]\n+#[must_not_suspend = \"holding a MappedMutexGuard across suspend \\\n+                      points can cause deadlocks, delays, \\\n+                      and cause Futures to not implement `Send`\"]\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+#[clippy::has_significant_drop]\n+pub struct MappedMutexGuard<'a, T: ?Sized + 'a> {\n+    // NB: we use a pointer instead of `&'a mut T` to avoid `noalias` violations, because a\n+    // `MappedMutexGuard` argument doesn't hold uniqueness for its whole scope, only until it drops.\n+    // `NonNull` is covariant over `T`, so we add a `PhantomData<&'a mut T>` field\n+    // below for the correct variance over `T` (invariance).\n+    data: NonNull<T>,\n+    inner: &'a sys::Mutex,\n+    _variance: PhantomData<&'a mut T>,\n+}\n+\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+impl<T: ?Sized> !Send for MappedMutexGuard<'_, T> {}\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+unsafe impl<T: ?Sized + Sync> Sync for MappedMutexGuard<'_, T> {}\n+\n+impl<T> Mutex<T> {\n+    /// Creates a new mutex in an unlocked state ready for use.\n+    ///\n+    /// # Examples\n+    ///\n+    /// ```\n+    /// use std::sync::nonpoison::Mutex;\n+    ///\n+    /// let mutex = Mutex::new(0);\n+    /// ```\n+    #[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+    #[inline]\n+    pub const fn new(t: T) -> Mutex<T> {\n+        Mutex { inner: sys::Mutex::new(), data: UnsafeCell::new(t) }\n+    }\n+}\n+\n+impl<T: ?Sized> Mutex<T> {\n+    /// Acquires a mutex, blocking the current thread until it is able to do so.\n+    ///\n+    /// This function will block the local thread until it is available to acquire\n+    /// the mutex. Upon returning, the thread is the only thread with the lock\n+    /// held. An RAII guard is returned to allow scoped unlock of the lock. When\n+    /// the guard goes out of scope, the mutex will be unlocked.\n+    ///\n+    /// The exact behavior on locking a mutex in the thread which already holds\n+    /// the lock is left unspecified. However, this function will not return on\n+    /// the second call (it might panic or deadlock, for example).\n+    ///",
    "repo_full_name": "rust-lang/rust",
    "discussion_comments": [
      {
        "comment_id": "1898401898",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 134663,
        "pr_file": "library/std/src/sync/nonpoison/mutex.rs",
        "discussion_id": "1898401898",
        "commented_code": "@@ -0,0 +1,492 @@\n+#[cfg(all(test, not(any(target_os = \"emscripten\", target_os = \"wasi\"))))]\n+mod tests;\n+\n+use crate::cell::UnsafeCell;\n+use crate::fmt;\n+use crate::marker::PhantomData;\n+use crate::mem::ManuallyDrop;\n+use crate::ops::{Deref, DerefMut};\n+use crate::ptr::NonNull;\n+use crate::sys::sync as sys;\n+\n+/// A mutual exclusion primitive useful for protecting shared data.\n+///\n+/// For more information about mutexes, check out the documentation for the\n+/// poisoning variant of this lock found at [std::sync::poison::Mutex](std::sync::Mutex).\n+///\n+/// # Example\n+///\n+/// ```\n+/// use std::sync::{Arc, nonpoison::Mutex};\n+/// use std::thread;\n+/// use std::sync::mpsc::channel;\n+///\n+/// const N: usize = 10;\n+///\n+/// // Spawn a few threads to increment a shared variable (non-atomically), and\n+/// // let the main thread know once all increments are done.\n+/// //\n+/// // Here we're using an Arc to share memory among threads, and the data inside\n+/// // the Arc is protected with a mutex.\n+/// let data = Arc::new(Mutex::new(0));\n+///\n+/// let (tx, rx) = channel();\n+/// for _ in 0..N {\n+///     let (data, tx) = (Arc::clone(&data), tx.clone());\n+///     thread::spawn(move || {\n+///         // The shared state can only be accessed once the lock is held.\n+///         // Our non-atomic increment is safe because we're the only thread\n+///         // which can access the shared state when the lock is held.\n+///         let mut data = data.lock();\n+///         *data += 1;\n+///         if *data == N {\n+///             tx.send(()).unwrap();\n+///         }\n+///         // the lock is unlocked here when `data` goes out of scope.\n+///     });\n+/// }\n+///\n+/// rx.recv().unwrap();\n+/// ```\n+///\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+#[cfg_attr(not(test), rustc_diagnostic_item = \"NonPoisonMutex\")]\n+pub struct Mutex<T: ?Sized> {\n+    inner: sys::Mutex,\n+    data: UnsafeCell<T>,\n+}\n+\n+// these are the only places where `T: Send` matters; all other\n+// functionality works fine on a single thread.\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+unsafe impl<T: ?Sized + Send> Send for Mutex<T> {}\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+unsafe impl<T: ?Sized + Send> Sync for Mutex<T> {}\n+\n+/// An RAII implementation of a \"scoped lock\" of a mutex. When this structure is\n+/// dropped (falls out of scope), the lock will be unlocked.\n+///\n+/// The data protected by the mutex can be accessed through this guard via its\n+/// [`Deref`] and [`DerefMut`] implementations.\n+///\n+/// This structure is created by the [`lock`] and [`try_lock`] methods on\n+/// [`Mutex`].\n+///\n+/// [`lock`]: Mutex::lock\n+/// [`try_lock`]: Mutex::try_lock\n+#[must_use = \"if unused the Mutex will immediately unlock\"]\n+#[must_not_suspend = \"holding a MutexGuard across suspend \\\n+                      points can cause deadlocks, delays, \\\n+                      and cause Futures to not implement `Send`\"]\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+#[clippy::has_significant_drop]\n+#[cfg_attr(not(test), rustc_diagnostic_item = \"NonPoisonMutexGuard\")]\n+pub struct MutexGuard<'a, T: ?Sized + 'a> {\n+    lock: &'a Mutex<T>,\n+}\n+\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+impl<T: ?Sized> !Send for MutexGuard<'_, T> {}\n+#[stable(feature = \"mutexguard\", since = \"1.19.0\")]\n+unsafe impl<T: ?Sized + Sync> Sync for MutexGuard<'_, T> {}\n+\n+/// An RAII mutex guard returned by `MutexGuard::map`, which can point to a\n+/// subfield of the protected data. When this structure is dropped (falls out\n+/// of scope), the lock will be unlocked.\n+///\n+/// The main difference between `MappedMutexGuard` and [`MutexGuard`] is that the\n+/// former cannot be used with [`Condvar`], since that\n+/// could introduce soundness issues if the locked object is modified by another\n+/// thread while the `Mutex` is unlocked.\n+///\n+/// The data protected by the mutex can be accessed through this guard via its\n+/// [`Deref`] and [`DerefMut`] implementations.\n+///\n+/// This structure is created by the [`map`] and [`try_map`] methods on\n+/// [`MutexGuard`].\n+///\n+/// [`map`]: MutexGuard::map\n+/// [`try_map`]: MutexGuard::try_map\n+/// [`Condvar`]: crate::sync::Condvar\n+#[must_use = \"if unused the Mutex will immediately unlock\"]\n+#[must_not_suspend = \"holding a MappedMutexGuard across suspend \\\n+                      points can cause deadlocks, delays, \\\n+                      and cause Futures to not implement `Send`\"]\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+#[clippy::has_significant_drop]\n+pub struct MappedMutexGuard<'a, T: ?Sized + 'a> {\n+    // NB: we use a pointer instead of `&'a mut T` to avoid `noalias` violations, because a\n+    // `MappedMutexGuard` argument doesn't hold uniqueness for its whole scope, only until it drops.\n+    // `NonNull` is covariant over `T`, so we add a `PhantomData<&'a mut T>` field\n+    // below for the correct variance over `T` (invariance).\n+    data: NonNull<T>,\n+    inner: &'a sys::Mutex,\n+    _variance: PhantomData<&'a mut T>,\n+}\n+\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+impl<T: ?Sized> !Send for MappedMutexGuard<'_, T> {}\n+#[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+unsafe impl<T: ?Sized + Sync> Sync for MappedMutexGuard<'_, T> {}\n+\n+impl<T> Mutex<T> {\n+    /// Creates a new mutex in an unlocked state ready for use.\n+    ///\n+    /// # Examples\n+    ///\n+    /// ```\n+    /// use std::sync::nonpoison::Mutex;\n+    ///\n+    /// let mutex = Mutex::new(0);\n+    /// ```\n+    #[unstable(feature = \"nonpoison_mutex\", issue = \"134645\")]\n+    #[inline]\n+    pub const fn new(t: T) -> Mutex<T> {\n+        Mutex { inner: sys::Mutex::new(), data: UnsafeCell::new(t) }\n+    }\n+}\n+\n+impl<T: ?Sized> Mutex<T> {\n+    /// Acquires a mutex, blocking the current thread until it is able to do so.\n+    ///\n+    /// This function will block the local thread until it is available to acquire\n+    /// the mutex. Upon returning, the thread is the only thread with the lock\n+    /// held. An RAII guard is returned to allow scoped unlock of the lock. When\n+    /// the guard goes out of scope, the mutex will be unlocked.\n+    ///\n+    /// The exact behavior on locking a mutex in the thread which already holds\n+    /// the lock is left unspecified. However, this function will not return on\n+    /// the second call (it might panic or deadlock, for example).\n+    ///",
        "comment_created_at": "2024-12-27T09:46:39+00:00",
        "comment_author": "tgross35",
        "comment_body": "Could you add something like \"If this thread panics while the lock is held, the lock will be released like normal.\"",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2217428629",
    "pr_number": 144188,
    "pr_file": "library/std/src/thread/mod.rs",
    "created_at": "2025-07-19T19:30:34+00:00",
    "commented_code": "///   which may take time on systems with large numbers of mountpoints.\n ///   (This does not apply to cgroup v2, or to processes not in a\n ///   cgroup.)\n+/// - It does not attempt to take `ulimit` into account. If there is a limit set on the number of\n+///   threads, `available_parallelism` cannot know how much of that limit a Rust program should\n+///   take, or know in a reliable and race-free way how much of that limit is already taken.",
    "repo_full_name": "rust-lang/rust",
    "discussion_comments": [
      {
        "comment_id": "2217428629",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 144188,
        "pr_file": "library/std/src/thread/mod.rs",
        "discussion_id": "2217428629",
        "commented_code": "@@ -2012,6 +2012,9 @@ fn _assert_sync_and_send() {\n ///   which may take time on systems with large numbers of mountpoints.\n ///   (This does not apply to cgroup v2, or to processes not in a\n ///   cgroup.)\n+/// - It does not attempt to take `ulimit` into account. If there is a limit set on the number of\n+///   threads, `available_parallelism` cannot know how much of that limit a Rust program should\n+///   take, or know in a reliable and race-free way how much of that limit is already taken.",
        "comment_created_at": "2025-07-19T19:30:34+00:00",
        "comment_author": "workingjubilee",
        "comment_body": ":thinking: If we did check `ulimit`, and made a guess based on it, getting it wrong and exceeding the value that is correct would just get the process killed anyways, right?",
        "pr_file_module": null
      },
      {
        "comment_id": "2217484659",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 144188,
        "pr_file": "library/std/src/thread/mod.rs",
        "discussion_id": "2217428629",
        "commented_code": "@@ -2012,6 +2012,9 @@ fn _assert_sync_and_send() {\n ///   which may take time on systems with large numbers of mountpoints.\n ///   (This does not apply to cgroup v2, or to processes not in a\n ///   cgroup.)\n+/// - It does not attempt to take `ulimit` into account. If there is a limit set on the number of\n+///   threads, `available_parallelism` cannot know how much of that limit a Rust program should\n+///   take, or know in a reliable and race-free way how much of that limit is already taken.",
        "comment_created_at": "2025-07-19T22:17:52+00:00",
        "comment_author": "Mark-Simulacrum",
        "comment_body": "I'd expect an error returned from thread::spawn, not getting killed (well, from the spawn builder, I think thread spawn would just panic). But, yes, I think that's right.",
        "pr_file_module": null
      },
      {
        "comment_id": "2217504158",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 144188,
        "pr_file": "library/std/src/thread/mod.rs",
        "discussion_id": "2217428629",
        "commented_code": "@@ -2012,6 +2012,9 @@ fn _assert_sync_and_send() {\n ///   which may take time on systems with large numbers of mountpoints.\n ///   (This does not apply to cgroup v2, or to processes not in a\n ///   cgroup.)\n+/// - It does not attempt to take `ulimit` into account. If there is a limit set on the number of\n+///   threads, `available_parallelism` cannot know how much of that limit a Rust program should\n+///   take, or know in a reliable and race-free way how much of that limit is already taken.",
        "comment_created_at": "2025-07-19T23:41:25+00:00",
        "comment_author": "the8472",
        "comment_body": "I think the distinction is deeper. ulimit restricts a different but related resource.\r\n\r\n`available_parallelism` is essentially about how much cpu-time we get, not threads. It's just that users then turn around and use that to calculate the number of threads to make use of that cpu-time and oversubscription will be managed via sharing. This is quite obvious once you have multiple threadpools in a rust program independently using `available_parallelism` to size themselves (e.g. rayon + tokio).\r\n\r\nulimit is how many threads one may have globally, in total, even idle ones, and no oversubscription allowed.\r\n\r\nIntended and existing usage of `available_parallelism` can't handle that discrepancy. It's not like runtimes check available_parallelism every time before they spawn a thread.",
        "pr_file_module": null
      },
      {
        "comment_id": "2217537626",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 144188,
        "pr_file": "library/std/src/thread/mod.rs",
        "discussion_id": "2217428629",
        "commented_code": "@@ -2012,6 +2012,9 @@ fn _assert_sync_and_send() {\n ///   which may take time on systems with large numbers of mountpoints.\n ///   (This does not apply to cgroup v2, or to processes not in a\n ///   cgroup.)\n+/// - It does not attempt to take `ulimit` into account. If there is a limit set on the number of\n+///   threads, `available_parallelism` cannot know how much of that limit a Rust program should\n+///   take, or know in a reliable and race-free way how much of that limit is already taken.",
        "comment_created_at": "2025-07-20T02:29:33+00:00",
        "comment_author": "workingjubilee",
        "comment_body": "Right, that makes sense. So we might as well encounter whatever fate awaits us when we exceed the `ulimit`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2217564204",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 144188,
        "pr_file": "library/std/src/thread/mod.rs",
        "discussion_id": "2217428629",
        "commented_code": "@@ -2012,6 +2012,9 @@ fn _assert_sync_and_send() {\n ///   which may take time on systems with large numbers of mountpoints.\n ///   (This does not apply to cgroup v2, or to processes not in a\n ///   cgroup.)\n+/// - It does not attempt to take `ulimit` into account. If there is a limit set on the number of\n+///   threads, `available_parallelism` cannot know how much of that limit a Rust program should\n+///   take, or know in a reliable and race-free way how much of that limit is already taken.",
        "comment_created_at": "2025-07-20T04:35:52+00:00",
        "comment_author": "workingjubilee",
        "comment_body": "@joshtriplett I think this revision is probably fine as-is if Mark likes it.\r\n\r\nBut would it make more sense to jump off the8472's remark and note that the only truly \"race-free\" way (because it must be enforced by OS-level concurrency limits) to determine whether spawning a thread is possible within the OS-defined limits is to try to actually spawn the thread? Otherwise it's a TOCTOU problem, so it's true it simply does not make any attempt to guess. And then underscore that the value returned is not a guarantee that you can successfully spawn that many threads, merely that spawning that many threads may be useful.",
        "pr_file_module": null
      },
      {
        "comment_id": "2217759404",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 144188,
        "pr_file": "library/std/src/thread/mod.rs",
        "discussion_id": "2217428629",
        "commented_code": "@@ -2012,6 +2012,9 @@ fn _assert_sync_and_send() {\n ///   which may take time on systems with large numbers of mountpoints.\n ///   (This does not apply to cgroup v2, or to processes not in a\n ///   cgroup.)\n+/// - It does not attempt to take `ulimit` into account. If there is a limit set on the number of\n+///   threads, `available_parallelism` cannot know how much of that limit a Rust program should\n+///   take, or know in a reliable and race-free way how much of that limit is already taken.",
        "comment_created_at": "2025-07-20T10:43:16+00:00",
        "comment_author": "the8472",
        "comment_body": "I was trying to say that I don't think not checking ulimit is really a limitation of the current implementation. It's out of scope.\r\n\r\nWe're not going to check if spawning a thread would run into memory or vma exhaustion either.\r\n\r\nMaybe it'd make sense to list this in a more general \"what this method isn't\" clarification paragraph.",
        "pr_file_module": null
      },
      {
        "comment_id": "2217927777",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 144188,
        "pr_file": "library/std/src/thread/mod.rs",
        "discussion_id": "2217428629",
        "commented_code": "@@ -2012,6 +2012,9 @@ fn _assert_sync_and_send() {\n ///   which may take time on systems with large numbers of mountpoints.\n ///   (This does not apply to cgroup v2, or to processes not in a\n ///   cgroup.)\n+/// - It does not attempt to take `ulimit` into account. If there is a limit set on the number of\n+///   threads, `available_parallelism` cannot know how much of that limit a Rust program should\n+///   take, or know in a reliable and race-free way how much of that limit is already taken.",
        "comment_created_at": "2025-07-20T19:23:40+00:00",
        "comment_author": "workingjubilee",
        "comment_body": "right, I probably didn't capture what you said correctly since it's a Nuance.\r\n\r\nmostly I am agreeing in the direction of \"what you said is correct, and in that sense this doesn't need to be a Linux-specific note\".",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2105211794",
    "pr_number": 141260,
    "pr_file": "library/core/src/ptr/mod.rs",
    "created_at": "2025-05-23T18:27:23+00:00",
    "commented_code": "}\n }\n \n-/// Performs a volatile read of the value from `src` without moving it. This\n-/// leaves the memory in `src` unchanged.\n-///\n-/// Volatile operations are intended to act on I/O memory, and are guaranteed\n-/// to not be elided or reordered by the compiler across other volatile\n-/// operations.\n-///\n-/// # Notes\n-///\n-/// Rust does not currently have a rigorously and formally defined memory model,\n-/// so the precise semantics of what \"volatile\" means here is subject to change\n-/// over time. That being said, the semantics will almost always end up pretty\n-/// similar to [C11's definition of volatile][c11].\n-///\n-/// The compiler shouldn't change the relative order or number of volatile\n-/// memory operations. However, volatile memory operations on zero-sized types\n-/// (e.g., if a zero-sized type is passed to `read_volatile`) are noops\n-/// and may be ignored.\n+/// Performs a volatile read of the value from `src` without moving it.\n+///\n+/// Rust does not currently have a rigorously and formally defined memory model, so the precise\n+/// semantics of what \"volatile\" means here is subject to change over time. That being said, the\n+/// semantics will almost always end up pretty similar to [C11's definition of volatile][c11].\n+///\n+/// Volatile operations are intended to act on I/O memory. As such, they are considered externally\n+/// observable events (just like syscalls), and are guaranteed to not be elided or reordered by the\n+/// compiler across other externally observable events. With this in mind, there are two cases of\n+/// usage that need to be distinguished:\n+///\n+/// - When a volatile operation is used for memory inside an [allocation], it behaves exactly like\n+///   [`read`], except for the additional guarantee that it won't be elided or reordered (see\n+///   above). This implies that the operation will actually access memory and not e.g. be lowered to\n+///   a register access or stack pop. Other than that, all the usual rules for memory accesses\n+///   apply. In particular, just like in C, whether an operation is volatile has no bearing\n+///   whatsoever on questions involving concurrent access from multiple threads. Volatile accesses\n+///   behave exactly like non-atomic accesses in that regard.\n+///\n+/// - Volatile operations, however, may also be used access memory that is _outside_ of any Rust\n+///   allocation. The main use-case is CPU and peripheral registers that must be accessed via an I/O\n+///   memory mapping, most commonly at fixed addresses reserved by the hardware. These often have\n+///   special semantics associated to their manipulation, and cannot be used as general purpose\n+///   memory. Here, any address value is possible, including 0 and [`usize::MAX`], so long as the\n+///   semantics of such a read are well-defined by the target hardware. The access must not trap. It\n+///   can (and usually will) cause side-effects, but those must not affect Rust-allocated memory in\n+///   in any way. In this use-case, the pointer does *not* have to be [valid] for reads.",
    "repo_full_name": "rust-lang/rust",
    "discussion_comments": [
      {
        "comment_id": "2105211794",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 141260,
        "pr_file": "library/core/src/ptr/mod.rs",
        "discussion_id": "2105211794",
        "commented_code": "@@ -1744,54 +1749,63 @@ pub const unsafe fn write_unaligned<T>(dst: *mut T, src: T) {\n     }\n }\n \n-/// Performs a volatile read of the value from `src` without moving it. This\n-/// leaves the memory in `src` unchanged.\n-///\n-/// Volatile operations are intended to act on I/O memory, and are guaranteed\n-/// to not be elided or reordered by the compiler across other volatile\n-/// operations.\n-///\n-/// # Notes\n-///\n-/// Rust does not currently have a rigorously and formally defined memory model,\n-/// so the precise semantics of what \"volatile\" means here is subject to change\n-/// over time. That being said, the semantics will almost always end up pretty\n-/// similar to [C11's definition of volatile][c11].\n-///\n-/// The compiler shouldn't change the relative order or number of volatile\n-/// memory operations. However, volatile memory operations on zero-sized types\n-/// (e.g., if a zero-sized type is passed to `read_volatile`) are noops\n-/// and may be ignored.\n+/// Performs a volatile read of the value from `src` without moving it.\n+///\n+/// Rust does not currently have a rigorously and formally defined memory model, so the precise\n+/// semantics of what \"volatile\" means here is subject to change over time. That being said, the\n+/// semantics will almost always end up pretty similar to [C11's definition of volatile][c11].\n+///\n+/// Volatile operations are intended to act on I/O memory. As such, they are considered externally\n+/// observable events (just like syscalls), and are guaranteed to not be elided or reordered by the\n+/// compiler across other externally observable events. With this in mind, there are two cases of\n+/// usage that need to be distinguished:\n+///\n+/// - When a volatile operation is used for memory inside an [allocation], it behaves exactly like\n+///   [`read`], except for the additional guarantee that it won't be elided or reordered (see\n+///   above). This implies that the operation will actually access memory and not e.g. be lowered to\n+///   a register access or stack pop. Other than that, all the usual rules for memory accesses\n+///   apply. In particular, just like in C, whether an operation is volatile has no bearing\n+///   whatsoever on questions involving concurrent access from multiple threads. Volatile accesses\n+///   behave exactly like non-atomic accesses in that regard.\n+///\n+/// - Volatile operations, however, may also be used access memory that is _outside_ of any Rust\n+///   allocation. The main use-case is CPU and peripheral registers that must be accessed via an I/O\n+///   memory mapping, most commonly at fixed addresses reserved by the hardware. These often have\n+///   special semantics associated to their manipulation, and cannot be used as general purpose\n+///   memory. Here, any address value is possible, including 0 and [`usize::MAX`], so long as the\n+///   semantics of such a read are well-defined by the target hardware. The access must not trap. It\n+///   can (and usually will) cause side-effects, but those must not affect Rust-allocated memory in\n+///   in any way. In this use-case, the pointer does *not* have to be [valid] for reads.",
        "comment_created_at": "2025-05-23T18:27:23+00:00",
        "comment_author": "RalfJung",
        "comment_body": "```suggestion\r\n///   in any way. This access is still not considered [atomic], and as such it cannot\r\n///   be used for inter-thread synchronization.\r\n```\r\n[atomic] should link to https://doc.rust-lang.org/nightly/std/sync/atomic/index.html#memory-model-for-atomic-accesses.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2126815019",
    "pr_number": 141260,
    "pr_file": "library/core/src/ptr/mod.rs",
    "created_at": "2025-06-04T14:53:50+00:00",
    "commented_code": "}\n }\n \n-/// Performs a volatile read of the value from `src` without moving it. This\n-/// leaves the memory in `src` unchanged.\n-///\n-/// Volatile operations are intended to act on I/O memory, and are guaranteed\n-/// to not be elided or reordered by the compiler across other volatile\n-/// operations.\n-///\n-/// # Notes\n-///\n-/// Rust does not currently have a rigorously and formally defined memory model,\n-/// so the precise semantics of what \"volatile\" means here is subject to change\n-/// over time. That being said, the semantics will almost always end up pretty\n-/// similar to [C11's definition of volatile][c11].\n-///\n-/// The compiler shouldn't change the relative order or number of volatile\n-/// memory operations. However, volatile memory operations on zero-sized types\n-/// (e.g., if a zero-sized type is passed to `read_volatile`) are noops\n-/// and may be ignored.\n+/// Performs a volatile read of the value from `src` without moving it.\n+///\n+/// Rust does not currently have a rigorously and formally defined memory model, so the precise\n+/// semantics of what \"volatile\" means here is subject to change over time. That being said, the\n+/// semantics will almost always end up pretty similar to [C11's definition of volatile][c11].\n+///\n+/// Volatile operations are intended to act on I/O memory. As such, they are considered externally\n+/// observable events (just like syscalls), and are guaranteed to not be elided or reordered by the",
    "repo_full_name": "rust-lang/rust",
    "discussion_comments": [
      {
        "comment_id": "2126815019",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 141260,
        "pr_file": "library/core/src/ptr/mod.rs",
        "discussion_id": "2126815019",
        "commented_code": "@@ -2021,54 +2026,68 @@ pub const unsafe fn write_unaligned<T>(dst: *mut T, src: T) {\n     }\n }\n \n-/// Performs a volatile read of the value from `src` without moving it. This\n-/// leaves the memory in `src` unchanged.\n-///\n-/// Volatile operations are intended to act on I/O memory, and are guaranteed\n-/// to not be elided or reordered by the compiler across other volatile\n-/// operations.\n-///\n-/// # Notes\n-///\n-/// Rust does not currently have a rigorously and formally defined memory model,\n-/// so the precise semantics of what \"volatile\" means here is subject to change\n-/// over time. That being said, the semantics will almost always end up pretty\n-/// similar to [C11's definition of volatile][c11].\n-///\n-/// The compiler shouldn't change the relative order or number of volatile\n-/// memory operations. However, volatile memory operations on zero-sized types\n-/// (e.g., if a zero-sized type is passed to `read_volatile`) are noops\n-/// and may be ignored.\n+/// Performs a volatile read of the value from `src` without moving it.\n+///\n+/// Rust does not currently have a rigorously and formally defined memory model, so the precise\n+/// semantics of what \"volatile\" means here is subject to change over time. That being said, the\n+/// semantics will almost always end up pretty similar to [C11's definition of volatile][c11].\n+///\n+/// Volatile operations are intended to act on I/O memory. As such, they are considered externally\n+/// observable events (just like syscalls), and are guaranteed to not be elided or reordered by the",
        "comment_created_at": "2025-06-04T14:53:50+00:00",
        "comment_author": "RalfJung",
        "comment_body": "```suggestion\r\n/// observable events (just like syscalls, but less opaque), and are guaranteed to not be elided or reordered by the\r\n```\r\nThis seems good to mention to avoid people thinking they can treat this as a compiler barrier",
        "pr_file_module": null
      }
    ]
  }
]