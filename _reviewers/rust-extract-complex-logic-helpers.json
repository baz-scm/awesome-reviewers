[
  {
    "discussion_id": "2206149031",
    "pr_number": 143719,
    "pr_file": "compiler/rustc_driver_impl/src/lib.rs",
    "created_at": "2025-07-15T02:28:14+00:00",
    "commented_code": "return None;\n     }\n \n+    // To avoid confusion, emit warning if no space\n+    // between `-o` and arg, e.g.`-optimize`, `-out-dir`, is applied, see issue #142812\n+    if let Some(name) = matches.opt_str(\"o\")\n+        && let Some(suspect) = args.iter().find(|arg| arg.starts_with(\"-o\") && *arg != \"-o\")\n+    {\n+        let confusables = [\"optimize\", \"o0\", \"o1\", \"o2\", \"o3\", \"ofast\", \"og\", \"os\", \"oz\"];\n+        if let Some(confusable) = check_confusables(&suspect, &confusables) {\n+            early_dcx.early_warn(\n+                \"option `-o` has no space between flag name and value, which can be confusing\",\n+            );\n+            early_dcx.early_note(format!(\n+                \"option `-o {}` is applied instead of a flag named `o{}` to specify output filename `{}`\",\n+                name, name, name\n+            ));\n+            if !confusable.is_empty() {\n+                early_dcx.early_note(format!(\"Do you mean `{}`?\", confusable));\n+            }\n+        }\n+    }\n+",
    "repo_full_name": "rust-lang/rust",
    "discussion_comments": [
      {
        "comment_id": "2206149031",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 143719,
        "pr_file": "compiler/rustc_driver_impl/src/lib.rs",
        "discussion_id": "2206149031",
        "commented_code": "@@ -1237,6 +1238,26 @@ pub fn handle_options(early_dcx: &EarlyDiagCtxt, args: &[String]) -> Option<geto\n         return None;\n     }\n \n+    // To avoid confusion, emit warning if no space\n+    // between `-o` and arg, e.g.`-optimize`, `-out-dir`, is applied, see issue #142812\n+    if let Some(name) = matches.opt_str(\"o\")\n+        && let Some(suspect) = args.iter().find(|arg| arg.starts_with(\"-o\") && *arg != \"-o\")\n+    {\n+        let confusables = [\"optimize\", \"o0\", \"o1\", \"o2\", \"o3\", \"ofast\", \"og\", \"os\", \"oz\"];\n+        if let Some(confusable) = check_confusables(&suspect, &confusables) {\n+            early_dcx.early_warn(\n+                \"option `-o` has no space between flag name and value, which can be confusing\",\n+            );\n+            early_dcx.early_note(format!(\n+                \"option `-o {}` is applied instead of a flag named `o{}` to specify output filename `{}`\",\n+                name, name, name\n+            ));\n+            if !confusable.is_empty() {\n+                early_dcx.early_note(format!(\"Do you mean `{}`?\", confusable));\n+            }\n+        }\n+    }\n+",
        "comment_created_at": "2025-07-15T02:28:14+00:00",
        "comment_author": "jieyouxu",
        "comment_body": "Suggestion: can you please further pull this out to a separate helper like `warn_on_confusing_output_filename_flag` to avoid cluttering the higher-level flow of `handle_options`?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2196772978",
    "pr_number": 142097,
    "pr_file": "compiler/rustc_codegen_llvm/src/builder/gpu_offload.rs",
    "created_at": "2025-07-10T06:56:03+00:00",
    "commented_code": "+use std::ffi::CString;\n+\n+use llvm::Linkage::*;\n+use rustc_abi::Align;\n+use rustc_codegen_ssa::back::write::CodegenContext;\n+use rustc_codegen_ssa::traits::BaseTypeCodegenMethods;\n+\n+use crate::builder::SBuilder;\n+use crate::common::AsCCharPtr;\n+use crate::llvm::AttributePlace::Function;\n+use crate::llvm::{self, Linkage};\n+use crate::{LlvmCodegenBackend, SimpleCx, attributes};\n+\n+pub(crate) fn handle_gpu_code<'ll>(\n+    _cgcx: &CodegenContext<LlvmCodegenBackend>,\n+    cx: &'ll SimpleCx<'_>,\n+) {\n+    let mut o_types = vec![];\n+    let mut kernels = vec![];\n+    let offload_entry_ty = add_tgt_offload_entry(&cx);\n+    for num in 0..9 {\n+        let kernel = cx.get_function(&format!(\"kernel_{num}\"));\n+        if let Some(kernel) = kernel {\n+            o_types.push(gen_define_handling(&cx, kernel, offload_entry_ty, num));\n+            kernels.push(kernel);\n+        }\n+    }\n+\n+    gen_call_handling(&cx, &kernels, &o_types);\n+}\n+\n+// What is our @1 here? A magic global, used in our data_{begin/update/end}_mapper:\n+// @0 = private unnamed_addr constant [23 x i8] c\";unknown;unknown;0;0;;\\00\", align 1\n+// @1 = private unnamed_addr constant %struct.ident_t { i32 0, i32 2, i32 0, i32 22, ptr @0 }, align 8\n+fn generate_at_one<'ll>(cx: &'ll SimpleCx<'_>) -> &'ll llvm::Value {\n+    // @0 = private unnamed_addr constant [23 x i8] c\";unknown;unknown;0;0;;\\00\", align 1\n+    let unknown_txt = \";unknown;unknown;0;0;;\";\n+    let c_entry_name = CString::new(unknown_txt).unwrap();\n+    let c_val = c_entry_name.as_bytes_with_nul();\n+    let initializer = crate::common::bytes_in_context(cx.llcx, c_val);\n+    let at_zero = add_unnamed_global(&cx, &\"\", initializer, PrivateLinkage);\n+    llvm::set_alignment(at_zero, Align::ONE);\n+\n+    // @1 = private unnamed_addr constant %struct.ident_t { i32 0, i32 2, i32 0, i32 22, ptr @0 }, align 8\n+    let struct_ident_ty = cx.type_named_struct(\"struct.ident_t\");\n+    let struct_elems: Vec<&llvm::Value> = vec![\n+        cx.get_const_i32(0),\n+        cx.get_const_i32(2),\n+        cx.get_const_i32(0),\n+        cx.get_const_i32(22),\n+        at_zero,\n+    ];\n+    let struct_elems_ty: Vec<_> = struct_elems.iter().map(|&x| cx.val_ty(x)).collect();\n+    let initializer = crate::common::named_struct(struct_ident_ty, &struct_elems);\n+    cx.set_struct_body(struct_ident_ty, &struct_elems_ty, false);\n+    let at_one = add_unnamed_global(&cx, &\"\", initializer, PrivateLinkage);\n+    llvm::set_alignment(at_one, Align::EIGHT);\n+    at_one\n+}\n+\n+pub(crate) fn add_tgt_offload_entry<'ll>(cx: &'ll SimpleCx<'_>) -> &'ll llvm::Type {\n+    let offload_entry_ty = cx.type_named_struct(\"struct.__tgt_offload_entry\");\n+    let tptr = cx.type_ptr();\n+    let ti64 = cx.type_i64();\n+    let ti32 = cx.type_i32();\n+    let ti16 = cx.type_i16();\n+    // For each kernel to run on the gpu, we will later generate one entry of this type.\n+    // coppied from LLVM\n+    // typedef struct {\n+    //   uint64_t Reserved;\n+    //   uint16_t Version;\n+    //   uint16_t Kind;\n+    //   uint32_t Flags; Flags associated with the entry (see Target Region Entry Flags)\n+    //   void *Address; Address of global symbol within device image (function or global)\n+    //   char *SymbolName;\n+    //   uint64_t Size; Size of the entry info (0 if it is a function)\n+    //   uint64_t Data;\n+    //   void *AuxAddr;\n+    // } __tgt_offload_entry;\n+    let entry_elements = vec![ti64, ti16, ti16, ti32, tptr, tptr, ti64, ti64, tptr];\n+    cx.set_struct_body(offload_entry_ty, &entry_elements, false);\n+    offload_entry_ty\n+}\n+\n+fn gen_tgt_kernel_global<'ll>(cx: &'ll SimpleCx<'_>) {\n+    let kernel_arguments_ty = cx.type_named_struct(\"struct.__tgt_kernel_arguments\");\n+    let tptr = cx.type_ptr();\n+    let ti64 = cx.type_i64();\n+    let ti32 = cx.type_i32();\n+    let tarr = cx.type_array(ti32, 3);\n+\n+    // Taken from the LLVM APITypes.h declaration:\n+    //struct KernelArgsTy {\n+    //  uint32_t Version = 0; // Version of this struct for ABI compatibility.\n+    //  uint32_t NumArgs = 0; // Number of arguments in each input pointer.\n+    //  void **ArgBasePtrs =\n+    //      nullptr;                 // Base pointer of each argument (e.g. a struct).\n+    //  void **ArgPtrs = nullptr;    // Pointer to the argument data.\n+    //  int64_t *ArgSizes = nullptr; // Size of the argument data in bytes.\n+    //  int64_t *ArgTypes = nullptr; // Type of the data (e.g. to / from).\n+    //  void **ArgNames = nullptr;   // Name of the data for debugging, possibly null.\n+    //  void **ArgMappers = nullptr; // User-defined mappers, possibly null.\n+    //  uint64_t Tripcount =\n+    //      0; // Tripcount for the teams / distribute loop, 0 otherwise.\n+    //  struct {\n+    //    uint64_t NoWait : 1; // Was this kernel spawned with a `nowait` clause.\n+    //    uint64_t IsCUDA : 1; // Was this kernel spawned via CUDA.\n+    //    uint64_t Unused : 62;\n+    //  } Flags = {0, 0, 0};\n+    //  // The number of teams (for x,y,z dimension).\n+    //  uint32_t NumTeams[3] = {0, 0, 0};\n+    //  // The number of threads (for x,y,z dimension).\n+    //  uint32_t ThreadLimit[3] = {0, 0, 0};\n+    //  uint32_t DynCGroupMem = 0; // Amount of dynamic cgroup memory requested.\n+    //};\n+    let kernel_elements =\n+        vec![ti32, ti32, tptr, tptr, tptr, tptr, tptr, tptr, ti64, ti64, tarr, tarr, ti32];\n+\n+    cx.set_struct_body(kernel_arguments_ty, &kernel_elements, false);\n+    // For now we don't handle kernels, so for now we just add a global dummy\n+    // to make sure that the __tgt_offload_entry is defined and handled correctly.\n+    cx.declare_global(\"my_struct_global2\", kernel_arguments_ty);\n+}\n+\n+fn gen_tgt_data_mappers<'ll>(\n+    cx: &'ll SimpleCx<'_>,\n+) -> (&'ll llvm::Value, &'ll llvm::Value, &'ll llvm::Value, &'ll llvm::Type) {\n+    let tptr = cx.type_ptr();\n+    let ti64 = cx.type_i64();\n+    let ti32 = cx.type_i32();\n+\n+    let args = vec![tptr, ti64, ti32, tptr, tptr, tptr, tptr, tptr, tptr];\n+    let mapper_fn_ty = cx.type_func(&args, cx.type_void());\n+    let mapper_begin = \"__tgt_target_data_begin_mapper\";\n+    let mapper_update = \"__tgt_target_data_update_mapper\";\n+    let mapper_end = \"__tgt_target_data_end_mapper\";\n+    let begin_mapper_decl = declare_offload_fn(&cx, mapper_begin, mapper_fn_ty);\n+    let update_mapper_decl = declare_offload_fn(&cx, mapper_update, mapper_fn_ty);\n+    let end_mapper_decl = declare_offload_fn(&cx, mapper_end, mapper_fn_ty);\n+\n+    let nounwind = llvm::AttributeKind::NoUnwind.create_attr(cx.llcx);\n+    attributes::apply_to_llfn(begin_mapper_decl, Function, &[nounwind]);\n+    attributes::apply_to_llfn(update_mapper_decl, Function, &[nounwind]);\n+    attributes::apply_to_llfn(end_mapper_decl, Function, &[nounwind]);\n+\n+    (begin_mapper_decl, update_mapper_decl, end_mapper_decl, mapper_fn_ty)\n+}\n+\n+fn add_priv_unnamed_arr<'ll>(cx: &SimpleCx<'ll>, name: &str, vals: &[u64]) -> &'ll llvm::Value {\n+    let ti64 = cx.type_i64();\n+    let mut size_val = Vec::with_capacity(vals.len());\n+    for &val in vals {\n+        size_val.push(cx.get_const_i64(val));\n+    }\n+    let initializer = cx.const_array(ti64, &size_val);\n+    add_unnamed_global(cx, name, initializer, PrivateLinkage)\n+}\n+\n+pub(crate) fn add_unnamed_global<'ll>(\n+    cx: &SimpleCx<'ll>,\n+    name: &str,\n+    initializer: &'ll llvm::Value,\n+    l: Linkage,\n+) -> &'ll llvm::Value {\n+    let llglobal = add_global(cx, name, initializer, l);\n+    unsafe { llvm::LLVMSetUnnamedAddress(llglobal, llvm::UnnamedAddr::Global) };\n+    llglobal\n+}\n+\n+pub(crate) fn add_global<'ll>(\n+    cx: &SimpleCx<'ll>,\n+    name: &str,\n+    initializer: &'ll llvm::Value,\n+    l: Linkage,\n+) -> &'ll llvm::Value {\n+    let c_name = CString::new(name).unwrap();\n+    let llglobal: &'ll llvm::Value = llvm::add_global(cx.llmod, cx.val_ty(initializer), &c_name);\n+    llvm::set_global_constant(llglobal, true);\n+    llvm::set_linkage(llglobal, l);\n+    llvm::set_initializer(llglobal, initializer);\n+    llglobal\n+}\n+\n+fn gen_define_handling<'ll>(\n+    cx: &'ll SimpleCx<'_>,\n+    kernel: &'ll llvm::Value,\n+    offload_entry_ty: &'ll llvm::Type,\n+    num: i64,\n+) -> &'ll llvm::Value {\n+    let types = cx.func_params_types(cx.get_type_of_global(kernel));\n+    // It seems like non-pointer values are automatically mapped. So here, we focus on pointer (or\n+    // reference) types.\n+    let num_ptr_types = types\n+        .iter()\n+        .map(|&x| matches!(cx.type_kind(x), rustc_codegen_ssa::common::TypeKind::Pointer))\n+        .count();\n+\n+    // We do not know their size anymore at this level, so hardcode a placeholder.\n+    // A follow-up pr will track these from the frontend, where we still have Rust types.\n+    // Then, we will be able to figure out that e.g. `&[f32;256]` will result in 4*256 bytes.\n+    // I decided that 1024 bytes is a great placeholder value for now.\n+    add_priv_unnamed_arr(&cx, &format!(\".offload_sizes.{num}\"), &vec![1024; num_ptr_types]);\n+    // Here we figure out whether something needs to be copied to the gpu (=1), from the gpu (=2),\n+    // or both to and from the gpu (=3). Other values shouldn't affect us for now.\n+    // A non-mutable reference or pointer will be 1, an array that's not read, but fully overwritten\n+    // will be 2. For now, everything is 3, untill we have our frontend set up.\n+    let o_types =\n+        add_priv_unnamed_arr(&cx, &format!(\".offload_maptypes.{num}\"), &vec![3; num_ptr_types]);\n+    // Next: For each function, generate these three entries. A weak constant,\n+    // the llvm.rodata entry name, and  the omp_offloading_entries value\n+\n+    let name = format!(\".kernel_{num}.region_id\");\n+    let initializer = cx.get_const_i8(0);\n+    let region_id = add_unnamed_global(&cx, &name, initializer, WeakAnyLinkage);\n+\n+    let c_entry_name = CString::new(format!(\"kernel_{num}\")).unwrap();\n+    let c_val = c_entry_name.as_bytes_with_nul();\n+    let offload_entry_name = format!(\".offloading.entry_name.{num}\");\n+\n+    let initializer = crate::common::bytes_in_context(cx.llcx, c_val);\n+    let llglobal = add_unnamed_global(&cx, &offload_entry_name, initializer, InternalLinkage);\n+    llvm::set_alignment(llglobal, Align::ONE);\n+    let c_section_name = CString::new(\".llvm.rodata.offloading\").unwrap();\n+    llvm::set_section(llglobal, &c_section_name);\n+\n+    // Not actively used yet, for calling real kernels\n+    let name = format!(\".offloading.entry.kernel_{num}\");\n+    let ci64_0 = cx.get_const_i64(0);\n+    let ci16_1 = cx.get_const_i16(1);\n+    let elems: Vec<&llvm::Value> = vec![\n+        ci64_0,\n+        ci16_1,\n+        ci16_1,\n+        cx.get_const_i32(0),\n+        region_id,\n+        llglobal,\n+        ci64_0,\n+        ci64_0,\n+        cx.const_null(cx.type_ptr()),\n+    ];\n+\n+    let initializer = crate::common::named_struct(offload_entry_ty, &elems);\n+    let c_name = CString::new(name).unwrap();\n+    let llglobal = llvm::add_global(cx.llmod, offload_entry_ty, &c_name);\n+    llvm::set_global_constant(llglobal, true);\n+    llvm::set_linkage(llglobal, WeakAnyLinkage);\n+    llvm::set_initializer(llglobal, initializer);\n+    llvm::set_alignment(llglobal, Align::ONE);\n+    let c_section_name = CString::new(\".omp_offloading_entries\").unwrap();\n+    llvm::set_section(llglobal, &c_section_name);\n+    o_types\n+}\n+\n+fn declare_offload_fn<'ll>(\n+    cx: &'ll SimpleCx<'_>,\n+    name: &str,\n+    ty: &'ll llvm::Type,\n+) -> &'ll llvm::Value {\n+    crate::declare::declare_simple_fn(\n+        cx,\n+        name,\n+        llvm::CallConv::CCallConv,\n+        llvm::UnnamedAddr::No,\n+        llvm::Visibility::Default,\n+        ty,\n+    )\n+}\n+\n+// For each kernel *call*, we now use some of our previous declared globals to move data to and from\n+// the gpu. We don't have a proper frontend yet, so we assume that every call to a kernel function\n+// from main is intended to run on the GPU. For now, we only handle the data transfer part of it.\n+// If two consecutive kernels use the same memory, we still move it to the host and back to the gpu.\n+// Since in our frontend users (by default) don't have to specify data transfer, this is something\n+// we should optimize in the future! We also assume that everything should be copied back and forth,\n+// but sometimes we can directly zero-allocate on the device and only move back, or if something is\n+// immutable, we might only copy it to the device, but not back.\n+//\n+// Current steps:\n+// 0. Alloca some variables for the following steps\n+// 1. set insert point before kernel call.\n+// 2. generate all the GEPS and stores, to be used in 3)\n+// 3. generate __tgt_target_data_begin calls to move data to the GPU\n+//\n+// unchanged: keep kernel call. Later move the kernel to the GPU\n+//\n+// 4. set insert point after kernel call.\n+// 5. generate all the GEPS and stores, to be used in 6)\n+// 6. generate __tgt_target_data_end calls to move data from the GPU\n+fn gen_call_handling<'ll>(\n+    cx: &'ll SimpleCx<'_>,\n+    _kernels: &[&'ll llvm::Value],\n+    o_types: &[&'ll llvm::Value],\n+) {\n+    // %struct.__tgt_bin_desc = type { i32, ptr, ptr, ptr }\n+    let tptr = cx.type_ptr();\n+    let ti32 = cx.type_i32();\n+    let tgt_bin_desc_ty = vec![ti32, tptr, tptr, tptr];\n+    let tgt_bin_desc = cx.type_named_struct(\"struct.__tgt_bin_desc\");\n+    cx.set_struct_body(tgt_bin_desc, &tgt_bin_desc_ty, false);\n+\n+    gen_tgt_kernel_global(&cx);\n+    let (begin_mapper_decl, _, end_mapper_decl, fn_ty) = gen_tgt_data_mappers(&cx);\n+\n+    let main_fn = cx.get_function(\"main\");\n+    let Some(main_fn) = main_fn else { return };\n+    let kernel_name = \"kernel_1\";\n+    let call = unsafe {\n+        llvm::LLVMRustGetFunctionCall(main_fn, kernel_name.as_c_char_ptr(), kernel_name.len())\n+    };\n+    let Some(kernel_call) = call else {\n+        return;\n+    };\n+    let kernel_call_bb = unsafe { llvm::LLVMGetInstructionParent(kernel_call) };\n+    let called = unsafe { llvm::LLVMGetCalledValue(kernel_call).unwrap() };\n+    let mut builder = SBuilder::build(cx, kernel_call_bb);\n+\n+    let types = cx.func_params_types(cx.get_type_of_global(called));\n+    let num_args = types.len() as u64;\n+\n+    // Step 0)\n+    // %struct.__tgt_bin_desc = type { i32, ptr, ptr, ptr }\n+    // %6 = alloca %struct.__tgt_bin_desc, align 8\n+    unsafe { llvm::LLVMRustPositionBuilderPastAllocas(builder.llbuilder, main_fn) };\n+\n+    let tgt_bin_desc_alloca = builder.direct_alloca(tgt_bin_desc, Align::EIGHT, \"EmptyDesc\");\n+\n+    let ty = cx.type_array(cx.type_ptr(), num_args);\n+    // Baseptr are just the input pointer to the kernel, stored in a local alloca\n+    let a1 = builder.direct_alloca(ty, Align::EIGHT, \".offload_baseptrs\");\n+    // Ptrs are the result of a gep into the baseptr, at least for our trivial types.\n+    let a2 = builder.direct_alloca(ty, Align::EIGHT, \".offload_ptrs\");\n+    // These represent the sizes in bytes, e.g. the entry for `&[f64; 16]` will be 8*16.\n+    let ty2 = cx.type_array(cx.type_i64(), num_args);\n+    let a4 = builder.direct_alloca(ty2, Align::EIGHT, \".offload_sizes\");\n+    // Now we allocate once per function param, a copy to be passed to one of our maps.\n+    let mut vals = vec![];\n+    let mut geps = vec![];\n+    let i32_0 = cx.get_const_i32(0);\n+    for (index, in_ty) in types.iter().enumerate() {\n+        // get function arg, store it into the alloca, and read it.\n+        let p = llvm::get_param(called, index as u32);\n+        let name = llvm::get_value_name(p);\n+        let name = str::from_utf8(name).unwrap();\n+        let arg_name = format!(\"{name}.addr\");\n+        let alloca = builder.direct_alloca(in_ty, Align::EIGHT, &arg_name);\n+\n+        builder.store(p, alloca, Align::EIGHT);\n+        let val = builder.load(in_ty, alloca, Align::EIGHT);\n+        let gep = builder.inbounds_gep(cx.type_f32(), val, &[i32_0]);\n+        vals.push(val);\n+        geps.push(gep);\n+    }\n+\n+    // Step 1)\n+    unsafe { llvm::LLVMRustPositionBefore(builder.llbuilder, kernel_call) };\n+    builder.memset(\n+        tgt_bin_desc_alloca,\n+        cx.get_const_i8(0),\n+        cx.get_const_i64(32),\n+        Align::from_bytes(8).unwrap(),\n+    );\n+\n+    let mapper_fn_ty = cx.type_func(&[cx.type_ptr()], cx.type_void());\n+    let register_lib_decl = declare_offload_fn(&cx, \"__tgt_register_lib\", mapper_fn_ty);\n+    let unregister_lib_decl = declare_offload_fn(&cx, \"__tgt_unregister_lib\", mapper_fn_ty);\n+    let init_ty = cx.type_func(&[], cx.type_void());\n+    let init_rtls_decl = declare_offload_fn(cx, \"__tgt_init_all_rtls\", init_ty);\n+\n+    // call void @__tgt_register_lib(ptr noundef %6)\n+    builder.call(mapper_fn_ty, register_lib_decl, &[tgt_bin_desc_alloca], None);\n+    // call void @__tgt_init_all_rtls()\n+    builder.call(init_ty, init_rtls_decl, &[], None);\n+\n+    for i in 0..num_args {\n+        let idx = cx.get_const_i32(i);\n+        let gep1 = builder.inbounds_gep(ty, a1, &[i32_0, idx]);\n+        builder.store(vals[i as usize], gep1, Align::EIGHT);\n+        let gep2 = builder.inbounds_gep(ty, a2, &[i32_0, idx]);\n+        builder.store(geps[i as usize], gep2, Align::EIGHT);\n+        let gep3 = builder.inbounds_gep(ty2, a4, &[i32_0, idx]);\n+        // As mentioned above, we don't use Rust type informatino yet. So for now we will just\n+        // assume that we have 1024 bytes, 256 f32 values.\n+        // FIXME(offload): write an offload frontend and handle arbitrary types.\n+        builder.store(cx.get_const_i64(1024), gep3, Align::EIGHT);\n+    }\n+\n+    // Step 2)\n+    let gep1 = builder.inbounds_gep(ty, a1, &[i32_0, i32_0]);\n+    let gep2 = builder.inbounds_gep(ty, a2, &[i32_0, i32_0]);\n+    let gep3 = builder.inbounds_gep(ty2, a4, &[i32_0, i32_0]);\n+\n+    let nullptr = cx.const_null(cx.type_ptr());\n+    let o_type = o_types[0];\n+    let s_ident_t = generate_at_one(&cx);\n+    let args = vec![\n+        s_ident_t,\n+        cx.get_const_i64(u64::MAX),\n+        cx.get_const_i32(num_args),\n+        gep1,\n+        gep2,\n+        gep3,\n+        o_type,\n+        nullptr,\n+        nullptr,\n+    ];\n+    builder.call(fn_ty, begin_mapper_decl, &args, None);\n+\n+    // Step 3)\n+    // Here we will add code for the actual kernel launches in a follow-up PR.\n+    // FIXME(offload): launch kernels\n+\n+    // Step 4)\n+    unsafe { llvm::LLVMRustPositionAfter(builder.llbuilder, kernel_call) };\n+\n+    let gep1 = builder.inbounds_gep(ty, a1, &[i32_0, i32_0]);\n+    let gep2 = builder.inbounds_gep(ty, a2, &[i32_0, i32_0]);\n+    let gep3 = builder.inbounds_gep(ty2, a4, &[i32_0, i32_0]);\n+\n+    let nullptr = cx.const_null(cx.type_ptr());\n+    let o_type = o_types[0];\n+    let args = vec![\n+        s_ident_t,\n+        cx.get_const_i64(u64::MAX),\n+        cx.get_const_i32(num_args),\n+        gep1,\n+        gep2,\n+        gep3,\n+        o_type,\n+        nullptr,\n+        nullptr,\n+    ];\n+    builder.call(fn_ty, end_mapper_decl, &args, None);",
    "repo_full_name": "rust-lang/rust",
    "discussion_comments": [
      {
        "comment_id": "2196772978",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 142097,
        "pr_file": "compiler/rustc_codegen_llvm/src/builder/gpu_offload.rs",
        "discussion_id": "2196772978",
        "commented_code": "@@ -0,0 +1,440 @@\n+use std::ffi::CString;\n+\n+use llvm::Linkage::*;\n+use rustc_abi::Align;\n+use rustc_codegen_ssa::back::write::CodegenContext;\n+use rustc_codegen_ssa::traits::BaseTypeCodegenMethods;\n+\n+use crate::builder::SBuilder;\n+use crate::common::AsCCharPtr;\n+use crate::llvm::AttributePlace::Function;\n+use crate::llvm::{self, Linkage};\n+use crate::{LlvmCodegenBackend, SimpleCx, attributes};\n+\n+pub(crate) fn handle_gpu_code<'ll>(\n+    _cgcx: &CodegenContext<LlvmCodegenBackend>,\n+    cx: &'ll SimpleCx<'_>,\n+) {\n+    let mut o_types = vec![];\n+    let mut kernels = vec![];\n+    let offload_entry_ty = add_tgt_offload_entry(&cx);\n+    for num in 0..9 {\n+        let kernel = cx.get_function(&format!(\"kernel_{num}\"));\n+        if let Some(kernel) = kernel {\n+            o_types.push(gen_define_handling(&cx, kernel, offload_entry_ty, num));\n+            kernels.push(kernel);\n+        }\n+    }\n+\n+    gen_call_handling(&cx, &kernels, &o_types);\n+}\n+\n+// What is our @1 here? A magic global, used in our data_{begin/update/end}_mapper:\n+// @0 = private unnamed_addr constant [23 x i8] c\";unknown;unknown;0;0;;\\00\", align 1\n+// @1 = private unnamed_addr constant %struct.ident_t { i32 0, i32 2, i32 0, i32 22, ptr @0 }, align 8\n+fn generate_at_one<'ll>(cx: &'ll SimpleCx<'_>) -> &'ll llvm::Value {\n+    // @0 = private unnamed_addr constant [23 x i8] c\";unknown;unknown;0;0;;\\00\", align 1\n+    let unknown_txt = \";unknown;unknown;0;0;;\";\n+    let c_entry_name = CString::new(unknown_txt).unwrap();\n+    let c_val = c_entry_name.as_bytes_with_nul();\n+    let initializer = crate::common::bytes_in_context(cx.llcx, c_val);\n+    let at_zero = add_unnamed_global(&cx, &\"\", initializer, PrivateLinkage);\n+    llvm::set_alignment(at_zero, Align::ONE);\n+\n+    // @1 = private unnamed_addr constant %struct.ident_t { i32 0, i32 2, i32 0, i32 22, ptr @0 }, align 8\n+    let struct_ident_ty = cx.type_named_struct(\"struct.ident_t\");\n+    let struct_elems: Vec<&llvm::Value> = vec![\n+        cx.get_const_i32(0),\n+        cx.get_const_i32(2),\n+        cx.get_const_i32(0),\n+        cx.get_const_i32(22),\n+        at_zero,\n+    ];\n+    let struct_elems_ty: Vec<_> = struct_elems.iter().map(|&x| cx.val_ty(x)).collect();\n+    let initializer = crate::common::named_struct(struct_ident_ty, &struct_elems);\n+    cx.set_struct_body(struct_ident_ty, &struct_elems_ty, false);\n+    let at_one = add_unnamed_global(&cx, &\"\", initializer, PrivateLinkage);\n+    llvm::set_alignment(at_one, Align::EIGHT);\n+    at_one\n+}\n+\n+pub(crate) fn add_tgt_offload_entry<'ll>(cx: &'ll SimpleCx<'_>) -> &'ll llvm::Type {\n+    let offload_entry_ty = cx.type_named_struct(\"struct.__tgt_offload_entry\");\n+    let tptr = cx.type_ptr();\n+    let ti64 = cx.type_i64();\n+    let ti32 = cx.type_i32();\n+    let ti16 = cx.type_i16();\n+    // For each kernel to run on the gpu, we will later generate one entry of this type.\n+    // coppied from LLVM\n+    // typedef struct {\n+    //   uint64_t Reserved;\n+    //   uint16_t Version;\n+    //   uint16_t Kind;\n+    //   uint32_t Flags; Flags associated with the entry (see Target Region Entry Flags)\n+    //   void *Address; Address of global symbol within device image (function or global)\n+    //   char *SymbolName;\n+    //   uint64_t Size; Size of the entry info (0 if it is a function)\n+    //   uint64_t Data;\n+    //   void *AuxAddr;\n+    // } __tgt_offload_entry;\n+    let entry_elements = vec![ti64, ti16, ti16, ti32, tptr, tptr, ti64, ti64, tptr];\n+    cx.set_struct_body(offload_entry_ty, &entry_elements, false);\n+    offload_entry_ty\n+}\n+\n+fn gen_tgt_kernel_global<'ll>(cx: &'ll SimpleCx<'_>) {\n+    let kernel_arguments_ty = cx.type_named_struct(\"struct.__tgt_kernel_arguments\");\n+    let tptr = cx.type_ptr();\n+    let ti64 = cx.type_i64();\n+    let ti32 = cx.type_i32();\n+    let tarr = cx.type_array(ti32, 3);\n+\n+    // Taken from the LLVM APITypes.h declaration:\n+    //struct KernelArgsTy {\n+    //  uint32_t Version = 0; // Version of this struct for ABI compatibility.\n+    //  uint32_t NumArgs = 0; // Number of arguments in each input pointer.\n+    //  void **ArgBasePtrs =\n+    //      nullptr;                 // Base pointer of each argument (e.g. a struct).\n+    //  void **ArgPtrs = nullptr;    // Pointer to the argument data.\n+    //  int64_t *ArgSizes = nullptr; // Size of the argument data in bytes.\n+    //  int64_t *ArgTypes = nullptr; // Type of the data (e.g. to / from).\n+    //  void **ArgNames = nullptr;   // Name of the data for debugging, possibly null.\n+    //  void **ArgMappers = nullptr; // User-defined mappers, possibly null.\n+    //  uint64_t Tripcount =\n+    //      0; // Tripcount for the teams / distribute loop, 0 otherwise.\n+    //  struct {\n+    //    uint64_t NoWait : 1; // Was this kernel spawned with a `nowait` clause.\n+    //    uint64_t IsCUDA : 1; // Was this kernel spawned via CUDA.\n+    //    uint64_t Unused : 62;\n+    //  } Flags = {0, 0, 0};\n+    //  // The number of teams (for x,y,z dimension).\n+    //  uint32_t NumTeams[3] = {0, 0, 0};\n+    //  // The number of threads (for x,y,z dimension).\n+    //  uint32_t ThreadLimit[3] = {0, 0, 0};\n+    //  uint32_t DynCGroupMem = 0; // Amount of dynamic cgroup memory requested.\n+    //};\n+    let kernel_elements =\n+        vec![ti32, ti32, tptr, tptr, tptr, tptr, tptr, tptr, ti64, ti64, tarr, tarr, ti32];\n+\n+    cx.set_struct_body(kernel_arguments_ty, &kernel_elements, false);\n+    // For now we don't handle kernels, so for now we just add a global dummy\n+    // to make sure that the __tgt_offload_entry is defined and handled correctly.\n+    cx.declare_global(\"my_struct_global2\", kernel_arguments_ty);\n+}\n+\n+fn gen_tgt_data_mappers<'ll>(\n+    cx: &'ll SimpleCx<'_>,\n+) -> (&'ll llvm::Value, &'ll llvm::Value, &'ll llvm::Value, &'ll llvm::Type) {\n+    let tptr = cx.type_ptr();\n+    let ti64 = cx.type_i64();\n+    let ti32 = cx.type_i32();\n+\n+    let args = vec![tptr, ti64, ti32, tptr, tptr, tptr, tptr, tptr, tptr];\n+    let mapper_fn_ty = cx.type_func(&args, cx.type_void());\n+    let mapper_begin = \"__tgt_target_data_begin_mapper\";\n+    let mapper_update = \"__tgt_target_data_update_mapper\";\n+    let mapper_end = \"__tgt_target_data_end_mapper\";\n+    let begin_mapper_decl = declare_offload_fn(&cx, mapper_begin, mapper_fn_ty);\n+    let update_mapper_decl = declare_offload_fn(&cx, mapper_update, mapper_fn_ty);\n+    let end_mapper_decl = declare_offload_fn(&cx, mapper_end, mapper_fn_ty);\n+\n+    let nounwind = llvm::AttributeKind::NoUnwind.create_attr(cx.llcx);\n+    attributes::apply_to_llfn(begin_mapper_decl, Function, &[nounwind]);\n+    attributes::apply_to_llfn(update_mapper_decl, Function, &[nounwind]);\n+    attributes::apply_to_llfn(end_mapper_decl, Function, &[nounwind]);\n+\n+    (begin_mapper_decl, update_mapper_decl, end_mapper_decl, mapper_fn_ty)\n+}\n+\n+fn add_priv_unnamed_arr<'ll>(cx: &SimpleCx<'ll>, name: &str, vals: &[u64]) -> &'ll llvm::Value {\n+    let ti64 = cx.type_i64();\n+    let mut size_val = Vec::with_capacity(vals.len());\n+    for &val in vals {\n+        size_val.push(cx.get_const_i64(val));\n+    }\n+    let initializer = cx.const_array(ti64, &size_val);\n+    add_unnamed_global(cx, name, initializer, PrivateLinkage)\n+}\n+\n+pub(crate) fn add_unnamed_global<'ll>(\n+    cx: &SimpleCx<'ll>,\n+    name: &str,\n+    initializer: &'ll llvm::Value,\n+    l: Linkage,\n+) -> &'ll llvm::Value {\n+    let llglobal = add_global(cx, name, initializer, l);\n+    unsafe { llvm::LLVMSetUnnamedAddress(llglobal, llvm::UnnamedAddr::Global) };\n+    llglobal\n+}\n+\n+pub(crate) fn add_global<'ll>(\n+    cx: &SimpleCx<'ll>,\n+    name: &str,\n+    initializer: &'ll llvm::Value,\n+    l: Linkage,\n+) -> &'ll llvm::Value {\n+    let c_name = CString::new(name).unwrap();\n+    let llglobal: &'ll llvm::Value = llvm::add_global(cx.llmod, cx.val_ty(initializer), &c_name);\n+    llvm::set_global_constant(llglobal, true);\n+    llvm::set_linkage(llglobal, l);\n+    llvm::set_initializer(llglobal, initializer);\n+    llglobal\n+}\n+\n+fn gen_define_handling<'ll>(\n+    cx: &'ll SimpleCx<'_>,\n+    kernel: &'ll llvm::Value,\n+    offload_entry_ty: &'ll llvm::Type,\n+    num: i64,\n+) -> &'ll llvm::Value {\n+    let types = cx.func_params_types(cx.get_type_of_global(kernel));\n+    // It seems like non-pointer values are automatically mapped. So here, we focus on pointer (or\n+    // reference) types.\n+    let num_ptr_types = types\n+        .iter()\n+        .map(|&x| matches!(cx.type_kind(x), rustc_codegen_ssa::common::TypeKind::Pointer))\n+        .count();\n+\n+    // We do not know their size anymore at this level, so hardcode a placeholder.\n+    // A follow-up pr will track these from the frontend, where we still have Rust types.\n+    // Then, we will be able to figure out that e.g. `&[f32;256]` will result in 4*256 bytes.\n+    // I decided that 1024 bytes is a great placeholder value for now.\n+    add_priv_unnamed_arr(&cx, &format!(\".offload_sizes.{num}\"), &vec![1024; num_ptr_types]);\n+    // Here we figure out whether something needs to be copied to the gpu (=1), from the gpu (=2),\n+    // or both to and from the gpu (=3). Other values shouldn't affect us for now.\n+    // A non-mutable reference or pointer will be 1, an array that's not read, but fully overwritten\n+    // will be 2. For now, everything is 3, untill we have our frontend set up.\n+    let o_types =\n+        add_priv_unnamed_arr(&cx, &format!(\".offload_maptypes.{num}\"), &vec![3; num_ptr_types]);\n+    // Next: For each function, generate these three entries. A weak constant,\n+    // the llvm.rodata entry name, and  the omp_offloading_entries value\n+\n+    let name = format!(\".kernel_{num}.region_id\");\n+    let initializer = cx.get_const_i8(0);\n+    let region_id = add_unnamed_global(&cx, &name, initializer, WeakAnyLinkage);\n+\n+    let c_entry_name = CString::new(format!(\"kernel_{num}\")).unwrap();\n+    let c_val = c_entry_name.as_bytes_with_nul();\n+    let offload_entry_name = format!(\".offloading.entry_name.{num}\");\n+\n+    let initializer = crate::common::bytes_in_context(cx.llcx, c_val);\n+    let llglobal = add_unnamed_global(&cx, &offload_entry_name, initializer, InternalLinkage);\n+    llvm::set_alignment(llglobal, Align::ONE);\n+    let c_section_name = CString::new(\".llvm.rodata.offloading\").unwrap();\n+    llvm::set_section(llglobal, &c_section_name);\n+\n+    // Not actively used yet, for calling real kernels\n+    let name = format!(\".offloading.entry.kernel_{num}\");\n+    let ci64_0 = cx.get_const_i64(0);\n+    let ci16_1 = cx.get_const_i16(1);\n+    let elems: Vec<&llvm::Value> = vec![\n+        ci64_0,\n+        ci16_1,\n+        ci16_1,\n+        cx.get_const_i32(0),\n+        region_id,\n+        llglobal,\n+        ci64_0,\n+        ci64_0,\n+        cx.const_null(cx.type_ptr()),\n+    ];\n+\n+    let initializer = crate::common::named_struct(offload_entry_ty, &elems);\n+    let c_name = CString::new(name).unwrap();\n+    let llglobal = llvm::add_global(cx.llmod, offload_entry_ty, &c_name);\n+    llvm::set_global_constant(llglobal, true);\n+    llvm::set_linkage(llglobal, WeakAnyLinkage);\n+    llvm::set_initializer(llglobal, initializer);\n+    llvm::set_alignment(llglobal, Align::ONE);\n+    let c_section_name = CString::new(\".omp_offloading_entries\").unwrap();\n+    llvm::set_section(llglobal, &c_section_name);\n+    o_types\n+}\n+\n+fn declare_offload_fn<'ll>(\n+    cx: &'ll SimpleCx<'_>,\n+    name: &str,\n+    ty: &'ll llvm::Type,\n+) -> &'ll llvm::Value {\n+    crate::declare::declare_simple_fn(\n+        cx,\n+        name,\n+        llvm::CallConv::CCallConv,\n+        llvm::UnnamedAddr::No,\n+        llvm::Visibility::Default,\n+        ty,\n+    )\n+}\n+\n+// For each kernel *call*, we now use some of our previous declared globals to move data to and from\n+// the gpu. We don't have a proper frontend yet, so we assume that every call to a kernel function\n+// from main is intended to run on the GPU. For now, we only handle the data transfer part of it.\n+// If two consecutive kernels use the same memory, we still move it to the host and back to the gpu.\n+// Since in our frontend users (by default) don't have to specify data transfer, this is something\n+// we should optimize in the future! We also assume that everything should be copied back and forth,\n+// but sometimes we can directly zero-allocate on the device and only move back, or if something is\n+// immutable, we might only copy it to the device, but not back.\n+//\n+// Current steps:\n+// 0. Alloca some variables for the following steps\n+// 1. set insert point before kernel call.\n+// 2. generate all the GEPS and stores, to be used in 3)\n+// 3. generate __tgt_target_data_begin calls to move data to the GPU\n+//\n+// unchanged: keep kernel call. Later move the kernel to the GPU\n+//\n+// 4. set insert point after kernel call.\n+// 5. generate all the GEPS and stores, to be used in 6)\n+// 6. generate __tgt_target_data_end calls to move data from the GPU\n+fn gen_call_handling<'ll>(\n+    cx: &'ll SimpleCx<'_>,\n+    _kernels: &[&'ll llvm::Value],\n+    o_types: &[&'ll llvm::Value],\n+) {\n+    // %struct.__tgt_bin_desc = type { i32, ptr, ptr, ptr }\n+    let tptr = cx.type_ptr();\n+    let ti32 = cx.type_i32();\n+    let tgt_bin_desc_ty = vec![ti32, tptr, tptr, tptr];\n+    let tgt_bin_desc = cx.type_named_struct(\"struct.__tgt_bin_desc\");\n+    cx.set_struct_body(tgt_bin_desc, &tgt_bin_desc_ty, false);\n+\n+    gen_tgt_kernel_global(&cx);\n+    let (begin_mapper_decl, _, end_mapper_decl, fn_ty) = gen_tgt_data_mappers(&cx);\n+\n+    let main_fn = cx.get_function(\"main\");\n+    let Some(main_fn) = main_fn else { return };\n+    let kernel_name = \"kernel_1\";\n+    let call = unsafe {\n+        llvm::LLVMRustGetFunctionCall(main_fn, kernel_name.as_c_char_ptr(), kernel_name.len())\n+    };\n+    let Some(kernel_call) = call else {\n+        return;\n+    };\n+    let kernel_call_bb = unsafe { llvm::LLVMGetInstructionParent(kernel_call) };\n+    let called = unsafe { llvm::LLVMGetCalledValue(kernel_call).unwrap() };\n+    let mut builder = SBuilder::build(cx, kernel_call_bb);\n+\n+    let types = cx.func_params_types(cx.get_type_of_global(called));\n+    let num_args = types.len() as u64;\n+\n+    // Step 0)\n+    // %struct.__tgt_bin_desc = type { i32, ptr, ptr, ptr }\n+    // %6 = alloca %struct.__tgt_bin_desc, align 8\n+    unsafe { llvm::LLVMRustPositionBuilderPastAllocas(builder.llbuilder, main_fn) };\n+\n+    let tgt_bin_desc_alloca = builder.direct_alloca(tgt_bin_desc, Align::EIGHT, \"EmptyDesc\");\n+\n+    let ty = cx.type_array(cx.type_ptr(), num_args);\n+    // Baseptr are just the input pointer to the kernel, stored in a local alloca\n+    let a1 = builder.direct_alloca(ty, Align::EIGHT, \".offload_baseptrs\");\n+    // Ptrs are the result of a gep into the baseptr, at least for our trivial types.\n+    let a2 = builder.direct_alloca(ty, Align::EIGHT, \".offload_ptrs\");\n+    // These represent the sizes in bytes, e.g. the entry for `&[f64; 16]` will be 8*16.\n+    let ty2 = cx.type_array(cx.type_i64(), num_args);\n+    let a4 = builder.direct_alloca(ty2, Align::EIGHT, \".offload_sizes\");\n+    // Now we allocate once per function param, a copy to be passed to one of our maps.\n+    let mut vals = vec![];\n+    let mut geps = vec![];\n+    let i32_0 = cx.get_const_i32(0);\n+    for (index, in_ty) in types.iter().enumerate() {\n+        // get function arg, store it into the alloca, and read it.\n+        let p = llvm::get_param(called, index as u32);\n+        let name = llvm::get_value_name(p);\n+        let name = str::from_utf8(name).unwrap();\n+        let arg_name = format!(\"{name}.addr\");\n+        let alloca = builder.direct_alloca(in_ty, Align::EIGHT, &arg_name);\n+\n+        builder.store(p, alloca, Align::EIGHT);\n+        let val = builder.load(in_ty, alloca, Align::EIGHT);\n+        let gep = builder.inbounds_gep(cx.type_f32(), val, &[i32_0]);\n+        vals.push(val);\n+        geps.push(gep);\n+    }\n+\n+    // Step 1)\n+    unsafe { llvm::LLVMRustPositionBefore(builder.llbuilder, kernel_call) };\n+    builder.memset(\n+        tgt_bin_desc_alloca,\n+        cx.get_const_i8(0),\n+        cx.get_const_i64(32),\n+        Align::from_bytes(8).unwrap(),\n+    );\n+\n+    let mapper_fn_ty = cx.type_func(&[cx.type_ptr()], cx.type_void());\n+    let register_lib_decl = declare_offload_fn(&cx, \"__tgt_register_lib\", mapper_fn_ty);\n+    let unregister_lib_decl = declare_offload_fn(&cx, \"__tgt_unregister_lib\", mapper_fn_ty);\n+    let init_ty = cx.type_func(&[], cx.type_void());\n+    let init_rtls_decl = declare_offload_fn(cx, \"__tgt_init_all_rtls\", init_ty);\n+\n+    // call void @__tgt_register_lib(ptr noundef %6)\n+    builder.call(mapper_fn_ty, register_lib_decl, &[tgt_bin_desc_alloca], None);\n+    // call void @__tgt_init_all_rtls()\n+    builder.call(init_ty, init_rtls_decl, &[], None);\n+\n+    for i in 0..num_args {\n+        let idx = cx.get_const_i32(i);\n+        let gep1 = builder.inbounds_gep(ty, a1, &[i32_0, idx]);\n+        builder.store(vals[i as usize], gep1, Align::EIGHT);\n+        let gep2 = builder.inbounds_gep(ty, a2, &[i32_0, idx]);\n+        builder.store(geps[i as usize], gep2, Align::EIGHT);\n+        let gep3 = builder.inbounds_gep(ty2, a4, &[i32_0, idx]);\n+        // As mentioned above, we don't use Rust type informatino yet. So for now we will just\n+        // assume that we have 1024 bytes, 256 f32 values.\n+        // FIXME(offload): write an offload frontend and handle arbitrary types.\n+        builder.store(cx.get_const_i64(1024), gep3, Align::EIGHT);\n+    }\n+\n+    // Step 2)\n+    let gep1 = builder.inbounds_gep(ty, a1, &[i32_0, i32_0]);\n+    let gep2 = builder.inbounds_gep(ty, a2, &[i32_0, i32_0]);\n+    let gep3 = builder.inbounds_gep(ty2, a4, &[i32_0, i32_0]);\n+\n+    let nullptr = cx.const_null(cx.type_ptr());\n+    let o_type = o_types[0];\n+    let s_ident_t = generate_at_one(&cx);\n+    let args = vec![\n+        s_ident_t,\n+        cx.get_const_i64(u64::MAX),\n+        cx.get_const_i32(num_args),\n+        gep1,\n+        gep2,\n+        gep3,\n+        o_type,\n+        nullptr,\n+        nullptr,\n+    ];\n+    builder.call(fn_ty, begin_mapper_decl, &args, None);\n+\n+    // Step 3)\n+    // Here we will add code for the actual kernel launches in a follow-up PR.\n+    // FIXME(offload): launch kernels\n+\n+    // Step 4)\n+    unsafe { llvm::LLVMRustPositionAfter(builder.llbuilder, kernel_call) };\n+\n+    let gep1 = builder.inbounds_gep(ty, a1, &[i32_0, i32_0]);\n+    let gep2 = builder.inbounds_gep(ty, a2, &[i32_0, i32_0]);\n+    let gep3 = builder.inbounds_gep(ty2, a4, &[i32_0, i32_0]);\n+\n+    let nullptr = cx.const_null(cx.type_ptr());\n+    let o_type = o_types[0];\n+    let args = vec![\n+        s_ident_t,\n+        cx.get_const_i64(u64::MAX),\n+        cx.get_const_i32(num_args),\n+        gep1,\n+        gep2,\n+        gep3,\n+        o_type,\n+        nullptr,\n+        nullptr,\n+    ];\n+    builder.call(fn_ty, end_mapper_decl, &args, None);",
        "comment_created_at": "2025-07-10T06:56:03+00:00",
        "comment_author": "oli-obk",
        "comment_body": "Since almost the exact same call is gonna be happening thrice in the future, pull this into a function that deduplicates as much as possible",
        "pr_file_module": null
      },
      {
        "comment_id": "2198686675",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 142097,
        "pr_file": "compiler/rustc_codegen_llvm/src/builder/gpu_offload.rs",
        "discussion_id": "2196772978",
        "commented_code": "@@ -0,0 +1,440 @@\n+use std::ffi::CString;\n+\n+use llvm::Linkage::*;\n+use rustc_abi::Align;\n+use rustc_codegen_ssa::back::write::CodegenContext;\n+use rustc_codegen_ssa::traits::BaseTypeCodegenMethods;\n+\n+use crate::builder::SBuilder;\n+use crate::common::AsCCharPtr;\n+use crate::llvm::AttributePlace::Function;\n+use crate::llvm::{self, Linkage};\n+use crate::{LlvmCodegenBackend, SimpleCx, attributes};\n+\n+pub(crate) fn handle_gpu_code<'ll>(\n+    _cgcx: &CodegenContext<LlvmCodegenBackend>,\n+    cx: &'ll SimpleCx<'_>,\n+) {\n+    let mut o_types = vec![];\n+    let mut kernels = vec![];\n+    let offload_entry_ty = add_tgt_offload_entry(&cx);\n+    for num in 0..9 {\n+        let kernel = cx.get_function(&format!(\"kernel_{num}\"));\n+        if let Some(kernel) = kernel {\n+            o_types.push(gen_define_handling(&cx, kernel, offload_entry_ty, num));\n+            kernels.push(kernel);\n+        }\n+    }\n+\n+    gen_call_handling(&cx, &kernels, &o_types);\n+}\n+\n+// What is our @1 here? A magic global, used in our data_{begin/update/end}_mapper:\n+// @0 = private unnamed_addr constant [23 x i8] c\";unknown;unknown;0;0;;\\00\", align 1\n+// @1 = private unnamed_addr constant %struct.ident_t { i32 0, i32 2, i32 0, i32 22, ptr @0 }, align 8\n+fn generate_at_one<'ll>(cx: &'ll SimpleCx<'_>) -> &'ll llvm::Value {\n+    // @0 = private unnamed_addr constant [23 x i8] c\";unknown;unknown;0;0;;\\00\", align 1\n+    let unknown_txt = \";unknown;unknown;0;0;;\";\n+    let c_entry_name = CString::new(unknown_txt).unwrap();\n+    let c_val = c_entry_name.as_bytes_with_nul();\n+    let initializer = crate::common::bytes_in_context(cx.llcx, c_val);\n+    let at_zero = add_unnamed_global(&cx, &\"\", initializer, PrivateLinkage);\n+    llvm::set_alignment(at_zero, Align::ONE);\n+\n+    // @1 = private unnamed_addr constant %struct.ident_t { i32 0, i32 2, i32 0, i32 22, ptr @0 }, align 8\n+    let struct_ident_ty = cx.type_named_struct(\"struct.ident_t\");\n+    let struct_elems: Vec<&llvm::Value> = vec![\n+        cx.get_const_i32(0),\n+        cx.get_const_i32(2),\n+        cx.get_const_i32(0),\n+        cx.get_const_i32(22),\n+        at_zero,\n+    ];\n+    let struct_elems_ty: Vec<_> = struct_elems.iter().map(|&x| cx.val_ty(x)).collect();\n+    let initializer = crate::common::named_struct(struct_ident_ty, &struct_elems);\n+    cx.set_struct_body(struct_ident_ty, &struct_elems_ty, false);\n+    let at_one = add_unnamed_global(&cx, &\"\", initializer, PrivateLinkage);\n+    llvm::set_alignment(at_one, Align::EIGHT);\n+    at_one\n+}\n+\n+pub(crate) fn add_tgt_offload_entry<'ll>(cx: &'ll SimpleCx<'_>) -> &'ll llvm::Type {\n+    let offload_entry_ty = cx.type_named_struct(\"struct.__tgt_offload_entry\");\n+    let tptr = cx.type_ptr();\n+    let ti64 = cx.type_i64();\n+    let ti32 = cx.type_i32();\n+    let ti16 = cx.type_i16();\n+    // For each kernel to run on the gpu, we will later generate one entry of this type.\n+    // coppied from LLVM\n+    // typedef struct {\n+    //   uint64_t Reserved;\n+    //   uint16_t Version;\n+    //   uint16_t Kind;\n+    //   uint32_t Flags; Flags associated with the entry (see Target Region Entry Flags)\n+    //   void *Address; Address of global symbol within device image (function or global)\n+    //   char *SymbolName;\n+    //   uint64_t Size; Size of the entry info (0 if it is a function)\n+    //   uint64_t Data;\n+    //   void *AuxAddr;\n+    // } __tgt_offload_entry;\n+    let entry_elements = vec![ti64, ti16, ti16, ti32, tptr, tptr, ti64, ti64, tptr];\n+    cx.set_struct_body(offload_entry_ty, &entry_elements, false);\n+    offload_entry_ty\n+}\n+\n+fn gen_tgt_kernel_global<'ll>(cx: &'ll SimpleCx<'_>) {\n+    let kernel_arguments_ty = cx.type_named_struct(\"struct.__tgt_kernel_arguments\");\n+    let tptr = cx.type_ptr();\n+    let ti64 = cx.type_i64();\n+    let ti32 = cx.type_i32();\n+    let tarr = cx.type_array(ti32, 3);\n+\n+    // Taken from the LLVM APITypes.h declaration:\n+    //struct KernelArgsTy {\n+    //  uint32_t Version = 0; // Version of this struct for ABI compatibility.\n+    //  uint32_t NumArgs = 0; // Number of arguments in each input pointer.\n+    //  void **ArgBasePtrs =\n+    //      nullptr;                 // Base pointer of each argument (e.g. a struct).\n+    //  void **ArgPtrs = nullptr;    // Pointer to the argument data.\n+    //  int64_t *ArgSizes = nullptr; // Size of the argument data in bytes.\n+    //  int64_t *ArgTypes = nullptr; // Type of the data (e.g. to / from).\n+    //  void **ArgNames = nullptr;   // Name of the data for debugging, possibly null.\n+    //  void **ArgMappers = nullptr; // User-defined mappers, possibly null.\n+    //  uint64_t Tripcount =\n+    //      0; // Tripcount for the teams / distribute loop, 0 otherwise.\n+    //  struct {\n+    //    uint64_t NoWait : 1; // Was this kernel spawned with a `nowait` clause.\n+    //    uint64_t IsCUDA : 1; // Was this kernel spawned via CUDA.\n+    //    uint64_t Unused : 62;\n+    //  } Flags = {0, 0, 0};\n+    //  // The number of teams (for x,y,z dimension).\n+    //  uint32_t NumTeams[3] = {0, 0, 0};\n+    //  // The number of threads (for x,y,z dimension).\n+    //  uint32_t ThreadLimit[3] = {0, 0, 0};\n+    //  uint32_t DynCGroupMem = 0; // Amount of dynamic cgroup memory requested.\n+    //};\n+    let kernel_elements =\n+        vec![ti32, ti32, tptr, tptr, tptr, tptr, tptr, tptr, ti64, ti64, tarr, tarr, ti32];\n+\n+    cx.set_struct_body(kernel_arguments_ty, &kernel_elements, false);\n+    // For now we don't handle kernels, so for now we just add a global dummy\n+    // to make sure that the __tgt_offload_entry is defined and handled correctly.\n+    cx.declare_global(\"my_struct_global2\", kernel_arguments_ty);\n+}\n+\n+fn gen_tgt_data_mappers<'ll>(\n+    cx: &'ll SimpleCx<'_>,\n+) -> (&'ll llvm::Value, &'ll llvm::Value, &'ll llvm::Value, &'ll llvm::Type) {\n+    let tptr = cx.type_ptr();\n+    let ti64 = cx.type_i64();\n+    let ti32 = cx.type_i32();\n+\n+    let args = vec![tptr, ti64, ti32, tptr, tptr, tptr, tptr, tptr, tptr];\n+    let mapper_fn_ty = cx.type_func(&args, cx.type_void());\n+    let mapper_begin = \"__tgt_target_data_begin_mapper\";\n+    let mapper_update = \"__tgt_target_data_update_mapper\";\n+    let mapper_end = \"__tgt_target_data_end_mapper\";\n+    let begin_mapper_decl = declare_offload_fn(&cx, mapper_begin, mapper_fn_ty);\n+    let update_mapper_decl = declare_offload_fn(&cx, mapper_update, mapper_fn_ty);\n+    let end_mapper_decl = declare_offload_fn(&cx, mapper_end, mapper_fn_ty);\n+\n+    let nounwind = llvm::AttributeKind::NoUnwind.create_attr(cx.llcx);\n+    attributes::apply_to_llfn(begin_mapper_decl, Function, &[nounwind]);\n+    attributes::apply_to_llfn(update_mapper_decl, Function, &[nounwind]);\n+    attributes::apply_to_llfn(end_mapper_decl, Function, &[nounwind]);\n+\n+    (begin_mapper_decl, update_mapper_decl, end_mapper_decl, mapper_fn_ty)\n+}\n+\n+fn add_priv_unnamed_arr<'ll>(cx: &SimpleCx<'ll>, name: &str, vals: &[u64]) -> &'ll llvm::Value {\n+    let ti64 = cx.type_i64();\n+    let mut size_val = Vec::with_capacity(vals.len());\n+    for &val in vals {\n+        size_val.push(cx.get_const_i64(val));\n+    }\n+    let initializer = cx.const_array(ti64, &size_val);\n+    add_unnamed_global(cx, name, initializer, PrivateLinkage)\n+}\n+\n+pub(crate) fn add_unnamed_global<'ll>(\n+    cx: &SimpleCx<'ll>,\n+    name: &str,\n+    initializer: &'ll llvm::Value,\n+    l: Linkage,\n+) -> &'ll llvm::Value {\n+    let llglobal = add_global(cx, name, initializer, l);\n+    unsafe { llvm::LLVMSetUnnamedAddress(llglobal, llvm::UnnamedAddr::Global) };\n+    llglobal\n+}\n+\n+pub(crate) fn add_global<'ll>(\n+    cx: &SimpleCx<'ll>,\n+    name: &str,\n+    initializer: &'ll llvm::Value,\n+    l: Linkage,\n+) -> &'ll llvm::Value {\n+    let c_name = CString::new(name).unwrap();\n+    let llglobal: &'ll llvm::Value = llvm::add_global(cx.llmod, cx.val_ty(initializer), &c_name);\n+    llvm::set_global_constant(llglobal, true);\n+    llvm::set_linkage(llglobal, l);\n+    llvm::set_initializer(llglobal, initializer);\n+    llglobal\n+}\n+\n+fn gen_define_handling<'ll>(\n+    cx: &'ll SimpleCx<'_>,\n+    kernel: &'ll llvm::Value,\n+    offload_entry_ty: &'ll llvm::Type,\n+    num: i64,\n+) -> &'ll llvm::Value {\n+    let types = cx.func_params_types(cx.get_type_of_global(kernel));\n+    // It seems like non-pointer values are automatically mapped. So here, we focus on pointer (or\n+    // reference) types.\n+    let num_ptr_types = types\n+        .iter()\n+        .map(|&x| matches!(cx.type_kind(x), rustc_codegen_ssa::common::TypeKind::Pointer))\n+        .count();\n+\n+    // We do not know their size anymore at this level, so hardcode a placeholder.\n+    // A follow-up pr will track these from the frontend, where we still have Rust types.\n+    // Then, we will be able to figure out that e.g. `&[f32;256]` will result in 4*256 bytes.\n+    // I decided that 1024 bytes is a great placeholder value for now.\n+    add_priv_unnamed_arr(&cx, &format!(\".offload_sizes.{num}\"), &vec![1024; num_ptr_types]);\n+    // Here we figure out whether something needs to be copied to the gpu (=1), from the gpu (=2),\n+    // or both to and from the gpu (=3). Other values shouldn't affect us for now.\n+    // A non-mutable reference or pointer will be 1, an array that's not read, but fully overwritten\n+    // will be 2. For now, everything is 3, untill we have our frontend set up.\n+    let o_types =\n+        add_priv_unnamed_arr(&cx, &format!(\".offload_maptypes.{num}\"), &vec![3; num_ptr_types]);\n+    // Next: For each function, generate these three entries. A weak constant,\n+    // the llvm.rodata entry name, and  the omp_offloading_entries value\n+\n+    let name = format!(\".kernel_{num}.region_id\");\n+    let initializer = cx.get_const_i8(0);\n+    let region_id = add_unnamed_global(&cx, &name, initializer, WeakAnyLinkage);\n+\n+    let c_entry_name = CString::new(format!(\"kernel_{num}\")).unwrap();\n+    let c_val = c_entry_name.as_bytes_with_nul();\n+    let offload_entry_name = format!(\".offloading.entry_name.{num}\");\n+\n+    let initializer = crate::common::bytes_in_context(cx.llcx, c_val);\n+    let llglobal = add_unnamed_global(&cx, &offload_entry_name, initializer, InternalLinkage);\n+    llvm::set_alignment(llglobal, Align::ONE);\n+    let c_section_name = CString::new(\".llvm.rodata.offloading\").unwrap();\n+    llvm::set_section(llglobal, &c_section_name);\n+\n+    // Not actively used yet, for calling real kernels\n+    let name = format!(\".offloading.entry.kernel_{num}\");\n+    let ci64_0 = cx.get_const_i64(0);\n+    let ci16_1 = cx.get_const_i16(1);\n+    let elems: Vec<&llvm::Value> = vec![\n+        ci64_0,\n+        ci16_1,\n+        ci16_1,\n+        cx.get_const_i32(0),\n+        region_id,\n+        llglobal,\n+        ci64_0,\n+        ci64_0,\n+        cx.const_null(cx.type_ptr()),\n+    ];\n+\n+    let initializer = crate::common::named_struct(offload_entry_ty, &elems);\n+    let c_name = CString::new(name).unwrap();\n+    let llglobal = llvm::add_global(cx.llmod, offload_entry_ty, &c_name);\n+    llvm::set_global_constant(llglobal, true);\n+    llvm::set_linkage(llglobal, WeakAnyLinkage);\n+    llvm::set_initializer(llglobal, initializer);\n+    llvm::set_alignment(llglobal, Align::ONE);\n+    let c_section_name = CString::new(\".omp_offloading_entries\").unwrap();\n+    llvm::set_section(llglobal, &c_section_name);\n+    o_types\n+}\n+\n+fn declare_offload_fn<'ll>(\n+    cx: &'ll SimpleCx<'_>,\n+    name: &str,\n+    ty: &'ll llvm::Type,\n+) -> &'ll llvm::Value {\n+    crate::declare::declare_simple_fn(\n+        cx,\n+        name,\n+        llvm::CallConv::CCallConv,\n+        llvm::UnnamedAddr::No,\n+        llvm::Visibility::Default,\n+        ty,\n+    )\n+}\n+\n+// For each kernel *call*, we now use some of our previous declared globals to move data to and from\n+// the gpu. We don't have a proper frontend yet, so we assume that every call to a kernel function\n+// from main is intended to run on the GPU. For now, we only handle the data transfer part of it.\n+// If two consecutive kernels use the same memory, we still move it to the host and back to the gpu.\n+// Since in our frontend users (by default) don't have to specify data transfer, this is something\n+// we should optimize in the future! We also assume that everything should be copied back and forth,\n+// but sometimes we can directly zero-allocate on the device and only move back, or if something is\n+// immutable, we might only copy it to the device, but not back.\n+//\n+// Current steps:\n+// 0. Alloca some variables for the following steps\n+// 1. set insert point before kernel call.\n+// 2. generate all the GEPS and stores, to be used in 3)\n+// 3. generate __tgt_target_data_begin calls to move data to the GPU\n+//\n+// unchanged: keep kernel call. Later move the kernel to the GPU\n+//\n+// 4. set insert point after kernel call.\n+// 5. generate all the GEPS and stores, to be used in 6)\n+// 6. generate __tgt_target_data_end calls to move data from the GPU\n+fn gen_call_handling<'ll>(\n+    cx: &'ll SimpleCx<'_>,\n+    _kernels: &[&'ll llvm::Value],\n+    o_types: &[&'ll llvm::Value],\n+) {\n+    // %struct.__tgt_bin_desc = type { i32, ptr, ptr, ptr }\n+    let tptr = cx.type_ptr();\n+    let ti32 = cx.type_i32();\n+    let tgt_bin_desc_ty = vec![ti32, tptr, tptr, tptr];\n+    let tgt_bin_desc = cx.type_named_struct(\"struct.__tgt_bin_desc\");\n+    cx.set_struct_body(tgt_bin_desc, &tgt_bin_desc_ty, false);\n+\n+    gen_tgt_kernel_global(&cx);\n+    let (begin_mapper_decl, _, end_mapper_decl, fn_ty) = gen_tgt_data_mappers(&cx);\n+\n+    let main_fn = cx.get_function(\"main\");\n+    let Some(main_fn) = main_fn else { return };\n+    let kernel_name = \"kernel_1\";\n+    let call = unsafe {\n+        llvm::LLVMRustGetFunctionCall(main_fn, kernel_name.as_c_char_ptr(), kernel_name.len())\n+    };\n+    let Some(kernel_call) = call else {\n+        return;\n+    };\n+    let kernel_call_bb = unsafe { llvm::LLVMGetInstructionParent(kernel_call) };\n+    let called = unsafe { llvm::LLVMGetCalledValue(kernel_call).unwrap() };\n+    let mut builder = SBuilder::build(cx, kernel_call_bb);\n+\n+    let types = cx.func_params_types(cx.get_type_of_global(called));\n+    let num_args = types.len() as u64;\n+\n+    // Step 0)\n+    // %struct.__tgt_bin_desc = type { i32, ptr, ptr, ptr }\n+    // %6 = alloca %struct.__tgt_bin_desc, align 8\n+    unsafe { llvm::LLVMRustPositionBuilderPastAllocas(builder.llbuilder, main_fn) };\n+\n+    let tgt_bin_desc_alloca = builder.direct_alloca(tgt_bin_desc, Align::EIGHT, \"EmptyDesc\");\n+\n+    let ty = cx.type_array(cx.type_ptr(), num_args);\n+    // Baseptr are just the input pointer to the kernel, stored in a local alloca\n+    let a1 = builder.direct_alloca(ty, Align::EIGHT, \".offload_baseptrs\");\n+    // Ptrs are the result of a gep into the baseptr, at least for our trivial types.\n+    let a2 = builder.direct_alloca(ty, Align::EIGHT, \".offload_ptrs\");\n+    // These represent the sizes in bytes, e.g. the entry for `&[f64; 16]` will be 8*16.\n+    let ty2 = cx.type_array(cx.type_i64(), num_args);\n+    let a4 = builder.direct_alloca(ty2, Align::EIGHT, \".offload_sizes\");\n+    // Now we allocate once per function param, a copy to be passed to one of our maps.\n+    let mut vals = vec![];\n+    let mut geps = vec![];\n+    let i32_0 = cx.get_const_i32(0);\n+    for (index, in_ty) in types.iter().enumerate() {\n+        // get function arg, store it into the alloca, and read it.\n+        let p = llvm::get_param(called, index as u32);\n+        let name = llvm::get_value_name(p);\n+        let name = str::from_utf8(name).unwrap();\n+        let arg_name = format!(\"{name}.addr\");\n+        let alloca = builder.direct_alloca(in_ty, Align::EIGHT, &arg_name);\n+\n+        builder.store(p, alloca, Align::EIGHT);\n+        let val = builder.load(in_ty, alloca, Align::EIGHT);\n+        let gep = builder.inbounds_gep(cx.type_f32(), val, &[i32_0]);\n+        vals.push(val);\n+        geps.push(gep);\n+    }\n+\n+    // Step 1)\n+    unsafe { llvm::LLVMRustPositionBefore(builder.llbuilder, kernel_call) };\n+    builder.memset(\n+        tgt_bin_desc_alloca,\n+        cx.get_const_i8(0),\n+        cx.get_const_i64(32),\n+        Align::from_bytes(8).unwrap(),\n+    );\n+\n+    let mapper_fn_ty = cx.type_func(&[cx.type_ptr()], cx.type_void());\n+    let register_lib_decl = declare_offload_fn(&cx, \"__tgt_register_lib\", mapper_fn_ty);\n+    let unregister_lib_decl = declare_offload_fn(&cx, \"__tgt_unregister_lib\", mapper_fn_ty);\n+    let init_ty = cx.type_func(&[], cx.type_void());\n+    let init_rtls_decl = declare_offload_fn(cx, \"__tgt_init_all_rtls\", init_ty);\n+\n+    // call void @__tgt_register_lib(ptr noundef %6)\n+    builder.call(mapper_fn_ty, register_lib_decl, &[tgt_bin_desc_alloca], None);\n+    // call void @__tgt_init_all_rtls()\n+    builder.call(init_ty, init_rtls_decl, &[], None);\n+\n+    for i in 0..num_args {\n+        let idx = cx.get_const_i32(i);\n+        let gep1 = builder.inbounds_gep(ty, a1, &[i32_0, idx]);\n+        builder.store(vals[i as usize], gep1, Align::EIGHT);\n+        let gep2 = builder.inbounds_gep(ty, a2, &[i32_0, idx]);\n+        builder.store(geps[i as usize], gep2, Align::EIGHT);\n+        let gep3 = builder.inbounds_gep(ty2, a4, &[i32_0, idx]);\n+        // As mentioned above, we don't use Rust type informatino yet. So for now we will just\n+        // assume that we have 1024 bytes, 256 f32 values.\n+        // FIXME(offload): write an offload frontend and handle arbitrary types.\n+        builder.store(cx.get_const_i64(1024), gep3, Align::EIGHT);\n+    }\n+\n+    // Step 2)\n+    let gep1 = builder.inbounds_gep(ty, a1, &[i32_0, i32_0]);\n+    let gep2 = builder.inbounds_gep(ty, a2, &[i32_0, i32_0]);\n+    let gep3 = builder.inbounds_gep(ty2, a4, &[i32_0, i32_0]);\n+\n+    let nullptr = cx.const_null(cx.type_ptr());\n+    let o_type = o_types[0];\n+    let s_ident_t = generate_at_one(&cx);\n+    let args = vec![\n+        s_ident_t,\n+        cx.get_const_i64(u64::MAX),\n+        cx.get_const_i32(num_args),\n+        gep1,\n+        gep2,\n+        gep3,\n+        o_type,\n+        nullptr,\n+        nullptr,\n+    ];\n+    builder.call(fn_ty, begin_mapper_decl, &args, None);\n+\n+    // Step 3)\n+    // Here we will add code for the actual kernel launches in a follow-up PR.\n+    // FIXME(offload): launch kernels\n+\n+    // Step 4)\n+    unsafe { llvm::LLVMRustPositionAfter(builder.llbuilder, kernel_call) };\n+\n+    let gep1 = builder.inbounds_gep(ty, a1, &[i32_0, i32_0]);\n+    let gep2 = builder.inbounds_gep(ty, a2, &[i32_0, i32_0]);\n+    let gep3 = builder.inbounds_gep(ty2, a4, &[i32_0, i32_0]);\n+\n+    let nullptr = cx.const_null(cx.type_ptr());\n+    let o_type = o_types[0];\n+    let args = vec![\n+        s_ident_t,\n+        cx.get_const_i64(u64::MAX),\n+        cx.get_const_i32(num_args),\n+        gep1,\n+        gep2,\n+        gep3,\n+        o_type,\n+        nullptr,\n+        nullptr,\n+    ];\n+    builder.call(fn_ty, end_mapper_decl, &args, None);",
        "comment_created_at": "2025-07-10T20:41:38+00:00",
        "comment_author": "ZuseZ4",
        "comment_body": "I split it up into two functions, since I expect that the gep indexing might see changes when we get to more interesting arguments.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2110858777",
    "pr_number": 138907,
    "pr_file": "src/librustdoc/clean/types.rs",
    "created_at": "2025-05-28T04:11:55+00:00",
    "commented_code": "Some(item)\n     }\n \n-    let mut cfg = if doc_cfg_active || doc_auto_cfg_active {\n-        let mut doc_cfg = attrs\n-            .clone()\n-            .filter(|attr| attr.has_name(sym::doc))\n-            .flat_map(|attr| attr.meta_item_list().unwrap_or_default())\n-            .filter(|attr| attr.has_name(sym::cfg))\n-            .peekable();\n-        if doc_cfg.peek().is_some() && doc_cfg_active {\n-            let sess = tcx.sess;\n-            doc_cfg.fold(Cfg::True, |mut cfg, item| {\n-                if let Some(cfg_mi) =\n-                    item.meta_item().and_then(|item| rustc_expand::config::parse_cfg(item, sess))\n+    let mut new_show_attrs = FxHashMap::default();\n+    let mut new_hide_attrs = FxHashMap::default();\n+\n+    let mut doc_cfg = attrs\n+        .clone()\n+        .filter(|attr| attr.has_name(sym::doc))\n+        .flat_map(|attr| attr.meta_item_list().unwrap_or_default())\n+        .filter(|attr| attr.has_name(sym::cfg))\n+        .peekable();\n+    // If the item uses `doc(cfg(...))`, then we ignore the other `cfg(...)` attributes.\n+    if doc_cfg.peek().is_some() {\n+        let sess = tcx.sess;\n+        // We overwrite existing `cfg`.\n+        if !cfg_info.parent_is_doc_cfg {\n+            cfg_info.current_cfg = Cfg::True;\n+            cfg_info.parent_is_doc_cfg = true;\n+        }\n+        for attr in doc_cfg {\n+            if let Some(cfg_mi) =\n+                attr.meta_item().and_then(|attr| rustc_expand::config::parse_cfg(attr, sess))\n+            {\n+                // The result is unused here but we can gate unstable predicates\n+                rustc_attr_parsing::cfg_matches(\n+                    cfg_mi,\n+                    tcx.sess,\n+                    rustc_ast::CRATE_NODE_ID,\n+                    Some(tcx.features()),\n+                );\n+                match Cfg::parse(cfg_mi) {\n+                    Ok(new_cfg) => cfg_info.current_cfg &= new_cfg,\n+                    Err(e) => {\n+                        sess.dcx().span_err(e.span, e.msg);\n+                    }\n+                }\n+            }\n+        }\n+    } else {\n+        cfg_info.parent_is_doc_cfg = false;\n+    }\n+\n+    let mut changed_auto_active_status = None;\n+\n+    // First we get all `doc(auto_cfg)` attributes.\n+    for attr in attrs.clone() {\n+        if let Some(ident) = attr.ident()\n+            && ident.name == sym::doc\n+            && let Some(attrs) = attr.meta_item_list()\n+        {\n+            for attr in attrs.iter().filter(|attr| attr.has_name(sym::auto_cfg)) {\n+                let MetaItemInner::MetaItem(attr) = attr else {\n+                    continue;\n+                };\n+                match &attr.kind {\n+                    MetaItemKind::Word => {\n+                        if let Some(first_change) = changed_auto_active_status {\n+                            if !cfg_info.auto_cfg_active {\n+                                tcx.sess.dcx().struct_span_err(\n+                                    vec![first_change, attr.span],\n+                                    \"`auto_cfg` was disabled and enabled more than once on the same item\",\n+                                ).emit();\n+                                return None;\n+                            }\n+                        } else {\n+                            changed_auto_active_status = Some(attr.span);\n+                        }\n+                        cfg_info.auto_cfg_active = true;\n+                    }\n+                    MetaItemKind::NameValue(lit) => {\n+                        if let LitKind::Bool(value) = lit.kind {\n+                            if let Some(first_change) = changed_auto_active_status {\n+                                if cfg_info.auto_cfg_active != value {\n+                                    tcx.sess.dcx().struct_span_err(\n+                                        vec![first_change, attr.span],\n+                                        \"`auto_cfg` was disabled and enabled more than once on the same item\",\n+                                    ).emit();\n+                                    return None;\n+                                }\n+                            } else {\n+                                changed_auto_active_status = Some(attr.span);\n+                            }\n+                            cfg_info.auto_cfg_active = value;",
    "repo_full_name": "rust-lang/rust",
    "discussion_comments": [
      {
        "comment_id": "2110858777",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 138907,
        "pr_file": "src/librustdoc/clean/types.rs",
        "discussion_id": "2110858777",
        "commented_code": "@@ -1047,67 +1108,176 @@ pub(crate) fn extract_cfg_from_attrs<'a, I: Iterator<Item = &'a hir::Attribute>\n         Some(item)\n     }\n \n-    let mut cfg = if doc_cfg_active || doc_auto_cfg_active {\n-        let mut doc_cfg = attrs\n-            .clone()\n-            .filter(|attr| attr.has_name(sym::doc))\n-            .flat_map(|attr| attr.meta_item_list().unwrap_or_default())\n-            .filter(|attr| attr.has_name(sym::cfg))\n-            .peekable();\n-        if doc_cfg.peek().is_some() && doc_cfg_active {\n-            let sess = tcx.sess;\n-            doc_cfg.fold(Cfg::True, |mut cfg, item| {\n-                if let Some(cfg_mi) =\n-                    item.meta_item().and_then(|item| rustc_expand::config::parse_cfg(item, sess))\n+    let mut new_show_attrs = FxHashMap::default();\n+    let mut new_hide_attrs = FxHashMap::default();\n+\n+    let mut doc_cfg = attrs\n+        .clone()\n+        .filter(|attr| attr.has_name(sym::doc))\n+        .flat_map(|attr| attr.meta_item_list().unwrap_or_default())\n+        .filter(|attr| attr.has_name(sym::cfg))\n+        .peekable();\n+    // If the item uses `doc(cfg(...))`, then we ignore the other `cfg(...)` attributes.\n+    if doc_cfg.peek().is_some() {\n+        let sess = tcx.sess;\n+        // We overwrite existing `cfg`.\n+        if !cfg_info.parent_is_doc_cfg {\n+            cfg_info.current_cfg = Cfg::True;\n+            cfg_info.parent_is_doc_cfg = true;\n+        }\n+        for attr in doc_cfg {\n+            if let Some(cfg_mi) =\n+                attr.meta_item().and_then(|attr| rustc_expand::config::parse_cfg(attr, sess))\n+            {\n+                // The result is unused here but we can gate unstable predicates\n+                rustc_attr_parsing::cfg_matches(\n+                    cfg_mi,\n+                    tcx.sess,\n+                    rustc_ast::CRATE_NODE_ID,\n+                    Some(tcx.features()),\n+                );\n+                match Cfg::parse(cfg_mi) {\n+                    Ok(new_cfg) => cfg_info.current_cfg &= new_cfg,\n+                    Err(e) => {\n+                        sess.dcx().span_err(e.span, e.msg);\n+                    }\n+                }\n+            }\n+        }\n+    } else {\n+        cfg_info.parent_is_doc_cfg = false;\n+    }\n+\n+    let mut changed_auto_active_status = None;\n+\n+    // First we get all `doc(auto_cfg)` attributes.\n+    for attr in attrs.clone() {\n+        if let Some(ident) = attr.ident()\n+            && ident.name == sym::doc\n+            && let Some(attrs) = attr.meta_item_list()\n+        {\n+            for attr in attrs.iter().filter(|attr| attr.has_name(sym::auto_cfg)) {\n+                let MetaItemInner::MetaItem(attr) = attr else {\n+                    continue;\n+                };\n+                match &attr.kind {\n+                    MetaItemKind::Word => {\n+                        if let Some(first_change) = changed_auto_active_status {\n+                            if !cfg_info.auto_cfg_active {\n+                                tcx.sess.dcx().struct_span_err(\n+                                    vec![first_change, attr.span],\n+                                    \"`auto_cfg` was disabled and enabled more than once on the same item\",\n+                                ).emit();\n+                                return None;\n+                            }\n+                        } else {\n+                            changed_auto_active_status = Some(attr.span);\n+                        }\n+                        cfg_info.auto_cfg_active = true;\n+                    }\n+                    MetaItemKind::NameValue(lit) => {\n+                        if let LitKind::Bool(value) = lit.kind {\n+                            if let Some(first_change) = changed_auto_active_status {\n+                                if cfg_info.auto_cfg_active != value {\n+                                    tcx.sess.dcx().struct_span_err(\n+                                        vec![first_change, attr.span],\n+                                        \"`auto_cfg` was disabled and enabled more than once on the same item\",\n+                                    ).emit();\n+                                    return None;\n+                                }\n+                            } else {\n+                                changed_auto_active_status = Some(attr.span);\n+                            }\n+                            cfg_info.auto_cfg_active = value;",
        "comment_created_at": "2025-05-28T04:11:55+00:00",
        "comment_author": "camelid",
        "comment_body": "Could you extract this as a closure and then use it in all 3 match arms? Or find some other way to avoid having 3 copies of this code.",
        "pr_file_module": null
      },
      {
        "comment_id": "2114303012",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 138907,
        "pr_file": "src/librustdoc/clean/types.rs",
        "discussion_id": "2110858777",
        "commented_code": "@@ -1047,67 +1108,176 @@ pub(crate) fn extract_cfg_from_attrs<'a, I: Iterator<Item = &'a hir::Attribute>\n         Some(item)\n     }\n \n-    let mut cfg = if doc_cfg_active || doc_auto_cfg_active {\n-        let mut doc_cfg = attrs\n-            .clone()\n-            .filter(|attr| attr.has_name(sym::doc))\n-            .flat_map(|attr| attr.meta_item_list().unwrap_or_default())\n-            .filter(|attr| attr.has_name(sym::cfg))\n-            .peekable();\n-        if doc_cfg.peek().is_some() && doc_cfg_active {\n-            let sess = tcx.sess;\n-            doc_cfg.fold(Cfg::True, |mut cfg, item| {\n-                if let Some(cfg_mi) =\n-                    item.meta_item().and_then(|item| rustc_expand::config::parse_cfg(item, sess))\n+    let mut new_show_attrs = FxHashMap::default();\n+    let mut new_hide_attrs = FxHashMap::default();\n+\n+    let mut doc_cfg = attrs\n+        .clone()\n+        .filter(|attr| attr.has_name(sym::doc))\n+        .flat_map(|attr| attr.meta_item_list().unwrap_or_default())\n+        .filter(|attr| attr.has_name(sym::cfg))\n+        .peekable();\n+    // If the item uses `doc(cfg(...))`, then we ignore the other `cfg(...)` attributes.\n+    if doc_cfg.peek().is_some() {\n+        let sess = tcx.sess;\n+        // We overwrite existing `cfg`.\n+        if !cfg_info.parent_is_doc_cfg {\n+            cfg_info.current_cfg = Cfg::True;\n+            cfg_info.parent_is_doc_cfg = true;\n+        }\n+        for attr in doc_cfg {\n+            if let Some(cfg_mi) =\n+                attr.meta_item().and_then(|attr| rustc_expand::config::parse_cfg(attr, sess))\n+            {\n+                // The result is unused here but we can gate unstable predicates\n+                rustc_attr_parsing::cfg_matches(\n+                    cfg_mi,\n+                    tcx.sess,\n+                    rustc_ast::CRATE_NODE_ID,\n+                    Some(tcx.features()),\n+                );\n+                match Cfg::parse(cfg_mi) {\n+                    Ok(new_cfg) => cfg_info.current_cfg &= new_cfg,\n+                    Err(e) => {\n+                        sess.dcx().span_err(e.span, e.msg);\n+                    }\n+                }\n+            }\n+        }\n+    } else {\n+        cfg_info.parent_is_doc_cfg = false;\n+    }\n+\n+    let mut changed_auto_active_status = None;\n+\n+    // First we get all `doc(auto_cfg)` attributes.\n+    for attr in attrs.clone() {\n+        if let Some(ident) = attr.ident()\n+            && ident.name == sym::doc\n+            && let Some(attrs) = attr.meta_item_list()\n+        {\n+            for attr in attrs.iter().filter(|attr| attr.has_name(sym::auto_cfg)) {\n+                let MetaItemInner::MetaItem(attr) = attr else {\n+                    continue;\n+                };\n+                match &attr.kind {\n+                    MetaItemKind::Word => {\n+                        if let Some(first_change) = changed_auto_active_status {\n+                            if !cfg_info.auto_cfg_active {\n+                                tcx.sess.dcx().struct_span_err(\n+                                    vec![first_change, attr.span],\n+                                    \"`auto_cfg` was disabled and enabled more than once on the same item\",\n+                                ).emit();\n+                                return None;\n+                            }\n+                        } else {\n+                            changed_auto_active_status = Some(attr.span);\n+                        }\n+                        cfg_info.auto_cfg_active = true;\n+                    }\n+                    MetaItemKind::NameValue(lit) => {\n+                        if let LitKind::Bool(value) = lit.kind {\n+                            if let Some(first_change) = changed_auto_active_status {\n+                                if cfg_info.auto_cfg_active != value {\n+                                    tcx.sess.dcx().struct_span_err(\n+                                        vec![first_change, attr.span],\n+                                        \"`auto_cfg` was disabled and enabled more than once on the same item\",\n+                                    ).emit();\n+                                    return None;\n+                                }\n+                            } else {\n+                                changed_auto_active_status = Some(attr.span);\n+                            }\n+                            cfg_info.auto_cfg_active = value;",
        "comment_created_at": "2025-05-29T16:20:21+00:00",
        "comment_author": "GuillaumeGomez",
        "comment_body": "Nice catch!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2136907415",
    "pr_number": 135846,
    "pr_file": "compiler/rustc_resolve/src/diagnostics.rs",
    "created_at": "2025-06-10T04:54:59+00:00",
    "commented_code": "let mut err =\n             self.dcx().create_err(errors::IsPrivate { span: ident.span, ident_descr, ident });\n \n+        if let Some(expr) = source",
    "repo_full_name": "rust-lang/rust",
    "discussion_comments": [
      {
        "comment_id": "2136907415",
        "repo_full_name": "rust-lang/rust",
        "pr_number": 135846,
        "pr_file": "compiler/rustc_resolve/src/diagnostics.rs",
        "discussion_id": "2136907415",
        "commented_code": "@@ -1767,6 +1774,55 @@ impl<'ra, 'tcx> Resolver<'ra, 'tcx> {\n         let mut err =\n             self.dcx().create_err(errors::IsPrivate { span: ident.span, ident_descr, ident });\n \n+        if let Some(expr) = source",
        "comment_created_at": "2025-06-10T04:54:59+00:00",
        "comment_author": "BoxyUwU",
        "comment_body": "Can you move this whole bit of logic out to a separate function? this function's already massive without another pile of logic for another special case in our diagnostics. At the very least there should be a comment saying what case we're handling here so its obvious you can skip over it if you don't care about this.",
        "pr_file_module": null
      }
    ]
  }
]