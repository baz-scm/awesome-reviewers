[
  {
    "discussion_id": "2342269845",
    "pr_number": 730,
    "pr_file": "src/strands/models/litellm.py",
    "created_at": "2025-09-11T20:29:46+00:00",
    "commented_code": "return super().format_request_message_content(content)\n \n+    def _format_request_message_contents(self, role: str, content: ContentBlock) -> list[dict[str, Any]]:\n+        \"\"\"Format LiteLLM compatible message contents.\n+\n+        LiteLLM expects content to be a string for simple text messages, not a list of content blocks.\n+        This method flattens the content structure to be compatible with LiteLLM providers like Cerebras and Groq.",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2342269845",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 730,
        "pr_file": "src/strands/models/litellm.py",
        "discussion_id": "2342269845",
        "commented_code": "@@ -103,6 +103,127 @@ def format_request_message_content(cls, content: ContentBlock) -> dict[str, Any]\n \n         return super().format_request_message_content(content)\n \n+    def _format_request_message_contents(self, role: str, content: ContentBlock) -> list[dict[str, Any]]:\n+        \"\"\"Format LiteLLM compatible message contents.\n+\n+        LiteLLM expects content to be a string for simple text messages, not a list of content blocks.\n+        This method flattens the content structure to be compatible with LiteLLM providers like Cerebras and Groq.",
        "comment_created_at": "2025-09-11T20:29:46+00:00",
        "comment_author": "dbschmigelski",
        "comment_body": "Hi thanks for this but can you help explain where the abstraction is broken.\n\nAre you indicating that LiteLLM has a broken abstraction for Cerebras and Groq? Or do you believe that Strands has always improperly implemented the spec but this was just exposed when Ceregras and Groq were added?",
        "pr_file_module": null
      },
      {
        "comment_id": "2364915768",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 730,
        "pr_file": "src/strands/models/litellm.py",
        "discussion_id": "2342269845",
        "commented_code": "@@ -103,6 +103,127 @@ def format_request_message_content(cls, content: ContentBlock) -> dict[str, Any]\n \n         return super().format_request_message_content(content)\n \n+    def _format_request_message_contents(self, role: str, content: ContentBlock) -> list[dict[str, Any]]:\n+        \"\"\"Format LiteLLM compatible message contents.\n+\n+        LiteLLM expects content to be a string for simple text messages, not a list of content blocks.\n+        This method flattens the content structure to be compatible with LiteLLM providers like Cerebras and Groq.",
        "comment_created_at": "2025-09-20T00:43:27+00:00",
        "comment_author": "dbschmigelski",
        "comment_body": "bump @aditya270520, thanks",
        "pr_file_module": null
      },
      {
        "comment_id": "2365380044",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 730,
        "pr_file": "src/strands/models/litellm.py",
        "discussion_id": "2342269845",
        "commented_code": "@@ -103,6 +103,127 @@ def format_request_message_content(cls, content: ContentBlock) -> dict[str, Any]\n \n         return super().format_request_message_content(content)\n \n+    def _format_request_message_contents(self, role: str, content: ContentBlock) -> list[dict[str, Any]]:\n+        \"\"\"Format LiteLLM compatible message contents.\n+\n+        LiteLLM expects content to be a string for simple text messages, not a list of content blocks.\n+        This method flattens the content structure to be compatible with LiteLLM providers like Cerebras and Groq.",
        "comment_created_at": "2025-09-20T07:02:10+00:00",
        "comment_author": "aditya270520",
        "comment_body": "LiteLLM has a broken abstraction for Cerebras and Groq providers. Strands has always properly implemented the OpenAI specification, but this inconsistency was exposed when Cerebras and Groq were added to LiteLLM's supported providers.\r\nThe abstraction should be fixed in LiteLLM itself to ensure all providers receive content in the same format, but until that happens, Strands needs this workaround to maintain compatibility.\r\nThis is a common issue when trying to create unified interfaces across multiple providers with different underlying APIs - the abstraction layer (LiteLLM) needs to handle the normalization, but it's not doing so consistently.",
        "pr_file_module": null
      },
      {
        "comment_id": "2365424402",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 730,
        "pr_file": "src/strands/models/litellm.py",
        "discussion_id": "2342269845",
        "commented_code": "@@ -103,6 +103,127 @@ def format_request_message_content(cls, content: ContentBlock) -> dict[str, Any]\n \n         return super().format_request_message_content(content)\n \n+    def _format_request_message_contents(self, role: str, content: ContentBlock) -> list[dict[str, Any]]:\n+        \"\"\"Format LiteLLM compatible message contents.\n+\n+        LiteLLM expects content to be a string for simple text messages, not a list of content blocks.\n+        This method flattens the content structure to be compatible with LiteLLM providers like Cerebras and Groq.",
        "comment_created_at": "2025-09-20T08:04:11+00:00",
        "comment_author": "aditya270520",
        "comment_body": "https://github.com/strands-agents/sdk-python/issues/729#issuecomment-3314723770",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2430008808",
    "pr_number": 1030,
    "pr_file": "tests_integ/models/providers.py",
    "created_at": "2025-10-14T17:56:42+00:00",
    "commented_code": "id=\"gemini\",\n     environment_variable=\"GOOGLE_API_KEY\",\n     factory=lambda: GeminiModel(\n-        api_key=os.getenv(\"GOOGLE_API_KEY\"),\n+        client_args={\"api_key\": os.getenv(\"GOOGLE_API_KEY\")},",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2430008808",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 1030,
        "pr_file": "tests_integ/models/providers.py",
        "discussion_id": "2430008808",
        "commented_code": "@@ -131,7 +131,7 @@ def __init__(self):\n     id=\"gemini\",\n     environment_variable=\"GOOGLE_API_KEY\",\n     factory=lambda: GeminiModel(\n-        api_key=os.getenv(\"GOOGLE_API_KEY\"),\n+        client_args={\"api_key\": os.getenv(\"GOOGLE_API_KEY\")},",
        "comment_created_at": "2025-10-14T17:56:42+00:00",
        "comment_author": "pgrayy",
        "comment_body": "The GeminiModel provider takes `client_args`, not `api_key` directly. This misconfiguration resulted in a warning. The model was still useable though because Gemini found the API key from the exported GOOGLE_API_KEY environment variable.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2410679825",
    "pr_number": 994,
    "pr_file": "src/strands/models/litellm.py",
    "created_at": "2025-10-07T13:39:49+00:00",
    "commented_code": "T = TypeVar(\"T\", bound=BaseModel)\n \n+LITELLM_CONTEXT_WINDOW_OVERFLOW_MESSAGES = [\n+    \"Context Window Error\",\n+    \"Context Window Exceeded\",\n+    \"ContextWindowExceeded\",\n+    \"Context window exceeded\",\n+    \"Input is too long\",\n+    \"ContextWindowExceededError\",\n+]\n+",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2410679825",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 994,
        "pr_file": "src/strands/models/litellm.py",
        "discussion_id": "2410679825",
        "commented_code": "@@ -22,6 +23,15 @@\n \n T = TypeVar(\"T\", bound=BaseModel)\n \n+LITELLM_CONTEXT_WINDOW_OVERFLOW_MESSAGES = [\n+    \"Context Window Error\",\n+    \"Context Window Exceeded\",\n+    \"ContextWindowExceeded\",\n+    \"Context window exceeded\",\n+    \"Input is too long\",\n+    \"ContextWindowExceededError\",\n+]\n+",
        "comment_created_at": "2025-10-07T13:39:49+00:00",
        "comment_author": "Unshure",
        "comment_body": "Can you add a link for where you are getting these exception messages?",
        "pr_file_module": null
      },
      {
        "comment_id": "2411124107",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 994,
        "pr_file": "src/strands/models/litellm.py",
        "discussion_id": "2410679825",
        "commented_code": "@@ -22,6 +23,15 @@\n \n T = TypeVar(\"T\", bound=BaseModel)\n \n+LITELLM_CONTEXT_WINDOW_OVERFLOW_MESSAGES = [\n+    \"Context Window Error\",\n+    \"Context Window Exceeded\",\n+    \"ContextWindowExceeded\",\n+    \"Context window exceeded\",\n+    \"Input is too long\",\n+    \"ContextWindowExceededError\",\n+]\n+",
        "comment_created_at": "2025-10-07T15:57:40+00:00",
        "comment_author": "Ratish1",
        "comment_body": "The ContextWindowExceededError is a class defined in litellm's own codebase, and I've added a comment with a link to that file. The other strings in the list, like \"Input is too long\", are not defined in one place. They are common error substrings that litellm passes through from the models it proxies (e.g., Bedrock, OpenAI). The original bug report raised by the user showed an example of this. This fallback check ensures we catch those cases as well, making the fix better.\r\nLmk if you'd like me to adjust the list.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2307392156",
    "pr_number": 725,
    "pr_file": "src/strands/models/gemini.py",
    "created_at": "2025-08-28T13:17:39+00:00",
    "commented_code": "+\"\"\"Google Gemini model provider.\n+\n+- Docs: https://ai.google.dev/api\n+\"\"\"\n+\n+import base64\n+import json\n+import logging\n+import mimetypes\n+import os\n+import time\n+from typing import Any, AsyncGenerator, Optional, Type, TypedDict, TypeVar, Union, cast\n+\n+from google import genai\n+from google.genai import types\n+from pydantic import BaseModel\n+from typing_extensions import Unpack, override\n+\n+from ..types.content import ContentBlock, Messages\n+from ..types.exceptions import ModelThrottledException\n+from ..types.streaming import StreamEvent\n+from ..types.tools import ToolResult, ToolSpec, ToolUse\n+from .model import Model\n+\n+logger = logging.getLogger(__name__)\n+\n+T = TypeVar(\"T\", bound=BaseModel)\n+\n+\n+class GeminiModel(Model):\n+    \"\"\"Google Gemini model provider implementation.\"\"\"\n+\n+    SAFETY_MESSAGES = {\"safety\", \"harmful\", \"content policy\", \"blocked due to safety\"}\n+\n+    QUOTA_MESSAGES = {\"quota\", \"limit\", \"rate limit\", \"exceeded\"}\n+\n+    class GeminiConfig(TypedDict, total=False):\n+        \"\"\"Configuration options for Gemini models.\"\"\"\n+\n+        model_id: str\n+        params: Optional[dict[str, Any]]\n+\n+    def __init__(\n+        self,\n+        *,\n+        api_key: Optional[str] = None,\n+        client_args: Optional[dict[str, Any]] = None,\n+        **model_config: Unpack[GeminiConfig],\n+    ) -> None:\n+        \"\"\"Initialize provider instance.\n+\n+        Args:\n+            api_key: Google AI API key. If not provided, will use GOOGLE_API_KEY env var.\n+            client_args: Additional arguments for the Gemini client configuration.\n+            **model_config: Configuration options for the Gemini model.\n+        \"\"\"\n+        self.config = GeminiModel.GeminiConfig(**model_config)\n+\n+        logger.debug(\"config=<%s> | initializing\", self.config)\n+\n+        client_config = {\"api_key\": api_key or os.environ.get(\"GOOGLE_API_KEY\"), **(client_args or {})}\n+\n+        self.client = genai.Client(**client_config)\n+\n+    @override\n+    def update_config(self, **model_config: Unpack[GeminiConfig]) -> None:  # type: ignore[override]\n+        \"\"\"Update the Gemini model configuration with the provided arguments.\n+\n+        Args:\n+            **model_config: Configuration overrides.\n+        \"\"\"\n+        self.config.update(model_config)\n+\n+    @override\n+    def get_config(self) -> GeminiConfig:\n+        \"\"\"Get the Gemini model configuration.\n+\n+        Returns:\n+            The Gemini model configuration.\n+        \"\"\"\n+        return self.config\n+\n+    def _format_inline_data_part(self, data: dict[str, Any], default_mime: str) -> dict[str, Any]:\n+        \"\"\"Formats an inline data part (image or document).\"\"\"\n+        file_format = data[\"format\"]\n+        source_bytes = data[\"source\"][\"bytes\"]\n+        mime_type = mimetypes.types_map.get(f\".{file_format}\", default_mime)\n+\n+        return {\"inlineData\": {\"mimeType\": mime_type, \"data\": base64.b64encode(source_bytes).decode(\"utf-8\")}}\n+\n+    def _format_request_message_content(self, content: ContentBlock) -> dict[str, Any]:\n+        \"\"\"Format a Gemini content block.\n+\n+        Args:\n+            content: Message content.\n+\n+        Returns:\n+            Gemini formatted content block.\n+\n+        Raises:\n+            TypeError: If the content block type cannot be converted to a Gemini-compatible format.\n+        \"\"\"\n+        if \"text\" in content:\n+            return {\"text\": content[\"text\"]}\n+\n+        if \"image\" in content:\n+            return self._format_inline_data_part(cast(dict[str, Any], content[\"image\"]), \"image/png\")\n+\n+        if \"document\" in content:\n+            return self._format_inline_data_part(cast(dict[str, Any], content[\"document\"]), \"application/octet-stream\")\n+\n+        raise TypeError(f\"content_type=<{next(iter(content))}> | unsupported type\")\n+\n+    def _format_function_call(self, tool_use: ToolUse) -> dict[str, Any]:\n+        \"\"\"Format a Gemini function call.\n+\n+        Args:\n+            tool_use: Tool use requested by the model.\n+\n+        Returns:\n+            Gemini formatted function call.\n+        \"\"\"\n+        return {\"functionCall\": {\"name\": tool_use[\"name\"], \"args\": tool_use[\"input\"]}}\n+\n+    def _format_function_response(self, tool_result: ToolResult) -> dict[str, Any]:\n+        \"\"\"Format a Gemini function response.\n+\n+        Args:\n+            tool_result: Tool result from execution.\n+\n+        Returns:\n+            Gemini formatted function response.\n+        \"\"\"\n+        response_parts = []\n+        for content in tool_result[\"content\"]:\n+            if \"json\" in content:\n+                response_parts.append(json.dumps(content[\"json\"]))\n+            elif \"text\" in content:\n+                response_parts.append(content[\"text\"])\n+\n+        return {\n+            \"functionResponse\": {\"name\": tool_result[\"toolUseId\"], \"response\": {\"content\": \"\n\".join(response_parts)}}\n+        }\n+\n+    def _format_request_messages(self, messages: Messages) -> list[dict[str, Any]]:\n+        \"\"\"Format messages for Gemini API.\n+\n+        Args:\n+            messages: List of message objects to be processed by the model.\n+\n+        Returns:\n+            Gemini formatted messages array.\n+        \"\"\"\n+        formatted_messages = []\n+\n+        for message in messages:\n+            role = \"user\" if message[\"role\"] == \"user\" else \"model\"\n+\n+            text_parts = []\n+            media_parts = []\n+            function_calls = []\n+            function_responses = []\n+\n+            for content in message[\"content\"]:\n+                if \"text\" in content:\n+                    text_parts.append(content[\"text\"])\n+                elif \"image\" in content or \"document\" in content:\n+                    media_parts.append(self._format_request_message_content(content))\n+                elif \"toolUse\" in content:\n+                    function_calls.append(self._format_function_call(content[\"toolUse\"]))\n+                elif \"toolResult\" in content:\n+                    function_responses.append(self._format_function_response(content[\"toolResult\"]))\n+\n+            parts = []\n+\n+            if text_parts:\n+                parts.append({\"text\": \"\n\n\".join(text_parts)})\n+\n+            if media_parts:\n+                parts.extend(media_parts)\n+\n+            parts.extend(function_calls)\n+            parts.extend(function_responses)\n+\n+            if parts:\n+                formatted_messages.append({\"role\": role, \"parts\": parts})\n+\n+        return formatted_messages\n+\n+    async def _process_chunk(\n+        self, chunk: Any, output_text_buffer: list[str], tool_calls: dict[str, str]\n+    ) -> AsyncGenerator[Union[StreamEvent, bool], None]:\n+        \"\"\"Process a single chunk from the streaming response.\"\"\"\n+        has_function_call = False\n+\n+        if hasattr(chunk, \"candidates\") and chunk.candidates:\n+            for candidate in chunk.candidates:\n+                if not hasattr(candidate, \"content\") or not candidate.content:\n+                    continue\n+\n+                for part in candidate.content.parts:\n+                    if part.text:\n+                        output_text_buffer.append(part.text)\n+                        yield self.format_event(\"content_block_delta\", part.text)\n+\n+                    # Handle function calls\n+                    elif hasattr(part, \"function_call\") and part.function_call:\n+                        function_call = part.function_call\n+                        has_function_call = True\n+\n+                        if function_call.name not in tool_calls:\n+                            yield self.format_event(\"content_block_stop\")\n+\n+                            tool_id = f\"tool_{len(tool_calls) + 1}\"\n+                            tool_calls[function_call.name] = tool_id\n+\n+                            yield self.format_event(\n+                                \"content_block_start\", {\"function_call\": function_call, \"tool_id\": tool_id}\n+                            )\n+\n+                        if hasattr(function_call, \"args\") and function_call.args:\n+                            args = self._extract_function_args(function_call)\n+                            yield self.format_event(\n+                                \"content_block_delta\", {\"function_call\": function_call, \"args\": args}\n+                            )\n+\n+        elif hasattr(chunk, \"text\") and chunk.text:\n+            output_text_buffer.append(chunk.text)\n+            yield self.format_event(\"content_block_delta\", chunk.text)\n+\n+        yield has_function_call\n+\n+    def _extract_function_args(self, function_call: Any) -> dict[str, Any]:\n+        \"\"\"Extract function arguments from various formats.\"\"\"\n+        if not hasattr(function_call, \"args\"):\n+            return {}\n+\n+        args = function_call.args\n+\n+        # Handle Struct type (protobuf)\n+        if hasattr(args, \"fields\"):\n+            return self._struct_to_dict(args)\n+\n+        # Handle JSON string\n+        if isinstance(args, str):\n+            try:\n+                parsed = json.loads(args)\n+                if isinstance(parsed, dict):\n+                    return parsed\n+                return {\"value\": args}\n+            except json.JSONDecodeError:\n+                return {\"value\": args}\n+\n+        if isinstance(args, dict):\n+            return dict(args)\n+\n+        return {}\n+\n+    def _struct_to_dict(self, struct_value: Any) -> dict[str, Any]:\n+        \"\"\"Convert protobuf Struct to dict.\"\"\"\n+        result = {}\n+        for key, value in struct_value.fields.items():\n+            if hasattr(value, \"string_value\"):\n+                result[key] = value.string_value\n+            elif hasattr(value, \"number_value\"):\n+                result[key] = value.number_value\n+            elif hasattr(value, \"bool_value\"):\n+                result[key] = value.bool_value\n+            elif hasattr(value, \"list_value\"):\n+                result[key] = [self._value_to_python(v) for v in value.list_value.values]\n+            elif hasattr(value, \"struct_value\"):\n+                result[key] = self._struct_to_dict(value.struct_value)\n+            else:\n+                result[key] = str(value)\n+        return result\n+\n+    def _value_to_python(self, value: Any) -> Any:\n+        \"\"\"Convert protobuf Value to Python type.\"\"\"\n+        if hasattr(value, \"string_value\"):\n+            return value.string_value\n+        elif hasattr(value, \"number_value\"):\n+            return value.number_value\n+        elif hasattr(value, \"bool_value\"):\n+            return value.bool_value\n+        elif hasattr(value, \"struct_value\"):\n+            return self._struct_to_dict(value.struct_value)\n+        else:\n+            return str(value)\n+\n+    async def _count_tokens_safely(self, model_id: str, contents: list[dict[str, Any]]) -> int:\n+        \"\"\"Safely count tokens with fallback to 0 on error.\n+\n+        Args:\n+            model_id: The Gemini model ID\n+            contents: The content to count tokens for\n+\n+        Returns:\n+            Token count, or 0 if counting fails\n+        \"\"\"\n+        try:\n+            token_count = await self.client.aio.models.count_tokens(model=model_id, contents=contents)\n+            if hasattr(token_count, \"total_tokens\"):\n+                return int(token_count.total_tokens or 0)\n+            return 0\n+        except Exception as e:\n+            logger.debug(\"Could not count tokens: %s\", str(e))\n+            return 0\n+\n+    def _format_tools(self, tool_specs: Optional[list[ToolSpec]]) -> Optional[list[dict[str, Any]]]:\n+        \"\"\"Format tool specifications for Gemini.\n+\n+        Args:\n+            tool_specs: List of tool specifications.\n+\n+        Returns:\n+            Gemini formatted tools array.\n+        \"\"\"\n+        if not tool_specs:\n+            return None\n+\n+        tools = []\n+        for tool_spec in tool_specs:\n+            tools.append(\n+                {\n+                    \"function_declarations\": [\n+                        {\n+                            \"name\": tool_spec[\"name\"],\n+                            \"description\": tool_spec[\"description\"],\n+                            \"parameters\": tool_spec[\"inputSchema\"][\"json\"],\n+                        }\n+                    ]\n+                }\n+            )\n+\n+        return tools\n+\n+    def format_request(\n+        self,\n+        messages: Messages,\n+        tool_specs: Optional[list[ToolSpec]] = None,\n+        system_prompt: Optional[str] = None,\n+        config: Optional[dict[str, Any]] = None,\n+    ) -> dict[str, Any]:\n+        \"\"\"Format a Gemini streaming request.\n+\n+        Args:\n+            messages: List of message objects to be processed by the model.\n+            tool_specs: List of tool specifications to make available to the model.\n+            system_prompt: System prompt to provide context to the model.\n+            config: Additional configuration options including response_schema for structured output.\n+\n+        Returns:\n+            A Gemini streaming request.\n+        \"\"\"\n+        generation_config: dict[str, Any] = {}\n+\n+        params = self.config.get(\"params\")\n+        if params:\n+            generation_config.update(params)\n+\n+        if config:\n+            if \"response_schema\" in config:\n+                generation_config[\"response_schema\"] = config[\"response_schema\"]\n+                generation_config[\"response_mime_type\"] = config.get(\"response_mime_type\", \"application/json\")\n+\n+            config_params = config.get(\"params\")\n+            if config_params:\n+                generation_config.update(config_params)\n+\n+        request = {\n+            \"contents\": self._format_request_messages(messages),\n+            \"generation_config\": generation_config,\n+            \"stream\": True,\n+        }\n+\n+        if system_prompt:\n+            request[\"system_instruction\"] = {\"parts\": [{\"text\": system_prompt}]}\n+\n+        tools = self._format_tools(tool_specs)\n+        if tools:\n+            request[\"tools\"] = tools\n+\n+        return request\n+\n+    def format_event(self, event_type: str, data: Any = None) -> StreamEvent:\n+        \"\"\"Format a Gemini event into a standardized message chunk.\n+\n+        Args:\n+            event_type: Type of event to format\n+            data: Data associated with the event\n+\n+        Returns:\n+            The formatted event\n+\n+        Raises:\n+            RuntimeError: If event_type is not recognized\n+        \"\"\"\n+        match event_type:\n+            case \"message_start\":\n+                return {\"messageStart\": {\"role\": \"assistant\"}}\n+\n+            case \"content_block_start\":\n+                if data and \"function_call\" in data:\n+                    function_call = data[\"function_call\"]\n+                    return {\n+                        \"contentBlockStart\": {\n+                            \"start\": {\"toolUse\": {\"name\": function_call.name, \"toolUseId\": data[\"tool_id\"]}}\n+                        }\n+                    }\n+                return {\"contentBlockStart\": {\"start\": {}}}\n+\n+            case \"content_block_delta\":\n+                if data and \"function_call\" in data:\n+                    args = data.get(\"args\", {})\n+                    return {\"contentBlockDelta\": {\"delta\": {\"toolUse\": {\"input\": args}}}}",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2307392156",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 725,
        "pr_file": "src/strands/models/gemini.py",
        "discussion_id": "2307392156",
        "commented_code": "@@ -0,0 +1,660 @@\n+\"\"\"Google Gemini model provider.\n+\n+- Docs: https://ai.google.dev/api\n+\"\"\"\n+\n+import base64\n+import json\n+import logging\n+import mimetypes\n+import os\n+import time\n+from typing import Any, AsyncGenerator, Optional, Type, TypedDict, TypeVar, Union, cast\n+\n+from google import genai\n+from google.genai import types\n+from pydantic import BaseModel\n+from typing_extensions import Unpack, override\n+\n+from ..types.content import ContentBlock, Messages\n+from ..types.exceptions import ModelThrottledException\n+from ..types.streaming import StreamEvent\n+from ..types.tools import ToolResult, ToolSpec, ToolUse\n+from .model import Model\n+\n+logger = logging.getLogger(__name__)\n+\n+T = TypeVar(\"T\", bound=BaseModel)\n+\n+\n+class GeminiModel(Model):\n+    \"\"\"Google Gemini model provider implementation.\"\"\"\n+\n+    SAFETY_MESSAGES = {\"safety\", \"harmful\", \"content policy\", \"blocked due to safety\"}\n+\n+    QUOTA_MESSAGES = {\"quota\", \"limit\", \"rate limit\", \"exceeded\"}\n+\n+    class GeminiConfig(TypedDict, total=False):\n+        \"\"\"Configuration options for Gemini models.\"\"\"\n+\n+        model_id: str\n+        params: Optional[dict[str, Any]]\n+\n+    def __init__(\n+        self,\n+        *,\n+        api_key: Optional[str] = None,\n+        client_args: Optional[dict[str, Any]] = None,\n+        **model_config: Unpack[GeminiConfig],\n+    ) -> None:\n+        \"\"\"Initialize provider instance.\n+\n+        Args:\n+            api_key: Google AI API key. If not provided, will use GOOGLE_API_KEY env var.\n+            client_args: Additional arguments for the Gemini client configuration.\n+            **model_config: Configuration options for the Gemini model.\n+        \"\"\"\n+        self.config = GeminiModel.GeminiConfig(**model_config)\n+\n+        logger.debug(\"config=<%s> | initializing\", self.config)\n+\n+        client_config = {\"api_key\": api_key or os.environ.get(\"GOOGLE_API_KEY\"), **(client_args or {})}\n+\n+        self.client = genai.Client(**client_config)\n+\n+    @override\n+    def update_config(self, **model_config: Unpack[GeminiConfig]) -> None:  # type: ignore[override]\n+        \"\"\"Update the Gemini model configuration with the provided arguments.\n+\n+        Args:\n+            **model_config: Configuration overrides.\n+        \"\"\"\n+        self.config.update(model_config)\n+\n+    @override\n+    def get_config(self) -> GeminiConfig:\n+        \"\"\"Get the Gemini model configuration.\n+\n+        Returns:\n+            The Gemini model configuration.\n+        \"\"\"\n+        return self.config\n+\n+    def _format_inline_data_part(self, data: dict[str, Any], default_mime: str) -> dict[str, Any]:\n+        \"\"\"Formats an inline data part (image or document).\"\"\"\n+        file_format = data[\"format\"]\n+        source_bytes = data[\"source\"][\"bytes\"]\n+        mime_type = mimetypes.types_map.get(f\".{file_format}\", default_mime)\n+\n+        return {\"inlineData\": {\"mimeType\": mime_type, \"data\": base64.b64encode(source_bytes).decode(\"utf-8\")}}\n+\n+    def _format_request_message_content(self, content: ContentBlock) -> dict[str, Any]:\n+        \"\"\"Format a Gemini content block.\n+\n+        Args:\n+            content: Message content.\n+\n+        Returns:\n+            Gemini formatted content block.\n+\n+        Raises:\n+            TypeError: If the content block type cannot be converted to a Gemini-compatible format.\n+        \"\"\"\n+        if \"text\" in content:\n+            return {\"text\": content[\"text\"]}\n+\n+        if \"image\" in content:\n+            return self._format_inline_data_part(cast(dict[str, Any], content[\"image\"]), \"image/png\")\n+\n+        if \"document\" in content:\n+            return self._format_inline_data_part(cast(dict[str, Any], content[\"document\"]), \"application/octet-stream\")\n+\n+        raise TypeError(f\"content_type=<{next(iter(content))}> | unsupported type\")\n+\n+    def _format_function_call(self, tool_use: ToolUse) -> dict[str, Any]:\n+        \"\"\"Format a Gemini function call.\n+\n+        Args:\n+            tool_use: Tool use requested by the model.\n+\n+        Returns:\n+            Gemini formatted function call.\n+        \"\"\"\n+        return {\"functionCall\": {\"name\": tool_use[\"name\"], \"args\": tool_use[\"input\"]}}\n+\n+    def _format_function_response(self, tool_result: ToolResult) -> dict[str, Any]:\n+        \"\"\"Format a Gemini function response.\n+\n+        Args:\n+            tool_result: Tool result from execution.\n+\n+        Returns:\n+            Gemini formatted function response.\n+        \"\"\"\n+        response_parts = []\n+        for content in tool_result[\"content\"]:\n+            if \"json\" in content:\n+                response_parts.append(json.dumps(content[\"json\"]))\n+            elif \"text\" in content:\n+                response_parts.append(content[\"text\"])\n+\n+        return {\n+            \"functionResponse\": {\"name\": tool_result[\"toolUseId\"], \"response\": {\"content\": \"\\n\".join(response_parts)}}\n+        }\n+\n+    def _format_request_messages(self, messages: Messages) -> list[dict[str, Any]]:\n+        \"\"\"Format messages for Gemini API.\n+\n+        Args:\n+            messages: List of message objects to be processed by the model.\n+\n+        Returns:\n+            Gemini formatted messages array.\n+        \"\"\"\n+        formatted_messages = []\n+\n+        for message in messages:\n+            role = \"user\" if message[\"role\"] == \"user\" else \"model\"\n+\n+            text_parts = []\n+            media_parts = []\n+            function_calls = []\n+            function_responses = []\n+\n+            for content in message[\"content\"]:\n+                if \"text\" in content:\n+                    text_parts.append(content[\"text\"])\n+                elif \"image\" in content or \"document\" in content:\n+                    media_parts.append(self._format_request_message_content(content))\n+                elif \"toolUse\" in content:\n+                    function_calls.append(self._format_function_call(content[\"toolUse\"]))\n+                elif \"toolResult\" in content:\n+                    function_responses.append(self._format_function_response(content[\"toolResult\"]))\n+\n+            parts = []\n+\n+            if text_parts:\n+                parts.append({\"text\": \"\\n\\n\".join(text_parts)})\n+\n+            if media_parts:\n+                parts.extend(media_parts)\n+\n+            parts.extend(function_calls)\n+            parts.extend(function_responses)\n+\n+            if parts:\n+                formatted_messages.append({\"role\": role, \"parts\": parts})\n+\n+        return formatted_messages\n+\n+    async def _process_chunk(\n+        self, chunk: Any, output_text_buffer: list[str], tool_calls: dict[str, str]\n+    ) -> AsyncGenerator[Union[StreamEvent, bool], None]:\n+        \"\"\"Process a single chunk from the streaming response.\"\"\"\n+        has_function_call = False\n+\n+        if hasattr(chunk, \"candidates\") and chunk.candidates:\n+            for candidate in chunk.candidates:\n+                if not hasattr(candidate, \"content\") or not candidate.content:\n+                    continue\n+\n+                for part in candidate.content.parts:\n+                    if part.text:\n+                        output_text_buffer.append(part.text)\n+                        yield self.format_event(\"content_block_delta\", part.text)\n+\n+                    # Handle function calls\n+                    elif hasattr(part, \"function_call\") and part.function_call:\n+                        function_call = part.function_call\n+                        has_function_call = True\n+\n+                        if function_call.name not in tool_calls:\n+                            yield self.format_event(\"content_block_stop\")\n+\n+                            tool_id = f\"tool_{len(tool_calls) + 1}\"\n+                            tool_calls[function_call.name] = tool_id\n+\n+                            yield self.format_event(\n+                                \"content_block_start\", {\"function_call\": function_call, \"tool_id\": tool_id}\n+                            )\n+\n+                        if hasattr(function_call, \"args\") and function_call.args:\n+                            args = self._extract_function_args(function_call)\n+                            yield self.format_event(\n+                                \"content_block_delta\", {\"function_call\": function_call, \"args\": args}\n+                            )\n+\n+        elif hasattr(chunk, \"text\") and chunk.text:\n+            output_text_buffer.append(chunk.text)\n+            yield self.format_event(\"content_block_delta\", chunk.text)\n+\n+        yield has_function_call\n+\n+    def _extract_function_args(self, function_call: Any) -> dict[str, Any]:\n+        \"\"\"Extract function arguments from various formats.\"\"\"\n+        if not hasattr(function_call, \"args\"):\n+            return {}\n+\n+        args = function_call.args\n+\n+        # Handle Struct type (protobuf)\n+        if hasattr(args, \"fields\"):\n+            return self._struct_to_dict(args)\n+\n+        # Handle JSON string\n+        if isinstance(args, str):\n+            try:\n+                parsed = json.loads(args)\n+                if isinstance(parsed, dict):\n+                    return parsed\n+                return {\"value\": args}\n+            except json.JSONDecodeError:\n+                return {\"value\": args}\n+\n+        if isinstance(args, dict):\n+            return dict(args)\n+\n+        return {}\n+\n+    def _struct_to_dict(self, struct_value: Any) -> dict[str, Any]:\n+        \"\"\"Convert protobuf Struct to dict.\"\"\"\n+        result = {}\n+        for key, value in struct_value.fields.items():\n+            if hasattr(value, \"string_value\"):\n+                result[key] = value.string_value\n+            elif hasattr(value, \"number_value\"):\n+                result[key] = value.number_value\n+            elif hasattr(value, \"bool_value\"):\n+                result[key] = value.bool_value\n+            elif hasattr(value, \"list_value\"):\n+                result[key] = [self._value_to_python(v) for v in value.list_value.values]\n+            elif hasattr(value, \"struct_value\"):\n+                result[key] = self._struct_to_dict(value.struct_value)\n+            else:\n+                result[key] = str(value)\n+        return result\n+\n+    def _value_to_python(self, value: Any) -> Any:\n+        \"\"\"Convert protobuf Value to Python type.\"\"\"\n+        if hasattr(value, \"string_value\"):\n+            return value.string_value\n+        elif hasattr(value, \"number_value\"):\n+            return value.number_value\n+        elif hasattr(value, \"bool_value\"):\n+            return value.bool_value\n+        elif hasattr(value, \"struct_value\"):\n+            return self._struct_to_dict(value.struct_value)\n+        else:\n+            return str(value)\n+\n+    async def _count_tokens_safely(self, model_id: str, contents: list[dict[str, Any]]) -> int:\n+        \"\"\"Safely count tokens with fallback to 0 on error.\n+\n+        Args:\n+            model_id: The Gemini model ID\n+            contents: The content to count tokens for\n+\n+        Returns:\n+            Token count, or 0 if counting fails\n+        \"\"\"\n+        try:\n+            token_count = await self.client.aio.models.count_tokens(model=model_id, contents=contents)\n+            if hasattr(token_count, \"total_tokens\"):\n+                return int(token_count.total_tokens or 0)\n+            return 0\n+        except Exception as e:\n+            logger.debug(\"Could not count tokens: %s\", str(e))\n+            return 0\n+\n+    def _format_tools(self, tool_specs: Optional[list[ToolSpec]]) -> Optional[list[dict[str, Any]]]:\n+        \"\"\"Format tool specifications for Gemini.\n+\n+        Args:\n+            tool_specs: List of tool specifications.\n+\n+        Returns:\n+            Gemini formatted tools array.\n+        \"\"\"\n+        if not tool_specs:\n+            return None\n+\n+        tools = []\n+        for tool_spec in tool_specs:\n+            tools.append(\n+                {\n+                    \"function_declarations\": [\n+                        {\n+                            \"name\": tool_spec[\"name\"],\n+                            \"description\": tool_spec[\"description\"],\n+                            \"parameters\": tool_spec[\"inputSchema\"][\"json\"],\n+                        }\n+                    ]\n+                }\n+            )\n+\n+        return tools\n+\n+    def format_request(\n+        self,\n+        messages: Messages,\n+        tool_specs: Optional[list[ToolSpec]] = None,\n+        system_prompt: Optional[str] = None,\n+        config: Optional[dict[str, Any]] = None,\n+    ) -> dict[str, Any]:\n+        \"\"\"Format a Gemini streaming request.\n+\n+        Args:\n+            messages: List of message objects to be processed by the model.\n+            tool_specs: List of tool specifications to make available to the model.\n+            system_prompt: System prompt to provide context to the model.\n+            config: Additional configuration options including response_schema for structured output.\n+\n+        Returns:\n+            A Gemini streaming request.\n+        \"\"\"\n+        generation_config: dict[str, Any] = {}\n+\n+        params = self.config.get(\"params\")\n+        if params:\n+            generation_config.update(params)\n+\n+        if config:\n+            if \"response_schema\" in config:\n+                generation_config[\"response_schema\"] = config[\"response_schema\"]\n+                generation_config[\"response_mime_type\"] = config.get(\"response_mime_type\", \"application/json\")\n+\n+            config_params = config.get(\"params\")\n+            if config_params:\n+                generation_config.update(config_params)\n+\n+        request = {\n+            \"contents\": self._format_request_messages(messages),\n+            \"generation_config\": generation_config,\n+            \"stream\": True,\n+        }\n+\n+        if system_prompt:\n+            request[\"system_instruction\"] = {\"parts\": [{\"text\": system_prompt}]}\n+\n+        tools = self._format_tools(tool_specs)\n+        if tools:\n+            request[\"tools\"] = tools\n+\n+        return request\n+\n+    def format_event(self, event_type: str, data: Any = None) -> StreamEvent:\n+        \"\"\"Format a Gemini event into a standardized message chunk.\n+\n+        Args:\n+            event_type: Type of event to format\n+            data: Data associated with the event\n+\n+        Returns:\n+            The formatted event\n+\n+        Raises:\n+            RuntimeError: If event_type is not recognized\n+        \"\"\"\n+        match event_type:\n+            case \"message_start\":\n+                return {\"messageStart\": {\"role\": \"assistant\"}}\n+\n+            case \"content_block_start\":\n+                if data and \"function_call\" in data:\n+                    function_call = data[\"function_call\"]\n+                    return {\n+                        \"contentBlockStart\": {\n+                            \"start\": {\"toolUse\": {\"name\": function_call.name, \"toolUseId\": data[\"tool_id\"]}}\n+                        }\n+                    }\n+                return {\"contentBlockStart\": {\"start\": {}}}\n+\n+            case \"content_block_delta\":\n+                if data and \"function_call\" in data:\n+                    args = data.get(\"args\", {})\n+                    return {\"contentBlockDelta\": {\"delta\": {\"toolUse\": {\"input\": args}}}}",
        "comment_created_at": "2025-08-28T13:17:39+00:00",
        "comment_author": "pgrayy",
        "comment_body": "Gemini it seems returns args as a complete JSON. This results in the following type of errors when we parse in `event_loop/streaming.py`:\r\n```txt\r\n>           state[\"current_tool_use\"][\"input\"] += delta_content[\"toolUse\"][\"input\"]\r\nE           TypeError: can only concatenate str (not \"dict\") to str\r\n```\r\n\r\nTo fix, we can use json.dumps:\r\n```python\r\nreturn {\"contentBlockDelta\": {\"delta\": {\"toolUse\": {\"input\": json.dumps(args)}}}}\r\n```\r\n\r\nThis is what we did for the Ollama provider ([source](https://github.com/strands-agents/sdk-python/blob/main/src/strands/models/ollama.py#L248))",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2345152287",
    "pr_number": 725,
    "pr_file": "src/strands/models/gemini.py",
    "created_at": "2025-09-12T18:49:08+00:00",
    "commented_code": "+\"\"\"Google Gemini model provider.\n+\n+- Docs: https://ai.google.dev/api\n+\"\"\"\n+\n+import json\n+import logging\n+import mimetypes\n+from typing import Any, AsyncGenerator, Optional, Type, TypedDict, TypeVar, Union, cast\n+\n+import pydantic\n+from google import genai\n+from typing_extensions import Required, Unpack, override\n+\n+from ..types.content import ContentBlock, Messages\n+from ..types.exceptions import ModelThrottledException\n+from ..types.streaming import StreamEvent\n+from ..types.tools import ToolChoice, ToolSpec\n+from ._validation import validate_config_keys\n+from .model import Model\n+\n+logger = logging.getLogger(__name__)\n+\n+T = TypeVar(\"T\", bound=pydantic.BaseModel)\n+\n+\n+class GeminiModel(Model):\n+    \"\"\"Google Gemini model provider implementation.\n+\n+    - Docs: https://ai.google.dev/api\n+    \"\"\"\n+\n+    class GeminiConfig(TypedDict, total=False):\n+        \"\"\"Configuration options for Gemini models.\n+\n+        Attributes:\n+            model_id: Gemini model ID (e.g., \"gemini-2.5-flash\").\n+                For a complete list of supported models, see\n+                https://ai.google.dev/gemini-api/docs/models\n+            params: Additional model parameters (e.g., temperature).\n+                For a complete list of supported parameters, see\n+                https://ai.google.dev/api/generate-content#generationconfig.\n+        \"\"\"\n+\n+        model_id: Required[str]\n+        params: Optional[dict[str, Any]]\n+\n+    def __init__(\n+        self,\n+        *,\n+        client_args: Optional[dict[str, Any]] = None,\n+        **model_config: Unpack[GeminiConfig],\n+    ) -> None:\n+        \"\"\"Initialize provider instance.\n+\n+        Args:\n+            client_args: Arguments for the underlying Gemini client (e.g., api_key).\n+                For a complete list of supported arguments, see https://googleapis.github.io/python-genai/.\n+            **model_config: Configuration options for the Gemini model.\n+        \"\"\"\n+        validate_config_keys(model_config, GeminiModel.GeminiConfig)\n+        self.config = GeminiModel.GeminiConfig(**model_config)\n+\n+        logger.debug(\"config=<%s> | initializing\", self.config)\n+\n+        client_args = client_args or {}\n+        self.client = genai.Client(**client_args)\n+\n+    @override\n+    def update_config(self, **model_config: Unpack[GeminiConfig]) -> None:  # type: ignore[override]\n+        \"\"\"Update the Gemini model configuration with the provided arguments.\n+\n+        Args:\n+            **model_config: Configuration overrides.\n+        \"\"\"\n+        self.config.update(model_config)\n+\n+    @override\n+    def get_config(self) -> GeminiConfig:\n+        \"\"\"Get the Gemini model configuration.\n+\n+        Returns:\n+            The Gemini model configuration.\n+        \"\"\"\n+        return self.config\n+\n+    def _format_request_content_part(self, content: ContentBlock) -> genai.types.Part:\n+        \"\"\"Format content block into a Gemini part instance.\n+\n+        - Docs: https://googleapis.github.io/python-genai/genai.html#genai.types.Part\n+\n+        Args:\n+            content: Message content to format.\n+\n+        Returns:\n+            Gemini part.\n+        \"\"\"\n+        if \"document\" in content:\n+            return genai.types.Part(\n+                inline_data=genai.types.Blob(\n+                    data=content[\"document\"][\"source\"][\"bytes\"],\n+                    mime_type=mimetypes.types_map.get(f\".{content['document']['format']}\", \"application/octet-stream\"),\n+                ),\n+            )\n+\n+        if \"image\" in content:\n+            return genai.types.Part(\n+                inline_data=genai.types.Blob(\n+                    data=content[\"image\"][\"source\"][\"bytes\"],\n+                    mime_type=mimetypes.types_map.get(f\".{content['image']['format']}\", \"application/octet-stream\"),\n+                ),\n+            )\n+\n+        if \"reasoningContent\" in content:\n+            return genai.types.Part(\n+                thought=True,\n+                text=content[\"reasoningContent\"][\"reasoningText\"][\"text\"],\n+            )\n+\n+        if \"text\" in content:\n+            return genai.types.Part(text=content[\"text\"])\n+\n+        if \"toolResult\" in content:\n+            return genai.types.Part(\n+                function_response=genai.types.FunctionResponse(\n+                    id=content[\"toolResult\"][\"toolUseId\"],\n+                    name=content[\"toolResult\"][\"toolUseId\"],\n+                    response={\n+                        \"output\": [\n+                            self._format_request_content_part(\n+                                {\"text\": json.dumps(tool_result_content[\"json\"])}\n+                                if \"json\" in tool_result_content\n+                                else cast(ContentBlock, tool_result_content)\n+                            ).to_json_dict()\n+                            for tool_result_content in content[\"toolResult\"][\"content\"]\n+                        ],\n+                    },\n+                ),\n+            )\n+\n+        if \"toolUse\" in content:\n+            return genai.types.Part(\n+                function_call=genai.types.FunctionCall(\n+                    args=content[\"toolUse\"][\"input\"],\n+                    id=content[\"toolUse\"][\"toolUseId\"],\n+                    name=content[\"toolUse\"][\"name\"],\n+                ),\n+            )\n+\n+        raise TypeError(f\"content_type=<{next(iter(content))}> | unsupported type\")\n+\n+    def _format_request_content(self, messages: Messages) -> list[genai.types.Content]:\n+        \"\"\"Format message content into Gemini content instances.\n+\n+        - Docs: https://googleapis.github.io/python-genai/genai.html#genai.types.Content\n+\n+        Args:\n+            messages: List of message objects to be processed by the model.\n+\n+        Returns:\n+            Gemini content list.\n+        \"\"\"\n+        return [\n+            genai.types.Content(\n+                parts=[self._format_request_content_part(content) for content in message[\"content\"]],\n+                role=\"user\" if message[\"role\"] == \"user\" else \"model\",\n+            )\n+            for message in messages\n+        ]\n+\n+    def _format_request_tools(self, tool_specs: Optional[list[ToolSpec]]) -> list[genai.types.Tool | Any]:\n+        \"\"\"Format tool specs into Gemini tools.\n+\n+        - Docs: https://googleapis.github.io/python-genai/genai.html#genai.types.Tool\n+\n+        Args:\n+            tool_specs: List of tool specifications to make available to the model.\n+\n+        Return:\n+            Gemini tool list.\n+        \"\"\"\n+        return [\n+            genai.types.Tool(\n+                function_declarations=[\n+                    genai.types.FunctionDeclaration(\n+                        name=tool_spec[\"name\"],\n+                        description=tool_spec[\"description\"],\n+                        parameters=tool_spec[\"inputSchema\"][\"json\"],\n+                    )\n+                    for tool_spec in tool_specs or []\n+                ],\n+            ),\n+        ]\n+\n+    def _format_request_config(\n+        self,\n+        tool_specs: Optional[list[ToolSpec]],\n+        system_prompt: Optional[str],\n+        params: Optional[dict[str, Any]],\n+    ) -> genai.types.GenerateContentConfig:\n+        \"\"\"Format Gemini request config.\n+\n+        - Docs: https://googleapis.github.io/python-genai/genai.html#genai.types.GenerateContentConfig\n+\n+        Args:\n+            tool_specs: List of tool specifications to make available to the model.\n+            system_prompt: System prompt to provide context to the model.\n+            params: Additional model parameters (e.g., temperature).\n+\n+        Returns:\n+            Gemini request config.\n+        \"\"\"\n+        return genai.types.GenerateContentConfig(\n+            system_instruction=system_prompt,\n+            tools=self._format_request_tools(tool_specs),\n+            **(params or {}),\n+        )\n+\n+    def _format_request(\n+        self,\n+        messages: Messages,\n+        tool_specs: Optional[list[ToolSpec]],\n+        system_prompt: Optional[str],\n+        params: Optional[dict[str, Any]],\n+    ) -> dict[str, Any]:\n+        \"\"\"Format a Gemini streaming request.\n+\n+        - Docs: https://ai.google.dev/api/generate-content#endpoint_1\n+\n+        Args:\n+            messages: List of message objects to be processed by the model.\n+            tool_specs: List of tool specifications to make available to the model.\n+            system_prompt: System prompt to provide context to the model.\n+            params: Additional model parameters (e.g., temperature).\n+\n+        Returns:\n+            A Gemini streaming request.\n+        \"\"\"\n+        return {\n+            \"config\": self._format_request_config(tool_specs, system_prompt, params).to_json_dict(),\n+            \"contents\": [content.to_json_dict() for content in self._format_request_content(messages)],\n+            \"model\": self.config[\"model_id\"],\n+        }\n+\n+    def _format_chunk(self, event: dict[str, Any]) -> StreamEvent:\n+        \"\"\"Format the Gemini response events into standardized message chunks.\n+\n+        Args:\n+            event: A response event from the Gemini model.\n+\n+        Returns:\n+            The formatted chunk.\n+\n+        Raises:\n+            RuntimeError: If chunk_type is not recognized.\n+                This error should never be encountered as we control chunk_type in the stream method.\n+        \"\"\"\n+        match event[\"chunk_type\"]:\n+            case \"message_start\":\n+                return {\"messageStart\": {\"role\": \"assistant\"}}\n+\n+            case \"content_start\":\n+                match event[\"data_type\"]:\n+                    case \"tool\":\n+                        # Note: toolUseId is the only identifier available in a tool result. However, Gemini requires\n+                        #       that name be set in the equivalent FunctionResponse type. Consequently, we assign\n+                        #       function name to toolUseId in our tool use block. And another reason, function_call is\n+                        #       not guaranteed to have id populated.\n+                        return {\n+                            \"contentBlockStart\": {\n+                                \"start\": {\n+                                    \"toolUse\": {\n+                                        \"name\": event[\"data\"].function_call.name,\n+                                        \"toolUseId\": event[\"data\"].function_call.name,\n+                                    },\n+                                },\n+                            },\n+                        }\n+\n+                    case _:\n+                        return {\"contentBlockStart\": {\"start\": {}}}\n+\n+            case \"content_delta\":\n+                match event[\"data_type\"]:\n+                    case \"tool\":\n+                        return {\n+                            \"contentBlockDelta\": {\n+                                \"delta\": {\"toolUse\": {\"input\": json.dumps(event[\"data\"].function_call.args)}}\n+                            }\n+                        }\n+\n+                    case \"reasoning_content\":\n+                        return {\n+                            \"contentBlockDelta\": {\n+                                \"delta\": {\n+                                    \"reasoningContent\": {\n+                                        \"text\": event[\"data\"].text,\n+                                    },\n+                                },\n+                            },\n+                        }\n+\n+                    case _:\n+                        return {\"contentBlockDelta\": {\"delta\": {\"text\": event[\"data\"].text}}}\n+\n+            case \"content_stop\":\n+                return {\"contentBlockStop\": {}}\n+\n+            case \"message_stop\":\n+                match event[\"data\"]:\n+                    case \"TOOL_USE\":\n+                        return {\"messageStop\": {\"stopReason\": \"tool_use\"}}\n+                    case \"MAX_TOKENS\":\n+                        return {\"messageStop\": {\"stopReason\": \"max_tokens\"}}\n+                    case _:\n+                        return {\"messageStop\": {\"stopReason\": \"end_turn\"}}\n+\n+            case \"metadata\":\n+                return {\n+                    \"metadata\": {\n+                        \"usage\": {\n+                            \"inputTokens\": event[\"data\"].prompt_token_count,\n+                            \"outputTokens\": event[\"data\"].total_token_count - event[\"data\"].prompt_token_count,",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2345152287",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 725,
        "pr_file": "src/strands/models/gemini.py",
        "discussion_id": "2345152287",
        "commented_code": "@@ -0,0 +1,430 @@\n+\"\"\"Google Gemini model provider.\n+\n+- Docs: https://ai.google.dev/api\n+\"\"\"\n+\n+import json\n+import logging\n+import mimetypes\n+from typing import Any, AsyncGenerator, Optional, Type, TypedDict, TypeVar, Union, cast\n+\n+import pydantic\n+from google import genai\n+from typing_extensions import Required, Unpack, override\n+\n+from ..types.content import ContentBlock, Messages\n+from ..types.exceptions import ModelThrottledException\n+from ..types.streaming import StreamEvent\n+from ..types.tools import ToolChoice, ToolSpec\n+from ._validation import validate_config_keys\n+from .model import Model\n+\n+logger = logging.getLogger(__name__)\n+\n+T = TypeVar(\"T\", bound=pydantic.BaseModel)\n+\n+\n+class GeminiModel(Model):\n+    \"\"\"Google Gemini model provider implementation.\n+\n+    - Docs: https://ai.google.dev/api\n+    \"\"\"\n+\n+    class GeminiConfig(TypedDict, total=False):\n+        \"\"\"Configuration options for Gemini models.\n+\n+        Attributes:\n+            model_id: Gemini model ID (e.g., \"gemini-2.5-flash\").\n+                For a complete list of supported models, see\n+                https://ai.google.dev/gemini-api/docs/models\n+            params: Additional model parameters (e.g., temperature).\n+                For a complete list of supported parameters, see\n+                https://ai.google.dev/api/generate-content#generationconfig.\n+        \"\"\"\n+\n+        model_id: Required[str]\n+        params: Optional[dict[str, Any]]\n+\n+    def __init__(\n+        self,\n+        *,\n+        client_args: Optional[dict[str, Any]] = None,\n+        **model_config: Unpack[GeminiConfig],\n+    ) -> None:\n+        \"\"\"Initialize provider instance.\n+\n+        Args:\n+            client_args: Arguments for the underlying Gemini client (e.g., api_key).\n+                For a complete list of supported arguments, see https://googleapis.github.io/python-genai/.\n+            **model_config: Configuration options for the Gemini model.\n+        \"\"\"\n+        validate_config_keys(model_config, GeminiModel.GeminiConfig)\n+        self.config = GeminiModel.GeminiConfig(**model_config)\n+\n+        logger.debug(\"config=<%s> | initializing\", self.config)\n+\n+        client_args = client_args or {}\n+        self.client = genai.Client(**client_args)\n+\n+    @override\n+    def update_config(self, **model_config: Unpack[GeminiConfig]) -> None:  # type: ignore[override]\n+        \"\"\"Update the Gemini model configuration with the provided arguments.\n+\n+        Args:\n+            **model_config: Configuration overrides.\n+        \"\"\"\n+        self.config.update(model_config)\n+\n+    @override\n+    def get_config(self) -> GeminiConfig:\n+        \"\"\"Get the Gemini model configuration.\n+\n+        Returns:\n+            The Gemini model configuration.\n+        \"\"\"\n+        return self.config\n+\n+    def _format_request_content_part(self, content: ContentBlock) -> genai.types.Part:\n+        \"\"\"Format content block into a Gemini part instance.\n+\n+        - Docs: https://googleapis.github.io/python-genai/genai.html#genai.types.Part\n+\n+        Args:\n+            content: Message content to format.\n+\n+        Returns:\n+            Gemini part.\n+        \"\"\"\n+        if \"document\" in content:\n+            return genai.types.Part(\n+                inline_data=genai.types.Blob(\n+                    data=content[\"document\"][\"source\"][\"bytes\"],\n+                    mime_type=mimetypes.types_map.get(f\".{content['document']['format']}\", \"application/octet-stream\"),\n+                ),\n+            )\n+\n+        if \"image\" in content:\n+            return genai.types.Part(\n+                inline_data=genai.types.Blob(\n+                    data=content[\"image\"][\"source\"][\"bytes\"],\n+                    mime_type=mimetypes.types_map.get(f\".{content['image']['format']}\", \"application/octet-stream\"),\n+                ),\n+            )\n+\n+        if \"reasoningContent\" in content:\n+            return genai.types.Part(\n+                thought=True,\n+                text=content[\"reasoningContent\"][\"reasoningText\"][\"text\"],\n+            )\n+\n+        if \"text\" in content:\n+            return genai.types.Part(text=content[\"text\"])\n+\n+        if \"toolResult\" in content:\n+            return genai.types.Part(\n+                function_response=genai.types.FunctionResponse(\n+                    id=content[\"toolResult\"][\"toolUseId\"],\n+                    name=content[\"toolResult\"][\"toolUseId\"],\n+                    response={\n+                        \"output\": [\n+                            self._format_request_content_part(\n+                                {\"text\": json.dumps(tool_result_content[\"json\"])}\n+                                if \"json\" in tool_result_content\n+                                else cast(ContentBlock, tool_result_content)\n+                            ).to_json_dict()\n+                            for tool_result_content in content[\"toolResult\"][\"content\"]\n+                        ],\n+                    },\n+                ),\n+            )\n+\n+        if \"toolUse\" in content:\n+            return genai.types.Part(\n+                function_call=genai.types.FunctionCall(\n+                    args=content[\"toolUse\"][\"input\"],\n+                    id=content[\"toolUse\"][\"toolUseId\"],\n+                    name=content[\"toolUse\"][\"name\"],\n+                ),\n+            )\n+\n+        raise TypeError(f\"content_type=<{next(iter(content))}> | unsupported type\")\n+\n+    def _format_request_content(self, messages: Messages) -> list[genai.types.Content]:\n+        \"\"\"Format message content into Gemini content instances.\n+\n+        - Docs: https://googleapis.github.io/python-genai/genai.html#genai.types.Content\n+\n+        Args:\n+            messages: List of message objects to be processed by the model.\n+\n+        Returns:\n+            Gemini content list.\n+        \"\"\"\n+        return [\n+            genai.types.Content(\n+                parts=[self._format_request_content_part(content) for content in message[\"content\"]],\n+                role=\"user\" if message[\"role\"] == \"user\" else \"model\",\n+            )\n+            for message in messages\n+        ]\n+\n+    def _format_request_tools(self, tool_specs: Optional[list[ToolSpec]]) -> list[genai.types.Tool | Any]:\n+        \"\"\"Format tool specs into Gemini tools.\n+\n+        - Docs: https://googleapis.github.io/python-genai/genai.html#genai.types.Tool\n+\n+        Args:\n+            tool_specs: List of tool specifications to make available to the model.\n+\n+        Return:\n+            Gemini tool list.\n+        \"\"\"\n+        return [\n+            genai.types.Tool(\n+                function_declarations=[\n+                    genai.types.FunctionDeclaration(\n+                        name=tool_spec[\"name\"],\n+                        description=tool_spec[\"description\"],\n+                        parameters=tool_spec[\"inputSchema\"][\"json\"],\n+                    )\n+                    for tool_spec in tool_specs or []\n+                ],\n+            ),\n+        ]\n+\n+    def _format_request_config(\n+        self,\n+        tool_specs: Optional[list[ToolSpec]],\n+        system_prompt: Optional[str],\n+        params: Optional[dict[str, Any]],\n+    ) -> genai.types.GenerateContentConfig:\n+        \"\"\"Format Gemini request config.\n+\n+        - Docs: https://googleapis.github.io/python-genai/genai.html#genai.types.GenerateContentConfig\n+\n+        Args:\n+            tool_specs: List of tool specifications to make available to the model.\n+            system_prompt: System prompt to provide context to the model.\n+            params: Additional model parameters (e.g., temperature).\n+\n+        Returns:\n+            Gemini request config.\n+        \"\"\"\n+        return genai.types.GenerateContentConfig(\n+            system_instruction=system_prompt,\n+            tools=self._format_request_tools(tool_specs),\n+            **(params or {}),\n+        )\n+\n+    def _format_request(\n+        self,\n+        messages: Messages,\n+        tool_specs: Optional[list[ToolSpec]],\n+        system_prompt: Optional[str],\n+        params: Optional[dict[str, Any]],\n+    ) -> dict[str, Any]:\n+        \"\"\"Format a Gemini streaming request.\n+\n+        - Docs: https://ai.google.dev/api/generate-content#endpoint_1\n+\n+        Args:\n+            messages: List of message objects to be processed by the model.\n+            tool_specs: List of tool specifications to make available to the model.\n+            system_prompt: System prompt to provide context to the model.\n+            params: Additional model parameters (e.g., temperature).\n+\n+        Returns:\n+            A Gemini streaming request.\n+        \"\"\"\n+        return {\n+            \"config\": self._format_request_config(tool_specs, system_prompt, params).to_json_dict(),\n+            \"contents\": [content.to_json_dict() for content in self._format_request_content(messages)],\n+            \"model\": self.config[\"model_id\"],\n+        }\n+\n+    def _format_chunk(self, event: dict[str, Any]) -> StreamEvent:\n+        \"\"\"Format the Gemini response events into standardized message chunks.\n+\n+        Args:\n+            event: A response event from the Gemini model.\n+\n+        Returns:\n+            The formatted chunk.\n+\n+        Raises:\n+            RuntimeError: If chunk_type is not recognized.\n+                This error should never be encountered as we control chunk_type in the stream method.\n+        \"\"\"\n+        match event[\"chunk_type\"]:\n+            case \"message_start\":\n+                return {\"messageStart\": {\"role\": \"assistant\"}}\n+\n+            case \"content_start\":\n+                match event[\"data_type\"]:\n+                    case \"tool\":\n+                        # Note: toolUseId is the only identifier available in a tool result. However, Gemini requires\n+                        #       that name be set in the equivalent FunctionResponse type. Consequently, we assign\n+                        #       function name to toolUseId in our tool use block. And another reason, function_call is\n+                        #       not guaranteed to have id populated.\n+                        return {\n+                            \"contentBlockStart\": {\n+                                \"start\": {\n+                                    \"toolUse\": {\n+                                        \"name\": event[\"data\"].function_call.name,\n+                                        \"toolUseId\": event[\"data\"].function_call.name,\n+                                    },\n+                                },\n+                            },\n+                        }\n+\n+                    case _:\n+                        return {\"contentBlockStart\": {\"start\": {}}}\n+\n+            case \"content_delta\":\n+                match event[\"data_type\"]:\n+                    case \"tool\":\n+                        return {\n+                            \"contentBlockDelta\": {\n+                                \"delta\": {\"toolUse\": {\"input\": json.dumps(event[\"data\"].function_call.args)}}\n+                            }\n+                        }\n+\n+                    case \"reasoning_content\":\n+                        return {\n+                            \"contentBlockDelta\": {\n+                                \"delta\": {\n+                                    \"reasoningContent\": {\n+                                        \"text\": event[\"data\"].text,\n+                                    },\n+                                },\n+                            },\n+                        }\n+\n+                    case _:\n+                        return {\"contentBlockDelta\": {\"delta\": {\"text\": event[\"data\"].text}}}\n+\n+            case \"content_stop\":\n+                return {\"contentBlockStop\": {}}\n+\n+            case \"message_stop\":\n+                match event[\"data\"]:\n+                    case \"TOOL_USE\":\n+                        return {\"messageStop\": {\"stopReason\": \"tool_use\"}}\n+                    case \"MAX_TOKENS\":\n+                        return {\"messageStop\": {\"stopReason\": \"max_tokens\"}}\n+                    case _:\n+                        return {\"messageStop\": {\"stopReason\": \"end_turn\"}}\n+\n+            case \"metadata\":\n+                return {\n+                    \"metadata\": {\n+                        \"usage\": {\n+                            \"inputTokens\": event[\"data\"].prompt_token_count,\n+                            \"outputTokens\": event[\"data\"].total_token_count - event[\"data\"].prompt_token_count,",
        "comment_created_at": "2025-09-12T18:49:08+00:00",
        "comment_author": "dbschmigelski",
        "comment_body": "do we also need to subtract tool_use_prompt_token_count or is that included or just not relevant?",
        "pr_file_module": null
      },
      {
        "comment_id": "2345214122",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 725,
        "pr_file": "src/strands/models/gemini.py",
        "discussion_id": "2345152287",
        "commented_code": "@@ -0,0 +1,430 @@\n+\"\"\"Google Gemini model provider.\n+\n+- Docs: https://ai.google.dev/api\n+\"\"\"\n+\n+import json\n+import logging\n+import mimetypes\n+from typing import Any, AsyncGenerator, Optional, Type, TypedDict, TypeVar, Union, cast\n+\n+import pydantic\n+from google import genai\n+from typing_extensions import Required, Unpack, override\n+\n+from ..types.content import ContentBlock, Messages\n+from ..types.exceptions import ModelThrottledException\n+from ..types.streaming import StreamEvent\n+from ..types.tools import ToolChoice, ToolSpec\n+from ._validation import validate_config_keys\n+from .model import Model\n+\n+logger = logging.getLogger(__name__)\n+\n+T = TypeVar(\"T\", bound=pydantic.BaseModel)\n+\n+\n+class GeminiModel(Model):\n+    \"\"\"Google Gemini model provider implementation.\n+\n+    - Docs: https://ai.google.dev/api\n+    \"\"\"\n+\n+    class GeminiConfig(TypedDict, total=False):\n+        \"\"\"Configuration options for Gemini models.\n+\n+        Attributes:\n+            model_id: Gemini model ID (e.g., \"gemini-2.5-flash\").\n+                For a complete list of supported models, see\n+                https://ai.google.dev/gemini-api/docs/models\n+            params: Additional model parameters (e.g., temperature).\n+                For a complete list of supported parameters, see\n+                https://ai.google.dev/api/generate-content#generationconfig.\n+        \"\"\"\n+\n+        model_id: Required[str]\n+        params: Optional[dict[str, Any]]\n+\n+    def __init__(\n+        self,\n+        *,\n+        client_args: Optional[dict[str, Any]] = None,\n+        **model_config: Unpack[GeminiConfig],\n+    ) -> None:\n+        \"\"\"Initialize provider instance.\n+\n+        Args:\n+            client_args: Arguments for the underlying Gemini client (e.g., api_key).\n+                For a complete list of supported arguments, see https://googleapis.github.io/python-genai/.\n+            **model_config: Configuration options for the Gemini model.\n+        \"\"\"\n+        validate_config_keys(model_config, GeminiModel.GeminiConfig)\n+        self.config = GeminiModel.GeminiConfig(**model_config)\n+\n+        logger.debug(\"config=<%s> | initializing\", self.config)\n+\n+        client_args = client_args or {}\n+        self.client = genai.Client(**client_args)\n+\n+    @override\n+    def update_config(self, **model_config: Unpack[GeminiConfig]) -> None:  # type: ignore[override]\n+        \"\"\"Update the Gemini model configuration with the provided arguments.\n+\n+        Args:\n+            **model_config: Configuration overrides.\n+        \"\"\"\n+        self.config.update(model_config)\n+\n+    @override\n+    def get_config(self) -> GeminiConfig:\n+        \"\"\"Get the Gemini model configuration.\n+\n+        Returns:\n+            The Gemini model configuration.\n+        \"\"\"\n+        return self.config\n+\n+    def _format_request_content_part(self, content: ContentBlock) -> genai.types.Part:\n+        \"\"\"Format content block into a Gemini part instance.\n+\n+        - Docs: https://googleapis.github.io/python-genai/genai.html#genai.types.Part\n+\n+        Args:\n+            content: Message content to format.\n+\n+        Returns:\n+            Gemini part.\n+        \"\"\"\n+        if \"document\" in content:\n+            return genai.types.Part(\n+                inline_data=genai.types.Blob(\n+                    data=content[\"document\"][\"source\"][\"bytes\"],\n+                    mime_type=mimetypes.types_map.get(f\".{content['document']['format']}\", \"application/octet-stream\"),\n+                ),\n+            )\n+\n+        if \"image\" in content:\n+            return genai.types.Part(\n+                inline_data=genai.types.Blob(\n+                    data=content[\"image\"][\"source\"][\"bytes\"],\n+                    mime_type=mimetypes.types_map.get(f\".{content['image']['format']}\", \"application/octet-stream\"),\n+                ),\n+            )\n+\n+        if \"reasoningContent\" in content:\n+            return genai.types.Part(\n+                thought=True,\n+                text=content[\"reasoningContent\"][\"reasoningText\"][\"text\"],\n+            )\n+\n+        if \"text\" in content:\n+            return genai.types.Part(text=content[\"text\"])\n+\n+        if \"toolResult\" in content:\n+            return genai.types.Part(\n+                function_response=genai.types.FunctionResponse(\n+                    id=content[\"toolResult\"][\"toolUseId\"],\n+                    name=content[\"toolResult\"][\"toolUseId\"],\n+                    response={\n+                        \"output\": [\n+                            self._format_request_content_part(\n+                                {\"text\": json.dumps(tool_result_content[\"json\"])}\n+                                if \"json\" in tool_result_content\n+                                else cast(ContentBlock, tool_result_content)\n+                            ).to_json_dict()\n+                            for tool_result_content in content[\"toolResult\"][\"content\"]\n+                        ],\n+                    },\n+                ),\n+            )\n+\n+        if \"toolUse\" in content:\n+            return genai.types.Part(\n+                function_call=genai.types.FunctionCall(\n+                    args=content[\"toolUse\"][\"input\"],\n+                    id=content[\"toolUse\"][\"toolUseId\"],\n+                    name=content[\"toolUse\"][\"name\"],\n+                ),\n+            )\n+\n+        raise TypeError(f\"content_type=<{next(iter(content))}> | unsupported type\")\n+\n+    def _format_request_content(self, messages: Messages) -> list[genai.types.Content]:\n+        \"\"\"Format message content into Gemini content instances.\n+\n+        - Docs: https://googleapis.github.io/python-genai/genai.html#genai.types.Content\n+\n+        Args:\n+            messages: List of message objects to be processed by the model.\n+\n+        Returns:\n+            Gemini content list.\n+        \"\"\"\n+        return [\n+            genai.types.Content(\n+                parts=[self._format_request_content_part(content) for content in message[\"content\"]],\n+                role=\"user\" if message[\"role\"] == \"user\" else \"model\",\n+            )\n+            for message in messages\n+        ]\n+\n+    def _format_request_tools(self, tool_specs: Optional[list[ToolSpec]]) -> list[genai.types.Tool | Any]:\n+        \"\"\"Format tool specs into Gemini tools.\n+\n+        - Docs: https://googleapis.github.io/python-genai/genai.html#genai.types.Tool\n+\n+        Args:\n+            tool_specs: List of tool specifications to make available to the model.\n+\n+        Return:\n+            Gemini tool list.\n+        \"\"\"\n+        return [\n+            genai.types.Tool(\n+                function_declarations=[\n+                    genai.types.FunctionDeclaration(\n+                        name=tool_spec[\"name\"],\n+                        description=tool_spec[\"description\"],\n+                        parameters=tool_spec[\"inputSchema\"][\"json\"],\n+                    )\n+                    for tool_spec in tool_specs or []\n+                ],\n+            ),\n+        ]\n+\n+    def _format_request_config(\n+        self,\n+        tool_specs: Optional[list[ToolSpec]],\n+        system_prompt: Optional[str],\n+        params: Optional[dict[str, Any]],\n+    ) -> genai.types.GenerateContentConfig:\n+        \"\"\"Format Gemini request config.\n+\n+        - Docs: https://googleapis.github.io/python-genai/genai.html#genai.types.GenerateContentConfig\n+\n+        Args:\n+            tool_specs: List of tool specifications to make available to the model.\n+            system_prompt: System prompt to provide context to the model.\n+            params: Additional model parameters (e.g., temperature).\n+\n+        Returns:\n+            Gemini request config.\n+        \"\"\"\n+        return genai.types.GenerateContentConfig(\n+            system_instruction=system_prompt,\n+            tools=self._format_request_tools(tool_specs),\n+            **(params or {}),\n+        )\n+\n+    def _format_request(\n+        self,\n+        messages: Messages,\n+        tool_specs: Optional[list[ToolSpec]],\n+        system_prompt: Optional[str],\n+        params: Optional[dict[str, Any]],\n+    ) -> dict[str, Any]:\n+        \"\"\"Format a Gemini streaming request.\n+\n+        - Docs: https://ai.google.dev/api/generate-content#endpoint_1\n+\n+        Args:\n+            messages: List of message objects to be processed by the model.\n+            tool_specs: List of tool specifications to make available to the model.\n+            system_prompt: System prompt to provide context to the model.\n+            params: Additional model parameters (e.g., temperature).\n+\n+        Returns:\n+            A Gemini streaming request.\n+        \"\"\"\n+        return {\n+            \"config\": self._format_request_config(tool_specs, system_prompt, params).to_json_dict(),\n+            \"contents\": [content.to_json_dict() for content in self._format_request_content(messages)],\n+            \"model\": self.config[\"model_id\"],\n+        }\n+\n+    def _format_chunk(self, event: dict[str, Any]) -> StreamEvent:\n+        \"\"\"Format the Gemini response events into standardized message chunks.\n+\n+        Args:\n+            event: A response event from the Gemini model.\n+\n+        Returns:\n+            The formatted chunk.\n+\n+        Raises:\n+            RuntimeError: If chunk_type is not recognized.\n+                This error should never be encountered as we control chunk_type in the stream method.\n+        \"\"\"\n+        match event[\"chunk_type\"]:\n+            case \"message_start\":\n+                return {\"messageStart\": {\"role\": \"assistant\"}}\n+\n+            case \"content_start\":\n+                match event[\"data_type\"]:\n+                    case \"tool\":\n+                        # Note: toolUseId is the only identifier available in a tool result. However, Gemini requires\n+                        #       that name be set in the equivalent FunctionResponse type. Consequently, we assign\n+                        #       function name to toolUseId in our tool use block. And another reason, function_call is\n+                        #       not guaranteed to have id populated.\n+                        return {\n+                            \"contentBlockStart\": {\n+                                \"start\": {\n+                                    \"toolUse\": {\n+                                        \"name\": event[\"data\"].function_call.name,\n+                                        \"toolUseId\": event[\"data\"].function_call.name,\n+                                    },\n+                                },\n+                            },\n+                        }\n+\n+                    case _:\n+                        return {\"contentBlockStart\": {\"start\": {}}}\n+\n+            case \"content_delta\":\n+                match event[\"data_type\"]:\n+                    case \"tool\":\n+                        return {\n+                            \"contentBlockDelta\": {\n+                                \"delta\": {\"toolUse\": {\"input\": json.dumps(event[\"data\"].function_call.args)}}\n+                            }\n+                        }\n+\n+                    case \"reasoning_content\":\n+                        return {\n+                            \"contentBlockDelta\": {\n+                                \"delta\": {\n+                                    \"reasoningContent\": {\n+                                        \"text\": event[\"data\"].text,\n+                                    },\n+                                },\n+                            },\n+                        }\n+\n+                    case _:\n+                        return {\"contentBlockDelta\": {\"delta\": {\"text\": event[\"data\"].text}}}\n+\n+            case \"content_stop\":\n+                return {\"contentBlockStop\": {}}\n+\n+            case \"message_stop\":\n+                match event[\"data\"]:\n+                    case \"TOOL_USE\":\n+                        return {\"messageStop\": {\"stopReason\": \"tool_use\"}}\n+                    case \"MAX_TOKENS\":\n+                        return {\"messageStop\": {\"stopReason\": \"max_tokens\"}}\n+                    case _:\n+                        return {\"messageStop\": {\"stopReason\": \"end_turn\"}}\n+\n+            case \"metadata\":\n+                return {\n+                    \"metadata\": {\n+                        \"usage\": {\n+                            \"inputTokens\": event[\"data\"].prompt_token_count,\n+                            \"outputTokens\": event[\"data\"].total_token_count - event[\"data\"].prompt_token_count,",
        "comment_created_at": "2025-09-12T19:16:08+00:00",
        "comment_author": "pgrayy",
        "comment_body": "In Gemini, the total output token count is spread across multiple fields (e.g., candidates_token_count, thoughts_token_count, etc.) ([docs](https://googleapis.github.io/python-genai/genai.html#genai.types.GenerateContentResponseUsageMetadata)). Thus to simplify the calculation, I just subtracted prompt_token_count from total_token_count.",
        "pr_file_module": null
      }
    ]
  }
]