[
  {
    "discussion_id": "2389158211",
    "pr_number": 900,
    "pr_file": "src/strands/multiagent/graph.py",
    "created_at": "2025-09-29T20:12:10+00:00",
    "commented_code": "self.reset_on_revisit = reset_on_revisit\n         self.state = GraphState()\n         self.tracer = get_tracer()\n+        self.session_manager = session_manager\n+        self.hooks = hooks or HookRegistry()\n+\n+        # Concurrncy lock\n+        self._lock = asyncio.Lock()",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2389158211",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 900,
        "pr_file": "src/strands/multiagent/graph.py",
        "discussion_id": "2389158211",
        "commented_code": "@@ -384,6 +455,16 @@ def __init__(\n         self.reset_on_revisit = reset_on_revisit\n         self.state = GraphState()\n         self.tracer = get_tracer()\n+        self.session_manager = session_manager\n+        self.hooks = hooks or HookRegistry()\n+\n+        # Concurrncy lock\n+        self._lock = asyncio.Lock()",
        "comment_created_at": "2025-09-29T20:12:10+00:00",
        "comment_author": "zastrowm",
        "comment_body": "Why do we use an asyncio lock here but a RLock elsewhere",
        "pr_file_module": null
      },
      {
        "comment_id": "2392347536",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 900,
        "pr_file": "src/strands/multiagent/graph.py",
        "discussion_id": "2389158211",
        "commented_code": "@@ -384,6 +455,16 @@ def __init__(\n         self.reset_on_revisit = reset_on_revisit\n         self.state = GraphState()\n         self.tracer = get_tracer()\n+        self.session_manager = session_manager\n+        self.hooks = hooks or HookRegistry()\n+\n+        # Concurrncy lock\n+        self._lock = asyncio.Lock()",
        "comment_created_at": "2025-09-30T17:29:21+00:00",
        "comment_author": "JackYPCOnline",
        "comment_body": "Because Graph run nodes concurrently as async tasks in single event loop. A thread lock will block execution.\r\nFor write_file is synchronous could be handled with thread lock and rlock is also reentranable",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2436817525",
    "pr_number": 900,
    "pr_file": "src/strands/multiagent/graph.py",
    "created_at": "2025-10-16T17:23:24+00:00",
    "commented_code": "# Build node input from satisfied dependencies\n             node_input = self._build_node_input(node)\n \n-            # Execute with timeout protection (only if node_timeout is set)\n-            try:\n-                # Execute based on node type and create unified NodeResult\n-                if isinstance(node.executor, MultiAgentBase):\n-                    if self.node_timeout is not None:\n-                        multi_agent_result = await asyncio.wait_for(\n-                            node.executor.invoke_async(node_input, invocation_state),\n-                            timeout=self.node_timeout,\n-                        )\n-                    else:\n-                        multi_agent_result = await node.executor.invoke_async(node_input, invocation_state)\n-\n-                    # Create NodeResult with MultiAgentResult directly\n-                    node_result = NodeResult(\n-                        result=multi_agent_result,  # type is MultiAgentResult\n-                        execution_time=multi_agent_result.execution_time,\n-                        status=Status.COMPLETED,\n-                        accumulated_usage=multi_agent_result.accumulated_usage,\n-                        accumulated_metrics=multi_agent_result.accumulated_metrics,\n-                        execution_count=multi_agent_result.execution_count,\n+            # Execute based on node type and create unified NodeResult\n+            if isinstance(node.executor, MultiAgentBase):\n+                if self.node_timeout is not None:\n+                    multiagent_result = await asyncio.wait_for(\n+                        node.executor.invoke_async(node_input, invocation_state),\n+                        timeout=self.node_timeout,\n                     )\n+                else:\n+                    multiagent_result = await node.executor.invoke_async(node_input, invocation_state)\n+\n+                # Create NodeResult with MultiAgentResult directly\n+                node_result = NodeResult(\n+                    result=multiagent_result,  # type is MultiAgentResult\n+                    execution_time=multiagent_result.execution_time,\n+                    status=Status.COMPLETED,\n+                    accumulated_usage=multiagent_result.accumulated_usage,\n+                    accumulated_metrics=multiagent_result.accumulated_metrics,\n+                    execution_count=multiagent_result.execution_count,\n+                )\n \n-                elif isinstance(node.executor, Agent):\n-                    if self.node_timeout is not None:\n-                        agent_response = await asyncio.wait_for(\n-                            node.executor.invoke_async(node_input, invocation_state=invocation_state),\n-                            timeout=self.node_timeout,\n-                        )\n-                    else:\n-                        agent_response = await node.executor.invoke_async(node_input, invocation_state=invocation_state)\n-\n-                    # Extract metrics from agent response\n-                    usage = Usage(inputTokens=0, outputTokens=0, totalTokens=0)\n-                    metrics = Metrics(latencyMs=0)\n-                    if hasattr(agent_response, \"metrics\") and agent_response.metrics:\n-                        if hasattr(agent_response.metrics, \"accumulated_usage\"):\n-                            usage = agent_response.metrics.accumulated_usage\n-                        if hasattr(agent_response.metrics, \"accumulated_metrics\"):\n-                            metrics = agent_response.metrics.accumulated_metrics\n-\n-                    node_result = NodeResult(\n-                        result=agent_response,  # type is AgentResult\n-                        execution_time=round((time.time() - start_time) * 1000),\n-                        status=Status.COMPLETED,\n-                        accumulated_usage=usage,\n-                        accumulated_metrics=metrics,\n-                        execution_count=1,\n+            elif isinstance(node.executor, Agent):\n+                if self.node_timeout is not None:\n+                    agent_response = await asyncio.wait_for(\n+                        node.executor.invoke_async(node_input, invocation_state=invocation_state),\n+                        timeout=self.node_timeout,\n                     )\n                 else:\n-                    raise ValueError(f\"Node '{node.node_id}' of type '{type(node.executor)}' is not supported\")\n-\n-            except asyncio.TimeoutError:\n-                timeout_msg = f\"Node '{node.node_id}' execution timed out after {self.node_timeout}s\"\n-                logger.exception(\n-                    \"node=<%s>, timeout=<%s>s | node execution timed out after timeout\",\n-                    node.node_id,\n-                    self.node_timeout,\n+                    agent_response = await node.executor.invoke_async(node_input, invocation_state=invocation_state)\n+\n+                # Extract metrics from agent response\n+                usage = Usage(inputTokens=0, outputTokens=0, totalTokens=0)\n+                metrics = Metrics(latencyMs=0)\n+                if hasattr(agent_response, \"metrics\") and agent_response.metrics:\n+                    if hasattr(agent_response.metrics, \"accumulated_usage\"):\n+                        usage = agent_response.metrics.accumulated_usage\n+                    if hasattr(agent_response.metrics, \"accumulated_metrics\"):\n+                        metrics = agent_response.metrics.accumulated_metrics\n+\n+                node_result = NodeResult(\n+                    result=agent_response,  # type is AgentResult\n+                    execution_time=round((time.time() - start_time) * 1000),\n+                    status=Status.COMPLETED,\n+                    accumulated_usage=usage,\n+                    accumulated_metrics=metrics,\n+                    execution_count=1,\n                 )\n-                raise Exception(timeout_msg) from None\n-\n-            # Mark as completed\n-            node.execution_status = Status.COMPLETED\n-            node.result = node_result\n-            node.execution_time = node_result.execution_time\n-            self.state.completed_nodes.add(node)\n-            self.state.results[node.node_id] = node_result\n-            self.state.execution_order.append(node)\n-\n-            # Accumulate metrics\n-            self._accumulate_metrics(node_result)\n+            else:\n+                raise ValueError(f\"Node '{node.node_id}' of type '{type(node.executor)}' is not supported\")\n+\n+            async with self._lock:",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2436817525",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 900,
        "pr_file": "src/strands/multiagent/graph.py",
        "discussion_id": "2436817525",
        "commented_code": "@@ -547,101 +627,83 @@ async def _execute_node(self, node: GraphNode, invocation_state: dict[str, Any])\n             # Build node input from satisfied dependencies\n             node_input = self._build_node_input(node)\n \n-            # Execute with timeout protection (only if node_timeout is set)\n-            try:\n-                # Execute based on node type and create unified NodeResult\n-                if isinstance(node.executor, MultiAgentBase):\n-                    if self.node_timeout is not None:\n-                        multi_agent_result = await asyncio.wait_for(\n-                            node.executor.invoke_async(node_input, invocation_state),\n-                            timeout=self.node_timeout,\n-                        )\n-                    else:\n-                        multi_agent_result = await node.executor.invoke_async(node_input, invocation_state)\n-\n-                    # Create NodeResult with MultiAgentResult directly\n-                    node_result = NodeResult(\n-                        result=multi_agent_result,  # type is MultiAgentResult\n-                        execution_time=multi_agent_result.execution_time,\n-                        status=Status.COMPLETED,\n-                        accumulated_usage=multi_agent_result.accumulated_usage,\n-                        accumulated_metrics=multi_agent_result.accumulated_metrics,\n-                        execution_count=multi_agent_result.execution_count,\n+            # Execute based on node type and create unified NodeResult\n+            if isinstance(node.executor, MultiAgentBase):\n+                if self.node_timeout is not None:\n+                    multiagent_result = await asyncio.wait_for(\n+                        node.executor.invoke_async(node_input, invocation_state),\n+                        timeout=self.node_timeout,\n                     )\n+                else:\n+                    multiagent_result = await node.executor.invoke_async(node_input, invocation_state)\n+\n+                # Create NodeResult with MultiAgentResult directly\n+                node_result = NodeResult(\n+                    result=multiagent_result,  # type is MultiAgentResult\n+                    execution_time=multiagent_result.execution_time,\n+                    status=Status.COMPLETED,\n+                    accumulated_usage=multiagent_result.accumulated_usage,\n+                    accumulated_metrics=multiagent_result.accumulated_metrics,\n+                    execution_count=multiagent_result.execution_count,\n+                )\n \n-                elif isinstance(node.executor, Agent):\n-                    if self.node_timeout is not None:\n-                        agent_response = await asyncio.wait_for(\n-                            node.executor.invoke_async(node_input, invocation_state=invocation_state),\n-                            timeout=self.node_timeout,\n-                        )\n-                    else:\n-                        agent_response = await node.executor.invoke_async(node_input, invocation_state=invocation_state)\n-\n-                    # Extract metrics from agent response\n-                    usage = Usage(inputTokens=0, outputTokens=0, totalTokens=0)\n-                    metrics = Metrics(latencyMs=0)\n-                    if hasattr(agent_response, \"metrics\") and agent_response.metrics:\n-                        if hasattr(agent_response.metrics, \"accumulated_usage\"):\n-                            usage = agent_response.metrics.accumulated_usage\n-                        if hasattr(agent_response.metrics, \"accumulated_metrics\"):\n-                            metrics = agent_response.metrics.accumulated_metrics\n-\n-                    node_result = NodeResult(\n-                        result=agent_response,  # type is AgentResult\n-                        execution_time=round((time.time() - start_time) * 1000),\n-                        status=Status.COMPLETED,\n-                        accumulated_usage=usage,\n-                        accumulated_metrics=metrics,\n-                        execution_count=1,\n+            elif isinstance(node.executor, Agent):\n+                if self.node_timeout is not None:\n+                    agent_response = await asyncio.wait_for(\n+                        node.executor.invoke_async(node_input, invocation_state=invocation_state),\n+                        timeout=self.node_timeout,\n                     )\n                 else:\n-                    raise ValueError(f\"Node '{node.node_id}' of type '{type(node.executor)}' is not supported\")\n-\n-            except asyncio.TimeoutError:\n-                timeout_msg = f\"Node '{node.node_id}' execution timed out after {self.node_timeout}s\"\n-                logger.exception(\n-                    \"node=<%s>, timeout=<%s>s | node execution timed out after timeout\",\n-                    node.node_id,\n-                    self.node_timeout,\n+                    agent_response = await node.executor.invoke_async(node_input, invocation_state=invocation_state)\n+\n+                # Extract metrics from agent response\n+                usage = Usage(inputTokens=0, outputTokens=0, totalTokens=0)\n+                metrics = Metrics(latencyMs=0)\n+                if hasattr(agent_response, \"metrics\") and agent_response.metrics:\n+                    if hasattr(agent_response.metrics, \"accumulated_usage\"):\n+                        usage = agent_response.metrics.accumulated_usage\n+                    if hasattr(agent_response.metrics, \"accumulated_metrics\"):\n+                        metrics = agent_response.metrics.accumulated_metrics\n+\n+                node_result = NodeResult(\n+                    result=agent_response,  # type is AgentResult\n+                    execution_time=round((time.time() - start_time) * 1000),\n+                    status=Status.COMPLETED,\n+                    accumulated_usage=usage,\n+                    accumulated_metrics=metrics,\n+                    execution_count=1,\n                 )\n-                raise Exception(timeout_msg) from None\n-\n-            # Mark as completed\n-            node.execution_status = Status.COMPLETED\n-            node.result = node_result\n-            node.execution_time = node_result.execution_time\n-            self.state.completed_nodes.add(node)\n-            self.state.results[node.node_id] = node_result\n-            self.state.execution_order.append(node)\n-\n-            # Accumulate metrics\n-            self._accumulate_metrics(node_result)\n+            else:\n+                raise ValueError(f\"Node '{node.node_id}' of type '{type(node.executor)}' is not supported\")\n+\n+            async with self._lock:",
        "comment_created_at": "2025-10-16T17:23:24+00:00",
        "comment_author": "pgrayy",
        "comment_body": "Lock is actually not necessary here in graph. Unlike threads, coroutines run one at a time, but in a cooperative manner. Because of this, there won't be any issues having them write to the same state. Technically speaking, even if you were using threads here, you wouldn't need a lock because of the GIL.\r\n\r\nIf you are interested to learn more though, you can read about a use case for asyncio.Lock at https://stackoverflow.com/questions/25799576/whats-python-asyncio-lock-for.",
        "pr_file_module": null
      },
      {
        "comment_id": "2440894767",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 900,
        "pr_file": "src/strands/multiagent/graph.py",
        "discussion_id": "2436817525",
        "commented_code": "@@ -547,101 +627,83 @@ async def _execute_node(self, node: GraphNode, invocation_state: dict[str, Any])\n             # Build node input from satisfied dependencies\n             node_input = self._build_node_input(node)\n \n-            # Execute with timeout protection (only if node_timeout is set)\n-            try:\n-                # Execute based on node type and create unified NodeResult\n-                if isinstance(node.executor, MultiAgentBase):\n-                    if self.node_timeout is not None:\n-                        multi_agent_result = await asyncio.wait_for(\n-                            node.executor.invoke_async(node_input, invocation_state),\n-                            timeout=self.node_timeout,\n-                        )\n-                    else:\n-                        multi_agent_result = await node.executor.invoke_async(node_input, invocation_state)\n-\n-                    # Create NodeResult with MultiAgentResult directly\n-                    node_result = NodeResult(\n-                        result=multi_agent_result,  # type is MultiAgentResult\n-                        execution_time=multi_agent_result.execution_time,\n-                        status=Status.COMPLETED,\n-                        accumulated_usage=multi_agent_result.accumulated_usage,\n-                        accumulated_metrics=multi_agent_result.accumulated_metrics,\n-                        execution_count=multi_agent_result.execution_count,\n+            # Execute based on node type and create unified NodeResult\n+            if isinstance(node.executor, MultiAgentBase):\n+                if self.node_timeout is not None:\n+                    multiagent_result = await asyncio.wait_for(\n+                        node.executor.invoke_async(node_input, invocation_state),\n+                        timeout=self.node_timeout,\n                     )\n+                else:\n+                    multiagent_result = await node.executor.invoke_async(node_input, invocation_state)\n+\n+                # Create NodeResult with MultiAgentResult directly\n+                node_result = NodeResult(\n+                    result=multiagent_result,  # type is MultiAgentResult\n+                    execution_time=multiagent_result.execution_time,\n+                    status=Status.COMPLETED,\n+                    accumulated_usage=multiagent_result.accumulated_usage,\n+                    accumulated_metrics=multiagent_result.accumulated_metrics,\n+                    execution_count=multiagent_result.execution_count,\n+                )\n \n-                elif isinstance(node.executor, Agent):\n-                    if self.node_timeout is not None:\n-                        agent_response = await asyncio.wait_for(\n-                            node.executor.invoke_async(node_input, invocation_state=invocation_state),\n-                            timeout=self.node_timeout,\n-                        )\n-                    else:\n-                        agent_response = await node.executor.invoke_async(node_input, invocation_state=invocation_state)\n-\n-                    # Extract metrics from agent response\n-                    usage = Usage(inputTokens=0, outputTokens=0, totalTokens=0)\n-                    metrics = Metrics(latencyMs=0)\n-                    if hasattr(agent_response, \"metrics\") and agent_response.metrics:\n-                        if hasattr(agent_response.metrics, \"accumulated_usage\"):\n-                            usage = agent_response.metrics.accumulated_usage\n-                        if hasattr(agent_response.metrics, \"accumulated_metrics\"):\n-                            metrics = agent_response.metrics.accumulated_metrics\n-\n-                    node_result = NodeResult(\n-                        result=agent_response,  # type is AgentResult\n-                        execution_time=round((time.time() - start_time) * 1000),\n-                        status=Status.COMPLETED,\n-                        accumulated_usage=usage,\n-                        accumulated_metrics=metrics,\n-                        execution_count=1,\n+            elif isinstance(node.executor, Agent):\n+                if self.node_timeout is not None:\n+                    agent_response = await asyncio.wait_for(\n+                        node.executor.invoke_async(node_input, invocation_state=invocation_state),\n+                        timeout=self.node_timeout,\n                     )\n                 else:\n-                    raise ValueError(f\"Node '{node.node_id}' of type '{type(node.executor)}' is not supported\")\n-\n-            except asyncio.TimeoutError:\n-                timeout_msg = f\"Node '{node.node_id}' execution timed out after {self.node_timeout}s\"\n-                logger.exception(\n-                    \"node=<%s>, timeout=<%s>s | node execution timed out after timeout\",\n-                    node.node_id,\n-                    self.node_timeout,\n+                    agent_response = await node.executor.invoke_async(node_input, invocation_state=invocation_state)\n+\n+                # Extract metrics from agent response\n+                usage = Usage(inputTokens=0, outputTokens=0, totalTokens=0)\n+                metrics = Metrics(latencyMs=0)\n+                if hasattr(agent_response, \"metrics\") and agent_response.metrics:\n+                    if hasattr(agent_response.metrics, \"accumulated_usage\"):\n+                        usage = agent_response.metrics.accumulated_usage\n+                    if hasattr(agent_response.metrics, \"accumulated_metrics\"):\n+                        metrics = agent_response.metrics.accumulated_metrics\n+\n+                node_result = NodeResult(\n+                    result=agent_response,  # type is AgentResult\n+                    execution_time=round((time.time() - start_time) * 1000),\n+                    status=Status.COMPLETED,\n+                    accumulated_usage=usage,\n+                    accumulated_metrics=metrics,\n+                    execution_count=1,\n                 )\n-                raise Exception(timeout_msg) from None\n-\n-            # Mark as completed\n-            node.execution_status = Status.COMPLETED\n-            node.result = node_result\n-            node.execution_time = node_result.execution_time\n-            self.state.completed_nodes.add(node)\n-            self.state.results[node.node_id] = node_result\n-            self.state.execution_order.append(node)\n-\n-            # Accumulate metrics\n-            self._accumulate_metrics(node_result)\n+            else:\n+                raise ValueError(f\"Node '{node.node_id}' of type '{type(node.executor)}' is not supported\")\n+\n+            async with self._lock:",
        "comment_created_at": "2025-10-17T18:43:29+00:00",
        "comment_author": "JackYPCOnline",
        "comment_body": "remove the lock",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2445873758",
    "pr_number": 900,
    "pr_file": "src/strands/session/session_manager.py",
    "created_at": "2025-10-20T19:10:16+00:00",
    "commented_code": "agent: Agent to initialize\n             **kwargs: Additional keyword arguments for future extensibility.\n         \"\"\"\n+\n+    def _persist_multi_agent_state(self, source: \"MultiAgentBase\") -> None:\n+        \"\"\"Thread-safe persistence of multi-agent state.\n+\n+        Args:\n+            source: Multi-agent orchestrator to persist\n+        \"\"\"\n+        with self._lock:",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2445873758",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 900,
        "pr_file": "src/strands/session/session_manager.py",
        "discussion_id": "2445873758",
        "commented_code": "@@ -71,3 +99,50 @@ def initialize(self, agent: \"Agent\", **kwargs: Any) -> None:\n             agent: Agent to initialize\n             **kwargs: Additional keyword arguments for future extensibility.\n         \"\"\"\n+\n+    def _persist_multi_agent_state(self, source: \"MultiAgentBase\") -> None:\n+        \"\"\"Thread-safe persistence of multi-agent state.\n+\n+        Args:\n+            source: Multi-agent orchestrator to persist\n+        \"\"\"\n+        with self._lock:",
        "comment_created_at": "2025-10-20T19:10:16+00:00",
        "comment_author": "dbschmigelski",
        "comment_body": "do we still need this lock if hooks execute sequentially and on the same main thread?",
        "pr_file_module": null
      },
      {
        "comment_id": "2446060891",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 900,
        "pr_file": "src/strands/session/session_manager.py",
        "discussion_id": "2445873758",
        "commented_code": "@@ -71,3 +99,50 @@ def initialize(self, agent: \"Agent\", **kwargs: Any) -> None:\n             agent: Agent to initialize\n             **kwargs: Additional keyword arguments for future extensibility.\n         \"\"\"\n+\n+    def _persist_multi_agent_state(self, source: \"MultiAgentBase\") -> None:\n+        \"\"\"Thread-safe persistence of multi-agent state.\n+\n+        Args:\n+            source: Multi-agent orchestrator to persist\n+        \"\"\"\n+        with self._lock:",
        "comment_created_at": "2025-10-20T20:34:32+00:00",
        "comment_author": "JackYPCOnline",
        "comment_body": "Removed.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2392143951",
    "pr_number": 924,
    "pr_file": "src/strands/experimental/bidirectional_streaming/event_loop/bidirectional_event_loop.py",
    "created_at": "2025-09-30T16:08:59+00:00",
    "commented_code": "+\"\"\"Bidirectional session management for concurrent streaming conversations.\n+\n+Manages bidirectional communication sessions with concurrent processing of model events,\n+tool execution, and audio processing. Provides coordination between background tasks\n+while maintaining a simple interface for agent interaction.\n+\n+Features:\n+- Concurrent task management for model events and tool execution\n+- Interruption handling with audio buffer clearing\n+- Tool execution with cancellation support\n+- Session lifecycle management\n+\"\"\"\n+\n+import asyncio\n+import json\n+import logging\n+import traceback\n+import uuid\n+from typing import Any, Dict\n+\n+from strands.tools._validator import validate_and_prepare_tools\n+from strands.types.content import Message\n+from strands.types.tools import ToolResult, ToolUse\n+\n+from ..models.bidirectional_model import BidirectionalModelSession\n+from ..utils.debug import log_event, log_flow\n+\n+logger = logging.getLogger(__name__)\n+\n+# Session constants\n+TOOL_QUEUE_TIMEOUT = 0.5\n+SUPERVISION_INTERVAL = 0.1\n+\n+\n+class BidirectionalConnection:\n+    \"\"\"Session wrapper for bidirectional communication with concurrent task management.\n+    \n+    Coordinates background tasks for model event processing, tool execution, and audio\n+    handling while providing a simple interface for agent interactions.\n+    \"\"\"\n+    \n+    def __init__(self, model_session: BidirectionalModelSession, agent):\n+        \"\"\"Initialize session with model session and agent reference.\n+        \n+        Args:\n+            model_session: Provider-specific bidirectional model session.\n+            agent: BidirectionalAgent instance for tool registry access.\n+        \"\"\"\n+        self.model_session = model_session\n+        self.agent = agent\n+        self.active = True\n+        \n+        # Background processing coordination\n+        self.background_tasks = []\n+        self.tool_queue = asyncio.Queue()\n+        self.audio_output_queue = asyncio.Queue()\n+        \n+        # Task management for cleanup\n+        self.pending_tool_tasks: Dict[str, asyncio.Task] = {}\n+        \n+        # Interruption handling (model-agnostic)\n+        self.interrupted = False\n+\n+async def start_bidirectional_connection(agent) -> BidirectionalConnection:\n+    \"\"\"Initialize bidirectional session with concurrent background tasks.\n+    \n+    Creates a model-specific session and starts background tasks for processing\n+    model events, executing tools, and managing the session lifecycle.\n+    \n+    Args:\n+        agent: BidirectionalAgent instance.\n+        \n+    Returns:\n+        BidirectionalConnection: Active session with background tasks running.\n+    \"\"\"    \n+    log_flow(\"session_start\", \"initializing model session\")\n+    \n+    # Create provider-specific session\n+    model_session = await agent.model.create_bidirectional_connection(\n+        system_prompt=agent.system_prompt,\n+        tools=agent.tool_registry.get_all_tool_specs(),\n+        messages=agent.messages\n+    )\n+    \n+    # Create session wrapper for background processing\n+    session = BidirectionalConnection(model_session=model_session, agent=agent)\n+    \n+    # Start concurrent background processors IMMEDIATELY after session creation\n+    # This is critical - Nova Sonic needs response processing during initialization\n+    log_flow(\"background_tasks\", \"starting processors\")\n+    session.background_tasks = [\n+        asyncio.create_task(_process_model_events(session)),    # Handle model responses\n+        asyncio.create_task(_process_tool_execution(session))   # Execute tools concurrently\n+    ]\n+    \n+    # Start main coordination cycle\n+    session.main_cycle_task = asyncio.create_task(\n+        bidirectional_event_loop_cycle(session)\n+    )\n+    \n+    # Give background tasks a moment to start\n+    await asyncio.sleep(0.1)\n+    log_event(\"session_ready\", tasks=len(session.background_tasks))\n+    \n+    return session\n+\n+\n+async def stop_bidirectional_connection(session: BidirectionalConnection) -> None:\n+    \"\"\"End session and cleanup resources including background tasks.\n+    \n+    Args:\n+        session: BidirectionalConnection to cleanup.\n+    \"\"\"\n+    if not session.active:\n+        return\n+    \n+    log_flow(\"session_cleanup\", \"starting\")\n+    session.active = False\n+    \n+    # Cancel pending tool tasks\n+    for _, task in session.pending_tool_tasks.items():\n+        if not task.done():\n+            task.cancel()\n+    \n+    # Cancel background tasks\n+    for task in session.background_tasks:\n+        if not task.done():\n+            task.cancel()\n+    \n+    # Cancel main cycle task\n+    if hasattr(session, 'main_cycle_task') and not session.main_cycle_task.done():\n+        session.main_cycle_task.cancel()\n+    \n+    # Wait for tasks to complete\n+    all_tasks = session.background_tasks + list(session.pending_tool_tasks.values())\n+    if hasattr(session, 'main_cycle_task'):\n+        all_tasks.append(session.main_cycle_task)\n+    \n+    if all_tasks:\n+        await asyncio.gather(*all_tasks, return_exceptions=True)\n+    \n+    # Close model session\n+    await session.model_session.close()\n+    log_event(\"session_closed\")\n+\n+\n+async def bidirectional_event_loop_cycle(session: BidirectionalConnection) -> None:\n+    \"\"\"Main event loop coordinator that runs continuously during the session.\n+    \n+    Monitors background tasks, manages session state, and handles session lifecycle.\n+    Provides supervision for concurrent model event processing and tool execution.\n+    \n+    Args:\n+        session: BidirectionalConnection to coordinate.\n+    \"\"\"\n+    while session.active:\n+        try:\n+            # Check if background processors are still running\n+            if all(task.done() for task in session.background_tasks):\n+                log_event(\"session_end\", reason=\"all_processors_completed\")\n+                session.active = False\n+                break\n+            \n+            # Check for failed background tasks\n+            for i, task in enumerate(session.background_tasks):\n+                if task.done() and not task.cancelled():\n+                    exception = task.exception()\n+                    if exception:\n+                        log_event(\"session_error\", processor=i, error=str(exception))\n+                        session.active = False\n+                        raise exception\n+            \n+            # Brief pause before next supervision check\n+            await asyncio.sleep(SUPERVISION_INTERVAL)\n+            \n+        except asyncio.CancelledError:\n+            break\n+        except Exception as e:\n+            log_event(\"event_loop_error\", error=str(e))\n+            session.active = False\n+            raise\n+\n+\n+async def _handle_interruption(session: BidirectionalConnection) -> None:\n+    \"\"\"Handle interruption detection with task cancellation and audio buffer clearing.\n+    \n+    Cancels pending tool tasks and clears audio output queues to ensure responsive\n+    interruption handling during conversations.\n+    \n+    Args:\n+        session: BidirectionalConnection to handle interruption for.\n+    \"\"\"\n+    log_event(\"interruption_detected\")\n+    session.interrupted = True",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2392143951",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 924,
        "pr_file": "src/strands/experimental/bidirectional_streaming/event_loop/bidirectional_event_loop.py",
        "discussion_id": "2392143951",
        "commented_code": "@@ -0,0 +1,535 @@\n+\"\"\"Bidirectional session management for concurrent streaming conversations.\n+\n+Manages bidirectional communication sessions with concurrent processing of model events,\n+tool execution, and audio processing. Provides coordination between background tasks\n+while maintaining a simple interface for agent interaction.\n+\n+Features:\n+- Concurrent task management for model events and tool execution\n+- Interruption handling with audio buffer clearing\n+- Tool execution with cancellation support\n+- Session lifecycle management\n+\"\"\"\n+\n+import asyncio\n+import json\n+import logging\n+import traceback\n+import uuid\n+from typing import Any, Dict\n+\n+from strands.tools._validator import validate_and_prepare_tools\n+from strands.types.content import Message\n+from strands.types.tools import ToolResult, ToolUse\n+\n+from ..models.bidirectional_model import BidirectionalModelSession\n+from ..utils.debug import log_event, log_flow\n+\n+logger = logging.getLogger(__name__)\n+\n+# Session constants\n+TOOL_QUEUE_TIMEOUT = 0.5\n+SUPERVISION_INTERVAL = 0.1\n+\n+\n+class BidirectionalConnection:\n+    \"\"\"Session wrapper for bidirectional communication with concurrent task management.\n+    \n+    Coordinates background tasks for model event processing, tool execution, and audio\n+    handling while providing a simple interface for agent interactions.\n+    \"\"\"\n+    \n+    def __init__(self, model_session: BidirectionalModelSession, agent):\n+        \"\"\"Initialize session with model session and agent reference.\n+        \n+        Args:\n+            model_session: Provider-specific bidirectional model session.\n+            agent: BidirectionalAgent instance for tool registry access.\n+        \"\"\"\n+        self.model_session = model_session\n+        self.agent = agent\n+        self.active = True\n+        \n+        # Background processing coordination\n+        self.background_tasks = []\n+        self.tool_queue = asyncio.Queue()\n+        self.audio_output_queue = asyncio.Queue()\n+        \n+        # Task management for cleanup\n+        self.pending_tool_tasks: Dict[str, asyncio.Task] = {}\n+        \n+        # Interruption handling (model-agnostic)\n+        self.interrupted = False\n+\n+async def start_bidirectional_connection(agent) -> BidirectionalConnection:\n+    \"\"\"Initialize bidirectional session with concurrent background tasks.\n+    \n+    Creates a model-specific session and starts background tasks for processing\n+    model events, executing tools, and managing the session lifecycle.\n+    \n+    Args:\n+        agent: BidirectionalAgent instance.\n+        \n+    Returns:\n+        BidirectionalConnection: Active session with background tasks running.\n+    \"\"\"    \n+    log_flow(\"session_start\", \"initializing model session\")\n+    \n+    # Create provider-specific session\n+    model_session = await agent.model.create_bidirectional_connection(\n+        system_prompt=agent.system_prompt,\n+        tools=agent.tool_registry.get_all_tool_specs(),\n+        messages=agent.messages\n+    )\n+    \n+    # Create session wrapper for background processing\n+    session = BidirectionalConnection(model_session=model_session, agent=agent)\n+    \n+    # Start concurrent background processors IMMEDIATELY after session creation\n+    # This is critical - Nova Sonic needs response processing during initialization\n+    log_flow(\"background_tasks\", \"starting processors\")\n+    session.background_tasks = [\n+        asyncio.create_task(_process_model_events(session)),    # Handle model responses\n+        asyncio.create_task(_process_tool_execution(session))   # Execute tools concurrently\n+    ]\n+    \n+    # Start main coordination cycle\n+    session.main_cycle_task = asyncio.create_task(\n+        bidirectional_event_loop_cycle(session)\n+    )\n+    \n+    # Give background tasks a moment to start\n+    await asyncio.sleep(0.1)\n+    log_event(\"session_ready\", tasks=len(session.background_tasks))\n+    \n+    return session\n+\n+\n+async def stop_bidirectional_connection(session: BidirectionalConnection) -> None:\n+    \"\"\"End session and cleanup resources including background tasks.\n+    \n+    Args:\n+        session: BidirectionalConnection to cleanup.\n+    \"\"\"\n+    if not session.active:\n+        return\n+    \n+    log_flow(\"session_cleanup\", \"starting\")\n+    session.active = False\n+    \n+    # Cancel pending tool tasks\n+    for _, task in session.pending_tool_tasks.items():\n+        if not task.done():\n+            task.cancel()\n+    \n+    # Cancel background tasks\n+    for task in session.background_tasks:\n+        if not task.done():\n+            task.cancel()\n+    \n+    # Cancel main cycle task\n+    if hasattr(session, 'main_cycle_task') and not session.main_cycle_task.done():\n+        session.main_cycle_task.cancel()\n+    \n+    # Wait for tasks to complete\n+    all_tasks = session.background_tasks + list(session.pending_tool_tasks.values())\n+    if hasattr(session, 'main_cycle_task'):\n+        all_tasks.append(session.main_cycle_task)\n+    \n+    if all_tasks:\n+        await asyncio.gather(*all_tasks, return_exceptions=True)\n+    \n+    # Close model session\n+    await session.model_session.close()\n+    log_event(\"session_closed\")\n+\n+\n+async def bidirectional_event_loop_cycle(session: BidirectionalConnection) -> None:\n+    \"\"\"Main event loop coordinator that runs continuously during the session.\n+    \n+    Monitors background tasks, manages session state, and handles session lifecycle.\n+    Provides supervision for concurrent model event processing and tool execution.\n+    \n+    Args:\n+        session: BidirectionalConnection to coordinate.\n+    \"\"\"\n+    while session.active:\n+        try:\n+            # Check if background processors are still running\n+            if all(task.done() for task in session.background_tasks):\n+                log_event(\"session_end\", reason=\"all_processors_completed\")\n+                session.active = False\n+                break\n+            \n+            # Check for failed background tasks\n+            for i, task in enumerate(session.background_tasks):\n+                if task.done() and not task.cancelled():\n+                    exception = task.exception()\n+                    if exception:\n+                        log_event(\"session_error\", processor=i, error=str(exception))\n+                        session.active = False\n+                        raise exception\n+            \n+            # Brief pause before next supervision check\n+            await asyncio.sleep(SUPERVISION_INTERVAL)\n+            \n+        except asyncio.CancelledError:\n+            break\n+        except Exception as e:\n+            log_event(\"event_loop_error\", error=str(e))\n+            session.active = False\n+            raise\n+\n+\n+async def _handle_interruption(session: BidirectionalConnection) -> None:\n+    \"\"\"Handle interruption detection with task cancellation and audio buffer clearing.\n+    \n+    Cancels pending tool tasks and clears audio output queues to ensure responsive\n+    interruption handling during conversations.\n+    \n+    Args:\n+        session: BidirectionalConnection to handle interruption for.\n+    \"\"\"\n+    log_event(\"interruption_detected\")\n+    session.interrupted = True",
        "comment_created_at": "2025-09-30T16:08:59+00:00",
        "comment_author": "Unshure",
        "comment_body": "Do we need to worry about race conditions here? Should we lock this method when its called so it doesnt execute twice at the same time?",
        "pr_file_module": null
      },
      {
        "comment_id": "2399767226",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 924,
        "pr_file": "src/strands/experimental/bidirectional_streaming/event_loop/bidirectional_event_loop.py",
        "discussion_id": "2392143951",
        "commented_code": "@@ -0,0 +1,535 @@\n+\"\"\"Bidirectional session management for concurrent streaming conversations.\n+\n+Manages bidirectional communication sessions with concurrent processing of model events,\n+tool execution, and audio processing. Provides coordination between background tasks\n+while maintaining a simple interface for agent interaction.\n+\n+Features:\n+- Concurrent task management for model events and tool execution\n+- Interruption handling with audio buffer clearing\n+- Tool execution with cancellation support\n+- Session lifecycle management\n+\"\"\"\n+\n+import asyncio\n+import json\n+import logging\n+import traceback\n+import uuid\n+from typing import Any, Dict\n+\n+from strands.tools._validator import validate_and_prepare_tools\n+from strands.types.content import Message\n+from strands.types.tools import ToolResult, ToolUse\n+\n+from ..models.bidirectional_model import BidirectionalModelSession\n+from ..utils.debug import log_event, log_flow\n+\n+logger = logging.getLogger(__name__)\n+\n+# Session constants\n+TOOL_QUEUE_TIMEOUT = 0.5\n+SUPERVISION_INTERVAL = 0.1\n+\n+\n+class BidirectionalConnection:\n+    \"\"\"Session wrapper for bidirectional communication with concurrent task management.\n+    \n+    Coordinates background tasks for model event processing, tool execution, and audio\n+    handling while providing a simple interface for agent interactions.\n+    \"\"\"\n+    \n+    def __init__(self, model_session: BidirectionalModelSession, agent):\n+        \"\"\"Initialize session with model session and agent reference.\n+        \n+        Args:\n+            model_session: Provider-specific bidirectional model session.\n+            agent: BidirectionalAgent instance for tool registry access.\n+        \"\"\"\n+        self.model_session = model_session\n+        self.agent = agent\n+        self.active = True\n+        \n+        # Background processing coordination\n+        self.background_tasks = []\n+        self.tool_queue = asyncio.Queue()\n+        self.audio_output_queue = asyncio.Queue()\n+        \n+        # Task management for cleanup\n+        self.pending_tool_tasks: Dict[str, asyncio.Task] = {}\n+        \n+        # Interruption handling (model-agnostic)\n+        self.interrupted = False\n+\n+async def start_bidirectional_connection(agent) -> BidirectionalConnection:\n+    \"\"\"Initialize bidirectional session with concurrent background tasks.\n+    \n+    Creates a model-specific session and starts background tasks for processing\n+    model events, executing tools, and managing the session lifecycle.\n+    \n+    Args:\n+        agent: BidirectionalAgent instance.\n+        \n+    Returns:\n+        BidirectionalConnection: Active session with background tasks running.\n+    \"\"\"    \n+    log_flow(\"session_start\", \"initializing model session\")\n+    \n+    # Create provider-specific session\n+    model_session = await agent.model.create_bidirectional_connection(\n+        system_prompt=agent.system_prompt,\n+        tools=agent.tool_registry.get_all_tool_specs(),\n+        messages=agent.messages\n+    )\n+    \n+    # Create session wrapper for background processing\n+    session = BidirectionalConnection(model_session=model_session, agent=agent)\n+    \n+    # Start concurrent background processors IMMEDIATELY after session creation\n+    # This is critical - Nova Sonic needs response processing during initialization\n+    log_flow(\"background_tasks\", \"starting processors\")\n+    session.background_tasks = [\n+        asyncio.create_task(_process_model_events(session)),    # Handle model responses\n+        asyncio.create_task(_process_tool_execution(session))   # Execute tools concurrently\n+    ]\n+    \n+    # Start main coordination cycle\n+    session.main_cycle_task = asyncio.create_task(\n+        bidirectional_event_loop_cycle(session)\n+    )\n+    \n+    # Give background tasks a moment to start\n+    await asyncio.sleep(0.1)\n+    log_event(\"session_ready\", tasks=len(session.background_tasks))\n+    \n+    return session\n+\n+\n+async def stop_bidirectional_connection(session: BidirectionalConnection) -> None:\n+    \"\"\"End session and cleanup resources including background tasks.\n+    \n+    Args:\n+        session: BidirectionalConnection to cleanup.\n+    \"\"\"\n+    if not session.active:\n+        return\n+    \n+    log_flow(\"session_cleanup\", \"starting\")\n+    session.active = False\n+    \n+    # Cancel pending tool tasks\n+    for _, task in session.pending_tool_tasks.items():\n+        if not task.done():\n+            task.cancel()\n+    \n+    # Cancel background tasks\n+    for task in session.background_tasks:\n+        if not task.done():\n+            task.cancel()\n+    \n+    # Cancel main cycle task\n+    if hasattr(session, 'main_cycle_task') and not session.main_cycle_task.done():\n+        session.main_cycle_task.cancel()\n+    \n+    # Wait for tasks to complete\n+    all_tasks = session.background_tasks + list(session.pending_tool_tasks.values())\n+    if hasattr(session, 'main_cycle_task'):\n+        all_tasks.append(session.main_cycle_task)\n+    \n+    if all_tasks:\n+        await asyncio.gather(*all_tasks, return_exceptions=True)\n+    \n+    # Close model session\n+    await session.model_session.close()\n+    log_event(\"session_closed\")\n+\n+\n+async def bidirectional_event_loop_cycle(session: BidirectionalConnection) -> None:\n+    \"\"\"Main event loop coordinator that runs continuously during the session.\n+    \n+    Monitors background tasks, manages session state, and handles session lifecycle.\n+    Provides supervision for concurrent model event processing and tool execution.\n+    \n+    Args:\n+        session: BidirectionalConnection to coordinate.\n+    \"\"\"\n+    while session.active:\n+        try:\n+            # Check if background processors are still running\n+            if all(task.done() for task in session.background_tasks):\n+                log_event(\"session_end\", reason=\"all_processors_completed\")\n+                session.active = False\n+                break\n+            \n+            # Check for failed background tasks\n+            for i, task in enumerate(session.background_tasks):\n+                if task.done() and not task.cancelled():\n+                    exception = task.exception()\n+                    if exception:\n+                        log_event(\"session_error\", processor=i, error=str(exception))\n+                        session.active = False\n+                        raise exception\n+            \n+            # Brief pause before next supervision check\n+            await asyncio.sleep(SUPERVISION_INTERVAL)\n+            \n+        except asyncio.CancelledError:\n+            break\n+        except Exception as e:\n+            log_event(\"event_loop_error\", error=str(e))\n+            session.active = False\n+            raise\n+\n+\n+async def _handle_interruption(session: BidirectionalConnection) -> None:\n+    \"\"\"Handle interruption detection with task cancellation and audio buffer clearing.\n+    \n+    Cancels pending tool tasks and clears audio output queues to ensure responsive\n+    interruption handling during conversations.\n+    \n+    Args:\n+        session: BidirectionalConnection to handle interruption for.\n+    \"\"\"\n+    log_event(\"interruption_detected\")\n+    session.interrupted = True",
        "comment_created_at": "2025-10-02T18:48:52+00:00",
        "comment_author": "mehtarac",
        "comment_body": "Absolutely, and it's a great callout. Modified the implementation to lock this method. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2501104484",
    "pr_number": 1146,
    "pr_file": "src/strands/_async.py",
    "created_at": "2025-11-06T22:52:49+00:00",
    "commented_code": "return asyncio.run(execute_async())\n \n     with ThreadPoolExecutor() as executor:\n-        future = executor.submit(execute)\n+        context = contextvars.copy_context()\n+        future = executor.submit(context.run, execute)",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2501104484",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 1146,
        "pr_file": "src/strands/_async.py",
        "discussion_id": "2501104484",
        "commented_code": "@@ -27,5 +28,6 @@ def execute() -> T:\n         return asyncio.run(execute_async())\n \n     with ThreadPoolExecutor() as executor:\n-        future = executor.submit(execute)\n+        context = contextvars.copy_context()\n+        future = executor.submit(context.run, execute)",
        "comment_created_at": "2025-11-06T22:52:49+00:00",
        "comment_author": "pgrayy",
        "comment_body": "I was initially hesitant to make this change because of the following scenario:\n* Customer defines a thread local contextvar in their sync tool func that is invoked by the agent, which itself is invoked synchronously.\n* Contextvars from the main thread are now copied into that tool func.\n* The tool func defines a contextvar with the same name as a contextvar in the main thread.\n* The contextvar in the main thread overwrites the contextvar in the tool thread.\n\nI think this is okay though because the use of threads is an internal implementation detail of Strands. Customers are not designing their tools with our use of threads in mind. Additionally, the use of threads here is specifically meant for asyncio event loop isolation, not context isolation. Thus, this is a bug we are fixing here.",
        "pr_file_module": null
      },
      {
        "comment_id": "2503882446",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 1146,
        "pr_file": "src/strands/_async.py",
        "discussion_id": "2501104484",
        "commented_code": "@@ -27,5 +28,6 @@ def execute() -> T:\n         return asyncio.run(execute_async())\n \n     with ThreadPoolExecutor() as executor:\n-        future = executor.submit(execute)\n+        context = contextvars.copy_context()\n+        future = executor.submit(context.run, execute)",
        "comment_created_at": "2025-11-07T14:37:00+00:00",
        "comment_author": "zastrowm",
        "comment_body": "This would be great in the PR description; I'd like to record the _why_ and thread comments tend to get \"resolved\".",
        "pr_file_module": null
      },
      {
        "comment_id": "2504098411",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 1146,
        "pr_file": "src/strands/_async.py",
        "discussion_id": "2501104484",
        "commented_code": "@@ -27,5 +28,6 @@ def execute() -> T:\n         return asyncio.run(execute_async())\n \n     with ThreadPoolExecutor() as executor:\n-        future = executor.submit(execute)\n+        context = contextvars.copy_context()\n+        future = executor.submit(context.run, execute)",
        "comment_created_at": "2025-11-07T15:17:43+00:00",
        "comment_author": "pgrayy",
        "comment_body": "Done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2248397657",
    "pr_number": 593,
    "pr_file": "src/strands/models/bedrock.py",
    "created_at": "2025-08-01T16:32:12+00:00",
    "commented_code": "ModelThrottledException: If the model service is throttling requests.\n         \"\"\"\n \n-        def callback(event: Optional[StreamEvent] = None) -> None:\n-            loop.call_soon_threadsafe(queue.put_nowait, event)\n-            if event is None:\n-                return\n+        def target() -> None:\n+            try:\n+                for event in self._stream(messages, tool_specs, system_prompt):\n+                    loop.call_soon_threadsafe(queue.put_nowait, event)\n+            except Exception as e:\n+                loop.call_soon_threadsafe(queue.put_nowait, e)\n+            finally:\n+                loop.call_soon_threadsafe(queue.put_nowait, None)\n \n         loop = asyncio.get_event_loop()\n-        queue: asyncio.Queue[Optional[StreamEvent]] = asyncio.Queue()\n+        queue: asyncio.Queue[StreamEvent | Exception | None] = asyncio.Queue()\n \n-        thread = asyncio.to_thread(self._stream, callback, messages, tool_specs, system_prompt)",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2248397657",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 593,
        "pr_file": "src/strands/models/bedrock.py",
        "discussion_id": "2248397657",
        "commented_code": "@@ -376,40 +377,41 @@ async def stream(\n             ModelThrottledException: If the model service is throttling requests.\n         \"\"\"\n \n-        def callback(event: Optional[StreamEvent] = None) -> None:\n-            loop.call_soon_threadsafe(queue.put_nowait, event)\n-            if event is None:\n-                return\n+        def target() -> None:\n+            try:\n+                for event in self._stream(messages, tool_specs, system_prompt):\n+                    loop.call_soon_threadsafe(queue.put_nowait, event)\n+            except Exception as e:\n+                loop.call_soon_threadsafe(queue.put_nowait, e)\n+            finally:\n+                loop.call_soon_threadsafe(queue.put_nowait, None)\n \n         loop = asyncio.get_event_loop()\n-        queue: asyncio.Queue[Optional[StreamEvent]] = asyncio.Queue()\n+        queue: asyncio.Queue[StreamEvent | Exception | None] = asyncio.Queue()\n \n-        thread = asyncio.to_thread(self._stream, callback, messages, tool_specs, system_prompt)",
        "comment_created_at": "2025-08-01T16:32:12+00:00",
        "comment_author": "pgrayy",
        "comment_body": "`asyncio.to_thread` uses a thread pool executor that is not directly configurable. Consequently, we have to create our own threads in order to control the daemon mode.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2248400062",
    "pr_number": 593,
    "pr_file": "src/strands/models/bedrock.py",
    "created_at": "2025-08-01T16:33:25+00:00",
    "commented_code": "ModelThrottledException: If the model service is throttling requests.\n         \"\"\"\n \n-        def callback(event: Optional[StreamEvent] = None) -> None:\n-            loop.call_soon_threadsafe(queue.put_nowait, event)\n-            if event is None:\n-                return\n+        def target() -> None:\n+            try:\n+                for event in self._stream(messages, tool_specs, system_prompt):\n+                    loop.call_soon_threadsafe(queue.put_nowait, event)\n+            except Exception as e:\n+                loop.call_soon_threadsafe(queue.put_nowait, e)\n+            finally:\n+                loop.call_soon_threadsafe(queue.put_nowait, None)\n \n         loop = asyncio.get_event_loop()\n-        queue: asyncio.Queue[Optional[StreamEvent]] = asyncio.Queue()\n+        queue: asyncio.Queue[StreamEvent | Exception | None] = asyncio.Queue()\n \n-        thread = asyncio.to_thread(self._stream, callback, messages, tool_specs, system_prompt)\n-        task = asyncio.create_task(thread)\n+        threading.Thread(target=target, daemon=True).start()",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2248400062",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 593,
        "pr_file": "src/strands/models/bedrock.py",
        "discussion_id": "2248400062",
        "commented_code": "@@ -376,40 +377,41 @@ async def stream(\n             ModelThrottledException: If the model service is throttling requests.\n         \"\"\"\n \n-        def callback(event: Optional[StreamEvent] = None) -> None:\n-            loop.call_soon_threadsafe(queue.put_nowait, event)\n-            if event is None:\n-                return\n+        def target() -> None:\n+            try:\n+                for event in self._stream(messages, tool_specs, system_prompt):\n+                    loop.call_soon_threadsafe(queue.put_nowait, event)\n+            except Exception as e:\n+                loop.call_soon_threadsafe(queue.put_nowait, e)\n+            finally:\n+                loop.call_soon_threadsafe(queue.put_nowait, None)\n \n         loop = asyncio.get_event_loop()\n-        queue: asyncio.Queue[Optional[StreamEvent]] = asyncio.Queue()\n+        queue: asyncio.Queue[StreamEvent | Exception | None] = asyncio.Queue()\n \n-        thread = asyncio.to_thread(self._stream, callback, messages, tool_specs, system_prompt)\n-        task = asyncio.create_task(thread)\n+        threading.Thread(target=target, daemon=True).start()",
        "comment_created_at": "2025-08-01T16:33:25+00:00",
        "comment_author": "pgrayy",
        "comment_body": "One down side here is that we create a new thread on every stream invocation rather than pull from a pool of existing threads. In this context however, the impact should be negligible as `bedrock.converse_stream` is a long running call.",
        "pr_file_module": null
      },
      {
        "comment_id": "2248594737",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 593,
        "pr_file": "src/strands/models/bedrock.py",
        "discussion_id": "2248400062",
        "commented_code": "@@ -376,40 +377,41 @@ async def stream(\n             ModelThrottledException: If the model service is throttling requests.\n         \"\"\"\n \n-        def callback(event: Optional[StreamEvent] = None) -> None:\n-            loop.call_soon_threadsafe(queue.put_nowait, event)\n-            if event is None:\n-                return\n+        def target() -> None:\n+            try:\n+                for event in self._stream(messages, tool_specs, system_prompt):\n+                    loop.call_soon_threadsafe(queue.put_nowait, event)\n+            except Exception as e:\n+                loop.call_soon_threadsafe(queue.put_nowait, e)\n+            finally:\n+                loop.call_soon_threadsafe(queue.put_nowait, None)\n \n         loop = asyncio.get_event_loop()\n-        queue: asyncio.Queue[Optional[StreamEvent]] = asyncio.Queue()\n+        queue: asyncio.Queue[StreamEvent | Exception | None] = asyncio.Queue()\n \n-        thread = asyncio.to_thread(self._stream, callback, messages, tool_specs, system_prompt)\n-        task = asyncio.create_task(thread)\n+        threading.Thread(target=target, daemon=True).start()",
        "comment_created_at": "2025-08-01T18:25:37+00:00",
        "comment_author": "dbschmigelski",
        "comment_body": "I'm not as much concerned about the cost (time/space) of creating the new thread. But for highly concurrent use cases this concerns me because of the failure mode.\n\nIn the previous case it seems we would have waited until a thread became in the ThreadPoolExecutor allowing the request to succeed. In the current case we are introducing a new thread exhaustion failure mode.",
        "pr_file_module": null
      },
      {
        "comment_id": "2248613964",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 593,
        "pr_file": "src/strands/models/bedrock.py",
        "discussion_id": "2248400062",
        "commented_code": "@@ -376,40 +377,41 @@ async def stream(\n             ModelThrottledException: If the model service is throttling requests.\n         \"\"\"\n \n-        def callback(event: Optional[StreamEvent] = None) -> None:\n-            loop.call_soon_threadsafe(queue.put_nowait, event)\n-            if event is None:\n-                return\n+        def target() -> None:\n+            try:\n+                for event in self._stream(messages, tool_specs, system_prompt):\n+                    loop.call_soon_threadsafe(queue.put_nowait, event)\n+            except Exception as e:\n+                loop.call_soon_threadsafe(queue.put_nowait, e)\n+            finally:\n+                loop.call_soon_threadsafe(queue.put_nowait, None)\n \n         loop = asyncio.get_event_loop()\n-        queue: asyncio.Queue[Optional[StreamEvent]] = asyncio.Queue()\n+        queue: asyncio.Queue[StreamEvent | Exception | None] = asyncio.Queue()\n \n-        thread = asyncio.to_thread(self._stream, callback, messages, tool_specs, system_prompt)\n-        task = asyncio.create_task(thread)\n+        threading.Thread(target=target, daemon=True).start()",
        "comment_created_at": "2025-08-01T18:35:36+00:00",
        "comment_author": "pgrayy",
        "comment_body": "This is a really good call out and should be thought through carefully. I'll note that I avoided use of a ThreadPoolExecutor here because there is no easy way to configure daemon mode. We would have to derive our own version with some overrides.",
        "pr_file_module": null
      }
    ]
  }
]