[
  {
    "discussion_id": "2487656935",
    "pr_number": 1123,
    "pr_file": "tests/strands/agent/test_agent.py",
    "created_at": "2025-11-03T19:44:34+00:00",
    "commented_code": "agent = Agent()\n     result = agent._redact_user_content(content, \"REDACTED\")\n     assert result == expected\n+\n+\n+def test_agent_fixes_orphaned_tool_use_on_new_prompt(mock_model, agenerator):\n+    \"\"\"Test that agent adds toolResult for orphaned toolUse when called with new prompt.\"\"\"\n+    mock_model.mock_stream.return_value = agenerator(\n+        [\n+            {\"messageStart\": {\"role\": \"assistant\"}},\n+            {\"contentBlockStart\": {\"start\": {\"text\": \"\"}}},\n+            {\"contentBlockDelta\": {\"delta\": {\"text\": \"Fixed!\"}}},\n+            {\"contentBlockStop\": {}},\n+            {\"messageStop\": {\"stopReason\": \"end_turn\"}},\n+        ]\n+    )\n+\n+    # Start with orphaned toolUse message\n+    messages = [\n+        {\n+            \"role\": \"assistant\",\n+            \"content\": [\n+                {\"toolUse\": {\"toolUseId\": \"orphaned-123\", \"name\": \"tool_decorated\", \"input\": {\"random_string\": \"test\"}}}\n+            ],\n+        }\n+    ]\n+\n+    agent = Agent(model=mock_model, messages=messages)\n+\n+    # Call with new prompt should fix orphaned toolUse\n+    agent(\"Continue conversation\")\n+\n+    # Should have added toolResult message\n+    assert len(agent.messages) >= 3\n+    assert agent.messages[1][\"role\"] == \"user\"\n+    assert \"toolResult\" in agent.messages[1][\"content\"][0]\n+    assert agent.messages[1][\"content\"][0][\"toolResult\"][\"toolUseId\"] == \"orphaned-123\"",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2487656935",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 1123,
        "pr_file": "tests/strands/agent/test_agent.py",
        "discussion_id": "2487656935",
        "commented_code": "@@ -2215,3 +2215,125 @@ def test_redact_user_content(content, expected):\n     agent = Agent()\n     result = agent._redact_user_content(content, \"REDACTED\")\n     assert result == expected\n+\n+\n+def test_agent_fixes_orphaned_tool_use_on_new_prompt(mock_model, agenerator):\n+    \"\"\"Test that agent adds toolResult for orphaned toolUse when called with new prompt.\"\"\"\n+    mock_model.mock_stream.return_value = agenerator(\n+        [\n+            {\"messageStart\": {\"role\": \"assistant\"}},\n+            {\"contentBlockStart\": {\"start\": {\"text\": \"\"}}},\n+            {\"contentBlockDelta\": {\"delta\": {\"text\": \"Fixed!\"}}},\n+            {\"contentBlockStop\": {}},\n+            {\"messageStop\": {\"stopReason\": \"end_turn\"}},\n+        ]\n+    )\n+\n+    # Start with orphaned toolUse message\n+    messages = [\n+        {\n+            \"role\": \"assistant\",\n+            \"content\": [\n+                {\"toolUse\": {\"toolUseId\": \"orphaned-123\", \"name\": \"tool_decorated\", \"input\": {\"random_string\": \"test\"}}}\n+            ],\n+        }\n+    ]\n+\n+    agent = Agent(model=mock_model, messages=messages)\n+\n+    # Call with new prompt should fix orphaned toolUse\n+    agent(\"Continue conversation\")\n+\n+    # Should have added toolResult message\n+    assert len(agent.messages) >= 3\n+    assert agent.messages[1][\"role\"] == \"user\"\n+    assert \"toolResult\" in agent.messages[1][\"content\"][0]\n+    assert agent.messages[1][\"content\"][0][\"toolResult\"][\"toolUseId\"] == \"orphaned-123\"",
        "comment_created_at": "2025-11-03T19:44:34+00:00",
        "comment_author": "zastrowm",
        "comment_body": "Nit: assert the entire content-block or message, not each individual property",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2422090496",
    "pr_number": 961,
    "pr_file": "tests/strands/multiagent/test_graph.py",
    "created_at": "2025-10-10T21:09:09+00:00",
    "commented_code": "test_invocation_state = {\"custom_param\": \"test_value\", \"another_param\": 42}\n     result = graph(\"Test kwargs passing sync\", test_invocation_state)\n \n-    kwargs_agent.invoke_async.assert_called_once_with([{\"text\": \"Test kwargs passing sync\"}], **test_invocation_state)\n+    # Verify stream_async was called (kwargs are passed through)\n+    assert kwargs_agent.stream_async.call_count == 1\n+    assert result.status == Status.COMPLETED\n+\n+\n+@pytest.mark.asyncio\n+async def test_graph_streaming_events(mock_strands_tracer, mock_use_span):\n+    \"\"\"Test that graph streaming emits proper events during execution.\"\"\"\n+    # Create agents with custom streaming behavior\n+    agent_a = create_mock_agent(\"agent_a\", \"Response A\")\n+    agent_b = create_mock_agent(\"agent_b\", \"Response B\")\n+\n+    # Track events from agent streams\n+    agent_a_events = [\n+        {\"agent_thinking\": True, \"thought\": \"Processing task A\"},\n+        {\"agent_progress\": True, \"step\": \"analyzing\"},\n+        {\"result\": agent_a.return_value},\n+    ]\n+\n+    agent_b_events = [\n+        {\"agent_thinking\": True, \"thought\": \"Processing task B\"},\n+        {\"agent_progress\": True, \"step\": \"computing\"},\n+        {\"result\": agent_b.return_value},\n+    ]\n+\n+    async def stream_a(*args, **kwargs):\n+        for event in agent_a_events:\n+            yield event\n+\n+    async def stream_b(*args, **kwargs):\n+        for event in agent_b_events:\n+            yield event\n+\n+    agent_a.stream_async = Mock(side_effect=stream_a)\n+    agent_b.stream_async = Mock(side_effect=stream_b)\n+\n+    # Build graph: A -> B\n+    builder = GraphBuilder()\n+    builder.add_node(agent_a, \"a\")\n+    builder.add_node(agent_b, \"b\")\n+    builder.add_edge(\"a\", \"b\")\n+    builder.set_entry_point(\"a\")\n+    graph = builder.build()\n+\n+    # Collect all streaming events\n+    events = []\n+    async for event in graph.stream_async(\"Test streaming\"):\n+        events.append(event)\n+\n+    # Verify event structure and order\n+    assert len(events) > 0\n+\n+    # Should have node start/complete events and forwarded agent events\n+    node_start_events = [e for e in events if e.get(\"multi_agent_node_start\")]\n+    node_complete_events = [e for e in events if e.get(\"multi_agent_node_complete\")]\n+    node_stream_events = [e for e in events if e.get(\"multi_agent_node_stream\")]\n+    result_events = [e for e in events if \"result\" in e and not e.get(\"multi_agent_node_stream\")]\n+\n+    # Should have start/complete events for both nodes\n+    assert len(node_start_events) == 2\n+    assert len(node_complete_events) == 2\n+\n+    # Should have forwarded agent events\n+    assert len(node_stream_events) >= 4  # At least 2 events per agent\n+\n+    # Should have final result\n+    assert len(result_events) == 1\n+\n+    # Verify node start events have correct structure\n+    for event in node_start_events:\n+        assert \"node_id\" in event\n+        assert \"node_type\" in event\n+        assert event[\"node_type\"] == \"agent\"\n+\n+    # Verify node complete events have execution time\n+    for event in node_complete_events:\n+        assert \"node_id\" in event\n+        assert \"execution_time\" in event\n+        assert isinstance(event[\"execution_time\"], int)\n+\n+    # Verify forwarded events maintain node context\n+    for event in node_stream_events:\n+        assert \"node_id\" in event\n+        assert event[\"node_id\"] in [\"a\", \"b\"]\n+\n+    # Verify final result\n+    final_result = result_events[0][\"result\"]\n+    assert final_result.status == Status.COMPLETED\n+\n+\n+@pytest.mark.asyncio\n+async def test_graph_streaming_parallel_events(mock_strands_tracer, mock_use_span):\n+    \"\"\"Test that parallel graph execution properly streams events from concurrent nodes.\"\"\"\n+    # Create agents that execute in parallel\n+    agent_a = create_mock_agent(\"agent_a\", \"Response A\")\n+    agent_b = create_mock_agent(\"agent_b\", \"Response B\")\n+    agent_c = create_mock_agent(\"agent_c\", \"Response C\")\n+\n+    # Track timing and events\n+    execution_order = []\n+\n+    async def stream_with_timing(node_id, delay=0.05):\n+        execution_order.append(f\"{node_id}_start\")\n+        yield {\"node_start\": True, \"node\": node_id}\n+        await asyncio.sleep(delay)\n+        yield {\"node_progress\": True, \"node\": node_id}\n+        execution_order.append(f\"{node_id}_end\")\n+        yield {\"result\": create_mock_agent(node_id, f\"Response {node_id}\").return_value}\n+\n+    agent_a.stream_async = Mock(side_effect=lambda *args, **kwargs: stream_with_timing(\"A\", 0.05))\n+    agent_b.stream_async = Mock(side_effect=lambda *args, **kwargs: stream_with_timing(\"B\", 0.05))\n+    agent_c.stream_async = Mock(side_effect=lambda *args, **kwargs: stream_with_timing(\"C\", 0.05))\n+\n+    # Build graph with parallel nodes\n+    builder = GraphBuilder()\n+    builder.add_node(agent_a, \"a\")\n+    builder.add_node(agent_b, \"b\")\n+    builder.add_node(agent_c, \"c\")\n+    # All are entry points (parallel execution)\n+    builder.set_entry_point(\"a\")\n+    builder.set_entry_point(\"b\")\n+    builder.set_entry_point(\"c\")\n+    graph = builder.build()\n+\n+    # Collect streaming events\n+    events = []\n+    start_time = time.time()\n+    async for event in graph.stream_async(\"Test parallel streaming\"):\n+        events.append(event)\n+    total_time = time.time() - start_time\n+\n+    # Verify parallel execution timing\n+    assert total_time < 0.2, f\"Expected parallel execution, took {total_time}s\"\n+\n+    # Verify we get events from all nodes\n+    node_stream_events = [e for e in events if e.get(\"multi_agent_node_stream\")]\n+    nodes_with_events = set(e[\"node_id\"] for e in node_stream_events)\n+    assert nodes_with_events == {\"a\", \"b\", \"c\"}\n+\n+    # Verify start events for all nodes\n+    node_start_events = [e for e in events if e.get(\"multi_agent_node_start\")]\n+    start_node_ids = set(e[\"node_id\"] for e in node_start_events)\n+    assert start_node_ids == {\"a\", \"b\", \"c\"}\n+\n+\n+@pytest.mark.asyncio\n+async def test_graph_streaming_with_failures(mock_strands_tracer, mock_use_span):\n+    \"\"\"Test graph streaming behavior when nodes fail.\"\"\"\n+    # Create a failing agent\n+    failing_agent = Mock(spec=Agent)\n+    failing_agent.name = \"failing_agent\"\n+    failing_agent.id = \"fail_node\"\n+    failing_agent._session_manager = None\n+    failing_agent.hooks = HookRegistry()\n+\n+    async def failing_stream(*args, **kwargs):\n+        yield {\"agent_start\": True}\n+        yield {\"agent_thinking\": True, \"thought\": \"About to fail\"}\n+        await asyncio.sleep(0.01)\n+        raise Exception(\"Simulated streaming failure\")\n+\n+    async def failing_invoke(*args, **kwargs):\n+        raise Exception(\"Simulated failure\")\n+\n+    failing_agent.stream_async = Mock(side_effect=failing_stream)\n+    failing_agent.invoke_async = failing_invoke\n+\n+    # Create successful agent\n+    success_agent = create_mock_agent(\"success_agent\", \"Success\")\n+\n+    # Build graph\n+    builder = GraphBuilder()\n+    builder.add_node(failing_agent, \"fail\")\n+    builder.add_node(success_agent, \"success\")\n+    builder.set_entry_point(\"fail\")\n+    builder.set_entry_point(\"success\")\n+    graph = builder.build()\n+\n+    # Collect events until failure\n+    events = []\n+    try:\n+        async for event in graph.stream_async(\"Test streaming with failure\"):\n+            events.append(event)\n+        raise AssertionError(\"Expected an exception\")",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2422090496",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 961,
        "pr_file": "tests/strands/multiagent/test_graph.py",
        "discussion_id": "2422090496",
        "commented_code": "@@ -1335,5 +1366,492 @@ def test_graph_kwargs_passing_sync(mock_strands_tracer, mock_use_span):\n     test_invocation_state = {\"custom_param\": \"test_value\", \"another_param\": 42}\n     result = graph(\"Test kwargs passing sync\", test_invocation_state)\n \n-    kwargs_agent.invoke_async.assert_called_once_with([{\"text\": \"Test kwargs passing sync\"}], **test_invocation_state)\n+    # Verify stream_async was called (kwargs are passed through)\n+    assert kwargs_agent.stream_async.call_count == 1\n+    assert result.status == Status.COMPLETED\n+\n+\n+@pytest.mark.asyncio\n+async def test_graph_streaming_events(mock_strands_tracer, mock_use_span):\n+    \"\"\"Test that graph streaming emits proper events during execution.\"\"\"\n+    # Create agents with custom streaming behavior\n+    agent_a = create_mock_agent(\"agent_a\", \"Response A\")\n+    agent_b = create_mock_agent(\"agent_b\", \"Response B\")\n+\n+    # Track events from agent streams\n+    agent_a_events = [\n+        {\"agent_thinking\": True, \"thought\": \"Processing task A\"},\n+        {\"agent_progress\": True, \"step\": \"analyzing\"},\n+        {\"result\": agent_a.return_value},\n+    ]\n+\n+    agent_b_events = [\n+        {\"agent_thinking\": True, \"thought\": \"Processing task B\"},\n+        {\"agent_progress\": True, \"step\": \"computing\"},\n+        {\"result\": agent_b.return_value},\n+    ]\n+\n+    async def stream_a(*args, **kwargs):\n+        for event in agent_a_events:\n+            yield event\n+\n+    async def stream_b(*args, **kwargs):\n+        for event in agent_b_events:\n+            yield event\n+\n+    agent_a.stream_async = Mock(side_effect=stream_a)\n+    agent_b.stream_async = Mock(side_effect=stream_b)\n+\n+    # Build graph: A -> B\n+    builder = GraphBuilder()\n+    builder.add_node(agent_a, \"a\")\n+    builder.add_node(agent_b, \"b\")\n+    builder.add_edge(\"a\", \"b\")\n+    builder.set_entry_point(\"a\")\n+    graph = builder.build()\n+\n+    # Collect all streaming events\n+    events = []\n+    async for event in graph.stream_async(\"Test streaming\"):\n+        events.append(event)\n+\n+    # Verify event structure and order\n+    assert len(events) > 0\n+\n+    # Should have node start/complete events and forwarded agent events\n+    node_start_events = [e for e in events if e.get(\"multi_agent_node_start\")]\n+    node_complete_events = [e for e in events if e.get(\"multi_agent_node_complete\")]\n+    node_stream_events = [e for e in events if e.get(\"multi_agent_node_stream\")]\n+    result_events = [e for e in events if \"result\" in e and not e.get(\"multi_agent_node_stream\")]\n+\n+    # Should have start/complete events for both nodes\n+    assert len(node_start_events) == 2\n+    assert len(node_complete_events) == 2\n+\n+    # Should have forwarded agent events\n+    assert len(node_stream_events) >= 4  # At least 2 events per agent\n+\n+    # Should have final result\n+    assert len(result_events) == 1\n+\n+    # Verify node start events have correct structure\n+    for event in node_start_events:\n+        assert \"node_id\" in event\n+        assert \"node_type\" in event\n+        assert event[\"node_type\"] == \"agent\"\n+\n+    # Verify node complete events have execution time\n+    for event in node_complete_events:\n+        assert \"node_id\" in event\n+        assert \"execution_time\" in event\n+        assert isinstance(event[\"execution_time\"], int)\n+\n+    # Verify forwarded events maintain node context\n+    for event in node_stream_events:\n+        assert \"node_id\" in event\n+        assert event[\"node_id\"] in [\"a\", \"b\"]\n+\n+    # Verify final result\n+    final_result = result_events[0][\"result\"]\n+    assert final_result.status == Status.COMPLETED\n+\n+\n+@pytest.mark.asyncio\n+async def test_graph_streaming_parallel_events(mock_strands_tracer, mock_use_span):\n+    \"\"\"Test that parallel graph execution properly streams events from concurrent nodes.\"\"\"\n+    # Create agents that execute in parallel\n+    agent_a = create_mock_agent(\"agent_a\", \"Response A\")\n+    agent_b = create_mock_agent(\"agent_b\", \"Response B\")\n+    agent_c = create_mock_agent(\"agent_c\", \"Response C\")\n+\n+    # Track timing and events\n+    execution_order = []\n+\n+    async def stream_with_timing(node_id, delay=0.05):\n+        execution_order.append(f\"{node_id}_start\")\n+        yield {\"node_start\": True, \"node\": node_id}\n+        await asyncio.sleep(delay)\n+        yield {\"node_progress\": True, \"node\": node_id}\n+        execution_order.append(f\"{node_id}_end\")\n+        yield {\"result\": create_mock_agent(node_id, f\"Response {node_id}\").return_value}\n+\n+    agent_a.stream_async = Mock(side_effect=lambda *args, **kwargs: stream_with_timing(\"A\", 0.05))\n+    agent_b.stream_async = Mock(side_effect=lambda *args, **kwargs: stream_with_timing(\"B\", 0.05))\n+    agent_c.stream_async = Mock(side_effect=lambda *args, **kwargs: stream_with_timing(\"C\", 0.05))\n+\n+    # Build graph with parallel nodes\n+    builder = GraphBuilder()\n+    builder.add_node(agent_a, \"a\")\n+    builder.add_node(agent_b, \"b\")\n+    builder.add_node(agent_c, \"c\")\n+    # All are entry points (parallel execution)\n+    builder.set_entry_point(\"a\")\n+    builder.set_entry_point(\"b\")\n+    builder.set_entry_point(\"c\")\n+    graph = builder.build()\n+\n+    # Collect streaming events\n+    events = []\n+    start_time = time.time()\n+    async for event in graph.stream_async(\"Test parallel streaming\"):\n+        events.append(event)\n+    total_time = time.time() - start_time\n+\n+    # Verify parallel execution timing\n+    assert total_time < 0.2, f\"Expected parallel execution, took {total_time}s\"\n+\n+    # Verify we get events from all nodes\n+    node_stream_events = [e for e in events if e.get(\"multi_agent_node_stream\")]\n+    nodes_with_events = set(e[\"node_id\"] for e in node_stream_events)\n+    assert nodes_with_events == {\"a\", \"b\", \"c\"}\n+\n+    # Verify start events for all nodes\n+    node_start_events = [e for e in events if e.get(\"multi_agent_node_start\")]\n+    start_node_ids = set(e[\"node_id\"] for e in node_start_events)\n+    assert start_node_ids == {\"a\", \"b\", \"c\"}\n+\n+\n+@pytest.mark.asyncio\n+async def test_graph_streaming_with_failures(mock_strands_tracer, mock_use_span):\n+    \"\"\"Test graph streaming behavior when nodes fail.\"\"\"\n+    # Create a failing agent\n+    failing_agent = Mock(spec=Agent)\n+    failing_agent.name = \"failing_agent\"\n+    failing_agent.id = \"fail_node\"\n+    failing_agent._session_manager = None\n+    failing_agent.hooks = HookRegistry()\n+\n+    async def failing_stream(*args, **kwargs):\n+        yield {\"agent_start\": True}\n+        yield {\"agent_thinking\": True, \"thought\": \"About to fail\"}\n+        await asyncio.sleep(0.01)\n+        raise Exception(\"Simulated streaming failure\")\n+\n+    async def failing_invoke(*args, **kwargs):\n+        raise Exception(\"Simulated failure\")\n+\n+    failing_agent.stream_async = Mock(side_effect=failing_stream)\n+    failing_agent.invoke_async = failing_invoke\n+\n+    # Create successful agent\n+    success_agent = create_mock_agent(\"success_agent\", \"Success\")\n+\n+    # Build graph\n+    builder = GraphBuilder()\n+    builder.add_node(failing_agent, \"fail\")\n+    builder.add_node(success_agent, \"success\")\n+    builder.set_entry_point(\"fail\")\n+    builder.set_entry_point(\"success\")\n+    graph = builder.build()\n+\n+    # Collect events until failure\n+    events = []\n+    try:\n+        async for event in graph.stream_async(\"Test streaming with failure\"):\n+            events.append(event)\n+        raise AssertionError(\"Expected an exception\")",
        "comment_created_at": "2025-10-10T21:09:09+00:00",
        "comment_author": "zastrowm",
        "comment_body": "Why aren't we using pytest-raises?\n\nhttps://docs.pytest.org/en/stable/reference/reference.html#pytest-raises",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2429659321",
    "pr_number": 1028,
    "pr_file": "tests/strands/tools/test_decorator.py",
    "created_at": "2025-10-14T15:46:37+00:00",
    "commented_code": "]\n \n     assert act_results == exp_results\n+\n+\n+def test_tool_with_mismatched_tool_context_param_name_raises_error():",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2429659321",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 1028,
        "pr_file": "tests/strands/tools/test_decorator.py",
        "discussion_id": "2429659321",
        "commented_code": "@@ -1363,3 +1363,38 @@ async def async_generator() -> AsyncGenerator:\n     ]\n \n     assert act_results == exp_results\n+\n+\n+def test_tool_with_mismatched_tool_context_param_name_raises_error():",
        "comment_created_at": "2025-10-14T15:46:37+00:00",
        "comment_author": "pgrayy",
        "comment_body": "Great tests, but I would recommend a few minor updates:\r\n```Python\r\ndef test_function_tool_metadata_validate_signature_default_context_name_mismatch():\r\n    with pytest.raises(ValueError, match=r\"param_name=<context> \\| ToolContext param must be named 'tool_context'\"):\r\n\r\n        @strands.tool(context=True)\r\n        def my_tool(context: ToolContext):\r\n            pass\r\n\r\n\r\ndef test_function_tool_metadata_validate_signature_custom_context_name_mismatch():\r\n    with pytest.raises(ValueError, match=r\"param_name=<tool_context> \\| ToolContext param must be named 'my_context'\"):\r\n\r\n        @strands.tool(context=\"my_context\")\r\n        def my_tool(tool_context: ToolContext):\r\n            pass\r\n\r\n\r\ndef test_function_tool_metadata_validate_signature_missing_context_config():\r\n    with pytest.raises(ValueError, match=r\"@tool\\(context\\) must be set if passing in ToolContext param\"):\r\n\r\n        @strands.tool\r\n        def my_tool(tool_context: ToolContext):\r\n            pass\r\n```\r\n\r\n* `match` allows us to capture the message so we don't need to check `excinfo` on a separate line.\r\n* The naming convention for unit tests in strands is `test_<CLASS_NAME>_<METHOD_NAME>_<DESCRIPTION>`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2429708301",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 1028,
        "pr_file": "tests/strands/tools/test_decorator.py",
        "discussion_id": "2429659321",
        "commented_code": "@@ -1363,3 +1363,38 @@ async def async_generator() -> AsyncGenerator:\n     ]\n \n     assert act_results == exp_results\n+\n+\n+def test_tool_with_mismatched_tool_context_param_name_raises_error():",
        "comment_created_at": "2025-10-14T16:03:12+00:00",
        "comment_author": "Ratish1",
        "comment_body": "It is fixed now.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2330744687",
    "pr_number": 816,
    "pr_file": "tests/strands/multiagent/test_graph.py",
    "created_at": "2025-09-08T16:21:25+00:00",
    "commented_code": "assert result.status == Status.COMPLETED\n     assert len(result.execution_order) >= 2\n     assert multi_agent.invoke_async.call_count >= 2\n+\n+\n+@pytest.mark.asyncio\n+async def test_graph_kwargs_passing_agent(mock_strands_tracer, mock_use_span):\n+    \"\"\"Test that kwargs are passed through to underlying Agent nodes.\"\"\"\n+    # Create a mock agent that captures kwargs\n+    kwargs_agent = create_mock_agent(\"kwargs_agent\", \"Response with kwargs\")\n+\n+    async def capture_kwargs(*args, **kwargs):\n+        # Store kwargs for verification\n+        capture_kwargs.captured_kwargs = kwargs\n+        return kwargs_agent.return_value\n+\n+    kwargs_agent.invoke_async = MagicMock(side_effect=capture_kwargs)",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2330744687",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 816,
        "pr_file": "tests/strands/multiagent/test_graph.py",
        "discussion_id": "2330744687",
        "commented_code": "@@ -1285,3 +1285,89 @@ def multi_loop_condition(state: GraphState) -> bool:\n     assert result.status == Status.COMPLETED\n     assert len(result.execution_order) >= 2\n     assert multi_agent.invoke_async.call_count >= 2\n+\n+\n+@pytest.mark.asyncio\n+async def test_graph_kwargs_passing_agent(mock_strands_tracer, mock_use_span):\n+    \"\"\"Test that kwargs are passed through to underlying Agent nodes.\"\"\"\n+    # Create a mock agent that captures kwargs\n+    kwargs_agent = create_mock_agent(\"kwargs_agent\", \"Response with kwargs\")\n+\n+    async def capture_kwargs(*args, **kwargs):\n+        # Store kwargs for verification\n+        capture_kwargs.captured_kwargs = kwargs\n+        return kwargs_agent.return_value\n+\n+    kwargs_agent.invoke_async = MagicMock(side_effect=capture_kwargs)",
        "comment_created_at": "2025-09-08T16:21:25+00:00",
        "comment_author": "zastrowm",
        "comment_body": "If you use a mock here, can't we verify the mock invocation `assert kwargs_agent.invoke_async.call_args == [...]` or something, rather than needing to explicitly catch them?\r\n\r\nSee https://github.com/mkmeral/sdk-python/blob/72d7e1a085d5b930757293d7104e3596b63847d2/tests/strands/multiagent/test_graph.py#L1102 for instance.\r\n\r\nSame comment below",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2104694593",
    "pr_number": 75,
    "pr_file": "tests-integ/test_model_bedrock.py",
    "created_at": "2025-05-23T14:16:17+00:00",
    "commented_code": "+import pytest\n+\n+import strands\n+from strands import Agent\n+from strands.models import BedrockModel\n+\n+\n+@pytest.fixture\n+def system_prompt():\n+    return \"You are an AI assistant that uses & instead of .\"\n+\n+\n+@pytest.fixture\n+def streaming_model():\n+    return BedrockModel(\n+        model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n+        streaming=True,\n+    )\n+\n+\n+@pytest.fixture\n+def non_streaming_model():\n+    return BedrockModel(\n+        model_id=\"us.meta.llama3-2-90b-instruct-v1:0\",\n+        streaming=False,\n+    )\n+\n+\n+@pytest.fixture\n+def streaming_agent(streaming_model, system_prompt):\n+    return Agent(model=streaming_model, system_prompt=system_prompt)\n+\n+\n+@pytest.fixture\n+def non_streaming_agent(non_streaming_model, system_prompt):\n+    return Agent(model=non_streaming_model, system_prompt=system_prompt)\n+\n+\n+def test_streaming_agent(streaming_agent):\n+    \"\"\"Test agent with streaming model.\"\"\"\n+    result = streaming_agent(\"Hello!\")\n+\n+    assert len(str(result)) > 0\n+\n+\n+def test_non_streaming_agent(non_streaming_agent):\n+    \"\"\"Test agent with non-streaming model.\"\"\"\n+    result = non_streaming_agent(\"Hello!\")\n+\n+    assert len(str(result)) > 0\n+\n+\n+def test_streaming_model_events(streaming_model):\n+    \"\"\"Test streaming model events.\"\"\"\n+    messages = [{\"role\": \"user\", \"content\": [{\"text\": \"Hello\"}]}]\n+\n+    # Call converse and collect events\n+    events = list(streaming_model.converse(messages))\n+\n+    # Verify basic structure of events\n+    assert any(\"messageStart\" in event for event in events)\n+    assert any(\"contentBlockDelta\" in event for event in events)\n+    assert any(\"messageStop\" in event for event in events)\n+\n+\n+def test_non_streaming_model_events(non_streaming_model):\n+    \"\"\"Test non-streaming model events.\"\"\"\n+    messages = [{\"role\": \"user\", \"content\": [{\"text\": \"Hello\"}]}]\n+\n+    # Call converse and collect events\n+    events = list(non_streaming_model.converse(messages))\n+\n+    # Verify basic structure of events\n+    assert any(\"messageStart\" in event for event in events)\n+    assert any(\"contentBlockDelta\" in event for event in events)\n+    assert any(\"messageStop\" in event for event in events)\n+\n+\n+def test_tool_use_streaming(streaming_model):\n+    \"\"\"Test tool use with streaming model.\"\"\"\n+\n+    @strands.tool\n+    def calculator(expression: str) -> float:\n+        \"\"\"Calculate the result of a mathematical expression.\"\"\"\n+        return eval(expression)\n+\n+    agent = Agent(model=streaming_model, tools=[calculator])\n+    result = agent(\"What is 123 + 456?\")\n+\n+    # Print the full message content for debugging\n+    print(\"\nFull message content:\")\n+    import json\n+\n+    print(json.dumps(result.message[\"content\"], indent=2))\n+\n+    # The test is passing as long as the agent successfully uses the tool",
    "repo_full_name": "strands-agents/sdk-python",
    "discussion_comments": [
      {
        "comment_id": "2104694593",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 75,
        "pr_file": "tests-integ/test_model_bedrock.py",
        "discussion_id": "2104694593",
        "commented_code": "@@ -0,0 +1,116 @@\n+import pytest\n+\n+import strands\n+from strands import Agent\n+from strands.models import BedrockModel\n+\n+\n+@pytest.fixture\n+def system_prompt():\n+    return \"You are an AI assistant that uses & instead of .\"\n+\n+\n+@pytest.fixture\n+def streaming_model():\n+    return BedrockModel(\n+        model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n+        streaming=True,\n+    )\n+\n+\n+@pytest.fixture\n+def non_streaming_model():\n+    return BedrockModel(\n+        model_id=\"us.meta.llama3-2-90b-instruct-v1:0\",\n+        streaming=False,\n+    )\n+\n+\n+@pytest.fixture\n+def streaming_agent(streaming_model, system_prompt):\n+    return Agent(model=streaming_model, system_prompt=system_prompt)\n+\n+\n+@pytest.fixture\n+def non_streaming_agent(non_streaming_model, system_prompt):\n+    return Agent(model=non_streaming_model, system_prompt=system_prompt)\n+\n+\n+def test_streaming_agent(streaming_agent):\n+    \"\"\"Test agent with streaming model.\"\"\"\n+    result = streaming_agent(\"Hello!\")\n+\n+    assert len(str(result)) > 0\n+\n+\n+def test_non_streaming_agent(non_streaming_agent):\n+    \"\"\"Test agent with non-streaming model.\"\"\"\n+    result = non_streaming_agent(\"Hello!\")\n+\n+    assert len(str(result)) > 0\n+\n+\n+def test_streaming_model_events(streaming_model):\n+    \"\"\"Test streaming model events.\"\"\"\n+    messages = [{\"role\": \"user\", \"content\": [{\"text\": \"Hello\"}]}]\n+\n+    # Call converse and collect events\n+    events = list(streaming_model.converse(messages))\n+\n+    # Verify basic structure of events\n+    assert any(\"messageStart\" in event for event in events)\n+    assert any(\"contentBlockDelta\" in event for event in events)\n+    assert any(\"messageStop\" in event for event in events)\n+\n+\n+def test_non_streaming_model_events(non_streaming_model):\n+    \"\"\"Test non-streaming model events.\"\"\"\n+    messages = [{\"role\": \"user\", \"content\": [{\"text\": \"Hello\"}]}]\n+\n+    # Call converse and collect events\n+    events = list(non_streaming_model.converse(messages))\n+\n+    # Verify basic structure of events\n+    assert any(\"messageStart\" in event for event in events)\n+    assert any(\"contentBlockDelta\" in event for event in events)\n+    assert any(\"messageStop\" in event for event in events)\n+\n+\n+def test_tool_use_streaming(streaming_model):\n+    \"\"\"Test tool use with streaming model.\"\"\"\n+\n+    @strands.tool\n+    def calculator(expression: str) -> float:\n+        \"\"\"Calculate the result of a mathematical expression.\"\"\"\n+        return eval(expression)\n+\n+    agent = Agent(model=streaming_model, tools=[calculator])\n+    result = agent(\"What is 123 + 456?\")\n+\n+    # Print the full message content for debugging\n+    print(\"\\nFull message content:\")\n+    import json\n+\n+    print(json.dumps(result.message[\"content\"], indent=2))\n+\n+    # The test is passing as long as the agent successfully uses the tool",
        "comment_created_at": "2025-05-23T14:16:17+00:00",
        "comment_author": "zastrowm",
        "comment_body": "> The test is passing as long as the agent successfully uses the tool\r\n\r\nWe can assert that the tool was called by having the tool call set a variable, no?\r\n\r\n```\r\ndef calculator():\r\n   nonlocal tool_was_called\r\n   tool_was_called = True\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2104695172",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 75,
        "pr_file": "tests-integ/test_model_bedrock.py",
        "discussion_id": "2104694593",
        "commented_code": "@@ -0,0 +1,116 @@\n+import pytest\n+\n+import strands\n+from strands import Agent\n+from strands.models import BedrockModel\n+\n+\n+@pytest.fixture\n+def system_prompt():\n+    return \"You are an AI assistant that uses & instead of .\"\n+\n+\n+@pytest.fixture\n+def streaming_model():\n+    return BedrockModel(\n+        model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n+        streaming=True,\n+    )\n+\n+\n+@pytest.fixture\n+def non_streaming_model():\n+    return BedrockModel(\n+        model_id=\"us.meta.llama3-2-90b-instruct-v1:0\",\n+        streaming=False,\n+    )\n+\n+\n+@pytest.fixture\n+def streaming_agent(streaming_model, system_prompt):\n+    return Agent(model=streaming_model, system_prompt=system_prompt)\n+\n+\n+@pytest.fixture\n+def non_streaming_agent(non_streaming_model, system_prompt):\n+    return Agent(model=non_streaming_model, system_prompt=system_prompt)\n+\n+\n+def test_streaming_agent(streaming_agent):\n+    \"\"\"Test agent with streaming model.\"\"\"\n+    result = streaming_agent(\"Hello!\")\n+\n+    assert len(str(result)) > 0\n+\n+\n+def test_non_streaming_agent(non_streaming_agent):\n+    \"\"\"Test agent with non-streaming model.\"\"\"\n+    result = non_streaming_agent(\"Hello!\")\n+\n+    assert len(str(result)) > 0\n+\n+\n+def test_streaming_model_events(streaming_model):\n+    \"\"\"Test streaming model events.\"\"\"\n+    messages = [{\"role\": \"user\", \"content\": [{\"text\": \"Hello\"}]}]\n+\n+    # Call converse and collect events\n+    events = list(streaming_model.converse(messages))\n+\n+    # Verify basic structure of events\n+    assert any(\"messageStart\" in event for event in events)\n+    assert any(\"contentBlockDelta\" in event for event in events)\n+    assert any(\"messageStop\" in event for event in events)\n+\n+\n+def test_non_streaming_model_events(non_streaming_model):\n+    \"\"\"Test non-streaming model events.\"\"\"\n+    messages = [{\"role\": \"user\", \"content\": [{\"text\": \"Hello\"}]}]\n+\n+    # Call converse and collect events\n+    events = list(non_streaming_model.converse(messages))\n+\n+    # Verify basic structure of events\n+    assert any(\"messageStart\" in event for event in events)\n+    assert any(\"contentBlockDelta\" in event for event in events)\n+    assert any(\"messageStop\" in event for event in events)\n+\n+\n+def test_tool_use_streaming(streaming_model):\n+    \"\"\"Test tool use with streaming model.\"\"\"\n+\n+    @strands.tool\n+    def calculator(expression: str) -> float:\n+        \"\"\"Calculate the result of a mathematical expression.\"\"\"\n+        return eval(expression)\n+\n+    agent = Agent(model=streaming_model, tools=[calculator])\n+    result = agent(\"What is 123 + 456?\")\n+\n+    # Print the full message content for debugging\n+    print(\"\\nFull message content:\")\n+    import json\n+\n+    print(json.dumps(result.message[\"content\"], indent=2))\n+\n+    # The test is passing as long as the agent successfully uses the tool",
        "comment_created_at": "2025-05-23T14:16:37+00:00",
        "comment_author": "zastrowm",
        "comment_body": "Same with below.",
        "pr_file_module": null
      },
      {
        "comment_id": "2105082833",
        "repo_full_name": "strands-agents/sdk-python",
        "pr_number": 75,
        "pr_file": "tests-integ/test_model_bedrock.py",
        "discussion_id": "2104694593",
        "commented_code": "@@ -0,0 +1,116 @@\n+import pytest\n+\n+import strands\n+from strands import Agent\n+from strands.models import BedrockModel\n+\n+\n+@pytest.fixture\n+def system_prompt():\n+    return \"You are an AI assistant that uses & instead of .\"\n+\n+\n+@pytest.fixture\n+def streaming_model():\n+    return BedrockModel(\n+        model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n+        streaming=True,\n+    )\n+\n+\n+@pytest.fixture\n+def non_streaming_model():\n+    return BedrockModel(\n+        model_id=\"us.meta.llama3-2-90b-instruct-v1:0\",\n+        streaming=False,\n+    )\n+\n+\n+@pytest.fixture\n+def streaming_agent(streaming_model, system_prompt):\n+    return Agent(model=streaming_model, system_prompt=system_prompt)\n+\n+\n+@pytest.fixture\n+def non_streaming_agent(non_streaming_model, system_prompt):\n+    return Agent(model=non_streaming_model, system_prompt=system_prompt)\n+\n+\n+def test_streaming_agent(streaming_agent):\n+    \"\"\"Test agent with streaming model.\"\"\"\n+    result = streaming_agent(\"Hello!\")\n+\n+    assert len(str(result)) > 0\n+\n+\n+def test_non_streaming_agent(non_streaming_agent):\n+    \"\"\"Test agent with non-streaming model.\"\"\"\n+    result = non_streaming_agent(\"Hello!\")\n+\n+    assert len(str(result)) > 0\n+\n+\n+def test_streaming_model_events(streaming_model):\n+    \"\"\"Test streaming model events.\"\"\"\n+    messages = [{\"role\": \"user\", \"content\": [{\"text\": \"Hello\"}]}]\n+\n+    # Call converse and collect events\n+    events = list(streaming_model.converse(messages))\n+\n+    # Verify basic structure of events\n+    assert any(\"messageStart\" in event for event in events)\n+    assert any(\"contentBlockDelta\" in event for event in events)\n+    assert any(\"messageStop\" in event for event in events)\n+\n+\n+def test_non_streaming_model_events(non_streaming_model):\n+    \"\"\"Test non-streaming model events.\"\"\"\n+    messages = [{\"role\": \"user\", \"content\": [{\"text\": \"Hello\"}]}]\n+\n+    # Call converse and collect events\n+    events = list(non_streaming_model.converse(messages))\n+\n+    # Verify basic structure of events\n+    assert any(\"messageStart\" in event for event in events)\n+    assert any(\"contentBlockDelta\" in event for event in events)\n+    assert any(\"messageStop\" in event for event in events)\n+\n+\n+def test_tool_use_streaming(streaming_model):\n+    \"\"\"Test tool use with streaming model.\"\"\"\n+\n+    @strands.tool\n+    def calculator(expression: str) -> float:\n+        \"\"\"Calculate the result of a mathematical expression.\"\"\"\n+        return eval(expression)\n+\n+    agent = Agent(model=streaming_model, tools=[calculator])\n+    result = agent(\"What is 123 + 456?\")\n+\n+    # Print the full message content for debugging\n+    print(\"\\nFull message content:\")\n+    import json\n+\n+    print(json.dumps(result.message[\"content\"], indent=2))\n+\n+    # The test is passing as long as the agent successfully uses the tool",
        "comment_created_at": "2025-05-23T17:21:02+00:00",
        "comment_author": "Unshure",
        "comment_body": "Good idea, will update",
        "pr_file_module": null
      }
    ]
  }
]