[
  {
    "discussion_id": "2155692978",
    "pr_number": 93882,
    "pr_file": "src/sentry/workflow_engine/processors/delayed_workflow.py",
    "created_at": "2025-06-18T23:07:04+00:00",
    "commented_code": "data_condition_groups: list[DataConditionGroup],\n     event_data: EventRedisData,\n     workflows_to_envs: Mapping[WorkflowId, int | None],\n-) -> dict[UniqueConditionQuery, set[GroupId]]:\n+) -> dict[UniqueConditionQuery, TimeAndGroups]:\n     \"\"\"\n     Map unique condition queries to the group IDs that need to checked for that query.\n     \"\"\"\n-    condition_groups: dict[UniqueConditionQuery, set[GroupId]] = defaultdict(set)\n+    condition_groups: dict[UniqueConditionQuery, TimeAndGroups] = defaultdict(TimeAndGroups)\n     dcg_to_slow_conditions = get_slow_conditions_for_groups(list(event_data.dcg_to_groups.keys()))\n \n     for dcg in data_condition_groups:\n         slow_conditions = dcg_to_slow_conditions[dcg.id]\n         workflow_id = event_data.dcg_to_workflow.get(dcg.id)\n         workflow_env = workflows_to_envs[workflow_id] if workflow_id else None\n+        timestamp = event_data.dcg_to_timestamp[dcg.id]\n         for condition in slow_conditions:\n             for condition_query in generate_unique_queries(condition, workflow_env):\n-                condition_groups[condition_query].update(event_data.dcg_to_groups[dcg.id])\n+                condition_groups[condition_query].group_ids.update(event_data.dcg_to_groups[dcg.id])\n+                condition_groups[condition_query].update_timestamp(timestamp)\n     return condition_groups\n \n \n @sentry_sdk.trace\n def get_condition_group_results(\n-    queries_to_groups: dict[UniqueConditionQuery, set[GroupId]],\n+    queries_to_groups: dict[UniqueConditionQuery, TimeAndGroups],\n ) -> dict[UniqueConditionQuery, QueryResult]:\n     condition_group_results = {}\n     current_time = timezone.now()\n \n-    for unique_condition, group_ids in queries_to_groups.items():\n+    for unique_condition, time_and_groups in queries_to_groups.items():\n         handler = unique_condition.handler()\n+        group_ids = time_and_groups.group_ids",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2155692978",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93882,
        "pr_file": "src/sentry/workflow_engine/processors/delayed_workflow.py",
        "discussion_id": "2155692978",
        "commented_code": "@@ -354,32 +391,36 @@ def get_condition_query_groups(\n     data_condition_groups: list[DataConditionGroup],\n     event_data: EventRedisData,\n     workflows_to_envs: Mapping[WorkflowId, int | None],\n-) -> dict[UniqueConditionQuery, set[GroupId]]:\n+) -> dict[UniqueConditionQuery, TimeAndGroups]:\n     \"\"\"\n     Map unique condition queries to the group IDs that need to checked for that query.\n     \"\"\"\n-    condition_groups: dict[UniqueConditionQuery, set[GroupId]] = defaultdict(set)\n+    condition_groups: dict[UniqueConditionQuery, TimeAndGroups] = defaultdict(TimeAndGroups)\n     dcg_to_slow_conditions = get_slow_conditions_for_groups(list(event_data.dcg_to_groups.keys()))\n \n     for dcg in data_condition_groups:\n         slow_conditions = dcg_to_slow_conditions[dcg.id]\n         workflow_id = event_data.dcg_to_workflow.get(dcg.id)\n         workflow_env = workflows_to_envs[workflow_id] if workflow_id else None\n+        timestamp = event_data.dcg_to_timestamp[dcg.id]\n         for condition in slow_conditions:\n             for condition_query in generate_unique_queries(condition, workflow_env):\n-                condition_groups[condition_query].update(event_data.dcg_to_groups[dcg.id])\n+                condition_groups[condition_query].group_ids.update(event_data.dcg_to_groups[dcg.id])\n+                condition_groups[condition_query].update_timestamp(timestamp)\n     return condition_groups\n \n \n @sentry_sdk.trace\n def get_condition_group_results(\n-    queries_to_groups: dict[UniqueConditionQuery, set[GroupId]],\n+    queries_to_groups: dict[UniqueConditionQuery, TimeAndGroups],\n ) -> dict[UniqueConditionQuery, QueryResult]:\n     condition_group_results = {}\n     current_time = timezone.now()\n \n-    for unique_condition, group_ids in queries_to_groups.items():\n+    for unique_condition, time_and_groups in queries_to_groups.items():\n         handler = unique_condition.handler()\n+        group_ids = time_and_groups.group_ids",
        "comment_created_at": "2025-06-18T23:07:04+00:00",
        "comment_author": "kcons",
        "comment_body": "another option is to make `groups` be a `dict[GroupId, datetime | None]` and \r\ndo `time = max(ts for ts in groups.values() if ts, default=current_time)`.\r\nStores a bit more data, but lets the summarizing happen where it is being forced, which has a certain appeal.",
        "pr_file_module": null
      },
      {
        "comment_id": "2162441351",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93882,
        "pr_file": "src/sentry/workflow_engine/processors/delayed_workflow.py",
        "discussion_id": "2155692978",
        "commented_code": "@@ -354,32 +391,36 @@ def get_condition_query_groups(\n     data_condition_groups: list[DataConditionGroup],\n     event_data: EventRedisData,\n     workflows_to_envs: Mapping[WorkflowId, int | None],\n-) -> dict[UniqueConditionQuery, set[GroupId]]:\n+) -> dict[UniqueConditionQuery, TimeAndGroups]:\n     \"\"\"\n     Map unique condition queries to the group IDs that need to checked for that query.\n     \"\"\"\n-    condition_groups: dict[UniqueConditionQuery, set[GroupId]] = defaultdict(set)\n+    condition_groups: dict[UniqueConditionQuery, TimeAndGroups] = defaultdict(TimeAndGroups)\n     dcg_to_slow_conditions = get_slow_conditions_for_groups(list(event_data.dcg_to_groups.keys()))\n \n     for dcg in data_condition_groups:\n         slow_conditions = dcg_to_slow_conditions[dcg.id]\n         workflow_id = event_data.dcg_to_workflow.get(dcg.id)\n         workflow_env = workflows_to_envs[workflow_id] if workflow_id else None\n+        timestamp = event_data.dcg_to_timestamp[dcg.id]\n         for condition in slow_conditions:\n             for condition_query in generate_unique_queries(condition, workflow_env):\n-                condition_groups[condition_query].update(event_data.dcg_to_groups[dcg.id])\n+                condition_groups[condition_query].group_ids.update(event_data.dcg_to_groups[dcg.id])\n+                condition_groups[condition_query].update_timestamp(timestamp)\n     return condition_groups\n \n \n @sentry_sdk.trace\n def get_condition_group_results(\n-    queries_to_groups: dict[UniqueConditionQuery, set[GroupId]],\n+    queries_to_groups: dict[UniqueConditionQuery, TimeAndGroups],\n ) -> dict[UniqueConditionQuery, QueryResult]:\n     condition_group_results = {}\n     current_time = timezone.now()\n \n-    for unique_condition, group_ids in queries_to_groups.items():\n+    for unique_condition, time_and_groups in queries_to_groups.items():\n         handler = unique_condition.handler()\n+        group_ids = time_and_groups.group_ids",
        "comment_created_at": "2025-06-23T20:06:46+00:00",
        "comment_author": "cathteng",
        "comment_body": "i need to refactor dcg_to_timestamp for this, if it comes to it we can do a refactor",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2150947292",
    "pr_number": 93569,
    "pr_file": "src/sentry/workflow_engine/processors/delayed_workflow.py",
    "created_at": "2025-06-16T21:44:03+00:00",
    "commented_code": ") as tracker:\n         for group, group_event in group_to_groupevent.items():\n             with tracker.track(str(group.id)):\n-                event_data = WorkflowEventData(event=group_event)\n-                detector = get_detector_by_event(event_data)\n+                workflow_event_data = WorkflowEventData(event=group_event)\n+                detector = get_detector_by_event(workflow_event_data)\n \n                 workflow_triggers: set[DataConditionGroup] = set()\n                 action_filters: set[DataConditionGroup] = set()\n                 for dcg in groups_to_fire[group.id]:\n                     if (\n                         dcg.id\n-                        in trigger_group_to_dcg_model[DataConditionHandler.Group.WORKFLOW_TRIGGER]\n+                        in event_data.trigger_group_to_dcg_model[\n+                            DataConditionHandler.Group.WORKFLOW_TRIGGER\n+                        ]\n                     ):\n                         workflow_triggers.add(dcg)\n                     elif (\n                         dcg.id\n-                        in trigger_group_to_dcg_model[DataConditionHandler.Group.ACTION_FILTER]\n+                        in event_data.trigger_group_to_dcg_model[\n+                            DataConditionHandler.Group.ACTION_FILTER\n+                        ]",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2150947292",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93569,
        "pr_file": "src/sentry/workflow_engine/processors/delayed_workflow.py",
        "discussion_id": "2150947292",
        "commented_code": "@@ -431,20 +486,24 @@ def fire_actions_for_groups(\n     ) as tracker:\n         for group, group_event in group_to_groupevent.items():\n             with tracker.track(str(group.id)):\n-                event_data = WorkflowEventData(event=group_event)\n-                detector = get_detector_by_event(event_data)\n+                workflow_event_data = WorkflowEventData(event=group_event)\n+                detector = get_detector_by_event(workflow_event_data)\n \n                 workflow_triggers: set[DataConditionGroup] = set()\n                 action_filters: set[DataConditionGroup] = set()\n                 for dcg in groups_to_fire[group.id]:\n                     if (\n                         dcg.id\n-                        in trigger_group_to_dcg_model[DataConditionHandler.Group.WORKFLOW_TRIGGER]\n+                        in event_data.trigger_group_to_dcg_model[\n+                            DataConditionHandler.Group.WORKFLOW_TRIGGER\n+                        ]\n                     ):\n                         workflow_triggers.add(dcg)\n                     elif (\n                         dcg.id\n-                        in trigger_group_to_dcg_model[DataConditionHandler.Group.ACTION_FILTER]\n+                        in event_data.trigger_group_to_dcg_model[\n+                            DataConditionHandler.Group.ACTION_FILTER\n+                        ]",
        "comment_created_at": "2025-06-16T21:44:03+00:00",
        "comment_author": "cathteng",
        "comment_body": "something i noticed about my own code is that we could probably just do set intersection to figure out which ones are workflow triggers and which ones are action filters",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2151169409",
    "pr_number": 93676,
    "pr_file": "src/sentry/incidents/typings/metric_detector.py",
    "created_at": "2025-06-17T01:53:53+00:00",
    "commented_code": "@classmethod\n     def from_workflow_engine_models(\n-        cls, detector: Detector, evidence_data: MetricIssueEvidenceData, group_status: GroupStatus\n+        cls,\n+        detector: Detector,\n+        evidence_data: MetricIssueEvidenceData,\n+        group_status: GroupStatus,\n+        priority_level: int | None,\n     ) -> AlertContext:\n-        threshold_type = fetch_threshold_type(evidence_data)\n-        resolve_threshold = fetch_resolve_threshold(evidence_data, group_status)\n-        alert_threshold = fetch_alert_threshold(evidence_data, group_status)\n-        sensitivity = fetch_sensitivity(evidence_data)\n+        # This should never happen, but we need to handle it for now\n+        if priority_level is None:\n+            raise ValueError(\"Priority level is required for metric issues\")\n+\n+        for condition in evidence_data.conditions:\n+            # The condition.condition_result is of type DetectorPriorityLevel\n+            if (\n+                condition[\"condition_result\"]\n+                == PRIORITY_LEVEL_TO_DETECTOR_PRIORITY_LEVEL[PriorityLevel(priority_level)]\n+            ):\n+                threshold_type = fetch_threshold_type(Condition(condition[\"type\"]))\n+                resolve_threshold = fetch_resolve_threshold(condition[\"comparison\"], group_status)\n+                alert_threshold = fetch_alert_threshold(condition[\"comparison\"], group_status)\n+                sensitivity = fetch_sensitivity(condition)\n+                break\n+        else:\n+            raise ValueError(\"No threshold type found for metric issues\")",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2151169409",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93676,
        "pr_file": "src/sentry/incidents/typings/metric_detector.py",
        "discussion_id": "2151169409",
        "commented_code": "@@ -94,12 +92,29 @@ def from_alert_rule_incident(\n \n     @classmethod\n     def from_workflow_engine_models(\n-        cls, detector: Detector, evidence_data: MetricIssueEvidenceData, group_status: GroupStatus\n+        cls,\n+        detector: Detector,\n+        evidence_data: MetricIssueEvidenceData,\n+        group_status: GroupStatus,\n+        priority_level: int | None,\n     ) -> AlertContext:\n-        threshold_type = fetch_threshold_type(evidence_data)\n-        resolve_threshold = fetch_resolve_threshold(evidence_data, group_status)\n-        alert_threshold = fetch_alert_threshold(evidence_data, group_status)\n-        sensitivity = fetch_sensitivity(evidence_data)\n+        # This should never happen, but we need to handle it for now\n+        if priority_level is None:\n+            raise ValueError(\"Priority level is required for metric issues\")\n+\n+        for condition in evidence_data.conditions:\n+            # The condition.condition_result is of type DetectorPriorityLevel\n+            if (\n+                condition[\"condition_result\"]\n+                == PRIORITY_LEVEL_TO_DETECTOR_PRIORITY_LEVEL[PriorityLevel(priority_level)]\n+            ):\n+                threshold_type = fetch_threshold_type(Condition(condition[\"type\"]))\n+                resolve_threshold = fetch_resolve_threshold(condition[\"comparison\"], group_status)\n+                alert_threshold = fetch_alert_threshold(condition[\"comparison\"], group_status)\n+                sensitivity = fetch_sensitivity(condition)\n+                break\n+        else:\n+            raise ValueError(\"No threshold type found for metric issues\")",
        "comment_created_at": "2025-06-17T01:53:53+00:00",
        "comment_author": "saponifi3d",
        "comment_body": "\ud83e\udd14 i wonder if we could split this up a little differently, first finding the condition that we care about, then getting the information about it.\r\n\r\n```py\r\ntarget_priority = PRIORITY_LEVEL_TO_DETECTOR_PRIORITY_LEVEL[PriorityLevel(priority_level)]\r\n\r\ntry:\r\n    condition = next(\r\n        cond for cond in evidence_data.conditions \r\n        if cond[\"condition_result\"] == target_priority\r\n    )\r\n    threshold_type = fetch_threshold_type(Condition(condition[\"type\"]))\r\n    resolve_threshold = fetch_resolve_threshold(condition[\"comparison\"], group_status)\r\n    alert_threshold = fetch_alert_threshold(condition[\"comparison\"], group_status)\r\n    sensitivity = fetch_sensitivity(condition)\r\nexcept StopIteration:\r\n    # thrown by `next` if no value is found\r\n    raise ValueError(\"No threshold type found for metric issues\")\r\n ```\r\n This works more or less the same way as you have it, but it will find the `condition` we care about, then we can find values. (more info about [next](https://docs.python.org/3/library/functions.html#next) here)",
        "pr_file_module": null
      }
    ]
  }
]