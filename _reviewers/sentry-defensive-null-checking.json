[
  {
    "discussion_id": "2196757416",
    "pr_number": 95055,
    "pr_file": "src/sentry/replays/endpoints/project_replay_summarize_breadcrumbs.py",
    "created_at": "2025-07-10T06:48:51+00:00",
    "commented_code": "match event_type:\n         case EventType.CLICK:\n-            return f\"User clicked on {event[\"data\"][\"payload\"][\"message\"]} at {timestamp}\"\n+            message = event[\"data\"][\"payload\"].get(\"message\")\n+            return f\"User clicked on {message} at {timestamp}\" if message else None\n         case EventType.DEAD_CLICK:\n-            return f\"User clicked on {event[\"data\"][\"payload\"][\"message\"]} but the triggered action was slow to complete at {timestamp}\"\n+            message = event[\"data\"][\"payload\"].get(\"message\")",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2196757416",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 95055,
        "pr_file": "src/sentry/replays/endpoints/project_replay_summarize_breadcrumbs.py",
        "discussion_id": "2196757416",
        "commented_code": "@@ -376,15 +376,28 @@ def as_log_message(event: dict[str, Any]) -> str | None:\n \n     match event_type:\n         case EventType.CLICK:\n-            return f\"User clicked on {event[\"data\"][\"payload\"][\"message\"]} at {timestamp}\"\n+            message = event[\"data\"][\"payload\"].get(\"message\")\n+            return f\"User clicked on {message} at {timestamp}\" if message else None\n         case EventType.DEAD_CLICK:\n-            return f\"User clicked on {event[\"data\"][\"payload\"][\"message\"]} but the triggered action was slow to complete at {timestamp}\"\n+            message = event[\"data\"][\"payload\"].get(\"message\")",
        "comment_created_at": "2025-07-10T06:48:51+00:00",
        "comment_author": "aliu39",
        "comment_body": "when do we expect `message` to be missing? If it's missing is it considered an error/not supposed to happen?\r\n\r\nFor the keys we expect to be there, we could still use `[]` and wrap the whole match with `try: ... except KeyError:`, then log that",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2155698564",
    "pr_number": 93886,
    "pr_file": "src/sentry/workflow_engine/endpoints/validators/base/data_condition.py",
    "created_at": "2025-06-18T23:10:47+00:00",
    "commented_code": "class BaseDataConditionValidator(\n     AbstractDataConditionValidator[Any, Any],\n ):\n+\n+    @property\n+    def condition_type(self) -> Condition:\n+        if isinstance(self.initial_data, list) and self.initial_data:\n+            return self.initial_data[0].get(\"type\")",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2155698564",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93886,
        "pr_file": "src/sentry/workflow_engine/endpoints/validators/base/data_condition.py",
        "discussion_id": "2155698564",
        "commented_code": "@@ -40,27 +40,31 @@ def validate_condition_result(self, value: Any) -> ConditionResult:\n class BaseDataConditionValidator(\n     AbstractDataConditionValidator[Any, Any],\n ):\n+\n+    @property\n+    def condition_type(self) -> Condition:\n+        if isinstance(self.initial_data, list) and self.initial_data:\n+            return self.initial_data[0].get(\"type\")",
        "comment_created_at": "2025-06-18T23:10:47+00:00",
        "comment_author": "ceorourke",
        "comment_body": "`self.initial_data` is a list of conditions in some cases (e.g. the case where we're subclassing it for the `MetricIssueComparisonConditionValidator`)",
        "pr_file_module": null
      },
      {
        "comment_id": "2162520803",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93886,
        "pr_file": "src/sentry/workflow_engine/endpoints/validators/base/data_condition.py",
        "discussion_id": "2155698564",
        "commented_code": "@@ -40,27 +40,31 @@ def validate_condition_result(self, value: Any) -> ConditionResult:\n class BaseDataConditionValidator(\n     AbstractDataConditionValidator[Any, Any],\n ):\n+\n+    @property\n+    def condition_type(self) -> Condition:\n+        if isinstance(self.initial_data, list) and self.initial_data:\n+            return self.initial_data[0].get(\"type\")",
        "comment_created_at": "2025-06-23T20:57:35+00:00",
        "comment_author": "mifu67",
        "comment_body": "Might be a silly question, but how do we know to get the type using for index 0? I'm assuming we know there's only one condition in the list?",
        "pr_file_module": null
      },
      {
        "comment_id": "2164527868",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93886,
        "pr_file": "src/sentry/workflow_engine/endpoints/validators/base/data_condition.py",
        "discussion_id": "2155698564",
        "commented_code": "@@ -40,27 +40,31 @@ def validate_condition_result(self, value: Any) -> ConditionResult:\n class BaseDataConditionValidator(\n     AbstractDataConditionValidator[Any, Any],\n ):\n+\n+    @property\n+    def condition_type(self) -> Condition:\n+        if isinstance(self.initial_data, list) and self.initial_data:\n+            return self.initial_data[0].get(\"type\")",
        "comment_created_at": "2025-06-24T17:16:58+00:00",
        "comment_author": "ceorourke",
        "comment_body": "This is a bit of an assumption yeah - for anomaly detection we may have 2 data conditions but their type would both be 'anomaly detection'. I don't think we'd ever have a case where we have 1 anomaly detection data condition and 1 of another type, right?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2153078306",
    "pr_number": 93669,
    "pr_file": "src/sentry/replays/endpoints/project_replay_summarize_breadcrumbs.py",
    "created_at": "2025-06-17T20:06:55+00:00",
    "commented_code": "):\n             return self.respond(status=404)\n \n+        filter_params = self.get_filter_params(request, project)\n+\n+        # Fetch the replay's error IDs from the replay_id.\n+        snuba_response = query_replay_instance(\n+            project_id=project.id,\n+            replay_id=replay_id,\n+            start=filter_params[\"start\"],\n+            end=filter_params[\"end\"],\n+            organization=project.organization,\n+            request_user_id=request.user.id,\n+        )\n+\n+        response = process_raw_response(\n+            snuba_response,\n+            fields=request.query_params.getlist(\"field\"),\n+        )\n+\n+        error_ids = response[0].get(\"error_ids\", []) if response else []\n+        error_events = fetch_error_details(project_id=project.id, error_ids=error_ids)\n+\n         return self.paginate(\n             request=request,\n             paginator_cls=GenericOffsetPaginator,\n             data_fn=functools.partial(fetch_segments_metadata, project.id, replay_id),\n-            on_results=analyze_recording_segments,\n+            on_results=functools.partial(analyze_recording_segments, error_events),\n         )\n \n \n+def fetch_error_details(project_id: int, error_ids: list[str]) -> list[dict[str, Any]]:\n+    \"\"\"Fetch error details given error IDs.\"\"\"\n+    try:\n+        node_ids = [Event.generate_node_id(project_id, event_id=id) for id in error_ids]\n+        events = nodestore.backend.get_multi(node_ids)\n+\n+        return [\n+            {\n+                \"category\": \"error\",\n+                \"id\": event_id,\n+                \"title\": data.get(\"title\", \"\"),\n+                \"timestamp\": data.get(\"timestamp\", 0.0),\n+                \"message\": data.get(\"message\", \"\"),\n+            }\n+            for event_id, data in zip(error_ids, events.values())\n+            if data is not None\n+        ]\n+    except Exception as e:\n+        sentry_sdk.capture_exception(e)\n+        return []\n+\n+\n+def generate_error_log_message(error: dict[str, Any]) -> str:\n+    title = error.get(\"title\", \"\")\n+    message = error.get(\"message\", \"\")\n+    timestamp = error.get(\"timestamp\", 0)",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2153078306",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93669,
        "pr_file": "src/sentry/replays/endpoints/project_replay_summarize_breadcrumbs.py",
        "discussion_id": "2153078306",
        "commented_code": "@@ -50,17 +54,108 @@ def get(self, request: Request, project, replay_id: str) -> Response:\n         ):\n             return self.respond(status=404)\n \n+        filter_params = self.get_filter_params(request, project)\n+\n+        # Fetch the replay's error IDs from the replay_id.\n+        snuba_response = query_replay_instance(\n+            project_id=project.id,\n+            replay_id=replay_id,\n+            start=filter_params[\"start\"],\n+            end=filter_params[\"end\"],\n+            organization=project.organization,\n+            request_user_id=request.user.id,\n+        )\n+\n+        response = process_raw_response(\n+            snuba_response,\n+            fields=request.query_params.getlist(\"field\"),\n+        )\n+\n+        error_ids = response[0].get(\"error_ids\", []) if response else []\n+        error_events = fetch_error_details(project_id=project.id, error_ids=error_ids)\n+\n         return self.paginate(\n             request=request,\n             paginator_cls=GenericOffsetPaginator,\n             data_fn=functools.partial(fetch_segments_metadata, project.id, replay_id),\n-            on_results=analyze_recording_segments,\n+            on_results=functools.partial(analyze_recording_segments, error_events),\n         )\n \n \n+def fetch_error_details(project_id: int, error_ids: list[str]) -> list[dict[str, Any]]:\n+    \"\"\"Fetch error details given error IDs.\"\"\"\n+    try:\n+        node_ids = [Event.generate_node_id(project_id, event_id=id) for id in error_ids]\n+        events = nodestore.backend.get_multi(node_ids)\n+\n+        return [\n+            {\n+                \"category\": \"error\",\n+                \"id\": event_id,\n+                \"title\": data.get(\"title\", \"\"),\n+                \"timestamp\": data.get(\"timestamp\", 0.0),\n+                \"message\": data.get(\"message\", \"\"),\n+            }\n+            for event_id, data in zip(error_ids, events.values())\n+            if data is not None\n+        ]\n+    except Exception as e:\n+        sentry_sdk.capture_exception(e)\n+        return []\n+\n+\n+def generate_error_log_message(error: dict[str, Any]) -> str:\n+    title = error.get(\"title\", \"\")\n+    message = error.get(\"message\", \"\")\n+    timestamp = error.get(\"timestamp\", 0)",
        "comment_created_at": "2025-06-17T20:06:55+00:00",
        "comment_author": "cmanallen",
        "comment_body": "These values should always exist because `fetch_error_details` populates them.  You can define a TypedDict to codify this.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2172555145",
    "pr_number": 94498,
    "pr_file": "src/sentry/replays/endpoints/project_replay_summarize_breadcrumbs.py",
    "created_at": "2025-06-27T17:49:45+00:00",
    "commented_code": "return []\n \n \n-def generate_error_log_message(error: ErrorEvent) -> str:\n+def fetch_trace_connected_errors(\n+    project: Project, trace_ids: list[str], start: float | None, end: float | None\n+) -> list[GroupEvent]:\n+    \"\"\"Fetch error details given trace IDs and return a list of ErrorEvent objects.\"\"\"\n+    try:\n+        if not trace_ids:\n+            return []\n+\n+        queries = []\n+        for trace_id in trace_ids:\n+            snuba_params = SnubaParams(\n+                projects=[project],\n+                start=start,\n+                end=end,\n+                organization=project.organization,\n+            )\n+\n+            # Generate a query for each trace ID. This will be executed in bulk.\n+            error_query = DiscoverQueryBuilder(\n+                Dataset.Events,\n+                params={},\n+                snuba_params=snuba_params,\n+                query=f\"trace:{trace_id}\",\n+                selected_columns=[\n+                    \"id\",\n+                    \"timestamp_ms\",\n+                    \"title\",\n+                    \"message\",\n+                ],\n+                orderby=[\"id\"],\n+                limit=100,\n+                config=QueryBuilderConfig(\n+                    auto_fields=False,\n+                ),\n+            )\n+            queries.append(error_query)\n+\n+        if not queries:\n+            return []\n+\n+        # Execute all queries\n+        results = bulk_snuba_queries(\n+            [query.get_snql_query() for query in queries],\n+            referrer=Referrer.API_REPLAY_SUMMARIZE_BREADCRUMBS.value,\n+        )\n+\n+        # Process results and convert to GroupEvent objects\n+        error_events = []\n+        for result, query in zip(results, queries):\n+            error_data = query.process_results(result)[\"data\"]\n+\n+            for event in error_data:\n+                timestamp_raw = event.get(\"timestamp_ms\", 0)\n+                if isinstance(timestamp_raw, str):\n+                    # The raw timestamp might be returned as a string.\n+                    try:\n+                        dt = datetime.fromisoformat(timestamp_raw.replace(\"Z\", \"+00:00\"))\n+                        timestamp = dt.timestamp() * 1000  # Convert to milliseconds\n+                    except (ValueError, AttributeError):\n+                        timestamp = 0.0\n+                else:\n+                    timestamp = float(timestamp_raw)  # Keep in milliseconds",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2172555145",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 94498,
        "pr_file": "src/sentry/replays/endpoints/project_replay_summarize_breadcrumbs.py",
        "discussion_id": "2172555145",
        "commented_code": "@@ -128,7 +146,111 @@ def fetch_error_details(project_id: int, error_ids: list[str]) -> list[ErrorEven\n         return []\n \n \n-def generate_error_log_message(error: ErrorEvent) -> str:\n+def fetch_trace_connected_errors(\n+    project: Project, trace_ids: list[str], start: float | None, end: float | None\n+) -> list[GroupEvent]:\n+    \"\"\"Fetch error details given trace IDs and return a list of ErrorEvent objects.\"\"\"\n+    try:\n+        if not trace_ids:\n+            return []\n+\n+        queries = []\n+        for trace_id in trace_ids:\n+            snuba_params = SnubaParams(\n+                projects=[project],\n+                start=start,\n+                end=end,\n+                organization=project.organization,\n+            )\n+\n+            # Generate a query for each trace ID. This will be executed in bulk.\n+            error_query = DiscoverQueryBuilder(\n+                Dataset.Events,\n+                params={},\n+                snuba_params=snuba_params,\n+                query=f\"trace:{trace_id}\",\n+                selected_columns=[\n+                    \"id\",\n+                    \"timestamp_ms\",\n+                    \"title\",\n+                    \"message\",\n+                ],\n+                orderby=[\"id\"],\n+                limit=100,\n+                config=QueryBuilderConfig(\n+                    auto_fields=False,\n+                ),\n+            )\n+            queries.append(error_query)\n+\n+        if not queries:\n+            return []\n+\n+        # Execute all queries\n+        results = bulk_snuba_queries(\n+            [query.get_snql_query() for query in queries],\n+            referrer=Referrer.API_REPLAY_SUMMARIZE_BREADCRUMBS.value,\n+        )\n+\n+        # Process results and convert to GroupEvent objects\n+        error_events = []\n+        for result, query in zip(results, queries):\n+            error_data = query.process_results(result)[\"data\"]\n+\n+            for event in error_data:\n+                timestamp_raw = event.get(\"timestamp_ms\", 0)\n+                if isinstance(timestamp_raw, str):\n+                    # The raw timestamp might be returned as a string.\n+                    try:\n+                        dt = datetime.fromisoformat(timestamp_raw.replace(\"Z\", \"+00:00\"))\n+                        timestamp = dt.timestamp() * 1000  # Convert to milliseconds\n+                    except (ValueError, AttributeError):\n+                        timestamp = 0.0\n+                else:\n+                    timestamp = float(timestamp_raw)  # Keep in milliseconds",
        "comment_created_at": "2025-06-27T17:49:45+00:00",
        "comment_author": "michellewzhang",
        "comment_body": "apparently the `timestamp_ms` results from Snuba can be inconsistently a string or a number, so we need to check both cases",
        "pr_file_module": null
      }
    ]
  }
]