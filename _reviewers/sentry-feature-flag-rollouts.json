[
  {
    "discussion_id": "2196039610",
    "pr_number": 95153,
    "pr_file": "src/sentry/options/defaults.py",
    "created_at": "2025-07-09T21:37:21+00:00",
    "commented_code": "flags=FLAG_AUTOMATOR_MODIFIABLE,\n )\n \n+# Taskbroker compression flags\n+register(\n+    \"taskworker.deletions.compression.rollout\",\n+    default=0.0,\n+    flags=FLAG_AUTOMATOR_MODIFIABLE,\n+)",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2196039610",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 95153,
        "pr_file": "src/sentry/options/defaults.py",
        "discussion_id": "2196039610",
        "commented_code": "@@ -3458,6 +3458,233 @@\n     flags=FLAG_AUTOMATOR_MODIFIABLE,\n )\n \n+# Taskbroker compression flags\n+register(\n+    \"taskworker.deletions.compression.rollout\",\n+    default=0.0,\n+    flags=FLAG_AUTOMATOR_MODIFIABLE,\n+)",
        "comment_created_at": "2025-07-09T21:37:21+00:00",
        "comment_author": "markstory",
        "comment_body": "Do we need to enable compression per namespace? Once we know that the general compression flow works, we could opt-tasks into compression via deploys.",
        "pr_file_module": null
      },
      {
        "comment_id": "2198010216",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 95153,
        "pr_file": "src/sentry/options/defaults.py",
        "discussion_id": "2196039610",
        "commented_code": "@@ -3458,6 +3458,233 @@\n     flags=FLAG_AUTOMATOR_MODIFIABLE,\n )\n \n+# Taskbroker compression flags\n+register(\n+    \"taskworker.deletions.compression.rollout\",\n+    default=0.0,\n+    flags=FLAG_AUTOMATOR_MODIFIABLE,\n+)",
        "comment_created_at": "2025-07-10T15:08:53+00:00",
        "comment_author": "enochtangg",
        "comment_body": "Yeah you're right, I think that's unnecessary control. Updated to use a single option `taskworker.enable_compression.rollout` which controls the rollout rate for all tasks that opted in to compression",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2196043967",
    "pr_number": 95153,
    "pr_file": "src/sentry/taskworker/task.py",
    "created_at": "2025-07-09T21:39:51+00:00",
    "commented_code": "f\"The `{key}` header value is of type {type(value)}\"\n                 )\n \n+        parameters_json = orjson.dumps({\"args\": args, \"kwargs\": kwargs})\n+        if self.compression_type == CompressionType.ZSTD:\n+            option_flag = f\"taskworker.{self._namespace.name}.compression.rollout\"",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2196043967",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 95153,
        "pr_file": "src/sentry/taskworker/task.py",
        "discussion_id": "2196043967",
        "commented_code": "@@ -169,12 +177,41 @@ def create_activation(\n                     f\"The `{key}` header value is of type {type(value)}\"\n                 )\n \n+        parameters_json = orjson.dumps({\"args\": args, \"kwargs\": kwargs})\n+        if self.compression_type == CompressionType.ZSTD:\n+            option_flag = f\"taskworker.{self._namespace.name}.compression.rollout\"",
        "comment_created_at": "2025-07-09T21:39:51+00:00",
        "comment_author": "markstory",
        "comment_body": "We could have a single option for enabling compression. We might also want the option check to be done with the `self.compression_type` comparison so that both the task has to have enabled compression and compression needs to be enabled.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2195849444",
    "pr_number": 95159,
    "pr_file": "src/sentry/workflow_engine/processors/delayed_workflow.py",
    "created_at": "2025-07-09T19:48:02+00:00",
    "commented_code": "organization,\n                 ):\n                     for action in filtered_actions:\n-                        action.trigger(workflow_event_data, detector)\n+                        task_params = build_trigger_action_task_params(\n+                            action, detector, workflow_event_data\n+                        )\n+                        trigger_action.delay(**task_params)",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2195849444",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 95159,
        "pr_file": "src/sentry/workflow_engine/processors/delayed_workflow.py",
        "discussion_id": "2195849444",
        "commented_code": "@@ -646,7 +647,10 @@ def fire_actions_for_groups(\n                     organization,\n                 ):\n                     for action in filtered_actions:\n-                        action.trigger(workflow_event_data, detector)\n+                        task_params = build_trigger_action_task_params(\n+                            action, detector, workflow_event_data\n+                        )\n+                        trigger_action.delay(**task_params)",
        "comment_created_at": "2025-07-09T19:48:02+00:00",
        "comment_author": "saponifi3d",
        "comment_body": "might be nice to have a feature flag gate for this while we test out the task, that way we can just flip a switch and fix the task rather than needing to revert everything.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1911002836",
    "pr_number": 83244,
    "pr_file": "src/sentry/tasks/base.py",
    "created_at": "2025-01-10T19:06:15+00:00",
    "commented_code": "def wrapped(func):\n         @wraps(func)\n         def _wrapped(*args, **kwargs):\n-            record_queue_wait_time = record_timing\n-\n-            # Use a try/catch here to contain the blast radius of an exception being unhandled through the options lib\n-            # Unhandled exception could cause all tasks to be effected and not work\n+            do_record_timing_rollout = False\n+            record_timing_rollout = options.get(\"sentry.tasks.record.timing.rollout\")\n+            if record_timing_rollout and record_timing_rollout > random():",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "1911002836",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 83244,
        "pr_file": "src/sentry/tasks/base.py",
        "discussion_id": "1911002836",
        "commented_code": "@@ -84,10 +86,11 @@ def instrumented_task(name, stat_suffix=None, silo_mode=None, record_timing=Fals\n     def wrapped(func):\n         @wraps(func)\n         def _wrapped(*args, **kwargs):\n-            record_queue_wait_time = record_timing\n-\n-            # Use a try/catch here to contain the blast radius of an exception being unhandled through the options lib\n-            # Unhandled exception could cause all tasks to be effected and not work\n+            do_record_timing_rollout = False\n+            record_timing_rollout = options.get(\"sentry.tasks.record.timing.rollout\")\n+            if record_timing_rollout and record_timing_rollout > random():",
        "comment_created_at": "2025-01-10T19:06:15+00:00",
        "comment_author": "markstory",
        "comment_body": "You could use `sentry.features.rollout.in_random_rollout` to handle the comparison with random and handling unset option values.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2154154607",
    "pr_number": 93592,
    "pr_file": "src/sentry/spans/buffer.py",
    "created_at": "2025-06-18T09:41:42+00:00",
    "commented_code": "self.assigned_shards = list(assigned_shards)\n         self.add_buffer_sha: str | None = None\n         self.any_shard_at_limit = False\n+        # Reuse compressor/decompressor objects for better performance\n+        compression_level = options.get(\"spans.buffer.compression.level\")\n+        if compression_level == -1:\n+            self._zstd_compressor = None\n+        else:\n+            self._zstd_compressor = zstandard.ZstdCompressor(level=compression_level)",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2154154607",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93592,
        "pr_file": "src/sentry/spans/buffer.py",
        "discussion_id": "2154154607",
        "commented_code": "@@ -153,6 +154,13 @@ def __init__(self, assigned_shards: list[int]):\n         self.assigned_shards = list(assigned_shards)\n         self.add_buffer_sha: str | None = None\n         self.any_shard_at_limit = False\n+        # Reuse compressor/decompressor objects for better performance\n+        compression_level = options.get(\"spans.buffer.compression.level\")\n+        if compression_level == -1:\n+            self._zstd_compressor = None\n+        else:\n+            self._zstd_compressor = zstandard.ZstdCompressor(level=compression_level)",
        "comment_created_at": "2025-06-18T09:41:42+00:00",
        "comment_author": "jan-auer",
        "comment_body": "Could we either instantiate or override the compressor for each batch separately, so we can change the compression level at runtime without a consumer restart? This would help with enabling this for the first time via options-automator.",
        "pr_file_module": null
      },
      {
        "comment_id": "2154199059",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93592,
        "pr_file": "src/sentry/spans/buffer.py",
        "discussion_id": "2154154607",
        "commented_code": "@@ -153,6 +154,13 @@ def __init__(self, assigned_shards: list[int]):\n         self.assigned_shards = list(assigned_shards)\n         self.add_buffer_sha: str | None = None\n         self.any_shard_at_limit = False\n+        # Reuse compressor/decompressor objects for better performance\n+        compression_level = options.get(\"spans.buffer.compression.level\")\n+        if compression_level == -1:\n+            self._zstd_compressor = None\n+        else:\n+            self._zstd_compressor = zstandard.ZstdCompressor(level=compression_level)",
        "comment_created_at": "2025-06-18T10:00:19+00:00",
        "comment_author": "untitaker",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2165027432",
    "pr_number": 94222,
    "pr_file": "src/sentry/tasks/post_process.py",
    "created_at": "2025-06-24T22:14:04+00:00",
    "commented_code": "with sentry_sdk.start_span(op=\"tasks.post_process_group.workflow_engine.process_workflow\"):\n         process_workflows(workflow_event_data)\n \n+    try:\n+        process_workflows_event.delay(",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2165027432",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 94222,
        "pr_file": "src/sentry/tasks/post_process.py",
        "discussion_id": "2165027432",
        "commented_code": "@@ -976,6 +977,20 @@ def process_workflow_engine(job: PostProcessJob) -> None:\n     with sentry_sdk.start_span(op=\"tasks.post_process_group.workflow_engine.process_workflow\"):\n         process_workflows(workflow_event_data)\n \n+    try:\n+        process_workflows_event.delay(",
        "comment_created_at": "2025-06-24T22:14:04+00:00",
        "comment_author": "saponifi3d",
        "comment_body": "thoughts on adding a feature flag here so we can gate which way we enter the workflow engine? mostly just incase there are any initial rollout issues.",
        "pr_file_module": null
      },
      {
        "comment_id": "2167434670",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 94222,
        "pr_file": "src/sentry/tasks/post_process.py",
        "discussion_id": "2165027432",
        "commented_code": "@@ -976,6 +977,20 @@ def process_workflow_engine(job: PostProcessJob) -> None:\n     with sentry_sdk.start_span(op=\"tasks.post_process_group.workflow_engine.process_workflow\"):\n         process_workflows(workflow_event_data)\n \n+    try:\n+        process_workflows_event.delay(",
        "comment_created_at": "2025-06-25T19:11:29+00:00",
        "comment_author": "kcons",
        "comment_body": "I did that now; rolling out with a single flag seems more reasonable.",
        "pr_file_module": null
      }
    ]
  }
]