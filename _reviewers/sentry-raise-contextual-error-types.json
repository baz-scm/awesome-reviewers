[
  {
    "discussion_id": "2143769867",
    "pr_number": 93256,
    "pr_file": "src/sentry/replays/endpoints/project_replay_summarize_breadcrumbs.py",
    "created_at": "2025-06-12T22:42:52+00:00",
    "commented_code": "+import functools\n+from collections.abc import Generator, Iterator\n+\n+import requests\n+import sentry_sdk\n+from django.conf import settings\n+from drf_spectacular.utils import extend_schema\n+from rest_framework.exceptions import ParseError\n+from rest_framework.request import Request\n+from rest_framework.response import Response\n+\n+from sentry import features\n+from sentry.api.api_owners import ApiOwner\n+from sentry.api.api_publish_status import ApiPublishStatus\n+from sentry.api.base import region_silo_endpoint\n+from sentry.api.bases.project import ProjectEndpoint\n+from sentry.api.paginator import GenericOffsetPaginator\n+from sentry.replays.lib.storage import RecordingSegmentStorageMeta, storage\n+from sentry.replays.usecases.ingest.event_parser import as_log_message\n+from sentry.replays.usecases.reader import fetch_segments_metadata, iter_segment_data\n+from sentry.seer.signed_seer_api import sign_with_seer_secret\n+from sentry.utils import json\n+\n+\n+@region_silo_endpoint\n+@extend_schema(tags=[\"Replays\"])\n+class ProjectReplaySummarizeBreadcrumbsEndpoint(ProjectEndpoint):\n+    owner = ApiOwner.REPLAY\n+    publish_status = {\n+        \"GET\": ApiPublishStatus.EXPERIMENTAL,\n+    }\n+\n+    def __init__(self, **options) -> None:\n+        storage.initialize_client()\n+        super().__init__(**options)\n+\n+    def get(self, request: Request, project, replay_id: str) -> Response:\n+        \"\"\"Return a collection of replay recording segments.\"\"\"\n+        if (\n+            not features.has(\n+                \"organizations:session-replay\", project.organization, actor=request.user\n+            )\n+            or not features.has(\n+                \"organizations:replay-ai-summaries\", project.organization, actor=request.user\n+            )\n+            or not features.has(\n+                \"organizations:gen-ai-features\", project.organization, actor=request.user\n+            )\n+        ):\n+            return self.respond(status=404)\n+\n+        return self.paginate(\n+            request=request,\n+            paginator_cls=GenericOffsetPaginator,\n+            data_fn=functools.partial(fetch_segments_metadata, project.id, replay_id),\n+            on_results=analyze_recording_segments,\n+        )\n+\n+\n+@sentry_sdk.trace\n+def analyze_recording_segments(segments: list[RecordingSegmentStorageMeta]) -> dict[str, Any]:\n+    request_data = json.dumps({\"logs\": get_request_data(iter_segment_data(segments))})\n+\n+    # XXX: I have to deserialize this request so it can be \"automatically\" reserialized by the\n+    # paginate method. This is less than ideal.\n+    return json.loads(make_seer_request(request_data).decode(\"utf-8\"))\n+\n+\n+def make_seer_request(request_data: str) -> bytes:\n+    # XXX: Request isn't streaming. Limitation of Seer authentication. Would be much faster if we\n+    # could stream the request data since the GCS download will (likely) dominate latency.\n+    response = requests.post(\n+        f\"{settings.SEER_AUTOFIX_URL}/v1/automation/summarize/replay/breadcrumbs\",\n+        data=request_data,\n+        headers={\n+            \"content-type\": \"application/json;charset=utf-8\",\n+            **sign_with_seer_secret(request_data.encode()),\n+        },\n+    )\n+    if response.status_code != 200:\n+        raise ParseError(\"A non 200 HTTP status code was returned.\")",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2143769867",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93256,
        "pr_file": "src/sentry/replays/endpoints/project_replay_summarize_breadcrumbs.py",
        "discussion_id": "2143769867",
        "commented_code": "@@ -0,0 +1,95 @@\n+import functools\n+from collections.abc import Generator, Iterator\n+\n+import requests\n+import sentry_sdk\n+from django.conf import settings\n+from drf_spectacular.utils import extend_schema\n+from rest_framework.exceptions import ParseError\n+from rest_framework.request import Request\n+from rest_framework.response import Response\n+\n+from sentry import features\n+from sentry.api.api_owners import ApiOwner\n+from sentry.api.api_publish_status import ApiPublishStatus\n+from sentry.api.base import region_silo_endpoint\n+from sentry.api.bases.project import ProjectEndpoint\n+from sentry.api.paginator import GenericOffsetPaginator\n+from sentry.replays.lib.storage import RecordingSegmentStorageMeta, storage\n+from sentry.replays.usecases.ingest.event_parser import as_log_message\n+from sentry.replays.usecases.reader import fetch_segments_metadata, iter_segment_data\n+from sentry.seer.signed_seer_api import sign_with_seer_secret\n+from sentry.utils import json\n+\n+\n+@region_silo_endpoint\n+@extend_schema(tags=[\"Replays\"])\n+class ProjectReplaySummarizeBreadcrumbsEndpoint(ProjectEndpoint):\n+    owner = ApiOwner.REPLAY\n+    publish_status = {\n+        \"GET\": ApiPublishStatus.EXPERIMENTAL,\n+    }\n+\n+    def __init__(self, **options) -> None:\n+        storage.initialize_client()\n+        super().__init__(**options)\n+\n+    def get(self, request: Request, project, replay_id: str) -> Response:\n+        \"\"\"Return a collection of replay recording segments.\"\"\"\n+        if (\n+            not features.has(\n+                \"organizations:session-replay\", project.organization, actor=request.user\n+            )\n+            or not features.has(\n+                \"organizations:replay-ai-summaries\", project.organization, actor=request.user\n+            )\n+            or not features.has(\n+                \"organizations:gen-ai-features\", project.organization, actor=request.user\n+            )\n+        ):\n+            return self.respond(status=404)\n+\n+        return self.paginate(\n+            request=request,\n+            paginator_cls=GenericOffsetPaginator,\n+            data_fn=functools.partial(fetch_segments_metadata, project.id, replay_id),\n+            on_results=analyze_recording_segments,\n+        )\n+\n+\n+@sentry_sdk.trace\n+def analyze_recording_segments(segments: list[RecordingSegmentStorageMeta]) -> dict[str, Any]:\n+    request_data = json.dumps({\"logs\": get_request_data(iter_segment_data(segments))})\n+\n+    # XXX: I have to deserialize this request so it can be \"automatically\" reserialized by the\n+    # paginate method. This is less than ideal.\n+    return json.loads(make_seer_request(request_data).decode(\"utf-8\"))\n+\n+\n+def make_seer_request(request_data: str) -> bytes:\n+    # XXX: Request isn't streaming. Limitation of Seer authentication. Would be much faster if we\n+    # could stream the request data since the GCS download will (likely) dominate latency.\n+    response = requests.post(\n+        f\"{settings.SEER_AUTOFIX_URL}/v1/automation/summarize/replay/breadcrumbs\",\n+        data=request_data,\n+        headers={\n+            \"content-type\": \"application/json;charset=utf-8\",\n+            **sign_with_seer_secret(request_data.encode()),\n+        },\n+    )\n+    if response.status_code != 200:\n+        raise ParseError(\"A non 200 HTTP status code was returned.\")",
        "comment_created_at": "2025-06-12T22:42:52+00:00",
        "comment_author": "michellewzhang",
        "comment_body": "could we make this error more detailed? when i was getting errors testing in devserver it was tough to figure out that it was coming from here, and what error was happening",
        "pr_file_module": null
      },
      {
        "comment_id": "2143770182",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93256,
        "pr_file": "src/sentry/replays/endpoints/project_replay_summarize_breadcrumbs.py",
        "discussion_id": "2143769867",
        "commented_code": "@@ -0,0 +1,95 @@\n+import functools\n+from collections.abc import Generator, Iterator\n+\n+import requests\n+import sentry_sdk\n+from django.conf import settings\n+from drf_spectacular.utils import extend_schema\n+from rest_framework.exceptions import ParseError\n+from rest_framework.request import Request\n+from rest_framework.response import Response\n+\n+from sentry import features\n+from sentry.api.api_owners import ApiOwner\n+from sentry.api.api_publish_status import ApiPublishStatus\n+from sentry.api.base import region_silo_endpoint\n+from sentry.api.bases.project import ProjectEndpoint\n+from sentry.api.paginator import GenericOffsetPaginator\n+from sentry.replays.lib.storage import RecordingSegmentStorageMeta, storage\n+from sentry.replays.usecases.ingest.event_parser import as_log_message\n+from sentry.replays.usecases.reader import fetch_segments_metadata, iter_segment_data\n+from sentry.seer.signed_seer_api import sign_with_seer_secret\n+from sentry.utils import json\n+\n+\n+@region_silo_endpoint\n+@extend_schema(tags=[\"Replays\"])\n+class ProjectReplaySummarizeBreadcrumbsEndpoint(ProjectEndpoint):\n+    owner = ApiOwner.REPLAY\n+    publish_status = {\n+        \"GET\": ApiPublishStatus.EXPERIMENTAL,\n+    }\n+\n+    def __init__(self, **options) -> None:\n+        storage.initialize_client()\n+        super().__init__(**options)\n+\n+    def get(self, request: Request, project, replay_id: str) -> Response:\n+        \"\"\"Return a collection of replay recording segments.\"\"\"\n+        if (\n+            not features.has(\n+                \"organizations:session-replay\", project.organization, actor=request.user\n+            )\n+            or not features.has(\n+                \"organizations:replay-ai-summaries\", project.organization, actor=request.user\n+            )\n+            or not features.has(\n+                \"organizations:gen-ai-features\", project.organization, actor=request.user\n+            )\n+        ):\n+            return self.respond(status=404)\n+\n+        return self.paginate(\n+            request=request,\n+            paginator_cls=GenericOffsetPaginator,\n+            data_fn=functools.partial(fetch_segments_metadata, project.id, replay_id),\n+            on_results=analyze_recording_segments,\n+        )\n+\n+\n+@sentry_sdk.trace\n+def analyze_recording_segments(segments: list[RecordingSegmentStorageMeta]) -> dict[str, Any]:\n+    request_data = json.dumps({\"logs\": get_request_data(iter_segment_data(segments))})\n+\n+    # XXX: I have to deserialize this request so it can be \"automatically\" reserialized by the\n+    # paginate method. This is less than ideal.\n+    return json.loads(make_seer_request(request_data).decode(\"utf-8\"))\n+\n+\n+def make_seer_request(request_data: str) -> bytes:\n+    # XXX: Request isn't streaming. Limitation of Seer authentication. Would be much faster if we\n+    # could stream the request data since the GCS download will (likely) dominate latency.\n+    response = requests.post(\n+        f\"{settings.SEER_AUTOFIX_URL}/v1/automation/summarize/replay/breadcrumbs\",\n+        data=request_data,\n+        headers={\n+            \"content-type\": \"application/json;charset=utf-8\",\n+            **sign_with_seer_secret(request_data.encode()),\n+        },\n+    )\n+    if response.status_code != 200:\n+        raise ParseError(\"A non 200 HTTP status code was returned.\")",
        "comment_created_at": "2025-06-12T22:43:16+00:00",
        "comment_author": "michellewzhang",
        "comment_body": "(it was a google cloud auth error but i didn't know until i looked at my seer terminal)",
        "pr_file_module": null
      },
      {
        "comment_id": "2143789198",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93256,
        "pr_file": "src/sentry/replays/endpoints/project_replay_summarize_breadcrumbs.py",
        "discussion_id": "2143769867",
        "commented_code": "@@ -0,0 +1,95 @@\n+import functools\n+from collections.abc import Generator, Iterator\n+\n+import requests\n+import sentry_sdk\n+from django.conf import settings\n+from drf_spectacular.utils import extend_schema\n+from rest_framework.exceptions import ParseError\n+from rest_framework.request import Request\n+from rest_framework.response import Response\n+\n+from sentry import features\n+from sentry.api.api_owners import ApiOwner\n+from sentry.api.api_publish_status import ApiPublishStatus\n+from sentry.api.base import region_silo_endpoint\n+from sentry.api.bases.project import ProjectEndpoint\n+from sentry.api.paginator import GenericOffsetPaginator\n+from sentry.replays.lib.storage import RecordingSegmentStorageMeta, storage\n+from sentry.replays.usecases.ingest.event_parser import as_log_message\n+from sentry.replays.usecases.reader import fetch_segments_metadata, iter_segment_data\n+from sentry.seer.signed_seer_api import sign_with_seer_secret\n+from sentry.utils import json\n+\n+\n+@region_silo_endpoint\n+@extend_schema(tags=[\"Replays\"])\n+class ProjectReplaySummarizeBreadcrumbsEndpoint(ProjectEndpoint):\n+    owner = ApiOwner.REPLAY\n+    publish_status = {\n+        \"GET\": ApiPublishStatus.EXPERIMENTAL,\n+    }\n+\n+    def __init__(self, **options) -> None:\n+        storage.initialize_client()\n+        super().__init__(**options)\n+\n+    def get(self, request: Request, project, replay_id: str) -> Response:\n+        \"\"\"Return a collection of replay recording segments.\"\"\"\n+        if (\n+            not features.has(\n+                \"organizations:session-replay\", project.organization, actor=request.user\n+            )\n+            or not features.has(\n+                \"organizations:replay-ai-summaries\", project.organization, actor=request.user\n+            )\n+            or not features.has(\n+                \"organizations:gen-ai-features\", project.organization, actor=request.user\n+            )\n+        ):\n+            return self.respond(status=404)\n+\n+        return self.paginate(\n+            request=request,\n+            paginator_cls=GenericOffsetPaginator,\n+            data_fn=functools.partial(fetch_segments_metadata, project.id, replay_id),\n+            on_results=analyze_recording_segments,\n+        )\n+\n+\n+@sentry_sdk.trace\n+def analyze_recording_segments(segments: list[RecordingSegmentStorageMeta]) -> dict[str, Any]:\n+    request_data = json.dumps({\"logs\": get_request_data(iter_segment_data(segments))})\n+\n+    # XXX: I have to deserialize this request so it can be \"automatically\" reserialized by the\n+    # paginate method. This is less than ideal.\n+    return json.loads(make_seer_request(request_data).decode(\"utf-8\"))\n+\n+\n+def make_seer_request(request_data: str) -> bytes:\n+    # XXX: Request isn't streaming. Limitation of Seer authentication. Would be much faster if we\n+    # could stream the request data since the GCS download will (likely) dominate latency.\n+    response = requests.post(\n+        f\"{settings.SEER_AUTOFIX_URL}/v1/automation/summarize/replay/breadcrumbs\",\n+        data=request_data,\n+        headers={\n+            \"content-type\": \"application/json;charset=utf-8\",\n+            **sign_with_seer_secret(request_data.encode()),\n+        },\n+    )\n+    if response.status_code != 200:\n+        raise ParseError(\"A non 200 HTTP status code was returned.\")",
        "comment_created_at": "2025-06-12T23:00:35+00:00",
        "comment_author": "aliu39",
        "comment_body": "+1, think it'd be better to raise a `ValueError` or `Exception` so the code is 500 instead of 400, since it's not a user triggered error right? And the error message can contain the specific Seer response code",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2155413726",
    "pr_number": 93836,
    "pr_file": "src/sentry/integrations/metric_alerts.py",
    "created_at": "2025-06-18T20:11:41+00:00",
    "commented_code": "except AlertRuleDetector.DoesNotExist:\n             raise ValueError(\"Alert rule detector not found when querying for AlertRuleDetector\")\n \n+        workflow_engine_params = title_link_params.copy()\n+\n         try:\n             open_period_incident = IncidentGroupOpenPeriod.objects.get(\n                 group_open_period_id=metric_issue_context.open_period_identifier\n             )\n-        except IncidentGroupOpenPeriod.DoesNotExist:\n-            raise ValueError(\n-                \"Incident group open period not found when querying for IncidentGroupOpenPeriod\"\n-            )\n-\n-        workflow_engine_params = title_link_params.copy()\n-\n-        workflow_engine_params[\"alert\"] = str(open_period_incident.incident_identifier)\n+            workflow_engine_params[\"alert\"] = str(open_period_incident.incident_identifier)\n+        except IncidentGroupOpenPeriod.DoesNotExist as e:\n+            sentry_sdk.capture_exception(e)",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2155413726",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93836,
        "pr_file": "src/sentry/integrations/metric_alerts.py",
        "discussion_id": "2155413726",
        "commented_code": "@@ -229,18 +230,16 @@ def incident_attachment_info(\n         except AlertRuleDetector.DoesNotExist:\n             raise ValueError(\"Alert rule detector not found when querying for AlertRuleDetector\")\n \n+        workflow_engine_params = title_link_params.copy()\n+\n         try:\n             open_period_incident = IncidentGroupOpenPeriod.objects.get(\n                 group_open_period_id=metric_issue_context.open_period_identifier\n             )\n-        except IncidentGroupOpenPeriod.DoesNotExist:\n-            raise ValueError(\n-                \"Incident group open period not found when querying for IncidentGroupOpenPeriod\"\n-            )\n-\n-        workflow_engine_params = title_link_params.copy()\n-\n-        workflow_engine_params[\"alert\"] = str(open_period_incident.incident_identifier)\n+            workflow_engine_params[\"alert\"] = str(open_period_incident.incident_identifier)\n+        except IncidentGroupOpenPeriod.DoesNotExist as e:\n+            sentry_sdk.capture_exception(e)",
        "comment_created_at": "2025-06-18T20:11:41+00:00",
        "comment_author": "GabeVillalobos",
        "comment_body": "Just an FYI, you don't need to call `capture_exception` with an exception in an except block. It should auto-capture the context already.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2150942729",
    "pr_number": 93569,
    "pr_file": "src/sentry/workflow_engine/processors/delayed_workflow.py",
    "created_at": "2025-06-16T21:41:03+00:00",
    "commented_code": "EVENT_LIMIT = 100\n COMPARISON_INTERVALS_VALUES = {k: v[1] for k, v in COMPARISON_INTERVALS.items()}\n \n-DataConditionGroupGroups = dict[int, set[int]]\n-WorkflowMapping = dict[int, Workflow]\n-WorkflowEnvMapping = dict[int, int | None]\n-DataConditionGroupEvent = dict[tuple[int, int], dict[str, str | None]]\n+GroupId: TypeAlias = int\n+DataConditionGroupId: TypeAlias = int\n+WorkflowId: TypeAlias = int\n+\n+\n+class EventInstance(BaseModel):\n+    event_id: str\n+    occurrence_id: str | None = None\n+\n+    @validator(\"occurrence_id\")\n+    def validate_occurrence_id(cls, v: str | None) -> str | None:\n+        if v is not None and not v.strip():\n+            return None\n+        return v\n+\n+\n+@dataclass(frozen=True)\n+class EventKey:\n+    workflow_id: WorkflowId\n+    group_id: GroupId\n+    dcg_ids: frozenset[DataConditionGroupId]\n+    dcg_type: DataConditionHandler.Group\n+    original_key: str\n+\n+    @classmethod\n+    def from_redis_key(cls, key: str) -> EventKey:\n+        parts = key.split(\":\")\n+        return cls(\n+            workflow_id=int(parts[0]),\n+            group_id=int(parts[1]),\n+            dcg_ids=frozenset(int(dcg_id) for dcg_id in parts[2].split(\",\")),\n+            dcg_type=DataConditionHandler.Group(parts[3]),\n+            original_key=key,\n+        )\n+\n+    def __str__(self) -> str:\n+        return self.original_key\n+\n+    def __hash__(self) -> int:\n+        return hash(self.original_key)\n+\n+    def __eq__(self, other: object) -> bool:\n+        if not isinstance(other, EventKey):\n+            return NotImplemented\n+        return self.original_key == other.original_key\n+\n+\n+@dataclass(frozen=True)\n+class EventRedisData:\n+    \"\"\"\n+    Immutable container for all data from Redis.\n+    Any lookups or summaries or other processing that can be purely derived\n+    from the data should be done on this object so that it's obvious where we're operating\n+    based on parameter data.\n+    \"\"\"\n+\n+    events: Mapping[EventKey, EventInstance]\n+\n+    @classmethod\n+    def from_redis_data(cls, redis_data: dict[str, str]) -> EventRedisData:\n+        events = {}\n+        for key, value in redis_data.items():\n+            try:\n+                event_key = EventKey.from_redis_key(key)\n+                event_instance = EventInstance.parse_raw(value)\n+                events[event_key] = event_instance\n+            except Exception as e:\n+                logger.exception(\n+                    \"Failed to parse workflow event data\",\n+                    extra={\"key\": key, \"value\": value, \"error\": str(e)},\n+                )\n+                raise ValueError(f\"Failed to parse Redis data: {str(e)}\") from e",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2150942729",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93569,
        "pr_file": "src/sentry/workflow_engine/processors/delayed_workflow.py",
        "discussion_id": "2150942729",
        "commented_code": "@@ -65,10 +70,129 @@\n EVENT_LIMIT = 100\n COMPARISON_INTERVALS_VALUES = {k: v[1] for k, v in COMPARISON_INTERVALS.items()}\n \n-DataConditionGroupGroups = dict[int, set[int]]\n-WorkflowMapping = dict[int, Workflow]\n-WorkflowEnvMapping = dict[int, int | None]\n-DataConditionGroupEvent = dict[tuple[int, int], dict[str, str | None]]\n+GroupId: TypeAlias = int\n+DataConditionGroupId: TypeAlias = int\n+WorkflowId: TypeAlias = int\n+\n+\n+class EventInstance(BaseModel):\n+    event_id: str\n+    occurrence_id: str | None = None\n+\n+    @validator(\"occurrence_id\")\n+    def validate_occurrence_id(cls, v: str | None) -> str | None:\n+        if v is not None and not v.strip():\n+            return None\n+        return v\n+\n+\n+@dataclass(frozen=True)\n+class EventKey:\n+    workflow_id: WorkflowId\n+    group_id: GroupId\n+    dcg_ids: frozenset[DataConditionGroupId]\n+    dcg_type: DataConditionHandler.Group\n+    original_key: str\n+\n+    @classmethod\n+    def from_redis_key(cls, key: str) -> EventKey:\n+        parts = key.split(\":\")\n+        return cls(\n+            workflow_id=int(parts[0]),\n+            group_id=int(parts[1]),\n+            dcg_ids=frozenset(int(dcg_id) for dcg_id in parts[2].split(\",\")),\n+            dcg_type=DataConditionHandler.Group(parts[3]),\n+            original_key=key,\n+        )\n+\n+    def __str__(self) -> str:\n+        return self.original_key\n+\n+    def __hash__(self) -> int:\n+        return hash(self.original_key)\n+\n+    def __eq__(self, other: object) -> bool:\n+        if not isinstance(other, EventKey):\n+            return NotImplemented\n+        return self.original_key == other.original_key\n+\n+\n+@dataclass(frozen=True)\n+class EventRedisData:\n+    \"\"\"\n+    Immutable container for all data from Redis.\n+    Any lookups or summaries or other processing that can be purely derived\n+    from the data should be done on this object so that it's obvious where we're operating\n+    based on parameter data.\n+    \"\"\"\n+\n+    events: Mapping[EventKey, EventInstance]\n+\n+    @classmethod\n+    def from_redis_data(cls, redis_data: dict[str, str]) -> EventRedisData:\n+        events = {}\n+        for key, value in redis_data.items():\n+            try:\n+                event_key = EventKey.from_redis_key(key)\n+                event_instance = EventInstance.parse_raw(value)\n+                events[event_key] = event_instance\n+            except Exception as e:\n+                logger.exception(\n+                    \"Failed to parse workflow event data\",\n+                    extra={\"key\": key, \"value\": value, \"error\": str(e)},\n+                )\n+                raise ValueError(f\"Failed to parse Redis data: {str(e)}\") from e",
        "comment_created_at": "2025-06-16T21:41:03+00:00",
        "comment_author": "cathteng",
        "comment_body": "even if one of the items in Redis is added incorrectly, should we still proceed with processing or error out on the whole batch?\r\n\r\nmaybe we should remove it from the batch if we're not going to process it?",
        "pr_file_module": null
      },
      {
        "comment_id": "2150979112",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93569,
        "pr_file": "src/sentry/workflow_engine/processors/delayed_workflow.py",
        "discussion_id": "2150942729",
        "commented_code": "@@ -65,10 +70,129 @@\n EVENT_LIMIT = 100\n COMPARISON_INTERVALS_VALUES = {k: v[1] for k, v in COMPARISON_INTERVALS.items()}\n \n-DataConditionGroupGroups = dict[int, set[int]]\n-WorkflowMapping = dict[int, Workflow]\n-WorkflowEnvMapping = dict[int, int | None]\n-DataConditionGroupEvent = dict[tuple[int, int], dict[str, str | None]]\n+GroupId: TypeAlias = int\n+DataConditionGroupId: TypeAlias = int\n+WorkflowId: TypeAlias = int\n+\n+\n+class EventInstance(BaseModel):\n+    event_id: str\n+    occurrence_id: str | None = None\n+\n+    @validator(\"occurrence_id\")\n+    def validate_occurrence_id(cls, v: str | None) -> str | None:\n+        if v is not None and not v.strip():\n+            return None\n+        return v\n+\n+\n+@dataclass(frozen=True)\n+class EventKey:\n+    workflow_id: WorkflowId\n+    group_id: GroupId\n+    dcg_ids: frozenset[DataConditionGroupId]\n+    dcg_type: DataConditionHandler.Group\n+    original_key: str\n+\n+    @classmethod\n+    def from_redis_key(cls, key: str) -> EventKey:\n+        parts = key.split(\":\")\n+        return cls(\n+            workflow_id=int(parts[0]),\n+            group_id=int(parts[1]),\n+            dcg_ids=frozenset(int(dcg_id) for dcg_id in parts[2].split(\",\")),\n+            dcg_type=DataConditionHandler.Group(parts[3]),\n+            original_key=key,\n+        )\n+\n+    def __str__(self) -> str:\n+        return self.original_key\n+\n+    def __hash__(self) -> int:\n+        return hash(self.original_key)\n+\n+    def __eq__(self, other: object) -> bool:\n+        if not isinstance(other, EventKey):\n+            return NotImplemented\n+        return self.original_key == other.original_key\n+\n+\n+@dataclass(frozen=True)\n+class EventRedisData:\n+    \"\"\"\n+    Immutable container for all data from Redis.\n+    Any lookups or summaries or other processing that can be purely derived\n+    from the data should be done on this object so that it's obvious where we're operating\n+    based on parameter data.\n+    \"\"\"\n+\n+    events: Mapping[EventKey, EventInstance]\n+\n+    @classmethod\n+    def from_redis_data(cls, redis_data: dict[str, str]) -> EventRedisData:\n+        events = {}\n+        for key, value in redis_data.items():\n+            try:\n+                event_key = EventKey.from_redis_key(key)\n+                event_instance = EventInstance.parse_raw(value)\n+                events[event_key] = event_instance\n+            except Exception as e:\n+                logger.exception(\n+                    \"Failed to parse workflow event data\",\n+                    extra={\"key\": key, \"value\": value, \"error\": str(e)},\n+                )\n+                raise ValueError(f\"Failed to parse Redis data: {str(e)}\") from e",
        "comment_created_at": "2025-06-16T22:06:03+00:00",
        "comment_author": "kcons",
        "comment_body": "Thank you for flagging this; I wanted to revisit this before sending it for review, but then the weekend came and I forgot.\r\nYeah, I agree that `log.exception` and continue is probably our preferred approach here. May even make it a required parameter to make the callsites obvious about it.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2140424552",
    "pr_number": 93134,
    "pr_file": "src/sentry/feedback/usecases/feedback_summaries.py",
    "created_at": "2025-06-11T14:57:45+00:00",
    "commented_code": "+import logging\n+import re\n+\n+from sentry.llm.usecases import LLMUseCase, complete_prompt\n+from sentry.utils import metrics\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def make_input_prompt(\n+    feedbacks,\n+):\n+    feedbacks_string = \"\n\".join(f\"- {msg}\" for msg in feedbacks)\n+    return f\"\"\"Task:\n+Instructions: You are an AI assistant that analyzes customer feedback.\n+Create a summary based on the user feedbacks that is at most three sentences, and complete the sentence \"Users say...\". Be concise, but specific in the summary.\n+\n+User Feedbacks:\n+\n+{feedbacks_string}\n+\n+Output Format:\n+\n+Summary: <1-3 sentence summary>\n+\"\"\"\n+\n+\n+SUMMARY_REGEX = re.compile(r\"Summary:\\s*(.*)\", re.DOTALL)\n+\n+\n+@metrics.wraps(\"feedback.summaries\", sample_rate=1.0)\n+def generate_summary(\n+    feedbacks: list[str],\n+):\n+    response = complete_prompt(  # This can throw\n+        usecase=LLMUseCase.FEEDBACK_SUMMARIES,\n+        message=make_input_prompt(feedbacks),\n+        temperature=0.3,\n+        max_output_tokens=150,\n+    )\n+\n+    if response:\n+        summary = parse_response(response)\n+    else:\n+        raise Exception(\"Invalid response from LLM\")",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2140424552",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93134,
        "pr_file": "src/sentry/feedback/usecases/feedback_summaries.py",
        "discussion_id": "2140424552",
        "commented_code": "@@ -0,0 +1,59 @@\n+import logging\n+import re\n+\n+from sentry.llm.usecases import LLMUseCase, complete_prompt\n+from sentry.utils import metrics\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def make_input_prompt(\n+    feedbacks,\n+):\n+    feedbacks_string = \"\\n\".join(f\"- {msg}\" for msg in feedbacks)\n+    return f\"\"\"Task:\n+Instructions: You are an AI assistant that analyzes customer feedback.\n+Create a summary based on the user feedbacks that is at most three sentences, and complete the sentence \"Users say...\". Be concise, but specific in the summary.\n+\n+User Feedbacks:\n+\n+{feedbacks_string}\n+\n+Output Format:\n+\n+Summary: <1-3 sentence summary>\n+\"\"\"\n+\n+\n+SUMMARY_REGEX = re.compile(r\"Summary:\\s*(.*)\", re.DOTALL)\n+\n+\n+@metrics.wraps(\"feedback.summaries\", sample_rate=1.0)\n+def generate_summary(\n+    feedbacks: list[str],\n+):\n+    response = complete_prompt(  # This can throw\n+        usecase=LLMUseCase.FEEDBACK_SUMMARIES,\n+        message=make_input_prompt(feedbacks),\n+        temperature=0.3,\n+        max_output_tokens=150,\n+    )\n+\n+    if response:\n+        summary = parse_response(response)\n+    else:\n+        raise Exception(\"Invalid response from LLM\")",
        "comment_created_at": "2025-06-11T14:57:45+00:00",
        "comment_author": "cmanallen",
        "comment_body": "Raising exceptions like this is discouraged.  Its better (though not required) to raise something contextual.  E.g. `raise ValueError` or `raise MyCustomErrorType(...)`",
        "pr_file_module": null
      },
      {
        "comment_id": "2140778122",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93134,
        "pr_file": "src/sentry/feedback/usecases/feedback_summaries.py",
        "discussion_id": "2140424552",
        "commented_code": "@@ -0,0 +1,59 @@\n+import logging\n+import re\n+\n+from sentry.llm.usecases import LLMUseCase, complete_prompt\n+from sentry.utils import metrics\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def make_input_prompt(\n+    feedbacks,\n+):\n+    feedbacks_string = \"\\n\".join(f\"- {msg}\" for msg in feedbacks)\n+    return f\"\"\"Task:\n+Instructions: You are an AI assistant that analyzes customer feedback.\n+Create a summary based on the user feedbacks that is at most three sentences, and complete the sentence \"Users say...\". Be concise, but specific in the summary.\n+\n+User Feedbacks:\n+\n+{feedbacks_string}\n+\n+Output Format:\n+\n+Summary: <1-3 sentence summary>\n+\"\"\"\n+\n+\n+SUMMARY_REGEX = re.compile(r\"Summary:\\s*(.*)\", re.DOTALL)\n+\n+\n+@metrics.wraps(\"feedback.summaries\", sample_rate=1.0)\n+def generate_summary(\n+    feedbacks: list[str],\n+):\n+    response = complete_prompt(  # This can throw\n+        usecase=LLMUseCase.FEEDBACK_SUMMARIES,\n+        message=make_input_prompt(feedbacks),\n+        temperature=0.3,\n+        max_output_tokens=150,\n+    )\n+\n+    if response:\n+        summary = parse_response(response)\n+    else:\n+        raise Exception(\"Invalid response from LLM\")",
        "comment_created_at": "2025-06-11T18:02:35+00:00",
        "comment_author": "vishnupsatish",
        "comment_body": "Makes sense, I have updated both exceptions in that file to be `ValueError`s",
        "pr_file_module": null
      }
    ]
  }
]