[
  {
    "discussion_id": "2162603737",
    "pr_number": 94131,
    "pr_file": "src/sentry/integrations/bitbucket/search.py",
    "created_at": "2025-06-23T22:05:49+00:00",
    "commented_code": "return Response(\n                         {\"detail\": \"Bitbucket Repository has no issue tracker.\"}, status=400\n                     )\n+                elif \"resource not found\" in str(e):\n+                    lifecycle.record_halt(\n+                        str(SourceCodeSearchEndpointHaltReason.MISSING_REPOSITORY_OR_NO_ACCESS)\n+                    )\n+                    logger.info(",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2162603737",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 94131,
        "pr_file": "src/sentry/integrations/bitbucket/search.py",
        "discussion_id": "2162603737",
        "commented_code": "@@ -59,6 +60,15 @@ def handle_search_issues(self, installation: T, query: str, repo: str | None) ->\n                     return Response(\n                         {\"detail\": \"Bitbucket Repository has no issue tracker.\"}, status=400\n                     )\n+                elif \"resource not found\" in str(e):\n+                    lifecycle.record_halt(\n+                        str(SourceCodeSearchEndpointHaltReason.MISSING_REPOSITORY_OR_NO_ACCESS)\n+                    )\n+                    logger.info(",
        "comment_created_at": "2025-06-23T22:05:49+00:00",
        "comment_author": "iamrajjoshi",
        "comment_body": "the lifecycle should do the log for us. if you want to add extra context, you can do something like\r\n\r\n`lifecycle.add_extras({\"installation_id: installation.model.id, \"repo\": repo})`\r\n\r\nimo we can also delete the other log from above as well",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2175895758",
    "pr_number": 94559,
    "pr_file": "src/sentry/integrations/gitlab/blame.py",
    "created_at": "2025-06-30T20:48:07+00:00",
    "commented_code": "params=params,\n             )\n             client.set_cache(cache_key, response, 60)\n-        except ApiError:\n-            logger.exception(\n+        except ApiError as e:\n+            sentry_sdk.set_context(\n                 \"fetch_file_blame_ApiError\",\n-                extra={\n+                {\n                     \"file_path\": file.path,\n                     \"request_path\": request_path,\n                     \"repo_org_id\": file.repo.organization_id,\n                     \"repo_integration_id\": file.repo.integration_id,\n                 },\n             )\n+            sentry_sdk.capture_exception(error=e, level=\"warning\")",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2175895758",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 94559,
        "pr_file": "src/sentry/integrations/gitlab/blame.py",
        "discussion_id": "2175895758",
        "commented_code": "@@ -106,16 +106,17 @@ def _fetch_file_blame(\n                 params=params,\n             )\n             client.set_cache(cache_key, response, 60)\n-        except ApiError:\n-            logger.exception(\n+        except ApiError as e:\n+            sentry_sdk.set_context(\n                 \"fetch_file_blame_ApiError\",\n-                extra={\n+                {\n                     \"file_path\": file.path,\n                     \"request_path\": request_path,\n                     \"repo_org_id\": file.repo.organization_id,\n                     \"repo_integration_id\": file.repo.integration_id,\n                 },\n             )\n+            sentry_sdk.capture_exception(error=e, level=\"warning\")",
        "comment_created_at": "2025-06-30T20:48:07+00:00",
        "comment_author": "Christinarlong",
        "comment_body": "we dont really action on this error so it's at level error 🤷‍♀️ ",
        "pr_file_module": null
      },
      {
        "comment_id": "2195288668",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 94559,
        "pr_file": "src/sentry/integrations/gitlab/blame.py",
        "discussion_id": "2175895758",
        "commented_code": "@@ -106,16 +106,17 @@ def _fetch_file_blame(\n                 params=params,\n             )\n             client.set_cache(cache_key, response, 60)\n-        except ApiError:\n-            logger.exception(\n+        except ApiError as e:\n+            sentry_sdk.set_context(\n                 \"fetch_file_blame_ApiError\",\n-                extra={\n+                {\n                     \"file_path\": file.path,\n                     \"request_path\": request_path,\n                     \"repo_org_id\": file.repo.organization_id,\n                     \"repo_integration_id\": file.repo.integration_id,\n                 },\n             )\n+            sentry_sdk.capture_exception(error=e, level=\"warning\")",
        "comment_created_at": "2025-07-09T15:13:48+00:00",
        "comment_author": "iamrajjoshi",
        "comment_body": "[SENTRY-44YY](https://sentry.sentry.io/issues/6723530557/events/6da0fc29c8714e1cbac3aec850fc775f/) - this is a pretty frequent error and for quality quarter/auditing our errors, i am going to just log this since we don't action on it.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2138818032",
    "pr_number": 93281,
    "pr_file": "src/sentry/integrations/slack/webhooks/action.py",
    "created_at": "2025-06-10T21:44:18+00:00",
    "commented_code": "else:\n                 self._update_modal(slack_client, external_id, modal_payload, slack_request)\n \n-            metrics.incr(\n-                SLACK_WEBHOOK_GROUP_ACTIONS_SUCCESS_DATADOG_METRIC,\n-                sample_rate=1.0,\n-                tags={\"type\": f\"{self.dialog_type}_modal_open\"},\n-            )\n         except SlackApiError:\n-            metrics.incr(\n-                SLACK_WEBHOOK_GROUP_ACTIONS_FAILURE_DATADOG_METRIC,\n-                sample_rate=1.0,\n-                tags={\"type\": f\"{self.dialog_type}_modal_open\"},\n-            )\n-            _logger.exception(\n+            _logger.info(\n                 \"slack.action.response-error\",",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2138818032",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93281,
        "pr_file": "src/sentry/integrations/slack/webhooks/action.py",
        "discussion_id": "2138818032",
        "commented_code": "@@ -927,24 +894,12 @@ def open_dialog(self, slack_request: SlackActionRequest, group: Group) -> None:\n             else:\n                 self._update_modal(slack_client, external_id, modal_payload, slack_request)\n \n-            metrics.incr(\n-                SLACK_WEBHOOK_GROUP_ACTIONS_SUCCESS_DATADOG_METRIC,\n-                sample_rate=1.0,\n-                tags={\"type\": f\"{self.dialog_type}_modal_open\"},\n-            )\n         except SlackApiError:\n-            metrics.incr(\n-                SLACK_WEBHOOK_GROUP_ACTIONS_FAILURE_DATADOG_METRIC,\n-                sample_rate=1.0,\n-                tags={\"type\": f\"{self.dialog_type}_modal_open\"},\n-            )\n-            _logger.exception(\n+            _logger.info(\n                 \"slack.action.response-error\",",
        "comment_created_at": "2025-06-10T21:44:18+00:00",
        "comment_author": "Christinarlong",
        "comment_body": "for here and in the other `logger.info -`>` logger.exception`, would it be helpful to add the `exc_info` or report the error to sentry?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190854188",
    "pr_number": 94840,
    "pr_file": "src/sentry/net/retry.py",
    "created_at": "2025-07-07T19:16:42+00:00",
    "commented_code": "+import logging\n+from types import TracebackType\n+from typing import Any, Self\n+\n+from urllib3 import BaseHTTPResponse\n+from urllib3.connectionpool import ConnectionPool\n+from urllib3.util.retry import Retry\n+\n+default_logger = logging.getLogger(__name__)\n+\n+\n+class LoggedRetry(Retry):\n+    def __init__(self, logger: logging.Logger | None = None, **kwargs: Any) -> None:\n+        super().__init__(**kwargs)\n+        self.logger = logger or default_logger\n+\n+    def increment(\n+        self,\n+        method: str | None = None,\n+        url: str | None = None,\n+        response: BaseHTTPResponse | None = None,\n+        error: Exception | None = None,\n+        _pool: ConnectionPool | None = None,\n+        _stacktrace: TracebackType | None = None,\n+    ) -> Self:\n+        # Increment uses Retry.new to instantiate a new instance so we need to\n+        # manually propagate the logger as it can't be passed through increment.\n+        retry = super().increment(\n+            method=method,\n+            url=url,\n+            response=response,\n+            error=error,\n+            _pool=_pool,\n+            _stacktrace=_stacktrace,\n+        )\n+        retry.logger = self.logger\n+\n+        extra: dict[str, str | int | None] = {\n+            \"request_method\": method,\n+            \"request_url\": url,\n+            \"retry_total_remaining\": retry.total,\n+        }\n+        if response is not None:\n+            extra[\"response_status\"] = response.status\n+        if error is not None:\n+            extra[\"error\"] = error.__class__.__name__",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2190854188",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 94840,
        "pr_file": "src/sentry/net/retry.py",
        "discussion_id": "2190854188",
        "commented_code": "@@ -0,0 +1,50 @@\n+import logging\n+from types import TracebackType\n+from typing import Any, Self\n+\n+from urllib3 import BaseHTTPResponse\n+from urllib3.connectionpool import ConnectionPool\n+from urllib3.util.retry import Retry\n+\n+default_logger = logging.getLogger(__name__)\n+\n+\n+class LoggedRetry(Retry):\n+    def __init__(self, logger: logging.Logger | None = None, **kwargs: Any) -> None:\n+        super().__init__(**kwargs)\n+        self.logger = logger or default_logger\n+\n+    def increment(\n+        self,\n+        method: str | None = None,\n+        url: str | None = None,\n+        response: BaseHTTPResponse | None = None,\n+        error: Exception | None = None,\n+        _pool: ConnectionPool | None = None,\n+        _stacktrace: TracebackType | None = None,\n+    ) -> Self:\n+        # Increment uses Retry.new to instantiate a new instance so we need to\n+        # manually propagate the logger as it can't be passed through increment.\n+        retry = super().increment(\n+            method=method,\n+            url=url,\n+            response=response,\n+            error=error,\n+            _pool=_pool,\n+            _stacktrace=_stacktrace,\n+        )\n+        retry.logger = self.logger\n+\n+        extra: dict[str, str | int | None] = {\n+            \"request_method\": method,\n+            \"request_url\": url,\n+            \"retry_total_remaining\": retry.total,\n+        }\n+        if response is not None:\n+            extra[\"response_status\"] = response.status\n+        if error is not None:\n+            extra[\"error\"] = error.__class__.__name__",
        "comment_created_at": "2025-07-07T19:16:42+00:00",
        "comment_author": "lobsterkatie",
        "comment_body": "Is there a reason we're logging the error type but not the error message?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2150963793",
    "pr_number": 93617,
    "pr_file": "src/sentry/rules/processing/delayed_processing.py",
    "created_at": "2025-06-16T21:58:01+00:00",
    "commented_code": "),\n     ),\n )\n+@log_context.root()\n def apply_delayed(project_id: int, batch_key: str | None = None, *args: Any, **kwargs: Any) -> None:\n     \"\"\"\n     Grab rules, groups, and events from the Redis buffer, evaluate the \"slow\" conditions in a bulk snuba query, and fire them if they pass\n     \"\"\"\n     sentry_sdk.get_current_scope().set_tag(\"project_id\", project_id)\n+    log_context.add_extras(project_id=project_id)\n     with sentry_sdk.start_span(\n         op=\"delayed_processing.prepare_data\", name=\"Fetch data from buffers in delayed processing\"\n     ):\n         project = fetch_project(project_id)\n         if not project:\n             return\n \n-        log_config = LogConfig.create(project)\n+        if features.has(\"projects:num-events-issue-debugging\", project) or features.has(\n+            \"organizations:workflow-engine-process-workflows\", project.organization\n+        ):\n+            log_context.set_verbose(True)",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2150963793",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 93617,
        "pr_file": "src/sentry/rules/processing/delayed_processing.py",
        "discussion_id": "2150963793",
        "commented_code": "@@ -659,19 +590,24 @@ def cleanup_redis_buffer(\n         ),\n     ),\n )\n+@log_context.root()\n def apply_delayed(project_id: int, batch_key: str | None = None, *args: Any, **kwargs: Any) -> None:\n     \"\"\"\n     Grab rules, groups, and events from the Redis buffer, evaluate the \"slow\" conditions in a bulk snuba query, and fire them if they pass\n     \"\"\"\n     sentry_sdk.get_current_scope().set_tag(\"project_id\", project_id)\n+    log_context.add_extras(project_id=project_id)\n     with sentry_sdk.start_span(\n         op=\"delayed_processing.prepare_data\", name=\"Fetch data from buffers in delayed processing\"\n     ):\n         project = fetch_project(project_id)\n         if not project:\n             return\n \n-        log_config = LogConfig.create(project)\n+        if features.has(\"projects:num-events-issue-debugging\", project) or features.has(\n+            \"organizations:workflow-engine-process-workflows\", project.organization\n+        ):\n+            log_context.set_verbose(True)",
        "comment_created_at": "2025-06-16T21:58:01+00:00",
        "comment_author": "saponifi3d",
        "comment_body": "I like this format for a general purpose logger -- is there a way we can simplify adoption of this inside of workflow engine so we can easily filter down to specific orgs by just setting a flag?\r\n\r\none thought i had around this is that we could initialize the logger with this context data in the processing methods? 🤔 ",
        "pr_file_module": null
      }
    ]
  }
]