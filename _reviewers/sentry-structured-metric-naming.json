[
  {
    "discussion_id": "2193338460",
    "pr_number": 94348,
    "pr_file": "src/sentry/workflow_engine/tasks/actions.py",
    "created_at": "2025-07-08T19:57:45+00:00",
    "commented_code": "+from django.db.models import Value\n+\n+from sentry.eventstore.models import GroupEvent\n+from sentry.eventstream.base import GroupState\n+from sentry.models.activity import Activity\n+from sentry.silo.base import SiloMode\n+from sentry.tasks.base import instrumented_task\n+from sentry.taskworker import config, namespaces\n+from sentry.taskworker.retry import Retry\n+from sentry.utils import metrics\n+from sentry.workflow_engine.models import Action, Detector\n+from sentry.workflow_engine.tasks.utils import build_workflow_event_data_from_event\n+from sentry.workflow_engine.types import WorkflowEventData\n+from sentry.workflow_engine.utils import log_context\n+\n+logger = log_context.get_logger(__name__)\n+\n+\n+def build_trigger_action_task_params(action, detector, event_data: WorkflowEventData):\n+    \"\"\"Build parameters for trigger_action.delay() call.\"\"\"\n+    event_id = None\n+    activity_id = None\n+    occurrence_id = None\n+\n+    if isinstance(event_data.event, GroupEvent):\n+        event_id = event_data.event.event_id\n+        occurrence_id = event_data.event.occurrence_id\n+    elif isinstance(event_data.event, Activity):\n+        activity_id = event_data.event.id\n+\n+    return {\n+        \"action_id\": action.id,\n+        \"detector_id\": detector.id,\n+        \"workflow_id\": getattr(action, \"workflow_id\", None),\n+        \"event_id\": event_id,\n+        \"activity_id\": activity_id,\n+        \"group_id\": event_data.event.group_id,\n+        \"occurrence_id\": occurrence_id,\n+        \"group_state\": event_data.group_state,\n+        \"has_reappeared\": event_data.has_reappeared,\n+        \"has_escalated\": event_data.has_escalated,\n+        \"workflow_env_id\": event_data.workflow_env.id if event_data.workflow_env else None,\n+    }\n+\n+\n+@instrumented_task(\n+    name=\"sentry.workflow_engine.tasks.trigger_action\",\n+    queue=\"workflow_engine.trigger_action\",\n+    acks_late=True,\n+    default_retry_delay=5,\n+    max_retries=3,\n+    soft_time_limit=25,\n+    time_limit=30,\n+    silo_mode=SiloMode.REGION,\n+    taskworker_config=config.TaskworkerConfig(\n+        namespace=namespaces.workflow_engine_tasks,\n+        processing_deadline_duration=30,\n+        retry=Retry(\n+            times=3,\n+            delay=5,\n+        ),\n+    ),\n+)\n+def trigger_action(\n+    action_id: int,\n+    detector_id: int,\n+    workflow_id: int,\n+    event_id: str | None,\n+    activity_id: int | None,\n+    group_id: int,\n+    occurrence_id: str | None,\n+    group_state: GroupState,\n+    has_reappeared: bool,\n+    has_escalated: bool,\n+    workflow_env_id: int | None,\n+) -> None:\n+\n+    # XOR check to ensure exactly one of event_id or activity_id is provided\n+    if (event_id is not None) == (activity_id is not None):\n+        logger.error(\n+            \"Exactly one of event_id or activity_id must be provided\",\n+            extra={\"event_id\": event_id, \"activity_id\": activity_id},\n+        )\n+        raise ValueError(\"Exactly one of event_id or activity_id must be provided\")\n+\n+    # Fetch the action and detector\n+    action = Action.objects.annotate(workflow_id=Value(workflow_id)).get(id=action_id)\n+    detector = Detector.objects.get(id=detector_id)\n+\n+    project_id = detector.project_id\n+\n+    if event_id is not None:\n+        event_data = build_workflow_event_data_from_event(\n+            project_id=project_id,\n+            event_id=event_id,\n+            group_id=group_id,\n+            occurrence_id=occurrence_id,\n+            group_state=group_state,\n+            has_reappeared=has_reappeared,\n+            has_escalated=has_escalated,\n+            workflow_env_id=workflow_env_id,\n+        )\n+    else:\n+        # Here, we probably build the event data from the activity\n+        raise NotImplementedError(\"Activity ID is not supported yet\")\n+\n+    action.trigger(event_data, detector)\n+\n+    metrics.incr(\n+        \"workflow_engine.tasks.trigger_action_task_executed\",\n+        tags={\"action_type\": action.type},\n+        sample_rate=1.0,\n+    )\n+\n+    logger.info(\n+        \"workflow_engine.trigger_workflow_action.success\",\n+        extra={\n+            \"action_id\": action_id,\n+            \"detector_id\": detector_id,\n+            \"workflow_id\": workflow_id,\n+            \"event_id\": event_id,\n+        },\n+    )",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2193338460",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 94348,
        "pr_file": "src/sentry/workflow_engine/tasks/actions.py",
        "discussion_id": "2193338460",
        "commented_code": "@@ -0,0 +1,123 @@\n+from django.db.models import Value\n+\n+from sentry.eventstore.models import GroupEvent\n+from sentry.eventstream.base import GroupState\n+from sentry.models.activity import Activity\n+from sentry.silo.base import SiloMode\n+from sentry.tasks.base import instrumented_task\n+from sentry.taskworker import config, namespaces\n+from sentry.taskworker.retry import Retry\n+from sentry.utils import metrics\n+from sentry.workflow_engine.models import Action, Detector\n+from sentry.workflow_engine.tasks.utils import build_workflow_event_data_from_event\n+from sentry.workflow_engine.types import WorkflowEventData\n+from sentry.workflow_engine.utils import log_context\n+\n+logger = log_context.get_logger(__name__)\n+\n+\n+def build_trigger_action_task_params(action, detector, event_data: WorkflowEventData):\n+    \"\"\"Build parameters for trigger_action.delay() call.\"\"\"\n+    event_id = None\n+    activity_id = None\n+    occurrence_id = None\n+\n+    if isinstance(event_data.event, GroupEvent):\n+        event_id = event_data.event.event_id\n+        occurrence_id = event_data.event.occurrence_id\n+    elif isinstance(event_data.event, Activity):\n+        activity_id = event_data.event.id\n+\n+    return {\n+        \"action_id\": action.id,\n+        \"detector_id\": detector.id,\n+        \"workflow_id\": getattr(action, \"workflow_id\", None),\n+        \"event_id\": event_id,\n+        \"activity_id\": activity_id,\n+        \"group_id\": event_data.event.group_id,\n+        \"occurrence_id\": occurrence_id,\n+        \"group_state\": event_data.group_state,\n+        \"has_reappeared\": event_data.has_reappeared,\n+        \"has_escalated\": event_data.has_escalated,\n+        \"workflow_env_id\": event_data.workflow_env.id if event_data.workflow_env else None,\n+    }\n+\n+\n+@instrumented_task(\n+    name=\"sentry.workflow_engine.tasks.trigger_action\",\n+    queue=\"workflow_engine.trigger_action\",\n+    acks_late=True,\n+    default_retry_delay=5,\n+    max_retries=3,\n+    soft_time_limit=25,\n+    time_limit=30,\n+    silo_mode=SiloMode.REGION,\n+    taskworker_config=config.TaskworkerConfig(\n+        namespace=namespaces.workflow_engine_tasks,\n+        processing_deadline_duration=30,\n+        retry=Retry(\n+            times=3,\n+            delay=5,\n+        ),\n+    ),\n+)\n+def trigger_action(\n+    action_id: int,\n+    detector_id: int,\n+    workflow_id: int,\n+    event_id: str | None,\n+    activity_id: int | None,\n+    group_id: int,\n+    occurrence_id: str | None,\n+    group_state: GroupState,\n+    has_reappeared: bool,\n+    has_escalated: bool,\n+    workflow_env_id: int | None,\n+) -> None:\n+\n+    # XOR check to ensure exactly one of event_id or activity_id is provided\n+    if (event_id is not None) == (activity_id is not None):\n+        logger.error(\n+            \"Exactly one of event_id or activity_id must be provided\",\n+            extra={\"event_id\": event_id, \"activity_id\": activity_id},\n+        )\n+        raise ValueError(\"Exactly one of event_id or activity_id must be provided\")\n+\n+    # Fetch the action and detector\n+    action = Action.objects.annotate(workflow_id=Value(workflow_id)).get(id=action_id)\n+    detector = Detector.objects.get(id=detector_id)\n+\n+    project_id = detector.project_id\n+\n+    if event_id is not None:\n+        event_data = build_workflow_event_data_from_event(\n+            project_id=project_id,\n+            event_id=event_id,\n+            group_id=group_id,\n+            occurrence_id=occurrence_id,\n+            group_state=group_state,\n+            has_reappeared=has_reappeared,\n+            has_escalated=has_escalated,\n+            workflow_env_id=workflow_env_id,\n+        )\n+    else:\n+        # Here, we probably build the event data from the activity\n+        raise NotImplementedError(\"Activity ID is not supported yet\")\n+\n+    action.trigger(event_data, detector)\n+\n+    metrics.incr(\n+        \"workflow_engine.tasks.trigger_action_task_executed\",\n+        tags={\"action_type\": action.type},\n+        sample_rate=1.0,\n+    )\n+\n+    logger.info(\n+        \"workflow_engine.trigger_workflow_action.success\",\n+        extra={\n+            \"action_id\": action_id,\n+            \"detector_id\": detector_id,\n+            \"workflow_id\": workflow_id,\n+            \"event_id\": event_id,\n+        },\n+    )",
        "comment_created_at": "2025-07-08T19:57:45+00:00",
        "comment_author": "saponifi3d",
        "comment_body": "\ud83e\udd14 in general i like this, but it seems replicated with the other log / metric. maybe we should just have a metric at the beginning of this method and another inside of action.trigger? ideally these are as co-located to the event as possible (i was trying to be fancy before having batch metrics, but it seems like that's causing a little confusing / problems)",
        "pr_file_module": null
      },
      {
        "comment_id": "2193452040",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 94348,
        "pr_file": "src/sentry/workflow_engine/tasks/actions.py",
        "discussion_id": "2193338460",
        "commented_code": "@@ -0,0 +1,123 @@\n+from django.db.models import Value\n+\n+from sentry.eventstore.models import GroupEvent\n+from sentry.eventstream.base import GroupState\n+from sentry.models.activity import Activity\n+from sentry.silo.base import SiloMode\n+from sentry.tasks.base import instrumented_task\n+from sentry.taskworker import config, namespaces\n+from sentry.taskworker.retry import Retry\n+from sentry.utils import metrics\n+from sentry.workflow_engine.models import Action, Detector\n+from sentry.workflow_engine.tasks.utils import build_workflow_event_data_from_event\n+from sentry.workflow_engine.types import WorkflowEventData\n+from sentry.workflow_engine.utils import log_context\n+\n+logger = log_context.get_logger(__name__)\n+\n+\n+def build_trigger_action_task_params(action, detector, event_data: WorkflowEventData):\n+    \"\"\"Build parameters for trigger_action.delay() call.\"\"\"\n+    event_id = None\n+    activity_id = None\n+    occurrence_id = None\n+\n+    if isinstance(event_data.event, GroupEvent):\n+        event_id = event_data.event.event_id\n+        occurrence_id = event_data.event.occurrence_id\n+    elif isinstance(event_data.event, Activity):\n+        activity_id = event_data.event.id\n+\n+    return {\n+        \"action_id\": action.id,\n+        \"detector_id\": detector.id,\n+        \"workflow_id\": getattr(action, \"workflow_id\", None),\n+        \"event_id\": event_id,\n+        \"activity_id\": activity_id,\n+        \"group_id\": event_data.event.group_id,\n+        \"occurrence_id\": occurrence_id,\n+        \"group_state\": event_data.group_state,\n+        \"has_reappeared\": event_data.has_reappeared,\n+        \"has_escalated\": event_data.has_escalated,\n+        \"workflow_env_id\": event_data.workflow_env.id if event_data.workflow_env else None,\n+    }\n+\n+\n+@instrumented_task(\n+    name=\"sentry.workflow_engine.tasks.trigger_action\",\n+    queue=\"workflow_engine.trigger_action\",\n+    acks_late=True,\n+    default_retry_delay=5,\n+    max_retries=3,\n+    soft_time_limit=25,\n+    time_limit=30,\n+    silo_mode=SiloMode.REGION,\n+    taskworker_config=config.TaskworkerConfig(\n+        namespace=namespaces.workflow_engine_tasks,\n+        processing_deadline_duration=30,\n+        retry=Retry(\n+            times=3,\n+            delay=5,\n+        ),\n+    ),\n+)\n+def trigger_action(\n+    action_id: int,\n+    detector_id: int,\n+    workflow_id: int,\n+    event_id: str | None,\n+    activity_id: int | None,\n+    group_id: int,\n+    occurrence_id: str | None,\n+    group_state: GroupState,\n+    has_reappeared: bool,\n+    has_escalated: bool,\n+    workflow_env_id: int | None,\n+) -> None:\n+\n+    # XOR check to ensure exactly one of event_id or activity_id is provided\n+    if (event_id is not None) == (activity_id is not None):\n+        logger.error(\n+            \"Exactly one of event_id or activity_id must be provided\",\n+            extra={\"event_id\": event_id, \"activity_id\": activity_id},\n+        )\n+        raise ValueError(\"Exactly one of event_id or activity_id must be provided\")\n+\n+    # Fetch the action and detector\n+    action = Action.objects.annotate(workflow_id=Value(workflow_id)).get(id=action_id)\n+    detector = Detector.objects.get(id=detector_id)\n+\n+    project_id = detector.project_id\n+\n+    if event_id is not None:\n+        event_data = build_workflow_event_data_from_event(\n+            project_id=project_id,\n+            event_id=event_id,\n+            group_id=group_id,\n+            occurrence_id=occurrence_id,\n+            group_state=group_state,\n+            has_reappeared=has_reappeared,\n+            has_escalated=has_escalated,\n+            workflow_env_id=workflow_env_id,\n+        )\n+    else:\n+        # Here, we probably build the event data from the activity\n+        raise NotImplementedError(\"Activity ID is not supported yet\")\n+\n+    action.trigger(event_data, detector)\n+\n+    metrics.incr(\n+        \"workflow_engine.tasks.trigger_action_task_executed\",\n+        tags={\"action_type\": action.type},\n+        sample_rate=1.0,\n+    )\n+\n+    logger.info(\n+        \"workflow_engine.trigger_workflow_action.success\",\n+        extra={\n+            \"action_id\": action_id,\n+            \"detector_id\": detector_id,\n+            \"workflow_id\": workflow_id,\n+            \"event_id\": event_id,\n+        },\n+    )",
        "comment_created_at": "2025-07-08T21:12:34+00:00",
        "comment_author": "iamrajjoshi",
        "comment_body": "did a bit of a overhaul for this",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2073941828",
    "pr_number": 90922,
    "pr_file": "src/sentry/issues/endpoints/group_details.py",
    "created_at": "2025-05-05T18:16:07+00:00",
    "commented_code": "data.update({\"participants\": participants})\n \n             metrics.incr(\n-                \"group.update.http_response\",",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2073941828",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 90922,
        "pr_file": "src/sentry/issues/endpoints/group_details.py",
        "discussion_id": "2073941828",
        "commented_code": "@@ -299,21 +307,21 @@ def get(self, request: Request, group: Group) -> Response:\n             data.update({\"participants\": participants})\n \n             metrics.incr(\n-                \"group.update.http_response\",",
        "comment_created_at": "2025-05-05T18:16:07+00:00",
        "comment_author": "armenzg",
        "comment_body": "All methods used the `group.update` prefix, thus, they were not distinguishable on Datadog.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2133048050",
    "pr_number": 92911,
    "pr_file": "src/sentry/workflow_engine/utils/metrics.py",
    "created_at": "2025-06-06T23:04:02+00:00",
    "commented_code": "+from typing import Any\n+\n+from sentry.utils import metrics\n+from sentry.workflow_engine.utils.workflow_context import WorkflowContext\n+\n+METRIC_PREFIX = \"workflow_engine\"\n+MetricTags = dict[str, Any]\n+\n+\n+def get_metric_name(metric_name: str) -> str:\n+    \"\"\"\n+    Add the prefix to the metric name\n+    metric_name (str): The name of the metric.\n+\n+    Returns:\n+        str: The full metric name with the prefix.\n+    \"\"\"\n+    # TODO - should this include the caller? would probably require us changing a lot of metrics.\n+    return f\"{METRIC_PREFIX}.{metric_name}\"\n+\n+\n+def metrics_incr(\n+    metric_name: str,\n+    value: int = 1,\n+    tags: MetricTags | None = None,\n+) -> None:\n+    \"\"\",\n+    Send a metric for the workflow engine.\n+\n+    metric_name (str): The name of the metric.\n+    value (int): The number to increment by\n+    tags: MetricTags | None: Optional tags to add to the metric.\n+    \"\"\"\n+    ctx = WorkflowContext.get()",
    "repo_full_name": "getsentry/sentry",
    "discussion_comments": [
      {
        "comment_id": "2133048050",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 92911,
        "pr_file": "src/sentry/workflow_engine/utils/metrics.py",
        "discussion_id": "2133048050",
        "commented_code": "@@ -0,0 +1,51 @@\n+from typing import Any\n+\n+from sentry.utils import metrics\n+from sentry.workflow_engine.utils.workflow_context import WorkflowContext\n+\n+METRIC_PREFIX = \"workflow_engine\"\n+MetricTags = dict[str, Any]\n+\n+\n+def get_metric_name(metric_name: str) -> str:\n+    \"\"\"\n+    Add the prefix to the metric name\n+    metric_name (str): The name of the metric.\n+\n+    Returns:\n+        str: The full metric name with the prefix.\n+    \"\"\"\n+    # TODO - should this include the caller? would probably require us changing a lot of metrics.\n+    return f\"{METRIC_PREFIX}.{metric_name}\"\n+\n+\n+def metrics_incr(\n+    metric_name: str,\n+    value: int = 1,\n+    tags: MetricTags | None = None,\n+) -> None:\n+    \"\"\",\n+    Send a metric for the workflow engine.\n+\n+    metric_name (str): The name of the metric.\n+    value (int): The number to increment by\n+    tags: MetricTags | None: Optional tags to add to the metric.\n+    \"\"\"\n+    ctx = WorkflowContext.get()",
        "comment_created_at": "2025-06-06T23:04:02+00:00",
        "comment_author": "kcons",
        "comment_body": "I think it'd be cool if we had `WorkflowContext.get().as_tags()` that gave us a dict suitable for logging/metrics.",
        "pr_file_module": null
      },
      {
        "comment_id": "2133081593",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 92911,
        "pr_file": "src/sentry/workflow_engine/utils/metrics.py",
        "discussion_id": "2133048050",
        "commented_code": "@@ -0,0 +1,51 @@\n+from typing import Any\n+\n+from sentry.utils import metrics\n+from sentry.workflow_engine.utils.workflow_context import WorkflowContext\n+\n+METRIC_PREFIX = \"workflow_engine\"\n+MetricTags = dict[str, Any]\n+\n+\n+def get_metric_name(metric_name: str) -> str:\n+    \"\"\"\n+    Add the prefix to the metric name\n+    metric_name (str): The name of the metric.\n+\n+    Returns:\n+        str: The full metric name with the prefix.\n+    \"\"\"\n+    # TODO - should this include the caller? would probably require us changing a lot of metrics.\n+    return f\"{METRIC_PREFIX}.{metric_name}\"\n+\n+\n+def metrics_incr(\n+    metric_name: str,\n+    value: int = 1,\n+    tags: MetricTags | None = None,\n+) -> None:\n+    \"\"\",\n+    Send a metric for the workflow engine.\n+\n+    metric_name (str): The name of the metric.\n+    value (int): The number to increment by\n+    tags: MetricTags | None: Optional tags to add to the metric.\n+    \"\"\"\n+    ctx = WorkflowContext.get()",
        "comment_created_at": "2025-06-06T23:49:58+00:00",
        "comment_author": "saponifi3d",
        "comment_body": "\ud83e\udd14 i generally like the idea, but i think the dict would be fairly different between logs and metrics since we need to limit in metrics to avoid high cardinality.\r\n\r\nsince we can't share the `as_tags` method, it feels like it'd be fairly similar to provide that method or decorate a centralized log.\r\n\r\n-- also want to make sure, is that what you were thinking the use would be for?",
        "pr_file_module": null
      },
      {
        "comment_id": "2136548337",
        "repo_full_name": "getsentry/sentry",
        "pr_number": 92911,
        "pr_file": "src/sentry/workflow_engine/utils/metrics.py",
        "discussion_id": "2133048050",
        "commented_code": "@@ -0,0 +1,51 @@\n+from typing import Any\n+\n+from sentry.utils import metrics\n+from sentry.workflow_engine.utils.workflow_context import WorkflowContext\n+\n+METRIC_PREFIX = \"workflow_engine\"\n+MetricTags = dict[str, Any]\n+\n+\n+def get_metric_name(metric_name: str) -> str:\n+    \"\"\"\n+    Add the prefix to the metric name\n+    metric_name (str): The name of the metric.\n+\n+    Returns:\n+        str: The full metric name with the prefix.\n+    \"\"\"\n+    # TODO - should this include the caller? would probably require us changing a lot of metrics.\n+    return f\"{METRIC_PREFIX}.{metric_name}\"\n+\n+\n+def metrics_incr(\n+    metric_name: str,\n+    value: int = 1,\n+    tags: MetricTags | None = None,\n+) -> None:\n+    \"\"\",\n+    Send a metric for the workflow engine.\n+\n+    metric_name (str): The name of the metric.\n+    value (int): The number to increment by\n+    tags: MetricTags | None: Optional tags to add to the metric.\n+    \"\"\"\n+    ctx = WorkflowContext.get()",
        "comment_created_at": "2025-06-09T21:36:00+00:00",
        "comment_author": "kcons",
        "comment_body": "Yep, that's where I was thinking. Avoiding it for the reasons you state makes sense to me, though.",
        "pr_file_module": null
      }
    ]
  }
]