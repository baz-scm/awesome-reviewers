[
  {
    "discussion_id": "1695642992",
    "pr_number": 12713,
    "pr_file": "lib/plugins/esbuild/index.js",
    "created_at": "2024-07-29T17:53:48+00:00",
    "commented_code": "this._functions = undefined\n \n     this._buildProperties = _.memoize(this._buildProperties.bind(this))\n+    this._readPackageJson = _.memoize(this._readPackageJson.bind(this))",
    "repo_full_name": "serverless/serverless",
    "discussion_comments": [
      {
        "comment_id": "1695642992",
        "repo_full_name": "serverless/serverless",
        "pr_number": 12713,
        "pr_file": "lib/plugins/esbuild/index.js",
        "discussion_id": "1695642992",
        "commented_code": "@@ -24,6 +24,7 @@ class Esbuild {\n     this._functions = undefined\n \n     this._buildProperties = _.memoize(this._buildProperties.bind(this))\n+    this._readPackageJson = _.memoize(this._readPackageJson.bind(this))",
        "comment_created_at": "2024-07-29T17:53:48+00:00",
        "comment_author": "eahefnawy",
        "comment_body": "I noticed that we read `package.json` multiple times, so i memoized it and replaced the functions below",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "694874727",
    "pr_number": 9876,
    "pr_file": "lib/plugins/aws/deploy/lib/uploadArtifacts.js",
    "created_at": "2021-08-24T13:55:15+00:00",
    "commented_code": "return this.provider.request('S3', 'upload', params);\n   },\n \n+  async isAlreadyUploaded(key) {\n+    try {\n+      const params = {\n+        Bucket: this.bucketName,\n+        Key: key,\n+      };\n+      await this.provider.request('S3', 'headObject', params);\n+      return true;\n+    } catch (error) {\n+      if (error.code !== 'AWS_S3_HEAD_OBJECT_NOT_FOUND') {\n+        throw error;\n+      }\n+      return false;\n+    }\n+  },\n+\n   async uploadZipFile(artifactFilePath) {\n-    const fileName = artifactFilePath.split(path.sep).pop();\n+    const stats = (() => {\n+      try {\n+        return fs.statSync(artifactFilePath);\n+      } catch (error) {\n+        throw new ServerlessError(\n+          `Cannot read file artifact \"${artifactFilePath}\": ${error.message}`,\n+          'INACCESSIBLE_FILE_ARTIFACT'\n+        );\n+      }\n+    })();\n \n     // TODO refactor to be async (use util function to compute checksum async)\n     const data = fs.readFileSync(artifactFilePath);\n     const fileHash = crypto.createHash('sha256').update(data).digest('base64');",
    "repo_full_name": "serverless/serverless",
    "discussion_comments": [
      {
        "comment_id": "694874727",
        "repo_full_name": "serverless/serverless",
        "pr_number": 9876,
        "pr_file": "lib/plugins/aws/deploy/lib/uploadArtifacts.js",
        "discussion_id": "694874727",
        "commented_code": "@@ -50,14 +51,52 @@ module.exports = {\n     return this.provider.request('S3', 'upload', params);\n   },\n \n+  async isAlreadyUploaded(key) {\n+    try {\n+      const params = {\n+        Bucket: this.bucketName,\n+        Key: key,\n+      };\n+      await this.provider.request('S3', 'headObject', params);\n+      return true;\n+    } catch (error) {\n+      if (error.code !== 'AWS_S3_HEAD_OBJECT_NOT_FOUND') {\n+        throw error;\n+      }\n+      return false;\n+    }\n+  },\n+\n   async uploadZipFile(artifactFilePath) {\n-    const fileName = artifactFilePath.split(path.sep).pop();\n+    const stats = (() => {\n+      try {\n+        return fs.statSync(artifactFilePath);\n+      } catch (error) {\n+        throw new ServerlessError(\n+          `Cannot read file artifact \"${artifactFilePath}\": ${error.message}`,\n+          'INACCESSIBLE_FILE_ARTIFACT'\n+        );\n+      }\n+    })();\n \n     // TODO refactor to be async (use util function to compute checksum async)\n     const data = fs.readFileSync(artifactFilePath);\n     const fileHash = crypto.createHash('sha256').update(data).digest('base64');",
        "comment_created_at": "2021-08-24T13:55:15+00:00",
        "comment_author": "medikoo",
        "comment_body": "It will be nice to introduce (with other PR) a refactor where we rely on memoized [`getHashForFilePath` util](https://github.com/serverless/serverless/blob/c6b5a5f4c6f206c2015267fd7c6b2ef9780c7dfd/lib/plugins/aws/package/lib/getHashForFilePath.js)",
        "pr_file_module": null
      },
      {
        "comment_id": "697469839",
        "repo_full_name": "serverless/serverless",
        "pr_number": 9876,
        "pr_file": "lib/plugins/aws/deploy/lib/uploadArtifacts.js",
        "discussion_id": "694874727",
        "commented_code": "@@ -50,14 +51,52 @@ module.exports = {\n     return this.provider.request('S3', 'upload', params);\n   },\n \n+  async isAlreadyUploaded(key) {\n+    try {\n+      const params = {\n+        Bucket: this.bucketName,\n+        Key: key,\n+      };\n+      await this.provider.request('S3', 'headObject', params);\n+      return true;\n+    } catch (error) {\n+      if (error.code !== 'AWS_S3_HEAD_OBJECT_NOT_FOUND') {\n+        throw error;\n+      }\n+      return false;\n+    }\n+  },\n+\n   async uploadZipFile(artifactFilePath) {\n-    const fileName = artifactFilePath.split(path.sep).pop();\n+    const stats = (() => {\n+      try {\n+        return fs.statSync(artifactFilePath);\n+      } catch (error) {\n+        throw new ServerlessError(\n+          `Cannot read file artifact \"${artifactFilePath}\": ${error.message}`,\n+          'INACCESSIBLE_FILE_ARTIFACT'\n+        );\n+      }\n+    })();\n \n     // TODO refactor to be async (use util function to compute checksum async)\n     const data = fs.readFileSync(artifactFilePath);\n     const fileHash = crypto.createHash('sha256').update(data).digest('base64');",
        "comment_created_at": "2021-08-27T14:06:43+00:00",
        "comment_author": "remi00",
        "comment_body": "Sounds like a good idea. Other PR sounds good too :)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "695733829",
    "pr_number": 9876,
    "pr_file": "lib/plugins/aws/deploy/lib/cleanupS3Bucket.js",
    "created_at": "2021-08-25T13:07:50+00:00",
    "commented_code": "5\n     );\n \n-    const service = this.serverless.service.service;\n-    const stage = this.provider.getStage();\n-    const prefix = this.provider.getDeploymentPrefix();\n-\n-    const response = await this.provider.request('S3', 'listObjectsV2', {\n-      Bucket: this.bucketName,\n-      Prefix: `${prefix}/${service}/${stage}`,\n-    });\n-    const stacks = findAndGroupDeployments(response, prefix, service, stage);\n+    const stacks = await this.findDeployments();",
    "repo_full_name": "serverless/serverless",
    "discussion_comments": [
      {
        "comment_id": "695733829",
        "repo_full_name": "serverless/serverless",
        "pr_number": 9876,
        "pr_file": "lib/plugins/aws/deploy/lib/cleanupS3Bucket.js",
        "discussion_id": "695733829",
        "commented_code": "@@ -12,18 +10,19 @@ module.exports = {\n       5\n     );\n \n-    const service = this.serverless.service.service;\n-    const stage = this.provider.getStage();\n-    const prefix = this.provider.getDeploymentPrefix();\n-\n-    const response = await this.provider.request('S3', 'listObjectsV2', {\n-      Bucket: this.bucketName,\n-      Prefix: `${prefix}/${service}/${stage}`,\n-    });\n-    const stacks = findAndGroupDeployments(response, prefix, service, stage);\n+    const stacks = await this.findDeployments();",
        "comment_created_at": "2021-08-25T13:07:50+00:00",
        "comment_author": "medikoo",
        "comment_body": "As we now need to download and inspect CF template to know which artifacts are involved in given deployment I think it'll be great to be more optimal.\r\n\r\nI wouldn't in all cases read all CF templates, but only those from which we need information. Therefore I would refolumate approach to instead of introducing `findDeployments` in current shape. Introduce:\r\n\r\n####  `resolveDeploymentsData()`\r\n\r\nWhich should return following:\r\n\r\n```javascript\r\n{\r\n  deployments: [[..], [..]] // Each item contains full paths to all files in given deployment folder\r\n  artifacts: [..]// full paths to all artifact files\r\n }\r\n```\r\n\r\n####  `resolveDeploymentsArtifacts(deploymentsData, from, to)`\r\n\r\nMethod which takes CF templates for deployments resolved as `deploymentsData.deployments.slice(from, to)` and returns (in form of set) all artifacts found referenced in those templates and in `deploymentsData.artifacts`\r\n\r\n--- \r\n\r\nHaving above, in `cleanupS3Bucket`, we can be more optimal and do as:\r\n\r\n```javascript\r\nconst deploymentsData = await resolveDeploymentsData()\r\nif (deploymentsData.deployments.length > stacksToKeepCount) {\r\n  const usedArtifacts = await resolveDeploymentsArtifacts(deploymentsData, -stacksToKeepCount);\r\n  // let's also upgrade `removeObjects` to introduce `{ Key: <path> }` wrap internally\r\n  await removeObjects([\r\n    ..._.flatten(deploymentsData.deployments.slice(0, -stacksToKeepCount),\r\n    ...deploymentsData.artifacts.filter(artifactPath => !usedArtifacts.has(artifactPath))])\r\n}\r\n```\r\n\r\nAnd in case where we upload, we may confirm on fact of artifact existence via:\r\n\r\n```javascript\r\nconst artifactBaseNames = new Set(Array.from(deploymentsData.artifacts, artifactPath = path.basename(artifactPath, \".zip\")));\r\n\r\nif (artifactBaseNames.has(artifactToUploadHash)) // already uploaded\r\n```\r\n\r\nAnd I think there are no other places we will really introduce changes\r\n\r\n\r\n\r\n\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "697466887",
        "repo_full_name": "serverless/serverless",
        "pr_number": 9876,
        "pr_file": "lib/plugins/aws/deploy/lib/cleanupS3Bucket.js",
        "discussion_id": "695733829",
        "commented_code": "@@ -12,18 +10,19 @@ module.exports = {\n       5\n     );\n \n-    const service = this.serverless.service.service;\n-    const stage = this.provider.getStage();\n-    const prefix = this.provider.getDeploymentPrefix();\n-\n-    const response = await this.provider.request('S3', 'listObjectsV2', {\n-      Bucket: this.bucketName,\n-      Prefix: `${prefix}/${service}/${stage}`,\n-    });\n-    const stacks = findAndGroupDeployments(response, prefix, service, stage);\n+    const stacks = await this.findDeployments();",
        "comment_created_at": "2021-08-27T14:02:36+00:00",
        "comment_author": "remi00",
        "comment_body": "I like the proposal but also have some doubts. This approach is less expressive (hiding the intention at least for the upload) and it sounds a little like a premature optimization. Are there any statistics that would confirm that the affected functionality (cleanup S3 bucket, rollback, deploy list) has to be optimized? For the upload I can already declare that `headObject` is almost seamless, compared to actual upload. For current `findDeployments` it will just read five JSON files...\r\nChanging is possible, however, considering how much changes (including plenty of test cases to be updated as well), I think we should be really sure they are required and they are bringing the value.",
        "pr_file_module": null
      },
      {
        "comment_id": "698406460",
        "repo_full_name": "serverless/serverless",
        "pr_number": 9876,
        "pr_file": "lib/plugins/aws/deploy/lib/cleanupS3Bucket.js",
        "discussion_id": "695733829",
        "commented_code": "@@ -12,18 +10,19 @@ module.exports = {\n       5\n     );\n \n-    const service = this.serverless.service.service;\n-    const stage = this.provider.getStage();\n-    const prefix = this.provider.getDeploymentPrefix();\n-\n-    const response = await this.provider.request('S3', 'listObjectsV2', {\n-      Bucket: this.bucketName,\n-      Prefix: `${prefix}/${service}/${stage}`,\n-    });\n-    const stacks = findAndGroupDeployments(response, prefix, service, stage);\n+    const stacks = await this.findDeployments();",
        "comment_created_at": "2021-08-30T11:27:36+00:00",
        "comment_author": "medikoo",
        "comment_body": "> Are there any statistics that would confirm that the affected functionality (cleanup S3 bucket, rollback, deploy list) has to be optimized? \r\n\r\nI believe that we need to change how things works only for _cleanup_ operation, and there are no changes to _rollback_ and _deploy list_ planned per spec. Do you see that some are needed? If so can you please elaborate in an issue, as that definitely needs to be included in a spec then.\r\n\r\nNow when speaking strictly about _cleanup_, this change will introduce a need to download CF templates as hosted on S3. In case of large services those templates can be big. I assume that downloading those templates will be relatively fast when comparing to the deploy operation in general, but I suspect that in case of large services it still may add 0.1-0.5s of overhead to the command, which we can avoid in some scenarios.\r\n\r\nAlso do you see anything particularly challenging in what I proposed? My feeling is that this doesn't make our implementation more complex, but rather better organized.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "695742352",
    "pr_number": 9876,
    "pr_file": "lib/plugins/aws/deploy/lib/uploadArtifacts.js",
    "created_at": "2021-08-25T13:17:37+00:00",
    "commented_code": "return this.provider.request('S3', 'upload', params);\n   },\n \n+  async isAlreadyUploaded(key) {\n+    try {\n+      const params = {\n+        Bucket: this.bucketName,\n+        Key: key,\n+      };\n+      await this.provider.request('S3', 'headObject', params);",
    "repo_full_name": "serverless/serverless",
    "discussion_comments": [
      {
        "comment_id": "695742352",
        "repo_full_name": "serverless/serverless",
        "pr_number": 9876,
        "pr_file": "lib/plugins/aws/deploy/lib/uploadArtifacts.js",
        "discussion_id": "695742352",
        "commented_code": "@@ -50,14 +51,52 @@ module.exports = {\n     return this.provider.request('S3', 'upload', params);\n   },\n \n+  async isAlreadyUploaded(key) {\n+    try {\n+      const params = {\n+        Bucket: this.bucketName,\n+        Key: key,\n+      };\n+      await this.provider.request('S3', 'headObject', params);",
        "comment_created_at": "2021-08-25T13:17:37+00:00",
        "comment_author": "medikoo",
        "comment_body": "I guess it's more efficient to resolve all file names with one call and confirm against that, instead of issuing AWS request per each file name",
        "pr_file_module": null
      },
      {
        "comment_id": "697466898",
        "repo_full_name": "serverless/serverless",
        "pr_number": 9876,
        "pr_file": "lib/plugins/aws/deploy/lib/uploadArtifacts.js",
        "discussion_id": "695742352",
        "commented_code": "@@ -50,14 +51,52 @@ module.exports = {\n     return this.provider.request('S3', 'upload', params);\n   },\n \n+  async isAlreadyUploaded(key) {\n+    try {\n+      const params = {\n+        Bucket: this.bucketName,\n+        Key: key,\n+      };\n+      await this.provider.request('S3', 'headObject', params);",
        "comment_created_at": "2021-08-27T14:02:37+00:00",
        "comment_author": "remi00",
        "comment_body": "As in my previous response, changing this sounds like premature optimization. AWS SDK for JS is by far robust. It reuses  connections from HTTP pool, and since we limit number of simultaneous upload processes, it takes advantage of that mechanism very nicely. Doing `headObject` is seamless. Checked within the project using Serverless with more than 25 Lambda functions.",
        "pr_file_module": null
      },
      {
        "comment_id": "698411133",
        "repo_full_name": "serverless/serverless",
        "pr_number": 9876,
        "pr_file": "lib/plugins/aws/deploy/lib/uploadArtifacts.js",
        "discussion_id": "695742352",
        "commented_code": "@@ -50,14 +51,52 @@ module.exports = {\n     return this.provider.request('S3', 'upload', params);\n   },\n \n+  async isAlreadyUploaded(key) {\n+    try {\n+      const params = {\n+        Bucket: this.bucketName,\n+        Key: key,\n+      };\n+      await this.provider.request('S3', 'headObject', params);",
        "comment_created_at": "2021-08-30T11:35:30+00:00",
        "comment_author": "medikoo",
        "comment_body": "@remi00 ok, so situation with AWS SDK, is that it creates some sort of long-polling connection? and then works in similar way as e.g. SSE on browser side?\r\n\r\nWhat are the observed times of above requests when done locally? (e.g. if you read time at 59 line and then check difference at 61 or 63, what difference you will observe)\r\n\r\nNote that reusing collection of obtained names will make this logic also simpler, this try/catch looks a bit convoluted",
        "pr_file_module": null
      },
      {
        "comment_id": "699399764",
        "repo_full_name": "serverless/serverless",
        "pr_number": 9876,
        "pr_file": "lib/plugins/aws/deploy/lib/uploadArtifacts.js",
        "discussion_id": "695742352",
        "commented_code": "@@ -50,14 +51,52 @@ module.exports = {\n     return this.provider.request('S3', 'upload', params);\n   },\n \n+  async isAlreadyUploaded(key) {\n+    try {\n+      const params = {\n+        Bucket: this.bucketName,\n+        Key: key,\n+      };\n+      await this.provider.request('S3', 'headObject', params);",
        "comment_created_at": "2021-08-31T14:47:25+00:00",
        "comment_author": "remi00",
        "comment_body": "Thanks, I will revisit this suggestion.\r\n\r\nHere some excerpt from the logs as advised. No patterns on the length I think.\r\n\r\n\r\n```\r\nserverless/***/dev/code-artifacts/83f3ab22220623a3e0a1c1006019aa0ffabf38f4a45968dfc4bc4502c7ab61d0.zip 218\r\nserverless/***/dev/code-artifacts/83f3ab22220623a3e0a1c1006019aa0ffabf38f4a45968dfc4bc4502c7ab61d0.zip 176\r\nserverless/***/dev/code-artifacts/b511cc239fc18dc2d18346fe920c3d0e121b91d20c81ac618c1d7b0f769959e7.zip 117\r\nserverless/***/dev/code-artifacts/485c82da1d982767d9eb21fade33ba806a7abd8dd771ef3de8543eb31629f669.zip 288\r\nserverless/***/dev/code-artifacts/1b833dd49100cb6c9e36b6c6d20415e7c466d6bb5f6923140cc819b9abc04103.zip 230\r\nserverless/***/dev/code-artifacts/dffd763a7238dbe18460add36070d3209ddac83cba220d3f5ecd8022a390601d.zip 153\r\nserverless/***/dev/code-artifacts/da861085fbf6494e427b50f32f9c927e7464d8e520c9713d5f09b7cc54ab8808.zip 204\r\nserverless/***/dev/code-artifacts/575d4ad4cec1c94d98e10093db22647a03bef195ba8af3cfedd5d5aac23f57af.zip 161\r\nserverless/***/dev/code-artifacts/d74729b6433436d3cceb680e14beec0395de1ff8caec18a2281839fc11cb7424.zip 234\r\nserverless/***/dev/code-artifacts/b63d6a4580589d317eeb70fc42a9af30f84fdb24cab4942231107cf83c5618c7.zip 175\r\nserverless/***/dev/code-artifacts/e4b3bac7adf8c7225cb85f1e325035c54a619da6826c300684a787f83c3e12e2.zip 259\r\nserverless/***/dev/code-artifacts/8ba0344dbe71bc345b24578f1c47bfac539d4cdb39fe28f82db1bf2977add7e2.zip 172\r\nserverless/***/dev/code-artifacts/77439124b0d653e31791603625324e307caeb512f6a4e56f741ec22823714f2d.zip 265\r\nserverless/***/dev/code-artifacts/54894d0ef4600e96d1daccd3b7f7044e594875e2f517876500dc2e2041d77ac1.zip 159\r\nserverless/***/dev/code-artifacts/fc90c0887ffb90dcd665f75b34fb6217d79af15dd933d2abe2205918e8db6172.zip 252\r\nserverless/***/dev/code-artifacts/bd026b838d11e1eb1dc94ba15cf4fe8d69914418a6526aee00e54daed314d40f.zip 222\r\nserverless/***/dev/code-artifacts/3ada44b12dcb546cd20822d72299e60ea65ed0afe34b87af5c43c61bfdc184a0.zip 358\r\nserverless/***/dev/code-artifacts/61462cfdf3d771a3db7ec9800e662a3adc628703e19d9937979639cbbf030c4b.zip 173\r\nserverless/***/dev/code-artifacts/49219f38ee34b704f85b97c703254f4a221daa0df66d057a0725bc0922c508f5.zip 245\r\nserverless/***/dev/code-artifacts/41ee01fc2e0ba0a24754b5fd0b9e624e8da4643f926fd2ac629312e8590d01cb.zip 146\r\nserverless/***/dev/code-artifacts/ca1c96a41c510f746cb672a4754814e3adeaf7f8c7e0d3e4988bdb88c7779054.zip 241\r\nserverless/***/dev/code-artifacts/351e89375570a91b23b6c95b395adbe9a1ba0f19930cc44e7521bf54f19809b5.zip 127\r\nserverless/***/dev/code-artifacts/3fd826bcaa3151bc6686d6fc71ffa22f217d5f3fa36aeefcf24ccf20078a1fbc.zip 224\r\nserverless/***/dev/code-artifacts/d16fbf824d82059a66774daa640b0eb86a5959547597357646a8711a09005029.zip 127\r\nserverless/***/dev/code-artifacts/43aac618fcbdc02de0c3c305c066304d161007671a33f52b2a031e79da77ca56.zip 232\r\nserverless/***/dev/code-artifacts/793a9307b335f1c3983c3a0772bdde95cd9f13f6a1d2749c8e7f2990c79f059d.zip 127\r\nserverless/***/dev/code-artifacts/f1277ced42f21780f1bb700c83e527e841cf5cf86568867059de5bb51dfb6fc3.zip 231\r\nserverless/***/dev/code-artifacts/9191bc6f87cc446b5c1443311b674ca0699ec8004a702a5fdd3c64ef8f79704f.zip 148\r\nserverless/***/dev/code-artifacts/17c0bfd498cd9e1099893c7239d541466db18d979f192bacdd1bebc0e3357ce1.zip 150\r\nserverless/***/dev/code-artifacts/799693245bf0be41ec88c98a9b6abb29bc23d39bf75fb9f994ef98b95020548d.zip 333\r\nserverless/***/dev/code-artifacts/5a047b1f7d7740e90d2c70c1d6dba9c08818423c845a88bdb6505a5608e7ad7c.zip 315\r\nserverless/***/dev/code-artifacts/4d4b58c3a0669b8aa0651926bc43fa7d9def1d709b6df22b1492f0eecb470604.zip 119\r\nserverless/***/dev/code-artifacts/c3bba113d4fa343160618174c07f5558386f00bffe6a78d4c55aac0afe77743d.zip 117\r\nserverless/***/dev/code-artifacts/70b74fd9325fbeaa9d358db1c462ce2c763ccce00c0044c595182627ef5257ae.zip 236\r\nserverless/***/dev/code-artifacts/4117fda13f45243c05b873400f1793863f03abde1417bf36eb63e6a88cf8aa69.zip 288\r\nserverless/***/dev/code-artifacts/15687690138badea2a00a73a72cfe55444e1b3d807688614a9aa0db657477706.zip 193\r\nserverless/***/dev/code-artifacts/1f446e0670f8b80edf91de08684bbfcab2779e82486037b09787498d1ade3868.zip 219\r\nserverless/***/dev/code-artifacts/fa28e148a46344b92cfad5072d689b05239cf60381fabeaa0b1c4e375b6882b5.zip 140\r\nserverless/***/dev/code-artifacts/82e85fa46cfc7824ba73d80675aa17dfdc5cdac36c241065cadd4828eee4f8cc.zip 223\r\nserverless/***/dev/code-artifacts/7b42199932a71a568a8bc009b4c73e38075f4a520b00740dcba77d023983069e.zip 140\r\nserverless/***/dev/code-artifacts/e622158507b1b09ab1dd38ec8a32deaeff5ecc5df747d64b31d45509ed3b728a.zip 212\r\nserverless/***/dev/code-artifacts/7452b7ed022dccfe826c1525bcd8dde91c52ec0f49a8602338aecaca66f6dd0b.zip 139\r\nserverless/***/dev/code-artifacts/4ab258938a9fe37d835411179248303cb3888e6d0525f0305902346179e32ea5.zip 208\r\nserverless/***/dev/code-artifacts/336fb9ecd70f00118f67d2be150b4fd924f5d8cf5cc517ab5c03d476f350fb31.zip 131\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1136072225",
    "pr_number": 11782,
    "pr_file": "lib/cli/interactive-setup/console-dev-mode-feed.js",
    "created_at": "2023-03-14T19:16:22+00:00",
    "commented_code": "+'use strict';\n+\n+const { writeText, style } = require('@serverless/utils/log');\n+const WebSocket = require('ws');\n+const chalk = require('chalk');\n+const { devModeFeed } = require('@serverless/utils/lib/auth/urls');\n+const consoleUi = require('@serverless/utils/console-ui');\n+const streamBuffers = require('stream-buffers');\n+const apiRequest = require('@serverless/utils/api-request');\n+const promptWithHistory = require('@serverless/utils/inquirer/prompt-with-history');\n+\n+const streamBuff = new streamBuffers.ReadableStreamBuffer({\n+  frequency: 500,\n+  chunkSize: 2048 * 1000000,\n+});\n+\n+const consoleMonitoringCounter = {\n+  logBatches: 0,\n+  events: 0,\n+  responses: 0,\n+};\n+\n+const handleSocketMessage = (data) => {\n+  try {\n+    const splitData = data.toString('utf-8').split(';;;');\n+    const jsonArray = splitData\n+      .filter((item) => item !== '' && Array.isArray(JSON.parse(item)))",
    "repo_full_name": "serverless/serverless",
    "discussion_comments": [
      {
        "comment_id": "1136072225",
        "repo_full_name": "serverless/serverless",
        "pr_number": 11782,
        "pr_file": "lib/cli/interactive-setup/console-dev-mode-feed.js",
        "discussion_id": "1136072225",
        "commented_code": "@@ -0,0 +1,187 @@\n+'use strict';\n+\n+const { writeText, style } = require('@serverless/utils/log');\n+const WebSocket = require('ws');\n+const chalk = require('chalk');\n+const { devModeFeed } = require('@serverless/utils/lib/auth/urls');\n+const consoleUi = require('@serverless/utils/console-ui');\n+const streamBuffers = require('stream-buffers');\n+const apiRequest = require('@serverless/utils/api-request');\n+const promptWithHistory = require('@serverless/utils/inquirer/prompt-with-history');\n+\n+const streamBuff = new streamBuffers.ReadableStreamBuffer({\n+  frequency: 500,\n+  chunkSize: 2048 * 1000000,\n+});\n+\n+const consoleMonitoringCounter = {\n+  logBatches: 0,\n+  events: 0,\n+  responses: 0,\n+};\n+\n+const handleSocketMessage = (data) => {\n+  try {\n+    const splitData = data.toString('utf-8').split(';;;');\n+    const jsonArray = splitData\n+      .filter((item) => item !== '' && Array.isArray(JSON.parse(item)))",
        "comment_created_at": "2023-03-14T19:16:22+00:00",
        "comment_author": "medikoo",
        "comment_body": "Wouldn't `item.startsWith('[') be good enough here? (relying on `JSON.parse` in filter and then repeating it in map doesn't feel optimal)",
        "pr_file_module": null
      },
      {
        "comment_id": "1136174787",
        "repo_full_name": "serverless/serverless",
        "pr_number": 11782,
        "pr_file": "lib/cli/interactive-setup/console-dev-mode-feed.js",
        "discussion_id": "1136072225",
        "commented_code": "@@ -0,0 +1,187 @@\n+'use strict';\n+\n+const { writeText, style } = require('@serverless/utils/log');\n+const WebSocket = require('ws');\n+const chalk = require('chalk');\n+const { devModeFeed } = require('@serverless/utils/lib/auth/urls');\n+const consoleUi = require('@serverless/utils/console-ui');\n+const streamBuffers = require('stream-buffers');\n+const apiRequest = require('@serverless/utils/api-request');\n+const promptWithHistory = require('@serverless/utils/inquirer/prompt-with-history');\n+\n+const streamBuff = new streamBuffers.ReadableStreamBuffer({\n+  frequency: 500,\n+  chunkSize: 2048 * 1000000,\n+});\n+\n+const consoleMonitoringCounter = {\n+  logBatches: 0,\n+  events: 0,\n+  responses: 0,\n+};\n+\n+const handleSocketMessage = (data) => {\n+  try {\n+    const splitData = data.toString('utf-8').split(';;;');\n+    const jsonArray = splitData\n+      .filter((item) => item !== '' && Array.isArray(JSON.parse(item)))",
        "comment_created_at": "2023-03-14T20:41:52+00:00",
        "comment_author": "Danwakeem",
        "comment_body": "This is a good idea, thanks for the suggestion \ud83d\ude0e ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "553601535",
    "pr_number": 8725,
    "pr_file": "lib/plugins/aws/remove/index.js",
    "created_at": "2021-01-07T21:33:46+00:00",
    "commented_code": "this.provider = this.serverless.getProvider('aws');\n \n     Object.assign(this, validate, emptyS3Bucket, removeStack, monitorStack);\n+    this.removeEcrRepository = removeEcrRepository;\n \n     this.hooks = {\n-      'remove:remove': () =>\n-        BbPromise.bind(this)\n-          .then(this.validate)\n-          .then(this.emptyS3Bucket)\n-          .then(this.removeStack)\n-          .then((cfData) => this.monitorStack('delete', cfData)),\n+      'remove:remove': async () => {\n+        await this.validate();\n+        await this.emptyS3Bucket();\n+        const cfData = await this.removeStack();\n+        await this.monitorStack('delete', cfData);\n+        await this.removeEcrRepository();",
    "repo_full_name": "serverless/serverless",
    "discussion_comments": [
      {
        "comment_id": "553601535",
        "repo_full_name": "serverless/serverless",
        "pr_number": 8725,
        "pr_file": "lib/plugins/aws/remove/index.js",
        "discussion_id": "553601535",
        "commented_code": "@@ -13,14 +13,16 @@ class AwsRemove {\n     this.provider = this.serverless.getProvider('aws');\n \n     Object.assign(this, validate, emptyS3Bucket, removeStack, monitorStack);\n+    this.removeEcrRepository = removeEcrRepository;\n \n     this.hooks = {\n-      'remove:remove': () =>\n-        BbPromise.bind(this)\n-          .then(this.validate)\n-          .then(this.emptyS3Bucket)\n-          .then(this.removeStack)\n-          .then((cfData) => this.monitorStack('delete', cfData)),\n+      'remove:remove': async () => {\n+        await this.validate();\n+        await this.emptyS3Bucket();\n+        const cfData = await this.removeStack();\n+        await this.monitorStack('delete', cfData);\n+        await this.removeEcrRepository();",
        "comment_created_at": "2021-01-07T21:33:46+00:00",
        "comment_author": "medikoo",
        "comment_body": "It would be nice to construct this in a way, that we have two methods. First `checkIfEcRepositoryExist`, which we should issue at beginning but do not `await`,\r\nand `await` after `monitorStack` succeds, and if it returns true proceed with `removeEcrRepository()`\r\n\r\nThis will ensure we do not add time to `remove` command for services where ECR is not involved",
        "pr_file_module": null
      },
      {
        "comment_id": "553828635",
        "repo_full_name": "serverless/serverless",
        "pr_number": 8725,
        "pr_file": "lib/plugins/aws/remove/index.js",
        "discussion_id": "553601535",
        "commented_code": "@@ -13,14 +13,16 @@ class AwsRemove {\n     this.provider = this.serverless.getProvider('aws');\n \n     Object.assign(this, validate, emptyS3Bucket, removeStack, monitorStack);\n+    this.removeEcrRepository = removeEcrRepository;\n \n     this.hooks = {\n-      'remove:remove': () =>\n-        BbPromise.bind(this)\n-          .then(this.validate)\n-          .then(this.emptyS3Bucket)\n-          .then(this.removeStack)\n-          .then((cfData) => this.monitorStack('delete', cfData)),\n+      'remove:remove': async () => {\n+        await this.validate();\n+        await this.emptyS3Bucket();\n+        const cfData = await this.removeStack();\n+        await this.monitorStack('delete', cfData);\n+        await this.removeEcrRepository();",
        "comment_created_at": "2021-01-08T09:18:39+00:00",
        "comment_author": "pgrzesik",
        "comment_body": "That's a clever way to skip adding extra time for people that don't use `docker` at all, I'll update it",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "795924657",
    "pr_number": 10576,
    "pr_file": "lib/utils/download-template-from-repo.js",
    "created_at": "2022-01-31T17:56:02+00:00",
    "commented_code": "});\n }\n \n+async function downloadTemplateFromExamples({ template, name, path: projectPath, isLegacy }) {\n+  const baseUrl = 'https://github.com/serverless/examples/tree/v3';\n+  let templateUrl;\n+  if (isLegacy) {\n+    templateUrl = `${baseUrl}/legacy/${template}`;\n+  } else {\n+    templateUrl = `${baseUrl}/${template}`;\n+  }\n+\n+  const repoInformation = await parseRepoURL(templateUrl);\n+  const { username, password } = repoInformation;\n+  const downloadServicePath = path.join(os.tmpdir(), repoInformation.repo);\n+\n+  const serviceDir = projectPath ? path.resolve(untildify(projectPath)) : process.cwd();\n+  const serviceName = name || resolveServiceName(serviceDir);\n+\n+  // We do not want to run this check if project should be setup in current directory\n+  if (serviceDir !== process.cwd() && (await dirExists(serviceDir))) {\n+    const errorMessage = [\n+      `The directory \"${serviceDir}\" already exists, and serverless will not overwrite it. `,\n+      'Rename or move the directory and try again if you want serverless to create it\"',\n+    ].join('');\n+    throw new ServerlessError(errorMessage, 'TARGET_FOLDER_ALREADY_EXISTS');\n+  }\n+\n+  const downloadOptions = {\n+    timeout: 30000,\n+    extract: true,\n+    strip: 1,\n+    mode: '755',\n+    username,\n+    password,\n+  };\n+\n+  await download(repoInformation.downloadUrl, downloadServicePath, downloadOptions);\n+  // Examples repo has all examples nested in directories\n+  const directory = path.join(downloadServicePath, repoInformation.pathToDirectory);\n+\n+  if (serviceDir === process.cwd()) {\n+    // ensure no template file already exists in current directory that we may overwrite\n+    const templateFullFilePaths = walkDirSync(directory);\n+\n+    await Promise.all(\n+      templateFullFilePaths.map(async (ffp) => {\n+        const filename = path.basename(ffp);\n+        if (await fileExists(path.join(process.cwd(), filename))) {\n+          const errorMessage = [\n+            `The file \"${filename}\" already exists, and serverless will not overwrite it. `,\n+            `Move the file and try again if you want serverless to write a new \"${filename}\"`,\n+          ].join('');\n+\n+          throw new ServerlessError(errorMessage, 'TEMPLATE_FILE_ALREADY_EXISTS');\n+        }\n+      })\n+    );\n+  }\n+  await copyDirContents(directory, serviceDir);",
    "repo_full_name": "serverless/serverless",
    "discussion_comments": [
      {
        "comment_id": "795924657",
        "repo_full_name": "serverless/serverless",
        "pr_number": 10576,
        "pr_file": "lib/utils/download-template-from-repo.js",
        "discussion_id": "795924657",
        "commented_code": "@@ -322,7 +336,73 @@ async function downloadTemplateFromRepo(inputUrl, templateName, downloadPath) {\n   });\n }\n \n+async function downloadTemplateFromExamples({ template, name, path: projectPath, isLegacy }) {\n+  const baseUrl = 'https://github.com/serverless/examples/tree/v3';\n+  let templateUrl;\n+  if (isLegacy) {\n+    templateUrl = `${baseUrl}/legacy/${template}`;\n+  } else {\n+    templateUrl = `${baseUrl}/${template}`;\n+  }\n+\n+  const repoInformation = await parseRepoURL(templateUrl);\n+  const { username, password } = repoInformation;\n+  const downloadServicePath = path.join(os.tmpdir(), repoInformation.repo);\n+\n+  const serviceDir = projectPath ? path.resolve(untildify(projectPath)) : process.cwd();\n+  const serviceName = name || resolveServiceName(serviceDir);\n+\n+  // We do not want to run this check if project should be setup in current directory\n+  if (serviceDir !== process.cwd() && (await dirExists(serviceDir))) {\n+    const errorMessage = [\n+      `The directory \"${serviceDir}\" already exists, and serverless will not overwrite it. `,\n+      'Rename or move the directory and try again if you want serverless to create it\"',\n+    ].join('');\n+    throw new ServerlessError(errorMessage, 'TARGET_FOLDER_ALREADY_EXISTS');\n+  }\n+\n+  const downloadOptions = {\n+    timeout: 30000,\n+    extract: true,\n+    strip: 1,\n+    mode: '755',\n+    username,\n+    password,\n+  };\n+\n+  await download(repoInformation.downloadUrl, downloadServicePath, downloadOptions);\n+  // Examples repo has all examples nested in directories\n+  const directory = path.join(downloadServicePath, repoInformation.pathToDirectory);\n+\n+  if (serviceDir === process.cwd()) {\n+    // ensure no template file already exists in current directory that we may overwrite\n+    const templateFullFilePaths = walkDirSync(directory);\n+\n+    await Promise.all(\n+      templateFullFilePaths.map(async (ffp) => {\n+        const filename = path.basename(ffp);\n+        if (await fileExists(path.join(process.cwd(), filename))) {\n+          const errorMessage = [\n+            `The file \"${filename}\" already exists, and serverless will not overwrite it. `,\n+            `Move the file and try again if you want serverless to write a new \"${filename}\"`,\n+          ].join('');\n+\n+          throw new ServerlessError(errorMessage, 'TEMPLATE_FILE_ALREADY_EXISTS');\n+        }\n+      })\n+    );\n+  }\n+  await copyDirContents(directory, serviceDir);",
        "comment_created_at": "2022-01-31T17:56:02+00:00",
        "comment_author": "medikoo",
        "comment_body": "Why not _move dir contents_ ? We've downloaded to temp dir, right?",
        "pr_file_module": null
      },
      {
        "comment_id": "798445594",
        "repo_full_name": "serverless/serverless",
        "pr_number": 10576,
        "pr_file": "lib/utils/download-template-from-repo.js",
        "discussion_id": "795924657",
        "commented_code": "@@ -322,7 +336,73 @@ async function downloadTemplateFromRepo(inputUrl, templateName, downloadPath) {\n   });\n }\n \n+async function downloadTemplateFromExamples({ template, name, path: projectPath, isLegacy }) {\n+  const baseUrl = 'https://github.com/serverless/examples/tree/v3';\n+  let templateUrl;\n+  if (isLegacy) {\n+    templateUrl = `${baseUrl}/legacy/${template}`;\n+  } else {\n+    templateUrl = `${baseUrl}/${template}`;\n+  }\n+\n+  const repoInformation = await parseRepoURL(templateUrl);\n+  const { username, password } = repoInformation;\n+  const downloadServicePath = path.join(os.tmpdir(), repoInformation.repo);\n+\n+  const serviceDir = projectPath ? path.resolve(untildify(projectPath)) : process.cwd();\n+  const serviceName = name || resolveServiceName(serviceDir);\n+\n+  // We do not want to run this check if project should be setup in current directory\n+  if (serviceDir !== process.cwd() && (await dirExists(serviceDir))) {\n+    const errorMessage = [\n+      `The directory \"${serviceDir}\" already exists, and serverless will not overwrite it. `,\n+      'Rename or move the directory and try again if you want serverless to create it\"',\n+    ].join('');\n+    throw new ServerlessError(errorMessage, 'TARGET_FOLDER_ALREADY_EXISTS');\n+  }\n+\n+  const downloadOptions = {\n+    timeout: 30000,\n+    extract: true,\n+    strip: 1,\n+    mode: '755',\n+    username,\n+    password,\n+  };\n+\n+  await download(repoInformation.downloadUrl, downloadServicePath, downloadOptions);\n+  // Examples repo has all examples nested in directories\n+  const directory = path.join(downloadServicePath, repoInformation.pathToDirectory);\n+\n+  if (serviceDir === process.cwd()) {\n+    // ensure no template file already exists in current directory that we may overwrite\n+    const templateFullFilePaths = walkDirSync(directory);\n+\n+    await Promise.all(\n+      templateFullFilePaths.map(async (ffp) => {\n+        const filename = path.basename(ffp);\n+        if (await fileExists(path.join(process.cwd(), filename))) {\n+          const errorMessage = [\n+            `The file \"${filename}\" already exists, and serverless will not overwrite it. `,\n+            `Move the file and try again if you want serverless to write a new \"${filename}\"`,\n+          ].join('');\n+\n+          throw new ServerlessError(errorMessage, 'TEMPLATE_FILE_ALREADY_EXISTS');\n+        }\n+      })\n+    );\n+  }\n+  await copyDirContents(directory, serviceDir);",
        "comment_created_at": "2022-02-03T10:57:20+00:00",
        "comment_author": "pgrzesik",
        "comment_body": "I've kept the original logic but I think you're right here, I don't see a reason why we couldn't use move instead of copy here",
        "pr_file_module": null
      }
    ]
  }
]