[
  {
    "discussion_id": "2212234160",
    "pr_number": 7721,
    "pr_file": "docker/Dockerfile",
    "created_at": "2025-07-17T04:37:25+00:00",
    "commented_code": "&& tar -xf nvshmem_src_cuda12-all-all-3.3.9.tar.gz && mv nvshmem_src nvshmem \\\n  && cd nvshmem \\\n  && rm -f /sgl-workspace/nvshmem_src_cuda12-all-all-3.3.9.tar.gz \\\n+ && if [ \"$BUILD_TYPE\" = \"blackwell\" ]; then CUDA_ARCH=100; else CUDA_ARCH=90; fi \\",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2212234160",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7721,
        "pr_file": "docker/Dockerfile",
        "discussion_id": "2212234160",
        "commented_code": "@@ -69,6 +70,7 @@ RUN wget https://developer.download.nvidia.com/compute/redist/nvshmem/3.3.9/sour\n  && tar -xf nvshmem_src_cuda12-all-all-3.3.9.tar.gz && mv nvshmem_src nvshmem \\\n  && cd nvshmem \\\n  && rm -f /sgl-workspace/nvshmem_src_cuda12-all-all-3.3.9.tar.gz \\\n+ && if [ \"$BUILD_TYPE\" = \"blackwell\" ]; then CUDA_ARCH=100; else CUDA_ARCH=90; fi \\",
        "comment_created_at": "2025-07-17T04:37:25+00:00",
        "comment_author": "cliffwoolley",
        "comment_body": "\"Blackwell\" encompasses more than sm_100.  (Same comment below for TORCH_CUDA_ARCH_LIST.)  Maybe we should be more expansive in the list here?",
        "pr_file_module": null
      },
      {
        "comment_id": "2216878803",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7721,
        "pr_file": "docker/Dockerfile",
        "discussion_id": "2212234160",
        "commented_code": "@@ -69,6 +70,7 @@ RUN wget https://developer.download.nvidia.com/compute/redist/nvshmem/3.3.9/sour\n  && tar -xf nvshmem_src_cuda12-all-all-3.3.9.tar.gz && mv nvshmem_src nvshmem \\\n  && cd nvshmem \\\n  && rm -f /sgl-workspace/nvshmem_src_cuda12-all-all-3.3.9.tar.gz \\\n+ && if [ \"$BUILD_TYPE\" = \"blackwell\" ]; then CUDA_ARCH=100; else CUDA_ARCH=90; fi \\",
        "comment_created_at": "2025-07-18T20:32:05+00:00",
        "comment_author": "kyleliang-nv",
        "comment_body": "Would CMAKE_CUDA_ARCHITECTURES=100;120 be sufficient?\r\nAnd for TORCH_CUDA_ARCH_LIST, will TORCH_CUDA_ARCH_LIST=\"10.0 12.0\" be sufficient?\r\nSince this is a blackwell build, I'm thinking if I should include sm90.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2216984722",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7721,
        "pr_file": "docker/Dockerfile",
        "discussion_id": "2212234160",
        "commented_code": "@@ -69,6 +70,7 @@ RUN wget https://developer.download.nvidia.com/compute/redist/nvshmem/3.3.9/sour\n  && tar -xf nvshmem_src_cuda12-all-all-3.3.9.tar.gz && mv nvshmem_src nvshmem \\\n  && cd nvshmem \\\n  && rm -f /sgl-workspace/nvshmem_src_cuda12-all-all-3.3.9.tar.gz \\\n+ && if [ \"$BUILD_TYPE\" = \"blackwell\" ]; then CUDA_ARCH=100; else CUDA_ARCH=90; fi \\",
        "comment_created_at": "2025-07-18T22:10:51+00:00",
        "comment_author": "cliffwoolley",
        "comment_body": "FP8 or FP4 kernels add complexity to the explanation because of arch and/or family conditional compilation.\r\n\r\nFor the kernels that aren't fp8 or fp4, 10.0 + 12.0 together covers a lot of ground for the Blackwell families, yes.  Thor is the other one; its number had been 10.1 but it will be renumbered as SM 11.0 in CUDA 13 here shortly.  (We could perhaps skip Thor for the moment but expect to add it later.)\r\n\r\nAs to the \"what about 9.0\", I think my best advice is to have one build that serves all the primary targets, so at least here that's Hopper and Blackwell together.  Instead of having the \"blackwell\" one not be the default.",
        "pr_file_module": null
      },
      {
        "comment_id": "2216990449",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7721,
        "pr_file": "docker/Dockerfile",
        "discussion_id": "2212234160",
        "commented_code": "@@ -69,6 +70,7 @@ RUN wget https://developer.download.nvidia.com/compute/redist/nvshmem/3.3.9/sour\n  && tar -xf nvshmem_src_cuda12-all-all-3.3.9.tar.gz && mv nvshmem_src nvshmem \\\n  && cd nvshmem \\\n  && rm -f /sgl-workspace/nvshmem_src_cuda12-all-all-3.3.9.tar.gz \\\n+ && if [ \"$BUILD_TYPE\" = \"blackwell\" ]; then CUDA_ARCH=100; else CUDA_ARCH=90; fi \\",
        "comment_created_at": "2025-07-18T22:18:52+00:00",
        "comment_author": "kyleliang-nv",
        "comment_body": "Thanks for the explaination Cliff. I'll work on these changes.\r\nClaiming it to be Hopper and Blackwell is a bit challenging right now, due to the DeepEP library where they explictly check that TORCH_CUDA_ARCH_LIST have to be exactly \"9.0\", otherwise will disable some features.\r\nhttps://github.com/deepseek-ai/DeepEP/blob/main/setup.py#L70-L73.\r\nNot sure how much SGLang depends on this specific feature in DeepEP package.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2105744152",
    "pr_number": 5744,
    "pr_file": "docker/Dockerfile",
    "created_at": "2025-05-24T07:15:21+00:00",
    "commented_code": "ARG CUDA_VERSION=12.4.1\n \n-FROM nvcr.io/nvidia/tritonserver:24.04-py3-min\n+FROM nvcr.io/nvidia/tritonserver:25.03-py3-min",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2105744152",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 5744,
        "pr_file": "docker/Dockerfile",
        "discussion_id": "2105744152",
        "commented_code": "@@ -1,6 +1,6 @@\n ARG CUDA_VERSION=12.4.1\n \n-FROM nvcr.io/nvidia/tritonserver:24.04-py3-min\n+FROM nvcr.io/nvidia/tritonserver:25.03-py3-min",
        "comment_created_at": "2025-05-24T07:15:21+00:00",
        "comment_author": "zhyncs",
        "comment_body": "What is the CUDA version in the new image?",
        "pr_file_module": null
      },
      {
        "comment_id": "2105757795",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 5744,
        "pr_file": "docker/Dockerfile",
        "discussion_id": "2105744152",
        "commented_code": "@@ -1,6 +1,6 @@\n ARG CUDA_VERSION=12.4.1\n \n-FROM nvcr.io/nvidia/tritonserver:24.04-py3-min\n+FROM nvcr.io/nvidia/tritonserver:25.03-py3-min",
        "comment_created_at": "2025-05-24T08:26:18+00:00",
        "comment_author": "Swipe4057",
        "comment_body": "According to the documentation https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel-25-04.html#rel-25-04 and my verification \r\n<img width=\"236\" alt=\"image\" src=\"https://github.com/user-attachments/assets/dafa8da1-3852-4f90-a85f-279530740acd\" />\r\n, the image has CUDA 12.8. If this is a problem and a lower CUDA version is needed, for example 12.6, then it can be fixed by using 24.12 as the base instead of 25.03.\r\n\r\nPyTorch 2.7.0 has dropped CUDA 12.4, so the remaining options are 12.6 and 12.8\r\nhttps://pytorch.org/blog/pytorch-2-7/",
        "pr_file_module": null
      },
      {
        "comment_id": "2105871091",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 5744,
        "pr_file": "docker/Dockerfile",
        "discussion_id": "2105744152",
        "commented_code": "@@ -1,6 +1,6 @@\n ARG CUDA_VERSION=12.4.1\n \n-FROM nvcr.io/nvidia/tritonserver:24.04-py3-min\n+FROM nvcr.io/nvidia/tritonserver:25.03-py3-min",
        "comment_created_at": "2025-05-24T16:11:31+00:00",
        "comment_author": "Swipe4057",
        "comment_body": "Support matrix:\r\nhttps://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html",
        "pr_file_module": null
      }
    ]
  }
]