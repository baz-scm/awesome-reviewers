[
  {
    "discussion_id": "2199303894",
    "pr_number": 6226,
    "pr_file": "docs/backend/quantization.md",
    "created_at": "2025-07-11T02:11:09+00:00",
    "commented_code": "# for VLMs\n from auto_round import AutoRoundMLLM\n from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, AutoTokenizer\n-\n model_name = \"Qwen/Qwen2-VL-2B-Instruct\" \n quant_path = \"Qwen2-VL-2B-Instruct-autoround-4bit\"\n model = Qwen2VLForConditionalGeneration.from_pretrained(\n     model_name, trust_remote_code=True, torch_dtype=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(model_name)\n processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n-\n-## quantize the model\n bits, group_size, sym = 4, 128, True\n autoround = AutoRoundMLLM(model, tokenizer, processor,\n                           bits=bits, group_size=group_size, sym=sym)\n-autoround.quantize()\n-# save the quantized model, set format='auto_gptq' or 'auto_awq' to use other formats\n-autoround.save_quantized(quant_path, format='auto_round', inplace=True)\n+format='auto_round'\n+autoround.quantize_and_save(quant_path, format=format) # quantize and save\n \n ```\n \n+- known issues\n+\n+Several limitations currently affect offline quantized model loading in sglang:\n+\n+1. Mixed-bit Quantization Limitations\n+\n+    Mixed-bit quantization is not fully supported. Due to vLLM's layer fusion (e.g., QKV fusion), applying different bit-widths to components within the same fused layer can lead to compatibility issues.\n+\n+\n+2. Limited Support for Quantized MoE Models\n+",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2199303894",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 6226,
        "pr_file": "docs/backend/quantization.md",
        "discussion_id": "2199303894",
        "commented_code": "@@ -74,24 +70,64 @@ autoround.quantize_and_save(quant_path, format=format) # quantize and save\n # for VLMs\n from auto_round import AutoRoundMLLM\n from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, AutoTokenizer\n-\n model_name = \"Qwen/Qwen2-VL-2B-Instruct\" \n quant_path = \"Qwen2-VL-2B-Instruct-autoround-4bit\"\n model = Qwen2VLForConditionalGeneration.from_pretrained(\n     model_name, trust_remote_code=True, torch_dtype=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(model_name)\n processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n-\n-## quantize the model\n bits, group_size, sym = 4, 128, True\n autoround = AutoRoundMLLM(model, tokenizer, processor,\n                           bits=bits, group_size=group_size, sym=sym)\n-autoround.quantize()\n-# save the quantized model, set format='auto_gptq' or 'auto_awq' to use other formats\n-autoround.save_quantized(quant_path, format='auto_round', inplace=True)\n+format='auto_round'\n+autoround.quantize_and_save(quant_path, format=format) # quantize and save\n \n ```\n \n+- known issues\n+\n+Several limitations currently affect offline quantized model loading in sglang:\n+\n+1. Mixed-bit Quantization Limitations\n+\n+    Mixed-bit quantization is not fully supported. Due to vLLM's layer fusion (e.g., QKV fusion), applying different bit-widths to components within the same fused layer can lead to compatibility issues.\n+\n+\n+2. Limited Support for Quantized MoE Models\n+    ",
        "comment_created_at": "2025-07-11T02:11:09+00:00",
        "comment_author": "wenhuach21",
        "comment_body": "Most quantized MoE models may encounter inference issues due to kernel-related limitation. These issues might be resolved in future updates of sglang. If you experience any problems, consider using Hugging Face Transformers as an alternative. Detailed failure cases are listed below.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2247169249",
    "pr_number": 8660,
    "pr_file": "docs/backend/attention_backend.md",
    "created_at": "2025-08-01T07:33:07+00:00",
    "commented_code": "| **FlashMLA**             | \u2705                | \u2705                 | \u2705      | \u274c                 | \u274c              |\n | **TRTLLM MLA**           | \u2705                | \u274c                 | \u2705      | \u2705                 | \u274c              |\n | **Ascend**               | \u2705                | \u274c                 | \u274c      | \u274c                 | \u274c              |\n+| **Wave**                 | \u2705                | \u274c                 | \u274c      | \u274c                 | \u274c              |",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2247209521",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8660,
        "pr_file": "docs/backend/attention_backend.md",
        "discussion_id": "2247169249",
        "commented_code": "@@ -11,6 +11,7 @@\n | **FlashMLA**             | \u2705                | \u2705                 | \u2705      | \u274c                 | \u274c              |\n | **TRTLLM MLA**           | \u2705                | \u274c                 | \u2705      | \u2705                 | \u274c              |\n | **Ascend**               | \u2705                | \u274c                 | \u274c      | \u274c                 | \u274c              |\n+| **Wave**                 | \u2705                | \u274c                 | \u274c      | \u274c                 | \u274c              |",
        "comment_created_at": "2025-08-01T07:33:07+00:00",
        "comment_author": "yichiche",
        "comment_body": "Page Size > 1: As currently implemented, SGLang\u2019s default page size is 1 and the Wave path only works with that, which is why the comment exists. I\u2019m happy to adjust whichever way the reviewers prefer.\r\n\r\nSpeculative Decoding: The existing logic in wave_backend.py exposes flags like is_target_verify and is_draft_extend for compatibility, but under the hood speculative decoding is still using the Triton implementation today. A follow-up PR will migrate this to a native Wave implementation.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2265194699",
    "pr_number": 8700,
    "pr_file": "docs/references/ascend_npu.md",
    "created_at": "2025-08-10T09:15:28+00:00",
    "commented_code": "+# SGLang on Ascend NPU\r\n+\r\n+You can install SGLang using any of the methods below. Please go through `System Settings` section to ensure the clusters are roaring at max performance. Feel free to leave an issue [here at sglang](https://github.com/sgl-project/sglang/issues) if you encouter any issues or have any problems.\r\n+\r\n+## System Settings\r\n+\r\n+### CPU performance power scheme\r\n+\r\n+The default power scheme on Ascend hardware is `ondemand` which could affect performance, changing it to `performance` is recommended.\r\n+\r\n+```shell\r\n+echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\r\n+\r\n+# Make sure changes are applied successfully\r\n+cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor # shows performance\r\n+```\r\n+\r\n+### Disable NUMA balancing\r\n+\r\n+```shell\r\n+sudo sysctl -w kernel.numa_balancing=0\r\n+\r\n+# Check\r\n+cat /proc/sys/kernel/numa_balancing # shows 0\r\n+```\r\n+\r\n+### Prevent swapping out system memory\r\n+\r\n+```shell\r\n+sudo sysctl -w vm.swappiness=10\r\n+\r\n+# Check\r\n+cat /proc/sys/vm/swappiness # shows 10\r\n+```\r\n+\r\n+## Installing SGLang\r\n+\r\n+### Method 1: Installing from source with prerequisites\r\n+\r\n+#### Python Version\r\n+\r\n+Only `python==3.11` is supported currently. If you don't want to break system pre-installed python, try installing with [conda](https://github.com/conda/conda).\r\n+\r\n+```shell\r\n+conda create --name sglang_npu python=3.11\r\n+conda activate sglang_npu\r\n+```\r\n+\r\n+#### MemFabric Adaptor\r\n+\r\n+_TODO: MemFabric is still a working project yet open sourced til August/September, 2025. We will release it as prebuilt wheel packge for now._\r\n+\r\n+_Notice: Prebuilt wheel packge is based on `aarch64`, please leave an issue [here at sglang](https://github.com/sgl-project/sglang/issues) to let us know the requests for `amd64` build._\r\n+\r\n+MemFabric Adaptor is a drop-in replacement of Mooncake Transfer Engine that enables KV cache transfer on Ascend NPU clusters.\r\n+\r\n+```shell\r\n+MF_WHL_NAME=\"mf_adapter-1.0.0-cp311-cp311-linux_aarch64.whl\"\r\n+MEMFABRIC_URL=\"https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/${MF_WHL_NAME}\"\r\n+wget -O \"${MF_WHL_NAME}\" \"${MEMFABRIC_URL}\" && pip install \"./${MF_WHL_NAME}\"\r\n+```\r\n+\r\n+#### Pytorch and Pytorch Framework Adaptor on Ascend\r\n+\r\n+Only `torch==2.6.0` is supported currently due to NPUgraph and Triton-on-Ascend's limitation, however a more generalized version will be release by the end of September, 2025.\r\n+\r\n+```shell\r\n+PYTORCH_VERSION=2.6.0\r\n+TORCHVISION_VERSION=0.21.0\r\n+pip install torch==$PYTORCH_VERSION torchvision==$TORCHVISION_VERSION --index-url https://download.pytorch.org/whl/cpu\r\n+\r\n+PTA_VERSION=\"v7.1.0.1-pytorch2.6.0\"\r\n+PTA_NAME=\"torch_npu-2.6.0.post1-cp311-cp311-manylinux_2_28_aarch64.whl\"\r\n+PTA_URL=\"https://gitee.com/ascend/pytorch/releases/download/${PTA_VERSION}/${PTA_WHL_NAME}\"\r\n+wget -O \"${PTA_NAME}\" \"${PTA_URL}\" && pip install \"./${PTA_NAME}\"\r\n+```",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2265194699",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8700,
        "pr_file": "docs/references/ascend_npu.md",
        "discussion_id": "2265194699",
        "commented_code": "@@ -0,0 +1,204 @@\n+# SGLang on Ascend NPU\r\n+\r\n+You can install SGLang using any of the methods below. Please go through `System Settings` section to ensure the clusters are roaring at max performance. Feel free to leave an issue [here at sglang](https://github.com/sgl-project/sglang/issues) if you encouter any issues or have any problems.\r\n+\r\n+## System Settings\r\n+\r\n+### CPU performance power scheme\r\n+\r\n+The default power scheme on Ascend hardware is `ondemand` which could affect performance, changing it to `performance` is recommended.\r\n+\r\n+```shell\r\n+echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\r\n+\r\n+# Make sure changes are applied successfully\r\n+cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor # shows performance\r\n+```\r\n+\r\n+### Disable NUMA balancing\r\n+\r\n+```shell\r\n+sudo sysctl -w kernel.numa_balancing=0\r\n+\r\n+# Check\r\n+cat /proc/sys/kernel/numa_balancing # shows 0\r\n+```\r\n+\r\n+### Prevent swapping out system memory\r\n+\r\n+```shell\r\n+sudo sysctl -w vm.swappiness=10\r\n+\r\n+# Check\r\n+cat /proc/sys/vm/swappiness # shows 10\r\n+```\r\n+\r\n+## Installing SGLang\r\n+\r\n+### Method 1: Installing from source with prerequisites\r\n+\r\n+#### Python Version\r\n+\r\n+Only `python==3.11` is supported currently. If you don't want to break system pre-installed python, try installing with [conda](https://github.com/conda/conda).\r\n+\r\n+```shell\r\n+conda create --name sglang_npu python=3.11\r\n+conda activate sglang_npu\r\n+```\r\n+\r\n+#### MemFabric Adaptor\r\n+\r\n+_TODO: MemFabric is still a working project yet open sourced til August/September, 2025. We will release it as prebuilt wheel packge for now._\r\n+\r\n+_Notice: Prebuilt wheel packge is based on `aarch64`, please leave an issue [here at sglang](https://github.com/sgl-project/sglang/issues) to let us know the requests for `amd64` build._\r\n+\r\n+MemFabric Adaptor is a drop-in replacement of Mooncake Transfer Engine that enables KV cache transfer on Ascend NPU clusters.\r\n+\r\n+```shell\r\n+MF_WHL_NAME=\"mf_adapter-1.0.0-cp311-cp311-linux_aarch64.whl\"\r\n+MEMFABRIC_URL=\"https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/${MF_WHL_NAME}\"\r\n+wget -O \"${MF_WHL_NAME}\" \"${MEMFABRIC_URL}\" && pip install \"./${MF_WHL_NAME}\"\r\n+```\r\n+\r\n+#### Pytorch and Pytorch Framework Adaptor on Ascend\r\n+\r\n+Only `torch==2.6.0` is supported currently due to NPUgraph and Triton-on-Ascend's limitation, however a more generalized version will be release by the end of September, 2025.\r\n+\r\n+```shell\r\n+PYTORCH_VERSION=2.6.0\r\n+TORCHVISION_VERSION=0.21.0\r\n+pip install torch==$PYTORCH_VERSION torchvision==$TORCHVISION_VERSION --index-url https://download.pytorch.org/whl/cpu\r\n+\r\n+PTA_VERSION=\"v7.1.0.1-pytorch2.6.0\"\r\n+PTA_NAME=\"torch_npu-2.6.0.post1-cp311-cp311-manylinux_2_28_aarch64.whl\"\r\n+PTA_URL=\"https://gitee.com/ascend/pytorch/releases/download/${PTA_VERSION}/${PTA_WHL_NAME}\"\r\n+wget -O \"${PTA_NAME}\" \"${PTA_URL}\" && pip install \"./${PTA_NAME}\"\r\n+```\r",
        "comment_created_at": "2025-08-10T09:15:28+00:00",
        "comment_author": "ping1jing2",
        "comment_body": "maybe we can use `pip install torch_npu==2.6.0` directly from [pypi]( https://pypi.org/project/torch-npu/)?",
        "pr_file_module": null
      }
    ]
  }
]