[
  {
    "discussion_id": "2280904692",
    "pr_number": 9186,
    "pr_file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "created_at": "2025-08-17T14:34:27+00:00",
    "commented_code": "encoder_lens=encoder_lens,\n             return_logprob=False,\n             positions=positions,\n-            global_num_tokens_gpu=self.global_num_tokens_gpu,\n-            global_num_tokens_for_logprob_gpu=self.global_num_tokens_for_logprob_gpu,\n-            dp_padding_mode=DpPaddingMode.get_default_mode_in_cuda_graph(),\n             global_dp_buffer_len=global_dp_buffer_len,\n+            global_num_tokens_gpu=(\n+                self.prefill_global_num_tokens_gpu\n+                if hasattr(self, \"capture_forward_mode\")\n+                and self.capture_forward_mode == ForwardMode.EXTEND\n+                else self.global_num_tokens_gpu\n+            ),\n+            global_num_tokens_for_logprob_gpu=(\n+                self.prefill_global_num_tokens_for_logprob_gpu\n+                if hasattr(self, \"capture_forward_mode\")\n+                and self.capture_forward_mode == ForwardMode.EXTEND\n+                else self.global_num_tokens_for_logprob_gpu",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2280904692",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9186,
        "pr_file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
        "discussion_id": "2280904692",
        "commented_code": "@@ -591,33 +895,82 @@ def capture_one_batch_size(self, bs: int, forward: Callable):\n             encoder_lens=encoder_lens,\n             return_logprob=False,\n             positions=positions,\n-            global_num_tokens_gpu=self.global_num_tokens_gpu,\n-            global_num_tokens_for_logprob_gpu=self.global_num_tokens_for_logprob_gpu,\n-            dp_padding_mode=DpPaddingMode.get_default_mode_in_cuda_graph(),\n             global_dp_buffer_len=global_dp_buffer_len,\n+            global_num_tokens_gpu=(\n+                self.prefill_global_num_tokens_gpu\n+                if hasattr(self, \"capture_forward_mode\")\n+                and self.capture_forward_mode == ForwardMode.EXTEND\n+                else self.global_num_tokens_gpu\n+            ),\n+            global_num_tokens_for_logprob_gpu=(\n+                self.prefill_global_num_tokens_for_logprob_gpu\n+                if hasattr(self, \"capture_forward_mode\")\n+                and self.capture_forward_mode == ForwardMode.EXTEND\n+                else self.global_num_tokens_for_logprob_gpu",
        "comment_created_at": "2025-08-17T14:34:27+00:00",
        "comment_author": "fzyzcjy",
        "comment_body": "question: I see a pattern - we have some symmetric \"blahblah\" and \"prefill_blahblah\", then is it possible we extract it.\r\n\r\napproach 1:\r\n\r\n```\r\n@dataclass\r\nclass Pack: # bad name, change it\r\n  blahblah: Tensor\r\n  another_thing: Tensor\r\n\r\npack = Pack(...)\r\nprefill_pack = Pack(...)\r\n```\r\n\r\nthen here we only need \r\n\r\n```\r\nchosen_pack = prefill_pack if mode == EXTEND else pack\r\n\r\n# several fields...\r\nglobal_num_tokens_gpu=chosen_pack.global_num_tokens_gpu,\r\nglobal_num_tokens_for_logprob_gpu=chosen_pack.global_num_tokens_for_logprob_gpu,\r\n...\r\n```\r\n\r\nNOTE (also appliable to other comments): we should not change the current code too much to avoid downstream forks be too hard to merge; but we also need to make necessary refactors to make code clean. That is a contradiction :( thus maybe we can firstly think about the ways before implementing one.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2290171269",
    "pr_number": 9429,
    "pr_file": "python/sglang/srt/lora/lora_manager.py",
    "created_at": "2025-08-21T07:41:48+00:00",
    "commented_code": "if target_modules is not None:\n             self.target_modules = set(target_modules)\n+            user_normalized_modules = get_normalized_target_modules(self.target_modules)\n+            for lora_id, config in self.configs.items():\n+                if not isinstance(config.target_modules, list):\n+                    raise ValueError(\n+                        f\"SGLang currently only supports inferring LoRA target modules when a list of \"\n+                        \"suffixes is provided in `target_modules` field of PEFT config. Please explicitly \"\n+                        \"specify `--lora-target-modules` during server startup. You can specify `all` to \"",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2290171269",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9429,
        "pr_file": "python/sglang/srt/lora/lora_manager.py",
        "discussion_id": "2290171269",
        "commented_code": "@@ -422,6 +422,28 @@ def init_lora_shapes(\n \n         if target_modules is not None:\n             self.target_modules = set(target_modules)\n+            user_normalized_modules = get_normalized_target_modules(self.target_modules)\n+            for lora_id, config in self.configs.items():\n+                if not isinstance(config.target_modules, list):\n+                    raise ValueError(\n+                        f\"SGLang currently only supports inferring LoRA target modules when a list of \"\n+                        \"suffixes is provided in `target_modules` field of PEFT config. Please explicitly \"\n+                        \"specify `--lora-target-modules` during server startup. You can specify `all` to \"",
        "comment_created_at": "2025-08-21T07:41:48+00:00",
        "comment_author": "lifuhuang",
        "comment_body": "nit: Is there any chance we can re-organize the code a bit to reduce repetition?\r\n\r\ne.g., would it be more succinct if we reuse the same loop for both cases and check subset only if target_modules is not None? That way you don't need to copy paste the error message twice.",
        "pr_file_module": null
      },
      {
        "comment_id": "2290265212",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9429,
        "pr_file": "python/sglang/srt/lora/lora_manager.py",
        "discussion_id": "2290171269",
        "commented_code": "@@ -422,6 +422,28 @@ def init_lora_shapes(\n \n         if target_modules is not None:\n             self.target_modules = set(target_modules)\n+            user_normalized_modules = get_normalized_target_modules(self.target_modules)\n+            for lora_id, config in self.configs.items():\n+                if not isinstance(config.target_modules, list):\n+                    raise ValueError(\n+                        f\"SGLang currently only supports inferring LoRA target modules when a list of \"\n+                        \"suffixes is provided in `target_modules` field of PEFT config. Please explicitly \"\n+                        \"specify `--lora-target-modules` during server startup. You can specify `all` to \"",
        "comment_created_at": "2025-08-21T08:12:23+00:00",
        "comment_author": "Beichen-Ma",
        "comment_body": "Sure, I already fixed it to reduce the redundant loop.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2264728162",
    "pr_number": 8616,
    "pr_file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "created_at": "2025-08-09T13:56:08+00:00",
    "commented_code": "global_workspace_buffer = None\n \n \n+class FlashInferMhaChunkKVRunner:",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2264728162",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8616,
        "pr_file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
        "discussion_id": "2264728162",
        "commented_code": "@@ -61,6 +61,129 @@ class PrefillMetadata:\n global_workspace_buffer = None\n \n \n+class FlashInferMhaChunkKVRunner:",
        "comment_created_at": "2025-08-09T13:56:08+00:00",
        "comment_author": "yuan-luo",
        "comment_body": "Here this class __init__ function has some duplicated code with class FlashInferMLAAttnBackend, can they be consolidated? \r\nFor example, the whole function be done in a branch in FlashInferMLAAttnBackend#forward_extend .",
        "pr_file_module": null
      },
      {
        "comment_id": "2265101170",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8616,
        "pr_file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
        "discussion_id": "2264728162",
        "commented_code": "@@ -61,6 +61,129 @@ class PrefillMetadata:\n global_workspace_buffer = None\n \n \n+class FlashInferMhaChunkKVRunner:",
        "comment_created_at": "2025-08-10T03:49:26+00:00",
        "comment_author": "xu-yfei",
        "comment_body": "@yuan-luo As described above, independent classes have less impact and interference on original functions, and the chunk kv function is relatively independent.",
        "pr_file_module": null
      },
      {
        "comment_id": "2277392737",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8616,
        "pr_file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
        "discussion_id": "2264728162",
        "commented_code": "@@ -61,6 +61,129 @@ class PrefillMetadata:\n global_workspace_buffer = None\n \n \n+class FlashInferMhaChunkKVRunner:",
        "comment_created_at": "2025-08-14T18:10:12+00:00",
        "comment_author": "elfiegg",
        "comment_body": "+1 to this actually. I don't see the need for duplicating a whole class. What you really need is just a update_chunked_kv function for chunk wrapper inside of `FlashInferMLAIndicesUpdaterPrefill`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2283814820",
    "pr_number": 9261,
    "pr_file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "created_at": "2025-08-19T01:24:32+00:00",
    "commented_code": "def decode_thread():\n             while True:\n-                (bootstrap_room, status, prefill_rank) = (\n-                    self.server_socket.recv_multipart()\n-                )\n+                msg = self.server_socket.recv_multipart()\n+                if msg[0] == b\"AUX_DATA\":\n+                    room = int(msg[1].decode(\"ascii\"))\n+                    buffer_index = int(msg[2].decode(\"ascii\"))\n+                    aux_index = int(msg[3].decode(\"ascii\"))\n+                    data_length = struct.unpack(\">I\", msg[4])[0]\n+                    data = msg[5]",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2283814820",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9261,
        "pr_file": "python/sglang/srt/disaggregation/mooncake/conn.py",
        "discussion_id": "2283814820",
        "commented_code": "@@ -784,9 +826,31 @@ def start_decode_thread(self):\n \n         def decode_thread():\n             while True:\n-                (bootstrap_room, status, prefill_rank) = (\n-                    self.server_socket.recv_multipart()\n-                )\n+                msg = self.server_socket.recv_multipart()\n+                if msg[0] == b\"AUX_DATA\":\n+                    room = int(msg[1].decode(\"ascii\"))\n+                    buffer_index = int(msg[2].decode(\"ascii\"))\n+                    aux_index = int(msg[3].decode(\"ascii\"))\n+                    data_length = struct.unpack(\">I\", msg[4])[0]\n+                    data = msg[5]",
        "comment_created_at": "2025-08-19T01:24:32+00:00",
        "comment_author": "fzyzcjy",
        "comment_body": "nit: what about extracting serialize and deserialize logic , e.g. I may do\n\n```\nclass AuxDataCodec:\n  @staticmethod def serialize(): ...\n  @staticmethod def deserialize(): ...\n```\n\nI may also extract other things but anyway this code may be temporary so it may be ok",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2284156369",
    "pr_number": 9261,
    "pr_file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "created_at": "2025-08-19T06:01:34+00:00",
    "commented_code": "def decode_thread():\n             while True:\n-                (bootstrap_room, status, prefill_rank) = (\n-                    self.server_socket.recv_multipart()\n-                )\n+                msg = self.server_socket.recv_multipart()\n+                if msg[0] == b\"AUX_DATA\":",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2284156369",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9261,
        "pr_file": "python/sglang/srt/disaggregation/mooncake/conn.py",
        "discussion_id": "2284156369",
        "commented_code": "@@ -784,9 +845,30 @@ def start_decode_thread(self):\n \n         def decode_thread():\n             while True:\n-                (bootstrap_room, status, prefill_rank) = (\n-                    self.server_socket.recv_multipart()\n-                )\n+                msg = self.server_socket.recv_multipart()\n+                if msg[0] == b\"AUX_DATA\":",
        "comment_created_at": "2025-08-19T06:01:34+00:00",
        "comment_author": "fzyzcjy",
        "comment_body": "I personally think we may make the code a bit clearer, e.g. extract such logics to func to avoid a single long func that contains many unrelated logic, but since this is temporary solution it may not be big issues",
        "pr_file_module": null
      },
      {
        "comment_id": "2284505971",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9261,
        "pr_file": "python/sglang/srt/disaggregation/mooncake/conn.py",
        "discussion_id": "2284156369",
        "commented_code": "@@ -784,9 +845,30 @@ def start_decode_thread(self):\n \n         def decode_thread():\n             while True:\n-                (bootstrap_room, status, prefill_rank) = (\n-                    self.server_socket.recv_multipart()\n-                )\n+                msg = self.server_socket.recv_multipart()\n+                if msg[0] == b\"AUX_DATA\":",
        "comment_created_at": "2025-08-19T08:21:02+00:00",
        "comment_author": "ShangmingCai",
        "comment_body": "Nice suggestion. Fixed.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2288430171",
    "pr_number": 9076,
    "pr_file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "created_at": "2025-08-20T14:47:43+00:00",
    "commented_code": "# Speculative_inference\n             if model_runner.spec_algorithm.is_eagle3():\n-                self.model_runner.model.set_eagle3_layers_to_capture()\n+                # load draft config\n+                draft_model_config = ModelConfig.from_server_args(\n+                    self.model_runner.server_args,\n+                    model_path=(\n+                        self.model_runner.server_args.speculative_draft_model_path\n+                    ),\n+                    is_draft_model=True,\n+                )\n+\n+                # get the aux layer from draft model config\n+                eagle_config = getattr(\n+                    draft_model_config.hf_config, \"eagle_config\", None\n+                )\n+                if eagle_config is None:\n+                    eagle_aux_hidden_state_layer_ids = None\n+                else:\n+                    eagle_aux_hidden_state_layer_ids = eagle_config.get(\n+                        \"eagle_aux_hidden_state_layer_ids\", None\n+                    )\n+                print(eagle_aux_hidden_state_layer_ids)",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2288430171",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9076,
        "pr_file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
        "discussion_id": "2288430171",
        "commented_code": "@@ -331,7 +332,29 @@ def __init__(self, model_runner: ModelRunner):\n \n             # Speculative_inference\n             if model_runner.spec_algorithm.is_eagle3():\n-                self.model_runner.model.set_eagle3_layers_to_capture()\n+                # load draft config\n+                draft_model_config = ModelConfig.from_server_args(\n+                    self.model_runner.server_args,\n+                    model_path=(\n+                        self.model_runner.server_args.speculative_draft_model_path\n+                    ),\n+                    is_draft_model=True,\n+                )\n+\n+                # get the aux layer from draft model config\n+                eagle_config = getattr(\n+                    draft_model_config.hf_config, \"eagle_config\", None\n+                )\n+                if eagle_config is None:\n+                    eagle_aux_hidden_state_layer_ids = None\n+                else:\n+                    eagle_aux_hidden_state_layer_ids = eagle_config.get(\n+                        \"eagle_aux_hidden_state_layer_ids\", None\n+                    )\n+                print(eagle_aux_hidden_state_layer_ids)",
        "comment_created_at": "2025-08-20T14:47:43+00:00",
        "comment_author": "fzyzcjy",
        "comment_body": "nit:\n\n* maybe we can extract it to a function\n* maybe we can simplify the code a bit, e.g.\n\n```\neagle_config = getattr(draft_model_config.hf_config, \"eagle_config\", {})\neagle_aux_hidden_state_layer_ids = eagle_config.get(\"eagle_aux_hidden_state_layer_ids\", None)\n```\n\n* btw the print may be removed or changed to a logger.info w/ more text",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2280734811",
    "pr_number": 9059,
    "pr_file": "python/sglang/srt/models/glm4v.py",
    "created_at": "2025-08-17T05:56:33+00:00",
    "commented_code": "video_embeds = torch.split(video_embeds, split_sizes)\n         return torch.cat(video_embeds)\n \n+    def _update_hf_config(self):\n+        \"\"\"update hf config to ensure vision attention num_attention_heads is divisible by tp_size\"\"\"\n+        tp_size = get_attention_tp_size()\n+        num_heads = self.config.vision_config.num_heads\n+        head_dim = self.config.vision_config.hidden_size // num_heads\n+        num_dummy_heads = 0\n+\n+        if num_heads % tp_size != 0:\n+            num_dummy_heads = (\n+                (num_heads + tp_size - 1) // tp_size\n+            ) * tp_size - num_heads\n+\n+        setattr(self.config.vision_config, \"head_dim\", head_dim)\n+        setattr(self.config.vision_config, \"num_dummy_heads\", num_dummy_heads)\n+\n+    def _pad_vit_attn_dummy_heads(self, name: str, loaded_weight: torch.Tensor):\n+        \"\"\"pad attn qkv weights for dummy heads\"\"\"\n+        num_dummy_heads = self.config.vision_config.num_dummy_heads\n+        if num_dummy_heads == 0:\n+            return loaded_weight\n+        head_dim = self.config.vision_config.head_dim\n+\n+        if \"attn.qkv_proj\" in name:\n+            wq, wk, wv = loaded_weight.chunk(3, dim=0)\n+            if name.endswith(\".weight\"):\n+                dummy_shape = [num_dummy_heads, head_dim, wq.shape[-1]]\n+            elif name.endswith(\".bias\"):\n+                dummy_shape = [num_dummy_heads, head_dim]\n+            else:\n+                raise RuntimeError(f\"Unsupported weight with name={name}\")\n+            pad_func = lambda x: torch.cat(\n+                [x.unflatten(0, (-1, head_dim)), x.new_zeros(dummy_shape)], dim=0\n+            ).flatten(0, 1)\n+            wq, wk, wv = pad_func(wq), pad_func(wk), pad_func(wv)\n+            loaded_weight = torch.cat([wq, wk, wv], dim=0)\n+        elif \"attn.proj.weight\" in name:\n+            padded_weight = loaded_weight.new_zeros(\n+                loaded_weight.shape[0], head_dim * num_dummy_heads\n+            )\n+            loaded_weight = torch.cat([loaded_weight, padded_weight], dim=-1)\n+        elif \"attn.q_norm.weight\" in name or \"attn.k_norm.weight\" in name:\n+            padded_weight = loaded_weight.new_zeros(head_dim * num_dummy_heads)\n+            loaded_weight = torch.cat([loaded_weight, padded_weight], dim=0)\n+        return loaded_weight\n+",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2280734811",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9059,
        "pr_file": "python/sglang/srt/models/glm4v.py",
        "discussion_id": "2280734811",
        "commented_code": "@@ -537,6 +539,51 @@ def get_video_feature(self, items: List[MultimodalDataItem]) -> torch.Tensor:\n         video_embeds = torch.split(video_embeds, split_sizes)\n         return torch.cat(video_embeds)\n \n+    def _update_hf_config(self):\n+        \"\"\"update hf config to ensure vision attention num_attention_heads is divisible by tp_size\"\"\"\n+        tp_size = get_attention_tp_size()\n+        num_heads = self.config.vision_config.num_heads\n+        head_dim = self.config.vision_config.hidden_size // num_heads\n+        num_dummy_heads = 0\n+\n+        if num_heads % tp_size != 0:\n+            num_dummy_heads = (\n+                (num_heads + tp_size - 1) // tp_size\n+            ) * tp_size - num_heads\n+\n+        setattr(self.config.vision_config, \"head_dim\", head_dim)\n+        setattr(self.config.vision_config, \"num_dummy_heads\", num_dummy_heads)\n+\n+    def _pad_vit_attn_dummy_heads(self, name: str, loaded_weight: torch.Tensor):\n+        \"\"\"pad attn qkv weights for dummy heads\"\"\"\n+        num_dummy_heads = self.config.vision_config.num_dummy_heads\n+        if num_dummy_heads == 0:\n+            return loaded_weight\n+        head_dim = self.config.vision_config.head_dim\n+\n+        if \"attn.qkv_proj\" in name:\n+            wq, wk, wv = loaded_weight.chunk(3, dim=0)\n+            if name.endswith(\".weight\"):\n+                dummy_shape = [num_dummy_heads, head_dim, wq.shape[-1]]\n+            elif name.endswith(\".bias\"):\n+                dummy_shape = [num_dummy_heads, head_dim]\n+            else:\n+                raise RuntimeError(f\"Unsupported weight with name={name}\")\n+            pad_func = lambda x: torch.cat(\n+                [x.unflatten(0, (-1, head_dim)), x.new_zeros(dummy_shape)], dim=0\n+            ).flatten(0, 1)\n+            wq, wk, wv = pad_func(wq), pad_func(wk), pad_func(wv)\n+            loaded_weight = torch.cat([wq, wk, wv], dim=0)\n+        elif \"attn.proj.weight\" in name:\n+            padded_weight = loaded_weight.new_zeros(\n+                loaded_weight.shape[0], head_dim * num_dummy_heads\n+            )\n+            loaded_weight = torch.cat([loaded_weight, padded_weight], dim=-1)\n+        elif \"attn.q_norm.weight\" in name or \"attn.k_norm.weight\" in name:\n+            padded_weight = loaded_weight.new_zeros(head_dim * num_dummy_heads)\n+            loaded_weight = torch.cat([loaded_weight, padded_weight], dim=0)\n+        return loaded_weight\n+",
        "comment_created_at": "2025-08-17T05:56:33+00:00",
        "comment_author": "JustinTong0323",
        "comment_body": "These utility functions seem universal and could also be applied to other ViTs, could you move them to a appropriate  utils.py file?",
        "pr_file_module": null
      },
      {
        "comment_id": "2280988482",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9059,
        "pr_file": "python/sglang/srt/models/glm4v.py",
        "discussion_id": "2280734811",
        "commented_code": "@@ -537,6 +539,51 @@ def get_video_feature(self, items: List[MultimodalDataItem]) -> torch.Tensor:\n         video_embeds = torch.split(video_embeds, split_sizes)\n         return torch.cat(video_embeds)\n \n+    def _update_hf_config(self):\n+        \"\"\"update hf config to ensure vision attention num_attention_heads is divisible by tp_size\"\"\"\n+        tp_size = get_attention_tp_size()\n+        num_heads = self.config.vision_config.num_heads\n+        head_dim = self.config.vision_config.hidden_size // num_heads\n+        num_dummy_heads = 0\n+\n+        if num_heads % tp_size != 0:\n+            num_dummy_heads = (\n+                (num_heads + tp_size - 1) // tp_size\n+            ) * tp_size - num_heads\n+\n+        setattr(self.config.vision_config, \"head_dim\", head_dim)\n+        setattr(self.config.vision_config, \"num_dummy_heads\", num_dummy_heads)\n+\n+    def _pad_vit_attn_dummy_heads(self, name: str, loaded_weight: torch.Tensor):\n+        \"\"\"pad attn qkv weights for dummy heads\"\"\"\n+        num_dummy_heads = self.config.vision_config.num_dummy_heads\n+        if num_dummy_heads == 0:\n+            return loaded_weight\n+        head_dim = self.config.vision_config.head_dim\n+\n+        if \"attn.qkv_proj\" in name:\n+            wq, wk, wv = loaded_weight.chunk(3, dim=0)\n+            if name.endswith(\".weight\"):\n+                dummy_shape = [num_dummy_heads, head_dim, wq.shape[-1]]\n+            elif name.endswith(\".bias\"):\n+                dummy_shape = [num_dummy_heads, head_dim]\n+            else:\n+                raise RuntimeError(f\"Unsupported weight with name={name}\")\n+            pad_func = lambda x: torch.cat(\n+                [x.unflatten(0, (-1, head_dim)), x.new_zeros(dummy_shape)], dim=0\n+            ).flatten(0, 1)\n+            wq, wk, wv = pad_func(wq), pad_func(wk), pad_func(wv)\n+            loaded_weight = torch.cat([wq, wk, wv], dim=0)\n+        elif \"attn.proj.weight\" in name:\n+            padded_weight = loaded_weight.new_zeros(\n+                loaded_weight.shape[0], head_dim * num_dummy_heads\n+            )\n+            loaded_weight = torch.cat([loaded_weight, padded_weight], dim=-1)\n+        elif \"attn.q_norm.weight\" in name or \"attn.k_norm.weight\" in name:\n+            padded_weight = loaded_weight.new_zeros(head_dim * num_dummy_heads)\n+            loaded_weight = torch.cat([loaded_weight, padded_weight], dim=0)\n+        return loaded_weight\n+",
        "comment_created_at": "2025-08-17T19:10:56+00:00",
        "comment_author": "byjiang1996",
        "comment_body": "Good point. Moved to vision_utils.py",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2233748615",
    "pr_number": 7211,
    "pr_file": "python/sglang/srt/managers/cache_controller.py",
    "created_at": "2025-07-27T06:12:49+00:00",
    "commented_code": "while not self.stop_event.is_set():\n             try:\n                 operation = self.prefetch_buffer.get(block=True, timeout=1)\n-                for h in operation.hash_value:\n-                    page_data = self.storage_backend.get(h)\n-                    if page_data is None:\n-                        logger.warning(\n-                            f\"Prefetch operation {operation.request_id} failed to retrieve page {h}.\"\n-                        )\n-                        break\n-                    self.mem_pool_host.set_from_flat_data_page(\n-                        operation.host_indices[operation.completed_tokens],\n-                        page_data,\n-                    )\n-                    operation.increment(self.page_size)\n-                    if operation.is_done():\n-                        # operation terminated by controller, release pre-allocated memory\n-                        self.mem_pool_host.free(\n-                            operation.host_indices[operation.completed_tokens :]\n-                        )\n-                        break\n+                if self.storage_batchedio:\n+                    if self.storage_zerocopy:",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2233748615",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7211,
        "pr_file": "python/sglang/srt/managers/cache_controller.py",
        "discussion_id": "2233748615",
        "commented_code": "@@ -521,24 +547,37 @@ def prefetch_io_aux_func(self):\n         while not self.stop_event.is_set():\n             try:\n                 operation = self.prefetch_buffer.get(block=True, timeout=1)\n-                for h in operation.hash_value:\n-                    page_data = self.storage_backend.get(h)\n-                    if page_data is None:\n-                        logger.warning(\n-                            f\"Prefetch operation {operation.request_id} failed to retrieve page {h}.\"\n-                        )\n-                        break\n-                    self.mem_pool_host.set_from_flat_data_page(\n-                        operation.host_indices[operation.completed_tokens],\n-                        page_data,\n-                    )\n-                    operation.increment(self.page_size)\n-                    if operation.is_done():\n-                        # operation terminated by controller, release pre-allocated memory\n-                        self.mem_pool_host.free(\n-                            operation.host_indices[operation.completed_tokens :]\n-                        )\n-                        break\n+                if self.storage_batchedio:\n+                    if self.storage_zerocopy:",
        "comment_created_at": "2025-07-27T06:12:49+00:00",
        "comment_author": "xiezhq-hermann",
        "comment_body": "actually, would you mind move the zero_copy logics into a function? it would be more modular.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2262738571",
    "pr_number": 8837,
    "pr_file": "python/sglang/srt/entrypoints/openai/protocol.py",
    "created_at": "2025-08-08T11:50:10+00:00",
    "commented_code": "modalities: List[str]\n     stop: List[str]\n     tool_call_constraint: Optional[Any] = None\n+\n+\n+class ResponseReasoningTextContent(BaseModel):\n+    text: str\n+    type: Literal[\"reasoning_text\"] = \"reasoning_text\"\n+\n+\n+class ResponseReasoningItem(BaseModel):",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2262738571",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8837,
        "pr_file": "python/sglang/srt/entrypoints/openai/protocol.py",
        "discussion_id": "2262738571",
        "commented_code": "@@ -645,3 +852,22 @@ class MessageProcessingResult:\n     modalities: List[str]\n     stop: List[str]\n     tool_call_constraint: Optional[Any] = None\n+\n+\n+class ResponseReasoningTextContent(BaseModel):\n+    text: str\n+    type: Literal[\"reasoning_text\"] = \"reasoning_text\"\n+\n+\n+class ResponseReasoningItem(BaseModel):",
        "comment_created_at": "2025-08-08T11:50:10+00:00",
        "comment_author": "merrymercy",
        "comment_body": "Why do you define this class, but also import it from `openai.types.responses`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2264476288",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8837,
        "pr_file": "python/sglang/srt/entrypoints/openai/protocol.py",
        "discussion_id": "2262738571",
        "commented_code": "@@ -645,3 +852,22 @@ class MessageProcessingResult:\n     modalities: List[str]\n     stop: List[str]\n     tool_call_constraint: Optional[Any] = None\n+\n+\n+class ResponseReasoningTextContent(BaseModel):\n+    text: str\n+    type: Literal[\"reasoning_text\"] = \"reasoning_text\"\n+\n+\n+class ResponseReasoningItem(BaseModel):",
        "comment_created_at": "2025-08-09T04:29:10+00:00",
        "comment_author": "JustinTong0323",
        "comment_body": "my bad, import this to solve pydantic ERROR but forgot to delete defined one, would have a cleanup PR. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2265606355",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8837,
        "pr_file": "python/sglang/srt/entrypoints/openai/protocol.py",
        "discussion_id": "2262738571",
        "commented_code": "@@ -645,3 +852,22 @@ class MessageProcessingResult:\n     modalities: List[str]\n     stop: List[str]\n     tool_call_constraint: Optional[Any] = None\n+\n+\n+class ResponseReasoningTextContent(BaseModel):\n+    text: str\n+    type: Literal[\"reasoning_text\"] = \"reasoning_text\"\n+\n+\n+class ResponseReasoningItem(BaseModel):",
        "comment_created_at": "2025-08-11T03:33:16+00:00",
        "comment_author": "CatherineSue",
        "comment_body": "Removed in #9043",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1911392555",
    "pr_number": 2786,
    "pr_file": "python/sglang/srt/server_args.py",
    "created_at": "2025-01-10T21:12:50+00:00",
    "commented_code": "logger = logging.getLogger(__name__)\n \n \n+def nullable_str(val: str):",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "1911392555",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 2786,
        "pr_file": "python/sglang/srt/server_args.py",
        "discussion_id": "1911392555",
        "commented_code": "@@ -37,6 +37,12 @@\n logger = logging.getLogger(__name__)\n \n \n+def nullable_str(val: str):",
        "comment_created_at": "2025-01-10T21:12:50+00:00",
        "comment_author": "merrymercy",
        "comment_body": "move this to `python/sglang/srt/utils.py`",
        "pr_file_module": null
      },
      {
        "comment_id": "1911966803",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 2786,
        "pr_file": "python/sglang/srt/server_args.py",
        "discussion_id": "1911392555",
        "commented_code": "@@ -37,6 +37,12 @@\n logger = logging.getLogger(__name__)\n \n \n+def nullable_str(val: str):",
        "comment_created_at": "2025-01-11T11:17:37+00:00",
        "comment_author": "bjmsong",
        "comment_body": "OK, it's done.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2220596933",
    "pr_number": 8192,
    "pr_file": "python/sglang/srt/managers/tokenizer_manager.py",
    "created_at": "2025-07-21T23:52:24+00:00",
    "commented_code": "get_zmq_socket,\n     kill_process_tree,\n )\n+QWEN3_RERANK_TYPE = \"qwen3-rerank\"",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2220596933",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8192,
        "pr_file": "python/sglang/srt/managers/tokenizer_manager.py",
        "discussion_id": "2220596933",
        "commented_code": "@@ -121,6 +121,7 @@\n     get_zmq_socket,\n     kill_process_tree,\n )\n+QWEN3_RERANK_TYPE = \"qwen3-rerank\"",
        "comment_created_at": "2025-07-21T23:52:24+00:00",
        "comment_author": "lambert0312",
        "comment_body": "It is not recommended to put this in tokenizer_manager, it is too low-level, it is recommended to put it in serving_score.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2220597470",
    "pr_number": 8192,
    "pr_file": "python/sglang/srt/managers/tokenizer_manager.py",
    "created_at": "2025-07-21T23:52:59+00:00",
    "commented_code": "if len(self.model_update_tmp) == self.server_args.dp_size:\n                 self.model_update_result.set_result(self.model_update_tmp)\n \n+    def _qwen3_rerank_customize_instruction(self, instruction: Optional[str], query: str, documents: List[str]) -> List[str]:",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2220597470",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8192,
        "pr_file": "python/sglang/srt/managers/tokenizer_manager.py",
        "discussion_id": "2220597470",
        "commented_code": "@@ -1701,13 +1702,29 @@ def _handle_update_weights_from_disk_req_output(self, recv_obj):\n             if len(self.model_update_tmp) == self.server_args.dp_size:\n                 self.model_update_result.set_result(self.model_update_tmp)\n \n+    def _qwen3_rerank_customize_instruction(self, instruction: Optional[str], query: str, documents: List[str]) -> List[str]:",
        "comment_created_at": "2025-07-21T23:52:59+00:00",
        "comment_author": "lambert0312",
        "comment_body": "This method not recommended to put this in tokenizer_manager, it is too low-level, it is recommended to put it in serving_score.",
        "pr_file_module": null
      }
    ]
  }
]