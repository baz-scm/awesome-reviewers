[
  {
    "discussion_id": "2292127560",
    "pr_number": 9429,
    "pr_file": "python/sglang/srt/lora/lora_manager.py",
    "created_at": "2025-08-21T21:02:17+00:00",
    "commented_code": "if target_modules is not None:\n             self.target_modules = set(target_modules)\n+            user_normalized_modules = get_normalized_target_modules(self.target_modules)\n+            for lora_id, config in self.configs.items():",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2292127560",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9429,
        "pr_file": "python/sglang/srt/lora/lora_manager.py",
        "discussion_id": "2292127560",
        "commented_code": "@@ -422,6 +422,28 @@ def init_lora_shapes(\n \n         if target_modules is not None:\n             self.target_modules = set(target_modules)\n+            user_normalized_modules = get_normalized_target_modules(self.target_modules)\n+            for lora_id, config in self.configs.items():",
        "comment_created_at": "2025-08-21T21:02:17+00:00",
        "comment_author": "lifuhuang",
        "comment_body": "nit: with your change, now L458 seems a bit redundant. In theory, we only need to call get_normalized_target_modules once for each target_modules (e.g., once for the --lora-target-modules and once for each adapter config).",
        "pr_file_module": null
      },
      {
        "comment_id": "2292131727",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9429,
        "pr_file": "python/sglang/srt/lora/lora_manager.py",
        "discussion_id": "2292127560",
        "commented_code": "@@ -422,6 +422,28 @@ def init_lora_shapes(\n \n         if target_modules is not None:\n             self.target_modules = set(target_modules)\n+            user_normalized_modules = get_normalized_target_modules(self.target_modules)\n+            for lora_id, config in self.configs.items():",
        "comment_created_at": "2025-08-21T21:04:56+00:00",
        "comment_author": "lifuhuang",
        "comment_body": "It's a bit hard to describe in text so I pushed a commit directly to your branch, please let me know if it looks good to you. Feel free to make adjustment / revert as needed. Thanks!",
        "pr_file_module": null
      },
      {
        "comment_id": "2292453982",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9429,
        "pr_file": "python/sglang/srt/lora/lora_manager.py",
        "discussion_id": "2292127560",
        "commented_code": "@@ -422,6 +422,28 @@ def init_lora_shapes(\n \n         if target_modules is not None:\n             self.target_modules = set(target_modules)\n+            user_normalized_modules = get_normalized_target_modules(self.target_modules)\n+            for lora_id, config in self.configs.items():",
        "comment_created_at": "2025-08-22T01:13:46+00:00",
        "comment_author": "Beichen-Ma",
        "comment_body": "Thanks for making the changes! I agree with your point and it makes sense to me. Appreciate your help!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2277373260",
    "pr_number": 8616,
    "pr_file": "python/sglang/srt/models/deepseek_v2.py",
    "created_at": "2025-08-14T17:59:49+00:00",
    "commented_code": "# will be helpful for understanding the purpose of this function.\n \n         # First do normal mha forward to get output for extended part\n-        if self.q_lora_rank is not None:\n-            q, latent_cache = self.fused_qkv_a_proj_with_mqa(hidden_states)[0].split(\n-                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1\n-            )\n-            q = self.q_a_layernorm(q)\n-            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)\n-        else:\n-            q = self.q_proj(hidden_states)[0].view(\n-                -1, self.num_local_heads, self.qk_head_dim\n-            )\n-            latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]\n-        _, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n-        kv_a, _ = latent_cache.split([self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n-        latent_cache = latent_cache.unsqueeze(1)\n-        kv_a = self.kv_a_layernorm(kv_a)\n-        kv = self.kv_b_proj(kv_a)[0]\n-        kv = kv.view(-1, self.num_local_heads, self.qk_nope_head_dim + self.v_head_dim)\n-        k_nope = kv[..., : self.qk_nope_head_dim]\n-        v = kv[..., self.qk_nope_head_dim :]\n-        k_pe = latent_cache[:, :, self.kv_lora_rank :]\n-\n-        q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)\n-        q[..., self.qk_nope_head_dim :] = q_pe\n-        k = torch.empty_like(q)\n-        k[..., : self.qk_nope_head_dim] = k_nope\n-        k[..., self.qk_nope_head_dim :] = k_pe\n-\n-        latent_cache[:, :, : self.kv_lora_rank] = kv_a.unsqueeze(1)\n-        latent_cache[:, :, self.kv_lora_rank :] = k_pe\n-\n-        # Save latent cache\n-        forward_batch.token_to_kv_pool.set_kv_buffer(\n-            self.attn_mha, forward_batch.out_cache_loc, latent_cache, None\n+        return self.forward_normal_prepare(\n+            positions, hidden_states, forward_batch, zero_allocator\n         )\n \n-        return q, k, v, forward_batch\n-\n     def forward_normal_chunked_kv_core(self, q, k, v, forward_batch):\n+        has_extend_prefix = any(forward_batch.extend_prefix_lens_cpu)\n+        # Only initialize the info once\n+        if has_extend_prefix and forward_batch.num_prefix_chunks is None:\n+            forward_batch.prepare_chunked_prefix_cache_info(q.device)\n+            if hasattr(forward_batch.attn_backend, \"init_mha_chunk_metadata\"):",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2277373260",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8616,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2277373260",
        "commented_code": "@@ -1718,55 +1719,26 @@ def forward_normal_chunked_kv_prepare(\n         # will be helpful for understanding the purpose of this function.\n \n         # First do normal mha forward to get output for extended part\n-        if self.q_lora_rank is not None:\n-            q, latent_cache = self.fused_qkv_a_proj_with_mqa(hidden_states)[0].split(\n-                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1\n-            )\n-            q = self.q_a_layernorm(q)\n-            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)\n-        else:\n-            q = self.q_proj(hidden_states)[0].view(\n-                -1, self.num_local_heads, self.qk_head_dim\n-            )\n-            latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]\n-        _, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n-        kv_a, _ = latent_cache.split([self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n-        latent_cache = latent_cache.unsqueeze(1)\n-        kv_a = self.kv_a_layernorm(kv_a)\n-        kv = self.kv_b_proj(kv_a)[0]\n-        kv = kv.view(-1, self.num_local_heads, self.qk_nope_head_dim + self.v_head_dim)\n-        k_nope = kv[..., : self.qk_nope_head_dim]\n-        v = kv[..., self.qk_nope_head_dim :]\n-        k_pe = latent_cache[:, :, self.kv_lora_rank :]\n-\n-        q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)\n-        q[..., self.qk_nope_head_dim :] = q_pe\n-        k = torch.empty_like(q)\n-        k[..., : self.qk_nope_head_dim] = k_nope\n-        k[..., self.qk_nope_head_dim :] = k_pe\n-\n-        latent_cache[:, :, : self.kv_lora_rank] = kv_a.unsqueeze(1)\n-        latent_cache[:, :, self.kv_lora_rank :] = k_pe\n-\n-        # Save latent cache\n-        forward_batch.token_to_kv_pool.set_kv_buffer(\n-            self.attn_mha, forward_batch.out_cache_loc, latent_cache, None\n+        return self.forward_normal_prepare(\n+            positions, hidden_states, forward_batch, zero_allocator\n         )\n \n-        return q, k, v, forward_batch\n-\n     def forward_normal_chunked_kv_core(self, q, k, v, forward_batch):\n+        has_extend_prefix = any(forward_batch.extend_prefix_lens_cpu)\n+        # Only initialize the info once\n+        if has_extend_prefix and forward_batch.num_prefix_chunks is None:\n+            forward_batch.prepare_chunked_prefix_cache_info(q.device)\n+            if hasattr(forward_batch.attn_backend, \"init_mha_chunk_metadata\"):",
        "comment_created_at": "2025-08-14T17:59:49+00:00",
        "comment_author": "elfiegg",
        "comment_body": "I think by doing this you're initiating ragged wrapper twice - once in init_forward_metadata another in init_mha_chunk_metadata. This would cause a small overhead everytime forward() is called.\r\nAlternatively you can check if chunked info is in init_forward_metadata, init it like below and init your chunk wrapper accordingly.\r\n```\r\nif forward_batch.num_prefix_chunks is None:\r\n                    forward_batch.prepare_chunked_prefix_cache_info(\r\n                        forward_batch.seq_lens.device\r\n                    )\r\n```\r\n\r\n\r\nSimilarly to PR #9132 ",
        "pr_file_module": null
      },
      {
        "comment_id": "2278132718",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8616,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2277373260",
        "commented_code": "@@ -1718,55 +1719,26 @@ def forward_normal_chunked_kv_prepare(\n         # will be helpful for understanding the purpose of this function.\n \n         # First do normal mha forward to get output for extended part\n-        if self.q_lora_rank is not None:\n-            q, latent_cache = self.fused_qkv_a_proj_with_mqa(hidden_states)[0].split(\n-                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1\n-            )\n-            q = self.q_a_layernorm(q)\n-            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)\n-        else:\n-            q = self.q_proj(hidden_states)[0].view(\n-                -1, self.num_local_heads, self.qk_head_dim\n-            )\n-            latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]\n-        _, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n-        kv_a, _ = latent_cache.split([self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n-        latent_cache = latent_cache.unsqueeze(1)\n-        kv_a = self.kv_a_layernorm(kv_a)\n-        kv = self.kv_b_proj(kv_a)[0]\n-        kv = kv.view(-1, self.num_local_heads, self.qk_nope_head_dim + self.v_head_dim)\n-        k_nope = kv[..., : self.qk_nope_head_dim]\n-        v = kv[..., self.qk_nope_head_dim :]\n-        k_pe = latent_cache[:, :, self.kv_lora_rank :]\n-\n-        q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)\n-        q[..., self.qk_nope_head_dim :] = q_pe\n-        k = torch.empty_like(q)\n-        k[..., : self.qk_nope_head_dim] = k_nope\n-        k[..., self.qk_nope_head_dim :] = k_pe\n-\n-        latent_cache[:, :, : self.kv_lora_rank] = kv_a.unsqueeze(1)\n-        latent_cache[:, :, self.kv_lora_rank :] = k_pe\n-\n-        # Save latent cache\n-        forward_batch.token_to_kv_pool.set_kv_buffer(\n-            self.attn_mha, forward_batch.out_cache_loc, latent_cache, None\n+        return self.forward_normal_prepare(\n+            positions, hidden_states, forward_batch, zero_allocator\n         )\n \n-        return q, k, v, forward_batch\n-\n     def forward_normal_chunked_kv_core(self, q, k, v, forward_batch):\n+        has_extend_prefix = any(forward_batch.extend_prefix_lens_cpu)\n+        # Only initialize the info once\n+        if has_extend_prefix and forward_batch.num_prefix_chunks is None:\n+            forward_batch.prepare_chunked_prefix_cache_info(q.device)\n+            if hasattr(forward_batch.attn_backend, \"init_mha_chunk_metadata\"):",
        "comment_created_at": "2025-08-15T03:05:03+00:00",
        "comment_author": "xu-yfei",
        "comment_body": "I want to integrate this logic with fa3. MHA with chunk prefix may only be enabled under certain conditions. Under some conditions, the MLA  absorb process is still required. Only the MHA process requires prepare_chunked_prefix_cache_info. I will try to avoid repeating plan ragged.",
        "pr_file_module": null
      },
      {
        "comment_id": "2278149961",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8616,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2277373260",
        "commented_code": "@@ -1718,55 +1719,26 @@ def forward_normal_chunked_kv_prepare(\n         # will be helpful for understanding the purpose of this function.\n \n         # First do normal mha forward to get output for extended part\n-        if self.q_lora_rank is not None:\n-            q, latent_cache = self.fused_qkv_a_proj_with_mqa(hidden_states)[0].split(\n-                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1\n-            )\n-            q = self.q_a_layernorm(q)\n-            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)\n-        else:\n-            q = self.q_proj(hidden_states)[0].view(\n-                -1, self.num_local_heads, self.qk_head_dim\n-            )\n-            latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]\n-        _, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n-        kv_a, _ = latent_cache.split([self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n-        latent_cache = latent_cache.unsqueeze(1)\n-        kv_a = self.kv_a_layernorm(kv_a)\n-        kv = self.kv_b_proj(kv_a)[0]\n-        kv = kv.view(-1, self.num_local_heads, self.qk_nope_head_dim + self.v_head_dim)\n-        k_nope = kv[..., : self.qk_nope_head_dim]\n-        v = kv[..., self.qk_nope_head_dim :]\n-        k_pe = latent_cache[:, :, self.kv_lora_rank :]\n-\n-        q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)\n-        q[..., self.qk_nope_head_dim :] = q_pe\n-        k = torch.empty_like(q)\n-        k[..., : self.qk_nope_head_dim] = k_nope\n-        k[..., self.qk_nope_head_dim :] = k_pe\n-\n-        latent_cache[:, :, : self.kv_lora_rank] = kv_a.unsqueeze(1)\n-        latent_cache[:, :, self.kv_lora_rank :] = k_pe\n-\n-        # Save latent cache\n-        forward_batch.token_to_kv_pool.set_kv_buffer(\n-            self.attn_mha, forward_batch.out_cache_loc, latent_cache, None\n+        return self.forward_normal_prepare(\n+            positions, hidden_states, forward_batch, zero_allocator\n         )\n \n-        return q, k, v, forward_batch\n-\n     def forward_normal_chunked_kv_core(self, q, k, v, forward_batch):\n+        has_extend_prefix = any(forward_batch.extend_prefix_lens_cpu)\n+        # Only initialize the info once\n+        if has_extend_prefix and forward_batch.num_prefix_chunks is None:\n+            forward_batch.prepare_chunked_prefix_cache_info(q.device)\n+            if hasattr(forward_batch.attn_backend, \"init_mha_chunk_metadata\"):",
        "comment_created_at": "2025-08-15T03:26:35+00:00",
        "comment_author": "xu-yfei",
        "comment_body": "`init_mha_chunk_metadata` only is called when prefix len > 0, so ragged wrapper is not initiated twice\r\n```\r\n    def forward_normal_chunked_kv_core(self, q, k, v, forward_batch):\r\n        has_extend_prefix = any(forward_batch.extend_prefix_lens_cpu)\r\n        # Only initialize the info once\r\n        if has_extend_prefix and forward_batch.num_prefix_chunks is None:\r\n            forward_batch.prepare_chunked_prefix_cache_info(q.device)\r\n            if hasattr(forward_batch.attn_backend, \"init_mha_chunk_metadata\"):\r\n                forward_batch.attn_backend.init_mha_chunk_metadata(forward_batch)\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2278175754",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8616,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2277373260",
        "commented_code": "@@ -1718,55 +1719,26 @@ def forward_normal_chunked_kv_prepare(\n         # will be helpful for understanding the purpose of this function.\n \n         # First do normal mha forward to get output for extended part\n-        if self.q_lora_rank is not None:\n-            q, latent_cache = self.fused_qkv_a_proj_with_mqa(hidden_states)[0].split(\n-                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1\n-            )\n-            q = self.q_a_layernorm(q)\n-            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)\n-        else:\n-            q = self.q_proj(hidden_states)[0].view(\n-                -1, self.num_local_heads, self.qk_head_dim\n-            )\n-            latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]\n-        _, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n-        kv_a, _ = latent_cache.split([self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n-        latent_cache = latent_cache.unsqueeze(1)\n-        kv_a = self.kv_a_layernorm(kv_a)\n-        kv = self.kv_b_proj(kv_a)[0]\n-        kv = kv.view(-1, self.num_local_heads, self.qk_nope_head_dim + self.v_head_dim)\n-        k_nope = kv[..., : self.qk_nope_head_dim]\n-        v = kv[..., self.qk_nope_head_dim :]\n-        k_pe = latent_cache[:, :, self.kv_lora_rank :]\n-\n-        q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)\n-        q[..., self.qk_nope_head_dim :] = q_pe\n-        k = torch.empty_like(q)\n-        k[..., : self.qk_nope_head_dim] = k_nope\n-        k[..., self.qk_nope_head_dim :] = k_pe\n-\n-        latent_cache[:, :, : self.kv_lora_rank] = kv_a.unsqueeze(1)\n-        latent_cache[:, :, self.kv_lora_rank :] = k_pe\n-\n-        # Save latent cache\n-        forward_batch.token_to_kv_pool.set_kv_buffer(\n-            self.attn_mha, forward_batch.out_cache_loc, latent_cache, None\n+        return self.forward_normal_prepare(\n+            positions, hidden_states, forward_batch, zero_allocator\n         )\n \n-        return q, k, v, forward_batch\n-\n     def forward_normal_chunked_kv_core(self, q, k, v, forward_batch):\n+        has_extend_prefix = any(forward_batch.extend_prefix_lens_cpu)\n+        # Only initialize the info once\n+        if has_extend_prefix and forward_batch.num_prefix_chunks is None:\n+            forward_batch.prepare_chunked_prefix_cache_info(q.device)\n+            if hasattr(forward_batch.attn_backend, \"init_mha_chunk_metadata\"):",
        "comment_created_at": "2025-08-15T03:57:49+00:00",
        "comment_author": "elfiegg",
        "comment_body": "My understanding is FlashInferMLAAttnBackend's init_forward_metadata will be called in forward() anyways so a ragged_wrapper will be initiated no matter what. And another call to init_mha_chunk_metadata() would initiate another ragged wrapper for new tokens",
        "pr_file_module": null
      },
      {
        "comment_id": "2278184762",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8616,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2277373260",
        "commented_code": "@@ -1718,55 +1719,26 @@ def forward_normal_chunked_kv_prepare(\n         # will be helpful for understanding the purpose of this function.\n \n         # First do normal mha forward to get output for extended part\n-        if self.q_lora_rank is not None:\n-            q, latent_cache = self.fused_qkv_a_proj_with_mqa(hidden_states)[0].split(\n-                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1\n-            )\n-            q = self.q_a_layernorm(q)\n-            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)\n-        else:\n-            q = self.q_proj(hidden_states)[0].view(\n-                -1, self.num_local_heads, self.qk_head_dim\n-            )\n-            latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]\n-        _, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n-        kv_a, _ = latent_cache.split([self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n-        latent_cache = latent_cache.unsqueeze(1)\n-        kv_a = self.kv_a_layernorm(kv_a)\n-        kv = self.kv_b_proj(kv_a)[0]\n-        kv = kv.view(-1, self.num_local_heads, self.qk_nope_head_dim + self.v_head_dim)\n-        k_nope = kv[..., : self.qk_nope_head_dim]\n-        v = kv[..., self.qk_nope_head_dim :]\n-        k_pe = latent_cache[:, :, self.kv_lora_rank :]\n-\n-        q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)\n-        q[..., self.qk_nope_head_dim :] = q_pe\n-        k = torch.empty_like(q)\n-        k[..., : self.qk_nope_head_dim] = k_nope\n-        k[..., self.qk_nope_head_dim :] = k_pe\n-\n-        latent_cache[:, :, : self.kv_lora_rank] = kv_a.unsqueeze(1)\n-        latent_cache[:, :, self.kv_lora_rank :] = k_pe\n-\n-        # Save latent cache\n-        forward_batch.token_to_kv_pool.set_kv_buffer(\n-            self.attn_mha, forward_batch.out_cache_loc, latent_cache, None\n+        return self.forward_normal_prepare(\n+            positions, hidden_states, forward_batch, zero_allocator\n         )\n \n-        return q, k, v, forward_batch\n-\n     def forward_normal_chunked_kv_core(self, q, k, v, forward_batch):\n+        has_extend_prefix = any(forward_batch.extend_prefix_lens_cpu)\n+        # Only initialize the info once\n+        if has_extend_prefix and forward_batch.num_prefix_chunks is None:\n+            forward_batch.prepare_chunked_prefix_cache_info(q.device)\n+            if hasattr(forward_batch.attn_backend, \"init_mha_chunk_metadata\"):",
        "comment_created_at": "2025-08-15T04:10:27+00:00",
        "comment_author": "elfiegg",
        "comment_body": "So - you might only need to init chunk prefill wrappers in init_mha_chunk_metadata()",
        "pr_file_module": null
      },
      {
        "comment_id": "2278186866",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8616,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2277373260",
        "commented_code": "@@ -1718,55 +1719,26 @@ def forward_normal_chunked_kv_prepare(\n         # will be helpful for understanding the purpose of this function.\n \n         # First do normal mha forward to get output for extended part\n-        if self.q_lora_rank is not None:\n-            q, latent_cache = self.fused_qkv_a_proj_with_mqa(hidden_states)[0].split(\n-                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1\n-            )\n-            q = self.q_a_layernorm(q)\n-            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)\n-        else:\n-            q = self.q_proj(hidden_states)[0].view(\n-                -1, self.num_local_heads, self.qk_head_dim\n-            )\n-            latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]\n-        _, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n-        kv_a, _ = latent_cache.split([self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n-        latent_cache = latent_cache.unsqueeze(1)\n-        kv_a = self.kv_a_layernorm(kv_a)\n-        kv = self.kv_b_proj(kv_a)[0]\n-        kv = kv.view(-1, self.num_local_heads, self.qk_nope_head_dim + self.v_head_dim)\n-        k_nope = kv[..., : self.qk_nope_head_dim]\n-        v = kv[..., self.qk_nope_head_dim :]\n-        k_pe = latent_cache[:, :, self.kv_lora_rank :]\n-\n-        q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)\n-        q[..., self.qk_nope_head_dim :] = q_pe\n-        k = torch.empty_like(q)\n-        k[..., : self.qk_nope_head_dim] = k_nope\n-        k[..., self.qk_nope_head_dim :] = k_pe\n-\n-        latent_cache[:, :, : self.kv_lora_rank] = kv_a.unsqueeze(1)\n-        latent_cache[:, :, self.kv_lora_rank :] = k_pe\n-\n-        # Save latent cache\n-        forward_batch.token_to_kv_pool.set_kv_buffer(\n-            self.attn_mha, forward_batch.out_cache_loc, latent_cache, None\n+        return self.forward_normal_prepare(\n+            positions, hidden_states, forward_batch, zero_allocator\n         )\n \n-        return q, k, v, forward_batch\n-\n     def forward_normal_chunked_kv_core(self, q, k, v, forward_batch):\n+        has_extend_prefix = any(forward_batch.extend_prefix_lens_cpu)\n+        # Only initialize the info once\n+        if has_extend_prefix and forward_batch.num_prefix_chunks is None:\n+            forward_batch.prepare_chunked_prefix_cache_info(q.device)\n+            if hasattr(forward_batch.attn_backend, \"init_mha_chunk_metadata\"):",
        "comment_created_at": "2025-08-15T04:13:21+00:00",
        "comment_author": "xu-yfei",
        "comment_body": "prefill_wrapper_paged in init_forward_metadata will be inited only when extend_no_prefix is True,     prefill_wrapper_paged in init_mha_chunk_metadata will be inited only when has_extend_prefix is True\r\n```\r\n    def init_forward_metadata(self, forward_batch: ForwardBatch):\r\n            extend_no_prefix = not any(forward_batch.extend_prefix_lens_cpu)\r\n            use_ragged = (\r\n                not global_server_args_dict[\"flashinfer_mla_disable_ragged\"]\r\n                and extend_no_prefix\r\n            )\r\n\r\n            self.indices_updater_prefill.update(\r\n                forward_batch.req_pool_indices,\r\n                forward_batch.seq_lens,\r\n                forward_batch.seq_lens_sum,\r\n                prefix_lens,\r\n                prefill_wrapper_paged=self.prefill_wrapper_paged,\r\n                use_ragged=use_ragged,\r\n            )\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2278194060",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8616,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2277373260",
        "commented_code": "@@ -1718,55 +1719,26 @@ def forward_normal_chunked_kv_prepare(\n         # will be helpful for understanding the purpose of this function.\n \n         # First do normal mha forward to get output for extended part\n-        if self.q_lora_rank is not None:\n-            q, latent_cache = self.fused_qkv_a_proj_with_mqa(hidden_states)[0].split(\n-                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1\n-            )\n-            q = self.q_a_layernorm(q)\n-            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)\n-        else:\n-            q = self.q_proj(hidden_states)[0].view(\n-                -1, self.num_local_heads, self.qk_head_dim\n-            )\n-            latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]\n-        _, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n-        kv_a, _ = latent_cache.split([self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n-        latent_cache = latent_cache.unsqueeze(1)\n-        kv_a = self.kv_a_layernorm(kv_a)\n-        kv = self.kv_b_proj(kv_a)[0]\n-        kv = kv.view(-1, self.num_local_heads, self.qk_nope_head_dim + self.v_head_dim)\n-        k_nope = kv[..., : self.qk_nope_head_dim]\n-        v = kv[..., self.qk_nope_head_dim :]\n-        k_pe = latent_cache[:, :, self.kv_lora_rank :]\n-\n-        q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)\n-        q[..., self.qk_nope_head_dim :] = q_pe\n-        k = torch.empty_like(q)\n-        k[..., : self.qk_nope_head_dim] = k_nope\n-        k[..., self.qk_nope_head_dim :] = k_pe\n-\n-        latent_cache[:, :, : self.kv_lora_rank] = kv_a.unsqueeze(1)\n-        latent_cache[:, :, self.kv_lora_rank :] = k_pe\n-\n-        # Save latent cache\n-        forward_batch.token_to_kv_pool.set_kv_buffer(\n-            self.attn_mha, forward_batch.out_cache_loc, latent_cache, None\n+        return self.forward_normal_prepare(\n+            positions, hidden_states, forward_batch, zero_allocator\n         )\n \n-        return q, k, v, forward_batch\n-\n     def forward_normal_chunked_kv_core(self, q, k, v, forward_batch):\n+        has_extend_prefix = any(forward_batch.extend_prefix_lens_cpu)\n+        # Only initialize the info once\n+        if has_extend_prefix and forward_batch.num_prefix_chunks is None:\n+            forward_batch.prepare_chunked_prefix_cache_info(q.device)\n+            if hasattr(forward_batch.attn_backend, \"init_mha_chunk_metadata\"):",
        "comment_created_at": "2025-08-15T04:21:58+00:00",
        "comment_author": "elfiegg",
        "comment_body": "Okay now I see, that case it will be fine. The logic is a bit of split, kinda puzzled me for a while",
        "pr_file_module": null
      },
      {
        "comment_id": "2278327793",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8616,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2277373260",
        "commented_code": "@@ -1718,55 +1719,26 @@ def forward_normal_chunked_kv_prepare(\n         # will be helpful for understanding the purpose of this function.\n \n         # First do normal mha forward to get output for extended part\n-        if self.q_lora_rank is not None:\n-            q, latent_cache = self.fused_qkv_a_proj_with_mqa(hidden_states)[0].split(\n-                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1\n-            )\n-            q = self.q_a_layernorm(q)\n-            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)\n-        else:\n-            q = self.q_proj(hidden_states)[0].view(\n-                -1, self.num_local_heads, self.qk_head_dim\n-            )\n-            latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]\n-        _, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n-        kv_a, _ = latent_cache.split([self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n-        latent_cache = latent_cache.unsqueeze(1)\n-        kv_a = self.kv_a_layernorm(kv_a)\n-        kv = self.kv_b_proj(kv_a)[0]\n-        kv = kv.view(-1, self.num_local_heads, self.qk_nope_head_dim + self.v_head_dim)\n-        k_nope = kv[..., : self.qk_nope_head_dim]\n-        v = kv[..., self.qk_nope_head_dim :]\n-        k_pe = latent_cache[:, :, self.kv_lora_rank :]\n-\n-        q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)\n-        q[..., self.qk_nope_head_dim :] = q_pe\n-        k = torch.empty_like(q)\n-        k[..., : self.qk_nope_head_dim] = k_nope\n-        k[..., self.qk_nope_head_dim :] = k_pe\n-\n-        latent_cache[:, :, : self.kv_lora_rank] = kv_a.unsqueeze(1)\n-        latent_cache[:, :, self.kv_lora_rank :] = k_pe\n-\n-        # Save latent cache\n-        forward_batch.token_to_kv_pool.set_kv_buffer(\n-            self.attn_mha, forward_batch.out_cache_loc, latent_cache, None\n+        return self.forward_normal_prepare(\n+            positions, hidden_states, forward_batch, zero_allocator\n         )\n \n-        return q, k, v, forward_batch\n-\n     def forward_normal_chunked_kv_core(self, q, k, v, forward_batch):\n+        has_extend_prefix = any(forward_batch.extend_prefix_lens_cpu)\n+        # Only initialize the info once\n+        if has_extend_prefix and forward_batch.num_prefix_chunks is None:\n+            forward_batch.prepare_chunked_prefix_cache_info(q.device)\n+            if hasattr(forward_batch.attn_backend, \"init_mha_chunk_metadata\"):",
        "comment_created_at": "2025-08-15T05:49:09+00:00",
        "comment_author": "xu-yfei",
        "comment_body": "This part can be optimized later maybe. It is necessary to know in advance whether mha chunk or mla. This logic is in the deepseek model code.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2284297385",
    "pr_number": 8593,
    "pr_file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "created_at": "2025-08-19T06:49:30+00:00",
    "commented_code": "**fast_decode_kwargs,\n     ):\n         bs = len(req_pool_indices)\n-        q_indptr = q_indptr[: bs + 1]\n+        q_indptr = torch.arange(0, bs + 1, dtype=torch.int32, device=q_indptr.device)",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2284297385",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8593,
        "pr_file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
        "discussion_id": "2284297385",
        "commented_code": "@@ -554,17 +558,24 @@ def call_begin_forward(\n         **fast_decode_kwargs,\n     ):\n         bs = len(req_pool_indices)\n-        q_indptr = q_indptr[: bs + 1]\n+        q_indptr = torch.arange(0, bs + 1, dtype=torch.int32, device=q_indptr.device)",
        "comment_created_at": "2025-08-19T06:49:30+00:00",
        "comment_author": "Fridge003",
        "comment_body": "No need to do this line. The passed in `q_indptr` has already been torch.arange(0, bs + 1)",
        "pr_file_module": null
      },
      {
        "comment_id": "2286774259",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8593,
        "pr_file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
        "discussion_id": "2284297385",
        "commented_code": "@@ -554,17 +558,24 @@ def call_begin_forward(\n         **fast_decode_kwargs,\n     ):\n         bs = len(req_pool_indices)\n-        q_indptr = q_indptr[: bs + 1]\n+        q_indptr = torch.arange(0, bs + 1, dtype=torch.int32, device=q_indptr.device)",
        "comment_created_at": "2025-08-20T01:36:11+00:00",
        "comment_author": "pavanimajety",
        "comment_body": "I added this because with DP attention I noticed cases where bs > max_bs initialized   in the constructor.",
        "pr_file_module": null
      },
      {
        "comment_id": "2286945460",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8593,
        "pr_file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
        "discussion_id": "2284297385",
        "commented_code": "@@ -554,17 +558,24 @@ def call_begin_forward(\n         **fast_decode_kwargs,\n     ):\n         bs = len(req_pool_indices)\n-        q_indptr = q_indptr[: bs + 1]\n+        q_indptr = torch.arange(0, bs + 1, dtype=torch.int32, device=q_indptr.device)",
        "comment_created_at": "2025-08-20T04:23:54+00:00",
        "comment_author": "Fridge003",
        "comment_body": "Can you preallocate a large q_indptr during initialization, and do the indexing here?\r\nSince torch.arange will introduce some latency",
        "pr_file_module": null
      },
      {
        "comment_id": "2288986223",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8593,
        "pr_file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
        "discussion_id": "2284297385",
        "commented_code": "@@ -554,17 +558,24 @@ def call_begin_forward(\n         **fast_decode_kwargs,\n     ):\n         bs = len(req_pool_indices)\n-        q_indptr = q_indptr[: bs + 1]\n+        q_indptr = torch.arange(0, bs + 1, dtype=torch.int32, device=q_indptr.device)",
        "comment_created_at": "2025-08-20T18:39:37+00:00",
        "comment_author": "pavanimajety",
        "comment_body": "yeah, good idea, thanks- Changed to this \r\n```\r\n# A hack to pre-initialize large batch size for dp attention\r\nif model_runner.server_args.enable_dp_attention:\r\n   max_bs = model_runner.server_args.dp_size * max_bs \r\nself.q_indptr_decode = torch.arange(\r\n   0, max_bs + 1, dtype=torch.int32, device=model_runner.device\r\n)\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2289460911",
    "pr_number": 8593,
    "pr_file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "created_at": "2025-08-20T22:49:08+00:00",
    "commented_code": "if spec_info is None:\n             assert len(seq_lens) == len(req_pool_indices)\n-            kv_indptr[1 : bs + 1] = torch.cumsum(paged_kernel_lens, dim=0)\n-            kv_indptr = kv_indptr[: bs + 1]\n+            num_pages_per_req = (\n+                paged_kernel_lens + self.page_size - 1\n+            ) // self.page_size\n+            kv_indptr = torch.zeros(",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2289460911",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8593,
        "pr_file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
        "discussion_id": "2289460911",
        "commented_code": "@@ -680,10 +694,15 @@ def call_begin_forward(\n \n         if spec_info is None:\n             assert len(seq_lens) == len(req_pool_indices)\n-            kv_indptr[1 : bs + 1] = torch.cumsum(paged_kernel_lens, dim=0)\n-            kv_indptr = kv_indptr[: bs + 1]\n+            num_pages_per_req = (\n+                paged_kernel_lens + self.page_size - 1\n+            ) // self.page_size\n+            kv_indptr = torch.zeros(",
        "comment_created_at": "2025-08-20T22:49:08+00:00",
        "comment_author": "Fridge003",
        "comment_body": "Here we also need to remove this torch.zeros to avoid extra latency, since kv_indptr has been preallocated",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2289113886",
    "pr_number": 8934,
    "pr_file": "python/sglang/srt/distributed/device_communicators/pynccl_allocator.py",
    "created_at": "2025-08-20T19:35:42+00:00",
    "commented_code": "def is_symmetric_memory_enabled():\n+    # Import here to avoid circular import\n+    from sglang.srt.managers.schedule_batch import global_server_args_dict\n+\n     return global_server_args_dict[\"enable_symm_mem\"]\n \n \n+def is_symmetric_memory_tensor(tensor: torch.Tensor):\n+    if not is_symmetric_memory_enabled():\n+        return False\n+    for segment in get_nccl_mem_pool().snapshot():",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2289113886",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8934,
        "pr_file": "python/sglang/srt/distributed/device_communicators/pynccl_allocator.py",
        "discussion_id": "2289113886",
        "commented_code": "@@ -32,9 +31,22 @@\n \n \n def is_symmetric_memory_enabled():\n+    # Import here to avoid circular import\n+    from sglang.srt.managers.schedule_batch import global_server_args_dict\n+\n     return global_server_args_dict[\"enable_symm_mem\"]\n \n \n+def is_symmetric_memory_tensor(tensor: torch.Tensor):\n+    if not is_symmetric_memory_enabled():\n+        return False\n+    for segment in get_nccl_mem_pool().snapshot():",
        "comment_created_at": "2025-08-20T19:35:42+00:00",
        "comment_author": "trevor-m",
        "comment_body": "It's great that we no longer need to tag the tensors, but I'm concerned about the efficiency of this check. [Collecting the snapshot](https://github.com/pytorch/pytorch/blob/c5cb255625deb4cdbc5780e6911b73498e17ed5a/torch/csrc/cuda/Module.cpp#L724) seems to do a lot and then we also have to iterate over all segments/blocks.\r\n\r\n1. Do you know if basic caching/memoization in this method would work using `tensor.untyped_storage.data_ptr()` as the key? Will the storage change from iteration to iteration or remain consistent?\r\n2. If that doesn't work, could you measure how long this check takes?",
        "pr_file_module": null
      },
      {
        "comment_id": "2289156385",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8934,
        "pr_file": "python/sglang/srt/distributed/device_communicators/pynccl_allocator.py",
        "discussion_id": "2289113886",
        "commented_code": "@@ -32,9 +31,22 @@\n \n \n def is_symmetric_memory_enabled():\n+    # Import here to avoid circular import\n+    from sglang.srt.managers.schedule_batch import global_server_args_dict\n+\n     return global_server_args_dict[\"enable_symm_mem\"]\n \n \n+def is_symmetric_memory_tensor(tensor: torch.Tensor):\n+    if not is_symmetric_memory_enabled():\n+        return False\n+    for segment in get_nccl_mem_pool().snapshot():",
        "comment_created_at": "2025-08-20T19:53:20+00:00",
        "comment_author": "nvcastet",
        "comment_body": "It would change, depending on what gets allocated in the pool But we could at least cache the snapshot at the exit of the context manager.\r\n\r\n2. Yes I could time the call.",
        "pr_file_module": null
      },
      {
        "comment_id": "2289232891",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8934,
        "pr_file": "python/sglang/srt/distributed/device_communicators/pynccl_allocator.py",
        "discussion_id": "2289113886",
        "commented_code": "@@ -32,9 +31,22 @@\n \n \n def is_symmetric_memory_enabled():\n+    # Import here to avoid circular import\n+    from sglang.srt.managers.schedule_batch import global_server_args_dict\n+\n     return global_server_args_dict[\"enable_symm_mem\"]\n \n \n+def is_symmetric_memory_tensor(tensor: torch.Tensor):\n+    if not is_symmetric_memory_enabled():\n+        return False\n+    for segment in get_nccl_mem_pool().snapshot():",
        "comment_created_at": "2025-08-20T20:32:17+00:00",
        "comment_author": "nvcastet",
        "comment_body": "133 us (not cached) vs 5us (cached)\r\nSo I made the changes to cache it at context manager exit.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2271230576",
    "pr_number": 7240,
    "pr_file": "python/sglang/srt/layers/quantization/fp8.py",
    "created_at": "2025-08-12T21:03:12+00:00",
    "commented_code": "from sglang.srt.layers.moe.topk import select_experts\n \n         # Expert selection\n-        topk_weights, topk_ids = select_experts(\n-            hidden_states=x,\n-            router_logits=router_logits,\n-            use_grouped_topk=use_grouped_topk,\n-            top_k=top_k,\n-            renormalize=renormalize,\n-            topk_group=topk_group,\n-            num_expert_group=num_expert_group,\n-            num_fused_shared_experts=num_fused_shared_experts,\n-            custom_routing_function=custom_routing_function,\n-            correction_bias=correction_bias,\n-            routed_scaling_factor=routed_scaling_factor,\n-        )\n+        if (\n+            _use_aiter\n+            and correction_bias is not None\n+            and hasattr(layer, \"non_shared_topk_weights\")",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2271230576",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7240,
        "pr_file": "python/sglang/srt/layers/quantization/fp8.py",
        "discussion_id": "2271230576",
        "commented_code": "@@ -957,19 +957,40 @@ def apply(\n         from sglang.srt.layers.moe.topk import select_experts\n \n         # Expert selection\n-        topk_weights, topk_ids = select_experts(\n-            hidden_states=x,\n-            router_logits=router_logits,\n-            use_grouped_topk=use_grouped_topk,\n-            top_k=top_k,\n-            renormalize=renormalize,\n-            topk_group=topk_group,\n-            num_expert_group=num_expert_group,\n-            num_fused_shared_experts=num_fused_shared_experts,\n-            custom_routing_function=custom_routing_function,\n-            correction_bias=correction_bias,\n-            routed_scaling_factor=routed_scaling_factor,\n-        )\n+        if (\n+            _use_aiter\n+            and correction_bias is not None\n+            and hasattr(layer, \"non_shared_topk_weights\")",
        "comment_created_at": "2025-08-12T21:03:12+00:00",
        "comment_author": "amdosoldea",
        "comment_body": "Nit: If the hasattr manifests time penalty, a caching mechanism could help here. I mean the double check at every iteration can affect some performance.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2268690740",
    "pr_number": 8610,
    "pr_file": "python/sglang/srt/managers/scheduler_profiler_mixin.py",
    "created_at": "2025-08-12T06:08:17+00:00",
    "commented_code": "from sglang.srt.managers.io_struct import ProfileReq, ProfileReqOutput, ProfileReqType\n from sglang.srt.model_executor.forward_batch_info import ForwardMode\n+from sglang.srt.utils import is_npu\n+\n+if is_npu():",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2268690740",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8610,
        "pr_file": "python/sglang/srt/managers/scheduler_profiler_mixin.py",
        "discussion_id": "2268690740",
        "commented_code": "@@ -8,6 +8,17 @@\n \n from sglang.srt.managers.io_struct import ProfileReq, ProfileReqOutput, ProfileReqType\n from sglang.srt.model_executor.forward_batch_info import ForwardMode\n+from sglang.srt.utils import is_npu\n+\n+if is_npu():",
        "comment_created_at": "2025-08-12T06:08:17+00:00",
        "comment_author": "zhyncs",
        "comment_body": "may you cache `is_npu` result",
        "pr_file_module": null
      },
      {
        "comment_id": "2268717010",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8610,
        "pr_file": "python/sglang/srt/managers/scheduler_profiler_mixin.py",
        "discussion_id": "2268690740",
        "commented_code": "@@ -8,6 +8,17 @@\n \n from sglang.srt.managers.io_struct import ProfileReq, ProfileReqOutput, ProfileReqType\n from sglang.srt.model_executor.forward_batch_info import ForwardMode\n+from sglang.srt.utils import is_npu\n+\n+if is_npu():",
        "comment_created_at": "2025-08-12T06:18:53+00:00",
        "comment_author": "li-you-ran",
        "comment_body": "thanks, this is a good suggestion, we will consider adding a caching mechanism to the **is_npu()** function in the future",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2250020438",
    "pr_number": 8731,
    "pr_file": "python/sglang/srt/models/deepseek_v2.py",
    "created_at": "2025-08-03T15:28:27+00:00",
    "commented_code": "hidden_states, residual, forward_batch\n         )\n \n-        can_fuse_mlp_allreduce = (\n-            self._should_fuse_mlp_allreduce_with_next_layer(forward_batch)\n-            and not (self.enable_dp_attention and self.speculative_algorithm.is_eagle())\n-            and not self.is_nextn\n+        batch_size = (\n+            forward_batch.input_ids.shape[0]\n+            if hasattr(forward_batch, \"input_ids\")\n+            else 0\n         )\n+        is_last_layer = self.layer_id == self.config.num_hidden_layers - 1\n+        cache_key = (is_last_layer, batch_size)",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2250020438",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8731,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2250020438",
        "commented_code": "@@ -1877,18 +1920,38 @@ def forward(\n             hidden_states, residual, forward_batch\n         )\n \n-        can_fuse_mlp_allreduce = (\n-            self._should_fuse_mlp_allreduce_with_next_layer(forward_batch)\n-            and not (self.enable_dp_attention and self.speculative_algorithm.is_eagle())\n-            and not self.is_nextn\n+        batch_size = (\n+            forward_batch.input_ids.shape[0]\n+            if hasattr(forward_batch, \"input_ids\")\n+            else 0\n         )\n+        is_last_layer = self.layer_id == self.config.num_hidden_layers - 1\n+        cache_key = (is_last_layer, batch_size)",
        "comment_created_at": "2025-08-03T15:28:27+00:00",
        "comment_author": "ispobock",
        "comment_body": "Can we just pre-compute all the valid cases in a table and lookup it in the run time? Instead of updating the cache table dynamically. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2250201671",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8731,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2250020438",
        "commented_code": "@@ -1877,18 +1920,38 @@ def forward(\n             hidden_states, residual, forward_batch\n         )\n \n-        can_fuse_mlp_allreduce = (\n-            self._should_fuse_mlp_allreduce_with_next_layer(forward_batch)\n-            and not (self.enable_dp_attention and self.speculative_algorithm.is_eagle())\n-            and not self.is_nextn\n+        batch_size = (\n+            forward_batch.input_ids.shape[0]\n+            if hasattr(forward_batch, \"input_ids\")\n+            else 0\n         )\n+        is_last_layer = self.layer_id == self.config.num_hidden_layers - 1\n+        cache_key = (is_last_layer, batch_size)",
        "comment_created_at": "2025-08-04T00:22:05+00:00",
        "comment_author": "BBuf",
        "comment_body": "Good idea, I have modified it.",
        "pr_file_module": null
      }
    ]
  }
]