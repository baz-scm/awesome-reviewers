[
  {
    "discussion_id": "2282968046",
    "pr_number": 8328,
    "pr_file": "python/sglang/srt/layers/attention/ascend_backend.py",
    "created_at": "2025-08-18T17:04:22+00:00",
    "commented_code": "scale=layer.scaling,\n                     actual_seq_lengths_kv=self.forward_metadata.seq_lens_cpu_list,\n                     workspace=workspace,\n-                    out=[output, softmax_lse],\n+                    out=[attn_output, softmax_lse],\n                 )\n             else:\n-                k_cache = forward_batch.token_to_kv_pool.get_key_buffer(layer.layer_id)\n-                v_cache = forward_batch.token_to_kv_pool.get_value_buffer(\n-                    layer.layer_id\n-                )\n+                if self.use_fia:\n+                    attn_output, _ = torch.ops.npu.npu_fused_infer_attention_score(\n+                        q.view(\n+                            forward_batch.batch_size,\n+                            -1,\n+                            layer.tp_q_head_num,\n+                            layer.qk_head_dim,\n+                        ),\n+                        k_cache.view(\n+                            -1, self.page_size, layer.tp_k_head_num * layer.qk_head_dim\n+                        ),\n+                        v_cache.view(\n+                            -1, self.page_size, layer.tp_v_head_num * layer.qk_head_dim\n+                        ),\n+                        num_heads=layer.tp_q_head_num,\n+                        num_key_value_heads=layer.tp_k_head_num,\n+                        input_layout=\"BSND\",\n+                        atten_mask=None,\n+                        block_size=self.page_size,\n+                        block_table=self.forward_metadata.block_tables,\n+                        actual_seq_lengths_kv=self.forward_metadata.seq_lens_cpu_int,\n+                        scale=layer.scaling,\n+                    )\n+                else:\n+                    query = q.view(-1, layer.tp_q_head_num, layer.qk_head_dim)\n+                    attn_output = torch.empty(\n+                        (num_tokens, layer.tp_q_head_num, layer.v_head_dim),\n+                        dtype=query.dtype,\n+                        device=query.device,\n+                    )\n \n-                query = q.view(-1, layer.tp_q_head_num, layer.qk_head_dim)\n-                num_tokens = query.shape[0]\n-                output = torch.empty(\n-                    (num_tokens, layer.tp_q_head_num, layer.v_head_dim),\n-                    dtype=query.dtype,\n-                    device=query.device,\n+                    torch_npu._npu_paged_attention(\n+                        query=query,\n+                        key_cache=k_cache,\n+                        value_cache=v_cache,\n+                        num_heads=layer.tp_q_head_num,\n+                        num_kv_heads=layer.tp_k_head_num,\n+                        scale_value=layer.scaling,\n+                        block_table=self.forward_metadata.block_tables,\n+                        context_lens=self.forward_metadata.seq_lens_cpu_int,\n+                        out=attn_output,\n+                    )\n+            return attn_output.view(num_tokens, layer.tp_q_head_num * layer.v_head_dim)\n+        else:\n+            if save_kv_cache:\n+                forward_batch.token_to_kv_pool.set_kv_buffer(\n+                    layer, forward_batch.out_cache_loc, k, k_rope\n                 )\n-\n-                torch_npu._npu_paged_attention(\n-                    query=query,\n-                    key_cache=k_cache,\n-                    value_cache=v_cache,\n+            num_tokens = q.shape[0]\n+            kv_c = forward_batch.token_to_kv_pool.get_key_buffer(layer.layer_id)\n+            k_pe = forward_batch.token_to_kv_pool.get_value_buffer(layer.layer_id)\n+\n+            if (self.graph_mode or self.use_fia) and (\n+                layer.tp_q_head_num // layer.tp_k_head_num\n+            ) >= 8:\n+                \"\"\"layer.tp_q_head_num // layer.tp_k_head_num < 8 will support in the later version of CANN\"\"\"\n+                kv_c = kv_c.view(\n+                    -1, self.page_size, layer.tp_k_head_num * self.kv_lora_rank\n+                )\n+                k_pe = k_pe.view(\n+                    -1, self.page_size, layer.tp_k_head_num * self.qk_rope_head_dim\n+                )\n+                q = q.view(\n+                    forward_batch.batch_size, -1, layer.tp_q_head_num, self.kv_lora_rank\n+                )\n+                q_rope = q_rope.view(\n+                    forward_batch.batch_size,\n+                    -1,\n+                    layer.tp_q_head_num,\n+                    self.qk_rope_head_dim,\n+                )\n+                attn_output, _ = torch.ops.npu.npu_fused_infer_attention_score(\n+                    q,\n+                    kv_c,\n+                    kv_c,\n+                    query_rope=q_rope,\n+                    key_rope=k_pe,\n                     num_heads=layer.tp_q_head_num,\n+                    num_key_value_heads=layer.tp_k_head_num,\n+                    input_layout=\"BSND\",\n+                    atten_mask=None,\n+                    sparse_mode=0,\n+                    scale=layer.scaling,\n+                    antiquant_mode=0,\n+                    antiquant_scale=None,\n+                    block_table=self.forward_metadata.block_tables,\n+                    block_size=self.page_size,\n+                    actual_seq_lengths_kv=self.forward_metadata.seq_lens_cpu_int,\n+                )\n+            else:\n+                assert (\n+                    get_bool_env_var(\"SGLANG_ENABLE_TORCH_COMPILE\") == False",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2282968046",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8328,
        "pr_file": "python/sglang/srt/layers/attention/ascend_backend.py",
        "discussion_id": "2282968046",
        "commented_code": "@@ -272,63 +313,127 @@ def forward_decode(\n                     scale=layer.scaling,\n                     actual_seq_lengths_kv=self.forward_metadata.seq_lens_cpu_list,\n                     workspace=workspace,\n-                    out=[output, softmax_lse],\n+                    out=[attn_output, softmax_lse],\n                 )\n             else:\n-                k_cache = forward_batch.token_to_kv_pool.get_key_buffer(layer.layer_id)\n-                v_cache = forward_batch.token_to_kv_pool.get_value_buffer(\n-                    layer.layer_id\n-                )\n+                if self.use_fia:\n+                    attn_output, _ = torch.ops.npu.npu_fused_infer_attention_score(\n+                        q.view(\n+                            forward_batch.batch_size,\n+                            -1,\n+                            layer.tp_q_head_num,\n+                            layer.qk_head_dim,\n+                        ),\n+                        k_cache.view(\n+                            -1, self.page_size, layer.tp_k_head_num * layer.qk_head_dim\n+                        ),\n+                        v_cache.view(\n+                            -1, self.page_size, layer.tp_v_head_num * layer.qk_head_dim\n+                        ),\n+                        num_heads=layer.tp_q_head_num,\n+                        num_key_value_heads=layer.tp_k_head_num,\n+                        input_layout=\"BSND\",\n+                        atten_mask=None,\n+                        block_size=self.page_size,\n+                        block_table=self.forward_metadata.block_tables,\n+                        actual_seq_lengths_kv=self.forward_metadata.seq_lens_cpu_int,\n+                        scale=layer.scaling,\n+                    )\n+                else:\n+                    query = q.view(-1, layer.tp_q_head_num, layer.qk_head_dim)\n+                    attn_output = torch.empty(\n+                        (num_tokens, layer.tp_q_head_num, layer.v_head_dim),\n+                        dtype=query.dtype,\n+                        device=query.device,\n+                    )\n \n-                query = q.view(-1, layer.tp_q_head_num, layer.qk_head_dim)\n-                num_tokens = query.shape[0]\n-                output = torch.empty(\n-                    (num_tokens, layer.tp_q_head_num, layer.v_head_dim),\n-                    dtype=query.dtype,\n-                    device=query.device,\n+                    torch_npu._npu_paged_attention(\n+                        query=query,\n+                        key_cache=k_cache,\n+                        value_cache=v_cache,\n+                        num_heads=layer.tp_q_head_num,\n+                        num_kv_heads=layer.tp_k_head_num,\n+                        scale_value=layer.scaling,\n+                        block_table=self.forward_metadata.block_tables,\n+                        context_lens=self.forward_metadata.seq_lens_cpu_int,\n+                        out=attn_output,\n+                    )\n+            return attn_output.view(num_tokens, layer.tp_q_head_num * layer.v_head_dim)\n+        else:\n+            if save_kv_cache:\n+                forward_batch.token_to_kv_pool.set_kv_buffer(\n+                    layer, forward_batch.out_cache_loc, k, k_rope\n                 )\n-\n-                torch_npu._npu_paged_attention(\n-                    query=query,\n-                    key_cache=k_cache,\n-                    value_cache=v_cache,\n+            num_tokens = q.shape[0]\n+            kv_c = forward_batch.token_to_kv_pool.get_key_buffer(layer.layer_id)\n+            k_pe = forward_batch.token_to_kv_pool.get_value_buffer(layer.layer_id)\n+\n+            if (self.graph_mode or self.use_fia) and (\n+                layer.tp_q_head_num // layer.tp_k_head_num\n+            ) >= 8:\n+                \"\"\"layer.tp_q_head_num // layer.tp_k_head_num < 8 will support in the later version of CANN\"\"\"\n+                kv_c = kv_c.view(\n+                    -1, self.page_size, layer.tp_k_head_num * self.kv_lora_rank\n+                )\n+                k_pe = k_pe.view(\n+                    -1, self.page_size, layer.tp_k_head_num * self.qk_rope_head_dim\n+                )\n+                q = q.view(\n+                    forward_batch.batch_size, -1, layer.tp_q_head_num, self.kv_lora_rank\n+                )\n+                q_rope = q_rope.view(\n+                    forward_batch.batch_size,\n+                    -1,\n+                    layer.tp_q_head_num,\n+                    self.qk_rope_head_dim,\n+                )\n+                attn_output, _ = torch.ops.npu.npu_fused_infer_attention_score(\n+                    q,\n+                    kv_c,\n+                    kv_c,\n+                    query_rope=q_rope,\n+                    key_rope=k_pe,\n                     num_heads=layer.tp_q_head_num,\n+                    num_key_value_heads=layer.tp_k_head_num,\n+                    input_layout=\"BSND\",\n+                    atten_mask=None,\n+                    sparse_mode=0,\n+                    scale=layer.scaling,\n+                    antiquant_mode=0,\n+                    antiquant_scale=None,\n+                    block_table=self.forward_metadata.block_tables,\n+                    block_size=self.page_size,\n+                    actual_seq_lengths_kv=self.forward_metadata.seq_lens_cpu_int,\n+                )\n+            else:\n+                assert (\n+                    get_bool_env_var(\"SGLANG_ENABLE_TORCH_COMPILE\") == False",
        "comment_created_at": "2025-08-18T17:04:22+00:00",
        "comment_author": "Alcanderian",
        "comment_body": "Suggestion: do not read env_var in forward phase.\r\nAnd do we mean\r\n`assert self.graph_mode and get_bool_env_var(\"SGLANG_ENABLE_TORCH_COMPILE\") == False` ?",
        "pr_file_module": null
      },
      {
        "comment_id": "2284166718",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8328,
        "pr_file": "python/sglang/srt/layers/attention/ascend_backend.py",
        "discussion_id": "2282968046",
        "commented_code": "@@ -272,63 +313,127 @@ def forward_decode(\n                     scale=layer.scaling,\n                     actual_seq_lengths_kv=self.forward_metadata.seq_lens_cpu_list,\n                     workspace=workspace,\n-                    out=[output, softmax_lse],\n+                    out=[attn_output, softmax_lse],\n                 )\n             else:\n-                k_cache = forward_batch.token_to_kv_pool.get_key_buffer(layer.layer_id)\n-                v_cache = forward_batch.token_to_kv_pool.get_value_buffer(\n-                    layer.layer_id\n-                )\n+                if self.use_fia:\n+                    attn_output, _ = torch.ops.npu.npu_fused_infer_attention_score(\n+                        q.view(\n+                            forward_batch.batch_size,\n+                            -1,\n+                            layer.tp_q_head_num,\n+                            layer.qk_head_dim,\n+                        ),\n+                        k_cache.view(\n+                            -1, self.page_size, layer.tp_k_head_num * layer.qk_head_dim\n+                        ),\n+                        v_cache.view(\n+                            -1, self.page_size, layer.tp_v_head_num * layer.qk_head_dim\n+                        ),\n+                        num_heads=layer.tp_q_head_num,\n+                        num_key_value_heads=layer.tp_k_head_num,\n+                        input_layout=\"BSND\",\n+                        atten_mask=None,\n+                        block_size=self.page_size,\n+                        block_table=self.forward_metadata.block_tables,\n+                        actual_seq_lengths_kv=self.forward_metadata.seq_lens_cpu_int,\n+                        scale=layer.scaling,\n+                    )\n+                else:\n+                    query = q.view(-1, layer.tp_q_head_num, layer.qk_head_dim)\n+                    attn_output = torch.empty(\n+                        (num_tokens, layer.tp_q_head_num, layer.v_head_dim),\n+                        dtype=query.dtype,\n+                        device=query.device,\n+                    )\n \n-                query = q.view(-1, layer.tp_q_head_num, layer.qk_head_dim)\n-                num_tokens = query.shape[0]\n-                output = torch.empty(\n-                    (num_tokens, layer.tp_q_head_num, layer.v_head_dim),\n-                    dtype=query.dtype,\n-                    device=query.device,\n+                    torch_npu._npu_paged_attention(\n+                        query=query,\n+                        key_cache=k_cache,\n+                        value_cache=v_cache,\n+                        num_heads=layer.tp_q_head_num,\n+                        num_kv_heads=layer.tp_k_head_num,\n+                        scale_value=layer.scaling,\n+                        block_table=self.forward_metadata.block_tables,\n+                        context_lens=self.forward_metadata.seq_lens_cpu_int,\n+                        out=attn_output,\n+                    )\n+            return attn_output.view(num_tokens, layer.tp_q_head_num * layer.v_head_dim)\n+        else:\n+            if save_kv_cache:\n+                forward_batch.token_to_kv_pool.set_kv_buffer(\n+                    layer, forward_batch.out_cache_loc, k, k_rope\n                 )\n-\n-                torch_npu._npu_paged_attention(\n-                    query=query,\n-                    key_cache=k_cache,\n-                    value_cache=v_cache,\n+            num_tokens = q.shape[0]\n+            kv_c = forward_batch.token_to_kv_pool.get_key_buffer(layer.layer_id)\n+            k_pe = forward_batch.token_to_kv_pool.get_value_buffer(layer.layer_id)\n+\n+            if (self.graph_mode or self.use_fia) and (\n+                layer.tp_q_head_num // layer.tp_k_head_num\n+            ) >= 8:\n+                \"\"\"layer.tp_q_head_num // layer.tp_k_head_num < 8 will support in the later version of CANN\"\"\"\n+                kv_c = kv_c.view(\n+                    -1, self.page_size, layer.tp_k_head_num * self.kv_lora_rank\n+                )\n+                k_pe = k_pe.view(\n+                    -1, self.page_size, layer.tp_k_head_num * self.qk_rope_head_dim\n+                )\n+                q = q.view(\n+                    forward_batch.batch_size, -1, layer.tp_q_head_num, self.kv_lora_rank\n+                )\n+                q_rope = q_rope.view(\n+                    forward_batch.batch_size,\n+                    -1,\n+                    layer.tp_q_head_num,\n+                    self.qk_rope_head_dim,\n+                )\n+                attn_output, _ = torch.ops.npu.npu_fused_infer_attention_score(\n+                    q,\n+                    kv_c,\n+                    kv_c,\n+                    query_rope=q_rope,\n+                    key_rope=k_pe,\n                     num_heads=layer.tp_q_head_num,\n+                    num_key_value_heads=layer.tp_k_head_num,\n+                    input_layout=\"BSND\",\n+                    atten_mask=None,\n+                    sparse_mode=0,\n+                    scale=layer.scaling,\n+                    antiquant_mode=0,\n+                    antiquant_scale=None,\n+                    block_table=self.forward_metadata.block_tables,\n+                    block_size=self.page_size,\n+                    actual_seq_lengths_kv=self.forward_metadata.seq_lens_cpu_int,\n+                )\n+            else:\n+                assert (\n+                    get_bool_env_var(\"SGLANG_ENABLE_TORCH_COMPILE\") == False",
        "comment_created_at": "2025-08-19T06:08:25+00:00",
        "comment_author": "ZhengdQin",
        "comment_body": "Thanks, we use assert self.graph_mode == False directly.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2228338447",
    "pr_number": 8247,
    "pr_file": "python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py",
    "created_at": "2025-07-24T12:11:11+00:00",
    "commented_code": "return hidden_states, topk_idx, topk_weights, previous_event\n \n     def dispatch_b(self, hidden_states, topk_idx, topk_weights, previous_event):\n-        if deep_gemm_wrapper.ENABLE_JIT_DEEPGEMM:\n+        if os.getenv(\"USE_W4A8\") != \"1\" and deep_gemm_wrapper.ENABLE_JIT_DEEPGEMM:",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2228338447",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8247,
        "pr_file": "python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py",
        "discussion_id": "2228338447",
        "commented_code": "@@ -255,7 +258,7 @@ def dispatch_a(\n         return hidden_states, topk_idx, topk_weights, previous_event\n \n     def dispatch_b(self, hidden_states, topk_idx, topk_weights, previous_event):\n-        if deep_gemm_wrapper.ENABLE_JIT_DEEPGEMM:\n+        if os.getenv(\"USE_W4A8\") != \"1\" and deep_gemm_wrapper.ENABLE_JIT_DEEPGEMM:",
        "comment_created_at": "2025-07-24T12:11:11+00:00",
        "comment_author": "fzyzcjy",
        "comment_body": "nit: `get_bool_env_var(\"SGLANG_USE_W4A8\")` or sth like that",
        "pr_file_module": null
      },
      {
        "comment_id": "2229960034",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8247,
        "pr_file": "python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py",
        "discussion_id": "2228338447",
        "commented_code": "@@ -255,7 +258,7 @@ def dispatch_a(\n         return hidden_states, topk_idx, topk_weights, previous_event\n \n     def dispatch_b(self, hidden_states, topk_idx, topk_weights, previous_event):\n-        if deep_gemm_wrapper.ENABLE_JIT_DEEPGEMM:\n+        if os.getenv(\"USE_W4A8\") != \"1\" and deep_gemm_wrapper.ENABLE_JIT_DEEPGEMM:",
        "comment_created_at": "2025-07-25T01:51:59+00:00",
        "comment_author": "ayrnb",
        "comment_body": "thx, done.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2186643523",
    "pr_number": 7369,
    "pr_file": "python/sglang/srt/managers/scheduler.py",
    "created_at": "2025-07-05T04:42:09+00:00",
    "commented_code": "page_size=self.page_size,\n             )\n         else:\n-            if self.enable_hierarchical_cache:\n+            if os.environ.get(\"SGLANG_EXPERIMENTAL_HIRADIX_CACHE\") == \"1\":",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2186643523",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7369,
        "pr_file": "python/sglang/srt/managers/scheduler.py",
        "discussion_id": "2186643523",
        "commented_code": "@@ -577,7 +578,20 @@ def init_memory_pool_and_cache(self):\n                 page_size=self.page_size,\n             )\n         else:\n-            if self.enable_hierarchical_cache:\n+            if os.environ.get(\"SGLANG_EXPERIMENTAL_HIRADIX_CACHE\") == \"1\":",
        "comment_created_at": "2025-07-05T04:42:09+00:00",
        "comment_author": "merrymercy",
        "comment_body": "```suggestion\r\n            if os.environ.get(\"SGLANG_EXPERIMENTAL_CPP_RADIX_TREE\") == \"1\":\r\n```",
        "pr_file_module": null
      }
    ]
  }
]