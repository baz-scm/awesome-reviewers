[
  {
    "discussion_id": "2258445953",
    "pr_number": 7667,
    "pr_file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "created_at": "2025-08-06T22:29:11+00:00",
    "commented_code": "f\"Unsupported weight_name {weight_name} for FusedMoE weight_loader_fused. Nothing is loaded.\"\n             )\n \n-    def forward(self, hidden_states: torch.Tensor, topk_output: StandardTopKOutput):\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        topk_output: StandardTopKOutput,\n+        forward_batch: Optional[ForwardBatch] = None,",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2258445953",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7667,
        "pr_file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
        "discussion_id": "2258445953",
        "commented_code": "@@ -783,7 +784,12 @@ def weight_loader_fused(\n                 f\"Unsupported weight_name {weight_name} for FusedMoE weight_loader_fused. Nothing is loaded.\"\n             )\n \n-    def forward(self, hidden_states: torch.Tensor, topk_output: StandardTopKOutput):\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        topk_output: StandardTopKOutput,\n+        forward_batch: Optional[ForwardBatch] = None,",
        "comment_created_at": "2025-08-06T22:29:11+00:00",
        "comment_author": "merrymercy",
        "comment_body": "What are the needed fields? Passing the whole `forward_batch` makes this function kind of opaque.\r\nWe want the function to be more explicit about the argument.",
        "pr_file_module": null
      },
      {
        "comment_id": "2258487415",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7667,
        "pr_file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
        "discussion_id": "2258445953",
        "commented_code": "@@ -783,7 +784,12 @@ def weight_loader_fused(\n                 f\"Unsupported weight_name {weight_name} for FusedMoE weight_loader_fused. Nothing is loaded.\"\n             )\n \n-    def forward(self, hidden_states: torch.Tensor, topk_output: StandardTopKOutput):\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        topk_output: StandardTopKOutput,\n+        forward_batch: Optional[ForwardBatch] = None,",
        "comment_created_at": "2025-08-06T23:03:45+00:00",
        "comment_author": "trevor-m",
        "comment_body": "Hi @merrymercy thank you for reviewing. In modelopt_quant.py, we use `forward_batch.dp_padding_mode.is_max_len()`, `forward_batch.input_ids.shape[0]`, `forward_batch.gathered_buffer`, `forward_batch.global_num_tokens_cpu`.\r\n\r\nOther backends like deepep which also integrate communication pass the whole forward_batch to MOE also: https://github.com/sgl-project/sglang/blob/cbbd685a46cd9345c0a87f243292c3ea59e5db32/python/sglang/srt/models/deepseek_v2.py#L611",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2271981647",
    "pr_number": 7667,
    "pr_file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "created_at": "2025-08-13T03:30:01+00:00",
    "commented_code": "ep_size: Optional[int] = None,\n         tp_rank: Optional[int] = None,\n         tp_size: Optional[int] = None,\n+        forward_batch: Optional[ForwardBatch] = None,",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2271981647",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7667,
        "pr_file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
        "discussion_id": "2271981647",
        "commented_code": "@@ -1151,6 +1155,7 @@ def apply(\n         ep_size: Optional[int] = None,\n         tp_rank: Optional[int] = None,\n         tp_size: Optional[int] = None,\n+        forward_batch: Optional[ForwardBatch] = None,",
        "comment_created_at": "2025-08-13T03:30:01+00:00",
        "comment_author": "merrymercy",
        "comment_body": "we should explicitly list all the required arguments instead of passing a big `forward_batch`.\r\nPassing a big `forward_batch` makes the input of this function very opaque.  ",
        "pr_file_module": null
      },
      {
        "comment_id": "2274390038",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7667,
        "pr_file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
        "discussion_id": "2271981647",
        "commented_code": "@@ -1151,6 +1155,7 @@ def apply(\n         ep_size: Optional[int] = None,\n         tp_rank: Optional[int] = None,\n         tp_size: Optional[int] = None,\n+        forward_batch: Optional[ForwardBatch] = None,",
        "comment_created_at": "2025-08-13T19:16:57+00:00",
        "comment_author": "trevor-m",
        "comment_body": "Thanks, done.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2265095528",
    "pr_number": 9014,
    "pr_file": "python/sglang/srt/layers/rotary_embedding.py",
    "created_at": "2025-08-10T03:19:44+00:00",
    "commented_code": "query: torch.Tensor,\n         key: torch.Tensor,\n         offsets: Optional[torch.Tensor] = None,\n+        layer: Any = None,  # RadixAttention\n+        forward_batch=None,\n+        save_kv_cache: bool = False,\n+        value: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n         if _is_cuda and (self.head_size in [64, 128, 256, 512]):\n+            from sglang.srt.model_executor.cuda_graph_runner import get_is_capture_mode\n+\n+            # import importlib.util\n+            # spec = importlib.util.spec_from_file_location(\"apply_rope_with_cos_sin_cache_inplace\", \"/sgl-workspace/sglang/sgl-kernel/python/sgl_kernel/elementwise.py\")\n+            # elementwise = importlib.util.module_from_spec(spec)\n+            # spec.loader.exec_module(elementwise)\n+            # elementwise.apply_rope_with_cos_sin_cache_inplace(\n             apply_rope_with_cos_sin_cache_inplace(\n                 positions=positions,\n                 query=query,\n                 key=key,\n                 head_size=self.head_size,\n                 cos_sin_cache=self.cos_sin_cache,\n                 is_neox=self.is_neox_style,\n+                layer=layer,\n+                forward_batch=forward_batch,",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2265095528",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9014,
        "pr_file": "python/sglang/srt/layers/rotary_embedding.py",
        "discussion_id": "2265095528",
        "commented_code": "@@ -222,17 +222,36 @@ def forward_cuda(\n         query: torch.Tensor,\n         key: torch.Tensor,\n         offsets: Optional[torch.Tensor] = None,\n+        layer: Any = None,  # RadixAttention\n+        forward_batch=None,\n+        save_kv_cache: bool = False,\n+        value: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n         if _is_cuda and (self.head_size in [64, 128, 256, 512]):\n+            from sglang.srt.model_executor.cuda_graph_runner import get_is_capture_mode\n+\n+            # import importlib.util\n+            # spec = importlib.util.spec_from_file_location(\"apply_rope_with_cos_sin_cache_inplace\", \"/sgl-workspace/sglang/sgl-kernel/python/sgl_kernel/elementwise.py\")\n+            # elementwise = importlib.util.module_from_spec(spec)\n+            # spec.loader.exec_module(elementwise)\n+            # elementwise.apply_rope_with_cos_sin_cache_inplace(\n             apply_rope_with_cos_sin_cache_inplace(\n                 positions=positions,\n                 query=query,\n                 key=key,\n                 head_size=self.head_size,\n                 cos_sin_cache=self.cos_sin_cache,\n                 is_neox=self.is_neox_style,\n+                layer=layer,\n+                forward_batch=forward_batch,",
        "comment_created_at": "2025-08-10T03:19:44+00:00",
        "comment_author": "fzyzcjy",
        "comment_body": "maybe we should not pass such objects to this API\n\nwhat about e.g.\n\n```\ndef apply_rope_with_cos_sin_cache_inplace(\n  ...,\n  # in non-fused version we do `k_buffer[loc] = data` etc\n  k_buffer: Tensor, v_buffer: Tensor, loc: Tensor,\n)\n```\n\nand if none, it means we do not save kv cache; if non-none then we need to save",
        "pr_file_module": null
      }
    ]
  }
]