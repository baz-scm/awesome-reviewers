[
  {
    "discussion_id": "2279426968",
    "pr_number": 8909,
    "pr_file": "python/sglang/srt/layers/rotary_embedding.py",
    "created_at": "2025-08-15T16:24:17+00:00",
    "commented_code": "sin: torch.Tensor,\n     unsqueeze_dim=1,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n-    orig_q_dtype = q.dtype\n-    orig_k_dtype = k.dtype\n-    q, k = q.float(), k.float()\n+    if not _is_npu:",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2279426968",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8909,
        "pr_file": "python/sglang/srt/layers/rotary_embedding.py",
        "discussion_id": "2279426968",
        "commented_code": "@@ -1882,20 +1882,32 @@ def apply_rotary_pos_emb(\n     sin: torch.Tensor,\n     unsqueeze_dim=1,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n-    orig_q_dtype = q.dtype\n-    orig_k_dtype = k.dtype\n-    q, k = q.float(), k.float()\n+    if not _is_npu:",
        "comment_created_at": "2025-08-15T16:24:17+00:00",
        "comment_author": "Alcanderian",
        "comment_body": "suggestion\r\n```\r\ndef apply_rotary_pos_emb():\r\n    ...\r\n\r\ndef apply_rotary_pos_emb_npu():\r\n    ...\r\n\r\nif _is_npu:\r\n    apply_rotary_pos_emb = apply_rotary_pos_emb_npu\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2285406419",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8909,
        "pr_file": "python/sglang/srt/layers/rotary_embedding.py",
        "discussion_id": "2279426968",
        "commented_code": "@@ -1882,20 +1882,32 @@ def apply_rotary_pos_emb(\n     sin: torch.Tensor,\n     unsqueeze_dim=1,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n-    orig_q_dtype = q.dtype\n-    orig_k_dtype = k.dtype\n-    q, k = q.float(), k.float()\n+    if not _is_npu:",
        "comment_created_at": "2025-08-19T14:11:11+00:00",
        "comment_author": "VDV1985",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2250791169",
    "pr_number": 8225,
    "pr_file": "python/sglang/srt/configs/update_config.py",
    "created_at": "2025-08-04T08:31:22+00:00",
    "commented_code": "if quant_config is not None and hasattr(quant_config, \"weight_block_size\"):\n         return getattr(quant_config, \"weight_block_size\")\n+\n+    if quant_config is not None and hasattr(quant_config, \"group_size\"):\n+        return [getattr(quant_config, \"group_size\")]\n+\n     return None\n \n \n def get_moe_padding_size(weight_block_size):\n     if weight_block_size is not None:\n         # See NOTE(HandH1998): To ensure proper alignment of the block-wise quantization scales, the output_size of the weights for both the gate and up layers must be divisible by block_n.\n         assert (\n-            len(weight_block_size) == 2\n-        ), \"Only len(weight_block_size) == 2 is supported\"\n-        assert (\n-            weight_block_size[0] == weight_block_size[1]\n-        ), \"Only weight_block_size[0] == weight_block_size[1] is supported\"\n-\n+            len(weight_block_size) == 2 or len(weight_block_size) == 1",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2250791169",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8225,
        "pr_file": "python/sglang/srt/configs/update_config.py",
        "discussion_id": "2250791169",
        "commented_code": "@@ -23,19 +23,23 @@ def may_get_weight_block_size(model_config, load_config):\n \n     if quant_config is not None and hasattr(quant_config, \"weight_block_size\"):\n         return getattr(quant_config, \"weight_block_size\")\n+\n+    if quant_config is not None and hasattr(quant_config, \"group_size\"):\n+        return [getattr(quant_config, \"group_size\")]\n+\n     return None\n \n \n def get_moe_padding_size(weight_block_size):\n     if weight_block_size is not None:\n         # See NOTE(HandH1998): To ensure proper alignment of the block-wise quantization scales, the output_size of the weights for both the gate and up layers must be divisible by block_n.\n         assert (\n-            len(weight_block_size) == 2\n-        ), \"Only len(weight_block_size) == 2 is supported\"\n-        assert (\n-            weight_block_size[0] == weight_block_size[1]\n-        ), \"Only weight_block_size[0] == weight_block_size[1] is supported\"\n-\n+            len(weight_block_size) == 2 or len(weight_block_size) == 1",
        "comment_created_at": "2025-08-04T08:31:22+00:00",
        "comment_author": "mingfeima",
        "comment_body": "```suggestion\r\n            len(weight_block_size) in [1, 2]\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2288387974",
    "pr_number": 8464,
    "pr_file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "created_at": "2025-08-20T14:35:30+00:00",
    "commented_code": "hidden_states, masked_m, event, hook = self._dispatch_core(\n             hidden_states,\n             topk_idx,\n-            use_fp8=True,\n+            use_fp8=False if get_bool_env_var(\"SGLANG_USE_W4A8\") else True,",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2288387974",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8464,
        "pr_file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
        "discussion_id": "2288387974",
        "commented_code": "@@ -479,7 +480,7 @@ def dispatch_a(\n         hidden_states, masked_m, event, hook = self._dispatch_core(\n             hidden_states,\n             topk_idx,\n-            use_fp8=True,\n+            use_fp8=False if get_bool_env_var(\"SGLANG_USE_W4A8\") else True,",
        "comment_created_at": "2025-08-20T14:35:30+00:00",
        "comment_author": "fzyzcjy",
        "comment_body": "```suggestion\n            use_fp8=not get_bool_env_var(\"SGLANG_USE_W4A8\"),\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2298053429",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8464,
        "pr_file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
        "discussion_id": "2288387974",
        "commented_code": "@@ -479,7 +480,7 @@ def dispatch_a(\n         hidden_states, masked_m, event, hook = self._dispatch_core(\n             hidden_states,\n             topk_idx,\n-            use_fp8=True,\n+            use_fp8=False if get_bool_env_var(\"SGLANG_USE_W4A8\") else True,",
        "comment_created_at": "2025-08-25T13:05:01+00:00",
        "comment_author": "ayrnb",
        "comment_body": "Done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2275551349",
    "pr_number": 7843,
    "pr_file": "python/sglang/srt/managers/scheduler_metrics_mixin.py",
    "created_at": "2025-08-14T05:49:36+00:00",
    "commented_code": "msg += f\"pre-allocated usage: {self.disagg_decode_prealloc_queue.num_tokens_pre_allocated / self.max_total_num_tokens:.2f}, \"\n             msg += f\"#retracted-req: {len(self.disagg_decode_prealloc_queue.retracted_queue)}, \"\n \n+        graph_msg = f\"cpu torch-compile\" if self.device == \"cpu\" else \"cuda graph\"\n         msg += (\n-            f\"cuda graph: {can_run_cuda_graph}, \"\n+            f\"{graph_msg}: {can_run_graph}, \"",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2275551349",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7843,
        "pr_file": "python/sglang/srt/managers/scheduler_metrics_mixin.py",
        "discussion_id": "2275551349",
        "commented_code": "@@ -184,8 +184,9 @@ def log_decode_stats(\n             msg += f\"pre-allocated usage: {self.disagg_decode_prealloc_queue.num_tokens_pre_allocated / self.max_total_num_tokens:.2f}, \"\n             msg += f\"#retracted-req: {len(self.disagg_decode_prealloc_queue.retracted_queue)}, \"\n \n+        graph_msg = f\"cpu torch-compile\" if self.device == \"cpu\" else \"cuda graph\"\n         msg += (\n-            f\"cuda graph: {can_run_cuda_graph}, \"\n+            f\"{graph_msg}: {can_run_graph}, \"",
        "comment_created_at": "2025-08-14T05:49:36+00:00",
        "comment_author": "mingfeima",
        "comment_body": "```suggestion\r\n f\"{'cpu torch-compile' if self.device == 'cpu' else 'cuda graph'}: {can_run_graph}, \"\r\n```\r\nor we just print `sefl.device` + graph: use `cpu graph` or `cuda graph` for the naming, skip torch compile.",
        "pr_file_module": null
      },
      {
        "comment_id": "2275618775",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7843,
        "pr_file": "python/sglang/srt/managers/scheduler_metrics_mixin.py",
        "discussion_id": "2275551349",
        "commented_code": "@@ -184,8 +184,9 @@ def log_decode_stats(\n             msg += f\"pre-allocated usage: {self.disagg_decode_prealloc_queue.num_tokens_pre_allocated / self.max_total_num_tokens:.2f}, \"\n             msg += f\"#retracted-req: {len(self.disagg_decode_prealloc_queue.retracted_queue)}, \"\n \n+        graph_msg = f\"cpu torch-compile\" if self.device == \"cpu\" else \"cuda graph\"\n         msg += (\n-            f\"cuda graph: {can_run_cuda_graph}, \"\n+            f\"{graph_msg}: {can_run_graph}, \"",
        "comment_created_at": "2025-08-14T06:34:46+00:00",
        "comment_author": "CaoE",
        "comment_body": "Use `f\"{'cpu graph' if self.device == 'cpu' else 'cuda graph'}: {can_run_graph}, \"` instead.",
        "pr_file_module": null
      },
      {
        "comment_id": "2275635776",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7843,
        "pr_file": "python/sglang/srt/managers/scheduler_metrics_mixin.py",
        "discussion_id": "2275551349",
        "commented_code": "@@ -184,8 +184,9 @@ def log_decode_stats(\n             msg += f\"pre-allocated usage: {self.disagg_decode_prealloc_queue.num_tokens_pre_allocated / self.max_total_num_tokens:.2f}, \"\n             msg += f\"#retracted-req: {len(self.disagg_decode_prealloc_queue.retracted_queue)}, \"\n \n+        graph_msg = f\"cpu torch-compile\" if self.device == \"cpu\" else \"cuda graph\"\n         msg += (\n-            f\"cuda graph: {can_run_cuda_graph}, \"\n+            f\"{graph_msg}: {can_run_graph}, \"",
        "comment_created_at": "2025-08-14T06:44:52+00:00",
        "comment_author": "mingfeima",
        "comment_body": "f\"{self.device} graph: {can_run_graph}, \"",
        "pr_file_module": null
      },
      {
        "comment_id": "2275947802",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7843,
        "pr_file": "python/sglang/srt/managers/scheduler_metrics_mixin.py",
        "discussion_id": "2275551349",
        "commented_code": "@@ -184,8 +184,9 @@ def log_decode_stats(\n             msg += f\"pre-allocated usage: {self.disagg_decode_prealloc_queue.num_tokens_pre_allocated / self.max_total_num_tokens:.2f}, \"\n             msg += f\"#retracted-req: {len(self.disagg_decode_prealloc_queue.retracted_queue)}, \"\n \n+        graph_msg = f\"cpu torch-compile\" if self.device == \"cpu\" else \"cuda graph\"\n         msg += (\n-            f\"cuda graph: {can_run_cuda_graph}, \"\n+            f\"{graph_msg}: {can_run_graph}, \"",
        "comment_created_at": "2025-08-14T08:49:54+00:00",
        "comment_author": "CaoE",
        "comment_body": "Use `cpu graph of torch.compile` to distinguish it from `cuda graph`.  Unlike cuda graph, cpu graph depends on torch compile, while torch compile is an option on cuda graph.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2220570473",
    "pr_number": 8222,
    "pr_file": "python/sglang/srt/lora/layers.py",
    "created_at": "2025-07-21T23:29:59+00:00",
    "commented_code": ")\n from sglang.srt.layers.vocab_parallel_embedding import VocabParallelEmbedding\n from sglang.srt.lora.backend.base_backend import BaseLoRABackend\n+from sglang.srt.lora.utils import LoRABatchInfo\n \n \n class BaseLayerWithLoRA(nn.Module):",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2220570473",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8222,
        "pr_file": "python/sglang/srt/lora/layers.py",
        "discussion_id": "2220570473",
        "commented_code": "@@ -17,6 +19,7 @@\n )\n from sglang.srt.layers.vocab_parallel_embedding import VocabParallelEmbedding\n from sglang.srt.lora.backend.base_backend import BaseLoRABackend\n+from sglang.srt.lora.utils import LoRABatchInfo\n \n \n class BaseLayerWithLoRA(nn.Module):",
        "comment_created_at": "2025-07-21T23:29:59+00:00",
        "comment_author": "lifuhuang",
        "comment_body": "Can you keep the base class clean and add a subclass specifically for embedding with LoRA?",
        "pr_file_module": null
      },
      {
        "comment_id": "2221894010",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8222,
        "pr_file": "python/sglang/srt/lora/layers.py",
        "discussion_id": "2220570473",
        "commented_code": "@@ -17,6 +19,7 @@\n )\n from sglang.srt.layers.vocab_parallel_embedding import VocabParallelEmbedding\n from sglang.srt.lora.backend.base_backend import BaseLoRABackend\n+from sglang.srt.lora.utils import LoRABatchInfo\n \n \n class BaseLayerWithLoRA(nn.Module):",
        "comment_created_at": "2025-07-22T09:31:05+00:00",
        "comment_author": "Beichen-Ma",
        "comment_body": "Thanks for your time and comments. Sure, I added a base class (e.g. EmbeddingLayerWithLoRA) for embedding layers to ensure the clean separation.",
        "pr_file_module": null
      },
      {
        "comment_id": "2243848861",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8222,
        "pr_file": "python/sglang/srt/lora/layers.py",
        "discussion_id": "2220570473",
        "commented_code": "@@ -17,6 +19,7 @@\n )\n from sglang.srt.layers.vocab_parallel_embedding import VocabParallelEmbedding\n from sglang.srt.lora.backend.base_backend import BaseLoRABackend\n+from sglang.srt.lora.utils import LoRABatchInfo\n \n \n class BaseLayerWithLoRA(nn.Module):",
        "comment_created_at": "2025-07-30T21:00:59+00:00",
        "comment_author": "lifuhuang",
        "comment_body": "Synced offline. I misread the code due to the github diff tools: your initial imlementation was correct. Sorry for the trouble!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2280028868",
    "pr_number": 9200,
    "pr_file": "sgl-kernel/python/sgl_kernel/gemm.py",
    "created_at": "2025-08-15T23:37:10+00:00",
    "commented_code": "return output_tensor\n \n \n+def scaled_fp4_grouped_quant(\n+    input_tensor: torch.Tensor,\n+    input_global_scale: torch.Tensor,\n+):\n+    \"\"\"\n+    Quantize input tensor to FP4 and return quantized tensor and scale, for\n+    grouped gemm inputs (e.g., grouped_gemm_nt_masked for flashinfer).\n+    Args:\n+        input: The input tensor to be quantized to FP4, with shape (l, m, k)\n+            l is number of groups, m is number of tokens per group, k is number of features.\n+        input_global_scale: A scalar scaling factor for the entire tensor, with\n+            shape (l,).\n+    Outputs:\n+        output: The quantized tensor in FP4, with shape (m, k // 2, l) but the physical\n+            layout is (l, m, k // 2). `// 2` is because two fp4 values are packed into\n+            an uint8.\n+        output_scales: The blockscale tensor in FP8-E4M3, with shape (32, 4, rm, 4, rk, l)\n+            but the physical layout is (l, rm, rk, 32, 4, 4).\n+    \"\"\"\n+    device = input_tensor.device\n+    l, m, k = input_tensor.shape\n+    sf_vec_size = 16\n+    assert k % sf_vec_size == 0, f\"k must be multiple of 16, but got {k}.\"\n+\n+    scale_k = k // sf_vec_size\n+    padded_k = (scale_k + (4 - 1)) // 4 * 4",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2280028868",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9200,
        "pr_file": "sgl-kernel/python/sgl_kernel/gemm.py",
        "discussion_id": "2280028868",
        "commented_code": "@@ -289,6 +289,68 @@ def shuffle_rows(input_tensor, dst2src_map, output_tensor_shape):\n     return output_tensor\n \n \n+def scaled_fp4_grouped_quant(\n+    input_tensor: torch.Tensor,\n+    input_global_scale: torch.Tensor,\n+):\n+    \"\"\"\n+    Quantize input tensor to FP4 and return quantized tensor and scale, for\n+    grouped gemm inputs (e.g., grouped_gemm_nt_masked for flashinfer).\n+    Args:\n+        input: The input tensor to be quantized to FP4, with shape (l, m, k)\n+            l is number of groups, m is number of tokens per group, k is number of features.\n+        input_global_scale: A scalar scaling factor for the entire tensor, with\n+            shape (l,).\n+    Outputs:\n+        output: The quantized tensor in FP4, with shape (m, k // 2, l) but the physical\n+            layout is (l, m, k // 2). `// 2` is because two fp4 values are packed into\n+            an uint8.\n+        output_scales: The blockscale tensor in FP8-E4M3, with shape (32, 4, rm, 4, rk, l)\n+            but the physical layout is (l, rm, rk, 32, 4, 4).\n+    \"\"\"\n+    device = input_tensor.device\n+    l, m, k = input_tensor.shape\n+    sf_vec_size = 16\n+    assert k % sf_vec_size == 0, f\"k must be multiple of 16, but got {k}.\"\n+\n+    scale_k = k // sf_vec_size\n+    padded_k = (scale_k + (4 - 1)) // 4 * 4",
        "comment_created_at": "2025-08-15T23:37:10+00:00",
        "comment_author": "elfiegg",
        "comment_body": "can you do me a favor to name 4 and 128 to a tile size constant? \r\nthe reason is trtllm-gen supports other tile sizes like 8x4, I was thinking about extending quantization API at some point and found it could be easier if all magic numbers are named",
        "pr_file_module": null
      },
      {
        "comment_id": "2283131964",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9200,
        "pr_file": "sgl-kernel/python/sgl_kernel/gemm.py",
        "discussion_id": "2280028868",
        "commented_code": "@@ -289,6 +289,68 @@ def shuffle_rows(input_tensor, dst2src_map, output_tensor_shape):\n     return output_tensor\n \n \n+def scaled_fp4_grouped_quant(\n+    input_tensor: torch.Tensor,\n+    input_global_scale: torch.Tensor,\n+):\n+    \"\"\"\n+    Quantize input tensor to FP4 and return quantized tensor and scale, for\n+    grouped gemm inputs (e.g., grouped_gemm_nt_masked for flashinfer).\n+    Args:\n+        input: The input tensor to be quantized to FP4, with shape (l, m, k)\n+            l is number of groups, m is number of tokens per group, k is number of features.\n+        input_global_scale: A scalar scaling factor for the entire tensor, with\n+            shape (l,).\n+    Outputs:\n+        output: The quantized tensor in FP4, with shape (m, k // 2, l) but the physical\n+            layout is (l, m, k // 2). `// 2` is because two fp4 values are packed into\n+            an uint8.\n+        output_scales: The blockscale tensor in FP8-E4M3, with shape (32, 4, rm, 4, rk, l)\n+            but the physical layout is (l, rm, rk, 32, 4, 4).\n+    \"\"\"\n+    device = input_tensor.device\n+    l, m, k = input_tensor.shape\n+    sf_vec_size = 16\n+    assert k % sf_vec_size == 0, f\"k must be multiple of 16, but got {k}.\"\n+\n+    scale_k = k // sf_vec_size\n+    padded_k = (scale_k + (4 - 1)) // 4 * 4",
        "comment_created_at": "2025-08-18T18:22:41+00:00",
        "comment_author": "kaixih",
        "comment_body": "I added a line to clarify what these constants mean. I\u2019m a bit reluctant to call them tile sizes, since I think they differ from the tile sizes you mentioned. The 128\u00d74 here is more related to the MMA instruction requirements of the Blackwell tcgen05 MMA instructions.",
        "pr_file_module": null
      },
      {
        "comment_id": "2283680610",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9200,
        "pr_file": "sgl-kernel/python/sgl_kernel/gemm.py",
        "discussion_id": "2280028868",
        "commented_code": "@@ -289,6 +289,68 @@ def shuffle_rows(input_tensor, dst2src_map, output_tensor_shape):\n     return output_tensor\n \n \n+def scaled_fp4_grouped_quant(\n+    input_tensor: torch.Tensor,\n+    input_global_scale: torch.Tensor,\n+):\n+    \"\"\"\n+    Quantize input tensor to FP4 and return quantized tensor and scale, for\n+    grouped gemm inputs (e.g., grouped_gemm_nt_masked for flashinfer).\n+    Args:\n+        input: The input tensor to be quantized to FP4, with shape (l, m, k)\n+            l is number of groups, m is number of tokens per group, k is number of features.\n+        input_global_scale: A scalar scaling factor for the entire tensor, with\n+            shape (l,).\n+    Outputs:\n+        output: The quantized tensor in FP4, with shape (m, k // 2, l) but the physical\n+            layout is (l, m, k // 2). `// 2` is because two fp4 values are packed into\n+            an uint8.\n+        output_scales: The blockscale tensor in FP8-E4M3, with shape (32, 4, rm, 4, rk, l)\n+            but the physical layout is (l, rm, rk, 32, 4, 4).\n+    \"\"\"\n+    device = input_tensor.device\n+    l, m, k = input_tensor.shape\n+    sf_vec_size = 16\n+    assert k % sf_vec_size == 0, f\"k must be multiple of 16, but got {k}.\"\n+\n+    scale_k = k // sf_vec_size\n+    padded_k = (scale_k + (4 - 1)) // 4 * 4",
        "comment_created_at": "2025-08-18T23:10:51+00:00",
        "comment_author": "elfiegg",
        "comment_body": "FYI\r\n128x4 quant in Flashinfer : https://github.com/flashinfer-ai/flashinfer/blob/main/csrc/nv_internal/tensorrt_llm/kernels/quantization.cuh#L615\r\nvs. \r\n8x4 quant in Flashinfer: https://github.com/flashinfer-ai/flashinfer/blob/main/csrc/nv_internal/tensorrt_llm/kernels/quantization.cuh#L657\r\nboth for trtllm fp4 gemm and bmm: https://github.com/flashinfer-ai/flashinfer/blob/main/csrc/nv_internal/tensorrt_llm/kernels/quantization.cuh#L710-L713",
        "pr_file_module": null
      },
      {
        "comment_id": "2283743588",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9200,
        "pr_file": "sgl-kernel/python/sgl_kernel/gemm.py",
        "discussion_id": "2280028868",
        "commented_code": "@@ -289,6 +289,68 @@ def shuffle_rows(input_tensor, dst2src_map, output_tensor_shape):\n     return output_tensor\n \n \n+def scaled_fp4_grouped_quant(\n+    input_tensor: torch.Tensor,\n+    input_global_scale: torch.Tensor,\n+):\n+    \"\"\"\n+    Quantize input tensor to FP4 and return quantized tensor and scale, for\n+    grouped gemm inputs (e.g., grouped_gemm_nt_masked for flashinfer).\n+    Args:\n+        input: The input tensor to be quantized to FP4, with shape (l, m, k)\n+            l is number of groups, m is number of tokens per group, k is number of features.\n+        input_global_scale: A scalar scaling factor for the entire tensor, with\n+            shape (l,).\n+    Outputs:\n+        output: The quantized tensor in FP4, with shape (m, k // 2, l) but the physical\n+            layout is (l, m, k // 2). `// 2` is because two fp4 values are packed into\n+            an uint8.\n+        output_scales: The blockscale tensor in FP8-E4M3, with shape (32, 4, rm, 4, rk, l)\n+            but the physical layout is (l, rm, rk, 32, 4, 4).\n+    \"\"\"\n+    device = input_tensor.device\n+    l, m, k = input_tensor.shape\n+    sf_vec_size = 16\n+    assert k % sf_vec_size == 0, f\"k must be multiple of 16, but got {k}.\"\n+\n+    scale_k = k // sf_vec_size\n+    padded_k = (scale_k + (4 - 1)) // 4 * 4",
        "comment_created_at": "2025-08-19T00:09:55+00:00",
        "comment_author": "pavanimajety",
        "comment_body": "Since Grouped Gemm is the next operation after Quantization - I am not sure if there  is support for more basic chunk shapes - https://github.com/NVIDIA/cutlass/blob/main/include/cutlass/detail/sm100_blockscaled_layout.hpp#L49-L59",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2288421105",
    "pr_number": 8882,
    "pr_file": "python/sglang/srt/server_args.py",
    "created_at": "2025-08-20T14:45:19+00:00",
    "commented_code": "default=ServerArgs.attention_backend,\n             help=\"Choose the kernels for attention layers.\",\n         )\n+        parser.add_argument(\n+            \"--draft-attention-backend\",\n+            type=str,\n+            choices=[\n+                \"aiter\",\n+                \"cutlass_mla\",\n+                \"fa3\",\n+                \"flashinfer\",\n+                \"flashmla\",\n+                \"intel_amx\",\n+                \"torch_native\",\n+                \"ascend\",\n+                \"triton\",\n+            ],",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2288421105",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8882,
        "pr_file": "python/sglang/srt/server_args.py",
        "discussion_id": "2288421105",
        "commented_code": "@@ -1378,6 +1379,23 @@ def add_cli_args(parser: argparse.ArgumentParser):\n             default=ServerArgs.attention_backend,\n             help=\"Choose the kernels for attention layers.\",\n         )\n+        parser.add_argument(\n+            \"--draft-attention-backend\",\n+            type=str,\n+            choices=[\n+                \"aiter\",\n+                \"cutlass_mla\",\n+                \"fa3\",\n+                \"flashinfer\",\n+                \"flashmla\",\n+                \"intel_amx\",\n+                \"torch_native\",\n+                \"ascend\",\n+                \"triton\",\n+            ],",
        "comment_created_at": "2025-08-20T14:45:19+00:00",
        "comment_author": "fzyzcjy",
        "comment_body": "```suggestion\n            choices=ATTN_BACKENDS,\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2288423563",
    "pr_number": 8882,
    "pr_file": "python/sglang/srt/speculative/eagle_worker.py",
    "created_at": "2025-08-20T14:45:55+00:00",
    "commented_code": "self.has_prefill_wrapper_verify = False\n         self.draft_extend_attn_backend = None\n \n-        if self.server_args.attention_backend == \"flashinfer\":\n+        draft_attention_backend = self.server_args.draft_attention_backend\n+        if draft_attention_backend is None:\n+            # If it is set to none, it is by default the same as target model attention backend\n+            draft_attention_backend = self.server_args.attention_backend",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2288423563",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8882,
        "pr_file": "python/sglang/srt/speculative/eagle_worker.py",
        "discussion_id": "2288423563",
        "commented_code": "@@ -179,7 +179,12 @@ def init_attention_backend(self):\n         self.has_prefill_wrapper_verify = False\n         self.draft_extend_attn_backend = None\n \n-        if self.server_args.attention_backend == \"flashinfer\":\n+        draft_attention_backend = self.server_args.draft_attention_backend\n+        if draft_attention_backend is None:\n+            # If it is set to none, it is by default the same as target model attention backend\n+            draft_attention_backend = self.server_args.attention_backend",
        "comment_created_at": "2025-08-20T14:45:55+00:00",
        "comment_author": "fzyzcjy",
        "comment_body": "```suggestion\n        draft_attention_backend = self.server_args.draft_attention_backend or self.server_args.attention_backend\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2258707249",
    "pr_number": 8666,
    "pr_file": "python/sglang/bench_serving.py",
    "created_at": "2025-08-07T02:13:28+00:00",
    "commented_code": "prompt = f\"Question: {question}\n\nAnswer: \"\n                 if apply_chat_template:\n                     try:\n-                        prompt = tokenizer.apply_chat_template(\n-                            [\n-                                {\n-                                    \"role\": \"user\",\n-                                    \"content\": [\n-                                        {\n-                                            \"type\": \"image_url\",\n-                                            \"image_url\": {\"url\": image_data},\n-                                        },\n-                                        {\"type\": \"text\", \"text\": prompt},\n-                                    ],\n-                                }\n-                            ],\n-                            add_generation_prompt=True,\n-                            tokenize=False,\n-                        )\n+                        if \"llama\" in tokenizer.name_or_path.lower():\n+                            prompt = tokenizer.apply_chat_template(\n+                                [\n+                                    {\n+                                        \"role\": \"user\",\n+                                        \"content\": [\n+                                            {\"type\": \"image\", \"image\": image_data},\n+                                            {\"type\": \"text\", \"text\": prompt},\n+                                        ],\n+                                    }\n+                                ],\n+                                add_generation_prompt=True,\n+                                tokenize=False,\n+                            )\n+                        else:\n+                            prompt = tokenizer.apply_chat_template(\n+                                [\n+                                    {\n+                                        \"role\": \"user\",\n+                                        \"content\": [\n+                                            {\n+                                                \"type\": \"image_url\",\n+                                                \"image_url\": {\"url\": image_data},\n+                                            },\n+                                            {\"type\": \"text\", \"text\": prompt},\n+                                        ],\n+                                    }\n+                                ],\n+                                add_generation_prompt=True,\n+                                tokenize=False,\n+                            )",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2258707249",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8666,
        "pr_file": "python/sglang/bench_serving.py",
        "discussion_id": "2258707249",
        "commented_code": "@@ -879,22 +880,37 @@ def sample_mmmu_requests(\n                 prompt = f\"Question: {question}\\n\\nAnswer: \"\n                 if apply_chat_template:\n                     try:\n-                        prompt = tokenizer.apply_chat_template(\n-                            [\n-                                {\n-                                    \"role\": \"user\",\n-                                    \"content\": [\n-                                        {\n-                                            \"type\": \"image_url\",\n-                                            \"image_url\": {\"url\": image_data},\n-                                        },\n-                                        {\"type\": \"text\", \"text\": prompt},\n-                                    ],\n-                                }\n-                            ],\n-                            add_generation_prompt=True,\n-                            tokenize=False,\n-                        )\n+                        if \"llama\" in tokenizer.name_or_path.lower():\n+                            prompt = tokenizer.apply_chat_template(\n+                                [\n+                                    {\n+                                        \"role\": \"user\",\n+                                        \"content\": [\n+                                            {\"type\": \"image\", \"image\": image_data},\n+                                            {\"type\": \"text\", \"text\": prompt},\n+                                        ],\n+                                    }\n+                                ],\n+                                add_generation_prompt=True,\n+                                tokenize=False,\n+                            )\n+                        else:\n+                            prompt = tokenizer.apply_chat_template(\n+                                [\n+                                    {\n+                                        \"role\": \"user\",\n+                                        \"content\": [\n+                                            {\n+                                                \"type\": \"image_url\",\n+                                                \"image_url\": {\"url\": image_data},\n+                                            },\n+                                            {\"type\": \"text\", \"text\": prompt},\n+                                        ],\n+                                    }\n+                                ],\n+                                add_generation_prompt=True,\n+                                tokenize=False,\n+                            )",
        "comment_created_at": "2025-08-07T02:13:28+00:00",
        "comment_author": "mingfeima",
        "comment_body": "```suggestion\r\n                        if \"llama\" in tokenizer.name_or_path.lower():\r\n                            image_field = {\"type\": \"image\", \"image\": image_data}\r\n                        else:\r\n                            image_field = {\"type\": \"image_url\", \"image_url\": {\"url\": image_data}}\r\n\r\n                       prompt = tokenizer.apply_chat_template(\r\n                            [\r\n                                 {\r\n                                     \"role\": \"user\",\r\n                                      \"content\": [\r\n                                          image_field,\r\n                                          {\"type\": \"text\", \"text\": prompt},\r\n                                       ],\r\n                                   }\r\n                               ],\r\n                              add_generation_prompt=True,\r\n                              tokenize=False,\r\n                          )\r\n```\r\n\r\ntry remove the duplicated code.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2258710065",
    "pr_number": 8666,
    "pr_file": "python/sglang/srt/layers/attention/intel_amx_backend.py",
    "created_at": "2025-08-07T02:16:09+00:00",
    "commented_code": "o = q.new_empty((q.shape[0], layer.tp_q_head_num * layer.v_head_dim))\n         else:\n             o = torch.empty_like(q)\n-\n+        cache_loc = (\n+            forward_batch.out_cache_loc\n+            if not layer.is_cross_attention\n+            else forward_batch.encoder_out_cache_loc\n+        )\n         if save_kv_cache:\n-            forward_batch.token_to_kv_pool.set_kv_buffer(\n-                layer, forward_batch.out_cache_loc, k, v\n-            )\n+            if k is not None:\n+                assert v is not None\n+                forward_batch.token_to_kv_pool.set_kv_buffer(layer, cache_loc, k, v)",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2258710065",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8666,
        "pr_file": "python/sglang/srt/layers/attention/intel_amx_backend.py",
        "discussion_id": "2258710065",
        "commented_code": "@@ -62,14 +62,17 @@ def forward_extend(\n             o = q.new_empty((q.shape[0], layer.tp_q_head_num * layer.v_head_dim))\n         else:\n             o = torch.empty_like(q)\n-\n+        cache_loc = (\n+            forward_batch.out_cache_loc\n+            if not layer.is_cross_attention\n+            else forward_batch.encoder_out_cache_loc\n+        )\n         if save_kv_cache:\n-            forward_batch.token_to_kv_pool.set_kv_buffer(\n-                layer, forward_batch.out_cache_loc, k, v\n-            )\n+            if k is not None:\n+                assert v is not None\n+                forward_batch.token_to_kv_pool.set_kv_buffer(layer, cache_loc, k, v)",
        "comment_created_at": "2025-08-07T02:16:09+00:00",
        "comment_author": "mingfeima",
        "comment_body": "```\r\nif save_kv_cache and k is not None and v is not None:\r\n    forward_batch.token_to_kv_pool.set_kv_buffer(layer, cache_loc, k, v)\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189274578",
    "pr_number": 7392,
    "pr_file": "python/sglang/srt/layers/quark_utils.py",
    "created_at": "2025-07-07T07:55:40+00:00",
    "commented_code": "+\"\"\"",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2189274578",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7392,
        "pr_file": "python/sglang/srt/layers/quark_utils.py",
        "discussion_id": "2189274578",
        "commented_code": "@@ -0,0 +1,60 @@\n+\"\"\"",
        "comment_created_at": "2025-07-07T07:55:40+00:00",
        "comment_author": "HaiShaw",
        "comment_body": "rename this file to `python/sglang/srt/layers/quantization/int4fp8_utils.py`\r\nnothing quark specific",
        "pr_file_module": null
      }
    ]
  }
]