[
  {
    "discussion_id": "2261570178",
    "pr_number": 8934,
    "pr_file": "python/sglang/srt/models/deepseek_v2.py",
    "created_at": "2025-08-08T20:08:21+00:00",
    "commented_code": "if not _is_cuda:\n                 final_hidden_states *= self.routed_scaling_factor\n         current_stream.wait_stream(self.alt_stream)\n-        with use_symmetric_memory(parallel_state.get_tp_group()) as sm:\n+        with use_symmetric_memory(\n+            parallel_state.get_tp_group(), disabled=disable_symmetric_memory\n+        ) as sm:\n             final_hidden_states_out = torch.empty_like(final_hidden_states)\n         torch.add(final_hidden_states, shared_output, out=final_hidden_states_out)\n         final_hidden_states = final_hidden_states_out",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2263901261",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8934,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2261570178",
        "commented_code": "@@ -495,7 +510,9 @@ def forward_normal_dual_stream(\n             if not _is_cuda:\n                 final_hidden_states *= self.routed_scaling_factor\n         current_stream.wait_stream(self.alt_stream)\n-        with use_symmetric_memory(parallel_state.get_tp_group()) as sm:\n+        with use_symmetric_memory(\n+            parallel_state.get_tp_group(), disabled=disable_symmetric_memory\n+        ) as sm:\n             final_hidden_states_out = torch.empty_like(final_hidden_states)\n         torch.add(final_hidden_states, shared_output, out=final_hidden_states_out)\n         final_hidden_states = final_hidden_states_out",
        "comment_created_at": "2025-08-08T20:08:21+00:00",
        "comment_author": "trevor-m",
        "comment_body": "Do we need to tag it?",
        "pr_file_module": null
      },
      {
        "comment_id": "2264006398",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8934,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2261570178",
        "commented_code": "@@ -495,7 +510,9 @@ def forward_normal_dual_stream(\n             if not _is_cuda:\n                 final_hidden_states *= self.routed_scaling_factor\n         current_stream.wait_stream(self.alt_stream)\n-        with use_symmetric_memory(parallel_state.get_tp_group()) as sm:\n+        with use_symmetric_memory(\n+            parallel_state.get_tp_group(), disabled=disable_symmetric_memory\n+        ) as sm:\n             final_hidden_states_out = torch.empty_like(final_hidden_states)\n         torch.add(final_hidden_states, shared_output, out=final_hidden_states_out)\n         final_hidden_states = final_hidden_states_out",
        "comment_created_at": "2025-08-08T21:19:49+00:00",
        "comment_author": "nvcastet",
        "comment_body": "Yes it is tag a few lines below",
        "pr_file_module": null
      },
      {
        "comment_id": "2271976307",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8934,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2261570178",
        "commented_code": "@@ -495,7 +510,9 @@ def forward_normal_dual_stream(\n             if not _is_cuda:\n                 final_hidden_states *= self.routed_scaling_factor\n         current_stream.wait_stream(self.alt_stream)\n-        with use_symmetric_memory(parallel_state.get_tp_group()) as sm:\n+        with use_symmetric_memory(\n+            parallel_state.get_tp_group(), disabled=disable_symmetric_memory\n+        ) as sm:\n             final_hidden_states_out = torch.empty_like(final_hidden_states)\n         torch.add(final_hidden_states, shared_output, out=final_hidden_states_out)\n         final_hidden_states = final_hidden_states_out",
        "comment_created_at": "2025-08-13T03:23:58+00:00",
        "comment_author": "merrymercy",
        "comment_body": "Why do you tag the tensor outside of the `with` scope?",
        "pr_file_module": null
      },
      {
        "comment_id": "2273788342",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8934,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2261570178",
        "commented_code": "@@ -495,7 +510,9 @@ def forward_normal_dual_stream(\n             if not _is_cuda:\n                 final_hidden_states *= self.routed_scaling_factor\n         current_stream.wait_stream(self.alt_stream)\n-        with use_symmetric_memory(parallel_state.get_tp_group()) as sm:\n+        with use_symmetric_memory(\n+            parallel_state.get_tp_group(), disabled=disable_symmetric_memory\n+        ) as sm:\n             final_hidden_states_out = torch.empty_like(final_hidden_states)\n         torch.add(final_hidden_states, shared_output, out=final_hidden_states_out)\n         final_hidden_states = final_hidden_states_out",
        "comment_created_at": "2025-08-13T15:13:41+00:00",
        "comment_author": "nvcastet",
        "comment_body": "The `with` scope make all pytorch allocations under it come from the symmetric memory pool.\r\nThe `.tag(tensor)` is just book-keeping to flag a tensor that has been allocated via symmetric memory so that when we call a collective on it we select NCCL to get best perf instead of alternative custom kernels.\r\n\r\nBut I can move it there if it is clearer for people and AIs. :) \r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2209179276",
    "pr_number": 8079,
    "pr_file": "python/sglang/srt/model_executor/model_runner.py",
    "created_at": "2025-07-16T04:23:05+00:00",
    "commented_code": "logger = logging.getLogger(__name__)\n \n \n+@dataclass\n+class FlattenedTensorMetadata:\n+    \"\"\"Metadata for a tensor in a flattened bucket\"\"\"\n+    name: str\n+    shape: torch.Size\n+    dtype: torch.dtype\n+    start_idx: int\n+    end_idx: int\n+    numel: int\n+\n+\n+class FlattenedTensorBucket:\n+    \"\"\"Helper class to reconstruct tensors from flattened bucket data\"\"\"\n+    \n+    def __init__(self, flattened_tensor: torch.Tensor, metadata: List[FlattenedTensorMetadata]):\n+        self.flattened_tensor = flattened_tensor\n+        self.metadata = metadata\n+        self.total_elements = flattened_tensor.numel()\n+    \n+    def reconstruct_tensors(self) -> List[Tuple[str, torch.Tensor]]:\n+        \"\"\"Reconstruct original tensors from flattened tensor\"\"\"\n+        reconstructed = []\n+        for meta in self.metadata:\n+            # Extract the slice for this tensor\n+            tensor_slice = self.flattened_tensor[meta.start_idx:meta.end_idx]\n+            \n+            # Reshape to original shape\n+            tensor = tensor_slice.reshape(meta.shape)\n+            \n+            # Ensure correct dtype\n+            if tensor.dtype != meta.dtype:\n+                tensor = tensor.to(meta.dtype)",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2209179276",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8079,
        "pr_file": "python/sglang/srt/model_executor/model_runner.py",
        "discussion_id": "2209179276",
        "commented_code": "@@ -134,6 +134,44 @@\n logger = logging.getLogger(__name__)\n \n \n+@dataclass\n+class FlattenedTensorMetadata:\n+    \"\"\"Metadata for a tensor in a flattened bucket\"\"\"\n+    name: str\n+    shape: torch.Size\n+    dtype: torch.dtype\n+    start_idx: int\n+    end_idx: int\n+    numel: int\n+\n+\n+class FlattenedTensorBucket:\n+    \"\"\"Helper class to reconstruct tensors from flattened bucket data\"\"\"\n+    \n+    def __init__(self, flattened_tensor: torch.Tensor, metadata: List[FlattenedTensorMetadata]):\n+        self.flattened_tensor = flattened_tensor\n+        self.metadata = metadata\n+        self.total_elements = flattened_tensor.numel()\n+    \n+    def reconstruct_tensors(self) -> List[Tuple[str, torch.Tensor]]:\n+        \"\"\"Reconstruct original tensors from flattened tensor\"\"\"\n+        reconstructed = []\n+        for meta in self.metadata:\n+            # Extract the slice for this tensor\n+            tensor_slice = self.flattened_tensor[meta.start_idx:meta.end_idx]\n+            \n+            # Reshape to original shape\n+            tensor = tensor_slice.reshape(meta.shape)\n+            \n+            # Ensure correct dtype\n+            if tensor.dtype != meta.dtype:\n+                tensor = tensor.to(meta.dtype)",
        "comment_created_at": "2025-07-16T04:23:05+00:00",
        "comment_author": "fzyzcjy",
        "comment_body": "this may cause copy iirc, what about tensor.view(dtype)",
        "pr_file_module": null
      },
      {
        "comment_id": "2209198508",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8079,
        "pr_file": "python/sglang/srt/model_executor/model_runner.py",
        "discussion_id": "2209179276",
        "commented_code": "@@ -134,6 +134,44 @@\n logger = logging.getLogger(__name__)\n \n \n+@dataclass\n+class FlattenedTensorMetadata:\n+    \"\"\"Metadata for a tensor in a flattened bucket\"\"\"\n+    name: str\n+    shape: torch.Size\n+    dtype: torch.dtype\n+    start_idx: int\n+    end_idx: int\n+    numel: int\n+\n+\n+class FlattenedTensorBucket:\n+    \"\"\"Helper class to reconstruct tensors from flattened bucket data\"\"\"\n+    \n+    def __init__(self, flattened_tensor: torch.Tensor, metadata: List[FlattenedTensorMetadata]):\n+        self.flattened_tensor = flattened_tensor\n+        self.metadata = metadata\n+        self.total_elements = flattened_tensor.numel()\n+    \n+    def reconstruct_tensors(self) -> List[Tuple[str, torch.Tensor]]:\n+        \"\"\"Reconstruct original tensors from flattened tensor\"\"\"\n+        reconstructed = []\n+        for meta in self.metadata:\n+            # Extract the slice for this tensor\n+            tensor_slice = self.flattened_tensor[meta.start_idx:meta.end_idx]\n+            \n+            # Reshape to original shape\n+            tensor = tensor_slice.reshape(meta.shape)\n+            \n+            # Ensure correct dtype\n+            if tensor.dtype != meta.dtype:\n+                tensor = tensor.to(meta.dtype)",
        "comment_created_at": "2025-07-16T04:40:42+00:00",
        "comment_author": "hebiao064",
        "comment_body": "i'll delete it ",
        "pr_file_module": null
      },
      {
        "comment_id": "2209260197",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8079,
        "pr_file": "python/sglang/srt/model_executor/model_runner.py",
        "discussion_id": "2209179276",
        "commented_code": "@@ -134,6 +134,44 @@\n logger = logging.getLogger(__name__)\n \n \n+@dataclass\n+class FlattenedTensorMetadata:\n+    \"\"\"Metadata for a tensor in a flattened bucket\"\"\"\n+    name: str\n+    shape: torch.Size\n+    dtype: torch.dtype\n+    start_idx: int\n+    end_idx: int\n+    numel: int\n+\n+\n+class FlattenedTensorBucket:\n+    \"\"\"Helper class to reconstruct tensors from flattened bucket data\"\"\"\n+    \n+    def __init__(self, flattened_tensor: torch.Tensor, metadata: List[FlattenedTensorMetadata]):\n+        self.flattened_tensor = flattened_tensor\n+        self.metadata = metadata\n+        self.total_elements = flattened_tensor.numel()\n+    \n+    def reconstruct_tensors(self) -> List[Tuple[str, torch.Tensor]]:\n+        \"\"\"Reconstruct original tensors from flattened tensor\"\"\"\n+        reconstructed = []\n+        for meta in self.metadata:\n+            # Extract the slice for this tensor\n+            tensor_slice = self.flattened_tensor[meta.start_idx:meta.end_idx]\n+            \n+            # Reshape to original shape\n+            tensor = tensor_slice.reshape(meta.shape)\n+            \n+            # Ensure correct dtype\n+            if tensor.dtype != meta.dtype:\n+                tensor = tensor.to(meta.dtype)",
        "comment_created_at": "2025-07-16T05:29:06+00:00",
        "comment_author": "fzyzcjy",
        "comment_body": "a bit confused, the flat buffer may be e.g. uint8 dtype, and thus we do need to convert back",
        "pr_file_module": null
      },
      {
        "comment_id": "2209307359",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8079,
        "pr_file": "python/sglang/srt/model_executor/model_runner.py",
        "discussion_id": "2209179276",
        "commented_code": "@@ -134,6 +134,44 @@\n logger = logging.getLogger(__name__)\n \n \n+@dataclass\n+class FlattenedTensorMetadata:\n+    \"\"\"Metadata for a tensor in a flattened bucket\"\"\"\n+    name: str\n+    shape: torch.Size\n+    dtype: torch.dtype\n+    start_idx: int\n+    end_idx: int\n+    numel: int\n+\n+\n+class FlattenedTensorBucket:\n+    \"\"\"Helper class to reconstruct tensors from flattened bucket data\"\"\"\n+    \n+    def __init__(self, flattened_tensor: torch.Tensor, metadata: List[FlattenedTensorMetadata]):\n+        self.flattened_tensor = flattened_tensor\n+        self.metadata = metadata\n+        self.total_elements = flattened_tensor.numel()\n+    \n+    def reconstruct_tensors(self) -> List[Tuple[str, torch.Tensor]]:\n+        \"\"\"Reconstruct original tensors from flattened tensor\"\"\"\n+        reconstructed = []\n+        for meta in self.metadata:\n+            # Extract the slice for this tensor\n+            tensor_slice = self.flattened_tensor[meta.start_idx:meta.end_idx]\n+            \n+            # Reshape to original shape\n+            tensor = tensor_slice.reshape(meta.shape)\n+            \n+            # Ensure correct dtype\n+            if tensor.dtype != meta.dtype:\n+                tensor = tensor.to(meta.dtype)",
        "comment_created_at": "2025-07-16T05:52:31+00:00",
        "comment_author": "hebiao064",
        "comment_body": "ic I will update to view and test it, thanks",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2159971939",
    "pr_number": 7279,
    "pr_file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "created_at": "2025-06-21T07:58:03+00:00",
    "commented_code": "spec_info = forward_batch.spec_info\n         qo_indptr = None\n         kv_last_page_len = None\n-        max_extend_len = None\n+        max_q_len = None\n \n         if forward_batch.forward_mode.is_decode_or_idle():\n             if spec_info is None:\n                 kv_indptr[1 : bs + 1] = torch.cumsum(forward_batch.seq_lens, dim=0)\n                 kv_indptr = kv_indptr[: bs + 1]\n-                kv_indices = torch.zeros(\n+                kv_indices = torch.empty(",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2159971939",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7279,
        "pr_file": "python/sglang/srt/layers/attention/aiter_backend.py",
        "discussion_id": "2159971939",
        "commented_code": "@@ -157,13 +162,13 @@ def init_forward_metadata(self, forward_batch: ForwardBatch):\n         spec_info = forward_batch.spec_info\n         qo_indptr = None\n         kv_last_page_len = None\n-        max_extend_len = None\n+        max_q_len = None\n \n         if forward_batch.forward_mode.is_decode_or_idle():\n             if spec_info is None:\n                 kv_indptr[1 : bs + 1] = torch.cumsum(forward_batch.seq_lens, dim=0)\n                 kv_indptr = kv_indptr[: bs + 1]\n-                kv_indices = torch.zeros(\n+                kv_indices = torch.empty(",
        "comment_created_at": "2025-06-21T07:58:03+00:00",
        "comment_author": "HaiShaw",
        "comment_body": "ROCm: torch.empty seems to be more troublesome than torch.zeros",
        "pr_file_module": null
      },
      {
        "comment_id": "2160068407",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7279,
        "pr_file": "python/sglang/srt/layers/attention/aiter_backend.py",
        "discussion_id": "2159971939",
        "commented_code": "@@ -157,13 +162,13 @@ def init_forward_metadata(self, forward_batch: ForwardBatch):\n         spec_info = forward_batch.spec_info\n         qo_indptr = None\n         kv_last_page_len = None\n-        max_extend_len = None\n+        max_q_len = None\n \n         if forward_batch.forward_mode.is_decode_or_idle():\n             if spec_info is None:\n                 kv_indptr[1 : bs + 1] = torch.cumsum(forward_batch.seq_lens, dim=0)\n                 kv_indptr = kv_indptr[: bs + 1]\n-                kv_indices = torch.zeros(\n+                kv_indices = torch.empty(",
        "comment_created_at": "2025-06-21T15:43:37+00:00",
        "comment_author": "valarLip",
        "comment_body": "it's ok for this case, we fill the whole tensor in next few lines. using zeros we will have a addition fill kernel launch...",
        "pr_file_module": null
      }
    ]
  }
]