[
  {
    "discussion_id": "2227009039",
    "pr_number": 7946,
    "pr_file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "created_at": "2025-07-24T00:25:26+00:00",
    "commented_code": "),\n         )\n \n-        if self.reduce_results and (self.tp_size > 1 or self.ep_size > 1):\n+        if (\n+            _is_flashinfer_available\n+            and global_server_args_dict[\"enable_flashinfer_allreduce\"]\n+            and hidden_states.shape[0] <= 128\n+            ):\n+            from sglang.srt.layers.flashinfer_comm import (\n+                flashinfer_allreduce,\n+            )\n+            final_hidden_states = flashinfer_allreduce(final_hidden_states)",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2227009039",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7946,
        "pr_file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
        "discussion_id": "2227009039",
        "commented_code": "@@ -947,7 +953,16 @@ def forward(self, hidden_states: torch.Tensor, router_logits: torch.Tensor):\n             ),\n         )\n \n-        if self.reduce_results and (self.tp_size > 1 or self.ep_size > 1):\n+        if (\n+            _is_flashinfer_available\n+            and global_server_args_dict[\"enable_flashinfer_allreduce\"]\n+            and hidden_states.shape[0] <= 128\n+            ):\n+            from sglang.srt.layers.flashinfer_comm import (\n+                flashinfer_allreduce,\n+            )\n+            final_hidden_states = flashinfer_allreduce(final_hidden_states)",
        "comment_created_at": "2025-07-24T00:25:26+00:00",
        "comment_author": "kaixih",
        "comment_body": "Shouldn't this check and replacement be under `self.reduce_results and (self.tp_size > 1 or self.ep_size > 1)`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2228325051",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7946,
        "pr_file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
        "discussion_id": "2227009039",
        "commented_code": "@@ -947,7 +953,16 @@ def forward(self, hidden_states: torch.Tensor, router_logits: torch.Tensor):\n             ),\n         )\n \n-        if self.reduce_results and (self.tp_size > 1 or self.ep_size > 1):\n+        if (\n+            _is_flashinfer_available\n+            and global_server_args_dict[\"enable_flashinfer_allreduce\"]\n+            and hidden_states.shape[0] <= 128\n+            ):\n+            from sglang.srt.layers.flashinfer_comm import (\n+                flashinfer_allreduce,\n+            )\n+            final_hidden_states = flashinfer_allreduce(final_hidden_states)",
        "comment_created_at": "2025-07-24T12:05:20+00:00",
        "comment_author": "yuan-luo",
        "comment_body": "Thanks for the comments. It is a bug. I'll fix it. But currently my priority is not on it. Will turn back to it as soon as being available.",
        "pr_file_module": null
      },
      {
        "comment_id": "2289854138",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 7946,
        "pr_file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
        "discussion_id": "2227009039",
        "commented_code": "@@ -947,7 +953,16 @@ def forward(self, hidden_states: torch.Tensor, router_logits: torch.Tensor):\n             ),\n         )\n \n-        if self.reduce_results and (self.tp_size > 1 or self.ep_size > 1):\n+        if (\n+            _is_flashinfer_available\n+            and global_server_args_dict[\"enable_flashinfer_allreduce\"]\n+            and hidden_states.shape[0] <= 128\n+            ):\n+            from sglang.srt.layers.flashinfer_comm import (\n+                flashinfer_allreduce,\n+            )\n+            final_hidden_states = flashinfer_allreduce(final_hidden_states)",
        "comment_created_at": "2025-08-21T04:57:18+00:00",
        "comment_author": "yuan-luo",
        "comment_body": "Fixed.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2281476015",
    "pr_number": 9295,
    "pr_file": "python/sglang/srt/models/deepseek_v2.py",
    "created_at": "2025-08-18T07:03:42+00:00",
    "commented_code": "else:\n                 return _dispatch_mla_subtype()\n         elif attention_backend == \"fa3\":\n+            # When the sequence length exceeds the threshold and the GPU is in compute-bound state, \n+            # MHA performs better than MQA. Attention complexity: O(bs\u00b7seq_len^2)\n+            # Where: \n+            # - average_seq_len approximately represents sequence length (average_seq_len \u2248 sum_seq_len / batch_size)\n+            # - Current computational workload estimation: current_workload \u2248 batch_size * average_seq_len^2\n+            current_workload = 0\n+            if(\n+                forward_batch.forward_mode.is_extend() \n+                and forward_batch.extend_seq_lens_cpu is not None\n+            ):\n+                sum_seq_lens = sum(forward_batch.extend_seq_lens_cpu) \n+                assert forward_batch.batch_size != 0, \"Invalid batch size: zero\"\n+                current_workload = sum_seq_lens // forward_batch.batch_size * sum_seq_lens",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2281476015",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9295,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2281476015",
        "commented_code": "@@ -1019,6 +1024,19 @@ def _dispatch_mla_subtype():\n             else:\n                 return _dispatch_mla_subtype()\n         elif attention_backend == \"fa3\":\n+            # When the sequence length exceeds the threshold and the GPU is in compute-bound state, \n+            # MHA performs better than MQA. Attention complexity: O(bs\u00b7seq_len^2)\n+            # Where: \n+            # - average_seq_len approximately represents sequence length (average_seq_len \u2248 sum_seq_len / batch_size)\n+            # - Current computational workload estimation: current_workload \u2248 batch_size * average_seq_len^2\n+            current_workload = 0\n+            if(\n+                forward_batch.forward_mode.is_extend() \n+                and forward_batch.extend_seq_lens_cpu is not None\n+            ):\n+                sum_seq_lens = sum(forward_batch.extend_seq_lens_cpu) \n+                assert forward_batch.batch_size != 0, \"Invalid batch size: zero\"\n+                current_workload = sum_seq_lens // forward_batch.batch_size * sum_seq_lens",
        "comment_created_at": "2025-08-18T07:03:42+00:00",
        "comment_author": "Fridge003",
        "comment_body": "Maybe here we should change `//` to `/`? Since mean length can be a floating point number.",
        "pr_file_module": null
      },
      {
        "comment_id": "2281594736",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 9295,
        "pr_file": "python/sglang/srt/models/deepseek_v2.py",
        "discussion_id": "2281476015",
        "commented_code": "@@ -1019,6 +1024,19 @@ def _dispatch_mla_subtype():\n             else:\n                 return _dispatch_mla_subtype()\n         elif attention_backend == \"fa3\":\n+            # When the sequence length exceeds the threshold and the GPU is in compute-bound state, \n+            # MHA performs better than MQA. Attention complexity: O(bs\u00b7seq_len^2)\n+            # Where: \n+            # - average_seq_len approximately represents sequence length (average_seq_len \u2248 sum_seq_len / batch_size)\n+            # - Current computational workload estimation: current_workload \u2248 batch_size * average_seq_len^2\n+            current_workload = 0\n+            if(\n+                forward_batch.forward_mode.is_extend() \n+                and forward_batch.extend_seq_lens_cpu is not None\n+            ):\n+                sum_seq_lens = sum(forward_batch.extend_seq_lens_cpu) \n+                assert forward_batch.batch_size != 0, \"Invalid batch size: zero\"\n+                current_workload = sum_seq_lens // forward_batch.batch_size * sum_seq_lens",
        "comment_created_at": "2025-08-18T07:58:08+00:00",
        "comment_author": "Starick-L",
        "comment_body": "Thanks for your suggestion,`/` would be more appropriate.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2258174168",
    "pr_number": 8840,
    "pr_file": "python/sglang/srt/managers/scheduler.py",
    "created_at": "2025-08-06T20:09:02+00:00",
    "commented_code": "# Run prefill first if possible\n             ret = new_batch\n         else:\n+            # For prefill-only requests, decoding is not needed.\n+            # Reset the current decode batch to an empty placeholder.\n+            if self.running_batch.is_prefill_only:\n+                self.running_batch = ScheduleBatch(reqs=[], batch_is_full=False)\n+                ret = None\n             # Run decode",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2258174168",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8840,
        "pr_file": "python/sglang/srt/managers/scheduler.py",
        "discussion_id": "2258174168",
        "commented_code": "@@ -1466,8 +1466,13 @@ def get_next_batch_to_run(self) -> Optional[ScheduleBatch]:\n             # Run prefill first if possible\n             ret = new_batch\n         else:\n+            # For prefill-only requests, decoding is not needed.\n+            # Reset the current decode batch to an empty placeholder.\n+            if self.running_batch.is_prefill_only:\n+                self.running_batch = ScheduleBatch(reqs=[], batch_is_full=False)\n+                ret = None\n             # Run decode",
        "comment_created_at": "2025-08-06T20:09:02+00:00",
        "comment_author": "chanh",
        "comment_body": "hm why do we need to reset the current batch to an empty placeholder? that doesn't really make any sense and could lead to issues. self.running_batch is currently pointing at the current batch and it should be cleared only when the batch is finished being processed, right?\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2258464430",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8840,
        "pr_file": "python/sglang/srt/managers/scheduler.py",
        "discussion_id": "2258174168",
        "commented_code": "@@ -1466,8 +1466,13 @@ def get_next_batch_to_run(self) -> Optional[ScheduleBatch]:\n             # Run prefill first if possible\n             ret = new_batch\n         else:\n+            # For prefill-only requests, decoding is not needed.\n+            # Reset the current decode batch to an empty placeholder.\n+            if self.running_batch.is_prefill_only:\n+                self.running_batch = ScheduleBatch(reqs=[], batch_is_full=False)\n+                ret = None\n             # Run decode",
        "comment_created_at": "2025-08-06T22:44:16+00:00",
        "comment_author": "sundar24295s",
        "comment_body": "Yeah make sense, thanks for pointing out, instead of removing it from the running_batch, we can prevent adding it to the running_batch.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2258722443",
    "pr_number": 8666,
    "pr_file": "python/sglang/srt/layers/attention/torch_native_backend.py",
    "created_at": "2025-08-07T02:29:10+00:00",
    "commented_code": "seq_len_kv = seq_lens[seq_idx]\n             end_q = start_q + extend_seq_len_q\n-            end_kv = start_kv + seq_len_kv\n-\n+            if encoder_lens is not None:\n+                start_kv = 0 if is_cross_attn else encoder_lens[seq_idx]\n+                end_kv = (\n+                    encoder_lens[seq_idx] if is_cross_attn else start_kv + seq_len_kv\n+                )\n+            else:\n+                start_kv = 0\n+                end_kv = start_kv + seq_len_kv",
    "repo_full_name": "sgl-project/sglang",
    "discussion_comments": [
      {
        "comment_id": "2258722443",
        "repo_full_name": "sgl-project/sglang",
        "pr_number": 8666,
        "pr_file": "python/sglang/srt/layers/attention/torch_native_backend.py",
        "discussion_id": "2258722443",
        "commented_code": "@@ -75,8 +79,14 @@ def _run_sdpa_forward_extend(\n \n             seq_len_kv = seq_lens[seq_idx]\n             end_q = start_q + extend_seq_len_q\n-            end_kv = start_kv + seq_len_kv\n-\n+            if encoder_lens is not None:\n+                start_kv = 0 if is_cross_attn else encoder_lens[seq_idx]\n+                end_kv = (\n+                    encoder_lens[seq_idx] if is_cross_attn else start_kv + seq_len_kv\n+                )\n+            else:\n+                start_kv = 0\n+                end_kv = start_kv + seq_len_kv",
        "comment_created_at": "2025-08-07T02:29:10+00:00",
        "comment_author": "mingfeima",
        "comment_body": "```suggestion\r\nif encoder_lens is not None:\r\n    if is_cross_attn:\r\n        start_kv = 0\r\n        end_kv = encoder_lens[seq_idx]\r\n    else:\r\n        start_kv = encoder_lens[seq_idx]\r\n        end_kv = start_kv + seq_len_kv\r\nelse:\r\n    start_kv = 0\r\n    end_kv = seq_len_kv\r\n```",
        "pr_file_module": null
      }
    ]
  }
]