[
  {
    "discussion_id": "2232821528",
    "pr_number": 51623,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala",
    "created_at": "2025-07-26T09:20:04+00:00",
    "commented_code": "}\n   }\n \n+  private lazy val childrenRDDs = children.map(_.execute())",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2232821528",
        "repo_full_name": "apache/spark",
        "pr_number": 51623,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala",
        "discussion_id": "2232821528",
        "commented_code": "@@ -699,8 +701,42 @@ case class UnionExec(children: Seq[SparkPlan]) extends SparkPlan {\n     }\n   }\n \n+  private lazy val childrenRDDs = children.map(_.execute())",
        "comment_created_at": "2025-07-26T09:20:04+00:00",
        "comment_author": "peter-toth",
        "comment_body": "Actaully, does this mean that children executions are triggered to get `outputPartitioning` of an `Union`?\r\nE.g. a simple explain to show the physical plan can now trigger execuion of union children?",
        "pr_file_module": null
      },
      {
        "comment_id": "2233063066",
        "repo_full_name": "apache/spark",
        "pr_number": 51623,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala",
        "discussion_id": "2232821528",
        "commented_code": "@@ -699,8 +701,42 @@ case class UnionExec(children: Seq[SparkPlan]) extends SparkPlan {\n     }\n   }\n \n+  private lazy val childrenRDDs = children.map(_.execute())",
        "comment_created_at": "2025-07-26T15:59:46+00:00",
        "comment_author": "viirya",
        "comment_body": "Yes, so this approach has this drawback. So as I mentioned https://github.com/apache/spark/pull/51623#discussion_r2232350704, this doesn't cover SQL cases generally. I plan to extend this to deal with `outputPartitioning` of children, i.e., no need to invoke `execute` on children.\r\n\r\nIt will be done in follow up works.",
        "pr_file_module": null
      },
      {
        "comment_id": "2233105944",
        "repo_full_name": "apache/spark",
        "pr_number": 51623,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala",
        "discussion_id": "2232821528",
        "commented_code": "@@ -699,8 +701,42 @@ case class UnionExec(children: Seq[SparkPlan]) extends SparkPlan {\n     }\n   }\n \n+  private lazy val childrenRDDs = children.map(_.execute())",
        "comment_created_at": "2025-07-26T17:09:23+00:00",
        "comment_author": "viirya",
        "comment_body": "I actually have the next PR ready locally. After this gets merged, I will open a new PR to improve it and get rid of this `execute` calls.",
        "pr_file_module": null
      },
      {
        "comment_id": "2233253746",
        "repo_full_name": "apache/spark",
        "pr_number": 51623,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala",
        "discussion_id": "2232821528",
        "commented_code": "@@ -699,8 +701,42 @@ case class UnionExec(children: Seq[SparkPlan]) extends SparkPlan {\n     }\n   }\n \n+  private lazy val childrenRDDs = children.map(_.execute())",
        "comment_created_at": "2025-07-26T20:42:17+00:00",
        "comment_author": "peter-toth",
        "comment_body": "IMO that's a serious drawback. But if we can fix it in a follow-up PR right after this PR then I'm ok with merging. Or just update this PR with you local changes.",
        "pr_file_module": null
      },
      {
        "comment_id": "2233257232",
        "repo_full_name": "apache/spark",
        "pr_number": 51623,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala",
        "discussion_id": "2232821528",
        "commented_code": "@@ -699,8 +701,42 @@ case class UnionExec(children: Seq[SparkPlan]) extends SparkPlan {\n     }\n   }\n \n+  private lazy val childrenRDDs = children.map(_.execute())",
        "comment_created_at": "2025-07-26T20:55:00+00:00",
        "comment_author": "viirya",
        "comment_body": "Okay, I updated to this PR.",
        "pr_file_module": null
      },
      {
        "comment_id": "2233260214",
        "repo_full_name": "apache/spark",
        "pr_number": 51623,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala",
        "discussion_id": "2232821528",
        "commented_code": "@@ -699,8 +701,42 @@ case class UnionExec(children: Seq[SparkPlan]) extends SparkPlan {\n     }\n   }\n \n+  private lazy val childrenRDDs = children.map(_.execute())",
        "comment_created_at": "2025-07-26T21:07:41+00:00",
        "comment_author": "peter-toth",
        "comment_body": "Thanks. I can check it tomorrow.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2218799876",
    "pr_number": 51591,
    "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveLateralColumnAliasReference.scala",
    "created_at": "2025-07-21T10:40:28+00:00",
    "commented_code": "}\n         val newProjectList = projectList.zipWithIndex.map {",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2218799876",
        "repo_full_name": "apache/spark",
        "pr_number": 51591,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveLateralColumnAliasReference.scala",
        "discussion_id": "2218799876",
        "commented_code": "@@ -172,12 +172,14 @@ object ResolveLateralColumnAliasReference extends Rule[LogicalPlan] {\n         }\n         val newProjectList = projectList.zipWithIndex.map {",
        "comment_created_at": "2025-07-21T10:40:28+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "nit: `projectList.map(trimNonTopLevelAliases).zipWithIndex.map ...`",
        "pr_file_module": null
      },
      {
        "comment_id": "2218803147",
        "repo_full_name": "apache/spark",
        "pr_number": 51591,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveLateralColumnAliasReference.scala",
        "discussion_id": "2218799876",
        "commented_code": "@@ -172,12 +172,14 @@ object ResolveLateralColumnAliasReference extends Rule[LogicalPlan] {\n         }\n         val newProjectList = projectList.zipWithIndex.map {",
        "comment_created_at": "2025-07-21T10:42:09+00:00",
        "comment_author": "mihailotim-db",
        "comment_body": "That would iterate over `projectList` twice, I intentionally wanted to avoid that",
        "pr_file_module": null
      },
      {
        "comment_id": "2219188156",
        "repo_full_name": "apache/spark",
        "pr_number": 51591,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveLateralColumnAliasReference.scala",
        "discussion_id": "2218799876",
        "commented_code": "@@ -172,12 +172,14 @@ object ResolveLateralColumnAliasReference extends Rule[LogicalPlan] {\n         }\n         val newProjectList = projectList.zipWithIndex.map {",
        "comment_created_at": "2025-07-21T13:17:18+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "then `projectList.toIterator.map...`",
        "pr_file_module": null
      },
      {
        "comment_id": "2219191411",
        "repo_full_name": "apache/spark",
        "pr_number": 51591,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveLateralColumnAliasReference.scala",
        "discussion_id": "2218799876",
        "commented_code": "@@ -172,12 +172,14 @@ object ResolveLateralColumnAliasReference extends Rule[LogicalPlan] {\n         }\n         val newProjectList = projectList.zipWithIndex.map {",
        "comment_created_at": "2025-07-21T13:18:18+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "BTW, `Seq.map` is used quite a lot in the Spark codebase. We shouldn't be nitpicking on non-perf-critical paths.",
        "pr_file_module": null
      },
      {
        "comment_id": "2219219883",
        "repo_full_name": "apache/spark",
        "pr_number": 51591,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveLateralColumnAliasReference.scala",
        "discussion_id": "2218799876",
        "commented_code": "@@ -172,12 +172,14 @@ object ResolveLateralColumnAliasReference extends Rule[LogicalPlan] {\n         }\n         val newProjectList = projectList.zipWithIndex.map {",
        "comment_created_at": "2025-07-21T13:28:14+00:00",
        "comment_author": "mihailotim-db",
        "comment_body": "Sounds good, I fixed it. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2204175148",
    "pr_number": 51451,
    "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
    "created_at": "2025-07-14T08:19:19+00:00",
    "commented_code": "override def maxRows: Option[Long] = {\n     joinType match {\n-      case Inner | Cross | FullOuter | LeftOuter | RightOuter | LeftSingle\n-          if left.maxRows.isDefined && right.maxRows.isDefined =>\n-        val leftMaxRows = BigInt(left.maxRows.get)\n-        val rightMaxRows = BigInt(right.maxRows.get)\n-        val minRows = joinType match {\n-          case LeftOuter | LeftSingle => leftMaxRows\n-          case RightOuter => rightMaxRows\n-          case FullOuter => leftMaxRows + rightMaxRows\n-          case _ => BigInt(0)\n-        }\n-        val maxRows = (leftMaxRows * rightMaxRows).max(minRows)\n-        if (maxRows.isValidLong) {\n-          Some(maxRows.toLong)\n+      case Inner | Cross | FullOuter | LeftOuter | RightOuter | LeftSingle =>\n+        val leftMaxRowsOption = left.maxRows\n+        val rightMaxRowsOption = right.maxRows",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2204175148",
        "repo_full_name": "apache/spark",
        "pr_number": 51451,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
        "discussion_id": "2204175148",
        "commented_code": "@@ -667,19 +667,24 @@ case class Join(\n \n   override def maxRows: Option[Long] = {\n     joinType match {\n-      case Inner | Cross | FullOuter | LeftOuter | RightOuter | LeftSingle\n-          if left.maxRows.isDefined && right.maxRows.isDefined =>\n-        val leftMaxRows = BigInt(left.maxRows.get)\n-        val rightMaxRows = BigInt(right.maxRows.get)\n-        val minRows = joinType match {\n-          case LeftOuter | LeftSingle => leftMaxRows\n-          case RightOuter => rightMaxRows\n-          case FullOuter => leftMaxRows + rightMaxRows\n-          case _ => BigInt(0)\n-        }\n-        val maxRows = (leftMaxRows * rightMaxRows).max(minRows)\n-        if (maxRows.isValidLong) {\n-          Some(maxRows.toLong)\n+      case Inner | Cross | FullOuter | LeftOuter | RightOuter | LeftSingle =>\n+        val leftMaxRowsOption = left.maxRows\n+        val rightMaxRowsOption = right.maxRows",
        "comment_created_at": "2025-07-14T08:19:19+00:00",
        "comment_author": "wForget",
        "comment_body": "nit: keep short-circuit \r\n\r\n```suggestion\r\n        val rightMaxRowsOption = if (leftMaxRowsOption.isDefined) right.maxRows else None\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2204201241",
        "repo_full_name": "apache/spark",
        "pr_number": 51451,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
        "discussion_id": "2204175148",
        "commented_code": "@@ -667,19 +667,24 @@ case class Join(\n \n   override def maxRows: Option[Long] = {\n     joinType match {\n-      case Inner | Cross | FullOuter | LeftOuter | RightOuter | LeftSingle\n-          if left.maxRows.isDefined && right.maxRows.isDefined =>\n-        val leftMaxRows = BigInt(left.maxRows.get)\n-        val rightMaxRows = BigInt(right.maxRows.get)\n-        val minRows = joinType match {\n-          case LeftOuter | LeftSingle => leftMaxRows\n-          case RightOuter => rightMaxRows\n-          case FullOuter => leftMaxRows + rightMaxRows\n-          case _ => BigInt(0)\n-        }\n-        val maxRows = (leftMaxRows * rightMaxRows).max(minRows)\n-        if (maxRows.isValidLong) {\n-          Some(maxRows.toLong)\n+      case Inner | Cross | FullOuter | LeftOuter | RightOuter | LeftSingle =>\n+        val leftMaxRowsOption = left.maxRows\n+        val rightMaxRowsOption = right.maxRows",
        "comment_created_at": "2025-07-14T08:27:08+00:00",
        "comment_author": "zml1206",
        "comment_body": "Good catch! This can avoid unnecessary right maxRows calculations, thanks.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2215530327",
    "pr_number": 51549,
    "pr_file": "sql/api/src/main/scala/org/apache/spark/sql/catalyst/util/SparkDateTimeUtils.scala",
    "created_at": "2025-07-18T09:14:05+00:00",
    "commented_code": "*/\n   def stringToTime(s: UTF8String): Option[Long] = {\n     try {\n-      val (segments, zoneIdOpt, justTime) = parseTimestampString(s)\n+      // Check for the AM/PM suffix.\n+      val trimmedString = s.toString.trim\n+      val strLength = trimmedString.length\n+      val suffix = trimmedString.substring(strLength - 2, strLength).toUpperCase()\n+      val (isAM, isPM) = (suffix.equals(\"AM\"), suffix.equals(\"PM\"))\n+      val hasSuffix = isAM || isPM\n+      val timeString = if (hasSuffix) {\n+        trimmedString.substring(0, strLength - 2).trim\n+      } else {\n+        trimmedString\n+      }\n+\n+      val (segments, zoneIdOpt, justTime) = parseTimestampString(UTF8String.fromString(timeString))",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2215530327",
        "repo_full_name": "apache/spark",
        "pr_number": 51549,
        "pr_file": "sql/api/src/main/scala/org/apache/spark/sql/catalyst/util/SparkDateTimeUtils.scala",
        "discussion_id": "2215530327",
        "commented_code": "@@ -717,14 +717,55 @@ trait SparkDateTimeUtils {\n    */\n   def stringToTime(s: UTF8String): Option[Long] = {\n     try {\n-      val (segments, zoneIdOpt, justTime) = parseTimestampString(s)\n+      // Check for the AM/PM suffix.\n+      val trimmedString = s.toString.trim\n+      val strLength = trimmedString.length\n+      val suffix = trimmedString.substring(strLength - 2, strLength).toUpperCase()\n+      val (isAM, isPM) = (suffix.equals(\"AM\"), suffix.equals(\"PM\"))\n+      val hasSuffix = isAM || isPM\n+      val timeString = if (hasSuffix) {\n+        trimmedString.substring(0, strLength - 2).trim\n+      } else {\n+        trimmedString\n+      }\n+\n+      val (segments, zoneIdOpt, justTime) = parseTimestampString(UTF8String.fromString(timeString))",
        "comment_created_at": "2025-07-18T09:14:05+00:00",
        "comment_author": "MaxGekk",
        "comment_body": "Could you try to avoid conversions `UTF8String` to `String`, and back to `UTF8String`. Can you just check chars like:\r\n\r\n```scala\r\n      val trimmed = s.trimRight()\r\n      val numChars = trimmed.numChars()\r\n      if (numChars > 2 && trimmed.getChar(numChars - 1) == 'M' or 'm') {\r\n        isAM = trimmed.getChar(numChars - 2) == 'A' or 'a'\r\n        isPM = trimmed.getChar(numChars - 2) == 'P' or 'p'\r\n      }\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2215540566",
        "repo_full_name": "apache/spark",
        "pr_number": 51549,
        "pr_file": "sql/api/src/main/scala/org/apache/spark/sql/catalyst/util/SparkDateTimeUtils.scala",
        "discussion_id": "2215530327",
        "commented_code": "@@ -717,14 +717,55 @@ trait SparkDateTimeUtils {\n    */\n   def stringToTime(s: UTF8String): Option[Long] = {\n     try {\n-      val (segments, zoneIdOpt, justTime) = parseTimestampString(s)\n+      // Check for the AM/PM suffix.\n+      val trimmedString = s.toString.trim\n+      val strLength = trimmedString.length\n+      val suffix = trimmedString.substring(strLength - 2, strLength).toUpperCase()\n+      val (isAM, isPM) = (suffix.equals(\"AM\"), suffix.equals(\"PM\"))\n+      val hasSuffix = isAM || isPM\n+      val timeString = if (hasSuffix) {\n+        trimmedString.substring(0, strLength - 2).trim\n+      } else {\n+        trimmedString\n+      }\n+\n+      val (segments, zoneIdOpt, justTime) = parseTimestampString(UTF8String.fromString(timeString))",
        "comment_created_at": "2025-07-18T09:17:30+00:00",
        "comment_author": "uros-db",
        "comment_body": "Good point, those conversions are expensive. Let me refactor the logic a bit according to your suggestion.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2135460905",
    "pr_number": 51117,
    "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/collect.scala",
    "created_at": "2025-06-09T10:17:51+00:00",
    "commented_code": "case class ListAgg(\n     child: Expression,\n     delimiter: Expression = Literal(null),\n-    orderExpressions: Seq[SortOrder] = Nil,\n+    orderChildExpressions: Seq[Expression] = Nil,\n+    orderDirections: Seq[SortDirection] = Nil,\n+    orderNullOrderings: Seq[NullOrdering] = Nil,\n+    orderSameOrderExpressions: Seq[Seq[Expression]] = Nil,\n     mutableAggBufferOffset: Int = 0,\n     inputAggBufferOffset: Int = 0)\n   extends Collect[mutable.ArrayBuffer[Any]]\n   with SupportsOrderingWithinGroup\n   with ImplicitCastInputTypes {\n \n+  val orderExpressions: Seq[SortOrder] = orderChildExpressions.zipWithIndex.map {",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2135460905",
        "repo_full_name": "apache/spark",
        "pr_number": 51117,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/collect.scala",
        "discussion_id": "2135460905",
        "commented_code": "@@ -317,21 +317,43 @@ private[aggregate] object CollectTopK {\n case class ListAgg(\n     child: Expression,\n     delimiter: Expression = Literal(null),\n-    orderExpressions: Seq[SortOrder] = Nil,\n+    orderChildExpressions: Seq[Expression] = Nil,\n+    orderDirections: Seq[SortDirection] = Nil,\n+    orderNullOrderings: Seq[NullOrdering] = Nil,\n+    orderSameOrderExpressions: Seq[Seq[Expression]] = Nil,\n     mutableAggBufferOffset: Int = 0,\n     inputAggBufferOffset: Int = 0)\n   extends Collect[mutable.ArrayBuffer[Any]]\n   with SupportsOrderingWithinGroup\n   with ImplicitCastInputTypes {\n \n+  val orderExpressions: Seq[SortOrder] = orderChildExpressions.zipWithIndex.map {",
        "comment_created_at": "2025-06-09T10:17:51+00:00",
        "comment_author": "mihailom-db",
        "comment_body": "Should this be a lazy val?",
        "pr_file_module": null
      },
      {
        "comment_id": "2135474330",
        "repo_full_name": "apache/spark",
        "pr_number": 51117,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/collect.scala",
        "discussion_id": "2135460905",
        "commented_code": "@@ -317,21 +317,43 @@ private[aggregate] object CollectTopK {\n case class ListAgg(\n     child: Expression,\n     delimiter: Expression = Literal(null),\n-    orderExpressions: Seq[SortOrder] = Nil,\n+    orderChildExpressions: Seq[Expression] = Nil,\n+    orderDirections: Seq[SortDirection] = Nil,\n+    orderNullOrderings: Seq[NullOrdering] = Nil,\n+    orderSameOrderExpressions: Seq[Seq[Expression]] = Nil,\n     mutableAggBufferOffset: Int = 0,\n     inputAggBufferOffset: Int = 0)\n   extends Collect[mutable.ArrayBuffer[Any]]\n   with SupportsOrderingWithinGroup\n   with ImplicitCastInputTypes {\n \n+  val orderExpressions: Seq[SortOrder] = orderChildExpressions.zipWithIndex.map {",
        "comment_created_at": "2025-06-09T10:27:18+00:00",
        "comment_author": "uros-db",
        "comment_body": "It can be, but not sure if it would bring any special value to this particular expression. No strong opinion, I can update if needed.",
        "pr_file_module": null
      },
      {
        "comment_id": "2138858337",
        "repo_full_name": "apache/spark",
        "pr_number": 51117,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/collect.scala",
        "discussion_id": "2135460905",
        "commented_code": "@@ -317,21 +317,43 @@ private[aggregate] object CollectTopK {\n case class ListAgg(\n     child: Expression,\n     delimiter: Expression = Literal(null),\n-    orderExpressions: Seq[SortOrder] = Nil,\n+    orderChildExpressions: Seq[Expression] = Nil,\n+    orderDirections: Seq[SortDirection] = Nil,\n+    orderNullOrderings: Seq[NullOrdering] = Nil,\n+    orderSameOrderExpressions: Seq[Seq[Expression]] = Nil,\n     mutableAggBufferOffset: Int = 0,\n     inputAggBufferOffset: Int = 0)\n   extends Collect[mutable.ArrayBuffer[Any]]\n   with SupportsOrderingWithinGroup\n   with ImplicitCastInputTypes {\n \n+  val orderExpressions: Seq[SortOrder] = orderChildExpressions.zipWithIndex.map {",
        "comment_created_at": "2025-06-10T22:20:58+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "it should be lazy val, to avoid repeatedly triggerring it during plan transformation when plan nodes are copied frequently.",
        "pr_file_module": null
      },
      {
        "comment_id": "2139683527",
        "repo_full_name": "apache/spark",
        "pr_number": 51117,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/collect.scala",
        "discussion_id": "2135460905",
        "commented_code": "@@ -317,21 +317,43 @@ private[aggregate] object CollectTopK {\n case class ListAgg(\n     child: Expression,\n     delimiter: Expression = Literal(null),\n-    orderExpressions: Seq[SortOrder] = Nil,\n+    orderChildExpressions: Seq[Expression] = Nil,\n+    orderDirections: Seq[SortDirection] = Nil,\n+    orderNullOrderings: Seq[NullOrdering] = Nil,\n+    orderSameOrderExpressions: Seq[Seq[Expression]] = Nil,\n     mutableAggBufferOffset: Int = 0,\n     inputAggBufferOffset: Int = 0)\n   extends Collect[mutable.ArrayBuffer[Any]]\n   with SupportsOrderingWithinGroup\n   with ImplicitCastInputTypes {\n \n+  val orderExpressions: Seq[SortOrder] = orderChildExpressions.zipWithIndex.map {",
        "comment_created_at": "2025-06-11T09:41:06+00:00",
        "comment_author": "uros-db",
        "comment_body": "Makes sense, I'll update to lazy.",
        "pr_file_module": null
      }
    ]
  }
]