[
  {
    "discussion_id": "2226035535",
    "pr_number": 51590,
    "pr_file": "python/pyspark/pipelines/block_imperative_construct.py",
    "created_at": "2025-07-23T15:56:04+00:00",
    "commented_code": "+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+from contextlib import contextmanager\n+from typing import Generator, NoReturn, List, Callable\n+\n+from pyspark.errors import PySparkException\n+from pyspark.sql.connect.catalog import Catalog\n+from pyspark.sql.connect.conf import RuntimeConf\n+from pyspark.sql.connect.dataframe import DataFrame\n+from pyspark.sql.connect.udf import UDFRegistration\n+\n+# pyspark methods that should be blocked from executing in python pipeline definition files\n+BLOCKED_METHODS: List = [\n+    {\n+        \"class\": RuntimeConf,\n+        \"method\": \"set\",\n+        \"suggestion\": \"Instead set configuration via the pipeline spec \"\n+        \"or use the 'spark_conf' argument in various decorators\",\n+    },\n+    {\n+        \"class\": Catalog,\n+        \"method\": \"setCurrentCatalog\",\n+        \"suggestion\": \"Instead set catalog via the pipeline spec \"\n+        \"or the 'name' argument on the dataset decorators\",\n+    },\n+    {\n+        \"class\": Catalog,\n+        \"method\": \"setCurrentDatabase\",\n+        \"suggestion\": \"Instead set database via the pipeline spec \"\n+        \"or the 'name' argument on the dataset decorators\",\n+    },\n+    {\n+        \"class\": Catalog,\n+        \"method\": \"dropTempView\",\n+        \"suggestion\": \"Instead remove the temporary view definition directly\",\n+    },\n+    {\n+        \"class\": Catalog,\n+        \"method\": \"dropGlobalTempView\",\n+        \"suggestion\": \"Instead remove the temporary view definition directly\",\n+    },\n+    {\n+        \"class\": DataFrame,\n+        \"method\": \"createTempView\",\n+        \"suggestion\": \"Instead use the @temporary_view decorator to define temporary views\",\n+    },\n+    {\n+        \"class\": DataFrame,\n+        \"method\": \"createOrReplaceTempView\",\n+        \"suggestion\": \"Instead use the @temporary_view decorator to define temporary views\",\n+    },\n+    {\n+        \"class\": DataFrame,\n+        \"method\": \"createGlobalTempView\",\n+        \"suggestion\": \"Instead use the @temporary_view decorator to define temporary views\",\n+    },\n+    {\n+        \"class\": DataFrame,\n+        \"method\": \"createOrReplaceGlobalTempView\",\n+        \"suggestion\": \"Instead use the @temporary_view decorator to define temporary views\",\n+    },\n+    {\n+        \"class\": UDFRegistration,\n+        \"method\": \"register\",\n+        \"suggestion\": \"\",\n+    },\n+    {\n+        \"class\": UDFRegistration,\n+        \"method\": \"registerJavaFunction\",\n+        \"suggestion\": \"\",\n+    },\n+    {\n+        \"class\": UDFRegistration,\n+        \"method\": \"registerJavaUDAF\",\n+        \"suggestion\": \"\",\n+    },\n+]\n+\n+\n+def _create_blocked_method(error_method_name: str, suggestion: str) -> Callable:\n+    def blocked_method(*args: object, **kwargs: object) -> NoReturn:\n+        raise PySparkException(\n+            errorClass=\"IMPERATIVE_CONSTRUCT_IN_DECLARATIVE_PIPELINE\",\n+            messageParameters={\n+                \"method\": error_method_name,\n+                \"suggestion\": suggestion,\n+            },\n+        )\n+\n+    return blocked_method\n+\n+\n+@contextmanager\n+def block_imperative_construct() -> Generator[None, None, None]:\n+    \"\"\"\n+    Context manager that blocks imperative constructs found in a pipeline python definition file\n+    Blocks:\n+        - imperative config set via: spark.conf.set(\"k\", \"v\")",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2226035535",
        "repo_full_name": "apache/spark",
        "pr_number": 51590,
        "pr_file": "python/pyspark/pipelines/block_imperative_construct.py",
        "discussion_id": "2226035535",
        "commented_code": "@@ -0,0 +1,141 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+from contextlib import contextmanager\n+from typing import Generator, NoReturn, List, Callable\n+\n+from pyspark.errors import PySparkException\n+from pyspark.sql.connect.catalog import Catalog\n+from pyspark.sql.connect.conf import RuntimeConf\n+from pyspark.sql.connect.dataframe import DataFrame\n+from pyspark.sql.connect.udf import UDFRegistration\n+\n+# pyspark methods that should be blocked from executing in python pipeline definition files\n+BLOCKED_METHODS: List = [\n+    {\n+        \"class\": RuntimeConf,\n+        \"method\": \"set\",\n+        \"suggestion\": \"Instead set configuration via the pipeline spec \"\n+        \"or use the 'spark_conf' argument in various decorators\",\n+    },\n+    {\n+        \"class\": Catalog,\n+        \"method\": \"setCurrentCatalog\",\n+        \"suggestion\": \"Instead set catalog via the pipeline spec \"\n+        \"or the 'name' argument on the dataset decorators\",\n+    },\n+    {\n+        \"class\": Catalog,\n+        \"method\": \"setCurrentDatabase\",\n+        \"suggestion\": \"Instead set database via the pipeline spec \"\n+        \"or the 'name' argument on the dataset decorators\",\n+    },\n+    {\n+        \"class\": Catalog,\n+        \"method\": \"dropTempView\",\n+        \"suggestion\": \"Instead remove the temporary view definition directly\",\n+    },\n+    {\n+        \"class\": Catalog,\n+        \"method\": \"dropGlobalTempView\",\n+        \"suggestion\": \"Instead remove the temporary view definition directly\",\n+    },\n+    {\n+        \"class\": DataFrame,\n+        \"method\": \"createTempView\",\n+        \"suggestion\": \"Instead use the @temporary_view decorator to define temporary views\",\n+    },\n+    {\n+        \"class\": DataFrame,\n+        \"method\": \"createOrReplaceTempView\",\n+        \"suggestion\": \"Instead use the @temporary_view decorator to define temporary views\",\n+    },\n+    {\n+        \"class\": DataFrame,\n+        \"method\": \"createGlobalTempView\",\n+        \"suggestion\": \"Instead use the @temporary_view decorator to define temporary views\",\n+    },\n+    {\n+        \"class\": DataFrame,\n+        \"method\": \"createOrReplaceGlobalTempView\",\n+        \"suggestion\": \"Instead use the @temporary_view decorator to define temporary views\",\n+    },\n+    {\n+        \"class\": UDFRegistration,\n+        \"method\": \"register\",\n+        \"suggestion\": \"\",\n+    },\n+    {\n+        \"class\": UDFRegistration,\n+        \"method\": \"registerJavaFunction\",\n+        \"suggestion\": \"\",\n+    },\n+    {\n+        \"class\": UDFRegistration,\n+        \"method\": \"registerJavaUDAF\",\n+        \"suggestion\": \"\",\n+    },\n+]\n+\n+\n+def _create_blocked_method(error_method_name: str, suggestion: str) -> Callable:\n+    def blocked_method(*args: object, **kwargs: object) -> NoReturn:\n+        raise PySparkException(\n+            errorClass=\"IMPERATIVE_CONSTRUCT_IN_DECLARATIVE_PIPELINE\",\n+            messageParameters={\n+                \"method\": error_method_name,\n+                \"suggestion\": suggestion,\n+            },\n+        )\n+\n+    return blocked_method\n+\n+\n+@contextmanager\n+def block_imperative_construct() -> Generator[None, None, None]:\n+    \"\"\"\n+    Context manager that blocks imperative constructs found in a pipeline python definition file\n+    Blocks:\n+        - imperative config set via: spark.conf.set(\"k\", \"v\")",
        "comment_created_at": "2025-07-23T15:56:04+00:00",
        "comment_author": "sryza",
        "comment_body": "It will be easy for this list to get out of sync with BLOCKED_METHODS if we add more. I think better to mention BLOCKED_METHODS here than to try to enumerate all its contents here.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2165960575",
    "pr_number": 51225,
    "pr_file": "python/pyspark/sql/tests/arrow/test_arrow_python_udf.py",
    "created_at": "2025-06-25T07:06:17+00:00",
    "commented_code": "udf(lambda x: str(x), useArrow=False).evalType, PythonEvalType.SQL_BATCHED_UDF\n             )\n \n+    def test_udf_use_arrow_and_session_conf(self):\n+        self._check_udf_use_arrow_and_session_conf()\n+\n+\n+@unittest.skipIf(\n+    not have_pandas or not have_pyarrow, pandas_requirement_message or pyarrow_requirement_message\n+)\n+class ArrowPythonUDFLegacyTestsMixin(BaseUDFTestsMixin):\n+    def test_complex_input_types(self):\n+        for pandas_conversion in [True, False]:\n+            with self.subTest(pandas_conversion=pandas_conversion), self.sql_conf(\n+                {\"spark.sql.legacy.execution.pythonUDF.pandas.conversion.enabled\": str(pandas_conversion).lower()}",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2165960575",
        "repo_full_name": "apache/spark",
        "pr_number": 51225,
        "pr_file": "python/pyspark/sql/tests/arrow/test_arrow_python_udf.py",
        "discussion_id": "2165960575",
        "commented_code": "@@ -243,6 +273,91 @@ def test_udf_use_arrow_and_session_conf(self):\n                 udf(lambda x: str(x), useArrow=False).evalType, PythonEvalType.SQL_BATCHED_UDF\n             )\n \n+    def test_udf_use_arrow_and_session_conf(self):\n+        self._check_udf_use_arrow_and_session_conf()\n+\n+\n+@unittest.skipIf(\n+    not have_pandas or not have_pyarrow, pandas_requirement_message or pyarrow_requirement_message\n+)\n+class ArrowPythonUDFLegacyTestsMixin(BaseUDFTestsMixin):\n+    def test_complex_input_types(self):\n+        for pandas_conversion in [True, False]:\n+            with self.subTest(pandas_conversion=pandas_conversion), self.sql_conf(\n+                {\"spark.sql.legacy.execution.pythonUDF.pandas.conversion.enabled\": str(pandas_conversion).lower()}",
        "comment_created_at": "2025-06-25T07:06:17+00:00",
        "comment_author": "zhengruifeng",
        "comment_body": "It seems we can move the config setting into the setupClass method",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2214779565",
    "pr_number": 51149,
    "pr_file": "dev/run-tests.py",
    "created_at": "2025-07-18T03:07:48+00:00",
    "commented_code": "print(\"[info] Building Spark using SBT with these arguments: \", \" \".join(profiles_and_goals))\n \n-    exec_sbt(profiles_and_goals)\n+    exec_sbt(profiles_and_goals, retry=3)",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2214779565",
        "repo_full_name": "apache/spark",
        "pr_number": 51149,
        "pr_file": "dev/run-tests.py",
        "discussion_id": "2214779565",
        "commented_code": "@@ -255,7 +278,7 @@ def build_spark_sbt(extra_profiles):\n \n     print(\"[info] Building Spark using SBT with these arguments: \", \" \".join(profiles_and_goals))\n \n-    exec_sbt(profiles_and_goals)\n+    exec_sbt(profiles_and_goals, retry=3)",
        "comment_created_at": "2025-07-18T03:07:48+00:00",
        "comment_author": "LuciferYang",
        "comment_body": "Would it be better to make `retry` a configurable environment variable with a default value of 3?",
        "pr_file_module": null
      },
      {
        "comment_id": "2214791102",
        "repo_full_name": "apache/spark",
        "pr_number": 51149,
        "pr_file": "dev/run-tests.py",
        "discussion_id": "2214779565",
        "commented_code": "@@ -255,7 +278,7 @@ def build_spark_sbt(extra_profiles):\n \n     print(\"[info] Building Spark using SBT with these arguments: \", \" \".join(profiles_and_goals))\n \n-    exec_sbt(profiles_and_goals)\n+    exec_sbt(profiles_and_goals, retry=3)",
        "comment_created_at": "2025-07-18T03:23:05+00:00",
        "comment_author": "zhengruifeng",
        "comment_body": "sounds good\uff01",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2211512061",
    "pr_number": 51507,
    "pr_file": "python/pyspark/pipelines/cli.py",
    "created_at": "2025-07-16T20:20:59+00:00",
    "commented_code": "os.chdir(prev)\n \n \n-def run(spec_path: Path) -> None:\n-    \"\"\"Run the pipeline defined with the given spec.\"\"\"\n+def run(",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2211512061",
        "repo_full_name": "apache/spark",
        "pr_number": 51507,
        "pr_file": "python/pyspark/pipelines/cli.py",
        "discussion_id": "2211512061",
        "commented_code": "@@ -217,8 +217,30 @@ def change_dir(path: Path) -> Generator[None, None, None]:\n         os.chdir(prev)\n \n \n-def run(spec_path: Path) -> None:\n-    \"\"\"Run the pipeline defined with the given spec.\"\"\"\n+def run(",
        "comment_created_at": "2025-07-16T20:20:59+00:00",
        "comment_author": "AnishMahto",
        "comment_body": "High level question: did we consider putting refresh selection options in the pipeline spec, rather than as a CLI arg?\r\n\r\nMore generally, what's the philosophy for whether a configuration should be accepted as a CLI arg vs a pipeline spec field?",
        "pr_file_module": null
      },
      {
        "comment_id": "2211630306",
        "repo_full_name": "apache/spark",
        "pr_number": 51507,
        "pr_file": "python/pyspark/pipelines/cli.py",
        "discussion_id": "2211512061",
        "commented_code": "@@ -217,8 +217,30 @@ def change_dir(path: Path) -> Generator[None, None, None]:\n         os.chdir(prev)\n \n \n-def run(spec_path: Path) -> None:\n-    \"\"\"Run the pipeline defined with the given spec.\"\"\"\n+def run(",
        "comment_created_at": "2025-07-16T21:17:07+00:00",
        "comment_author": "sryza",
        "comment_body": "If we expect it to vary across run for the same pipeline, it should be a CLI arg. If we expect it to be static for a pipeline, it should live in the spec. I would expect selections to vary across runs.",
        "pr_file_module": null
      }
    ]
  }
]