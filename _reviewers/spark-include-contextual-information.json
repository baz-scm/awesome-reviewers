[
  {
    "discussion_id": "2078913570",
    "pr_number": 50742,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBStateStoreProvider.scala",
    "created_at": "2025-05-08T05:30:02+00:00",
    "commented_code": "}\n       result\n     }\n+\n+    override def release(): Unit = {\n+      if (state != RELEASED) {\n+        logInfo(log\"Releasing ${MDC(VERSION_NUM, version + 1)} \" +",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2078913570",
        "repo_full_name": "apache/spark",
        "pr_number": 50742,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBStateStoreProvider.scala",
        "discussion_id": "2078913570",
        "commented_code": "@@ -365,6 +366,18 @@ private[sql] class RocksDBStateStoreProvider\n       }\n       result\n     }\n+\n+    override def release(): Unit = {\n+      if (state != RELEASED) {\n+        logInfo(log\"Releasing ${MDC(VERSION_NUM, version + 1)} \" +",
        "comment_created_at": "2025-05-08T05:30:02+00:00",
        "comment_author": "micheal-o",
        "comment_body": "why is it version + 1 here?",
        "pr_file_module": null
      },
      {
        "comment_id": "2080067114",
        "repo_full_name": "apache/spark",
        "pr_number": 50742,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBStateStoreProvider.scala",
        "discussion_id": "2078913570",
        "commented_code": "@@ -365,6 +366,18 @@ private[sql] class RocksDBStateStoreProvider\n       }\n       result\n     }\n+\n+    override def release(): Unit = {\n+      if (state != RELEASED) {\n+        logInfo(log\"Releasing ${MDC(VERSION_NUM, version + 1)} \" +",
        "comment_created_at": "2025-05-08T16:27:23+00:00",
        "comment_author": "ericm-db",
        "comment_body": "Hm, not sure but we log a similar line for abort [here](https://github.com/apache/spark/pull/50742/files#diff-76a55d7a6f0b91a19366e01c919c615a4d13afc1c1c37b017afa9df1c1705218L251)",
        "pr_file_module": null
      },
      {
        "comment_id": "2172553006",
        "repo_full_name": "apache/spark",
        "pr_number": 50742,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBStateStoreProvider.scala",
        "discussion_id": "2078913570",
        "commented_code": "@@ -365,6 +366,18 @@ private[sql] class RocksDBStateStoreProvider\n       }\n       result\n     }\n+\n+    override def release(): Unit = {\n+      if (state != RELEASED) {\n+        logInfo(log\"Releasing ${MDC(VERSION_NUM, version + 1)} \" +",
        "comment_created_at": "2025-06-27T17:48:36+00:00",
        "comment_author": "WweiL",
        "comment_body": "`version` is the last version, what the code is operating on right now is `version + 1`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2223808261",
    "pr_number": 51604,
    "pr_file": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala",
    "created_at": "2025-07-22T20:54:01+00:00",
    "commented_code": "Some(loadedUI)\n   }\n \n+  private def loadFromFallbackLocation(appId: String, attemptId: Option[String], logPath: String)\n+    : ApplicationInfoWrapper = {\n+    val date = new Date(0)\n+    val info = ApplicationAttemptInfo(attemptId, date, date, date, 0, \"spark\", false, \"unknown\")\n+    addListing(new ApplicationInfoWrapper(\n+      ApplicationInfo(appId, appId, None, None, None, None, List.empty),\n+      List(new AttemptInfoWrapper(info, logPath, 0, Some(1), None, None, None, None))))",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2223808261",
        "repo_full_name": "apache/spark",
        "pr_number": 51604,
        "pr_file": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala",
        "discussion_id": "2223808261",
        "commented_code": "@@ -364,6 +375,16 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)\n     Some(loadedUI)\n   }\n \n+  private def loadFromFallbackLocation(appId: String, attemptId: Option[String], logPath: String)\n+    : ApplicationInfoWrapper = {\n+    val date = new Date(0)\n+    val info = ApplicationAttemptInfo(attemptId, date, date, date, 0, \"spark\", false, \"unknown\")\n+    addListing(new ApplicationInfoWrapper(\n+      ApplicationInfo(appId, appId, None, None, None, None, List.empty),\n+      List(new AttemptInfoWrapper(info, logPath, 0, Some(1), None, None, None, None))))",
        "comment_created_at": "2025-07-22T20:54:01+00:00",
        "comment_author": "thejdeep",
        "comment_body": "Shouldn't we rely on the event log for information like `startTime`, `endTime`, `user` etc ? Will this not lead to incorrect information being displayed on the home page of SHS ?",
        "pr_file_module": null
      },
      {
        "comment_id": "2223816729",
        "repo_full_name": "apache/spark",
        "pr_number": 51604,
        "pr_file": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala",
        "discussion_id": "2223808261",
        "commented_code": "@@ -364,6 +375,16 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)\n     Some(loadedUI)\n   }\n \n+  private def loadFromFallbackLocation(appId: String, attemptId: Option[String], logPath: String)\n+    : ApplicationInfoWrapper = {\n+    val date = new Date(0)\n+    val info = ApplicationAttemptInfo(attemptId, date, date, date, 0, \"spark\", false, \"unknown\")\n+    addListing(new ApplicationInfoWrapper(\n+      ApplicationInfo(appId, appId, None, None, None, None, List.empty),\n+      List(new AttemptInfoWrapper(info, logPath, 0, Some(1), None, None, None, None))))",
        "comment_created_at": "2025-07-22T20:58:46+00:00",
        "comment_author": "dongjoon-hyun",
        "comment_body": "This is only a dummy place to allow SHS shows the application logs before periodic scanning happens. The periodic scanning will keep it in sync.\r\n\r\nBTW, I'm wondering how many times do you think this fallback is used in the production environments, @thejdeep ?  I'm curious if you are thinking about turning off the periodic scanning. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2223837583",
        "repo_full_name": "apache/spark",
        "pr_number": 51604,
        "pr_file": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala",
        "discussion_id": "2223808261",
        "commented_code": "@@ -364,6 +375,16 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)\n     Some(loadedUI)\n   }\n \n+  private def loadFromFallbackLocation(appId: String, attemptId: Option[String], logPath: String)\n+    : ApplicationInfoWrapper = {\n+    val date = new Date(0)\n+    val info = ApplicationAttemptInfo(attemptId, date, date, date, 0, \"spark\", false, \"unknown\")\n+    addListing(new ApplicationInfoWrapper(\n+      ApplicationInfo(appId, appId, None, None, None, None, List.empty),\n+      List(new AttemptInfoWrapper(info, logPath, 0, Some(1), None, None, None, None))))",
        "comment_created_at": "2025-07-22T21:11:59+00:00",
        "comment_author": "thejdeep",
        "comment_body": "Oh I see that the intention is just to have dummy placeholders until the scanning takes care of it. \r\n\r\nIf users operate with a large Spark cluster, my two cents are that users may tend to access their app on demand much more frequently and it might just lead to a incorrect listing page. For example, we noticed that a good fraction of our SHS requests are on demand since users would like to get their reports as soon as their app finishes and before `checkForLogs` completes.",
        "pr_file_module": null
      },
      {
        "comment_id": "2223844343",
        "repo_full_name": "apache/spark",
        "pr_number": 51604,
        "pr_file": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala",
        "discussion_id": "2223808261",
        "commented_code": "@@ -364,6 +375,16 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)\n     Some(loadedUI)\n   }\n \n+  private def loadFromFallbackLocation(appId: String, attemptId: Option[String], logPath: String)\n+    : ApplicationInfoWrapper = {\n+    val date = new Date(0)\n+    val info = ApplicationAttemptInfo(attemptId, date, date, date, 0, \"spark\", false, \"unknown\")\n+    addListing(new ApplicationInfoWrapper(\n+      ApplicationInfo(appId, appId, None, None, None, None, List.empty),\n+      List(new AttemptInfoWrapper(info, logPath, 0, Some(1), None, None, None, None))))",
        "comment_created_at": "2025-07-22T21:16:42+00:00",
        "comment_author": "dongjoon-hyun",
        "comment_body": "Yes, and technically, it's not exposed in the listing page. Could you build this PR and test it by yourself?\r\n> a incorrect listing page",
        "pr_file_module": null
      },
      {
        "comment_id": "2223846826",
        "repo_full_name": "apache/spark",
        "pr_number": 51604,
        "pr_file": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala",
        "discussion_id": "2223808261",
        "commented_code": "@@ -364,6 +375,16 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)\n     Some(loadedUI)\n   }\n \n+  private def loadFromFallbackLocation(appId: String, attemptId: Option[String], logPath: String)\n+    : ApplicationInfoWrapper = {\n+    val date = new Date(0)\n+    val info = ApplicationAttemptInfo(attemptId, date, date, date, 0, \"spark\", false, \"unknown\")\n+    addListing(new ApplicationInfoWrapper(\n+      ApplicationInfo(appId, appId, None, None, None, None, List.empty),\n+      List(new AttemptInfoWrapper(info, logPath, 0, Some(1), None, None, None, None))))",
        "comment_created_at": "2025-07-22T21:18:18+00:00",
        "comment_author": "dongjoon-hyun",
        "comment_body": "It sounds like a limitation of a single file event log, @thejdeep . If you have rolling event logs, SHS have the correct partial information already while your jobs are running.\r\n> For example, we noticed that a good fraction of our SHS requests are on demand since users would like to get their reports as soon as their app finishes and before checkForLogs completes.",
        "pr_file_module": null
      },
      {
        "comment_id": "2223853069",
        "repo_full_name": "apache/spark",
        "pr_number": 51604,
        "pr_file": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala",
        "discussion_id": "2223808261",
        "commented_code": "@@ -364,6 +375,16 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)\n     Some(loadedUI)\n   }\n \n+  private def loadFromFallbackLocation(appId: String, attemptId: Option[String], logPath: String)\n+    : ApplicationInfoWrapper = {\n+    val date = new Date(0)\n+    val info = ApplicationAttemptInfo(attemptId, date, date, date, 0, \"spark\", false, \"unknown\")\n+    addListing(new ApplicationInfoWrapper(\n+      ApplicationInfo(appId, appId, None, None, None, None, List.empty),\n+      List(new AttemptInfoWrapper(info, logPath, 0, Some(1), None, None, None, None))))",
        "comment_created_at": "2025-07-22T21:21:51+00:00",
        "comment_author": "dongjoon-hyun",
        "comment_body": "Just questions to understand your use cases:\r\n- How do you handle Spark Streaming Jobs with a single file event log ? Still your job doesn't use rolling event logs?\r\n- Are you assuming only Spark 2.x or 3.x jobs because Spark 4 jobs generates rolling events by default since SPARK-45771?",
        "pr_file_module": null
      },
      {
        "comment_id": "2223889116",
        "repo_full_name": "apache/spark",
        "pr_number": 51604,
        "pr_file": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala",
        "discussion_id": "2223808261",
        "commented_code": "@@ -364,6 +375,16 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)\n     Some(loadedUI)\n   }\n \n+  private def loadFromFallbackLocation(appId: String, attemptId: Option[String], logPath: String)\n+    : ApplicationInfoWrapper = {\n+    val date = new Date(0)\n+    val info = ApplicationAttemptInfo(attemptId, date, date, date, 0, \"spark\", false, \"unknown\")\n+    addListing(new ApplicationInfoWrapper(\n+      ApplicationInfo(appId, appId, None, None, None, None, List.empty),\n+      List(new AttemptInfoWrapper(info, logPath, 0, Some(1), None, None, None, None))))",
        "comment_created_at": "2025-07-22T21:44:39+00:00",
        "comment_author": "thejdeep",
        "comment_body": "Thanks for sharing context @dongjoon-hyun . \r\n\r\nWe currently do not use rolling event logs since we only currently serve batch use-cases. All applications are currently on 3.x.\r\n\r\nI can build your PR locally and test it on single file event logs to see how it works with listing and cleanup. I can get back to you earliest by tomorrow if that works.",
        "pr_file_module": null
      },
      {
        "comment_id": "2223892386",
        "repo_full_name": "apache/spark",
        "pr_number": 51604,
        "pr_file": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala",
        "discussion_id": "2223808261",
        "commented_code": "@@ -364,6 +375,16 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)\n     Some(loadedUI)\n   }\n \n+  private def loadFromFallbackLocation(appId: String, attemptId: Option[String], logPath: String)\n+    : ApplicationInfoWrapper = {\n+    val date = new Date(0)\n+    val info = ApplicationAttemptInfo(attemptId, date, date, date, 0, \"spark\", false, \"unknown\")\n+    addListing(new ApplicationInfoWrapper(\n+      ApplicationInfo(appId, appId, None, None, None, None, List.empty),\n+      List(new AttemptInfoWrapper(info, logPath, 0, Some(1), None, None, None, None))))",
        "comment_created_at": "2025-07-22T21:46:52+00:00",
        "comment_author": "dongjoon-hyun",
        "comment_body": "Thank you so much for the info and your efforts on reviewing this. Take your time.",
        "pr_file_module": null
      },
      {
        "comment_id": "2224016281",
        "repo_full_name": "apache/spark",
        "pr_number": 51604,
        "pr_file": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala",
        "discussion_id": "2223808261",
        "commented_code": "@@ -364,6 +375,16 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)\n     Some(loadedUI)\n   }\n \n+  private def loadFromFallbackLocation(appId: String, attemptId: Option[String], logPath: String)\n+    : ApplicationInfoWrapper = {\n+    val date = new Date(0)\n+    val info = ApplicationAttemptInfo(attemptId, date, date, date, 0, \"spark\", false, \"unknown\")\n+    addListing(new ApplicationInfoWrapper(\n+      ApplicationInfo(appId, appId, None, None, None, None, List.empty),\n+      List(new AttemptInfoWrapper(info, logPath, 0, Some(1), None, None, None, None))))",
        "comment_created_at": "2025-07-22T23:37:19+00:00",
        "comment_author": "thejdeep",
        "comment_body": "@dongjoon-hyun wanted to get your thoughts on https://github.com/apache/spark/pull/51604#issuecomment-3105120700\r\n\r\nThank you!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2055081591",
    "pr_number": 50595,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala",
    "created_at": "2025-04-23T00:36:18+00:00",
    "commented_code": "val otherProviderIds = loadedProviders.keys.filter(_ != storeProviderId).toSeq\n       val providerIdsToUnload = reportActiveStoreInstance(storeProviderId, otherProviderIds)\n-      providerIdsToUnload.foreach(unload(_))\n+      providerIdsToUnload.foreach(id => {\n+        loadedProviders.remove(id).foreach( provider => {\n+          // Trigger maintenance thread to immediately do maintenance on and close the provider.\n+          // Doing maintenance first allows us to do maintenance for a constantly-moving state\n+          // store.\n+          logInfo(log\"Task thread trigger maintenance on \" +",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2055081591",
        "repo_full_name": "apache/spark",
        "pr_number": 50595,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala",
        "discussion_id": "2055081591",
        "commented_code": "@@ -1009,14 +1013,45 @@ object StateStore extends Logging {\n \n       val otherProviderIds = loadedProviders.keys.filter(_ != storeProviderId).toSeq\n       val providerIdsToUnload = reportActiveStoreInstance(storeProviderId, otherProviderIds)\n-      providerIdsToUnload.foreach(unload(_))\n+      providerIdsToUnload.foreach(id => {\n+        loadedProviders.remove(id).foreach( provider => {\n+          // Trigger maintenance thread to immediately do maintenance on and close the provider.\n+          // Doing maintenance first allows us to do maintenance for a constantly-moving state\n+          // store.\n+          logInfo(log\"Task thread trigger maintenance on \" +",
        "comment_created_at": "2025-04-23T00:36:18+00:00",
        "comment_author": "liviazhu",
        "comment_body": "Can you add some more info in this log line, like task ID? Also add that provider was removed from loadedProviders.",
        "pr_file_module": null
      },
      {
        "comment_id": "2058915598",
        "repo_full_name": "apache/spark",
        "pr_number": 50595,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala",
        "discussion_id": "2055081591",
        "commented_code": "@@ -1009,14 +1013,45 @@ object StateStore extends Logging {\n \n       val otherProviderIds = loadedProviders.keys.filter(_ != storeProviderId).toSeq\n       val providerIdsToUnload = reportActiveStoreInstance(storeProviderId, otherProviderIds)\n-      providerIdsToUnload.foreach(unload(_))\n+      providerIdsToUnload.foreach(id => {\n+        loadedProviders.remove(id).foreach( provider => {\n+          // Trigger maintenance thread to immediately do maintenance on and close the provider.\n+          // Doing maintenance first allows us to do maintenance for a constantly-moving state\n+          // store.\n+          logInfo(log\"Task thread trigger maintenance on \" +",
        "comment_created_at": "2025-04-24T17:22:19+00:00",
        "comment_author": "micheal-o",
        "comment_body": "+1, that would help with investigation",
        "pr_file_module": null
      },
      {
        "comment_id": "2058990200",
        "repo_full_name": "apache/spark",
        "pr_number": 50595,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala",
        "discussion_id": "2055081591",
        "commented_code": "@@ -1009,14 +1013,45 @@ object StateStore extends Logging {\n \n       val otherProviderIds = loadedProviders.keys.filter(_ != storeProviderId).toSeq\n       val providerIdsToUnload = reportActiveStoreInstance(storeProviderId, otherProviderIds)\n-      providerIdsToUnload.foreach(unload(_))\n+      providerIdsToUnload.foreach(id => {\n+        loadedProviders.remove(id).foreach( provider => {\n+          // Trigger maintenance thread to immediately do maintenance on and close the provider.\n+          // Doing maintenance first allows us to do maintenance for a constantly-moving state\n+          // store.\n+          logInfo(log\"Task thread trigger maintenance on \" +",
        "comment_created_at": "2025-04-24T18:12:14+00:00",
        "comment_author": "micheal-o",
        "comment_body": "include `trigger maintenance to close provider`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2208435421",
    "pr_number": 51484,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala",
    "created_at": "2025-07-15T19:07:23+00:00",
    "commented_code": "override def stateStoreId: StateStoreId = stateStoreId_\n \n+  override protected def logName: String = s\"${super.logName} ${stateStoreProviderId}\"",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2208435421",
        "repo_full_name": "apache/spark",
        "pr_number": 51484,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala",
        "discussion_id": "2208435421",
        "commented_code": "@@ -370,6 +373,8 @@ private[sql] class HDFSBackedStateStoreProvider extends StateStoreProvider with\n \n   override def stateStoreId: StateStoreId = stateStoreId_\n \n+  override protected def logName: String = s\"${super.logName} ${stateStoreProviderId}\"",
        "comment_created_at": "2025-07-15T19:07:23+00:00",
        "comment_author": "anishshri-db",
        "comment_body": "In the final log, can we make sure to prefix the runId with the field name ?\r\n\r\nSo, have something like `runId = <queryRunId>` ? It seems like we are only printing the value here ?",
        "pr_file_module": null
      },
      {
        "comment_id": "2208585084",
        "repo_full_name": "apache/spark",
        "pr_number": 51484,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala",
        "discussion_id": "2208435421",
        "commented_code": "@@ -370,6 +373,8 @@ private[sql] class HDFSBackedStateStoreProvider extends StateStoreProvider with\n \n   override def stateStoreId: StateStoreId = stateStoreId_\n \n+  override protected def logName: String = s\"${super.logName} ${stateStoreProviderId}\"",
        "comment_created_at": "2025-07-15T20:23:56+00:00",
        "comment_author": "dylanwong250",
        "comment_body": "I added the override to the toString method on StateStoreProviderId to have a similar format as the StateStoreId toString method. The updated log prefix is ```StateStoreProviderId[ storeId=$storeId, queryRunId=$queryRunId ]```. I also updated the example in the PR description.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2096715456",
    "pr_number": 50942,
    "pr_file": "sql/connect/common/src/main/protobuf/spark/connect/pipelines.proto",
    "created_at": "2025-05-20T02:02:10+00:00",
    "commented_code": "+syntax = \"proto3\";\n+\n+package spark.connect;\n+\n+import \"spark/connect/relations.proto\";\n+import \"spark/connect/types.proto\";\n+\n+option java_multiple_files = true;\n+option java_package = \"org.apache.spark.connect.proto\";\n+\n+// Dispatch object for pipelines commands.\n+message PipelineCommand {\n+  oneof command_type {\n+    CreateDataflowGraph create_dataflow_graph = 1;\n+    DefineDataset define_dataset = 2;\n+    DefineFlow define_flow = 3;\n+    DropDataflowGraph drop_dataflow_graph = 4;\n+    StartRun start_run = 5;\n+    StopRun stop_run = 6;\n+    DefineSqlGraphElements define_sql_graph_elements = 7;\n+  }\n+\n+  message DefineSqlGraphElements {\n+    optional string dataflow_graph_id = 1;\n+    optional string sql_file_name = 2;\n+    optional string sql_text = 3;\n+  }\n+\n+  // Request to create a new pipeline.\n+  message CreateDataflowGraph {\n+    // The default catalog.\n+    optional string default_catalog = 1;\n+\n+    // The default database.\n+    optional string default_database = 2;\n+\n+    // Default SQL configurations for all flows in this graph.\n+    map<string, string> sql_conf = 5;\n+\n+    message Response {\n+      // The ID of the created graph.\n+      string dataflow_graph_id = 1;\n+    }\n+  }\n+\n+  // Drops the graph and stops any running attached flows.\n+  message DropDataflowGraph {\n+    // The graph to drop.\n+    string dataflow_graph_id = 1;\n+  }\n+\n+  // Request to define a dataset: a table, a materialized view, or a temporary view.\n+  message DefineDataset {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string dataset_name = 2;\n+\n+    // Table or view.\n+    DatasetType dataset_type = 3;\n+\n+    // Optional comment for the dataset.\n+    optional string comment = 4;\n+\n+    // Optional table properties.\n+    map<string, string> table_properties = 5;\n+\n+    // Optional partition columns for the dataset (if applicable).\n+    repeated string partition_cols = 6;\n+\n+    // Schema for the dataset. If unset, this will be inferred.\n+    optional spark.connect.DataType schema = 7;\n+\n+    // The output table format of the dataset.\n+    optional string format = 8;\n+  }\n+\n+  // Request to define a flow targeting a dataset.\n+  message DefineFlow {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string flow_name = 2;\n+\n+    string target_dataset_name = 3;\n+\n+    // An unresolved relation that defines the dataset's flow.\n+    spark.connect.Relation plan = 4;\n+\n+    // Default SQL configurations set when running this flow.\n+    map<string, string> sql_conf = 5;\n+\n+    // If true, this flow will only be run once per execution.\n+    bool once = 6;\n+  }\n+\n+  // Resolves all datasets and flows and start a pipeline update. Should be called after all\n+  // graph elements are registered.\n+  message StartRun {\n+    // The graph to start.\n+    string dataflow_graph_id = 1;\n+  }\n+  message StopRun {\n+    // The ID of the graph to stop..\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+// Dispatch object for pipelines command results.\n+message PipelineCommandResult {\n+  oneof result_type {\n+    CreateDataflowGraphResult create_dataflow_graph_result = 1;\n+  }\n+  message CreateDataflowGraphResult {\n+    // The ID of the created graph.\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+enum DatasetType {\n+  DATASET_UNSPECIFIED = 0;\n+  // A materialized view dataset which is published to the catalog\n+  MATERIALIZED_VIEW = 1;\n+  // A table which is published to the catalog\n+  TABLE = 2;\n+  // A view which is not published to the catalog\n+  TEMPORARY_VIEW = 3;\n+}\n+\n+// A response containing events emitted during the run of a pipeline.\n+message PipelineEventsResult {\n+  repeated PipelineEvent events = 1;\n+}\n+\n+message PipelineEvent {",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2096715456",
        "repo_full_name": "apache/spark",
        "pr_number": 50942,
        "pr_file": "sql/connect/common/src/main/protobuf/spark/connect/pipelines.proto",
        "discussion_id": "2096715456",
        "commented_code": "@@ -0,0 +1,142 @@\n+syntax = \"proto3\";\n+\n+package spark.connect;\n+\n+import \"spark/connect/relations.proto\";\n+import \"spark/connect/types.proto\";\n+\n+option java_multiple_files = true;\n+option java_package = \"org.apache.spark.connect.proto\";\n+\n+// Dispatch object for pipelines commands.\n+message PipelineCommand {\n+  oneof command_type {\n+    CreateDataflowGraph create_dataflow_graph = 1;\n+    DefineDataset define_dataset = 2;\n+    DefineFlow define_flow = 3;\n+    DropDataflowGraph drop_dataflow_graph = 4;\n+    StartRun start_run = 5;\n+    StopRun stop_run = 6;\n+    DefineSqlGraphElements define_sql_graph_elements = 7;\n+  }\n+\n+  message DefineSqlGraphElements {\n+    optional string dataflow_graph_id = 1;\n+    optional string sql_file_name = 2;\n+    optional string sql_text = 3;\n+  }\n+\n+  // Request to create a new pipeline.\n+  message CreateDataflowGraph {\n+    // The default catalog.\n+    optional string default_catalog = 1;\n+\n+    // The default database.\n+    optional string default_database = 2;\n+\n+    // Default SQL configurations for all flows in this graph.\n+    map<string, string> sql_conf = 5;\n+\n+    message Response {\n+      // The ID of the created graph.\n+      string dataflow_graph_id = 1;\n+    }\n+  }\n+\n+  // Drops the graph and stops any running attached flows.\n+  message DropDataflowGraph {\n+    // The graph to drop.\n+    string dataflow_graph_id = 1;\n+  }\n+\n+  // Request to define a dataset: a table, a materialized view, or a temporary view.\n+  message DefineDataset {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string dataset_name = 2;\n+\n+    // Table or view.\n+    DatasetType dataset_type = 3;\n+\n+    // Optional comment for the dataset.\n+    optional string comment = 4;\n+\n+    // Optional table properties.\n+    map<string, string> table_properties = 5;\n+\n+    // Optional partition columns for the dataset (if applicable).\n+    repeated string partition_cols = 6;\n+\n+    // Schema for the dataset. If unset, this will be inferred.\n+    optional spark.connect.DataType schema = 7;\n+\n+    // The output table format of the dataset.\n+    optional string format = 8;\n+  }\n+\n+  // Request to define a flow targeting a dataset.\n+  message DefineFlow {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string flow_name = 2;\n+\n+    string target_dataset_name = 3;\n+\n+    // An unresolved relation that defines the dataset's flow.\n+    spark.connect.Relation plan = 4;\n+\n+    // Default SQL configurations set when running this flow.\n+    map<string, string> sql_conf = 5;\n+\n+    // If true, this flow will only be run once per execution.\n+    bool once = 6;\n+  }\n+\n+  // Resolves all datasets and flows and start a pipeline update. Should be called after all\n+  // graph elements are registered.\n+  message StartRun {\n+    // The graph to start.\n+    string dataflow_graph_id = 1;\n+  }\n+  message StopRun {\n+    // The ID of the graph to stop..\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+// Dispatch object for pipelines command results.\n+message PipelineCommandResult {\n+  oneof result_type {\n+    CreateDataflowGraphResult create_dataflow_graph_result = 1;\n+  }\n+  message CreateDataflowGraphResult {\n+    // The ID of the created graph.\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+enum DatasetType {\n+  DATASET_UNSPECIFIED = 0;\n+  // A materialized view dataset which is published to the catalog\n+  MATERIALIZED_VIEW = 1;\n+  // A table which is published to the catalog\n+  TABLE = 2;\n+  // A view which is not published to the catalog\n+  TEMPORARY_VIEW = 3;\n+}\n+\n+// A response containing events emitted during the run of a pipeline.\n+message PipelineEventsResult {\n+  repeated PipelineEvent events = 1;\n+}\n+\n+message PipelineEvent {",
        "comment_created_at": "2025-05-20T02:02:10+00:00",
        "comment_author": "hvanhovell",
        "comment_body": "Is this also supposed to include errors? If so, it'd be nice to understand what has failed... In that case adding add flow/dataset name would be nice.",
        "pr_file_module": null
      },
      {
        "comment_id": "2098680921",
        "repo_full_name": "apache/spark",
        "pr_number": 50942,
        "pr_file": "sql/connect/common/src/main/protobuf/spark/connect/pipelines.proto",
        "discussion_id": "2096715456",
        "commented_code": "@@ -0,0 +1,142 @@\n+syntax = \"proto3\";\n+\n+package spark.connect;\n+\n+import \"spark/connect/relations.proto\";\n+import \"spark/connect/types.proto\";\n+\n+option java_multiple_files = true;\n+option java_package = \"org.apache.spark.connect.proto\";\n+\n+// Dispatch object for pipelines commands.\n+message PipelineCommand {\n+  oneof command_type {\n+    CreateDataflowGraph create_dataflow_graph = 1;\n+    DefineDataset define_dataset = 2;\n+    DefineFlow define_flow = 3;\n+    DropDataflowGraph drop_dataflow_graph = 4;\n+    StartRun start_run = 5;\n+    StopRun stop_run = 6;\n+    DefineSqlGraphElements define_sql_graph_elements = 7;\n+  }\n+\n+  message DefineSqlGraphElements {\n+    optional string dataflow_graph_id = 1;\n+    optional string sql_file_name = 2;\n+    optional string sql_text = 3;\n+  }\n+\n+  // Request to create a new pipeline.\n+  message CreateDataflowGraph {\n+    // The default catalog.\n+    optional string default_catalog = 1;\n+\n+    // The default database.\n+    optional string default_database = 2;\n+\n+    // Default SQL configurations for all flows in this graph.\n+    map<string, string> sql_conf = 5;\n+\n+    message Response {\n+      // The ID of the created graph.\n+      string dataflow_graph_id = 1;\n+    }\n+  }\n+\n+  // Drops the graph and stops any running attached flows.\n+  message DropDataflowGraph {\n+    // The graph to drop.\n+    string dataflow_graph_id = 1;\n+  }\n+\n+  // Request to define a dataset: a table, a materialized view, or a temporary view.\n+  message DefineDataset {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string dataset_name = 2;\n+\n+    // Table or view.\n+    DatasetType dataset_type = 3;\n+\n+    // Optional comment for the dataset.\n+    optional string comment = 4;\n+\n+    // Optional table properties.\n+    map<string, string> table_properties = 5;\n+\n+    // Optional partition columns for the dataset (if applicable).\n+    repeated string partition_cols = 6;\n+\n+    // Schema for the dataset. If unset, this will be inferred.\n+    optional spark.connect.DataType schema = 7;\n+\n+    // The output table format of the dataset.\n+    optional string format = 8;\n+  }\n+\n+  // Request to define a flow targeting a dataset.\n+  message DefineFlow {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string flow_name = 2;\n+\n+    string target_dataset_name = 3;\n+\n+    // An unresolved relation that defines the dataset's flow.\n+    spark.connect.Relation plan = 4;\n+\n+    // Default SQL configurations set when running this flow.\n+    map<string, string> sql_conf = 5;\n+\n+    // If true, this flow will only be run once per execution.\n+    bool once = 6;\n+  }\n+\n+  // Resolves all datasets and flows and start a pipeline update. Should be called after all\n+  // graph elements are registered.\n+  message StartRun {\n+    // The graph to start.\n+    string dataflow_graph_id = 1;\n+  }\n+  message StopRun {\n+    // The ID of the graph to stop..\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+// Dispatch object for pipelines command results.\n+message PipelineCommandResult {\n+  oneof result_type {\n+    CreateDataflowGraphResult create_dataflow_graph_result = 1;\n+  }\n+  message CreateDataflowGraphResult {\n+    // The ID of the created graph.\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+enum DatasetType {\n+  DATASET_UNSPECIFIED = 0;\n+  // A materialized view dataset which is published to the catalog\n+  MATERIALIZED_VIEW = 1;\n+  // A table which is published to the catalog\n+  TABLE = 2;\n+  // A view which is not published to the catalog\n+  TEMPORARY_VIEW = 3;\n+}\n+\n+// A response containing events emitted during the run of a pipeline.\n+message PipelineEventsResult {\n+  repeated PipelineEvent events = 1;\n+}\n+\n+message PipelineEvent {",
        "comment_created_at": "2025-05-20T19:07:49+00:00",
        "comment_author": "aakash-db",
        "comment_body": "Yeah, I can the see the value in adding dataset and flow name. But two things:\r\n1. OTOH, we wanted to keep PipelineEvent's as a generic event bus rather than a structured logging format.\r\n2. It's possible an error happens that isn't scoped to a dataset/flow, making this field unpredictably empty.\r\n\r\nBut at the very least, the dataset/flow name will be in the error message.",
        "pr_file_module": null
      },
      {
        "comment_id": "2100524659",
        "repo_full_name": "apache/spark",
        "pr_number": 50942,
        "pr_file": "sql/connect/common/src/main/protobuf/spark/connect/pipelines.proto",
        "discussion_id": "2096715456",
        "commented_code": "@@ -0,0 +1,142 @@\n+syntax = \"proto3\";\n+\n+package spark.connect;\n+\n+import \"spark/connect/relations.proto\";\n+import \"spark/connect/types.proto\";\n+\n+option java_multiple_files = true;\n+option java_package = \"org.apache.spark.connect.proto\";\n+\n+// Dispatch object for pipelines commands.\n+message PipelineCommand {\n+  oneof command_type {\n+    CreateDataflowGraph create_dataflow_graph = 1;\n+    DefineDataset define_dataset = 2;\n+    DefineFlow define_flow = 3;\n+    DropDataflowGraph drop_dataflow_graph = 4;\n+    StartRun start_run = 5;\n+    StopRun stop_run = 6;\n+    DefineSqlGraphElements define_sql_graph_elements = 7;\n+  }\n+\n+  message DefineSqlGraphElements {\n+    optional string dataflow_graph_id = 1;\n+    optional string sql_file_name = 2;\n+    optional string sql_text = 3;\n+  }\n+\n+  // Request to create a new pipeline.\n+  message CreateDataflowGraph {\n+    // The default catalog.\n+    optional string default_catalog = 1;\n+\n+    // The default database.\n+    optional string default_database = 2;\n+\n+    // Default SQL configurations for all flows in this graph.\n+    map<string, string> sql_conf = 5;\n+\n+    message Response {\n+      // The ID of the created graph.\n+      string dataflow_graph_id = 1;\n+    }\n+  }\n+\n+  // Drops the graph and stops any running attached flows.\n+  message DropDataflowGraph {\n+    // The graph to drop.\n+    string dataflow_graph_id = 1;\n+  }\n+\n+  // Request to define a dataset: a table, a materialized view, or a temporary view.\n+  message DefineDataset {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string dataset_name = 2;\n+\n+    // Table or view.\n+    DatasetType dataset_type = 3;\n+\n+    // Optional comment for the dataset.\n+    optional string comment = 4;\n+\n+    // Optional table properties.\n+    map<string, string> table_properties = 5;\n+\n+    // Optional partition columns for the dataset (if applicable).\n+    repeated string partition_cols = 6;\n+\n+    // Schema for the dataset. If unset, this will be inferred.\n+    optional spark.connect.DataType schema = 7;\n+\n+    // The output table format of the dataset.\n+    optional string format = 8;\n+  }\n+\n+  // Request to define a flow targeting a dataset.\n+  message DefineFlow {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string flow_name = 2;\n+\n+    string target_dataset_name = 3;\n+\n+    // An unresolved relation that defines the dataset's flow.\n+    spark.connect.Relation plan = 4;\n+\n+    // Default SQL configurations set when running this flow.\n+    map<string, string> sql_conf = 5;\n+\n+    // If true, this flow will only be run once per execution.\n+    bool once = 6;\n+  }\n+\n+  // Resolves all datasets and flows and start a pipeline update. Should be called after all\n+  // graph elements are registered.\n+  message StartRun {\n+    // The graph to start.\n+    string dataflow_graph_id = 1;\n+  }\n+  message StopRun {\n+    // The ID of the graph to stop..\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+// Dispatch object for pipelines command results.\n+message PipelineCommandResult {\n+  oneof result_type {\n+    CreateDataflowGraphResult create_dataflow_graph_result = 1;\n+  }\n+  message CreateDataflowGraphResult {\n+    // The ID of the created graph.\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+enum DatasetType {\n+  DATASET_UNSPECIFIED = 0;\n+  // A materialized view dataset which is published to the catalog\n+  MATERIALIZED_VIEW = 1;\n+  // A table which is published to the catalog\n+  TABLE = 2;\n+  // A view which is not published to the catalog\n+  TEMPORARY_VIEW = 3;\n+}\n+\n+// A response containing events emitted during the run of a pipeline.\n+message PipelineEventsResult {\n+  repeated PipelineEvent events = 1;\n+}\n+\n+message PipelineEvent {",
        "comment_created_at": "2025-05-21T15:01:08+00:00",
        "comment_author": "sryza",
        "comment_body": "To add on to what @aakash-db said, our main use case for these events is to print out to the console, and the string messages will include all the context that's needed for that. Once we have a use case that involves consuming the dataset/flow name programmatically, I'd be supportive of adding more structure to this.",
        "pr_file_module": null
      },
      {
        "comment_id": "2107826240",
        "repo_full_name": "apache/spark",
        "pr_number": 50942,
        "pr_file": "sql/connect/common/src/main/protobuf/spark/connect/pipelines.proto",
        "discussion_id": "2096715456",
        "commented_code": "@@ -0,0 +1,142 @@\n+syntax = \"proto3\";\n+\n+package spark.connect;\n+\n+import \"spark/connect/relations.proto\";\n+import \"spark/connect/types.proto\";\n+\n+option java_multiple_files = true;\n+option java_package = \"org.apache.spark.connect.proto\";\n+\n+// Dispatch object for pipelines commands.\n+message PipelineCommand {\n+  oneof command_type {\n+    CreateDataflowGraph create_dataflow_graph = 1;\n+    DefineDataset define_dataset = 2;\n+    DefineFlow define_flow = 3;\n+    DropDataflowGraph drop_dataflow_graph = 4;\n+    StartRun start_run = 5;\n+    StopRun stop_run = 6;\n+    DefineSqlGraphElements define_sql_graph_elements = 7;\n+  }\n+\n+  message DefineSqlGraphElements {\n+    optional string dataflow_graph_id = 1;\n+    optional string sql_file_name = 2;\n+    optional string sql_text = 3;\n+  }\n+\n+  // Request to create a new pipeline.\n+  message CreateDataflowGraph {\n+    // The default catalog.\n+    optional string default_catalog = 1;\n+\n+    // The default database.\n+    optional string default_database = 2;\n+\n+    // Default SQL configurations for all flows in this graph.\n+    map<string, string> sql_conf = 5;\n+\n+    message Response {\n+      // The ID of the created graph.\n+      string dataflow_graph_id = 1;\n+    }\n+  }\n+\n+  // Drops the graph and stops any running attached flows.\n+  message DropDataflowGraph {\n+    // The graph to drop.\n+    string dataflow_graph_id = 1;\n+  }\n+\n+  // Request to define a dataset: a table, a materialized view, or a temporary view.\n+  message DefineDataset {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string dataset_name = 2;\n+\n+    // Table or view.\n+    DatasetType dataset_type = 3;\n+\n+    // Optional comment for the dataset.\n+    optional string comment = 4;\n+\n+    // Optional table properties.\n+    map<string, string> table_properties = 5;\n+\n+    // Optional partition columns for the dataset (if applicable).\n+    repeated string partition_cols = 6;\n+\n+    // Schema for the dataset. If unset, this will be inferred.\n+    optional spark.connect.DataType schema = 7;\n+\n+    // The output table format of the dataset.\n+    optional string format = 8;\n+  }\n+\n+  // Request to define a flow targeting a dataset.\n+  message DefineFlow {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string flow_name = 2;\n+\n+    string target_dataset_name = 3;\n+\n+    // An unresolved relation that defines the dataset's flow.\n+    spark.connect.Relation plan = 4;\n+\n+    // Default SQL configurations set when running this flow.\n+    map<string, string> sql_conf = 5;\n+\n+    // If true, this flow will only be run once per execution.\n+    bool once = 6;\n+  }\n+\n+  // Resolves all datasets and flows and start a pipeline update. Should be called after all\n+  // graph elements are registered.\n+  message StartRun {\n+    // The graph to start.\n+    string dataflow_graph_id = 1;\n+  }\n+  message StopRun {\n+    // The ID of the graph to stop..\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+// Dispatch object for pipelines command results.\n+message PipelineCommandResult {\n+  oneof result_type {\n+    CreateDataflowGraphResult create_dataflow_graph_result = 1;\n+  }\n+  message CreateDataflowGraphResult {\n+    // The ID of the created graph.\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+enum DatasetType {\n+  DATASET_UNSPECIFIED = 0;\n+  // A materialized view dataset which is published to the catalog\n+  MATERIALIZED_VIEW = 1;\n+  // A table which is published to the catalog\n+  TABLE = 2;\n+  // A view which is not published to the catalog\n+  TEMPORARY_VIEW = 3;\n+}\n+\n+// A response containing events emitted during the run of a pipeline.\n+message PipelineEventsResult {\n+  repeated PipelineEvent events = 1;\n+}\n+\n+message PipelineEvent {",
        "comment_created_at": "2025-05-26T20:24:39+00:00",
        "comment_author": "grundprinzip",
        "comment_body": "Btw, errors should flow the regular way through the exception process and the error details. If we were to do it differently it would just create issues later. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2109447607",
        "repo_full_name": "apache/spark",
        "pr_number": 50942,
        "pr_file": "sql/connect/common/src/main/protobuf/spark/connect/pipelines.proto",
        "discussion_id": "2096715456",
        "commented_code": "@@ -0,0 +1,142 @@\n+syntax = \"proto3\";\n+\n+package spark.connect;\n+\n+import \"spark/connect/relations.proto\";\n+import \"spark/connect/types.proto\";\n+\n+option java_multiple_files = true;\n+option java_package = \"org.apache.spark.connect.proto\";\n+\n+// Dispatch object for pipelines commands.\n+message PipelineCommand {\n+  oneof command_type {\n+    CreateDataflowGraph create_dataflow_graph = 1;\n+    DefineDataset define_dataset = 2;\n+    DefineFlow define_flow = 3;\n+    DropDataflowGraph drop_dataflow_graph = 4;\n+    StartRun start_run = 5;\n+    StopRun stop_run = 6;\n+    DefineSqlGraphElements define_sql_graph_elements = 7;\n+  }\n+\n+  message DefineSqlGraphElements {\n+    optional string dataflow_graph_id = 1;\n+    optional string sql_file_name = 2;\n+    optional string sql_text = 3;\n+  }\n+\n+  // Request to create a new pipeline.\n+  message CreateDataflowGraph {\n+    // The default catalog.\n+    optional string default_catalog = 1;\n+\n+    // The default database.\n+    optional string default_database = 2;\n+\n+    // Default SQL configurations for all flows in this graph.\n+    map<string, string> sql_conf = 5;\n+\n+    message Response {\n+      // The ID of the created graph.\n+      string dataflow_graph_id = 1;\n+    }\n+  }\n+\n+  // Drops the graph and stops any running attached flows.\n+  message DropDataflowGraph {\n+    // The graph to drop.\n+    string dataflow_graph_id = 1;\n+  }\n+\n+  // Request to define a dataset: a table, a materialized view, or a temporary view.\n+  message DefineDataset {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string dataset_name = 2;\n+\n+    // Table or view.\n+    DatasetType dataset_type = 3;\n+\n+    // Optional comment for the dataset.\n+    optional string comment = 4;\n+\n+    // Optional table properties.\n+    map<string, string> table_properties = 5;\n+\n+    // Optional partition columns for the dataset (if applicable).\n+    repeated string partition_cols = 6;\n+\n+    // Schema for the dataset. If unset, this will be inferred.\n+    optional spark.connect.DataType schema = 7;\n+\n+    // The output table format of the dataset.\n+    optional string format = 8;\n+  }\n+\n+  // Request to define a flow targeting a dataset.\n+  message DefineFlow {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string flow_name = 2;\n+\n+    string target_dataset_name = 3;\n+\n+    // An unresolved relation that defines the dataset's flow.\n+    spark.connect.Relation plan = 4;\n+\n+    // Default SQL configurations set when running this flow.\n+    map<string, string> sql_conf = 5;\n+\n+    // If true, this flow will only be run once per execution.\n+    bool once = 6;\n+  }\n+\n+  // Resolves all datasets and flows and start a pipeline update. Should be called after all\n+  // graph elements are registered.\n+  message StartRun {\n+    // The graph to start.\n+    string dataflow_graph_id = 1;\n+  }\n+  message StopRun {\n+    // The ID of the graph to stop..\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+// Dispatch object for pipelines command results.\n+message PipelineCommandResult {\n+  oneof result_type {\n+    CreateDataflowGraphResult create_dataflow_graph_result = 1;\n+  }\n+  message CreateDataflowGraphResult {\n+    // The ID of the created graph.\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+enum DatasetType {\n+  DATASET_UNSPECIFIED = 0;\n+  // A materialized view dataset which is published to the catalog\n+  MATERIALIZED_VIEW = 1;\n+  // A table which is published to the catalog\n+  TABLE = 2;\n+  // A view which is not published to the catalog\n+  TEMPORARY_VIEW = 3;\n+}\n+\n+// A response containing events emitted during the run of a pipeline.\n+message PipelineEventsResult {\n+  repeated PipelineEvent events = 1;\n+}\n+\n+message PipelineEvent {",
        "comment_created_at": "2025-05-27T15:01:47+00:00",
        "comment_author": "hvanhovell",
        "comment_body": "@grundprinzip I actually agree with you. If the pipeline fails we should fail in the normal way. However, that failure can originate from multiple places. As I user I would like to able to figure out what failed. We could embed that failure information in these events.",
        "pr_file_module": null
      }
    ]
  }
]