[
  {
    "discussion_id": "2230793940",
    "pr_number": 51639,
    "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
    "created_at": "2025-07-25T10:56:06+00:00",
    "commented_code": "PosParameter(ctx.QUESTION().getSymbol.getStartIndex)\n   }\n \n+  private object SqlScriptingVariableContext {\n+    private val forbiddenVariableNames: immutable.Set[Regex] =\n+      immutable.Set(\"system\".r, \"session\".r)",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2230793940",
        "repo_full_name": "apache/spark",
        "pr_number": 51639,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
        "discussion_id": "2230793940",
        "commented_code": "@@ -6372,6 +6381,22 @@ class AstBuilder extends DataTypeAstBuilder\n     PosParameter(ctx.QUESTION().getSymbol.getStartIndex)\n   }\n \n+  private object SqlScriptingVariableContext {\n+    private val forbiddenVariableNames: immutable.Set[Regex] =\n+      immutable.Set(\"system\".r, \"session\".r)",
        "comment_created_at": "2025-07-25T10:56:06+00:00",
        "comment_author": "dusantism-db",
        "comment_body": "Since we dont use the regex functionality here, rather we just check the string literals, why not have this as a `Set[String]` and just use `contains`?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2070569934",
    "pr_number": 50230,
    "pr_file": "core/src/main/java/org/apache/spark/shuffle/checksum/RowBasedChecksum.scala",
    "created_at": "2025-05-01T17:41:23+00:00",
    "commented_code": "+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.checksum\n+\n+import java.io.ObjectOutputStream\n+import java.util.zip.Checksum\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper\n+import org.apache.spark.util.MyByteArrayOutputStream\n+\n+/**\n+ * A class for computing checksum for input (key, value) pairs. The checksum is independent of\n+ * the order of the input (key, value) pairs. It is done by computing a checksum for each row\n+ * first, and then computing the XOR for all the row checksums.\n+ */\n+abstract class RowBasedChecksum() extends Serializable with Logging {\n+  private var hasError: Boolean = false\n+  private var checksumValue: Long = 0\n+  /** Returns the checksum value computed. Tt returns the default checksum value (0) if there\n+   * are any errors encountered during the checksum computation.\n+   */\n+  def getValue: Long = {\n+    if (!hasError) checksumValue else 0\n+  }\n+\n+  /** Updates the row-based checksum with the given (key, value) pair */\n+  def update(key: Any, value: Any): Unit = {\n+    if (!hasError) {\n+      try {\n+        val rowChecksumValue = calculateRowChecksum(key, value)\n+        checksumValue = checksumValue ^ rowChecksumValue",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2070569934",
        "repo_full_name": "apache/spark",
        "pr_number": 50230,
        "pr_file": "core/src/main/java/org/apache/spark/shuffle/checksum/RowBasedChecksum.scala",
        "discussion_id": "2070569934",
        "commented_code": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.checksum\n+\n+import java.io.ObjectOutputStream\n+import java.util.zip.Checksum\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper\n+import org.apache.spark.util.MyByteArrayOutputStream\n+\n+/**\n+ * A class for computing checksum for input (key, value) pairs. The checksum is independent of\n+ * the order of the input (key, value) pairs. It is done by computing a checksum for each row\n+ * first, and then computing the XOR for all the row checksums.\n+ */\n+abstract class RowBasedChecksum() extends Serializable with Logging {\n+  private var hasError: Boolean = false\n+  private var checksumValue: Long = 0\n+  /** Returns the checksum value computed. Tt returns the default checksum value (0) if there\n+   * are any errors encountered during the checksum computation.\n+   */\n+  def getValue: Long = {\n+    if (!hasError) checksumValue else 0\n+  }\n+\n+  /** Updates the row-based checksum with the given (key, value) pair */\n+  def update(key: Any, value: Any): Unit = {\n+    if (!hasError) {\n+      try {\n+        val rowChecksumValue = calculateRowChecksum(key, value)\n+        checksumValue = checksumValue ^ rowChecksumValue",
        "comment_created_at": "2025-05-01T17:41:23+00:00",
        "comment_author": "peter-toth",
        "comment_body": "XOR has problems when the same (key, value) pair is used multiple times. Should we track the number of pairs as well?",
        "pr_file_module": null
      },
      {
        "comment_id": "2070893252",
        "repo_full_name": "apache/spark",
        "pr_number": 50230,
        "pr_file": "core/src/main/java/org/apache/spark/shuffle/checksum/RowBasedChecksum.scala",
        "discussion_id": "2070569934",
        "commented_code": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.checksum\n+\n+import java.io.ObjectOutputStream\n+import java.util.zip.Checksum\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper\n+import org.apache.spark.util.MyByteArrayOutputStream\n+\n+/**\n+ * A class for computing checksum for input (key, value) pairs. The checksum is independent of\n+ * the order of the input (key, value) pairs. It is done by computing a checksum for each row\n+ * first, and then computing the XOR for all the row checksums.\n+ */\n+abstract class RowBasedChecksum() extends Serializable with Logging {\n+  private var hasError: Boolean = false\n+  private var checksumValue: Long = 0\n+  /** Returns the checksum value computed. Tt returns the default checksum value (0) if there\n+   * are any errors encountered during the checksum computation.\n+   */\n+  def getValue: Long = {\n+    if (!hasError) checksumValue else 0\n+  }\n+\n+  /** Updates the row-based checksum with the given (key, value) pair */\n+  def update(key: Any, value: Any): Unit = {\n+    if (!hasError) {\n+      try {\n+        val rowChecksumValue = calculateRowChecksum(key, value)\n+        checksumValue = checksumValue ^ rowChecksumValue",
        "comment_created_at": "2025-05-01T22:40:51+00:00",
        "comment_author": "attilapiros",
        "comment_body": "I think this is a good point but hard to compute as this will be a bit more stateful. \n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2070895177",
        "repo_full_name": "apache/spark",
        "pr_number": 50230,
        "pr_file": "core/src/main/java/org/apache/spark/shuffle/checksum/RowBasedChecksum.scala",
        "discussion_id": "2070569934",
        "commented_code": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.checksum\n+\n+import java.io.ObjectOutputStream\n+import java.util.zip.Checksum\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper\n+import org.apache.spark.util.MyByteArrayOutputStream\n+\n+/**\n+ * A class for computing checksum for input (key, value) pairs. The checksum is independent of\n+ * the order of the input (key, value) pairs. It is done by computing a checksum for each row\n+ * first, and then computing the XOR for all the row checksums.\n+ */\n+abstract class RowBasedChecksum() extends Serializable with Logging {\n+  private var hasError: Boolean = false\n+  private var checksumValue: Long = 0\n+  /** Returns the checksum value computed. Tt returns the default checksum value (0) if there\n+   * are any errors encountered during the checksum computation.\n+   */\n+  def getValue: Long = {\n+    if (!hasError) checksumValue else 0\n+  }\n+\n+  /** Updates the row-based checksum with the given (key, value) pair */\n+  def update(key: Any, value: Any): Unit = {\n+    if (!hasError) {\n+      try {\n+        val rowChecksumValue = calculateRowChecksum(key, value)\n+        checksumValue = checksumValue ^ rowChecksumValue",
        "comment_created_at": "2025-05-01T22:43:59+00:00",
        "comment_author": "attilapiros",
        "comment_body": "What about in addition to the bitwise XOR (currently `checksumValue `) calculating a SUM as well and when the `getValue` is called combine those two into one number with an extra XOR (or just add together multiplying one with prime number)? \r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2071271082",
        "repo_full_name": "apache/spark",
        "pr_number": 50230,
        "pr_file": "core/src/main/java/org/apache/spark/shuffle/checksum/RowBasedChecksum.scala",
        "discussion_id": "2070569934",
        "commented_code": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.checksum\n+\n+import java.io.ObjectOutputStream\n+import java.util.zip.Checksum\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper\n+import org.apache.spark.util.MyByteArrayOutputStream\n+\n+/**\n+ * A class for computing checksum for input (key, value) pairs. The checksum is independent of\n+ * the order of the input (key, value) pairs. It is done by computing a checksum for each row\n+ * first, and then computing the XOR for all the row checksums.\n+ */\n+abstract class RowBasedChecksum() extends Serializable with Logging {\n+  private var hasError: Boolean = false\n+  private var checksumValue: Long = 0\n+  /** Returns the checksum value computed. Tt returns the default checksum value (0) if there\n+   * are any errors encountered during the checksum computation.\n+   */\n+  def getValue: Long = {\n+    if (!hasError) checksumValue else 0\n+  }\n+\n+  /** Updates the row-based checksum with the given (key, value) pair */\n+  def update(key: Any, value: Any): Unit = {\n+    if (!hasError) {\n+      try {\n+        val rowChecksumValue = calculateRowChecksum(key, value)\n+        checksumValue = checksumValue ^ rowChecksumValue",
        "comment_created_at": "2025-05-02T08:21:55+00:00",
        "comment_author": "peter-toth",
        "comment_body": "Yeah, that's what I was referring to. ~But combining the number of pairs (count, not the sum) into the final checksum should be fine.~\r\nUpdate: No, combining just the count of pairs into the final checksum still has problems with duplicates.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2071987945",
        "repo_full_name": "apache/spark",
        "pr_number": 50230,
        "pr_file": "core/src/main/java/org/apache/spark/shuffle/checksum/RowBasedChecksum.scala",
        "discussion_id": "2070569934",
        "commented_code": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.checksum\n+\n+import java.io.ObjectOutputStream\n+import java.util.zip.Checksum\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper\n+import org.apache.spark.util.MyByteArrayOutputStream\n+\n+/**\n+ * A class for computing checksum for input (key, value) pairs. The checksum is independent of\n+ * the order of the input (key, value) pairs. It is done by computing a checksum for each row\n+ * first, and then computing the XOR for all the row checksums.\n+ */\n+abstract class RowBasedChecksum() extends Serializable with Logging {\n+  private var hasError: Boolean = false\n+  private var checksumValue: Long = 0\n+  /** Returns the checksum value computed. Tt returns the default checksum value (0) if there\n+   * are any errors encountered during the checksum computation.\n+   */\n+  def getValue: Long = {\n+    if (!hasError) checksumValue else 0\n+  }\n+\n+  /** Updates the row-based checksum with the given (key, value) pair */\n+  def update(key: Any, value: Any): Unit = {\n+    if (!hasError) {\n+      try {\n+        val rowChecksumValue = calculateRowChecksum(key, value)\n+        checksumValue = checksumValue ^ rowChecksumValue",
        "comment_created_at": "2025-05-02T18:20:07+00:00",
        "comment_author": "mridulm",
        "comment_body": "Perhaps something like this might work ? https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function\r\n\r\nIt will be more expensive than xor, but should handle order and duplication.",
        "pr_file_module": null
      },
      {
        "comment_id": "2072037837",
        "repo_full_name": "apache/spark",
        "pr_number": 50230,
        "pr_file": "core/src/main/java/org/apache/spark/shuffle/checksum/RowBasedChecksum.scala",
        "discussion_id": "2070569934",
        "commented_code": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.checksum\n+\n+import java.io.ObjectOutputStream\n+import java.util.zip.Checksum\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper\n+import org.apache.spark.util.MyByteArrayOutputStream\n+\n+/**\n+ * A class for computing checksum for input (key, value) pairs. The checksum is independent of\n+ * the order of the input (key, value) pairs. It is done by computing a checksum for each row\n+ * first, and then computing the XOR for all the row checksums.\n+ */\n+abstract class RowBasedChecksum() extends Serializable with Logging {\n+  private var hasError: Boolean = false\n+  private var checksumValue: Long = 0\n+  /** Returns the checksum value computed. Tt returns the default checksum value (0) if there\n+   * are any errors encountered during the checksum computation.\n+   */\n+  def getValue: Long = {\n+    if (!hasError) checksumValue else 0\n+  }\n+\n+  /** Updates the row-based checksum with the given (key, value) pair */\n+  def update(key: Any, value: Any): Unit = {\n+    if (!hasError) {\n+      try {\n+        val rowChecksumValue = calculateRowChecksum(key, value)\n+        checksumValue = checksumValue ^ rowChecksumValue",
        "comment_created_at": "2025-05-02T19:07:20+00:00",
        "comment_author": "attilapiros",
        "comment_body": "@mridulm I think what we need is order insensitivity (within one partition the order of rows should not matter), fnv as I see is sensitive for the order",
        "pr_file_module": null
      },
      {
        "comment_id": "2072065116",
        "repo_full_name": "apache/spark",
        "pr_number": 50230,
        "pr_file": "core/src/main/java/org/apache/spark/shuffle/checksum/RowBasedChecksum.scala",
        "discussion_id": "2070569934",
        "commented_code": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.shuffle.checksum\n+\n+import java.io.ObjectOutputStream\n+import java.util.zip.Checksum\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper\n+import org.apache.spark.util.MyByteArrayOutputStream\n+\n+/**\n+ * A class for computing checksum for input (key, value) pairs. The checksum is independent of\n+ * the order of the input (key, value) pairs. It is done by computing a checksum for each row\n+ * first, and then computing the XOR for all the row checksums.\n+ */\n+abstract class RowBasedChecksum() extends Serializable with Logging {\n+  private var hasError: Boolean = false\n+  private var checksumValue: Long = 0\n+  /** Returns the checksum value computed. Tt returns the default checksum value (0) if there\n+   * are any errors encountered during the checksum computation.\n+   */\n+  def getValue: Long = {\n+    if (!hasError) checksumValue else 0\n+  }\n+\n+  /** Updates the row-based checksum with the given (key, value) pair */\n+  def update(key: Any, value: Any): Unit = {\n+    if (!hasError) {\n+      try {\n+        val rowChecksumValue = calculateRowChecksum(key, value)\n+        checksumValue = checksumValue ^ rowChecksumValue",
        "comment_created_at": "2025-05-02T19:34:45+00:00",
        "comment_author": "mridulm",
        "comment_body": "You are right, order sensitivity matter at beginning of task, not at end !\n\nSum + xor or sum + xor + multiplication with some xor folding to generate final hash might be cheap.\nCan't think of other alternatives which might work well and yet is reasonably robust to duplication ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2070945270",
    "pr_number": 50230,
    "pr_file": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRowChecksum.scala",
    "created_at": "2025-05-02T00:03:13+00:00",
    "commented_code": "+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions\n+\n+import org.apache.spark.shuffle.checksum.RowBasedChecksum\n+\n+/**\n+ * A concrete implementation of RowBasedChecksum for computing checksum for UnsafeRow.\n+ * The checksum for each row is computed by first casting or converting the baseObject\n+ * in the UnsafeRow to a byte array, and then computing the checksum for the byte array.\n+ *\n+ * Note that the input key is ignored in the checksum computation. As the Spark shuffle\n+ * currently uses a PartitionIdPassthrough partitioner, the keys are already the partition\n+ * IDs for sending the data, and they are the same for all rows in the same partition.\n+ */\n+class UnsafeRowChecksum extends RowBasedChecksum() {\n+\n+  override protected def calculateRowChecksum(key: Any, value: Any): Long = {\n+    assert(\n+      value.isInstanceOf[UnsafeRow],\n+      \"Expecting UnsafeRow but got \" + value.getClass.getName)\n+\n+    // Casts or converts the baseObject in UnsafeRow to a byte array.\n+    val unsafeRow = value.asInstanceOf[UnsafeRow]\n+    XXH64.hashUnsafeBytes(\n+      unsafeRow.getBaseObject,\n+      unsafeRow.getBaseOffset,\n+      unsafeRow.getSizeInBytes,\n+      0\n+    )\n+  }\n+}\n+\n+object UnsafeRowChecksum {\n+  def createUnsafeRowChecksums(numPartitions: Int): Array[RowBasedChecksum] = {\n+    val rowBasedChecksums: Array[RowBasedChecksum] = new Array[RowBasedChecksum](numPartitions)\n+    for (i <- 0 until numPartitions) {\n+      rowBasedChecksums(i) = new UnsafeRowChecksum()\n+    }\n+    rowBasedChecksums",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2070945270",
        "repo_full_name": "apache/spark",
        "pr_number": 50230,
        "pr_file": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRowChecksum.scala",
        "discussion_id": "2070945270",
        "commented_code": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.expressions\n+\n+import org.apache.spark.shuffle.checksum.RowBasedChecksum\n+\n+/**\n+ * A concrete implementation of RowBasedChecksum for computing checksum for UnsafeRow.\n+ * The checksum for each row is computed by first casting or converting the baseObject\n+ * in the UnsafeRow to a byte array, and then computing the checksum for the byte array.\n+ *\n+ * Note that the input key is ignored in the checksum computation. As the Spark shuffle\n+ * currently uses a PartitionIdPassthrough partitioner, the keys are already the partition\n+ * IDs for sending the data, and they are the same for all rows in the same partition.\n+ */\n+class UnsafeRowChecksum extends RowBasedChecksum() {\n+\n+  override protected def calculateRowChecksum(key: Any, value: Any): Long = {\n+    assert(\n+      value.isInstanceOf[UnsafeRow],\n+      \"Expecting UnsafeRow but got \" + value.getClass.getName)\n+\n+    // Casts or converts the baseObject in UnsafeRow to a byte array.\n+    val unsafeRow = value.asInstanceOf[UnsafeRow]\n+    XXH64.hashUnsafeBytes(\n+      unsafeRow.getBaseObject,\n+      unsafeRow.getBaseOffset,\n+      unsafeRow.getSizeInBytes,\n+      0\n+    )\n+  }\n+}\n+\n+object UnsafeRowChecksum {\n+  def createUnsafeRowChecksums(numPartitions: Int): Array[RowBasedChecksum] = {\n+    val rowBasedChecksums: Array[RowBasedChecksum] = new Array[RowBasedChecksum](numPartitions)\n+    for (i <- 0 until numPartitions) {\n+      rowBasedChecksums(i) = new UnsafeRowChecksum()\n+    }\n+    rowBasedChecksums",
        "comment_created_at": "2025-05-02T00:03:13+00:00",
        "comment_author": "attilapiros",
        "comment_body": "```\r\nArray.tabulate(numPartitions)(_ => new UnsafeRowChecksum())\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2219256031",
    "pr_number": 50170,
    "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala",
    "created_at": "2025-07-21T13:42:53+00:00",
    "commented_code": "}\n \n \n+/**\n+ * The \"ArrayContains\" node checks if a given value is present in a given array,\n+ * being similar to the \"IN\" predicate.\n+ *\n+ * This optimization rule replaces the \"ArrayContains\" nodes working on literal\n+ * lists with \"InSet\" nodes in the expression.\n+ */\n+object ReplaceArrayContainsWithInSet extends Rule[LogicalPlan] {\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan transformAllExpressions  {\n+      // In Catalyst, arrays evaluated as NULL values have \"arrayParam.value = NULL\" set,\n+      // so transforming its value to a set of objects will fail.\n+      case ArrayContains(arrayParam: Literal, col)\n+        if arrayParam.value != null =>\n+          InSet(col, arrayParam.value.asInstanceOf[GenericArrayData].array.toSet)",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2219256031",
        "repo_full_name": "apache/spark",
        "pr_number": 50170,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala",
        "discussion_id": "2219256031",
        "commented_code": "@@ -299,6 +300,26 @@ object ReorderAssociativeOperator extends Rule[LogicalPlan] {\n }\n \n \n+/**\n+ * The \"ArrayContains\" node checks if a given value is present in a given array,\n+ * being similar to the \"IN\" predicate.\n+ *\n+ * This optimization rule replaces the \"ArrayContains\" nodes working on literal\n+ * lists with \"InSet\" nodes in the expression.\n+ */\n+object ReplaceArrayContainsWithInSet extends Rule[LogicalPlan] {\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan transformAllExpressions  {\n+      // In Catalyst, arrays evaluated as NULL values have \"arrayParam.value = NULL\" set,\n+      // so transforming its value to a set of objects will fail.\n+      case ArrayContains(arrayParam: Literal, col)\n+        if arrayParam.value != null =>\n+          InSet(col, arrayParam.value.asInstanceOf[GenericArrayData].array.toSet)",
        "comment_created_at": "2025-07-21T13:42:53+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "It's risky to assume the value is `GenericArrayData`. We can do `arrayParam.value.toSeq(col.dataType).toSet`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2184470442",
    "pr_number": 50921,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
    "created_at": "2025-07-04T05:46:14+00:00",
    "commented_code": "private def createScanBuilder(plan: LogicalPlan) = plan.transform {\n     case r: DataSourceV2Relation =>\n-      ScanBuilderHolder(r.output, r, r.table.asReadable.newScanBuilder(r.options))\n+      val sHolder = ScanBuilderHolder(r.output, r, r.table.asReadable.newScanBuilder(r.options))\n+      sHolder.output.foreach { e =>\n+        // Join column names can change when joins are pushed down. We need to keep track of the\n+        // up-to-date column name in case connector do some duplicate resolving by aliasing the\n+        // columns. ScanBuilderHolder.output will be left unchanged.\n+        sHolder.pushedJoinOutputMap = sHolder.pushedJoinOutputMap + (e, e)",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2184470442",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2184470442",
        "commented_code": "@@ -58,7 +60,15 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n \n   private def createScanBuilder(plan: LogicalPlan) = plan.transform {\n     case r: DataSourceV2Relation =>\n-      ScanBuilderHolder(r.output, r, r.table.asReadable.newScanBuilder(r.options))\n+      val sHolder = ScanBuilderHolder(r.output, r, r.table.asReadable.newScanBuilder(r.options))\n+      sHolder.output.foreach { e =>\n+        // Join column names can change when joins are pushed down. We need to keep track of the\n+        // up-to-date column name in case connector do some duplicate resolving by aliasing the\n+        // columns. ScanBuilderHolder.output will be left unchanged.\n+        sHolder.pushedJoinOutputMap = sHolder.pushedJoinOutputMap + (e, e)",
        "comment_created_at": "2025-07-04T05:46:14+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "this looks very inefficient, how about `sHolder.pushedJoinOutputMap = sHolder.output.zip(sHolder.output).toMap`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2218090752",
    "pr_number": 51567,
    "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
    "created_at": "2025-07-21T02:41:30+00:00",
    "commented_code": "def resolveExprsWithAggregate(\n         exprs: Seq[Expression],\n         agg: Aggregate): (Seq[NamedExpression], Seq[Expression]) = {\n-      val extraAggExprs = ArrayBuffer.empty[NamedExpression]\n+      val extraAggExprs = new LinkedHashSet[NamedExpression]",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2218090752",
        "repo_full_name": "apache/spark",
        "pr_number": 51567,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
        "discussion_id": "2218090752",
        "commented_code": "@@ -2919,21 +2919,21 @@ class Analyzer(override val catalogManager: CatalogManager) extends RuleExecutor\n     def resolveExprsWithAggregate(\n         exprs: Seq[Expression],\n         agg: Aggregate): (Seq[NamedExpression], Seq[Expression]) = {\n-      val extraAggExprs = ArrayBuffer.empty[NamedExpression]\n+      val extraAggExprs = new LinkedHashSet[NamedExpression]",
        "comment_created_at": "2025-07-21T02:41:30+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "shall we use `ExpressionSet` which deduplicates expressions by their semantics instead of object equality?",
        "pr_file_module": null
      },
      {
        "comment_id": "2218250724",
        "repo_full_name": "apache/spark",
        "pr_number": 51567,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
        "discussion_id": "2218090752",
        "commented_code": "@@ -2919,21 +2919,21 @@ class Analyzer(override val catalogManager: CatalogManager) extends RuleExecutor\n     def resolveExprsWithAggregate(\n         exprs: Seq[Expression],\n         agg: Aggregate): (Seq[NamedExpression], Seq[Expression]) = {\n-      val extraAggExprs = ArrayBuffer.empty[NamedExpression]\n+      val extraAggExprs = new LinkedHashSet[NamedExpression]",
        "comment_created_at": "2025-07-21T05:49:03+00:00",
        "comment_author": "mihailotim-db",
        "comment_body": "This is a good point, but we can't actually use `ExpressionSet` since we need to keep the ordering in aggregate list deterministic. I actually found a separate bug here: https://github.com/apache/spark/pull/51557 and if we use a `LinkedHashMap` with `cannonicalized` expression as key, we can solve both issues. ",
        "pr_file_module": null
      }
    ]
  }
]