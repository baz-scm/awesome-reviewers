[
  {
    "discussion_id": "2222615519",
    "pr_number": 51616,
    "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala",
    "created_at": "2025-07-22T13:52:47+00:00",
    "commented_code": "conn.prepareStatement(\"CREATE TABLE test.partition (THEID INTEGER, `THE ID` INTEGER) \" +\n       \"AS SELECT 1, 1\")\n       .executeUpdate()\n-    conn.commit()\n \n     conn.prepareStatement(\"CREATE TABLE test.datetime (d DATE, t TIMESTAMP)\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-06', '2018-07-06 05:50:00.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-06', '2018-07-06 08:10:08.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-08', '2018-07-08 13:32:01.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-12', '2018-07-12 09:51:15.0')\").executeUpdate()\n-    conn.commit()\n \n     conn.prepareStatement(\n       \"CREATE TABLE test.composite_name (`last name` TEXT(32) NOT NULL, id INTEGER NOT NULL)\")\n       .executeUpdate()\n-    conn.prepareStatement(\"INSERT INTO test.composite_name VALUES ('smith', 1)\").executeUpdate()\n-    conn.prepareStatement(\"INSERT INTO test.composite_name VALUES ('jones', 2)\").executeUpdate()\n-    conn.commit()\n+\n+    val batchStmt = conn.createStatement()\n+\n+    batchStmt.addBatch(\"insert into test.people values ('fred', 1)\")",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2222615519",
        "repo_full_name": "apache/spark",
        "pr_number": 51616,
        "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala",
        "discussion_id": "2222615519",
        "commented_code": "@@ -278,25 +243,50 @@ class JDBCSuite extends QueryTest with SharedSparkSession {\n     conn.prepareStatement(\"CREATE TABLE test.partition (THEID INTEGER, `THE ID` INTEGER) \" +\n       \"AS SELECT 1, 1\")\n       .executeUpdate()\n-    conn.commit()\n \n     conn.prepareStatement(\"CREATE TABLE test.datetime (d DATE, t TIMESTAMP)\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-06', '2018-07-06 05:50:00.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-06', '2018-07-06 08:10:08.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-08', '2018-07-08 13:32:01.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-12', '2018-07-12 09:51:15.0')\").executeUpdate()\n-    conn.commit()\n \n     conn.prepareStatement(\n       \"CREATE TABLE test.composite_name (`last name` TEXT(32) NOT NULL, id INTEGER NOT NULL)\")\n       .executeUpdate()\n-    conn.prepareStatement(\"INSERT INTO test.composite_name VALUES ('smith', 1)\").executeUpdate()\n-    conn.prepareStatement(\"INSERT INTO test.composite_name VALUES ('jones', 2)\").executeUpdate()\n-    conn.commit()\n+\n+    val batchStmt = conn.createStatement()\n+\n+    batchStmt.addBatch(\"insert into test.people values ('fred', 1)\")",
        "comment_created_at": "2025-07-22T13:52:47+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "Can we include CREATE TABLE in the batch statement as well?",
        "pr_file_module": null
      },
      {
        "comment_id": "2222619184",
        "repo_full_name": "apache/spark",
        "pr_number": 51616,
        "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala",
        "discussion_id": "2222615519",
        "commented_code": "@@ -278,25 +243,50 @@ class JDBCSuite extends QueryTest with SharedSparkSession {\n     conn.prepareStatement(\"CREATE TABLE test.partition (THEID INTEGER, `THE ID` INTEGER) \" +\n       \"AS SELECT 1, 1\")\n       .executeUpdate()\n-    conn.commit()\n \n     conn.prepareStatement(\"CREATE TABLE test.datetime (d DATE, t TIMESTAMP)\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-06', '2018-07-06 05:50:00.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-06', '2018-07-06 08:10:08.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-08', '2018-07-08 13:32:01.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-12', '2018-07-12 09:51:15.0')\").executeUpdate()\n-    conn.commit()\n \n     conn.prepareStatement(\n       \"CREATE TABLE test.composite_name (`last name` TEXT(32) NOT NULL, id INTEGER NOT NULL)\")\n       .executeUpdate()\n-    conn.prepareStatement(\"INSERT INTO test.composite_name VALUES ('smith', 1)\").executeUpdate()\n-    conn.prepareStatement(\"INSERT INTO test.composite_name VALUES ('jones', 2)\").executeUpdate()\n-    conn.commit()\n+\n+    val batchStmt = conn.createStatement()\n+\n+    batchStmt.addBatch(\"insert into test.people values ('fred', 1)\")",
        "comment_created_at": "2025-07-22T13:54:08+00:00",
        "comment_author": "alekjarmov",
        "comment_body": "There is some SQL views which depend on these create statements which is why I did not batch them.",
        "pr_file_module": null
      },
      {
        "comment_id": "2222635699",
        "repo_full_name": "apache/spark",
        "pr_number": 51616,
        "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala",
        "discussion_id": "2222615519",
        "commented_code": "@@ -278,25 +243,50 @@ class JDBCSuite extends QueryTest with SharedSparkSession {\n     conn.prepareStatement(\"CREATE TABLE test.partition (THEID INTEGER, `THE ID` INTEGER) \" +\n       \"AS SELECT 1, 1\")\n       .executeUpdate()\n-    conn.commit()\n \n     conn.prepareStatement(\"CREATE TABLE test.datetime (d DATE, t TIMESTAMP)\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-06', '2018-07-06 05:50:00.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-06', '2018-07-06 08:10:08.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-08', '2018-07-08 13:32:01.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-12', '2018-07-12 09:51:15.0')\").executeUpdate()\n-    conn.commit()\n \n     conn.prepareStatement(\n       \"CREATE TABLE test.composite_name (`last name` TEXT(32) NOT NULL, id INTEGER NOT NULL)\")\n       .executeUpdate()\n-    conn.prepareStatement(\"INSERT INTO test.composite_name VALUES ('smith', 1)\").executeUpdate()\n-    conn.prepareStatement(\"INSERT INTO test.composite_name VALUES ('jones', 2)\").executeUpdate()\n-    conn.commit()\n+\n+    val batchStmt = conn.createStatement()\n+\n+    batchStmt.addBatch(\"insert into test.people values ('fred', 1)\")",
        "comment_created_at": "2025-07-22T13:59:52+00:00",
        "comment_author": "alekjarmov",
        "comment_body": "Actually I can have 2 batch statements, create, spark.sql commands, inserts",
        "pr_file_module": null
      },
      {
        "comment_id": "2222694739",
        "repo_full_name": "apache/spark",
        "pr_number": 51616,
        "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala",
        "discussion_id": "2222615519",
        "commented_code": "@@ -278,25 +243,50 @@ class JDBCSuite extends QueryTest with SharedSparkSession {\n     conn.prepareStatement(\"CREATE TABLE test.partition (THEID INTEGER, `THE ID` INTEGER) \" +\n       \"AS SELECT 1, 1\")\n       .executeUpdate()\n-    conn.commit()\n \n     conn.prepareStatement(\"CREATE TABLE test.datetime (d DATE, t TIMESTAMP)\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-06', '2018-07-06 05:50:00.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-06', '2018-07-06 08:10:08.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-08', '2018-07-08 13:32:01.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-12', '2018-07-12 09:51:15.0')\").executeUpdate()\n-    conn.commit()\n \n     conn.prepareStatement(\n       \"CREATE TABLE test.composite_name (`last name` TEXT(32) NOT NULL, id INTEGER NOT NULL)\")\n       .executeUpdate()\n-    conn.prepareStatement(\"INSERT INTO test.composite_name VALUES ('smith', 1)\").executeUpdate()\n-    conn.prepareStatement(\"INSERT INTO test.composite_name VALUES ('jones', 2)\").executeUpdate()\n-    conn.commit()\n+\n+    val batchStmt = conn.createStatement()\n+\n+    batchStmt.addBatch(\"insert into test.people values ('fred', 1)\")",
        "comment_created_at": "2025-07-22T14:19:44+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "can we move the spark sql commands to the end? after the JDBC inserts",
        "pr_file_module": null
      },
      {
        "comment_id": "2222720849",
        "repo_full_name": "apache/spark",
        "pr_number": 51616,
        "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala",
        "discussion_id": "2222615519",
        "commented_code": "@@ -278,25 +243,50 @@ class JDBCSuite extends QueryTest with SharedSparkSession {\n     conn.prepareStatement(\"CREATE TABLE test.partition (THEID INTEGER, `THE ID` INTEGER) \" +\n       \"AS SELECT 1, 1\")\n       .executeUpdate()\n-    conn.commit()\n \n     conn.prepareStatement(\"CREATE TABLE test.datetime (d DATE, t TIMESTAMP)\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-06', '2018-07-06 05:50:00.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-06', '2018-07-06 08:10:08.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-08', '2018-07-08 13:32:01.0')\").executeUpdate()\n-    conn.prepareStatement(\n-      \"INSERT INTO test.datetime VALUES ('2018-07-12', '2018-07-12 09:51:15.0')\").executeUpdate()\n-    conn.commit()\n \n     conn.prepareStatement(\n       \"CREATE TABLE test.composite_name (`last name` TEXT(32) NOT NULL, id INTEGER NOT NULL)\")\n       .executeUpdate()\n-    conn.prepareStatement(\"INSERT INTO test.composite_name VALUES ('smith', 1)\").executeUpdate()\n-    conn.prepareStatement(\"INSERT INTO test.composite_name VALUES ('jones', 2)\").executeUpdate()\n-    conn.commit()\n+\n+    val batchStmt = conn.createStatement()\n+\n+    batchStmt.addBatch(\"insert into test.people values ('fred', 1)\")",
        "comment_created_at": "2025-07-22T14:25:39+00:00",
        "comment_author": "alekjarmov",
        "comment_body": "Yes that's what I ended up doing.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2224416196",
    "pr_number": 51616,
    "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala",
    "created_at": "2025-07-23T05:41:15+00:00",
    "commented_code": "super.beforeAll()\n     Utils.classForName(\"org.h2.Driver\")\n     withConnection { conn =>\n-      conn.prepareStatement(\"CREATE SCHEMA \\\"test\\\"\").executeUpdate()\n-      conn.prepareStatement(\n+\n+      val batchStmt = conn.createStatement()\n+      batchStmt.addBatch(\"CREATE SCHEMA \\\"test\\\"\")\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"empty_table\\\" (name TEXT(32) NOT NULL, id INTEGER NOT NULL)\")\n-        .executeUpdate()\n-      conn.prepareStatement(\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"people\\\" (name TEXT(32) NOT NULL, id INTEGER NOT NULL)\")\n-        .executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"people\\\" VALUES ('fred', 1)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"people\\\" VALUES ('mary', 2)\").executeUpdate()\n-      conn.prepareStatement(\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"employee\\\" (dept INTEGER, name TEXT(32), salary NUMERIC(20, 2),\" +\n-          \" bonus DOUBLE, is_manager BOOLEAN)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (1, 'amy', 10000, 1000, true)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (2, 'alex', 12000, 1200, false)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (1, 'cathy', 9000, 1200, false)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (2, 'david', 10000, 1300, true)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (6, 'jen', 12000, 1200, true)\").executeUpdate()\n-      conn.prepareStatement(\n+          \" bonus DOUBLE, is_manager BOOLEAN)\")\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"dept\\\" (\\\"dept id\\\" INTEGER NOT NULL, \\\"dept.id\\\" INTEGER)\")\n-        .executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"dept\\\" VALUES (1, 1)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"dept\\\" VALUES (2, 1)\").executeUpdate()\n \n       // scalastyle:off\n-      conn.prepareStatement(\n-        \"CREATE TABLE \\\"test\\\".\\\"person\\\" (\\\"\u540d\\\" INTEGER NOT NULL)\").executeUpdate()\n+      batchStmt.addBatch(\"CREATE TABLE \\\"test\\\".\\\"person\\\" (\\\"\u540d\\\" INTEGER NOT NULL)\")\n       // scalastyle:on\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"person\\\" VALUES (1)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"person\\\" VALUES (2)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"\"\"CREATE TABLE \"test\".\"view1\" (\"|col1\" INTEGER, \"|col2\" INTEGER)\"\"\").executeUpdate()\n-      conn.prepareStatement(\n-        \"\"\"CREATE TABLE \"test\".\"view2\" (\"|col1\" INTEGER, \"|col3\" INTEGER)\"\"\").executeUpdate()\n-\n-      conn.prepareStatement(\n+\n+      batchStmt.addBatch(\n+        \"\"\"CREATE TABLE \"test\".\"view1\" (\"|col1\" INTEGER, \"|col2\" INTEGER)\"\"\")\n+      batchStmt.addBatch(\n+        \"\"\"CREATE TABLE \"test\".\"view2\" (\"|col1\" INTEGER, \"|col3\" INTEGER)\"\"\")\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"item\\\" (id INTEGER, name TEXT(32), price NUMERIC(23, 3))\")\n-        .executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"item\\\" VALUES \" +\n-        \"(1, 'bottle', 11111111111111111111.123)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"item\\\" VALUES \" +\n-        \"(1, 'bottle', 99999999999999999999.123)\").executeUpdate()\n \n-      conn.prepareStatement(\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"datetime\\\" (name TEXT(32), date1 DATE, time1 TIMESTAMP)\")\n-        .executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"datetime\\\" VALUES \" +\n-        \"('amy', '2022-05-19', '2022-05-19 00:00:00')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"datetime\\\" VALUES \" +\n-        \"('alex', '2022-05-18', '2022-05-18 00:00:00')\").executeUpdate()\n-\n-      conn.prepareStatement(\n-        \"CREATE TABLE \\\"test\\\".\\\"address\\\" (email TEXT(32) NOT NULL)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc_def@gmail.com')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc%def@gmail.com')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc%_def@gmail.com')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc_%def@gmail.com')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc_''%def@gmail.com')\").executeUpdate()\n-\n-      conn.prepareStatement(\"CREATE TABLE \\\"test\\\".\\\"binary_tab\\\" (name TEXT(32),b BINARY(20))\")\n-        .executeUpdate()\n+\n+      batchStmt.addBatch(\n+        \"CREATE TABLE \\\"test\\\".\\\"address\\\" (email TEXT(32) NOT NULL)\")\n+\n+      batchStmt.addBatch(\"CREATE TABLE \\\"test\\\".\\\"binary_tab\\\" (name TEXT(32),b BINARY(20))\")\n+\n+      batchStmt.addBatch(\"CREATE TABLE \\\"test\\\".\\\"employee_bonus\\\" \" +\n+        \"(name TEXT(32), salary NUMERIC(20, 2), bonus DOUBLE, factor DOUBLE)\")\n+\n+      batchStmt.addBatch(\n+        \"CREATE TABLE \\\"test\\\".\\\"strings_with_nulls\\\" (str TEXT(32))\")\n+\n+      batchStmt.addBatch(\"INSERT INTO \\\"test\\\".\\\"people\\\" VALUES ('fred', 1)\")",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2224416196",
        "repo_full_name": "apache/spark",
        "pr_number": 51616,
        "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala",
        "discussion_id": "2224416196",
        "commented_code": "@@ -157,102 +157,101 @@ class JDBCV2Suite extends QueryTest with SharedSparkSession with ExplainSuiteHel\n     super.beforeAll()\n     Utils.classForName(\"org.h2.Driver\")\n     withConnection { conn =>\n-      conn.prepareStatement(\"CREATE SCHEMA \\\"test\\\"\").executeUpdate()\n-      conn.prepareStatement(\n+\n+      val batchStmt = conn.createStatement()\n+      batchStmt.addBatch(\"CREATE SCHEMA \\\"test\\\"\")\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"empty_table\\\" (name TEXT(32) NOT NULL, id INTEGER NOT NULL)\")\n-        .executeUpdate()\n-      conn.prepareStatement(\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"people\\\" (name TEXT(32) NOT NULL, id INTEGER NOT NULL)\")\n-        .executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"people\\\" VALUES ('fred', 1)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"people\\\" VALUES ('mary', 2)\").executeUpdate()\n-      conn.prepareStatement(\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"employee\\\" (dept INTEGER, name TEXT(32), salary NUMERIC(20, 2),\" +\n-          \" bonus DOUBLE, is_manager BOOLEAN)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (1, 'amy', 10000, 1000, true)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (2, 'alex', 12000, 1200, false)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (1, 'cathy', 9000, 1200, false)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (2, 'david', 10000, 1300, true)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (6, 'jen', 12000, 1200, true)\").executeUpdate()\n-      conn.prepareStatement(\n+          \" bonus DOUBLE, is_manager BOOLEAN)\")\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"dept\\\" (\\\"dept id\\\" INTEGER NOT NULL, \\\"dept.id\\\" INTEGER)\")\n-        .executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"dept\\\" VALUES (1, 1)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"dept\\\" VALUES (2, 1)\").executeUpdate()\n \n       // scalastyle:off\n-      conn.prepareStatement(\n-        \"CREATE TABLE \\\"test\\\".\\\"person\\\" (\\\"\u540d\\\" INTEGER NOT NULL)\").executeUpdate()\n+      batchStmt.addBatch(\"CREATE TABLE \\\"test\\\".\\\"person\\\" (\\\"\u540d\\\" INTEGER NOT NULL)\")\n       // scalastyle:on\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"person\\\" VALUES (1)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"person\\\" VALUES (2)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"\"\"CREATE TABLE \"test\".\"view1\" (\"|col1\" INTEGER, \"|col2\" INTEGER)\"\"\").executeUpdate()\n-      conn.prepareStatement(\n-        \"\"\"CREATE TABLE \"test\".\"view2\" (\"|col1\" INTEGER, \"|col3\" INTEGER)\"\"\").executeUpdate()\n-\n-      conn.prepareStatement(\n+\n+      batchStmt.addBatch(\n+        \"\"\"CREATE TABLE \"test\".\"view1\" (\"|col1\" INTEGER, \"|col2\" INTEGER)\"\"\")\n+      batchStmt.addBatch(\n+        \"\"\"CREATE TABLE \"test\".\"view2\" (\"|col1\" INTEGER, \"|col3\" INTEGER)\"\"\")\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"item\\\" (id INTEGER, name TEXT(32), price NUMERIC(23, 3))\")\n-        .executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"item\\\" VALUES \" +\n-        \"(1, 'bottle', 11111111111111111111.123)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"item\\\" VALUES \" +\n-        \"(1, 'bottle', 99999999999999999999.123)\").executeUpdate()\n \n-      conn.prepareStatement(\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"datetime\\\" (name TEXT(32), date1 DATE, time1 TIMESTAMP)\")\n-        .executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"datetime\\\" VALUES \" +\n-        \"('amy', '2022-05-19', '2022-05-19 00:00:00')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"datetime\\\" VALUES \" +\n-        \"('alex', '2022-05-18', '2022-05-18 00:00:00')\").executeUpdate()\n-\n-      conn.prepareStatement(\n-        \"CREATE TABLE \\\"test\\\".\\\"address\\\" (email TEXT(32) NOT NULL)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc_def@gmail.com')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc%def@gmail.com')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc%_def@gmail.com')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc_%def@gmail.com')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc_''%def@gmail.com')\").executeUpdate()\n-\n-      conn.prepareStatement(\"CREATE TABLE \\\"test\\\".\\\"binary_tab\\\" (name TEXT(32),b BINARY(20))\")\n-        .executeUpdate()\n+\n+      batchStmt.addBatch(\n+        \"CREATE TABLE \\\"test\\\".\\\"address\\\" (email TEXT(32) NOT NULL)\")\n+\n+      batchStmt.addBatch(\"CREATE TABLE \\\"test\\\".\\\"binary_tab\\\" (name TEXT(32),b BINARY(20))\")\n+\n+      batchStmt.addBatch(\"CREATE TABLE \\\"test\\\".\\\"employee_bonus\\\" \" +\n+        \"(name TEXT(32), salary NUMERIC(20, 2), bonus DOUBLE, factor DOUBLE)\")\n+\n+      batchStmt.addBatch(\n+        \"CREATE TABLE \\\"test\\\".\\\"strings_with_nulls\\\" (str TEXT(32))\")\n+\n+      batchStmt.addBatch(\"INSERT INTO \\\"test\\\".\\\"people\\\" VALUES ('fred', 1)\")",
        "comment_created_at": "2025-07-23T05:41:15+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "in this suite, CREATE TABLE and INSERT are in one batch. Why can't we do the same for `JDBCSuite.scala`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2226008656",
        "repo_full_name": "apache/spark",
        "pr_number": 51616,
        "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala",
        "discussion_id": "2224416196",
        "commented_code": "@@ -157,102 +157,101 @@ class JDBCV2Suite extends QueryTest with SharedSparkSession with ExplainSuiteHel\n     super.beforeAll()\n     Utils.classForName(\"org.h2.Driver\")\n     withConnection { conn =>\n-      conn.prepareStatement(\"CREATE SCHEMA \\\"test\\\"\").executeUpdate()\n-      conn.prepareStatement(\n+\n+      val batchStmt = conn.createStatement()\n+      batchStmt.addBatch(\"CREATE SCHEMA \\\"test\\\"\")\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"empty_table\\\" (name TEXT(32) NOT NULL, id INTEGER NOT NULL)\")\n-        .executeUpdate()\n-      conn.prepareStatement(\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"people\\\" (name TEXT(32) NOT NULL, id INTEGER NOT NULL)\")\n-        .executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"people\\\" VALUES ('fred', 1)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"people\\\" VALUES ('mary', 2)\").executeUpdate()\n-      conn.prepareStatement(\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"employee\\\" (dept INTEGER, name TEXT(32), salary NUMERIC(20, 2),\" +\n-          \" bonus DOUBLE, is_manager BOOLEAN)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (1, 'amy', 10000, 1000, true)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (2, 'alex', 12000, 1200, false)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (1, 'cathy', 9000, 1200, false)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (2, 'david', 10000, 1300, true)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"INSERT INTO \\\"test\\\".\\\"employee\\\" VALUES (6, 'jen', 12000, 1200, true)\").executeUpdate()\n-      conn.prepareStatement(\n+          \" bonus DOUBLE, is_manager BOOLEAN)\")\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"dept\\\" (\\\"dept id\\\" INTEGER NOT NULL, \\\"dept.id\\\" INTEGER)\")\n-        .executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"dept\\\" VALUES (1, 1)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"dept\\\" VALUES (2, 1)\").executeUpdate()\n \n       // scalastyle:off\n-      conn.prepareStatement(\n-        \"CREATE TABLE \\\"test\\\".\\\"person\\\" (\\\"\u540d\\\" INTEGER NOT NULL)\").executeUpdate()\n+      batchStmt.addBatch(\"CREATE TABLE \\\"test\\\".\\\"person\\\" (\\\"\u540d\\\" INTEGER NOT NULL)\")\n       // scalastyle:on\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"person\\\" VALUES (1)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"person\\\" VALUES (2)\").executeUpdate()\n-      conn.prepareStatement(\n-        \"\"\"CREATE TABLE \"test\".\"view1\" (\"|col1\" INTEGER, \"|col2\" INTEGER)\"\"\").executeUpdate()\n-      conn.prepareStatement(\n-        \"\"\"CREATE TABLE \"test\".\"view2\" (\"|col1\" INTEGER, \"|col3\" INTEGER)\"\"\").executeUpdate()\n-\n-      conn.prepareStatement(\n+\n+      batchStmt.addBatch(\n+        \"\"\"CREATE TABLE \"test\".\"view1\" (\"|col1\" INTEGER, \"|col2\" INTEGER)\"\"\")\n+      batchStmt.addBatch(\n+        \"\"\"CREATE TABLE \"test\".\"view2\" (\"|col1\" INTEGER, \"|col3\" INTEGER)\"\"\")\n+\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"item\\\" (id INTEGER, name TEXT(32), price NUMERIC(23, 3))\")\n-        .executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"item\\\" VALUES \" +\n-        \"(1, 'bottle', 11111111111111111111.123)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"item\\\" VALUES \" +\n-        \"(1, 'bottle', 99999999999999999999.123)\").executeUpdate()\n \n-      conn.prepareStatement(\n+      batchStmt.addBatch(\n         \"CREATE TABLE \\\"test\\\".\\\"datetime\\\" (name TEXT(32), date1 DATE, time1 TIMESTAMP)\")\n-        .executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"datetime\\\" VALUES \" +\n-        \"('amy', '2022-05-19', '2022-05-19 00:00:00')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"datetime\\\" VALUES \" +\n-        \"('alex', '2022-05-18', '2022-05-18 00:00:00')\").executeUpdate()\n-\n-      conn.prepareStatement(\n-        \"CREATE TABLE \\\"test\\\".\\\"address\\\" (email TEXT(32) NOT NULL)\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc_def@gmail.com')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc%def@gmail.com')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc%_def@gmail.com')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc_%def@gmail.com')\").executeUpdate()\n-      conn.prepareStatement(\"INSERT INTO \\\"test\\\".\\\"address\\\" VALUES \" +\n-        \"('abc_''%def@gmail.com')\").executeUpdate()\n-\n-      conn.prepareStatement(\"CREATE TABLE \\\"test\\\".\\\"binary_tab\\\" (name TEXT(32),b BINARY(20))\")\n-        .executeUpdate()\n+\n+      batchStmt.addBatch(\n+        \"CREATE TABLE \\\"test\\\".\\\"address\\\" (email TEXT(32) NOT NULL)\")\n+\n+      batchStmt.addBatch(\"CREATE TABLE \\\"test\\\".\\\"binary_tab\\\" (name TEXT(32),b BINARY(20))\")\n+\n+      batchStmt.addBatch(\"CREATE TABLE \\\"test\\\".\\\"employee_bonus\\\" \" +\n+        \"(name TEXT(32), salary NUMERIC(20, 2), bonus DOUBLE, factor DOUBLE)\")\n+\n+      batchStmt.addBatch(\n+        \"CREATE TABLE \\\"test\\\".\\\"strings_with_nulls\\\" (str TEXT(32))\")\n+\n+      batchStmt.addBatch(\"INSERT INTO \\\"test\\\".\\\"people\\\" VALUES ('fred', 1)\")",
        "comment_created_at": "2025-07-23T15:45:58+00:00",
        "comment_author": "alekjarmov",
        "comment_body": "Did it in JDBCSuite as well.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2212635239",
    "pr_number": 51519,
    "pr_file": "connector/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/join/JDBCJoinPushdownIntegrationSuite.scala",
    "created_at": "2025-07-17T08:11:49+00:00",
    "commented_code": "+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc.v2.join\n+\n+import java.sql.Connection\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest}\n+import org.apache.spark.sql.catalyst.plans.logical.Join\n+import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.jdbc.{DockerIntegrationFunSuite, JdbcDialect}\n+import org.apache.spark.sql.jdbc.v2.V2JDBCPushdownTestUtils\n+import org.apache.spark.sql.test.SharedSparkSession\n+import org.apache.spark.sql.types.{DataType, DataTypes}\n+import org.apache.spark.tags.DockerTest\n+\n+@DockerTest\n+trait JDBCJoinPushdownIntegrationSuite\n+  extends QueryTest\n+  with SharedSparkSession\n+  with DockerIntegrationFunSuite\n+  with V2JDBCPushdownTestUtils {\n+  val catalogName: String\n+  val namespaceOpt: Option[String] = None\n+  val joinTableName1: String = \"join_table_1\"\n+  val joinTableName2: String = \"join_table_2\"\n+\n+  // Concrete suite must provide the dialect for its DB\n+  def jdbcDialect: JdbcDialect\n+\n+  private def catalogAndNamespace =\n+    namespaceOpt.map(namespace => s\"$catalogName.$namespace\").getOrElse(catalogName)\n+\n+  def fullyQualifiedTableName1: String = namespaceOpt\n+    .map(namespace => s\"$namespace.$joinTableName1\").getOrElse(joinTableName1)\n+\n+  def fullyQualifiedTableName2: String = namespaceOpt\n+    .map(namespace => s\"$namespace.$joinTableName2\").getOrElse(joinTableName2)\n+\n+  protected def getJDBCTypeString(dt: DataType): String = {\n+    JdbcUtils.getJdbcType(dt, jdbcDialect).databaseTypeDefinition.toUpperCase()\n+  }\n+\n+  protected def caseConvert(tableName: String): String = tableName\n+\n+  def dataPreparation(connection: Connection): Unit = {\n+    tablePreparation(connection)\n+    fillJoinTables(connection)\n+  }\n+\n+  def tablePreparation(connection: Connection): Unit = {\n+    connection.prepareStatement(\n+      s\"\"\"CREATE TABLE $fullyQualifiedTableName1 (\n+         |  id ${getJDBCTypeString(DataTypes.IntegerType)},\n+         |  amount ${getJDBCTypeString(DataTypes.createDecimalType(10, 2))},\n+         |  name ${getJDBCTypeString(DataTypes.StringType)}\n+         |)\"\"\".stripMargin).executeUpdate()\n+\n+    connection.prepareStatement(\n+      s\"\"\"CREATE TABLE $fullyQualifiedTableName2 (\n+         |  id ${getJDBCTypeString(DataTypes.LongType)},\n+         |  salary ${getJDBCTypeString(DataTypes.createDecimalType(10, 2))},\n+         |  surname ${getJDBCTypeString(DataTypes.StringType)}\n+         |)\"\"\".stripMargin).executeUpdate()\n+  }\n+\n+  def fillJoinTables(connection: Connection): Unit = {\n+    val random = new java.util.Random(42)\n+    val table1Data = (1 to 100).map { i =>\n+      val id = i % 11\n+      val amount = BigDecimal.valueOf(random.nextDouble() * 10000)\n+        .setScale(2, BigDecimal.RoundingMode.HALF_UP)\n+      val name = s\"name_$i\"\n+      (id, amount, name)\n+    }\n+    val table2Data = (1 to 100).map { i =>\n+      val id = (i % 17).toLong\n+      val salary = BigDecimal.valueOf(random.nextDouble() * 50000)\n+        .setScale(2, BigDecimal.RoundingMode.HALF_UP)\n+      val surname = s\"surname_$i\"\n+      (id, salary, surname)\n+    }\n+\n+    // Use parameterized queries to handle different data types properly\n+    val insertStmt1 = connection.prepareStatement(\n+      s\"INSERT INTO $fullyQualifiedTableName1 (id, amount, name) VALUES (?, ?, ?)\"\n+    )\n+    table1Data.foreach { case (id, amount, name) =>\n+      insertStmt1.setInt(1, id)\n+      insertStmt1.setBigDecimal(2, amount.bigDecimal)\n+      insertStmt1.setString(3, name)\n+      insertStmt1.executeUpdate()\n+    }\n+    insertStmt1.close()\n+\n+    val insertStmt2 = connection.prepareStatement(\n+      s\"INSERT INTO $fullyQualifiedTableName2 (id, salary, surname) VALUES (?, ?, ?)\"\n+    )\n+    table2Data.foreach { case (id, salary, surname) =>\n+      insertStmt2.setLong(1, id)\n+      insertStmt2.setBigDecimal(2, salary.bigDecimal)\n+      insertStmt2.setString(3, surname)\n+      insertStmt2.executeUpdate()\n+    }\n+    insertStmt2.close()\n+  }\n+\n+  /**\n+   * Runs the plan and makes sure the plans contains all of the keywords.\n+   */\n+  protected def checkKeywordsExistsInExplain(df: DataFrame, keywords: String*): Unit = {\n+    val output = new java.io.ByteArrayOutputStream()\n+    Console.withOut(output) {\n+      df.explain(extended = true)\n+    }\n+    val normalizedOutput = output.toString.replaceAll(\"#\\\\d+\", \"#x\")\n+    for (key <- keywords) {\n+      assert(normalizedOutput.contains(key), s\"Expected keyword '$key' not found in explain output\")\n+    }\n+  }\n+\n+  private def checkPushedInfo(df: DataFrame, expectedPlanFragment: String*): Unit = {\n+    withSQLConf(SQLConf.MAX_METADATA_STRING_LENGTH.key -> \"1000\") {\n+      df.queryExecution.optimizedPlan.collect {\n+        case _: DataSourceV2ScanRelation =>\n+          checkKeywordsExistsInExplain(df, expectedPlanFragment: _*)\n+      }\n+    }\n+  }\n+\n+  private def checkJoinNotPushed(df: DataFrame): Unit = {\n+    val joinNodes = df.queryExecution.optimizedPlan.collect {\n+      case j: Join => j\n+    }\n+    assert(joinNodes.nonEmpty, \"Join should not be pushed down\")\n+  }\n+\n+  private def checkJoinPushed(df: DataFrame, expectedTables: String*): Unit = {\n+    val joinNodes = df.queryExecution.optimizedPlan.collect {\n+      case j: Join => j\n+    }\n+    assert(joinNodes.isEmpty, \"Join should be pushed down\")\n+    if (expectedTables.nonEmpty) {\n+      checkPushedInfo(df, s\"PushedJoins: [${expectedTables.mkString(\", \")}]\")\n+    }\n+  }\n+\n+  test(\"Test basic inner join pushdown with column pruning\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname, t1.amount, t2.salary\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    // Verify we have non-empty results\n+    assert(rows.nonEmpty, \"Join should produce non-empty results\")\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+\n+  test(\"Test join with additional filters\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname, t1.amount, t2.salary\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |WHERE t1.amount > 5000 AND t2.salary > 25000\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkFilterPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test self join should be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.name as name2\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName1)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join without condition should not be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |CROSS JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinNotPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with complex condition\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname, t1.amount + t2.salary as total\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2\n+      |ON t1.id = t2.id AND t1.amount > 1000\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with limit and order\") {\n+    // ORDER BY is used to have same ordering on Spark and database. Otherwise, different results\n+    // could be returned.\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |ORDER BY t1.id, t1.name, t2.surname\n+      |LIMIT 5\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkSortRemoved(df)\n+      checkLimitRemoved(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with order by\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |ORDER BY t1.id, t1.name, t2.surname\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      // Order without limit is not supported in DSv2\n+      checkSortRemoved(df, false)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with aggregation\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, COUNT(*), AVG(t1.amount), MAX(t2.salary)\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |GROUP BY t1.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAggregateRemoved(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test left outer join should not be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |LEFT JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinNotPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test right outer join should not be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |RIGHT JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinNotPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test full outer join should not be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |FULL OUTER JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinNotPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with subquery should be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, sub.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN (\n+      |  SELECT id, surname FROM $catalogAndNamespace.${caseConvert(joinTableName2)}\n+      |  WHERE salary > 25000\n+      |) sub ON t1.id = sub.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with non-equality condition should be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id > t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+}",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2212635239",
        "repo_full_name": "apache/spark",
        "pr_number": 51519,
        "pr_file": "connector/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/join/JDBCJoinPushdownIntegrationSuite.scala",
        "discussion_id": "2212635239",
        "commented_code": "@@ -0,0 +1,454 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc.v2.join\n+\n+import java.sql.Connection\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest}\n+import org.apache.spark.sql.catalyst.plans.logical.Join\n+import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.jdbc.{DockerIntegrationFunSuite, JdbcDialect}\n+import org.apache.spark.sql.jdbc.v2.V2JDBCPushdownTestUtils\n+import org.apache.spark.sql.test.SharedSparkSession\n+import org.apache.spark.sql.types.{DataType, DataTypes}\n+import org.apache.spark.tags.DockerTest\n+\n+@DockerTest\n+trait JDBCJoinPushdownIntegrationSuite\n+  extends QueryTest\n+  with SharedSparkSession\n+  with DockerIntegrationFunSuite\n+  with V2JDBCPushdownTestUtils {\n+  val catalogName: String\n+  val namespaceOpt: Option[String] = None\n+  val joinTableName1: String = \"join_table_1\"\n+  val joinTableName2: String = \"join_table_2\"\n+\n+  // Concrete suite must provide the dialect for its DB\n+  def jdbcDialect: JdbcDialect\n+\n+  private def catalogAndNamespace =\n+    namespaceOpt.map(namespace => s\"$catalogName.$namespace\").getOrElse(catalogName)\n+\n+  def fullyQualifiedTableName1: String = namespaceOpt\n+    .map(namespace => s\"$namespace.$joinTableName1\").getOrElse(joinTableName1)\n+\n+  def fullyQualifiedTableName2: String = namespaceOpt\n+    .map(namespace => s\"$namespace.$joinTableName2\").getOrElse(joinTableName2)\n+\n+  protected def getJDBCTypeString(dt: DataType): String = {\n+    JdbcUtils.getJdbcType(dt, jdbcDialect).databaseTypeDefinition.toUpperCase()\n+  }\n+\n+  protected def caseConvert(tableName: String): String = tableName\n+\n+  def dataPreparation(connection: Connection): Unit = {\n+    tablePreparation(connection)\n+    fillJoinTables(connection)\n+  }\n+\n+  def tablePreparation(connection: Connection): Unit = {\n+    connection.prepareStatement(\n+      s\"\"\"CREATE TABLE $fullyQualifiedTableName1 (\n+         |  id ${getJDBCTypeString(DataTypes.IntegerType)},\n+         |  amount ${getJDBCTypeString(DataTypes.createDecimalType(10, 2))},\n+         |  name ${getJDBCTypeString(DataTypes.StringType)}\n+         |)\"\"\".stripMargin).executeUpdate()\n+\n+    connection.prepareStatement(\n+      s\"\"\"CREATE TABLE $fullyQualifiedTableName2 (\n+         |  id ${getJDBCTypeString(DataTypes.LongType)},\n+         |  salary ${getJDBCTypeString(DataTypes.createDecimalType(10, 2))},\n+         |  surname ${getJDBCTypeString(DataTypes.StringType)}\n+         |)\"\"\".stripMargin).executeUpdate()\n+  }\n+\n+  def fillJoinTables(connection: Connection): Unit = {\n+    val random = new java.util.Random(42)\n+    val table1Data = (1 to 100).map { i =>\n+      val id = i % 11\n+      val amount = BigDecimal.valueOf(random.nextDouble() * 10000)\n+        .setScale(2, BigDecimal.RoundingMode.HALF_UP)\n+      val name = s\"name_$i\"\n+      (id, amount, name)\n+    }\n+    val table2Data = (1 to 100).map { i =>\n+      val id = (i % 17).toLong\n+      val salary = BigDecimal.valueOf(random.nextDouble() * 50000)\n+        .setScale(2, BigDecimal.RoundingMode.HALF_UP)\n+      val surname = s\"surname_$i\"\n+      (id, salary, surname)\n+    }\n+\n+    // Use parameterized queries to handle different data types properly\n+    val insertStmt1 = connection.prepareStatement(\n+      s\"INSERT INTO $fullyQualifiedTableName1 (id, amount, name) VALUES (?, ?, ?)\"\n+    )\n+    table1Data.foreach { case (id, amount, name) =>\n+      insertStmt1.setInt(1, id)\n+      insertStmt1.setBigDecimal(2, amount.bigDecimal)\n+      insertStmt1.setString(3, name)\n+      insertStmt1.executeUpdate()\n+    }\n+    insertStmt1.close()\n+\n+    val insertStmt2 = connection.prepareStatement(\n+      s\"INSERT INTO $fullyQualifiedTableName2 (id, salary, surname) VALUES (?, ?, ?)\"\n+    )\n+    table2Data.foreach { case (id, salary, surname) =>\n+      insertStmt2.setLong(1, id)\n+      insertStmt2.setBigDecimal(2, salary.bigDecimal)\n+      insertStmt2.setString(3, surname)\n+      insertStmt2.executeUpdate()\n+    }\n+    insertStmt2.close()\n+  }\n+\n+  /**\n+   * Runs the plan and makes sure the plans contains all of the keywords.\n+   */\n+  protected def checkKeywordsExistsInExplain(df: DataFrame, keywords: String*): Unit = {\n+    val output = new java.io.ByteArrayOutputStream()\n+    Console.withOut(output) {\n+      df.explain(extended = true)\n+    }\n+    val normalizedOutput = output.toString.replaceAll(\"#\\\\d+\", \"#x\")\n+    for (key <- keywords) {\n+      assert(normalizedOutput.contains(key), s\"Expected keyword '$key' not found in explain output\")\n+    }\n+  }\n+\n+  private def checkPushedInfo(df: DataFrame, expectedPlanFragment: String*): Unit = {\n+    withSQLConf(SQLConf.MAX_METADATA_STRING_LENGTH.key -> \"1000\") {\n+      df.queryExecution.optimizedPlan.collect {\n+        case _: DataSourceV2ScanRelation =>\n+          checkKeywordsExistsInExplain(df, expectedPlanFragment: _*)\n+      }\n+    }\n+  }\n+\n+  private def checkJoinNotPushed(df: DataFrame): Unit = {\n+    val joinNodes = df.queryExecution.optimizedPlan.collect {\n+      case j: Join => j\n+    }\n+    assert(joinNodes.nonEmpty, \"Join should not be pushed down\")\n+  }\n+\n+  private def checkJoinPushed(df: DataFrame, expectedTables: String*): Unit = {\n+    val joinNodes = df.queryExecution.optimizedPlan.collect {\n+      case j: Join => j\n+    }\n+    assert(joinNodes.isEmpty, \"Join should be pushed down\")\n+    if (expectedTables.nonEmpty) {\n+      checkPushedInfo(df, s\"PushedJoins: [${expectedTables.mkString(\", \")}]\")\n+    }\n+  }\n+\n+  test(\"Test basic inner join pushdown with column pruning\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname, t1.amount, t2.salary\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    // Verify we have non-empty results\n+    assert(rows.nonEmpty, \"Join should produce non-empty results\")\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+\n+  test(\"Test join with additional filters\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname, t1.amount, t2.salary\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |WHERE t1.amount > 5000 AND t2.salary > 25000\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkFilterPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test self join should be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.name as name2\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName1)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join without condition should not be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |CROSS JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinNotPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with complex condition\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname, t1.amount + t2.salary as total\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2\n+      |ON t1.id = t2.id AND t1.amount > 1000\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with limit and order\") {\n+    // ORDER BY is used to have same ordering on Spark and database. Otherwise, different results\n+    // could be returned.\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |ORDER BY t1.id, t1.name, t2.surname\n+      |LIMIT 5\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkSortRemoved(df)\n+      checkLimitRemoved(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with order by\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |ORDER BY t1.id, t1.name, t2.surname\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      // Order without limit is not supported in DSv2\n+      checkSortRemoved(df, false)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with aggregation\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, COUNT(*), AVG(t1.amount), MAX(t2.salary)\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |GROUP BY t1.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAggregateRemoved(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test left outer join should not be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |LEFT JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinNotPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test right outer join should not be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |RIGHT JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinNotPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test full outer join should not be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |FULL OUTER JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinNotPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with subquery should be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, sub.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN (\n+      |  SELECT id, surname FROM $catalogAndNamespace.${caseConvert(joinTableName2)}\n+      |  WHERE salary > 25000\n+      |) sub ON t1.id = sub.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with non-equality condition should be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id > t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+}",
        "comment_created_at": "2025-07-17T08:11:49+00:00",
        "comment_author": "andrej-db",
        "comment_body": "can we have an ANTI JOIN as well?\r\nor is that unsupported?",
        "pr_file_module": null
      },
      {
        "comment_id": "2213507967",
        "repo_full_name": "apache/spark",
        "pr_number": 51519,
        "pr_file": "connector/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/join/JDBCJoinPushdownIntegrationSuite.scala",
        "discussion_id": "2212635239",
        "commented_code": "@@ -0,0 +1,454 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.jdbc.v2.join\n+\n+import java.sql.Connection\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest}\n+import org.apache.spark.sql.catalyst.plans.logical.Join\n+import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.jdbc.{DockerIntegrationFunSuite, JdbcDialect}\n+import org.apache.spark.sql.jdbc.v2.V2JDBCPushdownTestUtils\n+import org.apache.spark.sql.test.SharedSparkSession\n+import org.apache.spark.sql.types.{DataType, DataTypes}\n+import org.apache.spark.tags.DockerTest\n+\n+@DockerTest\n+trait JDBCJoinPushdownIntegrationSuite\n+  extends QueryTest\n+  with SharedSparkSession\n+  with DockerIntegrationFunSuite\n+  with V2JDBCPushdownTestUtils {\n+  val catalogName: String\n+  val namespaceOpt: Option[String] = None\n+  val joinTableName1: String = \"join_table_1\"\n+  val joinTableName2: String = \"join_table_2\"\n+\n+  // Concrete suite must provide the dialect for its DB\n+  def jdbcDialect: JdbcDialect\n+\n+  private def catalogAndNamespace =\n+    namespaceOpt.map(namespace => s\"$catalogName.$namespace\").getOrElse(catalogName)\n+\n+  def fullyQualifiedTableName1: String = namespaceOpt\n+    .map(namespace => s\"$namespace.$joinTableName1\").getOrElse(joinTableName1)\n+\n+  def fullyQualifiedTableName2: String = namespaceOpt\n+    .map(namespace => s\"$namespace.$joinTableName2\").getOrElse(joinTableName2)\n+\n+  protected def getJDBCTypeString(dt: DataType): String = {\n+    JdbcUtils.getJdbcType(dt, jdbcDialect).databaseTypeDefinition.toUpperCase()\n+  }\n+\n+  protected def caseConvert(tableName: String): String = tableName\n+\n+  def dataPreparation(connection: Connection): Unit = {\n+    tablePreparation(connection)\n+    fillJoinTables(connection)\n+  }\n+\n+  def tablePreparation(connection: Connection): Unit = {\n+    connection.prepareStatement(\n+      s\"\"\"CREATE TABLE $fullyQualifiedTableName1 (\n+         |  id ${getJDBCTypeString(DataTypes.IntegerType)},\n+         |  amount ${getJDBCTypeString(DataTypes.createDecimalType(10, 2))},\n+         |  name ${getJDBCTypeString(DataTypes.StringType)}\n+         |)\"\"\".stripMargin).executeUpdate()\n+\n+    connection.prepareStatement(\n+      s\"\"\"CREATE TABLE $fullyQualifiedTableName2 (\n+         |  id ${getJDBCTypeString(DataTypes.LongType)},\n+         |  salary ${getJDBCTypeString(DataTypes.createDecimalType(10, 2))},\n+         |  surname ${getJDBCTypeString(DataTypes.StringType)}\n+         |)\"\"\".stripMargin).executeUpdate()\n+  }\n+\n+  def fillJoinTables(connection: Connection): Unit = {\n+    val random = new java.util.Random(42)\n+    val table1Data = (1 to 100).map { i =>\n+      val id = i % 11\n+      val amount = BigDecimal.valueOf(random.nextDouble() * 10000)\n+        .setScale(2, BigDecimal.RoundingMode.HALF_UP)\n+      val name = s\"name_$i\"\n+      (id, amount, name)\n+    }\n+    val table2Data = (1 to 100).map { i =>\n+      val id = (i % 17).toLong\n+      val salary = BigDecimal.valueOf(random.nextDouble() * 50000)\n+        .setScale(2, BigDecimal.RoundingMode.HALF_UP)\n+      val surname = s\"surname_$i\"\n+      (id, salary, surname)\n+    }\n+\n+    // Use parameterized queries to handle different data types properly\n+    val insertStmt1 = connection.prepareStatement(\n+      s\"INSERT INTO $fullyQualifiedTableName1 (id, amount, name) VALUES (?, ?, ?)\"\n+    )\n+    table1Data.foreach { case (id, amount, name) =>\n+      insertStmt1.setInt(1, id)\n+      insertStmt1.setBigDecimal(2, amount.bigDecimal)\n+      insertStmt1.setString(3, name)\n+      insertStmt1.executeUpdate()\n+    }\n+    insertStmt1.close()\n+\n+    val insertStmt2 = connection.prepareStatement(\n+      s\"INSERT INTO $fullyQualifiedTableName2 (id, salary, surname) VALUES (?, ?, ?)\"\n+    )\n+    table2Data.foreach { case (id, salary, surname) =>\n+      insertStmt2.setLong(1, id)\n+      insertStmt2.setBigDecimal(2, salary.bigDecimal)\n+      insertStmt2.setString(3, surname)\n+      insertStmt2.executeUpdate()\n+    }\n+    insertStmt2.close()\n+  }\n+\n+  /**\n+   * Runs the plan and makes sure the plans contains all of the keywords.\n+   */\n+  protected def checkKeywordsExistsInExplain(df: DataFrame, keywords: String*): Unit = {\n+    val output = new java.io.ByteArrayOutputStream()\n+    Console.withOut(output) {\n+      df.explain(extended = true)\n+    }\n+    val normalizedOutput = output.toString.replaceAll(\"#\\\\d+\", \"#x\")\n+    for (key <- keywords) {\n+      assert(normalizedOutput.contains(key), s\"Expected keyword '$key' not found in explain output\")\n+    }\n+  }\n+\n+  private def checkPushedInfo(df: DataFrame, expectedPlanFragment: String*): Unit = {\n+    withSQLConf(SQLConf.MAX_METADATA_STRING_LENGTH.key -> \"1000\") {\n+      df.queryExecution.optimizedPlan.collect {\n+        case _: DataSourceV2ScanRelation =>\n+          checkKeywordsExistsInExplain(df, expectedPlanFragment: _*)\n+      }\n+    }\n+  }\n+\n+  private def checkJoinNotPushed(df: DataFrame): Unit = {\n+    val joinNodes = df.queryExecution.optimizedPlan.collect {\n+      case j: Join => j\n+    }\n+    assert(joinNodes.nonEmpty, \"Join should not be pushed down\")\n+  }\n+\n+  private def checkJoinPushed(df: DataFrame, expectedTables: String*): Unit = {\n+    val joinNodes = df.queryExecution.optimizedPlan.collect {\n+      case j: Join => j\n+    }\n+    assert(joinNodes.isEmpty, \"Join should be pushed down\")\n+    if (expectedTables.nonEmpty) {\n+      checkPushedInfo(df, s\"PushedJoins: [${expectedTables.mkString(\", \")}]\")\n+    }\n+  }\n+\n+  test(\"Test basic inner join pushdown with column pruning\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname, t1.amount, t2.salary\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    // Verify we have non-empty results\n+    assert(rows.nonEmpty, \"Join should produce non-empty results\")\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+\n+  test(\"Test join with additional filters\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname, t1.amount, t2.salary\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |WHERE t1.amount > 5000 AND t2.salary > 25000\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkFilterPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test self join should be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.name as name2\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName1)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join without condition should not be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |CROSS JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinNotPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with complex condition\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname, t1.amount + t2.salary as total\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2\n+      |ON t1.id = t2.id AND t1.amount > 1000\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with limit and order\") {\n+    // ORDER BY is used to have same ordering on Spark and database. Otherwise, different results\n+    // could be returned.\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |ORDER BY t1.id, t1.name, t2.surname\n+      |LIMIT 5\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkSortRemoved(df)\n+      checkLimitRemoved(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with order by\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |ORDER BY t1.id, t1.name, t2.surname\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      // Order without limit is not supported in DSv2\n+      checkSortRemoved(df, false)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with aggregation\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, COUNT(*), AVG(t1.amount), MAX(t2.salary)\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |GROUP BY t1.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAggregateRemoved(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test left outer join should not be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |LEFT JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinNotPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test right outer join should not be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |RIGHT JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinNotPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test full outer join should not be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |FULL OUTER JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id = t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinNotPushed(df)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with subquery should be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, sub.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN (\n+      |  SELECT id, surname FROM $catalogAndNamespace.${caseConvert(joinTableName2)}\n+      |  WHERE salary > 25000\n+      |) sub ON t1.id = sub.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test join with non-equality condition should be pushed down\") {\n+    val sqlQuery = s\"\"\"\n+      |SELECT t1.id, t1.name, t2.surname\n+      |FROM $catalogAndNamespace.${caseConvert(joinTableName1)} t1\n+      |JOIN $catalogAndNamespace.${caseConvert(joinTableName2)} t2 ON t1.id > t2.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      checkJoinPushed(\n+        df,\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName1)}\",\n+        s\"$catalogAndNamespace.${caseConvert(joinTableName2)}\"\n+      )\n+      checkAnswer(df, rows)\n+    }\n+  }\n+}",
        "comment_created_at": "2025-07-17T14:25:36+00:00",
        "comment_author": "PetarVasiljevic-DB",
        "comment_body": "anti join is not supported for pushdown, but yes, Spark has anti join",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2215961485",
    "pr_number": 51560,
    "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/scripting/SqlScriptingExecutionSuite.scala",
    "created_at": "2025-07-18T12:45:47+00:00",
    "commented_code": ")\n     verifySqlScriptResult(sqlScript, expected = expected)\n   }\n+\n+  test(\"Integer literal column in FOR query\") {\n+    val sqlScript1 =\n+      \"\"\"\n+        |BEGIN\n+        |  FOR SELECT 1 DO\n+        |    SELECT 1;\n+        |  END FOR;\n+        |END\n+        |\"\"\".stripMargin\n+    val expected = Seq(Seq(Row(1)))\n+    verifySqlScriptResult(sqlScript1, expected)\n+\n+    val sqlScript2 =\n+      \"\"\"\n+        |BEGIN\n+        |  FOR x AS SELECT 1 DO\n+        |    SELECT x.`1`;",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2215961485",
        "repo_full_name": "apache/spark",
        "pr_number": 51560,
        "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/scripting/SqlScriptingExecutionSuite.scala",
        "discussion_id": "2215961485",
        "commented_code": "@@ -2938,4 +2938,50 @@ class SqlScriptingExecutionSuite extends QueryTest with SharedSparkSession {\n     )\n     verifySqlScriptResult(sqlScript, expected = expected)\n   }\n+\n+  test(\"Integer literal column in FOR query\") {\n+    val sqlScript1 =\n+      \"\"\"\n+        |BEGIN\n+        |  FOR SELECT 1 DO\n+        |    SELECT 1;\n+        |  END FOR;\n+        |END\n+        |\"\"\".stripMargin\n+    val expected = Seq(Seq(Row(1)))\n+    verifySqlScriptResult(sqlScript1, expected)\n+\n+    val sqlScript2 =\n+      \"\"\"\n+        |BEGIN\n+        |  FOR x AS SELECT 1 DO\n+        |    SELECT x.`1`;",
        "comment_created_at": "2025-07-18T12:45:47+00:00",
        "comment_author": "miland-db",
        "comment_body": "What would happen if we do `SELECT x.1`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2215978172",
        "repo_full_name": "apache/spark",
        "pr_number": 51560,
        "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/scripting/SqlScriptingExecutionSuite.scala",
        "discussion_id": "2215961485",
        "commented_code": "@@ -2938,4 +2938,50 @@ class SqlScriptingExecutionSuite extends QueryTest with SharedSparkSession {\n     )\n     verifySqlScriptResult(sqlScript, expected = expected)\n   }\n+\n+  test(\"Integer literal column in FOR query\") {\n+    val sqlScript1 =\n+      \"\"\"\n+        |BEGIN\n+        |  FOR SELECT 1 DO\n+        |    SELECT 1;\n+        |  END FOR;\n+        |END\n+        |\"\"\".stripMargin\n+    val expected = Seq(Seq(Row(1)))\n+    verifySqlScriptResult(sqlScript1, expected)\n+\n+    val sqlScript2 =\n+      \"\"\"\n+        |BEGIN\n+        |  FOR x AS SELECT 1 DO\n+        |    SELECT x.`1`;",
        "comment_created_at": "2025-07-18T12:53:31+00:00",
        "comment_author": "umedvedev",
        "comment_body": "Nothing.\r\nHowever, in case of real column names whose naming conventions might be far away from good coding practices it might be much more beneficial:\r\n<img width=\"1029\" height=\"534\" alt=\"image\" src=\"https://github.com/user-attachments/assets/583c0bfb-8f03-4942-a670-2827d19af399\" />\r\n\r\nThis case is also referenced on task https://issues.apache.org/jira/browse/SPARK-52870\r\n\r\n<img width=\"587\" height=\"229\" alt=\"image\" src=\"https://github.com/user-attachments/assets/e7e74316-6064-424a-b126-169c0274c134\" />\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2216104963",
        "repo_full_name": "apache/spark",
        "pr_number": 51560,
        "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/scripting/SqlScriptingExecutionSuite.scala",
        "discussion_id": "2215961485",
        "commented_code": "@@ -2938,4 +2938,50 @@ class SqlScriptingExecutionSuite extends QueryTest with SharedSparkSession {\n     )\n     verifySqlScriptResult(sqlScript, expected = expected)\n   }\n+\n+  test(\"Integer literal column in FOR query\") {\n+    val sqlScript1 =\n+      \"\"\"\n+        |BEGIN\n+        |  FOR SELECT 1 DO\n+        |    SELECT 1;\n+        |  END FOR;\n+        |END\n+        |\"\"\".stripMargin\n+    val expected = Seq(Seq(Row(1)))\n+    verifySqlScriptResult(sqlScript1, expected)\n+\n+    val sqlScript2 =\n+      \"\"\"\n+        |BEGIN\n+        |  FOR x AS SELECT 1 DO\n+        |    SELECT x.`1`;",
        "comment_created_at": "2025-07-18T13:54:55+00:00",
        "comment_author": "dusantism-db",
        "comment_body": "> What would happen if we do `SELECT x.1`?\r\n\r\nA parse exception occurs, as SELECT x.1 by itself is not a valid query according to the grammar.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2108668373",
    "pr_number": 50921,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
    "created_at": "2025-05-27T09:12:26+00:00",
    "commented_code": "filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are 2 same tables in both sides of top level join and it not possible\n+    // to fully qualified the column names in condition. Therefore, query should be rewritten so\n+    // that each of the outputs of child joins are aliased, so there would be a projection\n+    // with aliases between top level join and scanBuilderHolder (that has pushed child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+      // TODO: I think projections will always be Seq[AttributeReference] because\n+      // When\n+      // SELECT tbl1.col+2, tbl2.* FROM tbl1 JOIN tlb2\n+      // is executed, col is pruned down, but col + 2 will be projected on top of join.\n+      leftProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      rightProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      lBuilder.isRightSideCompatibleForJoin(rBuilder) =>\n+      val normalizedLeftProjections = DataSourceStrategy.normalizeExprs(\n+        leftProjections,\n+        leftHolder.output\n+      ).asInstanceOf[Seq[AttributeReference]]\n+      val leftRequiredSchema = fromAttributes(normalizedLeftProjections)\n+\n+      val normalizedRightProjections = DataSourceStrategy.normalizeExprs(\n+        rightProjections,\n+        rightHolder.output\n+      ).asInstanceOf[Seq[AttributeReference]]\n+      val rightRequiredSchema = fromAttributes(normalizedRightProjections)\n+\n+      val normalizedCondition = condition.map { e =>\n+        DataSourceStrategy.normalizeExprs(\n+          Seq(e),\n+          leftHolder.output ++ rightHolder.output\n+        ).head\n+      }\n+\n+      val conditionWithJoinColumns = normalizedCondition.map { cond =>\n+        cond.transformUp {\n+          case a: AttributeReference =>\n+            val isInLeftSide = leftProjections.filter(_.exprId == a.exprId).nonEmpty\n+            JoinColumnReference(a, isInLeftSide)\n+        }\n+      }\n+\n+      val translatedCondition =\n+        conditionWithJoinColumns.flatMap(DataSourceV2Strategy.translateFilterV2(_))\n+      val translatedJoinType = DataSourceStrategy.translateJoinType(joinType)\n+\n+      if (translatedCondition.isDefined == condition.isDefined &&\n+        translatedJoinType.isDefined &&\n+        lBuilder.pushJoin(\n+          rBuilder,\n+          translatedJoinType.get,\n+          translatedCondition.toJava,\n+          leftRequiredSchema,\n+          rightRequiredSchema\n+        )) {\n+        leftHolder.joinedRelations = leftHolder.joinedRelations :+ rightHolder.relation\n+\n+        val newSchema = leftHolder.builder.build().readSchema()\n+        val newOutput = (leftProjections ++ rightProjections).asInstanceOf[Seq[AttributeReference]]\n+          .zip(newSchema.fields)",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2108668373",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2108668373",
        "commented_code": "@@ -98,6 +108,100 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n       filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are 2 same tables in both sides of top level join and it not possible\n+    // to fully qualified the column names in condition. Therefore, query should be rewritten so\n+    // that each of the outputs of child joins are aliased, so there would be a projection\n+    // with aliases between top level join and scanBuilderHolder (that has pushed child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+      // TODO: I think projections will always be Seq[AttributeReference] because\n+      // When\n+      // SELECT tbl1.col+2, tbl2.* FROM tbl1 JOIN tlb2\n+      // is executed, col is pruned down, but col + 2 will be projected on top of join.\n+      leftProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      rightProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      lBuilder.isRightSideCompatibleForJoin(rBuilder) =>\n+      val normalizedLeftProjections = DataSourceStrategy.normalizeExprs(\n+        leftProjections,\n+        leftHolder.output\n+      ).asInstanceOf[Seq[AttributeReference]]\n+      val leftRequiredSchema = fromAttributes(normalizedLeftProjections)\n+\n+      val normalizedRightProjections = DataSourceStrategy.normalizeExprs(\n+        rightProjections,\n+        rightHolder.output\n+      ).asInstanceOf[Seq[AttributeReference]]\n+      val rightRequiredSchema = fromAttributes(normalizedRightProjections)\n+\n+      val normalizedCondition = condition.map { e =>\n+        DataSourceStrategy.normalizeExprs(\n+          Seq(e),\n+          leftHolder.output ++ rightHolder.output\n+        ).head\n+      }\n+\n+      val conditionWithJoinColumns = normalizedCondition.map { cond =>\n+        cond.transformUp {\n+          case a: AttributeReference =>\n+            val isInLeftSide = leftProjections.filter(_.exprId == a.exprId).nonEmpty\n+            JoinColumnReference(a, isInLeftSide)\n+        }\n+      }\n+\n+      val translatedCondition =\n+        conditionWithJoinColumns.flatMap(DataSourceV2Strategy.translateFilterV2(_))\n+      val translatedJoinType = DataSourceStrategy.translateJoinType(joinType)\n+\n+      if (translatedCondition.isDefined == condition.isDefined &&\n+        translatedJoinType.isDefined &&\n+        lBuilder.pushJoin(\n+          rBuilder,\n+          translatedJoinType.get,\n+          translatedCondition.toJava,\n+          leftRequiredSchema,\n+          rightRequiredSchema\n+        )) {\n+        leftHolder.joinedRelations = leftHolder.joinedRelations :+ rightHolder.relation\n+\n+        val newSchema = leftHolder.builder.build().readSchema()\n+        val newOutput = (leftProjections ++ rightProjections).asInstanceOf[Seq[AttributeReference]]\n+          .zip(newSchema.fields)",
        "comment_created_at": "2025-05-27T09:12:26+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "We should fail if the number of columns doesn't match between Spark and the third-party data source",
        "pr_file_module": null
      },
      {
        "comment_id": "2108670200",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2108668373",
        "commented_code": "@@ -98,6 +108,100 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n       filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are 2 same tables in both sides of top level join and it not possible\n+    // to fully qualified the column names in condition. Therefore, query should be rewritten so\n+    // that each of the outputs of child joins are aliased, so there would be a projection\n+    // with aliases between top level join and scanBuilderHolder (that has pushed child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+      // TODO: I think projections will always be Seq[AttributeReference] because\n+      // When\n+      // SELECT tbl1.col+2, tbl2.* FROM tbl1 JOIN tlb2\n+      // is executed, col is pruned down, but col + 2 will be projected on top of join.\n+      leftProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      rightProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      lBuilder.isRightSideCompatibleForJoin(rBuilder) =>\n+      val normalizedLeftProjections = DataSourceStrategy.normalizeExprs(\n+        leftProjections,\n+        leftHolder.output\n+      ).asInstanceOf[Seq[AttributeReference]]\n+      val leftRequiredSchema = fromAttributes(normalizedLeftProjections)\n+\n+      val normalizedRightProjections = DataSourceStrategy.normalizeExprs(\n+        rightProjections,\n+        rightHolder.output\n+      ).asInstanceOf[Seq[AttributeReference]]\n+      val rightRequiredSchema = fromAttributes(normalizedRightProjections)\n+\n+      val normalizedCondition = condition.map { e =>\n+        DataSourceStrategy.normalizeExprs(\n+          Seq(e),\n+          leftHolder.output ++ rightHolder.output\n+        ).head\n+      }\n+\n+      val conditionWithJoinColumns = normalizedCondition.map { cond =>\n+        cond.transformUp {\n+          case a: AttributeReference =>\n+            val isInLeftSide = leftProjections.filter(_.exprId == a.exprId).nonEmpty\n+            JoinColumnReference(a, isInLeftSide)\n+        }\n+      }\n+\n+      val translatedCondition =\n+        conditionWithJoinColumns.flatMap(DataSourceV2Strategy.translateFilterV2(_))\n+      val translatedJoinType = DataSourceStrategy.translateJoinType(joinType)\n+\n+      if (translatedCondition.isDefined == condition.isDefined &&\n+        translatedJoinType.isDefined &&\n+        lBuilder.pushJoin(\n+          rBuilder,\n+          translatedJoinType.get,\n+          translatedCondition.toJava,\n+          leftRequiredSchema,\n+          rightRequiredSchema\n+        )) {\n+        leftHolder.joinedRelations = leftHolder.joinedRelations :+ rightHolder.relation\n+\n+        val newSchema = leftHolder.builder.build().readSchema()\n+        val newOutput = (leftProjections ++ rightProjections).asInstanceOf[Seq[AttributeReference]]\n+          .zip(newSchema.fields)",
        "comment_created_at": "2025-05-27T09:13:19+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "also check the data type.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2108703627",
    "pr_number": 50921,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
    "created_at": "2025-05-27T09:28:57+00:00",
    "commented_code": "}\n   }\n \n+  override def isRightSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url\n+  };\n+\n+  override def pushJoin(\n+    other: SupportsPushDownJoin,\n+    joinType: JoinType,\n+    condition: Optional[Predicate],\n+    leftRequiredSchema: StructType,\n+    rightRequiredSchema: StructType\n+  ): Boolean = {\n+    if (!jdbcOptions.pushDownJoin || !dialect.supportsJoin) return false\n+\n+    val leftNodeSQLQuery = buildSQLQuery()\n+    val rightNodeSQLQuery = other.asInstanceOf[JDBCScanBuilder].buildSQLQuery()\n+\n+    val leftSideQualifier = JoinOutputAliasIterator.get\n+    val rightSideQualifier = JoinOutputAliasIterator.get\n+\n+    val leftProjections: Seq[JoinColumn] = leftRequiredSchema.fields.map { e =>\n+      new JoinColumn(Array(leftSideQualifier), e.name, true)\n+    }.toSeq\n+    val rightProjections: Seq[JoinColumn] = rightRequiredSchema.fields.map { e =>\n+      new JoinColumn(Array(rightSideQualifier), e.name, false)\n+    }.toSeq\n+\n+    var aliasedLeftSchema = StructType(Seq())\n+    var aliasedRightSchema = StructType(Seq())\n+    val outputAliasPrefix = JoinOutputAliasIterator.get\n+\n+    val aliasedOutput = (leftProjections ++ rightProjections)\n+      .zipWithIndex\n+      .map { case (proj, i) =>\n+        val name = s\"${outputAliasPrefix}_col_$i\"\n+        val output = FieldReference(name)\n+        if (i < leftProjections.length) {\n+          val field = leftRequiredSchema.fields(i)\n+          aliasedLeftSchema =\n+            aliasedLeftSchema.add(name, field.dataType, field.nullable, field.metadata)\n+        } else {\n+          val field = rightRequiredSchema.fields(i - leftRequiredSchema.fields.length)\n+          aliasedRightSchema =\n+            aliasedRightSchema.add(name, field.dataType, field.nullable, field.metadata)\n+        }\n+\n+        s\"\"\"${dialect.compileExpression(proj).get} AS ${dialect.compileExpression(output).get}\"\"\"\n+      }.mkString(\",\")\n+\n+    val compiledJoinType = dialect.compileJoinType(joinType)\n+    if (!compiledJoinType.isDefined) return false\n+\n+    val conditionString = condition.toScala match {\n+      case Some(cond) =>\n+        qualifyCondition(cond, leftSideQualifier, rightSideQualifier)\n+        s\"ON ${dialect.compileExpression(cond).get}\"",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2108703627",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
        "discussion_id": "2108703627",
        "commented_code": "@@ -121,6 +126,128 @@ case class JDBCScanBuilder(\n     }\n   }\n \n+  override def isRightSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url\n+  };\n+\n+  override def pushJoin(\n+    other: SupportsPushDownJoin,\n+    joinType: JoinType,\n+    condition: Optional[Predicate],\n+    leftRequiredSchema: StructType,\n+    rightRequiredSchema: StructType\n+  ): Boolean = {\n+    if (!jdbcOptions.pushDownJoin || !dialect.supportsJoin) return false\n+\n+    val leftNodeSQLQuery = buildSQLQuery()\n+    val rightNodeSQLQuery = other.asInstanceOf[JDBCScanBuilder].buildSQLQuery()\n+\n+    val leftSideQualifier = JoinOutputAliasIterator.get\n+    val rightSideQualifier = JoinOutputAliasIterator.get\n+\n+    val leftProjections: Seq[JoinColumn] = leftRequiredSchema.fields.map { e =>\n+      new JoinColumn(Array(leftSideQualifier), e.name, true)\n+    }.toSeq\n+    val rightProjections: Seq[JoinColumn] = rightRequiredSchema.fields.map { e =>\n+      new JoinColumn(Array(rightSideQualifier), e.name, false)\n+    }.toSeq\n+\n+    var aliasedLeftSchema = StructType(Seq())\n+    var aliasedRightSchema = StructType(Seq())\n+    val outputAliasPrefix = JoinOutputAliasIterator.get\n+\n+    val aliasedOutput = (leftProjections ++ rightProjections)\n+      .zipWithIndex\n+      .map { case (proj, i) =>\n+        val name = s\"${outputAliasPrefix}_col_$i\"\n+        val output = FieldReference(name)\n+        if (i < leftProjections.length) {\n+          val field = leftRequiredSchema.fields(i)\n+          aliasedLeftSchema =\n+            aliasedLeftSchema.add(name, field.dataType, field.nullable, field.metadata)\n+        } else {\n+          val field = rightRequiredSchema.fields(i - leftRequiredSchema.fields.length)\n+          aliasedRightSchema =\n+            aliasedRightSchema.add(name, field.dataType, field.nullable, field.metadata)\n+        }\n+\n+        s\"\"\"${dialect.compileExpression(proj).get} AS ${dialect.compileExpression(output).get}\"\"\"\n+      }.mkString(\",\")\n+\n+    val compiledJoinType = dialect.compileJoinType(joinType)\n+    if (!compiledJoinType.isDefined) return false\n+\n+    val conditionString = condition.toScala match {\n+      case Some(cond) =>\n+        qualifyCondition(cond, leftSideQualifier, rightSideQualifier)\n+        s\"ON ${dialect.compileExpression(cond).get}\"",
        "comment_created_at": "2025-05-27T09:28:57+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "I think it's safer to pass the generated subquery aliases to the `compileExpression` function (or add a new `compileJoinCondition` function), which should respect the aliases when generating SQL for JoinColumn. It's better than making `JoinColumn` mutable.",
        "pr_file_module": null
      },
      {
        "comment_id": "2111889969",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
        "discussion_id": "2108703627",
        "commented_code": "@@ -121,6 +126,128 @@ case class JDBCScanBuilder(\n     }\n   }\n \n+  override def isRightSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url\n+  };\n+\n+  override def pushJoin(\n+    other: SupportsPushDownJoin,\n+    joinType: JoinType,\n+    condition: Optional[Predicate],\n+    leftRequiredSchema: StructType,\n+    rightRequiredSchema: StructType\n+  ): Boolean = {\n+    if (!jdbcOptions.pushDownJoin || !dialect.supportsJoin) return false\n+\n+    val leftNodeSQLQuery = buildSQLQuery()\n+    val rightNodeSQLQuery = other.asInstanceOf[JDBCScanBuilder].buildSQLQuery()\n+\n+    val leftSideQualifier = JoinOutputAliasIterator.get\n+    val rightSideQualifier = JoinOutputAliasIterator.get\n+\n+    val leftProjections: Seq[JoinColumn] = leftRequiredSchema.fields.map { e =>\n+      new JoinColumn(Array(leftSideQualifier), e.name, true)\n+    }.toSeq\n+    val rightProjections: Seq[JoinColumn] = rightRequiredSchema.fields.map { e =>\n+      new JoinColumn(Array(rightSideQualifier), e.name, false)\n+    }.toSeq\n+\n+    var aliasedLeftSchema = StructType(Seq())\n+    var aliasedRightSchema = StructType(Seq())\n+    val outputAliasPrefix = JoinOutputAliasIterator.get\n+\n+    val aliasedOutput = (leftProjections ++ rightProjections)\n+      .zipWithIndex\n+      .map { case (proj, i) =>\n+        val name = s\"${outputAliasPrefix}_col_$i\"\n+        val output = FieldReference(name)\n+        if (i < leftProjections.length) {\n+          val field = leftRequiredSchema.fields(i)\n+          aliasedLeftSchema =\n+            aliasedLeftSchema.add(name, field.dataType, field.nullable, field.metadata)\n+        } else {\n+          val field = rightRequiredSchema.fields(i - leftRequiredSchema.fields.length)\n+          aliasedRightSchema =\n+            aliasedRightSchema.add(name, field.dataType, field.nullable, field.metadata)\n+        }\n+\n+        s\"\"\"${dialect.compileExpression(proj).get} AS ${dialect.compileExpression(output).get}\"\"\"\n+      }.mkString(\",\")\n+\n+    val compiledJoinType = dialect.compileJoinType(joinType)\n+    if (!compiledJoinType.isDefined) return false\n+\n+    val conditionString = condition.toScala match {\n+      case Some(cond) =>\n+        qualifyCondition(cond, leftSideQualifier, rightSideQualifier)\n+        s\"ON ${dialect.compileExpression(cond).get}\"",
        "comment_created_at": "2025-05-28T13:21:21+00:00",
        "comment_author": "PetarVasiljevic-DB",
        "comment_body": "Adding new method is a bit tricky because we would need to fallback to `compileExpression` in case the expression is not `JoinColumn`. We can't easily call the overriden `compileExpression` methods.\r\n\r\nI went with expanding JDBCSQLBuilder with left and right qualifier opt. It seems like an overkill to just support `JoinColumn` but I think this is the safest way.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2175066636",
    "pr_number": 50921,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
    "created_at": "2025-06-30T13:21:22+00:00",
    "commented_code": "}\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url\n+  };\n+\n+  override def pushJoin(\n+    other: SupportsPushDownJoin,\n+    joinType: JoinType,\n+    condition: Predicate,\n+    leftSideQualifier: Array[String],\n+    rightSideQualifier: Array[String],\n+    leftSideRequiredSchema: StructType,\n+    rightSideRequiredSchema: StructType\n+  ): Boolean = {\n+    if (!jdbcOptions.pushDownJoin || !dialect.supportsJoin) return false\n+\n+    this.pushedPredicate = this.pushedPredicate :+ condition\n+\n+    // We are getting the qualifier of leftmost relation in tree.\n+    if (joinedScanBuilders.isEmpty) {\n+      originalScanTableName = jdbcOptions.tableOrQuery\n+      firstRelationQualifier = leftSideQualifier\n+    }\n+\n+    val otherJdbcScanBuilder = other.asInstanceOf[JDBCScanBuilder]\n+\n+    var newSchema = StructType(Seq())\n+\n+    // 1. If this is the first join in the chain then calculate the new schema for left side that\n+    // will have qualified fields.\n+    // 2. If this is not the first join in the chain, inherit the previous schema as it is already\n+    // qualified.\n+    newSchema = if (joinedScanBuilders.isEmpty) leftSideRequiredSchema else finalSchema\n+\n+    // Struct field names are unique, so we are fine with merging them\n+    newSchema = newSchema.merge(rightSideRequiredSchema)\n+\n+    val joinTypeString = joinType match {\n+      case JoinType.INNER_JOIN => \"INNER JOIN\"\n+      case _ =>\n+        val params = Map(\"joinType\" -> String.valueOf(joinType))\n+        throw new SparkUnsupportedOperationException(\"UNSUPPORTED_JOIN_TYPES\", params)\n+    }\n+\n+    joinedScanBuilders = joinedScanBuilders :+\n+      JoinedJDBCScanBuilderInfo(\n+        otherJdbcScanBuilder.jdbcOptions,\n+        joinTypeString,\n+        Some(rightSideQualifier.mkString(\".\")))\n+    finalSchema = newSchema\n+\n+    var joinClause = originalScanTableName\n+    joinClause += \" \" + firstRelationQualifier.mkString(\".\")\n+    joinedScanBuilders.foreach{jsb =>\n+      joinClause +=\n+        s\" ${jsb.joinType} ${jsb.options.tableOrQuery} ${jsb.relationQualifier.getOrElse(\"\")}\"\n+    }\n+\n+    val joinedRelationsQuery =\n+      s\"\"\"\n+         |$joinClause\n+         |\"\"\".stripMargin\n+\n+    val newMap = jdbcOptions.parameters.originalMap +",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2175066636",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
        "discussion_id": "2175066636",
        "commented_code": "@@ -121,6 +137,76 @@ case class JDBCScanBuilder(\n     }\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url\n+  };\n+\n+  override def pushJoin(\n+    other: SupportsPushDownJoin,\n+    joinType: JoinType,\n+    condition: Predicate,\n+    leftSideQualifier: Array[String],\n+    rightSideQualifier: Array[String],\n+    leftSideRequiredSchema: StructType,\n+    rightSideRequiredSchema: StructType\n+  ): Boolean = {\n+    if (!jdbcOptions.pushDownJoin || !dialect.supportsJoin) return false\n+\n+    this.pushedPredicate = this.pushedPredicate :+ condition\n+\n+    // We are getting the qualifier of leftmost relation in tree.\n+    if (joinedScanBuilders.isEmpty) {\n+      originalScanTableName = jdbcOptions.tableOrQuery\n+      firstRelationQualifier = leftSideQualifier\n+    }\n+\n+    val otherJdbcScanBuilder = other.asInstanceOf[JDBCScanBuilder]\n+\n+    var newSchema = StructType(Seq())\n+\n+    // 1. If this is the first join in the chain then calculate the new schema for left side that\n+    // will have qualified fields.\n+    // 2. If this is not the first join in the chain, inherit the previous schema as it is already\n+    // qualified.\n+    newSchema = if (joinedScanBuilders.isEmpty) leftSideRequiredSchema else finalSchema\n+\n+    // Struct field names are unique, so we are fine with merging them\n+    newSchema = newSchema.merge(rightSideRequiredSchema)\n+\n+    val joinTypeString = joinType match {\n+      case JoinType.INNER_JOIN => \"INNER JOIN\"\n+      case _ =>\n+        val params = Map(\"joinType\" -> String.valueOf(joinType))\n+        throw new SparkUnsupportedOperationException(\"UNSUPPORTED_JOIN_TYPES\", params)\n+    }\n+\n+    joinedScanBuilders = joinedScanBuilders :+\n+      JoinedJDBCScanBuilderInfo(\n+        otherJdbcScanBuilder.jdbcOptions,\n+        joinTypeString,\n+        Some(rightSideQualifier.mkString(\".\")))\n+    finalSchema = newSchema\n+\n+    var joinClause = originalScanTableName\n+    joinClause += \" \" + firstRelationQualifier.mkString(\".\")\n+    joinedScanBuilders.foreach{jsb =>\n+      joinClause +=\n+        s\" ${jsb.joinType} ${jsb.options.tableOrQuery} ${jsb.relationQualifier.getOrElse(\"\")}\"\n+    }\n+\n+    val joinedRelationsQuery =\n+      s\"\"\"\n+         |$joinClause\n+         |\"\"\".stripMargin\n+\n+    val newMap = jdbcOptions.parameters.originalMap +",
        "comment_created_at": "2025-06-30T13:21:22+00:00",
        "comment_author": "PetarVasiljevic-DB",
        "comment_body": "a little bit tricky, but we need to put it in table options instead of query options because JDBCOptions.tableOrQuery would wrap the query with parentheses. And we are putting only the joined relations, without filters or pruned schema.\r\n\r\nIf we crafted a subquery with pruned columns, it wouldn't be supported by some dialects because we would get a query \r\n`SELECT min(a.col), b.col FROM (SELECT a.col, b.col FROM tbl a JOIN tbl b) GROUP BY b.col`\r\n\r\nThe query above maybe doesn't work, because we can't reference the column `a.col` after the first projection within subquery. This happens in Oracle for example",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2184476378",
    "pr_number": 50921,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
    "created_at": "2025-07-04T05:51:43+00:00",
    "commented_code": "filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are 2 same tables in both sides of top level join and it not possible\n+    // to fully qualified the column names in condition. Therefore, query should be rewritten so\n+    // that each of the outputs of child joins are aliased, so there would be a projection\n+    // with aliases between top level join and scanBuilderHolder (that has pushed child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+      // We do not support pushing down anything besides AttributeReference.\n+      leftProjections.forall(_.isInstanceOf[AttributeReference]) &&",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2184476378",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2184476378",
        "commented_code": "@@ -98,6 +108,117 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n       filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are 2 same tables in both sides of top level join and it not possible\n+    // to fully qualified the column names in condition. Therefore, query should be rewritten so\n+    // that each of the outputs of child joins are aliased, so there would be a projection\n+    // with aliases between top level join and scanBuilderHolder (that has pushed child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+      // We do not support pushing down anything besides AttributeReference.\n+      leftProjections.forall(_.isInstanceOf[AttributeReference]) &&",
        "comment_created_at": "2025-07-04T05:51:43+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "how about `ExtractValue`, which means nested columns? Do we plan to support it?",
        "pr_file_module": null
      },
      {
        "comment_id": "2185005748",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2184476378",
        "commented_code": "@@ -98,6 +108,117 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n       filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are 2 same tables in both sides of top level join and it not possible\n+    // to fully qualified the column names in condition. Therefore, query should be rewritten so\n+    // that each of the outputs of child joins are aliased, so there would be a projection\n+    // with aliases between top level join and scanBuilderHolder (that has pushed child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+      // We do not support pushing down anything besides AttributeReference.\n+      leftProjections.forall(_.isInstanceOf[AttributeReference]) &&",
        "comment_created_at": "2025-07-04T10:34:50+00:00",
        "comment_author": "PetarVasiljevic-DB",
        "comment_body": "From code:\r\n```\r\nobject PhysicalOperation extends OperationHelper {\r\n  // Returns: (the final project list, filters to push down, relation)\r\n  type ReturnType = (Seq[NamedExpression], Seq[Expression], LogicalPlan)\r\n ```\r\n\r\ntherefore projections should never be `ExtractValue`. Whole DSv2 relies on `PhysicalOperation` so if we plan to support `ExtractValue` it not scoped down to join only but other rules as well.\r\n\r\nFor now, I don't plan to support it.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2192152901",
    "pr_number": 50921,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
    "created_at": "2025-07-08T10:49:19+00:00",
    "commented_code": "filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are 2 same tables in both sides of top level join and it not possible\n+    // to fully qualified the column names in condition. Therefore, query should be rewritten so\n+    // that each of the outputs of child joins are aliased, so there would be a projection\n+    // with aliases between top level join and scanBuilderHolder (that has pushed child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+      // We do not support pushing down anything besides AttributeReference.\n+      leftProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      rightProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      // Cross joins are not supported because they increase the amount of data.\n+      condition.isDefined &&\n+      // Joins on top of sampled tables are not supported\n+      leftHolder.pushedSample.isEmpty &&\n+      rightHolder.pushedSample.isEmpty &&\n+      lBuilder.isOtherSideCompatibleForJoin(rBuilder) =>\n+\n+      // projections' names are maybe not up to date if the joins have been previously pushed down.\n+      // For this reason, we need to use pushedJoinOutputMap to get up to date names.\n+      //\n+      // Normalized projections are then converted to StructType.\n+      def getRequiredSchema(\n+          projections: Seq[NamedExpression],\n+          sHolder: ScanBuilderHolder): StructType = {\n+        val normalizedProjections = DataSourceStrategy.normalizeExprs(\n+          projections,\n+          sHolder.output.map { a =>\n+            sHolder.pushedJoinOutputMap.getOrElse(a, a).asInstanceOf[AttributeReference]\n+          }\n+        ).asInstanceOf[Seq[AttributeReference]]\n+\n+        fromAttributes(normalizedProjections)\n+      }\n+\n+      def generateJoinOutputAlias(name: String): String =\n+        s\"${name}_${java.util.UUID.randomUUID().toString.take(5)}\"\n+\n+      val leftSideRequiredSchema = getRequiredSchema(leftProjections, leftHolder)\n+      val rightSideRequiredSchema = getRequiredSchema(rightProjections, rightHolder)\n+\n+      // Alias the duplicated columns from left side of the join.",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2192152901",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2192152901",
        "commented_code": "@@ -98,6 +107,138 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n       filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are 2 same tables in both sides of top level join and it not possible\n+    // to fully qualified the column names in condition. Therefore, query should be rewritten so\n+    // that each of the outputs of child joins are aliased, so there would be a projection\n+    // with aliases between top level join and scanBuilderHolder (that has pushed child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+      // We do not support pushing down anything besides AttributeReference.\n+      leftProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      rightProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      // Cross joins are not supported because they increase the amount of data.\n+      condition.isDefined &&\n+      // Joins on top of sampled tables are not supported\n+      leftHolder.pushedSample.isEmpty &&\n+      rightHolder.pushedSample.isEmpty &&\n+      lBuilder.isOtherSideCompatibleForJoin(rBuilder) =>\n+\n+      // projections' names are maybe not up to date if the joins have been previously pushed down.\n+      // For this reason, we need to use pushedJoinOutputMap to get up to date names.\n+      //\n+      // Normalized projections are then converted to StructType.\n+      def getRequiredSchema(\n+          projections: Seq[NamedExpression],\n+          sHolder: ScanBuilderHolder): StructType = {\n+        val normalizedProjections = DataSourceStrategy.normalizeExprs(\n+          projections,\n+          sHolder.output.map { a =>\n+            sHolder.pushedJoinOutputMap.getOrElse(a, a).asInstanceOf[AttributeReference]\n+          }\n+        ).asInstanceOf[Seq[AttributeReference]]\n+\n+        fromAttributes(normalizedProjections)\n+      }\n+\n+      def generateJoinOutputAlias(name: String): String =\n+        s\"${name}_${java.util.UUID.randomUUID().toString.take(5)}\"\n+\n+      val leftSideRequiredSchema = getRequiredSchema(leftProjections, leftHolder)\n+      val rightSideRequiredSchema = getRequiredSchema(rightProjections, rightHolder)\n+\n+      // Alias the duplicated columns from left side of the join.",
        "comment_created_at": "2025-07-08T10:49:19+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "how about name conflicts within one side? e.g. `(SELECT a, a FROM t) JOIN ...`",
        "pr_file_module": null
      },
      {
        "comment_id": "2192341600",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2192152901",
        "commented_code": "@@ -98,6 +107,138 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n       filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are 2 same tables in both sides of top level join and it not possible\n+    // to fully qualified the column names in condition. Therefore, query should be rewritten so\n+    // that each of the outputs of child joins are aliased, so there would be a projection\n+    // with aliases between top level join and scanBuilderHolder (that has pushed child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+      // We do not support pushing down anything besides AttributeReference.\n+      leftProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      rightProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      // Cross joins are not supported because they increase the amount of data.\n+      condition.isDefined &&\n+      // Joins on top of sampled tables are not supported\n+      leftHolder.pushedSample.isEmpty &&\n+      rightHolder.pushedSample.isEmpty &&\n+      lBuilder.isOtherSideCompatibleForJoin(rBuilder) =>\n+\n+      // projections' names are maybe not up to date if the joins have been previously pushed down.\n+      // For this reason, we need to use pushedJoinOutputMap to get up to date names.\n+      //\n+      // Normalized projections are then converted to StructType.\n+      def getRequiredSchema(\n+          projections: Seq[NamedExpression],\n+          sHolder: ScanBuilderHolder): StructType = {\n+        val normalizedProjections = DataSourceStrategy.normalizeExprs(\n+          projections,\n+          sHolder.output.map { a =>\n+            sHolder.pushedJoinOutputMap.getOrElse(a, a).asInstanceOf[AttributeReference]\n+          }\n+        ).asInstanceOf[Seq[AttributeReference]]\n+\n+        fromAttributes(normalizedProjections)\n+      }\n+\n+      def generateJoinOutputAlias(name: String): String =\n+        s\"${name}_${java.util.UUID.randomUUID().toString.take(5)}\"\n+\n+      val leftSideRequiredSchema = getRequiredSchema(leftProjections, leftHolder)\n+      val rightSideRequiredSchema = getRequiredSchema(rightProjections, rightHolder)\n+\n+      // Alias the duplicated columns from left side of the join.",
        "comment_created_at": "2025-07-08T12:12:07+00:00",
        "comment_author": "PetarVasiljevic-DB",
        "comment_body": "this should never happen in spark, right? If there are duplicated columns selected, they are in Project node. Scan nodes should still select single column only, join should work with single column and then the Project node on top of join will handle the duplication.",
        "pr_file_module": null
      },
      {
        "comment_id": "2192413802",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2192152901",
        "commented_code": "@@ -98,6 +107,138 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n       filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are 2 same tables in both sides of top level join and it not possible\n+    // to fully qualified the column names in condition. Therefore, query should be rewritten so\n+    // that each of the outputs of child joins are aliased, so there would be a projection\n+    // with aliases between top level join and scanBuilderHolder (that has pushed child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+      // We do not support pushing down anything besides AttributeReference.\n+      leftProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      rightProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      // Cross joins are not supported because they increase the amount of data.\n+      condition.isDefined &&\n+      // Joins on top of sampled tables are not supported\n+      leftHolder.pushedSample.isEmpty &&\n+      rightHolder.pushedSample.isEmpty &&\n+      lBuilder.isOtherSideCompatibleForJoin(rBuilder) =>\n+\n+      // projections' names are maybe not up to date if the joins have been previously pushed down.\n+      // For this reason, we need to use pushedJoinOutputMap to get up to date names.\n+      //\n+      // Normalized projections are then converted to StructType.\n+      def getRequiredSchema(\n+          projections: Seq[NamedExpression],\n+          sHolder: ScanBuilderHolder): StructType = {\n+        val normalizedProjections = DataSourceStrategy.normalizeExprs(\n+          projections,\n+          sHolder.output.map { a =>\n+            sHolder.pushedJoinOutputMap.getOrElse(a, a).asInstanceOf[AttributeReference]\n+          }\n+        ).asInstanceOf[Seq[AttributeReference]]\n+\n+        fromAttributes(normalizedProjections)\n+      }\n+\n+      def generateJoinOutputAlias(name: String): String =\n+        s\"${name}_${java.util.UUID.randomUUID().toString.take(5)}\"\n+\n+      val leftSideRequiredSchema = getRequiredSchema(leftProjections, leftHolder)\n+      val rightSideRequiredSchema = getRequiredSchema(rightProjections, rightHolder)\n+\n+      // Alias the duplicated columns from left side of the join.",
        "comment_created_at": "2025-07-08T12:42:26+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "will the generated JDBC SQL contain duplicated column names as well?",
        "pr_file_module": null
      },
      {
        "comment_id": "2192843839",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2192152901",
        "commented_code": "@@ -98,6 +107,138 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n       filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are 2 same tables in both sides of top level join and it not possible\n+    // to fully qualified the column names in condition. Therefore, query should be rewritten so\n+    // that each of the outputs of child joins are aliased, so there would be a projection\n+    // with aliases between top level join and scanBuilderHolder (that has pushed child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+      // We do not support pushing down anything besides AttributeReference.\n+      leftProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      rightProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+      // Cross joins are not supported because they increase the amount of data.\n+      condition.isDefined &&\n+      // Joins on top of sampled tables are not supported\n+      leftHolder.pushedSample.isEmpty &&\n+      rightHolder.pushedSample.isEmpty &&\n+      lBuilder.isOtherSideCompatibleForJoin(rBuilder) =>\n+\n+      // projections' names are maybe not up to date if the joins have been previously pushed down.\n+      // For this reason, we need to use pushedJoinOutputMap to get up to date names.\n+      //\n+      // Normalized projections are then converted to StructType.\n+      def getRequiredSchema(\n+          projections: Seq[NamedExpression],\n+          sHolder: ScanBuilderHolder): StructType = {\n+        val normalizedProjections = DataSourceStrategy.normalizeExprs(\n+          projections,\n+          sHolder.output.map { a =>\n+            sHolder.pushedJoinOutputMap.getOrElse(a, a).asInstanceOf[AttributeReference]\n+          }\n+        ).asInstanceOf[Seq[AttributeReference]]\n+\n+        fromAttributes(normalizedProjections)\n+      }\n+\n+      def generateJoinOutputAlias(name: String): String =\n+        s\"${name}_${java.util.UUID.randomUUID().toString.take(5)}\"\n+\n+      val leftSideRequiredSchema = getRequiredSchema(leftProjections, leftHolder)\n+      val rightSideRequiredSchema = getRequiredSchema(rightProjections, rightHolder)\n+\n+      // Alias the duplicated columns from left side of the join.",
        "comment_created_at": "2025-07-08T15:31:16+00:00",
        "comment_author": "PetarVasiljevic-DB",
        "comment_body": "For Dataframe API it is indeed problematic as the generated query would have duplicated names. I have changed it to alias the columns that are referenced multiple times on one side.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2192161308",
    "pr_number": 50921,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
    "created_at": "2025-07-08T10:53:25+00:00",
    "commented_code": "}\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2192161308",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
        "discussion_id": "2192161308",
        "commented_code": "@@ -121,6 +123,112 @@ case class JDBCScanBuilder(\n     }\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&",
        "comment_created_at": "2025-07-08T10:53:25+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "we should follow other pushdown methods to do early stop: `if (jdbcOptions.pushDownLimit && dialect.supportsLimit)`",
        "pr_file_module": null
      },
      {
        "comment_id": "2192332487",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
        "discussion_id": "2192161308",
        "commented_code": "@@ -121,6 +123,112 @@ case class JDBCScanBuilder(\n     }\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&",
        "comment_created_at": "2025-07-08T12:07:59+00:00",
        "comment_author": "PetarVasiljevic-DB",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2192321901",
    "pr_number": 50921,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
    "created_at": "2025-07-08T12:02:36+00:00",
    "commented_code": "}\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2192321901",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
        "discussion_id": "2192321901",
        "commented_code": "@@ -121,6 +123,112 @@ case class JDBCScanBuilder(\n     }\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url",
        "comment_created_at": "2025-07-08T12:02:36+00:00",
        "comment_author": "urosstan-db",
        "comment_body": "This does not seems like enough check to make pushdown possible.\r\nWe should check whether credentials are same as well (different users may have different privileges on the same database instance). Also, we might have problem if users do full qualification of `dbtable` (with database which is possible on many databases), and database does not support cross database joins. Can you check whether that scenario is possible.",
        "pr_file_module": null
      },
      {
        "comment_id": "2192875696",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
        "discussion_id": "2192321901",
        "commented_code": "@@ -121,6 +123,112 @@ case class JDBCScanBuilder(\n     }\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url",
        "comment_created_at": "2025-07-08T15:46:57+00:00",
        "comment_author": "PetarVasiljevic-DB",
        "comment_body": "Let's leave it for next PR. There is some logic that should be discussed",
        "pr_file_module": null
      },
      {
        "comment_id": "2192902919",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
        "discussion_id": "2192321901",
        "commented_code": "@@ -121,6 +123,112 @@ case class JDBCScanBuilder(\n     }\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url",
        "comment_created_at": "2025-07-08T16:00:37+00:00",
        "comment_author": "urosstan-db",
        "comment_body": "It is fine, can you add TODO?",
        "pr_file_module": null
      },
      {
        "comment_id": "2195049422",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
        "discussion_id": "2192321901",
        "commented_code": "@@ -121,6 +123,112 @@ case class JDBCScanBuilder(\n     }\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url",
        "comment_created_at": "2025-07-09T13:33:10+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "to be conservative, shall we require the full options are the same?",
        "pr_file_module": null
      },
      {
        "comment_id": "2197539031",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
        "discussion_id": "2192321901",
        "commented_code": "@@ -121,6 +123,112 @@ case class JDBCScanBuilder(\n     }\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url",
        "comment_created_at": "2025-07-10T12:07:33+00:00",
        "comment_author": "PetarVasiljevic-DB",
        "comment_body": "We can do that. I think it is better to be more strict in the beginning and make the check less strict in the future than making the check stricter over time as it could break someone.",
        "pr_file_module": null
      },
      {
        "comment_id": "2197693056",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
        "discussion_id": "2192321901",
        "commented_code": "@@ -121,6 +123,112 @@ case class JDBCScanBuilder(\n     }\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url",
        "comment_created_at": "2025-07-10T13:10:05+00:00",
        "comment_author": "urosstan-db",
        "comment_body": "We can't do comparison on entire options map because table/query are in options. Since JDBC connector have various options and unrecognized DF options goes to properties of JDBC connection, it would be ideal to make allowlist for options that does not prevent JOIN pushdown.\r\n\r\nSomething like (we would need some better name):\r\n`def getOptionsDoesNotPreventJoinPushdown() = Seq(\"query\", \"dbtable\")`\r\n\r\nAnd then in JDBC scan builder, condition would be:\r\n```\r\nthis.options.diff(this.getOptionsDoesNotPreventJoinPushdown) == other.options.diff(other.getOptionsDoesNotPreventJoinPushdown)\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2197727060",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
        "discussion_id": "2192321901",
        "commented_code": "@@ -121,6 +123,112 @@ case class JDBCScanBuilder(\n     }\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url",
        "comment_created_at": "2025-07-10T13:22:59+00:00",
        "comment_author": "PetarVasiljevic-DB",
        "comment_body": "Yes, I already have query and dbtable removed from the options checking (https://github.com/apache/spark/pull/50921/files#diff-da99fbd809633a3b3e7b0190bc3572306cfabc22bc0047d4839ddff59e38895cR146)\r\n\r\nI can add allowlist in separate PR when we have clear set of options that should not affect join pushdown. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2192354230",
    "pr_number": 50921,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
    "created_at": "2025-07-08T12:18:13+00:00",
    "commented_code": "filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2192354230",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2192354230",
        "commented_code": "@@ -98,6 +100,132 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n       filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)",
        "comment_created_at": "2025-07-08T12:18:13+00:00",
        "comment_author": "urosstan-db",
        "comment_body": "This depends on remote data sources? Some databases would have different output?\r\nSomething like this?\r\n```\r\n    // ON p.col = q.col (this is not possible)\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2192878985",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2192354230",
        "commented_code": "@@ -98,6 +100,132 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n       filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)",
        "comment_created_at": "2025-07-08T15:48:40+00:00",
        "comment_author": "PetarVasiljevic-DB",
        "comment_body": "No, this is Spark query. If you want the above query to work, you need to assign aliases in Spark query. Otherwise, you have no way to reference different columns because `p.t1.col` is not valid",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2192392331",
    "pr_number": 50921,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
    "created_at": "2025-07-08T12:32:00+00:00",
    "commented_code": "var pushedAggregate: Option[Aggregation] = None\n \n   var pushedAggOutputMap: AttributeMap[Expression] = AttributeMap.empty[Expression]\n+\n+  var joinedRelations: Seq[DataSourceV2RelationBase] = Seq(relation)",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2192392331",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2192392331",
        "commented_code": "@@ -573,6 +736,10 @@ case class ScanBuilderHolder(\n   var pushedAggregate: Option[Aggregation] = None\n \n   var pushedAggOutputMap: AttributeMap[Expression] = AttributeMap.empty[Expression]\n+\n+  var joinedRelations: Seq[DataSourceV2RelationBase] = Seq(relation)",
        "comment_created_at": "2025-07-08T12:32:00+00:00",
        "comment_author": "urosstan-db",
        "comment_body": "We loose some info here, it would be good to store joined scanBuilderHolders.\r\nThat allows us to have more info about joined relations (like what are pushdowns on joined relations).",
        "pr_file_module": null
      },
      {
        "comment_id": "2192709105",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2192392331",
        "commented_code": "@@ -573,6 +736,10 @@ case class ScanBuilderHolder(\n   var pushedAggregate: Option[Aggregation] = None\n \n   var pushedAggOutputMap: AttributeMap[Expression] = AttributeMap.empty[Expression]\n+\n+  var joinedRelations: Seq[DataSourceV2RelationBase] = Seq(relation)",
        "comment_created_at": "2025-07-08T14:37:25+00:00",
        "comment_author": "PetarVasiljevic-DB",
        "comment_body": "Duplicate of the comment below, so resolving this one.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2192418255",
    "pr_number": 50921,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
    "created_at": "2025-07-08T12:44:30+00:00",
    "commented_code": "}\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    if (!jdbcOptions.pushDownJoin || !dialect.supportsJoin) return false\n+\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url\n+  };\n+\n+\n+  /**\n+   * Helper method to calculate StructType based on the SupportsPushDownJoin.ColumnWithAlias and\n+   * the given schema.\n+   *\n+   * If ColumnWithAlias object has defined alias, new field with new name being equal to alias\n+   * should be returned. Otherwise, original field is returned.\n+   */\n+  private def calculateJoinOutputSchema(\n+      columnsWithAliases: Array[SupportsPushDownJoin.ColumnWithAlias],\n+      schema: StructType): StructType = {\n+    var newSchema = StructType(Seq())\n+    columnsWithAliases.foreach { columnWithAlias =>\n+      val colName = columnWithAlias.getColName\n+      val alias = columnWithAlias.getAlias\n+      val field = schema(colName)\n+\n+      val newName = if (alias == null) colName else alias\n+      newSchema = newSchema.add(newName, field.dataType, field.nullable, field.metadata)\n+    }\n+\n+    newSchema\n+  }\n+\n+  override def pushDownJoin(\n+      other: SupportsPushDownJoin,\n+      joinType: JoinType,\n+      leftSideRequiredColumnWithAliases: Array[SupportsPushDownJoin.ColumnWithAlias],\n+      rightSideRequiredColumnWithAliases: Array[SupportsPushDownJoin.ColumnWithAlias],\n+      condition: Predicate ): Boolean = {\n+    if (!jdbcOptions.pushDownJoin || !dialect.supportsJoin) return false\n+    val otherJdbcScanBuilder = other.asInstanceOf[JDBCScanBuilder]\n+\n+    // Get left side and right side of join sql queries. These will be used as subqueries in final\n+    // join query.\n+    val sqlQuery = buildSQLQueryUsedInJoinPushDown(leftSideRequiredColumnWithAliases)\n+    val otherSideSqlQuery = otherJdbcScanBuilder\n+      .buildSQLQueryUsedInJoinPushDown(rightSideRequiredColumnWithAliases)\n+\n+    // requiredSchema will become the finalSchema of this JDBCScanBuilder\n+    var requiredSchema = StructType(Seq())\n+    requiredSchema = calculateJoinOutputSchema(leftSideRequiredColumnWithAliases, finalSchema)\n+    requiredSchema = requiredSchema.merge(\n+      calculateJoinOutputSchema(\n+        rightSideRequiredColumnWithAliases,\n+        otherJdbcScanBuilder.finalSchema\n+      )\n+    )\n+\n+    val joinOutputColumnsString =\n+      requiredSchema.fields.map(f => dialect.quoteIdentifier(f.name)).mkString(\",\")\n+\n+    val joinTypeStringOption = joinType match {\n+      case JoinType.INNER_JOIN => Some(\"INNER JOIN\")\n+      case _ => None\n+    }\n+\n+    if (!joinTypeStringOption.isDefined) return false\n+\n+    val compiledCondition = dialect.compileExpression(condition)\n+    if (!compiledCondition.isDefined) return false\n+\n+    val conditionString = compiledCondition.get\n+\n+    val joinQuery = s\"\"\"",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2192418255",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
        "discussion_id": "2192418255",
        "commented_code": "@@ -121,6 +123,114 @@ case class JDBCScanBuilder(\n     }\n   }\n \n+  override def isOtherSideCompatibleForJoin(other: SupportsPushDownJoin): Boolean = {\n+    if (!jdbcOptions.pushDownJoin || !dialect.supportsJoin) return false\n+\n+    other.isInstanceOf[JDBCScanBuilder] &&\n+      jdbcOptions.url == other.asInstanceOf[JDBCScanBuilder].jdbcOptions.url\n+  };\n+\n+\n+  /**\n+   * Helper method to calculate StructType based on the SupportsPushDownJoin.ColumnWithAlias and\n+   * the given schema.\n+   *\n+   * If ColumnWithAlias object has defined alias, new field with new name being equal to alias\n+   * should be returned. Otherwise, original field is returned.\n+   */\n+  private def calculateJoinOutputSchema(\n+      columnsWithAliases: Array[SupportsPushDownJoin.ColumnWithAlias],\n+      schema: StructType): StructType = {\n+    var newSchema = StructType(Seq())\n+    columnsWithAliases.foreach { columnWithAlias =>\n+      val colName = columnWithAlias.getColName\n+      val alias = columnWithAlias.getAlias\n+      val field = schema(colName)\n+\n+      val newName = if (alias == null) colName else alias\n+      newSchema = newSchema.add(newName, field.dataType, field.nullable, field.metadata)\n+    }\n+\n+    newSchema\n+  }\n+\n+  override def pushDownJoin(\n+      other: SupportsPushDownJoin,\n+      joinType: JoinType,\n+      leftSideRequiredColumnWithAliases: Array[SupportsPushDownJoin.ColumnWithAlias],\n+      rightSideRequiredColumnWithAliases: Array[SupportsPushDownJoin.ColumnWithAlias],\n+      condition: Predicate ): Boolean = {\n+    if (!jdbcOptions.pushDownJoin || !dialect.supportsJoin) return false\n+    val otherJdbcScanBuilder = other.asInstanceOf[JDBCScanBuilder]\n+\n+    // Get left side and right side of join sql queries. These will be used as subqueries in final\n+    // join query.\n+    val sqlQuery = buildSQLQueryUsedInJoinPushDown(leftSideRequiredColumnWithAliases)\n+    val otherSideSqlQuery = otherJdbcScanBuilder\n+      .buildSQLQueryUsedInJoinPushDown(rightSideRequiredColumnWithAliases)\n+\n+    // requiredSchema will become the finalSchema of this JDBCScanBuilder\n+    var requiredSchema = StructType(Seq())\n+    requiredSchema = calculateJoinOutputSchema(leftSideRequiredColumnWithAliases, finalSchema)\n+    requiredSchema = requiredSchema.merge(\n+      calculateJoinOutputSchema(\n+        rightSideRequiredColumnWithAliases,\n+        otherJdbcScanBuilder.finalSchema\n+      )\n+    )\n+\n+    val joinOutputColumnsString =\n+      requiredSchema.fields.map(f => dialect.quoteIdentifier(f.name)).mkString(\",\")\n+\n+    val joinTypeStringOption = joinType match {\n+      case JoinType.INNER_JOIN => Some(\"INNER JOIN\")\n+      case _ => None\n+    }\n+\n+    if (!joinTypeStringOption.isDefined) return false\n+\n+    val compiledCondition = dialect.compileExpression(condition)\n+    if (!compiledCondition.isDefined) return false\n+\n+    val conditionString = compiledCondition.get\n+\n+    val joinQuery = s\"\"\"",
        "comment_created_at": "2025-07-08T12:44:30+00:00",
        "comment_author": "urosstan-db",
        "comment_body": "We should reuse JDBC Query builder class to build query (that class also have overrides for specific databases so we don't need another class hierarchy in the future)\r\nhttps://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcSQLQueryBuilder.scala\r\n\r\nSome API like:\r\n`withJoin(left: JdbcSQLQueryBuilder, right: JdbcSQLQueryBuilder): JdbcSQLQueryBuilder`\r\n\r\n\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2192461609",
    "pr_number": 50921,
    "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala",
    "created_at": "2025-07-08T13:00:30+00:00",
    "commented_code": "super.afterAll()\n   }\n \n+  test(\"Test 2-way join without condition - no join pushdown\") {\n+    val sqlQuery = \"SELECT * FROM h2.test.employee a, h2.test.employee b\"\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.nonEmpty)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test multi-way join without condition - no join pushdown\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT * FROM\n+      |h2.test.employee a,\n+      |h2.test.employee b,\n+      |h2.test.employee c\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.nonEmpty)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test self join with condition\") {\n+    val sqlQuery = \"SELECT * FROM h2.test.employee a JOIN h2.test.employee b ON a.dept = b.dept + 1\"\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.employee]\")\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test multi-way self join with conditions\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT * FROM\n+      |h2.test.employee a\n+      |JOIN h2.test.employee b ON b.dept = a.dept + 1\n+      |JOIN h2.test.employee c ON c.dept = b.dept - 1\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    assert(!rows.isEmpty)\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.employee, h2.test.employee]\")\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test self join with column pruning\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT a.dept + 2, b.dept, b.salary FROM\n+      |h2.test.employee a JOIN h2.test.employee b\n+      |ON a.dept = b.dept + 1\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.employee]\")\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test 2-way join with column pruning - different tables\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT * FROM\n+      |h2.test.employee a JOIN h2.test.people b\n+      |ON a.dept = b.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.people]\")\n+      checkPushedInfo(df,\n+        \"PushedFilters: [DEPT IS NOT NULL, ID IS NOT NULL, DEPT = ID]\")\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test multi-way self join with column pruning\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT a.dept, b.*, c.dept, c.salary + a.salary\n+      |FROM h2.test.employee a\n+      |JOIN h2.test.employee b ON b.dept = a.dept + 1\n+      |JOIN h2.test.employee c ON c.dept = b.dept - 1\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.employee, h2.test.employee]\")\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test aliases not supported in join pushdown\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT a.dept, bc.*\n+      |FROM h2.test.employee a\n+      |JOIN (\n+      |  SELECT b.*, c.dept AS c_dept, c.salary AS c_salary\n+      |  FROM h2.test.employee b\n+      |  JOIN h2.test.employee c ON c.dept = b.dept - 1\n+      |) bc ON bc.dept = a.dept + 1\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.nonEmpty)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test aggregate on top of 2-way self join\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT min(a.dept + b.dept), min(a.dept)\n+      |FROM h2.test.employee a\n+      |JOIN h2.test.employee b ON a.dept = b.dept + 1\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      val aggNodes = df.queryExecution.optimizedPlan.collect {\n+        case a: Aggregate => a\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      assert(aggNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.employee]\")\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test aggregate on top of multi-way self join\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT min(a.dept + b.dept), min(a.dept), min(c.dept - 2)\n+      |FROM h2.test.employee a\n+      |JOIN h2.test.employee b ON b.dept = a.dept + 1\n+      |JOIN h2.test.employee c ON c.dept = b.dept - 1\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      val aggNodes = df.queryExecution.optimizedPlan.collect {\n+        case a: Aggregate => a\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      assert(aggNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.employee, h2.test.employee]\")\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test sort limit on top of join is pushed down\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT min(a.dept + b.dept), a.dept, b.dept\n+      |FROM h2.test.employee a\n+      |JOIN h2.test.employee b ON b.dept = a.dept + 1\n+      |GROUP BY a.dept, b.dept\n+      |ORDER BY a.dept\n+      |LIMIT 1\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(\n+      SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      val sortNodes = df.queryExecution.optimizedPlan.collect {\n+        case s: Sort => s\n+      }\n+\n+      val limitNodes = df.queryExecution.optimizedPlan.collect {\n+        case l: GlobalLimit => l\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      assert(sortNodes.isEmpty)\n+      assert(limitNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.employee]\")\n+      checkAnswer(df, rows)\n+    }\n+  }",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2192461609",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala",
        "discussion_id": "2192461609",
        "commented_code": "@@ -265,6 +266,288 @@ class JDBCV2Suite extends QueryTest with SharedSparkSession with ExplainSuiteHel\n     super.afterAll()\n   }\n \n+  test(\"Test 2-way join without condition - no join pushdown\") {\n+    val sqlQuery = \"SELECT * FROM h2.test.employee a, h2.test.employee b\"\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.nonEmpty)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test multi-way join without condition - no join pushdown\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT * FROM\n+      |h2.test.employee a,\n+      |h2.test.employee b,\n+      |h2.test.employee c\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.nonEmpty)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test self join with condition\") {\n+    val sqlQuery = \"SELECT * FROM h2.test.employee a JOIN h2.test.employee b ON a.dept = b.dept + 1\"\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.employee]\")\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test multi-way self join with conditions\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT * FROM\n+      |h2.test.employee a\n+      |JOIN h2.test.employee b ON b.dept = a.dept + 1\n+      |JOIN h2.test.employee c ON c.dept = b.dept - 1\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    assert(!rows.isEmpty)\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.employee, h2.test.employee]\")\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test self join with column pruning\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT a.dept + 2, b.dept, b.salary FROM\n+      |h2.test.employee a JOIN h2.test.employee b\n+      |ON a.dept = b.dept + 1\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.employee]\")\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test 2-way join with column pruning - different tables\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT * FROM\n+      |h2.test.employee a JOIN h2.test.people b\n+      |ON a.dept = b.id\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.people]\")\n+      checkPushedInfo(df,\n+        \"PushedFilters: [DEPT IS NOT NULL, ID IS NOT NULL, DEPT = ID]\")\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test multi-way self join with column pruning\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT a.dept, b.*, c.dept, c.salary + a.salary\n+      |FROM h2.test.employee a\n+      |JOIN h2.test.employee b ON b.dept = a.dept + 1\n+      |JOIN h2.test.employee c ON c.dept = b.dept - 1\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.employee, h2.test.employee]\")\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test aliases not supported in join pushdown\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT a.dept, bc.*\n+      |FROM h2.test.employee a\n+      |JOIN (\n+      |  SELECT b.*, c.dept AS c_dept, c.salary AS c_salary\n+      |  FROM h2.test.employee b\n+      |  JOIN h2.test.employee c ON c.dept = b.dept - 1\n+      |) bc ON bc.dept = a.dept + 1\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      assert(joinNodes.nonEmpty)\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test aggregate on top of 2-way self join\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT min(a.dept + b.dept), min(a.dept)\n+      |FROM h2.test.employee a\n+      |JOIN h2.test.employee b ON a.dept = b.dept + 1\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      val aggNodes = df.queryExecution.optimizedPlan.collect {\n+        case a: Aggregate => a\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      assert(aggNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.employee]\")\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test aggregate on top of multi-way self join\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT min(a.dept + b.dept), min(a.dept), min(c.dept - 2)\n+      |FROM h2.test.employee a\n+      |JOIN h2.test.employee b ON b.dept = a.dept + 1\n+      |JOIN h2.test.employee c ON c.dept = b.dept - 1\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      val aggNodes = df.queryExecution.optimizedPlan.collect {\n+        case a: Aggregate => a\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      assert(aggNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.employee, h2.test.employee]\")\n+      checkAnswer(df, rows)\n+    }\n+  }\n+\n+  test(\"Test sort limit on top of join is pushed down\") {\n+    val sqlQuery = \"\"\"\n+      |SELECT min(a.dept + b.dept), a.dept, b.dept\n+      |FROM h2.test.employee a\n+      |JOIN h2.test.employee b ON b.dept = a.dept + 1\n+      |GROUP BY a.dept, b.dept\n+      |ORDER BY a.dept\n+      |LIMIT 1\n+      |\"\"\".stripMargin\n+\n+    val rows = withSQLConf(SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"false\") {\n+      sql(sqlQuery).collect().toSeq\n+    }\n+\n+    withSQLConf(\n+      SQLConf.DATA_SOURCE_V2_JOIN_PUSHDOWN.key -> \"true\") {\n+      val df = sql(sqlQuery)\n+      val joinNodes = df.queryExecution.optimizedPlan.collect {\n+        case j: Join => j\n+      }\n+\n+      val sortNodes = df.queryExecution.optimizedPlan.collect {\n+        case s: Sort => s\n+      }\n+\n+      val limitNodes = df.queryExecution.optimizedPlan.collect {\n+        case l: GlobalLimit => l\n+      }\n+\n+      assert(joinNodes.isEmpty)\n+      assert(sortNodes.isEmpty)\n+      assert(limitNodes.isEmpty)\n+      checkPushedInfo(df, \"PushedJoins: [h2.test.employee, h2.test.employee]\")\n+      checkAnswer(df, rows)\n+    }\n+  }",
        "comment_created_at": "2025-07-08T13:00:30+00:00",
        "comment_author": "urosstan-db",
        "comment_body": "We have no test with another table options. I suggest making grid tests here, all of your tests should pass with different DSv2 table options. For example, we don't have test to verify table with `query` option works fine when JOIN pushdown is enabled.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2195041104",
    "pr_number": 50921,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
    "created_at": "2025-07-09T13:29:32+00:00",
    "commented_code": "filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are duplicated columns in both sides of top level join and it's not\n+    // possible to fully qualified the column names in condition. Therefore, query should be\n+    // rewritten so that each of the outputs of child joins are aliased, so there would be a\n+    // projection with aliases between top level join and scanBuilderHolder (that has pushed\n+    // child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+        // We do not support pushing down anything besides AttributeReference.\n+        leftProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+        rightProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+        // Cross joins are not supported because they increase the amount of data.\n+        condition.isDefined &&\n+        // Joins on top of sampled tables are not supported\n+        leftHolder.pushedSample.isEmpty &&\n+        rightHolder.pushedSample.isEmpty &&\n+        lBuilder.isOtherSideCompatibleForJoin(rBuilder) =>\n+      val leftSideRequiredColumnNames = getRequiredColumnNames(leftProjections, leftHolder)\n+      val rightSideRequiredColumnNames = getRequiredColumnNames(rightProjections, rightHolder)\n+\n+      // Alias the duplicated columns from left side of the join.\n+      val leftSideRequiredColumnsWithAliases = leftSideRequiredColumnNames.map { name =>\n+        val aliasName =\n+          if (leftSideRequiredColumnNames.count(_ == name) > 1 ||",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2195041104",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2195041104",
        "commented_code": "@@ -98,6 +100,142 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n       filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are duplicated columns in both sides of top level join and it's not\n+    // possible to fully qualified the column names in condition. Therefore, query should be\n+    // rewritten so that each of the outputs of child joins are aliased, so there would be a\n+    // projection with aliases between top level join and scanBuilderHolder (that has pushed\n+    // child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+        // We do not support pushing down anything besides AttributeReference.\n+        leftProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+        rightProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+        // Cross joins are not supported because they increase the amount of data.\n+        condition.isDefined &&\n+        // Joins on top of sampled tables are not supported\n+        leftHolder.pushedSample.isEmpty &&\n+        rightHolder.pushedSample.isEmpty &&\n+        lBuilder.isOtherSideCompatibleForJoin(rBuilder) =>\n+      val leftSideRequiredColumnNames = getRequiredColumnNames(leftProjections, leftHolder)\n+      val rightSideRequiredColumnNames = getRequiredColumnNames(rightProjections, rightHolder)\n+\n+      // Alias the duplicated columns from left side of the join.\n+      val leftSideRequiredColumnsWithAliases = leftSideRequiredColumnNames.map { name =>\n+        val aliasName =\n+          if (leftSideRequiredColumnNames.count(_ == name) > 1 ||",
        "comment_created_at": "2025-07-09T13:29:32+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "shall we use case-insensitive comparison?",
        "pr_file_module": null
      },
      {
        "comment_id": "2195212137",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2195041104",
        "commented_code": "@@ -98,6 +100,142 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n       filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are duplicated columns in both sides of top level join and it's not\n+    // possible to fully qualified the column names in condition. Therefore, query should be\n+    // rewritten so that each of the outputs of child joins are aliased, so there would be a\n+    // projection with aliases between top level join and scanBuilderHolder (that has pushed\n+    // child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+        // We do not support pushing down anything besides AttributeReference.\n+        leftProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+        rightProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+        // Cross joins are not supported because they increase the amount of data.\n+        condition.isDefined &&\n+        // Joins on top of sampled tables are not supported\n+        leftHolder.pushedSample.isEmpty &&\n+        rightHolder.pushedSample.isEmpty &&\n+        lBuilder.isOtherSideCompatibleForJoin(rBuilder) =>\n+      val leftSideRequiredColumnNames = getRequiredColumnNames(leftProjections, leftHolder)\n+      val rightSideRequiredColumnNames = getRequiredColumnNames(rightProjections, rightHolder)\n+\n+      // Alias the duplicated columns from left side of the join.\n+      val leftSideRequiredColumnsWithAliases = leftSideRequiredColumnNames.map { name =>\n+        val aliasName =\n+          if (leftSideRequiredColumnNames.count(_ == name) > 1 ||",
        "comment_created_at": "2025-07-09T14:41:12+00:00",
        "comment_author": "urosstan-db",
        "comment_body": "@PetarVasiljevic-DB @cloud-fan Is it maybe safer to alias all columns unconditionally? If we hit aliasing always, we would easier debug errors as well and all join pushdowns will hit the same path.",
        "pr_file_module": null
      },
      {
        "comment_id": "2195219122",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2195041104",
        "commented_code": "@@ -98,6 +100,142 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n       filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are duplicated columns in both sides of top level join and it's not\n+    // possible to fully qualified the column names in condition. Therefore, query should be\n+    // rewritten so that each of the outputs of child joins are aliased, so there would be a\n+    // projection with aliases between top level join and scanBuilderHolder (that has pushed\n+    // child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+        // We do not support pushing down anything besides AttributeReference.\n+        leftProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+        rightProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+        // Cross joins are not supported because they increase the amount of data.\n+        condition.isDefined &&\n+        // Joins on top of sampled tables are not supported\n+        leftHolder.pushedSample.isEmpty &&\n+        rightHolder.pushedSample.isEmpty &&\n+        lBuilder.isOtherSideCompatibleForJoin(rBuilder) =>\n+      val leftSideRequiredColumnNames = getRequiredColumnNames(leftProjections, leftHolder)\n+      val rightSideRequiredColumnNames = getRequiredColumnNames(rightProjections, rightHolder)\n+\n+      // Alias the duplicated columns from left side of the join.\n+      val leftSideRequiredColumnsWithAliases = leftSideRequiredColumnNames.map { name =>\n+        val aliasName =\n+          if (leftSideRequiredColumnNames.count(_ == name) > 1 ||",
        "comment_created_at": "2025-07-09T14:43:40+00:00",
        "comment_author": "urosstan-db",
        "comment_body": "If we decide to keep optional aliasing, it would be good to make map[name, count] to avoid N^2 complexity here. I suppose column limit is not huge, but I saw certain tables and queries with 1000 columns, which means 1M operations here.",
        "pr_file_module": null
      },
      {
        "comment_id": "2197980549",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
        "discussion_id": "2195041104",
        "commented_code": "@@ -98,6 +100,142 @@ object V2ScanRelationPushDown extends Rule[LogicalPlan] with PredicateHelper {\n       filterCondition.map(Filter(_, sHolder)).getOrElse(sHolder)\n   }\n \n+  def pushDownJoin(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n+    // Join can be attempted to be pushed down only if left and right side of join are\n+    // compatible (same data source, for example). Also, another requirement is that if\n+    // there are projections between Join and ScanBuilderHolder, these projections need to be\n+    // AttributeReferences. We could probably support Alias as well, but this should be on\n+    // TODO list.\n+    // Alias can exist between Join and sHolder node because the query below is not valid:\n+    // SELECT * FROM\n+    // (SELECT * FROM tbl t1 JOIN tbl2 t2) p\n+    // JOIN\n+    // (SELECT * FROM tbl t3 JOIN tbl3 t4) q\n+    // ON p.t1.col = q.t3.col (this is not possible)\n+    // It's because there are duplicated columns in both sides of top level join and it's not\n+    // possible to fully qualified the column names in condition. Therefore, query should be\n+    // rewritten so that each of the outputs of child joins are aliased, so there would be a\n+    // projection with aliases between top level join and scanBuilderHolder (that has pushed\n+    // child joins).\n+    case node @ Join(\n+      PhysicalOperation(\n+        leftProjections,\n+        Nil,\n+        leftHolder @ ScanBuilderHolder(_, _, lBuilder: SupportsPushDownJoin)\n+      ),\n+      PhysicalOperation(\n+        rightProjections,\n+        Nil,\n+        rightHolder @ ScanBuilderHolder(_, _, rBuilder: SupportsPushDownJoin)\n+      ),\n+      joinType,\n+      condition,\n+    _) if conf.dataSourceV2JoinPushdown &&\n+        // We do not support pushing down anything besides AttributeReference.\n+        leftProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+        rightProjections.forall(_.isInstanceOf[AttributeReference]) &&\n+        // Cross joins are not supported because they increase the amount of data.\n+        condition.isDefined &&\n+        // Joins on top of sampled tables are not supported\n+        leftHolder.pushedSample.isEmpty &&\n+        rightHolder.pushedSample.isEmpty &&\n+        lBuilder.isOtherSideCompatibleForJoin(rBuilder) =>\n+      val leftSideRequiredColumnNames = getRequiredColumnNames(leftProjections, leftHolder)\n+      val rightSideRequiredColumnNames = getRequiredColumnNames(rightProjections, rightHolder)\n+\n+      // Alias the duplicated columns from left side of the join.\n+      val leftSideRequiredColumnsWithAliases = leftSideRequiredColumnNames.map { name =>\n+        val aliasName =\n+          if (leftSideRequiredColumnNames.count(_ == name) > 1 ||",
        "comment_created_at": "2025-07-10T14:56:06+00:00",
        "comment_author": "PetarVasiljevic-DB",
        "comment_body": "Good point. I have added a count map and also changed `rightSideRequiredColumnNames.contains(name)` to be set comparison by converting `rightSideRequiredColumnNames` to `Set`",
        "pr_file_module": null
      }
    ]
  }
]