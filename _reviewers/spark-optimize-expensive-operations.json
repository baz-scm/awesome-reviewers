[
  {
    "discussion_id": "2175726815",
    "pr_number": 51310,
    "pr_file": "python/pyspark/pandas/series.py",
    "created_at": "2025-06-30T19:09:26+00:00",
    "commented_code": "y   -14\n         dtype: int64\n         \"\"\"\n+        spark_session = self._internal.spark_frame.sparkSession\n         if not same_anchor(self, other):\n-            if get_option(\"compute.eager_check\") and not cast(\n-                ps.Index, self.index.sort_values()\n-            ).equals(cast(ps.Index, other.index.sort_values())):\n-                raise ValueError(\"matrices are not aligned\")\n+            if get_option(\"compute.eager_check\"):\n+                if is_ansi_mode_enabled(spark_session):\n+                    # In ANSI, \"equals\" leads to implicit casting which may cause CAST_INVALID_INPUT\n+                    # Instead, we compare raw index objects collected to the driver\n+                    if sorted(ps.Index(self.index).tolist()) != sorted(\n+                        ps.Index(other.index).tolist()\n+                    ):",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2175726815",
        "repo_full_name": "apache/spark",
        "pr_number": 51310,
        "pr_file": "python/pyspark/pandas/series.py",
        "discussion_id": "2175726815",
        "commented_code": "@@ -5660,11 +5661,21 @@ def dot(self, other: Union[\"Series\", DataFrame]) -> Union[Scalar, \"Series\"]:\n         y   -14\n         dtype: int64\n         \"\"\"\n+        spark_session = self._internal.spark_frame.sparkSession\n         if not same_anchor(self, other):\n-            if get_option(\"compute.eager_check\") and not cast(\n-                ps.Index, self.index.sort_values()\n-            ).equals(cast(ps.Index, other.index.sort_values())):\n-                raise ValueError(\"matrices are not aligned\")\n+            if get_option(\"compute.eager_check\"):\n+                if is_ansi_mode_enabled(spark_session):\n+                    # In ANSI, \"equals\" leads to implicit casting which may cause CAST_INVALID_INPUT\n+                    # Instead, we compare raw index objects collected to the driver\n+                    if sorted(ps.Index(self.index).tolist()) != sorted(\n+                        ps.Index(other.index).tolist()\n+                    ):",
        "comment_created_at": "2025-06-30T19:09:26+00:00",
        "comment_author": "ueshin",
        "comment_body": "I guess this means `index.sort_values()` has `CAST_INVALID_INPUT` issue?",
        "pr_file_module": null
      },
      {
        "comment_id": "2175848369",
        "repo_full_name": "apache/spark",
        "pr_number": 51310,
        "pr_file": "python/pyspark/pandas/series.py",
        "discussion_id": "2175726815",
        "commented_code": "@@ -5660,11 +5661,21 @@ def dot(self, other: Union[\"Series\", DataFrame]) -> Union[Scalar, \"Series\"]:\n         y   -14\n         dtype: int64\n         \"\"\"\n+        spark_session = self._internal.spark_frame.sparkSession\n         if not same_anchor(self, other):\n-            if get_option(\"compute.eager_check\") and not cast(\n-                ps.Index, self.index.sort_values()\n-            ).equals(cast(ps.Index, other.index.sort_values())):\n-                raise ValueError(\"matrices are not aligned\")\n+            if get_option(\"compute.eager_check\"):\n+                if is_ansi_mode_enabled(spark_session):\n+                    # In ANSI, \"equals\" leads to implicit casting which may cause CAST_INVALID_INPUT\n+                    # Instead, we compare raw index objects collected to the driver\n+                    if sorted(ps.Index(self.index).tolist()) != sorted(\n+                        ps.Index(other.index).tolist()\n+                    ):",
        "comment_created_at": "2025-06-30T20:32:56+00:00",
        "comment_author": "xinrong-meng",
        "comment_body": "It's `equals`\r\n\r\n```py\r\n>>> psidx1 = ps.Index(['x', 'y', 'z'])                     \r\n>>> psidx2 = ps.Index([1, 2, 3])\r\n>>> psidx1.sort_values().equals(psidx2.sort_values())\r\nTraceback (most recent call last):\r\n...\r\npyspark.errors.exceptions.captured.NumberFormatException: [CAST_INVALID_INPUT] The value 'x' of the type \"STRING\" cannot be cast to \"BIGINT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\r\n== DataFrame ==\r\n\"__eq__\" was called from\r\n<stdin>:1\r\n\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2175851455",
        "repo_full_name": "apache/spark",
        "pr_number": 51310,
        "pr_file": "python/pyspark/pandas/series.py",
        "discussion_id": "2175726815",
        "commented_code": "@@ -5660,11 +5661,21 @@ def dot(self, other: Union[\"Series\", DataFrame]) -> Union[Scalar, \"Series\"]:\n         y   -14\n         dtype: int64\n         \"\"\"\n+        spark_session = self._internal.spark_frame.sparkSession\n         if not same_anchor(self, other):\n-            if get_option(\"compute.eager_check\") and not cast(\n-                ps.Index, self.index.sort_values()\n-            ).equals(cast(ps.Index, other.index.sort_values())):\n-                raise ValueError(\"matrices are not aligned\")\n+            if get_option(\"compute.eager_check\"):\n+                if is_ansi_mode_enabled(spark_session):\n+                    # In ANSI, \"equals\" leads to implicit casting which may cause CAST_INVALID_INPUT\n+                    # Instead, we compare raw index objects collected to the driver\n+                    if sorted(ps.Index(self.index).tolist()) != sorted(\n+                        ps.Index(other.index).tolist()\n+                    ):",
        "comment_created_at": "2025-06-30T20:34:57+00:00",
        "comment_author": "xinrong-meng",
        "comment_body": "I know this collects the index to the driver which is not ideal, but I haven\u2019t found an alternative yet",
        "pr_file_module": null
      },
      {
        "comment_id": "2176031575",
        "repo_full_name": "apache/spark",
        "pr_number": 51310,
        "pr_file": "python/pyspark/pandas/series.py",
        "discussion_id": "2175726815",
        "commented_code": "@@ -5660,11 +5661,21 @@ def dot(self, other: Union[\"Series\", DataFrame]) -> Union[Scalar, \"Series\"]:\n         y   -14\n         dtype: int64\n         \"\"\"\n+        spark_session = self._internal.spark_frame.sparkSession\n         if not same_anchor(self, other):\n-            if get_option(\"compute.eager_check\") and not cast(\n-                ps.Index, self.index.sort_values()\n-            ).equals(cast(ps.Index, other.index.sort_values())):\n-                raise ValueError(\"matrices are not aligned\")\n+            if get_option(\"compute.eager_check\"):\n+                if is_ansi_mode_enabled(spark_session):\n+                    # In ANSI, \"equals\" leads to implicit casting which may cause CAST_INVALID_INPUT\n+                    # Instead, we compare raw index objects collected to the driver\n+                    if sorted(ps.Index(self.index).tolist()) != sorted(\n+                        ps.Index(other.index).tolist()\n+                    ):",
        "comment_created_at": "2025-06-30T21:53:12+00:00",
        "comment_author": "ueshin",
        "comment_body": "Let's fix `equals`, then. We should fix it anyway.",
        "pr_file_module": null
      },
      {
        "comment_id": "2176072694",
        "repo_full_name": "apache/spark",
        "pr_number": 51310,
        "pr_file": "python/pyspark/pandas/series.py",
        "discussion_id": "2175726815",
        "commented_code": "@@ -5660,11 +5661,21 @@ def dot(self, other: Union[\"Series\", DataFrame]) -> Union[Scalar, \"Series\"]:\n         y   -14\n         dtype: int64\n         \"\"\"\n+        spark_session = self._internal.spark_frame.sparkSession\n         if not same_anchor(self, other):\n-            if get_option(\"compute.eager_check\") and not cast(\n-                ps.Index, self.index.sort_values()\n-            ).equals(cast(ps.Index, other.index.sort_values())):\n-                raise ValueError(\"matrices are not aligned\")\n+            if get_option(\"compute.eager_check\"):\n+                if is_ansi_mode_enabled(spark_session):\n+                    # In ANSI, \"equals\" leads to implicit casting which may cause CAST_INVALID_INPUT\n+                    # Instead, we compare raw index objects collected to the driver\n+                    if sorted(ps.Index(self.index).tolist()) != sorted(\n+                        ps.Index(other.index).tolist()\n+                    ):",
        "comment_created_at": "2025-06-30T22:26:28+00:00",
        "comment_author": "ueshin",
        "comment_body": "The other comparisons, as well? like `!=`, `<`, `<=`, `>`, and `>=`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2229853915",
        "repo_full_name": "apache/spark",
        "pr_number": 51310,
        "pr_file": "python/pyspark/pandas/series.py",
        "discussion_id": "2175726815",
        "commented_code": "@@ -5660,11 +5661,21 @@ def dot(self, other: Union[\"Series\", DataFrame]) -> Union[Scalar, \"Series\"]:\n         y   -14\n         dtype: int64\n         \"\"\"\n+        spark_session = self._internal.spark_frame.sparkSession\n         if not same_anchor(self, other):\n-            if get_option(\"compute.eager_check\") and not cast(\n-                ps.Index, self.index.sort_values()\n-            ).equals(cast(ps.Index, other.index.sort_values())):\n-                raise ValueError(\"matrices are not aligned\")\n+            if get_option(\"compute.eager_check\"):\n+                if is_ansi_mode_enabled(spark_session):\n+                    # In ANSI, \"equals\" leads to implicit casting which may cause CAST_INVALID_INPUT\n+                    # Instead, we compare raw index objects collected to the driver\n+                    if sorted(ps.Index(self.index).tolist()) != sorted(\n+                        ps.Index(other.index).tolist()\n+                    ):",
        "comment_created_at": "2025-07-25T00:35:34+00:00",
        "comment_author": "xinrong-meng",
        "comment_body": "Pending on https://github.com/apache/spark/pull/51370",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2205994139",
    "pr_number": 51479,
    "pr_file": "python/pyspark/worker.py",
    "created_at": "2025-07-14T23:38:44+00:00",
    "commented_code": "def convert_to_arrow(data: Iterable):\n                 data = list(check_return_value(data))\n                 if len(data) == 0:\n-                    return [\n-                        pa.RecordBatch.from_pylist(data, schema=pa.schema(list(arrow_return_type)))\n-                    ]\n+                    return [pa.RecordBatch.from_pylist(data, schema=pa.schema(list(arrow_return_type)))]\n+                \n+                # if unnecessary, try to skip expensive LocalDataToArrowConversion\n+                needs_conversion = any(\n+                    LocalDataToArrowConversion._need_converter(field.dataType, field.nullable)\n+                    for field in return_type.fields\n+                )\n+                \n+                if not needs_conversion:\n+                    try:\n+                        return [pa.RecordBatch.from_pylist(data, schema=pa.schema(list(arrow_return_type)))]",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2205994139",
        "repo_full_name": "apache/spark",
        "pr_number": 51479,
        "pr_file": "python/pyspark/worker.py",
        "discussion_id": "2205994139",
        "commented_code": "@@ -1640,9 +1640,20 @@ def check_return_value(res):\n             def convert_to_arrow(data: Iterable):\n                 data = list(check_return_value(data))\n                 if len(data) == 0:\n-                    return [\n-                        pa.RecordBatch.from_pylist(data, schema=pa.schema(list(arrow_return_type)))\n-                    ]\n+                    return [pa.RecordBatch.from_pylist(data, schema=pa.schema(list(arrow_return_type)))]\n+                \n+                # if unnecessary, try to skip expensive LocalDataToArrowConversion\n+                needs_conversion = any(\n+                    LocalDataToArrowConversion._need_converter(field.dataType, field.nullable)\n+                    for field in return_type.fields\n+                )\n+                \n+                if not needs_conversion:\n+                    try:\n+                        return [pa.RecordBatch.from_pylist(data, schema=pa.schema(list(arrow_return_type)))]",
        "comment_created_at": "2025-07-14T23:38:44+00:00",
        "comment_author": "ueshin",
        "comment_body": "oh, wait, we need to at least transpose the `data` as Arrow will expect a columnar pylist; where as `data` is row-based?",
        "pr_file_module": null
      }
    ]
  }
]