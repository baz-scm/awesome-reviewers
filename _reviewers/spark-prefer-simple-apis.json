[
  {
    "discussion_id": "2230604219",
    "pr_number": 51377,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala",
    "created_at": "2025-07-25T09:17:46+00:00",
    "commented_code": "}\n       )\n \n+      val operationMetricOpt = getOperationMetrics(query)\n       logInfo(log\"Data source write support ${MDC(LogKeys.BATCH_WRITE, batchWrite)} is committing.\")\n-      batchWrite.commit(messages)\n+      operationMetricOpt match {\n+        case Some(metrics) => batchWrite.commitWithOperationMetrics(messages,\n+          metrics.map{ case (name, value) => name -> JLong.valueOf(value) }.asJava)\n+        case None => batchWrite.commit(messages)",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2230604219",
        "repo_full_name": "apache/spark",
        "pr_number": 51377,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala",
        "discussion_id": "2230604219",
        "commented_code": "@@ -451,8 +454,13 @@ trait V2TableWriteExec extends V2CommandExec with UnaryExecNode {\n         }\n       )\n \n+      val operationMetricOpt = getOperationMetrics(query)\n       logInfo(log\"Data source write support ${MDC(LogKeys.BATCH_WRITE, batchWrite)} is committing.\")\n-      batchWrite.commit(messages)\n+      operationMetricOpt match {\n+        case Some(metrics) => batchWrite.commitWithOperationMetrics(messages,\n+          metrics.map{ case (name, value) => name -> JLong.valueOf(value) }.asJava)\n+        case None => batchWrite.commit(messages)",
        "comment_created_at": "2025-07-25T09:17:46+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "I suggest we will always call the `commit` method with metrics parameter. If no metrics we can pass an empty Map. This will make the developer's life easier, as they only need to implement one `commit` method.",
        "pr_file_module": null
      },
      {
        "comment_id": "2232042967",
        "repo_full_name": "apache/spark",
        "pr_number": 51377,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala",
        "discussion_id": "2230604219",
        "commented_code": "@@ -451,8 +454,13 @@ trait V2TableWriteExec extends V2CommandExec with UnaryExecNode {\n         }\n       )\n \n+      val operationMetricOpt = getOperationMetrics(query)\n       logInfo(log\"Data source write support ${MDC(LogKeys.BATCH_WRITE, batchWrite)} is committing.\")\n-      batchWrite.commit(messages)\n+      operationMetricOpt match {\n+        case Some(metrics) => batchWrite.commitWithOperationMetrics(messages,\n+          metrics.map{ case (name, value) => name -> JLong.valueOf(value) }.asJava)\n+        case None => batchWrite.commit(messages)",
        "comment_created_at": "2025-07-25T21:30:03+00:00",
        "comment_author": "szehon-ho",
        "comment_body": "sounds good, done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2043844308",
    "pr_number": 50333,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala",
    "created_at": "2025-04-15T07:27:48+00:00",
    "commented_code": "@Since(\"3.3.0\")\n   def isSupportedFunction(funcName: String): Boolean = false\n \n+  /**\n+   * Returns whether the database supports extract.\n+   * @param extract The V2 Extract to be converted.\n+   * @return True if the database supports extract.\n+   */\n+  @Since(\"4.1.0\")\n+  def isSupportedExtract(extract: Extract): Boolean = false",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2043844308",
        "repo_full_name": "apache/spark",
        "pr_number": 50333,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala",
        "discussion_id": "2043844308",
        "commented_code": "@@ -499,6 +507,14 @@ abstract class JdbcDialect extends Serializable with Logging {\n   @Since(\"3.3.0\")\n   def isSupportedFunction(funcName: String): Boolean = false\n \n+  /**\n+   * Returns whether the database supports extract.\n+   * @param extract The V2 Extract to be converted.\n+   * @return True if the database supports extract.\n+   */\n+  @Since(\"4.1.0\")\n+  def isSupportedExtract(extract: Extract): Boolean = false",
        "comment_created_at": "2025-04-15T07:27:48+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "I think we are making the API overly complicated. Too many knobs. It looks fine to ask the implementation to override `visitExtract` carefully.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1428618073",
    "pr_number": 44119,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriterV2.scala",
    "created_at": "2023-12-16T00:52:23+00:00",
    "commented_code": "runCommand(overwrite)\n   }\n \n+  /**\n+   * Specifies the merge condition.\n+   *\n+   * Sets the condition, provided as a `String`, to be used for merging data. This condition\n+   * is converted internally to a `Column` and used to determine how rows from the source\n+   * DataFrame are matched with rows in the target table.\n+   *\n+   * @param condition a `String` representing the merge condition.",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "1428618073",
        "repo_full_name": "apache/spark",
        "pr_number": 44119,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriterV2.scala",
        "discussion_id": "1428618073",
        "commented_code": "@@ -167,6 +173,241 @@ final class DataFrameWriterV2[T] private[sql](table: String, ds: Dataset[T])\n     runCommand(overwrite)\n   }\n \n+  /**\n+   * Specifies the merge condition.\n+   *\n+   * Sets the condition, provided as a `String`, to be used for merging data. This condition\n+   * is converted internally to a `Column` and used to determine how rows from the source\n+   * DataFrame are matched with rows in the target table.\n+   *\n+   * @param condition a `String` representing the merge condition.",
        "comment_created_at": "2023-12-16T00:52:23+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "Ideally we don't need this overload. People can just cal `expr(string)` to do the same thing, and we keep the API clean.",
        "pr_file_module": null
      },
      {
        "comment_id": "1429394959",
        "repo_full_name": "apache/spark",
        "pr_number": 44119,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriterV2.scala",
        "discussion_id": "1428618073",
        "commented_code": "@@ -167,6 +173,241 @@ final class DataFrameWriterV2[T] private[sql](table: String, ds: Dataset[T])\n     runCommand(overwrite)\n   }\n \n+  /**\n+   * Specifies the merge condition.\n+   *\n+   * Sets the condition, provided as a `String`, to be used for merging data. This condition\n+   * is converted internally to a `Column` and used to determine how rows from the source\n+   * DataFrame are matched with rows in the target table.\n+   *\n+   * @param condition a `String` representing the merge condition.",
        "comment_created_at": "2023-12-18T02:25:13+00:00",
        "comment_author": "huaxingao",
        "comment_body": "Removed",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2096713920",
    "pr_number": 50942,
    "pr_file": "sql/connect/common/src/main/protobuf/spark/connect/pipelines.proto",
    "created_at": "2025-05-20T02:00:23+00:00",
    "commented_code": "+syntax = \"proto3\";\n+\n+package spark.connect;\n+\n+import \"spark/connect/relations.proto\";\n+import \"spark/connect/types.proto\";\n+\n+option java_multiple_files = true;\n+option java_package = \"org.apache.spark.connect.proto\";\n+\n+// Dispatch object for pipelines commands.\n+message PipelineCommand {\n+  oneof command_type {\n+    CreateDataflowGraph create_dataflow_graph = 1;\n+    DefineDataset define_dataset = 2;\n+    DefineFlow define_flow = 3;\n+    DropDataflowGraph drop_dataflow_graph = 4;\n+    StartRun start_run = 5;\n+    StopRun stop_run = 6;\n+    DefineSqlGraphElements define_sql_graph_elements = 7;\n+  }\n+\n+  message DefineSqlGraphElements {\n+    optional string dataflow_graph_id = 1;\n+    optional string sql_file_name = 2;\n+    optional string sql_text = 3;\n+  }\n+\n+  // Request to create a new pipeline.\n+  message CreateDataflowGraph {\n+    // The default catalog.\n+    optional string default_catalog = 1;\n+\n+    // The default database.\n+    optional string default_database = 2;\n+\n+    // Default SQL configurations for all flows in this graph.\n+    map<string, string> sql_conf = 5;\n+\n+    message Response {\n+      // The ID of the created graph.\n+      string dataflow_graph_id = 1;\n+    }\n+  }\n+\n+  // Drops the graph and stops any running attached flows.\n+  message DropDataflowGraph {\n+    // The graph to drop.\n+    string dataflow_graph_id = 1;\n+  }\n+\n+  // Request to define a dataset: a table, a materialized view, or a temporary view.\n+  message DefineDataset {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string dataset_name = 2;\n+\n+    // Table or view.\n+    DatasetType dataset_type = 3;\n+\n+    // Optional comment for the dataset.\n+    optional string comment = 4;\n+\n+    // Optional table properties.\n+    map<string, string> table_properties = 5;\n+\n+    // Optional partition columns for the dataset (if applicable).\n+    repeated string partition_cols = 6;\n+\n+    // Schema for the dataset. If unset, this will be inferred.\n+    optional spark.connect.DataType schema = 7;\n+\n+    // The output table format of the dataset.\n+    optional string format = 8;\n+  }\n+\n+  // Request to define a flow targeting a dataset.\n+  message DefineFlow {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string flow_name = 2;\n+\n+    string target_dataset_name = 3;\n+\n+    // An unresolved relation that defines the dataset's flow.\n+    spark.connect.Relation plan = 4;\n+\n+    // Default SQL configurations set when running this flow.\n+    map<string, string> sql_conf = 5;\n+\n+    // If true, this flow will only be run once per execution.\n+    bool once = 6;\n+  }\n+\n+  // Resolves all datasets and flows and start a pipeline update. Should be called after all\n+  // graph elements are registered.\n+  message StartRun {\n+    // The graph to start.\n+    string dataflow_graph_id = 1;\n+  }\n+  message StopRun {\n+    // The ID of the graph to stop..\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+// Dispatch object for pipelines command results.\n+message PipelineCommandResult {\n+  oneof result_type {\n+    CreateDataflowGraphResult create_dataflow_graph_result = 1;\n+  }\n+  message CreateDataflowGraphResult {\n+    // The ID of the created graph.\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+enum DatasetType {\n+  DATASET_UNSPECIFIED = 0;\n+  // A materialized view dataset which is published to the catalog\n+  MATERIALIZED_VIEW = 1;\n+  // A table which is published to the catalog\n+  TABLE = 2;\n+  // A view which is not published to the catalog\n+  TEMPORARY_VIEW = 3;\n+}\n+\n+// A response containing events emitted during the run of a pipeline.\n+message PipelineEventsResult {\n+  repeated PipelineEvent events = 1;",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2096713920",
        "repo_full_name": "apache/spark",
        "pr_number": 50942,
        "pr_file": "sql/connect/common/src/main/protobuf/spark/connect/pipelines.proto",
        "discussion_id": "2096713920",
        "commented_code": "@@ -0,0 +1,142 @@\n+syntax = \"proto3\";\n+\n+package spark.connect;\n+\n+import \"spark/connect/relations.proto\";\n+import \"spark/connect/types.proto\";\n+\n+option java_multiple_files = true;\n+option java_package = \"org.apache.spark.connect.proto\";\n+\n+// Dispatch object for pipelines commands.\n+message PipelineCommand {\n+  oneof command_type {\n+    CreateDataflowGraph create_dataflow_graph = 1;\n+    DefineDataset define_dataset = 2;\n+    DefineFlow define_flow = 3;\n+    DropDataflowGraph drop_dataflow_graph = 4;\n+    StartRun start_run = 5;\n+    StopRun stop_run = 6;\n+    DefineSqlGraphElements define_sql_graph_elements = 7;\n+  }\n+\n+  message DefineSqlGraphElements {\n+    optional string dataflow_graph_id = 1;\n+    optional string sql_file_name = 2;\n+    optional string sql_text = 3;\n+  }\n+\n+  // Request to create a new pipeline.\n+  message CreateDataflowGraph {\n+    // The default catalog.\n+    optional string default_catalog = 1;\n+\n+    // The default database.\n+    optional string default_database = 2;\n+\n+    // Default SQL configurations for all flows in this graph.\n+    map<string, string> sql_conf = 5;\n+\n+    message Response {\n+      // The ID of the created graph.\n+      string dataflow_graph_id = 1;\n+    }\n+  }\n+\n+  // Drops the graph and stops any running attached flows.\n+  message DropDataflowGraph {\n+    // The graph to drop.\n+    string dataflow_graph_id = 1;\n+  }\n+\n+  // Request to define a dataset: a table, a materialized view, or a temporary view.\n+  message DefineDataset {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string dataset_name = 2;\n+\n+    // Table or view.\n+    DatasetType dataset_type = 3;\n+\n+    // Optional comment for the dataset.\n+    optional string comment = 4;\n+\n+    // Optional table properties.\n+    map<string, string> table_properties = 5;\n+\n+    // Optional partition columns for the dataset (if applicable).\n+    repeated string partition_cols = 6;\n+\n+    // Schema for the dataset. If unset, this will be inferred.\n+    optional spark.connect.DataType schema = 7;\n+\n+    // The output table format of the dataset.\n+    optional string format = 8;\n+  }\n+\n+  // Request to define a flow targeting a dataset.\n+  message DefineFlow {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string flow_name = 2;\n+\n+    string target_dataset_name = 3;\n+\n+    // An unresolved relation that defines the dataset's flow.\n+    spark.connect.Relation plan = 4;\n+\n+    // Default SQL configurations set when running this flow.\n+    map<string, string> sql_conf = 5;\n+\n+    // If true, this flow will only be run once per execution.\n+    bool once = 6;\n+  }\n+\n+  // Resolves all datasets and flows and start a pipeline update. Should be called after all\n+  // graph elements are registered.\n+  message StartRun {\n+    // The graph to start.\n+    string dataflow_graph_id = 1;\n+  }\n+  message StopRun {\n+    // The ID of the graph to stop..\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+// Dispatch object for pipelines command results.\n+message PipelineCommandResult {\n+  oneof result_type {\n+    CreateDataflowGraphResult create_dataflow_graph_result = 1;\n+  }\n+  message CreateDataflowGraphResult {\n+    // The ID of the created graph.\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+enum DatasetType {\n+  DATASET_UNSPECIFIED = 0;\n+  // A materialized view dataset which is published to the catalog\n+  MATERIALIZED_VIEW = 1;\n+  // A table which is published to the catalog\n+  TABLE = 2;\n+  // A view which is not published to the catalog\n+  TEMPORARY_VIEW = 3;\n+}\n+\n+// A response containing events emitted during the run of a pipeline.\n+message PipelineEventsResult {\n+  repeated PipelineEvent events = 1;",
        "comment_created_at": "2025-05-20T02:00:23+00:00",
        "comment_author": "hvanhovell",
        "comment_body": "Batching events should not be needed. gRPC server side streaming can return multiple 'events' at the same time, provided it can fit them in a single window (~30k).",
        "pr_file_module": null
      },
      {
        "comment_id": "2098662068",
        "repo_full_name": "apache/spark",
        "pr_number": 50942,
        "pr_file": "sql/connect/common/src/main/protobuf/spark/connect/pipelines.proto",
        "discussion_id": "2096713920",
        "commented_code": "@@ -0,0 +1,142 @@\n+syntax = \"proto3\";\n+\n+package spark.connect;\n+\n+import \"spark/connect/relations.proto\";\n+import \"spark/connect/types.proto\";\n+\n+option java_multiple_files = true;\n+option java_package = \"org.apache.spark.connect.proto\";\n+\n+// Dispatch object for pipelines commands.\n+message PipelineCommand {\n+  oneof command_type {\n+    CreateDataflowGraph create_dataflow_graph = 1;\n+    DefineDataset define_dataset = 2;\n+    DefineFlow define_flow = 3;\n+    DropDataflowGraph drop_dataflow_graph = 4;\n+    StartRun start_run = 5;\n+    StopRun stop_run = 6;\n+    DefineSqlGraphElements define_sql_graph_elements = 7;\n+  }\n+\n+  message DefineSqlGraphElements {\n+    optional string dataflow_graph_id = 1;\n+    optional string sql_file_name = 2;\n+    optional string sql_text = 3;\n+  }\n+\n+  // Request to create a new pipeline.\n+  message CreateDataflowGraph {\n+    // The default catalog.\n+    optional string default_catalog = 1;\n+\n+    // The default database.\n+    optional string default_database = 2;\n+\n+    // Default SQL configurations for all flows in this graph.\n+    map<string, string> sql_conf = 5;\n+\n+    message Response {\n+      // The ID of the created graph.\n+      string dataflow_graph_id = 1;\n+    }\n+  }\n+\n+  // Drops the graph and stops any running attached flows.\n+  message DropDataflowGraph {\n+    // The graph to drop.\n+    string dataflow_graph_id = 1;\n+  }\n+\n+  // Request to define a dataset: a table, a materialized view, or a temporary view.\n+  message DefineDataset {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string dataset_name = 2;\n+\n+    // Table or view.\n+    DatasetType dataset_type = 3;\n+\n+    // Optional comment for the dataset.\n+    optional string comment = 4;\n+\n+    // Optional table properties.\n+    map<string, string> table_properties = 5;\n+\n+    // Optional partition columns for the dataset (if applicable).\n+    repeated string partition_cols = 6;\n+\n+    // Schema for the dataset. If unset, this will be inferred.\n+    optional spark.connect.DataType schema = 7;\n+\n+    // The output table format of the dataset.\n+    optional string format = 8;\n+  }\n+\n+  // Request to define a flow targeting a dataset.\n+  message DefineFlow {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string flow_name = 2;\n+\n+    string target_dataset_name = 3;\n+\n+    // An unresolved relation that defines the dataset's flow.\n+    spark.connect.Relation plan = 4;\n+\n+    // Default SQL configurations set when running this flow.\n+    map<string, string> sql_conf = 5;\n+\n+    // If true, this flow will only be run once per execution.\n+    bool once = 6;\n+  }\n+\n+  // Resolves all datasets and flows and start a pipeline update. Should be called after all\n+  // graph elements are registered.\n+  message StartRun {\n+    // The graph to start.\n+    string dataflow_graph_id = 1;\n+  }\n+  message StopRun {\n+    // The ID of the graph to stop..\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+// Dispatch object for pipelines command results.\n+message PipelineCommandResult {\n+  oneof result_type {\n+    CreateDataflowGraphResult create_dataflow_graph_result = 1;\n+  }\n+  message CreateDataflowGraphResult {\n+    // The ID of the created graph.\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+enum DatasetType {\n+  DATASET_UNSPECIFIED = 0;\n+  // A materialized view dataset which is published to the catalog\n+  MATERIALIZED_VIEW = 1;\n+  // A table which is published to the catalog\n+  TABLE = 2;\n+  // A view which is not published to the catalog\n+  TEMPORARY_VIEW = 3;\n+}\n+\n+// A response containing events emitted during the run of a pipeline.\n+message PipelineEventsResult {\n+  repeated PipelineEvent events = 1;",
        "comment_created_at": "2025-05-20T18:54:24+00:00",
        "comment_author": "aakash-db",
        "comment_body": "That's fair. But I think the repeated field adds more flexibility in general. We can group events logically, rather than just to avoid network latency.",
        "pr_file_module": null
      },
      {
        "comment_id": "2109571329",
        "repo_full_name": "apache/spark",
        "pr_number": 50942,
        "pr_file": "sql/connect/common/src/main/protobuf/spark/connect/pipelines.proto",
        "discussion_id": "2096713920",
        "commented_code": "@@ -0,0 +1,142 @@\n+syntax = \"proto3\";\n+\n+package spark.connect;\n+\n+import \"spark/connect/relations.proto\";\n+import \"spark/connect/types.proto\";\n+\n+option java_multiple_files = true;\n+option java_package = \"org.apache.spark.connect.proto\";\n+\n+// Dispatch object for pipelines commands.\n+message PipelineCommand {\n+  oneof command_type {\n+    CreateDataflowGraph create_dataflow_graph = 1;\n+    DefineDataset define_dataset = 2;\n+    DefineFlow define_flow = 3;\n+    DropDataflowGraph drop_dataflow_graph = 4;\n+    StartRun start_run = 5;\n+    StopRun stop_run = 6;\n+    DefineSqlGraphElements define_sql_graph_elements = 7;\n+  }\n+\n+  message DefineSqlGraphElements {\n+    optional string dataflow_graph_id = 1;\n+    optional string sql_file_name = 2;\n+    optional string sql_text = 3;\n+  }\n+\n+  // Request to create a new pipeline.\n+  message CreateDataflowGraph {\n+    // The default catalog.\n+    optional string default_catalog = 1;\n+\n+    // The default database.\n+    optional string default_database = 2;\n+\n+    // Default SQL configurations for all flows in this graph.\n+    map<string, string> sql_conf = 5;\n+\n+    message Response {\n+      // The ID of the created graph.\n+      string dataflow_graph_id = 1;\n+    }\n+  }\n+\n+  // Drops the graph and stops any running attached flows.\n+  message DropDataflowGraph {\n+    // The graph to drop.\n+    string dataflow_graph_id = 1;\n+  }\n+\n+  // Request to define a dataset: a table, a materialized view, or a temporary view.\n+  message DefineDataset {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string dataset_name = 2;\n+\n+    // Table or view.\n+    DatasetType dataset_type = 3;\n+\n+    // Optional comment for the dataset.\n+    optional string comment = 4;\n+\n+    // Optional table properties.\n+    map<string, string> table_properties = 5;\n+\n+    // Optional partition columns for the dataset (if applicable).\n+    repeated string partition_cols = 6;\n+\n+    // Schema for the dataset. If unset, this will be inferred.\n+    optional spark.connect.DataType schema = 7;\n+\n+    // The output table format of the dataset.\n+    optional string format = 8;\n+  }\n+\n+  // Request to define a flow targeting a dataset.\n+  message DefineFlow {\n+    // The graph to attach this dataset to.\n+    string dataflow_graph_id = 1;\n+\n+    // Name of the dataset.\n+    string flow_name = 2;\n+\n+    string target_dataset_name = 3;\n+\n+    // An unresolved relation that defines the dataset's flow.\n+    spark.connect.Relation plan = 4;\n+\n+    // Default SQL configurations set when running this flow.\n+    map<string, string> sql_conf = 5;\n+\n+    // If true, this flow will only be run once per execution.\n+    bool once = 6;\n+  }\n+\n+  // Resolves all datasets and flows and start a pipeline update. Should be called after all\n+  // graph elements are registered.\n+  message StartRun {\n+    // The graph to start.\n+    string dataflow_graph_id = 1;\n+  }\n+  message StopRun {\n+    // The ID of the graph to stop..\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+// Dispatch object for pipelines command results.\n+message PipelineCommandResult {\n+  oneof result_type {\n+    CreateDataflowGraphResult create_dataflow_graph_result = 1;\n+  }\n+  message CreateDataflowGraphResult {\n+    // The ID of the created graph.\n+    string dataflow_graph_id = 1;\n+  }\n+}\n+\n+enum DatasetType {\n+  DATASET_UNSPECIFIED = 0;\n+  // A materialized view dataset which is published to the catalog\n+  MATERIALIZED_VIEW = 1;\n+  // A table which is published to the catalog\n+  TABLE = 2;\n+  // A view which is not published to the catalog\n+  TEMPORARY_VIEW = 3;\n+}\n+\n+// A response containing events emitted during the run of a pipeline.\n+message PipelineEventsResult {\n+  repeated PipelineEvent events = 1;",
        "comment_created_at": "2025-05-27T15:50:54+00:00",
        "comment_author": "sryza",
        "comment_body": "Per further feedback from @grundprinzip and @hvanhovell, I'm going to take this batching out. We can always add it in in the future if we come up with a use case for logical grouping.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2107828233",
    "pr_number": 50942,
    "pr_file": "sql/connect/common/src/main/protobuf/spark/connect/pipelines.proto",
    "created_at": "2025-05-26T20:27:47+00:00",
    "commented_code": "+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+syntax = \"proto3\";\n+\n+package spark.connect;\n+\n+import \"spark/connect/relations.proto\";\n+import \"spark/connect/types.proto\";\n+\n+option java_multiple_files = true;\n+option java_package = \"org.apache.spark.connect.proto\";\n+\n+// Dispatch object for pipelines commands. See each individual command for documentation.\n+message PipelineCommand {\n+  oneof command_type {\n+    CreateDataflowGraph create_dataflow_graph = 1;\n+    DefineDataset define_dataset = 2;\n+    DefineFlow define_flow = 3;\n+    DropDataflowGraph drop_dataflow_graph = 4;\n+    StartRun start_run = 5;\n+    DefineSqlGraphElements define_sql_graph_elements = 6;\n+  }\n+\n+  // Request to create a new dataflow graph.\n+  message CreateDataflowGraph {\n+    // The default catalog.\n+    optional string default_catalog = 1;\n+\n+    // The default database.\n+    optional string default_database = 2;\n+\n+    // SQL configurations for all flows in this graph.\n+    map<string, string> sql_conf = 5;\n+\n+    message Response {\n+      // The ID of the created graph.\n+      optional string dataflow_graph_id = 1;\n+    }\n+  }\n+\n+  // Drops the graph and stops any running attached flows.\n+  message DropDataflowGraph {\n+    // The graph to drop.\n+    optional string dataflow_graph_id = 1;\n+  }\n+\n+  // Request to define a dataset: a table, a materialized view, or a temporary view.\n+  message DefineDataset {\n+    // The graph to attach this dataset to.\n+    optional string dataflow_graph_id = 1;\n+\n+    // Name of the dataset. Can be partially or fully qualified.\n+    optional string dataset_name = 2;\n+\n+    // The type of the dataset.\n+    optional DatasetType dataset_type = 3;\n+\n+    // Optional comment for the dataset.\n+    optional string comment = 4;\n+\n+    // Optional table properties. Only applies to dataset_type == TABLE and dataset_type == MATERIALIZED_VIEW.\n+    map<string, string> table_properties = 5;\n+\n+    // Optional partition columns for the dataset. Only applies to dataset_type == TABLE and\n+    // dataset_type == MATERIALIZED_VIEW.\n+    repeated string partition_cols = 6;\n+\n+    // Schema for the dataset. If unset, this will be inferred from incoming flows.\n+    optional spark.connect.DataType schema = 7;\n+\n+    // The output table format of the dataset. Only applies to dataset_type == TABLE and\n+    // dataset_type == MATERIALIZED_VIEW.\n+    optional string format = 8;\n+  }\n+\n+  // Request to define a flow targeting a dataset.\n+  message DefineFlow {\n+    // The graph to attach this flow to.\n+    optional string dataflow_graph_id = 1;\n+\n+    // Name of the flow. For standalone flows, this must be a single-part name.\n+    optional string flow_name = 2;\n+\n+    // Name of the dataset this flow writes to. Can be partially or fully qualified.\n+    optional string target_dataset_name = 3;\n+\n+    // An unresolved relation that defines the dataset's flow.\n+    optional spark.connect.Relation plan = 4;\n+\n+    // SQL configurations set when running this flow.\n+    map<string, string> sql_conf = 5;\n+\n+    // If true, this flow will only be run once per execution.\n+    optional bool once = 6;\n+  }\n+\n+  // Resolves all datasets and flows and start a pipeline update. Should be called after all\n+  // graph elements are registered.\n+  message StartRun {\n+    // The graph to start.\n+    optional string dataflow_graph_id = 1;\n+  }\n+}\n+\n+// Parses the SQL file and registers all datasets and flows.\n+message DefineSqlGraphElements {\n+  // The graph to attach this dataset to.\n+  optional string dataflow_graph_id = 1;\n+\n+  // The full path to the SQL file. Can be relative or absolute.\n+  optional string sql_file_path = 2;\n+\n+  // The contents of the SQL file.\n+  optional string sql_text = 3;\n+}\n+\n+// Dispatch object for pipelines command results.\n+message PipelineCommandResult {\n+  oneof result_type {\n+    CreateDataflowGraphResult create_dataflow_graph_result = 1;\n+  }\n+  message CreateDataflowGraphResult {\n+    // The ID of the created graph.\n+    optional string dataflow_graph_id = 1;\n+  }\n+}\n+\n+// The type of dataset.\n+enum DatasetType {\n+  // Safe default value. Should not be used.\n+  DATASET_UNSPECIFIED = 0;\n+  // A materialized view dataset which is published to the catalog\n+  MATERIALIZED_VIEW = 1;\n+  // A table which is published to the catalog\n+  TABLE = 2;\n+  // A view which is not published to the catalog\n+  TEMPORARY_VIEW = 3;\n+}\n+\n+// A response containing events emitted during the run of a pipeline.\n+message PipelineEventsResult {\n+  repeated PipelineEvent events = 1;",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2107828233",
        "repo_full_name": "apache/spark",
        "pr_number": 50942,
        "pr_file": "sql/connect/common/src/main/protobuf/spark/connect/pipelines.proto",
        "discussion_id": "2107828233",
        "commented_code": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+syntax = \"proto3\";\n+\n+package spark.connect;\n+\n+import \"spark/connect/relations.proto\";\n+import \"spark/connect/types.proto\";\n+\n+option java_multiple_files = true;\n+option java_package = \"org.apache.spark.connect.proto\";\n+\n+// Dispatch object for pipelines commands. See each individual command for documentation.\n+message PipelineCommand {\n+  oneof command_type {\n+    CreateDataflowGraph create_dataflow_graph = 1;\n+    DefineDataset define_dataset = 2;\n+    DefineFlow define_flow = 3;\n+    DropDataflowGraph drop_dataflow_graph = 4;\n+    StartRun start_run = 5;\n+    DefineSqlGraphElements define_sql_graph_elements = 6;\n+  }\n+\n+  // Request to create a new dataflow graph.\n+  message CreateDataflowGraph {\n+    // The default catalog.\n+    optional string default_catalog = 1;\n+\n+    // The default database.\n+    optional string default_database = 2;\n+\n+    // SQL configurations for all flows in this graph.\n+    map<string, string> sql_conf = 5;\n+\n+    message Response {\n+      // The ID of the created graph.\n+      optional string dataflow_graph_id = 1;\n+    }\n+  }\n+\n+  // Drops the graph and stops any running attached flows.\n+  message DropDataflowGraph {\n+    // The graph to drop.\n+    optional string dataflow_graph_id = 1;\n+  }\n+\n+  // Request to define a dataset: a table, a materialized view, or a temporary view.\n+  message DefineDataset {\n+    // The graph to attach this dataset to.\n+    optional string dataflow_graph_id = 1;\n+\n+    // Name of the dataset. Can be partially or fully qualified.\n+    optional string dataset_name = 2;\n+\n+    // The type of the dataset.\n+    optional DatasetType dataset_type = 3;\n+\n+    // Optional comment for the dataset.\n+    optional string comment = 4;\n+\n+    // Optional table properties. Only applies to dataset_type == TABLE and dataset_type == MATERIALIZED_VIEW.\n+    map<string, string> table_properties = 5;\n+\n+    // Optional partition columns for the dataset. Only applies to dataset_type == TABLE and\n+    // dataset_type == MATERIALIZED_VIEW.\n+    repeated string partition_cols = 6;\n+\n+    // Schema for the dataset. If unset, this will be inferred from incoming flows.\n+    optional spark.connect.DataType schema = 7;\n+\n+    // The output table format of the dataset. Only applies to dataset_type == TABLE and\n+    // dataset_type == MATERIALIZED_VIEW.\n+    optional string format = 8;\n+  }\n+\n+  // Request to define a flow targeting a dataset.\n+  message DefineFlow {\n+    // The graph to attach this flow to.\n+    optional string dataflow_graph_id = 1;\n+\n+    // Name of the flow. For standalone flows, this must be a single-part name.\n+    optional string flow_name = 2;\n+\n+    // Name of the dataset this flow writes to. Can be partially or fully qualified.\n+    optional string target_dataset_name = 3;\n+\n+    // An unresolved relation that defines the dataset's flow.\n+    optional spark.connect.Relation plan = 4;\n+\n+    // SQL configurations set when running this flow.\n+    map<string, string> sql_conf = 5;\n+\n+    // If true, this flow will only be run once per execution.\n+    optional bool once = 6;\n+  }\n+\n+  // Resolves all datasets and flows and start a pipeline update. Should be called after all\n+  // graph elements are registered.\n+  message StartRun {\n+    // The graph to start.\n+    optional string dataflow_graph_id = 1;\n+  }\n+}\n+\n+// Parses the SQL file and registers all datasets and flows.\n+message DefineSqlGraphElements {\n+  // The graph to attach this dataset to.\n+  optional string dataflow_graph_id = 1;\n+\n+  // The full path to the SQL file. Can be relative or absolute.\n+  optional string sql_file_path = 2;\n+\n+  // The contents of the SQL file.\n+  optional string sql_text = 3;\n+}\n+\n+// Dispatch object for pipelines command results.\n+message PipelineCommandResult {\n+  oneof result_type {\n+    CreateDataflowGraphResult create_dataflow_graph_result = 1;\n+  }\n+  message CreateDataflowGraphResult {\n+    // The ID of the created graph.\n+    optional string dataflow_graph_id = 1;\n+  }\n+}\n+\n+// The type of dataset.\n+enum DatasetType {\n+  // Safe default value. Should not be used.\n+  DATASET_UNSPECIFIED = 0;\n+  // A materialized view dataset which is published to the catalog\n+  MATERIALIZED_VIEW = 1;\n+  // A table which is published to the catalog\n+  TABLE = 2;\n+  // A view which is not published to the catalog\n+  TEMPORARY_VIEW = 3;\n+}\n+\n+// A response containing events emitted during the run of a pipeline.\n+message PipelineEventsResult {\n+  repeated PipelineEvent events = 1;",
        "comment_created_at": "2025-05-26T20:27:47+00:00",
        "comment_author": "grundprinzip",
        "comment_body": "The doc should be more explicit about how \"complete\" the set of events is that you receive here. Are these all events or just some? How do you know if more are coming or not. \n\nGenerally, I'd stand with Herman that if you don't expect to emit thousands of events per second, your code will be easier and simpler if you don't use a repeated field here and simply emit one event per message. ",
        "pr_file_module": null
      }
    ]
  }
]