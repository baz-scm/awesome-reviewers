[
  {
    "discussion_id": "2191477283",
    "pr_number": 51287,
    "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/xml/XMLTokenizer.scala",
    "created_at": "2025-07-08T04:58:12+00:00",
    "commented_code": "+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.xml\n+\n+import java.io.{FileNotFoundException, InputStream, IOException}\n+import javax.xml.stream.{XMLEventReader, XMLStreamException}\n+import javax.xml.stream.events.{Characters, EndDocument, EndElement, StartElement, XMLEvent}\n+import javax.xml.transform.stream.StreamSource\n+import javax.xml.validation.Schema\n+\n+import scala.jdk.CollectionConverters._\n+import scala.util.control.NonFatal\n+import scala.xml.SAXException\n+\n+import org.apache.commons.lang3.exception.ExceptionUtils\n+import org.apache.hadoop.hdfs.BlockMissingException\n+import org.apache.hadoop.security.AccessControlException\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * XML tokenizer that never buffers complete XML records in memory. It uses XMLEventReader to parse\n+ * XML file stream directly and can move to the next XML record based on the rowTag option.\n+ */\n+class XmlTokenizer(inputStream: () => InputStream, options: XmlOptions) extends Logging {\n+  // Primary XML event reader for parsing\n+  private val in1 = inputStream()\n+  private var reader = StaxXmlParserUtils.filteredReader(in1, options)\n+\n+  // Optional XML event reader for XSD validation.\n+  private val in2 = Option(options.rowValidationXSDPath).map(_ => inputStream())\n+  private val readerForXSDValidation = in2.map(in => StaxXmlParserUtils.filteredReader(in, options))\n+\n+  /**\n+   * Returns the next XML record as a positioned XMLEventReader.\n+   * This avoids creating intermediate string representations.\n+   */\n+  def next(): Option[XMLEventReaderWithXSDValidation] = {\n+    var nextRecord: Option[XMLEventReaderWithXSDValidation] = None\n+    try {\n+      // Skip to the next row start element\n+      if (skipToNextRowStart()) {\n+        nextRecord = Some(XMLEventReaderWithXSDValidation(reader, readerForXSDValidation, options))\n+      }\n+    } catch {\n+      case e: FileNotFoundException if options.ignoreMissingFiles =>\n+        logWarning(\"Skipping the rest of the content in the missing file\", e)\n+      case NonFatal(e) =>\n+        ExceptionUtils.getRootCause(e) match {\n+          case _: AccessControlException | _: BlockMissingException =>\n+            close()\n+            throw e\n+          case _: RuntimeException | _: IOException if options.ignoreCorruptFiles =>\n+            logWarning(\"Skipping the rest of the content in the corrupted file\", e)\n+          case _: XMLStreamException =>\n+            logWarning(\"Skipping the rest of the content in the corrupted file\", e)",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2191477283",
        "repo_full_name": "apache/spark",
        "pr_number": 51287,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/xml/XMLTokenizer.scala",
        "discussion_id": "2191477283",
        "commented_code": "@@ -0,0 +1,330 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.xml\n+\n+import java.io.{FileNotFoundException, InputStream, IOException}\n+import javax.xml.stream.{XMLEventReader, XMLStreamException}\n+import javax.xml.stream.events.{Characters, EndDocument, EndElement, StartElement, XMLEvent}\n+import javax.xml.transform.stream.StreamSource\n+import javax.xml.validation.Schema\n+\n+import scala.jdk.CollectionConverters._\n+import scala.util.control.NonFatal\n+import scala.xml.SAXException\n+\n+import org.apache.commons.lang3.exception.ExceptionUtils\n+import org.apache.hadoop.hdfs.BlockMissingException\n+import org.apache.hadoop.security.AccessControlException\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * XML tokenizer that never buffers complete XML records in memory. It uses XMLEventReader to parse\n+ * XML file stream directly and can move to the next XML record based on the rowTag option.\n+ */\n+class XmlTokenizer(inputStream: () => InputStream, options: XmlOptions) extends Logging {\n+  // Primary XML event reader for parsing\n+  private val in1 = inputStream()\n+  private var reader = StaxXmlParserUtils.filteredReader(in1, options)\n+\n+  // Optional XML event reader for XSD validation.\n+  private val in2 = Option(options.rowValidationXSDPath).map(_ => inputStream())\n+  private val readerForXSDValidation = in2.map(in => StaxXmlParserUtils.filteredReader(in, options))\n+\n+  /**\n+   * Returns the next XML record as a positioned XMLEventReader.\n+   * This avoids creating intermediate string representations.\n+   */\n+  def next(): Option[XMLEventReaderWithXSDValidation] = {\n+    var nextRecord: Option[XMLEventReaderWithXSDValidation] = None\n+    try {\n+      // Skip to the next row start element\n+      if (skipToNextRowStart()) {\n+        nextRecord = Some(XMLEventReaderWithXSDValidation(reader, readerForXSDValidation, options))\n+      }\n+    } catch {\n+      case e: FileNotFoundException if options.ignoreMissingFiles =>\n+        logWarning(\"Skipping the rest of the content in the missing file\", e)\n+      case NonFatal(e) =>\n+        ExceptionUtils.getRootCause(e) match {\n+          case _: AccessControlException | _: BlockMissingException =>\n+            close()\n+            throw e\n+          case _: RuntimeException | _: IOException if options.ignoreCorruptFiles =>\n+            logWarning(\"Skipping the rest of the content in the corrupted file\", e)\n+          case _: XMLStreamException =>\n+            logWarning(\"Skipping the rest of the content in the corrupted file\", e)",
        "comment_created_at": "2025-07-08T04:58:12+00:00",
        "comment_author": "sandip-db",
        "comment_body": "Why is `XMLStreamException` a separate `case`? Shouldn't there be a call to `close`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2193070469",
        "repo_full_name": "apache/spark",
        "pr_number": 51287,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/xml/XMLTokenizer.scala",
        "discussion_id": "2191477283",
        "commented_code": "@@ -0,0 +1,330 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.xml\n+\n+import java.io.{FileNotFoundException, InputStream, IOException}\n+import javax.xml.stream.{XMLEventReader, XMLStreamException}\n+import javax.xml.stream.events.{Characters, EndDocument, EndElement, StartElement, XMLEvent}\n+import javax.xml.transform.stream.StreamSource\n+import javax.xml.validation.Schema\n+\n+import scala.jdk.CollectionConverters._\n+import scala.util.control.NonFatal\n+import scala.xml.SAXException\n+\n+import org.apache.commons.lang3.exception.ExceptionUtils\n+import org.apache.hadoop.hdfs.BlockMissingException\n+import org.apache.hadoop.security.AccessControlException\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * XML tokenizer that never buffers complete XML records in memory. It uses XMLEventReader to parse\n+ * XML file stream directly and can move to the next XML record based on the rowTag option.\n+ */\n+class XmlTokenizer(inputStream: () => InputStream, options: XmlOptions) extends Logging {\n+  // Primary XML event reader for parsing\n+  private val in1 = inputStream()\n+  private var reader = StaxXmlParserUtils.filteredReader(in1, options)\n+\n+  // Optional XML event reader for XSD validation.\n+  private val in2 = Option(options.rowValidationXSDPath).map(_ => inputStream())\n+  private val readerForXSDValidation = in2.map(in => StaxXmlParserUtils.filteredReader(in, options))\n+\n+  /**\n+   * Returns the next XML record as a positioned XMLEventReader.\n+   * This avoids creating intermediate string representations.\n+   */\n+  def next(): Option[XMLEventReaderWithXSDValidation] = {\n+    var nextRecord: Option[XMLEventReaderWithXSDValidation] = None\n+    try {\n+      // Skip to the next row start element\n+      if (skipToNextRowStart()) {\n+        nextRecord = Some(XMLEventReaderWithXSDValidation(reader, readerForXSDValidation, options))\n+      }\n+    } catch {\n+      case e: FileNotFoundException if options.ignoreMissingFiles =>\n+        logWarning(\"Skipping the rest of the content in the missing file\", e)\n+      case NonFatal(e) =>\n+        ExceptionUtils.getRootCause(e) match {\n+          case _: AccessControlException | _: BlockMissingException =>\n+            close()\n+            throw e\n+          case _: RuntimeException | _: IOException if options.ignoreCorruptFiles =>\n+            logWarning(\"Skipping the rest of the content in the corrupted file\", e)\n+          case _: XMLStreamException =>\n+            logWarning(\"Skipping the rest of the content in the corrupted file\", e)",
        "comment_created_at": "2025-07-08T17:28:11+00:00",
        "comment_author": "xiaonanyang-db",
        "comment_body": "Oops, those two cases can be combined.",
        "pr_file_module": null
      },
      {
        "comment_id": "2193071865",
        "repo_full_name": "apache/spark",
        "pr_number": 51287,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/xml/XMLTokenizer.scala",
        "discussion_id": "2191477283",
        "commented_code": "@@ -0,0 +1,330 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.catalyst.xml\n+\n+import java.io.{FileNotFoundException, InputStream, IOException}\n+import javax.xml.stream.{XMLEventReader, XMLStreamException}\n+import javax.xml.stream.events.{Characters, EndDocument, EndElement, StartElement, XMLEvent}\n+import javax.xml.transform.stream.StreamSource\n+import javax.xml.validation.Schema\n+\n+import scala.jdk.CollectionConverters._\n+import scala.util.control.NonFatal\n+import scala.xml.SAXException\n+\n+import org.apache.commons.lang3.exception.ExceptionUtils\n+import org.apache.hadoop.hdfs.BlockMissingException\n+import org.apache.hadoop.security.AccessControlException\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * XML tokenizer that never buffers complete XML records in memory. It uses XMLEventReader to parse\n+ * XML file stream directly and can move to the next XML record based on the rowTag option.\n+ */\n+class XmlTokenizer(inputStream: () => InputStream, options: XmlOptions) extends Logging {\n+  // Primary XML event reader for parsing\n+  private val in1 = inputStream()\n+  private var reader = StaxXmlParserUtils.filteredReader(in1, options)\n+\n+  // Optional XML event reader for XSD validation.\n+  private val in2 = Option(options.rowValidationXSDPath).map(_ => inputStream())\n+  private val readerForXSDValidation = in2.map(in => StaxXmlParserUtils.filteredReader(in, options))\n+\n+  /**\n+   * Returns the next XML record as a positioned XMLEventReader.\n+   * This avoids creating intermediate string representations.\n+   */\n+  def next(): Option[XMLEventReaderWithXSDValidation] = {\n+    var nextRecord: Option[XMLEventReaderWithXSDValidation] = None\n+    try {\n+      // Skip to the next row start element\n+      if (skipToNextRowStart()) {\n+        nextRecord = Some(XMLEventReaderWithXSDValidation(reader, readerForXSDValidation, options))\n+      }\n+    } catch {\n+      case e: FileNotFoundException if options.ignoreMissingFiles =>\n+        logWarning(\"Skipping the rest of the content in the missing file\", e)\n+      case NonFatal(e) =>\n+        ExceptionUtils.getRootCause(e) match {\n+          case _: AccessControlException | _: BlockMissingException =>\n+            close()\n+            throw e\n+          case _: RuntimeException | _: IOException if options.ignoreCorruptFiles =>\n+            logWarning(\"Skipping the rest of the content in the corrupted file\", e)\n+          case _: XMLStreamException =>\n+            logWarning(\"Skipping the rest of the content in the corrupted file\", e)",
        "comment_created_at": "2025-07-08T17:29:04+00:00",
        "comment_author": "xiaonanyang-db",
        "comment_body": "Actually, we shouldn't need separate closes in each case, the parser will be closed in the `finally` block",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2191606028",
    "pr_number": 51287,
    "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/xml/XmlInferSchema.scala",
    "created_at": "2025-07-08T06:37:12+00:00",
    "commented_code": "*   3. Replace any remaining null fields with string, the top type\n    */\n   def infer(xml: RDD[String]): StructType = {\n-    val schemaData = if (options.samplingRatio < 1.0) {\n-      xml.sample(withReplacement = false, options.samplingRatio, 1)\n+    val inferredTypesRdd = xml.mapPartitions { iter =>\n+      iter.flatMap { xml =>\n+        val parser = XMLEventReaderWithXSDValidation(xml, options)",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2191606028",
        "repo_full_name": "apache/spark",
        "pr_number": 51287,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/xml/XmlInferSchema.scala",
        "discussion_id": "2191606028",
        "commented_code": "@@ -94,18 +92,25 @@ class XmlInferSchema(options: XmlOptions, caseSensitive: Boolean)\n    *   3. Replace any remaining null fields with string, the top type\n    */\n   def infer(xml: RDD[String]): StructType = {\n-    val schemaData = if (options.samplingRatio < 1.0) {\n-      xml.sample(withReplacement = false, options.samplingRatio, 1)\n+    val inferredTypesRdd = xml.mapPartitions { iter =>\n+      iter.flatMap { xml =>\n+        val parser = XMLEventReaderWithXSDValidation(xml, options)",
        "comment_created_at": "2025-07-08T06:37:12+00:00",
        "comment_author": "sandip-db",
        "comment_body": "when is this `parser` closed?",
        "pr_file_module": null
      },
      {
        "comment_id": "2193231091",
        "repo_full_name": "apache/spark",
        "pr_number": 51287,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/xml/XmlInferSchema.scala",
        "discussion_id": "2191606028",
        "commented_code": "@@ -94,18 +92,25 @@ class XmlInferSchema(options: XmlOptions, caseSensitive: Boolean)\n    *   3. Replace any remaining null fields with string, the top type\n    */\n   def infer(xml: RDD[String]): StructType = {\n-    val schemaData = if (options.samplingRatio < 1.0) {\n-      xml.sample(withReplacement = false, options.samplingRatio, 1)\n+    val inferredTypesRdd = xml.mapPartitions { iter =>\n+      iter.flatMap { xml =>\n+        val parser = XMLEventReaderWithXSDValidation(xml, options)",
        "comment_created_at": "2025-07-08T18:53:40+00:00",
        "comment_author": "xiaonanyang-db",
        "comment_body": "It's not closed, we should close that after the inference, let me add that.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2196272872",
    "pr_number": 51421,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCDatabaseMetadata.scala",
    "created_at": "2025-07-10T00:55:21+00:00",
    "commented_code": "+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.jdbc\n+\n+import java.sql.Connection\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Object that contains metadata about the external database.\n+ * The metadata is static database information such as the version, or the version\n+ * of the JDBC driver.\n+ *\n+ * This object is stored in JDBCRDD.\n+ */\n+case class JDBCDatabaseMetadata(\n+   databaseMajorVersion: Option[Int],\n+   databaseMinorVersion: Option[Int],\n+   databaseDriverMajorVersion: Option[Int],\n+   databaseDriverMinorVersion: Option[Int]\n+ )\n+\n+/**\n+ * Companion object for DatabaseMetadata.\n+ * Contains factory methods to build instances.\n+ */\n+object JDBCDatabaseMetadata extends Logging {\n+\n+  /**\n+   * Safely retrieves a piece of metadata.\n+   *\n+   * @param f A function that retrieves an integer value from DatabaseMetaData.\n+   * @return Some(value) on success, None on failure.\n+   */\n+  private def safeGet(f: => Int): Option[Int] = {\n+    try {\n+      Some(f)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(log\"Exception while getting specific database metadata\", e)\n+        None\n+    }\n+  }\n+\n+  /**\n+   * Creates a DatabaseMetadata instance from a JDBC Connection,\n+   * handling errors for each field individually.\n+   *\n+   * @param conn The active database connection.\n+   * @return A new instance of DatabaseMetadata containing the version metadata.\n+   */\n+  def fromJDBCConnection(conn: Connection): JDBCDatabaseMetadata = {\n+    try {\n+      // getMetaData itself can throw, so we catch that and return None for all fields\n+      val databaseMetadata = conn.getMetaData\n+\n+      JDBCDatabaseMetadata(\n+        databaseMajorVersion = safeGet(databaseMetadata.getDatabaseMajorVersion),\n+        databaseMinorVersion = safeGet(databaseMetadata.getDatabaseMinorVersion),\n+        databaseDriverMajorVersion = safeGet(databaseMetadata.getDriverMajorVersion),\n+        databaseDriverMinorVersion = safeGet(databaseMetadata.getDriverMinorVersion)\n+      )\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(log\"Exception while getting database metadata object from connection\", e)\n+        JDBCDatabaseMetadata(None, None, None, None)\n+    } finally {\n+      conn.close()",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2196272872",
        "repo_full_name": "apache/spark",
        "pr_number": 51421,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCDatabaseMetadata.scala",
        "discussion_id": "2196272872",
        "commented_code": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.jdbc\n+\n+import java.sql.Connection\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Object that contains metadata about the external database.\n+ * The metadata is static database information such as the version, or the version\n+ * of the JDBC driver.\n+ *\n+ * This object is stored in JDBCRDD.\n+ */\n+case class JDBCDatabaseMetadata(\n+   databaseMajorVersion: Option[Int],\n+   databaseMinorVersion: Option[Int],\n+   databaseDriverMajorVersion: Option[Int],\n+   databaseDriverMinorVersion: Option[Int]\n+ )\n+\n+/**\n+ * Companion object for DatabaseMetadata.\n+ * Contains factory methods to build instances.\n+ */\n+object JDBCDatabaseMetadata extends Logging {\n+\n+  /**\n+   * Safely retrieves a piece of metadata.\n+   *\n+   * @param f A function that retrieves an integer value from DatabaseMetaData.\n+   * @return Some(value) on success, None on failure.\n+   */\n+  private def safeGet(f: => Int): Option[Int] = {\n+    try {\n+      Some(f)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(log\"Exception while getting specific database metadata\", e)\n+        None\n+    }\n+  }\n+\n+  /**\n+   * Creates a DatabaseMetadata instance from a JDBC Connection,\n+   * handling errors for each field individually.\n+   *\n+   * @param conn The active database connection.\n+   * @return A new instance of DatabaseMetadata containing the version metadata.\n+   */\n+  def fromJDBCConnection(conn: Connection): JDBCDatabaseMetadata = {\n+    try {\n+      // getMetaData itself can throw, so we catch that and return None for all fields\n+      val databaseMetadata = conn.getMetaData\n+\n+      JDBCDatabaseMetadata(\n+        databaseMajorVersion = safeGet(databaseMetadata.getDatabaseMajorVersion),\n+        databaseMinorVersion = safeGet(databaseMetadata.getDatabaseMinorVersion),\n+        databaseDriverMajorVersion = safeGet(databaseMetadata.getDriverMajorVersion),\n+        databaseDriverMinorVersion = safeGet(databaseMetadata.getDriverMinorVersion)\n+      )\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(log\"Exception while getting database metadata object from connection\", e)\n+        JDBCDatabaseMetadata(None, None, None, None)\n+    } finally {\n+      conn.close()",
        "comment_created_at": "2025-07-10T00:55:21+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "is it really safe to close the connection here? It's not created within this method.",
        "pr_file_module": null
      },
      {
        "comment_id": "2196273387",
        "repo_full_name": "apache/spark",
        "pr_number": 51421,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCDatabaseMetadata.scala",
        "discussion_id": "2196272872",
        "commented_code": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.jdbc\n+\n+import java.sql.Connection\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Object that contains metadata about the external database.\n+ * The metadata is static database information such as the version, or the version\n+ * of the JDBC driver.\n+ *\n+ * This object is stored in JDBCRDD.\n+ */\n+case class JDBCDatabaseMetadata(\n+   databaseMajorVersion: Option[Int],\n+   databaseMinorVersion: Option[Int],\n+   databaseDriverMajorVersion: Option[Int],\n+   databaseDriverMinorVersion: Option[Int]\n+ )\n+\n+/**\n+ * Companion object for DatabaseMetadata.\n+ * Contains factory methods to build instances.\n+ */\n+object JDBCDatabaseMetadata extends Logging {\n+\n+  /**\n+   * Safely retrieves a piece of metadata.\n+   *\n+   * @param f A function that retrieves an integer value from DatabaseMetaData.\n+   * @return Some(value) on success, None on failure.\n+   */\n+  private def safeGet(f: => Int): Option[Int] = {\n+    try {\n+      Some(f)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(log\"Exception while getting specific database metadata\", e)\n+        None\n+    }\n+  }\n+\n+  /**\n+   * Creates a DatabaseMetadata instance from a JDBC Connection,\n+   * handling errors for each field individually.\n+   *\n+   * @param conn The active database connection.\n+   * @return A new instance of DatabaseMetadata containing the version metadata.\n+   */\n+  def fromJDBCConnection(conn: Connection): JDBCDatabaseMetadata = {\n+    try {\n+      // getMetaData itself can throw, so we catch that and return None for all fields\n+      val databaseMetadata = conn.getMetaData\n+\n+      JDBCDatabaseMetadata(\n+        databaseMajorVersion = safeGet(databaseMetadata.getDatabaseMajorVersion),\n+        databaseMinorVersion = safeGet(databaseMetadata.getDatabaseMinorVersion),\n+        databaseDriverMajorVersion = safeGet(databaseMetadata.getDriverMajorVersion),\n+        databaseDriverMinorVersion = safeGet(databaseMetadata.getDriverMinorVersion)\n+      )\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(log\"Exception while getting database metadata object from connection\", e)\n+        JDBCDatabaseMetadata(None, None, None, None)\n+    } finally {\n+      conn.close()",
        "comment_created_at": "2025-07-10T00:56:01+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "to be safe, this method should take a connection factor and create the connection by itself.",
        "pr_file_module": null
      },
      {
        "comment_id": "2197174208",
        "repo_full_name": "apache/spark",
        "pr_number": 51421,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCDatabaseMetadata.scala",
        "discussion_id": "2196272872",
        "commented_code": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.jdbc\n+\n+import java.sql.Connection\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Object that contains metadata about the external database.\n+ * The metadata is static database information such as the version, or the version\n+ * of the JDBC driver.\n+ *\n+ * This object is stored in JDBCRDD.\n+ */\n+case class JDBCDatabaseMetadata(\n+   databaseMajorVersion: Option[Int],\n+   databaseMinorVersion: Option[Int],\n+   databaseDriverMajorVersion: Option[Int],\n+   databaseDriverMinorVersion: Option[Int]\n+ )\n+\n+/**\n+ * Companion object for DatabaseMetadata.\n+ * Contains factory methods to build instances.\n+ */\n+object JDBCDatabaseMetadata extends Logging {\n+\n+  /**\n+   * Safely retrieves a piece of metadata.\n+   *\n+   * @param f A function that retrieves an integer value from DatabaseMetaData.\n+   * @return Some(value) on success, None on failure.\n+   */\n+  private def safeGet(f: => Int): Option[Int] = {\n+    try {\n+      Some(f)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(log\"Exception while getting specific database metadata\", e)\n+        None\n+    }\n+  }\n+\n+  /**\n+   * Creates a DatabaseMetadata instance from a JDBC Connection,\n+   * handling errors for each field individually.\n+   *\n+   * @param conn The active database connection.\n+   * @return A new instance of DatabaseMetadata containing the version metadata.\n+   */\n+  def fromJDBCConnection(conn: Connection): JDBCDatabaseMetadata = {\n+    try {\n+      // getMetaData itself can throw, so we catch that and return None for all fields\n+      val databaseMetadata = conn.getMetaData\n+\n+      JDBCDatabaseMetadata(\n+        databaseMajorVersion = safeGet(databaseMetadata.getDatabaseMajorVersion),\n+        databaseMinorVersion = safeGet(databaseMetadata.getDatabaseMinorVersion),\n+        databaseDriverMajorVersion = safeGet(databaseMetadata.getDriverMajorVersion),\n+        databaseDriverMinorVersion = safeGet(databaseMetadata.getDriverMinorVersion)\n+      )\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(log\"Exception while getting database metadata object from connection\", e)\n+        JDBCDatabaseMetadata(None, None, None, None)\n+    } finally {\n+      conn.close()",
        "comment_created_at": "2025-07-10T09:49:02+00:00",
        "comment_author": "alekjarmov",
        "comment_body": "Changed so we handle everything about to connection in this method, code should be safe now.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2197629405",
    "pr_number": 51421,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCDatabaseMetadata.scala",
    "created_at": "2025-07-10T12:41:42+00:00",
    "commented_code": "+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.jdbc\n+\n+import java.sql.Connection\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Object that contains metadata about the external database.\n+ * The metadata is static database information such as the version, or the version\n+ * of the JDBC driver.\n+ *\n+ * This object is stored in JDBCRDD.\n+ */\n+case class JDBCDatabaseMetadata(\n+   databaseMajorVersion: Option[Int],\n+   databaseMinorVersion: Option[Int],\n+   databaseDriverMajorVersion: Option[Int],\n+   databaseDriverMinorVersion: Option[Int]\n+ )\n+\n+/**\n+ * Companion object for DatabaseMetadata.\n+ * Contains factory methods to build instances.\n+ */\n+object JDBCDatabaseMetadata extends Logging {\n+\n+  /**\n+   * Safely retrieves a piece of metadata.\n+   *\n+   * @param f A function that retrieves an integer value from DatabaseMetaData.\n+   * @return Some(value) on success, None on failure.\n+   */\n+  private def safeGet(f: => Int): Option[Int] = {\n+    try {\n+      Some(f)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(log\"Exception while getting specific database metadata\", e)\n+        None\n+    }\n+  }\n+\n+  /**\n+   * Creates a DatabaseMetadata instance from a JDBC Connection,\n+   * handling errors for each field individually.\n+   *\n+   * @param getConnection A JDBC connection factory.\n+   * @return A new instance of DatabaseMetadata containing the version metadata.\n+   */\n+  def fromJDBCConnectionFactory(getConnection: Int => Connection): JDBCDatabaseMetadata = {\n+    var conn: Connection = null\n+\n+    def closeConnection(): Unit = {\n+      try {\n+        if (null != conn) {\n+          conn.close()\n+        }\n+        logInfo(\"closed connection during metadata fetch\")\n+      } catch {\n+        case e: Exception => logWarning(\"Exception closing connection during metadata fetch\", e)\n+      }\n+    }\n+\n+    conn = getConnection(-1)",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2197629405",
        "repo_full_name": "apache/spark",
        "pr_number": 51421,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCDatabaseMetadata.scala",
        "discussion_id": "2197629405",
        "commented_code": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.jdbc\n+\n+import java.sql.Connection\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Object that contains metadata about the external database.\n+ * The metadata is static database information such as the version, or the version\n+ * of the JDBC driver.\n+ *\n+ * This object is stored in JDBCRDD.\n+ */\n+case class JDBCDatabaseMetadata(\n+   databaseMajorVersion: Option[Int],\n+   databaseMinorVersion: Option[Int],\n+   databaseDriverMajorVersion: Option[Int],\n+   databaseDriverMinorVersion: Option[Int]\n+ )\n+\n+/**\n+ * Companion object for DatabaseMetadata.\n+ * Contains factory methods to build instances.\n+ */\n+object JDBCDatabaseMetadata extends Logging {\n+\n+  /**\n+   * Safely retrieves a piece of metadata.\n+   *\n+   * @param f A function that retrieves an integer value from DatabaseMetaData.\n+   * @return Some(value) on success, None on failure.\n+   */\n+  private def safeGet(f: => Int): Option[Int] = {\n+    try {\n+      Some(f)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(log\"Exception while getting specific database metadata\", e)\n+        None\n+    }\n+  }\n+\n+  /**\n+   * Creates a DatabaseMetadata instance from a JDBC Connection,\n+   * handling errors for each field individually.\n+   *\n+   * @param getConnection A JDBC connection factory.\n+   * @return A new instance of DatabaseMetadata containing the version metadata.\n+   */\n+  def fromJDBCConnectionFactory(getConnection: Int => Connection): JDBCDatabaseMetadata = {\n+    var conn: Connection = null\n+\n+    def closeConnection(): Unit = {\n+      try {\n+        if (null != conn) {\n+          conn.close()\n+        }\n+        logInfo(\"closed connection during metadata fetch\")\n+      } catch {\n+        case e: Exception => logWarning(\"Exception closing connection during metadata fetch\", e)\n+      }\n+    }\n+\n+    conn = getConnection(-1)",
        "comment_created_at": "2025-07-10T12:41:42+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "what if this line fails? shall we move it into the try-catch?",
        "pr_file_module": null
      },
      {
        "comment_id": "2197728967",
        "repo_full_name": "apache/spark",
        "pr_number": 51421,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCDatabaseMetadata.scala",
        "discussion_id": "2197629405",
        "commented_code": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.jdbc\n+\n+import java.sql.Connection\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Object that contains metadata about the external database.\n+ * The metadata is static database information such as the version, or the version\n+ * of the JDBC driver.\n+ *\n+ * This object is stored in JDBCRDD.\n+ */\n+case class JDBCDatabaseMetadata(\n+   databaseMajorVersion: Option[Int],\n+   databaseMinorVersion: Option[Int],\n+   databaseDriverMajorVersion: Option[Int],\n+   databaseDriverMinorVersion: Option[Int]\n+ )\n+\n+/**\n+ * Companion object for DatabaseMetadata.\n+ * Contains factory methods to build instances.\n+ */\n+object JDBCDatabaseMetadata extends Logging {\n+\n+  /**\n+   * Safely retrieves a piece of metadata.\n+   *\n+   * @param f A function that retrieves an integer value from DatabaseMetaData.\n+   * @return Some(value) on success, None on failure.\n+   */\n+  private def safeGet(f: => Int): Option[Int] = {\n+    try {\n+      Some(f)\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(log\"Exception while getting specific database metadata\", e)\n+        None\n+    }\n+  }\n+\n+  /**\n+   * Creates a DatabaseMetadata instance from a JDBC Connection,\n+   * handling errors for each field individually.\n+   *\n+   * @param getConnection A JDBC connection factory.\n+   * @return A new instance of DatabaseMetadata containing the version metadata.\n+   */\n+  def fromJDBCConnectionFactory(getConnection: Int => Connection): JDBCDatabaseMetadata = {\n+    var conn: Connection = null\n+\n+    def closeConnection(): Unit = {\n+      try {\n+        if (null != conn) {\n+          conn.close()\n+        }\n+        logInfo(\"closed connection during metadata fetch\")\n+      } catch {\n+        case e: Exception => logWarning(\"Exception closing connection during metadata fetch\", e)\n+      }\n+    }\n+\n+    conn = getConnection(-1)",
        "comment_created_at": "2025-07-10T13:23:52+00:00",
        "comment_author": "alekjarmov",
        "comment_body": "Good point, moved it inside the try-catch.",
        "pr_file_module": null
      }
    ]
  }
]