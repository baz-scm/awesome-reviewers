[
  {
    "discussion_id": "2236417941",
    "pr_number": 51458,
    "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala",
    "created_at": "2025-07-28T13:15:44+00:00",
    "commented_code": "val logical: LogicalPlan,\n     val tracker: QueryPlanningTracker = new QueryPlanningTracker,\n     val mode: CommandExecutionMode.Value = CommandExecutionMode.ALL,\n-    val shuffleCleanupMode: ShuffleCleanupMode = DoNotCleanup) extends Logging {\n+    val shuffleCleanupMode: ShuffleCleanupMode =\n+      determineShuffleCleanupMode(SQLConf.get)) extends Logging {",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2236417941",
        "repo_full_name": "apache/spark",
        "pr_number": 51458,
        "pr_file": "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala",
        "discussion_id": "2236417941",
        "commented_code": "@@ -63,7 +64,8 @@ class QueryExecution(\n     val logical: LogicalPlan,\n     val tracker: QueryPlanningTracker = new QueryPlanningTracker,\n     val mode: CommandExecutionMode.Value = CommandExecutionMode.ALL,\n-    val shuffleCleanupMode: ShuffleCleanupMode = DoNotCleanup) extends Logging {\n+    val shuffleCleanupMode: ShuffleCleanupMode =\n+      determineShuffleCleanupMode(SQLConf.get)) extends Logging {",
        "comment_created_at": "2025-07-28T13:15:44+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "we should access the conf from the passed in `sparkSession`. One idea is \r\n```\r\n    ...\r\n    val specifiedShuffleCleanupMode: Option[ShuffleCleanupMode] = None) extends Logging {\r\n  ...\r\n  def shuffleCleanupMode = specifiedShuffleCleanupMode.getOrElse(\r\n    determineShuffleCleanupMode(sparkSession.sessionState.conf))\r\n}\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2227230316",
    "pr_number": 51543,
    "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala",
    "created_at": "2025-07-24T03:15:20+00:00",
    "commented_code": "}\n }\n \n+case class MakeTimestampFromDateTime(\n+    date: Expression,\n+    time: Option[Expression] = None,\n+    timezone: Option[Expression] = None,\n+    timeZoneId: Option[String] = None)\n+  extends Expression with RuntimeReplaceable with ExpectsInputTypes with TimeZoneAwareExpression {\n+\n+  def this(date: Expression) =\n+    this(date, None, None, None)\n+\n+  def this(date: Expression, time: Expression) =\n+    this(date, Some(time), None, None)\n+\n+  def this(date: Expression, time: Expression, timezone: Expression) =\n+    this(date, Some(time), Some(timezone), None)\n+\n+  override def children: Seq[Expression] = Seq(date) ++ time ++ timezone\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(DateType, AnyTimeType) ++\n+    timezone.map(_ => StringTypeWithCollation(supportsTrimCollation = true))\n+\n+  override def replacement: Expression = {\n+    // If time is not provided, we use midnight, i.e. 00:00:00.\n+    val timeExpr = time.getOrElse(Literal(0L, TimeType(0)))\n+    // If timezone is not provided, we use UTC, i.e. +00:00.",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2227230316",
        "repo_full_name": "apache/spark",
        "pr_number": 51543,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala",
        "discussion_id": "2227230316",
        "commented_code": "@@ -3079,6 +3083,147 @@ case class TryMakeTimestamp(\n   }\n }\n \n+case class MakeTimestampFromDateTime(\n+    date: Expression,\n+    time: Option[Expression] = None,\n+    timezone: Option[Expression] = None,\n+    timeZoneId: Option[String] = None)\n+  extends Expression with RuntimeReplaceable with ExpectsInputTypes with TimeZoneAwareExpression {\n+\n+  def this(date: Expression) =\n+    this(date, None, None, None)\n+\n+  def this(date: Expression, time: Expression) =\n+    this(date, Some(time), None, None)\n+\n+  def this(date: Expression, time: Expression, timezone: Expression) =\n+    this(date, Some(time), Some(timezone), None)\n+\n+  override def children: Seq[Expression] = Seq(date) ++ time ++ timezone\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(DateType, AnyTimeType) ++\n+    timezone.map(_ => StringTypeWithCollation(supportsTrimCollation = true))\n+\n+  override def replacement: Expression = {\n+    // If time is not provided, we use midnight, i.e. 00:00:00.\n+    val timeExpr = time.getOrElse(Literal(0L, TimeType(0)))\n+    // If timezone is not provided, we use UTC, i.e. +00:00.",
        "comment_created_at": "2025-07-24T03:15:20+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "we should use session timezone, instead of UTC, to be consistent with the other `make_timestamp` overload.",
        "pr_file_module": null
      },
      {
        "comment_id": "2227827380",
        "repo_full_name": "apache/spark",
        "pr_number": 51543,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala",
        "discussion_id": "2227230316",
        "commented_code": "@@ -3079,6 +3083,147 @@ case class TryMakeTimestamp(\n   }\n }\n \n+case class MakeTimestampFromDateTime(\n+    date: Expression,\n+    time: Option[Expression] = None,\n+    timezone: Option[Expression] = None,\n+    timeZoneId: Option[String] = None)\n+  extends Expression with RuntimeReplaceable with ExpectsInputTypes with TimeZoneAwareExpression {\n+\n+  def this(date: Expression) =\n+    this(date, None, None, None)\n+\n+  def this(date: Expression, time: Expression) =\n+    this(date, Some(time), None, None)\n+\n+  def this(date: Expression, time: Expression, timezone: Expression) =\n+    this(date, Some(time), Some(timezone), None)\n+\n+  override def children: Seq[Expression] = Seq(date) ++ time ++ timezone\n+\n+  override def inputTypes: Seq[AbstractDataType] = Seq(DateType, AnyTimeType) ++\n+    timezone.map(_ => StringTypeWithCollation(supportsTrimCollation = true))\n+\n+  override def replacement: Expression = {\n+    // If time is not provided, we use midnight, i.e. 00:00:00.\n+    val timeExpr = time.getOrElse(Literal(0L, TimeType(0)))\n+    // If timezone is not provided, we use UTC, i.e. +00:00.",
        "comment_created_at": "2025-07-24T08:28:56+00:00",
        "comment_author": "uros-db",
        "comment_body": "Yes, this actually makes more sense. Updating.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2230643348",
    "pr_number": 51543,
    "pr_file": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala",
    "created_at": "2025-07-25T09:35:40+00:00",
    "commented_code": "}\n   }\n \n-  test(\"make timestamp_ntz from date and time\") {\n-    def dateLit(d: String): Expression = Literal(LocalDate.parse(d))\n-    def timeLit(t: String): Expression = Literal(LocalTime.parse(t))\n-    def tsNtz(s: String): Long = localDateTimeToMicros(LocalDateTime.parse(s), UTC)\n+  /**\n+   * Helper method to create a DATE literal from a string in date format.\n+   */\n+  private def dateLit(date: String): Expression = Literal(LocalDate.parse(date))\n+\n+  /**\n+   * Helper method to create a TIME literal from a string in time format.\n+   */\n+  private def timeLit(time: String): Expression = Literal(LocalTime.parse(time))\n+\n+  /**\n+   * Helper method to get the microseconds from a timestamp represented as a string.\n+   */\n+  private def timestampToMicros(timestamp: String, zoneId: ZoneId): Long = {\n+    localDateTimeToMicros(LocalDateTime.parse(timestamp), zoneId)\n+  }\n+\n+  test(\"SPARK-51415: make timestamp from date\") {\n+    // Test with valid date.\n+    checkEvaluation(\n+      MakeTimestampFromDateTime(dateLit(\"2023-10-01\")),\n+      timestampToMicros(\"2023-10-01T00:00:00\", LA)\n+    )\n+\n+    // Test with null date.\n+    checkEvaluation(\n+      MakeTimestampFromDateTime(Literal(null, DateType)),\n+      null\n+    )\n+  }\n+\n+  test(\"SPARK-51415: make timestamp from date and time\") {\n+    // Test with valid date and time.\n+    checkEvaluation(\n+      MakeTimestampFromDateTime(\n+        dateLit(\"2023-10-01\"),\n+        Some(timeLit(\"12:34:56.123456\"))\n+      ),\n+      timestampToMicros(\"2023-10-01T12:34:56.123456\", LA)",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2230643348",
        "repo_full_name": "apache/spark",
        "pr_number": 51543,
        "pr_file": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala",
        "discussion_id": "2230643348",
        "commented_code": "@@ -2142,14 +2142,158 @@ class DateExpressionsSuite extends SparkFunSuite with ExpressionEvalHelper {\n     }\n   }\n \n-  test(\"make timestamp_ntz from date and time\") {\n-    def dateLit(d: String): Expression = Literal(LocalDate.parse(d))\n-    def timeLit(t: String): Expression = Literal(LocalTime.parse(t))\n-    def tsNtz(s: String): Long = localDateTimeToMicros(LocalDateTime.parse(s), UTC)\n+  /**\n+   * Helper method to create a DATE literal from a string in date format.\n+   */\n+  private def dateLit(date: String): Expression = Literal(LocalDate.parse(date))\n+\n+  /**\n+   * Helper method to create a TIME literal from a string in time format.\n+   */\n+  private def timeLit(time: String): Expression = Literal(LocalTime.parse(time))\n+\n+  /**\n+   * Helper method to get the microseconds from a timestamp represented as a string.\n+   */\n+  private def timestampToMicros(timestamp: String, zoneId: ZoneId): Long = {\n+    localDateTimeToMicros(LocalDateTime.parse(timestamp), zoneId)\n+  }\n+\n+  test(\"SPARK-51415: make timestamp from date\") {\n+    // Test with valid date.\n+    checkEvaluation(\n+      MakeTimestampFromDateTime(dateLit(\"2023-10-01\")),\n+      timestampToMicros(\"2023-10-01T00:00:00\", LA)\n+    )\n+\n+    // Test with null date.\n+    checkEvaluation(\n+      MakeTimestampFromDateTime(Literal(null, DateType)),\n+      null\n+    )\n+  }\n+\n+  test(\"SPARK-51415: make timestamp from date and time\") {\n+    // Test with valid date and time.\n+    checkEvaluation(\n+      MakeTimestampFromDateTime(\n+        dateLit(\"2023-10-01\"),\n+        Some(timeLit(\"12:34:56.123456\"))\n+      ),\n+      timestampToMicros(\"2023-10-01T12:34:56.123456\", LA)",
        "comment_created_at": "2025-07-25T09:35:40+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "This assumes the session timezone is LA, which is true today but may not always be true. Let's use `DateTimeUtils.getZoneId(SQLConf.get.sessionLocalTimeZone)`",
        "pr_file_module": null
      },
      {
        "comment_id": "2231330039",
        "repo_full_name": "apache/spark",
        "pr_number": 51543,
        "pr_file": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala",
        "discussion_id": "2230643348",
        "commented_code": "@@ -2142,14 +2142,158 @@ class DateExpressionsSuite extends SparkFunSuite with ExpressionEvalHelper {\n     }\n   }\n \n-  test(\"make timestamp_ntz from date and time\") {\n-    def dateLit(d: String): Expression = Literal(LocalDate.parse(d))\n-    def timeLit(t: String): Expression = Literal(LocalTime.parse(t))\n-    def tsNtz(s: String): Long = localDateTimeToMicros(LocalDateTime.parse(s), UTC)\n+  /**\n+   * Helper method to create a DATE literal from a string in date format.\n+   */\n+  private def dateLit(date: String): Expression = Literal(LocalDate.parse(date))\n+\n+  /**\n+   * Helper method to create a TIME literal from a string in time format.\n+   */\n+  private def timeLit(time: String): Expression = Literal(LocalTime.parse(time))\n+\n+  /**\n+   * Helper method to get the microseconds from a timestamp represented as a string.\n+   */\n+  private def timestampToMicros(timestamp: String, zoneId: ZoneId): Long = {\n+    localDateTimeToMicros(LocalDateTime.parse(timestamp), zoneId)\n+  }\n+\n+  test(\"SPARK-51415: make timestamp from date\") {\n+    // Test with valid date.\n+    checkEvaluation(\n+      MakeTimestampFromDateTime(dateLit(\"2023-10-01\")),\n+      timestampToMicros(\"2023-10-01T00:00:00\", LA)\n+    )\n+\n+    // Test with null date.\n+    checkEvaluation(\n+      MakeTimestampFromDateTime(Literal(null, DateType)),\n+      null\n+    )\n+  }\n+\n+  test(\"SPARK-51415: make timestamp from date and time\") {\n+    // Test with valid date and time.\n+    checkEvaluation(\n+      MakeTimestampFromDateTime(\n+        dateLit(\"2023-10-01\"),\n+        Some(timeLit(\"12:34:56.123456\"))\n+      ),\n+      timestampToMicros(\"2023-10-01T12:34:56.123456\", LA)",
        "comment_created_at": "2025-07-25T14:54:14+00:00",
        "comment_author": "uros-db",
        "comment_body": "Updated.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2161073990",
    "pr_number": 51235,
    "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JSONOptions.scala",
    "created_at": "2025-06-23T08:52:54+00:00",
    "commented_code": "private val maxStringLen: Int = parameters\n     .get(\"maxStringLen\")\n     .map(_.toInt)\n-    .getOrElse(StreamReadConstraints.DEFAULT_MAX_STRING_LEN)\n+    .getOrElse(SQLConf.get.getConf(SQLConf.JSON_MAX_STRING_LENGTH))",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2161073990",
        "repo_full_name": "apache/spark",
        "pr_number": 51235,
        "pr_file": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JSONOptions.scala",
        "discussion_id": "2161073990",
        "commented_code": "@@ -55,7 +55,7 @@ class JSONOptions(\n   private val maxStringLen: Int = parameters\n     .get(\"maxStringLen\")\n     .map(_.toInt)\n-    .getOrElse(StreamReadConstraints.DEFAULT_MAX_STRING_LEN)\n+    .getOrElse(SQLConf.get.getConf(SQLConf.JSON_MAX_STRING_LENGTH))",
        "comment_created_at": "2025-06-23T08:52:54+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "can we follow `ParquetOptions` and pass a `SQLConf` instance to construct `JSONOptions`?",
        "pr_file_module": null
      }
    ]
  }
]