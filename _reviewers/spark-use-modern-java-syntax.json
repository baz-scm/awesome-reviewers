[
  {
    "discussion_id": "2190664803",
    "pr_number": 50933,
    "pr_file": "common/sketch/src/main/java/org/apache/spark/util/sketch/BloomFilter.java",
    "created_at": "2025-07-07T17:14:07+00:00",
    "commented_code": "* the stream.\n    */\n   public static BloomFilter readFrom(InputStream in) throws IOException {\n-    return BloomFilterImpl.readFrom(in);\n+      // peek into the inputstream so we can determine the version\n+      BufferedInputStream bin = new BufferedInputStream(in);\n+      bin.mark(4);\n+      int version = ByteBuffer.wrap(bin.readNBytes(4)).getInt();\n+      bin.reset();\n+\n+      BloomFilter result;\n+      switch (version) {\n+        case 1:\n+          result = BloomFilterImpl.readFrom(bin);\n+          break;\n+        case 2:\n+          result = BloomFilterImplV2.readFrom(bin);\n+          break;\n+        default:\n+          throw new IllegalArgumentException(\"Unknown BloomFilter version: \" + version);\n+      }\n+\n+      return result;",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2190664803",
        "repo_full_name": "apache/spark",
        "pr_number": 50933,
        "pr_file": "common/sketch/src/main/java/org/apache/spark/util/sketch/BloomFilter.java",
        "discussion_id": "2190664803",
        "commented_code": "@@ -175,14 +179,34 @@ public long cardinality() {\n    * the stream.\n    */\n   public static BloomFilter readFrom(InputStream in) throws IOException {\n-    return BloomFilterImpl.readFrom(in);\n+      // peek into the inputstream so we can determine the version\n+      BufferedInputStream bin = new BufferedInputStream(in);\n+      bin.mark(4);\n+      int version = ByteBuffer.wrap(bin.readNBytes(4)).getInt();\n+      bin.reset();\n+\n+      BloomFilter result;\n+      switch (version) {\n+        case 1:\n+          result = BloomFilterImpl.readFrom(bin);\n+          break;\n+        case 2:\n+          result = BloomFilterImplV2.readFrom(bin);\n+          break;\n+        default:\n+          throw new IllegalArgumentException(\"Unknown BloomFilter version: \" + version);\n+      }\n+\n+      return result;",
        "comment_created_at": "2025-07-07T17:14:07+00:00",
        "comment_author": "dongjoon-hyun",
        "comment_body": "FYI, since Apache Spark 4.0.0, we recommend to use more concise Java syntax [JEP-361](https://openjdk.org/jeps/361). For example, you can reduce these 13 lines into 5 lines. Could you try that?\r\n\r\n```java\r\n-      BloomFilter result;\r\n-      switch (version) {\r\n-        case 1:\r\n-          result = BloomFilterImpl.readFrom(bin);\r\n-          break;\r\n-        case 2:\r\n-          result = BloomFilterImplV2.readFrom(bin);\r\n-          break;\r\n-        default:\r\n-          throw new IllegalArgumentException(\"Unknown BloomFilter version: \" + version);\r\n-      }\r\n-\r\n-      return result;\r\n+      return switch (version) {\r\n+        case 1 -> BloomFilterImpl.readFrom(bin);\r\n+        case 2 -> BloomFilterImplV2.readFrom(bin);\r\n+        default -> throw new IllegalArgumentException(\"Unknown BloomFilter version: \" + version);\r\n+      };\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2190784612",
        "repo_full_name": "apache/spark",
        "pr_number": 50933,
        "pr_file": "common/sketch/src/main/java/org/apache/spark/util/sketch/BloomFilter.java",
        "discussion_id": "2190664803",
        "commented_code": "@@ -175,14 +179,34 @@ public long cardinality() {\n    * the stream.\n    */\n   public static BloomFilter readFrom(InputStream in) throws IOException {\n-    return BloomFilterImpl.readFrom(in);\n+      // peek into the inputstream so we can determine the version\n+      BufferedInputStream bin = new BufferedInputStream(in);\n+      bin.mark(4);\n+      int version = ByteBuffer.wrap(bin.readNBytes(4)).getInt();\n+      bin.reset();\n+\n+      BloomFilter result;\n+      switch (version) {\n+        case 1:\n+          result = BloomFilterImpl.readFrom(bin);\n+          break;\n+        case 2:\n+          result = BloomFilterImplV2.readFrom(bin);\n+          break;\n+        default:\n+          throw new IllegalArgumentException(\"Unknown BloomFilter version: \" + version);\n+      }\n+\n+      return result;",
        "comment_created_at": "2025-07-07T18:29:11+00:00",
        "comment_author": "ishnagy",
        "comment_body": "fixed in [1ee2e13](https://github.com/apache/spark/pull/50933/commits/1ee2e137607365017f3cf806411156b7fc05549a)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2126599385",
    "pr_number": 50921,
    "pr_file": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/join/JoinColumn.java",
    "created_at": "2025-06-04T13:24:43+00:00",
    "commented_code": "+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.join;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.connector.expressions.Expression;\n+import org.apache.spark.sql.connector.expressions.NamedReference;\n+\n+/**\n+ * Represents a column reference used in DSv2 Join pushdown.\n+ *\n+ * @since 4.0.0\n+ */\n+@Evolving\n+public final class JoinColumn implements NamedReference {",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2126599385",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/join/JoinColumn.java",
        "discussion_id": "2126599385",
        "commented_code": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.join;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.connector.expressions.Expression;\n+import org.apache.spark.sql.connector.expressions.NamedReference;\n+\n+/**\n+ * Represents a column reference used in DSv2 Join pushdown.\n+ *\n+ * @since 4.0.0\n+ */\n+@Evolving\n+public final class JoinColumn implements NamedReference {",
        "comment_created_at": "2025-06-04T13:24:43+00:00",
        "comment_author": "urosstan-db",
        "comment_body": "Java record seems like better fit here",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2192444826",
    "pr_number": 50921,
    "pr_file": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsPushDownJoin.java",
    "created_at": "2025-07-08T12:53:49+00:00",
    "commented_code": "+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.read;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.connector.expressions.filter.Predicate;\n+import org.apache.spark.sql.connector.join.JoinType;\n+\n+/**\n+ * A mix-in interface for {@link ScanBuilder}. Data sources can implement this interface to\n+ * push down join operators.\n+ *\n+ * @since 4.1.0\n+ */\n+@Evolving\n+public interface SupportsPushDownJoin extends ScanBuilder {\n+  /**\n+   * Returns true if the other side of the join is compatible with the\n+   * current {@code SupportsPushDownJoin} for a join push down, meaning both sides can be\n+   * processed together within the same underlying data source.\n+   * <br>\n+   * <br>\n+   * For example, JDBC connectors are compatible if they use the same\n+   * host, port, username, and password.\n+   */\n+  boolean isOtherSideCompatibleForJoin(SupportsPushDownJoin other);\n+\n+  /**\n+   * Pushes down the join of the current {@code SupportsPushDownJoin} and the other side of join\n+   * {@code SupportsPushDownJoin}.\n+   *\n+   * @param other {@code SupportsPushDownJoin} that this {@code SupportsPushDownJoin}\n+   * gets joined with.\n+   * @param joinType the type of join.\n+   * @param leftSideRequiredColumnWithAliases required output of the left side {@code SupportsPushDownJoin}\n+   * @param rightSideRequiredColumnWithAliases required output of the right side {@code SupportsPushDownJoin}\n+   * @param condition join condition. Columns are named after the specified aliases in\n+   * {@code leftSideRequiredColumnWithAliases} and {@code rightSideRequiredColumnWithAliases}\n+   * @return True if join has been successfully pushed down.\n+   */\n+  boolean pushDownJoin(\n+      SupportsPushDownJoin other,\n+      JoinType joinType,\n+      ColumnWithAlias[] leftSideRequiredColumnWithAliases,\n+      ColumnWithAlias[] rightSideRequiredColumnWithAliases,\n+      Predicate condition\n+  );\n+\n+  /**\n+   *  A helper class used when there are duplicated names coming from 2 sides of the join\n+   *  operator.\n+   *  <br>\n+   *  Holds information of original output name and the alias of the new output.\n+   */\n+  class ColumnWithAlias {",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2192444826",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsPushDownJoin.java",
        "discussion_id": "2192444826",
        "commented_code": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.read;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.connector.expressions.filter.Predicate;\n+import org.apache.spark.sql.connector.join.JoinType;\n+\n+/**\n+ * A mix-in interface for {@link ScanBuilder}. Data sources can implement this interface to\n+ * push down join operators.\n+ *\n+ * @since 4.1.0\n+ */\n+@Evolving\n+public interface SupportsPushDownJoin extends ScanBuilder {\n+  /**\n+   * Returns true if the other side of the join is compatible with the\n+   * current {@code SupportsPushDownJoin} for a join push down, meaning both sides can be\n+   * processed together within the same underlying data source.\n+   * <br>\n+   * <br>\n+   * For example, JDBC connectors are compatible if they use the same\n+   * host, port, username, and password.\n+   */\n+  boolean isOtherSideCompatibleForJoin(SupportsPushDownJoin other);\n+\n+  /**\n+   * Pushes down the join of the current {@code SupportsPushDownJoin} and the other side of join\n+   * {@code SupportsPushDownJoin}.\n+   *\n+   * @param other {@code SupportsPushDownJoin} that this {@code SupportsPushDownJoin}\n+   * gets joined with.\n+   * @param joinType the type of join.\n+   * @param leftSideRequiredColumnWithAliases required output of the left side {@code SupportsPushDownJoin}\n+   * @param rightSideRequiredColumnWithAliases required output of the right side {@code SupportsPushDownJoin}\n+   * @param condition join condition. Columns are named after the specified aliases in\n+   * {@code leftSideRequiredColumnWithAliases} and {@code rightSideRequiredColumnWithAliases}\n+   * @return True if join has been successfully pushed down.\n+   */\n+  boolean pushDownJoin(\n+      SupportsPushDownJoin other,\n+      JoinType joinType,\n+      ColumnWithAlias[] leftSideRequiredColumnWithAliases,\n+      ColumnWithAlias[] rightSideRequiredColumnWithAliases,\n+      Predicate condition\n+  );\n+\n+  /**\n+   *  A helper class used when there are duplicated names coming from 2 sides of the join\n+   *  operator.\n+   *  <br>\n+   *  Holds information of original output name and the alias of the new output.\n+   */\n+  class ColumnWithAlias {",
        "comment_created_at": "2025-07-08T12:53:49+00:00",
        "comment_author": "urosstan-db",
        "comment_body": "Java `record` can be used here instead of `class`, it provides more concise implementation for structs.",
        "pr_file_module": null
      },
      {
        "comment_id": "2192732818",
        "repo_full_name": "apache/spark",
        "pr_number": 50921,
        "pr_file": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsPushDownJoin.java",
        "discussion_id": "2192444826",
        "commented_code": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.read;\n+\n+import org.apache.spark.annotation.Evolving;\n+import org.apache.spark.sql.connector.expressions.filter.Predicate;\n+import org.apache.spark.sql.connector.join.JoinType;\n+\n+/**\n+ * A mix-in interface for {@link ScanBuilder}. Data sources can implement this interface to\n+ * push down join operators.\n+ *\n+ * @since 4.1.0\n+ */\n+@Evolving\n+public interface SupportsPushDownJoin extends ScanBuilder {\n+  /**\n+   * Returns true if the other side of the join is compatible with the\n+   * current {@code SupportsPushDownJoin} for a join push down, meaning both sides can be\n+   * processed together within the same underlying data source.\n+   * <br>\n+   * <br>\n+   * For example, JDBC connectors are compatible if they use the same\n+   * host, port, username, and password.\n+   */\n+  boolean isOtherSideCompatibleForJoin(SupportsPushDownJoin other);\n+\n+  /**\n+   * Pushes down the join of the current {@code SupportsPushDownJoin} and the other side of join\n+   * {@code SupportsPushDownJoin}.\n+   *\n+   * @param other {@code SupportsPushDownJoin} that this {@code SupportsPushDownJoin}\n+   * gets joined with.\n+   * @param joinType the type of join.\n+   * @param leftSideRequiredColumnWithAliases required output of the left side {@code SupportsPushDownJoin}\n+   * @param rightSideRequiredColumnWithAliases required output of the right side {@code SupportsPushDownJoin}\n+   * @param condition join condition. Columns are named after the specified aliases in\n+   * {@code leftSideRequiredColumnWithAliases} and {@code rightSideRequiredColumnWithAliases}\n+   * @return True if join has been successfully pushed down.\n+   */\n+  boolean pushDownJoin(\n+      SupportsPushDownJoin other,\n+      JoinType joinType,\n+      ColumnWithAlias[] leftSideRequiredColumnWithAliases,\n+      ColumnWithAlias[] rightSideRequiredColumnWithAliases,\n+      Predicate condition\n+  );\n+\n+  /**\n+   *  A helper class used when there are duplicated names coming from 2 sides of the join\n+   *  operator.\n+   *  <br>\n+   *  Holds information of original output name and the alias of the new output.\n+   */\n+  class ColumnWithAlias {",
        "comment_created_at": "2025-07-08T14:46:51+00:00",
        "comment_author": "PetarVasiljevic-DB",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  }
]