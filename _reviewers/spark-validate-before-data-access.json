[
  {
    "discussion_id": "2221201115",
    "pr_number": 51475,
    "pr_file": "python/pyspark/pandas/typedef/typehints.py",
    "created_at": "2025-07-22T05:20:50+00:00",
    "commented_code": "if dtype == np.dtype(\"object\"):\n         if len(pser) == 0 or pser.isnull().all():\n             return types.NullType()\n-        elif hasattr(pser.iloc[0], \"__UDT__\"):\n-            return pser.iloc[0].__UDT__\n+        first_idx = pser.first_valid_index()\n+        if first_idx is not None and hasattr(pser.loc[first_idx], \"__UDT__\"):\n+            return pser.loc[first_idx].__UDT__",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2221201115",
        "repo_full_name": "apache/spark",
        "pr_number": 51475,
        "pr_file": "python/pyspark/pandas/typedef/typehints.py",
        "discussion_id": "2221201115",
        "commented_code": "@@ -362,8 +362,9 @@ def infer_pd_series_spark_type(\n     if dtype == np.dtype(\"object\"):\n         if len(pser) == 0 or pser.isnull().all():\n             return types.NullType()\n-        elif hasattr(pser.iloc[0], \"__UDT__\"):\n-            return pser.iloc[0].__UDT__\n+        first_idx = pser.first_valid_index()\n+        if first_idx is not None and hasattr(pser.loc[first_idx], \"__UDT__\"):\n+            return pser.loc[first_idx].__UDT__",
        "comment_created_at": "2025-07-22T05:20:50+00:00",
        "comment_author": "ueshin",
        "comment_body": "Good catch! but using `.loc` sounds dangerous, e.g.:\r\n\r\n```py\r\n>>> pser = pd.Series([None,None,3], index=[1,0,1])\r\n>>> i = pser.first_valid_index()\r\n>>> pser.loc[i]\r\n1    NaN\r\n1    3.0\r\ndtype: float64\r\n```\r\n\r\nHow about using `notnull()`?\r\n\r\n```suggestion\r\n        notnull = pser[pser.notnull()]\r\n        if hasattr(notnull.iloc[0], \"__UDT__\"):\r\n            return notnull.iloc[0].__UDT__\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2223081908",
        "repo_full_name": "apache/spark",
        "pr_number": 51475,
        "pr_file": "python/pyspark/pandas/typedef/typehints.py",
        "discussion_id": "2221201115",
        "commented_code": "@@ -362,8 +362,9 @@ def infer_pd_series_spark_type(\n     if dtype == np.dtype(\"object\"):\n         if len(pser) == 0 or pser.isnull().all():\n             return types.NullType()\n-        elif hasattr(pser.iloc[0], \"__UDT__\"):\n-            return pser.iloc[0].__UDT__\n+        first_idx = pser.first_valid_index()\n+        if first_idx is not None and hasattr(pser.loc[first_idx], \"__UDT__\"):\n+            return pser.loc[first_idx].__UDT__",
        "comment_created_at": "2025-07-22T16:16:33+00:00",
        "comment_author": "petern48",
        "comment_body": "Nice! Accepted the change and all tests passing",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2211496072",
    "pr_number": 51507,
    "pr_file": "python/pyspark/pipelines/spark_connect_pipeline.py",
    "created_at": "2025-07-16T20:15:32+00:00",
    "commented_code": "log_with_provided_timestamp(event.message, dt)\n \n \n-def start_run(spark: SparkSession, dataflow_graph_id: str) -> Iterator[Dict[str, Any]]:\n+def start_run(\n+    spark: SparkSession,\n+    dataflow_graph_id: str,\n+    full_refresh: Optional[Sequence[str]] = None,\n+    full_refresh_all: bool = False,\n+    refresh: Optional[Sequence[str]] = None,\n+) -> Iterator[Dict[str, Any]]:\n     \"\"\"Start a run of the dataflow graph in the Spark Connect server.\n \n     :param dataflow_graph_id: The ID of the dataflow graph to start.\n+    :param full_refresh: List of datasets to reset and recompute.\n+    :param full_refresh_all: Perform a full graph reset and recompute.\n+    :param refresh: List of datasets to update.\n     \"\"\"\n-    inner_command = pb2.PipelineCommand.StartRun(dataflow_graph_id=dataflow_graph_id)\n+    inner_command = pb2.PipelineCommand.StartRun(\n+        dataflow_graph_id=dataflow_graph_id,\n+        full_refresh=full_refresh or [],",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2211496072",
        "repo_full_name": "apache/spark",
        "pr_number": 51507,
        "pr_file": "python/pyspark/pipelines/spark_connect_pipeline.py",
        "discussion_id": "2211496072",
        "commented_code": "@@ -65,12 +65,26 @@ def handle_pipeline_events(iter: Iterator[Dict[str, Any]]) -> None:\n             log_with_provided_timestamp(event.message, dt)\n \n \n-def start_run(spark: SparkSession, dataflow_graph_id: str) -> Iterator[Dict[str, Any]]:\n+def start_run(\n+    spark: SparkSession,\n+    dataflow_graph_id: str,\n+    full_refresh: Optional[Sequence[str]] = None,\n+    full_refresh_all: bool = False,\n+    refresh: Optional[Sequence[str]] = None,\n+) -> Iterator[Dict[str, Any]]:\n     \"\"\"Start a run of the dataflow graph in the Spark Connect server.\n \n     :param dataflow_graph_id: The ID of the dataflow graph to start.\n+    :param full_refresh: List of datasets to reset and recompute.\n+    :param full_refresh_all: Perform a full graph reset and recompute.\n+    :param refresh: List of datasets to update.\n     \"\"\"\n-    inner_command = pb2.PipelineCommand.StartRun(dataflow_graph_id=dataflow_graph_id)\n+    inner_command = pb2.PipelineCommand.StartRun(\n+        dataflow_graph_id=dataflow_graph_id,\n+        full_refresh=full_refresh or [],",
        "comment_created_at": "2025-07-16T20:15:32+00:00",
        "comment_author": "AnishMahto",
        "comment_body": "I'm a little confused by this. If `full_refresh` is None, we return an empty list. But in `flatten_table_lists`, which we use to construct this `full_refresh` list, we return None if the flattened list is empty. Can we simplify the logic here or there?",
        "pr_file_module": null
      }
    ]
  }
]