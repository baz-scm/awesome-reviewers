[
  {
    "discussion_id": "2208324544",
    "pr_number": 51499,
    "pr_file": ".github/workflows/build_and_test.yml",
    "created_at": "2025-07-15T18:35:30+00:00",
    "commented_code": "- name: R linter\n       run: ./dev/lint-r\n \n+  java17:\n+    needs: [precondition]\n+    if: fromJson(needs.precondition.outputs.required).java17 == 'true'\n+    name: Java 17 build with Maven\n+    runs-on: ubuntu-latest\n+    timeout-minutes: 120\n+    steps:\n+    - uses: actions/checkout@v4\n+    - uses: actions/setup-java@v4\n+      with:\n+        distribution: zulu\n+        java-version: 17",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2208324544",
        "repo_full_name": "apache/spark",
        "pr_number": 51499,
        "pr_file": ".github/workflows/build_and_test.yml",
        "discussion_id": "2208324544",
        "commented_code": "@@ -920,6 +921,24 @@ jobs:\n     - name: R linter\n       run: ./dev/lint-r\n \n+  java17:\n+    needs: [precondition]\n+    if: fromJson(needs.precondition.outputs.required).java17 == 'true'\n+    name: Java 17 build with Maven\n+    runs-on: ubuntu-latest\n+    timeout-minutes: 120\n+    steps:\n+    - uses: actions/checkout@v4\n+    - uses: actions/setup-java@v4\n+      with:\n+        distribution: zulu\n+        java-version: 17",
        "comment_created_at": "2025-07-15T18:35:30+00:00",
        "comment_author": "cloud-fan",
        "comment_body": "so Github Action will always install the latest java 17?",
        "pr_file_module": null
      },
      {
        "comment_id": "2216585255",
        "repo_full_name": "apache/spark",
        "pr_number": 51499,
        "pr_file": ".github/workflows/build_and_test.yml",
        "discussion_id": "2208324544",
        "commented_code": "@@ -920,6 +921,24 @@ jobs:\n     - name: R linter\n       run: ./dev/lint-r\n \n+  java17:\n+    needs: [precondition]\n+    if: fromJson(needs.precondition.outputs.required).java17 == 'true'\n+    name: Java 17 build with Maven\n+    runs-on: ubuntu-latest\n+    timeout-minutes: 120\n+    steps:\n+    - uses: actions/checkout@v4\n+    - uses: actions/setup-java@v4\n+      with:\n+        distribution: zulu\n+        java-version: 17",
        "comment_created_at": "2025-07-18T17:40:52+00:00",
        "comment_author": "dongjoon-hyun",
        "comment_body": "Oh, I missed your comment here. Yes, it does. If we don't specify the exact version like `17.0.x`, 17 is the alias.\r\n\r\nIn addition, `actions/setup-java@v4` itself is an alias of `v4.x.x`, too. We are consuming the auto-updated status, @cloud-fan .",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2022667425",
    "pr_number": 50463,
    "pr_file": ".github/workflows/maven_test.yml",
    "created_at": "2025-04-01T11:25:55+00:00",
    "commented_code": "if [[ \"$INCLUDED_TAGS\" != \"\" ]]; then\n             ./build/mvn $MAVEN_CLI_OPTS -pl \"$TEST_MODULES\" -Pyarn -Pkubernetes -Pvolcano -Phive -Phive-thriftserver -Phadoop-cloud -Pjvm-profiler -Pspark-ganglia-lgpl -Pkinesis-asl -Djava.version=${JAVA_VERSION/-ea} -Dtest.include.tags=\"$INCLUDED_TAGS\" test -fae\n           elif [[ \"$MODULES_TO_TEST\" == \"connect\" ]]; then\n-            ./build/mvn $MAVEN_CLI_OPTS -Dtest.exclude.tags=\"$EXCLUDED_TAGS\" -Djava.version=${JAVA_VERSION/-ea} -pl sql/connect/client/jvm,sql/connect/common,sql/connect/server test -fae\n+            ./build/mvn $MAVEN_CLI_OPTS -Dtest.exclude.tags=\"$EXCLUDED_TAGS\" -Djava.version=${JAVA_VERSION/-ea} -pl sql/connect/client/jvm,sql/connect/client/integration-tests,sql/connect/common,sql/connect/server package test -fae",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2022667425",
        "repo_full_name": "apache/spark",
        "pr_number": 50463,
        "pr_file": ".github/workflows/maven_test.yml",
        "discussion_id": "2022667425",
        "commented_code": "@@ -198,7 +198,7 @@ jobs:\n           if [[ \"$INCLUDED_TAGS\" != \"\" ]]; then\n             ./build/mvn $MAVEN_CLI_OPTS -pl \"$TEST_MODULES\" -Pyarn -Pkubernetes -Pvolcano -Phive -Phive-thriftserver -Phadoop-cloud -Pjvm-profiler -Pspark-ganglia-lgpl -Pkinesis-asl -Djava.version=${JAVA_VERSION/-ea} -Dtest.include.tags=\"$INCLUDED_TAGS\" test -fae\n           elif [[ \"$MODULES_TO_TEST\" == \"connect\" ]]; then\n-            ./build/mvn $MAVEN_CLI_OPTS -Dtest.exclude.tags=\"$EXCLUDED_TAGS\" -Djava.version=${JAVA_VERSION/-ea} -pl sql/connect/client/jvm,sql/connect/common,sql/connect/server test -fae\n+            ./build/mvn $MAVEN_CLI_OPTS -Dtest.exclude.tags=\"$EXCLUDED_TAGS\" -Djava.version=${JAVA_VERSION/-ea} -pl sql/connect/client/jvm,sql/connect/client/integration-tests,sql/connect/common,sql/connect/server package test -fae",
        "comment_created_at": "2025-04-01T11:25:55+00:00",
        "comment_author": "LuciferYang",
        "comment_body": "Here, it is necessary to use `package test`. If only `test` is specified, then `client-integration-tests` will use the `client-jvm`'s classes directory instead of a jar file as the build classpath, which will result in a failure of `testCompile`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2017827658",
    "pr_number": 50423,
    "pr_file": ".github/workflows/build_and_test.yml",
    "created_at": "2025-03-28T02:38:29+00:00",
    "commented_code": "if: inputs.branch != 'branch-3.5'\n       run: ./dev/check-protos.py\n \n+  repl:\n+    needs: [precondition]\n+    if: (!cancelled()) && fromJson(needs.precondition.outputs.required).repl == 'true'\n+    name: REPL (spark-sql, spark-shell and pyspark)\n+    runs-on: ubuntu-latest\n+    timeout-minutes: 45\n+    env:\n+      LC_ALL: C.UTF-8\n+      LANG: C.UTF-8\n+      PYSPARK_DRIVER_PYTHON: python3.11\n+      PYSPARK_PYTHON: python3.11\n+    steps:\n+      - name: Checkout Spark repository\n+        uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+          repository: apache/spark\n+          ref: ${{ inputs.branch }}\n+      - name: Sync the current branch with the latest in Apache Spark\n+        if: github.repository != 'apache/spark'\n+        run: |\n+          git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}\n+          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD\n+          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m \"Merged commit\" --allow-empty\n+      - name: Install Java 17\n+        uses: actions/setup-java@v4\n+        with:\n+          distribution: zulu\n+          java-version: 17\n+      - name: Install Python 3.11\n+        uses: actions/setup-python@v5\n+        with:\n+          python-version: '3.11'\n+      - name: Install dependencies for PySpark\n+        run: |\n+          python3.11 -m pip install ipython numpy scipy 'protobuf==5.28.3' 'pyarrow>=19.0.0' 'six==1.16.0' 'pandas==2.2.3' 'grpcio==1.67.0' 'grpcio-status==1.67.0' 'protobuf==5.28.3' 'googleapis-common-protos==1.65.0'\n+          python3.11 -m pip list\n+      - name: Build Spark\n+        run: |\n+          ./build/sbt -Phive -Phive-thriftserver clean package",
    "repo_full_name": "apache/spark",
    "discussion_comments": [
      {
        "comment_id": "2017827658",
        "repo_full_name": "apache/spark",
        "pr_number": 50423,
        "pr_file": ".github/workflows/build_and_test.yml",
        "discussion_id": "2017827658",
        "commented_code": "@@ -769,6 +772,85 @@ jobs:\n       if: inputs.branch != 'branch-3.5'\n       run: ./dev/check-protos.py\n \n+  repl:\n+    needs: [precondition]\n+    if: (!cancelled()) && fromJson(needs.precondition.outputs.required).repl == 'true'\n+    name: REPL (spark-sql, spark-shell and pyspark)\n+    runs-on: ubuntu-latest\n+    timeout-minutes: 45\n+    env:\n+      LC_ALL: C.UTF-8\n+      LANG: C.UTF-8\n+      PYSPARK_DRIVER_PYTHON: python3.11\n+      PYSPARK_PYTHON: python3.11\n+    steps:\n+      - name: Checkout Spark repository\n+        uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+          repository: apache/spark\n+          ref: ${{ inputs.branch }}\n+      - name: Sync the current branch with the latest in Apache Spark\n+        if: github.repository != 'apache/spark'\n+        run: |\n+          git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}\n+          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD\n+          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m \"Merged commit\" --allow-empty\n+      - name: Install Java 17\n+        uses: actions/setup-java@v4\n+        with:\n+          distribution: zulu\n+          java-version: 17\n+      - name: Install Python 3.11\n+        uses: actions/setup-python@v5\n+        with:\n+          python-version: '3.11'\n+      - name: Install dependencies for PySpark\n+        run: |\n+          python3.11 -m pip install ipython numpy scipy 'protobuf==5.28.3' 'pyarrow>=19.0.0' 'six==1.16.0' 'pandas==2.2.3' 'grpcio==1.67.0' 'grpcio-status==1.67.0' 'protobuf==5.28.3' 'googleapis-common-protos==1.65.0'\n+          python3.11 -m pip list\n+      - name: Build Spark\n+        run: |\n+          ./build/sbt -Phive -Phive-thriftserver clean package",
        "comment_created_at": "2025-03-28T02:38:29+00:00",
        "comment_author": "LuciferYang",
        "comment_body": "Ultimately, we still need to use Maven to compile and package the Spark Client. Although the result produced by `sbt package` can also be used for testing, I think it does not necessarily mean that the Client packaged by Maven will also be healthy and free of issues.",
        "pr_file_module": null
      },
      {
        "comment_id": "2017828991",
        "repo_full_name": "apache/spark",
        "pr_number": 50423,
        "pr_file": ".github/workflows/build_and_test.yml",
        "discussion_id": "2017827658",
        "commented_code": "@@ -769,6 +772,85 @@ jobs:\n       if: inputs.branch != 'branch-3.5'\n       run: ./dev/check-protos.py\n \n+  repl:\n+    needs: [precondition]\n+    if: (!cancelled()) && fromJson(needs.precondition.outputs.required).repl == 'true'\n+    name: REPL (spark-sql, spark-shell and pyspark)\n+    runs-on: ubuntu-latest\n+    timeout-minutes: 45\n+    env:\n+      LC_ALL: C.UTF-8\n+      LANG: C.UTF-8\n+      PYSPARK_DRIVER_PYTHON: python3.11\n+      PYSPARK_PYTHON: python3.11\n+    steps:\n+      - name: Checkout Spark repository\n+        uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+          repository: apache/spark\n+          ref: ${{ inputs.branch }}\n+      - name: Sync the current branch with the latest in Apache Spark\n+        if: github.repository != 'apache/spark'\n+        run: |\n+          git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}\n+          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD\n+          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m \"Merged commit\" --allow-empty\n+      - name: Install Java 17\n+        uses: actions/setup-java@v4\n+        with:\n+          distribution: zulu\n+          java-version: 17\n+      - name: Install Python 3.11\n+        uses: actions/setup-python@v5\n+        with:\n+          python-version: '3.11'\n+      - name: Install dependencies for PySpark\n+        run: |\n+          python3.11 -m pip install ipython numpy scipy 'protobuf==5.28.3' 'pyarrow>=19.0.0' 'six==1.16.0' 'pandas==2.2.3' 'grpcio==1.67.0' 'grpcio-status==1.67.0' 'protobuf==5.28.3' 'googleapis-common-protos==1.65.0'\n+          python3.11 -m pip list\n+      - name: Build Spark\n+        run: |\n+          ./build/sbt -Phive -Phive-thriftserver clean package",
        "comment_created_at": "2025-03-28T02:40:40+00:00",
        "comment_author": "LuciferYang",
        "comment_body": "So I believe that the verification of the output from `dev/make-distribution.sh` should be more rigorous and convincing.",
        "pr_file_module": null
      },
      {
        "comment_id": "2017857891",
        "repo_full_name": "apache/spark",
        "pr_number": 50423,
        "pr_file": ".github/workflows/build_and_test.yml",
        "discussion_id": "2017827658",
        "commented_code": "@@ -769,6 +772,85 @@ jobs:\n       if: inputs.branch != 'branch-3.5'\n       run: ./dev/check-protos.py\n \n+  repl:\n+    needs: [precondition]\n+    if: (!cancelled()) && fromJson(needs.precondition.outputs.required).repl == 'true'\n+    name: REPL (spark-sql, spark-shell and pyspark)\n+    runs-on: ubuntu-latest\n+    timeout-minutes: 45\n+    env:\n+      LC_ALL: C.UTF-8\n+      LANG: C.UTF-8\n+      PYSPARK_DRIVER_PYTHON: python3.11\n+      PYSPARK_PYTHON: python3.11\n+    steps:\n+      - name: Checkout Spark repository\n+        uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+          repository: apache/spark\n+          ref: ${{ inputs.branch }}\n+      - name: Sync the current branch with the latest in Apache Spark\n+        if: github.repository != 'apache/spark'\n+        run: |\n+          git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}\n+          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD\n+          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m \"Merged commit\" --allow-empty\n+      - name: Install Java 17\n+        uses: actions/setup-java@v4\n+        with:\n+          distribution: zulu\n+          java-version: 17\n+      - name: Install Python 3.11\n+        uses: actions/setup-python@v5\n+        with:\n+          python-version: '3.11'\n+      - name: Install dependencies for PySpark\n+        run: |\n+          python3.11 -m pip install ipython numpy scipy 'protobuf==5.28.3' 'pyarrow>=19.0.0' 'six==1.16.0' 'pandas==2.2.3' 'grpcio==1.67.0' 'grpcio-status==1.67.0' 'protobuf==5.28.3' 'googleapis-common-protos==1.65.0'\n+          python3.11 -m pip list\n+      - name: Build Spark\n+        run: |\n+          ./build/sbt -Phive -Phive-thriftserver clean package",
        "comment_created_at": "2025-03-28T03:31:37+00:00",
        "comment_author": "zhengruifeng",
        "comment_body": "My original thoughts is to protect contributors' daily development.\r\nIn case the shell built by `sbt` was broken (happened multiple times), it will be very hard to figure out the offending commits.\r\n\r\nI think we can also test `maven` here if necessary.\r\n",
        "pr_file_module": null
      }
    ]
  }
]