[
  {
    "discussion_id": "2007336028",
    "pr_number": 34237,
    "pr_file": "packages/pg-meta/src/query/table-row-query.ts",
    "created_at": "2025-03-21T10:56:43+00:00",
    "commented_code": "+import { ident } from '../pg-format'\n+import { PGTable } from '../pg-meta-tables'\n+import { Query } from './Query'\n+import { Sort, Filter } from './types'\n+import { PGView } from '../pg-meta-views'\n+import { PGForeignTable } from '../pg-meta-foreign-tables'\n+import { PGMaterializedView } from '../pg-meta-materialized-views'\n+\n+// Constants\n+export const MAX_CHARACTERS = 10 * 1024 // 10KB\n+// Max array size\n+export const MAX_ARRAY_SIZE = 50\n+\n+export type TableLikeEntity = PGTable | PGView | PGForeignTable | PGMaterializedView\n+\n+export interface BuildTableRowsQueryArgs {\n+  table: TableLikeEntity\n+  filters?: Filter[]\n+  sorts?: Sort[]\n+  limit?: number\n+  page?: number\n+  maxCharacters?: number\n+  maxArraySize?: number\n+}\n+\n+// Text and JSON types that should be truncated\n+export const TEXT_TYPES = ['text', 'varchar', 'char', 'character varying', 'character']\n+export const JSON_TYPES = ['json', 'jsonb']\n+\n+// Additional PostgreSQL types that can hold large values and should be truncated\n+export const ADDITIONAL_LARGE_TYPES = [\n+  // Standard PostgreSQL types\n+  'bytea', // Binary data\n+  'xml', // XML data\n+  'hstore', // Key-value store\n+  'clob', // Character large object\n+\n+  // Extension-specific types\n+  // pgvector extension (for AI/ML/RAG applications)\n+  'vector', // Vector type used for embeddings\n+\n+  // PostGIS extension types\n+  'geometry', // Spatial data type\n+  'geography', // Spatial data type\n+\n+  // Full-text search types\n+  'tsvector', // Text search vector\n+  'tsquery', // Text search query\n+\n+  // Range types\n+  'daterange', // Date range\n+  'tsrange', // Timestamp range\n+  'tstzrange', // Timestamp with timezone range\n+  'numrange', // Numeric range\n+  'int4range', // Integer range\n+  'int8range', // Bigint range\n+\n+  // Other extension types\n+  'cube', // Multi-dimensional cube\n+  'ltree', // Label tree\n+  'lquery', // Label tree query\n+  'jsonpath', // JSON path expressions\n+  'citext', // Case-insensitive text\n+]\n+\n+export const LARGE_COLUMNS_TYPES = [...TEXT_TYPES, ...JSON_TYPES, ...ADDITIONAL_LARGE_TYPES]\n+const LARGE_COLUMNS_TYPES_SET = new Set(LARGE_COLUMNS_TYPES)\n+\n+// Threshold count for applying default sort\n+export const THRESHOLD_COUNT = 100000\n+\n+// Return the primary key columns if exists, otherwise return the first column to use as a default sort\n+export const getDefaultOrderByColumns = (table: Pick<PGTable, 'primary_keys' | 'columns'>) => {\n+  const primaryKeyColumns = table.primary_keys?.map((pk) => pk.name)\n+  if (primaryKeyColumns && primaryKeyColumns.length > 0) {\n+    return primaryKeyColumns\n+  }\n+  if (table.columns && table.columns.length > 0) {\n+    return [table.columns[0].name]\n+  }\n+  return []\n+}\n+\n+/**\n+ * Determines if a column type should be truncated based on its format and dataType\n+ */\n+export const shouldTruncateColumn = (columnFormat: string): boolean =>\n+  LARGE_COLUMNS_TYPES_SET.has(columnFormat.toLowerCase())\n+\n+export const DEFAULT_PAGE_SIZE = 100\n+\n+export function getPagination(page?: number, size: number = DEFAULT_PAGE_SIZE) {\n+  const limit = size\n+  const from = page ? page * limit : 0\n+  const to = page ? from + size - 1 : size - 1\n+\n+  return { from, to }\n+}\n+\n+export const getTableRowsSql = ({\n+  table,\n+  filters = [],\n+  sorts = [],\n+  page,\n+  limit,\n+  maxCharacters = MAX_CHARACTERS,\n+  maxArraySize = MAX_ARRAY_SIZE,\n+}: BuildTableRowsQueryArgs) => {\n+  if (!table || !table.columns) return ``\n+\n+  const query = new Query()\n+\n+  const allColumnNames = table.columns\n+    .sort((a, b) => a.ordinal_position - b.ordinal_position)\n+    .map((column) => column.name)\n+\n+  // Identify columns that might need truncation\n+  const columnsToTruncate = table.columns\n+    .filter((column) => shouldTruncateColumn(column.format))\n+    .map((column) => column.name)\n+\n+  // Create select expressions for each column, applying truncation only to needed columns\n+  const selectExpressions = allColumnNames.map((columnName) => {\n+    const escapedColumnName = ident(columnName)\n+\n+    if (columnsToTruncate.includes(columnName)) {\n+      return `case\n+        when octet_length(${escapedColumnName}::text) > ${maxCharacters} \n+        then left(${escapedColumnName}::text, ${maxCharacters}) || '...'\n+        else ${escapedColumnName}::text\n+      end as ${escapedColumnName}`\n+    } else {\n+      return escapedColumnName\n+    }",
    "repo_full_name": "supabase/supabase",
    "discussion_comments": [
      {
        "comment_id": "2007336028",
        "repo_full_name": "supabase/supabase",
        "pr_number": 34237,
        "pr_file": "packages/pg-meta/src/query/table-row-query.ts",
        "discussion_id": "2007336028",
        "commented_code": "@@ -0,0 +1,197 @@\n+import { ident } from '../pg-format'\n+import { PGTable } from '../pg-meta-tables'\n+import { Query } from './Query'\n+import { Sort, Filter } from './types'\n+import { PGView } from '../pg-meta-views'\n+import { PGForeignTable } from '../pg-meta-foreign-tables'\n+import { PGMaterializedView } from '../pg-meta-materialized-views'\n+\n+// Constants\n+export const MAX_CHARACTERS = 10 * 1024 // 10KB\n+// Max array size\n+export const MAX_ARRAY_SIZE = 50\n+\n+export type TableLikeEntity = PGTable | PGView | PGForeignTable | PGMaterializedView\n+\n+export interface BuildTableRowsQueryArgs {\n+  table: TableLikeEntity\n+  filters?: Filter[]\n+  sorts?: Sort[]\n+  limit?: number\n+  page?: number\n+  maxCharacters?: number\n+  maxArraySize?: number\n+}\n+\n+// Text and JSON types that should be truncated\n+export const TEXT_TYPES = ['text', 'varchar', 'char', 'character varying', 'character']\n+export const JSON_TYPES = ['json', 'jsonb']\n+\n+// Additional PostgreSQL types that can hold large values and should be truncated\n+export const ADDITIONAL_LARGE_TYPES = [\n+  // Standard PostgreSQL types\n+  'bytea', // Binary data\n+  'xml', // XML data\n+  'hstore', // Key-value store\n+  'clob', // Character large object\n+\n+  // Extension-specific types\n+  // pgvector extension (for AI/ML/RAG applications)\n+  'vector', // Vector type used for embeddings\n+\n+  // PostGIS extension types\n+  'geometry', // Spatial data type\n+  'geography', // Spatial data type\n+\n+  // Full-text search types\n+  'tsvector', // Text search vector\n+  'tsquery', // Text search query\n+\n+  // Range types\n+  'daterange', // Date range\n+  'tsrange', // Timestamp range\n+  'tstzrange', // Timestamp with timezone range\n+  'numrange', // Numeric range\n+  'int4range', // Integer range\n+  'int8range', // Bigint range\n+\n+  // Other extension types\n+  'cube', // Multi-dimensional cube\n+  'ltree', // Label tree\n+  'lquery', // Label tree query\n+  'jsonpath', // JSON path expressions\n+  'citext', // Case-insensitive text\n+]\n+\n+export const LARGE_COLUMNS_TYPES = [...TEXT_TYPES, ...JSON_TYPES, ...ADDITIONAL_LARGE_TYPES]\n+const LARGE_COLUMNS_TYPES_SET = new Set(LARGE_COLUMNS_TYPES)\n+\n+// Threshold count for applying default sort\n+export const THRESHOLD_COUNT = 100000\n+\n+// Return the primary key columns if exists, otherwise return the first column to use as a default sort\n+export const getDefaultOrderByColumns = (table: Pick<PGTable, 'primary_keys' | 'columns'>) => {\n+  const primaryKeyColumns = table.primary_keys?.map((pk) => pk.name)\n+  if (primaryKeyColumns && primaryKeyColumns.length > 0) {\n+    return primaryKeyColumns\n+  }\n+  if (table.columns && table.columns.length > 0) {\n+    return [table.columns[0].name]\n+  }\n+  return []\n+}\n+\n+/**\n+ * Determines if a column type should be truncated based on its format and dataType\n+ */\n+export const shouldTruncateColumn = (columnFormat: string): boolean =>\n+  LARGE_COLUMNS_TYPES_SET.has(columnFormat.toLowerCase())\n+\n+export const DEFAULT_PAGE_SIZE = 100\n+\n+export function getPagination(page?: number, size: number = DEFAULT_PAGE_SIZE) {\n+  const limit = size\n+  const from = page ? page * limit : 0\n+  const to = page ? from + size - 1 : size - 1\n+\n+  return { from, to }\n+}\n+\n+export const getTableRowsSql = ({\n+  table,\n+  filters = [],\n+  sorts = [],\n+  page,\n+  limit,\n+  maxCharacters = MAX_CHARACTERS,\n+  maxArraySize = MAX_ARRAY_SIZE,\n+}: BuildTableRowsQueryArgs) => {\n+  if (!table || !table.columns) return ``\n+\n+  const query = new Query()\n+\n+  const allColumnNames = table.columns\n+    .sort((a, b) => a.ordinal_position - b.ordinal_position)\n+    .map((column) => column.name)\n+\n+  // Identify columns that might need truncation\n+  const columnsToTruncate = table.columns\n+    .filter((column) => shouldTruncateColumn(column.format))\n+    .map((column) => column.name)\n+\n+  // Create select expressions for each column, applying truncation only to needed columns\n+  const selectExpressions = allColumnNames.map((columnName) => {\n+    const escapedColumnName = ident(columnName)\n+\n+    if (columnsToTruncate.includes(columnName)) {\n+      return `case\n+        when octet_length(${escapedColumnName}::text) > ${maxCharacters} \n+        then left(${escapedColumnName}::text, ${maxCharacters}) || '...'\n+        else ${escapedColumnName}::text\n+      end as ${escapedColumnName}`\n+    } else {\n+      return escapedColumnName\n+    }",
        "comment_created_at": "2025-03-21T10:56:43+00:00",
        "comment_author": "avallete",
        "comment_body": "**note**\r\n\r\nWhy `octet_length` ? During benchmark, on a table with large text fields (~50Mo), switching from `length` to `octect_length` got the query time to go from ~11s to <100ms.\r\n\r\nProbably because this allow pg to skip the text conversion and just use his internal \"binary length\". Since it does that for every row in the switch case that end up pilling up.\r\n\r\nSince we don't really care about the actual exact \"length\" in terms of \"string characters\" but just want a \"weight size\" it should not really have any meaningful impact on the experience except speeding things up for all queries with this switch-case in it.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2007338766",
    "pr_number": 34237,
    "pr_file": "packages/pg-meta/src/query/table-row-query.ts",
    "created_at": "2025-03-21T10:58:46+00:00",
    "commented_code": "+import { ident } from '../pg-format'\n+import { PGTable } from '../pg-meta-tables'\n+import { Query } from './Query'\n+import { Sort, Filter } from './types'\n+import { PGView } from '../pg-meta-views'\n+import { PGForeignTable } from '../pg-meta-foreign-tables'\n+import { PGMaterializedView } from '../pg-meta-materialized-views'\n+\n+// Constants\n+export const MAX_CHARACTERS = 10 * 1024 // 10KB\n+// Max array size\n+export const MAX_ARRAY_SIZE = 50\n+\n+export type TableLikeEntity = PGTable | PGView | PGForeignTable | PGMaterializedView\n+\n+export interface BuildTableRowsQueryArgs {\n+  table: TableLikeEntity\n+  filters?: Filter[]\n+  sorts?: Sort[]\n+  limit?: number\n+  page?: number\n+  maxCharacters?: number\n+  maxArraySize?: number\n+}\n+\n+// Text and JSON types that should be truncated\n+export const TEXT_TYPES = ['text', 'varchar', 'char', 'character varying', 'character']\n+export const JSON_TYPES = ['json', 'jsonb']\n+\n+// Additional PostgreSQL types that can hold large values and should be truncated\n+export const ADDITIONAL_LARGE_TYPES = [\n+  // Standard PostgreSQL types\n+  'bytea', // Binary data\n+  'xml', // XML data\n+  'hstore', // Key-value store\n+  'clob', // Character large object\n+\n+  // Extension-specific types\n+  // pgvector extension (for AI/ML/RAG applications)\n+  'vector', // Vector type used for embeddings\n+\n+  // PostGIS extension types\n+  'geometry', // Spatial data type\n+  'geography', // Spatial data type\n+\n+  // Full-text search types\n+  'tsvector', // Text search vector\n+  'tsquery', // Text search query\n+\n+  // Range types\n+  'daterange', // Date range\n+  'tsrange', // Timestamp range\n+  'tstzrange', // Timestamp with timezone range\n+  'numrange', // Numeric range\n+  'int4range', // Integer range\n+  'int8range', // Bigint range\n+\n+  // Other extension types\n+  'cube', // Multi-dimensional cube\n+  'ltree', // Label tree\n+  'lquery', // Label tree query\n+  'jsonpath', // JSON path expressions\n+  'citext', // Case-insensitive text\n+]\n+\n+export const LARGE_COLUMNS_TYPES = [...TEXT_TYPES, ...JSON_TYPES, ...ADDITIONAL_LARGE_TYPES]\n+const LARGE_COLUMNS_TYPES_SET = new Set(LARGE_COLUMNS_TYPES)\n+\n+// Threshold count for applying default sort\n+export const THRESHOLD_COUNT = 100000\n+\n+// Return the primary key columns if exists, otherwise return the first column to use as a default sort\n+export const getDefaultOrderByColumns = (table: Pick<PGTable, 'primary_keys' | 'columns'>) => {\n+  const primaryKeyColumns = table.primary_keys?.map((pk) => pk.name)\n+  if (primaryKeyColumns && primaryKeyColumns.length > 0) {\n+    return primaryKeyColumns\n+  }\n+  if (table.columns && table.columns.length > 0) {\n+    return [table.columns[0].name]\n+  }\n+  return []\n+}\n+\n+/**\n+ * Determines if a column type should be truncated based on its format and dataType\n+ */\n+export const shouldTruncateColumn = (columnFormat: string): boolean =>\n+  LARGE_COLUMNS_TYPES_SET.has(columnFormat.toLowerCase())\n+\n+export const DEFAULT_PAGE_SIZE = 100\n+\n+export function getPagination(page?: number, size: number = DEFAULT_PAGE_SIZE) {\n+  const limit = size\n+  const from = page ? page * limit : 0\n+  const to = page ? from + size - 1 : size - 1\n+\n+  return { from, to }\n+}\n+\n+export const getTableRowsSql = ({\n+  table,\n+  filters = [],\n+  sorts = [],\n+  page,\n+  limit,\n+  maxCharacters = MAX_CHARACTERS,\n+  maxArraySize = MAX_ARRAY_SIZE,\n+}: BuildTableRowsQueryArgs) => {\n+  if (!table || !table.columns) return ``\n+\n+  const query = new Query()\n+\n+  const allColumnNames = table.columns\n+    .sort((a, b) => a.ordinal_position - b.ordinal_position)\n+    .map((column) => column.name)\n+\n+  // Identify columns that might need truncation\n+  const columnsToTruncate = table.columns\n+    .filter((column) => shouldTruncateColumn(column.format))\n+    .map((column) => column.name)\n+\n+  // Create select expressions for each column, applying truncation only to needed columns\n+  const selectExpressions = allColumnNames.map((columnName) => {\n+    const escapedColumnName = ident(columnName)\n+\n+    if (columnsToTruncate.includes(columnName)) {\n+      return `case\n+        when octet_length(${escapedColumnName}::text) > ${maxCharacters} \n+        then left(${escapedColumnName}::text, ${maxCharacters}) || '...'\n+        else ${escapedColumnName}::text\n+      end as ${escapedColumnName}`\n+    } else {\n+      return escapedColumnName\n+    }\n+  })\n+\n+  // Handle array-based columns\n+  const arrayBasedColumnNames = table.columns\n+    .filter((column) => column.data_type.toLowerCase() === 'array')\n+    .map((column) => column.name)\n+\n+  // Add array casting for array-based enum columns\n+  arrayBasedColumnNames.forEach((columnName) => {\n+    // Find this column in our select expressions\n+    const index = selectExpressions.findIndex(\n+      (expr) => expr === ident(columnName) // if the column is selected without any truncation applied to it\n+    )\n+    if (index >= 0) {\n+      // We cast to text[] but limit the array size if the total size of the array is too large (same logic than for text fields)\n+      // This returns the first MAX_ARRAY_SIZE elements of the array (adjustable) and adds '...' if truncated\n+      // NOTE: this is not optimal, as the first element in the array could still be very large (more than 10Kb) and in such case\n+      // the trimming might fail.\n+      selectExpressions[index] = `\n+        case \n+          when octet_length(${ident(columnName)}::text) > ${maxCharacters} \n+          then (select array_cat(${ident(columnName)}[1:${maxArraySize}]::text[], array['...']))::text[]\n+          else ${ident(columnName)}::text[]\n+        end\n+      `\n+    }",
    "repo_full_name": "supabase/supabase",
    "discussion_comments": [
      {
        "comment_id": "2007338766",
        "repo_full_name": "supabase/supabase",
        "pr_number": 34237,
        "pr_file": "packages/pg-meta/src/query/table-row-query.ts",
        "discussion_id": "2007338766",
        "commented_code": "@@ -0,0 +1,197 @@\n+import { ident } from '../pg-format'\n+import { PGTable } from '../pg-meta-tables'\n+import { Query } from './Query'\n+import { Sort, Filter } from './types'\n+import { PGView } from '../pg-meta-views'\n+import { PGForeignTable } from '../pg-meta-foreign-tables'\n+import { PGMaterializedView } from '../pg-meta-materialized-views'\n+\n+// Constants\n+export const MAX_CHARACTERS = 10 * 1024 // 10KB\n+// Max array size\n+export const MAX_ARRAY_SIZE = 50\n+\n+export type TableLikeEntity = PGTable | PGView | PGForeignTable | PGMaterializedView\n+\n+export interface BuildTableRowsQueryArgs {\n+  table: TableLikeEntity\n+  filters?: Filter[]\n+  sorts?: Sort[]\n+  limit?: number\n+  page?: number\n+  maxCharacters?: number\n+  maxArraySize?: number\n+}\n+\n+// Text and JSON types that should be truncated\n+export const TEXT_TYPES = ['text', 'varchar', 'char', 'character varying', 'character']\n+export const JSON_TYPES = ['json', 'jsonb']\n+\n+// Additional PostgreSQL types that can hold large values and should be truncated\n+export const ADDITIONAL_LARGE_TYPES = [\n+  // Standard PostgreSQL types\n+  'bytea', // Binary data\n+  'xml', // XML data\n+  'hstore', // Key-value store\n+  'clob', // Character large object\n+\n+  // Extension-specific types\n+  // pgvector extension (for AI/ML/RAG applications)\n+  'vector', // Vector type used for embeddings\n+\n+  // PostGIS extension types\n+  'geometry', // Spatial data type\n+  'geography', // Spatial data type\n+\n+  // Full-text search types\n+  'tsvector', // Text search vector\n+  'tsquery', // Text search query\n+\n+  // Range types\n+  'daterange', // Date range\n+  'tsrange', // Timestamp range\n+  'tstzrange', // Timestamp with timezone range\n+  'numrange', // Numeric range\n+  'int4range', // Integer range\n+  'int8range', // Bigint range\n+\n+  // Other extension types\n+  'cube', // Multi-dimensional cube\n+  'ltree', // Label tree\n+  'lquery', // Label tree query\n+  'jsonpath', // JSON path expressions\n+  'citext', // Case-insensitive text\n+]\n+\n+export const LARGE_COLUMNS_TYPES = [...TEXT_TYPES, ...JSON_TYPES, ...ADDITIONAL_LARGE_TYPES]\n+const LARGE_COLUMNS_TYPES_SET = new Set(LARGE_COLUMNS_TYPES)\n+\n+// Threshold count for applying default sort\n+export const THRESHOLD_COUNT = 100000\n+\n+// Return the primary key columns if exists, otherwise return the first column to use as a default sort\n+export const getDefaultOrderByColumns = (table: Pick<PGTable, 'primary_keys' | 'columns'>) => {\n+  const primaryKeyColumns = table.primary_keys?.map((pk) => pk.name)\n+  if (primaryKeyColumns && primaryKeyColumns.length > 0) {\n+    return primaryKeyColumns\n+  }\n+  if (table.columns && table.columns.length > 0) {\n+    return [table.columns[0].name]\n+  }\n+  return []\n+}\n+\n+/**\n+ * Determines if a column type should be truncated based on its format and dataType\n+ */\n+export const shouldTruncateColumn = (columnFormat: string): boolean =>\n+  LARGE_COLUMNS_TYPES_SET.has(columnFormat.toLowerCase())\n+\n+export const DEFAULT_PAGE_SIZE = 100\n+\n+export function getPagination(page?: number, size: number = DEFAULT_PAGE_SIZE) {\n+  const limit = size\n+  const from = page ? page * limit : 0\n+  const to = page ? from + size - 1 : size - 1\n+\n+  return { from, to }\n+}\n+\n+export const getTableRowsSql = ({\n+  table,\n+  filters = [],\n+  sorts = [],\n+  page,\n+  limit,\n+  maxCharacters = MAX_CHARACTERS,\n+  maxArraySize = MAX_ARRAY_SIZE,\n+}: BuildTableRowsQueryArgs) => {\n+  if (!table || !table.columns) return ``\n+\n+  const query = new Query()\n+\n+  const allColumnNames = table.columns\n+    .sort((a, b) => a.ordinal_position - b.ordinal_position)\n+    .map((column) => column.name)\n+\n+  // Identify columns that might need truncation\n+  const columnsToTruncate = table.columns\n+    .filter((column) => shouldTruncateColumn(column.format))\n+    .map((column) => column.name)\n+\n+  // Create select expressions for each column, applying truncation only to needed columns\n+  const selectExpressions = allColumnNames.map((columnName) => {\n+    const escapedColumnName = ident(columnName)\n+\n+    if (columnsToTruncate.includes(columnName)) {\n+      return `case\n+        when octet_length(${escapedColumnName}::text) > ${maxCharacters} \n+        then left(${escapedColumnName}::text, ${maxCharacters}) || '...'\n+        else ${escapedColumnName}::text\n+      end as ${escapedColumnName}`\n+    } else {\n+      return escapedColumnName\n+    }\n+  })\n+\n+  // Handle array-based columns\n+  const arrayBasedColumnNames = table.columns\n+    .filter((column) => column.data_type.toLowerCase() === 'array')\n+    .map((column) => column.name)\n+\n+  // Add array casting for array-based enum columns\n+  arrayBasedColumnNames.forEach((columnName) => {\n+    // Find this column in our select expressions\n+    const index = selectExpressions.findIndex(\n+      (expr) => expr === ident(columnName) // if the column is selected without any truncation applied to it\n+    )\n+    if (index >= 0) {\n+      // We cast to text[] but limit the array size if the total size of the array is too large (same logic than for text fields)\n+      // This returns the first MAX_ARRAY_SIZE elements of the array (adjustable) and adds '...' if truncated\n+      // NOTE: this is not optimal, as the first element in the array could still be very large (more than 10Kb) and in such case\n+      // the trimming might fail.\n+      selectExpressions[index] = `\n+        case \n+          when octet_length(${ident(columnName)}::text) > ${maxCharacters} \n+          then (select array_cat(${ident(columnName)}[1:${maxArraySize}]::text[], array['...']))::text[]\n+          else ${ident(columnName)}::text[]\n+        end\n+      `\n+    }",
        "comment_created_at": "2025-03-21T10:58:46+00:00",
        "comment_author": "avallete",
        "comment_body": "**note**\r\n\r\nThis is a BC (kind of) like mentioned in previous comment. We didn't trimmed out array type before, but they can also be quite large by nature. Note that the solution isn't perfect as you could still end up with the first element of your array being > 10Kb for instance, and that wouldn't be trimmed out properly.\r\n\r\nBut that seemed like a reasonable tradeoff to me as it allow to preserve an \"array like\" structure for the result.",
        "pr_file_module": null
      }
    ]
  }
]