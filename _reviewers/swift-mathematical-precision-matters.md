---
title: Mathematical precision matters
description: When implementing mathematical algorithms, ensure precise terminology
  and correct implementation of mathematical concepts. This is especially important
  for complex domains like differentiation, optimization, or numerical methods.
repository: tensorflow/swift
label: Algorithms
language: Markdown
comments_count: 4
repository_stars: 6136
---

When implementing mathematical algorithms, ensure precise terminology and correct implementation of mathematical concepts. This is especially important for complex domains like differentiation, optimization, or numerical methods.

Three key practices to follow:

1. **Use mathematically accurate terminology in documentation**
   Descriptions should precisely match mathematical definitions. For example, when documenting differentiation:

   ```swift
   // INCORRECT:
   // Speaking in terms of elementary calculus, only functions that have derivatives can be differentiated.
   
   // CORRECT:
   // Speaking in terms of elementary calculus, only functions are "differentiable": only functions can *have derivatives* and can *be differentiated*.
   ```

2. **Be explicit about algorithm limitations and edge cases**
   Clearly state when algorithms might fail or have special requirements:

   ```swift
   // GOOD EXPLANATION:
   // Differentiation can fail for several reasons:
   // * The function contains computation that cannot be differentiated
   // * The function is opaque (a function parameter with a non-@differentiable type)
   ```

3. **Ensure mathematical correctness in example code**
   Example implementations must correctly implement the mathematical operations they demonstrate:

   ```swift
   // INCORRECT pullback implementation (missing parameter):
   @differentiating(sillyExp)
   func sillyDerivative(_ x: Float) -> (value: Float, pullback: (Float) -> Float) {
     let y = sillyExp(x)
     return (value: y, pullback: { _ in y }) // Incorrect: ignores parameter
   }
   
   // CORRECT implementation:
   @differentiating(sillyExp)
   func sillyDerivative(_ x: Float) -> (value: Float, pullback: (Float) -> Float) {
     let y = sillyExp(x)
     return (value: y, pullback: { v in v * y }) // Correct: uses parameter
   }
   ```

Following these practices ensures that algorithms are implemented correctly and are properly documented, making them more maintainable and less prone to subtle errors that can emerge when mathematical concepts are imprecisely translated to code.


[
  {
    "discussion_id": "350304991",
    "pr_number": 330,
    "pr_file": "docs/DifferentiableTypes.md",
    "created_at": "2019-11-25T16:55:29+00:00",
    "commented_code": "## Preface\n\nSpeaking in terms of elementary calculus, only functions are \"differentiable\": only functions have derivatives and can be differentiated. In this document, the terminology \"differentiable types\" is used as a shorthand for \"types that can be used as arguments and results of differentiable functions\". This notion is important because not all types are \"differentiable\" in this sense. For example, types representing real numbers and vector spaces are \"differentiable\", but strings and integers are not.\nSpeaking in terms of elementary calculus, only functions are \"differentiable\": only functions that have derivatives can be differentiated. In this document, the terminology \"differentiable types\" is used as a shorthand for \"types that can be used as arguments and results of differentiable functions\". This notion is important because not all types are \"differentiable\" in this sense. For example, types representing real numbers and vector spaces are \"differentiable\", but strings and integers are not.",
    "repo_full_name": "tensorflow/swift",
    "discussion_comments": [
      {
        "comment_id": "350304991",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 330,
        "pr_file": "docs/DifferentiableTypes.md",
        "discussion_id": "350304991",
        "commented_code": "@@ -15,7 +15,7 @@ Last updated: March 2019\n \n ## Preface\n \n-Speaking in terms of elementary calculus, only functions are \"differentiable\": only functions have derivatives and can be differentiated. In this document, the terminology \"differentiable types\" is used as a shorthand for \"types that can be used as arguments and results of differentiable functions\". This notion is important because not all types are \"differentiable\" in this sense. For example, types representing real numbers and vector spaces are \"differentiable\", but strings and integers are not.\n+Speaking in terms of elementary calculus, only functions are \"differentiable\": only functions that have derivatives can be differentiated. In this document, the terminology \"differentiable types\" is used as a shorthand for \"types that can be used as arguments and results of differentiable functions\". This notion is important because not all types are \"differentiable\" in this sense. For example, types representing real numbers and vector spaces are \"differentiable\", but strings and integers are not.",
        "comment_created_at": "2019-11-25T16:55:29+00:00",
        "comment_author": "dan-zheng",
        "comment_body": "I think \"only functions can *have derivatives*\" is closer to the original intention:\r\n```suggestion\r\nSpeaking in terms of elementary calculus, only functions are \"differentiable\": only functions can *have derivatives* and can *be differentiated*. In this document, the terminology \"differentiable types\" is used as a shorthand for \"types that can be used as arguments and results of differentiable functions\". This notion is important because not all types are \"differentiable\" in this sense. For example, types representing real numbers and vector spaces are \"differentiable\", but strings and integers are not.\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "350561321",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 330,
        "pr_file": "docs/DifferentiableTypes.md",
        "discussion_id": "350304991",
        "commented_code": "@@ -15,7 +15,7 @@ Last updated: March 2019\n \n ## Preface\n \n-Speaking in terms of elementary calculus, only functions are \"differentiable\": only functions have derivatives and can be differentiated. In this document, the terminology \"differentiable types\" is used as a shorthand for \"types that can be used as arguments and results of differentiable functions\". This notion is important because not all types are \"differentiable\" in this sense. For example, types representing real numbers and vector spaces are \"differentiable\", but strings and integers are not.\n+Speaking in terms of elementary calculus, only functions are \"differentiable\": only functions that have derivatives can be differentiated. In this document, the terminology \"differentiable types\" is used as a shorthand for \"types that can be used as arguments and results of differentiable functions\". This notion is important because not all types are \"differentiable\" in this sense. For example, types representing real numbers and vector spaces are \"differentiable\", but strings and integers are not.",
        "comment_created_at": "2019-11-26T06:27:54+00:00",
        "comment_author": "stmugisha",
        "comment_body": "Alright. Thanks for the clarification",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "266174431",
    "pr_number": 145,
    "pr_file": "docs/DifferentiableFunctions.md",
    "created_at": "2019-03-15T23:29:32+00:00",
    "commented_code": "# Differentiable functions and differentiation APIs\n\n[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n\nLast updated: March 2019\n\n## Introduction\n\nSwift supports differentiable functions as part of the language. The\n`@differentiable` attribute appears in two locations in Swift syntax: as an\nannotation on function types and as an annotation on function declarations (and\nother similar declarations). This document explains the meaning of these\nannotations.\n\n## `@differentiable` function type attribute\n\n### Basics\n\nIn Swift, function types can have attributes. When a function type is annotated\nwith `@differentiable`, Swift guarantees that all values of that function type\ncan be differentiated.\n\n`@differentiable` functions can be called like normal functions, or be passed to\nAPIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\nrepresentation of a `@differentiable` function is a special data structure\ncontaining the original function along with extra information required for\ncomputing its derivatives. Usage `@differentiable` functions are a part of\nSwift's type system. Most notably, they are used by differentiation APIs in the\nstandard library. Here are some examples demonstrating differentiation APIs:\n\n```swift\nfunc square(_ x: Float) -> Float {\n    return x * x\n}\nlet x: Float = 3.0\n\n// Free function examples.\n\n// Computes the gradient of `square` at `x`.\nprint(gradient(at: x, in: square)) // 6.0\n// Computes the gradient of `square`, then applies it to `x`.\nprint(gradient(of: square)(x)) // 6.0\n// Computes the value and gradient of `square` at `x`.\nprint(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n\n// Method examples.\n\n// Computes the gradient of `square` at `x`.\nprint(x.gradient(in: square)) // 6.0\n// Computes the value and gradient of `square` at `x`.\nprint(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n```\n\nHere's a list of differentiation APIs provided by the standard library:\n\n| Differentiation APIs  | Description                                                                                                                                                      |\n|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n\n### Constructing `@differentiable` functions\n\nA value with a non-differentiable function type can be implicitly converted to\none with a corresponding `@differentiable` function type. In fact, this is what\nhappened in the example above:\n\n```swift\n// `square` has type `(Float) -> Float`.\nfunc square(_ x: Float) -> Float {\n    return x * x\n}\nlet x: Float = 3.0\n\n// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n// `@differentiable` type.\nprint(gradient(at: x, in: square)) // 6.0\n```\n\nThe implicit conversion from a value with type `(T) -> U` to a value with type\n`@differentiable (T) -> U` actually triggers differentiation by the compiler.\nThus, differentiation is type-driven.\n\nIf differentiation succeeds, the `@differentiable (T) -> U` value is\nconstructed. If differentiation fails, the compiler emits a compile-time error:\n\n```swift\nlet add: @differentiable (Float, Float) -> Float = { x, y in\n    // The `Int` initializer call below is non-differentiable.\n    Float(Int(x + y))\n}\n```\n\n```console\ntest.swift:1:52: error: function is not differentiable\nlet add: @differentiable (Float, Float) -> Float = { x, y in\n                                                   ^~~~~~~~~\ntest.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n    Float(Int(x + y))\n          ^\n```\n\nThere are a few reasons why differentiation can fail:\n* The function to differentiate contains non-differentiable computation along",
    "repo_full_name": "tensorflow/swift",
    "discussion_comments": [
      {
        "comment_id": "266174431",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 145,
        "pr_file": "docs/DifferentiableFunctions.md",
        "discussion_id": "266174431",
        "commented_code": "@@ -0,0 +1,444 @@\n+# Differentiable functions and differentiation APIs\n+\n+[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n+\n+Last updated: March 2019\n+\n+## Introduction\n+\n+Swift supports differentiable functions as part of the language. The\n+`@differentiable` attribute appears in two locations in Swift syntax: as an\n+annotation on function types and as an annotation on function declarations (and\n+other similar declarations). This document explains the meaning of these\n+annotations.\n+\n+## `@differentiable` function type attribute\n+\n+### Basics\n+\n+In Swift, function types can have attributes. When a function type is annotated\n+with `@differentiable`, Swift guarantees that all values of that function type\n+can be differentiated.\n+\n+`@differentiable` functions can be called like normal functions, or be passed to\n+APIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\n+representation of a `@differentiable` function is a special data structure\n+containing the original function along with extra information required for\n+computing its derivatives. Usage `@differentiable` functions are a part of\n+Swift's type system. Most notably, they are used by differentiation APIs in the\n+standard library. Here are some examples demonstrating differentiation APIs:\n+\n+```swift\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// Free function examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(gradient(at: x, in: square)) // 6.0\n+// Computes the gradient of `square`, then applies it to `x`.\n+print(gradient(of: square)(x)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n+\n+// Method examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(x.gradient(in: square)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n+```\n+\n+Here's a list of differentiation APIs provided by the standard library:\n+\n+| Differentiation APIs  | Description                                                                                                                                                      |\n+|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n+| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n+| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n+| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n+| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n+| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n+\n+### Constructing `@differentiable` functions\n+\n+A value with a non-differentiable function type can be implicitly converted to\n+one with a corresponding `@differentiable` function type. In fact, this is what\n+happened in the example above:\n+\n+```swift\n+// `square` has type `(Float) -> Float`.\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n+// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n+// `@differentiable` type.\n+print(gradient(at: x, in: square)) // 6.0\n+```\n+\n+The implicit conversion from a value with type `(T) -> U` to a value with type\n+`@differentiable (T) -> U` actually triggers differentiation by the compiler.\n+Thus, differentiation is type-driven.\n+\n+If differentiation succeeds, the `@differentiable (T) -> U` value is\n+constructed. If differentiation fails, the compiler emits a compile-time error:\n+\n+```swift\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+    // The `Int` initializer call below is non-differentiable.\n+    Float(Int(x + y))\n+}\n+```\n+\n+```console\n+test.swift:1:52: error: function is not differentiable\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+                                                   ^~~~~~~~~\n+test.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n+    Float(Int(x + y))\n+          ^\n+```\n+\n+There are a few reasons why differentiation can fail:\n+* The function to differentiate contains non-differentiable computation along",
        "comment_created_at": "2019-03-15T23:29:32+00:00",
        "comment_author": "rxwei",
        "comment_body": "Earlier Conal Elliott on Twitter made a good point about the distinction between \u201cnon-differentiable functions\u201d (mathematically non differentiable functions) and \u201cfunctions whose derivatives are not computable\u201d (opaque functions). Since our language doesn\u2019t know math, we don\u2019t really know what functions are really nondifferentiable in the mathematical sense. So we\u2019ve been using \u201ccannot differentiate xxx\u201d in our diagnostics instead of \u201cxxx is nondifferentiable\u201d. We should probably use the same wording here instead of \u201cnondifferentiable\u201d. ",
        "pr_file_module": null
      },
      {
        "comment_id": "266177850",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 145,
        "pr_file": "docs/DifferentiableFunctions.md",
        "discussion_id": "266174431",
        "commented_code": "@@ -0,0 +1,444 @@\n+# Differentiable functions and differentiation APIs\n+\n+[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n+\n+Last updated: March 2019\n+\n+## Introduction\n+\n+Swift supports differentiable functions as part of the language. The\n+`@differentiable` attribute appears in two locations in Swift syntax: as an\n+annotation on function types and as an annotation on function declarations (and\n+other similar declarations). This document explains the meaning of these\n+annotations.\n+\n+## `@differentiable` function type attribute\n+\n+### Basics\n+\n+In Swift, function types can have attributes. When a function type is annotated\n+with `@differentiable`, Swift guarantees that all values of that function type\n+can be differentiated.\n+\n+`@differentiable` functions can be called like normal functions, or be passed to\n+APIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\n+representation of a `@differentiable` function is a special data structure\n+containing the original function along with extra information required for\n+computing its derivatives. Usage `@differentiable` functions are a part of\n+Swift's type system. Most notably, they are used by differentiation APIs in the\n+standard library. Here are some examples demonstrating differentiation APIs:\n+\n+```swift\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// Free function examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(gradient(at: x, in: square)) // 6.0\n+// Computes the gradient of `square`, then applies it to `x`.\n+print(gradient(of: square)(x)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n+\n+// Method examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(x.gradient(in: square)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n+```\n+\n+Here's a list of differentiation APIs provided by the standard library:\n+\n+| Differentiation APIs  | Description                                                                                                                                                      |\n+|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n+| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n+| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n+| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n+| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n+| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n+\n+### Constructing `@differentiable` functions\n+\n+A value with a non-differentiable function type can be implicitly converted to\n+one with a corresponding `@differentiable` function type. In fact, this is what\n+happened in the example above:\n+\n+```swift\n+// `square` has type `(Float) -> Float`.\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n+// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n+// `@differentiable` type.\n+print(gradient(at: x, in: square)) // 6.0\n+```\n+\n+The implicit conversion from a value with type `(T) -> U` to a value with type\n+`@differentiable (T) -> U` actually triggers differentiation by the compiler.\n+Thus, differentiation is type-driven.\n+\n+If differentiation succeeds, the `@differentiable (T) -> U` value is\n+constructed. If differentiation fails, the compiler emits a compile-time error:\n+\n+```swift\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+    // The `Int` initializer call below is non-differentiable.\n+    Float(Int(x + y))\n+}\n+```\n+\n+```console\n+test.swift:1:52: error: function is not differentiable\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+                                                   ^~~~~~~~~\n+test.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n+    Float(Int(x + y))\n+          ^\n+```\n+\n+There are a few reasons why differentiation can fail:\n+* The function to differentiate contains non-differentiable computation along",
        "comment_created_at": "2019-03-15T23:57:53+00:00",
        "comment_author": "dan-zheng",
        "comment_body": "How about:\r\n`* The function to differentiate contains dataflow, from parameters to result, that cannot be differentiated.`",
        "pr_file_module": null
      },
      {
        "comment_id": "266178836",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 145,
        "pr_file": "docs/DifferentiableFunctions.md",
        "discussion_id": "266174431",
        "commented_code": "@@ -0,0 +1,444 @@\n+# Differentiable functions and differentiation APIs\n+\n+[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n+\n+Last updated: March 2019\n+\n+## Introduction\n+\n+Swift supports differentiable functions as part of the language. The\n+`@differentiable` attribute appears in two locations in Swift syntax: as an\n+annotation on function types and as an annotation on function declarations (and\n+other similar declarations). This document explains the meaning of these\n+annotations.\n+\n+## `@differentiable` function type attribute\n+\n+### Basics\n+\n+In Swift, function types can have attributes. When a function type is annotated\n+with `@differentiable`, Swift guarantees that all values of that function type\n+can be differentiated.\n+\n+`@differentiable` functions can be called like normal functions, or be passed to\n+APIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\n+representation of a `@differentiable` function is a special data structure\n+containing the original function along with extra information required for\n+computing its derivatives. Usage `@differentiable` functions are a part of\n+Swift's type system. Most notably, they are used by differentiation APIs in the\n+standard library. Here are some examples demonstrating differentiation APIs:\n+\n+```swift\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// Free function examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(gradient(at: x, in: square)) // 6.0\n+// Computes the gradient of `square`, then applies it to `x`.\n+print(gradient(of: square)(x)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n+\n+// Method examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(x.gradient(in: square)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n+```\n+\n+Here's a list of differentiation APIs provided by the standard library:\n+\n+| Differentiation APIs  | Description                                                                                                                                                      |\n+|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n+| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n+| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n+| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n+| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n+| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n+\n+### Constructing `@differentiable` functions\n+\n+A value with a non-differentiable function type can be implicitly converted to\n+one with a corresponding `@differentiable` function type. In fact, this is what\n+happened in the example above:\n+\n+```swift\n+// `square` has type `(Float) -> Float`.\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n+// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n+// `@differentiable` type.\n+print(gradient(at: x, in: square)) // 6.0\n+```\n+\n+The implicit conversion from a value with type `(T) -> U` to a value with type\n+`@differentiable (T) -> U` actually triggers differentiation by the compiler.\n+Thus, differentiation is type-driven.\n+\n+If differentiation succeeds, the `@differentiable (T) -> U` value is\n+constructed. If differentiation fails, the compiler emits a compile-time error:\n+\n+```swift\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+    // The `Int` initializer call below is non-differentiable.\n+    Float(Int(x + y))\n+}\n+```\n+\n+```console\n+test.swift:1:52: error: function is not differentiable\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+                                                   ^~~~~~~~~\n+test.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n+    Float(Int(x + y))\n+          ^\n+```\n+\n+There are a few reasons why differentiation can fail:\n+* The function to differentiate contains non-differentiable computation along",
        "comment_created_at": "2019-03-16T00:07:03+00:00",
        "comment_author": "rxwei",
        "comment_body": "\u201cData flow\u201d or \u201cdataflow\u201d is a compiler term IMO. The user understands \u201ccode\u201d or \u201ccomputation\u201d. ",
        "pr_file_module": null
      },
      {
        "comment_id": "266179578",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 145,
        "pr_file": "docs/DifferentiableFunctions.md",
        "discussion_id": "266174431",
        "commented_code": "@@ -0,0 +1,444 @@\n+# Differentiable functions and differentiation APIs\n+\n+[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n+\n+Last updated: March 2019\n+\n+## Introduction\n+\n+Swift supports differentiable functions as part of the language. The\n+`@differentiable` attribute appears in two locations in Swift syntax: as an\n+annotation on function types and as an annotation on function declarations (and\n+other similar declarations). This document explains the meaning of these\n+annotations.\n+\n+## `@differentiable` function type attribute\n+\n+### Basics\n+\n+In Swift, function types can have attributes. When a function type is annotated\n+with `@differentiable`, Swift guarantees that all values of that function type\n+can be differentiated.\n+\n+`@differentiable` functions can be called like normal functions, or be passed to\n+APIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\n+representation of a `@differentiable` function is a special data structure\n+containing the original function along with extra information required for\n+computing its derivatives. Usage `@differentiable` functions are a part of\n+Swift's type system. Most notably, they are used by differentiation APIs in the\n+standard library. Here are some examples demonstrating differentiation APIs:\n+\n+```swift\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// Free function examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(gradient(at: x, in: square)) // 6.0\n+// Computes the gradient of `square`, then applies it to `x`.\n+print(gradient(of: square)(x)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n+\n+// Method examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(x.gradient(in: square)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n+```\n+\n+Here's a list of differentiation APIs provided by the standard library:\n+\n+| Differentiation APIs  | Description                                                                                                                                                      |\n+|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n+| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n+| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n+| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n+| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n+| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n+\n+### Constructing `@differentiable` functions\n+\n+A value with a non-differentiable function type can be implicitly converted to\n+one with a corresponding `@differentiable` function type. In fact, this is what\n+happened in the example above:\n+\n+```swift\n+// `square` has type `(Float) -> Float`.\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n+// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n+// `@differentiable` type.\n+print(gradient(at: x, in: square)) // 6.0\n+```\n+\n+The implicit conversion from a value with type `(T) -> U` to a value with type\n+`@differentiable (T) -> U` actually triggers differentiation by the compiler.\n+Thus, differentiation is type-driven.\n+\n+If differentiation succeeds, the `@differentiable (T) -> U` value is\n+constructed. If differentiation fails, the compiler emits a compile-time error:\n+\n+```swift\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+    // The `Int` initializer call below is non-differentiable.\n+    Float(Int(x + y))\n+}\n+```\n+\n+```console\n+test.swift:1:52: error: function is not differentiable\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+                                                   ^~~~~~~~~\n+test.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n+    Float(Int(x + y))\n+          ^\n+```\n+\n+There are a few reasons why differentiation can fail:\n+* The function to differentiate contains non-differentiable computation along",
        "comment_created_at": "2019-03-16T00:16:17+00:00",
        "comment_author": "dan-zheng",
        "comment_body": "How about:\r\n```\r\n* The function to differentiate contains computation, from parameters to result, that cannot be differentiated.\r\n```\r\n\r\nAlso, how about adding another point for opaque functions?\r\n```\r\n* The function to differentiate is opaque, i.e. it is a function parameter with a non-`@differentiable` function type.\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "266180214",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 145,
        "pr_file": "docs/DifferentiableFunctions.md",
        "discussion_id": "266174431",
        "commented_code": "@@ -0,0 +1,444 @@\n+# Differentiable functions and differentiation APIs\n+\n+[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n+\n+Last updated: March 2019\n+\n+## Introduction\n+\n+Swift supports differentiable functions as part of the language. The\n+`@differentiable` attribute appears in two locations in Swift syntax: as an\n+annotation on function types and as an annotation on function declarations (and\n+other similar declarations). This document explains the meaning of these\n+annotations.\n+\n+## `@differentiable` function type attribute\n+\n+### Basics\n+\n+In Swift, function types can have attributes. When a function type is annotated\n+with `@differentiable`, Swift guarantees that all values of that function type\n+can be differentiated.\n+\n+`@differentiable` functions can be called like normal functions, or be passed to\n+APIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\n+representation of a `@differentiable` function is a special data structure\n+containing the original function along with extra information required for\n+computing its derivatives. Usage `@differentiable` functions are a part of\n+Swift's type system. Most notably, they are used by differentiation APIs in the\n+standard library. Here are some examples demonstrating differentiation APIs:\n+\n+```swift\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// Free function examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(gradient(at: x, in: square)) // 6.0\n+// Computes the gradient of `square`, then applies it to `x`.\n+print(gradient(of: square)(x)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n+\n+// Method examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(x.gradient(in: square)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n+```\n+\n+Here's a list of differentiation APIs provided by the standard library:\n+\n+| Differentiation APIs  | Description                                                                                                                                                      |\n+|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n+| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n+| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n+| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n+| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n+| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n+\n+### Constructing `@differentiable` functions\n+\n+A value with a non-differentiable function type can be implicitly converted to\n+one with a corresponding `@differentiable` function type. In fact, this is what\n+happened in the example above:\n+\n+```swift\n+// `square` has type `(Float) -> Float`.\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n+// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n+// `@differentiable` type.\n+print(gradient(at: x, in: square)) // 6.0\n+```\n+\n+The implicit conversion from a value with type `(T) -> U` to a value with type\n+`@differentiable (T) -> U` actually triggers differentiation by the compiler.\n+Thus, differentiation is type-driven.\n+\n+If differentiation succeeds, the `@differentiable (T) -> U` value is\n+constructed. If differentiation fails, the compiler emits a compile-time error:\n+\n+```swift\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+    // The `Int` initializer call below is non-differentiable.\n+    Float(Int(x + y))\n+}\n+```\n+\n+```console\n+test.swift:1:52: error: function is not differentiable\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+                                                   ^~~~~~~~~\n+test.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n+    Float(Int(x + y))\n+          ^\n+```\n+\n+There are a few reasons why differentiation can fail:\n+* The function to differentiate contains non-differentiable computation along",
        "comment_created_at": "2019-03-16T00:23:51+00:00",
        "comment_author": "rxwei",
        "comment_body": "I think both these are good. Any inability to differentiate something always comes from encountering an opaque, non-@differentiable function.",
        "pr_file_module": null
      },
      {
        "comment_id": "266181273",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 145,
        "pr_file": "docs/DifferentiableFunctions.md",
        "discussion_id": "266174431",
        "commented_code": "@@ -0,0 +1,444 @@\n+# Differentiable functions and differentiation APIs\n+\n+[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n+\n+Last updated: March 2019\n+\n+## Introduction\n+\n+Swift supports differentiable functions as part of the language. The\n+`@differentiable` attribute appears in two locations in Swift syntax: as an\n+annotation on function types and as an annotation on function declarations (and\n+other similar declarations). This document explains the meaning of these\n+annotations.\n+\n+## `@differentiable` function type attribute\n+\n+### Basics\n+\n+In Swift, function types can have attributes. When a function type is annotated\n+with `@differentiable`, Swift guarantees that all values of that function type\n+can be differentiated.\n+\n+`@differentiable` functions can be called like normal functions, or be passed to\n+APIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\n+representation of a `@differentiable` function is a special data structure\n+containing the original function along with extra information required for\n+computing its derivatives. Usage `@differentiable` functions are a part of\n+Swift's type system. Most notably, they are used by differentiation APIs in the\n+standard library. Here are some examples demonstrating differentiation APIs:\n+\n+```swift\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// Free function examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(gradient(at: x, in: square)) // 6.0\n+// Computes the gradient of `square`, then applies it to `x`.\n+print(gradient(of: square)(x)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n+\n+// Method examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(x.gradient(in: square)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n+```\n+\n+Here's a list of differentiation APIs provided by the standard library:\n+\n+| Differentiation APIs  | Description                                                                                                                                                      |\n+|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n+| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n+| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n+| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n+| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n+| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n+\n+### Constructing `@differentiable` functions\n+\n+A value with a non-differentiable function type can be implicitly converted to\n+one with a corresponding `@differentiable` function type. In fact, this is what\n+happened in the example above:\n+\n+```swift\n+// `square` has type `(Float) -> Float`.\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n+// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n+// `@differentiable` type.\n+print(gradient(at: x, in: square)) // 6.0\n+```\n+\n+The implicit conversion from a value with type `(T) -> U` to a value with type\n+`@differentiable (T) -> U` actually triggers differentiation by the compiler.\n+Thus, differentiation is type-driven.\n+\n+If differentiation succeeds, the `@differentiable (T) -> U` value is\n+constructed. If differentiation fails, the compiler emits a compile-time error:\n+\n+```swift\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+    // The `Int` initializer call below is non-differentiable.\n+    Float(Int(x + y))\n+}\n+```\n+\n+```console\n+test.swift:1:52: error: function is not differentiable\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+                                                   ^~~~~~~~~\n+test.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n+    Float(Int(x + y))\n+          ^\n+```\n+\n+There are a few reasons why differentiation can fail:\n+* The function to differentiate contains non-differentiable computation along",
        "comment_created_at": "2019-03-16T00:38:29+00:00",
        "comment_author": "dan-zheng",
        "comment_body": "Added both points.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "266180746",
    "pr_number": 145,
    "pr_file": "docs/DifferentiableFunctions.md",
    "created_at": "2019-03-16T00:31:20+00:00",
    "commented_code": "# Differentiable functions and differentiation APIs\n\n[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n\nLast updated: March 2019\n\n## Introduction\n\nSwift supports differentiable functions as part of the language. The\n`@differentiable` attribute appears in two locations in Swift syntax: as an\nannotation on function types and as an annotation on function declarations (and\nother similar declarations). This document explains the meaning of these\nannotations.\n\n## `@differentiable` function type attribute\n\n### Basics\n\nIn Swift, function types can have attributes. When a function type is annotated\nwith `@differentiable`, Swift guarantees that all values of that function type\ncan be differentiated.\n\n`@differentiable` functions can be called like normal functions, or be passed to\nAPIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\nrepresentation of a `@differentiable` function is a special data structure\ncontaining the original function along with extra information required for\ncomputing its derivatives. Usage `@differentiable` functions are a part of\nSwift's type system. Most notably, they are used by differentiation APIs in the\nstandard library. Here are some examples demonstrating differentiation APIs:\n\n```swift\nfunc square(_ x: Float) -> Float {\n    return x * x\n}\nlet x: Float = 3.0\n\n// Free function examples.\n\n// Computes the gradient of `square` at `x`.\nprint(gradient(at: x, in: square)) // 6.0\n// Computes the gradient of `square`, then applies it to `x`.\nprint(gradient(of: square)(x)) // 6.0\n// Computes the value and gradient of `square` at `x`.\nprint(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n\n// Method examples.\n\n// Computes the gradient of `square` at `x`.\nprint(x.gradient(in: square)) // 6.0\n// Computes the value and gradient of `square` at `x`.\nprint(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n```\n\nHere's a list of differentiation APIs provided by the standard library:\n\n| Differentiation APIs  | Description                                                                                                                                                      |\n|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n\n### Constructing `@differentiable` functions\n\nA value with a non-differentiable function type can be implicitly converted to\none with a corresponding `@differentiable` function type. In fact, this is what\nhappened in the example above:\n\n```swift\n// `square` has type `(Float) -> Float`.\nfunc square(_ x: Float) -> Float {\n    return x * x\n}\nlet x: Float = 3.0\n\n// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n// `@differentiable` type.\nprint(gradient(at: x, in: square)) // 6.0\n```\n\nThe implicit conversion from a value with type `(T) -> U` to a value with type\n`@differentiable (T) -> U` actually triggers differentiation by the compiler.\nThus, differentiation is type-driven.\n\nIf differentiation succeeds, the `@differentiable (T) -> U` value is\nconstructed. If differentiation fails, the compiler emits a compile-time error:\n\n```swift\nlet add: @differentiable (Float, Float) -> Float = { x, y in\n    // The `Int` initializer call below is non-differentiable.\n    Float(Int(x + y))\n}\n```\n\n```console\ntest.swift:1:52: error: function is not differentiable\nlet add: @differentiable (Float, Float) -> Float = { x, y in\n                                                   ^~~~~~~~~\ntest.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n    Float(Int(x + y))\n          ^\n```\n\nThere are a few reasons why differentiation can fail:\n* The function to differentiate contains non-differentiable computation along\n  the dataflow from parameters to function result.\n* The function to differentiate is defined in another module.\n* The function to differentiate uses [control\n  flow](https://docs.swift.org/swift-book/LanguageGuide/ControlFlow.html)\n  (if-statements, switch-statements, loops, etc). This restriction will be\n  lifted soon.\n  \n## `@differentiable` declaration attribute\n\n### Basics\n\nThe `@differentiable` attribute can also be applied to function declarations.\n`@differentiable` marks a function as being differentiable with respect to some\nparameters (the varying parameters, explained below). `@differentiable` requires\nthe types of the varying parameters and the function result type to all conform\nto the `Differentiable` protocol.\n\nThis annotation does not change the declaration to have a `@differentiable`\nfunction type; instead, it triggers differentiation by the compiler on the\nfunction. If differentiation succeeds, then conversion of the function to a\n`@differentiable` function is guaranteed to succeed later.\n\nYou may wonder about the purpose of the `@differentiable` declaration attribute,\ngiven that non-differentiable functions can implicitly be converted to\n`@differentiable` functions, as mentioned above. The main reason is that the\n`@differentiable` declaration attribute is a contract for differentiability: if\na function is declared with `@differentiable` and it compiles, then it is always\nguaranteed to be differentiable, even in other modules. On the other hand, if a\nfunction is not declared with `@differentiable`, then differentiation of the\nfunction in other modules will fail.\n\nThis is why floating-point operations in the standard library are declared with\n`@differentiable`:\n\n```swift\nextension Float {\n    @differentiable\n    public static func + (lhs: Float, rhs: Float) -> Float { ... }\n}\n```\n\nBesides function declarations, there are a few other function-like declarations\nthat can be marked with `@differentiable`:\n- Computed property getters. (This requires both the type defining the property\n  and the type of the property to conform to `Differentiable`.)\n- Initializers. (This requires the type defining the initializer to conform to\n  `Differentiable`.)\n\nFor instance methods defined on types that conform to `Differentiable`, the\n`self` property can be marked as a varying parameter. Derivatives of these\nmethods return the partial derivative with respect to `self`. For these methods,\n`@differentiable` infers `self` as a varying parameter by default.\n\n```swift\nstruct Vector: Differentiable, VectorNumeric {\n    var x, y: Float\n\n    // Differentiable computed property.\n    @differentiable // Implicitly: @differentiable(wrt: self)\n    var magnitude: Float {\n        return (x * x + y * y).squareRoot()\n    }\n\n    // Differentiable initializer.\n    @differentiable // Implicitly: @differentiable(wrt: (x, y))\n    init(x: Float, y: Float) {\n        self.x = x\n        self.y = y\n    }\n}\n\nlet v = Vector(x: 2, y: 2)\nprint(v.magnitude)\n// 2.828427\nprint(gradient(at: v) { v in v.magnitude })\n// Vector(x: 64.0, y: 64.0)\n```\n\n### Differentiating with respect to\n\nMathematically, let \"varying parameters\" refer to the parameters (i.e.\nindependent variables) of a differentiable function whose partial derivatives\nare computed by the function's derivative.\n\nBy default, the `@differentiable` attribute infers all function parameters that\nconform to `Differentiable` to be the varying parameters. However, this is not\nalways desirable. To explicitly declare functions as differentiable with respect\nto a subset of parameters, explicitly specify the varying parameters using the\n`@differentiable(wrt: ...)` syntax.\n\nHere's an example of a 2-D convolution operation, adapted from the TensorFlow\nlibrary. The convolution input and filter are the varying parameters; strides\nand padding are not.\n\n```swift\n@differentiable(wrt: (input, filter))\nfunc conv2d(input: Tensor<Float>, filter: Tensor<Float>, strides: (Int, Int), padding: Padding) {\n    ...\n}\n```\n\nFunctions can have multiple `@differentiable` attributes with differentiable\n`wrt` parameter lists. `@differentiable` protocol requirements If a protocol\nrequirement is marked with `@differentiable`, all implementations of the\nrequirement are required to specify the same attribute. This enables generic\ncode using differentiation defined in terms of protocol requirements.\n\nHere is an example of a neural network `Layer` protocol that defines a\n`@differentiable` required method called `applied(to:)`. As shown, the\n`applied(to:)` method can be differentiated in a `Layer` protocol extension,\neven though it is not a concrete method.\n\n```swift\nimport TensorFlow\n\n/// A neural network layer.\nprotocol Layer: Differentiable {\n    /// The input type of the layer.\n    associatedtype Input: Differentiable\n    /// The output type of the layer.\n    associatedtype Output: Differentiable\n    /// Returns the output obtained from applying the layer to the given input.\n    @differentiable\n    func applied(to input: Input) -> Output\n}\n\nextension Layer {\n    /// Returns the inference output and the backpropagation function obtained from applying the\n    /// layer to the given input.\n    ///\n    /// - Parameter input: The input to the layer.\n    /// - Returns: A tuple containing the output and the backpropagation function. The\n    ///   backpropagation function (a.k.a. backpropagator) takes a direction vector and returns the\n    ///   gradients at the layer and at the input, respectively.\n    func appliedForBackpropagation(to input: Input)\n        -> (output: Output,\n            backpropagator: (_ direction: Output.CotangentVector)\n                -> (layerGradient: CotangentVector, inputGradient: Input.CotangentVector)) {\n        let (out, pullback) = valueWithPullback(at: input) { layer, input in\n            return layer.applied(to: input)\n        }\n        return (out, pullback)\n    }\n}\n\n// Example neural network layer.\nstruct DenseLayer: Layer {\n    var weight: Tensor<Float>\n    var bias: Tensor<Float>\n\n    @differentiable\n    func applied(to input: Tensor<Float>) -> Tensor<Float> {\n        return matmul(input, weight) + bias\n    }\n}\n\n// Example usage of `appliedForBackpropagation(to:)`.\nlet dense = DenseLayer(weight: [[1, 1], [1, 1]], bias: [1, 1])\nlet input: Tensor<Float> = [[3, 3]]\nlet seed: Tensor<Float> = [[1, 1]]\n\nlet (output, backprop) = dense.appliedForBackpropagation(to: input)\nlet (\ud835\udec1dense, \ud835\udec1input) = backprop(seed)\n\ndump(\ud835\udec1dense)\n// \u25bf DenseLayer.AllDifferentiableVariables\n//   - weight: [[3.0, 3.0], [3.0, 3.0]]\n//   - bias: [1.0, 1.0]\nprint(\ud835\udec1input)\n// [[2.0, 2.0]]\n```\n\n## Providing a custom derivative\n\nUse the `@differentiating` attribute to mark a function as a custom derivative\nfor another function. This is useful for registering derivatives for primitive\noperations.\n\nNote: currently, the `@differentiating` attribute can only be used to define\nderivatives for functions in the same module. We plan to lift this limitation\nsoon so that derivatives can be retroactively declared for functions in other\nmodules - [see this forum\ndiscussion](https://forums.swift.org/t/help-needed-with-retroactive-differentiability/19927)\nfor more information.\n\n```swift\nimport Darwin\n\nfunc sillyExp(_ x: Float) -> Float {\n    let \ud835\udc52 = Float(M_E)\n    print(\"Taking \ud835\udc52(\\(\ud835\udc52)) to the power of \\(x)!\")\n    return pow(\ud835\udc52, x)\n}\n\n@differentiating(sillyExp)\nfunc sillyDerivative(_ x: Float) -> (value: Float, pullback: (Float) -> Float) {\n    let y = sillyExp(x)\n    return (value: y, pullback: { _ in y })",
    "repo_full_name": "tensorflow/swift",
    "discussion_comments": [
      {
        "comment_id": "266180746",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 145,
        "pr_file": "docs/DifferentiableFunctions.md",
        "discussion_id": "266180746",
        "commented_code": "@@ -0,0 +1,447 @@\n+# Differentiable functions and differentiation APIs\n+\n+[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n+\n+Last updated: March 2019\n+\n+## Introduction\n+\n+Swift supports differentiable functions as part of the language. The\n+`@differentiable` attribute appears in two locations in Swift syntax: as an\n+annotation on function types and as an annotation on function declarations (and\n+other similar declarations). This document explains the meaning of these\n+annotations.\n+\n+## `@differentiable` function type attribute\n+\n+### Basics\n+\n+In Swift, function types can have attributes. When a function type is annotated\n+with `@differentiable`, Swift guarantees that all values of that function type\n+can be differentiated.\n+\n+`@differentiable` functions can be called like normal functions, or be passed to\n+APIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\n+representation of a `@differentiable` function is a special data structure\n+containing the original function along with extra information required for\n+computing its derivatives. Usage `@differentiable` functions are a part of\n+Swift's type system. Most notably, they are used by differentiation APIs in the\n+standard library. Here are some examples demonstrating differentiation APIs:\n+\n+```swift\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// Free function examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(gradient(at: x, in: square)) // 6.0\n+// Computes the gradient of `square`, then applies it to `x`.\n+print(gradient(of: square)(x)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n+\n+// Method examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(x.gradient(in: square)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n+```\n+\n+Here's a list of differentiation APIs provided by the standard library:\n+\n+| Differentiation APIs  | Description                                                                                                                                                      |\n+|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n+| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n+| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n+| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n+| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n+| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n+\n+### Constructing `@differentiable` functions\n+\n+A value with a non-differentiable function type can be implicitly converted to\n+one with a corresponding `@differentiable` function type. In fact, this is what\n+happened in the example above:\n+\n+```swift\n+// `square` has type `(Float) -> Float`.\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n+// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n+// `@differentiable` type.\n+print(gradient(at: x, in: square)) // 6.0\n+```\n+\n+The implicit conversion from a value with type `(T) -> U` to a value with type\n+`@differentiable (T) -> U` actually triggers differentiation by the compiler.\n+Thus, differentiation is type-driven.\n+\n+If differentiation succeeds, the `@differentiable (T) -> U` value is\n+constructed. If differentiation fails, the compiler emits a compile-time error:\n+\n+```swift\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+    // The `Int` initializer call below is non-differentiable.\n+    Float(Int(x + y))\n+}\n+```\n+\n+```console\n+test.swift:1:52: error: function is not differentiable\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+                                                   ^~~~~~~~~\n+test.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n+    Float(Int(x + y))\n+          ^\n+```\n+\n+There are a few reasons why differentiation can fail:\n+* The function to differentiate contains non-differentiable computation along\n+  the dataflow from parameters to function result.\n+* The function to differentiate is defined in another module.\n+* The function to differentiate uses [control\n+  flow](https://docs.swift.org/swift-book/LanguageGuide/ControlFlow.html)\n+  (if-statements, switch-statements, loops, etc). This restriction will be\n+  lifted soon.\n+  \n+## `@differentiable` declaration attribute\n+\n+### Basics\n+\n+The `@differentiable` attribute can also be applied to function declarations.\n+`@differentiable` marks a function as being differentiable with respect to some\n+parameters (the varying parameters, explained below). `@differentiable` requires\n+the types of the varying parameters and the function result type to all conform\n+to the `Differentiable` protocol.\n+\n+This annotation does not change the declaration to have a `@differentiable`\n+function type; instead, it triggers differentiation by the compiler on the\n+function. If differentiation succeeds, then conversion of the function to a\n+`@differentiable` function is guaranteed to succeed later.\n+\n+You may wonder about the purpose of the `@differentiable` declaration attribute,\n+given that non-differentiable functions can implicitly be converted to\n+`@differentiable` functions, as mentioned above. The main reason is that the\n+`@differentiable` declaration attribute is a contract for differentiability: if\n+a function is declared with `@differentiable` and it compiles, then it is always\n+guaranteed to be differentiable, even in other modules. On the other hand, if a\n+function is not declared with `@differentiable`, then differentiation of the\n+function in other modules will fail.\n+\n+This is why floating-point operations in the standard library are declared with\n+`@differentiable`:\n+\n+```swift\n+extension Float {\n+    @differentiable\n+    public static func + (lhs: Float, rhs: Float) -> Float { ... }\n+}\n+```\n+\n+Besides function declarations, there are a few other function-like declarations\n+that can be marked with `@differentiable`:\n+- Computed property getters. (This requires both the type defining the property\n+  and the type of the property to conform to `Differentiable`.)\n+- Initializers. (This requires the type defining the initializer to conform to\n+  `Differentiable`.)\n+\n+For instance methods defined on types that conform to `Differentiable`, the\n+`self` property can be marked as a varying parameter. Derivatives of these\n+methods return the partial derivative with respect to `self`. For these methods,\n+`@differentiable` infers `self` as a varying parameter by default.\n+\n+```swift\n+struct Vector: Differentiable, VectorNumeric {\n+    var x, y: Float\n+\n+    // Differentiable computed property.\n+    @differentiable // Implicitly: @differentiable(wrt: self)\n+    var magnitude: Float {\n+        return (x * x + y * y).squareRoot()\n+    }\n+\n+    // Differentiable initializer.\n+    @differentiable // Implicitly: @differentiable(wrt: (x, y))\n+    init(x: Float, y: Float) {\n+        self.x = x\n+        self.y = y\n+    }\n+}\n+\n+let v = Vector(x: 2, y: 2)\n+print(v.magnitude)\n+// 2.828427\n+print(gradient(at: v) { v in v.magnitude })\n+// Vector(x: 64.0, y: 64.0)\n+```\n+\n+### Differentiating with respect to\n+\n+Mathematically, let \"varying parameters\" refer to the parameters (i.e.\n+independent variables) of a differentiable function whose partial derivatives\n+are computed by the function's derivative.\n+\n+By default, the `@differentiable` attribute infers all function parameters that\n+conform to `Differentiable` to be the varying parameters. However, this is not\n+always desirable. To explicitly declare functions as differentiable with respect\n+to a subset of parameters, explicitly specify the varying parameters using the\n+`@differentiable(wrt: ...)` syntax.\n+\n+Here's an example of a 2-D convolution operation, adapted from the TensorFlow\n+library. The convolution input and filter are the varying parameters; strides\n+and padding are not.\n+\n+```swift\n+@differentiable(wrt: (input, filter))\n+func conv2d(input: Tensor<Float>, filter: Tensor<Float>, strides: (Int, Int), padding: Padding) {\n+    ...\n+}\n+```\n+\n+Functions can have multiple `@differentiable` attributes with differentiable\n+`wrt` parameter lists. `@differentiable` protocol requirements If a protocol\n+requirement is marked with `@differentiable`, all implementations of the\n+requirement are required to specify the same attribute. This enables generic\n+code using differentiation defined in terms of protocol requirements.\n+\n+Here is an example of a neural network `Layer` protocol that defines a\n+`@differentiable` required method called `applied(to:)`. As shown, the\n+`applied(to:)` method can be differentiated in a `Layer` protocol extension,\n+even though it is not a concrete method.\n+\n+```swift\n+import TensorFlow\n+\n+/// A neural network layer.\n+protocol Layer: Differentiable {\n+    /// The input type of the layer.\n+    associatedtype Input: Differentiable\n+    /// The output type of the layer.\n+    associatedtype Output: Differentiable\n+    /// Returns the output obtained from applying the layer to the given input.\n+    @differentiable\n+    func applied(to input: Input) -> Output\n+}\n+\n+extension Layer {\n+    /// Returns the inference output and the backpropagation function obtained from applying the\n+    /// layer to the given input.\n+    ///\n+    /// - Parameter input: The input to the layer.\n+    /// - Returns: A tuple containing the output and the backpropagation function. The\n+    ///   backpropagation function (a.k.a. backpropagator) takes a direction vector and returns the\n+    ///   gradients at the layer and at the input, respectively.\n+    func appliedForBackpropagation(to input: Input)\n+        -> (output: Output,\n+            backpropagator: (_ direction: Output.CotangentVector)\n+                -> (layerGradient: CotangentVector, inputGradient: Input.CotangentVector)) {\n+        let (out, pullback) = valueWithPullback(at: input) { layer, input in\n+            return layer.applied(to: input)\n+        }\n+        return (out, pullback)\n+    }\n+}\n+\n+// Example neural network layer.\n+struct DenseLayer: Layer {\n+    var weight: Tensor<Float>\n+    var bias: Tensor<Float>\n+\n+    @differentiable\n+    func applied(to input: Tensor<Float>) -> Tensor<Float> {\n+        return matmul(input, weight) + bias\n+    }\n+}\n+\n+// Example usage of `appliedForBackpropagation(to:)`.\n+let dense = DenseLayer(weight: [[1, 1], [1, 1]], bias: [1, 1])\n+let input: Tensor<Float> = [[3, 3]]\n+let seed: Tensor<Float> = [[1, 1]]\n+\n+let (output, backprop) = dense.appliedForBackpropagation(to: input)\n+let (\ud835\udec1dense, \ud835\udec1input) = backprop(seed)\n+\n+dump(\ud835\udec1dense)\n+// \u25bf DenseLayer.AllDifferentiableVariables\n+//   - weight: [[3.0, 3.0], [3.0, 3.0]]\n+//   - bias: [1.0, 1.0]\n+print(\ud835\udec1input)\n+// [[2.0, 2.0]]\n+```\n+\n+## Providing a custom derivative\n+\n+Use the `@differentiating` attribute to mark a function as a custom derivative\n+for another function. This is useful for registering derivatives for primitive\n+operations.\n+\n+Note: currently, the `@differentiating` attribute can only be used to define\n+derivatives for functions in the same module. We plan to lift this limitation\n+soon so that derivatives can be retroactively declared for functions in other\n+modules - [see this forum\n+discussion](https://forums.swift.org/t/help-needed-with-retroactive-differentiability/19927)\n+for more information.\n+\n+```swift\n+import Darwin\n+\n+func sillyExp(_ x: Float) -> Float {\n+    let \ud835\udc52 = Float(M_E)\n+    print(\"Taking \ud835\udc52(\\(\ud835\udc52)) to the power of \\(x)!\")\n+    return pow(\ud835\udc52, x)\n+}\n+\n+@differentiating(sillyExp)\n+func sillyDerivative(_ x: Float) -> (value: Float, pullback: (Float) -> Float) {\n+    let y = sillyExp(x)\n+    return (value: y, pullback: { _ in y })",
        "comment_created_at": "2019-03-16T00:31:20+00:00",
        "comment_author": "rxwei",
        "comment_body": "Pullback should be v * y. ",
        "pr_file_module": null
      },
      {
        "comment_id": "266181261",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 145,
        "pr_file": "docs/DifferentiableFunctions.md",
        "discussion_id": "266180746",
        "commented_code": "@@ -0,0 +1,447 @@\n+# Differentiable functions and differentiation APIs\n+\n+[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n+\n+Last updated: March 2019\n+\n+## Introduction\n+\n+Swift supports differentiable functions as part of the language. The\n+`@differentiable` attribute appears in two locations in Swift syntax: as an\n+annotation on function types and as an annotation on function declarations (and\n+other similar declarations). This document explains the meaning of these\n+annotations.\n+\n+## `@differentiable` function type attribute\n+\n+### Basics\n+\n+In Swift, function types can have attributes. When a function type is annotated\n+with `@differentiable`, Swift guarantees that all values of that function type\n+can be differentiated.\n+\n+`@differentiable` functions can be called like normal functions, or be passed to\n+APIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\n+representation of a `@differentiable` function is a special data structure\n+containing the original function along with extra information required for\n+computing its derivatives. Usage `@differentiable` functions are a part of\n+Swift's type system. Most notably, they are used by differentiation APIs in the\n+standard library. Here are some examples demonstrating differentiation APIs:\n+\n+```swift\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// Free function examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(gradient(at: x, in: square)) // 6.0\n+// Computes the gradient of `square`, then applies it to `x`.\n+print(gradient(of: square)(x)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n+\n+// Method examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(x.gradient(in: square)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n+```\n+\n+Here's a list of differentiation APIs provided by the standard library:\n+\n+| Differentiation APIs  | Description                                                                                                                                                      |\n+|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n+| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n+| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n+| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n+| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n+| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n+\n+### Constructing `@differentiable` functions\n+\n+A value with a non-differentiable function type can be implicitly converted to\n+one with a corresponding `@differentiable` function type. In fact, this is what\n+happened in the example above:\n+\n+```swift\n+// `square` has type `(Float) -> Float`.\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n+// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n+// `@differentiable` type.\n+print(gradient(at: x, in: square)) // 6.0\n+```\n+\n+The implicit conversion from a value with type `(T) -> U` to a value with type\n+`@differentiable (T) -> U` actually triggers differentiation by the compiler.\n+Thus, differentiation is type-driven.\n+\n+If differentiation succeeds, the `@differentiable (T) -> U` value is\n+constructed. If differentiation fails, the compiler emits a compile-time error:\n+\n+```swift\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+    // The `Int` initializer call below is non-differentiable.\n+    Float(Int(x + y))\n+}\n+```\n+\n+```console\n+test.swift:1:52: error: function is not differentiable\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+                                                   ^~~~~~~~~\n+test.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n+    Float(Int(x + y))\n+          ^\n+```\n+\n+There are a few reasons why differentiation can fail:\n+* The function to differentiate contains non-differentiable computation along\n+  the dataflow from parameters to function result.\n+* The function to differentiate is defined in another module.\n+* The function to differentiate uses [control\n+  flow](https://docs.swift.org/swift-book/LanguageGuide/ControlFlow.html)\n+  (if-statements, switch-statements, loops, etc). This restriction will be\n+  lifted soon.\n+  \n+## `@differentiable` declaration attribute\n+\n+### Basics\n+\n+The `@differentiable` attribute can also be applied to function declarations.\n+`@differentiable` marks a function as being differentiable with respect to some\n+parameters (the varying parameters, explained below). `@differentiable` requires\n+the types of the varying parameters and the function result type to all conform\n+to the `Differentiable` protocol.\n+\n+This annotation does not change the declaration to have a `@differentiable`\n+function type; instead, it triggers differentiation by the compiler on the\n+function. If differentiation succeeds, then conversion of the function to a\n+`@differentiable` function is guaranteed to succeed later.\n+\n+You may wonder about the purpose of the `@differentiable` declaration attribute,\n+given that non-differentiable functions can implicitly be converted to\n+`@differentiable` functions, as mentioned above. The main reason is that the\n+`@differentiable` declaration attribute is a contract for differentiability: if\n+a function is declared with `@differentiable` and it compiles, then it is always\n+guaranteed to be differentiable, even in other modules. On the other hand, if a\n+function is not declared with `@differentiable`, then differentiation of the\n+function in other modules will fail.\n+\n+This is why floating-point operations in the standard library are declared with\n+`@differentiable`:\n+\n+```swift\n+extension Float {\n+    @differentiable\n+    public static func + (lhs: Float, rhs: Float) -> Float { ... }\n+}\n+```\n+\n+Besides function declarations, there are a few other function-like declarations\n+that can be marked with `@differentiable`:\n+- Computed property getters. (This requires both the type defining the property\n+  and the type of the property to conform to `Differentiable`.)\n+- Initializers. (This requires the type defining the initializer to conform to\n+  `Differentiable`.)\n+\n+For instance methods defined on types that conform to `Differentiable`, the\n+`self` property can be marked as a varying parameter. Derivatives of these\n+methods return the partial derivative with respect to `self`. For these methods,\n+`@differentiable` infers `self` as a varying parameter by default.\n+\n+```swift\n+struct Vector: Differentiable, VectorNumeric {\n+    var x, y: Float\n+\n+    // Differentiable computed property.\n+    @differentiable // Implicitly: @differentiable(wrt: self)\n+    var magnitude: Float {\n+        return (x * x + y * y).squareRoot()\n+    }\n+\n+    // Differentiable initializer.\n+    @differentiable // Implicitly: @differentiable(wrt: (x, y))\n+    init(x: Float, y: Float) {\n+        self.x = x\n+        self.y = y\n+    }\n+}\n+\n+let v = Vector(x: 2, y: 2)\n+print(v.magnitude)\n+// 2.828427\n+print(gradient(at: v) { v in v.magnitude })\n+// Vector(x: 64.0, y: 64.0)\n+```\n+\n+### Differentiating with respect to\n+\n+Mathematically, let \"varying parameters\" refer to the parameters (i.e.\n+independent variables) of a differentiable function whose partial derivatives\n+are computed by the function's derivative.\n+\n+By default, the `@differentiable` attribute infers all function parameters that\n+conform to `Differentiable` to be the varying parameters. However, this is not\n+always desirable. To explicitly declare functions as differentiable with respect\n+to a subset of parameters, explicitly specify the varying parameters using the\n+`@differentiable(wrt: ...)` syntax.\n+\n+Here's an example of a 2-D convolution operation, adapted from the TensorFlow\n+library. The convolution input and filter are the varying parameters; strides\n+and padding are not.\n+\n+```swift\n+@differentiable(wrt: (input, filter))\n+func conv2d(input: Tensor<Float>, filter: Tensor<Float>, strides: (Int, Int), padding: Padding) {\n+    ...\n+}\n+```\n+\n+Functions can have multiple `@differentiable` attributes with differentiable\n+`wrt` parameter lists. `@differentiable` protocol requirements If a protocol\n+requirement is marked with `@differentiable`, all implementations of the\n+requirement are required to specify the same attribute. This enables generic\n+code using differentiation defined in terms of protocol requirements.\n+\n+Here is an example of a neural network `Layer` protocol that defines a\n+`@differentiable` required method called `applied(to:)`. As shown, the\n+`applied(to:)` method can be differentiated in a `Layer` protocol extension,\n+even though it is not a concrete method.\n+\n+```swift\n+import TensorFlow\n+\n+/// A neural network layer.\n+protocol Layer: Differentiable {\n+    /// The input type of the layer.\n+    associatedtype Input: Differentiable\n+    /// The output type of the layer.\n+    associatedtype Output: Differentiable\n+    /// Returns the output obtained from applying the layer to the given input.\n+    @differentiable\n+    func applied(to input: Input) -> Output\n+}\n+\n+extension Layer {\n+    /// Returns the inference output and the backpropagation function obtained from applying the\n+    /// layer to the given input.\n+    ///\n+    /// - Parameter input: The input to the layer.\n+    /// - Returns: A tuple containing the output and the backpropagation function. The\n+    ///   backpropagation function (a.k.a. backpropagator) takes a direction vector and returns the\n+    ///   gradients at the layer and at the input, respectively.\n+    func appliedForBackpropagation(to input: Input)\n+        -> (output: Output,\n+            backpropagator: (_ direction: Output.CotangentVector)\n+                -> (layerGradient: CotangentVector, inputGradient: Input.CotangentVector)) {\n+        let (out, pullback) = valueWithPullback(at: input) { layer, input in\n+            return layer.applied(to: input)\n+        }\n+        return (out, pullback)\n+    }\n+}\n+\n+// Example neural network layer.\n+struct DenseLayer: Layer {\n+    var weight: Tensor<Float>\n+    var bias: Tensor<Float>\n+\n+    @differentiable\n+    func applied(to input: Tensor<Float>) -> Tensor<Float> {\n+        return matmul(input, weight) + bias\n+    }\n+}\n+\n+// Example usage of `appliedForBackpropagation(to:)`.\n+let dense = DenseLayer(weight: [[1, 1], [1, 1]], bias: [1, 1])\n+let input: Tensor<Float> = [[3, 3]]\n+let seed: Tensor<Float> = [[1, 1]]\n+\n+let (output, backprop) = dense.appliedForBackpropagation(to: input)\n+let (\ud835\udec1dense, \ud835\udec1input) = backprop(seed)\n+\n+dump(\ud835\udec1dense)\n+// \u25bf DenseLayer.AllDifferentiableVariables\n+//   - weight: [[3.0, 3.0], [3.0, 3.0]]\n+//   - bias: [1.0, 1.0]\n+print(\ud835\udec1input)\n+// [[2.0, 2.0]]\n+```\n+\n+## Providing a custom derivative\n+\n+Use the `@differentiating` attribute to mark a function as a custom derivative\n+for another function. This is useful for registering derivatives for primitive\n+operations.\n+\n+Note: currently, the `@differentiating` attribute can only be used to define\n+derivatives for functions in the same module. We plan to lift this limitation\n+soon so that derivatives can be retroactively declared for functions in other\n+modules - [see this forum\n+discussion](https://forums.swift.org/t/help-needed-with-retroactive-differentiability/19927)\n+for more information.\n+\n+```swift\n+import Darwin\n+\n+func sillyExp(_ x: Float) -> Float {\n+    let \ud835\udc52 = Float(M_E)\n+    print(\"Taking \ud835\udc52(\\(\ud835\udc52)) to the power of \\(x)!\")\n+    return pow(\ud835\udc52, x)\n+}\n+\n+@differentiating(sillyExp)\n+func sillyDerivative(_ x: Float) -> (value: Float, pullback: (Float) -> Float) {\n+    let y = sillyExp(x)\n+    return (value: y, pullback: { _ in y })",
        "comment_created_at": "2019-03-16T00:38:20+00:00",
        "comment_author": "dan-zheng",
        "comment_body": "Fixed, thanks.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "266192104",
    "pr_number": 145,
    "pr_file": "docs/DifferentiableFunctions.md",
    "created_at": "2019-03-16T07:01:00+00:00",
    "commented_code": "# Differentiable functions and differentiation APIs\n\n[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n\nLast updated: March 2019\n\n## Introduction\n\nSwift supports differentiable functions as part of the language. The\n`@differentiable` attribute appears in two locations in Swift syntax: as an\nannotation on function types and as an annotation on function declarations (and\nother similar declarations). This document explains the meaning of these\nannotations.\n\n## `@differentiable` function type attribute\n\n### Basics\n\nIn Swift, function types can have attributes. When a function type is annotated\nwith `@differentiable`, Swift guarantees that all values of that function type\ncan be differentiated.\n\n`@differentiable` functions can be called like normal functions, or be passed to\nAPIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\nrepresentation of a `@differentiable` function is a special data structure\ncontaining the original function along with extra information required for\ncomputing its derivatives. Usage `@differentiable` functions are a part of\nSwift's type system. Most notably, they are used by differentiation APIs in the\nstandard library. Here are some examples demonstrating differentiation APIs:\n\n```swift\nfunc square(_ x: Float) -> Float {\n    return x * x\n}\nlet x: Float = 3.0\n\n// Free function examples.\n\n// Computes the gradient of `square` at `x`.\nprint(gradient(at: x, in: square)) // 6.0\n// Computes the gradient of `square`, then applies it to `x`.\nprint(gradient(of: square)(x)) // 6.0\n// Computes the value and gradient of `square` at `x`.\nprint(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n\n// Method examples.\n\n// Computes the gradient of `square` at `x`.\nprint(x.gradient(in: square)) // 6.0\n// Computes the value and gradient of `square` at `x`.\nprint(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n```\n\nHere's a list of differentiation APIs provided by the standard library:\n\n| Differentiation APIs  | Description                                                                                                                                                      |\n|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n\n### Constructing `@differentiable` functions\n\nA value with a non-differentiable function type can be implicitly converted to\none with a corresponding `@differentiable` function type. In fact, this is what\nhappened in the example above:\n\n```swift\n// `square` has type `(Float) -> Float`.\nfunc square(_ x: Float) -> Float {\n    return x * x\n}\nlet x: Float = 3.0\n\n// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n// `@differentiable` type.\nprint(gradient(at: x, in: square)) // 6.0\n```\n\nThe implicit conversion from a value with type `(T) -> U` to a value with type\n`@differentiable (T) -> U` actually triggers differentiation by the compiler.\nThus, differentiation is type-driven.\n\nIf differentiation succeeds, the `@differentiable (T) -> U` value is\nconstructed. If differentiation fails, the compiler emits a compile-time error:\n\n```swift\nlet add: @differentiable (Float, Float) -> Float = { x, y in\n    // The `Int` initializer call below is non-differentiable.\n    Float(Int(x + y))\n}\n```\n\n```console\ntest.swift:1:52: error: function is not differentiable\nlet add: @differentiable (Float, Float) -> Float = { x, y in\n                                                   ^~~~~~~~~\ntest.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n    Float(Int(x + y))\n          ^\n```\n\nThere are a few reasons why differentiation can fail:\n* The function to differentiate contains computation, from parameters to result,\n  that cannot be differentiated.\n* The function to differentiate is opaque, i.e. it is a function parameter with\n  a non-`@differentiable` function type.\n* The function to differentiate is defined in another module.\n* The function to differentiate uses [control\n  flow](https://docs.swift.org/swift-book/LanguageGuide/ControlFlow.html)\n  (if-statements, switch-statements, loops, etc). This restriction will be\n  lifted soon.\n  \n## `@differentiable` declaration attribute\n\n### Basics\n\nThe `@differentiable` attribute can also be applied to function declarations.\n`@differentiable` marks a function as being differentiable with respect to some\nparameters (the varying parameters, explained below). `@differentiable` requires\nthe types of the varying parameters and the function result type to all conform\nto the `Differentiable` protocol.\n\nThis annotation does not change the declaration to have a `@differentiable`\nfunction type; instead, it triggers differentiation by the compiler on the\nfunction. If differentiation succeeds, then conversion of the function to a\n`@differentiable` function is guaranteed to succeed later.\n\nYou may wonder about the purpose of the `@differentiable` declaration attribute,\ngiven that non-differentiable functions can implicitly be converted to\n`@differentiable` functions, as mentioned above. The main reason is that the\n`@differentiable` declaration attribute is a contract for differentiability: if\na function is declared with `@differentiable` and it compiles, then it is always\nguaranteed to be differentiable, even in other modules. On the other hand, if a\nfunction is not declared with `@differentiable`, then differentiation of the\nfunction in other modules will fail.\n\nThis is why floating-point operations in the standard library are declared with\n`@differentiable`:\n\n```swift\nextension Float {\n    @differentiable\n    public static func + (lhs: Float, rhs: Float) -> Float { ... }\n}\n```\n\nBesides function declarations, there are a few other function-like declarations\nthat can be marked with `@differentiable`:\n- Stored and computed properties.\n  - This requires both the type defining the property\n    and the type of the property to conform to `Differentiable`.\n  - Property getters are differentiable with respect to `self`.\n- Initializers.\n  - This requires the type defining the initializer to conform to\n    `Differentiable`.\n\nFor instance methods defined on types that conform to `Differentiable`, the\n`self` property can be marked as a varying parameter. Derivatives of these\nmethods return the partial derivative with respect to `self`. For these methods,\n`@differentiable` infers `self` as a varying parameter by default.\n\n```swift\nstruct Vector: Differentiable, VectorNumeric {\n    var x, y: Float\n\n    // Differentiable computed property.\n    @differentiable // Implicitly: @differentiable(wrt: self)\n    var magnitude: Float {\n        return (x * x + y * y).squareRoot()\n    }\n\n    // Differentiable initializer.\n    @differentiable // Implicitly: @differentiable(wrt: (x, y))\n    init(x: Float, y: Float) {\n        self.x = x\n        self.y = y\n    }\n}\n\nlet v = Vector(x: 2, y: 2)\nprint(v.magnitude)\n// 2.828427\nprint(gradient(at: v) { v in v.magnitude })\n// Vector(x: 64.0, y: 64.0)\n```\n\n### Differentiating with respect to\n\nMathematically, let \"varying parameters\" refer to the parameters (i.e.\nindependent variables) of a differentiable function whose partial derivatives\nare computed by the function's derivative.\n\nBy default, the `@differentiable` attribute infers all function parameters that\nconform to `Differentiable` to be the varying parameters. However, this is not\nalways desirable. To explicitly declare functions as differentiable with respect\nto a subset of parameters, explicitly specify the varying parameters using the\n`@differentiable(wrt: ...)` syntax.\n\nHere's an example of a 2-D convolution operation, adapted from the TensorFlow\nlibrary. The convolution input and filter are the varying parameters; strides\nand padding are not.\n\n```swift\n@differentiable(wrt: (input, filter))\nfunc conv2d(input: Tensor<Float>, filter: Tensor<Float>, strides: (Int, Int), padding: Padding) {\n    ...\n}\n```\n\nFunctions can have multiple `@differentiable` attributes with differentiable\n`wrt` parameter lists. `@differentiable` protocol requirements If a protocol\nrequirement is marked with `@differentiable`, all implementations of the\nrequirement are required to specify the same attribute. This enables generic\ncode using differentiation defined in terms of protocol requirements.\n\nHere is an example of a neural network `Layer` protocol that defines a\n`@differentiable` required method called `applied(to:)`. As shown, the\n`applied(to:)` method can be differentiated in a `Layer` protocol extension,\neven though it is not a concrete method.\n\n```swift\nimport TensorFlow\n\n/// A neural network layer.\nprotocol Layer: Differentiable {\n    /// The input type of the layer.\n    associatedtype Input: Differentiable\n    /// The output type of the layer.\n    associatedtype Output: Differentiable\n    /// Returns the output obtained from applying the layer to the given input.\n    @differentiable\n    func applied(to input: Input) -> Output\n}\n\nextension Layer {\n    /// Returns the inference output and the backpropagation function obtained from applying the\n    /// layer to the given input.\n    ///\n    /// - Parameter input: The input to the layer.\n    /// - Returns: A tuple containing the output and the backpropagation function. The\n    ///   backpropagation function (a.k.a. backpropagator) takes a direction vector and returns the\n    ///   gradients at the layer and at the input, respectively.\n    func appliedForBackpropagation(to input: Input)\n        -> (output: Output,\n            backpropagator: (_ direction: Output.CotangentVector)\n                -> (layerGradient: CotangentVector, inputGradient: Input.CotangentVector)) {\n        let (out, pullback) = valueWithPullback(at: input) { layer, input in\n            return layer.applied(to: input)\n        }\n        return (out, pullback)\n    }\n}\n\n// Example neural network layer.\nstruct DenseLayer: Layer {\n    var weight: Tensor<Float>\n    var bias: Tensor<Float>\n\n    @differentiable\n    func applied(to input: Tensor<Float>) -> Tensor<Float> {\n        return matmul(input, weight) + bias\n    }\n}\n\n// Example usage of `appliedForBackpropagation(to:)`.\nlet dense = DenseLayer(weight: [[1, 1], [1, 1]], bias: [1, 1])\nlet input: Tensor<Float> = [[3, 3]]\nlet seed: Tensor<Float> = [[1, 1]]\n\nlet (output, backprop) = dense.appliedForBackpropagation(to: input)\nlet (\ud835\udec1dense, \ud835\udec1input) = backprop(seed)\n\ndump(\ud835\udec1dense)\n// \u25bf DenseLayer.AllDifferentiableVariables\n//   - weight: [[3.0, 3.0], [3.0, 3.0]]\n//   - bias: [1.0, 1.0]\nprint(\ud835\udec1input)\n// [[2.0, 2.0]]\n```\n\n## Providing a custom derivative\n\nUse the `@differentiating` attribute to mark a function as a custom derivative\nfor another function. This is useful for registering derivatives for primitive\noperations.",
    "repo_full_name": "tensorflow/swift",
    "discussion_comments": [
      {
        "comment_id": "266192104",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 145,
        "pr_file": "docs/DifferentiableFunctions.md",
        "discussion_id": "266192104",
        "commented_code": "@@ -0,0 +1,454 @@\n+# Differentiable functions and differentiation APIs\n+\n+[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n+\n+Last updated: March 2019\n+\n+## Introduction\n+\n+Swift supports differentiable functions as part of the language. The\n+`@differentiable` attribute appears in two locations in Swift syntax: as an\n+annotation on function types and as an annotation on function declarations (and\n+other similar declarations). This document explains the meaning of these\n+annotations.\n+\n+## `@differentiable` function type attribute\n+\n+### Basics\n+\n+In Swift, function types can have attributes. When a function type is annotated\n+with `@differentiable`, Swift guarantees that all values of that function type\n+can be differentiated.\n+\n+`@differentiable` functions can be called like normal functions, or be passed to\n+APIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\n+representation of a `@differentiable` function is a special data structure\n+containing the original function along with extra information required for\n+computing its derivatives. Usage `@differentiable` functions are a part of\n+Swift's type system. Most notably, they are used by differentiation APIs in the\n+standard library. Here are some examples demonstrating differentiation APIs:\n+\n+```swift\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// Free function examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(gradient(at: x, in: square)) // 6.0\n+// Computes the gradient of `square`, then applies it to `x`.\n+print(gradient(of: square)(x)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n+\n+// Method examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(x.gradient(in: square)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n+```\n+\n+Here's a list of differentiation APIs provided by the standard library:\n+\n+| Differentiation APIs  | Description                                                                                                                                                      |\n+|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n+| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n+| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n+| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n+| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n+| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n+\n+### Constructing `@differentiable` functions\n+\n+A value with a non-differentiable function type can be implicitly converted to\n+one with a corresponding `@differentiable` function type. In fact, this is what\n+happened in the example above:\n+\n+```swift\n+// `square` has type `(Float) -> Float`.\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n+// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n+// `@differentiable` type.\n+print(gradient(at: x, in: square)) // 6.0\n+```\n+\n+The implicit conversion from a value with type `(T) -> U` to a value with type\n+`@differentiable (T) -> U` actually triggers differentiation by the compiler.\n+Thus, differentiation is type-driven.\n+\n+If differentiation succeeds, the `@differentiable (T) -> U` value is\n+constructed. If differentiation fails, the compiler emits a compile-time error:\n+\n+```swift\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+    // The `Int` initializer call below is non-differentiable.\n+    Float(Int(x + y))\n+}\n+```\n+\n+```console\n+test.swift:1:52: error: function is not differentiable\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+                                                   ^~~~~~~~~\n+test.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n+    Float(Int(x + y))\n+          ^\n+```\n+\n+There are a few reasons why differentiation can fail:\n+* The function to differentiate contains computation, from parameters to result,\n+  that cannot be differentiated.\n+* The function to differentiate is opaque, i.e. it is a function parameter with\n+  a non-`@differentiable` function type.\n+* The function to differentiate is defined in another module.\n+* The function to differentiate uses [control\n+  flow](https://docs.swift.org/swift-book/LanguageGuide/ControlFlow.html)\n+  (if-statements, switch-statements, loops, etc). This restriction will be\n+  lifted soon.\n+  \n+## `@differentiable` declaration attribute\n+\n+### Basics\n+\n+The `@differentiable` attribute can also be applied to function declarations.\n+`@differentiable` marks a function as being differentiable with respect to some\n+parameters (the varying parameters, explained below). `@differentiable` requires\n+the types of the varying parameters and the function result type to all conform\n+to the `Differentiable` protocol.\n+\n+This annotation does not change the declaration to have a `@differentiable`\n+function type; instead, it triggers differentiation by the compiler on the\n+function. If differentiation succeeds, then conversion of the function to a\n+`@differentiable` function is guaranteed to succeed later.\n+\n+You may wonder about the purpose of the `@differentiable` declaration attribute,\n+given that non-differentiable functions can implicitly be converted to\n+`@differentiable` functions, as mentioned above. The main reason is that the\n+`@differentiable` declaration attribute is a contract for differentiability: if\n+a function is declared with `@differentiable` and it compiles, then it is always\n+guaranteed to be differentiable, even in other modules. On the other hand, if a\n+function is not declared with `@differentiable`, then differentiation of the\n+function in other modules will fail.\n+\n+This is why floating-point operations in the standard library are declared with\n+`@differentiable`:\n+\n+```swift\n+extension Float {\n+    @differentiable\n+    public static func + (lhs: Float, rhs: Float) -> Float { ... }\n+}\n+```\n+\n+Besides function declarations, there are a few other function-like declarations\n+that can be marked with `@differentiable`:\n+- Stored and computed properties.\n+  - This requires both the type defining the property\n+    and the type of the property to conform to `Differentiable`.\n+  - Property getters are differentiable with respect to `self`.\n+- Initializers.\n+  - This requires the type defining the initializer to conform to\n+    `Differentiable`.\n+\n+For instance methods defined on types that conform to `Differentiable`, the\n+`self` property can be marked as a varying parameter. Derivatives of these\n+methods return the partial derivative with respect to `self`. For these methods,\n+`@differentiable` infers `self` as a varying parameter by default.\n+\n+```swift\n+struct Vector: Differentiable, VectorNumeric {\n+    var x, y: Float\n+\n+    // Differentiable computed property.\n+    @differentiable // Implicitly: @differentiable(wrt: self)\n+    var magnitude: Float {\n+        return (x * x + y * y).squareRoot()\n+    }\n+\n+    // Differentiable initializer.\n+    @differentiable // Implicitly: @differentiable(wrt: (x, y))\n+    init(x: Float, y: Float) {\n+        self.x = x\n+        self.y = y\n+    }\n+}\n+\n+let v = Vector(x: 2, y: 2)\n+print(v.magnitude)\n+// 2.828427\n+print(gradient(at: v) { v in v.magnitude })\n+// Vector(x: 64.0, y: 64.0)\n+```\n+\n+### Differentiating with respect to\n+\n+Mathematically, let \"varying parameters\" refer to the parameters (i.e.\n+independent variables) of a differentiable function whose partial derivatives\n+are computed by the function's derivative.\n+\n+By default, the `@differentiable` attribute infers all function parameters that\n+conform to `Differentiable` to be the varying parameters. However, this is not\n+always desirable. To explicitly declare functions as differentiable with respect\n+to a subset of parameters, explicitly specify the varying parameters using the\n+`@differentiable(wrt: ...)` syntax.\n+\n+Here's an example of a 2-D convolution operation, adapted from the TensorFlow\n+library. The convolution input and filter are the varying parameters; strides\n+and padding are not.\n+\n+```swift\n+@differentiable(wrt: (input, filter))\n+func conv2d(input: Tensor<Float>, filter: Tensor<Float>, strides: (Int, Int), padding: Padding) {\n+    ...\n+}\n+```\n+\n+Functions can have multiple `@differentiable` attributes with differentiable\n+`wrt` parameter lists. `@differentiable` protocol requirements If a protocol\n+requirement is marked with `@differentiable`, all implementations of the\n+requirement are required to specify the same attribute. This enables generic\n+code using differentiation defined in terms of protocol requirements.\n+\n+Here is an example of a neural network `Layer` protocol that defines a\n+`@differentiable` required method called `applied(to:)`. As shown, the\n+`applied(to:)` method can be differentiated in a `Layer` protocol extension,\n+even though it is not a concrete method.\n+\n+```swift\n+import TensorFlow\n+\n+/// A neural network layer.\n+protocol Layer: Differentiable {\n+    /// The input type of the layer.\n+    associatedtype Input: Differentiable\n+    /// The output type of the layer.\n+    associatedtype Output: Differentiable\n+    /// Returns the output obtained from applying the layer to the given input.\n+    @differentiable\n+    func applied(to input: Input) -> Output\n+}\n+\n+extension Layer {\n+    /// Returns the inference output and the backpropagation function obtained from applying the\n+    /// layer to the given input.\n+    ///\n+    /// - Parameter input: The input to the layer.\n+    /// - Returns: A tuple containing the output and the backpropagation function. The\n+    ///   backpropagation function (a.k.a. backpropagator) takes a direction vector and returns the\n+    ///   gradients at the layer and at the input, respectively.\n+    func appliedForBackpropagation(to input: Input)\n+        -> (output: Output,\n+            backpropagator: (_ direction: Output.CotangentVector)\n+                -> (layerGradient: CotangentVector, inputGradient: Input.CotangentVector)) {\n+        let (out, pullback) = valueWithPullback(at: input) { layer, input in\n+            return layer.applied(to: input)\n+        }\n+        return (out, pullback)\n+    }\n+}\n+\n+// Example neural network layer.\n+struct DenseLayer: Layer {\n+    var weight: Tensor<Float>\n+    var bias: Tensor<Float>\n+\n+    @differentiable\n+    func applied(to input: Tensor<Float>) -> Tensor<Float> {\n+        return matmul(input, weight) + bias\n+    }\n+}\n+\n+// Example usage of `appliedForBackpropagation(to:)`.\n+let dense = DenseLayer(weight: [[1, 1], [1, 1]], bias: [1, 1])\n+let input: Tensor<Float> = [[3, 3]]\n+let seed: Tensor<Float> = [[1, 1]]\n+\n+let (output, backprop) = dense.appliedForBackpropagation(to: input)\n+let (\ud835\udec1dense, \ud835\udec1input) = backprop(seed)\n+\n+dump(\ud835\udec1dense)\n+// \u25bf DenseLayer.AllDifferentiableVariables\n+//   - weight: [[3.0, 3.0], [3.0, 3.0]]\n+//   - bias: [1.0, 1.0]\n+print(\ud835\udec1input)\n+// [[2.0, 2.0]]\n+```\n+\n+## Providing a custom derivative\n+\n+Use the `@differentiating` attribute to mark a function as a custom derivative\n+for another function. This is useful for registering derivatives for primitive\n+operations.",
        "comment_created_at": "2019-03-16T07:01:00+00:00",
        "comment_author": "rxwei",
        "comment_body": "\"primitive operations\" -> \"functions that cannot be differentiated by traversing the function body, typically a primitive math operator in a library\"",
        "pr_file_module": null
      },
      {
        "comment_id": "266192709",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 145,
        "pr_file": "docs/DifferentiableFunctions.md",
        "discussion_id": "266192104",
        "commented_code": "@@ -0,0 +1,454 @@\n+# Differentiable functions and differentiation APIs\n+\n+[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n+\n+Last updated: March 2019\n+\n+## Introduction\n+\n+Swift supports differentiable functions as part of the language. The\n+`@differentiable` attribute appears in two locations in Swift syntax: as an\n+annotation on function types and as an annotation on function declarations (and\n+other similar declarations). This document explains the meaning of these\n+annotations.\n+\n+## `@differentiable` function type attribute\n+\n+### Basics\n+\n+In Swift, function types can have attributes. When a function type is annotated\n+with `@differentiable`, Swift guarantees that all values of that function type\n+can be differentiated.\n+\n+`@differentiable` functions can be called like normal functions, or be passed to\n+APIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\n+representation of a `@differentiable` function is a special data structure\n+containing the original function along with extra information required for\n+computing its derivatives. Usage `@differentiable` functions are a part of\n+Swift's type system. Most notably, they are used by differentiation APIs in the\n+standard library. Here are some examples demonstrating differentiation APIs:\n+\n+```swift\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// Free function examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(gradient(at: x, in: square)) // 6.0\n+// Computes the gradient of `square`, then applies it to `x`.\n+print(gradient(of: square)(x)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n+\n+// Method examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(x.gradient(in: square)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n+```\n+\n+Here's a list of differentiation APIs provided by the standard library:\n+\n+| Differentiation APIs  | Description                                                                                                                                                      |\n+|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n+| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n+| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n+| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n+| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n+| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n+\n+### Constructing `@differentiable` functions\n+\n+A value with a non-differentiable function type can be implicitly converted to\n+one with a corresponding `@differentiable` function type. In fact, this is what\n+happened in the example above:\n+\n+```swift\n+// `square` has type `(Float) -> Float`.\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n+// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n+// `@differentiable` type.\n+print(gradient(at: x, in: square)) // 6.0\n+```\n+\n+The implicit conversion from a value with type `(T) -> U` to a value with type\n+`@differentiable (T) -> U` actually triggers differentiation by the compiler.\n+Thus, differentiation is type-driven.\n+\n+If differentiation succeeds, the `@differentiable (T) -> U` value is\n+constructed. If differentiation fails, the compiler emits a compile-time error:\n+\n+```swift\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+    // The `Int` initializer call below is non-differentiable.\n+    Float(Int(x + y))\n+}\n+```\n+\n+```console\n+test.swift:1:52: error: function is not differentiable\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+                                                   ^~~~~~~~~\n+test.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n+    Float(Int(x + y))\n+          ^\n+```\n+\n+There are a few reasons why differentiation can fail:\n+* The function to differentiate contains computation, from parameters to result,\n+  that cannot be differentiated.\n+* The function to differentiate is opaque, i.e. it is a function parameter with\n+  a non-`@differentiable` function type.\n+* The function to differentiate is defined in another module.\n+* The function to differentiate uses [control\n+  flow](https://docs.swift.org/swift-book/LanguageGuide/ControlFlow.html)\n+  (if-statements, switch-statements, loops, etc). This restriction will be\n+  lifted soon.\n+  \n+## `@differentiable` declaration attribute\n+\n+### Basics\n+\n+The `@differentiable` attribute can also be applied to function declarations.\n+`@differentiable` marks a function as being differentiable with respect to some\n+parameters (the varying parameters, explained below). `@differentiable` requires\n+the types of the varying parameters and the function result type to all conform\n+to the `Differentiable` protocol.\n+\n+This annotation does not change the declaration to have a `@differentiable`\n+function type; instead, it triggers differentiation by the compiler on the\n+function. If differentiation succeeds, then conversion of the function to a\n+`@differentiable` function is guaranteed to succeed later.\n+\n+You may wonder about the purpose of the `@differentiable` declaration attribute,\n+given that non-differentiable functions can implicitly be converted to\n+`@differentiable` functions, as mentioned above. The main reason is that the\n+`@differentiable` declaration attribute is a contract for differentiability: if\n+a function is declared with `@differentiable` and it compiles, then it is always\n+guaranteed to be differentiable, even in other modules. On the other hand, if a\n+function is not declared with `@differentiable`, then differentiation of the\n+function in other modules will fail.\n+\n+This is why floating-point operations in the standard library are declared with\n+`@differentiable`:\n+\n+```swift\n+extension Float {\n+    @differentiable\n+    public static func + (lhs: Float, rhs: Float) -> Float { ... }\n+}\n+```\n+\n+Besides function declarations, there are a few other function-like declarations\n+that can be marked with `@differentiable`:\n+- Stored and computed properties.\n+  - This requires both the type defining the property\n+    and the type of the property to conform to `Differentiable`.\n+  - Property getters are differentiable with respect to `self`.\n+- Initializers.\n+  - This requires the type defining the initializer to conform to\n+    `Differentiable`.\n+\n+For instance methods defined on types that conform to `Differentiable`, the\n+`self` property can be marked as a varying parameter. Derivatives of these\n+methods return the partial derivative with respect to `self`. For these methods,\n+`@differentiable` infers `self` as a varying parameter by default.\n+\n+```swift\n+struct Vector: Differentiable, VectorNumeric {\n+    var x, y: Float\n+\n+    // Differentiable computed property.\n+    @differentiable // Implicitly: @differentiable(wrt: self)\n+    var magnitude: Float {\n+        return (x * x + y * y).squareRoot()\n+    }\n+\n+    // Differentiable initializer.\n+    @differentiable // Implicitly: @differentiable(wrt: (x, y))\n+    init(x: Float, y: Float) {\n+        self.x = x\n+        self.y = y\n+    }\n+}\n+\n+let v = Vector(x: 2, y: 2)\n+print(v.magnitude)\n+// 2.828427\n+print(gradient(at: v) { v in v.magnitude })\n+// Vector(x: 64.0, y: 64.0)\n+```\n+\n+### Differentiating with respect to\n+\n+Mathematically, let \"varying parameters\" refer to the parameters (i.e.\n+independent variables) of a differentiable function whose partial derivatives\n+are computed by the function's derivative.\n+\n+By default, the `@differentiable` attribute infers all function parameters that\n+conform to `Differentiable` to be the varying parameters. However, this is not\n+always desirable. To explicitly declare functions as differentiable with respect\n+to a subset of parameters, explicitly specify the varying parameters using the\n+`@differentiable(wrt: ...)` syntax.\n+\n+Here's an example of a 2-D convolution operation, adapted from the TensorFlow\n+library. The convolution input and filter are the varying parameters; strides\n+and padding are not.\n+\n+```swift\n+@differentiable(wrt: (input, filter))\n+func conv2d(input: Tensor<Float>, filter: Tensor<Float>, strides: (Int, Int), padding: Padding) {\n+    ...\n+}\n+```\n+\n+Functions can have multiple `@differentiable` attributes with differentiable\n+`wrt` parameter lists. `@differentiable` protocol requirements If a protocol\n+requirement is marked with `@differentiable`, all implementations of the\n+requirement are required to specify the same attribute. This enables generic\n+code using differentiation defined in terms of protocol requirements.\n+\n+Here is an example of a neural network `Layer` protocol that defines a\n+`@differentiable` required method called `applied(to:)`. As shown, the\n+`applied(to:)` method can be differentiated in a `Layer` protocol extension,\n+even though it is not a concrete method.\n+\n+```swift\n+import TensorFlow\n+\n+/// A neural network layer.\n+protocol Layer: Differentiable {\n+    /// The input type of the layer.\n+    associatedtype Input: Differentiable\n+    /// The output type of the layer.\n+    associatedtype Output: Differentiable\n+    /// Returns the output obtained from applying the layer to the given input.\n+    @differentiable\n+    func applied(to input: Input) -> Output\n+}\n+\n+extension Layer {\n+    /// Returns the inference output and the backpropagation function obtained from applying the\n+    /// layer to the given input.\n+    ///\n+    /// - Parameter input: The input to the layer.\n+    /// - Returns: A tuple containing the output and the backpropagation function. The\n+    ///   backpropagation function (a.k.a. backpropagator) takes a direction vector and returns the\n+    ///   gradients at the layer and at the input, respectively.\n+    func appliedForBackpropagation(to input: Input)\n+        -> (output: Output,\n+            backpropagator: (_ direction: Output.CotangentVector)\n+                -> (layerGradient: CotangentVector, inputGradient: Input.CotangentVector)) {\n+        let (out, pullback) = valueWithPullback(at: input) { layer, input in\n+            return layer.applied(to: input)\n+        }\n+        return (out, pullback)\n+    }\n+}\n+\n+// Example neural network layer.\n+struct DenseLayer: Layer {\n+    var weight: Tensor<Float>\n+    var bias: Tensor<Float>\n+\n+    @differentiable\n+    func applied(to input: Tensor<Float>) -> Tensor<Float> {\n+        return matmul(input, weight) + bias\n+    }\n+}\n+\n+// Example usage of `appliedForBackpropagation(to:)`.\n+let dense = DenseLayer(weight: [[1, 1], [1, 1]], bias: [1, 1])\n+let input: Tensor<Float> = [[3, 3]]\n+let seed: Tensor<Float> = [[1, 1]]\n+\n+let (output, backprop) = dense.appliedForBackpropagation(to: input)\n+let (\ud835\udec1dense, \ud835\udec1input) = backprop(seed)\n+\n+dump(\ud835\udec1dense)\n+// \u25bf DenseLayer.AllDifferentiableVariables\n+//   - weight: [[3.0, 3.0], [3.0, 3.0]]\n+//   - bias: [1.0, 1.0]\n+print(\ud835\udec1input)\n+// [[2.0, 2.0]]\n+```\n+\n+## Providing a custom derivative\n+\n+Use the `@differentiating` attribute to mark a function as a custom derivative\n+for another function. This is useful for registering derivatives for primitive\n+operations.",
        "comment_created_at": "2019-03-16T07:27:30+00:00",
        "comment_author": "dan-zheng",
        "comment_body": "Changing to:\r\n```\r\nUse the `@differentiating` attribute to mark a function as a custom derivative\r\nfor another function. This is useful for registering derivatives for functions\r\nthat cannot be differentiated by traversing the function body, typically\r\nprimitive math library operators.\r\n```",
        "pr_file_module": null
      }
    ]
  }
]
