[
  {
    "discussion_id": "266212108",
    "pr_number": 147,
    "pr_file": "docs/DifferentiableTypes.md",
    "created_at": "2019-03-16T19:16:56+00:00",
    "commented_code": "}\n```\n\nMathematically, `Differentiable` represents a [differentiable manifold]: this is a technical term for smooth-surfaced objects like spheres and generalizes types that are compatible with differentiation, like `Float`, `Double`, [`Tensor`][TensorFlow_Tensor], and `SIMD4<Float>`. This definition comes from differential geometry and is quite technical, and not all details are relevant for most use cases.\nMathematically, `Differentiable` represents a [differentiable manifold]: this is a technical term for smooth-surfaced objects like spheres and generalizes types that are compatible with differentiation, like `Float`, `Double`, [`Tensor`][https://www.tensorflow.org/guide/tensors], and `SIMD4<Float>`. This definition comes from differential geometry and is quite technical, and not all details are relevant for most use cases.",
    "repo_full_name": "tensorflow/swift",
    "discussion_comments": [
      {
        "comment_id": "266212108",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 147,
        "pr_file": "docs/DifferentiableTypes.md",
        "discussion_id": "266212108",
        "commented_code": "@@ -121,7 +121,7 @@ public protocol Differentiable {\n }\n ```\n \n-Mathematically, `Differentiable` represents a [differentiable manifold]: this is a technical term for smooth-surfaced objects like spheres and generalizes types that are compatible with differentiation, like `Float`, `Double`, [`Tensor`][TensorFlow_Tensor], and `SIMD4<Float>`. This definition comes from differential geometry and is quite technical, and not all details are relevant for most use cases.\n+Mathematically, `Differentiable` represents a [differentiable manifold]: this is a technical term for smooth-surfaced objects like spheres and generalizes types that are compatible with differentiation, like `Float`, `Double`, [`Tensor`][https://www.tensorflow.org/guide/tensors], and `SIMD4<Float>`. This definition comes from differential geometry and is quite technical, and not all details are relevant for most use cases.",
        "comment_created_at": "2019-03-16T19:16:56+00:00",
        "comment_author": "dan-zheng",
        "comment_body": "Please fix the `TensorFlow_Tensor` link at the bottom of the file:\r\n\r\n```suggestion\r\nMathematically, `Differentiable` represents a [differentiable manifold]: this is a technical term for smooth-surfaced objects like spheres and generalizes types that are compatible with differentiation, like `Float`, `Double`, [`Tensor`][TensorFlow_Tensor], and `SIMD4<Float>`. This definition comes from differential geometry and is quite technical, and not all details are relevant for most use cases.\r\n\r\n... // At bottom of file:\r\n[TensorFlow_Tensor]: https://www.tensorflow.org/swift/api_docs/Structs/Tensor\r\n```\r\n\r\nPlease use the link above instead of https://www.tensorflow.org/guide/tensors, which points to the Python documentation for `tf.Tensor` and is not related to Swift for TensorFlow.",
        "pr_file_module": null
      },
      {
        "comment_id": "266212640",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 147,
        "pr_file": "docs/DifferentiableTypes.md",
        "discussion_id": "266212108",
        "commented_code": "@@ -121,7 +121,7 @@ public protocol Differentiable {\n }\n ```\n \n-Mathematically, `Differentiable` represents a [differentiable manifold]: this is a technical term for smooth-surfaced objects like spheres and generalizes types that are compatible with differentiation, like `Float`, `Double`, [`Tensor`][TensorFlow_Tensor], and `SIMD4<Float>`. This definition comes from differential geometry and is quite technical, and not all details are relevant for most use cases.\n+Mathematically, `Differentiable` represents a [differentiable manifold]: this is a technical term for smooth-surfaced objects like spheres and generalizes types that are compatible with differentiation, like `Float`, `Double`, [`Tensor`][https://www.tensorflow.org/guide/tensors], and `SIMD4<Float>`. This definition comes from differential geometry and is quite technical, and not all details are relevant for most use cases.",
        "comment_created_at": "2019-03-16T19:35:38+00:00",
        "comment_author": "rxwei",
        "comment_body": "To Ayush: The link at this location should remain as is, while the link defined at the bottom of the file (“TensorFlow_Tensor”) should be changed to the new URL.\r\n\r\nThanks for fixing this!",
        "pr_file_module": null
      },
      {
        "comment_id": "266212660",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 147,
        "pr_file": "docs/DifferentiableTypes.md",
        "discussion_id": "266212108",
        "commented_code": "@@ -121,7 +121,7 @@ public protocol Differentiable {\n }\n ```\n \n-Mathematically, `Differentiable` represents a [differentiable manifold]: this is a technical term for smooth-surfaced objects like spheres and generalizes types that are compatible with differentiation, like `Float`, `Double`, [`Tensor`][TensorFlow_Tensor], and `SIMD4<Float>`. This definition comes from differential geometry and is quite technical, and not all details are relevant for most use cases.\n+Mathematically, `Differentiable` represents a [differentiable manifold]: this is a technical term for smooth-surfaced objects like spheres and generalizes types that are compatible with differentiation, like `Float`, `Double`, [`Tensor`][https://www.tensorflow.org/guide/tensors], and `SIMD4<Float>`. This definition comes from differential geometry and is quite technical, and not all details are relevant for most use cases.",
        "comment_created_at": "2019-03-16T19:36:45+00:00",
        "comment_author": "dan-zheng",
        "comment_body": "Yes, please use https://www.tensorflow.org/swift/api_docs/Structs/Tensor as the link.\r\n\r\nBut please edit the existing [reference-style link](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#links) at the bottom of the page instead of creating an inline link.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "266160936",
    "pr_number": 145,
    "pr_file": "docs/DifferentiableFunctions.md",
    "created_at": "2019-03-15T22:07:09+00:00",
    "commented_code": "# Differentiable functions and differentiation APIs\n\n[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n\nLast updated: March 2019\n\n## Introduction\n\nSwift supports differentiable functions as part of the language. The\n`@differentiable` attribute appears in two locations in Swift syntax: as an\nannotation on function types and as an annotation on function declarations (and\nother similar declarations). This document explains the meaning of these\nannotations.\n\n## `@differentiable` function type attribute\n\n### Basics\n\nIn Swift, function types can have attributes. When a function type is annotated\nwith `@differentiable`, Swift guarantees that all values of that function type\ncan be differentiated.\n\n`@differentiable` functions can be called like normal functions, or be passed to\nAPIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\nrepresentation of a `@differentiable` function is a special data structure\ncontaining the original function along with extra information required for\ncomputing its derivatives. Usage `@differentiable` functions are a part of\nSwift's type system. Most notably, they are used by differentiation APIs in the\nstandard library. Here are some examples demonstrating differentiation APIs:\n\n```swift\nfunc square(_ x: Float) -> Float {\n    return x * x\n}\nlet x: Float = 3.0\n\n// Free function examples.\n\n// Computes the gradient of `square` at `x`.\nprint(gradient(at: x, in: square)) // 6.0\n// Computes the gradient of `square`, then applies it to `x`.\nprint(gradient(of: square)(x)) // 6.0\n// Computes the value and gradient of `square` at `x`.\nprint(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)",
    "repo_full_name": "tensorflow/swift",
    "discussion_comments": [
      {
        "comment_id": "266160936",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 145,
        "pr_file": "docs/DifferentiableFunctions.md",
        "discussion_id": "266160936",
        "commented_code": "@@ -0,0 +1,444 @@\n+# Differentiable functions and differentiation APIs\n+\n+[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n+\n+Last updated: March 2019\n+\n+## Introduction\n+\n+Swift supports differentiable functions as part of the language. The\n+`@differentiable` attribute appears in two locations in Swift syntax: as an\n+annotation on function types and as an annotation on function declarations (and\n+other similar declarations). This document explains the meaning of these\n+annotations.\n+\n+## `@differentiable` function type attribute\n+\n+### Basics\n+\n+In Swift, function types can have attributes. When a function type is annotated\n+with `@differentiable`, Swift guarantees that all values of that function type\n+can be differentiated.\n+\n+`@differentiable` functions can be called like normal functions, or be passed to\n+APIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\n+representation of a `@differentiable` function is a special data structure\n+containing the original function along with extra information required for\n+computing its derivatives. Usage `@differentiable` functions are a part of\n+Swift's type system. Most notably, they are used by differentiation APIs in the\n+standard library. Here are some examples demonstrating differentiation APIs:\n+\n+```swift\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// Free function examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(gradient(at: x, in: square)) // 6.0\n+// Computes the gradient of `square`, then applies it to `x`.\n+print(gradient(of: square)(x)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)",
        "comment_created_at": "2019-03-15T22:07:09+00:00",
        "comment_author": "dan-zheng",
        "comment_body": "It may be nice to include examples of `pullback` and `valueWithPullback`.\r\nAlso, it may be nice to include examples of differentiation APIs for higher arity functions.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "266187899",
    "pr_number": 145,
    "pr_file": "docs/DifferentiableFunctions.md",
    "created_at": "2019-03-16T03:46:25+00:00",
    "commented_code": "# Differentiable functions and differentiation APIs\n\n[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n\nLast updated: March 2019\n\n## Introduction\n\nSwift supports differentiable functions as part of the language. The\n`@differentiable` attribute appears in two locations in Swift syntax: as an\nannotation on function types and as an annotation on function declarations (and\nother similar declarations). This document explains the meaning of these\nannotations.\n\n## `@differentiable` function type attribute\n\n### Basics\n\nIn Swift, function types can have attributes. When a function type is annotated\nwith `@differentiable`, Swift guarantees that all values of that function type\ncan be differentiated.\n\n`@differentiable` functions can be called like normal functions, or be passed to\nAPIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\nrepresentation of a `@differentiable` function is a special data structure\ncontaining the original function along with extra information required for\ncomputing its derivatives. Usage `@differentiable` functions are a part of\nSwift's type system. Most notably, they are used by differentiation APIs in the\nstandard library. Here are some examples demonstrating differentiation APIs:\n\n```swift\nfunc square(_ x: Float) -> Float {\n    return x * x\n}\nlet x: Float = 3.0\n\n// Free function examples.\n\n// Computes the gradient of `square` at `x`.\nprint(gradient(at: x, in: square)) // 6.0\n// Computes the gradient of `square`, then applies it to `x`.\nprint(gradient(of: square)(x)) // 6.0\n// Computes the value and gradient of `square` at `x`.\nprint(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n\n// Method examples.\n\n// Computes the gradient of `square` at `x`.\nprint(x.gradient(in: square)) // 6.0\n// Computes the value and gradient of `square` at `x`.\nprint(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n```\n\nHere's a list of differentiation APIs provided by the standard library:\n\n| Differentiation APIs  | Description                                                                                                                                                      |\n|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n\n### Constructing `@differentiable` functions\n\nA value with a non-differentiable function type can be implicitly converted to\none with a corresponding `@differentiable` function type. In fact, this is what\nhappened in the example above:\n\n```swift\n// `square` has type `(Float) -> Float`.\nfunc square(_ x: Float) -> Float {\n    return x * x\n}\nlet x: Float = 3.0\n\n// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n// `@differentiable` type.\nprint(gradient(at: x, in: square)) // 6.0\n```\n\nThe implicit conversion from a value with type `(T) -> U` to a value with type\n`@differentiable (T) -> U` actually triggers differentiation by the compiler.\nThus, differentiation is type-driven.\n\nIf differentiation succeeds, the `@differentiable (T) -> U` value is\nconstructed. If differentiation fails, the compiler emits a compile-time error:\n\n```swift\nlet add: @differentiable (Float, Float) -> Float = { x, y in\n    // The `Int` initializer call below is non-differentiable.\n    Float(Int(x + y))\n}\n```\n\n```console\ntest.swift:1:52: error: function is not differentiable\nlet add: @differentiable (Float, Float) -> Float = { x, y in\n                                                   ^~~~~~~~~\ntest.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n    Float(Int(x + y))\n          ^\n```\n\nThere are a few reasons why differentiation can fail:\n* The function to differentiate contains computation, from parameters to result,\n  that cannot be differentiated.\n* The function to differentiate is opaque, i.e. it is a function parameter with\n  a non-`@differentiable` function type.\n* The function to differentiate is defined in another module.\n* The function to differentiate uses [control\n  flow](https://docs.swift.org/swift-book/LanguageGuide/ControlFlow.html)\n  (if-statements, switch-statements, loops, etc). This restriction will be\n  lifted soon.\n  \n## `@differentiable` declaration attribute\n\n### Basics\n\nThe `@differentiable` attribute can also be applied to function declarations.\n`@differentiable` marks a function as being differentiable with respect to some\nparameters (the varying parameters, explained below). `@differentiable` requires\nthe types of the varying parameters and the function result type to all conform\nto the `Differentiable` protocol.\n\nThis annotation does not change the declaration to have a `@differentiable`\nfunction type; instead, it triggers differentiation by the compiler on the\nfunction. If differentiation succeeds, then conversion of the function to a\n`@differentiable` function is guaranteed to succeed later.\n\nYou may wonder about the purpose of the `@differentiable` declaration attribute,\ngiven that non-differentiable functions can implicitly be converted to\n`@differentiable` functions, as mentioned above. The main reason is that the\n`@differentiable` declaration attribute is a contract for differentiability: if\na function is declared with `@differentiable` and it compiles, then it is always\nguaranteed to be differentiable, even in other modules. On the other hand, if a\nfunction is not declared with `@differentiable`, then differentiation of the\nfunction in other modules will fail.\n\nThis is why floating-point operations in the standard library are declared with\n`@differentiable`:\n\n```swift\nextension Float {\n    @differentiable\n    public static func + (lhs: Float, rhs: Float) -> Float { ... }\n}\n```\n\nBesides function declarations, there are a few other function-like declarations\nthat can be marked with `@differentiable`:\n- Computed property getters. (This requires both the type defining the property\n  and the type of the property to conform to `Differentiable`.)\n- Initializers. (This requires the type defining the initializer to conform to\n  `Differentiable`.)\n\nFor instance methods defined on types that conform to `Differentiable`, the\n`self` property can be marked as a varying parameter. Derivatives of these\nmethods return the partial derivative with respect to `self`. For these methods,\n`@differentiable` infers `self` as a varying parameter by default.\n\n```swift\nstruct Vector: Differentiable, VectorNumeric {\n    var x, y: Float\n\n    // Differentiable computed property.\n    @differentiable // Implicitly: @differentiable(wrt: self)\n    var magnitude: Float {\n        return (x * x + y * y).squareRoot()\n    }\n\n    // Differentiable initializer.\n    @differentiable // Implicitly: @differentiable(wrt: (x, y))\n    init(x: Float, y: Float) {\n        self.x = x\n        self.y = y\n    }\n}\n\nlet v = Vector(x: 2, y: 2)\nprint(v.magnitude)\n// 2.828427\nprint(gradient(at: v) { v in v.magnitude })\n// Vector(x: 64.0, y: 64.0)\n```\n\n### Differentiating with respect to\n\nMathematically, let \"varying parameters\" refer to the parameters (i.e.\nindependent variables) of a differentiable function whose partial derivatives\nare computed by the function's derivative.\n\nBy default, the `@differentiable` attribute infers all function parameters that\nconform to `Differentiable` to be the varying parameters. However, this is not\nalways desirable. To explicitly declare functions as differentiable with respect\nto a subset of parameters, explicitly specify the varying parameters using the\n`@differentiable(wrt: ...)` syntax.\n\nHere's an example of a 2-D convolution operation, adapted from the TensorFlow\nlibrary. The convolution input and filter are the varying parameters; strides\nand padding are not.\n\n```swift\n@differentiable(wrt: (input, filter))\nfunc conv2d(input: Tensor<Float>, filter: Tensor<Float>, strides: (Int, Int), padding: Padding) {\n    ...\n}\n```\n\nFunctions can have multiple `@differentiable` attributes with differentiable\n`wrt` parameter lists. `@differentiable` protocol requirements If a protocol\nrequirement is marked with `@differentiable`, all implementations of the\nrequirement are required to specify the same attribute. This enables generic\ncode using differentiation defined in terms of protocol requirements.\n\nHere is an example of a neural network `Layer` protocol that defines a\n`@differentiable` required method called `applied(to:)`. As shown, the\n`applied(to:)` method can be differentiated in a `Layer` protocol extension,\neven though it is not a concrete method.\n\n```swift\nimport TensorFlow\n\n/// A neural network layer.\nprotocol Layer: Differentiable {\n    /// The input type of the layer.\n    associatedtype Input: Differentiable\n    /// The output type of the layer.\n    associatedtype Output: Differentiable\n    /// Returns the output obtained from applying the layer to the given input.\n    @differentiable\n    func applied(to input: Input) -> Output\n}\n\nextension Layer {\n    /// Returns the inference output and the backpropagation function obtained from applying the\n    /// layer to the given input.\n    ///\n    /// - Parameter input: The input to the layer.\n    /// - Returns: A tuple containing the output and the backpropagation function. The\n    ///   backpropagation function (a.k.a. backpropagator) takes a direction vector and returns the\n    ///   gradients at the layer and at the input, respectively.\n    func appliedForBackpropagation(to input: Input)\n        -> (output: Output,\n            backpropagator: (_ direction: Output.CotangentVector)\n                -> (layerGradient: CotangentVector, inputGradient: Input.CotangentVector)) {\n        let (out, pullback) = valueWithPullback(at: input) { layer, input in\n            return layer.applied(to: input)\n        }\n        return (out, pullback)\n    }\n}\n\n// Example neural network layer.\nstruct DenseLayer: Layer {\n    var weight: Tensor<Float>\n    var bias: Tensor<Float>\n\n    @differentiable\n    func applied(to input: Tensor<Float>) -> Tensor<Float> {\n        return matmul(input, weight) + bias\n    }\n}\n\n// Example usage of `appliedForBackpropagation(to:)`.\nlet dense = DenseLayer(weight: [[1, 1], [1, 1]], bias: [1, 1])\nlet input: Tensor<Float> = [[3, 3]]\nlet seed: Tensor<Float> = [[1, 1]]\n\nlet (output, backprop) = dense.appliedForBackpropagation(to: input)\nlet (𝛁dense, 𝛁input) = backprop(seed)\n\ndump(𝛁dense)\n// ▿ DenseLayer.AllDifferentiableVariables\n//   - weight: [[3.0, 3.0], [3.0, 3.0]]\n//   - bias: [1.0, 1.0]\nprint(𝛁input)\n// [[2.0, 2.0]]\n```\n\n## Providing a custom derivative\n\nUse the `@differentiating` attribute to mark a function as a custom derivative\nfor another function. This is useful for registering derivatives for primitive\noperations.\n\nNote: currently, the `@differentiating` attribute can only be used to define\nderivatives for functions in the same module. We plan to lift this limitation\nsoon so that derivatives can be retroactively declared for functions in other\nmodules - [see this forum\ndiscussion](https://forums.swift.org/t/help-needed-with-retroactive-differentiability/19927)\nfor more information.\n\n```swift\nimport Darwin\n\nfunc sillyExp(_ x: Float) -> Float {\n    let 𝑒 = Float(M_E)\n    print(\"Taking 𝑒(\\(𝑒)) to the power of \\(x)!\")\n    return pow(𝑒, x)\n}\n\n@differentiating(sillyExp)\nfunc sillyDerivative(_ x: Float) -> (value: Float, pullback: (Float) -> Float) {\n    let y = sillyExp(x)\n    return (value: y, pullback: { v in v * y })\n}\n\nprint(gradient(of: sillyExp)(3))\n// Taking 𝑒(2.7182817) to the power of 3.0!\n// 20.085535\n```\n\n## Constructing a `@differentiable` function from a derivative\n\nGiven a function and its derivative, it is possible to construct a\n`@differentiable` version of the function using the\n[`differentiableFunction(from:)`] helper function defined in the standard\nlibrary.\n\nHere's an example:\n```swift\nlet multiply: @differentiable (Float, Float) -> Float =\n    differentiableFunction(from: { x, y in (value: x * y, pullback: { v in (v * y, v * x) }) })\n```\n\nInternally, `differentiableFunction(from:)` is defined just using the\n`@differentiating` attribute - there's no extra magic:\n\n```swift\n/// Returns a differentiable function given its derivative.\npublic func differentiableFunction<T: Differentiable, R: Differentiable>(\n    from vjp: @escaping (T) -> (value: R, pullback: (R.CotangentVector) -> T.CotangentVector)\n) -> @differentiable (T) -> R {\n    func original(_ x: T) -> R {\n        return vjp(x).value\n    }\n    @differentiating(original)\n    func derivative(_ x: T) -> (value: R, pullback: (R.CotangentVector) -> T.CotangentVector) {\n        return vjp(x)\n    }\n    return original\n}\n```\n\n## `@differentiable` functions and automatic differentiation\n\nAutomatic differentiation is the technique used by the compiler to automatically\ncompute function derivatives. This document does not go into detail about\nautomatic differentiation - but with an understanding of `@differentiable`\nfunctions and differentiation APIs, one can get a glimpse of how automatic\ndifferentiation works.\n\nThe key differentiation API is the [`valueWithPullback`] function, which takes a\n`@differentiable` function and arguments and returns two things: the result of\nthe function when applied to arguments, and a backpropagation function called a\n\"pullback\", which takes the gradient of the result and returns the gradient of\nthe arguments.\n\nLet's consider the following function `foo`:\n\n```swift\nfunc foo(_ x: Float) -> Float {\n    let double = x + x\n    let result = double * double\n    return result\n}\n```\n\nConceptually, here's how the compiler computes `valueWithPullback` for `foo`:\n\n```swift\nfunc fooValueWithPullback(_ x: Float) -> (value: Float, pullback: (Float) -> Float) {\n    // Replace function calls in `foo` with calls to `valueWithPullback`.\n    // Keep track of pullbacks and use them to compute pullback of `foo`.\n    let (double, doublePullback) =\n        valueWithPullback(at: x, x, in: (+) as @differentiable (Float, Float) -> Float)\n    let (result, resultPullback) =\n        valueWithPullback(at: double, double, in: (*) as @differentiable (Float, Float) -> Float)\n    let pullback: (Float) -> Float = { v in\n        let (𝛁result1, 𝛁result2) = resultPullback(v)\n        let (𝛁double1, 𝛁double2) = doublePullback(𝛁result1 + 𝛁result2)\n        return 𝛁double1 + 𝛁double2\n    }\n    return (value: result, pullback: pullback)\n}\n\n// Test.\nlet x: Float = 3.0\nlet (result, pullback) = fooValueWithPullback(x)\nprint(result) // 36.0\nprint(pullback(1)) // 24.0\n\n// Test the real `valueWithPullback` function.\ndo {\n    let (result, pullback) = valueWithPullback(at: x, in: foo)\n    print(result) // 36.0\n    print(pullback(1)) // 24.0\n}\n```\n\nAll other differentiation APIs are defined in terms of `valueWithPullback`.\nHere's an example for `gradient`:\n\n```swift\n// `gradient` returns the partial derivative with respect to varying parameters for scalar-result\n// functions. It simply returns `pullback(1)`.\nfunc fooGradient(_ x: Float) -> Float {",
    "repo_full_name": "tensorflow/swift",
    "discussion_comments": [
      {
        "comment_id": "266187899",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 145,
        "pr_file": "docs/DifferentiableFunctions.md",
        "discussion_id": "266187899",
        "commented_code": "@@ -0,0 +1,449 @@\n+# Differentiable functions and differentiation APIs\n+\n+[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n+\n+Last updated: March 2019\n+\n+## Introduction\n+\n+Swift supports differentiable functions as part of the language. The\n+`@differentiable` attribute appears in two locations in Swift syntax: as an\n+annotation on function types and as an annotation on function declarations (and\n+other similar declarations). This document explains the meaning of these\n+annotations.\n+\n+## `@differentiable` function type attribute\n+\n+### Basics\n+\n+In Swift, function types can have attributes. When a function type is annotated\n+with `@differentiable`, Swift guarantees that all values of that function type\n+can be differentiated.\n+\n+`@differentiable` functions can be called like normal functions, or be passed to\n+APIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\n+representation of a `@differentiable` function is a special data structure\n+containing the original function along with extra information required for\n+computing its derivatives. Usage `@differentiable` functions are a part of\n+Swift's type system. Most notably, they are used by differentiation APIs in the\n+standard library. Here are some examples demonstrating differentiation APIs:\n+\n+```swift\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// Free function examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(gradient(at: x, in: square)) // 6.0\n+// Computes the gradient of `square`, then applies it to `x`.\n+print(gradient(of: square)(x)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n+\n+// Method examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(x.gradient(in: square)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n+```\n+\n+Here's a list of differentiation APIs provided by the standard library:\n+\n+| Differentiation APIs  | Description                                                                                                                                                      |\n+|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n+| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n+| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n+| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n+| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n+| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n+\n+### Constructing `@differentiable` functions\n+\n+A value with a non-differentiable function type can be implicitly converted to\n+one with a corresponding `@differentiable` function type. In fact, this is what\n+happened in the example above:\n+\n+```swift\n+// `square` has type `(Float) -> Float`.\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n+// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n+// `@differentiable` type.\n+print(gradient(at: x, in: square)) // 6.0\n+```\n+\n+The implicit conversion from a value with type `(T) -> U` to a value with type\n+`@differentiable (T) -> U` actually triggers differentiation by the compiler.\n+Thus, differentiation is type-driven.\n+\n+If differentiation succeeds, the `@differentiable (T) -> U` value is\n+constructed. If differentiation fails, the compiler emits a compile-time error:\n+\n+```swift\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+    // The `Int` initializer call below is non-differentiable.\n+    Float(Int(x + y))\n+}\n+```\n+\n+```console\n+test.swift:1:52: error: function is not differentiable\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+                                                   ^~~~~~~~~\n+test.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n+    Float(Int(x + y))\n+          ^\n+```\n+\n+There are a few reasons why differentiation can fail:\n+* The function to differentiate contains computation, from parameters to result,\n+  that cannot be differentiated.\n+* The function to differentiate is opaque, i.e. it is a function parameter with\n+  a non-`@differentiable` function type.\n+* The function to differentiate is defined in another module.\n+* The function to differentiate uses [control\n+  flow](https://docs.swift.org/swift-book/LanguageGuide/ControlFlow.html)\n+  (if-statements, switch-statements, loops, etc). This restriction will be\n+  lifted soon.\n+  \n+## `@differentiable` declaration attribute\n+\n+### Basics\n+\n+The `@differentiable` attribute can also be applied to function declarations.\n+`@differentiable` marks a function as being differentiable with respect to some\n+parameters (the varying parameters, explained below). `@differentiable` requires\n+the types of the varying parameters and the function result type to all conform\n+to the `Differentiable` protocol.\n+\n+This annotation does not change the declaration to have a `@differentiable`\n+function type; instead, it triggers differentiation by the compiler on the\n+function. If differentiation succeeds, then conversion of the function to a\n+`@differentiable` function is guaranteed to succeed later.\n+\n+You may wonder about the purpose of the `@differentiable` declaration attribute,\n+given that non-differentiable functions can implicitly be converted to\n+`@differentiable` functions, as mentioned above. The main reason is that the\n+`@differentiable` declaration attribute is a contract for differentiability: if\n+a function is declared with `@differentiable` and it compiles, then it is always\n+guaranteed to be differentiable, even in other modules. On the other hand, if a\n+function is not declared with `@differentiable`, then differentiation of the\n+function in other modules will fail.\n+\n+This is why floating-point operations in the standard library are declared with\n+`@differentiable`:\n+\n+```swift\n+extension Float {\n+    @differentiable\n+    public static func + (lhs: Float, rhs: Float) -> Float { ... }\n+}\n+```\n+\n+Besides function declarations, there are a few other function-like declarations\n+that can be marked with `@differentiable`:\n+- Computed property getters. (This requires both the type defining the property\n+  and the type of the property to conform to `Differentiable`.)\n+- Initializers. (This requires the type defining the initializer to conform to\n+  `Differentiable`.)\n+\n+For instance methods defined on types that conform to `Differentiable`, the\n+`self` property can be marked as a varying parameter. Derivatives of these\n+methods return the partial derivative with respect to `self`. For these methods,\n+`@differentiable` infers `self` as a varying parameter by default.\n+\n+```swift\n+struct Vector: Differentiable, VectorNumeric {\n+    var x, y: Float\n+\n+    // Differentiable computed property.\n+    @differentiable // Implicitly: @differentiable(wrt: self)\n+    var magnitude: Float {\n+        return (x * x + y * y).squareRoot()\n+    }\n+\n+    // Differentiable initializer.\n+    @differentiable // Implicitly: @differentiable(wrt: (x, y))\n+    init(x: Float, y: Float) {\n+        self.x = x\n+        self.y = y\n+    }\n+}\n+\n+let v = Vector(x: 2, y: 2)\n+print(v.magnitude)\n+// 2.828427\n+print(gradient(at: v) { v in v.magnitude })\n+// Vector(x: 64.0, y: 64.0)\n+```\n+\n+### Differentiating with respect to\n+\n+Mathematically, let \"varying parameters\" refer to the parameters (i.e.\n+independent variables) of a differentiable function whose partial derivatives\n+are computed by the function's derivative.\n+\n+By default, the `@differentiable` attribute infers all function parameters that\n+conform to `Differentiable` to be the varying parameters. However, this is not\n+always desirable. To explicitly declare functions as differentiable with respect\n+to a subset of parameters, explicitly specify the varying parameters using the\n+`@differentiable(wrt: ...)` syntax.\n+\n+Here's an example of a 2-D convolution operation, adapted from the TensorFlow\n+library. The convolution input and filter are the varying parameters; strides\n+and padding are not.\n+\n+```swift\n+@differentiable(wrt: (input, filter))\n+func conv2d(input: Tensor<Float>, filter: Tensor<Float>, strides: (Int, Int), padding: Padding) {\n+    ...\n+}\n+```\n+\n+Functions can have multiple `@differentiable` attributes with differentiable\n+`wrt` parameter lists. `@differentiable` protocol requirements If a protocol\n+requirement is marked with `@differentiable`, all implementations of the\n+requirement are required to specify the same attribute. This enables generic\n+code using differentiation defined in terms of protocol requirements.\n+\n+Here is an example of a neural network `Layer` protocol that defines a\n+`@differentiable` required method called `applied(to:)`. As shown, the\n+`applied(to:)` method can be differentiated in a `Layer` protocol extension,\n+even though it is not a concrete method.\n+\n+```swift\n+import TensorFlow\n+\n+/// A neural network layer.\n+protocol Layer: Differentiable {\n+    /// The input type of the layer.\n+    associatedtype Input: Differentiable\n+    /// The output type of the layer.\n+    associatedtype Output: Differentiable\n+    /// Returns the output obtained from applying the layer to the given input.\n+    @differentiable\n+    func applied(to input: Input) -> Output\n+}\n+\n+extension Layer {\n+    /// Returns the inference output and the backpropagation function obtained from applying the\n+    /// layer to the given input.\n+    ///\n+    /// - Parameter input: The input to the layer.\n+    /// - Returns: A tuple containing the output and the backpropagation function. The\n+    ///   backpropagation function (a.k.a. backpropagator) takes a direction vector and returns the\n+    ///   gradients at the layer and at the input, respectively.\n+    func appliedForBackpropagation(to input: Input)\n+        -> (output: Output,\n+            backpropagator: (_ direction: Output.CotangentVector)\n+                -> (layerGradient: CotangentVector, inputGradient: Input.CotangentVector)) {\n+        let (out, pullback) = valueWithPullback(at: input) { layer, input in\n+            return layer.applied(to: input)\n+        }\n+        return (out, pullback)\n+    }\n+}\n+\n+// Example neural network layer.\n+struct DenseLayer: Layer {\n+    var weight: Tensor<Float>\n+    var bias: Tensor<Float>\n+\n+    @differentiable\n+    func applied(to input: Tensor<Float>) -> Tensor<Float> {\n+        return matmul(input, weight) + bias\n+    }\n+}\n+\n+// Example usage of `appliedForBackpropagation(to:)`.\n+let dense = DenseLayer(weight: [[1, 1], [1, 1]], bias: [1, 1])\n+let input: Tensor<Float> = [[3, 3]]\n+let seed: Tensor<Float> = [[1, 1]]\n+\n+let (output, backprop) = dense.appliedForBackpropagation(to: input)\n+let (𝛁dense, 𝛁input) = backprop(seed)\n+\n+dump(𝛁dense)\n+// ▿ DenseLayer.AllDifferentiableVariables\n+//   - weight: [[3.0, 3.0], [3.0, 3.0]]\n+//   - bias: [1.0, 1.0]\n+print(𝛁input)\n+// [[2.0, 2.0]]\n+```\n+\n+## Providing a custom derivative\n+\n+Use the `@differentiating` attribute to mark a function as a custom derivative\n+for another function. This is useful for registering derivatives for primitive\n+operations.\n+\n+Note: currently, the `@differentiating` attribute can only be used to define\n+derivatives for functions in the same module. We plan to lift this limitation\n+soon so that derivatives can be retroactively declared for functions in other\n+modules - [see this forum\n+discussion](https://forums.swift.org/t/help-needed-with-retroactive-differentiability/19927)\n+for more information.\n+\n+```swift\n+import Darwin\n+\n+func sillyExp(_ x: Float) -> Float {\n+    let 𝑒 = Float(M_E)\n+    print(\"Taking 𝑒(\\(𝑒)) to the power of \\(x)!\")\n+    return pow(𝑒, x)\n+}\n+\n+@differentiating(sillyExp)\n+func sillyDerivative(_ x: Float) -> (value: Float, pullback: (Float) -> Float) {\n+    let y = sillyExp(x)\n+    return (value: y, pullback: { v in v * y })\n+}\n+\n+print(gradient(of: sillyExp)(3))\n+// Taking 𝑒(2.7182817) to the power of 3.0!\n+// 20.085535\n+```\n+\n+## Constructing a `@differentiable` function from a derivative\n+\n+Given a function and its derivative, it is possible to construct a\n+`@differentiable` version of the function using the\n+[`differentiableFunction(from:)`] helper function defined in the standard\n+library.\n+\n+Here's an example:\n+```swift\n+let multiply: @differentiable (Float, Float) -> Float =\n+    differentiableFunction(from: { x, y in (value: x * y, pullback: { v in (v * y, v * x) }) })\n+```\n+\n+Internally, `differentiableFunction(from:)` is defined just using the\n+`@differentiating` attribute - there's no extra magic:\n+\n+```swift\n+/// Returns a differentiable function given its derivative.\n+public func differentiableFunction<T: Differentiable, R: Differentiable>(\n+    from vjp: @escaping (T) -> (value: R, pullback: (R.CotangentVector) -> T.CotangentVector)\n+) -> @differentiable (T) -> R {\n+    func original(_ x: T) -> R {\n+        return vjp(x).value\n+    }\n+    @differentiating(original)\n+    func derivative(_ x: T) -> (value: R, pullback: (R.CotangentVector) -> T.CotangentVector) {\n+        return vjp(x)\n+    }\n+    return original\n+}\n+```\n+\n+## `@differentiable` functions and automatic differentiation\n+\n+Automatic differentiation is the technique used by the compiler to automatically\n+compute function derivatives. This document does not go into detail about\n+automatic differentiation - but with an understanding of `@differentiable`\n+functions and differentiation APIs, one can get a glimpse of how automatic\n+differentiation works.\n+\n+The key differentiation API is the [`valueWithPullback`] function, which takes a\n+`@differentiable` function and arguments and returns two things: the result of\n+the function when applied to arguments, and a backpropagation function called a\n+\"pullback\", which takes the gradient of the result and returns the gradient of\n+the arguments.\n+\n+Let's consider the following function `foo`:\n+\n+```swift\n+func foo(_ x: Float) -> Float {\n+    let double = x + x\n+    let result = double * double\n+    return result\n+}\n+```\n+\n+Conceptually, here's how the compiler computes `valueWithPullback` for `foo`:\n+\n+```swift\n+func fooValueWithPullback(_ x: Float) -> (value: Float, pullback: (Float) -> Float) {\n+    // Replace function calls in `foo` with calls to `valueWithPullback`.\n+    // Keep track of pullbacks and use them to compute pullback of `foo`.\n+    let (double, doublePullback) =\n+        valueWithPullback(at: x, x, in: (+) as @differentiable (Float, Float) -> Float)\n+    let (result, resultPullback) =\n+        valueWithPullback(at: double, double, in: (*) as @differentiable (Float, Float) -> Float)\n+    let pullback: (Float) -> Float = { v in\n+        let (𝛁result1, 𝛁result2) = resultPullback(v)\n+        let (𝛁double1, 𝛁double2) = doublePullback(𝛁result1 + 𝛁result2)\n+        return 𝛁double1 + 𝛁double2\n+    }\n+    return (value: result, pullback: pullback)\n+}\n+\n+// Test.\n+let x: Float = 3.0\n+let (result, pullback) = fooValueWithPullback(x)\n+print(result) // 36.0\n+print(pullback(1)) // 24.0\n+\n+// Test the real `valueWithPullback` function.\n+do {\n+    let (result, pullback) = valueWithPullback(at: x, in: foo)\n+    print(result) // 36.0\n+    print(pullback(1)) // 24.0\n+}\n+```\n+\n+All other differentiation APIs are defined in terms of `valueWithPullback`.\n+Here's an example for `gradient`:\n+\n+```swift\n+// `gradient` returns the partial derivative with respect to varying parameters for scalar-result\n+// functions. It simply returns `pullback(1)`.\n+func fooGradient(_ x: Float) -> Float {",
        "comment_created_at": "2019-03-16T03:46:25+00:00",
        "comment_author": "rxwei",
        "comment_body": "The fact that 1) this is named something other than `gradient` and 2) this is not a differentiation API is a bit misleading. I'd suggest showing a completely generic `gradient(of:)` implementation here, and provide a link to the standard library implementation.",
        "pr_file_module": null
      },
      {
        "comment_id": "266189732",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 145,
        "pr_file": "docs/DifferentiableFunctions.md",
        "discussion_id": "266187899",
        "commented_code": "@@ -0,0 +1,449 @@\n+# Differentiable functions and differentiation APIs\n+\n+[Richard Wei], [Dan Zheng], [Marc Rasi], [Parker Schuh]\n+\n+Last updated: March 2019\n+\n+## Introduction\n+\n+Swift supports differentiable functions as part of the language. The\n+`@differentiable` attribute appears in two locations in Swift syntax: as an\n+annotation on function types and as an annotation on function declarations (and\n+other similar declarations). This document explains the meaning of these\n+annotations.\n+\n+## `@differentiable` function type attribute\n+\n+### Basics\n+\n+In Swift, function types can have attributes. When a function type is annotated\n+with `@differentiable`, Swift guarantees that all values of that function type\n+can be differentiated.\n+\n+`@differentiable` functions can be called like normal functions, or be passed to\n+APIs that take `@differentiable` functions, like [`gradient(of:)`]. The binary\n+representation of a `@differentiable` function is a special data structure\n+containing the original function along with extra information required for\n+computing its derivatives. Usage `@differentiable` functions are a part of\n+Swift's type system. Most notably, they are used by differentiation APIs in the\n+standard library. Here are some examples demonstrating differentiation APIs:\n+\n+```swift\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// Free function examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(gradient(at: x, in: square)) // 6.0\n+// Computes the gradient of `square`, then applies it to `x`.\n+print(gradient(of: square)(x)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(valueWithGradient(at: x, in: square)) // (value: 9.0, gradient: 6.0)\n+\n+// Method examples.\n+\n+// Computes the gradient of `square` at `x`.\n+print(x.gradient(in: square)) // 6.0\n+// Computes the value and gradient of `square` at `x`.\n+print(x.valueWithGradient(in: square)) // (value: 9.0, gradient: 6.0)\n+```\n+\n+Here's a list of differentiation APIs provided by the standard library:\n+\n+| Differentiation APIs  | Description                                                                                                                                                      |\n+|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| [`valueWithPullback(at:in:)`] <br/> [`valueWithPullback(at:_:in:)`] | Returns original result and backpropagation function. <br/> **Important note: this is the core differentiation API. All other APIs are defined in terms of `valueWithPullback`.** |\n+| [`pullback(at:in:)`] <br/> [`pullback(at:_:in:)`] | Returns backpropagation function. |\n+| [`gradient(at:in:)`] <br/> [`gradient(at:_:in:)`] | Returns partial derivatives with respect to arguments. |\n+| [`valueWithGradient(at:in:)`] <br/> [`valueWithGradient(at:_:in:)`] | Returns original result and partial derivatives with respect to arguments. |\n+| [`gradient(of:)`] <br/> [`gradient(of:)` (arity 2)] | Returns gradient function. |\n+| [`valueWithGradient(of:)`] <br/> [`valueWithGradient(of:)` (arity 2)] | Returns gradient function. |\n+\n+### Constructing `@differentiable` functions\n+\n+A value with a non-differentiable function type can be implicitly converted to\n+one with a corresponding `@differentiable` function type. In fact, this is what\n+happened in the example above:\n+\n+```swift\n+// `square` has type `(Float) -> Float`.\n+func square(_ x: Float) -> Float {\n+    return x * x\n+}\n+let x: Float = 3.0\n+\n+// The second argument of `gradient(at:in:)` has type `@differentiable (Float) -> Float`.\n+// When `square` is passed to `gradient(at:in:)`, it is implicitly converted to a value with the\n+// `@differentiable` type.\n+print(gradient(at: x, in: square)) // 6.0\n+```\n+\n+The implicit conversion from a value with type `(T) -> U` to a value with type\n+`@differentiable (T) -> U` actually triggers differentiation by the compiler.\n+Thus, differentiation is type-driven.\n+\n+If differentiation succeeds, the `@differentiable (T) -> U` value is\n+constructed. If differentiation fails, the compiler emits a compile-time error:\n+\n+```swift\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+    // The `Int` initializer call below is non-differentiable.\n+    Float(Int(x + y))\n+}\n+```\n+\n+```console\n+test.swift:1:52: error: function is not differentiable\n+let add: @differentiable (Float, Float) -> Float = { x, y in\n+                                                   ^~~~~~~~~\n+test.swift:2:11: note: cannot differentiate through a non-differentiable result; do you want to add '.withoutDerivative()'?\n+    Float(Int(x + y))\n+          ^\n+```\n+\n+There are a few reasons why differentiation can fail:\n+* The function to differentiate contains computation, from parameters to result,\n+  that cannot be differentiated.\n+* The function to differentiate is opaque, i.e. it is a function parameter with\n+  a non-`@differentiable` function type.\n+* The function to differentiate is defined in another module.\n+* The function to differentiate uses [control\n+  flow](https://docs.swift.org/swift-book/LanguageGuide/ControlFlow.html)\n+  (if-statements, switch-statements, loops, etc). This restriction will be\n+  lifted soon.\n+  \n+## `@differentiable` declaration attribute\n+\n+### Basics\n+\n+The `@differentiable` attribute can also be applied to function declarations.\n+`@differentiable` marks a function as being differentiable with respect to some\n+parameters (the varying parameters, explained below). `@differentiable` requires\n+the types of the varying parameters and the function result type to all conform\n+to the `Differentiable` protocol.\n+\n+This annotation does not change the declaration to have a `@differentiable`\n+function type; instead, it triggers differentiation by the compiler on the\n+function. If differentiation succeeds, then conversion of the function to a\n+`@differentiable` function is guaranteed to succeed later.\n+\n+You may wonder about the purpose of the `@differentiable` declaration attribute,\n+given that non-differentiable functions can implicitly be converted to\n+`@differentiable` functions, as mentioned above. The main reason is that the\n+`@differentiable` declaration attribute is a contract for differentiability: if\n+a function is declared with `@differentiable` and it compiles, then it is always\n+guaranteed to be differentiable, even in other modules. On the other hand, if a\n+function is not declared with `@differentiable`, then differentiation of the\n+function in other modules will fail.\n+\n+This is why floating-point operations in the standard library are declared with\n+`@differentiable`:\n+\n+```swift\n+extension Float {\n+    @differentiable\n+    public static func + (lhs: Float, rhs: Float) -> Float { ... }\n+}\n+```\n+\n+Besides function declarations, there are a few other function-like declarations\n+that can be marked with `@differentiable`:\n+- Computed property getters. (This requires both the type defining the property\n+  and the type of the property to conform to `Differentiable`.)\n+- Initializers. (This requires the type defining the initializer to conform to\n+  `Differentiable`.)\n+\n+For instance methods defined on types that conform to `Differentiable`, the\n+`self` property can be marked as a varying parameter. Derivatives of these\n+methods return the partial derivative with respect to `self`. For these methods,\n+`@differentiable` infers `self` as a varying parameter by default.\n+\n+```swift\n+struct Vector: Differentiable, VectorNumeric {\n+    var x, y: Float\n+\n+    // Differentiable computed property.\n+    @differentiable // Implicitly: @differentiable(wrt: self)\n+    var magnitude: Float {\n+        return (x * x + y * y).squareRoot()\n+    }\n+\n+    // Differentiable initializer.\n+    @differentiable // Implicitly: @differentiable(wrt: (x, y))\n+    init(x: Float, y: Float) {\n+        self.x = x\n+        self.y = y\n+    }\n+}\n+\n+let v = Vector(x: 2, y: 2)\n+print(v.magnitude)\n+// 2.828427\n+print(gradient(at: v) { v in v.magnitude })\n+// Vector(x: 64.0, y: 64.0)\n+```\n+\n+### Differentiating with respect to\n+\n+Mathematically, let \"varying parameters\" refer to the parameters (i.e.\n+independent variables) of a differentiable function whose partial derivatives\n+are computed by the function's derivative.\n+\n+By default, the `@differentiable` attribute infers all function parameters that\n+conform to `Differentiable` to be the varying parameters. However, this is not\n+always desirable. To explicitly declare functions as differentiable with respect\n+to a subset of parameters, explicitly specify the varying parameters using the\n+`@differentiable(wrt: ...)` syntax.\n+\n+Here's an example of a 2-D convolution operation, adapted from the TensorFlow\n+library. The convolution input and filter are the varying parameters; strides\n+and padding are not.\n+\n+```swift\n+@differentiable(wrt: (input, filter))\n+func conv2d(input: Tensor<Float>, filter: Tensor<Float>, strides: (Int, Int), padding: Padding) {\n+    ...\n+}\n+```\n+\n+Functions can have multiple `@differentiable` attributes with differentiable\n+`wrt` parameter lists. `@differentiable` protocol requirements If a protocol\n+requirement is marked with `@differentiable`, all implementations of the\n+requirement are required to specify the same attribute. This enables generic\n+code using differentiation defined in terms of protocol requirements.\n+\n+Here is an example of a neural network `Layer` protocol that defines a\n+`@differentiable` required method called `applied(to:)`. As shown, the\n+`applied(to:)` method can be differentiated in a `Layer` protocol extension,\n+even though it is not a concrete method.\n+\n+```swift\n+import TensorFlow\n+\n+/// A neural network layer.\n+protocol Layer: Differentiable {\n+    /// The input type of the layer.\n+    associatedtype Input: Differentiable\n+    /// The output type of the layer.\n+    associatedtype Output: Differentiable\n+    /// Returns the output obtained from applying the layer to the given input.\n+    @differentiable\n+    func applied(to input: Input) -> Output\n+}\n+\n+extension Layer {\n+    /// Returns the inference output and the backpropagation function obtained from applying the\n+    /// layer to the given input.\n+    ///\n+    /// - Parameter input: The input to the layer.\n+    /// - Returns: A tuple containing the output and the backpropagation function. The\n+    ///   backpropagation function (a.k.a. backpropagator) takes a direction vector and returns the\n+    ///   gradients at the layer and at the input, respectively.\n+    func appliedForBackpropagation(to input: Input)\n+        -> (output: Output,\n+            backpropagator: (_ direction: Output.CotangentVector)\n+                -> (layerGradient: CotangentVector, inputGradient: Input.CotangentVector)) {\n+        let (out, pullback) = valueWithPullback(at: input) { layer, input in\n+            return layer.applied(to: input)\n+        }\n+        return (out, pullback)\n+    }\n+}\n+\n+// Example neural network layer.\n+struct DenseLayer: Layer {\n+    var weight: Tensor<Float>\n+    var bias: Tensor<Float>\n+\n+    @differentiable\n+    func applied(to input: Tensor<Float>) -> Tensor<Float> {\n+        return matmul(input, weight) + bias\n+    }\n+}\n+\n+// Example usage of `appliedForBackpropagation(to:)`.\n+let dense = DenseLayer(weight: [[1, 1], [1, 1]], bias: [1, 1])\n+let input: Tensor<Float> = [[3, 3]]\n+let seed: Tensor<Float> = [[1, 1]]\n+\n+let (output, backprop) = dense.appliedForBackpropagation(to: input)\n+let (𝛁dense, 𝛁input) = backprop(seed)\n+\n+dump(𝛁dense)\n+// ▿ DenseLayer.AllDifferentiableVariables\n+//   - weight: [[3.0, 3.0], [3.0, 3.0]]\n+//   - bias: [1.0, 1.0]\n+print(𝛁input)\n+// [[2.0, 2.0]]\n+```\n+\n+## Providing a custom derivative\n+\n+Use the `@differentiating` attribute to mark a function as a custom derivative\n+for another function. This is useful for registering derivatives for primitive\n+operations.\n+\n+Note: currently, the `@differentiating` attribute can only be used to define\n+derivatives for functions in the same module. We plan to lift this limitation\n+soon so that derivatives can be retroactively declared for functions in other\n+modules - [see this forum\n+discussion](https://forums.swift.org/t/help-needed-with-retroactive-differentiability/19927)\n+for more information.\n+\n+```swift\n+import Darwin\n+\n+func sillyExp(_ x: Float) -> Float {\n+    let 𝑒 = Float(M_E)\n+    print(\"Taking 𝑒(\\(𝑒)) to the power of \\(x)!\")\n+    return pow(𝑒, x)\n+}\n+\n+@differentiating(sillyExp)\n+func sillyDerivative(_ x: Float) -> (value: Float, pullback: (Float) -> Float) {\n+    let y = sillyExp(x)\n+    return (value: y, pullback: { v in v * y })\n+}\n+\n+print(gradient(of: sillyExp)(3))\n+// Taking 𝑒(2.7182817) to the power of 3.0!\n+// 20.085535\n+```\n+\n+## Constructing a `@differentiable` function from a derivative\n+\n+Given a function and its derivative, it is possible to construct a\n+`@differentiable` version of the function using the\n+[`differentiableFunction(from:)`] helper function defined in the standard\n+library.\n+\n+Here's an example:\n+```swift\n+let multiply: @differentiable (Float, Float) -> Float =\n+    differentiableFunction(from: { x, y in (value: x * y, pullback: { v in (v * y, v * x) }) })\n+```\n+\n+Internally, `differentiableFunction(from:)` is defined just using the\n+`@differentiating` attribute - there's no extra magic:\n+\n+```swift\n+/// Returns a differentiable function given its derivative.\n+public func differentiableFunction<T: Differentiable, R: Differentiable>(\n+    from vjp: @escaping (T) -> (value: R, pullback: (R.CotangentVector) -> T.CotangentVector)\n+) -> @differentiable (T) -> R {\n+    func original(_ x: T) -> R {\n+        return vjp(x).value\n+    }\n+    @differentiating(original)\n+    func derivative(_ x: T) -> (value: R, pullback: (R.CotangentVector) -> T.CotangentVector) {\n+        return vjp(x)\n+    }\n+    return original\n+}\n+```\n+\n+## `@differentiable` functions and automatic differentiation\n+\n+Automatic differentiation is the technique used by the compiler to automatically\n+compute function derivatives. This document does not go into detail about\n+automatic differentiation - but with an understanding of `@differentiable`\n+functions and differentiation APIs, one can get a glimpse of how automatic\n+differentiation works.\n+\n+The key differentiation API is the [`valueWithPullback`] function, which takes a\n+`@differentiable` function and arguments and returns two things: the result of\n+the function when applied to arguments, and a backpropagation function called a\n+\"pullback\", which takes the gradient of the result and returns the gradient of\n+the arguments.\n+\n+Let's consider the following function `foo`:\n+\n+```swift\n+func foo(_ x: Float) -> Float {\n+    let double = x + x\n+    let result = double * double\n+    return result\n+}\n+```\n+\n+Conceptually, here's how the compiler computes `valueWithPullback` for `foo`:\n+\n+```swift\n+func fooValueWithPullback(_ x: Float) -> (value: Float, pullback: (Float) -> Float) {\n+    // Replace function calls in `foo` with calls to `valueWithPullback`.\n+    // Keep track of pullbacks and use them to compute pullback of `foo`.\n+    let (double, doublePullback) =\n+        valueWithPullback(at: x, x, in: (+) as @differentiable (Float, Float) -> Float)\n+    let (result, resultPullback) =\n+        valueWithPullback(at: double, double, in: (*) as @differentiable (Float, Float) -> Float)\n+    let pullback: (Float) -> Float = { v in\n+        let (𝛁result1, 𝛁result2) = resultPullback(v)\n+        let (𝛁double1, 𝛁double2) = doublePullback(𝛁result1 + 𝛁result2)\n+        return 𝛁double1 + 𝛁double2\n+    }\n+    return (value: result, pullback: pullback)\n+}\n+\n+// Test.\n+let x: Float = 3.0\n+let (result, pullback) = fooValueWithPullback(x)\n+print(result) // 36.0\n+print(pullback(1)) // 24.0\n+\n+// Test the real `valueWithPullback` function.\n+do {\n+    let (result, pullback) = valueWithPullback(at: x, in: foo)\n+    print(result) // 36.0\n+    print(pullback(1)) // 24.0\n+}\n+```\n+\n+All other differentiation APIs are defined in terms of `valueWithPullback`.\n+Here's an example for `gradient`:\n+\n+```swift\n+// `gradient` returns the partial derivative with respect to varying parameters for scalar-result\n+// functions. It simply returns `pullback(1)`.\n+func fooGradient(_ x: Float) -> Float {",
        "comment_created_at": "2019-03-16T05:22:30+00:00",
        "comment_author": "dan-zheng",
        "comment_body": "I see - showing a generic `gradient(of:)` implementation using `valueWithPullback` is indeed more insightful.\r\n\r\nDone.",
        "pr_file_module": null
      }
    ]
  }
]