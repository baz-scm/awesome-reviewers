[
  {
    "discussion_id": "263667558",
    "pr_number": 137,
    "pr_file": "docs/DifferentiableTypes.md",
    "created_at": "2019-03-08T05:16:34+00:00",
    "commented_code": "# Differentiable types\n\nRichard Wei, Dan Zheng\n\nMarch 2019\n\n## Preface\n\nMathematically speaking, only functions are \"differentiable\": only functions have derivatives and can be differentiated. In this document, the terminology \"differentiable types\" is used as a shorthand for \"types that can be used as arguments and results of differentiable functions\". This notion is important because not all types are \"differentiable\" in this sense. For example, types representing real numbers and vector spaces are \"differentiable\", but strings and integers are not.\n\n## Introduction\n\nElementary calculus defines differentiation on real numbers: most people are familiar with this definition of \"differentiation\". However, differentiation is defined for many concepts across different branches of mathematics:\n* Scalar differentiation: differentiation on real numbers. This is taught in introductory calculus.\n* [Vector calculus]: a branch of mathematics that involves differentiation of vector fields.\n* [Differential geometry]: a branch of mathematics that involves differentiation of functions over manifolds.\n\nIn Swift, we want to build a general system for differentiation that can represent all of these cases. Differentiation should not be limited to functions over specific types (e.g. functions over floating-point numbers); it should also work with functions whose parameters/result are custom types.\n\nThis raises some questions: what kind of types can be used as arguments and results of differentiable functions, and how can we generalize them using a protocol?\n\n## Design overview\n\nThe `Differentiable` protocol generalizes all types that can be used as arguments and results of differentiable functions.\n\nThe compiler can automatically provide default implementations of `Differentiable` protocol requirements for struct types whose stored properties all conform to `Differentiable`.\n\nHere are some examples:\n\n```swift\nstruct Vector: Differentiable, VectorNumeric {\n    // The compiler synthesizes all `Differentiable` protocol requirements\n    // when all stored properties conform to `Differentiable`.\n    var x, y, z: Float\n}\n\n// Differential operators like `gradient(at:in:)` just work!\nlet v = Vector(x: 1, y: 2, z: 3)\nlet ùõÅv = gradient(at: v) { v in (v + v).x }\n\nprint(ùõÅv)\n// Vector(x: 2.0, y: 0.0, z: 0.0)\n```\n\nA `Differentiable`-conforming type may have stored properties that are not meant to have a derivative with respect to `self`. Use the `@noDerivative` attribute to mark those properties; they will not have a corresponding entry in the synthesized `TangentVector`, `CotangentVector`, and `AllDifferentiableVariables` struct types.\n\nHere‚Äôs an example deep learning layer with some `@noDerivative` properties:\n\n```swift\nstruct DenseLayer : Differentiable {\n    // These properties should have derivative values.\n    var weight: Tensor<Float>\n    var bias: Tensor<Float>\n\n    // These auxiliary properties should not have derivative values.\n    // Thus, they are marked with `@noDerivative`.\n    //\n    // `@noDerivative` properties do not have a corresponding entry\n    // in synthesized associated struct types.\n    @noDerivative var useBias: Bool = true\n    @noDerivative var previousWeight: Tensor<Float> = Tensor(0)\n\n    init(weight: Tensor<Float>, bias: Tensor<Float>) {\n        self.weight = weight\n        self.bias = bias\n    }\n\n    // The compiler synthesizes all `Differentiable` protocol requirements,\n    // adding only properties not marked with `@noDerivative` to associated\n    // tangent space types.\n\n    func applied(to input: Tensor<Float>) -> Tensor<Float> {\n        return matmul(input, weight) + bias\n    }\n}\n\n// Differential operators like `gradient(at:in:)` just work!\nlet dense = DenseLayer(weight: [[1, 1], [1, 1]], bias: [0, 0])\nlet ùõÅdense = gradient(at: dense) { dense in dense.applied(to: [[3, 3]]).sum() }\n\ndump(ùõÅdense)\n// ‚ñø DenseLayer.AllDifferentiableVariables\n//   - weight: [[3.0, 3.0], [3.0, 3.0]]\n//   - bias: [1.0, 1.0]\n```\n\n## Protocol details\n\nHere is the full `Differentiable` protocol definition. More explanation is provided below.\n\n```swift\n/// A type that mathematically represents a differentiable manifold whose\n/// tangent spaces are finite-dimensional.\npublic protocol Differentiable {\n    /// The tangent bundle of this differentiable manifold.\n    associatedtype TangentVector : AdditiveArithmetic & Differentiable\n        where TangentVector.TangentVector == TangentVector,\n              TangentVector.CotangentVector == CotangentVector,\n              TangentVector.AllDifferentiableVariables == TangentVector\n    /// The cotangent bundle of this differentiable manifold.\n    associatedtype CotangentVector : AdditiveArithmetic & Differentiable\n        where CotangentVector.TangentVector == CotangentVector,\n              CotangentVector.CotangentVector == TangentVector,\n              CotangentVector.AllDifferentiableVariables == CotangentVector\n    /// The type of all differentiable variables in this type.\n    associatedtype AllDifferentiableVariables : Differentiable\n        where AllDifferentiableVariables.AllDifferentiableVariables ==\n                  AllDifferentiableVariables,\n              AllDifferentiableVariables.TangentVector == TangentVector,\n              AllDifferentiableVariables.CotangentVector == CotangentVector\n\n    /// All differentiable variables in this type.\n    var allDifferentiableVariables: AllDifferentiableVariables { get }\n\n    /// Returns `self` moved along the value space towards the given tangent\n    /// vector. In Riemannian geometry (mathematics), this represents an\n    /// exponential map.\n    func moved(along direction: TangentVector) -> Self\n\n    /// Convert a cotangent vector to its corresponding tangent vector.\n    func tangentVector(from cotangent: CotangentVector) -> TangentVector\n}\n```\n\nMathematically, `Differentiable` represents a differentiable manifold: this is a technical term for smooth-surfaced objects like spheres and generalizes types that are compatible with differentiation, like `Float`, `Double`, `Tensor`, and `SIMD4<Float>`. This definition comes from differential geometry and is quite technical, and not all details are relevant for most use cases.\n\n<p align=\"center\">\n  <img src=\"https://upload.wikimedia.org/wikipedia/commons/3/37/Pushforward.svg\" align=center>\n  <br>\n  <sub>Image showing two differentiable manifolds: a sphere and a spheroid.</sub>\n  <br>\n  <sub>From https://en.wikipedia.org/wiki/Pushforward_(differential).</sub>\n</p>\n\nHere are the important points about the `Differentiable` protocol:\n* `associatedtype TangentVector`: represents the type of directional derivatives computed via forward-mode differentiation.\n* `associatedtype CotangentVector`: represents the type of gradient values computed via reverse-mode differentiation.\n* `CotangentVector` types are used and produced by differential operators like `gradient` and `pullback`.\n* `var allDifferentiableVariables: AllDifferentiableVariables`: represents all differentiable variables in an instance of the conforming type, where `AllDifferentiableVariables` is the type of all differentiable variables.\n  * The motivation/design behind this is related to key path iteration .\n* `TangentVector`, `CotangentVector`, and `AllDifferentiableVariables` are closely related.\n  * All three associated types must themselves conform to `Differentiable`.\n  * The `Differentiable` protocol associated types of the associated types themselves are defined to be mathematically correct.\n    * `Foo.TangentVector.TangentVector` is `Foo.TangentVector` itself.\n    * `Foo.CotangentVector.TangentVector` is `Foo.CotangentVector` itself.\n    * `Foo.TangentVector.CotangentVector` is `Foo.CotangentVector`.\n    * `Foo.CotangentVector.CotangentVector` is `Foo.TangentVector`.\n    * `Foo.AllDifferentiableVariables` has the same `TangentVector` and `CotangentVector` as `Foo`.\n  * Additionally, `TangentVector` and  `CotangentVector` must conform to `AdditiveArithmetic`, so that they can be zero-initialized and accumulated via addition. These are necessary to perform the chain rule of differentiation.\n* Manifold operations.\n  * These currently involve `func tangentVector(from cotangent: CotangentVector)` and `func moved(along cotangent: CotangentVector`). These operations can be useful for implementing manifold-related algorithms, like optimization on manifolds, but are not relevant for simple differentiation use cases.\n\nThe standard library defines conformances to the `Differentiable` protocol for `Float` and `Double`, as well as a conditional conformance for `Tensor`:\n\n```swift\nextension Float: Differentiable {\n    public typealias TangentVector = Float\n    public typealias CotangentVector = Float\n    public typealias AllDifferentiableVariables = Float\n}\n// The conformance for `Double` is defined similarly.\n\nextension Tensor: Differentiable where Scalar : TensorFlowFloatingPoint {\n    public typealias TangentVector = Tensor\n    public typealias CotangentVector = Tensor\n    public typealias AllDifferentiableVariables = Tensor\n}\n```\n\n## Compiler-synthesized implementations\n\nAs shown above, the compiler automatically synthesizes implementations of `Differentiable` requirements for struct types.\n\nHere are the current conditions for synthesis:\n* The type must declare a conformance to `Differentiable`, either on the type declaration or on an extension.\n* The conforming type must be a struct type.\n* All stored properties of the conforming type must either conform to `Differentiable` or be marked with the `@noDerivative` attribute.\n\nThe synthesis behavior is explained below.\n\n### Associated type synthesis\n\nHere are the synthesis rules for the three `Differentiable` associated types: `TangentVector`, `CotangentVector`, and `AllDifferentiableVariables`.\n\nLet \"differentiation properties\" refer to all stored properties of the conforming type that are not marked with `@noDerivative`. These stored properties are guaranteed by the synthesis condition to all conform to `Differentiable`.\n\nThe synthesis rules are:\n* Set associated types to `Self`, if possible.\n  * If the conforming type conforms to `AdditiveArithmetic`, and no `@noDerivative` stored properties exist, and all stored properties satisfy `Self == Self.TangentVector == Self.CotangentVector == Self.AllDifferentiableVariables`, then all associated types can be set to typealiases of `Self`.\n* Synthesize a single `AllDifferentiableVariables` member struct. Set `TangentVector` and `CotangentVector` to `AllDifferentiableVariables` if possible; otherwise synthesize more member structs.\n  * Regarding member struct synthesis: for each \"differentiation property\" in the conforming type, a corresponding stored property is synthesized in the member structs, with type equal to the property‚Äôs associated type.\n  * `TangentVector` and `CotangentVector` can be set to `AllDifferentiableVariables` if all differentiation properties satisfy `Self.TangentVector == Self.CotangentVector == Self.AllDifferentiableVariables`.\n\nA memberwise initializer is synthesized for the conforming type itself, in addition to all associated structs. This is important for differentiating struct properties accesses and synthesizing manifold operation requirements.\n\n### Synthesis for other requirements\n\n`var allDifferentiableVariables: AllDifferentiableVariables` is synthesized as a computed property that mirrors the differentiation properties of the conforming type.\n\nIt is always synthesized with a getter.\nIt is synthesized with a setter only when all differentiation properties are mutable and themselves all have mutable `allDifferentiableVariables` properties.\n\nExamples:\n```swift\n// Example when `AllDifferentiableVariables == Self`.\nvar allDifferentiableVariables: AllDifferentiableVariables {\n    get { return self }\n    set { return newValue }\n}\n\n// Example when `AllDifferentiableVariables != Self`.\nvar allDifferentiableVariables: AllDifferentiableVariables {\n    get { return AllDifferentiableVariables(x: x, y: y, ...) }\n    set { x = newValue.x; y = newValue.y }",
    "repo_full_name": "tensorflow/swift",
    "discussion_comments": [
      {
        "comment_id": "263667558",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 137,
        "pr_file": "docs/DifferentiableTypes.md",
        "discussion_id": "263667558",
        "commented_code": "@@ -0,0 +1,289 @@\n+# Differentiable types\n+\n+Richard Wei, Dan Zheng\n+\n+March 2019\n+\n+## Preface\n+\n+Mathematically speaking, only functions are \"differentiable\": only functions have derivatives and can be differentiated. In this document, the terminology \"differentiable types\" is used as a shorthand for \"types that can be used as arguments and results of differentiable functions\". This notion is important because not all types are \"differentiable\" in this sense. For example, types representing real numbers and vector spaces are \"differentiable\", but strings and integers are not.\n+\n+## Introduction\n+\n+Elementary calculus defines differentiation on real numbers: most people are familiar with this definition of \"differentiation\". However, differentiation is defined for many concepts across different branches of mathematics:\n+* Scalar differentiation: differentiation on real numbers. This is taught in introductory calculus.\n+* [Vector calculus]: a branch of mathematics that involves differentiation of vector fields.\n+* [Differential geometry]: a branch of mathematics that involves differentiation of functions over manifolds.\n+\n+In Swift, we want to build a general system for differentiation that can represent all of these cases. Differentiation should not be limited to functions over specific types (e.g. functions over floating-point numbers); it should also work with functions whose parameters/result are custom types.\n+\n+This raises some questions: what kind of types can be used as arguments and results of differentiable functions, and how can we generalize them using a protocol?\n+\n+## Design overview\n+\n+The `Differentiable` protocol generalizes all types that can be used as arguments and results of differentiable functions.\n+\n+The compiler can automatically provide default implementations of `Differentiable` protocol requirements for struct types whose stored properties all conform to `Differentiable`.\n+\n+Here are some examples:\n+\n+```swift\n+struct Vector: Differentiable, VectorNumeric {\n+    // The compiler synthesizes all `Differentiable` protocol requirements\n+    // when all stored properties conform to `Differentiable`.\n+    var x, y, z: Float\n+}\n+\n+// Differential operators like `gradient(at:in:)` just work!\n+let v = Vector(x: 1, y: 2, z: 3)\n+let ùõÅv = gradient(at: v) { v in (v + v).x }\n+\n+print(ùõÅv)\n+// Vector(x: 2.0, y: 0.0, z: 0.0)\n+```\n+\n+A `Differentiable`-conforming type may have stored properties that are not meant to have a derivative with respect to `self`. Use the `@noDerivative` attribute to mark those properties; they will not have a corresponding entry in the synthesized `TangentVector`, `CotangentVector`, and `AllDifferentiableVariables` struct types.\n+\n+Here‚Äôs an example deep learning layer with some `@noDerivative` properties:\n+\n+```swift\n+struct DenseLayer : Differentiable {\n+    // These properties should have derivative values.\n+    var weight: Tensor<Float>\n+    var bias: Tensor<Float>\n+\n+    // These auxiliary properties should not have derivative values.\n+    // Thus, they are marked with `@noDerivative`.\n+    //\n+    // `@noDerivative` properties do not have a corresponding entry\n+    // in synthesized associated struct types.\n+    @noDerivative var useBias: Bool = true\n+    @noDerivative var previousWeight: Tensor<Float> = Tensor(0)\n+\n+    init(weight: Tensor<Float>, bias: Tensor<Float>) {\n+        self.weight = weight\n+        self.bias = bias\n+    }\n+\n+    // The compiler synthesizes all `Differentiable` protocol requirements,\n+    // adding only properties not marked with `@noDerivative` to associated\n+    // tangent space types.\n+\n+    func applied(to input: Tensor<Float>) -> Tensor<Float> {\n+        return matmul(input, weight) + bias\n+    }\n+}\n+\n+// Differential operators like `gradient(at:in:)` just work!\n+let dense = DenseLayer(weight: [[1, 1], [1, 1]], bias: [0, 0])\n+let ùõÅdense = gradient(at: dense) { dense in dense.applied(to: [[3, 3]]).sum() }\n+\n+dump(ùõÅdense)\n+// ‚ñø DenseLayer.AllDifferentiableVariables\n+//   - weight: [[3.0, 3.0], [3.0, 3.0]]\n+//   - bias: [1.0, 1.0]\n+```\n+\n+## Protocol details\n+\n+Here is the full `Differentiable` protocol definition. More explanation is provided below.\n+\n+```swift\n+/// A type that mathematically represents a differentiable manifold whose\n+/// tangent spaces are finite-dimensional.\n+public protocol Differentiable {\n+    /// The tangent bundle of this differentiable manifold.\n+    associatedtype TangentVector : AdditiveArithmetic & Differentiable\n+        where TangentVector.TangentVector == TangentVector,\n+              TangentVector.CotangentVector == CotangentVector,\n+              TangentVector.AllDifferentiableVariables == TangentVector\n+    /// The cotangent bundle of this differentiable manifold.\n+    associatedtype CotangentVector : AdditiveArithmetic & Differentiable\n+        where CotangentVector.TangentVector == CotangentVector,\n+              CotangentVector.CotangentVector == TangentVector,\n+              CotangentVector.AllDifferentiableVariables == CotangentVector\n+    /// The type of all differentiable variables in this type.\n+    associatedtype AllDifferentiableVariables : Differentiable\n+        where AllDifferentiableVariables.AllDifferentiableVariables ==\n+                  AllDifferentiableVariables,\n+              AllDifferentiableVariables.TangentVector == TangentVector,\n+              AllDifferentiableVariables.CotangentVector == CotangentVector\n+\n+    /// All differentiable variables in this type.\n+    var allDifferentiableVariables: AllDifferentiableVariables { get }\n+\n+    /// Returns `self` moved along the value space towards the given tangent\n+    /// vector. In Riemannian geometry (mathematics), this represents an\n+    /// exponential map.\n+    func moved(along direction: TangentVector) -> Self\n+\n+    /// Convert a cotangent vector to its corresponding tangent vector.\n+    func tangentVector(from cotangent: CotangentVector) -> TangentVector\n+}\n+```\n+\n+Mathematically, `Differentiable` represents a differentiable manifold: this is a technical term for smooth-surfaced objects like spheres and generalizes types that are compatible with differentiation, like `Float`, `Double`, `Tensor`, and `SIMD4<Float>`. This definition comes from differential geometry and is quite technical, and not all details are relevant for most use cases.\n+\n+<p align=\"center\">\n+  <img src=\"https://upload.wikimedia.org/wikipedia/commons/3/37/Pushforward.svg\" align=center>\n+  <br>\n+  <sub>Image showing two differentiable manifolds: a sphere and a spheroid.</sub>\n+  <br>\n+  <sub>From https://en.wikipedia.org/wiki/Pushforward_(differential).</sub>\n+</p>\n+\n+Here are the important points about the `Differentiable` protocol:\n+* `associatedtype TangentVector`: represents the type of directional derivatives computed via forward-mode differentiation.\n+* `associatedtype CotangentVector`: represents the type of gradient values computed via reverse-mode differentiation.\n+* `CotangentVector` types are used and produced by differential operators like `gradient` and `pullback`.\n+* `var allDifferentiableVariables: AllDifferentiableVariables`: represents all differentiable variables in an instance of the conforming type, where `AllDifferentiableVariables` is the type of all differentiable variables.\n+  * The motivation/design behind this is related to key path iteration .\n+* `TangentVector`, `CotangentVector`, and `AllDifferentiableVariables` are closely related.\n+  * All three associated types must themselves conform to `Differentiable`.\n+  * The `Differentiable` protocol associated types of the associated types themselves are defined to be mathematically correct.\n+    * `Foo.TangentVector.TangentVector` is `Foo.TangentVector` itself.\n+    * `Foo.CotangentVector.TangentVector` is `Foo.CotangentVector` itself.\n+    * `Foo.TangentVector.CotangentVector` is `Foo.CotangentVector`.\n+    * `Foo.CotangentVector.CotangentVector` is `Foo.TangentVector`.\n+    * `Foo.AllDifferentiableVariables` has the same `TangentVector` and `CotangentVector` as `Foo`.\n+  * Additionally, `TangentVector` and  `CotangentVector` must conform to `AdditiveArithmetic`, so that they can be zero-initialized and accumulated via addition. These are necessary to perform the chain rule of differentiation.\n+* Manifold operations.\n+  * These currently involve `func tangentVector(from cotangent: CotangentVector)` and `func moved(along cotangent: CotangentVector`). These operations can be useful for implementing manifold-related algorithms, like optimization on manifolds, but are not relevant for simple differentiation use cases.\n+\n+The standard library defines conformances to the `Differentiable` protocol for `Float` and `Double`, as well as a conditional conformance for `Tensor`:\n+\n+```swift\n+extension Float: Differentiable {\n+    public typealias TangentVector = Float\n+    public typealias CotangentVector = Float\n+    public typealias AllDifferentiableVariables = Float\n+}\n+// The conformance for `Double` is defined similarly.\n+\n+extension Tensor: Differentiable where Scalar : TensorFlowFloatingPoint {\n+    public typealias TangentVector = Tensor\n+    public typealias CotangentVector = Tensor\n+    public typealias AllDifferentiableVariables = Tensor\n+}\n+```\n+\n+## Compiler-synthesized implementations\n+\n+As shown above, the compiler automatically synthesizes implementations of `Differentiable` requirements for struct types.\n+\n+Here are the current conditions for synthesis:\n+* The type must declare a conformance to `Differentiable`, either on the type declaration or on an extension.\n+* The conforming type must be a struct type.\n+* All stored properties of the conforming type must either conform to `Differentiable` or be marked with the `@noDerivative` attribute.\n+\n+The synthesis behavior is explained below.\n+\n+### Associated type synthesis\n+\n+Here are the synthesis rules for the three `Differentiable` associated types: `TangentVector`, `CotangentVector`, and `AllDifferentiableVariables`.\n+\n+Let \"differentiation properties\" refer to all stored properties of the conforming type that are not marked with `@noDerivative`. These stored properties are guaranteed by the synthesis condition to all conform to `Differentiable`.\n+\n+The synthesis rules are:\n+* Set associated types to `Self`, if possible.\n+  * If the conforming type conforms to `AdditiveArithmetic`, and no `@noDerivative` stored properties exist, and all stored properties satisfy `Self == Self.TangentVector == Self.CotangentVector == Self.AllDifferentiableVariables`, then all associated types can be set to typealiases of `Self`.\n+* Synthesize a single `AllDifferentiableVariables` member struct. Set `TangentVector` and `CotangentVector` to `AllDifferentiableVariables` if possible; otherwise synthesize more member structs.\n+  * Regarding member struct synthesis: for each \"differentiation property\" in the conforming type, a corresponding stored property is synthesized in the member structs, with type equal to the property‚Äôs associated type.\n+  * `TangentVector` and `CotangentVector` can be set to `AllDifferentiableVariables` if all differentiation properties satisfy `Self.TangentVector == Self.CotangentVector == Self.AllDifferentiableVariables`.\n+\n+A memberwise initializer is synthesized for the conforming type itself, in addition to all associated structs. This is important for differentiating struct properties accesses and synthesizing manifold operation requirements.\n+\n+### Synthesis for other requirements\n+\n+`var allDifferentiableVariables: AllDifferentiableVariables` is synthesized as a computed property that mirrors the differentiation properties of the conforming type.\n+\n+It is always synthesized with a getter.\n+It is synthesized with a setter only when all differentiation properties are mutable and themselves all have mutable `allDifferentiableVariables` properties.\n+\n+Examples:\n+```swift\n+// Example when `AllDifferentiableVariables == Self`.\n+var allDifferentiableVariables: AllDifferentiableVariables {\n+    get { return self }\n+    set { return newValue }\n+}\n+\n+// Example when `AllDifferentiableVariables != Self`.\n+var allDifferentiableVariables: AllDifferentiableVariables {\n+    get { return AllDifferentiableVariables(x: x, y: y, ...) }\n+    set { x = newValue.x; y = newValue.y }",
        "comment_created_at": "2019-03-08T05:16:34+00:00",
        "comment_author": "rxwei",
        "comment_body": "For consistency with `get`:\r\n```suggestion\r\n    set { x = newValue.x; y = newValue.y; ... }\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "256632620",
    "pr_number": 114,
    "pr_file": "README.md",
    "created_at": "2019-02-13T23:20:47+00:00",
    "commented_code": "# Swift for TensorFlow\n\nWelcome to the Swift for TensorFlow development community! For discussions, join the [swift@tensorflow.org mailing list](https://groups.google.com/a/tensorflow.org/d/forum/swift).\n> Swift for TensorFlow: No barriers.\n\nSwift for TensorFlow (aka S4TF) is a next-generation system for machine\nlearning, incorporating the latest research across: machine learning,\ncompilers, differentiable programming, systems design, and beyond. This project\nis approaching version _0.2_; it is neither feature complete nor\nproduction-ready. But it is ready for _pioneers_ to try it for your own\nprojects, give us feedback, and help shape the future!\n\nThe S4TF project is currently focusing on 2 kinds of users:\n\n 1. **Advanced ML researchers** who are limited by current ML frameworks.\n    S4TF's advantages include a seamless integration with a modern general-purpose\n    language, allowing for more dynamic and sophisticated models. Fast\n    abstractions can be developed \"in user-space\" (as opposed to in C/C++\n    aka \"framework-space\"), resulting in modular APIs that can be easily\n    customized.\n\n 2. **ML learners** who are just getting started with machine learning. Thanks\n    to Swift's support for quality tooling (e.g. context-aware autocomplete),\n    Swift for TensorFlow can be one of the most productive ways to get started\n    learning the fundamentals of machine learning.\n\n## Getting started\n\n - **Google Colaboratory**: The fastest way to get started is to try out S4TF right in your\n   browser. Just open up our [getting started\n   notebook](https://colab.research.google.com/github/tensorflow/swift-tutorials/blob/master/iris/swift_tensorflow_tutorial.ipynb) (or start from a\n   [blank notebook](https://colab.research.google.com/github/tensorflow/swift/blob/master/notebooks/blank_swift.ipynb))!\n   Read more in our [usage guide](Usage.md).\n\n - **Install locally**: you can [download a pre-built Swift for TensorFlow\n   package](Installation.md). After installation, you can follow these\n   [step-by-step instructions](Usage.md) to build and execute a Swift script\n   on your computer.\n\n - **Compile from source**: If you'd like to customize S4TF or even contribute\n   back, follow our [instructions on building the S4TF compiler from\n   source](https://github.com/apple/swift/tree/tensorflow).\n\nPlease do join the\n[swift@tensorflow.org mailing list](https://groups.google.com/a/tensorflow.org/d/forum/swift)\nto hear the latest announcements, get help, and share your thoughts!\n\n## Why Swift for TensorFlow?\n\nSwift for TensorFlow is a new way to develop machine learning models. It\ngives you the power of\n[TensorFlow](https://www.tensorflow.org) directly integrated into the\n[Swift programming language](https://swift.org/about).\nWith Swift, you can write the following imperative code, and Swift \nautomatically turns it into **a single TensorFlow Graph** and runs it \nwith the full performance of TensorFlow Sessions on CPU, GPU and \n[TPU](https://cloud.google.com/tpu).\n[Swift programming language](https://swift.org/about). We believe that\nmachine learning paradigms are so important that they deserve\n**first-class language and compiler support**. \n\n```swift\nimport TensorFlow\nA fundamental primitive in machine learning is gradient-based optimization:\ncomputing function derivatives to optimize parameters. With Swift for\nTensorFlow, you can easily differentiate custom functions using differential\noperators like `gradient(of:)`, or differentiate with respect to an entire\nmodel by calling standard library method `model.gradient { ... }`.\n\nvar x = Tensor<Float>([[1, 2], [3, 4]])\n```swift\n// Custom differentiable type.\nstruct Model : Differentiable {",
    "repo_full_name": "tensorflow/swift",
    "discussion_comments": [
      {
        "comment_id": "256632620",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 114,
        "pr_file": "README.md",
        "discussion_id": "256632620",
        "commented_code": "@@ -4,130 +4,183 @@\n \n # Swift for TensorFlow\n \n-Welcome to the Swift for TensorFlow development community! For discussions, join the [swift@tensorflow.org mailing list](https://groups.google.com/a/tensorflow.org/d/forum/swift).\n+> Swift for TensorFlow: No barriers.\n+\n+Swift for TensorFlow (aka S4TF) is a next-generation system for machine\n+learning, incorporating the latest research across: machine learning,\n+compilers, differentiable programming, systems design, and beyond. This project\n+is approaching version _0.2_; it is neither feature complete nor\n+production-ready. But it is ready for _pioneers_ to try it for your own\n+projects, give us feedback, and help shape the future!\n+\n+The S4TF project is currently focusing on 2 kinds of users:\n+\n+ 1. **Advanced ML researchers** who are limited by current ML frameworks.\n+    S4TF's advantages include a seamless integration with a modern general-purpose\n+    language, allowing for more dynamic and sophisticated models. Fast\n+    abstractions can be developed \"in user-space\" (as opposed to in C/C++\n+    aka \"framework-space\"), resulting in modular APIs that can be easily\n+    customized.\n+\n+ 2. **ML learners** who are just getting started with machine learning. Thanks\n+    to Swift's support for quality tooling (e.g. context-aware autocomplete),\n+    Swift for TensorFlow can be one of the most productive ways to get started\n+    learning the fundamentals of machine learning.\n+\n+## Getting started\n+\n+ - **Google Colaboratory**: The fastest way to get started is to try out S4TF right in your\n+   browser. Just open up our [getting started\n+   notebook](https://colab.research.google.com/github/tensorflow/swift-tutorials/blob/master/iris/swift_tensorflow_tutorial.ipynb) (or start from a\n+   [blank notebook](https://colab.research.google.com/github/tensorflow/swift/blob/master/notebooks/blank_swift.ipynb))!\n+   Read more in our [usage guide](Usage.md).\n+\n+ - **Install locally**: you can [download a pre-built Swift for TensorFlow\n+   package](Installation.md). After installation, you can follow these\n+   [step-by-step instructions](Usage.md) to build and execute a Swift script\n+   on your computer.\n+\n+ - **Compile from source**: If you'd like to customize S4TF or even contribute\n+   back, follow our [instructions on building the S4TF compiler from\n+   source](https://github.com/apple/swift/tree/tensorflow).\n+\n+Please do join the\n+[swift@tensorflow.org mailing list](https://groups.google.com/a/tensorflow.org/d/forum/swift)\n+to hear the latest announcements, get help, and share your thoughts!\n+\n+## Why Swift for TensorFlow?\n \n Swift for TensorFlow is a new way to develop machine learning models. It\n gives you the power of\n [TensorFlow](https://www.tensorflow.org) directly integrated into the\n-[Swift programming language](https://swift.org/about).\n-With Swift, you can write the following imperative code, and Swift \n-automatically turns it into **a single TensorFlow Graph** and runs it \n-with the full performance of TensorFlow Sessions on CPU, GPU and \n-[TPU](https://cloud.google.com/tpu).\n+[Swift programming language](https://swift.org/about). We believe that\n+machine learning paradigms are so important that they deserve\n+**first-class language and compiler support**. \n \n-```swift\n-import TensorFlow\n+A fundamental primitive in machine learning is gradient-based optimization:\n+computing function derivatives to optimize parameters. With Swift for\n+TensorFlow, you can easily differentiate custom functions using differential\n+operators like `gradient(of:)`, or differentiate with respect to an entire\n+model by calling standard library method `model.gradient { ... }`.\n \n-var x = Tensor<Float>([[1, 2], [3, 4]])\n+```swift\n+// Custom differentiable type.\n+struct Model : Differentiable {",
        "comment_created_at": "2019-02-13T23:20:47+00:00",
        "comment_author": "rxwei",
        "comment_body": "In public code, we use 4-space indentation and the \"no space before any colon\" style.\r\n\r\n```suggestion\r\nstruct Model: Differentiable {\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "256635470",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 114,
        "pr_file": "README.md",
        "discussion_id": "256632620",
        "commented_code": "@@ -4,130 +4,183 @@\n \n # Swift for TensorFlow\n \n-Welcome to the Swift for TensorFlow development community! For discussions, join the [swift@tensorflow.org mailing list](https://groups.google.com/a/tensorflow.org/d/forum/swift).\n+> Swift for TensorFlow: No barriers.\n+\n+Swift for TensorFlow (aka S4TF) is a next-generation system for machine\n+learning, incorporating the latest research across: machine learning,\n+compilers, differentiable programming, systems design, and beyond. This project\n+is approaching version _0.2_; it is neither feature complete nor\n+production-ready. But it is ready for _pioneers_ to try it for your own\n+projects, give us feedback, and help shape the future!\n+\n+The S4TF project is currently focusing on 2 kinds of users:\n+\n+ 1. **Advanced ML researchers** who are limited by current ML frameworks.\n+    S4TF's advantages include a seamless integration with a modern general-purpose\n+    language, allowing for more dynamic and sophisticated models. Fast\n+    abstractions can be developed \"in user-space\" (as opposed to in C/C++\n+    aka \"framework-space\"), resulting in modular APIs that can be easily\n+    customized.\n+\n+ 2. **ML learners** who are just getting started with machine learning. Thanks\n+    to Swift's support for quality tooling (e.g. context-aware autocomplete),\n+    Swift for TensorFlow can be one of the most productive ways to get started\n+    learning the fundamentals of machine learning.\n+\n+## Getting started\n+\n+ - **Google Colaboratory**: The fastest way to get started is to try out S4TF right in your\n+   browser. Just open up our [getting started\n+   notebook](https://colab.research.google.com/github/tensorflow/swift-tutorials/blob/master/iris/swift_tensorflow_tutorial.ipynb) (or start from a\n+   [blank notebook](https://colab.research.google.com/github/tensorflow/swift/blob/master/notebooks/blank_swift.ipynb))!\n+   Read more in our [usage guide](Usage.md).\n+\n+ - **Install locally**: you can [download a pre-built Swift for TensorFlow\n+   package](Installation.md). After installation, you can follow these\n+   [step-by-step instructions](Usage.md) to build and execute a Swift script\n+   on your computer.\n+\n+ - **Compile from source**: If you'd like to customize S4TF or even contribute\n+   back, follow our [instructions on building the S4TF compiler from\n+   source](https://github.com/apple/swift/tree/tensorflow).\n+\n+Please do join the\n+[swift@tensorflow.org mailing list](https://groups.google.com/a/tensorflow.org/d/forum/swift)\n+to hear the latest announcements, get help, and share your thoughts!\n+\n+## Why Swift for TensorFlow?\n \n Swift for TensorFlow is a new way to develop machine learning models. It\n gives you the power of\n [TensorFlow](https://www.tensorflow.org) directly integrated into the\n-[Swift programming language](https://swift.org/about).\n-With Swift, you can write the following imperative code, and Swift \n-automatically turns it into **a single TensorFlow Graph** and runs it \n-with the full performance of TensorFlow Sessions on CPU, GPU and \n-[TPU](https://cloud.google.com/tpu).\n+[Swift programming language](https://swift.org/about). We believe that\n+machine learning paradigms are so important that they deserve\n+**first-class language and compiler support**. \n \n-```swift\n-import TensorFlow\n+A fundamental primitive in machine learning is gradient-based optimization:\n+computing function derivatives to optimize parameters. With Swift for\n+TensorFlow, you can easily differentiate custom functions using differential\n+operators like `gradient(of:)`, or differentiate with respect to an entire\n+model by calling standard library method `model.gradient { ... }`.\n \n-var x = Tensor<Float>([[1, 2], [3, 4]])\n+```swift\n+// Custom differentiable type.\n+struct Model : Differentiable {",
        "comment_created_at": "2019-02-13T23:32:46+00:00",
        "comment_author": "saeta",
        "comment_body": "Ack; fixed.",
        "pr_file_module": null
      }
    ]
  }
]