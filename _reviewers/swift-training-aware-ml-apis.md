---
title: Training-aware ML APIs
description: Design machine learning APIs with explicit parameters for distinguishing
  between training and inference phases rather than relying on global state or implicit
  context. This pattern enables concurrent training and testing, improves model reusability,
  and prevents subtle bugs in distributed training scenarios.
repository: tensorflow/swift
label: AI
language: Markdown
comments_count: 3
repository_stars: 6136
---

Design machine learning APIs with explicit parameters for distinguishing between training and inference phases rather than relying on global state or implicit context. This pattern enables concurrent training and testing, improves model reusability, and prevents subtle bugs in distributed training scenarios.

```swift
// Instead of using global context:
protocol Layer {
    func call(_ input: Input) -> Output  // Implicit context is problematic
}

// Use explicit training parameters:
protocol Layer {
    associatedtype Input: Differentiable
    associatedtype Output: Differentiable
    
    @differentiable(wrt: (self, input))
    func call(_ input: Input, training: Bool) -> Output
}

// Implementation example:
struct BatchNorm: Layer {
    var scale: Tensor<Float>
    var offset: Tensor<Float>
    @noDerivative var runningMean: Tensor<Float>
    @noDerivative var runningVariance: Tensor<Float>
    
    @differentiable(wrt: (self, input))
    func call(_ input: Tensor<Float>, training: Bool) -> Tensor<Float> {
        if training {
            // Use batch statistics, update running statistics
            let batchStatistics = calculateBatchStatistics(input)
            updateRunningStatistics(batchStatistics)
            return normalize(input, using: batchStatistics)
        } else {
            // Use running statistics
            return normalize(input, using: (runningMean, runningVariance))
        }
    }
}
```


[
  {
    "discussion_id": "257443361",
    "pr_number": 115,
    "pr_file": "proposals/LayerProtocolRevision.md",
    "created_at": "2019-02-16T03:04:32+00:00",
    "commented_code": "# Layer Protocol - Training and Concurrent Testing\n\n* Author: [@ewconnell](https://github.com/ewconnell)\n\n## Introduction\nThis document discusses the requirements and proposed design changes to enable concurrent testing while training. It also discusses problems with use of the _LearningPhaseIndicator_ class in the current design.\n\n## Performance\nSignificant training time improvement can be achieved by performing model test passes concurrently, allowing training to continue uninterrupted. The larger the training sample, number of samples in the test set, and more expensive the model design, the greater the benefit. Concurrent test passes can utilize idle GPU capacity, utilize additional GPUs, or be distributed to other nodes.\n\nThe current S4TF implementation does allow this to a limited degree. The following example runs correctly. In this case very little benefit is achieved because the S4TF training pass is currently much slower than the inference pass. In Netlib a training pass is very fast, so I was able to confirm that a concurrent test pass achieves a significant performance gain. The following is an example of a concurrent training loop that currently works with the simple addition of a _DispatchQueue_ and _DispatchGroup_. Since Layer is a struct, models copy correctly. As training proceeds the learned parameters mutate, making the copies independent.\n```swift\nvar model = MNISTClassifier()\nlet optimizer = SGD<MNISTClassifier, Float>(learningRate: 0.1, momentum: 0.9)\nlet batchSize: Int32 = 60\nlet testBatchSize: Int32 = 1000\nlet trainingIterations: Int32 = trainingImages.shape[0] / batchSize\nlet epochs = 10\nlet testQueue = DispatchQueue(label: \"testQueue\")\nlet testGroup = DispatchGroup()\n\nfunc minibatch<T>(_ x: Tensor<T>, size: Int32, batchIndex: Int32) -> Tensor<T> {\n  let start = batchIndex * size\n  return x[start..<start + size]\n}\n\nprint(\"Begin training for \\(epochs) epochs\" )\nlet start = Date()\n\nfor epoch in 0..<epochs {\n  var totalLoss: Float = 0\n  // train\n  for i in 0..<trainingIterations {\n    let images = minibatch(trainingImages, size: batchSize, batchIndex: i)\n    let labels = minibatch(oneHotTrainingLabels, size: batchSize, batchIndex: i)\n\n    let gradients = gradient(at: model) { model -> Tensor<Float> in\n      let logits = model.applied(to: images)\n      let batchLoss = softmaxCrossEntropy(logits: logits, labels: labels)\n      totalLoss += batchLoss.scalarized()\n      return batchLoss\n    }\n    optimizer.update(&model.allDifferentiableVariables, along: gradients)\n  }\n  // test\n  testQueue.async(group: testGroup) {\n    var totalCorrect: Int32 = 0\n    for i in 0..<Int32(10) {\n      let images = minibatch(testImages, size: testBatchSize, batchIndex: i)\n      let labels = minibatch(numericTestLabels, size: testBatchSize, batchIndex: i)\n      let predictions = model.infer(from: images)\n      let correct = predictions.argmax(squeezingAxis: 1) .== labels\n      totalCorrect += Tensor<Int32>(correct).sum().scalarized()\n    }\n\n    let accuracy = Float(totalCorrect) / Float(numericTestLabels.shape[0])\n    print(\"epoch \\(epoch) accuracy: \\(accuracy) loss: \\(totalLoss)\")\n  }\n}\ntestGroup.wait()\nprint(\"Training complete: \\(String(timeInterval: Date().timeIntervalSince(start)))\")\n```\n## Copying Layers and the LearningPhaseIndicator\nSome operators such as BatchNorm and Dropout need to behave differently depending on whether they are performing training or inference. The current design defines the LearningPhaseIndicator class which is intended to behave like a global variable scoped to a single model. The training loop would toggle the _.training_ value depending on whether training or inference is being performed.\n\nThe examples I saw in [tensorflow swift-models](https://github.com/tensorflow/swift-models) had the LearningPhaseIndicator declared and manipulated separately from the model it was affecting. One object having a side effect on another is problematic. Declaring it as a member of the root layer would have been better. In any case this design won\u2019t work because as soon as you copy a model, the same LearningPhaseIndicator will be affecting both models. This would make it impossible to perform concurrent testing, or work with model copies in general. I don\u2019t believe there is any clean way to have a pseudo global variable scoped to a Layer tree. I ran into the same design problem several years ago.\n\n## Suggested Design Change\nA simple and performative solution is to modify the Layer _applied(to:_ API to include a training parameter. Perhaps add an extension function to drop the parameter when calling layers that don\u2019t make a distinction.\n```swift\npublic protocol Layer: Differentiable & KeyPathIterable\n    where AllDifferentiableVariables: KeyPathIterable {\n    /// The input type of the layer.\n    associatedtype Input: Differentiable\n    /// The output type of the layer.\n    associatedtype Output: Differentiable\n\n    /// Returns the output obtained from applying to an input.\n    @differentiable(wrt: (self, input))\n    func applied(to input: Input, training: Bool) -> Output",
    "repo_full_name": "tensorflow/swift",
    "discussion_comments": [
      {
        "comment_id": "257443361",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 115,
        "pr_file": "proposals/LayerProtocolRevision.md",
        "discussion_id": "257443361",
        "commented_code": "@@ -0,0 +1,172 @@\n+# Layer Protocol - Training and Concurrent Testing\n+\n+* Author: [@ewconnell](https://github.com/ewconnell)\n+\n+## Introduction\n+This document discusses the requirements and proposed design changes to enable concurrent testing while training. It also discusses problems with use of the _LearningPhaseIndicator_ class in the current design.\n+\n+## Performance\n+Significant training time improvement can be achieved by performing model test passes concurrently, allowing training to continue uninterrupted. The larger the training sample, number of samples in the test set, and more expensive the model design, the greater the benefit. Concurrent test passes can utilize idle GPU capacity, utilize additional GPUs, or be distributed to other nodes.\n+\n+The current S4TF implementation does allow this to a limited degree. The following example runs correctly. In this case very little benefit is achieved because the S4TF training pass is currently much slower than the inference pass. In Netlib a training pass is very fast, so I was able to confirm that a concurrent test pass achieves a significant performance gain. The following is an example of a concurrent training loop that currently works with the simple addition of a _DispatchQueue_ and _DispatchGroup_. Since Layer is a struct, models copy correctly. As training proceeds the learned parameters mutate, making the copies independent.\n+```swift\n+var model = MNISTClassifier()\n+let optimizer = SGD<MNISTClassifier, Float>(learningRate: 0.1, momentum: 0.9)\n+let batchSize: Int32 = 60\n+let testBatchSize: Int32 = 1000\n+let trainingIterations: Int32 = trainingImages.shape[0] / batchSize\n+let epochs = 10\n+let testQueue = DispatchQueue(label: \"testQueue\")\n+let testGroup = DispatchGroup()\n+\n+func minibatch<T>(_ x: Tensor<T>, size: Int32, batchIndex: Int32) -> Tensor<T> {\n+  let start = batchIndex * size\n+  return x[start..<start + size]\n+}\n+\n+print(\"Begin training for \\(epochs) epochs\" )\n+let start = Date()\n+\n+for epoch in 0..<epochs {\n+  var totalLoss: Float = 0\n+  // train\n+  for i in 0..<trainingIterations {\n+    let images = minibatch(trainingImages, size: batchSize, batchIndex: i)\n+    let labels = minibatch(oneHotTrainingLabels, size: batchSize, batchIndex: i)\n+\n+    let gradients = gradient(at: model) { model -> Tensor<Float> in\n+      let logits = model.applied(to: images)\n+      let batchLoss = softmaxCrossEntropy(logits: logits, labels: labels)\n+      totalLoss += batchLoss.scalarized()\n+      return batchLoss\n+    }\n+    optimizer.update(&model.allDifferentiableVariables, along: gradients)\n+  }\n+  // test\n+  testQueue.async(group: testGroup) {\n+    var totalCorrect: Int32 = 0\n+    for i in 0..<Int32(10) {\n+      let images = minibatch(testImages, size: testBatchSize, batchIndex: i)\n+      let labels = minibatch(numericTestLabels, size: testBatchSize, batchIndex: i)\n+      let predictions = model.infer(from: images)\n+      let correct = predictions.argmax(squeezingAxis: 1) .== labels\n+      totalCorrect += Tensor<Int32>(correct).sum().scalarized()\n+    }\n+\n+    let accuracy = Float(totalCorrect) / Float(numericTestLabels.shape[0])\n+    print(\"epoch \\(epoch) accuracy: \\(accuracy) loss: \\(totalLoss)\")\n+  }\n+}\n+testGroup.wait()\n+print(\"Training complete: \\(String(timeInterval: Date().timeIntervalSince(start)))\")\n+```\n+## Copying Layers and the LearningPhaseIndicator\n+Some operators such as BatchNorm and Dropout need to behave differently depending on whether they are performing training or inference. The current design defines the LearningPhaseIndicator class which is intended to behave like a global variable scoped to a single model. The training loop would toggle the _.training_ value depending on whether training or inference is being performed.\n+\n+The examples I saw in [tensorflow swift-models](https://github.com/tensorflow/swift-models) had the LearningPhaseIndicator declared and manipulated separately from the model it was affecting. One object having a side effect on another is problematic. Declaring it as a member of the root layer would have been better. In any case this design won\u2019t work because as soon as you copy a model, the same LearningPhaseIndicator will be affecting both models. This would make it impossible to perform concurrent testing, or work with model copies in general. I don\u2019t believe there is any clean way to have a pseudo global variable scoped to a Layer tree. I ran into the same design problem several years ago.\n+\n+## Suggested Design Change\n+A simple and performative solution is to modify the Layer _applied(to:_ API to include a training parameter. Perhaps add an extension function to drop the parameter when calling layers that don\u2019t make a distinction.\n+```swift\n+public protocol Layer: Differentiable & KeyPathIterable\n+    where AllDifferentiableVariables: KeyPathIterable {\n+    /// The input type of the layer.\n+    associatedtype Input: Differentiable\n+    /// The output type of the layer.\n+    associatedtype Output: Differentiable\n+\n+    /// Returns the output obtained from applying to an input.\n+    @differentiable(wrt: (self, input))\n+    func applied(to input: Input, training: Bool) -> Output",
        "comment_created_at": "2019-02-16T03:04:32+00:00",
        "comment_author": "rxwei",
        "comment_body": "A more swifty name would be `isTraining`, but I think an enum `LearningPhase` would be more intuitive.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "257518365",
    "pr_number": 115,
    "pr_file": "proposals/LayerProtocolRevision.md",
    "created_at": "2019-02-17T19:05:42+00:00",
    "commented_code": "# Layer Protocol - Training and Concurrent Testing\n\n* Author: [@ewconnell](https://github.com/ewconnell)\n\n## Introduction\nThis document discusses the requirements and proposed design changes to enable concurrent testing while training. It also discusses problems with use of the _LearningPhaseIndicator_ class in the current design.\n\n## Performance\nSignificant training time improvement can be achieved by performing model test passes concurrently, allowing training to continue uninterrupted. The larger the training sample, number of samples in the test set, and more expensive the model design, the greater the benefit. Concurrent test passes can utilize idle GPU capacity, utilize additional GPUs, or be distributed to other nodes.\n\nThe current S4TF implementation does allow this to a limited degree. The following example runs correctly. In this case very little benefit is achieved because the S4TF training pass is currently much slower than the inference pass. In Netlib a training pass is very fast, so I was able to confirm that a concurrent test pass achieves a significant performance gain. The following is an example of a concurrent training loop that currently works with the simple addition of a _DispatchQueue_ and _DispatchGroup_. Since Layer is a struct, models copy correctly. As training proceeds the learned parameters mutate, making the copies independent.\n```swift\nvar model = MNISTClassifier()\nlet optimizer = SGD<MNISTClassifier, Float>(learningRate: 0.1, momentum: 0.9)\nlet batchSize: Int32 = 60\nlet testBatchSize: Int32 = 1000\nlet trainingIterations: Int32 = trainingImages.shape[0] / batchSize\nlet epochs = 10\nlet testQueue = DispatchQueue(label: \"testQueue\")\nlet testGroup = DispatchGroup()\n\nfunc minibatch<T>(_ x: Tensor<T>, size: Int32, batchIndex: Int32) -> Tensor<T> {\n  let start = batchIndex * size\n  return x[start..<start + size]\n}\n\nprint(\"Begin training for \\(epochs) epochs\" )\nlet start = Date()\n\nfor epoch in 0..<epochs {\n  var totalLoss: Float = 0\n  // train\n  for i in 0..<trainingIterations {\n    let images = minibatch(trainingImages, size: batchSize, batchIndex: i)\n    let labels = minibatch(oneHotTrainingLabels, size: batchSize, batchIndex: i)\n\n    let gradients = gradient(at: model) { model -> Tensor<Float> in\n      let logits = model.applied(to: images)\n      let batchLoss = softmaxCrossEntropy(logits: logits, labels: labels)\n      totalLoss += batchLoss.scalarized()\n      return batchLoss\n    }\n    optimizer.update(&model.allDifferentiableVariables, along: gradients)\n  }\n  // test\n  testQueue.async(group: testGroup) {\n    var totalCorrect: Int32 = 0\n    for i in 0..<Int32(10) {\n      let images = minibatch(testImages, size: testBatchSize, batchIndex: i)\n      let labels = minibatch(numericTestLabels, size: testBatchSize, batchIndex: i)\n      let predictions = model.infer(from: images)\n      let correct = predictions.argmax(squeezingAxis: 1) .== labels\n      totalCorrect += Tensor<Int32>(correct).sum().scalarized()\n    }\n\n    let accuracy = Float(totalCorrect) / Float(numericTestLabels.shape[0])\n    print(\"epoch \\(epoch) accuracy: \\(accuracy) loss: \\(totalLoss)\")\n  }\n}\ntestGroup.wait()\nprint(\"Training complete: \\(String(timeInterval: Date().timeIntervalSince(start)))\")\n```\n## Copying Layers and the LearningPhaseIndicator\nSome operators such as BatchNorm and Dropout need to behave differently depending on whether they are performing training or inference. The current design defines the LearningPhaseIndicator class which is intended to behave like a global variable scoped to a single model. The training loop would toggle the _.training_ value depending on whether training or inference is being performed.\n\nThe examples I saw in [tensorflow swift-models](https://github.com/tensorflow/swift-models) had the LearningPhaseIndicator declared and manipulated separately from the model it was affecting. One object having a side effect on another is problematic. Declaring it as a member of the root layer would have been better. In any case this design won\u2019t work because as soon as you copy a model, the same LearningPhaseIndicator will be affecting both models. This would make it impossible to perform concurrent testing, or work with model copies in general. I don\u2019t believe there is any clean way to have a pseudo global variable scoped to a Layer tree. I ran into the same design problem several years ago.\n\n## Suggested Design Change\nA simple and performative solution is to modify the Layer _applied(to:_ API to include a training parameter. Perhaps add an extension function to drop the parameter when calling layers that don\u2019t make a distinction.\n```swift\npublic protocol Layer: Differentiable & KeyPathIterable\n    where AllDifferentiableVariables: KeyPathIterable {\n    /// The input type of the layer.\n    associatedtype Input: Differentiable\n    /// The output type of the layer.\n    associatedtype Output: Differentiable\n\n    /// Returns the output obtained from applying to an input.\n    @differentiable(wrt: (self, input))\n    func applied(to input: Input, training: Bool) -> Output",
    "repo_full_name": "tensorflow/swift",
    "discussion_comments": [
      {
        "comment_id": "257518365",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 115,
        "pr_file": "proposals/LayerProtocolRevision.md",
        "discussion_id": "257518365",
        "commented_code": "@@ -0,0 +1,172 @@\n+# Layer Protocol - Training and Concurrent Testing\n+\n+* Author: [@ewconnell](https://github.com/ewconnell)\n+\n+## Introduction\n+This document discusses the requirements and proposed design changes to enable concurrent testing while training. It also discusses problems with use of the _LearningPhaseIndicator_ class in the current design.\n+\n+## Performance\n+Significant training time improvement can be achieved by performing model test passes concurrently, allowing training to continue uninterrupted. The larger the training sample, number of samples in the test set, and more expensive the model design, the greater the benefit. Concurrent test passes can utilize idle GPU capacity, utilize additional GPUs, or be distributed to other nodes.\n+\n+The current S4TF implementation does allow this to a limited degree. The following example runs correctly. In this case very little benefit is achieved because the S4TF training pass is currently much slower than the inference pass. In Netlib a training pass is very fast, so I was able to confirm that a concurrent test pass achieves a significant performance gain. The following is an example of a concurrent training loop that currently works with the simple addition of a _DispatchQueue_ and _DispatchGroup_. Since Layer is a struct, models copy correctly. As training proceeds the learned parameters mutate, making the copies independent.\n+```swift\n+var model = MNISTClassifier()\n+let optimizer = SGD<MNISTClassifier, Float>(learningRate: 0.1, momentum: 0.9)\n+let batchSize: Int32 = 60\n+let testBatchSize: Int32 = 1000\n+let trainingIterations: Int32 = trainingImages.shape[0] / batchSize\n+let epochs = 10\n+let testQueue = DispatchQueue(label: \"testQueue\")\n+let testGroup = DispatchGroup()\n+\n+func minibatch<T>(_ x: Tensor<T>, size: Int32, batchIndex: Int32) -> Tensor<T> {\n+  let start = batchIndex * size\n+  return x[start..<start + size]\n+}\n+\n+print(\"Begin training for \\(epochs) epochs\" )\n+let start = Date()\n+\n+for epoch in 0..<epochs {\n+  var totalLoss: Float = 0\n+  // train\n+  for i in 0..<trainingIterations {\n+    let images = minibatch(trainingImages, size: batchSize, batchIndex: i)\n+    let labels = minibatch(oneHotTrainingLabels, size: batchSize, batchIndex: i)\n+\n+    let gradients = gradient(at: model) { model -> Tensor<Float> in\n+      let logits = model.applied(to: images)\n+      let batchLoss = softmaxCrossEntropy(logits: logits, labels: labels)\n+      totalLoss += batchLoss.scalarized()\n+      return batchLoss\n+    }\n+    optimizer.update(&model.allDifferentiableVariables, along: gradients)\n+  }\n+  // test\n+  testQueue.async(group: testGroup) {\n+    var totalCorrect: Int32 = 0\n+    for i in 0..<Int32(10) {\n+      let images = minibatch(testImages, size: testBatchSize, batchIndex: i)\n+      let labels = minibatch(numericTestLabels, size: testBatchSize, batchIndex: i)\n+      let predictions = model.infer(from: images)\n+      let correct = predictions.argmax(squeezingAxis: 1) .== labels\n+      totalCorrect += Tensor<Int32>(correct).sum().scalarized()\n+    }\n+\n+    let accuracy = Float(totalCorrect) / Float(numericTestLabels.shape[0])\n+    print(\"epoch \\(epoch) accuracy: \\(accuracy) loss: \\(totalLoss)\")\n+  }\n+}\n+testGroup.wait()\n+print(\"Training complete: \\(String(timeInterval: Date().timeIntervalSince(start)))\")\n+```\n+## Copying Layers and the LearningPhaseIndicator\n+Some operators such as BatchNorm and Dropout need to behave differently depending on whether they are performing training or inference. The current design defines the LearningPhaseIndicator class which is intended to behave like a global variable scoped to a single model. The training loop would toggle the _.training_ value depending on whether training or inference is being performed.\n+\n+The examples I saw in [tensorflow swift-models](https://github.com/tensorflow/swift-models) had the LearningPhaseIndicator declared and manipulated separately from the model it was affecting. One object having a side effect on another is problematic. Declaring it as a member of the root layer would have been better. In any case this design won\u2019t work because as soon as you copy a model, the same LearningPhaseIndicator will be affecting both models. This would make it impossible to perform concurrent testing, or work with model copies in general. I don\u2019t believe there is any clean way to have a pseudo global variable scoped to a Layer tree. I ran into the same design problem several years ago.\n+\n+## Suggested Design Change\n+A simple and performative solution is to modify the Layer _applied(to:_ API to include a training parameter. Perhaps add an extension function to drop the parameter when calling layers that don\u2019t make a distinction.\n+```swift\n+public protocol Layer: Differentiable & KeyPathIterable\n+    where AllDifferentiableVariables: KeyPathIterable {\n+    /// The input type of the layer.\n+    associatedtype Input: Differentiable\n+    /// The output type of the layer.\n+    associatedtype Output: Differentiable\n+\n+    /// Returns the output obtained from applying to an input.\n+    @differentiable(wrt: (self, input))\n+    func applied(to input: Input, training: Bool) -> Output",
        "comment_created_at": "2019-02-17T19:05:42+00:00",
        "comment_author": "saeta",
        "comment_body": "```suggestion\r\n    func applied(to input: Input, in context: Context) -> Output\r\n```\r\n\r\nwhere `Context` is defined as (something similar to):\r\n\r\n```swift\r\nprotocol Context {\r\n  var training: Bool { get }\r\n}\r\n```\r\n\r\nThe reason being is that: I think it'd be a good idea to make this a bit more general / flexible. I suspect that it may be valuable for layers to (eventually) be able to know a bit more about the context within which they're executing. e.g. if we're fine-tuning for quantization. This way, if we decide we need to have an additional orthogonal value (or if we decide that there's actually more than 2 modes (e.g. not just training and inference but say fine-tuning) we have the flexibility to make that change without causing a lot of churn.\r\n\r\nThe risk I see with going in this direction, however, is that the `Context` object could easily become a dumping ground for arbitrary utility things. So, it'd be very good we are careful.\r\n\r\nAdditionally, I think there's an open question as to whether `context` should be the first parameter or the second parameter. (e.g. Golang as a similar pattern, and always requires `Context` to be the first value.)\r\n\r\nFinally, I've suggested `Context` to be a protocol, but I'm not sure if it would better be served as a `struct`. I imagine this decision has some flexibility tradeoffs, as well as performance implications. I'd be curious to hear more from those who have carefully tuned performance in Swift.\r\n\r\nWhat do folks think of this direction?",
        "pr_file_module": null
      },
      {
        "comment_id": "257531162",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 115,
        "pr_file": "proposals/LayerProtocolRevision.md",
        "discussion_id": "257518365",
        "commented_code": "@@ -0,0 +1,172 @@\n+# Layer Protocol - Training and Concurrent Testing\n+\n+* Author: [@ewconnell](https://github.com/ewconnell)\n+\n+## Introduction\n+This document discusses the requirements and proposed design changes to enable concurrent testing while training. It also discusses problems with use of the _LearningPhaseIndicator_ class in the current design.\n+\n+## Performance\n+Significant training time improvement can be achieved by performing model test passes concurrently, allowing training to continue uninterrupted. The larger the training sample, number of samples in the test set, and more expensive the model design, the greater the benefit. Concurrent test passes can utilize idle GPU capacity, utilize additional GPUs, or be distributed to other nodes.\n+\n+The current S4TF implementation does allow this to a limited degree. The following example runs correctly. In this case very little benefit is achieved because the S4TF training pass is currently much slower than the inference pass. In Netlib a training pass is very fast, so I was able to confirm that a concurrent test pass achieves a significant performance gain. The following is an example of a concurrent training loop that currently works with the simple addition of a _DispatchQueue_ and _DispatchGroup_. Since Layer is a struct, models copy correctly. As training proceeds the learned parameters mutate, making the copies independent.\n+```swift\n+var model = MNISTClassifier()\n+let optimizer = SGD<MNISTClassifier, Float>(learningRate: 0.1, momentum: 0.9)\n+let batchSize: Int32 = 60\n+let testBatchSize: Int32 = 1000\n+let trainingIterations: Int32 = trainingImages.shape[0] / batchSize\n+let epochs = 10\n+let testQueue = DispatchQueue(label: \"testQueue\")\n+let testGroup = DispatchGroup()\n+\n+func minibatch<T>(_ x: Tensor<T>, size: Int32, batchIndex: Int32) -> Tensor<T> {\n+  let start = batchIndex * size\n+  return x[start..<start + size]\n+}\n+\n+print(\"Begin training for \\(epochs) epochs\" )\n+let start = Date()\n+\n+for epoch in 0..<epochs {\n+  var totalLoss: Float = 0\n+  // train\n+  for i in 0..<trainingIterations {\n+    let images = minibatch(trainingImages, size: batchSize, batchIndex: i)\n+    let labels = minibatch(oneHotTrainingLabels, size: batchSize, batchIndex: i)\n+\n+    let gradients = gradient(at: model) { model -> Tensor<Float> in\n+      let logits = model.applied(to: images)\n+      let batchLoss = softmaxCrossEntropy(logits: logits, labels: labels)\n+      totalLoss += batchLoss.scalarized()\n+      return batchLoss\n+    }\n+    optimizer.update(&model.allDifferentiableVariables, along: gradients)\n+  }\n+  // test\n+  testQueue.async(group: testGroup) {\n+    var totalCorrect: Int32 = 0\n+    for i in 0..<Int32(10) {\n+      let images = minibatch(testImages, size: testBatchSize, batchIndex: i)\n+      let labels = minibatch(numericTestLabels, size: testBatchSize, batchIndex: i)\n+      let predictions = model.infer(from: images)\n+      let correct = predictions.argmax(squeezingAxis: 1) .== labels\n+      totalCorrect += Tensor<Int32>(correct).sum().scalarized()\n+    }\n+\n+    let accuracy = Float(totalCorrect) / Float(numericTestLabels.shape[0])\n+    print(\"epoch \\(epoch) accuracy: \\(accuracy) loss: \\(totalLoss)\")\n+  }\n+}\n+testGroup.wait()\n+print(\"Training complete: \\(String(timeInterval: Date().timeIntervalSince(start)))\")\n+```\n+## Copying Layers and the LearningPhaseIndicator\n+Some operators such as BatchNorm and Dropout need to behave differently depending on whether they are performing training or inference. The current design defines the LearningPhaseIndicator class which is intended to behave like a global variable scoped to a single model. The training loop would toggle the _.training_ value depending on whether training or inference is being performed.\n+\n+The examples I saw in [tensorflow swift-models](https://github.com/tensorflow/swift-models) had the LearningPhaseIndicator declared and manipulated separately from the model it was affecting. One object having a side effect on another is problematic. Declaring it as a member of the root layer would have been better. In any case this design won\u2019t work because as soon as you copy a model, the same LearningPhaseIndicator will be affecting both models. This would make it impossible to perform concurrent testing, or work with model copies in general. I don\u2019t believe there is any clean way to have a pseudo global variable scoped to a Layer tree. I ran into the same design problem several years ago.\n+\n+## Suggested Design Change\n+A simple and performative solution is to modify the Layer _applied(to:_ API to include a training parameter. Perhaps add an extension function to drop the parameter when calling layers that don\u2019t make a distinction.\n+```swift\n+public protocol Layer: Differentiable & KeyPathIterable\n+    where AllDifferentiableVariables: KeyPathIterable {\n+    /// The input type of the layer.\n+    associatedtype Input: Differentiable\n+    /// The output type of the layer.\n+    associatedtype Output: Differentiable\n+\n+    /// Returns the output obtained from applying to an input.\n+    @differentiable(wrt: (self, input))\n+    func applied(to input: Input, training: Bool) -> Output",
        "comment_created_at": "2019-02-18T00:08:06+00:00",
        "comment_author": "ewconnell",
        "comment_body": "I think defining a Context protocol opens the door to forward flexibility and is a reasonable idea. If the notion of a Context is adopted then I agree stylistically it would be best as the first parameter.\r\n\r\nModels are intended to run in a variety of execution contexts, so this could be an ideal way to bundle up and pass through a collection of control parameters from User level to the back end. For example there are a variety of parameters that can be used to specify Cuda behavior, and in the GCP case there might be a completely different set of context information for accounts, specific resources, and resource configuration parameters.\r\n\r\nA set of objects conforming to _Context_ with additional specialized protocols for each of the common environments (CPU, Cuda, GCP) could be included as part of the product to cover 99% of the user scenarios. The collection could be added to over time if needed and passed through without requiring modifications to the rest of the API.\r\n\r\nThe compiler seems to do a pretty good job of efficiently passing structs down a call tree. I believe even though they are value types, the compiler simply passes a ref until there is an attempt to mutate, which I don't expect would happen other than at initialization time.\r\n\r\nYour concern about the context being abused and becoming a dumping ground for poorly designed 3rd party components is quite valid. I don't know what the company policy is about asserting control over things. I expect most users will have no need to use anything other than the standard set. In any case, clear guidelines should be provided to discuss the issues involved in creating a custom context.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "258230291",
    "pr_number": 115,
    "pr_file": "proposals/LayerProtocolRevision.md",
    "created_at": "2019-02-19T21:09:04+00:00",
    "commented_code": "# Layer Protocol - Training and Concurrent Testing\n\n* Author: [@ewconnell](https://github.com/ewconnell)\n\n## Introduction\nThis document discusses the requirements and proposed design changes to enable concurrent testing while training. It also discusses problems with use of the _LearningPhaseIndicator_ class in the current design.\n\n## Performance\nSignificant training time improvement can be achieved by performing model test passes concurrently, allowing training to continue uninterrupted. The larger the training sample, number of samples in the test set, and more expensive the model design, the greater the benefit. Concurrent test passes can utilize idle GPU capacity, utilize additional GPUs, or be distributed to other nodes.\n\nThe current S4TF implementation does allow this to a limited degree. The following example runs correctly. In this case very little benefit is achieved because the S4TF training pass is currently much slower than the inference pass. In Netlib a training pass is very fast, so I was able to confirm that a concurrent test pass achieves a significant performance gain. The following is an example of a concurrent training loop that currently works with the simple addition of a _DispatchQueue_ and _DispatchGroup_. Since Layer is a struct, models copy correctly. As training proceeds the learned parameters mutate, making the copies independent.\n```swift\nvar model = MNISTClassifier()\nlet optimizer = SGD<MNISTClassifier, Float>(learningRate: 0.1, momentum: 0.9)\nlet batchSize: Int32 = 60\nlet testBatchSize: Int32 = 1000\nlet trainingIterations: Int32 = trainingImages.shape[0] / batchSize\nlet epochs = 10\nlet testQueue = DispatchQueue(label: \"testQueue\")\nlet testGroup = DispatchGroup()\n\nfunc minibatch<T>(_ x: Tensor<T>, size: Int32, batchIndex: Int32) -> Tensor<T> {\n  let start = batchIndex * size\n  return x[start..<start + size]\n}\n\nprint(\"Begin training for \\(epochs) epochs\" )\nlet start = Date()\n\nfor epoch in 0..<epochs {\n  var totalLoss: Float = 0\n  // train\n  for i in 0..<trainingIterations {\n    let images = minibatch(trainingImages, size: batchSize, batchIndex: i)\n    let labels = minibatch(oneHotTrainingLabels, size: batchSize, batchIndex: i)\n\n    let gradients = gradient(at: model) { model -> Tensor<Float> in\n      let logits = model.applied(to: images)\n      let batchLoss = softmaxCrossEntropy(logits: logits, labels: labels)\n      totalLoss += batchLoss.scalarized()\n      return batchLoss\n    }\n    optimizer.update(&model.allDifferentiableVariables, along: gradients)\n  }\n  // test\n  testQueue.async(group: testGroup) {\n    var totalCorrect: Int32 = 0\n    for i in 0..<Int32(10) {\n      let images = minibatch(testImages, size: testBatchSize, batchIndex: i)\n      let labels = minibatch(numericTestLabels, size: testBatchSize, batchIndex: i)\n      let predictions = model.infer(from: images)\n      let correct = predictions.argmax(squeezingAxis: 1) .== labels\n      totalCorrect += Tensor<Int32>(correct).sum().scalarized()\n    }\n\n    let accuracy = Float(totalCorrect) / Float(numericTestLabels.shape[0])\n    print(\"epoch \\(epoch) accuracy: \\(accuracy) loss: \\(totalLoss)\")\n  }\n}\ntestGroup.wait()\nprint(\"Training complete: \\(String(timeInterval: Date().timeIntervalSince(start)))\")\n```\n## Copying Layers and the LearningPhaseIndicator\nSome operators such as BatchNorm and Dropout need to behave differently depending on whether they are performing training or inference. The current design defines the LearningPhaseIndicator class which is intended to behave like a global variable scoped to a single model. The training loop would toggle the _.training_ value depending on whether training or inference is being performed.\n\nThe examples I saw in [tensorflow swift-models](https://github.com/tensorflow/swift-models) had the LearningPhaseIndicator declared and manipulated separately from the model it was affecting. One object having a side effect on another is problematic. Declaring it as a member of the root layer would have been better. In any case this design won\u2019t work because as soon as you copy a model, the same LearningPhaseIndicator will be affecting both models. This would make it impossible to perform concurrent testing, or work with model copies in general. I don\u2019t believe there is any clean way to have a pseudo global variable scoped to a Layer tree. I ran into the same design problem several years ago.\n\n## Suggested Design Change\nA simple and performative solution is to modify the Layer _applied(to:_ API to include a training parameter. Perhaps add an extension function to drop the parameter when calling layers that don\u2019t make a distinction.\n```swift\npublic protocol Layer: Differentiable & KeyPathIterable\n    where AllDifferentiableVariables: KeyPathIterable {\n    /// The input type of the layer.\n    associatedtype Input: Differentiable\n    /// The output type of the layer.\n    associatedtype Output: Differentiable\n\n    /// Returns the output obtained from applying to an input.\n    @differentiable(wrt: (self, input))\n    func applied(to input: Input, training: Bool) -> Output\n}\n\npublic extension Layer {\n  func applied(to input: Input) -> Output {\n    return applied(to: input, training: false)",
    "repo_full_name": "tensorflow/swift",
    "discussion_comments": [
      {
        "comment_id": "258230291",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 115,
        "pr_file": "proposals/LayerProtocolRevision.md",
        "discussion_id": "258230291",
        "commented_code": "@@ -0,0 +1,172 @@\n+# Layer Protocol - Training and Concurrent Testing\n+\n+* Author: [@ewconnell](https://github.com/ewconnell)\n+\n+## Introduction\n+This document discusses the requirements and proposed design changes to enable concurrent testing while training. It also discusses problems with use of the _LearningPhaseIndicator_ class in the current design.\n+\n+## Performance\n+Significant training time improvement can be achieved by performing model test passes concurrently, allowing training to continue uninterrupted. The larger the training sample, number of samples in the test set, and more expensive the model design, the greater the benefit. Concurrent test passes can utilize idle GPU capacity, utilize additional GPUs, or be distributed to other nodes.\n+\n+The current S4TF implementation does allow this to a limited degree. The following example runs correctly. In this case very little benefit is achieved because the S4TF training pass is currently much slower than the inference pass. In Netlib a training pass is very fast, so I was able to confirm that a concurrent test pass achieves a significant performance gain. The following is an example of a concurrent training loop that currently works with the simple addition of a _DispatchQueue_ and _DispatchGroup_. Since Layer is a struct, models copy correctly. As training proceeds the learned parameters mutate, making the copies independent.\n+```swift\n+var model = MNISTClassifier()\n+let optimizer = SGD<MNISTClassifier, Float>(learningRate: 0.1, momentum: 0.9)\n+let batchSize: Int32 = 60\n+let testBatchSize: Int32 = 1000\n+let trainingIterations: Int32 = trainingImages.shape[0] / batchSize\n+let epochs = 10\n+let testQueue = DispatchQueue(label: \"testQueue\")\n+let testGroup = DispatchGroup()\n+\n+func minibatch<T>(_ x: Tensor<T>, size: Int32, batchIndex: Int32) -> Tensor<T> {\n+  let start = batchIndex * size\n+  return x[start..<start + size]\n+}\n+\n+print(\"Begin training for \\(epochs) epochs\" )\n+let start = Date()\n+\n+for epoch in 0..<epochs {\n+  var totalLoss: Float = 0\n+  // train\n+  for i in 0..<trainingIterations {\n+    let images = minibatch(trainingImages, size: batchSize, batchIndex: i)\n+    let labels = minibatch(oneHotTrainingLabels, size: batchSize, batchIndex: i)\n+\n+    let gradients = gradient(at: model) { model -> Tensor<Float> in\n+      let logits = model.applied(to: images)\n+      let batchLoss = softmaxCrossEntropy(logits: logits, labels: labels)\n+      totalLoss += batchLoss.scalarized()\n+      return batchLoss\n+    }\n+    optimizer.update(&model.allDifferentiableVariables, along: gradients)\n+  }\n+  // test\n+  testQueue.async(group: testGroup) {\n+    var totalCorrect: Int32 = 0\n+    for i in 0..<Int32(10) {\n+      let images = minibatch(testImages, size: testBatchSize, batchIndex: i)\n+      let labels = minibatch(numericTestLabels, size: testBatchSize, batchIndex: i)\n+      let predictions = model.infer(from: images)\n+      let correct = predictions.argmax(squeezingAxis: 1) .== labels\n+      totalCorrect += Tensor<Int32>(correct).sum().scalarized()\n+    }\n+\n+    let accuracy = Float(totalCorrect) / Float(numericTestLabels.shape[0])\n+    print(\"epoch \\(epoch) accuracy: \\(accuracy) loss: \\(totalLoss)\")\n+  }\n+}\n+testGroup.wait()\n+print(\"Training complete: \\(String(timeInterval: Date().timeIntervalSince(start)))\")\n+```\n+## Copying Layers and the LearningPhaseIndicator\n+Some operators such as BatchNorm and Dropout need to behave differently depending on whether they are performing training or inference. The current design defines the LearningPhaseIndicator class which is intended to behave like a global variable scoped to a single model. The training loop would toggle the _.training_ value depending on whether training or inference is being performed.\n+\n+The examples I saw in [tensorflow swift-models](https://github.com/tensorflow/swift-models) had the LearningPhaseIndicator declared and manipulated separately from the model it was affecting. One object having a side effect on another is problematic. Declaring it as a member of the root layer would have been better. In any case this design won\u2019t work because as soon as you copy a model, the same LearningPhaseIndicator will be affecting both models. This would make it impossible to perform concurrent testing, or work with model copies in general. I don\u2019t believe there is any clean way to have a pseudo global variable scoped to a Layer tree. I ran into the same design problem several years ago.\n+\n+## Suggested Design Change\n+A simple and performative solution is to modify the Layer _applied(to:_ API to include a training parameter. Perhaps add an extension function to drop the parameter when calling layers that don\u2019t make a distinction.\n+```swift\n+public protocol Layer: Differentiable & KeyPathIterable\n+    where AllDifferentiableVariables: KeyPathIterable {\n+    /// The input type of the layer.\n+    associatedtype Input: Differentiable\n+    /// The output type of the layer.\n+    associatedtype Output: Differentiable\n+\n+    /// Returns the output obtained from applying to an input.\n+    @differentiable(wrt: (self, input))\n+    func applied(to input: Input, training: Bool) -> Output\n+}\n+\n+public extension Layer {\n+  func applied(to input: Input) -> Output {\n+    return applied(to: input, training: false)",
        "comment_created_at": "2019-02-19T21:09:04+00:00",
        "comment_author": "rxwei",
        "comment_body": "I think it could lead to unexpected behavior make this method available. For instance, you could pass `batchNorm.applied(to: x)` without a second argument in a model's `applied(to:)` method, but it could be misleading and cause unintended behavior even though it looks okay. Moreover, semantically, sublayers' learning phase should be training, when the parent `applied(to:)` is training, so passing `false` would be semantically inconsistent.",
        "pr_file_module": null
      },
      {
        "comment_id": "258233777",
        "repo_full_name": "tensorflow/swift",
        "pr_number": 115,
        "pr_file": "proposals/LayerProtocolRevision.md",
        "discussion_id": "258230291",
        "commented_code": "@@ -0,0 +1,172 @@\n+# Layer Protocol - Training and Concurrent Testing\n+\n+* Author: [@ewconnell](https://github.com/ewconnell)\n+\n+## Introduction\n+This document discusses the requirements and proposed design changes to enable concurrent testing while training. It also discusses problems with use of the _LearningPhaseIndicator_ class in the current design.\n+\n+## Performance\n+Significant training time improvement can be achieved by performing model test passes concurrently, allowing training to continue uninterrupted. The larger the training sample, number of samples in the test set, and more expensive the model design, the greater the benefit. Concurrent test passes can utilize idle GPU capacity, utilize additional GPUs, or be distributed to other nodes.\n+\n+The current S4TF implementation does allow this to a limited degree. The following example runs correctly. In this case very little benefit is achieved because the S4TF training pass is currently much slower than the inference pass. In Netlib a training pass is very fast, so I was able to confirm that a concurrent test pass achieves a significant performance gain. The following is an example of a concurrent training loop that currently works with the simple addition of a _DispatchQueue_ and _DispatchGroup_. Since Layer is a struct, models copy correctly. As training proceeds the learned parameters mutate, making the copies independent.\n+```swift\n+var model = MNISTClassifier()\n+let optimizer = SGD<MNISTClassifier, Float>(learningRate: 0.1, momentum: 0.9)\n+let batchSize: Int32 = 60\n+let testBatchSize: Int32 = 1000\n+let trainingIterations: Int32 = trainingImages.shape[0] / batchSize\n+let epochs = 10\n+let testQueue = DispatchQueue(label: \"testQueue\")\n+let testGroup = DispatchGroup()\n+\n+func minibatch<T>(_ x: Tensor<T>, size: Int32, batchIndex: Int32) -> Tensor<T> {\n+  let start = batchIndex * size\n+  return x[start..<start + size]\n+}\n+\n+print(\"Begin training for \\(epochs) epochs\" )\n+let start = Date()\n+\n+for epoch in 0..<epochs {\n+  var totalLoss: Float = 0\n+  // train\n+  for i in 0..<trainingIterations {\n+    let images = minibatch(trainingImages, size: batchSize, batchIndex: i)\n+    let labels = minibatch(oneHotTrainingLabels, size: batchSize, batchIndex: i)\n+\n+    let gradients = gradient(at: model) { model -> Tensor<Float> in\n+      let logits = model.applied(to: images)\n+      let batchLoss = softmaxCrossEntropy(logits: logits, labels: labels)\n+      totalLoss += batchLoss.scalarized()\n+      return batchLoss\n+    }\n+    optimizer.update(&model.allDifferentiableVariables, along: gradients)\n+  }\n+  // test\n+  testQueue.async(group: testGroup) {\n+    var totalCorrect: Int32 = 0\n+    for i in 0..<Int32(10) {\n+      let images = minibatch(testImages, size: testBatchSize, batchIndex: i)\n+      let labels = minibatch(numericTestLabels, size: testBatchSize, batchIndex: i)\n+      let predictions = model.infer(from: images)\n+      let correct = predictions.argmax(squeezingAxis: 1) .== labels\n+      totalCorrect += Tensor<Int32>(correct).sum().scalarized()\n+    }\n+\n+    let accuracy = Float(totalCorrect) / Float(numericTestLabels.shape[0])\n+    print(\"epoch \\(epoch) accuracy: \\(accuracy) loss: \\(totalLoss)\")\n+  }\n+}\n+testGroup.wait()\n+print(\"Training complete: \\(String(timeInterval: Date().timeIntervalSince(start)))\")\n+```\n+## Copying Layers and the LearningPhaseIndicator\n+Some operators such as BatchNorm and Dropout need to behave differently depending on whether they are performing training or inference. The current design defines the LearningPhaseIndicator class which is intended to behave like a global variable scoped to a single model. The training loop would toggle the _.training_ value depending on whether training or inference is being performed.\n+\n+The examples I saw in [tensorflow swift-models](https://github.com/tensorflow/swift-models) had the LearningPhaseIndicator declared and manipulated separately from the model it was affecting. One object having a side effect on another is problematic. Declaring it as a member of the root layer would have been better. In any case this design won\u2019t work because as soon as you copy a model, the same LearningPhaseIndicator will be affecting both models. This would make it impossible to perform concurrent testing, or work with model copies in general. I don\u2019t believe there is any clean way to have a pseudo global variable scoped to a Layer tree. I ran into the same design problem several years ago.\n+\n+## Suggested Design Change\n+A simple and performative solution is to modify the Layer _applied(to:_ API to include a training parameter. Perhaps add an extension function to drop the parameter when calling layers that don\u2019t make a distinction.\n+```swift\n+public protocol Layer: Differentiable & KeyPathIterable\n+    where AllDifferentiableVariables: KeyPathIterable {\n+    /// The input type of the layer.\n+    associatedtype Input: Differentiable\n+    /// The output type of the layer.\n+    associatedtype Output: Differentiable\n+\n+    /// Returns the output obtained from applying to an input.\n+    @differentiable(wrt: (self, input))\n+    func applied(to input: Input, training: Bool) -> Output\n+}\n+\n+public extension Layer {\n+  func applied(to input: Input) -> Output {\n+    return applied(to: input, training: false)",
        "comment_created_at": "2019-02-19T21:18:57+00:00",
        "comment_author": "saeta",
        "comment_body": "+1 to making the context parameter required (and not defining this extension). It'd probably be a code smell to not pass through the context / generate a new context implicitly. Thanks!",
        "pr_file_module": null
      }
    ]
  }
]
