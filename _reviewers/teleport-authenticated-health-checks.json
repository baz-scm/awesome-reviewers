[
  {
    "discussion_id": "2299198938",
    "pr_number": 58065,
    "pr_file": "rfd/0223-k8s-health-checks.md",
    "created_at": "2025-08-25T21:39:26+00:00",
    "commented_code": "+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleterm UI, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. \n+\n+Main change points are:\n+- [lib/healthcheck/worker.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/worker.go#L343)\n+  - Modifying the `dialEndpoint()` function to make TLS requests.\n+\n+[manager.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/manager.go) and [target.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/target.go) have minor changes.\n+\n+Prometheus gauge metrics are added to the `healthcheck` package, and described in the [Prometheus Implementation](#prometheus-implementation).\n+\n+\n+#### Health States\n+\n+A Kubernetes cluster may be in a health state of `unknown`, `healthy` or `unhealthy`.\n+- `unknown` indicates a Kubernetes cluster cannot be contacted\n+- `healthy` indicates a Kubernetes cluster is accepting requests\n+- `unhealthy` indicates an error state, and includes an error message with verbose debugging information, if available\n+\n+See the database health check RFD for [more details](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#health-status).\n+\n+\n+##### Health Check Endpoint\n+\n+> [!NOTE]\n+> Kubernetes health checked at `https://<address>/readyz`\n+\n+> [!IMPORTANT]\n+> Requires Kubernetes v1.16 or higher\n+\n+The Kubernetes API `/readyz` endpoint is selected for health checking, and indicates that a Kubernetes API server is ready to accept requests. Kubernetes serves the endpoint through [TLS by default](https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security).\n+\n+The Kubernetes API offers [several health check endpoints](https://kubernetes.io/docs/reference/using-api/health-checks), as well as TCP checks being available.\n+\n+| Approach        | Description                                              |\n+|-----------------|----------------------------------------------------------|\n+| /readyz         | Ready to accept API requests                             |\n+| /readyz?verbose | Ready to accept API requests (detailed)                  |\n+| /livez          | kube-apiserver process is alive/running                  |\n+| /livez?verbose  | kube-apiserver process is alive/running (detailed)       |\n+| /healthz        | Ambiguously alive or ready. Deprecated in 2019 at v1.16  |\n+| TCP             | Can establish TCP connection to API server port          |\n+\n+Let's explore the options and reasoning for selecting `/readyz`.\n+\n+`/readyz` means that the cluster is accepting API requests, and can be used.\n+\n+`/livez` indicates the Kubernetes kube-apiserver process is alive. API requests may or may not be accepted. There's no implication of whole cluster readiness.\n+\n+`/healthz` is deprecated, and not supported with the Kubernetes health check feature. `/healthz` was [deprecated in September of 2019 with Kubernetes v1.16](https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/). At the time of writing, [Kubernetes is at v1.33](https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/), and six years have passed since v1.16. It seems reasonable not to support the `/healthz` endpoint. The choice then sets up a requirement for customers to use Kubernetes v1.16 or higher with Teleport Kubernetes health checks.\n+\n+Moving on to `TCP`, `TCP` indicates that network connectivity is available. No further knowledge of Kubernetes health would be known. In scenarios where servers don't offer explicit health checks, such as databases, `TCP` may be the only choice. Since Kubernetes offers health checks, we can skip `TCP` checks.\n+\n+So, `/livez` and `TCP` indicate some level of health, but do not necessarily mean the Kubernetes cluster can be used. \n+\n+Let's look at the `verbose` query parameter.\n+\n+`/readyz?verbose` provides a list of Kubernetes modules with `ok / not ok` states. The verbose information is not critical in the common case of a healthy cluster returning a `200` HTTP status code. The verbose information may be helpful to an administrator diagnosing an unhealthy cluster.\n+\n+For efficiency in the common case of a healthy cluster, the `/readyz` endpoint is called and checked for a `200` status code. In nearly all cases we only need to check `200`. The `verbose` body message is not sent, reducing unneeded network, memory, and processor consumption. Also, the Kubernetes authors recommend [relying on the status code](https://kubernetes.io/docs/reference/using-api/health-checks/) for checking state.\n+\n+In the case of non-`200` response codes, a follow-up call to `/readyz?verbose` is made. The follow-up verbose message is appended to a Go error, and eventually forwarded to the Web UI for a Teleport administrator to view.",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "2299198938",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299198938",
        "commented_code": "@@ -0,0 +1,594 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleterm UI, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. \n+\n+Main change points are:\n+- [lib/healthcheck/worker.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/worker.go#L343)\n+  - Modifying the `dialEndpoint()` function to make TLS requests.\n+\n+[manager.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/manager.go) and [target.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/target.go) have minor changes.\n+\n+Prometheus gauge metrics are added to the `healthcheck` package, and described in the [Prometheus Implementation](#prometheus-implementation).\n+\n+\n+#### Health States\n+\n+A Kubernetes cluster may be in a health state of `unknown`, `healthy` or `unhealthy`.\n+- `unknown` indicates a Kubernetes cluster cannot be contacted\n+- `healthy` indicates a Kubernetes cluster is accepting requests\n+- `unhealthy` indicates an error state, and includes an error message with verbose debugging information, if available\n+\n+See the database health check RFD for [more details](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#health-status).\n+\n+\n+##### Health Check Endpoint\n+\n+> [!NOTE]\n+> Kubernetes health checked at `https://<address>/readyz`\n+\n+> [!IMPORTANT]\n+> Requires Kubernetes v1.16 or higher\n+\n+The Kubernetes API `/readyz` endpoint is selected for health checking, and indicates that a Kubernetes API server is ready to accept requests. Kubernetes serves the endpoint through [TLS by default](https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security).\n+\n+The Kubernetes API offers [several health check endpoints](https://kubernetes.io/docs/reference/using-api/health-checks), as well as TCP checks being available.\n+\n+| Approach        | Description                                              |\n+|-----------------|----------------------------------------------------------|\n+| /readyz         | Ready to accept API requests                             |\n+| /readyz?verbose | Ready to accept API requests (detailed)                  |\n+| /livez          | kube-apiserver process is alive/running                  |\n+| /livez?verbose  | kube-apiserver process is alive/running (detailed)       |\n+| /healthz        | Ambiguously alive or ready. Deprecated in 2019 at v1.16  |\n+| TCP             | Can establish TCP connection to API server port          |\n+\n+Let's explore the options and reasoning for selecting `/readyz`.\n+\n+`/readyz` means that the cluster is accepting API requests, and can be used.\n+\n+`/livez` indicates the Kubernetes kube-apiserver process is alive. API requests may or may not be accepted. There's no implication of whole cluster readiness.\n+\n+`/healthz` is deprecated, and not supported with the Kubernetes health check feature. `/healthz` was [deprecated in September of 2019 with Kubernetes v1.16](https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/). At the time of writing, [Kubernetes is at v1.33](https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/), and six years have passed since v1.16. It seems reasonable not to support the `/healthz` endpoint. The choice then sets up a requirement for customers to use Kubernetes v1.16 or higher with Teleport Kubernetes health checks.\n+\n+Moving on to `TCP`, `TCP` indicates that network connectivity is available. No further knowledge of Kubernetes health would be known. In scenarios where servers don't offer explicit health checks, such as databases, `TCP` may be the only choice. Since Kubernetes offers health checks, we can skip `TCP` checks.\n+\n+So, `/livez` and `TCP` indicate some level of health, but do not necessarily mean the Kubernetes cluster can be used. \n+\n+Let's look at the `verbose` query parameter.\n+\n+`/readyz?verbose` provides a list of Kubernetes modules with `ok / not ok` states. The verbose information is not critical in the common case of a healthy cluster returning a `200` HTTP status code. The verbose information may be helpful to an administrator diagnosing an unhealthy cluster.\n+\n+For efficiency in the common case of a healthy cluster, the `/readyz` endpoint is called and checked for a `200` status code. In nearly all cases we only need to check `200`. The `verbose` body message is not sent, reducing unneeded network, memory, and processor consumption. Also, the Kubernetes authors recommend [relying on the status code](https://kubernetes.io/docs/reference/using-api/health-checks/) for checking state.\n+\n+In the case of non-`200` response codes, a follow-up call to `/readyz?verbose` is made. The follow-up verbose message is appended to a Go error, and eventually forwarded to the Web UI for a Teleport administrator to view.",
        "comment_created_at": "2025-08-25T21:39:26+00:00",
        "comment_author": "rosstimothy",
        "comment_body": "Is /readyz rate limited? Do we need to be concerned about similar behavior to https://github.com/gravitational/teleport/issues/58118?",
        "pr_file_module": null
      },
      {
        "comment_id": "2299409489",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299198938",
        "commented_code": "@@ -0,0 +1,594 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleterm UI, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. \n+\n+Main change points are:\n+- [lib/healthcheck/worker.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/worker.go#L343)\n+  - Modifying the `dialEndpoint()` function to make TLS requests.\n+\n+[manager.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/manager.go) and [target.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/target.go) have minor changes.\n+\n+Prometheus gauge metrics are added to the `healthcheck` package, and described in the [Prometheus Implementation](#prometheus-implementation).\n+\n+\n+#### Health States\n+\n+A Kubernetes cluster may be in a health state of `unknown`, `healthy` or `unhealthy`.\n+- `unknown` indicates a Kubernetes cluster cannot be contacted\n+- `healthy` indicates a Kubernetes cluster is accepting requests\n+- `unhealthy` indicates an error state, and includes an error message with verbose debugging information, if available\n+\n+See the database health check RFD for [more details](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#health-status).\n+\n+\n+##### Health Check Endpoint\n+\n+> [!NOTE]\n+> Kubernetes health checked at `https://<address>/readyz`\n+\n+> [!IMPORTANT]\n+> Requires Kubernetes v1.16 or higher\n+\n+The Kubernetes API `/readyz` endpoint is selected for health checking, and indicates that a Kubernetes API server is ready to accept requests. Kubernetes serves the endpoint through [TLS by default](https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security).\n+\n+The Kubernetes API offers [several health check endpoints](https://kubernetes.io/docs/reference/using-api/health-checks), as well as TCP checks being available.\n+\n+| Approach        | Description                                              |\n+|-----------------|----------------------------------------------------------|\n+| /readyz         | Ready to accept API requests                             |\n+| /readyz?verbose | Ready to accept API requests (detailed)                  |\n+| /livez          | kube-apiserver process is alive/running                  |\n+| /livez?verbose  | kube-apiserver process is alive/running (detailed)       |\n+| /healthz        | Ambiguously alive or ready. Deprecated in 2019 at v1.16  |\n+| TCP             | Can establish TCP connection to API server port          |\n+\n+Let's explore the options and reasoning for selecting `/readyz`.\n+\n+`/readyz` means that the cluster is accepting API requests, and can be used.\n+\n+`/livez` indicates the Kubernetes kube-apiserver process is alive. API requests may or may not be accepted. There's no implication of whole cluster readiness.\n+\n+`/healthz` is deprecated, and not supported with the Kubernetes health check feature. `/healthz` was [deprecated in September of 2019 with Kubernetes v1.16](https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/). At the time of writing, [Kubernetes is at v1.33](https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/), and six years have passed since v1.16. It seems reasonable not to support the `/healthz` endpoint. The choice then sets up a requirement for customers to use Kubernetes v1.16 or higher with Teleport Kubernetes health checks.\n+\n+Moving on to `TCP`, `TCP` indicates that network connectivity is available. No further knowledge of Kubernetes health would be known. In scenarios where servers don't offer explicit health checks, such as databases, `TCP` may be the only choice. Since Kubernetes offers health checks, we can skip `TCP` checks.\n+\n+So, `/livez` and `TCP` indicate some level of health, but do not necessarily mean the Kubernetes cluster can be used. \n+\n+Let's look at the `verbose` query parameter.\n+\n+`/readyz?verbose` provides a list of Kubernetes modules with `ok / not ok` states. The verbose information is not critical in the common case of a healthy cluster returning a `200` HTTP status code. The verbose information may be helpful to an administrator diagnosing an unhealthy cluster.\n+\n+For efficiency in the common case of a healthy cluster, the `/readyz` endpoint is called and checked for a `200` status code. In nearly all cases we only need to check `200`. The `verbose` body message is not sent, reducing unneeded network, memory, and processor consumption. Also, the Kubernetes authors recommend [relying on the status code](https://kubernetes.io/docs/reference/using-api/health-checks/) for checking state.\n+\n+In the case of non-`200` response codes, a follow-up call to `/readyz?verbose` is made. The follow-up verbose message is appended to a Go error, and eventually forwarded to the Web UI for a Teleport administrator to view.",
        "comment_created_at": "2025-08-26T00:14:20+00:00",
        "comment_author": "rana",
        "comment_body": "Not a concern. \r\n\r\nThere isn't any rate limiting to the KubeAPI `/readyz` endpoint.\r\n\r\nUnder [high load scenarios](https://kubernetes.io/docs/concepts/cluster-administration/flow-control/#prioritylevelconfiguration) an `HTTP 429 (Too Many Requests)` error is possible, but health endpoints are exempted. See Kubernetes PR [apf: exempt probes /healthz /livez /readyz](https://github.com/kubernetes/kubernetes/pull/100678) for a discussion, and [schema](https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/priority-and-fairness/health-for-strangers.yaml). That feature is backported to [Kube v1.18](https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/) (March 2020). \r\n\r\nKube API has [API Priority and Fairness](https://kubernetes.io/docs/concepts/cluster-administration/flow-control/), which is analogous to rate limits, but not identical. Essentially, certain types of requests are scheduled in as fair way as possible, until some limit.\r\n\r\nIf there were to be failed requests, the Teleport `healthcheck` package has interval configuration. Increasing the interval period from the default `30s` to `1m` or greater might help. Maybe that could release pressure for MySQL.\r\n\r\nIf the Kubernetes cluster is under load stress, and not able to respond, seeing failing health is probably a good thing.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2299669320",
    "pr_number": 58065,
    "pr_file": "rfd/0223-k8s-health-checks.md",
    "created_at": "2025-08-26T03:42:26+00:00",
    "commented_code": "+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. \n+\n+Main change points are:\n+- [lib/healthcheck/worker.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/worker.go#L343)\n+  - Modifying the `dialEndpoint()` function to make TLS requests.\n+\n+[manager.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/manager.go) and [target.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/target.go) have minor changes.\n+\n+Prometheus gauge metrics are added to the `healthcheck` package, and described in the [Prometheus Implementation](#prometheus-implementation).\n+\n+\n+#### Health States\n+\n+A Kubernetes cluster may be in a health state of `unknown`, `healthy` or `unhealthy`.\n+- `unknown` indicates a Kubernetes cluster cannot be contacted\n+- `healthy` indicates a Kubernetes cluster is accepting requests\n+- `unhealthy` indicates an error state, and includes an error message with verbose debugging information, if available\n+\n+See the database health check RFD for [more details](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#health-status).\n+\n+\n+##### Health Check Endpoint\n+\n+> [!NOTE]\n+> Kubernetes health checked at `https://<address>/readyz`\n+\n+> [!IMPORTANT]\n+> Requires Kubernetes v1.16 or higher\n+\n+The Kubernetes API `/readyz` endpoint is selected for health checking, and indicates that a Kubernetes API server is ready to accept requests. Kubernetes serves the endpoint through [TLS by default](https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security).\n+\n+The Kubernetes API offers [several health check endpoints](https://kubernetes.io/docs/reference/using-api/health-checks), as well as TCP checks being available.\n+\n+| Approach        | Description                                              |\n+|-----------------|----------------------------------------------------------|\n+| /readyz         | Ready to accept API requests                             |\n+| /readyz?verbose | Ready to accept API requests (detailed)                  |\n+| /livez          | kube-apiserver process is alive/running                  |\n+| /livez?verbose  | kube-apiserver process is alive/running (detailed)       |\n+| /healthz        | Ambiguously alive or ready. Deprecated in 2019 at v1.16  |\n+| TCP             | Can establish TCP connection to API server port          |\n+\n+Let's explore the options and reasoning for selecting `/readyz`.\n+\n+`/readyz` means that the cluster is accepting API requests, and can be used.\n+\n+`/livez` indicates the Kubernetes kube-apiserver process is alive. API requests may or may not be accepted. There's no implication of whole cluster readiness.\n+\n+`/healthz` is deprecated, and not supported with the Kubernetes health check feature. `/healthz` was [deprecated in September of 2019 with Kubernetes v1.16](https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/). At the time of writing, [Kubernetes is at v1.33](https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/), and six years have passed since v1.16. It seems reasonable not to support the `/healthz` endpoint. The choice then sets up a requirement for customers to use Kubernetes v1.16 or higher with Teleport Kubernetes health checks.\n+\n+Moving on to `TCP`, `TCP` indicates that network connectivity is available. No further knowledge of Kubernetes health would be known. In scenarios where servers don't offer explicit health checks, such as databases, `TCP` may be the only choice. Since Kubernetes offers health checks, we can skip `TCP` checks.\n+\n+So, `/livez` and `TCP` indicate some level of health, but do not necessarily mean the Kubernetes cluster can be used.",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "2299669320",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299669320",
        "commented_code": "@@ -0,0 +1,623 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. \n+\n+Main change points are:\n+- [lib/healthcheck/worker.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/worker.go#L343)\n+  - Modifying the `dialEndpoint()` function to make TLS requests.\n+\n+[manager.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/manager.go) and [target.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/target.go) have minor changes.\n+\n+Prometheus gauge metrics are added to the `healthcheck` package, and described in the [Prometheus Implementation](#prometheus-implementation).\n+\n+\n+#### Health States\n+\n+A Kubernetes cluster may be in a health state of `unknown`, `healthy` or `unhealthy`.\n+- `unknown` indicates a Kubernetes cluster cannot be contacted\n+- `healthy` indicates a Kubernetes cluster is accepting requests\n+- `unhealthy` indicates an error state, and includes an error message with verbose debugging information, if available\n+\n+See the database health check RFD for [more details](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#health-status).\n+\n+\n+##### Health Check Endpoint\n+\n+> [!NOTE]\n+> Kubernetes health checked at `https://<address>/readyz`\n+\n+> [!IMPORTANT]\n+> Requires Kubernetes v1.16 or higher\n+\n+The Kubernetes API `/readyz` endpoint is selected for health checking, and indicates that a Kubernetes API server is ready to accept requests. Kubernetes serves the endpoint through [TLS by default](https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security).\n+\n+The Kubernetes API offers [several health check endpoints](https://kubernetes.io/docs/reference/using-api/health-checks), as well as TCP checks being available.\n+\n+| Approach        | Description                                              |\n+|-----------------|----------------------------------------------------------|\n+| /readyz         | Ready to accept API requests                             |\n+| /readyz?verbose | Ready to accept API requests (detailed)                  |\n+| /livez          | kube-apiserver process is alive/running                  |\n+| /livez?verbose  | kube-apiserver process is alive/running (detailed)       |\n+| /healthz        | Ambiguously alive or ready. Deprecated in 2019 at v1.16  |\n+| TCP             | Can establish TCP connection to API server port          |\n+\n+Let's explore the options and reasoning for selecting `/readyz`.\n+\n+`/readyz` means that the cluster is accepting API requests, and can be used.\n+\n+`/livez` indicates the Kubernetes kube-apiserver process is alive. API requests may or may not be accepted. There's no implication of whole cluster readiness.\n+\n+`/healthz` is deprecated, and not supported with the Kubernetes health check feature. `/healthz` was [deprecated in September of 2019 with Kubernetes v1.16](https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/). At the time of writing, [Kubernetes is at v1.33](https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/), and six years have passed since v1.16. It seems reasonable not to support the `/healthz` endpoint. The choice then sets up a requirement for customers to use Kubernetes v1.16 or higher with Teleport Kubernetes health checks.\n+\n+Moving on to `TCP`, `TCP` indicates that network connectivity is available. No further knowledge of Kubernetes health would be known. In scenarios where servers don't offer explicit health checks, such as databases, `TCP` may be the only choice. Since Kubernetes offers health checks, we can skip `TCP` checks.\n+\n+So, `/livez` and `TCP` indicate some level of health, but do not necessarily mean the Kubernetes cluster can be used. ",
        "comment_created_at": "2025-08-26T03:42:26+00:00",
        "comment_author": "GavinFrazar",
        "comment_body": "I think Teleport health checks should be used to detect connectivity or configuration issues between Teleport and some resource.\r\n\r\n/readyz is getting into the weeds of health monitoring and may indicate a kube cluster problem that has nothing to do with Teleport, so I'm not sure how much value it brings to a Teleport cluster operator.\r\n\r\nAre there any concerns with doing just TCP health checks?\r\nOr any major benefits of doing HTTP checks?\r\nIf not, I think TCP is a reasonable starting point for kube health checks.",
        "pr_file_module": null
      },
      {
        "comment_id": "2301231673",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299669320",
        "commented_code": "@@ -0,0 +1,623 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. \n+\n+Main change points are:\n+- [lib/healthcheck/worker.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/worker.go#L343)\n+  - Modifying the `dialEndpoint()` function to make TLS requests.\n+\n+[manager.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/manager.go) and [target.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/target.go) have minor changes.\n+\n+Prometheus gauge metrics are added to the `healthcheck` package, and described in the [Prometheus Implementation](#prometheus-implementation).\n+\n+\n+#### Health States\n+\n+A Kubernetes cluster may be in a health state of `unknown`, `healthy` or `unhealthy`.\n+- `unknown` indicates a Kubernetes cluster cannot be contacted\n+- `healthy` indicates a Kubernetes cluster is accepting requests\n+- `unhealthy` indicates an error state, and includes an error message with verbose debugging information, if available\n+\n+See the database health check RFD for [more details](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#health-status).\n+\n+\n+##### Health Check Endpoint\n+\n+> [!NOTE]\n+> Kubernetes health checked at `https://<address>/readyz`\n+\n+> [!IMPORTANT]\n+> Requires Kubernetes v1.16 or higher\n+\n+The Kubernetes API `/readyz` endpoint is selected for health checking, and indicates that a Kubernetes API server is ready to accept requests. Kubernetes serves the endpoint through [TLS by default](https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security).\n+\n+The Kubernetes API offers [several health check endpoints](https://kubernetes.io/docs/reference/using-api/health-checks), as well as TCP checks being available.\n+\n+| Approach        | Description                                              |\n+|-----------------|----------------------------------------------------------|\n+| /readyz         | Ready to accept API requests                             |\n+| /readyz?verbose | Ready to accept API requests (detailed)                  |\n+| /livez          | kube-apiserver process is alive/running                  |\n+| /livez?verbose  | kube-apiserver process is alive/running (detailed)       |\n+| /healthz        | Ambiguously alive or ready. Deprecated in 2019 at v1.16  |\n+| TCP             | Can establish TCP connection to API server port          |\n+\n+Let's explore the options and reasoning for selecting `/readyz`.\n+\n+`/readyz` means that the cluster is accepting API requests, and can be used.\n+\n+`/livez` indicates the Kubernetes kube-apiserver process is alive. API requests may or may not be accepted. There's no implication of whole cluster readiness.\n+\n+`/healthz` is deprecated, and not supported with the Kubernetes health check feature. `/healthz` was [deprecated in September of 2019 with Kubernetes v1.16](https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/). At the time of writing, [Kubernetes is at v1.33](https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/), and six years have passed since v1.16. It seems reasonable not to support the `/healthz` endpoint. The choice then sets up a requirement for customers to use Kubernetes v1.16 or higher with Teleport Kubernetes health checks.\n+\n+Moving on to `TCP`, `TCP` indicates that network connectivity is available. No further knowledge of Kubernetes health would be known. In scenarios where servers don't offer explicit health checks, such as databases, `TCP` may be the only choice. Since Kubernetes offers health checks, we can skip `TCP` checks.\n+\n+So, `/livez` and `TCP` indicate some level of health, but do not necessarily mean the Kubernetes cluster can be used. ",
        "comment_created_at": "2025-08-26T14:41:50+00:00",
        "comment_author": "rosstimothy",
        "comment_body": "TCP might be a good starting point, though we've got at least one customer that has stated they want Kubernetes health checks to determine if the cluster is online AND if the Teleport agent is configured properly to communicate to the cluster. By using `/readyz` we can potentially identify cases where the agent was deployed with insufficient credentials and is unable to talk to the Kubernetes API.",
        "pr_file_module": null
      },
      {
        "comment_id": "2301876515",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299669320",
        "commented_code": "@@ -0,0 +1,623 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. \n+\n+Main change points are:\n+- [lib/healthcheck/worker.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/worker.go#L343)\n+  - Modifying the `dialEndpoint()` function to make TLS requests.\n+\n+[manager.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/manager.go) and [target.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/target.go) have minor changes.\n+\n+Prometheus gauge metrics are added to the `healthcheck` package, and described in the [Prometheus Implementation](#prometheus-implementation).\n+\n+\n+#### Health States\n+\n+A Kubernetes cluster may be in a health state of `unknown`, `healthy` or `unhealthy`.\n+- `unknown` indicates a Kubernetes cluster cannot be contacted\n+- `healthy` indicates a Kubernetes cluster is accepting requests\n+- `unhealthy` indicates an error state, and includes an error message with verbose debugging information, if available\n+\n+See the database health check RFD for [more details](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#health-status).\n+\n+\n+##### Health Check Endpoint\n+\n+> [!NOTE]\n+> Kubernetes health checked at `https://<address>/readyz`\n+\n+> [!IMPORTANT]\n+> Requires Kubernetes v1.16 or higher\n+\n+The Kubernetes API `/readyz` endpoint is selected for health checking, and indicates that a Kubernetes API server is ready to accept requests. Kubernetes serves the endpoint through [TLS by default](https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security).\n+\n+The Kubernetes API offers [several health check endpoints](https://kubernetes.io/docs/reference/using-api/health-checks), as well as TCP checks being available.\n+\n+| Approach        | Description                                              |\n+|-----------------|----------------------------------------------------------|\n+| /readyz         | Ready to accept API requests                             |\n+| /readyz?verbose | Ready to accept API requests (detailed)                  |\n+| /livez          | kube-apiserver process is alive/running                  |\n+| /livez?verbose  | kube-apiserver process is alive/running (detailed)       |\n+| /healthz        | Ambiguously alive or ready. Deprecated in 2019 at v1.16  |\n+| TCP             | Can establish TCP connection to API server port          |\n+\n+Let's explore the options and reasoning for selecting `/readyz`.\n+\n+`/readyz` means that the cluster is accepting API requests, and can be used.\n+\n+`/livez` indicates the Kubernetes kube-apiserver process is alive. API requests may or may not be accepted. There's no implication of whole cluster readiness.\n+\n+`/healthz` is deprecated, and not supported with the Kubernetes health check feature. `/healthz` was [deprecated in September of 2019 with Kubernetes v1.16](https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/). At the time of writing, [Kubernetes is at v1.33](https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/), and six years have passed since v1.16. It seems reasonable not to support the `/healthz` endpoint. The choice then sets up a requirement for customers to use Kubernetes v1.16 or higher with Teleport Kubernetes health checks.\n+\n+Moving on to `TCP`, `TCP` indicates that network connectivity is available. No further knowledge of Kubernetes health would be known. In scenarios where servers don't offer explicit health checks, such as databases, `TCP` may be the only choice. Since Kubernetes offers health checks, we can skip `TCP` checks.\n+\n+So, `/livez` and `TCP` indicate some level of health, but do not necessarily mean the Kubernetes cluster can be used. ",
        "comment_created_at": "2025-08-26T19:00:41+00:00",
        "comment_author": "GavinFrazar",
        "comment_body": "> By using /readyz we can potentially identify cases where the agent was deployed with insufficient credentials and is unable to talk to the Kubernetes API.\r\n\r\nYou can curl that endpoint without credentials, although now I looked it up and it looks like you could disable anonymous access to it. Do people enforce RBAC on /readyz though?",
        "pr_file_module": null
      },
      {
        "comment_id": "2301949569",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299669320",
        "commented_code": "@@ -0,0 +1,623 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. \n+\n+Main change points are:\n+- [lib/healthcheck/worker.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/worker.go#L343)\n+  - Modifying the `dialEndpoint()` function to make TLS requests.\n+\n+[manager.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/manager.go) and [target.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/target.go) have minor changes.\n+\n+Prometheus gauge metrics are added to the `healthcheck` package, and described in the [Prometheus Implementation](#prometheus-implementation).\n+\n+\n+#### Health States\n+\n+A Kubernetes cluster may be in a health state of `unknown`, `healthy` or `unhealthy`.\n+- `unknown` indicates a Kubernetes cluster cannot be contacted\n+- `healthy` indicates a Kubernetes cluster is accepting requests\n+- `unhealthy` indicates an error state, and includes an error message with verbose debugging information, if available\n+\n+See the database health check RFD for [more details](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#health-status).\n+\n+\n+##### Health Check Endpoint\n+\n+> [!NOTE]\n+> Kubernetes health checked at `https://<address>/readyz`\n+\n+> [!IMPORTANT]\n+> Requires Kubernetes v1.16 or higher\n+\n+The Kubernetes API `/readyz` endpoint is selected for health checking, and indicates that a Kubernetes API server is ready to accept requests. Kubernetes serves the endpoint through [TLS by default](https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security).\n+\n+The Kubernetes API offers [several health check endpoints](https://kubernetes.io/docs/reference/using-api/health-checks), as well as TCP checks being available.\n+\n+| Approach        | Description                                              |\n+|-----------------|----------------------------------------------------------|\n+| /readyz         | Ready to accept API requests                             |\n+| /readyz?verbose | Ready to accept API requests (detailed)                  |\n+| /livez          | kube-apiserver process is alive/running                  |\n+| /livez?verbose  | kube-apiserver process is alive/running (detailed)       |\n+| /healthz        | Ambiguously alive or ready. Deprecated in 2019 at v1.16  |\n+| TCP             | Can establish TCP connection to API server port          |\n+\n+Let's explore the options and reasoning for selecting `/readyz`.\n+\n+`/readyz` means that the cluster is accepting API requests, and can be used.\n+\n+`/livez` indicates the Kubernetes kube-apiserver process is alive. API requests may or may not be accepted. There's no implication of whole cluster readiness.\n+\n+`/healthz` is deprecated, and not supported with the Kubernetes health check feature. `/healthz` was [deprecated in September of 2019 with Kubernetes v1.16](https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/). At the time of writing, [Kubernetes is at v1.33](https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/), and six years have passed since v1.16. It seems reasonable not to support the `/healthz` endpoint. The choice then sets up a requirement for customers to use Kubernetes v1.16 or higher with Teleport Kubernetes health checks.\n+\n+Moving on to `TCP`, `TCP` indicates that network connectivity is available. No further knowledge of Kubernetes health would be known. In scenarios where servers don't offer explicit health checks, such as databases, `TCP` may be the only choice. Since Kubernetes offers health checks, we can skip `TCP` checks.\n+\n+So, `/livez` and `TCP` indicate some level of health, but do not necessarily mean the Kubernetes cluster can be used. ",
        "comment_created_at": "2025-08-26T19:33:18+00:00",
        "comment_author": "rana",
        "comment_body": "TCP is a valid health check, and TCP checks to kube are already functioning in the PoC.\r\n\r\n> Are there any concerns with doing just TCP health checks?\r\n\r\nTCP check reports ok, but the Teleport agent isn't configured properly to communicate to the cluster.\r\n\r\n> /readyz is getting into the weeds of health monitoring and may indicate a kube cluster problem that has nothing to do with Teleport, so I'm not sure how much value it brings to a Teleport cluster operator.\r\n\r\nThat's a great point. It's true that kube cluster problems that have nothing to do with Teleport may be reported by `/readyz`. It really is a question of where to draw the line.\r\n\r\nPerspective A:\r\n- Focus on the Teleport access layer\r\n- TCP is sufficient\r\n- Operator follows up with `tctl`, kube tools (`kubectl get --raw='/readyz?verbose'`) \r\n- Operator follows up with observability solutions (Prometheus, DataDog, etc)\r\n\r\nPerspective B:\r\n- Provide more value to the customer where possible\r\n- Use existing resource health mechanisms (Kube health API)\r\n- Moderate use of kube health APIs (skip node and pod health)\r\n- Operator follows up with `tctl`, kube tools (`kubectl`) \r\n- Operator follows up with observability solutions (Prometheus, DataDog, etc)\r\n\r\nHTTP checks add more meaning to what \"healthy\" is. `/readyz` indicates healthy Kube components are capable of receiving requests. Also, calling `/readyz?verbose` for an unhealthy cluster might help to identify an unhealthy component. Kubernetes intentionally built an API for checking health. \r\n\r\nIt sounds like the key is:\r\n\r\n> AND if the Teleport agent is configured properly to communicate to the cluster.",
        "pr_file_module": null
      },
      {
        "comment_id": "2307291794",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299669320",
        "commented_code": "@@ -0,0 +1,623 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. \n+\n+Main change points are:\n+- [lib/healthcheck/worker.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/worker.go#L343)\n+  - Modifying the `dialEndpoint()` function to make TLS requests.\n+\n+[manager.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/manager.go) and [target.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/target.go) have minor changes.\n+\n+Prometheus gauge metrics are added to the `healthcheck` package, and described in the [Prometheus Implementation](#prometheus-implementation).\n+\n+\n+#### Health States\n+\n+A Kubernetes cluster may be in a health state of `unknown`, `healthy` or `unhealthy`.\n+- `unknown` indicates a Kubernetes cluster cannot be contacted\n+- `healthy` indicates a Kubernetes cluster is accepting requests\n+- `unhealthy` indicates an error state, and includes an error message with verbose debugging information, if available\n+\n+See the database health check RFD for [more details](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#health-status).\n+\n+\n+##### Health Check Endpoint\n+\n+> [!NOTE]\n+> Kubernetes health checked at `https://<address>/readyz`\n+\n+> [!IMPORTANT]\n+> Requires Kubernetes v1.16 or higher\n+\n+The Kubernetes API `/readyz` endpoint is selected for health checking, and indicates that a Kubernetes API server is ready to accept requests. Kubernetes serves the endpoint through [TLS by default](https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security).\n+\n+The Kubernetes API offers [several health check endpoints](https://kubernetes.io/docs/reference/using-api/health-checks), as well as TCP checks being available.\n+\n+| Approach        | Description                                              |\n+|-----------------|----------------------------------------------------------|\n+| /readyz         | Ready to accept API requests                             |\n+| /readyz?verbose | Ready to accept API requests (detailed)                  |\n+| /livez          | kube-apiserver process is alive/running                  |\n+| /livez?verbose  | kube-apiserver process is alive/running (detailed)       |\n+| /healthz        | Ambiguously alive or ready. Deprecated in 2019 at v1.16  |\n+| TCP             | Can establish TCP connection to API server port          |\n+\n+Let's explore the options and reasoning for selecting `/readyz`.\n+\n+`/readyz` means that the cluster is accepting API requests, and can be used.\n+\n+`/livez` indicates the Kubernetes kube-apiserver process is alive. API requests may or may not be accepted. There's no implication of whole cluster readiness.\n+\n+`/healthz` is deprecated, and not supported with the Kubernetes health check feature. `/healthz` was [deprecated in September of 2019 with Kubernetes v1.16](https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/). At the time of writing, [Kubernetes is at v1.33](https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/), and six years have passed since v1.16. It seems reasonable not to support the `/healthz` endpoint. The choice then sets up a requirement for customers to use Kubernetes v1.16 or higher with Teleport Kubernetes health checks.\n+\n+Moving on to `TCP`, `TCP` indicates that network connectivity is available. No further knowledge of Kubernetes health would be known. In scenarios where servers don't offer explicit health checks, such as databases, `TCP` may be the only choice. Since Kubernetes offers health checks, we can skip `TCP` checks.\n+\n+So, `/livez` and `TCP` indicate some level of health, but do not necessarily mean the Kubernetes cluster can be used. ",
        "comment_created_at": "2025-08-28T12:39:46+00:00",
        "comment_author": "rosstimothy",
        "comment_body": "Tiago [echoed](https://github.com/gravitational/teleport/pull/58065#pullrequestreview-3163464098) the same sentiment. We likely need to go beyond is the Kubernetes API available in order to ensure that the agent can perform the operations needed for users to successfully connect to Kubernetes clusters via Teleport. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2317497324",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299669320",
        "commented_code": "@@ -0,0 +1,623 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. \n+\n+Main change points are:\n+- [lib/healthcheck/worker.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/worker.go#L343)\n+  - Modifying the `dialEndpoint()` function to make TLS requests.\n+\n+[manager.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/manager.go) and [target.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/target.go) have minor changes.\n+\n+Prometheus gauge metrics are added to the `healthcheck` package, and described in the [Prometheus Implementation](#prometheus-implementation).\n+\n+\n+#### Health States\n+\n+A Kubernetes cluster may be in a health state of `unknown`, `healthy` or `unhealthy`.\n+- `unknown` indicates a Kubernetes cluster cannot be contacted\n+- `healthy` indicates a Kubernetes cluster is accepting requests\n+- `unhealthy` indicates an error state, and includes an error message with verbose debugging information, if available\n+\n+See the database health check RFD for [more details](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#health-status).\n+\n+\n+##### Health Check Endpoint\n+\n+> [!NOTE]\n+> Kubernetes health checked at `https://<address>/readyz`\n+\n+> [!IMPORTANT]\n+> Requires Kubernetes v1.16 or higher\n+\n+The Kubernetes API `/readyz` endpoint is selected for health checking, and indicates that a Kubernetes API server is ready to accept requests. Kubernetes serves the endpoint through [TLS by default](https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security).\n+\n+The Kubernetes API offers [several health check endpoints](https://kubernetes.io/docs/reference/using-api/health-checks), as well as TCP checks being available.\n+\n+| Approach        | Description                                              |\n+|-----------------|----------------------------------------------------------|\n+| /readyz         | Ready to accept API requests                             |\n+| /readyz?verbose | Ready to accept API requests (detailed)                  |\n+| /livez          | kube-apiserver process is alive/running                  |\n+| /livez?verbose  | kube-apiserver process is alive/running (detailed)       |\n+| /healthz        | Ambiguously alive or ready. Deprecated in 2019 at v1.16  |\n+| TCP             | Can establish TCP connection to API server port          |\n+\n+Let's explore the options and reasoning for selecting `/readyz`.\n+\n+`/readyz` means that the cluster is accepting API requests, and can be used.\n+\n+`/livez` indicates the Kubernetes kube-apiserver process is alive. API requests may or may not be accepted. There's no implication of whole cluster readiness.\n+\n+`/healthz` is deprecated, and not supported with the Kubernetes health check feature. `/healthz` was [deprecated in September of 2019 with Kubernetes v1.16](https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/). At the time of writing, [Kubernetes is at v1.33](https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/), and six years have passed since v1.16. It seems reasonable not to support the `/healthz` endpoint. The choice then sets up a requirement for customers to use Kubernetes v1.16 or higher with Teleport Kubernetes health checks.\n+\n+Moving on to `TCP`, `TCP` indicates that network connectivity is available. No further knowledge of Kubernetes health would be known. In scenarios where servers don't offer explicit health checks, such as databases, `TCP` may be the only choice. Since Kubernetes offers health checks, we can skip `TCP` checks.\n+\n+So, `/livez` and `TCP` indicate some level of health, but do not necessarily mean the Kubernetes cluster can be used. ",
        "comment_created_at": "2025-09-03T00:54:16+00:00",
        "comment_author": "rana",
        "comment_body": "@creack, @tigrato What are your thoughts on whether and how to health check Kubernetes clusters with RBAC? If the health check goal is \"Can the customer use the cluster\", would exercising a single Teleport Kubernetes operation, such as `exec`, `port-forward`, or `ephemeral container` be sufficient? Or, do you seen another route for exercising RBAC?",
        "pr_file_module": null
      },
      {
        "comment_id": "2318356571",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299669320",
        "commented_code": "@@ -0,0 +1,623 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. \n+\n+Main change points are:\n+- [lib/healthcheck/worker.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/worker.go#L343)\n+  - Modifying the `dialEndpoint()` function to make TLS requests.\n+\n+[manager.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/manager.go) and [target.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/target.go) have minor changes.\n+\n+Prometheus gauge metrics are added to the `healthcheck` package, and described in the [Prometheus Implementation](#prometheus-implementation).\n+\n+\n+#### Health States\n+\n+A Kubernetes cluster may be in a health state of `unknown`, `healthy` or `unhealthy`.\n+- `unknown` indicates a Kubernetes cluster cannot be contacted\n+- `healthy` indicates a Kubernetes cluster is accepting requests\n+- `unhealthy` indicates an error state, and includes an error message with verbose debugging information, if available\n+\n+See the database health check RFD for [more details](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#health-status).\n+\n+\n+##### Health Check Endpoint\n+\n+> [!NOTE]\n+> Kubernetes health checked at `https://<address>/readyz`\n+\n+> [!IMPORTANT]\n+> Requires Kubernetes v1.16 or higher\n+\n+The Kubernetes API `/readyz` endpoint is selected for health checking, and indicates that a Kubernetes API server is ready to accept requests. Kubernetes serves the endpoint through [TLS by default](https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security).\n+\n+The Kubernetes API offers [several health check endpoints](https://kubernetes.io/docs/reference/using-api/health-checks), as well as TCP checks being available.\n+\n+| Approach        | Description                                              |\n+|-----------------|----------------------------------------------------------|\n+| /readyz         | Ready to accept API requests                             |\n+| /readyz?verbose | Ready to accept API requests (detailed)                  |\n+| /livez          | kube-apiserver process is alive/running                  |\n+| /livez?verbose  | kube-apiserver process is alive/running (detailed)       |\n+| /healthz        | Ambiguously alive or ready. Deprecated in 2019 at v1.16  |\n+| TCP             | Can establish TCP connection to API server port          |\n+\n+Let's explore the options and reasoning for selecting `/readyz`.\n+\n+`/readyz` means that the cluster is accepting API requests, and can be used.\n+\n+`/livez` indicates the Kubernetes kube-apiserver process is alive. API requests may or may not be accepted. There's no implication of whole cluster readiness.\n+\n+`/healthz` is deprecated, and not supported with the Kubernetes health check feature. `/healthz` was [deprecated in September of 2019 with Kubernetes v1.16](https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/). At the time of writing, [Kubernetes is at v1.33](https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/), and six years have passed since v1.16. It seems reasonable not to support the `/healthz` endpoint. The choice then sets up a requirement for customers to use Kubernetes v1.16 or higher with Teleport Kubernetes health checks.\n+\n+Moving on to `TCP`, `TCP` indicates that network connectivity is available. No further knowledge of Kubernetes health would be known. In scenarios where servers don't offer explicit health checks, such as databases, `TCP` may be the only choice. Since Kubernetes offers health checks, we can skip `TCP` checks.\n+\n+So, `/livez` and `TCP` indicate some level of health, but do not necessarily mean the Kubernetes cluster can be used. ",
        "comment_created_at": "2025-09-03T09:22:56+00:00",
        "comment_author": "tigrato",
        "comment_body": "@rana usually this check is performed using https://github.com/gravitational/teleport/blob/b1b6bb231bf61b9b839242c97e38801ad93cf957/lib/kube/proxy/auth.go#L251-L269\r\n\r\nYou can add an extra check for get pods. This will check if the agent is able to impersonate users, service accounts and groups in the cluster. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2323921137",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299669320",
        "commented_code": "@@ -0,0 +1,623 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. \n+\n+Main change points are:\n+- [lib/healthcheck/worker.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/worker.go#L343)\n+  - Modifying the `dialEndpoint()` function to make TLS requests.\n+\n+[manager.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/manager.go) and [target.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/target.go) have minor changes.\n+\n+Prometheus gauge metrics are added to the `healthcheck` package, and described in the [Prometheus Implementation](#prometheus-implementation).\n+\n+\n+#### Health States\n+\n+A Kubernetes cluster may be in a health state of `unknown`, `healthy` or `unhealthy`.\n+- `unknown` indicates a Kubernetes cluster cannot be contacted\n+- `healthy` indicates a Kubernetes cluster is accepting requests\n+- `unhealthy` indicates an error state, and includes an error message with verbose debugging information, if available\n+\n+See the database health check RFD for [more details](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#health-status).\n+\n+\n+##### Health Check Endpoint\n+\n+> [!NOTE]\n+> Kubernetes health checked at `https://<address>/readyz`\n+\n+> [!IMPORTANT]\n+> Requires Kubernetes v1.16 or higher\n+\n+The Kubernetes API `/readyz` endpoint is selected for health checking, and indicates that a Kubernetes API server is ready to accept requests. Kubernetes serves the endpoint through [TLS by default](https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security).\n+\n+The Kubernetes API offers [several health check endpoints](https://kubernetes.io/docs/reference/using-api/health-checks), as well as TCP checks being available.\n+\n+| Approach        | Description                                              |\n+|-----------------|----------------------------------------------------------|\n+| /readyz         | Ready to accept API requests                             |\n+| /readyz?verbose | Ready to accept API requests (detailed)                  |\n+| /livez          | kube-apiserver process is alive/running                  |\n+| /livez?verbose  | kube-apiserver process is alive/running (detailed)       |\n+| /healthz        | Ambiguously alive or ready. Deprecated in 2019 at v1.16  |\n+| TCP             | Can establish TCP connection to API server port          |\n+\n+Let's explore the options and reasoning for selecting `/readyz`.\n+\n+`/readyz` means that the cluster is accepting API requests, and can be used.\n+\n+`/livez` indicates the Kubernetes kube-apiserver process is alive. API requests may or may not be accepted. There's no implication of whole cluster readiness.\n+\n+`/healthz` is deprecated, and not supported with the Kubernetes health check feature. `/healthz` was [deprecated in September of 2019 with Kubernetes v1.16](https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/). At the time of writing, [Kubernetes is at v1.33](https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/), and six years have passed since v1.16. It seems reasonable not to support the `/healthz` endpoint. The choice then sets up a requirement for customers to use Kubernetes v1.16 or higher with Teleport Kubernetes health checks.\n+\n+Moving on to `TCP`, `TCP` indicates that network connectivity is available. No further knowledge of Kubernetes health would be known. In scenarios where servers don't offer explicit health checks, such as databases, `TCP` may be the only choice. Since Kubernetes offers health checks, we can skip `TCP` checks.\n+\n+So, `/livez` and `TCP` indicate some level of health, but do not necessarily mean the Kubernetes cluster can be used. ",
        "comment_created_at": "2025-09-05T01:54:13+00:00",
        "comment_author": "rana",
        "comment_body": "Thanks Tiago \ud83d\udc4f \r\n\r\nA PoC is running with sequential checking of:\r\n- `SelfSubjectAccessReview`,\r\n-  `Pods(kubeCluster.GetNamespace()).List()`\r\n- `Namespaces().List()`.\r\n\r\n`Namespaces().List()` is also an option.\r\n\r\n@rosstimothy, @creack What are your thoughts on checking both `SelfSubjectAccessReview` and `Pods().List()`?\r\n\r\nhttps://github.com/gravitational/teleport/blob/fdd671bf7be973830118369becb157f4839fc3b5/lib/kube/proxy/server.go#L566-L579\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2324703753",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299669320",
        "commented_code": "@@ -0,0 +1,623 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. \n+\n+Main change points are:\n+- [lib/healthcheck/worker.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/worker.go#L343)\n+  - Modifying the `dialEndpoint()` function to make TLS requests.\n+\n+[manager.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/manager.go) and [target.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/target.go) have minor changes.\n+\n+Prometheus gauge metrics are added to the `healthcheck` package, and described in the [Prometheus Implementation](#prometheus-implementation).\n+\n+\n+#### Health States\n+\n+A Kubernetes cluster may be in a health state of `unknown`, `healthy` or `unhealthy`.\n+- `unknown` indicates a Kubernetes cluster cannot be contacted\n+- `healthy` indicates a Kubernetes cluster is accepting requests\n+- `unhealthy` indicates an error state, and includes an error message with verbose debugging information, if available\n+\n+See the database health check RFD for [more details](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#health-status).\n+\n+\n+##### Health Check Endpoint\n+\n+> [!NOTE]\n+> Kubernetes health checked at `https://<address>/readyz`\n+\n+> [!IMPORTANT]\n+> Requires Kubernetes v1.16 or higher\n+\n+The Kubernetes API `/readyz` endpoint is selected for health checking, and indicates that a Kubernetes API server is ready to accept requests. Kubernetes serves the endpoint through [TLS by default](https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security).\n+\n+The Kubernetes API offers [several health check endpoints](https://kubernetes.io/docs/reference/using-api/health-checks), as well as TCP checks being available.\n+\n+| Approach        | Description                                              |\n+|-----------------|----------------------------------------------------------|\n+| /readyz         | Ready to accept API requests                             |\n+| /readyz?verbose | Ready to accept API requests (detailed)                  |\n+| /livez          | kube-apiserver process is alive/running                  |\n+| /livez?verbose  | kube-apiserver process is alive/running (detailed)       |\n+| /healthz        | Ambiguously alive or ready. Deprecated in 2019 at v1.16  |\n+| TCP             | Can establish TCP connection to API server port          |\n+\n+Let's explore the options and reasoning for selecting `/readyz`.\n+\n+`/readyz` means that the cluster is accepting API requests, and can be used.\n+\n+`/livez` indicates the Kubernetes kube-apiserver process is alive. API requests may or may not be accepted. There's no implication of whole cluster readiness.\n+\n+`/healthz` is deprecated, and not supported with the Kubernetes health check feature. `/healthz` was [deprecated in September of 2019 with Kubernetes v1.16](https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/). At the time of writing, [Kubernetes is at v1.33](https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/), and six years have passed since v1.16. It seems reasonable not to support the `/healthz` endpoint. The choice then sets up a requirement for customers to use Kubernetes v1.16 or higher with Teleport Kubernetes health checks.\n+\n+Moving on to `TCP`, `TCP` indicates that network connectivity is available. No further knowledge of Kubernetes health would be known. In scenarios where servers don't offer explicit health checks, such as databases, `TCP` may be the only choice. Since Kubernetes offers health checks, we can skip `TCP` checks.\n+\n+So, `/livez` and `TCP` indicate some level of health, but do not necessarily mean the Kubernetes cluster can be used. ",
        "comment_created_at": "2025-09-05T10:23:43+00:00",
        "comment_author": "tigrato",
        "comment_body": "The agent doesn't have permissions to list pods or namespaces. Do that with the SelfSubjectAccessReviewSpec for \r\n\r\n```\r\nresp, err := sarClient.Create(ctx, &authzapi.SelfSubjectAccessReview{ \r\n \t\t\tSpec: authzapi.SelfSubjectAccessReviewSpec{ \r\n \t\t\t\tResourceAttributes: &authzapi.ResourceAttributes{ \r\n \t\t\t\t\tVerb:     \"get\", \r\n \t\t\t\t\tResource: \"pods\", \r\n \t\t\t\t}, \r\n \t\t\t}, \r\n \t\t}, metav1.CreateOptions{}) \r\n```\r\n`SelfSubjectAccessReview` allow an identity to see if they have some kind of permissions for a specific resource and verb. This is what kubectl auth can-i get pods does behind the scenes\r\n\r\n\r\nFor full list of permisions https://github.com/gravitational/teleport/blob/master/examples/chart/teleport-kube-agent/templates/clusterrole.yaml. If auto discovery is enabled, we should also check services but we can skip it if you want",
        "pr_file_module": null
      },
      {
        "comment_id": "2325867840",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299669320",
        "commented_code": "@@ -0,0 +1,623 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. \n+\n+Main change points are:\n+- [lib/healthcheck/worker.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/worker.go#L343)\n+  - Modifying the `dialEndpoint()` function to make TLS requests.\n+\n+[manager.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/manager.go) and [target.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/target.go) have minor changes.\n+\n+Prometheus gauge metrics are added to the `healthcheck` package, and described in the [Prometheus Implementation](#prometheus-implementation).\n+\n+\n+#### Health States\n+\n+A Kubernetes cluster may be in a health state of `unknown`, `healthy` or `unhealthy`.\n+- `unknown` indicates a Kubernetes cluster cannot be contacted\n+- `healthy` indicates a Kubernetes cluster is accepting requests\n+- `unhealthy` indicates an error state, and includes an error message with verbose debugging information, if available\n+\n+See the database health check RFD for [more details](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#health-status).\n+\n+\n+##### Health Check Endpoint\n+\n+> [!NOTE]\n+> Kubernetes health checked at `https://<address>/readyz`\n+\n+> [!IMPORTANT]\n+> Requires Kubernetes v1.16 or higher\n+\n+The Kubernetes API `/readyz` endpoint is selected for health checking, and indicates that a Kubernetes API server is ready to accept requests. Kubernetes serves the endpoint through [TLS by default](https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security).\n+\n+The Kubernetes API offers [several health check endpoints](https://kubernetes.io/docs/reference/using-api/health-checks), as well as TCP checks being available.\n+\n+| Approach        | Description                                              |\n+|-----------------|----------------------------------------------------------|\n+| /readyz         | Ready to accept API requests                             |\n+| /readyz?verbose | Ready to accept API requests (detailed)                  |\n+| /livez          | kube-apiserver process is alive/running                  |\n+| /livez?verbose  | kube-apiserver process is alive/running (detailed)       |\n+| /healthz        | Ambiguously alive or ready. Deprecated in 2019 at v1.16  |\n+| TCP             | Can establish TCP connection to API server port          |\n+\n+Let's explore the options and reasoning for selecting `/readyz`.\n+\n+`/readyz` means that the cluster is accepting API requests, and can be used.\n+\n+`/livez` indicates the Kubernetes kube-apiserver process is alive. API requests may or may not be accepted. There's no implication of whole cluster readiness.\n+\n+`/healthz` is deprecated, and not supported with the Kubernetes health check feature. `/healthz` was [deprecated in September of 2019 with Kubernetes v1.16](https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/). At the time of writing, [Kubernetes is at v1.33](https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/), and six years have passed since v1.16. It seems reasonable not to support the `/healthz` endpoint. The choice then sets up a requirement for customers to use Kubernetes v1.16 or higher with Teleport Kubernetes health checks.\n+\n+Moving on to `TCP`, `TCP` indicates that network connectivity is available. No further knowledge of Kubernetes health would be known. In scenarios where servers don't offer explicit health checks, such as databases, `TCP` may be the only choice. Since Kubernetes offers health checks, we can skip `TCP` checks.\n+\n+So, `/livez` and `TCP` indicate some level of health, but do not necessarily mean the Kubernetes cluster can be used. ",
        "comment_created_at": "2025-09-05T19:23:19+00:00",
        "comment_author": "rosstimothy",
        "comment_body": "@rana let's get the RFD updated with this information.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2325862393",
    "pr_number": 58065,
    "pr_file": "rfd/0223-k8s-health-checks.md",
    "created_at": "2025-09-05T19:20:01+00:00",
    "commented_code": "+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes.",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "2325862393",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2325862393",
        "commented_code": "@@ -0,0 +1,660 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. ",
        "comment_created_at": "2025-09-05T19:20:01+00:00",
        "comment_author": "rosstimothy",
        "comment_body": "> Adding TLS checks is a focus for Kubernetes.\n\nIs this accurate? We don't so much want just a TLS handshake to be successful as we want to make fully authenticated requests to a particular endpoint of the Kubernetes API and examine the response right?",
        "pr_file_module": null
      },
      {
        "comment_id": "2325888119",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2325862393",
        "commented_code": "@@ -0,0 +1,660 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`\n+- `(*TLSServer).Serve()` - Starts `healthcheck.Manager` \n+- `(*TLSServer).startStaticClustersHeartbeat()` - Registers all Kubernetes clusters for health monitoring\n+- `(*TLSServer).close()` - Unregisters all Kubernetes clusters from health monitoring\n+\n+Adding methods:\n+- `(*TLSServer).startTargetHealth()` - Registers a single Kubernetes cluster for health monitoring\n+- `(*TLSServer).stopTargetHealth()` - Unregisters a single Kubernetes cluster from health monitoring\n+- `(*TLSServer).getTargetHealth()` - Gets health for a single Kubernetes cluster\n+\n+\n+##### Core `healthcheck` Package\n+\n+The `healthcheck` package performs recurring health checks on one or more Teleport resources: databases, Kubernetes clusters, etc. It's a general library that currently supports TCP checks. Adding TLS checks is a focus for Kubernetes. ",
        "comment_created_at": "2025-09-05T19:31:57+00:00",
        "comment_author": "rana",
        "comment_body": "Not accurate as of today. I'll update that to reflect the focus being Kubernetes API calls with `SelfSubjectAccessReview`. That TLS is in use is incidental. Also, the [Health Check Endpoint](https://github.com/gravitational/teleport/blob/rfd/0223-k8s-health-checks/rfd/0223-k8s-health-checks.md#health-check-endpoint) section can be revised to be reframed as something that was considered.\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1502679436",
    "pr_number": 38078,
    "pr_file": "rfd/0164-scoped-rbac.md",
    "created_at": "2024-02-26T14:13:02+00:00",
    "commented_code": "+\n+---\n+authors: @klizhentas (sasha@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0163 - Scoped RBAC\n+\n+## Required Approvers\n+\n+* Engineering @r0mant && (@tigrato || @marcoandredinis)\n+* Security: (@reedloden || @jentfoo)\n+* Product: (@xinding33 || @klizhentas )\n+\n+## What\n+\n+This RFD introduces resource hierarchies and scopes to existing RBAC. \n+Our goal is simplify and evolve access control in Teleport without drastic changes or new policy languages. \n+Make it easier to integrate Teleport RBAC with cloud IAMs of AWS, GCP and Azure.\n+This RFD is closely modeled and inspired by Azure RBAC model, the most advanced out of 3 clouds. \n+\n+Read about it here https://learn.microsoft.com/en-us/azure/role-based-access-control/overview before diving into this RFD.\n+\n+## Why\n+\n+There are several structural issues with the current RBAC model in Teleport.\n+\n+### Scalability issues \n+\n+Every role has to be distributed to every node or proxy that has to perform authorization. \n+Current roles are brittle - to evaluate access, every single role has to be fetched and processed, \n+because each role can have a deny rule that can block access to any allowed rule.\n+\n+### Scoping issues\n+\n+It is not possible to describe \u201cdelegated admins\u201d in RBAC, when one user has administrative access over part of the cluster.\n+It is also not possible to specify that certain role options only apply in certain use-cases. \n+\n+For example, the setting `permit-agent-forward: false` will deny agent-forward to any matching resource with no exceptions, even if other roles allow it.\n+\n+It is not possible to allow admins to grant roles to other users but with certain restrictions, as in the example of issue https://github.com/gravitational/teleport/issues/16914. \n+\n+### Complexity \n+\n+Roles today have both labels and label expressions, login rules to inject traits and claims and templates. \n+This creates a complicated system that is hard to understand and reason about.\n+Role mapping is brittle and requires updating OIDC/SAML connector resources, which can break access.\n+\n+### Security issues\n+\n+Every role assignment and trait is encoded in certificate, and each time a user gets their roles updated, they have to get a new certificate. \n+Old certificates can be re-used to get privileges that have been removed, \n+creating \u201ca new enemy problem\u201d described in [Zanzibar Paper](https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/).\n+\n+Many roles allow \u201crole escapes\u201d, as any person who gets assigned a role that can create other roles, \n+would become an admin, see for example issue https://github.com/gravitational/teleport.e/issues/3111\n+\n+### Goals\n+\n+Our key goal is to evolve Teleport\u2019s roles without asking users to rewrite their existing RBAC.\n+We also would like to better integrate Teleport RBAC with cloud provider\u2019s IAM systems out of the box.\n+We would like to give Teleport\u2019s users \u201cbatteries included\u201d approach, when they can get 90% of the use-cases done without c\n+reating any new roles, or modifying existing ones.\n+\n+### Non-Goals\n+\n+We are not going to implement backwards-incompatible changes that require our customers rewrite their RBAC stack or adopt a completely new policy language. \n+\n+## Details\n+To understand the required changes, let\u2019s first take a look at Teleport RBAC structure. \n+\n+### RBAC Primer\n+\n+Let\u2019s start with fundamental Teleport RBAC concepts and highlight some issues as we review them.\n+\n+#### Roles\n+\n+Each user or bot in Teleport is assigned one or several roles. \n+For the full reference, take a look at the documentation at https://goteleport.com/docs/reference/resources/#role.\n+\n+Here is a role structure:\n+\n+```yaml\n+kind: role\n+version: v7\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  # options are a set of knobs that apply across the entire role set.\n+  # the most restrictive options wins\n+  options:\n+    # max_session_ttl defines the TTL (time to live) of certificates\n+    # issued to the users with this role.\n+    max_session_ttl: 8h\n+    # forward_agent controls whether SSH agent forwarding is allowed\n+    forward_agent: true\n+  # The allow section declares a list of resource/verb combinations that are\n+  # allowed for the users of this role. By default, nothing is allowed.\n+  #\n+  # Allow rules specify both actions that are allowed and match the resources\n+  # they are applying to.\n+  allow:\n+    # Some Allow fields specify target protocol\n+    #  login principals, like in this example, SSH logins\n+    logins: [root, '{{internal.logins}}']\n+\n+    # In this example, the fields specify a set of windows desktop logins\n+    windows_desktop_logins: [Administrator, '{{internal.logins}}']\n+\n+    # There are multiple types of labels and label expressions that \n+    # match different computing resources.\n+    node_labels:\n+      # literal strings:\n+      'env': 'test'\n+    # regexp expressions\n+      'reg': '^us-west-1|eu-central-1$'\n+\n+    # node_labels_expression has the same purpose as node_labels but\n+    # supports predicate expressions to configure custom logic.\n+    # A user with this role will be allowed to access nodes if they are in the\n+    # staging environment *or* if they belong to one of the user's own teams.\n+    node_labels_expression: |\n+      labels[\"env\"] == \"staging\" ||\n+      contains(user.spec.traits[\"teams\"] , labels[\"team\"])\n+ \n+    # rules allow a user holding this role to modify other resources\n+    # matching the expressions below.\n+    # rules match both resources and specify what actions are allowed\n+    rules:\n+      - resources: [role]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+We can spot several issues with this role design:\n+\n+* There is no way to specify the scope that role applies to, all roles apply globally to all resources they match and all users they assign to all the time.\n+* There is no way to specify resource hierarchies, like computing environments (env vs prod), which makes it hard to partition the infrastructure and forces customers to specify labels.\n+\n+\n+#### Labels\n+\n+Teleport RBAC\u2019s advice to engineers to partition their resource is to first, label their computing resource or use AWS labels, and second, match the labels in RBAC.\n+\n+Admins can set the tags on the resource configuration file statically, or, for some resources, use `server_info` to set the tags for each resource:\n+\n+```yaml\n+# server_info.yaml\n+kind: server_info\n+metadata:\n+   name: si-<node-name>\n+spec:\n+   new_labels:\n+      \"foo\": \"bar\"\n+```\n+\n+This creates several issues:\n+\n+* It is not always secure to delegate labeling to owners of computing resources, as anyone with root access to the node config file can update its labels impacting everyone else.\n+* It is not scalable, as it\u2019s not always useful to set and updates tags for each individual resource\n+* It\u2019s hard or impossible to partition infrastructure with two-dimensional labels, although users can use `env: prod` to mark all resources in the production environment, there is no way to say that `env: lab` is a subset of `env: prod`.\n+\n+#### Current Roles mapping\n+\n+In Teleport there are multiple ways to map roles to users: static roles mapping to local users and bots, \n+dynamic mapping to SSO via connectors and on-demand assignment via access lists and access requests.\n+\n+Most users start with local static mapping and SSO mapping, later graduating to access requests and access lists.\n+\n+https://goteleport.com/docs/access-controls/sso/#creating-an-authentication-connector\n+\n+For SSO users, on login, Teleport checks SSO connector, and maps attributes of a user to a list of roles:\n+\n+```yaml\n+# connector.yaml\n+kind: saml\n+version: v2\n+metadata:\n+  name: corporate\n+spec:\n+  attributes_to_roles:\n+    - {name: \"groups\", value: \"okta-admin\", roles: [\"access\"]}\n+     # regular expressions with capture are also supported. \n+     # the next line instructs Teleport\n+     # to assign users to roles `admin-1` if his SAML \"group\" \n+     # attribute equals 'ssh_admin_1':\n+   - { name: \"group\", value: \"^ssh_admin_(.*)$\", roles: [\"admin-$1\"] }\n+```\n+\n+This creates several security and scalability issues:\n+\n+* Every time administrators need to change roles assignment, they have to update the resource.\n+* Every time the Identity Provider changes user\u2019s attributes, the users would have to relogin to get new roles. \n+\n+For local users, administrators have to specify roles and traits in the local resource:\n+\n+```yaml\n+kind: user\n+version: v2\n+metadata:\n+  name: joe\n+spec:\n+  # roles is a list of roles assigned to this user\n+  roles:\n+  - admin\n+  # traits are key, list of values pairs assigned to a user resource.\n+  # Traits can be used in role templates as variables.\n+  traits:\n+    logins:\n+    - joe\n+    - root\n+```\n+\n+This also creates some challenges, as administrators are forced to update local resources each time they have to assign a user new permission.\n+\n+### Modifications\n+\n+Let\u2019s now introduce the missing pieces of the puzzle and review how new roles will simplify or deprecate legacy concepts.\n+\n+#### Hierarchical Resource Groups\n+\n+Resource groups are one such missing piece - in most cloud environments, resources are split into hierarchy, \n+for example, host `luna` is a member of a resource group `lab`, in turn a member of environment `prod`, which is in turn a member of a Teleport cluster.\n+\n+In Teleport, we will make cluster a default root of this hierarchy. Every computing resource by default will be a direct member of a root `cluster` resource group.\n+\n+One resource can be assigned to multiple resource groups at a time, or none. \n+\n+In our example, the cluster administrator would define lab resource group in the following way:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent: prod_env\n+```\n+\n+Administrators can assign resources to resource groups:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+   parent: prod_env\n+ match_kinds:\n+   - node\n+   - database\n+   - role\n+   - access_list\n+   - '.*'\n+ match_labels:  \n+   env: prod\n+```\n+\n+In this case, any resource that matches `env:prod` label will be assigned to this resource group. We will use the same label matching algorithm as in \n+\n+This will let administrators to gradually migrate their existing flat infrastructure to resource groups one.\n+\n+In some cases it makes sense to specify parent resource group inline:\n+\n+```yaml\n+kind: role\n+spec:\n+  parent_resource_group: /env/prod\n+```\n+\n+By default, if unspecified, a resource is a member of a root-level - `/` cluster resource group. If specified by the resource, it won't be a member of a root `/` resource group.\n+\n+Resource groups are hierarchical, and we can refer to the `lab` resource group by its full path as `/env/prod/lab`. \n+\n+Most Teleport resources, with some exceptions, like users, SSO connectors can be a member of a resource group. \n+\n+We will list those resources separately below.\n+\n+##### Default Resource groups via auto-discovery\n+\n+Teleport can create resource groups if admins turn on auto discovery. This will significanly simplify configuration. \n+\n+Here are some of the resource groups that Teleport Discovery service will create:\n+\n+* For AWS, Teleport discovery service will place each computing resource in `/aws/[account-id]/[region]/[resource-type]/[resource-id]`.\n+  + When EKS auto-discovery is on, this hierarchy will include discovered apps - `/aws/account-id/[region]/k8s/[cluster-name]/namespaces/[namespace]/[app-id]`\n+* For Azure, Teleport will use Azure's hierarchy - `/azure/[management-group]/[subscription]/[resource-group]/[resource-type]/[resource-id]`\n+* For GCP, we will use GCP hierarchy of `/gcp/[organization]/[folder]/[project]/[resource-type]/[resource-id]`\n+\n+Discovery service will create and remove these hierarchies based on the cloud state, and will create resources with `parent_resource_group` field to place them in those resource groups.\n+\n+If users are not happy with a default hierarchy, they can create a different one.\n+\n+#### Access Lists\n+\n+Teleport has a concept of access lists, that lists an owner, members, and optionally a parent access list.\n+Access List in Teleport represents a group of users with a hierarchy. \n+\n+We will further assume that the root of this hierarchy is a cluster. \n+\n+Unlike in resource groups, a user can be an owner and a member of none, one or several access lists at once.\n+\n+In addition to that, access list grants a role to a set of members, like in this example:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: \"lab-engineers\"\n+spec:\n+  desc: \"Access list for lab engineers\"\n+  grants:\n+    roles: [access]\n+  members:\n+    - name: bob@example.com\n+```\n+\n+We will return to access lists later, but let\u2019s now recall that access lists contain a list of members, who are, in turn, granted one or several roles.\n+\n+#### Scopes\n+\n+By default, in Teleport a role is granted it applies to all resources in the Teleport cluster.\n+\n+However, with this change, will be able to grant a set of roles that apply only to resources that belong to a specific resource group. \n+\n+In this case, we will say that the roles apply at the scope of the resource group.\n+\n+Scopes define a set of resources roles apply to. \n+\n+We will introduce scopes in a couple of places, first, for access list:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: \"lab-engineers\"\n+spec:\n+  desc: \"Access list for lab engineers\"\n+  # this grant applies only at the scope of the resource group `/env/prod/lab`\n+  scopes: [\u2018/env/prod/lab']\n+  grants:\n+    roles: [access]\n+  members:\n+    - name: bob@example.com\n+```\n+\n+By default, all existing access lists will grant roles at the cluster scope, cascading to all resources, just like before the migration. \n+\n+However, going forward, admins will be able to set scopes to more granular levels.\n+\n+The second place where we introduce scopes is in the roles:\n+\n+```yaml\n+kind: role\n+metadata:\n+ name: access\n+spec:\n+  grantable_scopes: ['/env/prod']\n+```\n+\n+Grantable scopes specifiy maximum scope this role can be granted on. \n+\n+**Important:** By default, if the `grantable_scopes` are missing, we assume empty scope - that will prevent the role from being granted on any scopes. When migrating existing roles, we would set `/` - root scope to avoid breaking the cluster. \n+\n+To sum it up, any role is granted to a set of users present in the access list, to a set of resources specified in the resource group.\n+\n+Grants are cascading, if a role is granted to a parent access list, it is also granted to members of any child access lists.\n+\n+If a role is granted at a scope of an access list, and this role can in turn give ability to request roles or impersonate, the roles and resources \n+and impersonation must be bound to the same scope or the scope smaller than the original one.\n+\n+For example, let\u2019s assume that the access list granted Alice the requester role described below at the scope of `/env/prod/lab`.  \n+\n+In this case, Alice would get the ability to search and request resources with an access role, but only in the scope of `/env/prod/lab`.\n+\n+```yaml\n+# requester.yaml\n+kind: role\n+version: v5\n+metadata:\n+  name: requester\n+spec:\n+  allow:\n+    request:\n+      search_as_roles:\n+        - access\n+```\n+\n+The same applies to impersonation, if access list granted Alice the role `impersonator` below at scope `/env/prod/lab`, \n+Alice would be able to impersonate role `jenkins`, but only at the scope `/env/prod/lab` or a more specific one, e.g. `/env/prod/lab/cabinet-west`.\n+\n+```yaml\n+kind: role\n+version: v5\n+metadata:\n+  name: impersonator\n+spec:\n+  allow:\n+    impersonate:\n+      users: ['jenkins']\n+      roles: ['jenkins']\n+```\n+\n+**Note:** While it\u2019s tempting to support scope templates, we will push this out of the scope of this RFD.\n+\n+Each scope is a valid URI, either starting with a path `/leafs/path/etc` or with scheme prefix: `admin:/path`. URI syntax will let us expand definition of the scope going forward.\n+\n+#### The Access verb\n+\n+Alice would like to create a role that denies access to all apps in some scope. To achieve this without labels, we introduce a new verb `access`:\n+\n+```yaml\n+kind: role\n+metadata:\n+   name: no-apps\n+spec:\n+  deny:\n+      rules:\n+      - resources: [app]\n+        verbs: [access]\n+```\n+\n+When granted at scope, the role above will deny access to any apps in this scope.\n+\n+We will use `access` verb during migration. The following V7 and V8 roles are equivalent:\n+\n+```yaml\n+kind: role\n+version: V7\n+metadata:\n+   name: all-apps\n+spec:\n+  app_labels:\n+    '*': '*'\n+```\n+\n+```yaml\n+kind: role\n+version: V8\n+metadata:\n+   name: all-apps\n+spec:\n+  allow:\n+    rules:\n+    - resources: [app]\n+      verbs: [access]\n+```\n+\n+We will use the verb `access` for any supported resource, `node`, `k8s`, `db`, `app`, `desktop`, etc.\n+\n+Most of Teleport's preset roles have labels `*`: `*`, so migration will be straightforward.\n+\n+#### Roles and Access Lists in resource groups\n+\n+A special case is when a role or an access list is assigned to a certain resource group.  \n+\n+Only roles that have `grantable_scope` matching the resource group can be assigned to the resource group.\n+\n+The same applies to access lists, the scope of the access list grants should always match the scope of the roles it grants access to and \n+should not exceed the scope of the access list itself.\n+\n+In both of those cases, parent resource group must be specified both for access lists and roles and should equal or be more specific than the scope it was created in:\n+\n+```yaml\n+kind: role\n+metadata:\n+  name: lab-admin\n+spec:\n+  grantable_scopes: ['/env/prod/lab']\n+  parent_resource_group: /env/prod/lab\n+```\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: lab-personnel\n+spec:\n+  scopes: ['/env/prod/lab']\n+  parent_resource_group: /env/prod/lab\n+```\n+\n+Roles created within a scope will have `grantable_scope` and `parent_resource_group` to be equal to scope, or more specific. \n+\n+For example, any role created within a scope `/env/prod/lab` must have the same `grantable_scope` and `parent_resource_group`  - `/env/prod/lab` or more specific one, \n+for example `/env/prod/lab/west-wing`.\n+\n+The roles and access lists created in the scope must grant access to scopes equal, or more specific than the scope they were created in to prevent lateral expansion or permissions.\n+\n+These invariants will let us make sure that any role created within a scope, will only grant permissions in the same, or more specific scope. \n+\n+We will apply the same invariants to any other resources created within a scope. \n+\n+#### Scoped Join Tokens\n+\n+Join tokens with `scopes` present will limit the resource groups the resources can join to.\n+\n+For example, Teleport service using the scoped token below will only be able to join the resource group `dev`\n+\n+```yaml\n+# token.yaml\n+kind: token\n+version: v2\n+metadata:\n+  name: my-token-name\n+spec:\n+  scopes: ['/dev']\n+  roles: \n+    - Node\n+    - App\n+```\n+\n+Join tokens with a `parent_resource_group` set can only have scope equal to this resource group.\n+\n+Join tokens created by roles granted within a scope must have `parent_resource_group` and `scopes` equal to this scope `/dev` or a more specific scope, e.g. `/dev/lab`. \n+\n+**Note:** To implement this, the token can be exchanged for the host certificate with `parent_resource_id` encoded in it. This way nodes can't set the nodes to any resource groups other than the parent.\n+By default, all existing join tokens will use `/` as a default resource group.\n+\n+#### Access Requests\n+\n+Access requests are bound to the scope they are created within, when Alice requests access to environment `/dev/lab` with role access, \n+the access request will capture the scope `/dev/lab` and create grant at this scope for Alice when approved.\n+\n+#### Scoped Audit Log Events and Session Recordings\n+\n+The audit events and session recordings generated by activity in some scope, will have a property that binds them to the same scope. \n+\n+This will let us filter and grant access to a subset of events and session recordings within a scope.\n+\n+#### Resources that can\u2019t be created at non-admin scope\n+\n+Some resources don't have a clear cut behavior at nested scopes, like SSO connectors, or are difficult to define, like for users. To address this issue, we will define a new `admin:` scope hierarchy that is parallel\n+to the `/` hierarchy and will require roles that grant access to this set of resources to use this and only this hierarchy.\n+\n+Here is a list of resources that can\u2019t be created at any scopes other than `admin:`\n+\n+* SSO connectors\n+* Users\n+* Bots\n+* Clusters\n+* Login rules\n+* Devices\n+* UI Configs\n+* Cluster auth preference\n+* Join tokens for roles Proxy, Auth\n+\n+In the `V7` role version, we will let users to mix `admin:` scope that includes resources other than `admin:`, however in role `V8` we will require the role to **only** include the admin scope resources:\n+\n+```yaml\n+kind: role\n+version: v8\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  grantable_scopes: 'admin:'\n+  allow:\n+    # only admin scope is allowed in V8, \n+    rules:\n+      - resources: [user, bot]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+Access lists that grant roles in `admin:` scope also have to have scopes to be explicitly set to only `admin:` scope and nothing else.\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: access-to-lab\n+spec:\n+  grants: \n+    roles: [access]\n+    traits:\n+      'internal.logins' : 'root'\n+  # only root and `admin:` scope is allowed if grants include roles that contain resources requiring `admin:` scope.\n+  scopes: ['admin:']\n+  members:\n+    - bob@example.com\n+```\n+\n+This will help to separate non-admin and admin resources more clearly and in the UI we should mark roles and access list in `admin:` scopes with label `admin`.\n+\n+### Trusted Clusters\n+\n+With new scoped RBAC approach, leaf clusters will sync users from the root cluster, similarly to how we sync users from Okta via SCIM. \n+Leaf clusters will also sync root cluster's Access Lists similarly to how Teleport syncs access lists from Okta, see RFD 0019e - Okta Access Lists Sync.\n+Combined together, Users and Access Lists sync will let leaf clusters mirror permissions from the root cluster, while remaining independent, as leafs can have their own access lists and users.\n+\n+To let users access resources in specific clusters, we will use `/=leafs/[cluster-name]` scope. Leafs hierarchy is a part of a root `/` cluster hierarchy. This will allow transparent migration of resources. To avoid name collisions we will reserve paths that contain `=` and will prohibit users from creating resource groups that have character `=` in them.\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: access-to-lab\n+spec:\n+  grants: \n+    roles: [access]\n+    traits:\n+      'internal.logins' : 'root'\n+  scopes: ['/dev/lab']\n+  members:\n+    - bob@example.com\n+```\n+\n+Leaf clusters syncing access lists and users from the root clusters should avoid name collisions - if a local or SSO leaf cluster, role or access list exists, the sync should avoid overwriting the local leaf cluster data, emitting audit event that mentions that the system did not sync the list.\n+\n+This architecture lets leafs to have their own indpendent grants, while mirroring users, access lists from the root, which represents majority of today's use cases.\n+\n+### Features we will deprecate over time\n+\n+All existing Teleport features will keep working with no changes, however, with this design we plan to deprecate:\n+\n+* The mappings in connectors `attributes_to_roles` in favor of Access Lists integrated with SCIM and identity providers. Any grants of roles will be governed by access lists.\n+* Certificate extensions with roles and traits. A new access control system will no longer rely on certificate metadata to identify what roles have been assigned to users. The only data that new access control requires is information about user identity - username. Teleport will propagate grants via backend. This will lower Teleport\u2019s resiliency to auth server failures, but this will be compensated with modern database backends like CockroachDB that provide multi-region resiliency and failover. \n+* Label matchers and resource matchers in roles. We will have to support those for a long time, but those labels will always apply at the granted scope to resources in a resource group, and will become redundant, with later versions of Teleport relying on auto-discovery and assignment of resources to resource groups.\n+* Login rules. We will recommend replacing login rules with access lists that provide similar functionality.\n+\n+## User stories\n+\n+Let\u2019s get back to the issues we outlined  in the start of this RFD and review how the new system will  help to resolve them.\n+\n+### Gradual scoping\n+\n+Most Teleport customers will start with all resources and roles in the cluster scope. \n+\n+We will let them introduce resource groups gradually. Let\u2019s create two resource groups, `prod` and `dev` with resource groups `west` and `lab`.\n+\n+Here is the resource groups hierarchy, where we will assume that mars and luna servers matched the assignments:\n+\n+```mermaid\n+graph TD;\n+    luna(Server luna)-->west;\n+    west-->prod;\n+    prod-->cluster;\n+    lab-->dev;\n+    mars(Server mars)-->lab;\n+    dev-->cluster;\n+```\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: prod\n+---\n+kind: resource_group\n+metadata:\n+  name: west\n+  parent_resource_group: prod\n+  # would be nice if we could match on AWS specific right away with match_aws\n+  match_aws:\n+    account_id: aws-account-id\n+    region: west\n+---\n+kind: resource_group\n+metadata:\n+  name: dev\n+---\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent_resource_group: dev\n+  # here we will just match on labels\n+  match_labels:\n+    kinds: [node]\n+    env: lab\n+```\n+\n+This will let administrators create a resource hierarchy by  mapping computing resources using AWS metadata or labels.\n+\n+We will use this setup in the following examples.\n+\n+### SSH access to specific hosts\n+\n+The most prominent use-case is our over-engineered access role. We can keep this role as is. Today, it grants blanket access to any computing resource of Teleport.\n+\n+Alice, who is an administrator, would like to restrict access for a user bob@example.com to any server in the lab as root\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: access-to-lab\n+spec:\n+  grants: \n+    roles: [access]\n+    traits:\n+      'internal.logins' : 'root'\n+  scopes: ['/dev/lab']\n+  members:\n+    - bob@example.com\n+```\n+\n+Teleport will grant role access  and traits internal.logins: root to `bob@example.com`, but only when Bob would access servers in the resource group `/dev/lab`. \n+\n+This grant will not be valid out of the scope of `/dev/lab`, so Bob won\u2019t be able to SSH as root to any other servers.\n+\n+K8s access to specific clusters\n+\n+Teleport can autodiscover clusters echo and bravo with namespaces default and prod, creating the following resource group hierarchy:\n+\n+```\n+/k8s/namespaces/prod/bravo\n+/k8s/namespaces/prod/echo\n+\n+/k8s/namespaces/default/bravo\n+/k8s/namespaces/default/echo\n+```\n+\n+Note that here we have set namespaces, and not cluster names as the root of the resource hierarchy,\n+so we can group different cluster names by namespace.\n+\n+We can then use this hierarchy to create access lists specifying access to default namespace in any cluster:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: access-to-default\n+spec:\n+  grants: \n+    roles: [access]\n+    traits:\n+      'internal.logins' : 'root'\n+  scopes:  ['/k8s/namespaces/default']\n+  members:\n+    - bob@example.com\n+```\n+\n+### Scoped search-based access requests\n+\n+Search-based access requests let users to search and request access to individual resources. Here is a standard requester role:\n+\n+```yaml\n+# requester.yaml\n+kind: role\n+version: v5\n+metadata:\n+  name: requester\n+spec:\n+  allow:\n+    request:\n+      search_as_roles:\n+        - access\n+```\n+\n+Here is a standard reviewer role:\n+\n+```yaml\n+# reviewer.yaml\n+kind: role\n+version: v5\n+metadata:\n+  name: reviewer\n+spec:\n+  allow:\n+    review_requests:\n+      roles:\n+        - access\n+      preview_as_roles:\n+        - access\n+```\n+\n+Without changing those roles, we can assign both requester and reviewer roles in a specific scope with access list:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: access-to-default\n+spec:\n+  grants: \n+    roles: [requester, reviewer]\n+  scopes:  ['/dev']\n+  members:\n+    - bob@example.com\n+    - alice@example.com\n+```\n+\n+In this case, `bob@example.com` and `alice@example.com` will get an ability to search, request and review requests, but only in the scope of any resource in `/dev` resource group.\n+\n+Customers frequently ask a question of how to scale this with multiple teams, \n+with this approach, we\u2019d have to create an access list for each individual team. \n+\n+Previously we\u2019ve been recommending to use role templates. \n+However, new access lists integration mirrors any group hierarchy in identity providers, \n+so there is no need to use templates - Teleport will create access lists and keep members up to date.\n+\n+The only thing we are missing is to let customers specify the scope when importing Okta groups or apps as access lists. \n+For example, access list for Okta group `devs` can automatically have scope `/dev`\n+\n+Additionally, one access list can be a member of another access list. Let\u2019s review a case when we have a group devs that needs access to both staging and production.\n+\n+Let\u2019s create access list for Alice and Bob, this special access list does no grants and scopes, we are going to use it to keep list of our developers:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: dev-team\n+spec:\n+  members:\n+    - bob@example.com\n+    - alice@example.com\n+---\n+kind: access_list\n+metadata:\n+  name: access-to-prod\n+spec:\n+  grants: \n+    roles: [requester, reviewer]\n+  scopes:  ['/dev']\n+  member_lists:\n+    - dev-team\n+---\n+kind: access_list\n+metadata:\n+  name: access-to-stage\n+spec:\n+  grants: \n+    roles: [requester, reviewer]\n+  scopes:  ['/prod']\n+  member_lists:\n+    - dev-team\n+```\n+\n+**Note:** We have to make sure that the child access list is a strict subset of the roles, traits and scopes of it's parent or has no scopes or grants at all.\n+\n+### Scoped Impersonation\n+\n+To make sure Alice and Ketanji can impersonate Jenkins, but only when accessing dev infrastructure, we will grant impersonator role via access list with scope `/dev`\n+\n+```yaml\n+kind: role\n+version: v5\n+metadata:\n+  name: impersonator\n+spec:\n+  allow:\n+    impersonate:\n+      users: ['jenkins']\n+      roles: ['jenkins']\n+---\n+kind: access_list\n+metadata:\n+  name: access-to-dev\n+spec:\n+  grants: \n+    roles: [impersonator]\n+  scopes:  ['/dev']\n+  members:\n+    - alice@example.com\n+    - ketanji@example.com\n+```\n+\n+### Scoped admins\n+\n+Large organizations would like to grant some users admin rights scoped for part of the infrastructure. \n+Scoped admins can manage access to users and resources within the scopes of their access lists and resource groups.\n+\n+Let\u2019s review how we can create a scoped admin structure and even let delegated admins create new roles in the scopes of their environments.\n+Scoped admins can create new roles, and access lists, however only if those roles have `grantable_scopes` and access lists have `scope` equal or more specific \n+than the scope of the granted roles of admins.\n+\n+Let\u2019s say Alice would like to delegate admin rights for the lab environment to Ketanji and Bob:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: scoped-dev-admin-dev\n+spec:\n+  scopes: ['/dev/lab']\n+  grants: \n+  roles: [editor, auditor]\n+  members:\n+    - bob@example.com\n+    - ketanji@example.com\n+```\n+\n+Ketanji can now create roles, access lists and join tokens, but they all will have scopes set to /dev/lab\n+\n+```yaml\n+kind: role\n+version: v5\n+metadata:\n+  name: impersonator\n+spec:\n+  grantable_scope: '/dev/lab'\n+  parent_resource_group: '/dev/lab' \n+  allow:\n+    impersonate:\n+      users: ['jenkins']\n+      roles: ['jenkins']\n+---\n+kind: access_list\n+metadata:\n+  name: scoped-dev-admin-dev-access\n+spec:\n+  parent_resource_group: '/dev/lab'\n+  scopes: ['/dev/lab']\n+  grants: \n+  roles: [access]\n+  members:\n+    - bob@example.com\n+    - ketanji@example.com\n+---\n+# token.yaml\n+kind: token\n+version: v2\n+metadata:\n+  name: my-token-name\n+spec:\n+  parent_resource_group: '/dev/lab'\n+  roles: \n+    - Node\n+    - App\n+```\n+\n+By propagating scope for any resource created by a user leveraging the roles the granted scope, \n+we  make sure that our security invariant remains - Ketanji and Bob can\u2019t expand the scope of their cluster access beyond /dev/lab.\n+\n+### Allow agent forwarding for some hosts\n+\n+Issue https://github.com/gravitational/teleport/issues/23790 is looking for a way to allow agent forwarding for specific hosts.\n+\n+This can be done with access lists and scoped assignment:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: access-to-default\n+spec:\n+  grants: \n+    roles: [access-with-agent-forward]\n+  scopes:  ['/dev/lab']\n+  members:\n+    - bob@example.com\n+```\n+\n+This will let bob to ssh with agent forwarding on into hosts in scope of /dev/lab only.\n+\n+## Implementation\n+\n+Teleport will stop issuing new certificate extensions for access granted by Access Lists or Access Requests. \n+\n+Instead, Teleport will create and distribute internal `Grant` resources. These resources will be used by Teleport only and will never be exposed to users.\n+\n+Here is a high-level structure of an internal `grant` resource:\n+\n+```yaml\n+kind: grant\n+version: v1\n+metadata:\n+  # name is a unique random identifier uuid-v4 of this grant \n+  name: uuid\n+spec:\n+  # the time when the grant expires in RFC3339 format\n+  create_time: \"2023-02-22T21:04:26.312862Z\"\n+  # the time when the grant has been updated in RFC3339 format\n+  update_time: \"2023-02-22T21:04:26.312862Z\"\n+  # the time when the grant expires in RFC3339 format\n+  expires: \"2021-08-14T22:27:00Z\"\n+  # roles that are granted\n+  roles: [admin, editor]\n+  # traits that are granted\n+  traits:\n+    - 'internal.logins': 'root'\n+  identity: 'alice@example.com'\n+  scopes: ['/dev/lab']\n+  # not all grants need to be always evaluated,\n+  # when set, this grant will be evaluated only when users specify access request,\n+  # and this grant is issued as a part of access request\n+  access_request_id: 'bc8ca931-fec9-4b15-9a6f-20c13c5641a9'\n+  # resource_uuids is used for search based access requests and further limits access to specific resources,\n+  # if those are allowed by granted roles in the given scopes.\n+  resource_uuids:\n+  - 'e99dcc3b-11ba-4b87-96c5-c3f1e886e9ec'\n+```\n+\n+We will also assume that resource groups, roles `V8` and Access Lists `V2` all have `create_time` and `update_time` fields.\n+\n+Teleport will store grants reflecting the scope hierarchy, for example, if there are two grants, one for `alice@example.com` \n+at scope `/dev/lab` and another for `bob@example.com` for scope `/grants/dev`, we will store them in a tree:\n+\n+```\n+# the _root node only contains the empty grant to capture last update time.\n+/grants/dev/_root\n+/grants/dev/_members/bob@example.com: grant-2-uuid\n+\n+/grants/dev/lab/root\n+/grants/dev/lab/_members/alice@example.com: grant-1-uuid\n+```\n+\n+Every time we add, or update the grant in the hierarchy, we will also update it's `_root` with the timestamp.\n+\n+We will also store `ResourceGroup` as a hierarchy:\n+\n+```\n+/resource_groups/dev/_root\n+/resource_groups/dev/_members/luna\n+\n+/resource_groups/dev/lab/_root\n+/resource_groups/dev/lab/_members/mars\n+```\n+\n+Every time we add or remove a resource, we update the `_root` of the hierarchy with the timestamp.\n+\n+When stored this way, each part of Teleport can subscribe and fetch grants for resource groups relevant to it, and would know when the grant hierarchy or resource hierarchy was last updated.\n+\n+### Grants and access requests\n+\n+When access request is approved, Teleport will create a grant with `access_request_id` set to the approved access request id:\n+\n+```yaml\n+kind: grant\n+version: v1\n+metadata:\n+  # name is a unique random identifier uuid-v4 of this grant \n+  name: bc8ca931-fec9-4b13-9a6f-20c13c5835a7\n+spec:\n+  # this grant is issued as a part of access request\n+  access_request_id: 'bc8ca931-fec9-4b15-9a6f-20c13c5641a9'\n+  # resource_uuids is used for search based access requests and further limits access to specific resources,\n+  # if those are allowed by granted roles in the given scopes.\n+  resource_uuids:\n+  - 'e99dcc3b-11ba-4b87-96c5-c3f1e886e9ec'\n+```\n+\n+Teleport will only evaluate those grants when user's certificate has `access-request-id` in the certificate extension.\n+\n+Instead of `assumed-role` and `resource-uuids` extensions issued in certificates and described in [RFD 0059 Search Based Access Requests](https://github.com/gravitational/teleport/blob/master/rfd/0059-search-based-access-requests.md#certificate-issuance-and-rbac), Teleport will issue a certificate with new extension `access-request-id: [bc8ca931-fec9-4b15-9a6f-20c13c5641a9]`\n+that will evaluate grants assigned to the identity at the matching scope and access request ids.\n+\n+### The \"New enemy\" problem\n+\n+The Zanzibar paper describes two cases of \"new enemy problem\" (https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/):\n+\n+**Case 1 - Neglecting update order**\n+\n+* Step 1. Alice removes Bob from the ACL of a folder.\n+* Step 2. Alice asks Charlie to add new docs to a folder.\n+* Step 3. Bob should not see new documents, but can do so, if the ACL neglects the ordering. (Because in Zanzibar, individual documents may inherit ACL of the parent folder).\n+\n+**Case 2 - Misapplying old ACL to new content**\n+\n+* Step 1. Alice removes Bob from the ACL of the document.\n+* Step 2. Alice asks Charlie to add new contents to the document.\n+* Step 3. Bob should not be able to see new content, but may do so, if ACL evaluates Bob's permissions at Step 2 before reading Step 1.\n+\n+Let's review how those cases can apply to Teleport's Grants.\n+\n+Bob was granted role `access` in scope `/dev/lab`.\n+\n+**Case 1 - Neglecting update order** \n+\n+* Step 1. Alice removes Bob from the access list granting `access` in scope of `/dev/lab`\n+* Step 2. Alice adds new resource `luna` to the scope of `/dev/lab`\n+* Step 3. Bob should not get access to `luna`, but can do so, if the Teleport Proxy evaluating access skips `Step 1` before evaluating `Step 2`\n+\n+Let's assume that every time a resource is added or removed to a resource group, it has it's timestamp updated.\n+\n+* Before any of the step, we can assume that grant hierarchy `G` - `/dev/lab` has some timestamp `Tg` and a resource hierarchy - `R` has some timestamp `Tr`.\n+* At a `Step 1`, at the timestamp `T1` Alice removed Bob from access list granting role `access` in the scope `/dev/lab`. Teleport has updated the `_root` nodes of grant hierarchy `/dev` and `/dev/lab` with this timestamp in the backend.\n+* At a `Step 2`, at the timestamp `T2` > `T1`, Alice added a new resource `luna` to the scope of the `/dev/lab`. Teleport has updated the `_root` node of the resource group hierarchy `/dev` and `/dev/lab` with this timestamp in the backend.\n+* At a `Step 3`, Teleport proxy would have to evaluate access to host luna. The proxy would have to make sure that it's copy of the grant hierarchies `/dev/lab` is not stale.\n+\n+In our example `T2 > T1 > Tr` and `T2 > T1 > Tg`. There are two sub cases when `Tr > Tg` and `Tr < Tg`\n+\n+Here are possible combinations of versions that The Proxy can encounter:\n+\n+```\n+1. G - Tg, R - Tr\n+2. G - T1, R - Tr\n+3. G - Tg, R - T2\n+4. G - T1, R - T2\n+```\n+\n+Let's review the cases in the reverse order:\n+\n+* In the cases 3 and 4, the Proxy sees that the resource group has been updated after the grant, because `T2 > T1 > Tg` and can proceed.\n+* In the case 2, The proxy sees that `R` may be stale, because it's `Tr` less than `T1 and has to make a call to the backend making sure it fetched update of `R` least as fresh as `T1` with membership of `luna`.",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "1502679436",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 38078,
        "pr_file": "rfd/0164-scoped-rbac.md",
        "discussion_id": "1502679436",
        "commented_code": "@@ -0,0 +1,1122 @@\n+\n+---\n+authors: @klizhentas (sasha@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0163 - Scoped RBAC\n+\n+## Required Approvers\n+\n+* Engineering @r0mant && (@tigrato || @marcoandredinis)\n+* Security: (@reedloden || @jentfoo)\n+* Product: (@xinding33 || @klizhentas )\n+\n+## What\n+\n+This RFD introduces resource hierarchies and scopes to existing RBAC. \n+Our goal is simplify and evolve access control in Teleport without drastic changes or new policy languages. \n+Make it easier to integrate Teleport RBAC with cloud IAMs of AWS, GCP and Azure.\n+This RFD is closely modeled and inspired by Azure RBAC model, the most advanced out of 3 clouds. \n+\n+Read about it here https://learn.microsoft.com/en-us/azure/role-based-access-control/overview before diving into this RFD.\n+\n+## Why\n+\n+There are several structural issues with the current RBAC model in Teleport.\n+\n+### Scalability issues \n+\n+Every role has to be distributed to every node or proxy that has to perform authorization. \n+Current roles are brittle - to evaluate access, every single role has to be fetched and processed, \n+because each role can have a deny rule that can block access to any allowed rule.\n+\n+### Scoping issues\n+\n+It is not possible to describe \u201cdelegated admins\u201d in RBAC, when one user has administrative access over part of the cluster.\n+It is also not possible to specify that certain role options only apply in certain use-cases. \n+\n+For example, the setting `permit-agent-forward: false` will deny agent-forward to any matching resource with no exceptions, even if other roles allow it.\n+\n+It is not possible to allow admins to grant roles to other users but with certain restrictions, as in the example of issue https://github.com/gravitational/teleport/issues/16914. \n+\n+### Complexity \n+\n+Roles today have both labels and label expressions, login rules to inject traits and claims and templates. \n+This creates a complicated system that is hard to understand and reason about.\n+Role mapping is brittle and requires updating OIDC/SAML connector resources, which can break access.\n+\n+### Security issues\n+\n+Every role assignment and trait is encoded in certificate, and each time a user gets their roles updated, they have to get a new certificate. \n+Old certificates can be re-used to get privileges that have been removed, \n+creating \u201ca new enemy problem\u201d described in [Zanzibar Paper](https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/).\n+\n+Many roles allow \u201crole escapes\u201d, as any person who gets assigned a role that can create other roles, \n+would become an admin, see for example issue https://github.com/gravitational/teleport.e/issues/3111\n+\n+### Goals\n+\n+Our key goal is to evolve Teleport\u2019s roles without asking users to rewrite their existing RBAC.\n+We also would like to better integrate Teleport RBAC with cloud provider\u2019s IAM systems out of the box.\n+We would like to give Teleport\u2019s users \u201cbatteries included\u201d approach, when they can get 90% of the use-cases done without c\n+reating any new roles, or modifying existing ones.\n+\n+### Non-Goals\n+\n+We are not going to implement backwards-incompatible changes that require our customers rewrite their RBAC stack or adopt a completely new policy language. \n+\n+## Details\n+To understand the required changes, let\u2019s first take a look at Teleport RBAC structure. \n+\n+### RBAC Primer\n+\n+Let\u2019s start with fundamental Teleport RBAC concepts and highlight some issues as we review them.\n+\n+#### Roles\n+\n+Each user or bot in Teleport is assigned one or several roles. \n+For the full reference, take a look at the documentation at https://goteleport.com/docs/reference/resources/#role.\n+\n+Here is a role structure:\n+\n+```yaml\n+kind: role\n+version: v7\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  # options are a set of knobs that apply across the entire role set.\n+  # the most restrictive options wins\n+  options:\n+    # max_session_ttl defines the TTL (time to live) of certificates\n+    # issued to the users with this role.\n+    max_session_ttl: 8h\n+    # forward_agent controls whether SSH agent forwarding is allowed\n+    forward_agent: true\n+  # The allow section declares a list of resource/verb combinations that are\n+  # allowed for the users of this role. By default, nothing is allowed.\n+  #\n+  # Allow rules specify both actions that are allowed and match the resources\n+  # they are applying to.\n+  allow:\n+    # Some Allow fields specify target protocol\n+    #  login principals, like in this example, SSH logins\n+    logins: [root, '{{internal.logins}}']\n+\n+    # In this example, the fields specify a set of windows desktop logins\n+    windows_desktop_logins: [Administrator, '{{internal.logins}}']\n+\n+    # There are multiple types of labels and label expressions that \n+    # match different computing resources.\n+    node_labels:\n+      # literal strings:\n+      'env': 'test'\n+    # regexp expressions\n+      'reg': '^us-west-1|eu-central-1$'\n+\n+    # node_labels_expression has the same purpose as node_labels but\n+    # supports predicate expressions to configure custom logic.\n+    # A user with this role will be allowed to access nodes if they are in the\n+    # staging environment *or* if they belong to one of the user's own teams.\n+    node_labels_expression: |\n+      labels[\"env\"] == \"staging\" ||\n+      contains(user.spec.traits[\"teams\"] , labels[\"team\"])\n+ \n+    # rules allow a user holding this role to modify other resources\n+    # matching the expressions below.\n+    # rules match both resources and specify what actions are allowed\n+    rules:\n+      - resources: [role]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+We can spot several issues with this role design:\n+\n+* There is no way to specify the scope that role applies to, all roles apply globally to all resources they match and all users they assign to all the time.\n+* There is no way to specify resource hierarchies, like computing environments (env vs prod), which makes it hard to partition the infrastructure and forces customers to specify labels.\n+\n+\n+#### Labels\n+\n+Teleport RBAC\u2019s advice to engineers to partition their resource is to first, label their computing resource or use AWS labels, and second, match the labels in RBAC.\n+\n+Admins can set the tags on the resource configuration file statically, or, for some resources, use `server_info` to set the tags for each resource:\n+\n+```yaml\n+# server_info.yaml\n+kind: server_info\n+metadata:\n+   name: si-<node-name>\n+spec:\n+   new_labels:\n+      \"foo\": \"bar\"\n+```\n+\n+This creates several issues:\n+\n+* It is not always secure to delegate labeling to owners of computing resources, as anyone with root access to the node config file can update its labels impacting everyone else.\n+* It is not scalable, as it\u2019s not always useful to set and updates tags for each individual resource\n+* It\u2019s hard or impossible to partition infrastructure with two-dimensional labels, although users can use `env: prod` to mark all resources in the production environment, there is no way to say that `env: lab` is a subset of `env: prod`.\n+\n+#### Current Roles mapping\n+\n+In Teleport there are multiple ways to map roles to users: static roles mapping to local users and bots, \n+dynamic mapping to SSO via connectors and on-demand assignment via access lists and access requests.\n+\n+Most users start with local static mapping and SSO mapping, later graduating to access requests and access lists.\n+\n+https://goteleport.com/docs/access-controls/sso/#creating-an-authentication-connector\n+\n+For SSO users, on login, Teleport checks SSO connector, and maps attributes of a user to a list of roles:\n+\n+```yaml\n+# connector.yaml\n+kind: saml\n+version: v2\n+metadata:\n+  name: corporate\n+spec:\n+  attributes_to_roles:\n+    - {name: \"groups\", value: \"okta-admin\", roles: [\"access\"]}\n+     # regular expressions with capture are also supported. \n+     # the next line instructs Teleport\n+     # to assign users to roles `admin-1` if his SAML \"group\" \n+     # attribute equals 'ssh_admin_1':\n+   - { name: \"group\", value: \"^ssh_admin_(.*)$\", roles: [\"admin-$1\"] }\n+```\n+\n+This creates several security and scalability issues:\n+\n+* Every time administrators need to change roles assignment, they have to update the resource.\n+* Every time the Identity Provider changes user\u2019s attributes, the users would have to relogin to get new roles. \n+\n+For local users, administrators have to specify roles and traits in the local resource:\n+\n+```yaml\n+kind: user\n+version: v2\n+metadata:\n+  name: joe\n+spec:\n+  # roles is a list of roles assigned to this user\n+  roles:\n+  - admin\n+  # traits are key, list of values pairs assigned to a user resource.\n+  # Traits can be used in role templates as variables.\n+  traits:\n+    logins:\n+    - joe\n+    - root\n+```\n+\n+This also creates some challenges, as administrators are forced to update local resources each time they have to assign a user new permission.\n+\n+### Modifications\n+\n+Let\u2019s now introduce the missing pieces of the puzzle and review how new roles will simplify or deprecate legacy concepts.\n+\n+#### Hierarchical Resource Groups\n+\n+Resource groups are one such missing piece - in most cloud environments, resources are split into hierarchy, \n+for example, host `luna` is a member of a resource group `lab`, in turn a member of environment `prod`, which is in turn a member of a Teleport cluster.\n+\n+In Teleport, we will make cluster a default root of this hierarchy. Every computing resource by default will be a direct member of a root `cluster` resource group.\n+\n+One resource can be assigned to multiple resource groups at a time, or none. \n+\n+In our example, the cluster administrator would define lab resource group in the following way:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent: prod_env\n+```\n+\n+Administrators can assign resources to resource groups:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+   parent: prod_env\n+ match_kinds:\n+   - node\n+   - database\n+   - role\n+   - access_list\n+   - '.*'\n+ match_labels:  \n+   env: prod\n+```\n+\n+In this case, any resource that matches `env:prod` label will be assigned to this resource group. We will use the same label matching algorithm as in \n+\n+This will let administrators to gradually migrate their existing flat infrastructure to resource groups one.\n+\n+In some cases it makes sense to specify parent resource group inline:\n+\n+```yaml\n+kind: role\n+spec:\n+  parent_resource_group: /env/prod\n+```\n+\n+By default, if unspecified, a resource is a member of a root-level - `/` cluster resource group. If specified by the resource, it won't be a member of a root `/` resource group.\n+\n+Resource groups are hierarchical, and we can refer to the `lab` resource group by its full path as `/env/prod/lab`. \n+\n+Most Teleport resources, with some exceptions, like users, SSO connectors can be a member of a resource group. \n+\n+We will list those resources separately below.\n+\n+##### Default Resource groups via auto-discovery\n+\n+Teleport can create resource groups if admins turn on auto discovery. This will significanly simplify configuration. \n+\n+Here are some of the resource groups that Teleport Discovery service will create:\n+\n+* For AWS, Teleport discovery service will place each computing resource in `/aws/[account-id]/[region]/[resource-type]/[resource-id]`.\n+  + When EKS auto-discovery is on, this hierarchy will include discovered apps - `/aws/account-id/[region]/k8s/[cluster-name]/namespaces/[namespace]/[app-id]`\n+* For Azure, Teleport will use Azure's hierarchy - `/azure/[management-group]/[subscription]/[resource-group]/[resource-type]/[resource-id]`\n+* For GCP, we will use GCP hierarchy of `/gcp/[organization]/[folder]/[project]/[resource-type]/[resource-id]`\n+\n+Discovery service will create and remove these hierarchies based on the cloud state, and will create resources with `parent_resource_group` field to place them in those resource groups.\n+\n+If users are not happy with a default hierarchy, they can create a different one.\n+\n+#### Access Lists\n+\n+Teleport has a concept of access lists, that lists an owner, members, and optionally a parent access list.\n+Access List in Teleport represents a group of users with a hierarchy. \n+\n+We will further assume that the root of this hierarchy is a cluster. \n+\n+Unlike in resource groups, a user can be an owner and a member of none, one or several access lists at once.\n+\n+In addition to that, access list grants a role to a set of members, like in this example:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: \"lab-engineers\"\n+spec:\n+  desc: \"Access list for lab engineers\"\n+  grants:\n+    roles: [access]\n+  members:\n+    - name: bob@example.com\n+```\n+\n+We will return to access lists later, but let\u2019s now recall that access lists contain a list of members, who are, in turn, granted one or several roles.\n+\n+#### Scopes\n+\n+By default, in Teleport a role is granted it applies to all resources in the Teleport cluster.\n+\n+However, with this change, will be able to grant a set of roles that apply only to resources that belong to a specific resource group. \n+\n+In this case, we will say that the roles apply at the scope of the resource group.\n+\n+Scopes define a set of resources roles apply to. \n+\n+We will introduce scopes in a couple of places, first, for access list:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: \"lab-engineers\"\n+spec:\n+  desc: \"Access list for lab engineers\"\n+  # this grant applies only at the scope of the resource group `/env/prod/lab`\n+  scopes: [\u2018/env/prod/lab']\n+  grants:\n+    roles: [access]\n+  members:\n+    - name: bob@example.com\n+```\n+\n+By default, all existing access lists will grant roles at the cluster scope, cascading to all resources, just like before the migration. \n+\n+However, going forward, admins will be able to set scopes to more granular levels.\n+\n+The second place where we introduce scopes is in the roles:\n+\n+```yaml\n+kind: role\n+metadata:\n+ name: access\n+spec:\n+  grantable_scopes: ['/env/prod']\n+```\n+\n+Grantable scopes specifiy maximum scope this role can be granted on. \n+\n+**Important:** By default, if the `grantable_scopes` are missing, we assume empty scope - that will prevent the role from being granted on any scopes. When migrating existing roles, we would set `/` - root scope to avoid breaking the cluster. \n+\n+To sum it up, any role is granted to a set of users present in the access list, to a set of resources specified in the resource group.\n+\n+Grants are cascading, if a role is granted to a parent access list, it is also granted to members of any child access lists.\n+\n+If a role is granted at a scope of an access list, and this role can in turn give ability to request roles or impersonate, the roles and resources \n+and impersonation must be bound to the same scope or the scope smaller than the original one.\n+\n+For example, let\u2019s assume that the access list granted Alice the requester role described below at the scope of `/env/prod/lab`.  \n+\n+In this case, Alice would get the ability to search and request resources with an access role, but only in the scope of `/env/prod/lab`.\n+\n+```yaml\n+# requester.yaml\n+kind: role\n+version: v5\n+metadata:\n+  name: requester\n+spec:\n+  allow:\n+    request:\n+      search_as_roles:\n+        - access\n+```\n+\n+The same applies to impersonation, if access list granted Alice the role `impersonator` below at scope `/env/prod/lab`, \n+Alice would be able to impersonate role `jenkins`, but only at the scope `/env/prod/lab` or a more specific one, e.g. `/env/prod/lab/cabinet-west`.\n+\n+```yaml\n+kind: role\n+version: v5\n+metadata:\n+  name: impersonator\n+spec:\n+  allow:\n+    impersonate:\n+      users: ['jenkins']\n+      roles: ['jenkins']\n+```\n+\n+**Note:** While it\u2019s tempting to support scope templates, we will push this out of the scope of this RFD.\n+\n+Each scope is a valid URI, either starting with a path `/leafs/path/etc` or with scheme prefix: `admin:/path`. URI syntax will let us expand definition of the scope going forward.\n+\n+#### The Access verb\n+\n+Alice would like to create a role that denies access to all apps in some scope. To achieve this without labels, we introduce a new verb `access`:\n+\n+```yaml\n+kind: role\n+metadata:\n+   name: no-apps\n+spec:\n+  deny:\n+      rules:\n+      - resources: [app]\n+        verbs: [access]\n+```\n+\n+When granted at scope, the role above will deny access to any apps in this scope.\n+\n+We will use `access` verb during migration. The following V7 and V8 roles are equivalent:\n+\n+```yaml\n+kind: role\n+version: V7\n+metadata:\n+   name: all-apps\n+spec:\n+  app_labels:\n+    '*': '*'\n+```\n+\n+```yaml\n+kind: role\n+version: V8\n+metadata:\n+   name: all-apps\n+spec:\n+  allow:\n+    rules:\n+    - resources: [app]\n+      verbs: [access]\n+```\n+\n+We will use the verb `access` for any supported resource, `node`, `k8s`, `db`, `app`, `desktop`, etc.\n+\n+Most of Teleport's preset roles have labels `*`: `*`, so migration will be straightforward.\n+\n+#### Roles and Access Lists in resource groups\n+\n+A special case is when a role or an access list is assigned to a certain resource group.  \n+\n+Only roles that have `grantable_scope` matching the resource group can be assigned to the resource group.\n+\n+The same applies to access lists, the scope of the access list grants should always match the scope of the roles it grants access to and \n+should not exceed the scope of the access list itself.\n+\n+In both of those cases, parent resource group must be specified both for access lists and roles and should equal or be more specific than the scope it was created in:\n+\n+```yaml\n+kind: role\n+metadata:\n+  name: lab-admin\n+spec:\n+  grantable_scopes: ['/env/prod/lab']\n+  parent_resource_group: /env/prod/lab\n+```\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: lab-personnel\n+spec:\n+  scopes: ['/env/prod/lab']\n+  parent_resource_group: /env/prod/lab\n+```\n+\n+Roles created within a scope will have `grantable_scope` and `parent_resource_group` to be equal to scope, or more specific. \n+\n+For example, any role created within a scope `/env/prod/lab` must have the same `grantable_scope` and `parent_resource_group`  - `/env/prod/lab` or more specific one, \n+for example `/env/prod/lab/west-wing`.\n+\n+The roles and access lists created in the scope must grant access to scopes equal, or more specific than the scope they were created in to prevent lateral expansion or permissions.\n+\n+These invariants will let us make sure that any role created within a scope, will only grant permissions in the same, or more specific scope. \n+\n+We will apply the same invariants to any other resources created within a scope. \n+\n+#### Scoped Join Tokens\n+\n+Join tokens with `scopes` present will limit the resource groups the resources can join to.\n+\n+For example, Teleport service using the scoped token below will only be able to join the resource group `dev`\n+\n+```yaml\n+# token.yaml\n+kind: token\n+version: v2\n+metadata:\n+  name: my-token-name\n+spec:\n+  scopes: ['/dev']\n+  roles: \n+    - Node\n+    - App\n+```\n+\n+Join tokens with a `parent_resource_group` set can only have scope equal to this resource group.\n+\n+Join tokens created by roles granted within a scope must have `parent_resource_group` and `scopes` equal to this scope `/dev` or a more specific scope, e.g. `/dev/lab`. \n+\n+**Note:** To implement this, the token can be exchanged for the host certificate with `parent_resource_id` encoded in it. This way nodes can't set the nodes to any resource groups other than the parent.\n+By default, all existing join tokens will use `/` as a default resource group.\n+\n+#### Access Requests\n+\n+Access requests are bound to the scope they are created within, when Alice requests access to environment `/dev/lab` with role access, \n+the access request will capture the scope `/dev/lab` and create grant at this scope for Alice when approved.\n+\n+#### Scoped Audit Log Events and Session Recordings\n+\n+The audit events and session recordings generated by activity in some scope, will have a property that binds them to the same scope. \n+\n+This will let us filter and grant access to a subset of events and session recordings within a scope.\n+\n+#### Resources that can\u2019t be created at non-admin scope\n+\n+Some resources don't have a clear cut behavior at nested scopes, like SSO connectors, or are difficult to define, like for users. To address this issue, we will define a new `admin:` scope hierarchy that is parallel\n+to the `/` hierarchy and will require roles that grant access to this set of resources to use this and only this hierarchy.\n+\n+Here is a list of resources that can\u2019t be created at any scopes other than `admin:`\n+\n+* SSO connectors\n+* Users\n+* Bots\n+* Clusters\n+* Login rules\n+* Devices\n+* UI Configs\n+* Cluster auth preference\n+* Join tokens for roles Proxy, Auth\n+\n+In the `V7` role version, we will let users to mix `admin:` scope that includes resources other than `admin:`, however in role `V8` we will require the role to **only** include the admin scope resources:\n+\n+```yaml\n+kind: role\n+version: v8\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  grantable_scopes: 'admin:'\n+  allow:\n+    # only admin scope is allowed in V8, \n+    rules:\n+      - resources: [user, bot]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+Access lists that grant roles in `admin:` scope also have to have scopes to be explicitly set to only `admin:` scope and nothing else.\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: access-to-lab\n+spec:\n+  grants: \n+    roles: [access]\n+    traits:\n+      'internal.logins' : 'root'\n+  # only root and `admin:` scope is allowed if grants include roles that contain resources requiring `admin:` scope.\n+  scopes: ['admin:']\n+  members:\n+    - bob@example.com\n+```\n+\n+This will help to separate non-admin and admin resources more clearly and in the UI we should mark roles and access list in `admin:` scopes with label `admin`.\n+\n+### Trusted Clusters\n+\n+With new scoped RBAC approach, leaf clusters will sync users from the root cluster, similarly to how we sync users from Okta via SCIM. \n+Leaf clusters will also sync root cluster's Access Lists similarly to how Teleport syncs access lists from Okta, see RFD 0019e - Okta Access Lists Sync.\n+Combined together, Users and Access Lists sync will let leaf clusters mirror permissions from the root cluster, while remaining independent, as leafs can have their own access lists and users.\n+\n+To let users access resources in specific clusters, we will use `/=leafs/[cluster-name]` scope. Leafs hierarchy is a part of a root `/` cluster hierarchy. This will allow transparent migration of resources. To avoid name collisions we will reserve paths that contain `=` and will prohibit users from creating resource groups that have character `=` in them.\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: access-to-lab\n+spec:\n+  grants: \n+    roles: [access]\n+    traits:\n+      'internal.logins' : 'root'\n+  scopes: ['/dev/lab']\n+  members:\n+    - bob@example.com\n+```\n+\n+Leaf clusters syncing access lists and users from the root clusters should avoid name collisions - if a local or SSO leaf cluster, role or access list exists, the sync should avoid overwriting the local leaf cluster data, emitting audit event that mentions that the system did not sync the list.\n+\n+This architecture lets leafs to have their own indpendent grants, while mirroring users, access lists from the root, which represents majority of today's use cases.\n+\n+### Features we will deprecate over time\n+\n+All existing Teleport features will keep working with no changes, however, with this design we plan to deprecate:\n+\n+* The mappings in connectors `attributes_to_roles` in favor of Access Lists integrated with SCIM and identity providers. Any grants of roles will be governed by access lists.\n+* Certificate extensions with roles and traits. A new access control system will no longer rely on certificate metadata to identify what roles have been assigned to users. The only data that new access control requires is information about user identity - username. Teleport will propagate grants via backend. This will lower Teleport\u2019s resiliency to auth server failures, but this will be compensated with modern database backends like CockroachDB that provide multi-region resiliency and failover. \n+* Label matchers and resource matchers in roles. We will have to support those for a long time, but those labels will always apply at the granted scope to resources in a resource group, and will become redundant, with later versions of Teleport relying on auto-discovery and assignment of resources to resource groups.\n+* Login rules. We will recommend replacing login rules with access lists that provide similar functionality.\n+\n+## User stories\n+\n+Let\u2019s get back to the issues we outlined  in the start of this RFD and review how the new system will  help to resolve them.\n+\n+### Gradual scoping\n+\n+Most Teleport customers will start with all resources and roles in the cluster scope. \n+\n+We will let them introduce resource groups gradually. Let\u2019s create two resource groups, `prod` and `dev` with resource groups `west` and `lab`.\n+\n+Here is the resource groups hierarchy, where we will assume that mars and luna servers matched the assignments:\n+\n+```mermaid\n+graph TD;\n+    luna(Server luna)-->west;\n+    west-->prod;\n+    prod-->cluster;\n+    lab-->dev;\n+    mars(Server mars)-->lab;\n+    dev-->cluster;\n+```\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: prod\n+---\n+kind: resource_group\n+metadata:\n+  name: west\n+  parent_resource_group: prod\n+  # would be nice if we could match on AWS specific right away with match_aws\n+  match_aws:\n+    account_id: aws-account-id\n+    region: west\n+---\n+kind: resource_group\n+metadata:\n+  name: dev\n+---\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent_resource_group: dev\n+  # here we will just match on labels\n+  match_labels:\n+    kinds: [node]\n+    env: lab\n+```\n+\n+This will let administrators create a resource hierarchy by  mapping computing resources using AWS metadata or labels.\n+\n+We will use this setup in the following examples.\n+\n+### SSH access to specific hosts\n+\n+The most prominent use-case is our over-engineered access role. We can keep this role as is. Today, it grants blanket access to any computing resource of Teleport.\n+\n+Alice, who is an administrator, would like to restrict access for a user bob@example.com to any server in the lab as root\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: access-to-lab\n+spec:\n+  grants: \n+    roles: [access]\n+    traits:\n+      'internal.logins' : 'root'\n+  scopes: ['/dev/lab']\n+  members:\n+    - bob@example.com\n+```\n+\n+Teleport will grant role access  and traits internal.logins: root to `bob@example.com`, but only when Bob would access servers in the resource group `/dev/lab`. \n+\n+This grant will not be valid out of the scope of `/dev/lab`, so Bob won\u2019t be able to SSH as root to any other servers.\n+\n+K8s access to specific clusters\n+\n+Teleport can autodiscover clusters echo and bravo with namespaces default and prod, creating the following resource group hierarchy:\n+\n+```\n+/k8s/namespaces/prod/bravo\n+/k8s/namespaces/prod/echo\n+\n+/k8s/namespaces/default/bravo\n+/k8s/namespaces/default/echo\n+```\n+\n+Note that here we have set namespaces, and not cluster names as the root of the resource hierarchy,\n+so we can group different cluster names by namespace.\n+\n+We can then use this hierarchy to create access lists specifying access to default namespace in any cluster:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: access-to-default\n+spec:\n+  grants: \n+    roles: [access]\n+    traits:\n+      'internal.logins' : 'root'\n+  scopes:  ['/k8s/namespaces/default']\n+  members:\n+    - bob@example.com\n+```\n+\n+### Scoped search-based access requests\n+\n+Search-based access requests let users to search and request access to individual resources. Here is a standard requester role:\n+\n+```yaml\n+# requester.yaml\n+kind: role\n+version: v5\n+metadata:\n+  name: requester\n+spec:\n+  allow:\n+    request:\n+      search_as_roles:\n+        - access\n+```\n+\n+Here is a standard reviewer role:\n+\n+```yaml\n+# reviewer.yaml\n+kind: role\n+version: v5\n+metadata:\n+  name: reviewer\n+spec:\n+  allow:\n+    review_requests:\n+      roles:\n+        - access\n+      preview_as_roles:\n+        - access\n+```\n+\n+Without changing those roles, we can assign both requester and reviewer roles in a specific scope with access list:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: access-to-default\n+spec:\n+  grants: \n+    roles: [requester, reviewer]\n+  scopes:  ['/dev']\n+  members:\n+    - bob@example.com\n+    - alice@example.com\n+```\n+\n+In this case, `bob@example.com` and `alice@example.com` will get an ability to search, request and review requests, but only in the scope of any resource in `/dev` resource group.\n+\n+Customers frequently ask a question of how to scale this with multiple teams, \n+with this approach, we\u2019d have to create an access list for each individual team. \n+\n+Previously we\u2019ve been recommending to use role templates. \n+However, new access lists integration mirrors any group hierarchy in identity providers, \n+so there is no need to use templates - Teleport will create access lists and keep members up to date.\n+\n+The only thing we are missing is to let customers specify the scope when importing Okta groups or apps as access lists. \n+For example, access list for Okta group `devs` can automatically have scope `/dev`\n+\n+Additionally, one access list can be a member of another access list. Let\u2019s review a case when we have a group devs that needs access to both staging and production.\n+\n+Let\u2019s create access list for Alice and Bob, this special access list does no grants and scopes, we are going to use it to keep list of our developers:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: dev-team\n+spec:\n+  members:\n+    - bob@example.com\n+    - alice@example.com\n+---\n+kind: access_list\n+metadata:\n+  name: access-to-prod\n+spec:\n+  grants: \n+    roles: [requester, reviewer]\n+  scopes:  ['/dev']\n+  member_lists:\n+    - dev-team\n+---\n+kind: access_list\n+metadata:\n+  name: access-to-stage\n+spec:\n+  grants: \n+    roles: [requester, reviewer]\n+  scopes:  ['/prod']\n+  member_lists:\n+    - dev-team\n+```\n+\n+**Note:** We have to make sure that the child access list is a strict subset of the roles, traits and scopes of it's parent or has no scopes or grants at all.\n+\n+### Scoped Impersonation\n+\n+To make sure Alice and Ketanji can impersonate Jenkins, but only when accessing dev infrastructure, we will grant impersonator role via access list with scope `/dev`\n+\n+```yaml\n+kind: role\n+version: v5\n+metadata:\n+  name: impersonator\n+spec:\n+  allow:\n+    impersonate:\n+      users: ['jenkins']\n+      roles: ['jenkins']\n+---\n+kind: access_list\n+metadata:\n+  name: access-to-dev\n+spec:\n+  grants: \n+    roles: [impersonator]\n+  scopes:  ['/dev']\n+  members:\n+    - alice@example.com\n+    - ketanji@example.com\n+```\n+\n+### Scoped admins\n+\n+Large organizations would like to grant some users admin rights scoped for part of the infrastructure. \n+Scoped admins can manage access to users and resources within the scopes of their access lists and resource groups.\n+\n+Let\u2019s review how we can create a scoped admin structure and even let delegated admins create new roles in the scopes of their environments.\n+Scoped admins can create new roles, and access lists, however only if those roles have `grantable_scopes` and access lists have `scope` equal or more specific \n+than the scope of the granted roles of admins.\n+\n+Let\u2019s say Alice would like to delegate admin rights for the lab environment to Ketanji and Bob:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: scoped-dev-admin-dev\n+spec:\n+  scopes: ['/dev/lab']\n+  grants: \n+  roles: [editor, auditor]\n+  members:\n+    - bob@example.com\n+    - ketanji@example.com\n+```\n+\n+Ketanji can now create roles, access lists and join tokens, but they all will have scopes set to /dev/lab\n+\n+```yaml\n+kind: role\n+version: v5\n+metadata:\n+  name: impersonator\n+spec:\n+  grantable_scope: '/dev/lab'\n+  parent_resource_group: '/dev/lab' \n+  allow:\n+    impersonate:\n+      users: ['jenkins']\n+      roles: ['jenkins']\n+---\n+kind: access_list\n+metadata:\n+  name: scoped-dev-admin-dev-access\n+spec:\n+  parent_resource_group: '/dev/lab'\n+  scopes: ['/dev/lab']\n+  grants: \n+  roles: [access]\n+  members:\n+    - bob@example.com\n+    - ketanji@example.com\n+---\n+# token.yaml\n+kind: token\n+version: v2\n+metadata:\n+  name: my-token-name\n+spec:\n+  parent_resource_group: '/dev/lab'\n+  roles: \n+    - Node\n+    - App\n+```\n+\n+By propagating scope for any resource created by a user leveraging the roles the granted scope, \n+we  make sure that our security invariant remains - Ketanji and Bob can\u2019t expand the scope of their cluster access beyond /dev/lab.\n+\n+### Allow agent forwarding for some hosts\n+\n+Issue https://github.com/gravitational/teleport/issues/23790 is looking for a way to allow agent forwarding for specific hosts.\n+\n+This can be done with access lists and scoped assignment:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: access-to-default\n+spec:\n+  grants: \n+    roles: [access-with-agent-forward]\n+  scopes:  ['/dev/lab']\n+  members:\n+    - bob@example.com\n+```\n+\n+This will let bob to ssh with agent forwarding on into hosts in scope of /dev/lab only.\n+\n+## Implementation\n+\n+Teleport will stop issuing new certificate extensions for access granted by Access Lists or Access Requests. \n+\n+Instead, Teleport will create and distribute internal `Grant` resources. These resources will be used by Teleport only and will never be exposed to users.\n+\n+Here is a high-level structure of an internal `grant` resource:\n+\n+```yaml\n+kind: grant\n+version: v1\n+metadata:\n+  # name is a unique random identifier uuid-v4 of this grant \n+  name: uuid\n+spec:\n+  # the time when the grant expires in RFC3339 format\n+  create_time: \"2023-02-22T21:04:26.312862Z\"\n+  # the time when the grant has been updated in RFC3339 format\n+  update_time: \"2023-02-22T21:04:26.312862Z\"\n+  # the time when the grant expires in RFC3339 format\n+  expires: \"2021-08-14T22:27:00Z\"\n+  # roles that are granted\n+  roles: [admin, editor]\n+  # traits that are granted\n+  traits:\n+    - 'internal.logins': 'root'\n+  identity: 'alice@example.com'\n+  scopes: ['/dev/lab']\n+  # not all grants need to be always evaluated,\n+  # when set, this grant will be evaluated only when users specify access request,\n+  # and this grant is issued as a part of access request\n+  access_request_id: 'bc8ca931-fec9-4b15-9a6f-20c13c5641a9'\n+  # resource_uuids is used for search based access requests and further limits access to specific resources,\n+  # if those are allowed by granted roles in the given scopes.\n+  resource_uuids:\n+  - 'e99dcc3b-11ba-4b87-96c5-c3f1e886e9ec'\n+```\n+\n+We will also assume that resource groups, roles `V8` and Access Lists `V2` all have `create_time` and `update_time` fields.\n+\n+Teleport will store grants reflecting the scope hierarchy, for example, if there are two grants, one for `alice@example.com` \n+at scope `/dev/lab` and another for `bob@example.com` for scope `/grants/dev`, we will store them in a tree:\n+\n+```\n+# the _root node only contains the empty grant to capture last update time.\n+/grants/dev/_root\n+/grants/dev/_members/bob@example.com: grant-2-uuid\n+\n+/grants/dev/lab/root\n+/grants/dev/lab/_members/alice@example.com: grant-1-uuid\n+```\n+\n+Every time we add, or update the grant in the hierarchy, we will also update it's `_root` with the timestamp.\n+\n+We will also store `ResourceGroup` as a hierarchy:\n+\n+```\n+/resource_groups/dev/_root\n+/resource_groups/dev/_members/luna\n+\n+/resource_groups/dev/lab/_root\n+/resource_groups/dev/lab/_members/mars\n+```\n+\n+Every time we add or remove a resource, we update the `_root` of the hierarchy with the timestamp.\n+\n+When stored this way, each part of Teleport can subscribe and fetch grants for resource groups relevant to it, and would know when the grant hierarchy or resource hierarchy was last updated.\n+\n+### Grants and access requests\n+\n+When access request is approved, Teleport will create a grant with `access_request_id` set to the approved access request id:\n+\n+```yaml\n+kind: grant\n+version: v1\n+metadata:\n+  # name is a unique random identifier uuid-v4 of this grant \n+  name: bc8ca931-fec9-4b13-9a6f-20c13c5835a7\n+spec:\n+  # this grant is issued as a part of access request\n+  access_request_id: 'bc8ca931-fec9-4b15-9a6f-20c13c5641a9'\n+  # resource_uuids is used for search based access requests and further limits access to specific resources,\n+  # if those are allowed by granted roles in the given scopes.\n+  resource_uuids:\n+  - 'e99dcc3b-11ba-4b87-96c5-c3f1e886e9ec'\n+```\n+\n+Teleport will only evaluate those grants when user's certificate has `access-request-id` in the certificate extension.\n+\n+Instead of `assumed-role` and `resource-uuids` extensions issued in certificates and described in [RFD 0059 Search Based Access Requests](https://github.com/gravitational/teleport/blob/master/rfd/0059-search-based-access-requests.md#certificate-issuance-and-rbac), Teleport will issue a certificate with new extension `access-request-id: [bc8ca931-fec9-4b15-9a6f-20c13c5641a9]`\n+that will evaluate grants assigned to the identity at the matching scope and access request ids.\n+\n+### The \"New enemy\" problem\n+\n+The Zanzibar paper describes two cases of \"new enemy problem\" (https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/):\n+\n+**Case 1 - Neglecting update order**\n+\n+* Step 1. Alice removes Bob from the ACL of a folder.\n+* Step 2. Alice asks Charlie to add new docs to a folder.\n+* Step 3. Bob should not see new documents, but can do so, if the ACL neglects the ordering. (Because in Zanzibar, individual documents may inherit ACL of the parent folder).\n+\n+**Case 2 - Misapplying old ACL to new content**\n+\n+* Step 1. Alice removes Bob from the ACL of the document.\n+* Step 2. Alice asks Charlie to add new contents to the document.\n+* Step 3. Bob should not be able to see new content, but may do so, if ACL evaluates Bob's permissions at Step 2 before reading Step 1.\n+\n+Let's review how those cases can apply to Teleport's Grants.\n+\n+Bob was granted role `access` in scope `/dev/lab`.\n+\n+**Case 1 - Neglecting update order** \n+\n+* Step 1. Alice removes Bob from the access list granting `access` in scope of `/dev/lab`\n+* Step 2. Alice adds new resource `luna` to the scope of `/dev/lab`\n+* Step 3. Bob should not get access to `luna`, but can do so, if the Teleport Proxy evaluating access skips `Step 1` before evaluating `Step 2`\n+\n+Let's assume that every time a resource is added or removed to a resource group, it has it's timestamp updated.\n+\n+* Before any of the step, we can assume that grant hierarchy `G` - `/dev/lab` has some timestamp `Tg` and a resource hierarchy - `R` has some timestamp `Tr`.\n+* At a `Step 1`, at the timestamp `T1` Alice removed Bob from access list granting role `access` in the scope `/dev/lab`. Teleport has updated the `_root` nodes of grant hierarchy `/dev` and `/dev/lab` with this timestamp in the backend.\n+* At a `Step 2`, at the timestamp `T2` > `T1`, Alice added a new resource `luna` to the scope of the `/dev/lab`. Teleport has updated the `_root` node of the resource group hierarchy `/dev` and `/dev/lab` with this timestamp in the backend.\n+* At a `Step 3`, Teleport proxy would have to evaluate access to host luna. The proxy would have to make sure that it's copy of the grant hierarchies `/dev/lab` is not stale.\n+\n+In our example `T2 > T1 > Tr` and `T2 > T1 > Tg`. There are two sub cases when `Tr > Tg` and `Tr < Tg`\n+\n+Here are possible combinations of versions that The Proxy can encounter:\n+\n+```\n+1. G - Tg, R - Tr\n+2. G - T1, R - Tr\n+3. G - Tg, R - T2\n+4. G - T1, R - T2\n+```\n+\n+Let's review the cases in the reverse order:\n+\n+* In the cases 3 and 4, the Proxy sees that the resource group has been updated after the grant, because `T2 > T1 > Tg` and can proceed.\n+* In the case 2, The proxy sees that `R` may be stale, because it's `Tr` less than `T1 and has to make a call to the backend making sure it fetched update of `R` least as fresh as `T1` with membership of `luna`.",
        "comment_created_at": "2024-02-26T14:13:02+00:00",
        "comment_author": "webvictim",
        "comment_body": "```suggestion\r\n* In the case 2, The proxy sees that `R` may be stale, because it's `Tr` less than `T1` and has to make a call to the backend making sure it fetched update of `R` least as fresh as `T1` with membership of `luna`.\r\n```",
        "pr_file_module": null
      }
    ]
  }
]