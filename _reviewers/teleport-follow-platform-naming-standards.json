[
  {
    "discussion_id": "2299185201",
    "pr_number": 58065,
    "pr_file": "rfd/0223-k8s-health-checks.md",
    "created_at": "2025-08-25T21:32:01+00:00",
    "commented_code": "+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleterm UI, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "2299185201",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299185201",
        "commented_code": "@@ -0,0 +1,594 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleterm UI, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}",
        "comment_created_at": "2025-08-25T21:32:01+00:00",
        "comment_author": "rosstimothy",
        "comment_body": "I think we should suffix these with a _total to follow naming best practices: https://prometheus.io/docs/practices/naming/#metric-names\r\n```suggestion\r\nteleport_health_resources_total{type=\"kubernetes\"}\r\n# Returns 3, the expected number of healthy Kubernetes clusters\r\n\r\nteleport_health_resources_available_total{type=\"kubernetes\"}\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2299335647",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299185201",
        "commented_code": "@@ -0,0 +1,594 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleterm UI, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}",
        "comment_created_at": "2025-08-25T23:14:26+00:00",
        "comment_author": "rana",
        "comment_body": "`_total` is supposedly only for another type of metric.\r\n\r\nIt started out with the `_total` suffix until learning about `_total` implying a counter type that doesn't decrement. Both health check metrics being added support decrementing, and so are _gauge_ type metrics.\r\n\r\nFrom [other Prometheus metric naming docs](https://prometheus.io/docs/instrumenting/writing_exporters/#naming):\r\n> The `_sum`, `_count`, `_bucket` and `_total` suffixes are used by Summaries, Histograms and Counters. Unless you\u2019re producing one of those, avoid these suffixes.\r\n> `_total` is a convention for counters, you should use it if you\u2019re using the COUNTER type.\r\n\r\nPrometheus has [Counter](https://prometheus.io/docs/concepts/metric_types/#counter) and [Gauge](https://prometheus.io/docs/concepts/metric_types/#gauge) types:\r\n\r\n> Counter\r\n>\r\n> A counter is a cumulative metric that represents a single [monotonically increasing counter](https://en.wikipedia.org/wiki/Monotonic_function) whose value can only increase or be reset to zero on restart. For example, you can use a counter to represent the number of requests served, tasks completed, or errors.\r\n> Do not use a counter to expose a value that can decrease. For example, do not use a counter for the number of currently running processes; instead use a gauge.\r\n\r\n> Gauge\r\n>\r\n> A gauge is a metric that represents a single numerical value that can arbitrarily go up and down.\r\n> Gauges are typically used for measured values like temperatures or current memory usage, but also \"counts\" that can go up and down, like the number of concurrent requests.\r\n\r\nSimple enough.\r\n\r\nThe best practices naming guide also says, \"an accumulating count has `total` as a suffix\". It's implying its for counters. The guide could be more explicit about the `counter-gauge` distinction.\r\n\r\nWhat are current Teleport naming practices? Teleport gauge naming is currently a mix of `_total` and `s` suffixes:\r\n- [auth_generate_requests](https://github.com/gravitational/teleport/blob/d1c0ea7dc9ec5c16f538134534e7f95c465aa741/lib/auth/auth.go#L918)\r\n- [teleport_registered_servers](https://github.com/gravitational/teleport/blob/d1c0ea7dc9ec5c16f538134534e7f95c465aa741/lib/auth/auth.go#L956)\r\n- [teleport_registered_servers_by_install_methods](https://github.com/gravitational/teleport/blob/d1c0ea7dc9ec5c16f538134534e7f95c465aa741/lib/auth/auth.go#L968)\r\n- [teleport_migrations](https://github.com/gravitational/teleport/blob/d1c0ea7dc9ec5c16f538134534e7f95c465aa741/lib/auth/auth.go#L977)\r\n- [teleport_total_instances](https://github.com/gravitational/teleport/blob/d1c0ea7dc9ec5c16f538134534e7f95c465aa741/lib/auth/auth.go#L986)\r\n- [teleport_enrolled_in_upgrades](https://github.com/gravitational/teleport/blob/29860001cf406d42cf88d4c2fb53a76eefba179f/lib/auth/auth.go#L994)\r\n- [teleport_upgrader_counts](https://github.com/gravitational/teleport/blob/d1c0ea7dc9ec5c16f538134534e7f95c465aa741/lib/auth/auth.go#L1002)\r\n- [teleport_connected_resources](https://github.com/gravitational/teleport/blob/408ca87d9f8af2f33fa4699c6e69b79e3a240fae/lib/auth/grpcserver.go#L167)\r\n- [teleport_unstable_createauditstream_limit](https://github.com/gravitational/teleport/blob/408ca87d9f8af2f33fa4699c6e69b79e3a240fae/lib/auth/grpcserver.go#L188)\r\n- [heartbeats_missed_total](https://github.com/gravitational/teleport/blob/d1c0ea7dc9ec5c16f538134534e7f95c465aa741/lib/auth/auth.go#L941)\r\n- [teleport_roles_total](https://github.com/gravitational/teleport/blob/d1c0ea7dc9ec5c16f538134534e7f95c465aa741/lib/auth/auth.go#L948)\r\n- And many more gauge names with a similar mix of suffixes\r\n\r\nWith that said, naming is supposed to be helpful. If you think `_total` is helpful, I'm good with `_total` as well.",
        "pr_file_module": null
      },
      {
        "comment_id": "2301218343",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2299185201",
        "commented_code": "@@ -0,0 +1,594 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleterm UI, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}",
        "comment_created_at": "2025-08-26T14:37:43+00:00",
        "comment_author": "rosstimothy",
        "comment_body": "Hrm yeah that is somewhat misleading advice in the naming best practices document. I'm fine omitting `_total` if that is reserved for counters.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2075057666",
    "pr_number": 38078,
    "pr_file": "rfd/0164-scoped-rbac.md",
    "created_at": "2025-05-06T09:12:15+00:00",
    "commented_code": "+\n+---\n+authors: @klizhentas (sasha@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0163 - Scoped RBAC\n+\n+## Required Approvers\n+\n+* Engineering @r0mant && (@tigrato || @marcoandredinis)\n+* Security: (@reedloden || @jentfoo)\n+* Product: (@xinding33 || @klizhentas )\n+\n+## What\n+\n+This RFD introduces resource hierarchies and scopes to existing RBAC. \n+Our goal is simplify and evolve access control in Teleport without drastic changes or new policy languages. \n+Make it easier to integrate Teleport RBAC with cloud IAMs of AWS, GCP and Azure.\n+This RFD is closely modeled and inspired by Azure RBAC model, the most advanced out of 3 clouds. \n+\n+Read about it here https://learn.microsoft.com/en-us/azure/role-based-access-control/overview before diving into this RFD.\n+\n+## Why\n+\n+There are several structural issues with the current RBAC model in Teleport.\n+\n+### Scalability issues \n+\n+Every role has to be distributed to every node or proxy that has to perform authorization. \n+Current roles are brittle - to evaluate access, every single role has to be fetched and processed, \n+because each role can have a deny rule that can block access to any allowed rule.\n+\n+### Scoping issues\n+\n+It is not possible to describe \u201cdelegated admins\u201d in RBAC, when one user has administrative access over part of the cluster.\n+It is also not possible to specify that certain role options only apply in certain use-cases. \n+\n+For example, the setting `permit-agent-forward: false` will deny agent-forward to any matching resource with no exceptions, even if other roles allow it.\n+\n+It is not possible to allow admins to grant roles to other users but with certain restrictions, as in the example of issue https://github.com/gravitational/teleport/issues/16914. \n+\n+### Complexity \n+\n+Roles today have both labels and label expressions, login rules to inject traits and claims and templates. \n+This creates a complicated system that is hard to understand and reason about.\n+Role mapping is brittle and requires updating OIDC/SAML connector resources, which can break access.\n+\n+### Security issues\n+\n+Every role assignment and trait is encoded in certificate, and each time a user gets their roles updated, they have to get a new certificate. \n+Old certificates can be re-used to get privileges that have been removed, \n+creating \u201ca new enemy problem\u201d described in [Zanzibar Paper](https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/).\n+\n+Many roles allow \u201crole escapes\u201d, as any person who gets assigned a role that can create other roles, \n+would become an admin, see for example issue https://github.com/gravitational/teleport.e/issues/3111\n+\n+### Goals\n+\n+Our key goal is to evolve Teleport\u2019s roles without asking users to rewrite their existing RBAC.\n+We also would like to better integrate Teleport RBAC with cloud provider\u2019s IAM systems out of the box.\n+We would like to give Teleport\u2019s users \u201cbatteries included\u201d approach, when they can get 90% of the use-cases done without c\n+reating any new roles, or modifying existing ones.\n+\n+### Non-Goals\n+\n+We are not going to implement backwards-incompatible changes that require our customers rewrite their RBAC stack or adopt a completely new policy language. \n+\n+## Details\n+To understand the required changes, let\u2019s first take a look at Teleport RBAC structure. \n+\n+### RBAC Primer\n+\n+Let\u2019s start with fundamental Teleport RBAC concepts and highlight some issues as we review them.\n+\n+#### Roles\n+\n+Each user or bot in Teleport is assigned one or several roles. \n+For the full reference, take a look at the documentation at https://goteleport.com/docs/reference/resources/#role.\n+\n+Here is a role structure:\n+\n+```yaml\n+kind: role\n+version: v7\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  # options are a set of knobs that apply across the entire role set.\n+  # the most restrictive options wins\n+  options:\n+    # max_session_ttl defines the TTL (time to live) of certificates\n+    # issued to the users with this role.\n+    max_session_ttl: 8h\n+    # forward_agent controls whether SSH agent forwarding is allowed\n+    forward_agent: true\n+  # The allow section declares a list of resource/verb combinations that are\n+  # allowed for the users of this role. By default, nothing is allowed.\n+  #\n+  # Allow rules specify both actions that are allowed and match the resources\n+  # they are applying to.\n+  allow:\n+    # Some Allow fields specify target protocol\n+    #  login principals, like in this example, SSH logins\n+    logins: [root, '{{internal.logins}}']\n+\n+    # In this example, the fields specify a set of windows desktop logins\n+    windows_desktop_logins: [Administrator, '{{internal.logins}}']\n+\n+    # There are multiple types of labels and label expressions that \n+    # match different computing resources.\n+    node_labels:\n+      # literal strings:\n+      'env': 'test'\n+    # regexp expressions\n+      'reg': '^us-west-1|eu-central-1$'\n+\n+    # node_labels_expression has the same purpose as node_labels but\n+    # supports predicate expressions to configure custom logic.\n+    # A user with this role will be allowed to access nodes if they are in the\n+    # staging environment *or* if they belong to one of the user's own teams.\n+    node_labels_expression: |\n+      labels[\"env\"] == \"staging\" ||\n+      contains(user.spec.traits[\"teams\"] , labels[\"team\"])\n+ \n+    # rules allow a user holding this role to modify other resources\n+    # matching the expressions below.\n+    # rules match both resources and specify what actions are allowed\n+    rules:\n+      - resources: [role]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+We can spot several issues with this role design:\n+\n+* There is no way to specify the scope that role applies to, all roles apply globally to all resources they match and all users they assign to all the time.\n+* There is no way to specify resource hierarchies, like computing environments (env vs prod), which makes it hard to partition the infrastructure and forces customers to specify labels.\n+\n+\n+#### Labels\n+\n+Teleport RBAC\u2019s advice to engineers to partition their resource is to first, label their computing resource or use AWS labels, and second, match the labels in RBAC.\n+\n+Admins can set the tags on the resource configuration file statically, or, for some resources, use `server_info` to set the tags for each resource:\n+\n+```yaml\n+# server_info.yaml\n+kind: server_info\n+metadata:\n+   name: si-<node-name>\n+spec:\n+   new_labels:\n+      \"foo\": \"bar\"\n+```\n+\n+This creates several issues:\n+\n+* It is not always secure to delegate labeling to owners of computing resources, as anyone with root access to the node config file can update its labels impacting everyone else.\n+* It is not scalable, as it\u2019s not always useful to set and updates tags for each individual resource\n+* It\u2019s hard or impossible to partition infrastructure with two-dimensional labels, although users can use `env: prod` to mark all resources in the production environment, there is no way to say that `env: lab` is a subset of `env: prod`.\n+\n+#### Current Roles mapping\n+\n+In Teleport there are multiple ways to map roles to users: static roles mapping to local users and bots, \n+dynamic mapping to SSO via connectors and on-demand assignment via access lists and access requests.\n+\n+Most users start with local static mapping and SSO mapping, later graduating to access requests and access lists.\n+\n+https://goteleport.com/docs/access-controls/sso/#creating-an-authentication-connector\n+\n+For SSO users, on login, Teleport checks SSO connector, and maps attributes of a user to a list of roles:\n+\n+```yaml\n+# connector.yaml\n+kind: saml\n+version: v2\n+metadata:\n+  name: corporate\n+spec:\n+  attributes_to_roles:\n+    - {name: \"groups\", value: \"okta-admin\", roles: [\"access\"]}\n+     # regular expressions with capture are also supported. \n+     # the next line instructs Teleport\n+     # to assign users to roles `admin-1` if his SAML \"group\" \n+     # attribute equals 'ssh_admin_1':\n+   - { name: \"group\", value: \"^ssh_admin_(.*)$\", roles: [\"admin-$1\"] }\n+```\n+\n+This creates several security and scalability issues:\n+\n+* Every time administrators need to change roles assignment, they have to update the resource.\n+* Every time the Identity Provider changes user\u2019s attributes, the users would have to relogin to get new roles. \n+\n+For local users, administrators have to specify roles and traits in the local resource:\n+\n+```yaml\n+kind: user\n+version: v2\n+metadata:\n+  name: joe\n+spec:\n+  # roles is a list of roles assigned to this user\n+  roles:\n+  - admin\n+  # traits are key, list of values pairs assigned to a user resource.\n+  # Traits can be used in role templates as variables.\n+  traits:\n+    logins:\n+    - joe\n+    - root\n+```\n+\n+This also creates some challenges, as administrators are forced to update local resources each time they have to assign a user new permission.\n+\n+### Modifications\n+\n+Let\u2019s now introduce the missing pieces of the puzzle and review how new roles will simplify or deprecate legacy concepts.\n+\n+#### Hierarchical Resource Groups\n+\n+Resource groups are one such missing piece - in most cloud environments, resources are split into hierarchy, \n+for example, host `luna` is a member of a resource group `lab`, in turn a member of environment `prod`, which is in turn a member of a Teleport cluster.\n+\n+In Teleport, we will make cluster a default root of this hierarchy. Every computing resource by default will be a direct member of a root `cluster` resource group.\n+\n+One resource can be assigned to multiple resource groups at a time, or none. \n+\n+In our example, the cluster administrator would define lab resource group in the following way:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent: prod_env\n+```\n+\n+Administrators can assign resources to resource groups:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+   parent: prod_env\n+ match_kinds:\n+   - node\n+   - database\n+   - role\n+   - access_list\n+   - '.*'\n+ match_labels:  \n+   env: prod\n+```\n+\n+In this case, any resource that matches `env:prod` label will be assigned to this resource group. We will use the same label matching algorithm as in today's `discovery_service`. \n+\n+This will let administrators to gradually migrate their existing flat infrastructure to resource groups one.\n+\n+In some cases it makes sense to specify parent resource group inline:\n+\n+```yaml\n+kind: role\n+spec:\n+  parent_resource_group: /env/prod\n+```\n+\n+By default, if unspecified, a resource is a member of a root-level - `/` cluster resource group. If specified by the resource, it won't be a member of a root `/` resource group.\n+\n+Resource groups are hierarchical, and we can refer to the `lab` resource group by its full path as `/env/prod/lab`. \n+\n+Most Teleport resources, with some exceptions, like users, SSO connectors can be a member of a resource group. \n+\n+We will list those resources separately below.\n+\n+##### Default Resource groups via auto-discovery\n+\n+Teleport can create resource groups if admins turn on auto discovery. This will significantly simplify configuration. \n+\n+Here are some of the resource groups that Teleport Discovery service will create:\n+\n+* For AWS, Teleport discovery service will place each computing resource in `/aws/[account-id]/[region]/[resource-type]/[resource-id]`.\n+  + When EKS auto-discovery is on, this hierarchy will include discovered apps - `/aws/account-id/[region]/k8s/[cluster-name]/namespaces/[namespace]/[app-id]`\n+* For Azure, Teleport will use Azure's hierarchy - `/azure/[management-group]/[subscription]/[resource-group]/[resource-type]/[resource-id]`\n+* For GCP, we will use GCP hierarchy of `/gcp/[organization]/[folder]/[project]/[resource-type]/[resource-id]`\n+\n+Discovery service will create and remove these hierarchies based on the cloud state, and will create resources with `parent_resource_group` field to place them in those resource groups.\n+\n+If users are not happy with a default hierarchy, they can create a different one.\n+\n+#### Access Lists\n+\n+Teleport has a concept of access lists, that lists an owner, members, and optionally a parent access list.\n+Access List in Teleport represents a group of users with a hierarchy. \n+\n+We will further assume that the root of this hierarchy is a cluster. \n+\n+Unlike in resource groups, a user can be an owner and a member of none, one or several access lists at once.\n+\n+In addition to that, access list grants a role to a set of members, like in this example:\n+\n+```yaml\n+kind: access_list",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "2075057666",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 38078,
        "pr_file": "rfd/0164-scoped-rbac.md",
        "discussion_id": "2075057666",
        "commented_code": "@@ -0,0 +1,1144 @@\n+\n+---\n+authors: @klizhentas (sasha@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0163 - Scoped RBAC\n+\n+## Required Approvers\n+\n+* Engineering @r0mant && (@tigrato || @marcoandredinis)\n+* Security: (@reedloden || @jentfoo)\n+* Product: (@xinding33 || @klizhentas )\n+\n+## What\n+\n+This RFD introduces resource hierarchies and scopes to existing RBAC. \n+Our goal is simplify and evolve access control in Teleport without drastic changes or new policy languages. \n+Make it easier to integrate Teleport RBAC with cloud IAMs of AWS, GCP and Azure.\n+This RFD is closely modeled and inspired by Azure RBAC model, the most advanced out of 3 clouds. \n+\n+Read about it here https://learn.microsoft.com/en-us/azure/role-based-access-control/overview before diving into this RFD.\n+\n+## Why\n+\n+There are several structural issues with the current RBAC model in Teleport.\n+\n+### Scalability issues \n+\n+Every role has to be distributed to every node or proxy that has to perform authorization. \n+Current roles are brittle - to evaluate access, every single role has to be fetched and processed, \n+because each role can have a deny rule that can block access to any allowed rule.\n+\n+### Scoping issues\n+\n+It is not possible to describe \u201cdelegated admins\u201d in RBAC, when one user has administrative access over part of the cluster.\n+It is also not possible to specify that certain role options only apply in certain use-cases. \n+\n+For example, the setting `permit-agent-forward: false` will deny agent-forward to any matching resource with no exceptions, even if other roles allow it.\n+\n+It is not possible to allow admins to grant roles to other users but with certain restrictions, as in the example of issue https://github.com/gravitational/teleport/issues/16914. \n+\n+### Complexity \n+\n+Roles today have both labels and label expressions, login rules to inject traits and claims and templates. \n+This creates a complicated system that is hard to understand and reason about.\n+Role mapping is brittle and requires updating OIDC/SAML connector resources, which can break access.\n+\n+### Security issues\n+\n+Every role assignment and trait is encoded in certificate, and each time a user gets their roles updated, they have to get a new certificate. \n+Old certificates can be re-used to get privileges that have been removed, \n+creating \u201ca new enemy problem\u201d described in [Zanzibar Paper](https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/).\n+\n+Many roles allow \u201crole escapes\u201d, as any person who gets assigned a role that can create other roles, \n+would become an admin, see for example issue https://github.com/gravitational/teleport.e/issues/3111\n+\n+### Goals\n+\n+Our key goal is to evolve Teleport\u2019s roles without asking users to rewrite their existing RBAC.\n+We also would like to better integrate Teleport RBAC with cloud provider\u2019s IAM systems out of the box.\n+We would like to give Teleport\u2019s users \u201cbatteries included\u201d approach, when they can get 90% of the use-cases done without c\n+reating any new roles, or modifying existing ones.\n+\n+### Non-Goals\n+\n+We are not going to implement backwards-incompatible changes that require our customers rewrite their RBAC stack or adopt a completely new policy language. \n+\n+## Details\n+To understand the required changes, let\u2019s first take a look at Teleport RBAC structure. \n+\n+### RBAC Primer\n+\n+Let\u2019s start with fundamental Teleport RBAC concepts and highlight some issues as we review them.\n+\n+#### Roles\n+\n+Each user or bot in Teleport is assigned one or several roles. \n+For the full reference, take a look at the documentation at https://goteleport.com/docs/reference/resources/#role.\n+\n+Here is a role structure:\n+\n+```yaml\n+kind: role\n+version: v7\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  # options are a set of knobs that apply across the entire role set.\n+  # the most restrictive options wins\n+  options:\n+    # max_session_ttl defines the TTL (time to live) of certificates\n+    # issued to the users with this role.\n+    max_session_ttl: 8h\n+    # forward_agent controls whether SSH agent forwarding is allowed\n+    forward_agent: true\n+  # The allow section declares a list of resource/verb combinations that are\n+  # allowed for the users of this role. By default, nothing is allowed.\n+  #\n+  # Allow rules specify both actions that are allowed and match the resources\n+  # they are applying to.\n+  allow:\n+    # Some Allow fields specify target protocol\n+    #  login principals, like in this example, SSH logins\n+    logins: [root, '{{internal.logins}}']\n+\n+    # In this example, the fields specify a set of windows desktop logins\n+    windows_desktop_logins: [Administrator, '{{internal.logins}}']\n+\n+    # There are multiple types of labels and label expressions that \n+    # match different computing resources.\n+    node_labels:\n+      # literal strings:\n+      'env': 'test'\n+    # regexp expressions\n+      'reg': '^us-west-1|eu-central-1$'\n+\n+    # node_labels_expression has the same purpose as node_labels but\n+    # supports predicate expressions to configure custom logic.\n+    # A user with this role will be allowed to access nodes if they are in the\n+    # staging environment *or* if they belong to one of the user's own teams.\n+    node_labels_expression: |\n+      labels[\"env\"] == \"staging\" ||\n+      contains(user.spec.traits[\"teams\"] , labels[\"team\"])\n+ \n+    # rules allow a user holding this role to modify other resources\n+    # matching the expressions below.\n+    # rules match both resources and specify what actions are allowed\n+    rules:\n+      - resources: [role]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+We can spot several issues with this role design:\n+\n+* There is no way to specify the scope that role applies to, all roles apply globally to all resources they match and all users they assign to all the time.\n+* There is no way to specify resource hierarchies, like computing environments (env vs prod), which makes it hard to partition the infrastructure and forces customers to specify labels.\n+\n+\n+#### Labels\n+\n+Teleport RBAC\u2019s advice to engineers to partition their resource is to first, label their computing resource or use AWS labels, and second, match the labels in RBAC.\n+\n+Admins can set the tags on the resource configuration file statically, or, for some resources, use `server_info` to set the tags for each resource:\n+\n+```yaml\n+# server_info.yaml\n+kind: server_info\n+metadata:\n+   name: si-<node-name>\n+spec:\n+   new_labels:\n+      \"foo\": \"bar\"\n+```\n+\n+This creates several issues:\n+\n+* It is not always secure to delegate labeling to owners of computing resources, as anyone with root access to the node config file can update its labels impacting everyone else.\n+* It is not scalable, as it\u2019s not always useful to set and updates tags for each individual resource\n+* It\u2019s hard or impossible to partition infrastructure with two-dimensional labels, although users can use `env: prod` to mark all resources in the production environment, there is no way to say that `env: lab` is a subset of `env: prod`.\n+\n+#### Current Roles mapping\n+\n+In Teleport there are multiple ways to map roles to users: static roles mapping to local users and bots, \n+dynamic mapping to SSO via connectors and on-demand assignment via access lists and access requests.\n+\n+Most users start with local static mapping and SSO mapping, later graduating to access requests and access lists.\n+\n+https://goteleport.com/docs/access-controls/sso/#creating-an-authentication-connector\n+\n+For SSO users, on login, Teleport checks SSO connector, and maps attributes of a user to a list of roles:\n+\n+```yaml\n+# connector.yaml\n+kind: saml\n+version: v2\n+metadata:\n+  name: corporate\n+spec:\n+  attributes_to_roles:\n+    - {name: \"groups\", value: \"okta-admin\", roles: [\"access\"]}\n+     # regular expressions with capture are also supported. \n+     # the next line instructs Teleport\n+     # to assign users to roles `admin-1` if his SAML \"group\" \n+     # attribute equals 'ssh_admin_1':\n+   - { name: \"group\", value: \"^ssh_admin_(.*)$\", roles: [\"admin-$1\"] }\n+```\n+\n+This creates several security and scalability issues:\n+\n+* Every time administrators need to change roles assignment, they have to update the resource.\n+* Every time the Identity Provider changes user\u2019s attributes, the users would have to relogin to get new roles. \n+\n+For local users, administrators have to specify roles and traits in the local resource:\n+\n+```yaml\n+kind: user\n+version: v2\n+metadata:\n+  name: joe\n+spec:\n+  # roles is a list of roles assigned to this user\n+  roles:\n+  - admin\n+  # traits are key, list of values pairs assigned to a user resource.\n+  # Traits can be used in role templates as variables.\n+  traits:\n+    logins:\n+    - joe\n+    - root\n+```\n+\n+This also creates some challenges, as administrators are forced to update local resources each time they have to assign a user new permission.\n+\n+### Modifications\n+\n+Let\u2019s now introduce the missing pieces of the puzzle and review how new roles will simplify or deprecate legacy concepts.\n+\n+#### Hierarchical Resource Groups\n+\n+Resource groups are one such missing piece - in most cloud environments, resources are split into hierarchy, \n+for example, host `luna` is a member of a resource group `lab`, in turn a member of environment `prod`, which is in turn a member of a Teleport cluster.\n+\n+In Teleport, we will make cluster a default root of this hierarchy. Every computing resource by default will be a direct member of a root `cluster` resource group.\n+\n+One resource can be assigned to multiple resource groups at a time, or none. \n+\n+In our example, the cluster administrator would define lab resource group in the following way:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent: prod_env\n+```\n+\n+Administrators can assign resources to resource groups:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+   parent: prod_env\n+ match_kinds:\n+   - node\n+   - database\n+   - role\n+   - access_list\n+   - '.*'\n+ match_labels:  \n+   env: prod\n+```\n+\n+In this case, any resource that matches `env:prod` label will be assigned to this resource group. We will use the same label matching algorithm as in today's `discovery_service`. \n+\n+This will let administrators to gradually migrate their existing flat infrastructure to resource groups one.\n+\n+In some cases it makes sense to specify parent resource group inline:\n+\n+```yaml\n+kind: role\n+spec:\n+  parent_resource_group: /env/prod\n+```\n+\n+By default, if unspecified, a resource is a member of a root-level - `/` cluster resource group. If specified by the resource, it won't be a member of a root `/` resource group.\n+\n+Resource groups are hierarchical, and we can refer to the `lab` resource group by its full path as `/env/prod/lab`. \n+\n+Most Teleport resources, with some exceptions, like users, SSO connectors can be a member of a resource group. \n+\n+We will list those resources separately below.\n+\n+##### Default Resource groups via auto-discovery\n+\n+Teleport can create resource groups if admins turn on auto discovery. This will significantly simplify configuration. \n+\n+Here are some of the resource groups that Teleport Discovery service will create:\n+\n+* For AWS, Teleport discovery service will place each computing resource in `/aws/[account-id]/[region]/[resource-type]/[resource-id]`.\n+  + When EKS auto-discovery is on, this hierarchy will include discovered apps - `/aws/account-id/[region]/k8s/[cluster-name]/namespaces/[namespace]/[app-id]`\n+* For Azure, Teleport will use Azure's hierarchy - `/azure/[management-group]/[subscription]/[resource-group]/[resource-type]/[resource-id]`\n+* For GCP, we will use GCP hierarchy of `/gcp/[organization]/[folder]/[project]/[resource-type]/[resource-id]`\n+\n+Discovery service will create and remove these hierarchies based on the cloud state, and will create resources with `parent_resource_group` field to place them in those resource groups.\n+\n+If users are not happy with a default hierarchy, they can create a different one.\n+\n+#### Access Lists\n+\n+Teleport has a concept of access lists, that lists an owner, members, and optionally a parent access list.\n+Access List in Teleport represents a group of users with a hierarchy. \n+\n+We will further assume that the root of this hierarchy is a cluster. \n+\n+Unlike in resource groups, a user can be an owner and a member of none, one or several access lists at once.\n+\n+In addition to that, access list grants a role to a set of members, like in this example:\n+\n+```yaml\n+kind: access_list",
        "comment_created_at": "2025-05-06T09:12:15+00:00",
        "comment_author": "FireDrunk",
        "comment_body": "I don't think underscores are allowed in Kubernetes CRD's, perhaps it's best to follow that practice.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2310238207",
    "pr_number": 57888,
    "pr_file": "rfd/0222-bot-instances-at-scale.md",
    "created_at": "2025-08-29T13:57:16+00:00",
    "commented_code": "+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine & Workload ID bots.\n+\n+It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine & Workload ID, it has become cumbersome to manage many bots and hundreds of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web app allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major version behind the control plane (thereby preventing a safe upgrade). To facilitate this, the filter will allow queries such as `semver_lt(version, \"18.1\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `semver_gte(version, \"18.1.0\") && semver_lt(version, \"19\")`.\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. A filter such as `contains(notices, \"Proxy URL not set\")` will be pre-populated.\n+\n+**As a cluster owner (Infrastructure Security team), I'd like to be able to figure out what a Bot Instance is for/who it belongs to when making decisions (e.g can we upgrade/break this Bot safely).**\n+\n+How bot/instances are tagged with an owner, organizational area or purpose will be the subject of a later phase of delivery. As such it is left undefined for the time being. It may simply be a series of optional fields (or labels) which can be included in the `tbot` configuration, via flags or environment variables. These would be sent along with heartbeats and be visible in the web UI.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. Either of these values can be used to filter the instances list, and should make finding the relevant instance easy.\n+\n+Once found, the instance can be selected to view instance details. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure.\n+\n+### Instances dashboard\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+\n+![](assets/0222-details-overview.png)\n+\n+![](assets/0222-details-services.png)\n+\n+![](assets/0222-details-notices.png)\n+\n+![](assets/0222-details-config.png)\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance. A custom language parser will be used to provide instance-specific functions such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `semver_lt(version, 18.1)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `semver_gte(version, \"18\") && semver_lt(version, \"18.1\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Proto Specification\n+\n+### Heartbeat protobuf additions\n+\n+``` protobuf\n+message BotInstanceStatusHeartbeat {\n+\t// ...[snip]...\n+\n+\t// Kind identifies whether the bot is running in the tbot binary or embedded\n+\t// in another component.\n+\tBotKind kind = 10;\n+\n+\t// Notices emitted since the last heartbeat.\n+\t//\n+\t// The server will clear any previous notices if `is_startup` is true, so that\n+\t// editing tbot's configuration and restarting it clears any warnings from a\n+\t// previous bad configuration.\n+\trepeated BotInstanceNotice notices = 11;\n+\n+\t// Snapshots of the bot instance services' health.\n+  repeated BotInstanceServiceStatus service_statuses = 12;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 13;\n+}\n+\n+// BotKind identifies whether the bot is the tbot binary or embedded in another\n+// component.\n+enum BotKind {\n+\t\t// UNSET is the enum zero value.\n+\t\tUNSET = 0;\n+\n+\t\t// TBOT_BINARY means the bot is running the tbot binary.\n+\t\tTBOT_BINARY = 1;\n+\n+\t\t// TERRAFORM_PROVIDER means the bot is running inside the Teleport Terraform\n+\t\t// provider.\n+\t\tTERRAFORM_PROVIDER = 2;\n+\n+\t\t// KUBERNETES_OPERATOR means the bot is running inside the Teleport Kubernetes\n+\t\t// operator.\n+\t\tKUBERNETES_OPERATOR = 3;",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "2310238207",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2310238207",
        "commented_code": "@@ -0,0 +1,367 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine & Workload ID bots.\n+\n+It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine & Workload ID, it has become cumbersome to manage many bots and hundreds of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web app allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major version behind the control plane (thereby preventing a safe upgrade). To facilitate this, the filter will allow queries such as `semver_lt(version, \"18.1\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `semver_gte(version, \"18.1.0\") && semver_lt(version, \"19\")`.\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. A filter such as `contains(notices, \"Proxy URL not set\")` will be pre-populated.\n+\n+**As a cluster owner (Infrastructure Security team), I'd like to be able to figure out what a Bot Instance is for/who it belongs to when making decisions (e.g can we upgrade/break this Bot safely).**\n+\n+How bot/instances are tagged with an owner, organizational area or purpose will be the subject of a later phase of delivery. As such it is left undefined for the time being. It may simply be a series of optional fields (or labels) which can be included in the `tbot` configuration, via flags or environment variables. These would be sent along with heartbeats and be visible in the web UI.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. Either of these values can be used to filter the instances list, and should make finding the relevant instance easy.\n+\n+Once found, the instance can be selected to view instance details. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure.\n+\n+### Instances dashboard\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+\n+![](assets/0222-details-overview.png)\n+\n+![](assets/0222-details-services.png)\n+\n+![](assets/0222-details-notices.png)\n+\n+![](assets/0222-details-config.png)\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance. A custom language parser will be used to provide instance-specific functions such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `semver_lt(version, 18.1)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `semver_gte(version, \"18\") && semver_lt(version, \"18.1\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Proto Specification\n+\n+### Heartbeat protobuf additions\n+\n+``` protobuf\n+message BotInstanceStatusHeartbeat {\n+\t// ...[snip]...\n+\n+\t// Kind identifies whether the bot is running in the tbot binary or embedded\n+\t// in another component.\n+\tBotKind kind = 10;\n+\n+\t// Notices emitted since the last heartbeat.\n+\t//\n+\t// The server will clear any previous notices if `is_startup` is true, so that\n+\t// editing tbot's configuration and restarting it clears any warnings from a\n+\t// previous bad configuration.\n+\trepeated BotInstanceNotice notices = 11;\n+\n+\t// Snapshots of the bot instance services' health.\n+  repeated BotInstanceServiceStatus service_statuses = 12;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 13;\n+}\n+\n+// BotKind identifies whether the bot is the tbot binary or embedded in another\n+// component.\n+enum BotKind {\n+\t\t// UNSET is the enum zero value.\n+\t\tUNSET = 0;\n+\n+\t\t// TBOT_BINARY means the bot is running the tbot binary.\n+\t\tTBOT_BINARY = 1;\n+\n+\t\t// TERRAFORM_PROVIDER means the bot is running inside the Teleport Terraform\n+\t\t// provider.\n+\t\tTERRAFORM_PROVIDER = 2;\n+\n+\t\t// KUBERNETES_OPERATOR means the bot is running inside the Teleport Kubernetes\n+\t\t// operator.\n+\t\tKUBERNETES_OPERATOR = 3;",
        "comment_created_at": "2025-08-29T13:57:16+00:00",
        "comment_author": "rosstimothy",
        "comment_body": "This enum should follow proto best practices. The linter would surely complain about these presently.\r\n```suggestion\r\n\t\tBOT_KIND_UNSPECIFIED = 0;\r\n\r\n\t\t// TBOT_BINARY means the bot is running the tbot binary.\r\n\t\tBOT_KIND_TBOT_BINARY = 1;\r\n\r\n\t\t// TERRAFORM_PROVIDER means the bot is running inside the Teleport Terraform\r\n\t\t// provider.\r\n\t\tBOT_KIND_TERRAFORM_PROVIDER = 2;\r\n\r\n\t\t// KUBERNETES_OPERATOR means the bot is running inside the Teleport Kubernetes\r\n\t\t// operator.\r\n\t\tBOT_KIND_KUBERNETES_OPERATOR = 3;\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2313688496",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2310238207",
        "commented_code": "@@ -0,0 +1,367 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine & Workload ID bots.\n+\n+It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine & Workload ID, it has become cumbersome to manage many bots and hundreds of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web app allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major version behind the control plane (thereby preventing a safe upgrade). To facilitate this, the filter will allow queries such as `semver_lt(version, \"18.1\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `semver_gte(version, \"18.1.0\") && semver_lt(version, \"19\")`.\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. A filter such as `contains(notices, \"Proxy URL not set\")` will be pre-populated.\n+\n+**As a cluster owner (Infrastructure Security team), I'd like to be able to figure out what a Bot Instance is for/who it belongs to when making decisions (e.g can we upgrade/break this Bot safely).**\n+\n+How bot/instances are tagged with an owner, organizational area or purpose will be the subject of a later phase of delivery. As such it is left undefined for the time being. It may simply be a series of optional fields (or labels) which can be included in the `tbot` configuration, via flags or environment variables. These would be sent along with heartbeats and be visible in the web UI.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. Either of these values can be used to filter the instances list, and should make finding the relevant instance easy.\n+\n+Once found, the instance can be selected to view instance details. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure.\n+\n+### Instances dashboard\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+\n+![](assets/0222-details-overview.png)\n+\n+![](assets/0222-details-services.png)\n+\n+![](assets/0222-details-notices.png)\n+\n+![](assets/0222-details-config.png)\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance. A custom language parser will be used to provide instance-specific functions such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `semver_lt(version, 18.1)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `semver_gte(version, \"18\") && semver_lt(version, \"18.1\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Proto Specification\n+\n+### Heartbeat protobuf additions\n+\n+``` protobuf\n+message BotInstanceStatusHeartbeat {\n+\t// ...[snip]...\n+\n+\t// Kind identifies whether the bot is running in the tbot binary or embedded\n+\t// in another component.\n+\tBotKind kind = 10;\n+\n+\t// Notices emitted since the last heartbeat.\n+\t//\n+\t// The server will clear any previous notices if `is_startup` is true, so that\n+\t// editing tbot's configuration and restarting it clears any warnings from a\n+\t// previous bad configuration.\n+\trepeated BotInstanceNotice notices = 11;\n+\n+\t// Snapshots of the bot instance services' health.\n+  repeated BotInstanceServiceStatus service_statuses = 12;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 13;\n+}\n+\n+// BotKind identifies whether the bot is the tbot binary or embedded in another\n+// component.\n+enum BotKind {\n+\t\t// UNSET is the enum zero value.\n+\t\tUNSET = 0;\n+\n+\t\t// TBOT_BINARY means the bot is running the tbot binary.\n+\t\tTBOT_BINARY = 1;\n+\n+\t\t// TERRAFORM_PROVIDER means the bot is running inside the Teleport Terraform\n+\t\t// provider.\n+\t\tTERRAFORM_PROVIDER = 2;\n+\n+\t\t// KUBERNETES_OPERATOR means the bot is running inside the Teleport Kubernetes\n+\t\t// operator.\n+\t\tKUBERNETES_OPERATOR = 3;",
        "comment_created_at": "2025-09-01T11:24:38+00:00",
        "comment_author": "boxofrad",
        "comment_body": "My bad, this was hastily written pseudo-code \ud83d\ude1b",
        "pr_file_module": null
      }
    ]
  }
]