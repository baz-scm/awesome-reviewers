[
  {
    "discussion_id": "2297228474",
    "pr_number": 58297,
    "pr_file": "rfd/0222-access-list-templates.md",
    "created_at": "2025-08-25T06:27:21+00:00",
    "commented_code": "+---\n+author: Lisa Kim (lisa@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 222 - Templated Access Lists\n+\n+# Required Approvers\n+\n+- Engineering: @r0mant || @smallinsky (marek) && @kopiczko (pavel)\n+- Product: @r0mant\n+\n+## What\n+\n+Add support for templated Access List that have system behaviors such as Teleport creating the required roles and then Teleport assigning those roles to members and owners upon creating an Access List.\n+\n+## Why\n+\n+Improves Access List usability especially for day one users. Templated Access List allows an admin to focus on users and what resources users should have access to. It removes the need for an admin to learn how to create roles and removes how roles have relation to an Access List because Teleport will do it for them.\n+\n+### User story: As an admin, I want to create an access list that require members to request for access and then grants short-term access to resources\n+\n+The template type to use for this case is `short-term`.\n+\n+Template type `short-term` represents an access list that utilizes JIT. Owners are reviewers. Members are requesters that are required to request access to resources and then upon approval are granted short-term access to requested Teleport resources.\n+\n+Admin will define what resources members will have access to by specifying the resource kinds and its labels and defining the resource principals upon access list creation.\n+\n+### User story: As an admin, I want to create an accesss list that grants long-term access to resources for members\n+\n+The template type to use for this case is `long-term`.\n+\n+Template type `long-term` represents an access list that grants members standing access to Teleport resources. Owners will have no special purpose other than to audit.\n+\n+Admin will define what resources members will have access to by specifying the resource kinds and its labels and defining the resource principals upon access list creation.\n+\n+This type of template is similar to how access list works now (non integrated types). Only difference is Teleport will create the necessary role for the admin.\n+\n+### Data Model\n+\n+#### Type Field\n+\n+In the current [AccessListSpec](https://github.com/gravitational/teleport/blob/bbb0f46b22ff88299908bef8dcf85d292aa379e1/api/proto/teleport/accesslist/v1/accesslist.proto#L75) model, there exists a field called `type`. We will introduce an additional type called `template` to indicate an access list used a template upon creation.\n+\n+```proto\n+message AccessListSpec {\n+  // ... other existing fields\n+\n+  // Existing type values: dynamic, scim, and static\n+  // NEW value: \"template\"\n+  string type = 12;\n+}\n+```\n+\n+#### Template Specification\n+\n+A new field `template_config` will be introduced in the current [AccessListSpec](https://github.com/gravitational/teleport/blob/bbb0f46b22ff88299908bef8dcf85d292aa379e1/api/proto/teleport/accesslist/v1/accesslist.proto#L75) model.\n+\n+```proto\n+message AccessListSpec {\n+  // ... other existing fields\n+\n+  AccessListTemplateConfig template_config = 13;\n+}\n+\n+// AccessListTemplateConfig describes the template used.\n+message AccessListTemplateConfig {\n+  oneof template {\n+    TemplateLongTerm long_term = 1;\n+    TemplateShortTerm short_term = 2;\n+  }\n+}\n+\n+// TemplateLongTerm describes fields required to create\n+// an access list with long term access grant.\n+message TemplateLongTerm {\n+  // access_condition defines access to resources\n+  // and its principals.\n+  AllowResourceAccessConditions access_condition = 1;\n+}\n+\n+// TemplateShortTerm describes fields required to create\n+// an access list with short term access grant.\n+message TemplateShortTerm {\n+  // access_condition defines access to resources\n+  // and its principals.\n+  AllowResourceAccessConditions access_condition = 1;\n+}\n+```\n+\n+The field `AllowResourceAccessConditions` is really just a copy and paste of the existing type [RoleConditions](https://github.com/gravitational/teleport/blob/31143cca86ec73d7404e5f90044996eafff199c8/api/proto/teleport/legacy/types/types.proto#L3759) but only takes the fields that is relevant to an access list and molds it to an organized structure so it will be easier to define with `tctl` users.\n+\n+The `AllowResourceAccessConditions` model describes access to resources by its labels and relevant resource principals.\n+\n+```proto\n+// AllowResourceAccessConditions defines the access to different resources.\n+message AllowResourceAccessConditions {\n+  ApplicationAccess application = 1;\n+  DatabaseAccess database = 2;\n+  GitServerAccess git_server = 3;\n+  KubernetesAccess kubernetes = 4;\n+  ServerAccess server = 5;\n+  WindowsDesktopAccess windows_desktop = 6;\n+}\n+\n+// ApplicationAccess are access related fields for application resource.\n+message ApplicationAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string aws_role_arns = 2 [(gogoproto.jsontag) = \"aws_role_arns,omitempty\"];\n+  repeated string azure_identities = 3 [(gogoproto.jsontag) = \"azure_identities,omitempty\"];\n+  repeated string gcp_service_accounts = 4 [(gogoproto.jsontag) = \"gcp_service_accounts,omitempty\"];\n+  types.MCPPermissions mcp = 5 [(gogoproto.jsontag) = \"mcp,omitempty\"];\n+}\n+\n+// DatabaseAccess are access related fields for db resource.\n+message DatabaseAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string names = 2 [(gogoproto.jsontag) = \"names,omitempty\"];\n+  repeated string users = 3 [(gogoproto.jsontag) = \"users,omitempty\"];\n+}\n+\n+// GitServerAccess are access related fields for git server resource.\n+message GitServerAccess {\n+  repeated types.GitHubPermission permissions = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"permissions,omitempty\"\n+  ];\n+}\n+\n+// KubernetesAccess are access related fields for kube resource.\n+message KubernetesAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string groups = 2 [(gogoproto.jsontag) = \"groups,omitempty\"];\n+  repeated string users = 3 [(gogoproto.jsontag) = \"users,omitempty\"];\n+  repeated types.KubernetesResource resources = 4 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"resources,omitempty\"\n+  ];\n+}\n+\n+// ServerAccess are access related fields for server resource.\n+message ServerAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string logins = 2 [(gogoproto.jsontag) = \"logins,omitempty\"];\n+}\n+\n+// WindowsDesktopAccess are access related fields for windows desktop resource.\n+message WindowsDesktopAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string logins = 2 [(gogoproto.jsontag) = \"logins,omitempty\"];\n+}\n+```\n+\n+### Implementation\n+\n+#### Create\n+\n+A templated access list can only be created if access list fields `type: template` and `template_config` defines at least one resource field with its labels set. Resource principals will not be required.\n+\n+Once these fields are validated, Teleport will take the template specifications and create system roles and then assign them to member and owner.\n+\n+##### System roles for short-term template\n+\n+- access: a role that defines the access to resources - this role is not assigned directly to anyone\n+- reviewer: a role assigned to owner grants that allow them to review requests to resources defined in `access` role\n+- requester: a role assigned to member grants that allow them to search and request for resources defined in `access` role\n+\n+##### System roles for long-term template\n+\n+- access: a role that defines the access to resources and is assigned to member grants\n+\n+##### Naming system roles\n+\n+In order to ensure uniqueness and help identifying which roles belong to access lists, the naming convention takes the following format:\n+\n+`<purpose>-acl-role-<access list metadata name (UID)>`\n+\n+| Parts                             |                      Explanation                      |              Example Values |\n+| :-------------------------------- | :---------------------------------------------------: | --------------------------: |\n+| \\<purpose\\>                       |     short word that describes the purpose of role     | requester, reviewer, access |\n+| acl-role                          |             stands for \"access list role\"             |                         n/a |\n+| <access list metadata name (UID)> | helps identify which access list this role belongs to |                         n/a |\n+\n+Example names of system roles, if an access list with metadata.name is set to `abcd1234`\n+\n+- requester-acl-role-abcd1234\n+- reviewer-acl-role-abcd1234\n+- access-acl-role-abcd1234\n+\n+#### Update\n+\n+The `type: template` field cannot be modified.\n+\n+Minus the fields that are already modifiable, `template_config` field is partly modifiable. The field that defines access to resources can be modified. The `oneof type` will not be modifiable eg: if an access list template was originally `short-term`, a user cannot change the template to `long-term`.\n+\n+For both templates, since only the `access` part of the `template_config` can be modified, the system role related to `access` will be updated.\n+\n+##### Update quirk\n+\n+There is a quirk where the `access` definition set on an access list might not be in sync with the actual role resource because we don't prevent users from editing system roles with `tctl` (the web UI does not allow reading/updating system roles). If such a case happens, ultimately the role resource is the source of truth. Any updates made to `template_config` will overwrite any previous edits directly made to the system roles.\n+\n+#### Delete\n+\n+In the backend, after an access list is successfully deleted, all system roles tied to that deleted access list will also be deleted.\n+\n+In the case deleting roles fail for some reason, we can offer a retry if we detect the failure was due to clean up. An API endpoint will be created that is specific to cleaning up templated access list (which is to just delete roles at this moment).\n+\n+### Phases\n+\n+#### Phase 1\n+\n+Add CRUD support for templated access list (both short and long term) through the web UI and tctl.\n+\n+#### Phase 2\n+\n+Support scaling templated access list with terraform. In addition, add scaling directions on the web UI as `next steps` after creating an access list is successful.",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "2297228474",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58297,
        "pr_file": "rfd/0222-access-list-templates.md",
        "discussion_id": "2297228474",
        "commented_code": "@@ -0,0 +1,258 @@\n+---\n+author: Lisa Kim (lisa@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 222 - Templated Access Lists\n+\n+# Required Approvers\n+\n+- Engineering: @r0mant || @smallinsky (marek) && @kopiczko (pavel)\n+- Product: @r0mant\n+\n+## What\n+\n+Add support for templated Access List that have system behaviors such as Teleport creating the required roles and then Teleport assigning those roles to members and owners upon creating an Access List.\n+\n+## Why\n+\n+Improves Access List usability especially for day one users. Templated Access List allows an admin to focus on users and what resources users should have access to. It removes the need for an admin to learn how to create roles and removes how roles have relation to an Access List because Teleport will do it for them.\n+\n+### User story: As an admin, I want to create an access list that require members to request for access and then grants short-term access to resources\n+\n+The template type to use for this case is `short-term`.\n+\n+Template type `short-term` represents an access list that utilizes JIT. Owners are reviewers. Members are requesters that are required to request access to resources and then upon approval are granted short-term access to requested Teleport resources.\n+\n+Admin will define what resources members will have access to by specifying the resource kinds and its labels and defining the resource principals upon access list creation.\n+\n+### User story: As an admin, I want to create an accesss list that grants long-term access to resources for members\n+\n+The template type to use for this case is `long-term`.\n+\n+Template type `long-term` represents an access list that grants members standing access to Teleport resources. Owners will have no special purpose other than to audit.\n+\n+Admin will define what resources members will have access to by specifying the resource kinds and its labels and defining the resource principals upon access list creation.\n+\n+This type of template is similar to how access list works now (non integrated types). Only difference is Teleport will create the necessary role for the admin.\n+\n+### Data Model\n+\n+#### Type Field\n+\n+In the current [AccessListSpec](https://github.com/gravitational/teleport/blob/bbb0f46b22ff88299908bef8dcf85d292aa379e1/api/proto/teleport/accesslist/v1/accesslist.proto#L75) model, there exists a field called `type`. We will introduce an additional type called `template` to indicate an access list used a template upon creation.\n+\n+```proto\n+message AccessListSpec {\n+  // ... other existing fields\n+\n+  // Existing type values: dynamic, scim, and static\n+  // NEW value: \"template\"\n+  string type = 12;\n+}\n+```\n+\n+#### Template Specification\n+\n+A new field `template_config` will be introduced in the current [AccessListSpec](https://github.com/gravitational/teleport/blob/bbb0f46b22ff88299908bef8dcf85d292aa379e1/api/proto/teleport/accesslist/v1/accesslist.proto#L75) model.\n+\n+```proto\n+message AccessListSpec {\n+  // ... other existing fields\n+\n+  AccessListTemplateConfig template_config = 13;\n+}\n+\n+// AccessListTemplateConfig describes the template used.\n+message AccessListTemplateConfig {\n+  oneof template {\n+    TemplateLongTerm long_term = 1;\n+    TemplateShortTerm short_term = 2;\n+  }\n+}\n+\n+// TemplateLongTerm describes fields required to create\n+// an access list with long term access grant.\n+message TemplateLongTerm {\n+  // access_condition defines access to resources\n+  // and its principals.\n+  AllowResourceAccessConditions access_condition = 1;\n+}\n+\n+// TemplateShortTerm describes fields required to create\n+// an access list with short term access grant.\n+message TemplateShortTerm {\n+  // access_condition defines access to resources\n+  // and its principals.\n+  AllowResourceAccessConditions access_condition = 1;\n+}\n+```\n+\n+The field `AllowResourceAccessConditions` is really just a copy and paste of the existing type [RoleConditions](https://github.com/gravitational/teleport/blob/31143cca86ec73d7404e5f90044996eafff199c8/api/proto/teleport/legacy/types/types.proto#L3759) but only takes the fields that is relevant to an access list and molds it to an organized structure so it will be easier to define with `tctl` users.\n+\n+The `AllowResourceAccessConditions` model describes access to resources by its labels and relevant resource principals.\n+\n+```proto\n+// AllowResourceAccessConditions defines the access to different resources.\n+message AllowResourceAccessConditions {\n+  ApplicationAccess application = 1;\n+  DatabaseAccess database = 2;\n+  GitServerAccess git_server = 3;\n+  KubernetesAccess kubernetes = 4;\n+  ServerAccess server = 5;\n+  WindowsDesktopAccess windows_desktop = 6;\n+}\n+\n+// ApplicationAccess are access related fields for application resource.\n+message ApplicationAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string aws_role_arns = 2 [(gogoproto.jsontag) = \"aws_role_arns,omitempty\"];\n+  repeated string azure_identities = 3 [(gogoproto.jsontag) = \"azure_identities,omitempty\"];\n+  repeated string gcp_service_accounts = 4 [(gogoproto.jsontag) = \"gcp_service_accounts,omitempty\"];\n+  types.MCPPermissions mcp = 5 [(gogoproto.jsontag) = \"mcp,omitempty\"];\n+}\n+\n+// DatabaseAccess are access related fields for db resource.\n+message DatabaseAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string names = 2 [(gogoproto.jsontag) = \"names,omitempty\"];\n+  repeated string users = 3 [(gogoproto.jsontag) = \"users,omitempty\"];\n+}\n+\n+// GitServerAccess are access related fields for git server resource.\n+message GitServerAccess {\n+  repeated types.GitHubPermission permissions = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"permissions,omitempty\"\n+  ];\n+}\n+\n+// KubernetesAccess are access related fields for kube resource.\n+message KubernetesAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string groups = 2 [(gogoproto.jsontag) = \"groups,omitempty\"];\n+  repeated string users = 3 [(gogoproto.jsontag) = \"users,omitempty\"];\n+  repeated types.KubernetesResource resources = 4 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"resources,omitempty\"\n+  ];\n+}\n+\n+// ServerAccess are access related fields for server resource.\n+message ServerAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string logins = 2 [(gogoproto.jsontag) = \"logins,omitempty\"];\n+}\n+\n+// WindowsDesktopAccess are access related fields for windows desktop resource.\n+message WindowsDesktopAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string logins = 2 [(gogoproto.jsontag) = \"logins,omitempty\"];\n+}\n+```\n+\n+### Implementation\n+\n+#### Create\n+\n+A templated access list can only be created if access list fields `type: template` and `template_config` defines at least one resource field with its labels set. Resource principals will not be required.\n+\n+Once these fields are validated, Teleport will take the template specifications and create system roles and then assign them to member and owner.\n+\n+##### System roles for short-term template\n+\n+- access: a role that defines the access to resources - this role is not assigned directly to anyone\n+- reviewer: a role assigned to owner grants that allow them to review requests to resources defined in `access` role\n+- requester: a role assigned to member grants that allow them to search and request for resources defined in `access` role\n+\n+##### System roles for long-term template\n+\n+- access: a role that defines the access to resources and is assigned to member grants\n+\n+##### Naming system roles\n+\n+In order to ensure uniqueness and help identifying which roles belong to access lists, the naming convention takes the following format:\n+\n+`<purpose>-acl-role-<access list metadata name (UID)>`\n+\n+| Parts                             |                      Explanation                      |              Example Values |\n+| :-------------------------------- | :---------------------------------------------------: | --------------------------: |\n+| \\<purpose\\>                       |     short word that describes the purpose of role     | requester, reviewer, access |\n+| acl-role                          |             stands for \"access list role\"             |                         n/a |\n+| <access list metadata name (UID)> | helps identify which access list this role belongs to |                         n/a |\n+\n+Example names of system roles, if an access list with metadata.name is set to `abcd1234`\n+\n+- requester-acl-role-abcd1234\n+- reviewer-acl-role-abcd1234\n+- access-acl-role-abcd1234\n+\n+#### Update\n+\n+The `type: template` field cannot be modified.\n+\n+Minus the fields that are already modifiable, `template_config` field is partly modifiable. The field that defines access to resources can be modified. The `oneof type` will not be modifiable eg: if an access list template was originally `short-term`, a user cannot change the template to `long-term`.\n+\n+For both templates, since only the `access` part of the `template_config` can be modified, the system role related to `access` will be updated.\n+\n+##### Update quirk\n+\n+There is a quirk where the `access` definition set on an access list might not be in sync with the actual role resource because we don't prevent users from editing system roles with `tctl` (the web UI does not allow reading/updating system roles). If such a case happens, ultimately the role resource is the source of truth. Any updates made to `template_config` will overwrite any previous edits directly made to the system roles.\n+\n+#### Delete\n+\n+In the backend, after an access list is successfully deleted, all system roles tied to that deleted access list will also be deleted.\n+\n+In the case deleting roles fail for some reason, we can offer a retry if we detect the failure was due to clean up. An API endpoint will be created that is specific to cleaning up templated access list (which is to just delete roles at this moment).\n+\n+### Phases\n+\n+#### Phase 1\n+\n+Add CRUD support for templated access list (both short and long term) through the web UI and tctl.\n+\n+#### Phase 2\n+\n+Support scaling templated access list with terraform. In addition, add scaling directions on the web UI as `next steps` after creating an access list is successful.",
        "comment_created_at": "2025-08-25T06:27:21+00:00",
        "comment_author": "kimlisa",
        "comment_body": "i wasn't sure if I can punt support for terraform? (i saw access list member support added later, so that's where i got my assumption from)",
        "pr_file_module": null
      },
      {
        "comment_id": "2302490987",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58297,
        "pr_file": "rfd/0222-access-list-templates.md",
        "discussion_id": "2297228474",
        "commented_code": "@@ -0,0 +1,258 @@\n+---\n+author: Lisa Kim (lisa@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 222 - Templated Access Lists\n+\n+# Required Approvers\n+\n+- Engineering: @r0mant || @smallinsky (marek) && @kopiczko (pavel)\n+- Product: @r0mant\n+\n+## What\n+\n+Add support for templated Access List that have system behaviors such as Teleport creating the required roles and then Teleport assigning those roles to members and owners upon creating an Access List.\n+\n+## Why\n+\n+Improves Access List usability especially for day one users. Templated Access List allows an admin to focus on users and what resources users should have access to. It removes the need for an admin to learn how to create roles and removes how roles have relation to an Access List because Teleport will do it for them.\n+\n+### User story: As an admin, I want to create an access list that require members to request for access and then grants short-term access to resources\n+\n+The template type to use for this case is `short-term`.\n+\n+Template type `short-term` represents an access list that utilizes JIT. Owners are reviewers. Members are requesters that are required to request access to resources and then upon approval are granted short-term access to requested Teleport resources.\n+\n+Admin will define what resources members will have access to by specifying the resource kinds and its labels and defining the resource principals upon access list creation.\n+\n+### User story: As an admin, I want to create an accesss list that grants long-term access to resources for members\n+\n+The template type to use for this case is `long-term`.\n+\n+Template type `long-term` represents an access list that grants members standing access to Teleport resources. Owners will have no special purpose other than to audit.\n+\n+Admin will define what resources members will have access to by specifying the resource kinds and its labels and defining the resource principals upon access list creation.\n+\n+This type of template is similar to how access list works now (non integrated types). Only difference is Teleport will create the necessary role for the admin.\n+\n+### Data Model\n+\n+#### Type Field\n+\n+In the current [AccessListSpec](https://github.com/gravitational/teleport/blob/bbb0f46b22ff88299908bef8dcf85d292aa379e1/api/proto/teleport/accesslist/v1/accesslist.proto#L75) model, there exists a field called `type`. We will introduce an additional type called `template` to indicate an access list used a template upon creation.\n+\n+```proto\n+message AccessListSpec {\n+  // ... other existing fields\n+\n+  // Existing type values: dynamic, scim, and static\n+  // NEW value: \"template\"\n+  string type = 12;\n+}\n+```\n+\n+#### Template Specification\n+\n+A new field `template_config` will be introduced in the current [AccessListSpec](https://github.com/gravitational/teleport/blob/bbb0f46b22ff88299908bef8dcf85d292aa379e1/api/proto/teleport/accesslist/v1/accesslist.proto#L75) model.\n+\n+```proto\n+message AccessListSpec {\n+  // ... other existing fields\n+\n+  AccessListTemplateConfig template_config = 13;\n+}\n+\n+// AccessListTemplateConfig describes the template used.\n+message AccessListTemplateConfig {\n+  oneof template {\n+    TemplateLongTerm long_term = 1;\n+    TemplateShortTerm short_term = 2;\n+  }\n+}\n+\n+// TemplateLongTerm describes fields required to create\n+// an access list with long term access grant.\n+message TemplateLongTerm {\n+  // access_condition defines access to resources\n+  // and its principals.\n+  AllowResourceAccessConditions access_condition = 1;\n+}\n+\n+// TemplateShortTerm describes fields required to create\n+// an access list with short term access grant.\n+message TemplateShortTerm {\n+  // access_condition defines access to resources\n+  // and its principals.\n+  AllowResourceAccessConditions access_condition = 1;\n+}\n+```\n+\n+The field `AllowResourceAccessConditions` is really just a copy and paste of the existing type [RoleConditions](https://github.com/gravitational/teleport/blob/31143cca86ec73d7404e5f90044996eafff199c8/api/proto/teleport/legacy/types/types.proto#L3759) but only takes the fields that is relevant to an access list and molds it to an organized structure so it will be easier to define with `tctl` users.\n+\n+The `AllowResourceAccessConditions` model describes access to resources by its labels and relevant resource principals.\n+\n+```proto\n+// AllowResourceAccessConditions defines the access to different resources.\n+message AllowResourceAccessConditions {\n+  ApplicationAccess application = 1;\n+  DatabaseAccess database = 2;\n+  GitServerAccess git_server = 3;\n+  KubernetesAccess kubernetes = 4;\n+  ServerAccess server = 5;\n+  WindowsDesktopAccess windows_desktop = 6;\n+}\n+\n+// ApplicationAccess are access related fields for application resource.\n+message ApplicationAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string aws_role_arns = 2 [(gogoproto.jsontag) = \"aws_role_arns,omitempty\"];\n+  repeated string azure_identities = 3 [(gogoproto.jsontag) = \"azure_identities,omitempty\"];\n+  repeated string gcp_service_accounts = 4 [(gogoproto.jsontag) = \"gcp_service_accounts,omitempty\"];\n+  types.MCPPermissions mcp = 5 [(gogoproto.jsontag) = \"mcp,omitempty\"];\n+}\n+\n+// DatabaseAccess are access related fields for db resource.\n+message DatabaseAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string names = 2 [(gogoproto.jsontag) = \"names,omitempty\"];\n+  repeated string users = 3 [(gogoproto.jsontag) = \"users,omitempty\"];\n+}\n+\n+// GitServerAccess are access related fields for git server resource.\n+message GitServerAccess {\n+  repeated types.GitHubPermission permissions = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"permissions,omitempty\"\n+  ];\n+}\n+\n+// KubernetesAccess are access related fields for kube resource.\n+message KubernetesAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string groups = 2 [(gogoproto.jsontag) = \"groups,omitempty\"];\n+  repeated string users = 3 [(gogoproto.jsontag) = \"users,omitempty\"];\n+  repeated types.KubernetesResource resources = 4 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"resources,omitempty\"\n+  ];\n+}\n+\n+// ServerAccess are access related fields for server resource.\n+message ServerAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string logins = 2 [(gogoproto.jsontag) = \"logins,omitempty\"];\n+}\n+\n+// WindowsDesktopAccess are access related fields for windows desktop resource.\n+message WindowsDesktopAccess {\n+  wrappers.LabelValues labels = 1 [\n+    (gogoproto.nullable) = false,\n+    (gogoproto.jsontag) = \"labels,omitempty\",\n+    (gogoproto.customtype) = \"Labels\"\n+  ];\n+  repeated string logins = 2 [(gogoproto.jsontag) = \"logins,omitempty\"];\n+}\n+```\n+\n+### Implementation\n+\n+#### Create\n+\n+A templated access list can only be created if access list fields `type: template` and `template_config` defines at least one resource field with its labels set. Resource principals will not be required.\n+\n+Once these fields are validated, Teleport will take the template specifications and create system roles and then assign them to member and owner.\n+\n+##### System roles for short-term template\n+\n+- access: a role that defines the access to resources - this role is not assigned directly to anyone\n+- reviewer: a role assigned to owner grants that allow them to review requests to resources defined in `access` role\n+- requester: a role assigned to member grants that allow them to search and request for resources defined in `access` role\n+\n+##### System roles for long-term template\n+\n+- access: a role that defines the access to resources and is assigned to member grants\n+\n+##### Naming system roles\n+\n+In order to ensure uniqueness and help identifying which roles belong to access lists, the naming convention takes the following format:\n+\n+`<purpose>-acl-role-<access list metadata name (UID)>`\n+\n+| Parts                             |                      Explanation                      |              Example Values |\n+| :-------------------------------- | :---------------------------------------------------: | --------------------------: |\n+| \\<purpose\\>                       |     short word that describes the purpose of role     | requester, reviewer, access |\n+| acl-role                          |             stands for \"access list role\"             |                         n/a |\n+| <access list metadata name (UID)> | helps identify which access list this role belongs to |                         n/a |\n+\n+Example names of system roles, if an access list with metadata.name is set to `abcd1234`\n+\n+- requester-acl-role-abcd1234\n+- reviewer-acl-role-abcd1234\n+- access-acl-role-abcd1234\n+\n+#### Update\n+\n+The `type: template` field cannot be modified.\n+\n+Minus the fields that are already modifiable, `template_config` field is partly modifiable. The field that defines access to resources can be modified. The `oneof type` will not be modifiable eg: if an access list template was originally `short-term`, a user cannot change the template to `long-term`.\n+\n+For both templates, since only the `access` part of the `template_config` can be modified, the system role related to `access` will be updated.\n+\n+##### Update quirk\n+\n+There is a quirk where the `access` definition set on an access list might not be in sync with the actual role resource because we don't prevent users from editing system roles with `tctl` (the web UI does not allow reading/updating system roles). If such a case happens, ultimately the role resource is the source of truth. Any updates made to `template_config` will overwrite any previous edits directly made to the system roles.\n+\n+#### Delete\n+\n+In the backend, after an access list is successfully deleted, all system roles tied to that deleted access list will also be deleted.\n+\n+In the case deleting roles fail for some reason, we can offer a retry if we detect the failure was due to clean up. An API endpoint will be created that is specific to cleaning up templated access list (which is to just delete roles at this moment).\n+\n+### Phases\n+\n+#### Phase 1\n+\n+Add CRUD support for templated access list (both short and long term) through the web UI and tctl.\n+\n+#### Phase 2\n+\n+Support scaling templated access list with terraform. In addition, add scaling directions on the web UI as `next steps` after creating an access list is successful.",
        "comment_created_at": "2025-08-27T00:42:25+00:00",
        "comment_author": "kopiczko",
        "comment_body": "Yes it should be generated when you change proto files. The command is `make -C ./integrations/terraform gen-tfschema docs install`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2325859307",
    "pr_number": 58065,
    "pr_file": "rfd/0223-k8s-health-checks.md",
    "created_at": "2025-09-05T19:18:21+00:00",
    "commented_code": "+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "2325859307",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2325859307",
        "commented_code": "@@ -0,0 +1,660 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`",
        "comment_created_at": "2025-09-05T19:18:21+00:00",
        "comment_author": "rosstimothy",
        "comment_body": "CheckAndSetDefaults feels like the wrong place to initialize the healthcheck.Manager, I imagine what we instead want is CheckAndSetDefaults to return an error if a healthcheck.Manager was not configured.",
        "pr_file_module": null
      },
      {
        "comment_id": "2325935534",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2325859307",
        "commented_code": "@@ -0,0 +1,660 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`",
        "comment_created_at": "2025-09-05T19:59:32+00:00",
        "comment_author": "rana",
        "comment_body": "`CheckAndSetDefaults` is a pretty good place to initialize it actually. Looking at [db server config](https://github.com/gravitational/teleport/blob/e3396dcac52ccfd1707d51097e633eaa9377e96b/lib/srv/db/server.go#L212) and [kube server config](https://github.com/gravitational/teleport/blob/e3396dcac52ccfd1707d51097e633eaa9377e96b/lib/kube/proxy/server.go#L130), `CheckAndSetDefaults` is initializing internal components, `healthCheckManager` being one of them.\r\n\r\nFrom the db server config:\r\nhttps://github.com/gravitational/teleport/blob/e3396dcac52ccfd1707d51097e633eaa9377e96b/lib/srv/db/server.go#L363-L373\r\n\r\nThat's the main motivation for choosing `CheckAndSetDefaults` for kube.\r\n\r\nAn alternative can be to initialize `healthCheckManager` before the Teleport process kube server initialization in `lib/service/service.go`. Similar to `kubeServerWatcher`:\r\n\r\nhttps://github.com/gravitational/teleport/blob/e3396dcac52ccfd1707d51097e633eaa9377e96b/lib/service/service.go#L5635-L5638\r\n\r\nThe server initialization components like `kubeServerWatcher` are public-facing. There doesn't appear to be a need to make `healthCheckManager` public-facing though.\r\n\r\nEither place works, of course.",
        "pr_file_module": null
      },
      {
        "comment_id": "2325946342",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2325859307",
        "commented_code": "@@ -0,0 +1,660 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`",
        "comment_created_at": "2025-09-05T20:07:02+00:00",
        "comment_author": "rosstimothy",
        "comment_body": "> That's the main motivation for choosing CheckAndSetDefaults for kube.\r\n\r\nWe don't need to follow the road paved by databases 100%. [CheckAndSetDefaults](https://github.com/gravitational/teleport/blob/e3396dcac52ccfd1707d51097e633eaa9377e96b/lib/kube/proxy/server.go#L134-L145) is also aborting when components are not injected properly. \r\n\r\nIn any case this is very much an implementation detail and likely a bit too granular for this RFD. We can discuss the appropriate place for this during implementation. For now I might advise to scale back calling out the exact locations that every change made during implementation will happen in this RFD.",
        "pr_file_module": null
      },
      {
        "comment_id": "2325951970",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58065,
        "pr_file": "rfd/0223-k8s-health-checks.md",
        "discussion_id": "2325859307",
        "commented_code": "@@ -0,0 +1,660 @@\n+---\n+authors: Rana Ian (rana.ian@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0223 - Kubernetes Health Checks\n+\n+## Required Approvals\n+\n+- Engineering: @rosstimothy && @creack && @tigrato\n+\n+\n+## What\n+\n+Enable automated Kubernetes cluster health checks that are viewed by the Web UI, Teleport Connect, `tctl` command, and Prometheus metrics.\n+\n+## Why\n+\n+Proxied Kubernetes cluster health may be manually exercised with Kubernetes operations in the Web UI, or `kubectl` command, then observing the result. While effective, manual checking is also slow and unscalable. \n+\n+Automated Kubernetes health checks with new methods of viewing improve maintainability of Teleport Kubernetes clusters, while enabling new scenarios.\n+\n+Automated health checks:\n+- Improve time to observe and resolve Kubernetes cluster errors\n+- Improve manageability of Kubernetes clusters, especially at scale\n+- Improve the enrollment experience for Kubernetes clusters\n+- Enables alerting on unhealthy Kubernetes clusters\n+- Provides feature parity with databases and machine workload identities\n+\n+## Details\n+\n+### UX\n+\n+#### User Story: Web UI - Enrolling Kubernetes Clusters\n+\n+Alice enrolls three Amazon EKS clusters into Teleport through the Web UI.\n+\n+The next day she returns to the Web UI _Resources_ tab to find the Amazon EKS clusters are highlighted with warnings. She clicks on an Amazon EKS tile and a health message is displayed in a side panel.\n+\n+```\n+Kubernetes Cluster Issues\n+\n+3 Teleport Kubernetes clusters report issues.\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: sol\n+  UUID: 52dedbd0-b165-4bf6-9bc3-961f95bf481d\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: jupiter\n+  UUID: bb4dc171-ffa7-4a31-ba8c-7bf91c59e250\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+\n+Affected Teleport Kubernetes cluster:\n+- Hostname: saturn\n+  UUID: 2be08cb1-56a4-401f-a3f3-c755a73f3ff6\n+  Error: 503 Service Unavailable\n+    [+]ping ok\n+    [+]log ok\n+    [-]etcd not ok: client: etcd cluster is unavailable or misconfigured: context deadline exceeded\n+```\n+\n+Alice notices that each cluster has a similar 503 message saying etcd is the source of the error.\n+\n+Alice resolves the `etcd` error, and each Amazon EKS cluster returns to a healthy state.\n+\n+As she monitors the Teleport Web UI, she sees each Amazon EKS tile switch from a warning state to a normal state.\n+\n+\n+#### User Story: `tctl` - Configuring a New Health Check\n+\n+Bob reads about Kubernetes health checks in a Teleport changelog, and updates a Teleport cluster to the new major version.\n+\n+Bob runs `tctl get health_check_config/default` from a terminal to view the default health settings.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+```\n+\n+He notices a new `kubernetes_labels` matcher.\n+\n+He vets the Kubernetes health checks in non-production environments.\n+\n+Bob runs `tctl edit health_check_config/default` from a terminal, updating the default settings to exclude Kubernetes health checks from a production environment.\n+\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    db_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Bob runs `tctl get kube_server/luna` from a terminal, validating that the expected Kubernetes cluster is monitoring health. \n+```yaml\n+kind: kube_server\n+metadata:\n+  expires: \"2025-10-26T00:00:00.000000Z\"\n+  name: luna\n+  revision: 43e96231-faaf-43c3-b9b8-15cf91813389\n+spec:\n+  host_id: 278be63c-c87e-4d7e-a286-86002c7c45c3\n+  hostname: luna\n+status:\n+  target_health:\n+    addr: luna:3027\n+    protocol: TLS\n+    transition_timestamp: \"2025-10-25T00:00:00.000000Z\"\n+    transition_reason: \"healthy threshold reached\"\n+    status: healthy\n+  version: 19.0.0\n+version: v3\n+```\n+\n+\n+#### User Story: Prometheus - Alerting on an Unhealthy Kubernetes Cluster\n+\n+Charlie relies on Prometheus to notify him of outages and calls to action.\n+\n+He reads about Kubernetes cluster health being available with Prometheus metrics.\n+\n+Charlie tests the feature.\n+\n+He enrolls three GKE instances into Teleport.\n+\n+He queries the new Teleport Prometheus health metrics.\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 3, the actual number of healthy Kubernetes clusters\n+```\n+\n+Charlie sets one GKE instance into an unhealthy state and requeries.\n+\n+```promql\n+teleport_health_resources{type=\"kubernetes\"}\n+# Returns 3, the expected number of healthy Kubernetes clusters\n+\n+teleport_health_resources_available{type=\"kubernetes\"}\n+# Returns 2, the actual number of healthy Kubernetes clusters\n+```\n+\n+Seeing the metric values returning, he sets up a Prometheus alerting rule.\n+\n+```yaml\n+groups:\n+  - name: teleport_kubernetes\n+    rules:\n+      - alert: KubernetesClusterUnhealthy\n+        expr: |\n+          (teleport_health_resources{type=\"kubernetes\"} - \n+           teleport_health_resources_available{type=\"kubernetes\"}) > 0\n+        for: 5m\n+        labels:\n+          severity: warning\n+          team: platform\n+          component: teleport\n+          service: kubernetes\n+        annotations:\n+          summary: \"{{ $value }} Kubernetes cluster(s) unhealthy in Teleport\"\n+          description: \"Teleport reports {{ $value }} unhealthy Kubernetes cluster(s). This indicates that one or more Kubernetes clusters registered with Teleport are not responding or failing health checks. Check Teleport Web UI or use tctl for details.\"\n+          runbook_url: \"https://wiki.goteleport.com/runbooks/teleport-k8s-unhealthy\"\n+          dashboard_url: \"https://grafana.luna.com/d/teleport-k8s/teleport-kubernetes-health\"\n+          query: 'teleport_health_resources{type=\"kubernetes\"} - teleport_health_resources_available{type=\"kubernetes\"}'\n+```\n+\n+Prometheus alerts him about the unhealthy Kubernetes cluster.\n+\n+Charlie sets the GKE instance into a healthy state and moves on with his day.\n+\n+\n+### Implementation Details\n+\n+Kubernetes health checks are discussed by functional areas of core logic, `tctl` command, Web UI, and Prometheus metrics.\n+\n+#### Core Implementation\n+\n+Teleport Kubernetes health checks use the Teleport `healthcheck` package, and is based on existing [database health check](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#user-content-fn-1-b6df2ad8fd7a63ee3ca0af227e74ab87) design patterns. The `healthcheck` components are written, tested, and in production. The focus and effort for Kubernetes health checks is integrating health checks into the Kubernetes agent, extending existing `healthcheck` mechanisms, and updating the UI.\n+\n+##### Core Configuration\n+\n+A first step to enabling Kubernetes health checks is adding new matchers to the `HealthCheckConfig` service. `HealthCheckConfig` identifies servers which choose to participate in health checking. `HealthCheckConfig` supports databases. Kubernetes additions mirror the database features.\n+\n+The configuration adds matchers `kubernetes_labels` and `kubernetes_labels_expression` which specify labeled Kubernetes clusters. By default, all Kubernetes clusters participate in health checks. Matchers may filter Kubernetes clusters. Deleting the matchers excludes all Kubernetes clusters.\n+\n+An example yaml `health_check_config`:\n+```yaml\n+version: v1\n+metadata:\n+  name: \"default\"\n+  labels:\n+    teleport.internal/resource-type: preset\n+spec:\n+  match:\n+    kubernetes_labels:\n+      - name: \"*\"\n+        values:\n+          - \"*\"\n+    kubernetes_labels_expression: \"labels.env != `prod`\"\n+```\n+\n+Change points for health check configuration:\n+- [api/proto/teleport/healthcheckconfig/v1/health_check_config.proto](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/api/proto/teleport/healthcheckconfig/v1/health_check_config.proto#L59)\n+  - Adds `kubernetes_labels` and `kubernetes_labels_expression` to proto message `Matcher`\n+- [lib/services/health_check_config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/health_check_config.go#L58)\n+  - Adds Kubernetes label matcher validation to function `ValidateHealthCheckConfig()`\n+- [lib/healthcheck/config.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/healthcheck/config.go#L35)\n+  - Adds Kubernetes label matcher field to type `healthCheckConfig`, and update functions `newHealthCheckConfig()` and `getLabelMatchers()`\n+- [lib/services/presets.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/services/presets.go#L830)\n+  - Adds wildcard Kubernetes label matchers to function `NewPresetHealthCheckConfig()`\n+\n+`HealthCheckConfig` is communicated via proxy and cached on a Kubernetes agent. Kubernetes interfaces are updated to support the communication and caching.\n+\n+Change points for communication and caching:\n+- [lib/auth/authclient/api.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/auth/authclient/api.go#L472)\n+  - Adds `services.HealthCheckConfigReader` to interfaces `ReadKubernetesAccessPoint` and `ProxyAccessPoint` \n+- api/types/kubernetes_server.go\n+  - Adds functions `GetTargetHealth()`, `SetTargetHealth()`, `GetTargetHealthStatus()`, and `SetTargetHealthStatus()` to the `KubeServer` interface for implementing interface `services.HealthCheckConfigReader`\n+- [lib/cache/cache.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/cache/cache.go#L356)\n+  - Adds watches for `types.KindHealthCheckConfig` in `ForKubernetes()` and `ForProxy()`\n+- [lib/authz/permissions.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/authz/permissions.go#L1183)\n+  - Adds new rules for `types.KindHealthCheckConfig`\n+\n+Details for configuring `HealthCheckConfig` with interval, timeout, and healthy/unhealthy thresholds are described in the [database health check RFD](https://github.com/gravitational/teleport/blob/master/rfd/0203-database-healthchecks.md#configuration).\n+\n+\n+##### Core Kubernetes Agent\n+\n+The Kubernetes agent registers one or more Kubernetes clusters, checks the health of proxied Kubernetes clusters, and communicates the health state to the auth server. The agent adds a `healthcheck.Manager` which performs the registration and health check operations. The Kubernetes agent is named `TLSServer`, and is located in [lib/kube/proxy/server.go](https://github.com/gravitational/teleport/blob/590c85a765a6d8d16d5f34503179f27a97a4625c/lib/kube/proxy/server.go#L187).\n+\n+Change points include:\n+\n+Modifying methods:\n+- `(*TLSServerConfig).CheckAndSetDefaults()` - Initializes the `healthcheck.Manager`",
        "comment_created_at": "2025-09-05T20:10:26+00:00",
        "comment_author": "rana",
        "comment_body": "Appreciate the perspective. Will make the RFD higher-level.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2316346408",
    "pr_number": 58518,
    "pr_file": "rfd/XXXX-scopes.md",
    "created_at": "2025-09-02T14:53:56+00:00",
    "commented_code": "+---\n+authors: Forrest Marshall (forrest@goteleport.com)\n+state: draft\n+---\n+\n+# RFD XXXX - Scopes\n+\n+## What\n+\n+A system for heirarchical organization and isolation for resources and permissions.\n+\n+\n+## Why\n+\n+Historically, teleport's resource and permission organization systems have been very \"flat\". In particular,\n+administrative and resource-provisioning permissions tend to be \"all or nothing\". Resource labels and selectors\n+allow organization of resources/permissions, but said organization can only be performed by what are effectively\n+global admins. Any user with the ability to create node join tokens can join any node to the cluster\n+with any labels they like. Similarly, a user with role creation permissions can create a role that grants access\n+to anything. This poses a significant challenge when trying to delegate any meaningful responsibility and/or to\n+apply the prinicial of least privilege to users that need to administer resources/permissions in an meaningful way.\n+\n+Similarly, teleport user credentials tend to be all or nothing. There isn't a good way to get credentials that\n+are only useable for the specific task at hand. Instead, if a user is logged into a teleport cluster they always\n+have all their available permissions applied. This increases blast radius, both in terms of compromise and accidental\n+misuse.\n+\n+We would like to provide a mechanism for organizing resources and permissions in a manner that allows for both\n+isolation and heirarchy. This system should support admins that have powerful control over provisioning of, and\n+access to, resources within their domain of influence. Said admin privileges must not be able to affect resources\n+and permissions outside the scope of their domain(s). We would also like to provide means of limiting the blast\n+radius of compromise/misuse as part of this organizational system. Finally, we require that this organizational\n+system be backwards compatible with existing teleport resources, permissions, and usage patterns to the greatest\n+extent possible.\n+\n+\n+## Intro\n+\n+### Overview\n+\n+We will introduce the concept of a \"scope\" as a new means of organizing resources and permissions. The scope of\n+a resource or permission will be a simple attribute formatted as a path-like string (e.g. `/staging/west`, `/prod/east`,\n+etc).\n+\n+Permissions that are scoped will apply only to resources of the same scope, or a descendant scope. For example, having\n+permission to ssh into nodes assigned at scope `/staging` will permit ssh access for nodes that have a scope of `/staging`\n+or `/staging/west`, but not `/prod` or `/prod/west`.\n+\n+Scoping will apply to administrative privileges as well. A user with the the permission to join teleport nodes assigned\n+at `/staging` will *only* be able to join nodes with that scope or a descendant scope. Same goes for role creation/assignment,\n+with users effectively being able to be granted admin-like powers \"within\" a scope but not outside of it.\n+\n+In order to improve useability and reduce blast radius of compromise/misuse, we will also introduce the concept of scope\n+\"pinning\". Rather than logging into the teleport cluster as a whole, users will be able to login to a specific scope. This\n+will result in the credentials granted to the user *only* being usable for the target scope and its descendants. For example,\n+if a user has permissions at `/prod` and `/staging`, and logs in to `/staging`, they will only be able to see and interact\n+with `/staging` scoped resources.\n+\n+We will be targeting a basic user experience that looks something like this:\n+\n+```shell\n+$ tsh login --scope=/staging/east\n+# authentication flow\n+\n+$ tsh ls\n+Node Name      Address Labels\n+-------------- ------- ------\n+some-node-east ...     ...\n+\n+$ tsh ssh some-node-east\n+# success\n+\n+$ tsh login --scope=/staging/west\n+# authentication flow\n+\n+$ tsh ls\n+Node Name      Address Labels\n+-------------- ------- ------\n+some-node-west ...     ...\n+\n+$ tsh ssh some-node-east\n+ERROR: access denied\n+\n+$ tsh ssh some-node-west\n+# success\n+```\n+\n+Note that the nature of commands after login are unchanged. Ordinary (non-admin) users should be able to ignore the concept\n+of scoping once they have logged in to the appropriate scope. Scripts that work today with an already logged-in `tsh` should\n+continue to function as expected with scoped credentials, with the only change being that the resources affected by the opeartions\n+are now limited to the subset assigned to the pinned scope.\n+\n+Scoping will be a large and complex feature to implement, as it will meaningfully change most access-control\n+checks and APIs in teleport. In order to make this transition more manageable, we will be gradually implementing scoped\n+features over time, with the initial scopes MVP only providing very basic scoped joining, role management, and ssh access.\n+The intent will be that users will be able to start adopting a mixed use style as scope features become sufficiently robust\n+to start addressing their specific usecases.\n+\n+\n+### Scoping Semantics\n+\n+A scope is a simple path-like string (e.g. `/staging/west`). A resource will be said to be \"scoped\" if it has a `scope` attribute\n+that obeys this format. Likewise, a permission will be scoped if it is granted by a configuration resource with a\n+`scope` attribute.\n+\n+Scopes are heirarchical. The scope `/staging` is a \"parent\" of `/staging/west`, and `/staging/west` is a \"child\" of `/staging`.\n+Permissions granted at a parent scope apply to all child scopes.\n+\n+Scope heirarchy is segment based, not prefix based. I.e. `/staging` is a parent of `/staging/west`, but not of `/stagingwest`.\n+\n+Scopes are attributes, not objects. There is no need to create a scope object before creating a resource with a given\n+scope attribute. I.e. if no resource exists with scope `/staging/west`, a node can still be created with that scope without\n+first performing any kind of scope setup/creation/configuration ceremony.\n+\n+\n+### Core Design Goals\n+\n+**Heirarchical Isolation**: Permissions within a given scope cannot be used to \"reach up\" and affect resources/access defined\n+in parent scopes or \"reach across\" and affect resources/access defined in orthogonal scopes.\n+\n+**Blast Radius Reduction**: Scoping will be a robust tool for further reducing the blast radius of compromised or misused\n+credentials.\n+\n+**Delegated/Limited Administration**: Scoping will unlock the ability to create \"scoped admins\" with powerful control over\n+resources and permissions within their scopes, without being able to affect resources/permissions outside of their scope.\n+\n+**Minimal Effect on User Experience**: After selecting the scope to login to, the user experience for normal (non-administrative)\n+tasks will be unchanged.\n+\n+**Backwards Compatibility**: Scoping will not change the function or meaning of existing teleport resources/permissions.\n+\n+**Gradual Rollout and Adoption**: Scoping features will be rolled out gradually and mixed use of scoped and unscoped\n+resources/permissions will be supported.\n+\n+\n+### Comparison to Other Systems\n+\n+Scoping has a lot in common with systems like the AWS organization/account model, GCP's folder/project model, and Azure's\n+own RBAC scopes. Scopes differ in a few key ways:\n+\n+- Scopes are arbitrarily heirarchical. Resources can be assigned at any level of the heirarchy. Other similar systems tend\n+  to have a heirarchy where all resources live in the leaf nodes, and often the depth of the heirarchy is fixed.\n+\n+- Scopes are an attribute, not an object. There is no additional creation step like there would be with a system that organizes\n+  resources into accounts/projects/etc. Other systems tend to require the heirarchy be defined separately as a standalone entity\n+  prior to resources being created.\n+\n+- Credentials can be pinned to arbitrarily narrow scopes. A user can opt to access a resource in `/staging/west` by logging\n+  into `/staging` *or* logging into `/staging/west`. This gives granular control over the blast radius of credentials, and\n+  ensures that heirarchies and practices can evolve and refine over time without needing to be fully rebuilt. By virtue of\n+  the fixed leaf nodes of other systems, options for constraint of credentials via the organizing heirarchy tend to be more\n+  limited.\n+\n+\n+## Details\n+\n+### Scoping of Resources\n+\n+Within teleport's existing resource format, `scope` will be a new top-level field of type `string`.  Ex:\n+\n+```yaml\n+kind: example_resource\n+metadata:\n+  name: example\n+scope: /staging/west # this resource is \"scoped\" to /staging/west",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "2316346408",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58518,
        "pr_file": "rfd/XXXX-scopes.md",
        "discussion_id": "2316346408",
        "commented_code": "@@ -0,0 +1,438 @@\n+---\n+authors: Forrest Marshall (forrest@goteleport.com)\n+state: draft\n+---\n+\n+# RFD XXXX - Scopes\n+\n+## What\n+\n+A system for heirarchical organization and isolation for resources and permissions.\n+\n+\n+## Why\n+\n+Historically, teleport's resource and permission organization systems have been very \"flat\". In particular,\n+administrative and resource-provisioning permissions tend to be \"all or nothing\". Resource labels and selectors\n+allow organization of resources/permissions, but said organization can only be performed by what are effectively\n+global admins. Any user with the ability to create node join tokens can join any node to the cluster\n+with any labels they like. Similarly, a user with role creation permissions can create a role that grants access\n+to anything. This poses a significant challenge when trying to delegate any meaningful responsibility and/or to\n+apply the prinicial of least privilege to users that need to administer resources/permissions in an meaningful way.\n+\n+Similarly, teleport user credentials tend to be all or nothing. There isn't a good way to get credentials that\n+are only useable for the specific task at hand. Instead, if a user is logged into a teleport cluster they always\n+have all their available permissions applied. This increases blast radius, both in terms of compromise and accidental\n+misuse.\n+\n+We would like to provide a mechanism for organizing resources and permissions in a manner that allows for both\n+isolation and heirarchy. This system should support admins that have powerful control over provisioning of, and\n+access to, resources within their domain of influence. Said admin privileges must not be able to affect resources\n+and permissions outside the scope of their domain(s). We would also like to provide means of limiting the blast\n+radius of compromise/misuse as part of this organizational system. Finally, we require that this organizational\n+system be backwards compatible with existing teleport resources, permissions, and usage patterns to the greatest\n+extent possible.\n+\n+\n+## Intro\n+\n+### Overview\n+\n+We will introduce the concept of a \"scope\" as a new means of organizing resources and permissions. The scope of\n+a resource or permission will be a simple attribute formatted as a path-like string (e.g. `/staging/west`, `/prod/east`,\n+etc).\n+\n+Permissions that are scoped will apply only to resources of the same scope, or a descendant scope. For example, having\n+permission to ssh into nodes assigned at scope `/staging` will permit ssh access for nodes that have a scope of `/staging`\n+or `/staging/west`, but not `/prod` or `/prod/west`.\n+\n+Scoping will apply to administrative privileges as well. A user with the the permission to join teleport nodes assigned\n+at `/staging` will *only* be able to join nodes with that scope or a descendant scope. Same goes for role creation/assignment,\n+with users effectively being able to be granted admin-like powers \"within\" a scope but not outside of it.\n+\n+In order to improve useability and reduce blast radius of compromise/misuse, we will also introduce the concept of scope\n+\"pinning\". Rather than logging into the teleport cluster as a whole, users will be able to login to a specific scope. This\n+will result in the credentials granted to the user *only* being usable for the target scope and its descendants. For example,\n+if a user has permissions at `/prod` and `/staging`, and logs in to `/staging`, they will only be able to see and interact\n+with `/staging` scoped resources.\n+\n+We will be targeting a basic user experience that looks something like this:\n+\n+```shell\n+$ tsh login --scope=/staging/east\n+# authentication flow\n+\n+$ tsh ls\n+Node Name      Address Labels\n+-------------- ------- ------\n+some-node-east ...     ...\n+\n+$ tsh ssh some-node-east\n+# success\n+\n+$ tsh login --scope=/staging/west\n+# authentication flow\n+\n+$ tsh ls\n+Node Name      Address Labels\n+-------------- ------- ------\n+some-node-west ...     ...\n+\n+$ tsh ssh some-node-east\n+ERROR: access denied\n+\n+$ tsh ssh some-node-west\n+# success\n+```\n+\n+Note that the nature of commands after login are unchanged. Ordinary (non-admin) users should be able to ignore the concept\n+of scoping once they have logged in to the appropriate scope. Scripts that work today with an already logged-in `tsh` should\n+continue to function as expected with scoped credentials, with the only change being that the resources affected by the opeartions\n+are now limited to the subset assigned to the pinned scope.\n+\n+Scoping will be a large and complex feature to implement, as it will meaningfully change most access-control\n+checks and APIs in teleport. In order to make this transition more manageable, we will be gradually implementing scoped\n+features over time, with the initial scopes MVP only providing very basic scoped joining, role management, and ssh access.\n+The intent will be that users will be able to start adopting a mixed use style as scope features become sufficiently robust\n+to start addressing their specific usecases.\n+\n+\n+### Scoping Semantics\n+\n+A scope is a simple path-like string (e.g. `/staging/west`). A resource will be said to be \"scoped\" if it has a `scope` attribute\n+that obeys this format. Likewise, a permission will be scoped if it is granted by a configuration resource with a\n+`scope` attribute.\n+\n+Scopes are heirarchical. The scope `/staging` is a \"parent\" of `/staging/west`, and `/staging/west` is a \"child\" of `/staging`.\n+Permissions granted at a parent scope apply to all child scopes.\n+\n+Scope heirarchy is segment based, not prefix based. I.e. `/staging` is a parent of `/staging/west`, but not of `/stagingwest`.\n+\n+Scopes are attributes, not objects. There is no need to create a scope object before creating a resource with a given\n+scope attribute. I.e. if no resource exists with scope `/staging/west`, a node can still be created with that scope without\n+first performing any kind of scope setup/creation/configuration ceremony.\n+\n+\n+### Core Design Goals\n+\n+**Heirarchical Isolation**: Permissions within a given scope cannot be used to \"reach up\" and affect resources/access defined\n+in parent scopes or \"reach across\" and affect resources/access defined in orthogonal scopes.\n+\n+**Blast Radius Reduction**: Scoping will be a robust tool for further reducing the blast radius of compromised or misused\n+credentials.\n+\n+**Delegated/Limited Administration**: Scoping will unlock the ability to create \"scoped admins\" with powerful control over\n+resources and permissions within their scopes, without being able to affect resources/permissions outside of their scope.\n+\n+**Minimal Effect on User Experience**: After selecting the scope to login to, the user experience for normal (non-administrative)\n+tasks will be unchanged.\n+\n+**Backwards Compatibility**: Scoping will not change the function or meaning of existing teleport resources/permissions.\n+\n+**Gradual Rollout and Adoption**: Scoping features will be rolled out gradually and mixed use of scoped and unscoped\n+resources/permissions will be supported.\n+\n+\n+### Comparison to Other Systems\n+\n+Scoping has a lot in common with systems like the AWS organization/account model, GCP's folder/project model, and Azure's\n+own RBAC scopes. Scopes differ in a few key ways:\n+\n+- Scopes are arbitrarily heirarchical. Resources can be assigned at any level of the heirarchy. Other similar systems tend\n+  to have a heirarchy where all resources live in the leaf nodes, and often the depth of the heirarchy is fixed.\n+\n+- Scopes are an attribute, not an object. There is no additional creation step like there would be with a system that organizes\n+  resources into accounts/projects/etc. Other systems tend to require the heirarchy be defined separately as a standalone entity\n+  prior to resources being created.\n+\n+- Credentials can be pinned to arbitrarily narrow scopes. A user can opt to access a resource in `/staging/west` by logging\n+  into `/staging` *or* logging into `/staging/west`. This gives granular control over the blast radius of credentials, and\n+  ensures that heirarchies and practices can evolve and refine over time without needing to be fully rebuilt. By virtue of\n+  the fixed leaf nodes of other systems, options for constraint of credentials via the organizing heirarchy tend to be more\n+  limited.\n+\n+\n+## Details\n+\n+### Scoping of Resources\n+\n+Within teleport's existing resource format, `scope` will be a new top-level field of type `string`.  Ex:\n+\n+```yaml\n+kind: example_resource\n+metadata:\n+  name: example\n+scope: /staging/west # this resource is \"scoped\" to /staging/west",
        "comment_created_at": "2025-09-02T14:53:56+00:00",
        "comment_author": "zmb3",
        "comment_body": "Did we consider putting scope in the metadata?",
        "pr_file_module": null
      },
      {
        "comment_id": "2316602600",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 58518,
        "pr_file": "rfd/XXXX-scopes.md",
        "discussion_id": "2316346408",
        "commented_code": "@@ -0,0 +1,438 @@\n+---\n+authors: Forrest Marshall (forrest@goteleport.com)\n+state: draft\n+---\n+\n+# RFD XXXX - Scopes\n+\n+## What\n+\n+A system for heirarchical organization and isolation for resources and permissions.\n+\n+\n+## Why\n+\n+Historically, teleport's resource and permission organization systems have been very \"flat\". In particular,\n+administrative and resource-provisioning permissions tend to be \"all or nothing\". Resource labels and selectors\n+allow organization of resources/permissions, but said organization can only be performed by what are effectively\n+global admins. Any user with the ability to create node join tokens can join any node to the cluster\n+with any labels they like. Similarly, a user with role creation permissions can create a role that grants access\n+to anything. This poses a significant challenge when trying to delegate any meaningful responsibility and/or to\n+apply the prinicial of least privilege to users that need to administer resources/permissions in an meaningful way.\n+\n+Similarly, teleport user credentials tend to be all or nothing. There isn't a good way to get credentials that\n+are only useable for the specific task at hand. Instead, if a user is logged into a teleport cluster they always\n+have all their available permissions applied. This increases blast radius, both in terms of compromise and accidental\n+misuse.\n+\n+We would like to provide a mechanism for organizing resources and permissions in a manner that allows for both\n+isolation and heirarchy. This system should support admins that have powerful control over provisioning of, and\n+access to, resources within their domain of influence. Said admin privileges must not be able to affect resources\n+and permissions outside the scope of their domain(s). We would also like to provide means of limiting the blast\n+radius of compromise/misuse as part of this organizational system. Finally, we require that this organizational\n+system be backwards compatible with existing teleport resources, permissions, and usage patterns to the greatest\n+extent possible.\n+\n+\n+## Intro\n+\n+### Overview\n+\n+We will introduce the concept of a \"scope\" as a new means of organizing resources and permissions. The scope of\n+a resource or permission will be a simple attribute formatted as a path-like string (e.g. `/staging/west`, `/prod/east`,\n+etc).\n+\n+Permissions that are scoped will apply only to resources of the same scope, or a descendant scope. For example, having\n+permission to ssh into nodes assigned at scope `/staging` will permit ssh access for nodes that have a scope of `/staging`\n+or `/staging/west`, but not `/prod` or `/prod/west`.\n+\n+Scoping will apply to administrative privileges as well. A user with the the permission to join teleport nodes assigned\n+at `/staging` will *only* be able to join nodes with that scope or a descendant scope. Same goes for role creation/assignment,\n+with users effectively being able to be granted admin-like powers \"within\" a scope but not outside of it.\n+\n+In order to improve useability and reduce blast radius of compromise/misuse, we will also introduce the concept of scope\n+\"pinning\". Rather than logging into the teleport cluster as a whole, users will be able to login to a specific scope. This\n+will result in the credentials granted to the user *only* being usable for the target scope and its descendants. For example,\n+if a user has permissions at `/prod` and `/staging`, and logs in to `/staging`, they will only be able to see and interact\n+with `/staging` scoped resources.\n+\n+We will be targeting a basic user experience that looks something like this:\n+\n+```shell\n+$ tsh login --scope=/staging/east\n+# authentication flow\n+\n+$ tsh ls\n+Node Name      Address Labels\n+-------------- ------- ------\n+some-node-east ...     ...\n+\n+$ tsh ssh some-node-east\n+# success\n+\n+$ tsh login --scope=/staging/west\n+# authentication flow\n+\n+$ tsh ls\n+Node Name      Address Labels\n+-------------- ------- ------\n+some-node-west ...     ...\n+\n+$ tsh ssh some-node-east\n+ERROR: access denied\n+\n+$ tsh ssh some-node-west\n+# success\n+```\n+\n+Note that the nature of commands after login are unchanged. Ordinary (non-admin) users should be able to ignore the concept\n+of scoping once they have logged in to the appropriate scope. Scripts that work today with an already logged-in `tsh` should\n+continue to function as expected with scoped credentials, with the only change being that the resources affected by the opeartions\n+are now limited to the subset assigned to the pinned scope.\n+\n+Scoping will be a large and complex feature to implement, as it will meaningfully change most access-control\n+checks and APIs in teleport. In order to make this transition more manageable, we will be gradually implementing scoped\n+features over time, with the initial scopes MVP only providing very basic scoped joining, role management, and ssh access.\n+The intent will be that users will be able to start adopting a mixed use style as scope features become sufficiently robust\n+to start addressing their specific usecases.\n+\n+\n+### Scoping Semantics\n+\n+A scope is a simple path-like string (e.g. `/staging/west`). A resource will be said to be \"scoped\" if it has a `scope` attribute\n+that obeys this format. Likewise, a permission will be scoped if it is granted by a configuration resource with a\n+`scope` attribute.\n+\n+Scopes are heirarchical. The scope `/staging` is a \"parent\" of `/staging/west`, and `/staging/west` is a \"child\" of `/staging`.\n+Permissions granted at a parent scope apply to all child scopes.\n+\n+Scope heirarchy is segment based, not prefix based. I.e. `/staging` is a parent of `/staging/west`, but not of `/stagingwest`.\n+\n+Scopes are attributes, not objects. There is no need to create a scope object before creating a resource with a given\n+scope attribute. I.e. if no resource exists with scope `/staging/west`, a node can still be created with that scope without\n+first performing any kind of scope setup/creation/configuration ceremony.\n+\n+\n+### Core Design Goals\n+\n+**Heirarchical Isolation**: Permissions within a given scope cannot be used to \"reach up\" and affect resources/access defined\n+in parent scopes or \"reach across\" and affect resources/access defined in orthogonal scopes.\n+\n+**Blast Radius Reduction**: Scoping will be a robust tool for further reducing the blast radius of compromised or misused\n+credentials.\n+\n+**Delegated/Limited Administration**: Scoping will unlock the ability to create \"scoped admins\" with powerful control over\n+resources and permissions within their scopes, without being able to affect resources/permissions outside of their scope.\n+\n+**Minimal Effect on User Experience**: After selecting the scope to login to, the user experience for normal (non-administrative)\n+tasks will be unchanged.\n+\n+**Backwards Compatibility**: Scoping will not change the function or meaning of existing teleport resources/permissions.\n+\n+**Gradual Rollout and Adoption**: Scoping features will be rolled out gradually and mixed use of scoped and unscoped\n+resources/permissions will be supported.\n+\n+\n+### Comparison to Other Systems\n+\n+Scoping has a lot in common with systems like the AWS organization/account model, GCP's folder/project model, and Azure's\n+own RBAC scopes. Scopes differ in a few key ways:\n+\n+- Scopes are arbitrarily heirarchical. Resources can be assigned at any level of the heirarchy. Other similar systems tend\n+  to have a heirarchy where all resources live in the leaf nodes, and often the depth of the heirarchy is fixed.\n+\n+- Scopes are an attribute, not an object. There is no additional creation step like there would be with a system that organizes\n+  resources into accounts/projects/etc. Other systems tend to require the heirarchy be defined separately as a standalone entity\n+  prior to resources being created.\n+\n+- Credentials can be pinned to arbitrarily narrow scopes. A user can opt to access a resource in `/staging/west` by logging\n+  into `/staging` *or* logging into `/staging/west`. This gives granular control over the blast radius of credentials, and\n+  ensures that heirarchies and practices can evolve and refine over time without needing to be fully rebuilt. By virtue of\n+  the fixed leaf nodes of other systems, options for constraint of credentials via the organizing heirarchy tend to be more\n+  limited.\n+\n+\n+## Details\n+\n+### Scoping of Resources\n+\n+Within teleport's existing resource format, `scope` will be a new top-level field of type `string`.  Ex:\n+\n+```yaml\n+kind: example_resource\n+metadata:\n+  name: example\n+scope: /staging/west # this resource is \"scoped\" to /staging/west",
        "comment_created_at": "2025-09-02T16:27:28+00:00",
        "comment_author": "fspmarshall",
        "comment_body": "Yes, ended up opting against. This was mostly due to metadata being a commonly defined shared type (i.e. adding `scope` to metadata would have added a scope field to all present and future resources that used the type).  I ended up deciding I wanted the scope field to obey the following criteria:\r\n\r\n- Addition should be able to be done on a case-by-case basis without worry about side-effects/edge cases.\r\n- Position of scope within the resource should be prominent s.t. it is visually distinct and easy to find at a glance across resource types.\r\n- The position of the scope within the resource should be s.t. its GRPC getters are easy to abstract over with interfaces (i.e. no burying it in a type that is different for each individual resource message), and the method for getting a resource scope should look the same across resources to enhance readability of scoped code.\r\n\r\nThis led pretty naturally to one of two options.  Either a top-level `scope` string field, or a new top-level message like `Metadata` but for scoped stuff.  The latter felt more flexible/future-proof but was too noisy and I decided that it was better to commit to just the conventional top-level string and have any other scope stuff be tackled case-by-case.  Hence why scoped roles will have `assignable_scopes` in `spec`, and scoped join tokens will have `assign_to_scope` (or whatever we end up calling it) in `spec` as well.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1487076407",
    "pr_number": 38078,
    "pr_file": "rfd/0164-scoped-rbac.md",
    "created_at": "2024-02-13T02:34:20+00:00",
    "commented_code": "+\n+---\n+authors: @klizhentas (sasha@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0163 - Scoped RBAC\n+\n+## Required Approvers\n+\n+* Engineering @r0mant && (@tigrato || @marcoandredinis)\n+* Security: (@reedloden || @jentfoo)\n+* Product: (@xinding33 || @klizhentas )\n+\n+## What\n+\n+This RFD introduces resource hierarchies and scopes to existing RBAC. \n+Our goal is simplify and evolve access control in Teleport without drastic changes or new policy languages. \n+Make it easier to integrate Teleport RBAC with cloud IAMs of AWS, GCP and Azure.\n+This RFD is closely modeled and inspired by Azure RBAC model, the most advanced out of 3 clouds. \n+\n+Read about it here https://learn.microsoft.com/en-us/azure/role-based-access-control/overview before diving into this RFD.\n+\n+## Why\n+\n+There are several structural issues with the current RBAC model in Teleport.\n+\n+### Scalability issues \n+\n+Every role has to be distributed to every node or proxy that has to perform authorization. \n+Current roles are brittle - to evaluate access, every single role has to be fetched and processed, \n+because each role can have a deny rule that can block access to any allowed rule.\n+\n+### Scoping issues\n+\n+It is not possible to describe \u201cdelegated admins\u201d in RBAC, when one user has administrative access over part of the cluster.\n+It is also not possible to specify that certain role options only apply in certain use-cases. \n+\n+For example, the setting `permit-agent-forward: false` will deny agent-forward to any matching resource with no exceptions, even if other roles allow it.\n+\n+It is not possible to allow admins to grant roles to other users but with certain restrictions, as in the example of issue https://github.com/gravitational/teleport/issues/16914. \n+\n+### Complexity \n+\n+Roles today have both labels and label expressions, login rules to inject traits and claims and templates. \n+This creates a complicated system that is hard to understand and reason about.\n+Role mapping is brittle and requires updating OIDC/SAML connector resources, which can break access.\n+\n+### Security issues\n+\n+Every role assignment and trait is encoded in certificate, and each time a user gets their roles updated, they have to get a new certificate. \n+Old certificates can be re-used to get privileges that have been removed, \n+creating \u201ca new enemy problem\u201d described in [Zanzibar Paper](https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/).\n+\n+Many roles allow \u201crole escapes\u201d, as any person who gets assigned a role that can create other roles, \n+would become an admin, see for example issue https://github.com/gravitational/teleport.e/issues/3111\n+\n+### Goals\n+\n+Our key goal is to evolve Teleport\u2019s roles without asking users to rewrite their existing RBAC.\n+We also would like to better integrate Teleport RBAC with cloud provider\u2019s IAM systems out of the box.\n+We would like to give Teleport\u2019s users \u201cbatteries included\u201d approach, when they can get 90% of the use-cases done without c\n+reating any new roles, or modifying existing ones.\n+\n+### Non-Goals\n+\n+We are not going to implement backwards-incompatible changes that require our customers rewrite their RBAC stack or adopt a completely new policy language. \n+\n+## Details\n+To understand the required changes, let\u2019s first take a look at Teleport RBAC structure. \n+\n+### RBAC Primer\n+\n+Let\u2019s start with fundamental Teleport RBAC concepts and highlight some issues as we review them.\n+\n+#### Roles\n+\n+Each user or bot in Teleport is assigned one or several roles. \n+For the full reference, take a look at the documentation at https://goteleport.com/docs/reference/resources/#role.\n+\n+Here is a role structure:\n+\n+```yaml\n+kind: role\n+version: v7\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  # options are a set of knobs that apply across the entire role set.\n+  # the most restrictive options wins\n+  options:\n+    # max_session_ttl defines the TTL (time to live) of certificates\n+    # issued to the users with this role.\n+    max_session_ttl: 8h\n+    # forward_agent controls whether SSH agent forwarding is allowed\n+    forward_agent: true\n+  # The allow section declares a list of resource/verb combinations that are\n+  # allowed for the users of this role. By default, nothing is allowed.\n+  #\n+  # Allow rules specify both actions that are allowed and match the resources\n+  # they are applying to.\n+  allow:\n+    # Some Allow fields specify target protocol\n+    #  login principals, like in this example, SSH logins\n+    logins: [root, '{{internal.logins}}']\n+\n+    # In this example, the fields specify a set of windows desktop logins\n+    windows_desktop_logins: [Administrator, '{{internal.logins}}']\n+\n+    # There are multiple types of labels and label expressions that \n+    # match different computing resources.\n+    node_labels:\n+      # literal strings:\n+      'env': 'test'\n+    # regexp expressions\n+      'reg': '^us-west-1|eu-central-1$'\n+\n+    # node_labels_expression has the same purpose as node_labels but\n+    # supports predicate expressions to configure custom logic.\n+    # A user with this role will be allowed to access nodes if they are in the\n+    # staging environment *or* if they belong to one of the user's own teams.\n+    node_labels_expression: |\n+      labels[\"env\"] == \"staging\" ||\n+      contains(user.spec.traits[\"teams\"] , labels[\"team\"])\n+ \n+    # rules allow a user holding this role to modify other resources\n+    # matching the expressions below.\n+    # rules match both resources and specify what actions are allowed\n+    rules:\n+      - resources: [role]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+We can spot several issues with this role design:\n+\n+* There is no way to specify the scope that role applies to, all roles apply globally to all resources they match and all users they assign to all the time.\n+* There is no way to specify resource hierarchies, like computing environments (env vs prod), which makes it hard to partition the infrastructure and forces customers to specify labels.\n+\n+\n+#### Labels\n+\n+Teleport RBAC\u2019s advice to engineers to partition their resource is to first, label their computing resource or use AWS labels, and second, match the labels in RBAC.\n+\n+Admins can set the tags on the resource configuration file statically, or, for some resources, use `server_info` to set the tags for each resource:\n+\n+```yaml\n+# server_info.yaml\n+kind: server_info\n+metadata:\n+   name: si-<node-name>\n+spec:\n+   new_labels:\n+      \"foo\": \"bar\"\n+```\n+\n+This creates several issues:\n+\n+* It is not always secure to delegate labeling to owners of computing resources, as anyone with root access to the node config file can update its labels impacting everyone else.\n+* It is not scalable, as it\u2019s not always useful to set and updates tags for each individual resource\n+* It\u2019s hard or impossible to partition infrastructure with two-dimensional labels, although users can use `env: prod` to mark all resources in the production environment, there is no way to say that `env: lab` is a subset of `env: prod`.\n+\n+#### Current Roles mapping\n+\n+In Teleport there are multiple ways to map roles to users: static roles mapping to local users and bots, \n+dynamic mapping to SSO via connectors and on-demand assignment via access lists and access requests.\n+\n+Most users start with local static mapping and SSO mapping, later graduating to access requests and access lists.\n+\n+https://goteleport.com/docs/access-controls/sso/#creating-an-authentication-connector\n+\n+For SSO users, on login, Teleport checks SSO connector, and maps attributes of a user to a list of roles:\n+\n+```yaml\n+# connector.yaml\n+kind: saml\n+version: v2\n+metadata:\n+  name: corporate\n+spec:\n+  attributes_to_roles:\n+    - {name: \"groups\", value: \"okta-admin\", roles: [\"access\"]}\n+     # regular expressions with capture are also supported. \n+     # the next line instructs Teleport\n+     # to assign users to roles `admin-1` if his SAML \"group\" \n+     # attribute equals 'ssh_admin_1':\n+     - { name: \"group\", value: \"^ssh_admin_(.*)$\", roles: [\"admin-$1\"] }\n+```\n+\n+This creates several security and scalability issues:\n+\n+* Every time administrators need to change roles assignment, they have to update the resource.\n+* Every time the Identity Provider changes user\u2019s attributes, the users would have to relogin to get new roles. \n+\n+For local users, administrators have to specify roles and traits in the local resource:\n+\n+```yaml\n+kind: user\n+version: v2\n+metadata:\n+  name: joe\n+spec:\n+  # roles is a list of roles assigned to this user\n+  roles:\n+  - admin\n+  # traits are key, list of values pairs assigned to a user resource.\n+  # Traits can be used in role templates as variables.\n+  traits:\n+    logins:\n+    - joe\n+    - root\n+```\n+\n+This also creates some challenges, as administrators are forced to update local resources each time they have to assign a user new permission.\n+\n+### Modifications\n+\n+Let\u2019s now introduce the missing pieces of the puzzle and review how new roles will simplify or deprecate legacy concepts.\n+\n+#### Hierarchical Resource Groups\n+\n+Resource groups are one such missing piece - in most cloud environments, resources are split into hierarchy, \n+for example, host `luna` is a member of a resource group `lab`, in turn a member of environment `prod`, which is in turn a member of a Teleport cluster.\n+\n+In Teleport, we will make cluster a default root of this hierarchy. Every computing resource by default will be a direct member of a cluster resource group.\n+\n+We will also restrict a resource to be a member of only one resource group at a time.  \n+It\u2019s tempting to have a resource to be assigned to multiple resource groups, but we push this out of the scope of this RFD for simplicity.\n+\n+In our example, the cluster administrator would define lab resource group in the following way:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent: prod_env\n+```\n+\n+Administrators can assign resources to resource groups:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+   parent: prod_env\n+ match_kinds:\n+   - node\n+   - database\n+   - role\n+   - access_list\n+   - '.*'\n+ match_labels:  \n+   env: prod\n+```\n+\n+In this case, any resource that matches `env:prod` label will be assigned to this resource group. \n+\n+This will let administrators to gradually migrate their existing flat infrastructure to resource groups one.\n+\n+Such centralized assignment will also ensure that one resource can be only assigned to one resource group at a time.",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "1487076407",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 38078,
        "pr_file": "rfd/0164-scoped-rbac.md",
        "discussion_id": "1487076407",
        "commented_code": "@@ -0,0 +1,956 @@\n+\n+---\n+authors: @klizhentas (sasha@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0163 - Scoped RBAC\n+\n+## Required Approvers\n+\n+* Engineering @r0mant && (@tigrato || @marcoandredinis)\n+* Security: (@reedloden || @jentfoo)\n+* Product: (@xinding33 || @klizhentas )\n+\n+## What\n+\n+This RFD introduces resource hierarchies and scopes to existing RBAC. \n+Our goal is simplify and evolve access control in Teleport without drastic changes or new policy languages. \n+Make it easier to integrate Teleport RBAC with cloud IAMs of AWS, GCP and Azure.\n+This RFD is closely modeled and inspired by Azure RBAC model, the most advanced out of 3 clouds. \n+\n+Read about it here https://learn.microsoft.com/en-us/azure/role-based-access-control/overview before diving into this RFD.\n+\n+## Why\n+\n+There are several structural issues with the current RBAC model in Teleport.\n+\n+### Scalability issues \n+\n+Every role has to be distributed to every node or proxy that has to perform authorization. \n+Current roles are brittle - to evaluate access, every single role has to be fetched and processed, \n+because each role can have a deny rule that can block access to any allowed rule.\n+\n+### Scoping issues\n+\n+It is not possible to describe \u201cdelegated admins\u201d in RBAC, when one user has administrative access over part of the cluster.\n+It is also not possible to specify that certain role options only apply in certain use-cases. \n+\n+For example, the setting `permit-agent-forward: false` will deny agent-forward to any matching resource with no exceptions, even if other roles allow it.\n+\n+It is not possible to allow admins to grant roles to other users but with certain restrictions, as in the example of issue https://github.com/gravitational/teleport/issues/16914. \n+\n+### Complexity \n+\n+Roles today have both labels and label expressions, login rules to inject traits and claims and templates. \n+This creates a complicated system that is hard to understand and reason about.\n+Role mapping is brittle and requires updating OIDC/SAML connector resources, which can break access.\n+\n+### Security issues\n+\n+Every role assignment and trait is encoded in certificate, and each time a user gets their roles updated, they have to get a new certificate. \n+Old certificates can be re-used to get privileges that have been removed, \n+creating \u201ca new enemy problem\u201d described in [Zanzibar Paper](https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/).\n+\n+Many roles allow \u201crole escapes\u201d, as any person who gets assigned a role that can create other roles, \n+would become an admin, see for example issue https://github.com/gravitational/teleport.e/issues/3111\n+\n+### Goals\n+\n+Our key goal is to evolve Teleport\u2019s roles without asking users to rewrite their existing RBAC.\n+We also would like to better integrate Teleport RBAC with cloud provider\u2019s IAM systems out of the box.\n+We would like to give Teleport\u2019s users \u201cbatteries included\u201d approach, when they can get 90% of the use-cases done without c\n+reating any new roles, or modifying existing ones.\n+\n+### Non-Goals\n+\n+We are not going to implement backwards-incompatible changes that require our customers rewrite their RBAC stack or adopt a completely new policy language. \n+\n+## Details\n+To understand the required changes, let\u2019s first take a look at Teleport RBAC structure. \n+\n+### RBAC Primer\n+\n+Let\u2019s start with fundamental Teleport RBAC concepts and highlight some issues as we review them.\n+\n+#### Roles\n+\n+Each user or bot in Teleport is assigned one or several roles. \n+For the full reference, take a look at the documentation at https://goteleport.com/docs/reference/resources/#role.\n+\n+Here is a role structure:\n+\n+```yaml\n+kind: role\n+version: v7\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  # options are a set of knobs that apply across the entire role set.\n+  # the most restrictive options wins\n+  options:\n+    # max_session_ttl defines the TTL (time to live) of certificates\n+    # issued to the users with this role.\n+    max_session_ttl: 8h\n+    # forward_agent controls whether SSH agent forwarding is allowed\n+    forward_agent: true\n+  # The allow section declares a list of resource/verb combinations that are\n+  # allowed for the users of this role. By default, nothing is allowed.\n+  #\n+  # Allow rules specify both actions that are allowed and match the resources\n+  # they are applying to.\n+  allow:\n+    # Some Allow fields specify target protocol\n+    #  login principals, like in this example, SSH logins\n+    logins: [root, '{{internal.logins}}']\n+\n+    # In this example, the fields specify a set of windows desktop logins\n+    windows_desktop_logins: [Administrator, '{{internal.logins}}']\n+\n+    # There are multiple types of labels and label expressions that \n+    # match different computing resources.\n+    node_labels:\n+      # literal strings:\n+      'env': 'test'\n+    # regexp expressions\n+      'reg': '^us-west-1|eu-central-1$'\n+\n+    # node_labels_expression has the same purpose as node_labels but\n+    # supports predicate expressions to configure custom logic.\n+    # A user with this role will be allowed to access nodes if they are in the\n+    # staging environment *or* if they belong to one of the user's own teams.\n+    node_labels_expression: |\n+      labels[\"env\"] == \"staging\" ||\n+      contains(user.spec.traits[\"teams\"] , labels[\"team\"])\n+ \n+    # rules allow a user holding this role to modify other resources\n+    # matching the expressions below.\n+    # rules match both resources and specify what actions are allowed\n+    rules:\n+      - resources: [role]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+We can spot several issues with this role design:\n+\n+* There is no way to specify the scope that role applies to, all roles apply globally to all resources they match and all users they assign to all the time.\n+* There is no way to specify resource hierarchies, like computing environments (env vs prod), which makes it hard to partition the infrastructure and forces customers to specify labels.\n+\n+\n+#### Labels\n+\n+Teleport RBAC\u2019s advice to engineers to partition their resource is to first, label their computing resource or use AWS labels, and second, match the labels in RBAC.\n+\n+Admins can set the tags on the resource configuration file statically, or, for some resources, use `server_info` to set the tags for each resource:\n+\n+```yaml\n+# server_info.yaml\n+kind: server_info\n+metadata:\n+   name: si-<node-name>\n+spec:\n+   new_labels:\n+      \"foo\": \"bar\"\n+```\n+\n+This creates several issues:\n+\n+* It is not always secure to delegate labeling to owners of computing resources, as anyone with root access to the node config file can update its labels impacting everyone else.\n+* It is not scalable, as it\u2019s not always useful to set and updates tags for each individual resource\n+* It\u2019s hard or impossible to partition infrastructure with two-dimensional labels, although users can use `env: prod` to mark all resources in the production environment, there is no way to say that `env: lab` is a subset of `env: prod`.\n+\n+#### Current Roles mapping\n+\n+In Teleport there are multiple ways to map roles to users: static roles mapping to local users and bots, \n+dynamic mapping to SSO via connectors and on-demand assignment via access lists and access requests.\n+\n+Most users start with local static mapping and SSO mapping, later graduating to access requests and access lists.\n+\n+https://goteleport.com/docs/access-controls/sso/#creating-an-authentication-connector\n+\n+For SSO users, on login, Teleport checks SSO connector, and maps attributes of a user to a list of roles:\n+\n+```yaml\n+# connector.yaml\n+kind: saml\n+version: v2\n+metadata:\n+  name: corporate\n+spec:\n+  attributes_to_roles:\n+    - {name: \"groups\", value: \"okta-admin\", roles: [\"access\"]}\n+     # regular expressions with capture are also supported. \n+     # the next line instructs Teleport\n+     # to assign users to roles `admin-1` if his SAML \"group\" \n+     # attribute equals 'ssh_admin_1':\n+     - { name: \"group\", value: \"^ssh_admin_(.*)$\", roles: [\"admin-$1\"] }\n+```\n+\n+This creates several security and scalability issues:\n+\n+* Every time administrators need to change roles assignment, they have to update the resource.\n+* Every time the Identity Provider changes user\u2019s attributes, the users would have to relogin to get new roles. \n+\n+For local users, administrators have to specify roles and traits in the local resource:\n+\n+```yaml\n+kind: user\n+version: v2\n+metadata:\n+  name: joe\n+spec:\n+  # roles is a list of roles assigned to this user\n+  roles:\n+  - admin\n+  # traits are key, list of values pairs assigned to a user resource.\n+  # Traits can be used in role templates as variables.\n+  traits:\n+    logins:\n+    - joe\n+    - root\n+```\n+\n+This also creates some challenges, as administrators are forced to update local resources each time they have to assign a user new permission.\n+\n+### Modifications\n+\n+Let\u2019s now introduce the missing pieces of the puzzle and review how new roles will simplify or deprecate legacy concepts.\n+\n+#### Hierarchical Resource Groups\n+\n+Resource groups are one such missing piece - in most cloud environments, resources are split into hierarchy, \n+for example, host `luna` is a member of a resource group `lab`, in turn a member of environment `prod`, which is in turn a member of a Teleport cluster.\n+\n+In Teleport, we will make cluster a default root of this hierarchy. Every computing resource by default will be a direct member of a cluster resource group.\n+\n+We will also restrict a resource to be a member of only one resource group at a time.  \n+It\u2019s tempting to have a resource to be assigned to multiple resource groups, but we push this out of the scope of this RFD for simplicity.\n+\n+In our example, the cluster administrator would define lab resource group in the following way:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent: prod_env\n+```\n+\n+Administrators can assign resources to resource groups:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+   parent: prod_env\n+ match_kinds:\n+   - node\n+   - database\n+   - role\n+   - access_list\n+   - '.*'\n+ match_labels:  \n+   env: prod\n+```\n+\n+In this case, any resource that matches `env:prod` label will be assigned to this resource group. \n+\n+This will let administrators to gradually migrate their existing flat infrastructure to resource groups one.\n+\n+Such centralized assignment will also ensure that one resource can be only assigned to one resource group at a time.",
        "comment_created_at": "2024-02-13T02:34:20+00:00",
        "comment_author": "nklaassen",
        "comment_body": "How would we make sure the resource is only in one group? Say I have one group with `match_labels: env: prod` and another with `match_labels: region: west2`. There may be no resource matching both when the resource_group is created, but it could be created later",
        "pr_file_module": null
      },
      {
        "comment_id": "1488749641",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 38078,
        "pr_file": "rfd/0164-scoped-rbac.md",
        "discussion_id": "1487076407",
        "commented_code": "@@ -0,0 +1,956 @@\n+\n+---\n+authors: @klizhentas (sasha@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0163 - Scoped RBAC\n+\n+## Required Approvers\n+\n+* Engineering @r0mant && (@tigrato || @marcoandredinis)\n+* Security: (@reedloden || @jentfoo)\n+* Product: (@xinding33 || @klizhentas )\n+\n+## What\n+\n+This RFD introduces resource hierarchies and scopes to existing RBAC. \n+Our goal is simplify and evolve access control in Teleport without drastic changes or new policy languages. \n+Make it easier to integrate Teleport RBAC with cloud IAMs of AWS, GCP and Azure.\n+This RFD is closely modeled and inspired by Azure RBAC model, the most advanced out of 3 clouds. \n+\n+Read about it here https://learn.microsoft.com/en-us/azure/role-based-access-control/overview before diving into this RFD.\n+\n+## Why\n+\n+There are several structural issues with the current RBAC model in Teleport.\n+\n+### Scalability issues \n+\n+Every role has to be distributed to every node or proxy that has to perform authorization. \n+Current roles are brittle - to evaluate access, every single role has to be fetched and processed, \n+because each role can have a deny rule that can block access to any allowed rule.\n+\n+### Scoping issues\n+\n+It is not possible to describe \u201cdelegated admins\u201d in RBAC, when one user has administrative access over part of the cluster.\n+It is also not possible to specify that certain role options only apply in certain use-cases. \n+\n+For example, the setting `permit-agent-forward: false` will deny agent-forward to any matching resource with no exceptions, even if other roles allow it.\n+\n+It is not possible to allow admins to grant roles to other users but with certain restrictions, as in the example of issue https://github.com/gravitational/teleport/issues/16914. \n+\n+### Complexity \n+\n+Roles today have both labels and label expressions, login rules to inject traits and claims and templates. \n+This creates a complicated system that is hard to understand and reason about.\n+Role mapping is brittle and requires updating OIDC/SAML connector resources, which can break access.\n+\n+### Security issues\n+\n+Every role assignment and trait is encoded in certificate, and each time a user gets their roles updated, they have to get a new certificate. \n+Old certificates can be re-used to get privileges that have been removed, \n+creating \u201ca new enemy problem\u201d described in [Zanzibar Paper](https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/).\n+\n+Many roles allow \u201crole escapes\u201d, as any person who gets assigned a role that can create other roles, \n+would become an admin, see for example issue https://github.com/gravitational/teleport.e/issues/3111\n+\n+### Goals\n+\n+Our key goal is to evolve Teleport\u2019s roles without asking users to rewrite their existing RBAC.\n+We also would like to better integrate Teleport RBAC with cloud provider\u2019s IAM systems out of the box.\n+We would like to give Teleport\u2019s users \u201cbatteries included\u201d approach, when they can get 90% of the use-cases done without c\n+reating any new roles, or modifying existing ones.\n+\n+### Non-Goals\n+\n+We are not going to implement backwards-incompatible changes that require our customers rewrite their RBAC stack or adopt a completely new policy language. \n+\n+## Details\n+To understand the required changes, let\u2019s first take a look at Teleport RBAC structure. \n+\n+### RBAC Primer\n+\n+Let\u2019s start with fundamental Teleport RBAC concepts and highlight some issues as we review them.\n+\n+#### Roles\n+\n+Each user or bot in Teleport is assigned one or several roles. \n+For the full reference, take a look at the documentation at https://goteleport.com/docs/reference/resources/#role.\n+\n+Here is a role structure:\n+\n+```yaml\n+kind: role\n+version: v7\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  # options are a set of knobs that apply across the entire role set.\n+  # the most restrictive options wins\n+  options:\n+    # max_session_ttl defines the TTL (time to live) of certificates\n+    # issued to the users with this role.\n+    max_session_ttl: 8h\n+    # forward_agent controls whether SSH agent forwarding is allowed\n+    forward_agent: true\n+  # The allow section declares a list of resource/verb combinations that are\n+  # allowed for the users of this role. By default, nothing is allowed.\n+  #\n+  # Allow rules specify both actions that are allowed and match the resources\n+  # they are applying to.\n+  allow:\n+    # Some Allow fields specify target protocol\n+    #  login principals, like in this example, SSH logins\n+    logins: [root, '{{internal.logins}}']\n+\n+    # In this example, the fields specify a set of windows desktop logins\n+    windows_desktop_logins: [Administrator, '{{internal.logins}}']\n+\n+    # There are multiple types of labels and label expressions that \n+    # match different computing resources.\n+    node_labels:\n+      # literal strings:\n+      'env': 'test'\n+    # regexp expressions\n+      'reg': '^us-west-1|eu-central-1$'\n+\n+    # node_labels_expression has the same purpose as node_labels but\n+    # supports predicate expressions to configure custom logic.\n+    # A user with this role will be allowed to access nodes if they are in the\n+    # staging environment *or* if they belong to one of the user's own teams.\n+    node_labels_expression: |\n+      labels[\"env\"] == \"staging\" ||\n+      contains(user.spec.traits[\"teams\"] , labels[\"team\"])\n+ \n+    # rules allow a user holding this role to modify other resources\n+    # matching the expressions below.\n+    # rules match both resources and specify what actions are allowed\n+    rules:\n+      - resources: [role]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+We can spot several issues with this role design:\n+\n+* There is no way to specify the scope that role applies to, all roles apply globally to all resources they match and all users they assign to all the time.\n+* There is no way to specify resource hierarchies, like computing environments (env vs prod), which makes it hard to partition the infrastructure and forces customers to specify labels.\n+\n+\n+#### Labels\n+\n+Teleport RBAC\u2019s advice to engineers to partition their resource is to first, label their computing resource or use AWS labels, and second, match the labels in RBAC.\n+\n+Admins can set the tags on the resource configuration file statically, or, for some resources, use `server_info` to set the tags for each resource:\n+\n+```yaml\n+# server_info.yaml\n+kind: server_info\n+metadata:\n+   name: si-<node-name>\n+spec:\n+   new_labels:\n+      \"foo\": \"bar\"\n+```\n+\n+This creates several issues:\n+\n+* It is not always secure to delegate labeling to owners of computing resources, as anyone with root access to the node config file can update its labels impacting everyone else.\n+* It is not scalable, as it\u2019s not always useful to set and updates tags for each individual resource\n+* It\u2019s hard or impossible to partition infrastructure with two-dimensional labels, although users can use `env: prod` to mark all resources in the production environment, there is no way to say that `env: lab` is a subset of `env: prod`.\n+\n+#### Current Roles mapping\n+\n+In Teleport there are multiple ways to map roles to users: static roles mapping to local users and bots, \n+dynamic mapping to SSO via connectors and on-demand assignment via access lists and access requests.\n+\n+Most users start with local static mapping and SSO mapping, later graduating to access requests and access lists.\n+\n+https://goteleport.com/docs/access-controls/sso/#creating-an-authentication-connector\n+\n+For SSO users, on login, Teleport checks SSO connector, and maps attributes of a user to a list of roles:\n+\n+```yaml\n+# connector.yaml\n+kind: saml\n+version: v2\n+metadata:\n+  name: corporate\n+spec:\n+  attributes_to_roles:\n+    - {name: \"groups\", value: \"okta-admin\", roles: [\"access\"]}\n+     # regular expressions with capture are also supported. \n+     # the next line instructs Teleport\n+     # to assign users to roles `admin-1` if his SAML \"group\" \n+     # attribute equals 'ssh_admin_1':\n+     - { name: \"group\", value: \"^ssh_admin_(.*)$\", roles: [\"admin-$1\"] }\n+```\n+\n+This creates several security and scalability issues:\n+\n+* Every time administrators need to change roles assignment, they have to update the resource.\n+* Every time the Identity Provider changes user\u2019s attributes, the users would have to relogin to get new roles. \n+\n+For local users, administrators have to specify roles and traits in the local resource:\n+\n+```yaml\n+kind: user\n+version: v2\n+metadata:\n+  name: joe\n+spec:\n+  # roles is a list of roles assigned to this user\n+  roles:\n+  - admin\n+  # traits are key, list of values pairs assigned to a user resource.\n+  # Traits can be used in role templates as variables.\n+  traits:\n+    logins:\n+    - joe\n+    - root\n+```\n+\n+This also creates some challenges, as administrators are forced to update local resources each time they have to assign a user new permission.\n+\n+### Modifications\n+\n+Let\u2019s now introduce the missing pieces of the puzzle and review how new roles will simplify or deprecate legacy concepts.\n+\n+#### Hierarchical Resource Groups\n+\n+Resource groups are one such missing piece - in most cloud environments, resources are split into hierarchy, \n+for example, host `luna` is a member of a resource group `lab`, in turn a member of environment `prod`, which is in turn a member of a Teleport cluster.\n+\n+In Teleport, we will make cluster a default root of this hierarchy. Every computing resource by default will be a direct member of a cluster resource group.\n+\n+We will also restrict a resource to be a member of only one resource group at a time.  \n+It\u2019s tempting to have a resource to be assigned to multiple resource groups, but we push this out of the scope of this RFD for simplicity.\n+\n+In our example, the cluster administrator would define lab resource group in the following way:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent: prod_env\n+```\n+\n+Administrators can assign resources to resource groups:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+   parent: prod_env\n+ match_kinds:\n+   - node\n+   - database\n+   - role\n+   - access_list\n+   - '.*'\n+ match_labels:  \n+   env: prod\n+```\n+\n+In this case, any resource that matches `env:prod` label will be assigned to this resource group. \n+\n+This will let administrators to gradually migrate their existing flat infrastructure to resource groups one.\n+\n+Such centralized assignment will also ensure that one resource can be only assigned to one resource group at a time.",
        "comment_created_at": "2024-02-14T00:25:32+00:00",
        "comment_author": "klizhentas",
        "comment_body": "Let me think a bit more about this,  we can relax this constraint, but I need to think through possible issues of that.",
        "pr_file_module": null
      },
      {
        "comment_id": "1489411431",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 38078,
        "pr_file": "rfd/0164-scoped-rbac.md",
        "discussion_id": "1487076407",
        "commented_code": "@@ -0,0 +1,956 @@\n+\n+---\n+authors: @klizhentas (sasha@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0163 - Scoped RBAC\n+\n+## Required Approvers\n+\n+* Engineering @r0mant && (@tigrato || @marcoandredinis)\n+* Security: (@reedloden || @jentfoo)\n+* Product: (@xinding33 || @klizhentas )\n+\n+## What\n+\n+This RFD introduces resource hierarchies and scopes to existing RBAC. \n+Our goal is simplify and evolve access control in Teleport without drastic changes or new policy languages. \n+Make it easier to integrate Teleport RBAC with cloud IAMs of AWS, GCP and Azure.\n+This RFD is closely modeled and inspired by Azure RBAC model, the most advanced out of 3 clouds. \n+\n+Read about it here https://learn.microsoft.com/en-us/azure/role-based-access-control/overview before diving into this RFD.\n+\n+## Why\n+\n+There are several structural issues with the current RBAC model in Teleport.\n+\n+### Scalability issues \n+\n+Every role has to be distributed to every node or proxy that has to perform authorization. \n+Current roles are brittle - to evaluate access, every single role has to be fetched and processed, \n+because each role can have a deny rule that can block access to any allowed rule.\n+\n+### Scoping issues\n+\n+It is not possible to describe \u201cdelegated admins\u201d in RBAC, when one user has administrative access over part of the cluster.\n+It is also not possible to specify that certain role options only apply in certain use-cases. \n+\n+For example, the setting `permit-agent-forward: false` will deny agent-forward to any matching resource with no exceptions, even if other roles allow it.\n+\n+It is not possible to allow admins to grant roles to other users but with certain restrictions, as in the example of issue https://github.com/gravitational/teleport/issues/16914. \n+\n+### Complexity \n+\n+Roles today have both labels and label expressions, login rules to inject traits and claims and templates. \n+This creates a complicated system that is hard to understand and reason about.\n+Role mapping is brittle and requires updating OIDC/SAML connector resources, which can break access.\n+\n+### Security issues\n+\n+Every role assignment and trait is encoded in certificate, and each time a user gets their roles updated, they have to get a new certificate. \n+Old certificates can be re-used to get privileges that have been removed, \n+creating \u201ca new enemy problem\u201d described in [Zanzibar Paper](https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/).\n+\n+Many roles allow \u201crole escapes\u201d, as any person who gets assigned a role that can create other roles, \n+would become an admin, see for example issue https://github.com/gravitational/teleport.e/issues/3111\n+\n+### Goals\n+\n+Our key goal is to evolve Teleport\u2019s roles without asking users to rewrite their existing RBAC.\n+We also would like to better integrate Teleport RBAC with cloud provider\u2019s IAM systems out of the box.\n+We would like to give Teleport\u2019s users \u201cbatteries included\u201d approach, when they can get 90% of the use-cases done without c\n+reating any new roles, or modifying existing ones.\n+\n+### Non-Goals\n+\n+We are not going to implement backwards-incompatible changes that require our customers rewrite their RBAC stack or adopt a completely new policy language. \n+\n+## Details\n+To understand the required changes, let\u2019s first take a look at Teleport RBAC structure. \n+\n+### RBAC Primer\n+\n+Let\u2019s start with fundamental Teleport RBAC concepts and highlight some issues as we review them.\n+\n+#### Roles\n+\n+Each user or bot in Teleport is assigned one or several roles. \n+For the full reference, take a look at the documentation at https://goteleport.com/docs/reference/resources/#role.\n+\n+Here is a role structure:\n+\n+```yaml\n+kind: role\n+version: v7\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  # options are a set of knobs that apply across the entire role set.\n+  # the most restrictive options wins\n+  options:\n+    # max_session_ttl defines the TTL (time to live) of certificates\n+    # issued to the users with this role.\n+    max_session_ttl: 8h\n+    # forward_agent controls whether SSH agent forwarding is allowed\n+    forward_agent: true\n+  # The allow section declares a list of resource/verb combinations that are\n+  # allowed for the users of this role. By default, nothing is allowed.\n+  #\n+  # Allow rules specify both actions that are allowed and match the resources\n+  # they are applying to.\n+  allow:\n+    # Some Allow fields specify target protocol\n+    #  login principals, like in this example, SSH logins\n+    logins: [root, '{{internal.logins}}']\n+\n+    # In this example, the fields specify a set of windows desktop logins\n+    windows_desktop_logins: [Administrator, '{{internal.logins}}']\n+\n+    # There are multiple types of labels and label expressions that \n+    # match different computing resources.\n+    node_labels:\n+      # literal strings:\n+      'env': 'test'\n+    # regexp expressions\n+      'reg': '^us-west-1|eu-central-1$'\n+\n+    # node_labels_expression has the same purpose as node_labels but\n+    # supports predicate expressions to configure custom logic.\n+    # A user with this role will be allowed to access nodes if they are in the\n+    # staging environment *or* if they belong to one of the user's own teams.\n+    node_labels_expression: |\n+      labels[\"env\"] == \"staging\" ||\n+      contains(user.spec.traits[\"teams\"] , labels[\"team\"])\n+ \n+    # rules allow a user holding this role to modify other resources\n+    # matching the expressions below.\n+    # rules match both resources and specify what actions are allowed\n+    rules:\n+      - resources: [role]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+We can spot several issues with this role design:\n+\n+* There is no way to specify the scope that role applies to, all roles apply globally to all resources they match and all users they assign to all the time.\n+* There is no way to specify resource hierarchies, like computing environments (env vs prod), which makes it hard to partition the infrastructure and forces customers to specify labels.\n+\n+\n+#### Labels\n+\n+Teleport RBAC\u2019s advice to engineers to partition their resource is to first, label their computing resource or use AWS labels, and second, match the labels in RBAC.\n+\n+Admins can set the tags on the resource configuration file statically, or, for some resources, use `server_info` to set the tags for each resource:\n+\n+```yaml\n+# server_info.yaml\n+kind: server_info\n+metadata:\n+   name: si-<node-name>\n+spec:\n+   new_labels:\n+      \"foo\": \"bar\"\n+```\n+\n+This creates several issues:\n+\n+* It is not always secure to delegate labeling to owners of computing resources, as anyone with root access to the node config file can update its labels impacting everyone else.\n+* It is not scalable, as it\u2019s not always useful to set and updates tags for each individual resource\n+* It\u2019s hard or impossible to partition infrastructure with two-dimensional labels, although users can use `env: prod` to mark all resources in the production environment, there is no way to say that `env: lab` is a subset of `env: prod`.\n+\n+#### Current Roles mapping\n+\n+In Teleport there are multiple ways to map roles to users: static roles mapping to local users and bots, \n+dynamic mapping to SSO via connectors and on-demand assignment via access lists and access requests.\n+\n+Most users start with local static mapping and SSO mapping, later graduating to access requests and access lists.\n+\n+https://goteleport.com/docs/access-controls/sso/#creating-an-authentication-connector\n+\n+For SSO users, on login, Teleport checks SSO connector, and maps attributes of a user to a list of roles:\n+\n+```yaml\n+# connector.yaml\n+kind: saml\n+version: v2\n+metadata:\n+  name: corporate\n+spec:\n+  attributes_to_roles:\n+    - {name: \"groups\", value: \"okta-admin\", roles: [\"access\"]}\n+     # regular expressions with capture are also supported. \n+     # the next line instructs Teleport\n+     # to assign users to roles `admin-1` if his SAML \"group\" \n+     # attribute equals 'ssh_admin_1':\n+     - { name: \"group\", value: \"^ssh_admin_(.*)$\", roles: [\"admin-$1\"] }\n+```\n+\n+This creates several security and scalability issues:\n+\n+* Every time administrators need to change roles assignment, they have to update the resource.\n+* Every time the Identity Provider changes user\u2019s attributes, the users would have to relogin to get new roles. \n+\n+For local users, administrators have to specify roles and traits in the local resource:\n+\n+```yaml\n+kind: user\n+version: v2\n+metadata:\n+  name: joe\n+spec:\n+  # roles is a list of roles assigned to this user\n+  roles:\n+  - admin\n+  # traits are key, list of values pairs assigned to a user resource.\n+  # Traits can be used in role templates as variables.\n+  traits:\n+    logins:\n+    - joe\n+    - root\n+```\n+\n+This also creates some challenges, as administrators are forced to update local resources each time they have to assign a user new permission.\n+\n+### Modifications\n+\n+Let\u2019s now introduce the missing pieces of the puzzle and review how new roles will simplify or deprecate legacy concepts.\n+\n+#### Hierarchical Resource Groups\n+\n+Resource groups are one such missing piece - in most cloud environments, resources are split into hierarchy, \n+for example, host `luna` is a member of a resource group `lab`, in turn a member of environment `prod`, which is in turn a member of a Teleport cluster.\n+\n+In Teleport, we will make cluster a default root of this hierarchy. Every computing resource by default will be a direct member of a cluster resource group.\n+\n+We will also restrict a resource to be a member of only one resource group at a time.  \n+It\u2019s tempting to have a resource to be assigned to multiple resource groups, but we push this out of the scope of this RFD for simplicity.\n+\n+In our example, the cluster administrator would define lab resource group in the following way:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent: prod_env\n+```\n+\n+Administrators can assign resources to resource groups:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+   parent: prod_env\n+ match_kinds:\n+   - node\n+   - database\n+   - role\n+   - access_list\n+   - '.*'\n+ match_labels:  \n+   env: prod\n+```\n+\n+In this case, any resource that matches `env:prod` label will be assigned to this resource group. \n+\n+This will let administrators to gradually migrate their existing flat infrastructure to resource groups one.\n+\n+Such centralized assignment will also ensure that one resource can be only assigned to one resource group at a time.",
        "comment_created_at": "2024-02-14T12:44:33+00:00",
        "comment_author": "tigrato",
        "comment_body": "I agree with Nic here.\r\n\r\nWhile I haven't come across a compelling use case requiring a resource to belong to multiple groups - I think nested groups can address all scenarios if done correctly - , enabling users to define such situations would greatly simplify the migration path from complex roles to the new hierarchical pattern.\r\n\r\nAllowing quick migration enables them to simplify the knowledge structure and improve it later. Blocking at the creation stage would make migrating large role sets difficult and could discourage larger clusters.",
        "pr_file_module": null
      },
      {
        "comment_id": "1493087740",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 38078,
        "pr_file": "rfd/0164-scoped-rbac.md",
        "discussion_id": "1487076407",
        "commented_code": "@@ -0,0 +1,956 @@\n+\n+---\n+authors: @klizhentas (sasha@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0163 - Scoped RBAC\n+\n+## Required Approvers\n+\n+* Engineering @r0mant && (@tigrato || @marcoandredinis)\n+* Security: (@reedloden || @jentfoo)\n+* Product: (@xinding33 || @klizhentas )\n+\n+## What\n+\n+This RFD introduces resource hierarchies and scopes to existing RBAC. \n+Our goal is simplify and evolve access control in Teleport without drastic changes or new policy languages. \n+Make it easier to integrate Teleport RBAC with cloud IAMs of AWS, GCP and Azure.\n+This RFD is closely modeled and inspired by Azure RBAC model, the most advanced out of 3 clouds. \n+\n+Read about it here https://learn.microsoft.com/en-us/azure/role-based-access-control/overview before diving into this RFD.\n+\n+## Why\n+\n+There are several structural issues with the current RBAC model in Teleport.\n+\n+### Scalability issues \n+\n+Every role has to be distributed to every node or proxy that has to perform authorization. \n+Current roles are brittle - to evaluate access, every single role has to be fetched and processed, \n+because each role can have a deny rule that can block access to any allowed rule.\n+\n+### Scoping issues\n+\n+It is not possible to describe \u201cdelegated admins\u201d in RBAC, when one user has administrative access over part of the cluster.\n+It is also not possible to specify that certain role options only apply in certain use-cases. \n+\n+For example, the setting `permit-agent-forward: false` will deny agent-forward to any matching resource with no exceptions, even if other roles allow it.\n+\n+It is not possible to allow admins to grant roles to other users but with certain restrictions, as in the example of issue https://github.com/gravitational/teleport/issues/16914. \n+\n+### Complexity \n+\n+Roles today have both labels and label expressions, login rules to inject traits and claims and templates. \n+This creates a complicated system that is hard to understand and reason about.\n+Role mapping is brittle and requires updating OIDC/SAML connector resources, which can break access.\n+\n+### Security issues\n+\n+Every role assignment and trait is encoded in certificate, and each time a user gets their roles updated, they have to get a new certificate. \n+Old certificates can be re-used to get privileges that have been removed, \n+creating \u201ca new enemy problem\u201d described in [Zanzibar Paper](https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/).\n+\n+Many roles allow \u201crole escapes\u201d, as any person who gets assigned a role that can create other roles, \n+would become an admin, see for example issue https://github.com/gravitational/teleport.e/issues/3111\n+\n+### Goals\n+\n+Our key goal is to evolve Teleport\u2019s roles without asking users to rewrite their existing RBAC.\n+We also would like to better integrate Teleport RBAC with cloud provider\u2019s IAM systems out of the box.\n+We would like to give Teleport\u2019s users \u201cbatteries included\u201d approach, when they can get 90% of the use-cases done without c\n+reating any new roles, or modifying existing ones.\n+\n+### Non-Goals\n+\n+We are not going to implement backwards-incompatible changes that require our customers rewrite their RBAC stack or adopt a completely new policy language. \n+\n+## Details\n+To understand the required changes, let\u2019s first take a look at Teleport RBAC structure. \n+\n+### RBAC Primer\n+\n+Let\u2019s start with fundamental Teleport RBAC concepts and highlight some issues as we review them.\n+\n+#### Roles\n+\n+Each user or bot in Teleport is assigned one or several roles. \n+For the full reference, take a look at the documentation at https://goteleport.com/docs/reference/resources/#role.\n+\n+Here is a role structure:\n+\n+```yaml\n+kind: role\n+version: v7\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  # options are a set of knobs that apply across the entire role set.\n+  # the most restrictive options wins\n+  options:\n+    # max_session_ttl defines the TTL (time to live) of certificates\n+    # issued to the users with this role.\n+    max_session_ttl: 8h\n+    # forward_agent controls whether SSH agent forwarding is allowed\n+    forward_agent: true\n+  # The allow section declares a list of resource/verb combinations that are\n+  # allowed for the users of this role. By default, nothing is allowed.\n+  #\n+  # Allow rules specify both actions that are allowed and match the resources\n+  # they are applying to.\n+  allow:\n+    # Some Allow fields specify target protocol\n+    #  login principals, like in this example, SSH logins\n+    logins: [root, '{{internal.logins}}']\n+\n+    # In this example, the fields specify a set of windows desktop logins\n+    windows_desktop_logins: [Administrator, '{{internal.logins}}']\n+\n+    # There are multiple types of labels and label expressions that \n+    # match different computing resources.\n+    node_labels:\n+      # literal strings:\n+      'env': 'test'\n+    # regexp expressions\n+      'reg': '^us-west-1|eu-central-1$'\n+\n+    # node_labels_expression has the same purpose as node_labels but\n+    # supports predicate expressions to configure custom logic.\n+    # A user with this role will be allowed to access nodes if they are in the\n+    # staging environment *or* if they belong to one of the user's own teams.\n+    node_labels_expression: |\n+      labels[\"env\"] == \"staging\" ||\n+      contains(user.spec.traits[\"teams\"] , labels[\"team\"])\n+ \n+    # rules allow a user holding this role to modify other resources\n+    # matching the expressions below.\n+    # rules match both resources and specify what actions are allowed\n+    rules:\n+      - resources: [role]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+We can spot several issues with this role design:\n+\n+* There is no way to specify the scope that role applies to, all roles apply globally to all resources they match and all users they assign to all the time.\n+* There is no way to specify resource hierarchies, like computing environments (env vs prod), which makes it hard to partition the infrastructure and forces customers to specify labels.\n+\n+\n+#### Labels\n+\n+Teleport RBAC\u2019s advice to engineers to partition their resource is to first, label their computing resource or use AWS labels, and second, match the labels in RBAC.\n+\n+Admins can set the tags on the resource configuration file statically, or, for some resources, use `server_info` to set the tags for each resource:\n+\n+```yaml\n+# server_info.yaml\n+kind: server_info\n+metadata:\n+   name: si-<node-name>\n+spec:\n+   new_labels:\n+      \"foo\": \"bar\"\n+```\n+\n+This creates several issues:\n+\n+* It is not always secure to delegate labeling to owners of computing resources, as anyone with root access to the node config file can update its labels impacting everyone else.\n+* It is not scalable, as it\u2019s not always useful to set and updates tags for each individual resource\n+* It\u2019s hard or impossible to partition infrastructure with two-dimensional labels, although users can use `env: prod` to mark all resources in the production environment, there is no way to say that `env: lab` is a subset of `env: prod`.\n+\n+#### Current Roles mapping\n+\n+In Teleport there are multiple ways to map roles to users: static roles mapping to local users and bots, \n+dynamic mapping to SSO via connectors and on-demand assignment via access lists and access requests.\n+\n+Most users start with local static mapping and SSO mapping, later graduating to access requests and access lists.\n+\n+https://goteleport.com/docs/access-controls/sso/#creating-an-authentication-connector\n+\n+For SSO users, on login, Teleport checks SSO connector, and maps attributes of a user to a list of roles:\n+\n+```yaml\n+# connector.yaml\n+kind: saml\n+version: v2\n+metadata:\n+  name: corporate\n+spec:\n+  attributes_to_roles:\n+    - {name: \"groups\", value: \"okta-admin\", roles: [\"access\"]}\n+     # regular expressions with capture are also supported. \n+     # the next line instructs Teleport\n+     # to assign users to roles `admin-1` if his SAML \"group\" \n+     # attribute equals 'ssh_admin_1':\n+     - { name: \"group\", value: \"^ssh_admin_(.*)$\", roles: [\"admin-$1\"] }\n+```\n+\n+This creates several security and scalability issues:\n+\n+* Every time administrators need to change roles assignment, they have to update the resource.\n+* Every time the Identity Provider changes user\u2019s attributes, the users would have to relogin to get new roles. \n+\n+For local users, administrators have to specify roles and traits in the local resource:\n+\n+```yaml\n+kind: user\n+version: v2\n+metadata:\n+  name: joe\n+spec:\n+  # roles is a list of roles assigned to this user\n+  roles:\n+  - admin\n+  # traits are key, list of values pairs assigned to a user resource.\n+  # Traits can be used in role templates as variables.\n+  traits:\n+    logins:\n+    - joe\n+    - root\n+```\n+\n+This also creates some challenges, as administrators are forced to update local resources each time they have to assign a user new permission.\n+\n+### Modifications\n+\n+Let\u2019s now introduce the missing pieces of the puzzle and review how new roles will simplify or deprecate legacy concepts.\n+\n+#### Hierarchical Resource Groups\n+\n+Resource groups are one such missing piece - in most cloud environments, resources are split into hierarchy, \n+for example, host `luna` is a member of a resource group `lab`, in turn a member of environment `prod`, which is in turn a member of a Teleport cluster.\n+\n+In Teleport, we will make cluster a default root of this hierarchy. Every computing resource by default will be a direct member of a cluster resource group.\n+\n+We will also restrict a resource to be a member of only one resource group at a time.  \n+It\u2019s tempting to have a resource to be assigned to multiple resource groups, but we push this out of the scope of this RFD for simplicity.\n+\n+In our example, the cluster administrator would define lab resource group in the following way:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent: prod_env\n+```\n+\n+Administrators can assign resources to resource groups:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+   parent: prod_env\n+ match_kinds:\n+   - node\n+   - database\n+   - role\n+   - access_list\n+   - '.*'\n+ match_labels:  \n+   env: prod\n+```\n+\n+In this case, any resource that matches `env:prod` label will be assigned to this resource group. \n+\n+This will let administrators to gradually migrate their existing flat infrastructure to resource groups one.\n+\n+Such centralized assignment will also ensure that one resource can be only assigned to one resource group at a time.",
        "comment_created_at": "2024-02-17T00:04:08+00:00",
        "comment_author": "klizhentas",
        "comment_body": "I've updated the doc and removed the constraint",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1572954011",
    "pr_number": 38078,
    "pr_file": "rfd/0164-scoped-rbac.md",
    "created_at": "2024-04-19T21:01:58+00:00",
    "commented_code": "+\n+---\n+authors: @klizhentas (sasha@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0163 - Scoped RBAC\n+\n+## Required Approvers\n+\n+* Engineering @r0mant && (@tigrato || @marcoandredinis)\n+* Security: (@reedloden || @jentfoo)\n+* Product: (@xinding33 || @klizhentas )\n+\n+## What\n+\n+This RFD introduces resource hierarchies and scopes to existing RBAC. \n+Our goal is simplify and evolve access control in Teleport without drastic changes or new policy languages. \n+Make it easier to integrate Teleport RBAC with cloud IAMs of AWS, GCP and Azure.\n+This RFD is closely modeled and inspired by Azure RBAC model, the most advanced out of 3 clouds. \n+\n+Read about it here https://learn.microsoft.com/en-us/azure/role-based-access-control/overview before diving into this RFD.\n+\n+## Why\n+\n+There are several structural issues with the current RBAC model in Teleport.\n+\n+### Scalability issues \n+\n+Every role has to be distributed to every node or proxy that has to perform authorization. \n+Current roles are brittle - to evaluate access, every single role has to be fetched and processed, \n+because each role can have a deny rule that can block access to any allowed rule.\n+\n+### Scoping issues\n+\n+It is not possible to describe \u201cdelegated admins\u201d in RBAC, when one user has administrative access over part of the cluster.\n+It is also not possible to specify that certain role options only apply in certain use-cases. \n+\n+For example, the setting `permit-agent-forward: false` will deny agent-forward to any matching resource with no exceptions, even if other roles allow it.\n+\n+It is not possible to allow admins to grant roles to other users but with certain restrictions, as in the example of issue https://github.com/gravitational/teleport/issues/16914. \n+\n+### Complexity \n+\n+Roles today have both labels and label expressions, login rules to inject traits and claims and templates. \n+This creates a complicated system that is hard to understand and reason about.\n+Role mapping is brittle and requires updating OIDC/SAML connector resources, which can break access.\n+\n+### Security issues\n+\n+Every role assignment and trait is encoded in certificate, and each time a user gets their roles updated, they have to get a new certificate. \n+Old certificates can be re-used to get privileges that have been removed, \n+creating \u201ca new enemy problem\u201d described in [Zanzibar Paper](https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/).\n+\n+Many roles allow \u201crole escapes\u201d, as any person who gets assigned a role that can create other roles, \n+would become an admin, see for example issue https://github.com/gravitational/teleport.e/issues/3111\n+\n+### Goals\n+\n+Our key goal is to evolve Teleport\u2019s roles without asking users to rewrite their existing RBAC.\n+We also would like to better integrate Teleport RBAC with cloud provider\u2019s IAM systems out of the box.\n+We would like to give Teleport\u2019s users \u201cbatteries included\u201d approach, when they can get 90% of the use-cases done without c\n+reating any new roles, or modifying existing ones.\n+\n+### Non-Goals\n+\n+We are not going to implement backwards-incompatible changes that require our customers rewrite their RBAC stack or adopt a completely new policy language. \n+\n+## Details\n+To understand the required changes, let\u2019s first take a look at Teleport RBAC structure. \n+\n+### RBAC Primer\n+\n+Let\u2019s start with fundamental Teleport RBAC concepts and highlight some issues as we review them.\n+\n+#### Roles\n+\n+Each user or bot in Teleport is assigned one or several roles. \n+For the full reference, take a look at the documentation at https://goteleport.com/docs/reference/resources/#role.\n+\n+Here is a role structure:\n+\n+```yaml\n+kind: role\n+version: v7\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  # options are a set of knobs that apply across the entire role set.\n+  # the most restrictive options wins\n+  options:\n+    # max_session_ttl defines the TTL (time to live) of certificates\n+    # issued to the users with this role.\n+    max_session_ttl: 8h\n+    # forward_agent controls whether SSH agent forwarding is allowed\n+    forward_agent: true\n+  # The allow section declares a list of resource/verb combinations that are\n+  # allowed for the users of this role. By default, nothing is allowed.\n+  #\n+  # Allow rules specify both actions that are allowed and match the resources\n+  # they are applying to.\n+  allow:\n+    # Some Allow fields specify target protocol\n+    #  login principals, like in this example, SSH logins\n+    logins: [root, '{{internal.logins}}']\n+\n+    # In this example, the fields specify a set of windows desktop logins\n+    windows_desktop_logins: [Administrator, '{{internal.logins}}']\n+\n+    # There are multiple types of labels and label expressions that \n+    # match different computing resources.\n+    node_labels:\n+      # literal strings:\n+      'env': 'test'\n+    # regexp expressions\n+      'reg': '^us-west-1|eu-central-1$'\n+\n+    # node_labels_expression has the same purpose as node_labels but\n+    # supports predicate expressions to configure custom logic.\n+    # A user with this role will be allowed to access nodes if they are in the\n+    # staging environment *or* if they belong to one of the user's own teams.\n+    node_labels_expression: |\n+      labels[\"env\"] == \"staging\" ||\n+      contains(user.spec.traits[\"teams\"] , labels[\"team\"])\n+ \n+    # rules allow a user holding this role to modify other resources\n+    # matching the expressions below.\n+    # rules match both resources and specify what actions are allowed\n+    rules:\n+      - resources: [role]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+We can spot several issues with this role design:\n+\n+* There is no way to specify the scope that role applies to, all roles apply globally to all resources they match and all users they assign to all the time.\n+* There is no way to specify resource hierarchies, like computing environments (env vs prod), which makes it hard to partition the infrastructure and forces customers to specify labels.\n+\n+\n+#### Labels\n+\n+Teleport RBAC\u2019s advice to engineers to partition their resource is to first, label their computing resource or use AWS labels, and second, match the labels in RBAC.\n+\n+Admins can set the tags on the resource configuration file statically, or, for some resources, use `server_info` to set the tags for each resource:\n+\n+```yaml\n+# server_info.yaml\n+kind: server_info\n+metadata:\n+   name: si-<node-name>\n+spec:\n+   new_labels:\n+      \"foo\": \"bar\"\n+```\n+\n+This creates several issues:\n+\n+* It is not always secure to delegate labeling to owners of computing resources, as anyone with root access to the node config file can update its labels impacting everyone else.\n+* It is not scalable, as it\u2019s not always useful to set and updates tags for each individual resource\n+* It\u2019s hard or impossible to partition infrastructure with two-dimensional labels, although users can use `env: prod` to mark all resources in the production environment, there is no way to say that `env: lab` is a subset of `env: prod`.\n+\n+#### Current Roles mapping\n+\n+In Teleport there are multiple ways to map roles to users: static roles mapping to local users and bots, \n+dynamic mapping to SSO via connectors and on-demand assignment via access lists and access requests.\n+\n+Most users start with local static mapping and SSO mapping, later graduating to access requests and access lists.\n+\n+https://goteleport.com/docs/access-controls/sso/#creating-an-authentication-connector\n+\n+For SSO users, on login, Teleport checks SSO connector, and maps attributes of a user to a list of roles:\n+\n+```yaml\n+# connector.yaml\n+kind: saml\n+version: v2\n+metadata:\n+  name: corporate\n+spec:\n+  attributes_to_roles:\n+    - {name: \"groups\", value: \"okta-admin\", roles: [\"access\"]}\n+     # regular expressions with capture are also supported. \n+     # the next line instructs Teleport\n+     # to assign users to roles `admin-1` if his SAML \"group\" \n+     # attribute equals 'ssh_admin_1':\n+   - { name: \"group\", value: \"^ssh_admin_(.*)$\", roles: [\"admin-$1\"] }\n+```\n+\n+This creates several security and scalability issues:\n+\n+* Every time administrators need to change roles assignment, they have to update the resource.\n+* Every time the Identity Provider changes user\u2019s attributes, the users would have to relogin to get new roles. \n+\n+For local users, administrators have to specify roles and traits in the local resource:\n+\n+```yaml\n+kind: user\n+version: v2\n+metadata:\n+  name: joe\n+spec:\n+  # roles is a list of roles assigned to this user\n+  roles:\n+  - admin\n+  # traits are key, list of values pairs assigned to a user resource.\n+  # Traits can be used in role templates as variables.\n+  traits:\n+    logins:\n+    - joe\n+    - root\n+```\n+\n+This also creates some challenges, as administrators are forced to update local resources each time they have to assign a user new permission.\n+\n+### Modifications\n+\n+Let\u2019s now introduce the missing pieces of the puzzle and review how new roles will simplify or deprecate legacy concepts.\n+\n+#### Hierarchical Resource Groups\n+\n+Resource groups are one such missing piece - in most cloud environments, resources are split into hierarchy, \n+for example, host `luna` is a member of a resource group `lab`, in turn a member of environment `prod`, which is in turn a member of a Teleport cluster.\n+\n+In Teleport, we will make cluster a default root of this hierarchy. Every computing resource by default will be a direct member of a root `cluster` resource group.\n+\n+One resource can be assigned to multiple resource groups at a time, or none. \n+\n+In our example, the cluster administrator would define lab resource group in the following way:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent: prod_env\n+```\n+\n+Administrators can assign resources to resource groups:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+   parent: prod_env\n+ match_kinds:\n+   - node\n+   - database\n+   - role\n+   - access_list\n+   - '.*'\n+ match_labels:  \n+   env: prod",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "1572954011",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 38078,
        "pr_file": "rfd/0164-scoped-rbac.md",
        "discussion_id": "1572954011",
        "commented_code": "@@ -0,0 +1,1144 @@\n+\n+---\n+authors: @klizhentas (sasha@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0163 - Scoped RBAC\n+\n+## Required Approvers\n+\n+* Engineering @r0mant && (@tigrato || @marcoandredinis)\n+* Security: (@reedloden || @jentfoo)\n+* Product: (@xinding33 || @klizhentas )\n+\n+## What\n+\n+This RFD introduces resource hierarchies and scopes to existing RBAC. \n+Our goal is simplify and evolve access control in Teleport without drastic changes or new policy languages. \n+Make it easier to integrate Teleport RBAC with cloud IAMs of AWS, GCP and Azure.\n+This RFD is closely modeled and inspired by Azure RBAC model, the most advanced out of 3 clouds. \n+\n+Read about it here https://learn.microsoft.com/en-us/azure/role-based-access-control/overview before diving into this RFD.\n+\n+## Why\n+\n+There are several structural issues with the current RBAC model in Teleport.\n+\n+### Scalability issues \n+\n+Every role has to be distributed to every node or proxy that has to perform authorization. \n+Current roles are brittle - to evaluate access, every single role has to be fetched and processed, \n+because each role can have a deny rule that can block access to any allowed rule.\n+\n+### Scoping issues\n+\n+It is not possible to describe \u201cdelegated admins\u201d in RBAC, when one user has administrative access over part of the cluster.\n+It is also not possible to specify that certain role options only apply in certain use-cases. \n+\n+For example, the setting `permit-agent-forward: false` will deny agent-forward to any matching resource with no exceptions, even if other roles allow it.\n+\n+It is not possible to allow admins to grant roles to other users but with certain restrictions, as in the example of issue https://github.com/gravitational/teleport/issues/16914. \n+\n+### Complexity \n+\n+Roles today have both labels and label expressions, login rules to inject traits and claims and templates. \n+This creates a complicated system that is hard to understand and reason about.\n+Role mapping is brittle and requires updating OIDC/SAML connector resources, which can break access.\n+\n+### Security issues\n+\n+Every role assignment and trait is encoded in certificate, and each time a user gets their roles updated, they have to get a new certificate. \n+Old certificates can be re-used to get privileges that have been removed, \n+creating \u201ca new enemy problem\u201d described in [Zanzibar Paper](https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/).\n+\n+Many roles allow \u201crole escapes\u201d, as any person who gets assigned a role that can create other roles, \n+would become an admin, see for example issue https://github.com/gravitational/teleport.e/issues/3111\n+\n+### Goals\n+\n+Our key goal is to evolve Teleport\u2019s roles without asking users to rewrite their existing RBAC.\n+We also would like to better integrate Teleport RBAC with cloud provider\u2019s IAM systems out of the box.\n+We would like to give Teleport\u2019s users \u201cbatteries included\u201d approach, when they can get 90% of the use-cases done without c\n+reating any new roles, or modifying existing ones.\n+\n+### Non-Goals\n+\n+We are not going to implement backwards-incompatible changes that require our customers rewrite their RBAC stack or adopt a completely new policy language. \n+\n+## Details\n+To understand the required changes, let\u2019s first take a look at Teleport RBAC structure. \n+\n+### RBAC Primer\n+\n+Let\u2019s start with fundamental Teleport RBAC concepts and highlight some issues as we review them.\n+\n+#### Roles\n+\n+Each user or bot in Teleport is assigned one or several roles. \n+For the full reference, take a look at the documentation at https://goteleport.com/docs/reference/resources/#role.\n+\n+Here is a role structure:\n+\n+```yaml\n+kind: role\n+version: v7\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  # options are a set of knobs that apply across the entire role set.\n+  # the most restrictive options wins\n+  options:\n+    # max_session_ttl defines the TTL (time to live) of certificates\n+    # issued to the users with this role.\n+    max_session_ttl: 8h\n+    # forward_agent controls whether SSH agent forwarding is allowed\n+    forward_agent: true\n+  # The allow section declares a list of resource/verb combinations that are\n+  # allowed for the users of this role. By default, nothing is allowed.\n+  #\n+  # Allow rules specify both actions that are allowed and match the resources\n+  # they are applying to.\n+  allow:\n+    # Some Allow fields specify target protocol\n+    #  login principals, like in this example, SSH logins\n+    logins: [root, '{{internal.logins}}']\n+\n+    # In this example, the fields specify a set of windows desktop logins\n+    windows_desktop_logins: [Administrator, '{{internal.logins}}']\n+\n+    # There are multiple types of labels and label expressions that \n+    # match different computing resources.\n+    node_labels:\n+      # literal strings:\n+      'env': 'test'\n+    # regexp expressions\n+      'reg': '^us-west-1|eu-central-1$'\n+\n+    # node_labels_expression has the same purpose as node_labels but\n+    # supports predicate expressions to configure custom logic.\n+    # A user with this role will be allowed to access nodes if they are in the\n+    # staging environment *or* if they belong to one of the user's own teams.\n+    node_labels_expression: |\n+      labels[\"env\"] == \"staging\" ||\n+      contains(user.spec.traits[\"teams\"] , labels[\"team\"])\n+ \n+    # rules allow a user holding this role to modify other resources\n+    # matching the expressions below.\n+    # rules match both resources and specify what actions are allowed\n+    rules:\n+      - resources: [role]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+We can spot several issues with this role design:\n+\n+* There is no way to specify the scope that role applies to, all roles apply globally to all resources they match and all users they assign to all the time.\n+* There is no way to specify resource hierarchies, like computing environments (env vs prod), which makes it hard to partition the infrastructure and forces customers to specify labels.\n+\n+\n+#### Labels\n+\n+Teleport RBAC\u2019s advice to engineers to partition their resource is to first, label their computing resource or use AWS labels, and second, match the labels in RBAC.\n+\n+Admins can set the tags on the resource configuration file statically, or, for some resources, use `server_info` to set the tags for each resource:\n+\n+```yaml\n+# server_info.yaml\n+kind: server_info\n+metadata:\n+   name: si-<node-name>\n+spec:\n+   new_labels:\n+      \"foo\": \"bar\"\n+```\n+\n+This creates several issues:\n+\n+* It is not always secure to delegate labeling to owners of computing resources, as anyone with root access to the node config file can update its labels impacting everyone else.\n+* It is not scalable, as it\u2019s not always useful to set and updates tags for each individual resource\n+* It\u2019s hard or impossible to partition infrastructure with two-dimensional labels, although users can use `env: prod` to mark all resources in the production environment, there is no way to say that `env: lab` is a subset of `env: prod`.\n+\n+#### Current Roles mapping\n+\n+In Teleport there are multiple ways to map roles to users: static roles mapping to local users and bots, \n+dynamic mapping to SSO via connectors and on-demand assignment via access lists and access requests.\n+\n+Most users start with local static mapping and SSO mapping, later graduating to access requests and access lists.\n+\n+https://goteleport.com/docs/access-controls/sso/#creating-an-authentication-connector\n+\n+For SSO users, on login, Teleport checks SSO connector, and maps attributes of a user to a list of roles:\n+\n+```yaml\n+# connector.yaml\n+kind: saml\n+version: v2\n+metadata:\n+  name: corporate\n+spec:\n+  attributes_to_roles:\n+    - {name: \"groups\", value: \"okta-admin\", roles: [\"access\"]}\n+     # regular expressions with capture are also supported. \n+     # the next line instructs Teleport\n+     # to assign users to roles `admin-1` if his SAML \"group\" \n+     # attribute equals 'ssh_admin_1':\n+   - { name: \"group\", value: \"^ssh_admin_(.*)$\", roles: [\"admin-$1\"] }\n+```\n+\n+This creates several security and scalability issues:\n+\n+* Every time administrators need to change roles assignment, they have to update the resource.\n+* Every time the Identity Provider changes user\u2019s attributes, the users would have to relogin to get new roles. \n+\n+For local users, administrators have to specify roles and traits in the local resource:\n+\n+```yaml\n+kind: user\n+version: v2\n+metadata:\n+  name: joe\n+spec:\n+  # roles is a list of roles assigned to this user\n+  roles:\n+  - admin\n+  # traits are key, list of values pairs assigned to a user resource.\n+  # Traits can be used in role templates as variables.\n+  traits:\n+    logins:\n+    - joe\n+    - root\n+```\n+\n+This also creates some challenges, as administrators are forced to update local resources each time they have to assign a user new permission.\n+\n+### Modifications\n+\n+Let\u2019s now introduce the missing pieces of the puzzle and review how new roles will simplify or deprecate legacy concepts.\n+\n+#### Hierarchical Resource Groups\n+\n+Resource groups are one such missing piece - in most cloud environments, resources are split into hierarchy, \n+for example, host `luna` is a member of a resource group `lab`, in turn a member of environment `prod`, which is in turn a member of a Teleport cluster.\n+\n+In Teleport, we will make cluster a default root of this hierarchy. Every computing resource by default will be a direct member of a root `cluster` resource group.\n+\n+One resource can be assigned to multiple resource groups at a time, or none. \n+\n+In our example, the cluster administrator would define lab resource group in the following way:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent: prod_env\n+```\n+\n+Administrators can assign resources to resource groups:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+   parent: prod_env\n+ match_kinds:\n+   - node\n+   - database\n+   - role\n+   - access_list\n+   - '.*'\n+ match_labels:  \n+   env: prod",
        "comment_created_at": "2024-04-19T21:01:58+00:00",
        "comment_author": "fspmarshall",
        "comment_body": "Minor nit, but defining group/scope membership based on labels strikes me as risky.  As I understand it, one of the great strengths of the scope/group system is that it will be designed from the ground up to be free of the various issues that plague labels (self-report, asynchronous discovery, etc).\r\n\r\nIMO if we want to provide a means of assigning to a resource group based on label, it should be something more akin to an asynchronous task definition rather than a fundamental part of the resource group's configuration.  Label discovery is fallible and asynchronous, and I think the UX around any \"assign to group based on labels\" operation should reflect that somehow.  Even if it's just living under a separate resource with docs clearly explaining the asynchronous nature of the operation.",
        "pr_file_module": null
      },
      {
        "comment_id": "1573277478",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 38078,
        "pr_file": "rfd/0164-scoped-rbac.md",
        "discussion_id": "1572954011",
        "commented_code": "@@ -0,0 +1,1144 @@\n+\n+---\n+authors: @klizhentas (sasha@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0163 - Scoped RBAC\n+\n+## Required Approvers\n+\n+* Engineering @r0mant && (@tigrato || @marcoandredinis)\n+* Security: (@reedloden || @jentfoo)\n+* Product: (@xinding33 || @klizhentas )\n+\n+## What\n+\n+This RFD introduces resource hierarchies and scopes to existing RBAC. \n+Our goal is simplify and evolve access control in Teleport without drastic changes or new policy languages. \n+Make it easier to integrate Teleport RBAC with cloud IAMs of AWS, GCP and Azure.\n+This RFD is closely modeled and inspired by Azure RBAC model, the most advanced out of 3 clouds. \n+\n+Read about it here https://learn.microsoft.com/en-us/azure/role-based-access-control/overview before diving into this RFD.\n+\n+## Why\n+\n+There are several structural issues with the current RBAC model in Teleport.\n+\n+### Scalability issues \n+\n+Every role has to be distributed to every node or proxy that has to perform authorization. \n+Current roles are brittle - to evaluate access, every single role has to be fetched and processed, \n+because each role can have a deny rule that can block access to any allowed rule.\n+\n+### Scoping issues\n+\n+It is not possible to describe \u201cdelegated admins\u201d in RBAC, when one user has administrative access over part of the cluster.\n+It is also not possible to specify that certain role options only apply in certain use-cases. \n+\n+For example, the setting `permit-agent-forward: false` will deny agent-forward to any matching resource with no exceptions, even if other roles allow it.\n+\n+It is not possible to allow admins to grant roles to other users but with certain restrictions, as in the example of issue https://github.com/gravitational/teleport/issues/16914. \n+\n+### Complexity \n+\n+Roles today have both labels and label expressions, login rules to inject traits and claims and templates. \n+This creates a complicated system that is hard to understand and reason about.\n+Role mapping is brittle and requires updating OIDC/SAML connector resources, which can break access.\n+\n+### Security issues\n+\n+Every role assignment and trait is encoded in certificate, and each time a user gets their roles updated, they have to get a new certificate. \n+Old certificates can be re-used to get privileges that have been removed, \n+creating \u201ca new enemy problem\u201d described in [Zanzibar Paper](https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/).\n+\n+Many roles allow \u201crole escapes\u201d, as any person who gets assigned a role that can create other roles, \n+would become an admin, see for example issue https://github.com/gravitational/teleport.e/issues/3111\n+\n+### Goals\n+\n+Our key goal is to evolve Teleport\u2019s roles without asking users to rewrite their existing RBAC.\n+We also would like to better integrate Teleport RBAC with cloud provider\u2019s IAM systems out of the box.\n+We would like to give Teleport\u2019s users \u201cbatteries included\u201d approach, when they can get 90% of the use-cases done without c\n+reating any new roles, or modifying existing ones.\n+\n+### Non-Goals\n+\n+We are not going to implement backwards-incompatible changes that require our customers rewrite their RBAC stack or adopt a completely new policy language. \n+\n+## Details\n+To understand the required changes, let\u2019s first take a look at Teleport RBAC structure. \n+\n+### RBAC Primer\n+\n+Let\u2019s start with fundamental Teleport RBAC concepts and highlight some issues as we review them.\n+\n+#### Roles\n+\n+Each user or bot in Teleport is assigned one or several roles. \n+For the full reference, take a look at the documentation at https://goteleport.com/docs/reference/resources/#role.\n+\n+Here is a role structure:\n+\n+```yaml\n+kind: role\n+version: v7\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  # options are a set of knobs that apply across the entire role set.\n+  # the most restrictive options wins\n+  options:\n+    # max_session_ttl defines the TTL (time to live) of certificates\n+    # issued to the users with this role.\n+    max_session_ttl: 8h\n+    # forward_agent controls whether SSH agent forwarding is allowed\n+    forward_agent: true\n+  # The allow section declares a list of resource/verb combinations that are\n+  # allowed for the users of this role. By default, nothing is allowed.\n+  #\n+  # Allow rules specify both actions that are allowed and match the resources\n+  # they are applying to.\n+  allow:\n+    # Some Allow fields specify target protocol\n+    #  login principals, like in this example, SSH logins\n+    logins: [root, '{{internal.logins}}']\n+\n+    # In this example, the fields specify a set of windows desktop logins\n+    windows_desktop_logins: [Administrator, '{{internal.logins}}']\n+\n+    # There are multiple types of labels and label expressions that \n+    # match different computing resources.\n+    node_labels:\n+      # literal strings:\n+      'env': 'test'\n+    # regexp expressions\n+      'reg': '^us-west-1|eu-central-1$'\n+\n+    # node_labels_expression has the same purpose as node_labels but\n+    # supports predicate expressions to configure custom logic.\n+    # A user with this role will be allowed to access nodes if they are in the\n+    # staging environment *or* if they belong to one of the user's own teams.\n+    node_labels_expression: |\n+      labels[\"env\"] == \"staging\" ||\n+      contains(user.spec.traits[\"teams\"] , labels[\"team\"])\n+ \n+    # rules allow a user holding this role to modify other resources\n+    # matching the expressions below.\n+    # rules match both resources and specify what actions are allowed\n+    rules:\n+      - resources: [role]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+We can spot several issues with this role design:\n+\n+* There is no way to specify the scope that role applies to, all roles apply globally to all resources they match and all users they assign to all the time.\n+* There is no way to specify resource hierarchies, like computing environments (env vs prod), which makes it hard to partition the infrastructure and forces customers to specify labels.\n+\n+\n+#### Labels\n+\n+Teleport RBAC\u2019s advice to engineers to partition their resource is to first, label their computing resource or use AWS labels, and second, match the labels in RBAC.\n+\n+Admins can set the tags on the resource configuration file statically, or, for some resources, use `server_info` to set the tags for each resource:\n+\n+```yaml\n+# server_info.yaml\n+kind: server_info\n+metadata:\n+   name: si-<node-name>\n+spec:\n+   new_labels:\n+      \"foo\": \"bar\"\n+```\n+\n+This creates several issues:\n+\n+* It is not always secure to delegate labeling to owners of computing resources, as anyone with root access to the node config file can update its labels impacting everyone else.\n+* It is not scalable, as it\u2019s not always useful to set and updates tags for each individual resource\n+* It\u2019s hard or impossible to partition infrastructure with two-dimensional labels, although users can use `env: prod` to mark all resources in the production environment, there is no way to say that `env: lab` is a subset of `env: prod`.\n+\n+#### Current Roles mapping\n+\n+In Teleport there are multiple ways to map roles to users: static roles mapping to local users and bots, \n+dynamic mapping to SSO via connectors and on-demand assignment via access lists and access requests.\n+\n+Most users start with local static mapping and SSO mapping, later graduating to access requests and access lists.\n+\n+https://goteleport.com/docs/access-controls/sso/#creating-an-authentication-connector\n+\n+For SSO users, on login, Teleport checks SSO connector, and maps attributes of a user to a list of roles:\n+\n+```yaml\n+# connector.yaml\n+kind: saml\n+version: v2\n+metadata:\n+  name: corporate\n+spec:\n+  attributes_to_roles:\n+    - {name: \"groups\", value: \"okta-admin\", roles: [\"access\"]}\n+     # regular expressions with capture are also supported. \n+     # the next line instructs Teleport\n+     # to assign users to roles `admin-1` if his SAML \"group\" \n+     # attribute equals 'ssh_admin_1':\n+   - { name: \"group\", value: \"^ssh_admin_(.*)$\", roles: [\"admin-$1\"] }\n+```\n+\n+This creates several security and scalability issues:\n+\n+* Every time administrators need to change roles assignment, they have to update the resource.\n+* Every time the Identity Provider changes user\u2019s attributes, the users would have to relogin to get new roles. \n+\n+For local users, administrators have to specify roles and traits in the local resource:\n+\n+```yaml\n+kind: user\n+version: v2\n+metadata:\n+  name: joe\n+spec:\n+  # roles is a list of roles assigned to this user\n+  roles:\n+  - admin\n+  # traits are key, list of values pairs assigned to a user resource.\n+  # Traits can be used in role templates as variables.\n+  traits:\n+    logins:\n+    - joe\n+    - root\n+```\n+\n+This also creates some challenges, as administrators are forced to update local resources each time they have to assign a user new permission.\n+\n+### Modifications\n+\n+Let\u2019s now introduce the missing pieces of the puzzle and review how new roles will simplify or deprecate legacy concepts.\n+\n+#### Hierarchical Resource Groups\n+\n+Resource groups are one such missing piece - in most cloud environments, resources are split into hierarchy, \n+for example, host `luna` is a member of a resource group `lab`, in turn a member of environment `prod`, which is in turn a member of a Teleport cluster.\n+\n+In Teleport, we will make cluster a default root of this hierarchy. Every computing resource by default will be a direct member of a root `cluster` resource group.\n+\n+One resource can be assigned to multiple resource groups at a time, or none. \n+\n+In our example, the cluster administrator would define lab resource group in the following way:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent: prod_env\n+```\n+\n+Administrators can assign resources to resource groups:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+   parent: prod_env\n+ match_kinds:\n+   - node\n+   - database\n+   - role\n+   - access_list\n+   - '.*'\n+ match_labels:  \n+   env: prod",
        "comment_created_at": "2024-04-20T12:58:53+00:00",
        "comment_author": "francesco-doyensec",
        "comment_body": "I agree ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2075061840",
    "pr_number": 38078,
    "pr_file": "rfd/0164-scoped-rbac.md",
    "created_at": "2025-05-06T09:14:29+00:00",
    "commented_code": "+\n+---\n+authors: @klizhentas (sasha@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0163 - Scoped RBAC\n+\n+## Required Approvers\n+\n+* Engineering @r0mant && (@tigrato || @marcoandredinis)\n+* Security: (@reedloden || @jentfoo)\n+* Product: (@xinding33 || @klizhentas )\n+\n+## What\n+\n+This RFD introduces resource hierarchies and scopes to existing RBAC. \n+Our goal is simplify and evolve access control in Teleport without drastic changes or new policy languages. \n+Make it easier to integrate Teleport RBAC with cloud IAMs of AWS, GCP and Azure.\n+This RFD is closely modeled and inspired by Azure RBAC model, the most advanced out of 3 clouds. \n+\n+Read about it here https://learn.microsoft.com/en-us/azure/role-based-access-control/overview before diving into this RFD.\n+\n+## Why\n+\n+There are several structural issues with the current RBAC model in Teleport.\n+\n+### Scalability issues \n+\n+Every role has to be distributed to every node or proxy that has to perform authorization. \n+Current roles are brittle - to evaluate access, every single role has to be fetched and processed, \n+because each role can have a deny rule that can block access to any allowed rule.\n+\n+### Scoping issues\n+\n+It is not possible to describe \u201cdelegated admins\u201d in RBAC, when one user has administrative access over part of the cluster.\n+It is also not possible to specify that certain role options only apply in certain use-cases. \n+\n+For example, the setting `permit-agent-forward: false` will deny agent-forward to any matching resource with no exceptions, even if other roles allow it.\n+\n+It is not possible to allow admins to grant roles to other users but with certain restrictions, as in the example of issue https://github.com/gravitational/teleport/issues/16914. \n+\n+### Complexity \n+\n+Roles today have both labels and label expressions, login rules to inject traits and claims and templates. \n+This creates a complicated system that is hard to understand and reason about.\n+Role mapping is brittle and requires updating OIDC/SAML connector resources, which can break access.\n+\n+### Security issues\n+\n+Every role assignment and trait is encoded in certificate, and each time a user gets their roles updated, they have to get a new certificate. \n+Old certificates can be re-used to get privileges that have been removed, \n+creating \u201ca new enemy problem\u201d described in [Zanzibar Paper](https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/).\n+\n+Many roles allow \u201crole escapes\u201d, as any person who gets assigned a role that can create other roles, \n+would become an admin, see for example issue https://github.com/gravitational/teleport.e/issues/3111\n+\n+### Goals\n+\n+Our key goal is to evolve Teleport\u2019s roles without asking users to rewrite their existing RBAC.\n+We also would like to better integrate Teleport RBAC with cloud provider\u2019s IAM systems out of the box.\n+We would like to give Teleport\u2019s users \u201cbatteries included\u201d approach, when they can get 90% of the use-cases done without c\n+reating any new roles, or modifying existing ones.\n+\n+### Non-Goals\n+\n+We are not going to implement backwards-incompatible changes that require our customers rewrite their RBAC stack or adopt a completely new policy language. \n+\n+## Details\n+To understand the required changes, let\u2019s first take a look at Teleport RBAC structure. \n+\n+### RBAC Primer\n+\n+Let\u2019s start with fundamental Teleport RBAC concepts and highlight some issues as we review them.\n+\n+#### Roles\n+\n+Each user or bot in Teleport is assigned one or several roles. \n+For the full reference, take a look at the documentation at https://goteleport.com/docs/reference/resources/#role.\n+\n+Here is a role structure:\n+\n+```yaml\n+kind: role\n+version: v7\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  # options are a set of knobs that apply across the entire role set.\n+  # the most restrictive options wins\n+  options:\n+    # max_session_ttl defines the TTL (time to live) of certificates\n+    # issued to the users with this role.\n+    max_session_ttl: 8h\n+    # forward_agent controls whether SSH agent forwarding is allowed\n+    forward_agent: true\n+  # The allow section declares a list of resource/verb combinations that are\n+  # allowed for the users of this role. By default, nothing is allowed.\n+  #\n+  # Allow rules specify both actions that are allowed and match the resources\n+  # they are applying to.\n+  allow:\n+    # Some Allow fields specify target protocol\n+    #  login principals, like in this example, SSH logins\n+    logins: [root, '{{internal.logins}}']\n+\n+    # In this example, the fields specify a set of windows desktop logins\n+    windows_desktop_logins: [Administrator, '{{internal.logins}}']\n+\n+    # There are multiple types of labels and label expressions that \n+    # match different computing resources.\n+    node_labels:\n+      # literal strings:\n+      'env': 'test'\n+    # regexp expressions\n+      'reg': '^us-west-1|eu-central-1$'\n+\n+    # node_labels_expression has the same purpose as node_labels but\n+    # supports predicate expressions to configure custom logic.\n+    # A user with this role will be allowed to access nodes if they are in the\n+    # staging environment *or* if they belong to one of the user's own teams.\n+    node_labels_expression: |\n+      labels[\"env\"] == \"staging\" ||\n+      contains(user.spec.traits[\"teams\"] , labels[\"team\"])\n+ \n+    # rules allow a user holding this role to modify other resources\n+    # matching the expressions below.\n+    # rules match both resources and specify what actions are allowed\n+    rules:\n+      - resources: [role]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+We can spot several issues with this role design:\n+\n+* There is no way to specify the scope that role applies to, all roles apply globally to all resources they match and all users they assign to all the time.\n+* There is no way to specify resource hierarchies, like computing environments (env vs prod), which makes it hard to partition the infrastructure and forces customers to specify labels.\n+\n+\n+#### Labels\n+\n+Teleport RBAC\u2019s advice to engineers to partition their resource is to first, label their computing resource or use AWS labels, and second, match the labels in RBAC.\n+\n+Admins can set the tags on the resource configuration file statically, or, for some resources, use `server_info` to set the tags for each resource:\n+\n+```yaml\n+# server_info.yaml\n+kind: server_info\n+metadata:\n+   name: si-<node-name>\n+spec:\n+   new_labels:\n+      \"foo\": \"bar\"\n+```\n+\n+This creates several issues:\n+\n+* It is not always secure to delegate labeling to owners of computing resources, as anyone with root access to the node config file can update its labels impacting everyone else.\n+* It is not scalable, as it\u2019s not always useful to set and updates tags for each individual resource\n+* It\u2019s hard or impossible to partition infrastructure with two-dimensional labels, although users can use `env: prod` to mark all resources in the production environment, there is no way to say that `env: lab` is a subset of `env: prod`.\n+\n+#### Current Roles mapping\n+\n+In Teleport there are multiple ways to map roles to users: static roles mapping to local users and bots, \n+dynamic mapping to SSO via connectors and on-demand assignment via access lists and access requests.\n+\n+Most users start with local static mapping and SSO mapping, later graduating to access requests and access lists.\n+\n+https://goteleport.com/docs/access-controls/sso/#creating-an-authentication-connector\n+\n+For SSO users, on login, Teleport checks SSO connector, and maps attributes of a user to a list of roles:\n+\n+```yaml\n+# connector.yaml\n+kind: saml\n+version: v2\n+metadata:\n+  name: corporate\n+spec:\n+  attributes_to_roles:\n+    - {name: \"groups\", value: \"okta-admin\", roles: [\"access\"]}\n+     # regular expressions with capture are also supported. \n+     # the next line instructs Teleport\n+     # to assign users to roles `admin-1` if his SAML \"group\" \n+     # attribute equals 'ssh_admin_1':\n+   - { name: \"group\", value: \"^ssh_admin_(.*)$\", roles: [\"admin-$1\"] }\n+```\n+\n+This creates several security and scalability issues:\n+\n+* Every time administrators need to change roles assignment, they have to update the resource.\n+* Every time the Identity Provider changes user\u2019s attributes, the users would have to relogin to get new roles. \n+\n+For local users, administrators have to specify roles and traits in the local resource:\n+\n+```yaml\n+kind: user\n+version: v2\n+metadata:\n+  name: joe\n+spec:\n+  # roles is a list of roles assigned to this user\n+  roles:\n+  - admin\n+  # traits are key, list of values pairs assigned to a user resource.\n+  # Traits can be used in role templates as variables.\n+  traits:\n+    logins:\n+    - joe\n+    - root\n+```\n+\n+This also creates some challenges, as administrators are forced to update local resources each time they have to assign a user new permission.\n+\n+### Modifications\n+\n+Let\u2019s now introduce the missing pieces of the puzzle and review how new roles will simplify or deprecate legacy concepts.\n+\n+#### Hierarchical Resource Groups\n+\n+Resource groups are one such missing piece - in most cloud environments, resources are split into hierarchy, \n+for example, host `luna` is a member of a resource group `lab`, in turn a member of environment `prod`, which is in turn a member of a Teleport cluster.\n+\n+In Teleport, we will make cluster a default root of this hierarchy. Every computing resource by default will be a direct member of a root `cluster` resource group.\n+\n+One resource can be assigned to multiple resource groups at a time, or none. \n+\n+In our example, the cluster administrator would define lab resource group in the following way:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent: prod_env\n+```\n+\n+Administrators can assign resources to resource groups:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+   parent: prod_env\n+ match_kinds:\n+   - node\n+   - database\n+   - role\n+   - access_list\n+   - '.*'\n+ match_labels:  \n+   env: prod\n+```\n+\n+In this case, any resource that matches `env:prod` label will be assigned to this resource group. We will use the same label matching algorithm as in today's `discovery_service`. \n+\n+This will let administrators to gradually migrate their existing flat infrastructure to resource groups one.\n+\n+In some cases it makes sense to specify parent resource group inline:\n+\n+```yaml\n+kind: role\n+spec:\n+  parent_resource_group: /env/prod\n+```\n+\n+By default, if unspecified, a resource is a member of a root-level - `/` cluster resource group. If specified by the resource, it won't be a member of a root `/` resource group.\n+\n+Resource groups are hierarchical, and we can refer to the `lab` resource group by its full path as `/env/prod/lab`. \n+\n+Most Teleport resources, with some exceptions, like users, SSO connectors can be a member of a resource group. \n+\n+We will list those resources separately below.\n+\n+##### Default Resource groups via auto-discovery\n+\n+Teleport can create resource groups if admins turn on auto discovery. This will significantly simplify configuration. \n+\n+Here are some of the resource groups that Teleport Discovery service will create:\n+\n+* For AWS, Teleport discovery service will place each computing resource in `/aws/[account-id]/[region]/[resource-type]/[resource-id]`.\n+  + When EKS auto-discovery is on, this hierarchy will include discovered apps - `/aws/account-id/[region]/k8s/[cluster-name]/namespaces/[namespace]/[app-id]`\n+* For Azure, Teleport will use Azure's hierarchy - `/azure/[management-group]/[subscription]/[resource-group]/[resource-type]/[resource-id]`\n+* For GCP, we will use GCP hierarchy of `/gcp/[organization]/[folder]/[project]/[resource-type]/[resource-id]`\n+\n+Discovery service will create and remove these hierarchies based on the cloud state, and will create resources with `parent_resource_group` field to place them in those resource groups.\n+\n+If users are not happy with a default hierarchy, they can create a different one.\n+\n+#### Access Lists\n+\n+Teleport has a concept of access lists, that lists an owner, members, and optionally a parent access list.\n+Access List in Teleport represents a group of users with a hierarchy. \n+\n+We will further assume that the root of this hierarchy is a cluster. \n+\n+Unlike in resource groups, a user can be an owner and a member of none, one or several access lists at once.\n+\n+In addition to that, access list grants a role to a set of members, like in this example:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: \"lab-engineers\"\n+spec:\n+  desc: \"Access list for lab engineers\"\n+  grants:\n+    roles: [access]\n+  members:\n+    - name: bob@example.com\n+```\n+\n+We will return to access lists later, but let\u2019s now recall that access lists contain a list of members, who are, in turn, granted one or several roles.\n+\n+#### Scopes\n+\n+By default, in Teleport a role is granted it applies to all resources in the Teleport cluster.\n+\n+However, with this change, will be able to grant a set of roles that apply only to resources that belong to a specific resource group. \n+\n+In this case, we will say that the roles apply at the scope of the resource group.\n+\n+Scopes define a set of resources roles apply to. \n+\n+We will introduce scopes in a couple of places, first, for access list:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: \"lab-engineers\"\n+spec:\n+  desc: \"Access list for lab engineers\"\n+  # this grant applies only at the scope of the resource group `/env/prod/lab`\n+  scopes: [\u2018/env/prod/lab']\n+  grants:\n+    roles: [access]\n+  members:\n+    - name: bob@example.com\n+```\n+\n+By default, all existing access lists will grant roles at the cluster scope, cascading to all resources, just like before the migration. \n+\n+However, going forward, admins will be able to set scopes to more granular levels.\n+\n+The second place where we introduce scopes is in the roles:\n+\n+```yaml\n+kind: role\n+metadata:\n+ name: access\n+spec:\n+  grantable_scopes: ['/env/prod']\n+```\n+\n+Grantable scopes specifiy maximum scope this role can be granted on. \n+\n+**Important:** By default, if the `grantable_scopes` are missing, we assume empty scope - that will prevent the role from being granted on any scopes. When migrating existing roles, we would set `/` - root scope to avoid breaking the cluster.",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "2075061840",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 38078,
        "pr_file": "rfd/0164-scoped-rbac.md",
        "discussion_id": "2075061840",
        "commented_code": "@@ -0,0 +1,1144 @@\n+\n+---\n+authors: @klizhentas (sasha@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0163 - Scoped RBAC\n+\n+## Required Approvers\n+\n+* Engineering @r0mant && (@tigrato || @marcoandredinis)\n+* Security: (@reedloden || @jentfoo)\n+* Product: (@xinding33 || @klizhentas )\n+\n+## What\n+\n+This RFD introduces resource hierarchies and scopes to existing RBAC. \n+Our goal is simplify and evolve access control in Teleport without drastic changes or new policy languages. \n+Make it easier to integrate Teleport RBAC with cloud IAMs of AWS, GCP and Azure.\n+This RFD is closely modeled and inspired by Azure RBAC model, the most advanced out of 3 clouds. \n+\n+Read about it here https://learn.microsoft.com/en-us/azure/role-based-access-control/overview before diving into this RFD.\n+\n+## Why\n+\n+There are several structural issues with the current RBAC model in Teleport.\n+\n+### Scalability issues \n+\n+Every role has to be distributed to every node or proxy that has to perform authorization. \n+Current roles are brittle - to evaluate access, every single role has to be fetched and processed, \n+because each role can have a deny rule that can block access to any allowed rule.\n+\n+### Scoping issues\n+\n+It is not possible to describe \u201cdelegated admins\u201d in RBAC, when one user has administrative access over part of the cluster.\n+It is also not possible to specify that certain role options only apply in certain use-cases. \n+\n+For example, the setting `permit-agent-forward: false` will deny agent-forward to any matching resource with no exceptions, even if other roles allow it.\n+\n+It is not possible to allow admins to grant roles to other users but with certain restrictions, as in the example of issue https://github.com/gravitational/teleport/issues/16914. \n+\n+### Complexity \n+\n+Roles today have both labels and label expressions, login rules to inject traits and claims and templates. \n+This creates a complicated system that is hard to understand and reason about.\n+Role mapping is brittle and requires updating OIDC/SAML connector resources, which can break access.\n+\n+### Security issues\n+\n+Every role assignment and trait is encoded in certificate, and each time a user gets their roles updated, they have to get a new certificate. \n+Old certificates can be re-used to get privileges that have been removed, \n+creating \u201ca new enemy problem\u201d described in [Zanzibar Paper](https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/).\n+\n+Many roles allow \u201crole escapes\u201d, as any person who gets assigned a role that can create other roles, \n+would become an admin, see for example issue https://github.com/gravitational/teleport.e/issues/3111\n+\n+### Goals\n+\n+Our key goal is to evolve Teleport\u2019s roles without asking users to rewrite their existing RBAC.\n+We also would like to better integrate Teleport RBAC with cloud provider\u2019s IAM systems out of the box.\n+We would like to give Teleport\u2019s users \u201cbatteries included\u201d approach, when they can get 90% of the use-cases done without c\n+reating any new roles, or modifying existing ones.\n+\n+### Non-Goals\n+\n+We are not going to implement backwards-incompatible changes that require our customers rewrite their RBAC stack or adopt a completely new policy language. \n+\n+## Details\n+To understand the required changes, let\u2019s first take a look at Teleport RBAC structure. \n+\n+### RBAC Primer\n+\n+Let\u2019s start with fundamental Teleport RBAC concepts and highlight some issues as we review them.\n+\n+#### Roles\n+\n+Each user or bot in Teleport is assigned one or several roles. \n+For the full reference, take a look at the documentation at https://goteleport.com/docs/reference/resources/#role.\n+\n+Here is a role structure:\n+\n+```yaml\n+kind: role\n+version: v7\n+metadata:\n+  # role name is unique across the entire cluster\n+  name: example\n+spec:\n+  # options are a set of knobs that apply across the entire role set.\n+  # the most restrictive options wins\n+  options:\n+    # max_session_ttl defines the TTL (time to live) of certificates\n+    # issued to the users with this role.\n+    max_session_ttl: 8h\n+    # forward_agent controls whether SSH agent forwarding is allowed\n+    forward_agent: true\n+  # The allow section declares a list of resource/verb combinations that are\n+  # allowed for the users of this role. By default, nothing is allowed.\n+  #\n+  # Allow rules specify both actions that are allowed and match the resources\n+  # they are applying to.\n+  allow:\n+    # Some Allow fields specify target protocol\n+    #  login principals, like in this example, SSH logins\n+    logins: [root, '{{internal.logins}}']\n+\n+    # In this example, the fields specify a set of windows desktop logins\n+    windows_desktop_logins: [Administrator, '{{internal.logins}}']\n+\n+    # There are multiple types of labels and label expressions that \n+    # match different computing resources.\n+    node_labels:\n+      # literal strings:\n+      'env': 'test'\n+    # regexp expressions\n+      'reg': '^us-west-1|eu-central-1$'\n+\n+    # node_labels_expression has the same purpose as node_labels but\n+    # supports predicate expressions to configure custom logic.\n+    # A user with this role will be allowed to access nodes if they are in the\n+    # staging environment *or* if they belong to one of the user's own teams.\n+    node_labels_expression: |\n+      labels[\"env\"] == \"staging\" ||\n+      contains(user.spec.traits[\"teams\"] , labels[\"team\"])\n+ \n+    # rules allow a user holding this role to modify other resources\n+    # matching the expressions below.\n+    # rules match both resources and specify what actions are allowed\n+    rules:\n+      - resources: [role]\n+        verbs: [list, create, read, update, delete]\n+\n+  # The deny section uses the identical format as the 'allow' section.\n+  # The deny rules always override allow rules.\n+  deny: {}\n+```\n+\n+We can spot several issues with this role design:\n+\n+* There is no way to specify the scope that role applies to, all roles apply globally to all resources they match and all users they assign to all the time.\n+* There is no way to specify resource hierarchies, like computing environments (env vs prod), which makes it hard to partition the infrastructure and forces customers to specify labels.\n+\n+\n+#### Labels\n+\n+Teleport RBAC\u2019s advice to engineers to partition their resource is to first, label their computing resource or use AWS labels, and second, match the labels in RBAC.\n+\n+Admins can set the tags on the resource configuration file statically, or, for some resources, use `server_info` to set the tags for each resource:\n+\n+```yaml\n+# server_info.yaml\n+kind: server_info\n+metadata:\n+   name: si-<node-name>\n+spec:\n+   new_labels:\n+      \"foo\": \"bar\"\n+```\n+\n+This creates several issues:\n+\n+* It is not always secure to delegate labeling to owners of computing resources, as anyone with root access to the node config file can update its labels impacting everyone else.\n+* It is not scalable, as it\u2019s not always useful to set and updates tags for each individual resource\n+* It\u2019s hard or impossible to partition infrastructure with two-dimensional labels, although users can use `env: prod` to mark all resources in the production environment, there is no way to say that `env: lab` is a subset of `env: prod`.\n+\n+#### Current Roles mapping\n+\n+In Teleport there are multiple ways to map roles to users: static roles mapping to local users and bots, \n+dynamic mapping to SSO via connectors and on-demand assignment via access lists and access requests.\n+\n+Most users start with local static mapping and SSO mapping, later graduating to access requests and access lists.\n+\n+https://goteleport.com/docs/access-controls/sso/#creating-an-authentication-connector\n+\n+For SSO users, on login, Teleport checks SSO connector, and maps attributes of a user to a list of roles:\n+\n+```yaml\n+# connector.yaml\n+kind: saml\n+version: v2\n+metadata:\n+  name: corporate\n+spec:\n+  attributes_to_roles:\n+    - {name: \"groups\", value: \"okta-admin\", roles: [\"access\"]}\n+     # regular expressions with capture are also supported. \n+     # the next line instructs Teleport\n+     # to assign users to roles `admin-1` if his SAML \"group\" \n+     # attribute equals 'ssh_admin_1':\n+   - { name: \"group\", value: \"^ssh_admin_(.*)$\", roles: [\"admin-$1\"] }\n+```\n+\n+This creates several security and scalability issues:\n+\n+* Every time administrators need to change roles assignment, they have to update the resource.\n+* Every time the Identity Provider changes user\u2019s attributes, the users would have to relogin to get new roles. \n+\n+For local users, administrators have to specify roles and traits in the local resource:\n+\n+```yaml\n+kind: user\n+version: v2\n+metadata:\n+  name: joe\n+spec:\n+  # roles is a list of roles assigned to this user\n+  roles:\n+  - admin\n+  # traits are key, list of values pairs assigned to a user resource.\n+  # Traits can be used in role templates as variables.\n+  traits:\n+    logins:\n+    - joe\n+    - root\n+```\n+\n+This also creates some challenges, as administrators are forced to update local resources each time they have to assign a user new permission.\n+\n+### Modifications\n+\n+Let\u2019s now introduce the missing pieces of the puzzle and review how new roles will simplify or deprecate legacy concepts.\n+\n+#### Hierarchical Resource Groups\n+\n+Resource groups are one such missing piece - in most cloud environments, resources are split into hierarchy, \n+for example, host `luna` is a member of a resource group `lab`, in turn a member of environment `prod`, which is in turn a member of a Teleport cluster.\n+\n+In Teleport, we will make cluster a default root of this hierarchy. Every computing resource by default will be a direct member of a root `cluster` resource group.\n+\n+One resource can be assigned to multiple resource groups at a time, or none. \n+\n+In our example, the cluster administrator would define lab resource group in the following way:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+  parent: prod_env\n+```\n+\n+Administrators can assign resources to resource groups:\n+\n+```yaml\n+kind: resource_group\n+metadata:\n+  name: lab\n+spec:\n+   parent: prod_env\n+ match_kinds:\n+   - node\n+   - database\n+   - role\n+   - access_list\n+   - '.*'\n+ match_labels:  \n+   env: prod\n+```\n+\n+In this case, any resource that matches `env:prod` label will be assigned to this resource group. We will use the same label matching algorithm as in today's `discovery_service`. \n+\n+This will let administrators to gradually migrate their existing flat infrastructure to resource groups one.\n+\n+In some cases it makes sense to specify parent resource group inline:\n+\n+```yaml\n+kind: role\n+spec:\n+  parent_resource_group: /env/prod\n+```\n+\n+By default, if unspecified, a resource is a member of a root-level - `/` cluster resource group. If specified by the resource, it won't be a member of a root `/` resource group.\n+\n+Resource groups are hierarchical, and we can refer to the `lab` resource group by its full path as `/env/prod/lab`. \n+\n+Most Teleport resources, with some exceptions, like users, SSO connectors can be a member of a resource group. \n+\n+We will list those resources separately below.\n+\n+##### Default Resource groups via auto-discovery\n+\n+Teleport can create resource groups if admins turn on auto discovery. This will significantly simplify configuration. \n+\n+Here are some of the resource groups that Teleport Discovery service will create:\n+\n+* For AWS, Teleport discovery service will place each computing resource in `/aws/[account-id]/[region]/[resource-type]/[resource-id]`.\n+  + When EKS auto-discovery is on, this hierarchy will include discovered apps - `/aws/account-id/[region]/k8s/[cluster-name]/namespaces/[namespace]/[app-id]`\n+* For Azure, Teleport will use Azure's hierarchy - `/azure/[management-group]/[subscription]/[resource-group]/[resource-type]/[resource-id]`\n+* For GCP, we will use GCP hierarchy of `/gcp/[organization]/[folder]/[project]/[resource-type]/[resource-id]`\n+\n+Discovery service will create and remove these hierarchies based on the cloud state, and will create resources with `parent_resource_group` field to place them in those resource groups.\n+\n+If users are not happy with a default hierarchy, they can create a different one.\n+\n+#### Access Lists\n+\n+Teleport has a concept of access lists, that lists an owner, members, and optionally a parent access list.\n+Access List in Teleport represents a group of users with a hierarchy. \n+\n+We will further assume that the root of this hierarchy is a cluster. \n+\n+Unlike in resource groups, a user can be an owner and a member of none, one or several access lists at once.\n+\n+In addition to that, access list grants a role to a set of members, like in this example:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: \"lab-engineers\"\n+spec:\n+  desc: \"Access list for lab engineers\"\n+  grants:\n+    roles: [access]\n+  members:\n+    - name: bob@example.com\n+```\n+\n+We will return to access lists later, but let\u2019s now recall that access lists contain a list of members, who are, in turn, granted one or several roles.\n+\n+#### Scopes\n+\n+By default, in Teleport a role is granted it applies to all resources in the Teleport cluster.\n+\n+However, with this change, will be able to grant a set of roles that apply only to resources that belong to a specific resource group. \n+\n+In this case, we will say that the roles apply at the scope of the resource group.\n+\n+Scopes define a set of resources roles apply to. \n+\n+We will introduce scopes in a couple of places, first, for access list:\n+\n+```yaml\n+kind: access_list\n+metadata:\n+  name: \"lab-engineers\"\n+spec:\n+  desc: \"Access list for lab engineers\"\n+  # this grant applies only at the scope of the resource group `/env/prod/lab`\n+  scopes: [\u2018/env/prod/lab']\n+  grants:\n+    roles: [access]\n+  members:\n+    - name: bob@example.com\n+```\n+\n+By default, all existing access lists will grant roles at the cluster scope, cascading to all resources, just like before the migration. \n+\n+However, going forward, admins will be able to set scopes to more granular levels.\n+\n+The second place where we introduce scopes is in the roles:\n+\n+```yaml\n+kind: role\n+metadata:\n+ name: access\n+spec:\n+  grantable_scopes: ['/env/prod']\n+```\n+\n+Grantable scopes specifiy maximum scope this role can be granted on. \n+\n+**Important:** By default, if the `grantable_scopes` are missing, we assume empty scope - that will prevent the role from being granted on any scopes. When migrating existing roles, we would set `/` - root scope to avoid breaking the cluster. ",
        "comment_created_at": "2025-05-06T09:14:29+00:00",
        "comment_author": "FireDrunk",
        "comment_body": "I think this might be backwards-incompatible when using a Terraform provider.\r\nThe scope field might get reset to 'null' by providers that explicitly check for drift.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2310234713",
    "pr_number": 57888,
    "pr_file": "rfd/0222-bot-instances-at-scale.md",
    "created_at": "2025-08-29T13:56:21+00:00",
    "commented_code": "+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine & Workload ID bots.\n+\n+It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine & Workload ID, it has become cumbersome to manage many bots and hundreds of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web app allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major version behind the control plane (thereby preventing a safe upgrade). To facilitate this, the filter will allow queries such as `semver_lt(version, \"18.1\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `semver_gte(version, \"18.1.0\") && semver_lt(version, \"19\")`.\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. A filter such as `contains(notices, \"Proxy URL not set\")` will be pre-populated.\n+\n+**As a cluster owner (Infrastructure Security team), I'd like to be able to figure out what a Bot Instance is for/who it belongs to when making decisions (e.g can we upgrade/break this Bot safely).**\n+\n+How bot/instances are tagged with an owner, organizational area or purpose will be the subject of a later phase of delivery. As such it is left undefined for the time being. It may simply be a series of optional fields (or labels) which can be included in the `tbot` configuration, via flags or environment variables. These would be sent along with heartbeats and be visible in the web UI.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. Either of these values can be used to filter the instances list, and should make finding the relevant instance easy.\n+\n+Once found, the instance can be selected to view instance details. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure.\n+\n+### Instances dashboard\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+\n+![](assets/0222-details-overview.png)\n+\n+![](assets/0222-details-services.png)\n+\n+![](assets/0222-details-notices.png)\n+\n+![](assets/0222-details-config.png)\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance. A custom language parser will be used to provide instance-specific functions such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `semver_lt(version, 18.1)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `semver_gte(version, \"18\") && semver_lt(version, \"18.1\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Proto Specification\n+\n+### Heartbeat protobuf additions\n+\n+``` protobuf\n+message BotInstanceStatusHeartbeat {\n+\t// ...[snip]...\n+\n+\t// Kind identifies whether the bot is running in the tbot binary or embedded\n+\t// in another component.\n+\tBotKind kind = 10;\n+\n+\t// Notices emitted since the last heartbeat.\n+\t//\n+\t// The server will clear any previous notices if `is_startup` is true, so that\n+\t// editing tbot's configuration and restarting it clears any warnings from a\n+\t// previous bad configuration.\n+\trepeated BotInstanceNotice notices = 11;\n+\n+\t// Snapshots of the bot instance services' health.\n+  repeated BotInstanceServiceStatus service_statuses = 12;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 13;",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "2310234713",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2310234713",
        "commented_code": "@@ -0,0 +1,367 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine & Workload ID bots.\n+\n+It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine & Workload ID, it has become cumbersome to manage many bots and hundreds of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web app allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major version behind the control plane (thereby preventing a safe upgrade). To facilitate this, the filter will allow queries such as `semver_lt(version, \"18.1\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `semver_gte(version, \"18.1.0\") && semver_lt(version, \"19\")`.\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. A filter such as `contains(notices, \"Proxy URL not set\")` will be pre-populated.\n+\n+**As a cluster owner (Infrastructure Security team), I'd like to be able to figure out what a Bot Instance is for/who it belongs to when making decisions (e.g can we upgrade/break this Bot safely).**\n+\n+How bot/instances are tagged with an owner, organizational area or purpose will be the subject of a later phase of delivery. As such it is left undefined for the time being. It may simply be a series of optional fields (or labels) which can be included in the `tbot` configuration, via flags or environment variables. These would be sent along with heartbeats and be visible in the web UI.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. Either of these values can be used to filter the instances list, and should make finding the relevant instance easy.\n+\n+Once found, the instance can be selected to view instance details. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure.\n+\n+### Instances dashboard\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+\n+![](assets/0222-details-overview.png)\n+\n+![](assets/0222-details-services.png)\n+\n+![](assets/0222-details-notices.png)\n+\n+![](assets/0222-details-config.png)\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance. A custom language parser will be used to provide instance-specific functions such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `semver_lt(version, 18.1)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `semver_gte(version, \"18\") && semver_lt(version, \"18.1\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Proto Specification\n+\n+### Heartbeat protobuf additions\n+\n+``` protobuf\n+message BotInstanceStatusHeartbeat {\n+\t// ...[snip]...\n+\n+\t// Kind identifies whether the bot is running in the tbot binary or embedded\n+\t// in another component.\n+\tBotKind kind = 10;\n+\n+\t// Notices emitted since the last heartbeat.\n+\t//\n+\t// The server will clear any previous notices if `is_startup` is true, so that\n+\t// editing tbot's configuration and restarting it clears any warnings from a\n+\t// previous bad configuration.\n+\trepeated BotInstanceNotice notices = 11;\n+\n+\t// Snapshots of the bot instance services' health.\n+  repeated BotInstanceServiceStatus service_statuses = 12;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 13;",
        "comment_created_at": "2025-08-29T13:56:21+00:00",
        "comment_author": "rosstimothy",
        "comment_body": "What would happen if I have a very large and verbose configuration?\r\n\r\n- Could this cause a heartbeat to exceed gRPC max message sizes?\r\n- Could this cause s heartbeat to exceed backend item size limits?",
        "pr_file_module": null
      },
      {
        "comment_id": "2313687382",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2310234713",
        "commented_code": "@@ -0,0 +1,367 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine & Workload ID bots.\n+\n+It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine & Workload ID, it has become cumbersome to manage many bots and hundreds of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web app allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major version behind the control plane (thereby preventing a safe upgrade). To facilitate this, the filter will allow queries such as `semver_lt(version, \"18.1\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `semver_gte(version, \"18.1.0\") && semver_lt(version, \"19\")`.\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. A filter such as `contains(notices, \"Proxy URL not set\")` will be pre-populated.\n+\n+**As a cluster owner (Infrastructure Security team), I'd like to be able to figure out what a Bot Instance is for/who it belongs to when making decisions (e.g can we upgrade/break this Bot safely).**\n+\n+How bot/instances are tagged with an owner, organizational area or purpose will be the subject of a later phase of delivery. As such it is left undefined for the time being. It may simply be a series of optional fields (or labels) which can be included in the `tbot` configuration, via flags or environment variables. These would be sent along with heartbeats and be visible in the web UI.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. Either of these values can be used to filter the instances list, and should make finding the relevant instance easy.\n+\n+Once found, the instance can be selected to view instance details. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure.\n+\n+### Instances dashboard\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+\n+![](assets/0222-details-overview.png)\n+\n+![](assets/0222-details-services.png)\n+\n+![](assets/0222-details-notices.png)\n+\n+![](assets/0222-details-config.png)\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance. A custom language parser will be used to provide instance-specific functions such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `semver_lt(version, 18.1)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `semver_gte(version, \"18\") && semver_lt(version, \"18.1\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Proto Specification\n+\n+### Heartbeat protobuf additions\n+\n+``` protobuf\n+message BotInstanceStatusHeartbeat {\n+\t// ...[snip]...\n+\n+\t// Kind identifies whether the bot is running in the tbot binary or embedded\n+\t// in another component.\n+\tBotKind kind = 10;\n+\n+\t// Notices emitted since the last heartbeat.\n+\t//\n+\t// The server will clear any previous notices if `is_startup` is true, so that\n+\t// editing tbot's configuration and restarting it clears any warnings from a\n+\t// previous bad configuration.\n+\trepeated BotInstanceNotice notices = 11;\n+\n+\t// Snapshots of the bot instance services' health.\n+  repeated BotInstanceServiceStatus service_statuses = 12;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 13;",
        "comment_created_at": "2025-09-01T11:24:05+00:00",
        "comment_author": "boxofrad",
        "comment_body": "Good points. We'll use `proto.Size` to calculate the config size and then omit it if it's over 1MiB (possibly even smaller?)",
        "pr_file_module": null
      },
      {
        "comment_id": "2319525009",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2310234713",
        "commented_code": "@@ -0,0 +1,367 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine & Workload ID bots.\n+\n+It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine & Workload ID, it has become cumbersome to manage many bots and hundreds of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web app allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major version behind the control plane (thereby preventing a safe upgrade). To facilitate this, the filter will allow queries such as `semver_lt(version, \"18.1\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `semver_gte(version, \"18.1.0\") && semver_lt(version, \"19\")`.\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. A filter such as `contains(notices, \"Proxy URL not set\")` will be pre-populated.\n+\n+**As a cluster owner (Infrastructure Security team), I'd like to be able to figure out what a Bot Instance is for/who it belongs to when making decisions (e.g can we upgrade/break this Bot safely).**\n+\n+How bot/instances are tagged with an owner, organizational area or purpose will be the subject of a later phase of delivery. As such it is left undefined for the time being. It may simply be a series of optional fields (or labels) which can be included in the `tbot` configuration, via flags or environment variables. These would be sent along with heartbeats and be visible in the web UI.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. Either of these values can be used to filter the instances list, and should make finding the relevant instance easy.\n+\n+Once found, the instance can be selected to view instance details. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure.\n+\n+### Instances dashboard\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+\n+![](assets/0222-details-overview.png)\n+\n+![](assets/0222-details-services.png)\n+\n+![](assets/0222-details-notices.png)\n+\n+![](assets/0222-details-config.png)\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance. A custom language parser will be used to provide instance-specific functions such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `semver_lt(version, 18.1)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `semver_gte(version, \"18\") && semver_lt(version, \"18.1\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Proto Specification\n+\n+### Heartbeat protobuf additions\n+\n+``` protobuf\n+message BotInstanceStatusHeartbeat {\n+\t// ...[snip]...\n+\n+\t// Kind identifies whether the bot is running in the tbot binary or embedded\n+\t// in another component.\n+\tBotKind kind = 10;\n+\n+\t// Notices emitted since the last heartbeat.\n+\t//\n+\t// The server will clear any previous notices if `is_startup` is true, so that\n+\t// editing tbot's configuration and restarting it clears any warnings from a\n+\t// previous bad configuration.\n+\trepeated BotInstanceNotice notices = 11;\n+\n+\t// Snapshots of the bot instance services' health.\n+  repeated BotInstanceServiceStatus service_statuses = 12;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 13;",
        "comment_created_at": "2025-09-03T16:39:16+00:00",
        "comment_author": "espadolini",
        "comment_body": "Writing a single backend item at 1MB in dynamodb would burn one full second of guaranteed write capacity for the whole cluster state if not for the fact that the size per document is hard limited to 400KiB. If your item might be bigger than a handful of KB, especially if it's at all periodically written and/or potentially existing in a large amount, you should probably outright not write it in the backend at all.",
        "pr_file_module": null
      },
      {
        "comment_id": "2319528810",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2310234713",
        "commented_code": "@@ -0,0 +1,367 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine & Workload ID bots.\n+\n+It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine & Workload ID, it has become cumbersome to manage many bots and hundreds of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web app allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major version behind the control plane (thereby preventing a safe upgrade). To facilitate this, the filter will allow queries such as `semver_lt(version, \"18.1\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `semver_gte(version, \"18.1.0\") && semver_lt(version, \"19\")`.\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. A filter such as `contains(notices, \"Proxy URL not set\")` will be pre-populated.\n+\n+**As a cluster owner (Infrastructure Security team), I'd like to be able to figure out what a Bot Instance is for/who it belongs to when making decisions (e.g can we upgrade/break this Bot safely).**\n+\n+How bot/instances are tagged with an owner, organizational area or purpose will be the subject of a later phase of delivery. As such it is left undefined for the time being. It may simply be a series of optional fields (or labels) which can be included in the `tbot` configuration, via flags or environment variables. These would be sent along with heartbeats and be visible in the web UI.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. Either of these values can be used to filter the instances list, and should make finding the relevant instance easy.\n+\n+Once found, the instance can be selected to view instance details. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure.\n+\n+### Instances dashboard\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+\n+![](assets/0222-details-overview.png)\n+\n+![](assets/0222-details-services.png)\n+\n+![](assets/0222-details-notices.png)\n+\n+![](assets/0222-details-config.png)\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance. A custom language parser will be used to provide instance-specific functions such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `semver_lt(version, 18.1)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `semver_gte(version, \"18\") && semver_lt(version, \"18.1\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Proto Specification\n+\n+### Heartbeat protobuf additions\n+\n+``` protobuf\n+message BotInstanceStatusHeartbeat {\n+\t// ...[snip]...\n+\n+\t// Kind identifies whether the bot is running in the tbot binary or embedded\n+\t// in another component.\n+\tBotKind kind = 10;\n+\n+\t// Notices emitted since the last heartbeat.\n+\t//\n+\t// The server will clear any previous notices if `is_startup` is true, so that\n+\t// editing tbot's configuration and restarting it clears any warnings from a\n+\t// previous bad configuration.\n+\trepeated BotInstanceNotice notices = 11;\n+\n+\t// Snapshots of the bot instance services' health.\n+  repeated BotInstanceServiceStatus service_statuses = 12;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 13;",
        "comment_created_at": "2025-09-03T16:41:07+00:00",
        "comment_author": "espadolini",
        "comment_body": "Speaking of the bot configuration, could we consider writing it in a separate item, if we absolutely have to store it in the cluster state?\r\n\r\nIs it possible that the bot configuration contains sensitive items (paths, for example) that should _not_ be pushed to the cluster?",
        "pr_file_module": null
      },
      {
        "comment_id": "2322678535",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2310234713",
        "commented_code": "@@ -0,0 +1,367 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine & Workload ID bots.\n+\n+It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine & Workload ID, it has become cumbersome to manage many bots and hundreds of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web app allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major version behind the control plane (thereby preventing a safe upgrade). To facilitate this, the filter will allow queries such as `semver_lt(version, \"18.1\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `semver_gte(version, \"18.1.0\") && semver_lt(version, \"19\")`.\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. A filter such as `contains(notices, \"Proxy URL not set\")` will be pre-populated.\n+\n+**As a cluster owner (Infrastructure Security team), I'd like to be able to figure out what a Bot Instance is for/who it belongs to when making decisions (e.g can we upgrade/break this Bot safely).**\n+\n+How bot/instances are tagged with an owner, organizational area or purpose will be the subject of a later phase of delivery. As such it is left undefined for the time being. It may simply be a series of optional fields (or labels) which can be included in the `tbot` configuration, via flags or environment variables. These would be sent along with heartbeats and be visible in the web UI.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. Either of these values can be used to filter the instances list, and should make finding the relevant instance easy.\n+\n+Once found, the instance can be selected to view instance details. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure.\n+\n+### Instances dashboard\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+\n+![](assets/0222-details-overview.png)\n+\n+![](assets/0222-details-services.png)\n+\n+![](assets/0222-details-notices.png)\n+\n+![](assets/0222-details-config.png)\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance. A custom language parser will be used to provide instance-specific functions such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `semver_lt(version, 18.1)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `semver_gte(version, \"18\") && semver_lt(version, \"18.1\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Proto Specification\n+\n+### Heartbeat protobuf additions\n+\n+``` protobuf\n+message BotInstanceStatusHeartbeat {\n+\t// ...[snip]...\n+\n+\t// Kind identifies whether the bot is running in the tbot binary or embedded\n+\t// in another component.\n+\tBotKind kind = 10;\n+\n+\t// Notices emitted since the last heartbeat.\n+\t//\n+\t// The server will clear any previous notices if `is_startup` is true, so that\n+\t// editing tbot's configuration and restarting it clears any warnings from a\n+\t// previous bad configuration.\n+\trepeated BotInstanceNotice notices = 11;\n+\n+\t// Snapshots of the bot instance services' health.\n+  repeated BotInstanceServiceStatus service_statuses = 12;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 13;",
        "comment_created_at": "2025-09-04T16:03:22+00:00",
        "comment_author": "nicholasmarais1158",
        "comment_body": "Perhaps `tbot` should limit the config to <400Kb then. @boxofrad Do you envisage a problem lowering the limit?\r\n\r\n> Speaking of the bot configuration, could we consider writing it in a separate item, if we absolutely have to store it in the cluster state?\r\n\r\nI've added a section which covers what will be stored where. tl;dr config is stored as a separate items, as you suggested.\r\n\r\n> Is it possible that the bot configuration contains sensitive items (paths, for example) that should not be pushed to the cluster?\r\n\r\nIt's possible. There are a number of user-defined values and names that could hold something sensitive (like \"my-super-secret-business-critical-project\"). We feel this is mitigated well enough, as you'd need to be a logged-in and authorised user to access this info.",
        "pr_file_module": null
      },
      {
        "comment_id": "2322750015",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2310234713",
        "commented_code": "@@ -0,0 +1,367 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine & Workload ID bots.\n+\n+It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine & Workload ID, it has become cumbersome to manage many bots and hundreds of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web app allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major version behind the control plane (thereby preventing a safe upgrade). To facilitate this, the filter will allow queries such as `semver_lt(version, \"18.1\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `semver_gte(version, \"18.1.0\") && semver_lt(version, \"19\")`.\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. A filter such as `contains(notices, \"Proxy URL not set\")` will be pre-populated.\n+\n+**As a cluster owner (Infrastructure Security team), I'd like to be able to figure out what a Bot Instance is for/who it belongs to when making decisions (e.g can we upgrade/break this Bot safely).**\n+\n+How bot/instances are tagged with an owner, organizational area or purpose will be the subject of a later phase of delivery. As such it is left undefined for the time being. It may simply be a series of optional fields (or labels) which can be included in the `tbot` configuration, via flags or environment variables. These would be sent along with heartbeats and be visible in the web UI.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. Either of these values can be used to filter the instances list, and should make finding the relevant instance easy.\n+\n+Once found, the instance can be selected to view instance details. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure.\n+\n+### Instances dashboard\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+\n+![](assets/0222-details-overview.png)\n+\n+![](assets/0222-details-services.png)\n+\n+![](assets/0222-details-notices.png)\n+\n+![](assets/0222-details-config.png)\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance. A custom language parser will be used to provide instance-specific functions such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `semver_lt(version, 18.1)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `semver_gte(version, \"18\") && semver_lt(version, \"18.1\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Proto Specification\n+\n+### Heartbeat protobuf additions\n+\n+``` protobuf\n+message BotInstanceStatusHeartbeat {\n+\t// ...[snip]...\n+\n+\t// Kind identifies whether the bot is running in the tbot binary or embedded\n+\t// in another component.\n+\tBotKind kind = 10;\n+\n+\t// Notices emitted since the last heartbeat.\n+\t//\n+\t// The server will clear any previous notices if `is_startup` is true, so that\n+\t// editing tbot's configuration and restarting it clears any warnings from a\n+\t// previous bad configuration.\n+\trepeated BotInstanceNotice notices = 11;\n+\n+\t// Snapshots of the bot instance services' health.\n+  repeated BotInstanceServiceStatus service_statuses = 12;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 13;",
        "comment_created_at": "2025-09-04T16:32:21+00:00",
        "comment_author": "nicholasmarais1158",
        "comment_body": "Updated to a 320Kb limit on yaml config size based on DynamoDB limits and accounting for yaml to json conversion.",
        "pr_file_module": null
      },
      {
        "comment_id": "2324649388",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2310234713",
        "commented_code": "@@ -0,0 +1,367 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine & Workload ID bots.\n+\n+It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine & Workload ID, it has become cumbersome to manage many bots and hundreds of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web app allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major version behind the control plane (thereby preventing a safe upgrade). To facilitate this, the filter will allow queries such as `semver_lt(version, \"18.1\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `semver_gte(version, \"18.1.0\") && semver_lt(version, \"19\")`.\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. A filter such as `contains(notices, \"Proxy URL not set\")` will be pre-populated.\n+\n+**As a cluster owner (Infrastructure Security team), I'd like to be able to figure out what a Bot Instance is for/who it belongs to when making decisions (e.g can we upgrade/break this Bot safely).**\n+\n+How bot/instances are tagged with an owner, organizational area or purpose will be the subject of a later phase of delivery. As such it is left undefined for the time being. It may simply be a series of optional fields (or labels) which can be included in the `tbot` configuration, via flags or environment variables. These would be sent along with heartbeats and be visible in the web UI.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. Either of these values can be used to filter the instances list, and should make finding the relevant instance easy.\n+\n+Once found, the instance can be selected to view instance details. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure.\n+\n+### Instances dashboard\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+\n+![](assets/0222-details-overview.png)\n+\n+![](assets/0222-details-services.png)\n+\n+![](assets/0222-details-notices.png)\n+\n+![](assets/0222-details-config.png)\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance. A custom language parser will be used to provide instance-specific functions such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `semver_lt(version, 18.1)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `semver_gte(version, \"18\") && semver_lt(version, \"18.1\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Proto Specification\n+\n+### Heartbeat protobuf additions\n+\n+``` protobuf\n+message BotInstanceStatusHeartbeat {\n+\t// ...[snip]...\n+\n+\t// Kind identifies whether the bot is running in the tbot binary or embedded\n+\t// in another component.\n+\tBotKind kind = 10;\n+\n+\t// Notices emitted since the last heartbeat.\n+\t//\n+\t// The server will clear any previous notices if `is_startup` is true, so that\n+\t// editing tbot's configuration and restarting it clears any warnings from a\n+\t// previous bad configuration.\n+\trepeated BotInstanceNotice notices = 11;\n+\n+\t// Snapshots of the bot instance services' health.\n+  repeated BotInstanceServiceStatus service_statuses = 12;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 13;",
        "comment_created_at": "2025-09-05T09:58:49+00:00",
        "comment_author": "strideynet",
        "comment_body": "> Updated to a 320Kb limit on yaml config size based on DynamoDB limits and accounting for yaml to json conversion.\r\n\r\nPerhaps erring on the side of caution, I don't think it would be a major concession if we used a more conservative limit. If I take my largest/most complex `tbot` configuration (which is probably much more complex than a normal configuraiton), it comes out to around 2KiB. Perhaps 32KiB would be a more reasonable starting point and if we find people running into those limits, we can bump these in the future.\r\n\r\nFrom my understanding, the failure mode if someone hits the limit is fairly graceful (we emit a notice saying their configuration was too large to heartbeat), but the failure mode if we hit the write capacity of dynamodb is degradation of cluster performance.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2316397048",
    "pr_number": 57888,
    "pr_file": "rfd/0222-bot-instances-at-scale.md",
    "created_at": "2025-09-02T15:12:11+00:00",
    "commented_code": "+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+Machine ID bots allow non-human users to access resources in a Teleport cluster. They are useful for automating tasks, such as CI/CD runners, monitoring systems, and running scripts. The bot resource encapsulates a bot's access rights and roles. The `tbot` binary is used to start an instance of a bot and enrol it with the cluster (using a join token). When `tbot` starts and receives its credentials, a bot instance record is created in the cluster. Bot instance records expire when their credentials expire. Bot instances can be long-running processes which periodically renew their credentials, or short-lived, ephemeral processes which are created and destroyed on demand. Many instances of the same bot can exist at once, and clusters can have many thousands of bot instances.\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine ID bots. It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine ID, it has become cumbersome to manage many bots and thousands of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web UI allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major versions behind the control plane (thereby preventing a safe upgrade). To facilitate this in the web UI, the filter will allow queries such as `older_than(version, \"18.1.0\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible). For the CLI, the `query` flag can be used to filter instances (e.g. `tctl bots instances ls --query=older_than(version, \"18.1.0\")`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently in the web UI the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `newer_than_or_equal(version, \"18.1.0\") && older_than(version, \"19.0.0\")`. This works though the web UI and the CLI (`tctl`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. An advanced filter such as `contains(notices, \"Proxy URL not set\")` will be applied.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. The instance ID can be used to filter the instances list in the web UI, and should make finding the relevant instance easy. In the CLI (`tctl`), both the bot name and instance ID are required to perform the look-up; `tctl bots instances show [bot_name]/[instance_id]`.\n+\n+Once found, in the web UI or CLI, the instance's details can be seen. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure. Notices are raised by `tbot` for actionable items that require attention, such as invalid config or deprecations.\n+\n+### Instances dashboard\n+\n+This mock-up shows the bot instance page as it would be when first arriving at the page. No filters are set, so all instances are available in the list. Results are paginated, and the default sort order is by recency - instances with the most recent heartbeat are displayed first. Sorting can be toggled between ascending and descending, and the following sort fields are available; recency, version, hostname. Filtering can be performed using a basic text search over supported fields, or an advanced search using the Teleport predicate language.\n+\n+The right-hand half of the page displays the dashboard, which is a summary over all instances. The visualizations use aggregate data prepared and updated by the auth server. An indication of when the data current is provided, as well as a refresh button which retrieves the most recently available data. A selection of timeframes is also available.\n+\n+The Activity visualization shows the number of events (joins, authentications, and heartbeats) that occurred over time. This can be used to identify trends and patterns in activity. It is non-interactive.\n+\n+The Upgrade Status visualization show a summary of all instances grouped by upgrade status; up-to-date, upgrade available, patch available, or incompatible. Status labels are selectable, and will populate the advanced search with the appropriate filter. For example, if the auth server is running v18 the filter will be populated with `older_than(version, \"16.0.0\")` when a user selects \"not supported\".\n+\n+The Notices visualization shows a summary of all notices across all instances. They're conveniently grouped by notice title and a count is included. Each item is selectable, and will apply an advanced filter (e.g. `contains(notices, \"Proxy URL not set\")`). This visualization will be hidden if there is no data to display.\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+These mock-ups shows the state of the page once an item had been selected from the instances list by clicking it. The dashboard section is replaced by the selected instance's details.\n+\n+The overview tab is displayed by default when an instance is selected. It shows further information about the instance, the join token that was used to enrol, and a summary of service health.\n+\n+![](assets/0222-details-overview.png)\n+\n+The services tab shows a list of all configured services (or outputs). Each includes it's name, type and health status. If there is extra health info available (such as and error message), this is also displayed.\n+\n+![](assets/0222-details-services.png)\n+\n+The notices tab is a listing of all notices raised since the instance was started. Notices have a type, a title and a message body. Some notice types contain extra data such as a planned removal version for deprecation notices. An indication of when the notice was raised is included, and notices are ordered by most recent first and this is not user-configurable.\n+\n+![](assets/0222-details-notices.png)\n+\n+The configuration tab show the _effective_ `tbot` configuration as readonly yaml.\n+\n+![](assets/0222-details-config.png)\n+\n+### tctl bots instances ls --search [term] --query [tql]\n+\n+The list bot instances command will include extra information about each instance; version, health status and notices count. A search term or advanced query can be used to filter the results - in this case a filter summary is included below the results confirming the applied filter and giving a result count. The data is not paginated and all instances are returned and displayed.\n+\n+```diff\n+- ID                                         Join Method Hostname      Joined               Last Seen            Generation\n+- ------------------------------------------ ----------- ------------- -------------------- -------------------- ----------\n+- bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         ip-10-0-15-34 2025-08-29T06:09:26Z 2025-09-01T12:49:26Z 237\n+-\n++ ID                                         Join Method Version Hostname      Status    Notices Last Seen\n++ ------------------------------------------ ----------- ------- ------------- --------- ------- --------------------\n++ bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         v18.2.1 ip-10-0-15-34 UNHEALTHY 6       2025-09-01T12:49:26Z\n++\n++ Filter:\n++ Search text: \"ip-10-0-15\"\n++ Query: older_than(version, \"18.0.0\")\n++ Results: 128\n++\n+To view more information on a particular instance, run:\n+\n+> /Users/nick.marais/.tsh/bin/tctl bots instances show [id]\n+```\n+\n+### tctl bots instances show [id]\n+\n+The show bot instance command gives an overall health summary as well as a listing of services and their respective health status. A list of notices is also included.\n+\n+```diff\n+Bot: w2w-demo-app-bot\n+ID:  d83b381d-b46c-4b92-a899-755991a6d0f5\n++ Status: UNHEALTHY\n+\n+Initial Authentication:\n+  Authenticated At: 2025-08-29T06:09:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       1\n+  Public Key:       <178 bytes>\n+\n+Latest Authentication:\n+  Authenticated At: 2025-09-01T12:49:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       237\n+  Public Key:       <178 bytes>\n+\n+Latest Heartbeat:\n+  Recorded At:  2025-09-01T12:39:26Z\n+  Is Startup:   false\n+  Version:      18.1.5\n+  Hostname:     ip-10-0-15-34\n+  Uptime:       78h30m0.539099441s\n+  Join Method:  iam\n+  One Shot:     false\n+  Architecture: arm64\n+  OS:           linux\n++\n++ Service status:\n++ Status    Name        Type                Reason         Updated At\n++ --------- ----------- ------------------- -------------- --------------------\n++ UNHEALTHY prod-aws-01 X509-output-service out of bananas 2025-09-01T12:49:26Z\n++\n++ Notices:\n++ Type                Service     Message               Raised At\n++ ------------------- ----------- --------------------- --------------------\n++ DEPRECATION_WARNING prod-aws-01 Lorem ipsum (v19.0.0) 2025-09-01T12:49:26Z\n+\n+To view a full, machine-readable record including past heartbeats and authentication records, run:\n+> /Users/nick.marais/.tsh/bin/tctl get bot_instance/w2w-demo-app-bot/d83b381d-b46c-4b92-a899-755991a6d0f5\n+\n+To onboard a new instance for this bot, run:\n+> /Users/nick.marais/.tsh/bin/tctl bots instances add w2w-demo-app-bot\n+```\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance.\n+\n+Instance-specific functions will be supported by implementing a custom `typical.ParserSpec`, such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `older_than(version, 18.1.0)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `newer_than_or_equal(version, \"18.0.0\") && older_than(version, \"18.1.0\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Heartbeat additions\n+\n+```protobuf\n+message BotInstanceStatusHeartbeat {\n+  // ...[snip]...\n+\n+  // The health of the services/output `tbot` is running.\n+  repeated BotInstanceServiceHealth service_health = 10;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 11;\n+\n+  // Kind identifies whether the bot is running in the tbot binary or embedded\n+  // in another component.\n+  BotKind kind = 12;\n+\n+  // Notices emitted since the last heartbeat.\n+  //\n+  // The server will clear any previous notices if `is_startup` is true, so that\n+  // editing tbot's configuration and restarting it clears any warnings from a\n+  // previous bad configuration.\n+  repeated BotInstanceNotice notices = 13;\n+}\n+\n+// BotKind identifies whether the bot is the tbot binary or embedded in another\n+// component.\n+enum BotKind {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_KIND_UNSET = 0;\n+\n+  // Means the bot is running the tbot binary.\n+  BOT_KIND_TBOT_BINARY = 1;\n+\n+  // Means the bot is running inside the Teleport Terraform provider.\n+  BOT_KIND_TERRAFORM_PROVIDER = 2;\n+\n+  // Means the bot is running inside the Teleport Kubernetes operator.\n+  BOT_KIND_KUBERNETES_OPERATOR = 3;\n+}\n+\n+// BotInstanceNotice contains an error message, deprecation warning, etc. emitted\n+// by the bot instance.\n+message BotInstanceNotice {\n+  // ID is a client-generated identifier (i.e. UUID) that can be used by the\n+  // auth server to detect and discard duplicate notices caused by partially\n+  // failed heartbeat RPCs.\n+  string id = 1;\n+\n+  // Type of notice (e.g. deprecation or warning).\n+  BotInstanceNoticeType type = 1;\n+\n+  // Service this notice relates to (or nil if it relates to the bot instance\n+  // more generally).\n+  optional BotInstanceService service = 2;\n+\n+  // Timestamp at which this notice was emitted.\n+  google.protobuf.Timestamp timestamp = 3;\n+\n+  oneof notice {\n+    // Deprecation warning details.\n+    BotInstanceDeprecationWarning deprecation_warning = 4;\n+\n+    // Generic message text.\n+    string message = 5;\n+  }\n+}\n+\n+// BotInstanceNoticeType identifies the type of notice.\n+enum BotInstanceNoticeType {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_INSTANCE_NOTICE_TYPE_UNSPECIFIED = 0;\n+\n+  // Means the notice contains a warning that the user is using a configuration\n+  // option that will be removed in a future release.\n+  BOT_INSTANCE_NOTICE_TYPE_DEPRECATION_WARNING = 1;\n+\n+  // Means the notice contains a generic error message.\n+  BOT_INSTANCE_NOTICE_TYPE_MESSAGE = 2;\n+}\n+\n+// BotInstanceDeprecationWarning contains the details of a deprecation warning.\n+message BotInstanceDeprecationWarning {\n+  // Message explaining the deprecation.\n+  string message = 1;\n+\n+  // The major version in which the deprecated configuration will no longer work.\n+  string removal_version = 2;\n+}\n+\n+// BotInstanceHealthStatus describes the healthiness of a `tbot` service.\n+enum BotInstanceHealthStatus {\n+  // The enum zero-value, it means no status was included.\n+  BOT_INSTANCE_HEALTH_STATUS_UNSPECIFIED = 0;\n+\n+  // Means the service is still \"starting up\" and hasn't reported its status.\n+  BOT_INSTANCE_HEALTH_STATUS_INITIALIZING = 1;\n+\n+  // Means the service is healthy and ready to serve traffic, or it has\n+  // recently succeeded in generating an output.\n+  BOT_INSTANCE_HEALTH_STATUS_HEALTHY = 2;\n+\n+  // Means the service is failing to serve traffic or generate output.\n+  BOT_INSTANCE_HEALTH_STATUS_UNHEALTHY = 3;\n+}\n+\n+// BotInstanceServiceIdentifier uniquely identifies a `tbot` service.\n+message BotInstanceServiceIdentifier {\n+  // Type of service (e.g. database-tunnel, ssh-multiplexer).\n+  string type = 1;\n+\n+  // Name of the service, either given by the user or auto-generated.\n+  string name = 2;\n+}\n+\n+// BotInstanceServiceHealth is a snapshot of a `tbot` service's health.\n+message BotInstanceServiceHealth {\n+  // Service identifies the service.\n+  BotInstanceServiceIdentifier service = 1;\n+\n+  // Status describes the service's healthiness.\n+  BotInstanceHealthStatus status = 2;\n+\n+  // Reason is a human-readable explanation for the service's status. It might\n+  // include an error message.\n+  optional string reason = 3;\n+\n+  // UpdatedAt is the time at which the service's health last changed.\n+  google.protobuf.Timestamp updated_at = 4;\n+}\n+```\n+\n+### Data fields and expected quantities\n+\n+| Field | Description | Example | Quantity | Limitations |\n+| --- | --- | --- | --- | --- |\n+| Bot | A collection of roles and access assumed by `tbot` using a join token |  | 0-300+ per cluster |  |\n+| Bot instance | A unique joined instance of `tbot` in either a long-running or ephemeral environment |  | 1-300+ per bot |  |\n+| Authentication record | Created for each join or renewal |  | 0-10 per instance (max enforced) |  |\n+| Instance heartbeat | Self-reported by each bot instance |  | 0-10 per instance (max enforced) | Data is **not** validated by the auth server, and cannot be used for making access decisions. |\n+| Service | An independent, internal part of `tbot`. Generally maps 1:1 with configured outputs/tunnels. | `application-tunnel`, `workload-identity-api` | 1-30+ per heartbeat |  |\n+| Notice | An item created by `tbot` to capture an unusual event, configuration warning, or important status |  | 0-100+ per heartbeat |  |\n+| OS | Operating system from `runtime.GOOS` | linux, windows or darwin | Once per heartbeat |  |\n+| Version | Version of `tbot` | 18.1.0 | Once per heartbeat |  |\n+| Hostname |  |  | Once per heartbeat |  |\n+| Uptime | How long `tbot` has been running |  | Once per heartbeat |  |\n+| Raw config | `tbot`\u2019s local config (combination of YAML and CLI flags) as a protobuf struct |  | Once per heartbeat |  |\n+| Join token name |  |  | Once per auth |  |\n+| Join method |  | github, iam, kubernetes | Once per auth |  |\n+| Join attributes | Metadata specific to a join method | GitHub repository name | Once per auth |  |\n+| Health status |  | INITIALIZING, HEALTHY, UNHEALTHY,\n+UNKNOWN | Once per service |  |",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "2316397048",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2316397048",
        "commented_code": "@@ -0,0 +1,464 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+Machine ID bots allow non-human users to access resources in a Teleport cluster. They are useful for automating tasks, such as CI/CD runners, monitoring systems, and running scripts. The bot resource encapsulates a bot's access rights and roles. The `tbot` binary is used to start an instance of a bot and enrol it with the cluster (using a join token). When `tbot` starts and receives its credentials, a bot instance record is created in the cluster. Bot instance records expire when their credentials expire. Bot instances can be long-running processes which periodically renew their credentials, or short-lived, ephemeral processes which are created and destroyed on demand. Many instances of the same bot can exist at once, and clusters can have many thousands of bot instances.\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine ID bots. It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine ID, it has become cumbersome to manage many bots and thousands of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web UI allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major versions behind the control plane (thereby preventing a safe upgrade). To facilitate this in the web UI, the filter will allow queries such as `older_than(version, \"18.1.0\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible). For the CLI, the `query` flag can be used to filter instances (e.g. `tctl bots instances ls --query=older_than(version, \"18.1.0\")`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently in the web UI the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `newer_than_or_equal(version, \"18.1.0\") && older_than(version, \"19.0.0\")`. This works though the web UI and the CLI (`tctl`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. An advanced filter such as `contains(notices, \"Proxy URL not set\")` will be applied.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. The instance ID can be used to filter the instances list in the web UI, and should make finding the relevant instance easy. In the CLI (`tctl`), both the bot name and instance ID are required to perform the look-up; `tctl bots instances show [bot_name]/[instance_id]`.\n+\n+Once found, in the web UI or CLI, the instance's details can be seen. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure. Notices are raised by `tbot` for actionable items that require attention, such as invalid config or deprecations.\n+\n+### Instances dashboard\n+\n+This mock-up shows the bot instance page as it would be when first arriving at the page. No filters are set, so all instances are available in the list. Results are paginated, and the default sort order is by recency - instances with the most recent heartbeat are displayed first. Sorting can be toggled between ascending and descending, and the following sort fields are available; recency, version, hostname. Filtering can be performed using a basic text search over supported fields, or an advanced search using the Teleport predicate language.\n+\n+The right-hand half of the page displays the dashboard, which is a summary over all instances. The visualizations use aggregate data prepared and updated by the auth server. An indication of when the data current is provided, as well as a refresh button which retrieves the most recently available data. A selection of timeframes is also available.\n+\n+The Activity visualization shows the number of events (joins, authentications, and heartbeats) that occurred over time. This can be used to identify trends and patterns in activity. It is non-interactive.\n+\n+The Upgrade Status visualization show a summary of all instances grouped by upgrade status; up-to-date, upgrade available, patch available, or incompatible. Status labels are selectable, and will populate the advanced search with the appropriate filter. For example, if the auth server is running v18 the filter will be populated with `older_than(version, \"16.0.0\")` when a user selects \"not supported\".\n+\n+The Notices visualization shows a summary of all notices across all instances. They're conveniently grouped by notice title and a count is included. Each item is selectable, and will apply an advanced filter (e.g. `contains(notices, \"Proxy URL not set\")`). This visualization will be hidden if there is no data to display.\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+These mock-ups shows the state of the page once an item had been selected from the instances list by clicking it. The dashboard section is replaced by the selected instance's details.\n+\n+The overview tab is displayed by default when an instance is selected. It shows further information about the instance, the join token that was used to enrol, and a summary of service health.\n+\n+![](assets/0222-details-overview.png)\n+\n+The services tab shows a list of all configured services (or outputs). Each includes it's name, type and health status. If there is extra health info available (such as and error message), this is also displayed.\n+\n+![](assets/0222-details-services.png)\n+\n+The notices tab is a listing of all notices raised since the instance was started. Notices have a type, a title and a message body. Some notice types contain extra data such as a planned removal version for deprecation notices. An indication of when the notice was raised is included, and notices are ordered by most recent first and this is not user-configurable.\n+\n+![](assets/0222-details-notices.png)\n+\n+The configuration tab show the _effective_ `tbot` configuration as readonly yaml.\n+\n+![](assets/0222-details-config.png)\n+\n+### tctl bots instances ls --search [term] --query [tql]\n+\n+The list bot instances command will include extra information about each instance; version, health status and notices count. A search term or advanced query can be used to filter the results - in this case a filter summary is included below the results confirming the applied filter and giving a result count. The data is not paginated and all instances are returned and displayed.\n+\n+```diff\n+- ID                                         Join Method Hostname      Joined               Last Seen            Generation\n+- ------------------------------------------ ----------- ------------- -------------------- -------------------- ----------\n+- bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         ip-10-0-15-34 2025-08-29T06:09:26Z 2025-09-01T12:49:26Z 237\n+-\n++ ID                                         Join Method Version Hostname      Status    Notices Last Seen\n++ ------------------------------------------ ----------- ------- ------------- --------- ------- --------------------\n++ bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         v18.2.1 ip-10-0-15-34 UNHEALTHY 6       2025-09-01T12:49:26Z\n++\n++ Filter:\n++ Search text: \"ip-10-0-15\"\n++ Query: older_than(version, \"18.0.0\")\n++ Results: 128\n++\n+To view more information on a particular instance, run:\n+\n+> /Users/nick.marais/.tsh/bin/tctl bots instances show [id]\n+```\n+\n+### tctl bots instances show [id]\n+\n+The show bot instance command gives an overall health summary as well as a listing of services and their respective health status. A list of notices is also included.\n+\n+```diff\n+Bot: w2w-demo-app-bot\n+ID:  d83b381d-b46c-4b92-a899-755991a6d0f5\n++ Status: UNHEALTHY\n+\n+Initial Authentication:\n+  Authenticated At: 2025-08-29T06:09:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       1\n+  Public Key:       <178 bytes>\n+\n+Latest Authentication:\n+  Authenticated At: 2025-09-01T12:49:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       237\n+  Public Key:       <178 bytes>\n+\n+Latest Heartbeat:\n+  Recorded At:  2025-09-01T12:39:26Z\n+  Is Startup:   false\n+  Version:      18.1.5\n+  Hostname:     ip-10-0-15-34\n+  Uptime:       78h30m0.539099441s\n+  Join Method:  iam\n+  One Shot:     false\n+  Architecture: arm64\n+  OS:           linux\n++\n++ Service status:\n++ Status    Name        Type                Reason         Updated At\n++ --------- ----------- ------------------- -------------- --------------------\n++ UNHEALTHY prod-aws-01 X509-output-service out of bananas 2025-09-01T12:49:26Z\n++\n++ Notices:\n++ Type                Service     Message               Raised At\n++ ------------------- ----------- --------------------- --------------------\n++ DEPRECATION_WARNING prod-aws-01 Lorem ipsum (v19.0.0) 2025-09-01T12:49:26Z\n+\n+To view a full, machine-readable record including past heartbeats and authentication records, run:\n+> /Users/nick.marais/.tsh/bin/tctl get bot_instance/w2w-demo-app-bot/d83b381d-b46c-4b92-a899-755991a6d0f5\n+\n+To onboard a new instance for this bot, run:\n+> /Users/nick.marais/.tsh/bin/tctl bots instances add w2w-demo-app-bot\n+```\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance.\n+\n+Instance-specific functions will be supported by implementing a custom `typical.ParserSpec`, such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `older_than(version, 18.1.0)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `newer_than_or_equal(version, \"18.0.0\") && older_than(version, \"18.1.0\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Heartbeat additions\n+\n+```protobuf\n+message BotInstanceStatusHeartbeat {\n+  // ...[snip]...\n+\n+  // The health of the services/output `tbot` is running.\n+  repeated BotInstanceServiceHealth service_health = 10;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 11;\n+\n+  // Kind identifies whether the bot is running in the tbot binary or embedded\n+  // in another component.\n+  BotKind kind = 12;\n+\n+  // Notices emitted since the last heartbeat.\n+  //\n+  // The server will clear any previous notices if `is_startup` is true, so that\n+  // editing tbot's configuration and restarting it clears any warnings from a\n+  // previous bad configuration.\n+  repeated BotInstanceNotice notices = 13;\n+}\n+\n+// BotKind identifies whether the bot is the tbot binary or embedded in another\n+// component.\n+enum BotKind {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_KIND_UNSET = 0;\n+\n+  // Means the bot is running the tbot binary.\n+  BOT_KIND_TBOT_BINARY = 1;\n+\n+  // Means the bot is running inside the Teleport Terraform provider.\n+  BOT_KIND_TERRAFORM_PROVIDER = 2;\n+\n+  // Means the bot is running inside the Teleport Kubernetes operator.\n+  BOT_KIND_KUBERNETES_OPERATOR = 3;\n+}\n+\n+// BotInstanceNotice contains an error message, deprecation warning, etc. emitted\n+// by the bot instance.\n+message BotInstanceNotice {\n+  // ID is a client-generated identifier (i.e. UUID) that can be used by the\n+  // auth server to detect and discard duplicate notices caused by partially\n+  // failed heartbeat RPCs.\n+  string id = 1;\n+\n+  // Type of notice (e.g. deprecation or warning).\n+  BotInstanceNoticeType type = 1;\n+\n+  // Service this notice relates to (or nil if it relates to the bot instance\n+  // more generally).\n+  optional BotInstanceService service = 2;\n+\n+  // Timestamp at which this notice was emitted.\n+  google.protobuf.Timestamp timestamp = 3;\n+\n+  oneof notice {\n+    // Deprecation warning details.\n+    BotInstanceDeprecationWarning deprecation_warning = 4;\n+\n+    // Generic message text.\n+    string message = 5;\n+  }\n+}\n+\n+// BotInstanceNoticeType identifies the type of notice.\n+enum BotInstanceNoticeType {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_INSTANCE_NOTICE_TYPE_UNSPECIFIED = 0;\n+\n+  // Means the notice contains a warning that the user is using a configuration\n+  // option that will be removed in a future release.\n+  BOT_INSTANCE_NOTICE_TYPE_DEPRECATION_WARNING = 1;\n+\n+  // Means the notice contains a generic error message.\n+  BOT_INSTANCE_NOTICE_TYPE_MESSAGE = 2;\n+}\n+\n+// BotInstanceDeprecationWarning contains the details of a deprecation warning.\n+message BotInstanceDeprecationWarning {\n+  // Message explaining the deprecation.\n+  string message = 1;\n+\n+  // The major version in which the deprecated configuration will no longer work.\n+  string removal_version = 2;\n+}\n+\n+// BotInstanceHealthStatus describes the healthiness of a `tbot` service.\n+enum BotInstanceHealthStatus {\n+  // The enum zero-value, it means no status was included.\n+  BOT_INSTANCE_HEALTH_STATUS_UNSPECIFIED = 0;\n+\n+  // Means the service is still \"starting up\" and hasn't reported its status.\n+  BOT_INSTANCE_HEALTH_STATUS_INITIALIZING = 1;\n+\n+  // Means the service is healthy and ready to serve traffic, or it has\n+  // recently succeeded in generating an output.\n+  BOT_INSTANCE_HEALTH_STATUS_HEALTHY = 2;\n+\n+  // Means the service is failing to serve traffic or generate output.\n+  BOT_INSTANCE_HEALTH_STATUS_UNHEALTHY = 3;\n+}\n+\n+// BotInstanceServiceIdentifier uniquely identifies a `tbot` service.\n+message BotInstanceServiceIdentifier {\n+  // Type of service (e.g. database-tunnel, ssh-multiplexer).\n+  string type = 1;\n+\n+  // Name of the service, either given by the user or auto-generated.\n+  string name = 2;\n+}\n+\n+// BotInstanceServiceHealth is a snapshot of a `tbot` service's health.\n+message BotInstanceServiceHealth {\n+  // Service identifies the service.\n+  BotInstanceServiceIdentifier service = 1;\n+\n+  // Status describes the service's healthiness.\n+  BotInstanceHealthStatus status = 2;\n+\n+  // Reason is a human-readable explanation for the service's status. It might\n+  // include an error message.\n+  optional string reason = 3;\n+\n+  // UpdatedAt is the time at which the service's health last changed.\n+  google.protobuf.Timestamp updated_at = 4;\n+}\n+```\n+\n+### Data fields and expected quantities\n+\n+| Field | Description | Example | Quantity | Limitations |\n+| --- | --- | --- | --- | --- |\n+| Bot | A collection of roles and access assumed by `tbot` using a join token |  | 0-300+ per cluster |  |\n+| Bot instance | A unique joined instance of `tbot` in either a long-running or ephemeral environment |  | 1-300+ per bot |  |\n+| Authentication record | Created for each join or renewal |  | 0-10 per instance (max enforced) |  |\n+| Instance heartbeat | Self-reported by each bot instance |  | 0-10 per instance (max enforced) | Data is **not** validated by the auth server, and cannot be used for making access decisions. |\n+| Service | An independent, internal part of `tbot`. Generally maps 1:1 with configured outputs/tunnels. | `application-tunnel`, `workload-identity-api` | 1-30+ per heartbeat |  |\n+| Notice | An item created by `tbot` to capture an unusual event, configuration warning, or important status |  | 0-100+ per heartbeat |  |\n+| OS | Operating system from `runtime.GOOS` | linux, windows or darwin | Once per heartbeat |  |\n+| Version | Version of `tbot` | 18.1.0 | Once per heartbeat |  |\n+| Hostname |  |  | Once per heartbeat |  |\n+| Uptime | How long `tbot` has been running |  | Once per heartbeat |  |\n+| Raw config | `tbot`\u2019s local config (combination of YAML and CLI flags) as a protobuf struct |  | Once per heartbeat |  |\n+| Join token name |  |  | Once per auth |  |\n+| Join method |  | github, iam, kubernetes | Once per auth |  |\n+| Join attributes | Metadata specific to a join method | GitHub repository name | Once per auth |  |\n+| Health status |  | INITIALIZING, HEALTHY, UNHEALTHY,\n+UNKNOWN | Once per service |  |",
        "comment_created_at": "2025-09-02T15:12:11+00:00",
        "comment_author": "thedevelopnik",
        "comment_body": "To make sure I understand correctly:\r\n* For privacy reasons we will not send along unrecognized config\r\n* But we do want to highlight when there is unrecognized config\r\n* UNKNOWN is the representation that there is invalid config(?)\r\n* But I don't see it in the additions above, it it already part of the heartbeat message?\r\n\r\nI think this story is told across a couple of paragraphs spread through the RFD, but exactly how it will be expressed is unclear to me.\r\n\r\nI get how missing required fields or deprecated fields will show up, but a common problem is people typo a service name or put it at the wrong yaml level, and so the rest of the config is valid but there's one part we just ignore, and then the service doesn't show up and it's unclear why. Calling out that there's config we skipped (even if we don't send the details since we can't be sure it's not private extra information) would be helpful.",
        "pr_file_module": null
      },
      {
        "comment_id": "2316499416",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2316397048",
        "commented_code": "@@ -0,0 +1,464 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+Machine ID bots allow non-human users to access resources in a Teleport cluster. They are useful for automating tasks, such as CI/CD runners, monitoring systems, and running scripts. The bot resource encapsulates a bot's access rights and roles. The `tbot` binary is used to start an instance of a bot and enrol it with the cluster (using a join token). When `tbot` starts and receives its credentials, a bot instance record is created in the cluster. Bot instance records expire when their credentials expire. Bot instances can be long-running processes which periodically renew their credentials, or short-lived, ephemeral processes which are created and destroyed on demand. Many instances of the same bot can exist at once, and clusters can have many thousands of bot instances.\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine ID bots. It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine ID, it has become cumbersome to manage many bots and thousands of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web UI allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major versions behind the control plane (thereby preventing a safe upgrade). To facilitate this in the web UI, the filter will allow queries such as `older_than(version, \"18.1.0\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible). For the CLI, the `query` flag can be used to filter instances (e.g. `tctl bots instances ls --query=older_than(version, \"18.1.0\")`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently in the web UI the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `newer_than_or_equal(version, \"18.1.0\") && older_than(version, \"19.0.0\")`. This works though the web UI and the CLI (`tctl`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. An advanced filter such as `contains(notices, \"Proxy URL not set\")` will be applied.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. The instance ID can be used to filter the instances list in the web UI, and should make finding the relevant instance easy. In the CLI (`tctl`), both the bot name and instance ID are required to perform the look-up; `tctl bots instances show [bot_name]/[instance_id]`.\n+\n+Once found, in the web UI or CLI, the instance's details can be seen. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure. Notices are raised by `tbot` for actionable items that require attention, such as invalid config or deprecations.\n+\n+### Instances dashboard\n+\n+This mock-up shows the bot instance page as it would be when first arriving at the page. No filters are set, so all instances are available in the list. Results are paginated, and the default sort order is by recency - instances with the most recent heartbeat are displayed first. Sorting can be toggled between ascending and descending, and the following sort fields are available; recency, version, hostname. Filtering can be performed using a basic text search over supported fields, or an advanced search using the Teleport predicate language.\n+\n+The right-hand half of the page displays the dashboard, which is a summary over all instances. The visualizations use aggregate data prepared and updated by the auth server. An indication of when the data current is provided, as well as a refresh button which retrieves the most recently available data. A selection of timeframes is also available.\n+\n+The Activity visualization shows the number of events (joins, authentications, and heartbeats) that occurred over time. This can be used to identify trends and patterns in activity. It is non-interactive.\n+\n+The Upgrade Status visualization show a summary of all instances grouped by upgrade status; up-to-date, upgrade available, patch available, or incompatible. Status labels are selectable, and will populate the advanced search with the appropriate filter. For example, if the auth server is running v18 the filter will be populated with `older_than(version, \"16.0.0\")` when a user selects \"not supported\".\n+\n+The Notices visualization shows a summary of all notices across all instances. They're conveniently grouped by notice title and a count is included. Each item is selectable, and will apply an advanced filter (e.g. `contains(notices, \"Proxy URL not set\")`). This visualization will be hidden if there is no data to display.\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+These mock-ups shows the state of the page once an item had been selected from the instances list by clicking it. The dashboard section is replaced by the selected instance's details.\n+\n+The overview tab is displayed by default when an instance is selected. It shows further information about the instance, the join token that was used to enrol, and a summary of service health.\n+\n+![](assets/0222-details-overview.png)\n+\n+The services tab shows a list of all configured services (or outputs). Each includes it's name, type and health status. If there is extra health info available (such as and error message), this is also displayed.\n+\n+![](assets/0222-details-services.png)\n+\n+The notices tab is a listing of all notices raised since the instance was started. Notices have a type, a title and a message body. Some notice types contain extra data such as a planned removal version for deprecation notices. An indication of when the notice was raised is included, and notices are ordered by most recent first and this is not user-configurable.\n+\n+![](assets/0222-details-notices.png)\n+\n+The configuration tab show the _effective_ `tbot` configuration as readonly yaml.\n+\n+![](assets/0222-details-config.png)\n+\n+### tctl bots instances ls --search [term] --query [tql]\n+\n+The list bot instances command will include extra information about each instance; version, health status and notices count. A search term or advanced query can be used to filter the results - in this case a filter summary is included below the results confirming the applied filter and giving a result count. The data is not paginated and all instances are returned and displayed.\n+\n+```diff\n+- ID                                         Join Method Hostname      Joined               Last Seen            Generation\n+- ------------------------------------------ ----------- ------------- -------------------- -------------------- ----------\n+- bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         ip-10-0-15-34 2025-08-29T06:09:26Z 2025-09-01T12:49:26Z 237\n+-\n++ ID                                         Join Method Version Hostname      Status    Notices Last Seen\n++ ------------------------------------------ ----------- ------- ------------- --------- ------- --------------------\n++ bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         v18.2.1 ip-10-0-15-34 UNHEALTHY 6       2025-09-01T12:49:26Z\n++\n++ Filter:\n++ Search text: \"ip-10-0-15\"\n++ Query: older_than(version, \"18.0.0\")\n++ Results: 128\n++\n+To view more information on a particular instance, run:\n+\n+> /Users/nick.marais/.tsh/bin/tctl bots instances show [id]\n+```\n+\n+### tctl bots instances show [id]\n+\n+The show bot instance command gives an overall health summary as well as a listing of services and their respective health status. A list of notices is also included.\n+\n+```diff\n+Bot: w2w-demo-app-bot\n+ID:  d83b381d-b46c-4b92-a899-755991a6d0f5\n++ Status: UNHEALTHY\n+\n+Initial Authentication:\n+  Authenticated At: 2025-08-29T06:09:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       1\n+  Public Key:       <178 bytes>\n+\n+Latest Authentication:\n+  Authenticated At: 2025-09-01T12:49:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       237\n+  Public Key:       <178 bytes>\n+\n+Latest Heartbeat:\n+  Recorded At:  2025-09-01T12:39:26Z\n+  Is Startup:   false\n+  Version:      18.1.5\n+  Hostname:     ip-10-0-15-34\n+  Uptime:       78h30m0.539099441s\n+  Join Method:  iam\n+  One Shot:     false\n+  Architecture: arm64\n+  OS:           linux\n++\n++ Service status:\n++ Status    Name        Type                Reason         Updated At\n++ --------- ----------- ------------------- -------------- --------------------\n++ UNHEALTHY prod-aws-01 X509-output-service out of bananas 2025-09-01T12:49:26Z\n++\n++ Notices:\n++ Type                Service     Message               Raised At\n++ ------------------- ----------- --------------------- --------------------\n++ DEPRECATION_WARNING prod-aws-01 Lorem ipsum (v19.0.0) 2025-09-01T12:49:26Z\n+\n+To view a full, machine-readable record including past heartbeats and authentication records, run:\n+> /Users/nick.marais/.tsh/bin/tctl get bot_instance/w2w-demo-app-bot/d83b381d-b46c-4b92-a899-755991a6d0f5\n+\n+To onboard a new instance for this bot, run:\n+> /Users/nick.marais/.tsh/bin/tctl bots instances add w2w-demo-app-bot\n+```\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance.\n+\n+Instance-specific functions will be supported by implementing a custom `typical.ParserSpec`, such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `older_than(version, 18.1.0)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `newer_than_or_equal(version, \"18.0.0\") && older_than(version, \"18.1.0\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Heartbeat additions\n+\n+```protobuf\n+message BotInstanceStatusHeartbeat {\n+  // ...[snip]...\n+\n+  // The health of the services/output `tbot` is running.\n+  repeated BotInstanceServiceHealth service_health = 10;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 11;\n+\n+  // Kind identifies whether the bot is running in the tbot binary or embedded\n+  // in another component.\n+  BotKind kind = 12;\n+\n+  // Notices emitted since the last heartbeat.\n+  //\n+  // The server will clear any previous notices if `is_startup` is true, so that\n+  // editing tbot's configuration and restarting it clears any warnings from a\n+  // previous bad configuration.\n+  repeated BotInstanceNotice notices = 13;\n+}\n+\n+// BotKind identifies whether the bot is the tbot binary or embedded in another\n+// component.\n+enum BotKind {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_KIND_UNSET = 0;\n+\n+  // Means the bot is running the tbot binary.\n+  BOT_KIND_TBOT_BINARY = 1;\n+\n+  // Means the bot is running inside the Teleport Terraform provider.\n+  BOT_KIND_TERRAFORM_PROVIDER = 2;\n+\n+  // Means the bot is running inside the Teleport Kubernetes operator.\n+  BOT_KIND_KUBERNETES_OPERATOR = 3;\n+}\n+\n+// BotInstanceNotice contains an error message, deprecation warning, etc. emitted\n+// by the bot instance.\n+message BotInstanceNotice {\n+  // ID is a client-generated identifier (i.e. UUID) that can be used by the\n+  // auth server to detect and discard duplicate notices caused by partially\n+  // failed heartbeat RPCs.\n+  string id = 1;\n+\n+  // Type of notice (e.g. deprecation or warning).\n+  BotInstanceNoticeType type = 1;\n+\n+  // Service this notice relates to (or nil if it relates to the bot instance\n+  // more generally).\n+  optional BotInstanceService service = 2;\n+\n+  // Timestamp at which this notice was emitted.\n+  google.protobuf.Timestamp timestamp = 3;\n+\n+  oneof notice {\n+    // Deprecation warning details.\n+    BotInstanceDeprecationWarning deprecation_warning = 4;\n+\n+    // Generic message text.\n+    string message = 5;\n+  }\n+}\n+\n+// BotInstanceNoticeType identifies the type of notice.\n+enum BotInstanceNoticeType {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_INSTANCE_NOTICE_TYPE_UNSPECIFIED = 0;\n+\n+  // Means the notice contains a warning that the user is using a configuration\n+  // option that will be removed in a future release.\n+  BOT_INSTANCE_NOTICE_TYPE_DEPRECATION_WARNING = 1;\n+\n+  // Means the notice contains a generic error message.\n+  BOT_INSTANCE_NOTICE_TYPE_MESSAGE = 2;\n+}\n+\n+// BotInstanceDeprecationWarning contains the details of a deprecation warning.\n+message BotInstanceDeprecationWarning {\n+  // Message explaining the deprecation.\n+  string message = 1;\n+\n+  // The major version in which the deprecated configuration will no longer work.\n+  string removal_version = 2;\n+}\n+\n+// BotInstanceHealthStatus describes the healthiness of a `tbot` service.\n+enum BotInstanceHealthStatus {\n+  // The enum zero-value, it means no status was included.\n+  BOT_INSTANCE_HEALTH_STATUS_UNSPECIFIED = 0;\n+\n+  // Means the service is still \"starting up\" and hasn't reported its status.\n+  BOT_INSTANCE_HEALTH_STATUS_INITIALIZING = 1;\n+\n+  // Means the service is healthy and ready to serve traffic, or it has\n+  // recently succeeded in generating an output.\n+  BOT_INSTANCE_HEALTH_STATUS_HEALTHY = 2;\n+\n+  // Means the service is failing to serve traffic or generate output.\n+  BOT_INSTANCE_HEALTH_STATUS_UNHEALTHY = 3;\n+}\n+\n+// BotInstanceServiceIdentifier uniquely identifies a `tbot` service.\n+message BotInstanceServiceIdentifier {\n+  // Type of service (e.g. database-tunnel, ssh-multiplexer).\n+  string type = 1;\n+\n+  // Name of the service, either given by the user or auto-generated.\n+  string name = 2;\n+}\n+\n+// BotInstanceServiceHealth is a snapshot of a `tbot` service's health.\n+message BotInstanceServiceHealth {\n+  // Service identifies the service.\n+  BotInstanceServiceIdentifier service = 1;\n+\n+  // Status describes the service's healthiness.\n+  BotInstanceHealthStatus status = 2;\n+\n+  // Reason is a human-readable explanation for the service's status. It might\n+  // include an error message.\n+  optional string reason = 3;\n+\n+  // UpdatedAt is the time at which the service's health last changed.\n+  google.protobuf.Timestamp updated_at = 4;\n+}\n+```\n+\n+### Data fields and expected quantities\n+\n+| Field | Description | Example | Quantity | Limitations |\n+| --- | --- | --- | --- | --- |\n+| Bot | A collection of roles and access assumed by `tbot` using a join token |  | 0-300+ per cluster |  |\n+| Bot instance | A unique joined instance of `tbot` in either a long-running or ephemeral environment |  | 1-300+ per bot |  |\n+| Authentication record | Created for each join or renewal |  | 0-10 per instance (max enforced) |  |\n+| Instance heartbeat | Self-reported by each bot instance |  | 0-10 per instance (max enforced) | Data is **not** validated by the auth server, and cannot be used for making access decisions. |\n+| Service | An independent, internal part of `tbot`. Generally maps 1:1 with configured outputs/tunnels. | `application-tunnel`, `workload-identity-api` | 1-30+ per heartbeat |  |\n+| Notice | An item created by `tbot` to capture an unusual event, configuration warning, or important status |  | 0-100+ per heartbeat |  |\n+| OS | Operating system from `runtime.GOOS` | linux, windows or darwin | Once per heartbeat |  |\n+| Version | Version of `tbot` | 18.1.0 | Once per heartbeat |  |\n+| Hostname |  |  | Once per heartbeat |  |\n+| Uptime | How long `tbot` has been running |  | Once per heartbeat |  |\n+| Raw config | `tbot`\u2019s local config (combination of YAML and CLI flags) as a protobuf struct |  | Once per heartbeat |  |\n+| Join token name |  |  | Once per auth |  |\n+| Join method |  | github, iam, kubernetes | Once per auth |  |\n+| Join attributes | Metadata specific to a join method | GitHub repository name | Once per auth |  |\n+| Health status |  | INITIALIZING, HEALTHY, UNHEALTHY,\n+UNKNOWN | Once per service |  |",
        "comment_created_at": "2025-09-02T15:47:52+00:00",
        "comment_author": "strideynet",
        "comment_body": "> UNKNOWN is the representation that there is invalid config(?)\r\n\r\nIIRC health status is unrelated to the configuration and more related to the runtime health of the service. `UNKNOWN` is effectively just a fall-back state where the service has not yet indicated some status - I'd expect this to be pretty rare.\r\n\r\n> For privacy reasons we will not send along unrecognized config\r\n\r\nIt's more to do with whether we send the raw YAML of the configuration file OR send the parsed \"live\" configuration. There's a few reasons in my mind why sending the parsed \"live\" configuration is preferable:\r\n\r\n- Primarily, it takes into account other forms of configuration beyond files. Sending the raw YAML would not indicate if environment variables or CLI args were overriding the configuration whereas the \"live\" configuration will indicate what options have actually been applied.\r\n- Sending the parsed configuration allows us to much more easily redact or trim specific fields of the configuration. Which is good for avoiding us sending sensitive information but also for properly handling extremely large configurations.\r\n\r\nThe only \"downside\" of sending the live configuration is that we'd be none-the-wiser when it comes to people having made some syntactic errors within the configuration file. However, I think in most cases this is mostly a moot point - configuring a service that does not exist or making grave syntactic errors will prevent `tbot` itself from starting and getting to the point where it can heartbeat.",
        "pr_file_module": null
      },
      {
        "comment_id": "2316993080",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2316397048",
        "commented_code": "@@ -0,0 +1,464 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+Machine ID bots allow non-human users to access resources in a Teleport cluster. They are useful for automating tasks, such as CI/CD runners, monitoring systems, and running scripts. The bot resource encapsulates a bot's access rights and roles. The `tbot` binary is used to start an instance of a bot and enrol it with the cluster (using a join token). When `tbot` starts and receives its credentials, a bot instance record is created in the cluster. Bot instance records expire when their credentials expire. Bot instances can be long-running processes which periodically renew their credentials, or short-lived, ephemeral processes which are created and destroyed on demand. Many instances of the same bot can exist at once, and clusters can have many thousands of bot instances.\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine ID bots. It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine ID, it has become cumbersome to manage many bots and thousands of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web UI allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major versions behind the control plane (thereby preventing a safe upgrade). To facilitate this in the web UI, the filter will allow queries such as `older_than(version, \"18.1.0\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible). For the CLI, the `query` flag can be used to filter instances (e.g. `tctl bots instances ls --query=older_than(version, \"18.1.0\")`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently in the web UI the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `newer_than_or_equal(version, \"18.1.0\") && older_than(version, \"19.0.0\")`. This works though the web UI and the CLI (`tctl`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. An advanced filter such as `contains(notices, \"Proxy URL not set\")` will be applied.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. The instance ID can be used to filter the instances list in the web UI, and should make finding the relevant instance easy. In the CLI (`tctl`), both the bot name and instance ID are required to perform the look-up; `tctl bots instances show [bot_name]/[instance_id]`.\n+\n+Once found, in the web UI or CLI, the instance's details can be seen. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure. Notices are raised by `tbot` for actionable items that require attention, such as invalid config or deprecations.\n+\n+### Instances dashboard\n+\n+This mock-up shows the bot instance page as it would be when first arriving at the page. No filters are set, so all instances are available in the list. Results are paginated, and the default sort order is by recency - instances with the most recent heartbeat are displayed first. Sorting can be toggled between ascending and descending, and the following sort fields are available; recency, version, hostname. Filtering can be performed using a basic text search over supported fields, or an advanced search using the Teleport predicate language.\n+\n+The right-hand half of the page displays the dashboard, which is a summary over all instances. The visualizations use aggregate data prepared and updated by the auth server. An indication of when the data current is provided, as well as a refresh button which retrieves the most recently available data. A selection of timeframes is also available.\n+\n+The Activity visualization shows the number of events (joins, authentications, and heartbeats) that occurred over time. This can be used to identify trends and patterns in activity. It is non-interactive.\n+\n+The Upgrade Status visualization show a summary of all instances grouped by upgrade status; up-to-date, upgrade available, patch available, or incompatible. Status labels are selectable, and will populate the advanced search with the appropriate filter. For example, if the auth server is running v18 the filter will be populated with `older_than(version, \"16.0.0\")` when a user selects \"not supported\".\n+\n+The Notices visualization shows a summary of all notices across all instances. They're conveniently grouped by notice title and a count is included. Each item is selectable, and will apply an advanced filter (e.g. `contains(notices, \"Proxy URL not set\")`). This visualization will be hidden if there is no data to display.\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+These mock-ups shows the state of the page once an item had been selected from the instances list by clicking it. The dashboard section is replaced by the selected instance's details.\n+\n+The overview tab is displayed by default when an instance is selected. It shows further information about the instance, the join token that was used to enrol, and a summary of service health.\n+\n+![](assets/0222-details-overview.png)\n+\n+The services tab shows a list of all configured services (or outputs). Each includes it's name, type and health status. If there is extra health info available (such as and error message), this is also displayed.\n+\n+![](assets/0222-details-services.png)\n+\n+The notices tab is a listing of all notices raised since the instance was started. Notices have a type, a title and a message body. Some notice types contain extra data such as a planned removal version for deprecation notices. An indication of when the notice was raised is included, and notices are ordered by most recent first and this is not user-configurable.\n+\n+![](assets/0222-details-notices.png)\n+\n+The configuration tab show the _effective_ `tbot` configuration as readonly yaml.\n+\n+![](assets/0222-details-config.png)\n+\n+### tctl bots instances ls --search [term] --query [tql]\n+\n+The list bot instances command will include extra information about each instance; version, health status and notices count. A search term or advanced query can be used to filter the results - in this case a filter summary is included below the results confirming the applied filter and giving a result count. The data is not paginated and all instances are returned and displayed.\n+\n+```diff\n+- ID                                         Join Method Hostname      Joined               Last Seen            Generation\n+- ------------------------------------------ ----------- ------------- -------------------- -------------------- ----------\n+- bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         ip-10-0-15-34 2025-08-29T06:09:26Z 2025-09-01T12:49:26Z 237\n+-\n++ ID                                         Join Method Version Hostname      Status    Notices Last Seen\n++ ------------------------------------------ ----------- ------- ------------- --------- ------- --------------------\n++ bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         v18.2.1 ip-10-0-15-34 UNHEALTHY 6       2025-09-01T12:49:26Z\n++\n++ Filter:\n++ Search text: \"ip-10-0-15\"\n++ Query: older_than(version, \"18.0.0\")\n++ Results: 128\n++\n+To view more information on a particular instance, run:\n+\n+> /Users/nick.marais/.tsh/bin/tctl bots instances show [id]\n+```\n+\n+### tctl bots instances show [id]\n+\n+The show bot instance command gives an overall health summary as well as a listing of services and their respective health status. A list of notices is also included.\n+\n+```diff\n+Bot: w2w-demo-app-bot\n+ID:  d83b381d-b46c-4b92-a899-755991a6d0f5\n++ Status: UNHEALTHY\n+\n+Initial Authentication:\n+  Authenticated At: 2025-08-29T06:09:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       1\n+  Public Key:       <178 bytes>\n+\n+Latest Authentication:\n+  Authenticated At: 2025-09-01T12:49:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       237\n+  Public Key:       <178 bytes>\n+\n+Latest Heartbeat:\n+  Recorded At:  2025-09-01T12:39:26Z\n+  Is Startup:   false\n+  Version:      18.1.5\n+  Hostname:     ip-10-0-15-34\n+  Uptime:       78h30m0.539099441s\n+  Join Method:  iam\n+  One Shot:     false\n+  Architecture: arm64\n+  OS:           linux\n++\n++ Service status:\n++ Status    Name        Type                Reason         Updated At\n++ --------- ----------- ------------------- -------------- --------------------\n++ UNHEALTHY prod-aws-01 X509-output-service out of bananas 2025-09-01T12:49:26Z\n++\n++ Notices:\n++ Type                Service     Message               Raised At\n++ ------------------- ----------- --------------------- --------------------\n++ DEPRECATION_WARNING prod-aws-01 Lorem ipsum (v19.0.0) 2025-09-01T12:49:26Z\n+\n+To view a full, machine-readable record including past heartbeats and authentication records, run:\n+> /Users/nick.marais/.tsh/bin/tctl get bot_instance/w2w-demo-app-bot/d83b381d-b46c-4b92-a899-755991a6d0f5\n+\n+To onboard a new instance for this bot, run:\n+> /Users/nick.marais/.tsh/bin/tctl bots instances add w2w-demo-app-bot\n+```\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance.\n+\n+Instance-specific functions will be supported by implementing a custom `typical.ParserSpec`, such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `older_than(version, 18.1.0)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `newer_than_or_equal(version, \"18.0.0\") && older_than(version, \"18.1.0\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Heartbeat additions\n+\n+```protobuf\n+message BotInstanceStatusHeartbeat {\n+  // ...[snip]...\n+\n+  // The health of the services/output `tbot` is running.\n+  repeated BotInstanceServiceHealth service_health = 10;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 11;\n+\n+  // Kind identifies whether the bot is running in the tbot binary or embedded\n+  // in another component.\n+  BotKind kind = 12;\n+\n+  // Notices emitted since the last heartbeat.\n+  //\n+  // The server will clear any previous notices if `is_startup` is true, so that\n+  // editing tbot's configuration and restarting it clears any warnings from a\n+  // previous bad configuration.\n+  repeated BotInstanceNotice notices = 13;\n+}\n+\n+// BotKind identifies whether the bot is the tbot binary or embedded in another\n+// component.\n+enum BotKind {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_KIND_UNSET = 0;\n+\n+  // Means the bot is running the tbot binary.\n+  BOT_KIND_TBOT_BINARY = 1;\n+\n+  // Means the bot is running inside the Teleport Terraform provider.\n+  BOT_KIND_TERRAFORM_PROVIDER = 2;\n+\n+  // Means the bot is running inside the Teleport Kubernetes operator.\n+  BOT_KIND_KUBERNETES_OPERATOR = 3;\n+}\n+\n+// BotInstanceNotice contains an error message, deprecation warning, etc. emitted\n+// by the bot instance.\n+message BotInstanceNotice {\n+  // ID is a client-generated identifier (i.e. UUID) that can be used by the\n+  // auth server to detect and discard duplicate notices caused by partially\n+  // failed heartbeat RPCs.\n+  string id = 1;\n+\n+  // Type of notice (e.g. deprecation or warning).\n+  BotInstanceNoticeType type = 1;\n+\n+  // Service this notice relates to (or nil if it relates to the bot instance\n+  // more generally).\n+  optional BotInstanceService service = 2;\n+\n+  // Timestamp at which this notice was emitted.\n+  google.protobuf.Timestamp timestamp = 3;\n+\n+  oneof notice {\n+    // Deprecation warning details.\n+    BotInstanceDeprecationWarning deprecation_warning = 4;\n+\n+    // Generic message text.\n+    string message = 5;\n+  }\n+}\n+\n+// BotInstanceNoticeType identifies the type of notice.\n+enum BotInstanceNoticeType {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_INSTANCE_NOTICE_TYPE_UNSPECIFIED = 0;\n+\n+  // Means the notice contains a warning that the user is using a configuration\n+  // option that will be removed in a future release.\n+  BOT_INSTANCE_NOTICE_TYPE_DEPRECATION_WARNING = 1;\n+\n+  // Means the notice contains a generic error message.\n+  BOT_INSTANCE_NOTICE_TYPE_MESSAGE = 2;\n+}\n+\n+// BotInstanceDeprecationWarning contains the details of a deprecation warning.\n+message BotInstanceDeprecationWarning {\n+  // Message explaining the deprecation.\n+  string message = 1;\n+\n+  // The major version in which the deprecated configuration will no longer work.\n+  string removal_version = 2;\n+}\n+\n+// BotInstanceHealthStatus describes the healthiness of a `tbot` service.\n+enum BotInstanceHealthStatus {\n+  // The enum zero-value, it means no status was included.\n+  BOT_INSTANCE_HEALTH_STATUS_UNSPECIFIED = 0;\n+\n+  // Means the service is still \"starting up\" and hasn't reported its status.\n+  BOT_INSTANCE_HEALTH_STATUS_INITIALIZING = 1;\n+\n+  // Means the service is healthy and ready to serve traffic, or it has\n+  // recently succeeded in generating an output.\n+  BOT_INSTANCE_HEALTH_STATUS_HEALTHY = 2;\n+\n+  // Means the service is failing to serve traffic or generate output.\n+  BOT_INSTANCE_HEALTH_STATUS_UNHEALTHY = 3;\n+}\n+\n+// BotInstanceServiceIdentifier uniquely identifies a `tbot` service.\n+message BotInstanceServiceIdentifier {\n+  // Type of service (e.g. database-tunnel, ssh-multiplexer).\n+  string type = 1;\n+\n+  // Name of the service, either given by the user or auto-generated.\n+  string name = 2;\n+}\n+\n+// BotInstanceServiceHealth is a snapshot of a `tbot` service's health.\n+message BotInstanceServiceHealth {\n+  // Service identifies the service.\n+  BotInstanceServiceIdentifier service = 1;\n+\n+  // Status describes the service's healthiness.\n+  BotInstanceHealthStatus status = 2;\n+\n+  // Reason is a human-readable explanation for the service's status. It might\n+  // include an error message.\n+  optional string reason = 3;\n+\n+  // UpdatedAt is the time at which the service's health last changed.\n+  google.protobuf.Timestamp updated_at = 4;\n+}\n+```\n+\n+### Data fields and expected quantities\n+\n+| Field | Description | Example | Quantity | Limitations |\n+| --- | --- | --- | --- | --- |\n+| Bot | A collection of roles and access assumed by `tbot` using a join token |  | 0-300+ per cluster |  |\n+| Bot instance | A unique joined instance of `tbot` in either a long-running or ephemeral environment |  | 1-300+ per bot |  |\n+| Authentication record | Created for each join or renewal |  | 0-10 per instance (max enforced) |  |\n+| Instance heartbeat | Self-reported by each bot instance |  | 0-10 per instance (max enforced) | Data is **not** validated by the auth server, and cannot be used for making access decisions. |\n+| Service | An independent, internal part of `tbot`. Generally maps 1:1 with configured outputs/tunnels. | `application-tunnel`, `workload-identity-api` | 1-30+ per heartbeat |  |\n+| Notice | An item created by `tbot` to capture an unusual event, configuration warning, or important status |  | 0-100+ per heartbeat |  |\n+| OS | Operating system from `runtime.GOOS` | linux, windows or darwin | Once per heartbeat |  |\n+| Version | Version of `tbot` | 18.1.0 | Once per heartbeat |  |\n+| Hostname |  |  | Once per heartbeat |  |\n+| Uptime | How long `tbot` has been running |  | Once per heartbeat |  |\n+| Raw config | `tbot`\u2019s local config (combination of YAML and CLI flags) as a protobuf struct |  | Once per heartbeat |  |\n+| Join token name |  |  | Once per auth |  |\n+| Join method |  | github, iam, kubernetes | Once per auth |  |\n+| Join attributes | Metadata specific to a join method | GitHub repository name | Once per auth |  |\n+| Health status |  | INITIALIZING, HEALTHY, UNHEALTHY,\n+UNKNOWN | Once per service |  |",
        "comment_created_at": "2025-09-02T19:38:42+00:00",
        "comment_author": "thedevelopnik",
        "comment_body": "Ok, I understand. And yeah I support sending the live configuration.\r\n\r\nThe case I'm talking about is when an extraneous field or section is included and we ignore it, which is quite easy to do. This config where \"service\" is not \"services\" \r\n```\r\nversion: v2\r\nproxy_server: my.cluster:443\r\nonboarding:\r\n  join_method: iam\r\n  token: my-token\r\nstorage:\r\n  type: memory\r\noutputs:\r\n- type: identity\r\n  destination: \r\n    type: directory\r\n    path: /opt/tbot/identity\r\nservice:\r\n  - type: ssh-multiplexer\r\n    destination:\r\n      type: directory\r\n      path: /opt/tbot/ssh\r\n    enable_resumption: true\r\n    proxy_command:\r\n    - /usr/local/bin/fdpass-teleport\r\n```\r\n\r\nand this where \"identity\" is incorrectly put under services instead of outputs\r\n\r\n```\r\nversion: v2\r\nproxy_server: my.cluster:443\r\nonboarding:\r\n  join_method: iam\r\n  token: my-token\r\nstorage:\r\n  type: memory\r\nservices:\r\n  - type: ssh-multiplexer\r\n    destination:\r\n      type: directory\r\n      path: /opt/tbot/ssh\r\n    enable_resumption: true\r\n    proxy_command:\r\n    - /usr/local/bin/fdpass-teleport\r\n- type: identity\r\n  destination: \r\n    type: directory\r\n    path: /opt/tbot/identity\r\n```\r\n\r\nare both configs that tbot will start up with no problem, as it just ignores stuff it doesn't expect/recognize. And then it can be very confusing to troubleshoot. Doesn't sound like something we can handle for this iteration, but wish fulfillment would be a warning somewhere that was \"your config contained fields we ignored.\"",
        "pr_file_module": null
      },
      {
        "comment_id": "2317227799",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2316397048",
        "commented_code": "@@ -0,0 +1,464 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+Machine ID bots allow non-human users to access resources in a Teleport cluster. They are useful for automating tasks, such as CI/CD runners, monitoring systems, and running scripts. The bot resource encapsulates a bot's access rights and roles. The `tbot` binary is used to start an instance of a bot and enrol it with the cluster (using a join token). When `tbot` starts and receives its credentials, a bot instance record is created in the cluster. Bot instance records expire when their credentials expire. Bot instances can be long-running processes which periodically renew their credentials, or short-lived, ephemeral processes which are created and destroyed on demand. Many instances of the same bot can exist at once, and clusters can have many thousands of bot instances.\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine ID bots. It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine ID, it has become cumbersome to manage many bots and thousands of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web UI allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major versions behind the control plane (thereby preventing a safe upgrade). To facilitate this in the web UI, the filter will allow queries such as `older_than(version, \"18.1.0\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible). For the CLI, the `query` flag can be used to filter instances (e.g. `tctl bots instances ls --query=older_than(version, \"18.1.0\")`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently in the web UI the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `newer_than_or_equal(version, \"18.1.0\") && older_than(version, \"19.0.0\")`. This works though the web UI and the CLI (`tctl`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. An advanced filter such as `contains(notices, \"Proxy URL not set\")` will be applied.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. The instance ID can be used to filter the instances list in the web UI, and should make finding the relevant instance easy. In the CLI (`tctl`), both the bot name and instance ID are required to perform the look-up; `tctl bots instances show [bot_name]/[instance_id]`.\n+\n+Once found, in the web UI or CLI, the instance's details can be seen. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure. Notices are raised by `tbot` for actionable items that require attention, such as invalid config or deprecations.\n+\n+### Instances dashboard\n+\n+This mock-up shows the bot instance page as it would be when first arriving at the page. No filters are set, so all instances are available in the list. Results are paginated, and the default sort order is by recency - instances with the most recent heartbeat are displayed first. Sorting can be toggled between ascending and descending, and the following sort fields are available; recency, version, hostname. Filtering can be performed using a basic text search over supported fields, or an advanced search using the Teleport predicate language.\n+\n+The right-hand half of the page displays the dashboard, which is a summary over all instances. The visualizations use aggregate data prepared and updated by the auth server. An indication of when the data current is provided, as well as a refresh button which retrieves the most recently available data. A selection of timeframes is also available.\n+\n+The Activity visualization shows the number of events (joins, authentications, and heartbeats) that occurred over time. This can be used to identify trends and patterns in activity. It is non-interactive.\n+\n+The Upgrade Status visualization show a summary of all instances grouped by upgrade status; up-to-date, upgrade available, patch available, or incompatible. Status labels are selectable, and will populate the advanced search with the appropriate filter. For example, if the auth server is running v18 the filter will be populated with `older_than(version, \"16.0.0\")` when a user selects \"not supported\".\n+\n+The Notices visualization shows a summary of all notices across all instances. They're conveniently grouped by notice title and a count is included. Each item is selectable, and will apply an advanced filter (e.g. `contains(notices, \"Proxy URL not set\")`). This visualization will be hidden if there is no data to display.\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+These mock-ups shows the state of the page once an item had been selected from the instances list by clicking it. The dashboard section is replaced by the selected instance's details.\n+\n+The overview tab is displayed by default when an instance is selected. It shows further information about the instance, the join token that was used to enrol, and a summary of service health.\n+\n+![](assets/0222-details-overview.png)\n+\n+The services tab shows a list of all configured services (or outputs). Each includes it's name, type and health status. If there is extra health info available (such as and error message), this is also displayed.\n+\n+![](assets/0222-details-services.png)\n+\n+The notices tab is a listing of all notices raised since the instance was started. Notices have a type, a title and a message body. Some notice types contain extra data such as a planned removal version for deprecation notices. An indication of when the notice was raised is included, and notices are ordered by most recent first and this is not user-configurable.\n+\n+![](assets/0222-details-notices.png)\n+\n+The configuration tab show the _effective_ `tbot` configuration as readonly yaml.\n+\n+![](assets/0222-details-config.png)\n+\n+### tctl bots instances ls --search [term] --query [tql]\n+\n+The list bot instances command will include extra information about each instance; version, health status and notices count. A search term or advanced query can be used to filter the results - in this case a filter summary is included below the results confirming the applied filter and giving a result count. The data is not paginated and all instances are returned and displayed.\n+\n+```diff\n+- ID                                         Join Method Hostname      Joined               Last Seen            Generation\n+- ------------------------------------------ ----------- ------------- -------------------- -------------------- ----------\n+- bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         ip-10-0-15-34 2025-08-29T06:09:26Z 2025-09-01T12:49:26Z 237\n+-\n++ ID                                         Join Method Version Hostname      Status    Notices Last Seen\n++ ------------------------------------------ ----------- ------- ------------- --------- ------- --------------------\n++ bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         v18.2.1 ip-10-0-15-34 UNHEALTHY 6       2025-09-01T12:49:26Z\n++\n++ Filter:\n++ Search text: \"ip-10-0-15\"\n++ Query: older_than(version, \"18.0.0\")\n++ Results: 128\n++\n+To view more information on a particular instance, run:\n+\n+> /Users/nick.marais/.tsh/bin/tctl bots instances show [id]\n+```\n+\n+### tctl bots instances show [id]\n+\n+The show bot instance command gives an overall health summary as well as a listing of services and their respective health status. A list of notices is also included.\n+\n+```diff\n+Bot: w2w-demo-app-bot\n+ID:  d83b381d-b46c-4b92-a899-755991a6d0f5\n++ Status: UNHEALTHY\n+\n+Initial Authentication:\n+  Authenticated At: 2025-08-29T06:09:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       1\n+  Public Key:       <178 bytes>\n+\n+Latest Authentication:\n+  Authenticated At: 2025-09-01T12:49:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       237\n+  Public Key:       <178 bytes>\n+\n+Latest Heartbeat:\n+  Recorded At:  2025-09-01T12:39:26Z\n+  Is Startup:   false\n+  Version:      18.1.5\n+  Hostname:     ip-10-0-15-34\n+  Uptime:       78h30m0.539099441s\n+  Join Method:  iam\n+  One Shot:     false\n+  Architecture: arm64\n+  OS:           linux\n++\n++ Service status:\n++ Status    Name        Type                Reason         Updated At\n++ --------- ----------- ------------------- -------------- --------------------\n++ UNHEALTHY prod-aws-01 X509-output-service out of bananas 2025-09-01T12:49:26Z\n++\n++ Notices:\n++ Type                Service     Message               Raised At\n++ ------------------- ----------- --------------------- --------------------\n++ DEPRECATION_WARNING prod-aws-01 Lorem ipsum (v19.0.0) 2025-09-01T12:49:26Z\n+\n+To view a full, machine-readable record including past heartbeats and authentication records, run:\n+> /Users/nick.marais/.tsh/bin/tctl get bot_instance/w2w-demo-app-bot/d83b381d-b46c-4b92-a899-755991a6d0f5\n+\n+To onboard a new instance for this bot, run:\n+> /Users/nick.marais/.tsh/bin/tctl bots instances add w2w-demo-app-bot\n+```\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance.\n+\n+Instance-specific functions will be supported by implementing a custom `typical.ParserSpec`, such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `older_than(version, 18.1.0)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `newer_than_or_equal(version, \"18.0.0\") && older_than(version, \"18.1.0\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Heartbeat additions\n+\n+```protobuf\n+message BotInstanceStatusHeartbeat {\n+  // ...[snip]...\n+\n+  // The health of the services/output `tbot` is running.\n+  repeated BotInstanceServiceHealth service_health = 10;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 11;\n+\n+  // Kind identifies whether the bot is running in the tbot binary or embedded\n+  // in another component.\n+  BotKind kind = 12;\n+\n+  // Notices emitted since the last heartbeat.\n+  //\n+  // The server will clear any previous notices if `is_startup` is true, so that\n+  // editing tbot's configuration and restarting it clears any warnings from a\n+  // previous bad configuration.\n+  repeated BotInstanceNotice notices = 13;\n+}\n+\n+// BotKind identifies whether the bot is the tbot binary or embedded in another\n+// component.\n+enum BotKind {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_KIND_UNSET = 0;\n+\n+  // Means the bot is running the tbot binary.\n+  BOT_KIND_TBOT_BINARY = 1;\n+\n+  // Means the bot is running inside the Teleport Terraform provider.\n+  BOT_KIND_TERRAFORM_PROVIDER = 2;\n+\n+  // Means the bot is running inside the Teleport Kubernetes operator.\n+  BOT_KIND_KUBERNETES_OPERATOR = 3;\n+}\n+\n+// BotInstanceNotice contains an error message, deprecation warning, etc. emitted\n+// by the bot instance.\n+message BotInstanceNotice {\n+  // ID is a client-generated identifier (i.e. UUID) that can be used by the\n+  // auth server to detect and discard duplicate notices caused by partially\n+  // failed heartbeat RPCs.\n+  string id = 1;\n+\n+  // Type of notice (e.g. deprecation or warning).\n+  BotInstanceNoticeType type = 1;\n+\n+  // Service this notice relates to (or nil if it relates to the bot instance\n+  // more generally).\n+  optional BotInstanceService service = 2;\n+\n+  // Timestamp at which this notice was emitted.\n+  google.protobuf.Timestamp timestamp = 3;\n+\n+  oneof notice {\n+    // Deprecation warning details.\n+    BotInstanceDeprecationWarning deprecation_warning = 4;\n+\n+    // Generic message text.\n+    string message = 5;\n+  }\n+}\n+\n+// BotInstanceNoticeType identifies the type of notice.\n+enum BotInstanceNoticeType {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_INSTANCE_NOTICE_TYPE_UNSPECIFIED = 0;\n+\n+  // Means the notice contains a warning that the user is using a configuration\n+  // option that will be removed in a future release.\n+  BOT_INSTANCE_NOTICE_TYPE_DEPRECATION_WARNING = 1;\n+\n+  // Means the notice contains a generic error message.\n+  BOT_INSTANCE_NOTICE_TYPE_MESSAGE = 2;\n+}\n+\n+// BotInstanceDeprecationWarning contains the details of a deprecation warning.\n+message BotInstanceDeprecationWarning {\n+  // Message explaining the deprecation.\n+  string message = 1;\n+\n+  // The major version in which the deprecated configuration will no longer work.\n+  string removal_version = 2;\n+}\n+\n+// BotInstanceHealthStatus describes the healthiness of a `tbot` service.\n+enum BotInstanceHealthStatus {\n+  // The enum zero-value, it means no status was included.\n+  BOT_INSTANCE_HEALTH_STATUS_UNSPECIFIED = 0;\n+\n+  // Means the service is still \"starting up\" and hasn't reported its status.\n+  BOT_INSTANCE_HEALTH_STATUS_INITIALIZING = 1;\n+\n+  // Means the service is healthy and ready to serve traffic, or it has\n+  // recently succeeded in generating an output.\n+  BOT_INSTANCE_HEALTH_STATUS_HEALTHY = 2;\n+\n+  // Means the service is failing to serve traffic or generate output.\n+  BOT_INSTANCE_HEALTH_STATUS_UNHEALTHY = 3;\n+}\n+\n+// BotInstanceServiceIdentifier uniquely identifies a `tbot` service.\n+message BotInstanceServiceIdentifier {\n+  // Type of service (e.g. database-tunnel, ssh-multiplexer).\n+  string type = 1;\n+\n+  // Name of the service, either given by the user or auto-generated.\n+  string name = 2;\n+}\n+\n+// BotInstanceServiceHealth is a snapshot of a `tbot` service's health.\n+message BotInstanceServiceHealth {\n+  // Service identifies the service.\n+  BotInstanceServiceIdentifier service = 1;\n+\n+  // Status describes the service's healthiness.\n+  BotInstanceHealthStatus status = 2;\n+\n+  // Reason is a human-readable explanation for the service's status. It might\n+  // include an error message.\n+  optional string reason = 3;\n+\n+  // UpdatedAt is the time at which the service's health last changed.\n+  google.protobuf.Timestamp updated_at = 4;\n+}\n+```\n+\n+### Data fields and expected quantities\n+\n+| Field | Description | Example | Quantity | Limitations |\n+| --- | --- | --- | --- | --- |\n+| Bot | A collection of roles and access assumed by `tbot` using a join token |  | 0-300+ per cluster |  |\n+| Bot instance | A unique joined instance of `tbot` in either a long-running or ephemeral environment |  | 1-300+ per bot |  |\n+| Authentication record | Created for each join or renewal |  | 0-10 per instance (max enforced) |  |\n+| Instance heartbeat | Self-reported by each bot instance |  | 0-10 per instance (max enforced) | Data is **not** validated by the auth server, and cannot be used for making access decisions. |\n+| Service | An independent, internal part of `tbot`. Generally maps 1:1 with configured outputs/tunnels. | `application-tunnel`, `workload-identity-api` | 1-30+ per heartbeat |  |\n+| Notice | An item created by `tbot` to capture an unusual event, configuration warning, or important status |  | 0-100+ per heartbeat |  |\n+| OS | Operating system from `runtime.GOOS` | linux, windows or darwin | Once per heartbeat |  |\n+| Version | Version of `tbot` | 18.1.0 | Once per heartbeat |  |\n+| Hostname |  |  | Once per heartbeat |  |\n+| Uptime | How long `tbot` has been running |  | Once per heartbeat |  |\n+| Raw config | `tbot`\u2019s local config (combination of YAML and CLI flags) as a protobuf struct |  | Once per heartbeat |  |\n+| Join token name |  |  | Once per auth |  |\n+| Join method |  | github, iam, kubernetes | Once per auth |  |\n+| Join attributes | Metadata specific to a join method | GitHub repository name | Once per auth |  |\n+| Health status |  | INITIALIZING, HEALTHY, UNHEALTHY,\n+UNKNOWN | Once per service |  |",
        "comment_created_at": "2025-09-02T21:36:40+00:00",
        "comment_author": "strideynet",
        "comment_body": "Honestly - I would consider it a bug if our current configuration parsing is not throwing an error when unexpected fields are present. If you have a repro, we can open a ticket and consider how we can address that.",
        "pr_file_module": null
      },
      {
        "comment_id": "2318366518",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2316397048",
        "commented_code": "@@ -0,0 +1,464 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+Machine ID bots allow non-human users to access resources in a Teleport cluster. They are useful for automating tasks, such as CI/CD runners, monitoring systems, and running scripts. The bot resource encapsulates a bot's access rights and roles. The `tbot` binary is used to start an instance of a bot and enrol it with the cluster (using a join token). When `tbot` starts and receives its credentials, a bot instance record is created in the cluster. Bot instance records expire when their credentials expire. Bot instances can be long-running processes which periodically renew their credentials, or short-lived, ephemeral processes which are created and destroyed on demand. Many instances of the same bot can exist at once, and clusters can have many thousands of bot instances.\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine ID bots. It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine ID, it has become cumbersome to manage many bots and thousands of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web UI allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major versions behind the control plane (thereby preventing a safe upgrade). To facilitate this in the web UI, the filter will allow queries such as `older_than(version, \"18.1.0\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible). For the CLI, the `query` flag can be used to filter instances (e.g. `tctl bots instances ls --query=older_than(version, \"18.1.0\")`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently in the web UI the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `newer_than_or_equal(version, \"18.1.0\") && older_than(version, \"19.0.0\")`. This works though the web UI and the CLI (`tctl`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. An advanced filter such as `contains(notices, \"Proxy URL not set\")` will be applied.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. The instance ID can be used to filter the instances list in the web UI, and should make finding the relevant instance easy. In the CLI (`tctl`), both the bot name and instance ID are required to perform the look-up; `tctl bots instances show [bot_name]/[instance_id]`.\n+\n+Once found, in the web UI or CLI, the instance's details can be seen. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure. Notices are raised by `tbot` for actionable items that require attention, such as invalid config or deprecations.\n+\n+### Instances dashboard\n+\n+This mock-up shows the bot instance page as it would be when first arriving at the page. No filters are set, so all instances are available in the list. Results are paginated, and the default sort order is by recency - instances with the most recent heartbeat are displayed first. Sorting can be toggled between ascending and descending, and the following sort fields are available; recency, version, hostname. Filtering can be performed using a basic text search over supported fields, or an advanced search using the Teleport predicate language.\n+\n+The right-hand half of the page displays the dashboard, which is a summary over all instances. The visualizations use aggregate data prepared and updated by the auth server. An indication of when the data current is provided, as well as a refresh button which retrieves the most recently available data. A selection of timeframes is also available.\n+\n+The Activity visualization shows the number of events (joins, authentications, and heartbeats) that occurred over time. This can be used to identify trends and patterns in activity. It is non-interactive.\n+\n+The Upgrade Status visualization show a summary of all instances grouped by upgrade status; up-to-date, upgrade available, patch available, or incompatible. Status labels are selectable, and will populate the advanced search with the appropriate filter. For example, if the auth server is running v18 the filter will be populated with `older_than(version, \"16.0.0\")` when a user selects \"not supported\".\n+\n+The Notices visualization shows a summary of all notices across all instances. They're conveniently grouped by notice title and a count is included. Each item is selectable, and will apply an advanced filter (e.g. `contains(notices, \"Proxy URL not set\")`). This visualization will be hidden if there is no data to display.\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+These mock-ups shows the state of the page once an item had been selected from the instances list by clicking it. The dashboard section is replaced by the selected instance's details.\n+\n+The overview tab is displayed by default when an instance is selected. It shows further information about the instance, the join token that was used to enrol, and a summary of service health.\n+\n+![](assets/0222-details-overview.png)\n+\n+The services tab shows a list of all configured services (or outputs). Each includes it's name, type and health status. If there is extra health info available (such as and error message), this is also displayed.\n+\n+![](assets/0222-details-services.png)\n+\n+The notices tab is a listing of all notices raised since the instance was started. Notices have a type, a title and a message body. Some notice types contain extra data such as a planned removal version for deprecation notices. An indication of when the notice was raised is included, and notices are ordered by most recent first and this is not user-configurable.\n+\n+![](assets/0222-details-notices.png)\n+\n+The configuration tab show the _effective_ `tbot` configuration as readonly yaml.\n+\n+![](assets/0222-details-config.png)\n+\n+### tctl bots instances ls --search [term] --query [tql]\n+\n+The list bot instances command will include extra information about each instance; version, health status and notices count. A search term or advanced query can be used to filter the results - in this case a filter summary is included below the results confirming the applied filter and giving a result count. The data is not paginated and all instances are returned and displayed.\n+\n+```diff\n+- ID                                         Join Method Hostname      Joined               Last Seen            Generation\n+- ------------------------------------------ ----------- ------------- -------------------- -------------------- ----------\n+- bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         ip-10-0-15-34 2025-08-29T06:09:26Z 2025-09-01T12:49:26Z 237\n+-\n++ ID                                         Join Method Version Hostname      Status    Notices Last Seen\n++ ------------------------------------------ ----------- ------- ------------- --------- ------- --------------------\n++ bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         v18.2.1 ip-10-0-15-34 UNHEALTHY 6       2025-09-01T12:49:26Z\n++\n++ Filter:\n++ Search text: \"ip-10-0-15\"\n++ Query: older_than(version, \"18.0.0\")\n++ Results: 128\n++\n+To view more information on a particular instance, run:\n+\n+> /Users/nick.marais/.tsh/bin/tctl bots instances show [id]\n+```\n+\n+### tctl bots instances show [id]\n+\n+The show bot instance command gives an overall health summary as well as a listing of services and their respective health status. A list of notices is also included.\n+\n+```diff\n+Bot: w2w-demo-app-bot\n+ID:  d83b381d-b46c-4b92-a899-755991a6d0f5\n++ Status: UNHEALTHY\n+\n+Initial Authentication:\n+  Authenticated At: 2025-08-29T06:09:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       1\n+  Public Key:       <178 bytes>\n+\n+Latest Authentication:\n+  Authenticated At: 2025-09-01T12:49:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       237\n+  Public Key:       <178 bytes>\n+\n+Latest Heartbeat:\n+  Recorded At:  2025-09-01T12:39:26Z\n+  Is Startup:   false\n+  Version:      18.1.5\n+  Hostname:     ip-10-0-15-34\n+  Uptime:       78h30m0.539099441s\n+  Join Method:  iam\n+  One Shot:     false\n+  Architecture: arm64\n+  OS:           linux\n++\n++ Service status:\n++ Status    Name        Type                Reason         Updated At\n++ --------- ----------- ------------------- -------------- --------------------\n++ UNHEALTHY prod-aws-01 X509-output-service out of bananas 2025-09-01T12:49:26Z\n++\n++ Notices:\n++ Type                Service     Message               Raised At\n++ ------------------- ----------- --------------------- --------------------\n++ DEPRECATION_WARNING prod-aws-01 Lorem ipsum (v19.0.0) 2025-09-01T12:49:26Z\n+\n+To view a full, machine-readable record including past heartbeats and authentication records, run:\n+> /Users/nick.marais/.tsh/bin/tctl get bot_instance/w2w-demo-app-bot/d83b381d-b46c-4b92-a899-755991a6d0f5\n+\n+To onboard a new instance for this bot, run:\n+> /Users/nick.marais/.tsh/bin/tctl bots instances add w2w-demo-app-bot\n+```\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance.\n+\n+Instance-specific functions will be supported by implementing a custom `typical.ParserSpec`, such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `older_than(version, 18.1.0)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `newer_than_or_equal(version, \"18.0.0\") && older_than(version, \"18.1.0\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Heartbeat additions\n+\n+```protobuf\n+message BotInstanceStatusHeartbeat {\n+  // ...[snip]...\n+\n+  // The health of the services/output `tbot` is running.\n+  repeated BotInstanceServiceHealth service_health = 10;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 11;\n+\n+  // Kind identifies whether the bot is running in the tbot binary or embedded\n+  // in another component.\n+  BotKind kind = 12;\n+\n+  // Notices emitted since the last heartbeat.\n+  //\n+  // The server will clear any previous notices if `is_startup` is true, so that\n+  // editing tbot's configuration and restarting it clears any warnings from a\n+  // previous bad configuration.\n+  repeated BotInstanceNotice notices = 13;\n+}\n+\n+// BotKind identifies whether the bot is the tbot binary or embedded in another\n+// component.\n+enum BotKind {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_KIND_UNSET = 0;\n+\n+  // Means the bot is running the tbot binary.\n+  BOT_KIND_TBOT_BINARY = 1;\n+\n+  // Means the bot is running inside the Teleport Terraform provider.\n+  BOT_KIND_TERRAFORM_PROVIDER = 2;\n+\n+  // Means the bot is running inside the Teleport Kubernetes operator.\n+  BOT_KIND_KUBERNETES_OPERATOR = 3;\n+}\n+\n+// BotInstanceNotice contains an error message, deprecation warning, etc. emitted\n+// by the bot instance.\n+message BotInstanceNotice {\n+  // ID is a client-generated identifier (i.e. UUID) that can be used by the\n+  // auth server to detect and discard duplicate notices caused by partially\n+  // failed heartbeat RPCs.\n+  string id = 1;\n+\n+  // Type of notice (e.g. deprecation or warning).\n+  BotInstanceNoticeType type = 1;\n+\n+  // Service this notice relates to (or nil if it relates to the bot instance\n+  // more generally).\n+  optional BotInstanceService service = 2;\n+\n+  // Timestamp at which this notice was emitted.\n+  google.protobuf.Timestamp timestamp = 3;\n+\n+  oneof notice {\n+    // Deprecation warning details.\n+    BotInstanceDeprecationWarning deprecation_warning = 4;\n+\n+    // Generic message text.\n+    string message = 5;\n+  }\n+}\n+\n+// BotInstanceNoticeType identifies the type of notice.\n+enum BotInstanceNoticeType {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_INSTANCE_NOTICE_TYPE_UNSPECIFIED = 0;\n+\n+  // Means the notice contains a warning that the user is using a configuration\n+  // option that will be removed in a future release.\n+  BOT_INSTANCE_NOTICE_TYPE_DEPRECATION_WARNING = 1;\n+\n+  // Means the notice contains a generic error message.\n+  BOT_INSTANCE_NOTICE_TYPE_MESSAGE = 2;\n+}\n+\n+// BotInstanceDeprecationWarning contains the details of a deprecation warning.\n+message BotInstanceDeprecationWarning {\n+  // Message explaining the deprecation.\n+  string message = 1;\n+\n+  // The major version in which the deprecated configuration will no longer work.\n+  string removal_version = 2;\n+}\n+\n+// BotInstanceHealthStatus describes the healthiness of a `tbot` service.\n+enum BotInstanceHealthStatus {\n+  // The enum zero-value, it means no status was included.\n+  BOT_INSTANCE_HEALTH_STATUS_UNSPECIFIED = 0;\n+\n+  // Means the service is still \"starting up\" and hasn't reported its status.\n+  BOT_INSTANCE_HEALTH_STATUS_INITIALIZING = 1;\n+\n+  // Means the service is healthy and ready to serve traffic, or it has\n+  // recently succeeded in generating an output.\n+  BOT_INSTANCE_HEALTH_STATUS_HEALTHY = 2;\n+\n+  // Means the service is failing to serve traffic or generate output.\n+  BOT_INSTANCE_HEALTH_STATUS_UNHEALTHY = 3;\n+}\n+\n+// BotInstanceServiceIdentifier uniquely identifies a `tbot` service.\n+message BotInstanceServiceIdentifier {\n+  // Type of service (e.g. database-tunnel, ssh-multiplexer).\n+  string type = 1;\n+\n+  // Name of the service, either given by the user or auto-generated.\n+  string name = 2;\n+}\n+\n+// BotInstanceServiceHealth is a snapshot of a `tbot` service's health.\n+message BotInstanceServiceHealth {\n+  // Service identifies the service.\n+  BotInstanceServiceIdentifier service = 1;\n+\n+  // Status describes the service's healthiness.\n+  BotInstanceHealthStatus status = 2;\n+\n+  // Reason is a human-readable explanation for the service's status. It might\n+  // include an error message.\n+  optional string reason = 3;\n+\n+  // UpdatedAt is the time at which the service's health last changed.\n+  google.protobuf.Timestamp updated_at = 4;\n+}\n+```\n+\n+### Data fields and expected quantities\n+\n+| Field | Description | Example | Quantity | Limitations |\n+| --- | --- | --- | --- | --- |\n+| Bot | A collection of roles and access assumed by `tbot` using a join token |  | 0-300+ per cluster |  |\n+| Bot instance | A unique joined instance of `tbot` in either a long-running or ephemeral environment |  | 1-300+ per bot |  |\n+| Authentication record | Created for each join or renewal |  | 0-10 per instance (max enforced) |  |\n+| Instance heartbeat | Self-reported by each bot instance |  | 0-10 per instance (max enforced) | Data is **not** validated by the auth server, and cannot be used for making access decisions. |\n+| Service | An independent, internal part of `tbot`. Generally maps 1:1 with configured outputs/tunnels. | `application-tunnel`, `workload-identity-api` | 1-30+ per heartbeat |  |\n+| Notice | An item created by `tbot` to capture an unusual event, configuration warning, or important status |  | 0-100+ per heartbeat |  |\n+| OS | Operating system from `runtime.GOOS` | linux, windows or darwin | Once per heartbeat |  |\n+| Version | Version of `tbot` | 18.1.0 | Once per heartbeat |  |\n+| Hostname |  |  | Once per heartbeat |  |\n+| Uptime | How long `tbot` has been running |  | Once per heartbeat |  |\n+| Raw config | `tbot`\u2019s local config (combination of YAML and CLI flags) as a protobuf struct |  | Once per heartbeat |  |\n+| Join token name |  |  | Once per auth |  |\n+| Join method |  | github, iam, kubernetes | Once per auth |  |\n+| Join attributes | Metadata specific to a join method | GitHub repository name | Once per auth |  |\n+| Health status |  | INITIALIZING, HEALTHY, UNHEALTHY,\n+UNKNOWN | Once per service |  |",
        "comment_created_at": "2025-09-03T09:26:55+00:00",
        "comment_author": "nicholasmarais1158",
        "comment_body": "`tbot` could detect the presence of unknown fields and raise it as a notice, while still only sending the live config. \r\n\r\n@boxofrad, would that require a new notice type?",
        "pr_file_module": null
      },
      {
        "comment_id": "2318597404",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2316397048",
        "commented_code": "@@ -0,0 +1,464 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+Machine ID bots allow non-human users to access resources in a Teleport cluster. They are useful for automating tasks, such as CI/CD runners, monitoring systems, and running scripts. The bot resource encapsulates a bot's access rights and roles. The `tbot` binary is used to start an instance of a bot and enrol it with the cluster (using a join token). When `tbot` starts and receives its credentials, a bot instance record is created in the cluster. Bot instance records expire when their credentials expire. Bot instances can be long-running processes which periodically renew their credentials, or short-lived, ephemeral processes which are created and destroyed on demand. Many instances of the same bot can exist at once, and clusters can have many thousands of bot instances.\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine ID bots. It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine ID, it has become cumbersome to manage many bots and thousands of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web UI allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major versions behind the control plane (thereby preventing a safe upgrade). To facilitate this in the web UI, the filter will allow queries such as `older_than(version, \"18.1.0\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible). For the CLI, the `query` flag can be used to filter instances (e.g. `tctl bots instances ls --query=older_than(version, \"18.1.0\")`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently in the web UI the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `newer_than_or_equal(version, \"18.1.0\") && older_than(version, \"19.0.0\")`. This works though the web UI and the CLI (`tctl`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. An advanced filter such as `contains(notices, \"Proxy URL not set\")` will be applied.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. The instance ID can be used to filter the instances list in the web UI, and should make finding the relevant instance easy. In the CLI (`tctl`), both the bot name and instance ID are required to perform the look-up; `tctl bots instances show [bot_name]/[instance_id]`.\n+\n+Once found, in the web UI or CLI, the instance's details can be seen. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure. Notices are raised by `tbot` for actionable items that require attention, such as invalid config or deprecations.\n+\n+### Instances dashboard\n+\n+This mock-up shows the bot instance page as it would be when first arriving at the page. No filters are set, so all instances are available in the list. Results are paginated, and the default sort order is by recency - instances with the most recent heartbeat are displayed first. Sorting can be toggled between ascending and descending, and the following sort fields are available; recency, version, hostname. Filtering can be performed using a basic text search over supported fields, or an advanced search using the Teleport predicate language.\n+\n+The right-hand half of the page displays the dashboard, which is a summary over all instances. The visualizations use aggregate data prepared and updated by the auth server. An indication of when the data current is provided, as well as a refresh button which retrieves the most recently available data. A selection of timeframes is also available.\n+\n+The Activity visualization shows the number of events (joins, authentications, and heartbeats) that occurred over time. This can be used to identify trends and patterns in activity. It is non-interactive.\n+\n+The Upgrade Status visualization show a summary of all instances grouped by upgrade status; up-to-date, upgrade available, patch available, or incompatible. Status labels are selectable, and will populate the advanced search with the appropriate filter. For example, if the auth server is running v18 the filter will be populated with `older_than(version, \"16.0.0\")` when a user selects \"not supported\".\n+\n+The Notices visualization shows a summary of all notices across all instances. They're conveniently grouped by notice title and a count is included. Each item is selectable, and will apply an advanced filter (e.g. `contains(notices, \"Proxy URL not set\")`). This visualization will be hidden if there is no data to display.\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+These mock-ups shows the state of the page once an item had been selected from the instances list by clicking it. The dashboard section is replaced by the selected instance's details.\n+\n+The overview tab is displayed by default when an instance is selected. It shows further information about the instance, the join token that was used to enrol, and a summary of service health.\n+\n+![](assets/0222-details-overview.png)\n+\n+The services tab shows a list of all configured services (or outputs). Each includes it's name, type and health status. If there is extra health info available (such as and error message), this is also displayed.\n+\n+![](assets/0222-details-services.png)\n+\n+The notices tab is a listing of all notices raised since the instance was started. Notices have a type, a title and a message body. Some notice types contain extra data such as a planned removal version for deprecation notices. An indication of when the notice was raised is included, and notices are ordered by most recent first and this is not user-configurable.\n+\n+![](assets/0222-details-notices.png)\n+\n+The configuration tab show the _effective_ `tbot` configuration as readonly yaml.\n+\n+![](assets/0222-details-config.png)\n+\n+### tctl bots instances ls --search [term] --query [tql]\n+\n+The list bot instances command will include extra information about each instance; version, health status and notices count. A search term or advanced query can be used to filter the results - in this case a filter summary is included below the results confirming the applied filter and giving a result count. The data is not paginated and all instances are returned and displayed.\n+\n+```diff\n+- ID                                         Join Method Hostname      Joined               Last Seen            Generation\n+- ------------------------------------------ ----------- ------------- -------------------- -------------------- ----------\n+- bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         ip-10-0-15-34 2025-08-29T06:09:26Z 2025-09-01T12:49:26Z 237\n+-\n++ ID                                         Join Method Version Hostname      Status    Notices Last Seen\n++ ------------------------------------------ ----------- ------- ------------- --------- ------- --------------------\n++ bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         v18.2.1 ip-10-0-15-34 UNHEALTHY 6       2025-09-01T12:49:26Z\n++\n++ Filter:\n++ Search text: \"ip-10-0-15\"\n++ Query: older_than(version, \"18.0.0\")\n++ Results: 128\n++\n+To view more information on a particular instance, run:\n+\n+> /Users/nick.marais/.tsh/bin/tctl bots instances show [id]\n+```\n+\n+### tctl bots instances show [id]\n+\n+The show bot instance command gives an overall health summary as well as a listing of services and their respective health status. A list of notices is also included.\n+\n+```diff\n+Bot: w2w-demo-app-bot\n+ID:  d83b381d-b46c-4b92-a899-755991a6d0f5\n++ Status: UNHEALTHY\n+\n+Initial Authentication:\n+  Authenticated At: 2025-08-29T06:09:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       1\n+  Public Key:       <178 bytes>\n+\n+Latest Authentication:\n+  Authenticated At: 2025-09-01T12:49:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       237\n+  Public Key:       <178 bytes>\n+\n+Latest Heartbeat:\n+  Recorded At:  2025-09-01T12:39:26Z\n+  Is Startup:   false\n+  Version:      18.1.5\n+  Hostname:     ip-10-0-15-34\n+  Uptime:       78h30m0.539099441s\n+  Join Method:  iam\n+  One Shot:     false\n+  Architecture: arm64\n+  OS:           linux\n++\n++ Service status:\n++ Status    Name        Type                Reason         Updated At\n++ --------- ----------- ------------------- -------------- --------------------\n++ UNHEALTHY prod-aws-01 X509-output-service out of bananas 2025-09-01T12:49:26Z\n++\n++ Notices:\n++ Type                Service     Message               Raised At\n++ ------------------- ----------- --------------------- --------------------\n++ DEPRECATION_WARNING prod-aws-01 Lorem ipsum (v19.0.0) 2025-09-01T12:49:26Z\n+\n+To view a full, machine-readable record including past heartbeats and authentication records, run:\n+> /Users/nick.marais/.tsh/bin/tctl get bot_instance/w2w-demo-app-bot/d83b381d-b46c-4b92-a899-755991a6d0f5\n+\n+To onboard a new instance for this bot, run:\n+> /Users/nick.marais/.tsh/bin/tctl bots instances add w2w-demo-app-bot\n+```\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance.\n+\n+Instance-specific functions will be supported by implementing a custom `typical.ParserSpec`, such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `older_than(version, 18.1.0)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `newer_than_or_equal(version, \"18.0.0\") && older_than(version, \"18.1.0\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Heartbeat additions\n+\n+```protobuf\n+message BotInstanceStatusHeartbeat {\n+  // ...[snip]...\n+\n+  // The health of the services/output `tbot` is running.\n+  repeated BotInstanceServiceHealth service_health = 10;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 11;\n+\n+  // Kind identifies whether the bot is running in the tbot binary or embedded\n+  // in another component.\n+  BotKind kind = 12;\n+\n+  // Notices emitted since the last heartbeat.\n+  //\n+  // The server will clear any previous notices if `is_startup` is true, so that\n+  // editing tbot's configuration and restarting it clears any warnings from a\n+  // previous bad configuration.\n+  repeated BotInstanceNotice notices = 13;\n+}\n+\n+// BotKind identifies whether the bot is the tbot binary or embedded in another\n+// component.\n+enum BotKind {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_KIND_UNSET = 0;\n+\n+  // Means the bot is running the tbot binary.\n+  BOT_KIND_TBOT_BINARY = 1;\n+\n+  // Means the bot is running inside the Teleport Terraform provider.\n+  BOT_KIND_TERRAFORM_PROVIDER = 2;\n+\n+  // Means the bot is running inside the Teleport Kubernetes operator.\n+  BOT_KIND_KUBERNETES_OPERATOR = 3;\n+}\n+\n+// BotInstanceNotice contains an error message, deprecation warning, etc. emitted\n+// by the bot instance.\n+message BotInstanceNotice {\n+  // ID is a client-generated identifier (i.e. UUID) that can be used by the\n+  // auth server to detect and discard duplicate notices caused by partially\n+  // failed heartbeat RPCs.\n+  string id = 1;\n+\n+  // Type of notice (e.g. deprecation or warning).\n+  BotInstanceNoticeType type = 1;\n+\n+  // Service this notice relates to (or nil if it relates to the bot instance\n+  // more generally).\n+  optional BotInstanceService service = 2;\n+\n+  // Timestamp at which this notice was emitted.\n+  google.protobuf.Timestamp timestamp = 3;\n+\n+  oneof notice {\n+    // Deprecation warning details.\n+    BotInstanceDeprecationWarning deprecation_warning = 4;\n+\n+    // Generic message text.\n+    string message = 5;\n+  }\n+}\n+\n+// BotInstanceNoticeType identifies the type of notice.\n+enum BotInstanceNoticeType {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_INSTANCE_NOTICE_TYPE_UNSPECIFIED = 0;\n+\n+  // Means the notice contains a warning that the user is using a configuration\n+  // option that will be removed in a future release.\n+  BOT_INSTANCE_NOTICE_TYPE_DEPRECATION_WARNING = 1;\n+\n+  // Means the notice contains a generic error message.\n+  BOT_INSTANCE_NOTICE_TYPE_MESSAGE = 2;\n+}\n+\n+// BotInstanceDeprecationWarning contains the details of a deprecation warning.\n+message BotInstanceDeprecationWarning {\n+  // Message explaining the deprecation.\n+  string message = 1;\n+\n+  // The major version in which the deprecated configuration will no longer work.\n+  string removal_version = 2;\n+}\n+\n+// BotInstanceHealthStatus describes the healthiness of a `tbot` service.\n+enum BotInstanceHealthStatus {\n+  // The enum zero-value, it means no status was included.\n+  BOT_INSTANCE_HEALTH_STATUS_UNSPECIFIED = 0;\n+\n+  // Means the service is still \"starting up\" and hasn't reported its status.\n+  BOT_INSTANCE_HEALTH_STATUS_INITIALIZING = 1;\n+\n+  // Means the service is healthy and ready to serve traffic, or it has\n+  // recently succeeded in generating an output.\n+  BOT_INSTANCE_HEALTH_STATUS_HEALTHY = 2;\n+\n+  // Means the service is failing to serve traffic or generate output.\n+  BOT_INSTANCE_HEALTH_STATUS_UNHEALTHY = 3;\n+}\n+\n+// BotInstanceServiceIdentifier uniquely identifies a `tbot` service.\n+message BotInstanceServiceIdentifier {\n+  // Type of service (e.g. database-tunnel, ssh-multiplexer).\n+  string type = 1;\n+\n+  // Name of the service, either given by the user or auto-generated.\n+  string name = 2;\n+}\n+\n+// BotInstanceServiceHealth is a snapshot of a `tbot` service's health.\n+message BotInstanceServiceHealth {\n+  // Service identifies the service.\n+  BotInstanceServiceIdentifier service = 1;\n+\n+  // Status describes the service's healthiness.\n+  BotInstanceHealthStatus status = 2;\n+\n+  // Reason is a human-readable explanation for the service's status. It might\n+  // include an error message.\n+  optional string reason = 3;\n+\n+  // UpdatedAt is the time at which the service's health last changed.\n+  google.protobuf.Timestamp updated_at = 4;\n+}\n+```\n+\n+### Data fields and expected quantities\n+\n+| Field | Description | Example | Quantity | Limitations |\n+| --- | --- | --- | --- | --- |\n+| Bot | A collection of roles and access assumed by `tbot` using a join token |  | 0-300+ per cluster |  |\n+| Bot instance | A unique joined instance of `tbot` in either a long-running or ephemeral environment |  | 1-300+ per bot |  |\n+| Authentication record | Created for each join or renewal |  | 0-10 per instance (max enforced) |  |\n+| Instance heartbeat | Self-reported by each bot instance |  | 0-10 per instance (max enforced) | Data is **not** validated by the auth server, and cannot be used for making access decisions. |\n+| Service | An independent, internal part of `tbot`. Generally maps 1:1 with configured outputs/tunnels. | `application-tunnel`, `workload-identity-api` | 1-30+ per heartbeat |  |\n+| Notice | An item created by `tbot` to capture an unusual event, configuration warning, or important status |  | 0-100+ per heartbeat |  |\n+| OS | Operating system from `runtime.GOOS` | linux, windows or darwin | Once per heartbeat |  |\n+| Version | Version of `tbot` | 18.1.0 | Once per heartbeat |  |\n+| Hostname |  |  | Once per heartbeat |  |\n+| Uptime | How long `tbot` has been running |  | Once per heartbeat |  |\n+| Raw config | `tbot`\u2019s local config (combination of YAML and CLI flags) as a protobuf struct |  | Once per heartbeat |  |\n+| Join token name |  |  | Once per auth |  |\n+| Join method |  | github, iam, kubernetes | Once per auth |  |\n+| Join attributes | Metadata specific to a join method | GitHub repository name | Once per auth |  |\n+| Health status |  | INITIALIZING, HEALTHY, UNHEALTHY,\n+UNKNOWN | Once per service |  |",
        "comment_created_at": "2025-09-03T10:54:40+00:00",
        "comment_author": "strideynet",
        "comment_body": "A notice could definitely be the appropriate way of communicating this - but - I think outside of the scope of this RFD. I'd like to get at the root of this behaviour since my understanding and intention was that our current configuration parser would reject unknown fields.\r\n\r\nParticularly, my gut feeling is that failing explicitly and immediately on an invalid configuration makes more sense for the user story where an engineer is setting up `tbot` and modifying the configuration. It seems like it would be inconvenient to miss the warning message within `tbot` and then realise days later via the UI that it is misconfigured.",
        "pr_file_module": null
      },
      {
        "comment_id": "2319379839",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2316397048",
        "commented_code": "@@ -0,0 +1,464 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+Machine ID bots allow non-human users to access resources in a Teleport cluster. They are useful for automating tasks, such as CI/CD runners, monitoring systems, and running scripts. The bot resource encapsulates a bot's access rights and roles. The `tbot` binary is used to start an instance of a bot and enrol it with the cluster (using a join token). When `tbot` starts and receives its credentials, a bot instance record is created in the cluster. Bot instance records expire when their credentials expire. Bot instances can be long-running processes which periodically renew their credentials, or short-lived, ephemeral processes which are created and destroyed on demand. Many instances of the same bot can exist at once, and clusters can have many thousands of bot instances.\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine ID bots. It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine ID, it has become cumbersome to manage many bots and thousands of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web UI allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major versions behind the control plane (thereby preventing a safe upgrade). To facilitate this in the web UI, the filter will allow queries such as `older_than(version, \"18.1.0\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible). For the CLI, the `query` flag can be used to filter instances (e.g. `tctl bots instances ls --query=older_than(version, \"18.1.0\")`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently in the web UI the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `newer_than_or_equal(version, \"18.1.0\") && older_than(version, \"19.0.0\")`. This works though the web UI and the CLI (`tctl`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. An advanced filter such as `contains(notices, \"Proxy URL not set\")` will be applied.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. The instance ID can be used to filter the instances list in the web UI, and should make finding the relevant instance easy. In the CLI (`tctl`), both the bot name and instance ID are required to perform the look-up; `tctl bots instances show [bot_name]/[instance_id]`.\n+\n+Once found, in the web UI or CLI, the instance's details can be seen. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure. Notices are raised by `tbot` for actionable items that require attention, such as invalid config or deprecations.\n+\n+### Instances dashboard\n+\n+This mock-up shows the bot instance page as it would be when first arriving at the page. No filters are set, so all instances are available in the list. Results are paginated, and the default sort order is by recency - instances with the most recent heartbeat are displayed first. Sorting can be toggled between ascending and descending, and the following sort fields are available; recency, version, hostname. Filtering can be performed using a basic text search over supported fields, or an advanced search using the Teleport predicate language.\n+\n+The right-hand half of the page displays the dashboard, which is a summary over all instances. The visualizations use aggregate data prepared and updated by the auth server. An indication of when the data current is provided, as well as a refresh button which retrieves the most recently available data. A selection of timeframes is also available.\n+\n+The Activity visualization shows the number of events (joins, authentications, and heartbeats) that occurred over time. This can be used to identify trends and patterns in activity. It is non-interactive.\n+\n+The Upgrade Status visualization show a summary of all instances grouped by upgrade status; up-to-date, upgrade available, patch available, or incompatible. Status labels are selectable, and will populate the advanced search with the appropriate filter. For example, if the auth server is running v18 the filter will be populated with `older_than(version, \"16.0.0\")` when a user selects \"not supported\".\n+\n+The Notices visualization shows a summary of all notices across all instances. They're conveniently grouped by notice title and a count is included. Each item is selectable, and will apply an advanced filter (e.g. `contains(notices, \"Proxy URL not set\")`). This visualization will be hidden if there is no data to display.\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+These mock-ups shows the state of the page once an item had been selected from the instances list by clicking it. The dashboard section is replaced by the selected instance's details.\n+\n+The overview tab is displayed by default when an instance is selected. It shows further information about the instance, the join token that was used to enrol, and a summary of service health.\n+\n+![](assets/0222-details-overview.png)\n+\n+The services tab shows a list of all configured services (or outputs). Each includes it's name, type and health status. If there is extra health info available (such as and error message), this is also displayed.\n+\n+![](assets/0222-details-services.png)\n+\n+The notices tab is a listing of all notices raised since the instance was started. Notices have a type, a title and a message body. Some notice types contain extra data such as a planned removal version for deprecation notices. An indication of when the notice was raised is included, and notices are ordered by most recent first and this is not user-configurable.\n+\n+![](assets/0222-details-notices.png)\n+\n+The configuration tab show the _effective_ `tbot` configuration as readonly yaml.\n+\n+![](assets/0222-details-config.png)\n+\n+### tctl bots instances ls --search [term] --query [tql]\n+\n+The list bot instances command will include extra information about each instance; version, health status and notices count. A search term or advanced query can be used to filter the results - in this case a filter summary is included below the results confirming the applied filter and giving a result count. The data is not paginated and all instances are returned and displayed.\n+\n+```diff\n+- ID                                         Join Method Hostname      Joined               Last Seen            Generation\n+- ------------------------------------------ ----------- ------------- -------------------- -------------------- ----------\n+- bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         ip-10-0-15-34 2025-08-29T06:09:26Z 2025-09-01T12:49:26Z 237\n+-\n++ ID                                         Join Method Version Hostname      Status    Notices Last Seen\n++ ------------------------------------------ ----------- ------- ------------- --------- ------- --------------------\n++ bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         v18.2.1 ip-10-0-15-34 UNHEALTHY 6       2025-09-01T12:49:26Z\n++\n++ Filter:\n++ Search text: \"ip-10-0-15\"\n++ Query: older_than(version, \"18.0.0\")\n++ Results: 128\n++\n+To view more information on a particular instance, run:\n+\n+> /Users/nick.marais/.tsh/bin/tctl bots instances show [id]\n+```\n+\n+### tctl bots instances show [id]\n+\n+The show bot instance command gives an overall health summary as well as a listing of services and their respective health status. A list of notices is also included.\n+\n+```diff\n+Bot: w2w-demo-app-bot\n+ID:  d83b381d-b46c-4b92-a899-755991a6d0f5\n++ Status: UNHEALTHY\n+\n+Initial Authentication:\n+  Authenticated At: 2025-08-29T06:09:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       1\n+  Public Key:       <178 bytes>\n+\n+Latest Authentication:\n+  Authenticated At: 2025-09-01T12:49:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       237\n+  Public Key:       <178 bytes>\n+\n+Latest Heartbeat:\n+  Recorded At:  2025-09-01T12:39:26Z\n+  Is Startup:   false\n+  Version:      18.1.5\n+  Hostname:     ip-10-0-15-34\n+  Uptime:       78h30m0.539099441s\n+  Join Method:  iam\n+  One Shot:     false\n+  Architecture: arm64\n+  OS:           linux\n++\n++ Service status:\n++ Status    Name        Type                Reason         Updated At\n++ --------- ----------- ------------------- -------------- --------------------\n++ UNHEALTHY prod-aws-01 X509-output-service out of bananas 2025-09-01T12:49:26Z\n++\n++ Notices:\n++ Type                Service     Message               Raised At\n++ ------------------- ----------- --------------------- --------------------\n++ DEPRECATION_WARNING prod-aws-01 Lorem ipsum (v19.0.0) 2025-09-01T12:49:26Z\n+\n+To view a full, machine-readable record including past heartbeats and authentication records, run:\n+> /Users/nick.marais/.tsh/bin/tctl get bot_instance/w2w-demo-app-bot/d83b381d-b46c-4b92-a899-755991a6d0f5\n+\n+To onboard a new instance for this bot, run:\n+> /Users/nick.marais/.tsh/bin/tctl bots instances add w2w-demo-app-bot\n+```\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance.\n+\n+Instance-specific functions will be supported by implementing a custom `typical.ParserSpec`, such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `older_than(version, 18.1.0)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `newer_than_or_equal(version, \"18.0.0\") && older_than(version, \"18.1.0\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Heartbeat additions\n+\n+```protobuf\n+message BotInstanceStatusHeartbeat {\n+  // ...[snip]...\n+\n+  // The health of the services/output `tbot` is running.\n+  repeated BotInstanceServiceHealth service_health = 10;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 11;\n+\n+  // Kind identifies whether the bot is running in the tbot binary or embedded\n+  // in another component.\n+  BotKind kind = 12;\n+\n+  // Notices emitted since the last heartbeat.\n+  //\n+  // The server will clear any previous notices if `is_startup` is true, so that\n+  // editing tbot's configuration and restarting it clears any warnings from a\n+  // previous bad configuration.\n+  repeated BotInstanceNotice notices = 13;\n+}\n+\n+// BotKind identifies whether the bot is the tbot binary or embedded in another\n+// component.\n+enum BotKind {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_KIND_UNSET = 0;\n+\n+  // Means the bot is running the tbot binary.\n+  BOT_KIND_TBOT_BINARY = 1;\n+\n+  // Means the bot is running inside the Teleport Terraform provider.\n+  BOT_KIND_TERRAFORM_PROVIDER = 2;\n+\n+  // Means the bot is running inside the Teleport Kubernetes operator.\n+  BOT_KIND_KUBERNETES_OPERATOR = 3;\n+}\n+\n+// BotInstanceNotice contains an error message, deprecation warning, etc. emitted\n+// by the bot instance.\n+message BotInstanceNotice {\n+  // ID is a client-generated identifier (i.e. UUID) that can be used by the\n+  // auth server to detect and discard duplicate notices caused by partially\n+  // failed heartbeat RPCs.\n+  string id = 1;\n+\n+  // Type of notice (e.g. deprecation or warning).\n+  BotInstanceNoticeType type = 1;\n+\n+  // Service this notice relates to (or nil if it relates to the bot instance\n+  // more generally).\n+  optional BotInstanceService service = 2;\n+\n+  // Timestamp at which this notice was emitted.\n+  google.protobuf.Timestamp timestamp = 3;\n+\n+  oneof notice {\n+    // Deprecation warning details.\n+    BotInstanceDeprecationWarning deprecation_warning = 4;\n+\n+    // Generic message text.\n+    string message = 5;\n+  }\n+}\n+\n+// BotInstanceNoticeType identifies the type of notice.\n+enum BotInstanceNoticeType {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_INSTANCE_NOTICE_TYPE_UNSPECIFIED = 0;\n+\n+  // Means the notice contains a warning that the user is using a configuration\n+  // option that will be removed in a future release.\n+  BOT_INSTANCE_NOTICE_TYPE_DEPRECATION_WARNING = 1;\n+\n+  // Means the notice contains a generic error message.\n+  BOT_INSTANCE_NOTICE_TYPE_MESSAGE = 2;\n+}\n+\n+// BotInstanceDeprecationWarning contains the details of a deprecation warning.\n+message BotInstanceDeprecationWarning {\n+  // Message explaining the deprecation.\n+  string message = 1;\n+\n+  // The major version in which the deprecated configuration will no longer work.\n+  string removal_version = 2;\n+}\n+\n+// BotInstanceHealthStatus describes the healthiness of a `tbot` service.\n+enum BotInstanceHealthStatus {\n+  // The enum zero-value, it means no status was included.\n+  BOT_INSTANCE_HEALTH_STATUS_UNSPECIFIED = 0;\n+\n+  // Means the service is still \"starting up\" and hasn't reported its status.\n+  BOT_INSTANCE_HEALTH_STATUS_INITIALIZING = 1;\n+\n+  // Means the service is healthy and ready to serve traffic, or it has\n+  // recently succeeded in generating an output.\n+  BOT_INSTANCE_HEALTH_STATUS_HEALTHY = 2;\n+\n+  // Means the service is failing to serve traffic or generate output.\n+  BOT_INSTANCE_HEALTH_STATUS_UNHEALTHY = 3;\n+}\n+\n+// BotInstanceServiceIdentifier uniquely identifies a `tbot` service.\n+message BotInstanceServiceIdentifier {\n+  // Type of service (e.g. database-tunnel, ssh-multiplexer).\n+  string type = 1;\n+\n+  // Name of the service, either given by the user or auto-generated.\n+  string name = 2;\n+}\n+\n+// BotInstanceServiceHealth is a snapshot of a `tbot` service's health.\n+message BotInstanceServiceHealth {\n+  // Service identifies the service.\n+  BotInstanceServiceIdentifier service = 1;\n+\n+  // Status describes the service's healthiness.\n+  BotInstanceHealthStatus status = 2;\n+\n+  // Reason is a human-readable explanation for the service's status. It might\n+  // include an error message.\n+  optional string reason = 3;\n+\n+  // UpdatedAt is the time at which the service's health last changed.\n+  google.protobuf.Timestamp updated_at = 4;\n+}\n+```\n+\n+### Data fields and expected quantities\n+\n+| Field | Description | Example | Quantity | Limitations |\n+| --- | --- | --- | --- | --- |\n+| Bot | A collection of roles and access assumed by `tbot` using a join token |  | 0-300+ per cluster |  |\n+| Bot instance | A unique joined instance of `tbot` in either a long-running or ephemeral environment |  | 1-300+ per bot |  |\n+| Authentication record | Created for each join or renewal |  | 0-10 per instance (max enforced) |  |\n+| Instance heartbeat | Self-reported by each bot instance |  | 0-10 per instance (max enforced) | Data is **not** validated by the auth server, and cannot be used for making access decisions. |\n+| Service | An independent, internal part of `tbot`. Generally maps 1:1 with configured outputs/tunnels. | `application-tunnel`, `workload-identity-api` | 1-30+ per heartbeat |  |\n+| Notice | An item created by `tbot` to capture an unusual event, configuration warning, or important status |  | 0-100+ per heartbeat |  |\n+| OS | Operating system from `runtime.GOOS` | linux, windows or darwin | Once per heartbeat |  |\n+| Version | Version of `tbot` | 18.1.0 | Once per heartbeat |  |\n+| Hostname |  |  | Once per heartbeat |  |\n+| Uptime | How long `tbot` has been running |  | Once per heartbeat |  |\n+| Raw config | `tbot`\u2019s local config (combination of YAML and CLI flags) as a protobuf struct |  | Once per heartbeat |  |\n+| Join token name |  |  | Once per auth |  |\n+| Join method |  | github, iam, kubernetes | Once per auth |  |\n+| Join attributes | Metadata specific to a join method | GitHub repository name | Once per auth |  |\n+| Health status |  | INITIALIZING, HEALTHY, UNHEALTHY,\n+UNKNOWN | Once per service |  |",
        "comment_created_at": "2025-09-03T15:41:14+00:00",
        "comment_author": "thedevelopnik",
        "comment_body": "Yes I agree. The above configs are specific examples I used in my demo env yesterday. I can make a video or something if you want, but it's trivial to repro. Just put an unrecognized field in any tbot.yaml (or teleport.yaml). If it causes a missing required field it will throw, but if it's just unrecognized we go right past it. Here's a new one I just tried: \r\n```\r\noutputs:\r\n- type: identity\r\n  ssh_configs: on\r\n  destination:\r\n    type: directory\r\n    path: /home/awesomeagent/machine-id\r\n```\r\n\r\n`ssh_config` is not a required field, so putting `ssh_configs: on` causes no problems.",
        "pr_file_module": null
      },
      {
        "comment_id": "2319381705",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 57888,
        "pr_file": "rfd/0222-bot-instances-at-scale.md",
        "discussion_id": "2316397048",
        "commented_code": "@@ -0,0 +1,464 @@\n+---\n+authors: Nick Marais (nicholas.marais@goteleport.com), Dan Upton (dan.upton@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+## Required Approvers\n+\n+- Engineering: @strideynet\n+- Product: @thedevelopnik\n+\n+# RFD 0222 - Bot Instances at Scale\n+\n+# What\n+\n+Machine ID bots allow non-human users to access resources in a Teleport cluster. They are useful for automating tasks, such as CI/CD runners, monitoring systems, and running scripts. The bot resource encapsulates a bot's access rights and roles. The `tbot` binary is used to start an instance of a bot and enrol it with the cluster (using a join token). When `tbot` starts and receives its credentials, a bot instance record is created in the cluster. Bot instance records expire when their credentials expire. Bot instances can be long-running processes which periodically renew their credentials, or short-lived, ephemeral processes which are created and destroyed on demand. Many instances of the same bot can exist at once, and clusters can have many thousands of bot instances.\n+\n+This proposal seeks to address the pain points of configuring and running a large fleet of Machine ID bots. It will focus solely on the Day 1 experience, and users are expected to be familiar with Teleport in general as well as the config and setup of their respective clusters. Day 0 tutorialization of setup steps and guided beginner scenarios are left to other initiatives.\n+\n+# Why\n+\n+As adoption of Machine & Workload ID increases, in part due to the drive to increase efficiency through automation as well as trends like Agentive AI, customers expect managing large fleets of bots to be simple and easy.\n+\n+It\u2019s the responsibility of the **infrastructure security team** to own and manage the Teleport cluster and enrol protected resources. For teams which make extensive use of Machine ID, it has become cumbersome to manage many bots and thousands of instances. Where Dev/Dev Ops teams deploy bot instances themselves, it can be doubly difficult to coordinate upgrades and security initiatives.\n+\n+# Details\n+\n+## UX\n+\n+### User Stories\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are blocking a safe cluster upgrade (major) due to their version.**\n+\n+The upgrade process can vary depending on the flavour of Teleport in use (cloud, oss, etc), and how it\u2019s deployed. A common step is to query for agents running a version which would become incompatible should an upgrade be done - using `tctl inventory ls --older-than=v18.0.0`. This command does not include bot instances, and `tctl bots instances ls` doesn\u2019t return versions numbers for instances.\n+\n+As such, it is a difficult task to identify bot instances that may be running an old version of `tbot`. This is especially difficult at scale. The current bot instance list available in the web UI allows filtering by version, although it\u2019s a text search and it is not aware of semantic versioning - finding versions older than a given version is not possible.\n+\n+A breakdown of active instance versions will make the process of monitoring the version status easy at a glance, as well as provide convenient links to filter the instance list for versions already one or more major versions behind the control plane (thereby preventing a safe upgrade). To facilitate this in the web UI, the filter will allow queries such as `older_than(version, \"18.1.0\")`. The instance list will also indicate the status of an instance\u2019s most recent version (as up-to-date, upgrade available, patch available, or incompatible). For the CLI, the `query` flag can be used to filter instances (e.g. `tctl bots instances ls --query=older_than(version, \"18.1.0\")`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances, across all Bots, are running vulnerable versions.**\n+\n+Currently in the web UI the instances list can be filtered by version, but this is a text search and it is not aware of semantic versioning. It\u2019s possible to find a specific version number, but it\u2019s not easy to isolate a range of versions, such as \u201c>18 & <18.2.1\u201d, which is likely required to find instances between a vulnerable and patched version.\n+\n+To support this use-case, the filter for bot instances will support the predicate language and allow queries such as `newer_than_or_equal(version, \"18.1.0\") && older_than(version, \"19.0.0\")`. This works though the web UI and the CLI (`tctl`).\n+\n+**As a cluster owner (Infrastructure Security team), I want to know which Bot Instances are running with deprecated/problematic configuration.**\n+\n+Issues in `tbot` (or just typos) can be difficult to detect and logs may not adequately highlight these. To improve the rate of these events reaching users, `tbot` will detect and collate notices which are sent with the next heartbeat. They will then be available to view for a bot instance. To help in situations where it\u2019s infeasible to check each individual instance, notices will be summarized by title and presented in aggregate form. Each aggregated item will be selectable and will filter the bot instances list. An advanced filter such as `contains(notices, \"Proxy URL not set\")` will be applied.\n+\n+**As a Bot Instance owner (Dev/Dev Ops team), I'd like help in understanding why my Bot Instance is not working properly.**\n+\n+For somebody diagnosing an issue with `tbot`, they\u2019re likely to have access to the `tbot` log output. Such as;\n+\n+```\n+INFO [TBOT:IDENTITY] Fetched new bot identity identity:mwi-demo-aws-manager, id=5c6af2e6-13a4-48c1-855f-74d8b8e01d86 | valid: after=2025-08-21T12:10:15Z, before=2025-08-21T12:31:13Z, duration=20m58s | kind=tls, renewable=false, disallow-reissue=false, roles=[bot-mwi-demo-aws-manager], principals=[-teleport-internal-join], generation=1 tbot/service_bot_identity.go:224\n+```\n+\n+This log entry contains the bot name (as `identity`) and the instance\u2019s ID. The instance ID can be used to filter the instances list in the web UI, and should make finding the relevant instance easy. In the CLI (`tctl`), both the bot name and instance ID are required to perform the look-up; `tctl bots instances show [bot_name]/[instance_id]`.\n+\n+Once found, in the web UI or CLI, the instance's details can be seen. Here a health status can be found for each `tbot` service (outputs, tunnels, etc), which includes failure info for those services which are unhealthy. Additionally, a listing of all notices raised by the instance in it\u2019s last run can be viewed, which may reveal the root cause of a failure. Notices are raised by `tbot` for actionable items that require attention, such as invalid config or deprecations.\n+\n+### Instances dashboard\n+\n+This mock-up shows the bot instance page as it would be when first arriving at the page. No filters are set, so all instances are available in the list. Results are paginated, and the default sort order is by recency - instances with the most recent heartbeat are displayed first. Sorting can be toggled between ascending and descending, and the following sort fields are available; recency, version, hostname. Filtering can be performed using a basic text search over supported fields, or an advanced search using the Teleport predicate language.\n+\n+The right-hand half of the page displays the dashboard, which is a summary over all instances. The visualizations use aggregate data prepared and updated by the auth server. An indication of when the data current is provided, as well as a refresh button which retrieves the most recently available data. A selection of timeframes is also available.\n+\n+The Activity visualization shows the number of events (joins, authentications, and heartbeats) that occurred over time. This can be used to identify trends and patterns in activity. It is non-interactive.\n+\n+The Upgrade Status visualization show a summary of all instances grouped by upgrade status; up-to-date, upgrade available, patch available, or incompatible. Status labels are selectable, and will populate the advanced search with the appropriate filter. For example, if the auth server is running v18 the filter will be populated with `older_than(version, \"16.0.0\")` when a user selects \"not supported\".\n+\n+The Notices visualization shows a summary of all notices across all instances. They're conveniently grouped by notice title and a count is included. Each item is selectable, and will apply an advanced filter (e.g. `contains(notices, \"Proxy URL not set\")`). This visualization will be hidden if there is no data to display.\n+\n+![](assets/0222-dashboard.png)\n+\n+### Instance details\n+These mock-ups shows the state of the page once an item had been selected from the instances list by clicking it. The dashboard section is replaced by the selected instance's details.\n+\n+The overview tab is displayed by default when an instance is selected. It shows further information about the instance, the join token that was used to enrol, and a summary of service health.\n+\n+![](assets/0222-details-overview.png)\n+\n+The services tab shows a list of all configured services (or outputs). Each includes it's name, type and health status. If there is extra health info available (such as and error message), this is also displayed.\n+\n+![](assets/0222-details-services.png)\n+\n+The notices tab is a listing of all notices raised since the instance was started. Notices have a type, a title and a message body. Some notice types contain extra data such as a planned removal version for deprecation notices. An indication of when the notice was raised is included, and notices are ordered by most recent first and this is not user-configurable.\n+\n+![](assets/0222-details-notices.png)\n+\n+The configuration tab show the _effective_ `tbot` configuration as readonly yaml.\n+\n+![](assets/0222-details-config.png)\n+\n+### tctl bots instances ls --search [term] --query [tql]\n+\n+The list bot instances command will include extra information about each instance; version, health status and notices count. A search term or advanced query can be used to filter the results - in this case a filter summary is included below the results confirming the applied filter and giving a result count. The data is not paginated and all instances are returned and displayed.\n+\n+```diff\n+- ID                                         Join Method Hostname      Joined               Last Seen            Generation\n+- ------------------------------------------ ----------- ------------- -------------------- -------------------- ----------\n+- bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         ip-10-0-15-34 2025-08-29T06:09:26Z 2025-09-01T12:49:26Z 237\n+-\n++ ID                                         Join Method Version Hostname      Status    Notices Last Seen\n++ ------------------------------------------ ----------- ------- ------------- --------- ------- --------------------\n++ bot-1/d83b381d-b46c-4b92-a899-755991a6d0f5 iam         v18.2.1 ip-10-0-15-34 UNHEALTHY 6       2025-09-01T12:49:26Z\n++\n++ Filter:\n++ Search text: \"ip-10-0-15\"\n++ Query: older_than(version, \"18.0.0\")\n++ Results: 128\n++\n+To view more information on a particular instance, run:\n+\n+> /Users/nick.marais/.tsh/bin/tctl bots instances show [id]\n+```\n+\n+### tctl bots instances show [id]\n+\n+The show bot instance command gives an overall health summary as well as a listing of services and their respective health status. A list of notices is also included.\n+\n+```diff\n+Bot: w2w-demo-app-bot\n+ID:  d83b381d-b46c-4b92-a899-755991a6d0f5\n++ Status: UNHEALTHY\n+\n+Initial Authentication:\n+  Authenticated At: 2025-08-29T06:09:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       1\n+  Public Key:       <178 bytes>\n+\n+Latest Authentication:\n+  Authenticated At: 2025-09-01T12:49:26Z\n+  Join Method:      iam\n+  Join Token:       w2w-demo-web-bot\n+  Join Metadata:    meta:{join_token_name:\"w2w-demo-web-bot\" join_method:\"iam\"} iam:{account:\"668558765449\" arn:\"arn:aws:sts::668558765449:assumed-role/MWIw2wDemoInstance/i-0b7667843950debfd\"}\n+  Generation:       237\n+  Public Key:       <178 bytes>\n+\n+Latest Heartbeat:\n+  Recorded At:  2025-09-01T12:39:26Z\n+  Is Startup:   false\n+  Version:      18.1.5\n+  Hostname:     ip-10-0-15-34\n+  Uptime:       78h30m0.539099441s\n+  Join Method:  iam\n+  One Shot:     false\n+  Architecture: arm64\n+  OS:           linux\n++\n++ Service status:\n++ Status    Name        Type                Reason         Updated At\n++ --------- ----------- ------------------- -------------- --------------------\n++ UNHEALTHY prod-aws-01 X509-output-service out of bananas 2025-09-01T12:49:26Z\n++\n++ Notices:\n++ Type                Service     Message               Raised At\n++ ------------------- ----------- --------------------- --------------------\n++ DEPRECATION_WARNING prod-aws-01 Lorem ipsum (v19.0.0) 2025-09-01T12:49:26Z\n+\n+To view a full, machine-readable record including past heartbeats and authentication records, run:\n+> /Users/nick.marais/.tsh/bin/tctl get bot_instance/w2w-demo-app-bot/d83b381d-b46c-4b92-a899-755991a6d0f5\n+\n+To onboard a new instance for this bot, run:\n+> /Users/nick.marais/.tsh/bin/tctl bots instances add w2w-demo-app-bot\n+```\n+\n+### Predicate language for instance filters\n+\n+The predicate language will be used to provide advanced filtering for instances. The filter query will be applied in the same way the existing filters work, and no changes to indexes are required. As items are read out of the backend storage, they are filtered one by one until the page size is reached or the end of the list. For a narrow filter, many or even all records will be scanned - this inefficiency is mitigated by the in-memory caching layer's performance.\n+\n+Instance-specific functions will be supported by implementing a custom `typical.ParserSpec`, such as those in the table below.\n+\n+| Purpose | Example |\n+| --- | --- |\n+| Find instances running versions less than a given version - based on the most recent heartbeat | `older_than(version, 18.1.0)` |\n+| Find instances running versions between a vulnerable version and a fix version - based on the most recent heartbeat | `newer_than_or_equal(version, \"18.0.0\") && older_than(version, \"18.1.0\")` |\n+| Find instances which have a particular notice (by title) | `contains(notices, \"Proxy URL not set\")` |\n+\n+## Privacy and Security\n+\n+The proposed changes are mainly capturing extra data and presenting it in the web UI. As such, it is light on security and privacy concerns.\n+\n+In order to allow instance config to be viewed without needing log in to the machine running `tbot` the complete configuration will be included in the start-up heartbeat and stored for the lifetime of the instance. Instead of capturing the config YAML verbatim, the _effective_ configuration will be used. This includes any environment variable and flag overrides. For security reasons, the join token will be omitted. For privacy reasons, any unrecognized values as well as comments will also be omitted. There may be other sensitive information such as service/output names, but these are only visible to authorised users.\n+\n+## Heartbeat additions\n+\n+```protobuf\n+message BotInstanceStatusHeartbeat {\n+  // ...[snip]...\n+\n+  // The health of the services/output `tbot` is running.\n+  repeated BotInstanceServiceHealth service_health = 10;\n+\n+  // tbot configuration, sourced from YAML configuration file and CLI flags.\n+  //\n+  // Will only be sent on startup. Could later be whenever the configuration\n+  // changes if we support reloading by sending SIGHUP or something.\n+  structpb.Struct config = 11;\n+\n+  // Kind identifies whether the bot is running in the tbot binary or embedded\n+  // in another component.\n+  BotKind kind = 12;\n+\n+  // Notices emitted since the last heartbeat.\n+  //\n+  // The server will clear any previous notices if `is_startup` is true, so that\n+  // editing tbot's configuration and restarting it clears any warnings from a\n+  // previous bad configuration.\n+  repeated BotInstanceNotice notices = 13;\n+}\n+\n+// BotKind identifies whether the bot is the tbot binary or embedded in another\n+// component.\n+enum BotKind {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_KIND_UNSET = 0;\n+\n+  // Means the bot is running the tbot binary.\n+  BOT_KIND_TBOT_BINARY = 1;\n+\n+  // Means the bot is running inside the Teleport Terraform provider.\n+  BOT_KIND_TERRAFORM_PROVIDER = 2;\n+\n+  // Means the bot is running inside the Teleport Kubernetes operator.\n+  BOT_KIND_KUBERNETES_OPERATOR = 3;\n+}\n+\n+// BotInstanceNotice contains an error message, deprecation warning, etc. emitted\n+// by the bot instance.\n+message BotInstanceNotice {\n+  // ID is a client-generated identifier (i.e. UUID) that can be used by the\n+  // auth server to detect and discard duplicate notices caused by partially\n+  // failed heartbeat RPCs.\n+  string id = 1;\n+\n+  // Type of notice (e.g. deprecation or warning).\n+  BotInstanceNoticeType type = 1;\n+\n+  // Service this notice relates to (or nil if it relates to the bot instance\n+  // more generally).\n+  optional BotInstanceService service = 2;\n+\n+  // Timestamp at which this notice was emitted.\n+  google.protobuf.Timestamp timestamp = 3;\n+\n+  oneof notice {\n+    // Deprecation warning details.\n+    BotInstanceDeprecationWarning deprecation_warning = 4;\n+\n+    // Generic message text.\n+    string message = 5;\n+  }\n+}\n+\n+// BotInstanceNoticeType identifies the type of notice.\n+enum BotInstanceNoticeType {\n+  // The enum zero-value, it means no notice type was included.\n+  BOT_INSTANCE_NOTICE_TYPE_UNSPECIFIED = 0;\n+\n+  // Means the notice contains a warning that the user is using a configuration\n+  // option that will be removed in a future release.\n+  BOT_INSTANCE_NOTICE_TYPE_DEPRECATION_WARNING = 1;\n+\n+  // Means the notice contains a generic error message.\n+  BOT_INSTANCE_NOTICE_TYPE_MESSAGE = 2;\n+}\n+\n+// BotInstanceDeprecationWarning contains the details of a deprecation warning.\n+message BotInstanceDeprecationWarning {\n+  // Message explaining the deprecation.\n+  string message = 1;\n+\n+  // The major version in which the deprecated configuration will no longer work.\n+  string removal_version = 2;\n+}\n+\n+// BotInstanceHealthStatus describes the healthiness of a `tbot` service.\n+enum BotInstanceHealthStatus {\n+  // The enum zero-value, it means no status was included.\n+  BOT_INSTANCE_HEALTH_STATUS_UNSPECIFIED = 0;\n+\n+  // Means the service is still \"starting up\" and hasn't reported its status.\n+  BOT_INSTANCE_HEALTH_STATUS_INITIALIZING = 1;\n+\n+  // Means the service is healthy and ready to serve traffic, or it has\n+  // recently succeeded in generating an output.\n+  BOT_INSTANCE_HEALTH_STATUS_HEALTHY = 2;\n+\n+  // Means the service is failing to serve traffic or generate output.\n+  BOT_INSTANCE_HEALTH_STATUS_UNHEALTHY = 3;\n+}\n+\n+// BotInstanceServiceIdentifier uniquely identifies a `tbot` service.\n+message BotInstanceServiceIdentifier {\n+  // Type of service (e.g. database-tunnel, ssh-multiplexer).\n+  string type = 1;\n+\n+  // Name of the service, either given by the user or auto-generated.\n+  string name = 2;\n+}\n+\n+// BotInstanceServiceHealth is a snapshot of a `tbot` service's health.\n+message BotInstanceServiceHealth {\n+  // Service identifies the service.\n+  BotInstanceServiceIdentifier service = 1;\n+\n+  // Status describes the service's healthiness.\n+  BotInstanceHealthStatus status = 2;\n+\n+  // Reason is a human-readable explanation for the service's status. It might\n+  // include an error message.\n+  optional string reason = 3;\n+\n+  // UpdatedAt is the time at which the service's health last changed.\n+  google.protobuf.Timestamp updated_at = 4;\n+}\n+```\n+\n+### Data fields and expected quantities\n+\n+| Field | Description | Example | Quantity | Limitations |\n+| --- | --- | --- | --- | --- |\n+| Bot | A collection of roles and access assumed by `tbot` using a join token |  | 0-300+ per cluster |  |\n+| Bot instance | A unique joined instance of `tbot` in either a long-running or ephemeral environment |  | 1-300+ per bot |  |\n+| Authentication record | Created for each join or renewal |  | 0-10 per instance (max enforced) |  |\n+| Instance heartbeat | Self-reported by each bot instance |  | 0-10 per instance (max enforced) | Data is **not** validated by the auth server, and cannot be used for making access decisions. |\n+| Service | An independent, internal part of `tbot`. Generally maps 1:1 with configured outputs/tunnels. | `application-tunnel`, `workload-identity-api` | 1-30+ per heartbeat |  |\n+| Notice | An item created by `tbot` to capture an unusual event, configuration warning, or important status |  | 0-100+ per heartbeat |  |\n+| OS | Operating system from `runtime.GOOS` | linux, windows or darwin | Once per heartbeat |  |\n+| Version | Version of `tbot` | 18.1.0 | Once per heartbeat |  |\n+| Hostname |  |  | Once per heartbeat |  |\n+| Uptime | How long `tbot` has been running |  | Once per heartbeat |  |\n+| Raw config | `tbot`\u2019s local config (combination of YAML and CLI flags) as a protobuf struct |  | Once per heartbeat |  |\n+| Join token name |  |  | Once per auth |  |\n+| Join method |  | github, iam, kubernetes | Once per auth |  |\n+| Join attributes | Metadata specific to a join method | GitHub repository name | Once per auth |  |\n+| Health status |  | INITIALIZING, HEALTHY, UNHEALTHY,\n+UNKNOWN | Once per service |  |",
        "comment_created_at": "2025-09-03T15:41:57+00:00",
        "comment_author": "thedevelopnik",
        "comment_body": "That said, I agree we've moved outside the scope of this RFD and this should be a future improvement. I'll work with you on filing a ticket later.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2203685433",
    "pr_number": 56214,
    "pr_file": "rfd/0219-k8s-rbac-simplification.md",
    "created_at": "2025-07-14T01:39:07+00:00",
    "commented_code": "+---\n+authors: Guillaume Charmes (guillaume.charmes@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0219 - Kubernetes RBAC Simplification\n+\n+## Required Approvers\n+\n+- Engineering: @rosstimothy && @hugoShaka && @tigrato\n+Product: klizhentas\n+\n+## What\n+\n+The purpose of this RFD is to propose a simplification of the Kubernetes\n+RBAC (Role-Based Access Control) within the Teleport project.\n+\n+The goal is to streamline the user experience and reduce the complexity of\n+getting up and running on day 1 and managing permissions later on.\n+\n+## Why\n+\n+Today, the Kubernetes RBAC in Teleport is complex and can be difficult for\n+users to manage.\n+A proper setup requires some external Kubernetes config and matching role\n+setup.\n+This complexity leads to misconfigurations and security issues.\n+\n+Simplifying the RBAC model will help improve usability and security.\n+\n+### References\n+\n+A common issue currently is the un-intuitive result of complex rule sets and\n+more critically, difficult to get setup on day 1.\n+\n+- @klizhentas struggling to setup a cluster following the `self-hosted` flow:\n+  (internal) <https://gravitational.slack.com/archives/C03SULRAAG3/p1715394587181739>\n+- Customer confusion on role resolution:\n+  - <https://github.com/gravitational/teleport/issues/45475>\n+  - (internal) <https://goteleport.slack.com/archives/C0582MBSMHN/p1738187497666819>\n+- Unexpected `exec` grant on read-only user:\n+  (internal) <https://gravitational.slack.com/archives/C32M8FP1V/p1739462454321459?thread_ts=1739462384.714419&cid=C32M8FP1V>\n+\n+## Goals\n+\n+- Discourage the use of `kubernetes_groups` and `kubernetes_users` fields from\n+role with custom values in favor of a preset cluster-admin one.\n+- Clarify / Improve documentation for the various ways to setup Kubernetes in\n+  Teleport.\n+\n+## Proposal\n+\n+### Background\n+\n+Currently, when enrolling a Kubernetes cluster and setting it up to use the\n+group `system:masters`, everything works as expected (except on GKE Autopilot\n+where this group is forbidden).\n+Teleport is setup to impersonate to yield reduced permissions to the user\n+based on the `kubernetes_resources` and `kubernetes_label` fields from the\n+Teleport role.\n+\n+### Prior work\n+\n+While initially planned to be included in this RFD, due to customer requests,\n+the following has already been implemented and shipped:\n+\n+- Support arbitrary resources, including CRDs #54426\n+- Simplify Namespace/Kind behavior #54938\n+- Handle all known resources on the Teleport side #55099\n+\n+Given this, as RBAC has already been simplified quite a lot, a scoped down\n+version of the proposal would be to focus on new installs without impacting\n+existing customers.\n+\n+### Authoritative RBAC Documentation\n+\n+To simplify initial setup and long-term management, we'll update the\n+documentation to encourage users to use a cluster-wide admin group that will be\n+created by provisioning scripts or auto-discovery.\n+This will allow users to avoid the complexity of managing RBAC in multiple\n+places, avoid confusion around the `exec` subresource.\n+While we won't change or remove the `kubernetes_groups` and `kubernetes_users`\n+fields, we will discourage their use, allowing existing customers as well as\n+advanced users to continue leveraging the underlying Kubernetes RBAC for more\n+special use cases.\n+\n+### Provisioning\n+\n+To enable easy setup, provisioning methods will be updated to create an\n+cluster admin ClusterRole as well as a ClusterRoleBinding with a known group\n+name.\n+\n+#### Helm Chart\n+\n+The Helm Chart will provide the ability to specify custom names for all\n+resources, (clusterrole, clusterrolebinding) and for the Group.\n+\n+Example:\n+\n+```yaml\n+roles: kube,app,discovery\n+authToken: foo\n+proxyAddr: example.devteleport.com:443\n+kubeClusterName: myCluster\n+rbac:\n+  adminClusterRoleName: teleport-agent-cluster-admin\n+  adminClusterRoleBindingName: teleport-agent-cluster-admin\n+  adminGroupName: teleport-agent-cluster-admin\n+```\n+\n+#### Provision Script\n+\n+To adjust the values when provisioning with the provided script, environment\n+variables will be used to follow the existing pattern.\n+\n+Example:\n+\n+```bash\n+TELEPORT_KUBE_ADMIN_GROUP=teleport-cluster-admin ./get-kubeconfig.sh\n+```\n+\n+#### Auto Discovery\n+\n+##### EKS / AKS\n+\n+For both EKS and AKS, auto-discovery will be updated to create the proper admin\n+roles/bindings.\n+\n+##### GKE\n+\n+For GKE, we will need to change the documented GCP IAM role to enable admin\n+privileges.\n+\n+#### Auto-update\n+\n+Auto-update will not attempt to apply the new model, with the assumption that\n+existing setups are already working.\n+\n+### UX\n+\n+#### Exec confusion\n+\n+As the underlying Kubernetes RBAC gets discouraged, it will help with the\n+confusion around _exec_ being allowed with a `get` verb is gone.\n+Teleport uses a dedicated verb to control access to `exec`.\n+\n+#### Resource enrollment UI\n+\n+On the Web UI, the initial page generates a _Helm_ command line.\n+\n+After enrollment, a test page is shown, prompting for a `kubernetes_groups`\n+value, which defaults to the user's trait.\n+This step will be updated to verify the underlying Kubernetes RBAC permissions\n+and notify the user if the impersonation doesn't have sufficient\n+permissions to be authoritative.\n+\n+#### Role Editor\n+\n+The Web UI Role Editor will move the `kubernetes_groups` and `kubernetes_users`\n+based on the role version dropdown to be less prominent (currently they are\n+the first thing shown). They will still be available under a \"summary\" toggle,\n+folded by default, to update as needed for advanced use cases and existing\n+customers.\n+\n+- https://github.com/gravitational/teleport/blob/22eb8c6645909a26d1493d01d291e222a87b35e6/web/packages/teleport/src/Roles/RoleEditor/StandardEditor/Resources.tsx#L291-L321\n+\n+#### Error management\n+\n+The error messages when using `kubectl` will be improved to include a link to\n+the documentation and more details on what is expected. This will help with\n+initial custom setups that skipped the provided provisioning scripts.\n+\n+#### New preset role\n+\n+To help onboarding, as currently no preset role grants Kubernetes access,\n+a new preset role will be added, `kube-editor`, which will have wildcard access\n+to Kubernetes resources as well as the predefined `kubernetes_groups` mapping\n+to the cluster admin group.\n+\n+```yaml\n+kind: role\n+version: v8\n+metadata:\n+  name: kube-editor\n+spec:\n+  allow:\n+    kubernetes_groups:\n+      - teleport-cluster-admin\n+    kubernetes_labels:\n+      '*': '*'\n+    kubernetes_resources:\n+      - api_group: '*'\n+        kind: '*'\n+        namespace: '*'\n+        name: '*'\n+        verbs:\n+          - '*'\n+```\n+\n+### User flow\n+\n+#### Current flow\n+\n+- Day 0 - Initial setup:\n+  - User installs Teleport and sets up a Kubernetes cluster.\n+  - User provisions the Kubernetes cluster using a script, discovery, or Helm\n+    chart to create the impersonation ClusterRole/ClusterRoleBinding.\n+  - User configures Kubernetes RBAC with a custom ClusterRole with some\n+    permissions\n+  - User configures Kubernetes RBAC with a custom ClusterRoleBinding, which\n+    needs to be understood matches with the `kubernetes_groups` field in the\n+    Teleport role.\n+  - User configures Teleport role with `kubernetes_groups`, (unclear what\n+    `kubernetes_users` does from the docs) fields to match the custom\n+    ClusterRoleBinding.\n+  - User configures Teleport role with `kubernetes_resources` and\n+    `kubernetes_labels` to match to match or reduce the permissions granted by\n+    the custom ClusterRole.\n+- Day 1 - Ongoing management:\n+  - User needs to understand the interaction between the Teleport role and the\n+    underlying Kubernetes RBAC.\n+  - User may need to update the Teleport role if the underlying Kubernetes\n+    RBAC changes.\n+  - No clear guidance on how to which permission should be set where (Teleport\n+    or Kubernetes).\n+  - User may need to troubleshoot unexpected permissions or access issues.\n+\n+#### Proposed flow",
    "repo_full_name": "gravitational/teleport",
    "discussion_comments": [
      {
        "comment_id": "2203685433",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 56214,
        "pr_file": "rfd/0219-k8s-rbac-simplification.md",
        "discussion_id": "2203685433",
        "commented_code": "@@ -0,0 +1,242 @@\n+---\n+authors: Guillaume Charmes (guillaume.charmes@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0219 - Kubernetes RBAC Simplification\n+\n+## Required Approvers\n+\n+- Engineering: @rosstimothy && @hugoShaka && @tigrato\n+Product: klizhentas\n+\n+## What\n+\n+The purpose of this RFD is to propose a simplification of the Kubernetes\n+RBAC (Role-Based Access Control) within the Teleport project.\n+\n+The goal is to streamline the user experience and reduce the complexity of\n+getting up and running on day 1 and managing permissions later on.\n+\n+## Why\n+\n+Today, the Kubernetes RBAC in Teleport is complex and can be difficult for\n+users to manage.\n+A proper setup requires some external Kubernetes config and matching role\n+setup.\n+This complexity leads to misconfigurations and security issues.\n+\n+Simplifying the RBAC model will help improve usability and security.\n+\n+### References\n+\n+A common issue currently is the un-intuitive result of complex rule sets and\n+more critically, difficult to get setup on day 1.\n+\n+- @klizhentas struggling to setup a cluster following the `self-hosted` flow:\n+  (internal) <https://gravitational.slack.com/archives/C03SULRAAG3/p1715394587181739>\n+- Customer confusion on role resolution:\n+  - <https://github.com/gravitational/teleport/issues/45475>\n+  - (internal) <https://goteleport.slack.com/archives/C0582MBSMHN/p1738187497666819>\n+- Unexpected `exec` grant on read-only user:\n+  (internal) <https://gravitational.slack.com/archives/C32M8FP1V/p1739462454321459?thread_ts=1739462384.714419&cid=C32M8FP1V>\n+\n+## Goals\n+\n+- Discourage the use of `kubernetes_groups` and `kubernetes_users` fields from\n+role with custom values in favor of a preset cluster-admin one.\n+- Clarify / Improve documentation for the various ways to setup Kubernetes in\n+  Teleport.\n+\n+## Proposal\n+\n+### Background\n+\n+Currently, when enrolling a Kubernetes cluster and setting it up to use the\n+group `system:masters`, everything works as expected (except on GKE Autopilot\n+where this group is forbidden).\n+Teleport is setup to impersonate to yield reduced permissions to the user\n+based on the `kubernetes_resources` and `kubernetes_label` fields from the\n+Teleport role.\n+\n+### Prior work\n+\n+While initially planned to be included in this RFD, due to customer requests,\n+the following has already been implemented and shipped:\n+\n+- Support arbitrary resources, including CRDs #54426\n+- Simplify Namespace/Kind behavior #54938\n+- Handle all known resources on the Teleport side #55099\n+\n+Given this, as RBAC has already been simplified quite a lot, a scoped down\n+version of the proposal would be to focus on new installs without impacting\n+existing customers.\n+\n+### Authoritative RBAC Documentation\n+\n+To simplify initial setup and long-term management, we'll update the\n+documentation to encourage users to use a cluster-wide admin group that will be\n+created by provisioning scripts or auto-discovery.\n+This will allow users to avoid the complexity of managing RBAC in multiple\n+places, avoid confusion around the `exec` subresource.\n+While we won't change or remove the `kubernetes_groups` and `kubernetes_users`\n+fields, we will discourage their use, allowing existing customers as well as\n+advanced users to continue leveraging the underlying Kubernetes RBAC for more\n+special use cases.\n+\n+### Provisioning\n+\n+To enable easy setup, provisioning methods will be updated to create an\n+cluster admin ClusterRole as well as a ClusterRoleBinding with a known group\n+name.\n+\n+#### Helm Chart\n+\n+The Helm Chart will provide the ability to specify custom names for all\n+resources, (clusterrole, clusterrolebinding) and for the Group.\n+\n+Example:\n+\n+```yaml\n+roles: kube,app,discovery\n+authToken: foo\n+proxyAddr: example.devteleport.com:443\n+kubeClusterName: myCluster\n+rbac:\n+  adminClusterRoleName: teleport-agent-cluster-admin\n+  adminClusterRoleBindingName: teleport-agent-cluster-admin\n+  adminGroupName: teleport-agent-cluster-admin\n+```\n+\n+#### Provision Script\n+\n+To adjust the values when provisioning with the provided script, environment\n+variables will be used to follow the existing pattern.\n+\n+Example:\n+\n+```bash\n+TELEPORT_KUBE_ADMIN_GROUP=teleport-cluster-admin ./get-kubeconfig.sh\n+```\n+\n+#### Auto Discovery\n+\n+##### EKS / AKS\n+\n+For both EKS and AKS, auto-discovery will be updated to create the proper admin\n+roles/bindings.\n+\n+##### GKE\n+\n+For GKE, we will need to change the documented GCP IAM role to enable admin\n+privileges.\n+\n+#### Auto-update\n+\n+Auto-update will not attempt to apply the new model, with the assumption that\n+existing setups are already working.\n+\n+### UX\n+\n+#### Exec confusion\n+\n+As the underlying Kubernetes RBAC gets discouraged, it will help with the\n+confusion around _exec_ being allowed with a `get` verb is gone.\n+Teleport uses a dedicated verb to control access to `exec`.\n+\n+#### Resource enrollment UI\n+\n+On the Web UI, the initial page generates a _Helm_ command line.\n+\n+After enrollment, a test page is shown, prompting for a `kubernetes_groups`\n+value, which defaults to the user's trait.\n+This step will be updated to verify the underlying Kubernetes RBAC permissions\n+and notify the user if the impersonation doesn't have sufficient\n+permissions to be authoritative.\n+\n+#### Role Editor\n+\n+The Web UI Role Editor will move the `kubernetes_groups` and `kubernetes_users`\n+based on the role version dropdown to be less prominent (currently they are\n+the first thing shown). They will still be available under a \"summary\" toggle,\n+folded by default, to update as needed for advanced use cases and existing\n+customers.\n+\n+- https://github.com/gravitational/teleport/blob/22eb8c6645909a26d1493d01d291e222a87b35e6/web/packages/teleport/src/Roles/RoleEditor/StandardEditor/Resources.tsx#L291-L321\n+\n+#### Error management\n+\n+The error messages when using `kubectl` will be improved to include a link to\n+the documentation and more details on what is expected. This will help with\n+initial custom setups that skipped the provided provisioning scripts.\n+\n+#### New preset role\n+\n+To help onboarding, as currently no preset role grants Kubernetes access,\n+a new preset role will be added, `kube-editor`, which will have wildcard access\n+to Kubernetes resources as well as the predefined `kubernetes_groups` mapping\n+to the cluster admin group.\n+\n+```yaml\n+kind: role\n+version: v8\n+metadata:\n+  name: kube-editor\n+spec:\n+  allow:\n+    kubernetes_groups:\n+      - teleport-cluster-admin\n+    kubernetes_labels:\n+      '*': '*'\n+    kubernetes_resources:\n+      - api_group: '*'\n+        kind: '*'\n+        namespace: '*'\n+        name: '*'\n+        verbs:\n+          - '*'\n+```\n+\n+### User flow\n+\n+#### Current flow\n+\n+- Day 0 - Initial setup:\n+  - User installs Teleport and sets up a Kubernetes cluster.\n+  - User provisions the Kubernetes cluster using a script, discovery, or Helm\n+    chart to create the impersonation ClusterRole/ClusterRoleBinding.\n+  - User configures Kubernetes RBAC with a custom ClusterRole with some\n+    permissions\n+  - User configures Kubernetes RBAC with a custom ClusterRoleBinding, which\n+    needs to be understood matches with the `kubernetes_groups` field in the\n+    Teleport role.\n+  - User configures Teleport role with `kubernetes_groups`, (unclear what\n+    `kubernetes_users` does from the docs) fields to match the custom\n+    ClusterRoleBinding.\n+  - User configures Teleport role with `kubernetes_resources` and\n+    `kubernetes_labels` to match to match or reduce the permissions granted by\n+    the custom ClusterRole.\n+- Day 1 - Ongoing management:\n+  - User needs to understand the interaction between the Teleport role and the\n+    underlying Kubernetes RBAC.\n+  - User may need to update the Teleport role if the underlying Kubernetes\n+    RBAC changes.\n+  - No clear guidance on how to which permission should be set where (Teleport\n+    or Kubernetes).\n+  - User may need to troubleshoot unexpected permissions or access issues.\n+\n+#### Proposed flow",
        "comment_created_at": "2025-07-14T01:39:07+00:00",
        "comment_author": "klizhentas",
        "comment_body": "I think what's confusing is why we create a `kube-editor` preset role and why in this role there is a mapping to `kubernetes_groups`, how is this mapping utilized and how is it mapped to kubernetes RBAC?",
        "pr_file_module": null
      },
      {
        "comment_id": "2203752994",
        "repo_full_name": "gravitational/teleport",
        "pr_number": 56214,
        "pr_file": "rfd/0219-k8s-rbac-simplification.md",
        "discussion_id": "2203685433",
        "commented_code": "@@ -0,0 +1,242 @@\n+---\n+authors: Guillaume Charmes (guillaume.charmes@goteleport.com)\n+state: draft\n+---\n+\n+# RFD 0219 - Kubernetes RBAC Simplification\n+\n+## Required Approvers\n+\n+- Engineering: @rosstimothy && @hugoShaka && @tigrato\n+Product: klizhentas\n+\n+## What\n+\n+The purpose of this RFD is to propose a simplification of the Kubernetes\n+RBAC (Role-Based Access Control) within the Teleport project.\n+\n+The goal is to streamline the user experience and reduce the complexity of\n+getting up and running on day 1 and managing permissions later on.\n+\n+## Why\n+\n+Today, the Kubernetes RBAC in Teleport is complex and can be difficult for\n+users to manage.\n+A proper setup requires some external Kubernetes config and matching role\n+setup.\n+This complexity leads to misconfigurations and security issues.\n+\n+Simplifying the RBAC model will help improve usability and security.\n+\n+### References\n+\n+A common issue currently is the un-intuitive result of complex rule sets and\n+more critically, difficult to get setup on day 1.\n+\n+- @klizhentas struggling to setup a cluster following the `self-hosted` flow:\n+  (internal) <https://gravitational.slack.com/archives/C03SULRAAG3/p1715394587181739>\n+- Customer confusion on role resolution:\n+  - <https://github.com/gravitational/teleport/issues/45475>\n+  - (internal) <https://goteleport.slack.com/archives/C0582MBSMHN/p1738187497666819>\n+- Unexpected `exec` grant on read-only user:\n+  (internal) <https://gravitational.slack.com/archives/C32M8FP1V/p1739462454321459?thread_ts=1739462384.714419&cid=C32M8FP1V>\n+\n+## Goals\n+\n+- Discourage the use of `kubernetes_groups` and `kubernetes_users` fields from\n+role with custom values in favor of a preset cluster-admin one.\n+- Clarify / Improve documentation for the various ways to setup Kubernetes in\n+  Teleport.\n+\n+## Proposal\n+\n+### Background\n+\n+Currently, when enrolling a Kubernetes cluster and setting it up to use the\n+group `system:masters`, everything works as expected (except on GKE Autopilot\n+where this group is forbidden).\n+Teleport is setup to impersonate to yield reduced permissions to the user\n+based on the `kubernetes_resources` and `kubernetes_label` fields from the\n+Teleport role.\n+\n+### Prior work\n+\n+While initially planned to be included in this RFD, due to customer requests,\n+the following has already been implemented and shipped:\n+\n+- Support arbitrary resources, including CRDs #54426\n+- Simplify Namespace/Kind behavior #54938\n+- Handle all known resources on the Teleport side #55099\n+\n+Given this, as RBAC has already been simplified quite a lot, a scoped down\n+version of the proposal would be to focus on new installs without impacting\n+existing customers.\n+\n+### Authoritative RBAC Documentation\n+\n+To simplify initial setup and long-term management, we'll update the\n+documentation to encourage users to use a cluster-wide admin group that will be\n+created by provisioning scripts or auto-discovery.\n+This will allow users to avoid the complexity of managing RBAC in multiple\n+places, avoid confusion around the `exec` subresource.\n+While we won't change or remove the `kubernetes_groups` and `kubernetes_users`\n+fields, we will discourage their use, allowing existing customers as well as\n+advanced users to continue leveraging the underlying Kubernetes RBAC for more\n+special use cases.\n+\n+### Provisioning\n+\n+To enable easy setup, provisioning methods will be updated to create an\n+cluster admin ClusterRole as well as a ClusterRoleBinding with a known group\n+name.\n+\n+#### Helm Chart\n+\n+The Helm Chart will provide the ability to specify custom names for all\n+resources, (clusterrole, clusterrolebinding) and for the Group.\n+\n+Example:\n+\n+```yaml\n+roles: kube,app,discovery\n+authToken: foo\n+proxyAddr: example.devteleport.com:443\n+kubeClusterName: myCluster\n+rbac:\n+  adminClusterRoleName: teleport-agent-cluster-admin\n+  adminClusterRoleBindingName: teleport-agent-cluster-admin\n+  adminGroupName: teleport-agent-cluster-admin\n+```\n+\n+#### Provision Script\n+\n+To adjust the values when provisioning with the provided script, environment\n+variables will be used to follow the existing pattern.\n+\n+Example:\n+\n+```bash\n+TELEPORT_KUBE_ADMIN_GROUP=teleport-cluster-admin ./get-kubeconfig.sh\n+```\n+\n+#### Auto Discovery\n+\n+##### EKS / AKS\n+\n+For both EKS and AKS, auto-discovery will be updated to create the proper admin\n+roles/bindings.\n+\n+##### GKE\n+\n+For GKE, we will need to change the documented GCP IAM role to enable admin\n+privileges.\n+\n+#### Auto-update\n+\n+Auto-update will not attempt to apply the new model, with the assumption that\n+existing setups are already working.\n+\n+### UX\n+\n+#### Exec confusion\n+\n+As the underlying Kubernetes RBAC gets discouraged, it will help with the\n+confusion around _exec_ being allowed with a `get` verb is gone.\n+Teleport uses a dedicated verb to control access to `exec`.\n+\n+#### Resource enrollment UI\n+\n+On the Web UI, the initial page generates a _Helm_ command line.\n+\n+After enrollment, a test page is shown, prompting for a `kubernetes_groups`\n+value, which defaults to the user's trait.\n+This step will be updated to verify the underlying Kubernetes RBAC permissions\n+and notify the user if the impersonation doesn't have sufficient\n+permissions to be authoritative.\n+\n+#### Role Editor\n+\n+The Web UI Role Editor will move the `kubernetes_groups` and `kubernetes_users`\n+based on the role version dropdown to be less prominent (currently they are\n+the first thing shown). They will still be available under a \"summary\" toggle,\n+folded by default, to update as needed for advanced use cases and existing\n+customers.\n+\n+- https://github.com/gravitational/teleport/blob/22eb8c6645909a26d1493d01d291e222a87b35e6/web/packages/teleport/src/Roles/RoleEditor/StandardEditor/Resources.tsx#L291-L321\n+\n+#### Error management\n+\n+The error messages when using `kubectl` will be improved to include a link to\n+the documentation and more details on what is expected. This will help with\n+initial custom setups that skipped the provided provisioning scripts.\n+\n+#### New preset role\n+\n+To help onboarding, as currently no preset role grants Kubernetes access,\n+a new preset role will be added, `kube-editor`, which will have wildcard access\n+to Kubernetes resources as well as the predefined `kubernetes_groups` mapping\n+to the cluster admin group.\n+\n+```yaml\n+kind: role\n+version: v8\n+metadata:\n+  name: kube-editor\n+spec:\n+  allow:\n+    kubernetes_groups:\n+      - teleport-cluster-admin\n+    kubernetes_labels:\n+      '*': '*'\n+    kubernetes_resources:\n+      - api_group: '*'\n+        kind: '*'\n+        namespace: '*'\n+        name: '*'\n+        verbs:\n+          - '*'\n+```\n+\n+### User flow\n+\n+#### Current flow\n+\n+- Day 0 - Initial setup:\n+  - User installs Teleport and sets up a Kubernetes cluster.\n+  - User provisions the Kubernetes cluster using a script, discovery, or Helm\n+    chart to create the impersonation ClusterRole/ClusterRoleBinding.\n+  - User configures Kubernetes RBAC with a custom ClusterRole with some\n+    permissions\n+  - User configures Kubernetes RBAC with a custom ClusterRoleBinding, which\n+    needs to be understood matches with the `kubernetes_groups` field in the\n+    Teleport role.\n+  - User configures Teleport role with `kubernetes_groups`, (unclear what\n+    `kubernetes_users` does from the docs) fields to match the custom\n+    ClusterRoleBinding.\n+  - User configures Teleport role with `kubernetes_resources` and\n+    `kubernetes_labels` to match to match or reduce the permissions granted by\n+    the custom ClusterRole.\n+- Day 1 - Ongoing management:\n+  - User needs to understand the interaction between the Teleport role and the\n+    underlying Kubernetes RBAC.\n+  - User may need to update the Teleport role if the underlying Kubernetes\n+    RBAC changes.\n+  - No clear guidance on how to which permission should be set where (Teleport\n+    or Kubernetes).\n+  - User may need to troubleshoot unexpected permissions or access issues.\n+\n+#### Proposed flow",
        "comment_created_at": "2025-07-14T02:46:03+00:00",
        "comment_author": "creack",
        "comment_body": "We create a new preset to make day 0 easier as there is currently no preset roles granting kubernetes access.\r\nThe `kubernetes_groups` field is required and allows Teleport itself to have cluster-admin permissions, allowing us to be authoritative and let the user only worry about labels and resources instead.",
        "pr_file_module": null
      }
    ]
  }
]