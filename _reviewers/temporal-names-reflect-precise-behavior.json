[
  {
    "discussion_id": "2214841988",
    "pr_number": 8053,
    "pr_file": "service/frontend/workflow_handler.go",
    "created_at": "2025-07-18T04:05:29+00:00",
    "commented_code": "errTooManyDeleteDeploymentRequests  = \"Too many DeleteWorkerDeployment requests have been issued in rapid succession. Please throttle the request rate to avoid exceeding Worker Deployment resource limits.\"\n \terrTooManyDeleteVersionRequests     = \"Too many DeleteWorkerDeploymentVersion requests have been issued in rapid succession. Please throttle the request rate to avoid exceeding Worker Deployment resource limits.\"\n \terrTooManyVersionMetadataRequests   = \"Too many UpdateWorkerDeploymentVersionMetadata requests have been issued in rapid succession. Please throttle the request rate to avoid exceeding Worker Deployment resource limits.\"\n+\n+\tmaxReasonLength   = 256 // Maximum length for the reason field in RateLimitUpdate configurations.\n+\tmaxIdentityLength = 256 // Maximum length for the identity field in UpdateRateLimitRequest",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2214841988",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 8053,
        "pr_file": "service/frontend/workflow_handler.go",
        "discussion_id": "2214841988",
        "commented_code": "@@ -99,6 +99,9 @@ const (\n \terrTooManyDeleteDeploymentRequests  = \"Too many DeleteWorkerDeployment requests have been issued in rapid succession. Please throttle the request rate to avoid exceeding Worker Deployment resource limits.\"\n \terrTooManyDeleteVersionRequests     = \"Too many DeleteWorkerDeploymentVersion requests have been issued in rapid succession. Please throttle the request rate to avoid exceeding Worker Deployment resource limits.\"\n \terrTooManyVersionMetadataRequests   = \"Too many UpdateWorkerDeploymentVersionMetadata requests have been issued in rapid succession. Please throttle the request rate to avoid exceeding Worker Deployment resource limits.\"\n+\n+\tmaxReasonLength   = 256 // Maximum length for the reason field in RateLimitUpdate configurations.\n+\tmaxIdentityLength = 256 // Maximum length for the identity field in UpdateRateLimitRequest",
        "comment_created_at": "2025-07-18T04:05:29+00:00",
        "comment_author": "dnr",
        "comment_body": "elsewhere in this file I see a lot of checks against `wh.config.MaxIDLengthLimit()`, so might as well use that one for identity to be consistent",
        "pr_file_module": null
      },
      {
        "comment_id": "2215131736",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 8053,
        "pr_file": "service/frontend/workflow_handler.go",
        "discussion_id": "2214841988",
        "commented_code": "@@ -99,6 +99,9 @@ const (\n \terrTooManyDeleteDeploymentRequests  = \"Too many DeleteWorkerDeployment requests have been issued in rapid succession. Please throttle the request rate to avoid exceeding Worker Deployment resource limits.\"\n \terrTooManyDeleteVersionRequests     = \"Too many DeleteWorkerDeploymentVersion requests have been issued in rapid succession. Please throttle the request rate to avoid exceeding Worker Deployment resource limits.\"\n \terrTooManyVersionMetadataRequests   = \"Too many UpdateWorkerDeploymentVersionMetadata requests have been issued in rapid succession. Please throttle the request rate to avoid exceeding Worker Deployment resource limits.\"\n+\n+\tmaxReasonLength   = 256 // Maximum length for the reason field in RateLimitUpdate configurations.\n+\tmaxIdentityLength = 256 // Maximum length for the identity field in UpdateRateLimitRequest",
        "comment_created_at": "2025-07-18T06:42:53+00:00",
        "comment_author": "sivagirish81",
        "comment_body": "Used wh.config.MaxIDLengthLimit() for identity length check to be consistent.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190637141",
    "pr_number": 7967,
    "pr_file": "service/matching/fair_task_reader.go",
    "created_at": "2025-07-07T16:56:29+00:00",
    "commented_code": "+package matching\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"slices\"\n+\t\"sync\"\n+\t\"time\"\n+\n+\t\"github.com/emirpasic/gods/maps/treemap\"\n+\t\"go.temporal.io/api/serviceerror\"\n+\tpersistencespb \"go.temporal.io/server/api/persistence/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/backoff\"\n+\t\"go.temporal.io/server/common/clock\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/persistence\"\n+\tserviceerrors \"go.temporal.io/server/common/serviceerror\"\n+\t\"go.temporal.io/server/common/softassert\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"golang.org/x/sync/semaphore\"\n+)\n+\n+type (\n+\tfairTaskReader struct {\n+\t\tbacklogMgr *fairBacklogManagerImpl\n+\t\tsubqueue   int\n+\t\tlogger     log.Logger\n+\n+\t\tlock sync.Mutex\n+\n+\t\treadPending  bool\n+\t\tbackoffTimer *time.Timer\n+\t\tretrier      backoff.Retrier\n+\t\taddRetries   *semaphore.Weighted\n+\n+\t\tbacklogAge       backlogAgeTracker\n+\t\toutstandingTasks treemap.Map // fairLevel -> *internalTask if unacked, or nil if acked\n+\t\tloadedTasks      int         // == number of unacked (non-nil) entries in outstandingTasks\n+\t\treadLevel        fairLevel   // == highest level in outstandingTasks, or if empty, the level we should read next\n+\t\tackLevel         fairLevel   // inclusive: task exactly at ackLevel _has_ been acked\n+\t\tackLevelPinned   bool        // pinned while writing tasks so that we don't delete just-written tasks\n+\t\tatEnd            bool        // whether we believe outstandingTasks represents the entire queue right now\n+\n+\t\t// gc state\n+\t\tinGC       bool\n+\t\tnumToGC    int       // counts approximately how many tasks we can delete with a GC\n+\t\tlastGCTime time.Time // last time GCed\n+\t}\n+\n+\tmergeMode int\n+)\n+\n+const (\n+\tmergeReadMiddle mergeMode = iota\n+\tmergeReadToEnd\n+\tmergeWrite\n+)\n+\n+func newFairTaskReader(\n+\tbacklogMgr *fairBacklogManagerImpl,\n+\tsubqueue int,\n+\tinitialAckLevel fairLevel,\n+) *fairTaskReader {\n+\treturn &fairTaskReader{\n+\t\tbacklogMgr: backlogMgr,\n+\t\tsubqueue:   subqueue,\n+\t\tlogger:     backlogMgr.logger,\n+\t\tretrier: backoff.NewRetrier(\n+\t\t\tcommon.CreateReadTaskRetryPolicy(),\n+\t\t\tclock.NewRealTimeSource(),\n+\t\t),\n+\t\tbacklogAge: newBacklogAgeTracker(),\n+\t\taddRetries: semaphore.NewWeighted(concurrentAddRetries),\n+\n+\t\t// ack manager\n+\t\toutstandingTasks: *newFairLevelTreeMap(),\n+\t\treadLevel:        initialAckLevel,\n+\t\tackLevel:         initialAckLevel,\n+\n+\t\t// gc state\n+\t\tlastGCTime: time.Now(),\n+\t}\n+}\n+\n+func (tr *fairTaskReader) Start() {\n+\ttr.lock.Lock()\n+\tdefer tr.lock.Unlock()\n+\ttr.maybeReadTasksLocked()\n+}\n+\n+func (tr *fairTaskReader) getOldestBacklogTime() time.Time {\n+\ttr.lock.Lock()\n+\tdefer tr.lock.Unlock()\n+\treturn tr.backlogAge.oldestTime()\n+}\n+\n+func (tr *fairTaskReader) completeTask(task *internalTask, res taskResponse) {\n+\ttr.lock.Lock()\n+\n+\t// We might have a race where mergeTasks tries to read a task from matcher (because new tasks\n+\t// came in under it), but it had already been matched and removed. In that case the\n+\t// removeFromMatcher will be a no-op, and we'll eventually end up here. We can tell because\n+\t// the task won't be present in outstandingTasks.\n+\t//\n+\t// We can't ack the task, so we'll eventually read it again and then discover that it's a\n+\t// duplicate when we try to RecordTaskStarted.\n+\tif task, found := tr.outstandingTasks.Get(fairLevelFromAllocatedTask(task.event.AllocatedTaskInfo)); !found {\n+\t\tmetrics.TaskCompletedMissing.With(tr.backlogMgr.metricsHandler).Record(1)\n+\t\ttr.lock.Unlock()\n+\t\treturn\n+\t} else if _, ok := task.(*internalTask); !softassert.That(tr.logger, ok, \"completed task was already acked\") {\n+\t\ttr.lock.Unlock()\n+\t\treturn\n+\t}\n+\n+\t// Handle happy path first:\n+\terr := res.err()\n+\tif err == nil {\n+\t\ttr.completeTaskLocked(task)\n+\t\ttr.lock.Unlock()\n+\t\treturn\n+\t}\n+\n+\ttr.lock.Unlock()\n+\n+\t// We can handle some transient errors by just putting the task back in the matcher to\n+\t// match again. Note that for forwarded tasks, it's expected to get DeadlineExceeded when\n+\t// the task doesn't match on the root after backlogTaskForwardTimeout, and also expected to\n+\t// get errRemoteSyncMatchFailed, which is a serviceerror.Canceled error.\n+\tif common.IsServiceClientTransientError(err) ||\n+\t\tcommon.IsContextDeadlineExceededErr(err) ||\n+\t\tcommon.IsContextCanceledErr(err) {\n+\t\t// TODO(pri): if this was a start error (not a forwarding error): consider adding a\n+\t\t// per-task backoff here, in case the error was workflow busy, we don't want to end up\n+\t\t// trying the same task immediately. maybe also: after a few attempts on the same task,\n+\t\t// let it get cycled to the end of the queue, in case there's some task/wf-specific\n+\t\t// thing.\n+\t\ttr.addTaskToMatcher(task)\n+\t\tmetrics.TaskRetryTransient.With(tr.backlogMgr.metricsHandler).Record(1)\n+\t\treturn\n+\t}\n+\n+\t// On other errors: ask backlog manager to re-spool to persistence\n+\tif tr.backlogMgr.respoolTaskAfterError(task.event.Data) != nil {\n+\t\treturn // task queue will unload now\n+\t}\n+\n+\t// If we re-spooled successfully, remove the old version of the task.\n+\ttr.lock.Lock()\n+\tdefer tr.lock.Unlock()\n+\ttr.completeTaskLocked(task)\n+}\n+\n+func (tr *fairTaskReader) completeTaskLocked(task *internalTask) {\n+\ttr.backlogAge.record(task.event.Data.CreateTime, -1)\n+\ttr.outstandingTasks.Put(fairLevelFromAllocatedTask(task.event.AllocatedTaskInfo), nil)\n+\ttr.loadedTasks--\n+\tsoftassert.That(tr.logger, tr.loadedTasks >= 0, \"loadedTasks went negative\")\n+\n+\ttr.advanceAckLevelLocked()\n+\ttr.maybeReadTasksLocked()\n+}\n+\n+func (tr *fairTaskReader) maybeReadTasksLocked() {\n+\t// If readPending is true here, readTasksImpl is running and will check\n+\t// shouldReadMoreLocked before it exits.\n+\tif tr.readPending || !tr.shouldReadMoreLocked() {\n+\t\treturn\n+\t}\n+\ttr.readPending = true\n+\tgo tr.readTasksImpl()\n+}\n+\n+func (tr *fairTaskReader) shouldReadMoreLocked() bool {\n+\tif tr.atEnd {\n+\t\t// If we have the whole backlog in memory, we don't need to read anything.\n+\t\treturn false\n+\t} else if tr.loadedTasks > tr.backlogMgr.config.GetTasksReloadAt() {\n+\t\t// Too many loaded already. We'll get called again when loadedTasks drops.\n+\t\treturn false\n+\t}\n+\treturn true\n+}\n+\n+func (tr *fairTaskReader) readTasksImpl() {\n+\tvar lastErr error\n+\tfor {\n+\t\ttr.lock.Lock()\n+\t\tif lastErr != nil || !tr.shouldReadMoreLocked() {\n+\t\t\ttr.readPending = false\n+\t\t\ttr.lock.Unlock()\n+\t\t\treturn\n+\t\t}\n+\t\treadLevel, loadedTasks := tr.readLevel, int(tr.loadedTasks)\n+\t\ttr.lock.Unlock()\n+\n+\t\tlastErr = tr.readTaskBatch(readLevel, loadedTasks)\n+\t}\n+}\n+\n+func (tr *fairTaskReader) readTaskBatch(readLevel fairLevel, loadedTasks int) error {\n+\tbatchSize := tr.backlogMgr.config.GetTasksBatchSize() - loadedTasks\n+\treadFrom := readLevel.max(fairLevel{pass: 1, id: 0}).inc()\n+\tres, err := tr.backlogMgr.db.GetFairTasks(tr.backlogMgr.tqCtx, tr.subqueue, readFrom, batchSize)\n+\tif err != nil {\n+\t\ttr.backlogMgr.signalIfFatal(err)\n+\t\t// TODO: Should we ever stop retrying on db errors?\n+\t\tif common.IsResourceExhausted(err) {\n+\t\t\ttr.retryReadAfter(taskReaderThrottleRetryDelay)\n+\t\t} else {\n+\t\t\ttr.retryReadAfter(tr.retrier.NextBackOff(err))\n+\t\t}\n+\t\treturn err\n+\t}\n+\ttr.retrier.Reset()\n+\n+\t// If we got less than we asked for, we know we hit the end.\n+\tmode := mergeReadMiddle\n+\tif len(res.Tasks) < batchSize {\n+\t\tmode = mergeReadToEnd\n+\t}\n+\n+\t// filter out expired\n+\t// TODO(fairness): if we have _only_ expired tasks, and we filter them out here, we won't move\n+\t// the ack level and delete them. maybe we should put them in outstandingTasks as pre-acked.\n+\ttasks := slices.DeleteFunc(res.Tasks, func(t *persistencespb.AllocatedTaskInfo) bool {\n+\t\tif IsTaskExpired(t) {\n+\t\t\tmetrics.ExpiredTasksPerTaskQueueCounter.With(tr.backlogMgr.metricsHandler).Record(1)\n+\t\t\treturn true\n+\t\t}\n+\t\treturn false\n+\t})\n+\n+\t// Note: even if (especially if) len(tasks) == 0, we should go through the mergeTasks logic\n+\t// to update atEnd and the backlog size estimate.\n+\ttr.mergeTasks(tasks, mode)\n+\n+\treturn nil\n+}\n+\n+// call with_out_ lock held\n+func (tr *fairTaskReader) addTaskToMatcher(task *internalTask) {\n+\terr := tr.backlogMgr.addSpooledTask(task)\n+\tif err == nil {\n+\t\treturn\n+\t}\n+\n+\tif drop, retry := tr.addErrorBehavior(err); drop {\n+\t\ttask.finish(nil, false)\n+\t} else if retry {\n+\t\t// This should only be due to persistence problems. Retry in a new goroutine\n+\t\t// to not block other tasks, up to some concurrency limit.\n+\t\tif tr.addRetries.Acquire(tr.backlogMgr.tqCtx, 1) != nil {\n+\t\t\treturn\n+\t\t}\n+\t\tgo tr.retryAddAfterError(task)\n+\t}\n+}\n+\n+func (tr *fairTaskReader) addErrorBehavior(err error) (drop, retry bool) {\n+\t// addSpooledTask can only fail due to:\n+\t// - the task queue is closed (errTaskQueueClosed or context.Canceled)\n+\t// - ValidateDeployment failed (InvalidArgument)\n+\t// - versioning wants to get a versioned queue and it can't be initialized\n+\t// - versioning wants to re-spool the task on a different queue and that failed\n+\t// - versioning says StickyWorkerUnavailable\n+\tif errors.Is(err, errTaskQueueClosed) || common.IsContextCanceledErr(err) {\n+\t\treturn false, false\n+\t}\n+\tvar stickyUnavailable *serviceerrors.StickyWorkerUnavailable\n+\tif errors.As(err, &stickyUnavailable) {\n+\t\treturn true, false // drop the task\n+\t}\n+\tvar invalid *serviceerror.InvalidArgument\n+\tvar internal *serviceerror.Internal\n+\tif errors.As(err, &invalid) || errors.As(err, &internal) {\n+\t\ttr.backlogMgr.throttledLogger.Error(\"nonretryable error processing spooled task\", tag.Error(err))\n+\t\treturn true, false // drop the task\n+\t}\n+\t// For any other error (this should be very rare), we can retry.\n+\ttr.backlogMgr.throttledLogger.Error(\"retryable error processing spooled task\", tag.Error(err))\n+\treturn false, true\n+}\n+\n+func (tr *fairTaskReader) retryAddAfterError(task *internalTask) {\n+\tdefer tr.addRetries.Release(1)\n+\tmetrics.BufferThrottlePerTaskQueueCounter.With(tr.backlogMgr.metricsHandler).Record(1)\n+\n+\t// initial sleep since we just tried once\n+\t_ = util.InterruptibleSleep(tr.backlogMgr.tqCtx, time.Second)\n+\n+\t_ = backoff.ThrottleRetryContext(\n+\t\ttr.backlogMgr.tqCtx,\n+\t\tfunc(context.Context) error {\n+\t\t\tif IsTaskExpired(task.event.AllocatedTaskInfo) {\n+\t\t\t\ttask.finish(nil, false)\n+\t\t\t\treturn nil\n+\t\t\t}\n+\t\t\terr := tr.backlogMgr.addSpooledTask(task)\n+\t\t\tif drop, retry := tr.addErrorBehavior(err); drop {\n+\t\t\t\ttask.finish(nil, false)\n+\t\t\t} else if retry {\n+\t\t\t\tmetrics.BufferThrottlePerTaskQueueCounter.With(tr.backlogMgr.metricsHandler).Record(1)\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\treturn nil\n+\t\t},\n+\t\taddErrorRetryPolicy,\n+\t\tnil,\n+\t)\n+}\n+\n+func (tr *fairTaskReader) wroteNewTasks(tasks []*persistencespb.AllocatedTaskInfo) {\n+\ttr.mergeTasks(tasks, mergeWrite)\n+}\n+\n+func (tr *fairTaskReader) mergeTasks(tasks []*persistencespb.AllocatedTaskInfo, mode mergeMode) {\n+\ttr.lock.Lock()\n+\n+\t// Take the tasks in the matcher plus the tasks that were just written and sort them by level:\n+\n+\t// Get currently loaded tasks. Note these values are *internalTask.\n+\tmerged := tr.outstandingTasks.Select(func(k, v any) bool {\n+\t\t_, ok := v.(*internalTask)\n+\t\treturn ok\n+\t})\n+\t// Add the tasks we just read/wrote. Note these values are *AllocatedTaskInfo.\n+\tfor _, t := range tasks {\n+\t\tlevel := fairLevelFromAllocatedTask(t)\n+\t\tif mode == mergeWrite && !tr.atEnd && tr.readLevel.less(level) {\n+\t\t\t// If we're writing and we're not at the end, then we have to ignore tasks\n+\t\t\t// above readLevel since we don't know what's in between readLevel and there.\n+\t\t\tcontinue\n+\t\t} else if _, have := merged.Get(level); have {\n+\t\t\t// If write/read race in certain ways, we may read something we had already\n+\t\t\t// added to the matcher. Ignore tasks we already have.\n+\t\t\tcontinue\n+\t\t}\n+\t\tmerged.Put(level, t)\n+\t}\n+\n+\t// Take as many of those as we want to keep in memory. The ones that are not already in the\n+\t// matcher, we have to add to the matcher.\n+\tbatchSize := tr.backlogMgr.config.GetTasksBatchSize()\n+\tit := merged.Iterator()\n+\tvar highestLevel fairLevel\n+\ttasks = tasks[:0] // reuse incoming slice to avoid an allocation\n+\tfor b := 0; it.Next() && b < batchSize; b++ {\n+\t\tif t, ok := it.Value().(*persistencespb.AllocatedTaskInfo); ok {\n+\t\t\t// new task we need to add to the matcher\n+\t\t\ttasks = append(tasks, t)\n+\t\t}\n+\t\thighestLevel = it.Key().(fairLevel) // nolint:revive\n+\t}\n+\n+\tif highestLevel.id != 0 {\n+\t\t// If we have any tasks at all in memory, set readLevel to the maximum of that set.\n+\t\t// Otherwise leave readLevel unchanged.\n+\t\ttr.readLevel = highestLevel\n+\t}\n+\n+\t// If there are remaining tasks in the merged set, they can't fit in memory. If they came\n+\t// from the tasks we just wrote, ignore them. If they came from matcher, remove them.\n+\tdroppedAnyTasks := false",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2190637141",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7967,
        "pr_file": "service/matching/fair_task_reader.go",
        "discussion_id": "2190637141",
        "commented_code": "@@ -0,0 +1,560 @@\n+package matching\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"slices\"\n+\t\"sync\"\n+\t\"time\"\n+\n+\t\"github.com/emirpasic/gods/maps/treemap\"\n+\t\"go.temporal.io/api/serviceerror\"\n+\tpersistencespb \"go.temporal.io/server/api/persistence/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/backoff\"\n+\t\"go.temporal.io/server/common/clock\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/persistence\"\n+\tserviceerrors \"go.temporal.io/server/common/serviceerror\"\n+\t\"go.temporal.io/server/common/softassert\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"golang.org/x/sync/semaphore\"\n+)\n+\n+type (\n+\tfairTaskReader struct {\n+\t\tbacklogMgr *fairBacklogManagerImpl\n+\t\tsubqueue   int\n+\t\tlogger     log.Logger\n+\n+\t\tlock sync.Mutex\n+\n+\t\treadPending  bool\n+\t\tbackoffTimer *time.Timer\n+\t\tretrier      backoff.Retrier\n+\t\taddRetries   *semaphore.Weighted\n+\n+\t\tbacklogAge       backlogAgeTracker\n+\t\toutstandingTasks treemap.Map // fairLevel -> *internalTask if unacked, or nil if acked\n+\t\tloadedTasks      int         // == number of unacked (non-nil) entries in outstandingTasks\n+\t\treadLevel        fairLevel   // == highest level in outstandingTasks, or if empty, the level we should read next\n+\t\tackLevel         fairLevel   // inclusive: task exactly at ackLevel _has_ been acked\n+\t\tackLevelPinned   bool        // pinned while writing tasks so that we don't delete just-written tasks\n+\t\tatEnd            bool        // whether we believe outstandingTasks represents the entire queue right now\n+\n+\t\t// gc state\n+\t\tinGC       bool\n+\t\tnumToGC    int       // counts approximately how many tasks we can delete with a GC\n+\t\tlastGCTime time.Time // last time GCed\n+\t}\n+\n+\tmergeMode int\n+)\n+\n+const (\n+\tmergeReadMiddle mergeMode = iota\n+\tmergeReadToEnd\n+\tmergeWrite\n+)\n+\n+func newFairTaskReader(\n+\tbacklogMgr *fairBacklogManagerImpl,\n+\tsubqueue int,\n+\tinitialAckLevel fairLevel,\n+) *fairTaskReader {\n+\treturn &fairTaskReader{\n+\t\tbacklogMgr: backlogMgr,\n+\t\tsubqueue:   subqueue,\n+\t\tlogger:     backlogMgr.logger,\n+\t\tretrier: backoff.NewRetrier(\n+\t\t\tcommon.CreateReadTaskRetryPolicy(),\n+\t\t\tclock.NewRealTimeSource(),\n+\t\t),\n+\t\tbacklogAge: newBacklogAgeTracker(),\n+\t\taddRetries: semaphore.NewWeighted(concurrentAddRetries),\n+\n+\t\t// ack manager\n+\t\toutstandingTasks: *newFairLevelTreeMap(),\n+\t\treadLevel:        initialAckLevel,\n+\t\tackLevel:         initialAckLevel,\n+\n+\t\t// gc state\n+\t\tlastGCTime: time.Now(),\n+\t}\n+}\n+\n+func (tr *fairTaskReader) Start() {\n+\ttr.lock.Lock()\n+\tdefer tr.lock.Unlock()\n+\ttr.maybeReadTasksLocked()\n+}\n+\n+func (tr *fairTaskReader) getOldestBacklogTime() time.Time {\n+\ttr.lock.Lock()\n+\tdefer tr.lock.Unlock()\n+\treturn tr.backlogAge.oldestTime()\n+}\n+\n+func (tr *fairTaskReader) completeTask(task *internalTask, res taskResponse) {\n+\ttr.lock.Lock()\n+\n+\t// We might have a race where mergeTasks tries to read a task from matcher (because new tasks\n+\t// came in under it), but it had already been matched and removed. In that case the\n+\t// removeFromMatcher will be a no-op, and we'll eventually end up here. We can tell because\n+\t// the task won't be present in outstandingTasks.\n+\t//\n+\t// We can't ack the task, so we'll eventually read it again and then discover that it's a\n+\t// duplicate when we try to RecordTaskStarted.\n+\tif task, found := tr.outstandingTasks.Get(fairLevelFromAllocatedTask(task.event.AllocatedTaskInfo)); !found {\n+\t\tmetrics.TaskCompletedMissing.With(tr.backlogMgr.metricsHandler).Record(1)\n+\t\ttr.lock.Unlock()\n+\t\treturn\n+\t} else if _, ok := task.(*internalTask); !softassert.That(tr.logger, ok, \"completed task was already acked\") {\n+\t\ttr.lock.Unlock()\n+\t\treturn\n+\t}\n+\n+\t// Handle happy path first:\n+\terr := res.err()\n+\tif err == nil {\n+\t\ttr.completeTaskLocked(task)\n+\t\ttr.lock.Unlock()\n+\t\treturn\n+\t}\n+\n+\ttr.lock.Unlock()\n+\n+\t// We can handle some transient errors by just putting the task back in the matcher to\n+\t// match again. Note that for forwarded tasks, it's expected to get DeadlineExceeded when\n+\t// the task doesn't match on the root after backlogTaskForwardTimeout, and also expected to\n+\t// get errRemoteSyncMatchFailed, which is a serviceerror.Canceled error.\n+\tif common.IsServiceClientTransientError(err) ||\n+\t\tcommon.IsContextDeadlineExceededErr(err) ||\n+\t\tcommon.IsContextCanceledErr(err) {\n+\t\t// TODO(pri): if this was a start error (not a forwarding error): consider adding a\n+\t\t// per-task backoff here, in case the error was workflow busy, we don't want to end up\n+\t\t// trying the same task immediately. maybe also: after a few attempts on the same task,\n+\t\t// let it get cycled to the end of the queue, in case there's some task/wf-specific\n+\t\t// thing.\n+\t\ttr.addTaskToMatcher(task)\n+\t\tmetrics.TaskRetryTransient.With(tr.backlogMgr.metricsHandler).Record(1)\n+\t\treturn\n+\t}\n+\n+\t// On other errors: ask backlog manager to re-spool to persistence\n+\tif tr.backlogMgr.respoolTaskAfterError(task.event.Data) != nil {\n+\t\treturn // task queue will unload now\n+\t}\n+\n+\t// If we re-spooled successfully, remove the old version of the task.\n+\ttr.lock.Lock()\n+\tdefer tr.lock.Unlock()\n+\ttr.completeTaskLocked(task)\n+}\n+\n+func (tr *fairTaskReader) completeTaskLocked(task *internalTask) {\n+\ttr.backlogAge.record(task.event.Data.CreateTime, -1)\n+\ttr.outstandingTasks.Put(fairLevelFromAllocatedTask(task.event.AllocatedTaskInfo), nil)\n+\ttr.loadedTasks--\n+\tsoftassert.That(tr.logger, tr.loadedTasks >= 0, \"loadedTasks went negative\")\n+\n+\ttr.advanceAckLevelLocked()\n+\ttr.maybeReadTasksLocked()\n+}\n+\n+func (tr *fairTaskReader) maybeReadTasksLocked() {\n+\t// If readPending is true here, readTasksImpl is running and will check\n+\t// shouldReadMoreLocked before it exits.\n+\tif tr.readPending || !tr.shouldReadMoreLocked() {\n+\t\treturn\n+\t}\n+\ttr.readPending = true\n+\tgo tr.readTasksImpl()\n+}\n+\n+func (tr *fairTaskReader) shouldReadMoreLocked() bool {\n+\tif tr.atEnd {\n+\t\t// If we have the whole backlog in memory, we don't need to read anything.\n+\t\treturn false\n+\t} else if tr.loadedTasks > tr.backlogMgr.config.GetTasksReloadAt() {\n+\t\t// Too many loaded already. We'll get called again when loadedTasks drops.\n+\t\treturn false\n+\t}\n+\treturn true\n+}\n+\n+func (tr *fairTaskReader) readTasksImpl() {\n+\tvar lastErr error\n+\tfor {\n+\t\ttr.lock.Lock()\n+\t\tif lastErr != nil || !tr.shouldReadMoreLocked() {\n+\t\t\ttr.readPending = false\n+\t\t\ttr.lock.Unlock()\n+\t\t\treturn\n+\t\t}\n+\t\treadLevel, loadedTasks := tr.readLevel, int(tr.loadedTasks)\n+\t\ttr.lock.Unlock()\n+\n+\t\tlastErr = tr.readTaskBatch(readLevel, loadedTasks)\n+\t}\n+}\n+\n+func (tr *fairTaskReader) readTaskBatch(readLevel fairLevel, loadedTasks int) error {\n+\tbatchSize := tr.backlogMgr.config.GetTasksBatchSize() - loadedTasks\n+\treadFrom := readLevel.max(fairLevel{pass: 1, id: 0}).inc()\n+\tres, err := tr.backlogMgr.db.GetFairTasks(tr.backlogMgr.tqCtx, tr.subqueue, readFrom, batchSize)\n+\tif err != nil {\n+\t\ttr.backlogMgr.signalIfFatal(err)\n+\t\t// TODO: Should we ever stop retrying on db errors?\n+\t\tif common.IsResourceExhausted(err) {\n+\t\t\ttr.retryReadAfter(taskReaderThrottleRetryDelay)\n+\t\t} else {\n+\t\t\ttr.retryReadAfter(tr.retrier.NextBackOff(err))\n+\t\t}\n+\t\treturn err\n+\t}\n+\ttr.retrier.Reset()\n+\n+\t// If we got less than we asked for, we know we hit the end.\n+\tmode := mergeReadMiddle\n+\tif len(res.Tasks) < batchSize {\n+\t\tmode = mergeReadToEnd\n+\t}\n+\n+\t// filter out expired\n+\t// TODO(fairness): if we have _only_ expired tasks, and we filter them out here, we won't move\n+\t// the ack level and delete them. maybe we should put them in outstandingTasks as pre-acked.\n+\ttasks := slices.DeleteFunc(res.Tasks, func(t *persistencespb.AllocatedTaskInfo) bool {\n+\t\tif IsTaskExpired(t) {\n+\t\t\tmetrics.ExpiredTasksPerTaskQueueCounter.With(tr.backlogMgr.metricsHandler).Record(1)\n+\t\t\treturn true\n+\t\t}\n+\t\treturn false\n+\t})\n+\n+\t// Note: even if (especially if) len(tasks) == 0, we should go through the mergeTasks logic\n+\t// to update atEnd and the backlog size estimate.\n+\ttr.mergeTasks(tasks, mode)\n+\n+\treturn nil\n+}\n+\n+// call with_out_ lock held\n+func (tr *fairTaskReader) addTaskToMatcher(task *internalTask) {\n+\terr := tr.backlogMgr.addSpooledTask(task)\n+\tif err == nil {\n+\t\treturn\n+\t}\n+\n+\tif drop, retry := tr.addErrorBehavior(err); drop {\n+\t\ttask.finish(nil, false)\n+\t} else if retry {\n+\t\t// This should only be due to persistence problems. Retry in a new goroutine\n+\t\t// to not block other tasks, up to some concurrency limit.\n+\t\tif tr.addRetries.Acquire(tr.backlogMgr.tqCtx, 1) != nil {\n+\t\t\treturn\n+\t\t}\n+\t\tgo tr.retryAddAfterError(task)\n+\t}\n+}\n+\n+func (tr *fairTaskReader) addErrorBehavior(err error) (drop, retry bool) {\n+\t// addSpooledTask can only fail due to:\n+\t// - the task queue is closed (errTaskQueueClosed or context.Canceled)\n+\t// - ValidateDeployment failed (InvalidArgument)\n+\t// - versioning wants to get a versioned queue and it can't be initialized\n+\t// - versioning wants to re-spool the task on a different queue and that failed\n+\t// - versioning says StickyWorkerUnavailable\n+\tif errors.Is(err, errTaskQueueClosed) || common.IsContextCanceledErr(err) {\n+\t\treturn false, false\n+\t}\n+\tvar stickyUnavailable *serviceerrors.StickyWorkerUnavailable\n+\tif errors.As(err, &stickyUnavailable) {\n+\t\treturn true, false // drop the task\n+\t}\n+\tvar invalid *serviceerror.InvalidArgument\n+\tvar internal *serviceerror.Internal\n+\tif errors.As(err, &invalid) || errors.As(err, &internal) {\n+\t\ttr.backlogMgr.throttledLogger.Error(\"nonretryable error processing spooled task\", tag.Error(err))\n+\t\treturn true, false // drop the task\n+\t}\n+\t// For any other error (this should be very rare), we can retry.\n+\ttr.backlogMgr.throttledLogger.Error(\"retryable error processing spooled task\", tag.Error(err))\n+\treturn false, true\n+}\n+\n+func (tr *fairTaskReader) retryAddAfterError(task *internalTask) {\n+\tdefer tr.addRetries.Release(1)\n+\tmetrics.BufferThrottlePerTaskQueueCounter.With(tr.backlogMgr.metricsHandler).Record(1)\n+\n+\t// initial sleep since we just tried once\n+\t_ = util.InterruptibleSleep(tr.backlogMgr.tqCtx, time.Second)\n+\n+\t_ = backoff.ThrottleRetryContext(\n+\t\ttr.backlogMgr.tqCtx,\n+\t\tfunc(context.Context) error {\n+\t\t\tif IsTaskExpired(task.event.AllocatedTaskInfo) {\n+\t\t\t\ttask.finish(nil, false)\n+\t\t\t\treturn nil\n+\t\t\t}\n+\t\t\terr := tr.backlogMgr.addSpooledTask(task)\n+\t\t\tif drop, retry := tr.addErrorBehavior(err); drop {\n+\t\t\t\ttask.finish(nil, false)\n+\t\t\t} else if retry {\n+\t\t\t\tmetrics.BufferThrottlePerTaskQueueCounter.With(tr.backlogMgr.metricsHandler).Record(1)\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\treturn nil\n+\t\t},\n+\t\taddErrorRetryPolicy,\n+\t\tnil,\n+\t)\n+}\n+\n+func (tr *fairTaskReader) wroteNewTasks(tasks []*persistencespb.AllocatedTaskInfo) {\n+\ttr.mergeTasks(tasks, mergeWrite)\n+}\n+\n+func (tr *fairTaskReader) mergeTasks(tasks []*persistencespb.AllocatedTaskInfo, mode mergeMode) {\n+\ttr.lock.Lock()\n+\n+\t// Take the tasks in the matcher plus the tasks that were just written and sort them by level:\n+\n+\t// Get currently loaded tasks. Note these values are *internalTask.\n+\tmerged := tr.outstandingTasks.Select(func(k, v any) bool {\n+\t\t_, ok := v.(*internalTask)\n+\t\treturn ok\n+\t})\n+\t// Add the tasks we just read/wrote. Note these values are *AllocatedTaskInfo.\n+\tfor _, t := range tasks {\n+\t\tlevel := fairLevelFromAllocatedTask(t)\n+\t\tif mode == mergeWrite && !tr.atEnd && tr.readLevel.less(level) {\n+\t\t\t// If we're writing and we're not at the end, then we have to ignore tasks\n+\t\t\t// above readLevel since we don't know what's in between readLevel and there.\n+\t\t\tcontinue\n+\t\t} else if _, have := merged.Get(level); have {\n+\t\t\t// If write/read race in certain ways, we may read something we had already\n+\t\t\t// added to the matcher. Ignore tasks we already have.\n+\t\t\tcontinue\n+\t\t}\n+\t\tmerged.Put(level, t)\n+\t}\n+\n+\t// Take as many of those as we want to keep in memory. The ones that are not already in the\n+\t// matcher, we have to add to the matcher.\n+\tbatchSize := tr.backlogMgr.config.GetTasksBatchSize()\n+\tit := merged.Iterator()\n+\tvar highestLevel fairLevel\n+\ttasks = tasks[:0] // reuse incoming slice to avoid an allocation\n+\tfor b := 0; it.Next() && b < batchSize; b++ {\n+\t\tif t, ok := it.Value().(*persistencespb.AllocatedTaskInfo); ok {\n+\t\t\t// new task we need to add to the matcher\n+\t\t\ttasks = append(tasks, t)\n+\t\t}\n+\t\thighestLevel = it.Key().(fairLevel) // nolint:revive\n+\t}\n+\n+\tif highestLevel.id != 0 {\n+\t\t// If we have any tasks at all in memory, set readLevel to the maximum of that set.\n+\t\t// Otherwise leave readLevel unchanged.\n+\t\ttr.readLevel = highestLevel\n+\t}\n+\n+\t// If there are remaining tasks in the merged set, they can't fit in memory. If they came\n+\t// from the tasks we just wrote, ignore them. If they came from matcher, remove them.\n+\tdroppedAnyTasks := false",
        "comment_created_at": "2025-07-07T16:56:29+00:00",
        "comment_author": "stephanos",
        "comment_body": "nit: how about `evictedAnyTasks`? Would make it a tad clearer that these are essentially \"cached\". \"Drop\" sounds a bit more consequential.",
        "pr_file_module": null
      },
      {
        "comment_id": "2191154921",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7967,
        "pr_file": "service/matching/fair_task_reader.go",
        "discussion_id": "2190637141",
        "commented_code": "@@ -0,0 +1,560 @@\n+package matching\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"slices\"\n+\t\"sync\"\n+\t\"time\"\n+\n+\t\"github.com/emirpasic/gods/maps/treemap\"\n+\t\"go.temporal.io/api/serviceerror\"\n+\tpersistencespb \"go.temporal.io/server/api/persistence/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/backoff\"\n+\t\"go.temporal.io/server/common/clock\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/persistence\"\n+\tserviceerrors \"go.temporal.io/server/common/serviceerror\"\n+\t\"go.temporal.io/server/common/softassert\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"golang.org/x/sync/semaphore\"\n+)\n+\n+type (\n+\tfairTaskReader struct {\n+\t\tbacklogMgr *fairBacklogManagerImpl\n+\t\tsubqueue   int\n+\t\tlogger     log.Logger\n+\n+\t\tlock sync.Mutex\n+\n+\t\treadPending  bool\n+\t\tbackoffTimer *time.Timer\n+\t\tretrier      backoff.Retrier\n+\t\taddRetries   *semaphore.Weighted\n+\n+\t\tbacklogAge       backlogAgeTracker\n+\t\toutstandingTasks treemap.Map // fairLevel -> *internalTask if unacked, or nil if acked\n+\t\tloadedTasks      int         // == number of unacked (non-nil) entries in outstandingTasks\n+\t\treadLevel        fairLevel   // == highest level in outstandingTasks, or if empty, the level we should read next\n+\t\tackLevel         fairLevel   // inclusive: task exactly at ackLevel _has_ been acked\n+\t\tackLevelPinned   bool        // pinned while writing tasks so that we don't delete just-written tasks\n+\t\tatEnd            bool        // whether we believe outstandingTasks represents the entire queue right now\n+\n+\t\t// gc state\n+\t\tinGC       bool\n+\t\tnumToGC    int       // counts approximately how many tasks we can delete with a GC\n+\t\tlastGCTime time.Time // last time GCed\n+\t}\n+\n+\tmergeMode int\n+)\n+\n+const (\n+\tmergeReadMiddle mergeMode = iota\n+\tmergeReadToEnd\n+\tmergeWrite\n+)\n+\n+func newFairTaskReader(\n+\tbacklogMgr *fairBacklogManagerImpl,\n+\tsubqueue int,\n+\tinitialAckLevel fairLevel,\n+) *fairTaskReader {\n+\treturn &fairTaskReader{\n+\t\tbacklogMgr: backlogMgr,\n+\t\tsubqueue:   subqueue,\n+\t\tlogger:     backlogMgr.logger,\n+\t\tretrier: backoff.NewRetrier(\n+\t\t\tcommon.CreateReadTaskRetryPolicy(),\n+\t\t\tclock.NewRealTimeSource(),\n+\t\t),\n+\t\tbacklogAge: newBacklogAgeTracker(),\n+\t\taddRetries: semaphore.NewWeighted(concurrentAddRetries),\n+\n+\t\t// ack manager\n+\t\toutstandingTasks: *newFairLevelTreeMap(),\n+\t\treadLevel:        initialAckLevel,\n+\t\tackLevel:         initialAckLevel,\n+\n+\t\t// gc state\n+\t\tlastGCTime: time.Now(),\n+\t}\n+}\n+\n+func (tr *fairTaskReader) Start() {\n+\ttr.lock.Lock()\n+\tdefer tr.lock.Unlock()\n+\ttr.maybeReadTasksLocked()\n+}\n+\n+func (tr *fairTaskReader) getOldestBacklogTime() time.Time {\n+\ttr.lock.Lock()\n+\tdefer tr.lock.Unlock()\n+\treturn tr.backlogAge.oldestTime()\n+}\n+\n+func (tr *fairTaskReader) completeTask(task *internalTask, res taskResponse) {\n+\ttr.lock.Lock()\n+\n+\t// We might have a race where mergeTasks tries to read a task from matcher (because new tasks\n+\t// came in under it), but it had already been matched and removed. In that case the\n+\t// removeFromMatcher will be a no-op, and we'll eventually end up here. We can tell because\n+\t// the task won't be present in outstandingTasks.\n+\t//\n+\t// We can't ack the task, so we'll eventually read it again and then discover that it's a\n+\t// duplicate when we try to RecordTaskStarted.\n+\tif task, found := tr.outstandingTasks.Get(fairLevelFromAllocatedTask(task.event.AllocatedTaskInfo)); !found {\n+\t\tmetrics.TaskCompletedMissing.With(tr.backlogMgr.metricsHandler).Record(1)\n+\t\ttr.lock.Unlock()\n+\t\treturn\n+\t} else if _, ok := task.(*internalTask); !softassert.That(tr.logger, ok, \"completed task was already acked\") {\n+\t\ttr.lock.Unlock()\n+\t\treturn\n+\t}\n+\n+\t// Handle happy path first:\n+\terr := res.err()\n+\tif err == nil {\n+\t\ttr.completeTaskLocked(task)\n+\t\ttr.lock.Unlock()\n+\t\treturn\n+\t}\n+\n+\ttr.lock.Unlock()\n+\n+\t// We can handle some transient errors by just putting the task back in the matcher to\n+\t// match again. Note that for forwarded tasks, it's expected to get DeadlineExceeded when\n+\t// the task doesn't match on the root after backlogTaskForwardTimeout, and also expected to\n+\t// get errRemoteSyncMatchFailed, which is a serviceerror.Canceled error.\n+\tif common.IsServiceClientTransientError(err) ||\n+\t\tcommon.IsContextDeadlineExceededErr(err) ||\n+\t\tcommon.IsContextCanceledErr(err) {\n+\t\t// TODO(pri): if this was a start error (not a forwarding error): consider adding a\n+\t\t// per-task backoff here, in case the error was workflow busy, we don't want to end up\n+\t\t// trying the same task immediately. maybe also: after a few attempts on the same task,\n+\t\t// let it get cycled to the end of the queue, in case there's some task/wf-specific\n+\t\t// thing.\n+\t\ttr.addTaskToMatcher(task)\n+\t\tmetrics.TaskRetryTransient.With(tr.backlogMgr.metricsHandler).Record(1)\n+\t\treturn\n+\t}\n+\n+\t// On other errors: ask backlog manager to re-spool to persistence\n+\tif tr.backlogMgr.respoolTaskAfterError(task.event.Data) != nil {\n+\t\treturn // task queue will unload now\n+\t}\n+\n+\t// If we re-spooled successfully, remove the old version of the task.\n+\ttr.lock.Lock()\n+\tdefer tr.lock.Unlock()\n+\ttr.completeTaskLocked(task)\n+}\n+\n+func (tr *fairTaskReader) completeTaskLocked(task *internalTask) {\n+\ttr.backlogAge.record(task.event.Data.CreateTime, -1)\n+\ttr.outstandingTasks.Put(fairLevelFromAllocatedTask(task.event.AllocatedTaskInfo), nil)\n+\ttr.loadedTasks--\n+\tsoftassert.That(tr.logger, tr.loadedTasks >= 0, \"loadedTasks went negative\")\n+\n+\ttr.advanceAckLevelLocked()\n+\ttr.maybeReadTasksLocked()\n+}\n+\n+func (tr *fairTaskReader) maybeReadTasksLocked() {\n+\t// If readPending is true here, readTasksImpl is running and will check\n+\t// shouldReadMoreLocked before it exits.\n+\tif tr.readPending || !tr.shouldReadMoreLocked() {\n+\t\treturn\n+\t}\n+\ttr.readPending = true\n+\tgo tr.readTasksImpl()\n+}\n+\n+func (tr *fairTaskReader) shouldReadMoreLocked() bool {\n+\tif tr.atEnd {\n+\t\t// If we have the whole backlog in memory, we don't need to read anything.\n+\t\treturn false\n+\t} else if tr.loadedTasks > tr.backlogMgr.config.GetTasksReloadAt() {\n+\t\t// Too many loaded already. We'll get called again when loadedTasks drops.\n+\t\treturn false\n+\t}\n+\treturn true\n+}\n+\n+func (tr *fairTaskReader) readTasksImpl() {\n+\tvar lastErr error\n+\tfor {\n+\t\ttr.lock.Lock()\n+\t\tif lastErr != nil || !tr.shouldReadMoreLocked() {\n+\t\t\ttr.readPending = false\n+\t\t\ttr.lock.Unlock()\n+\t\t\treturn\n+\t\t}\n+\t\treadLevel, loadedTasks := tr.readLevel, int(tr.loadedTasks)\n+\t\ttr.lock.Unlock()\n+\n+\t\tlastErr = tr.readTaskBatch(readLevel, loadedTasks)\n+\t}\n+}\n+\n+func (tr *fairTaskReader) readTaskBatch(readLevel fairLevel, loadedTasks int) error {\n+\tbatchSize := tr.backlogMgr.config.GetTasksBatchSize() - loadedTasks\n+\treadFrom := readLevel.max(fairLevel{pass: 1, id: 0}).inc()\n+\tres, err := tr.backlogMgr.db.GetFairTasks(tr.backlogMgr.tqCtx, tr.subqueue, readFrom, batchSize)\n+\tif err != nil {\n+\t\ttr.backlogMgr.signalIfFatal(err)\n+\t\t// TODO: Should we ever stop retrying on db errors?\n+\t\tif common.IsResourceExhausted(err) {\n+\t\t\ttr.retryReadAfter(taskReaderThrottleRetryDelay)\n+\t\t} else {\n+\t\t\ttr.retryReadAfter(tr.retrier.NextBackOff(err))\n+\t\t}\n+\t\treturn err\n+\t}\n+\ttr.retrier.Reset()\n+\n+\t// If we got less than we asked for, we know we hit the end.\n+\tmode := mergeReadMiddle\n+\tif len(res.Tasks) < batchSize {\n+\t\tmode = mergeReadToEnd\n+\t}\n+\n+\t// filter out expired\n+\t// TODO(fairness): if we have _only_ expired tasks, and we filter them out here, we won't move\n+\t// the ack level and delete them. maybe we should put them in outstandingTasks as pre-acked.\n+\ttasks := slices.DeleteFunc(res.Tasks, func(t *persistencespb.AllocatedTaskInfo) bool {\n+\t\tif IsTaskExpired(t) {\n+\t\t\tmetrics.ExpiredTasksPerTaskQueueCounter.With(tr.backlogMgr.metricsHandler).Record(1)\n+\t\t\treturn true\n+\t\t}\n+\t\treturn false\n+\t})\n+\n+\t// Note: even if (especially if) len(tasks) == 0, we should go through the mergeTasks logic\n+\t// to update atEnd and the backlog size estimate.\n+\ttr.mergeTasks(tasks, mode)\n+\n+\treturn nil\n+}\n+\n+// call with_out_ lock held\n+func (tr *fairTaskReader) addTaskToMatcher(task *internalTask) {\n+\terr := tr.backlogMgr.addSpooledTask(task)\n+\tif err == nil {\n+\t\treturn\n+\t}\n+\n+\tif drop, retry := tr.addErrorBehavior(err); drop {\n+\t\ttask.finish(nil, false)\n+\t} else if retry {\n+\t\t// This should only be due to persistence problems. Retry in a new goroutine\n+\t\t// to not block other tasks, up to some concurrency limit.\n+\t\tif tr.addRetries.Acquire(tr.backlogMgr.tqCtx, 1) != nil {\n+\t\t\treturn\n+\t\t}\n+\t\tgo tr.retryAddAfterError(task)\n+\t}\n+}\n+\n+func (tr *fairTaskReader) addErrorBehavior(err error) (drop, retry bool) {\n+\t// addSpooledTask can only fail due to:\n+\t// - the task queue is closed (errTaskQueueClosed or context.Canceled)\n+\t// - ValidateDeployment failed (InvalidArgument)\n+\t// - versioning wants to get a versioned queue and it can't be initialized\n+\t// - versioning wants to re-spool the task on a different queue and that failed\n+\t// - versioning says StickyWorkerUnavailable\n+\tif errors.Is(err, errTaskQueueClosed) || common.IsContextCanceledErr(err) {\n+\t\treturn false, false\n+\t}\n+\tvar stickyUnavailable *serviceerrors.StickyWorkerUnavailable\n+\tif errors.As(err, &stickyUnavailable) {\n+\t\treturn true, false // drop the task\n+\t}\n+\tvar invalid *serviceerror.InvalidArgument\n+\tvar internal *serviceerror.Internal\n+\tif errors.As(err, &invalid) || errors.As(err, &internal) {\n+\t\ttr.backlogMgr.throttledLogger.Error(\"nonretryable error processing spooled task\", tag.Error(err))\n+\t\treturn true, false // drop the task\n+\t}\n+\t// For any other error (this should be very rare), we can retry.\n+\ttr.backlogMgr.throttledLogger.Error(\"retryable error processing spooled task\", tag.Error(err))\n+\treturn false, true\n+}\n+\n+func (tr *fairTaskReader) retryAddAfterError(task *internalTask) {\n+\tdefer tr.addRetries.Release(1)\n+\tmetrics.BufferThrottlePerTaskQueueCounter.With(tr.backlogMgr.metricsHandler).Record(1)\n+\n+\t// initial sleep since we just tried once\n+\t_ = util.InterruptibleSleep(tr.backlogMgr.tqCtx, time.Second)\n+\n+\t_ = backoff.ThrottleRetryContext(\n+\t\ttr.backlogMgr.tqCtx,\n+\t\tfunc(context.Context) error {\n+\t\t\tif IsTaskExpired(task.event.AllocatedTaskInfo) {\n+\t\t\t\ttask.finish(nil, false)\n+\t\t\t\treturn nil\n+\t\t\t}\n+\t\t\terr := tr.backlogMgr.addSpooledTask(task)\n+\t\t\tif drop, retry := tr.addErrorBehavior(err); drop {\n+\t\t\t\ttask.finish(nil, false)\n+\t\t\t} else if retry {\n+\t\t\t\tmetrics.BufferThrottlePerTaskQueueCounter.With(tr.backlogMgr.metricsHandler).Record(1)\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\treturn nil\n+\t\t},\n+\t\taddErrorRetryPolicy,\n+\t\tnil,\n+\t)\n+}\n+\n+func (tr *fairTaskReader) wroteNewTasks(tasks []*persistencespb.AllocatedTaskInfo) {\n+\ttr.mergeTasks(tasks, mergeWrite)\n+}\n+\n+func (tr *fairTaskReader) mergeTasks(tasks []*persistencespb.AllocatedTaskInfo, mode mergeMode) {\n+\ttr.lock.Lock()\n+\n+\t// Take the tasks in the matcher plus the tasks that were just written and sort them by level:\n+\n+\t// Get currently loaded tasks. Note these values are *internalTask.\n+\tmerged := tr.outstandingTasks.Select(func(k, v any) bool {\n+\t\t_, ok := v.(*internalTask)\n+\t\treturn ok\n+\t})\n+\t// Add the tasks we just read/wrote. Note these values are *AllocatedTaskInfo.\n+\tfor _, t := range tasks {\n+\t\tlevel := fairLevelFromAllocatedTask(t)\n+\t\tif mode == mergeWrite && !tr.atEnd && tr.readLevel.less(level) {\n+\t\t\t// If we're writing and we're not at the end, then we have to ignore tasks\n+\t\t\t// above readLevel since we don't know what's in between readLevel and there.\n+\t\t\tcontinue\n+\t\t} else if _, have := merged.Get(level); have {\n+\t\t\t// If write/read race in certain ways, we may read something we had already\n+\t\t\t// added to the matcher. Ignore tasks we already have.\n+\t\t\tcontinue\n+\t\t}\n+\t\tmerged.Put(level, t)\n+\t}\n+\n+\t// Take as many of those as we want to keep in memory. The ones that are not already in the\n+\t// matcher, we have to add to the matcher.\n+\tbatchSize := tr.backlogMgr.config.GetTasksBatchSize()\n+\tit := merged.Iterator()\n+\tvar highestLevel fairLevel\n+\ttasks = tasks[:0] // reuse incoming slice to avoid an allocation\n+\tfor b := 0; it.Next() && b < batchSize; b++ {\n+\t\tif t, ok := it.Value().(*persistencespb.AllocatedTaskInfo); ok {\n+\t\t\t// new task we need to add to the matcher\n+\t\t\ttasks = append(tasks, t)\n+\t\t}\n+\t\thighestLevel = it.Key().(fairLevel) // nolint:revive\n+\t}\n+\n+\tif highestLevel.id != 0 {\n+\t\t// If we have any tasks at all in memory, set readLevel to the maximum of that set.\n+\t\t// Otherwise leave readLevel unchanged.\n+\t\ttr.readLevel = highestLevel\n+\t}\n+\n+\t// If there are remaining tasks in the merged set, they can't fit in memory. If they came\n+\t// from the tasks we just wrote, ignore them. If they came from matcher, remove them.\n+\tdroppedAnyTasks := false",
        "comment_created_at": "2025-07-07T23:05:33+00:00",
        "comment_author": "dnr",
        "comment_body": "Oh that's much better!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2091963607",
    "pr_number": 7740,
    "pr_file": "service/matching/matcher_test.go",
    "created_at": "2025-05-15T21:09:06+00:00",
    "commented_code": "suite.Run(t, new(MatcherTestSuite))\n }\n \n+// attachRateLimiter attaches a rate limiter to the matcher. This behavior is only implemented in tests,\n+// as the matcher relies on the rate limiter that is initialized by the task queue partition manager in production.\n+func (t *MatcherTestSuite) attachRateLimiter(taskQueueConfig *taskQueueConfig) quotas.RateLimiter {",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2091963607",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7740,
        "pr_file": "service/matching/matcher_test.go",
        "discussion_id": "2091963607",
        "commented_code": "@@ -50,6 +51,20 @@ func TestMatcherSuite(t *testing.T) {\n \tsuite.Run(t, new(MatcherTestSuite))\n }\n \n+// attachRateLimiter attaches a rate limiter to the matcher. This behavior is only implemented in tests,\n+// as the matcher relies on the rate limiter that is initialized by the task queue partition manager in production.\n+func (t *MatcherTestSuite) attachRateLimiter(taskQueueConfig *taskQueueConfig) quotas.RateLimiter {",
        "comment_created_at": "2025-05-15T21:09:06+00:00",
        "comment_author": "dnr",
        "comment_body": "\"attachRateLimiter\" doesn't \"attach\" anything, it just returns a new RateLimiter. also it doesn't use anything from the taskQueueConfig or the MatcherTestSuite. so this should just be `func newDefaultRateLimiter() quotasRateLimiter`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2091596200",
    "pr_number": 7778,
    "pr_file": "common/testing/testlogger/testlogger.go",
    "created_at": "2025-05-15T16:42:25+00:00",
    "commented_code": "return tl.state.panicOnError.Swap(v)\n }\n \n+func (tl *TestLogger) mergeTags(tags []tag.Tag) []tag.Tag {",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2091596200",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7778,
        "pr_file": "common/testing/testlogger/testlogger.go",
        "discussion_id": "2091596200",
        "commented_code": "@@ -299,16 +299,32 @@ func (tl *TestLogger) PanicOnError(v bool) bool {\n \treturn tl.state.panicOnError.Swap(v)\n }\n \n+func (tl *TestLogger) mergeTags(tags []tag.Tag) []tag.Tag {",
        "comment_created_at": "2025-05-15T16:42:25+00:00",
        "comment_author": "paulnpdev",
        "comment_body": "```suggestion\r\nfunc (tl *TestLogger) getMergedTags(tags []tag.Tag) []tag.Tag {\r\n```\r\nNit: to me, merge implies to me that you are \"merging them in\" (modifying the internal tag set) which you are not.  \r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2078540284",
    "pr_number": 7709,
    "pr_file": "service/frontend/health_check.go",
    "created_at": "2025-05-07T21:47:01+00:00",
    "commented_code": "}\n \n \tvar failedHostCount float64\n+\tvar hostDeclinedServingCount float64\n \tfor i := 0; i < len(hosts); i++ {\n \t\thealthState := <-receiveCh\n-\t\tif healthState == enumsspb.HEALTH_STATE_NOT_SERVING {\n+\t\tswitch healthState {\n+\t\tcase enumsspb.HEALTH_STATE_NOT_SERVING, enumsspb.HEALTH_STATE_UNSPECIFIED:\n \t\t\tfailedHostCount++\n+\t\tcase enumsspb.HEALTH_STATE_DECLINED_SERVING:\n+\t\t\thostDeclinedServingCount++\n+\t\tcase enumsspb.HEALTH_STATE_SERVING:\n+\t\t\t// Do nothing.\n \t\t}\n \t}\n \tclose(receiveCh)\n \n-\tif (failedHostCount / float64(len(hosts))) > h.hostFailurePercentage() {\n+\t// Make sure that at lease 2 hosts must be not ready to trigger this check.\n+\tproportionOfDeclinedServiceHosts := getProportionOfDeclinedServiceHosts(hostDeclinedServingCount/float64(len(hosts)), len(hosts))\n+\n+\thostDeclinedServingProportion := hostDeclinedServingCount / float64(len(hosts))\n+\tif hostDeclinedServingProportion > proportionOfDeclinedServiceHosts {\n+\t\th.logger.Warn(\"health check exceeded host declined serving proportion threshold\", tag.NewFloat64(\"host declined serving proportion threshold\", proportionOfDeclinedServiceHosts))",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2078540284",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7709,
        "pr_file": "service/frontend/health_check.go",
        "discussion_id": "2078540284",
        "commented_code": "@@ -91,17 +95,49 @@ func (h *healthCheckerImpl) Check(ctx context.Context) (enumsspb.HealthState, er\n \t}\n \n \tvar failedHostCount float64\n+\tvar hostDeclinedServingCount float64\n \tfor i := 0; i < len(hosts); i++ {\n \t\thealthState := <-receiveCh\n-\t\tif healthState == enumsspb.HEALTH_STATE_NOT_SERVING {\n+\t\tswitch healthState {\n+\t\tcase enumsspb.HEALTH_STATE_NOT_SERVING, enumsspb.HEALTH_STATE_UNSPECIFIED:\n \t\t\tfailedHostCount++\n+\t\tcase enumsspb.HEALTH_STATE_DECLINED_SERVING:\n+\t\t\thostDeclinedServingCount++\n+\t\tcase enumsspb.HEALTH_STATE_SERVING:\n+\t\t\t// Do nothing.\n \t\t}\n \t}\n \tclose(receiveCh)\n \n-\tif (failedHostCount / float64(len(hosts))) > h.hostFailurePercentage() {\n+\t// Make sure that at lease 2 hosts must be not ready to trigger this check.\n+\tproportionOfDeclinedServiceHosts := getProportionOfDeclinedServiceHosts(hostDeclinedServingCount/float64(len(hosts)), len(hosts))\n+\n+\thostDeclinedServingProportion := hostDeclinedServingCount / float64(len(hosts))\n+\tif hostDeclinedServingProportion > proportionOfDeclinedServiceHosts {\n+\t\th.logger.Warn(\"health check exceeded host declined serving proportion threshold\", tag.NewFloat64(\"host declined serving proportion threshold\", proportionOfDeclinedServiceHosts))",
        "comment_created_at": "2025-05-07T21:47:01+00:00",
        "comment_author": "Copilot",
        "comment_body": "The local variable 'hostDeclinedServingProportion' declared here shadows the configured threshold value from the dynamic config, leading to a comparison against a computed measurement rather than a fixed configured threshold. Consider renaming this local variable to 'measuredDeclinedProportion' and using h.hostDeclinedServingProportion() to compute the intended threshold via getProportionOfDeclinedServiceHosts.\n```suggestion\n\t// Make sure that at least 2 hosts must be not ready to trigger this check.\n\tmeasuredDeclinedProportion := hostDeclinedServingCount / float64(len(hosts))\n\n\tif measuredDeclinedProportion > h.hostDeclinedServingProportion() {\n\t\th.logger.Warn(\"health check exceeded host declined serving proportion threshold\", tag.NewFloat64(\"host declined serving proportion threshold\", h.hostDeclinedServingProportion()))\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1927662425",
    "pr_number": 7152,
    "pr_file": "components/scheduler/executor_executors.go",
    "created_at": "2025-01-23T20:50:43+00:00",
    "commented_code": "+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\tworkflowpb \"go.temporal.io/api/workflow/v1\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/sdk/temporal\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tExecutorTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\texecutorTaskExecutor struct {",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "1927662425",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7152,
        "pr_file": "components/scheduler/executor_executors.go",
        "discussion_id": "1927662425",
        "commented_code": "@@ -0,0 +1,493 @@\n+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\tworkflowpb \"go.temporal.io/api/workflow/v1\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/sdk/temporal\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tExecutorTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\texecutorTaskExecutor struct {",
        "comment_created_at": "2025-01-23T20:50:43+00:00",
        "comment_author": "lina-temporal",
        "comment_body": "I'm not sure about how everyone else feels, but this name makes my eyes bleed. At the same time, I didn't want to be inconsistent with other HSM naming. Maybe the path of least resistance is to just use a few synonyms for this? Like, `starterTaskExecutor` (and rename the state machine to `Starter`.. or something like that). Really open to suggestions. Ditto for renaming the tasks. In general I'm not satisfied with the naming.",
        "pr_file_module": null
      },
      {
        "comment_id": "1927963104",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7152,
        "pr_file": "components/scheduler/executor_executors.go",
        "discussion_id": "1927662425",
        "commented_code": "@@ -0,0 +1,493 @@\n+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\tworkflowpb \"go.temporal.io/api/workflow/v1\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/sdk/temporal\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tExecutorTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\texecutorTaskExecutor struct {",
        "comment_created_at": "2025-01-24T01:33:28+00:00",
        "comment_author": "bergundy",
        "comment_body": "It's not just a starter though, right? Will this also e.g. terminate workflows? You can call it an invoker maybe?",
        "pr_file_module": null
      },
      {
        "comment_id": "1931158213",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7152,
        "pr_file": "components/scheduler/executor_executors.go",
        "discussion_id": "1927662425",
        "commented_code": "@@ -0,0 +1,493 @@\n+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\tworkflowpb \"go.temporal.io/api/workflow/v1\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/sdk/temporal\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tExecutorTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\texecutorTaskExecutor struct {",
        "comment_created_at": "2025-01-27T20:37:54+00:00",
        "comment_author": "lina-temporal",
        "comment_body": "True, it also terminates/cancels as needed. `Invoker` works for me!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1927976011",
    "pr_number": 7152,
    "pr_file": "components/scheduler/executor_executors.go",
    "created_at": "2025-01-24T01:49:38+00:00",
    "commented_code": "+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\tworkflowpb \"go.temporal.io/api/workflow/v1\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/sdk/temporal\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tExecutorTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\texecutorTaskExecutor struct {\n+\t\tExecutorTaskExecutorOptions\n+\t}\n+\n+\trateLimitedDetails struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tDelay time.Duration\n+\t}\n+)\n+\n+const (\n+\tstartWorkflowMinDeadline = 5 * time.Second // Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMaxDeadline = 1 * time.Hour   // Upper bound for the deadline in which buffered actions are dropped.\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = startWorkflowMaxDeadline\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tExecutorMaxStartAttempts = 10\n+\n+\terrTypeRetryLimitExceeded = \"RetryLimitExceeded\"\n+\terrTypeRateLimited        = \"RateLimited\"\n+\terrTypeAlreadyStarted     = \"serviceerror.WorkflowExecutionAlreadyStarted\"\n+)\n+\n+func RegisterExecutorExecutors(registry *hsm.Registry, options ExecutorTaskExecutorOptions) error {\n+\te := executorTaskExecutor{\n+\t\tExecutorTaskExecutorOptions: options,\n+\t}\n+\treturn hsm.RegisterTimerExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e executorTaskExecutor) executeExecuteTask(env hsm.Environment, node *hsm.Node, task ExecuteTask) error {\n+\tschedulerNode := node.Parent\n+\tscheduler, err := loadScheduler(schedulerNode)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, scheduler)\n+\n+\texecutor, err := e.loadExecutor(node)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Make sure we have something to start. If not, we can clear the buffer and\n+\t// transition to the waiting state.\n+\texecutionInfo := scheduler.Schedule.Action.GetStartWorkflow()\n+\tif executionInfo == nil || len(executor.BufferedStarts) == 0 {\n+\t\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\t\te.ExecutorInternal.BufferedStarts = nil\n+\t\t\treturn TransitionWait.Apply(e, EventWait{\n+\t\t\t\tNode: node,\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t// Drain the Executor's BufferedStarts. drainBuffer will update Executor's\n+\t// BufferedStarts, as well as Scheduler metadata.\n+\te.drainBuffer(logger, env, executor, scheduler, executionInfo)\n+\n+\t// Update SchedulerInfo metadata.\n+\terr = hsm.MachineTransition(schedulerNode, func(s Scheduler) (hsm.TransitionOutput, error) {\n+\t\ts.Info = scheduler.Info\n+\t\treturn hsm.TransitionOutput{}, nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Find the earliest possible time a BufferedStart can be retried, if any, based\n+\t// on BackoffTime.\n+\tvar retryAt time.Time\n+\tfor _, start := range executor.BufferedStarts {\n+\t\tbackoffTime := start.BackoffTime.AsTime()\n+\n+\t\t// Zero-value protobuf timestamps deserialize to the UNIX epoch, skip if the\n+\t\t// backoffTime looks invalid/unset.\n+\t\tif backoffTime.Before(start.ActualTime.AsTime()) {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tif retryAt.IsZero() || backoffTime.Before(retryAt) {\n+\t\t\tretryAt = backoffTime\n+\t\t}\n+\t}\n+\n+\tif !retryAt.IsZero() {\n+\t\t// When retryAt is set, reschedule.\n+\t\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\t\te.ExecutorInternal.BufferedStarts = executor.BufferedStarts\n+\t\t\treturn TransitionExecute.Apply(e, EventExecute{\n+\t\t\t\tNode:     node,\n+\t\t\t\tDeadline: retryAt,\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t// No more buffered starts, or remaining starts are waiting for a workflow to be\n+\t// closed. We can transition to waiting.\n+\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\te.ExecutorInternal.BufferedStarts = executor.BufferedStarts\n+\t\treturn TransitionWait.Apply(e, EventWait{\n+\t\t\tNode: node,\n+\t\t})\n+\t})\n+}\n+\n+// drainBuffer uses ProcessBuffer to resolve the Executor's remaining buffered\n+// starts, and then carries out the returned action.\n+func (e executorTaskExecutor) drainBuffer(\n+\tlogger log.Logger,\n+\tenv hsm.Environment,\n+\texecutor Executor,\n+\tscheduler Scheduler,\n+\trequest *workflowpb.NewWorkflowExecutionInfo,\n+) {\n+\tmetricsWithTag := e.MetricsHandler.WithTags(\n+\t\tmetrics.StringTag(metrics.ScheduleActionTypeTag, metrics.ScheduleActionStartWorkflow))\n+\tisRunning := len(scheduler.Info.RunningWorkflows) > 0\n+\n+\t// Resolve overlap policies and prepare next workflows to start.\n+\taction := scheduler1.ProcessBuffer(executor.BufferedStarts, isRunning, scheduler.resolveOverlapPolicy)",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "1927976011",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7152,
        "pr_file": "components/scheduler/executor_executors.go",
        "discussion_id": "1927976011",
        "commented_code": "@@ -0,0 +1,493 @@\n+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\tworkflowpb \"go.temporal.io/api/workflow/v1\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/sdk/temporal\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tExecutorTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\texecutorTaskExecutor struct {\n+\t\tExecutorTaskExecutorOptions\n+\t}\n+\n+\trateLimitedDetails struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tDelay time.Duration\n+\t}\n+)\n+\n+const (\n+\tstartWorkflowMinDeadline = 5 * time.Second // Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMaxDeadline = 1 * time.Hour   // Upper bound for the deadline in which buffered actions are dropped.\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = startWorkflowMaxDeadline\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tExecutorMaxStartAttempts = 10\n+\n+\terrTypeRetryLimitExceeded = \"RetryLimitExceeded\"\n+\terrTypeRateLimited        = \"RateLimited\"\n+\terrTypeAlreadyStarted     = \"serviceerror.WorkflowExecutionAlreadyStarted\"\n+)\n+\n+func RegisterExecutorExecutors(registry *hsm.Registry, options ExecutorTaskExecutorOptions) error {\n+\te := executorTaskExecutor{\n+\t\tExecutorTaskExecutorOptions: options,\n+\t}\n+\treturn hsm.RegisterTimerExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e executorTaskExecutor) executeExecuteTask(env hsm.Environment, node *hsm.Node, task ExecuteTask) error {\n+\tschedulerNode := node.Parent\n+\tscheduler, err := loadScheduler(schedulerNode)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, scheduler)\n+\n+\texecutor, err := e.loadExecutor(node)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Make sure we have something to start. If not, we can clear the buffer and\n+\t// transition to the waiting state.\n+\texecutionInfo := scheduler.Schedule.Action.GetStartWorkflow()\n+\tif executionInfo == nil || len(executor.BufferedStarts) == 0 {\n+\t\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\t\te.ExecutorInternal.BufferedStarts = nil\n+\t\t\treturn TransitionWait.Apply(e, EventWait{\n+\t\t\t\tNode: node,\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t// Drain the Executor's BufferedStarts. drainBuffer will update Executor's\n+\t// BufferedStarts, as well as Scheduler metadata.\n+\te.drainBuffer(logger, env, executor, scheduler, executionInfo)\n+\n+\t// Update SchedulerInfo metadata.\n+\terr = hsm.MachineTransition(schedulerNode, func(s Scheduler) (hsm.TransitionOutput, error) {\n+\t\ts.Info = scheduler.Info\n+\t\treturn hsm.TransitionOutput{}, nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Find the earliest possible time a BufferedStart can be retried, if any, based\n+\t// on BackoffTime.\n+\tvar retryAt time.Time\n+\tfor _, start := range executor.BufferedStarts {\n+\t\tbackoffTime := start.BackoffTime.AsTime()\n+\n+\t\t// Zero-value protobuf timestamps deserialize to the UNIX epoch, skip if the\n+\t\t// backoffTime looks invalid/unset.\n+\t\tif backoffTime.Before(start.ActualTime.AsTime()) {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tif retryAt.IsZero() || backoffTime.Before(retryAt) {\n+\t\t\tretryAt = backoffTime\n+\t\t}\n+\t}\n+\n+\tif !retryAt.IsZero() {\n+\t\t// When retryAt is set, reschedule.\n+\t\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\t\te.ExecutorInternal.BufferedStarts = executor.BufferedStarts\n+\t\t\treturn TransitionExecute.Apply(e, EventExecute{\n+\t\t\t\tNode:     node,\n+\t\t\t\tDeadline: retryAt,\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t// No more buffered starts, or remaining starts are waiting for a workflow to be\n+\t// closed. We can transition to waiting.\n+\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\te.ExecutorInternal.BufferedStarts = executor.BufferedStarts\n+\t\treturn TransitionWait.Apply(e, EventWait{\n+\t\t\tNode: node,\n+\t\t})\n+\t})\n+}\n+\n+// drainBuffer uses ProcessBuffer to resolve the Executor's remaining buffered\n+// starts, and then carries out the returned action.\n+func (e executorTaskExecutor) drainBuffer(\n+\tlogger log.Logger,\n+\tenv hsm.Environment,\n+\texecutor Executor,\n+\tscheduler Scheduler,\n+\trequest *workflowpb.NewWorkflowExecutionInfo,\n+) {\n+\tmetricsWithTag := e.MetricsHandler.WithTags(\n+\t\tmetrics.StringTag(metrics.ScheduleActionTypeTag, metrics.ScheduleActionStartWorkflow))\n+\tisRunning := len(scheduler.Info.RunningWorkflows) > 0\n+\n+\t// Resolve overlap policies and prepare next workflows to start.\n+\taction := scheduler1.ProcessBuffer(executor.BufferedStarts, isRunning, scheduler.resolveOverlapPolicy)",
        "comment_created_at": "2025-01-24T01:49:38+00:00",
        "comment_author": "bergundy",
        "comment_body": "Does this actually drain the entire buffer? I only see one action being returned here. (I don't have deep familiarity with the code so will just defer to you to confirm).",
        "pr_file_module": null
      },
      {
        "comment_id": "1931171460",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7152,
        "pr_file": "components/scheduler/executor_executors.go",
        "discussion_id": "1927976011",
        "commented_code": "@@ -0,0 +1,493 @@\n+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\tworkflowpb \"go.temporal.io/api/workflow/v1\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/sdk/temporal\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tExecutorTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\texecutorTaskExecutor struct {\n+\t\tExecutorTaskExecutorOptions\n+\t}\n+\n+\trateLimitedDetails struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tDelay time.Duration\n+\t}\n+)\n+\n+const (\n+\tstartWorkflowMinDeadline = 5 * time.Second // Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMaxDeadline = 1 * time.Hour   // Upper bound for the deadline in which buffered actions are dropped.\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = startWorkflowMaxDeadline\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tExecutorMaxStartAttempts = 10\n+\n+\terrTypeRetryLimitExceeded = \"RetryLimitExceeded\"\n+\terrTypeRateLimited        = \"RateLimited\"\n+\terrTypeAlreadyStarted     = \"serviceerror.WorkflowExecutionAlreadyStarted\"\n+)\n+\n+func RegisterExecutorExecutors(registry *hsm.Registry, options ExecutorTaskExecutorOptions) error {\n+\te := executorTaskExecutor{\n+\t\tExecutorTaskExecutorOptions: options,\n+\t}\n+\treturn hsm.RegisterTimerExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e executorTaskExecutor) executeExecuteTask(env hsm.Environment, node *hsm.Node, task ExecuteTask) error {\n+\tschedulerNode := node.Parent\n+\tscheduler, err := loadScheduler(schedulerNode)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, scheduler)\n+\n+\texecutor, err := e.loadExecutor(node)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Make sure we have something to start. If not, we can clear the buffer and\n+\t// transition to the waiting state.\n+\texecutionInfo := scheduler.Schedule.Action.GetStartWorkflow()\n+\tif executionInfo == nil || len(executor.BufferedStarts) == 0 {\n+\t\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\t\te.ExecutorInternal.BufferedStarts = nil\n+\t\t\treturn TransitionWait.Apply(e, EventWait{\n+\t\t\t\tNode: node,\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t// Drain the Executor's BufferedStarts. drainBuffer will update Executor's\n+\t// BufferedStarts, as well as Scheduler metadata.\n+\te.drainBuffer(logger, env, executor, scheduler, executionInfo)\n+\n+\t// Update SchedulerInfo metadata.\n+\terr = hsm.MachineTransition(schedulerNode, func(s Scheduler) (hsm.TransitionOutput, error) {\n+\t\ts.Info = scheduler.Info\n+\t\treturn hsm.TransitionOutput{}, nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Find the earliest possible time a BufferedStart can be retried, if any, based\n+\t// on BackoffTime.\n+\tvar retryAt time.Time\n+\tfor _, start := range executor.BufferedStarts {\n+\t\tbackoffTime := start.BackoffTime.AsTime()\n+\n+\t\t// Zero-value protobuf timestamps deserialize to the UNIX epoch, skip if the\n+\t\t// backoffTime looks invalid/unset.\n+\t\tif backoffTime.Before(start.ActualTime.AsTime()) {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tif retryAt.IsZero() || backoffTime.Before(retryAt) {\n+\t\t\tretryAt = backoffTime\n+\t\t}\n+\t}\n+\n+\tif !retryAt.IsZero() {\n+\t\t// When retryAt is set, reschedule.\n+\t\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\t\te.ExecutorInternal.BufferedStarts = executor.BufferedStarts\n+\t\t\treturn TransitionExecute.Apply(e, EventExecute{\n+\t\t\t\tNode:     node,\n+\t\t\t\tDeadline: retryAt,\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t// No more buffered starts, or remaining starts are waiting for a workflow to be\n+\t// closed. We can transition to waiting.\n+\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\te.ExecutorInternal.BufferedStarts = executor.BufferedStarts\n+\t\treturn TransitionWait.Apply(e, EventWait{\n+\t\t\tNode: node,\n+\t\t})\n+\t})\n+}\n+\n+// drainBuffer uses ProcessBuffer to resolve the Executor's remaining buffered\n+// starts, and then carries out the returned action.\n+func (e executorTaskExecutor) drainBuffer(\n+\tlogger log.Logger,\n+\tenv hsm.Environment,\n+\texecutor Executor,\n+\tscheduler Scheduler,\n+\trequest *workflowpb.NewWorkflowExecutionInfo,\n+) {\n+\tmetricsWithTag := e.MetricsHandler.WithTags(\n+\t\tmetrics.StringTag(metrics.ScheduleActionTypeTag, metrics.ScheduleActionStartWorkflow))\n+\tisRunning := len(scheduler.Info.RunningWorkflows) > 0\n+\n+\t// Resolve overlap policies and prepare next workflows to start.\n+\taction := scheduler1.ProcessBuffer(executor.BufferedStarts, isRunning, scheduler.resolveOverlapPolicy)",
        "comment_created_at": "2025-01-27T20:49:58+00:00",
        "comment_author": "lina-temporal",
        "comment_body": "> Does this actually drain the entire buffer? I only see one action being returned here.\r\n\r\nIt depends on the overlap policy; if something like `OVERLAP_ALL` is specified, it'll drain the buffer. It drains as much of the buffer as possible; I guess I could rename the calling function from `drainBuffer` to `processBuffer` as well. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1933104474",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7152,
        "pr_file": "components/scheduler/executor_executors.go",
        "discussion_id": "1927976011",
        "commented_code": "@@ -0,0 +1,493 @@\n+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\tworkflowpb \"go.temporal.io/api/workflow/v1\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/sdk/temporal\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tExecutorTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\texecutorTaskExecutor struct {\n+\t\tExecutorTaskExecutorOptions\n+\t}\n+\n+\trateLimitedDetails struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tDelay time.Duration\n+\t}\n+)\n+\n+const (\n+\tstartWorkflowMinDeadline = 5 * time.Second // Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMaxDeadline = 1 * time.Hour   // Upper bound for the deadline in which buffered actions are dropped.\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = startWorkflowMaxDeadline\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tExecutorMaxStartAttempts = 10\n+\n+\terrTypeRetryLimitExceeded = \"RetryLimitExceeded\"\n+\terrTypeRateLimited        = \"RateLimited\"\n+\terrTypeAlreadyStarted     = \"serviceerror.WorkflowExecutionAlreadyStarted\"\n+)\n+\n+func RegisterExecutorExecutors(registry *hsm.Registry, options ExecutorTaskExecutorOptions) error {\n+\te := executorTaskExecutor{\n+\t\tExecutorTaskExecutorOptions: options,\n+\t}\n+\treturn hsm.RegisterTimerExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e executorTaskExecutor) executeExecuteTask(env hsm.Environment, node *hsm.Node, task ExecuteTask) error {\n+\tschedulerNode := node.Parent\n+\tscheduler, err := loadScheduler(schedulerNode)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, scheduler)\n+\n+\texecutor, err := e.loadExecutor(node)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Make sure we have something to start. If not, we can clear the buffer and\n+\t// transition to the waiting state.\n+\texecutionInfo := scheduler.Schedule.Action.GetStartWorkflow()\n+\tif executionInfo == nil || len(executor.BufferedStarts) == 0 {\n+\t\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\t\te.ExecutorInternal.BufferedStarts = nil\n+\t\t\treturn TransitionWait.Apply(e, EventWait{\n+\t\t\t\tNode: node,\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t// Drain the Executor's BufferedStarts. drainBuffer will update Executor's\n+\t// BufferedStarts, as well as Scheduler metadata.\n+\te.drainBuffer(logger, env, executor, scheduler, executionInfo)\n+\n+\t// Update SchedulerInfo metadata.\n+\terr = hsm.MachineTransition(schedulerNode, func(s Scheduler) (hsm.TransitionOutput, error) {\n+\t\ts.Info = scheduler.Info\n+\t\treturn hsm.TransitionOutput{}, nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Find the earliest possible time a BufferedStart can be retried, if any, based\n+\t// on BackoffTime.\n+\tvar retryAt time.Time\n+\tfor _, start := range executor.BufferedStarts {\n+\t\tbackoffTime := start.BackoffTime.AsTime()\n+\n+\t\t// Zero-value protobuf timestamps deserialize to the UNIX epoch, skip if the\n+\t\t// backoffTime looks invalid/unset.\n+\t\tif backoffTime.Before(start.ActualTime.AsTime()) {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tif retryAt.IsZero() || backoffTime.Before(retryAt) {\n+\t\t\tretryAt = backoffTime\n+\t\t}\n+\t}\n+\n+\tif !retryAt.IsZero() {\n+\t\t// When retryAt is set, reschedule.\n+\t\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\t\te.ExecutorInternal.BufferedStarts = executor.BufferedStarts\n+\t\t\treturn TransitionExecute.Apply(e, EventExecute{\n+\t\t\t\tNode:     node,\n+\t\t\t\tDeadline: retryAt,\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t// No more buffered starts, or remaining starts are waiting for a workflow to be\n+\t// closed. We can transition to waiting.\n+\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\te.ExecutorInternal.BufferedStarts = executor.BufferedStarts\n+\t\treturn TransitionWait.Apply(e, EventWait{\n+\t\t\tNode: node,\n+\t\t})\n+\t})\n+}\n+\n+// drainBuffer uses ProcessBuffer to resolve the Executor's remaining buffered\n+// starts, and then carries out the returned action.\n+func (e executorTaskExecutor) drainBuffer(\n+\tlogger log.Logger,\n+\tenv hsm.Environment,\n+\texecutor Executor,\n+\tscheduler Scheduler,\n+\trequest *workflowpb.NewWorkflowExecutionInfo,\n+) {\n+\tmetricsWithTag := e.MetricsHandler.WithTags(\n+\t\tmetrics.StringTag(metrics.ScheduleActionTypeTag, metrics.ScheduleActionStartWorkflow))\n+\tisRunning := len(scheduler.Info.RunningWorkflows) > 0\n+\n+\t// Resolve overlap policies and prepare next workflows to start.\n+\taction := scheduler1.ProcessBuffer(executor.BufferedStarts, isRunning, scheduler.resolveOverlapPolicy)",
        "comment_created_at": "2025-01-29T01:46:08+00:00",
        "comment_author": "dnr",
        "comment_body": "I'd agree with that rename.. \"drainBuffer\" sounds like it'll be empty when it returns",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2030309235",
    "pr_number": 7420,
    "pr_file": "common/dynamicconfig/constants.go",
    "created_at": "2025-04-06T23:52:46+00:00",
    "commented_code": "timeout timer when execution timeout is specified when starting a workflow.\n For backward compatibility, this feature is disabled by default and should only be enabled after server version\n containing this flag is deployed to all history service nodes in the cluster.`,\n+\t)\n+\tEnableUpdateClosedWorkflowByMutation = NewGlobalBoolSetting(",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2030309235",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7420,
        "pr_file": "common/dynamicconfig/constants.go",
        "discussion_id": "2030309235",
        "commented_code": "@@ -1425,6 +1425,12 @@ HistoryCacheSizeBasedLimit is set to true.`,\n timeout timer when execution timeout is specified when starting a workflow.\n For backward compatibility, this feature is disabled by default and should only be enabled after server version\n containing this flag is deployed to all history service nodes in the cluster.`,\n+\t)\n+\tEnableUpdateClosedWorkflowByMutation = NewGlobalBoolSetting(",
        "comment_created_at": "2025-04-06T23:52:46+00:00",
        "comment_author": "gow",
        "comment_body": "Nit: Name this `EnableUpdateWorkflowModeSkipCurrent` or something similar since that is what it enables. It makes it easier to discover this config and its purpose.",
        "pr_file_module": null
      }
    ]
  }
]