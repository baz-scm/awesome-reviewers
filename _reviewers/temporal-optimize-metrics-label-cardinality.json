[
  {
    "discussion_id": "2070416591",
    "pr_number": 7688,
    "pr_file": "common/rpc/interceptor/redirection.go",
    "created_at": "2025-05-01T15:34:03+00:00",
    "commented_code": "metricsHandler metrics.Handler,\n \tstartTime time.Time,\n \tclusterName string,\n+\tnamespaceName string,\n \tretError error,\n ) {\n \tmetricsHandler = metricsHandler.WithTags(metrics.TargetClusterTag(clusterName))\n-\tmetrics.ClientRedirectionRequests.With(metricsHandler).Record(1)\n \tmetrics.ClientRedirectionLatency.With(metricsHandler).Record(i.timeSource.Now().Sub(startTime))\n+\n+\tif len(namespaceName) != 0 {\n+\t\tmetricsHandler = metricsHandler.WithTags(metrics.NamespaceTag(namespaceName))\n+\t}\n+\tmetrics.ClientRedirectionRequests.With(metricsHandler).Record(1)",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2070416591",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7688,
        "pr_file": "common/rpc/interceptor/redirection.go",
        "discussion_id": "2070416591",
        "commented_code": "@@ -296,11 +296,16 @@ func (i *Redirection) AfterCall(\n \tmetricsHandler metrics.Handler,\n \tstartTime time.Time,\n \tclusterName string,\n+\tnamespaceName string,\n \tretError error,\n ) {\n \tmetricsHandler = metricsHandler.WithTags(metrics.TargetClusterTag(clusterName))\n-\tmetrics.ClientRedirectionRequests.With(metricsHandler).Record(1)\n \tmetrics.ClientRedirectionLatency.With(metricsHandler).Record(i.timeSource.Now().Sub(startTime))\n+\n+\tif len(namespaceName) != 0 {\n+\t\tmetricsHandler = metricsHandler.WithTags(metrics.NamespaceTag(namespaceName))\n+\t}\n+\tmetrics.ClientRedirectionRequests.With(metricsHandler).Record(1)",
        "comment_created_at": "2025-05-01T15:34:03+00:00",
        "comment_author": "yux0",
        "comment_body": "Can we remove the counter and depends on the latency metric count?",
        "pr_file_module": null
      },
      {
        "comment_id": "2070940845",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7688,
        "pr_file": "common/rpc/interceptor/redirection.go",
        "discussion_id": "2070416591",
        "commented_code": "@@ -296,11 +296,16 @@ func (i *Redirection) AfterCall(\n \tmetricsHandler metrics.Handler,\n \tstartTime time.Time,\n \tclusterName string,\n+\tnamespaceName string,\n \tretError error,\n ) {\n \tmetricsHandler = metricsHandler.WithTags(metrics.TargetClusterTag(clusterName))\n-\tmetrics.ClientRedirectionRequests.With(metricsHandler).Record(1)\n \tmetrics.ClientRedirectionLatency.With(metricsHandler).Record(i.timeSource.Now().Sub(startTime))\n+\n+\tif len(namespaceName) != 0 {\n+\t\tmetricsHandler = metricsHandler.WithTags(metrics.NamespaceTag(namespaceName))\n+\t}\n+\tmetrics.ClientRedirectionRequests.With(metricsHandler).Record(1)",
        "comment_created_at": "2025-05-01T23:55:25+00:00",
        "comment_author": "hehaifengcn",
        "comment_body": "we need counter for namespace label. latency + namespace is not recommended. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2071843415",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7688,
        "pr_file": "common/rpc/interceptor/redirection.go",
        "discussion_id": "2070416591",
        "commented_code": "@@ -296,11 +296,16 @@ func (i *Redirection) AfterCall(\n \tmetricsHandler metrics.Handler,\n \tstartTime time.Time,\n \tclusterName string,\n+\tnamespaceName string,\n \tretError error,\n ) {\n \tmetricsHandler = metricsHandler.WithTags(metrics.TargetClusterTag(clusterName))\n-\tmetrics.ClientRedirectionRequests.With(metricsHandler).Record(1)\n \tmetrics.ClientRedirectionLatency.With(metricsHandler).Record(i.timeSource.Now().Sub(startTime))\n+\n+\tif len(namespaceName) != 0 {\n+\t\tmetricsHandler = metricsHandler.WithTags(metrics.NamespaceTag(namespaceName))\n+\t}\n+\tmetrics.ClientRedirectionRequests.With(metricsHandler).Record(1)",
        "comment_created_at": "2025-05-02T16:11:50+00:00",
        "comment_author": "yux0",
        "comment_body": "do we also need the namespace tag on error count?",
        "pr_file_module": null
      },
      {
        "comment_id": "2071928701",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7688,
        "pr_file": "common/rpc/interceptor/redirection.go",
        "discussion_id": "2070416591",
        "commented_code": "@@ -296,11 +296,16 @@ func (i *Redirection) AfterCall(\n \tmetricsHandler metrics.Handler,\n \tstartTime time.Time,\n \tclusterName string,\n+\tnamespaceName string,\n \tretError error,\n ) {\n \tmetricsHandler = metricsHandler.WithTags(metrics.TargetClusterTag(clusterName))\n-\tmetrics.ClientRedirectionRequests.With(metricsHandler).Record(1)\n \tmetrics.ClientRedirectionLatency.With(metricsHandler).Record(i.timeSource.Now().Sub(startTime))\n+\n+\tif len(namespaceName) != 0 {\n+\t\tmetricsHandler = metricsHandler.WithTags(metrics.NamespaceTag(namespaceName))\n+\t}\n+\tmetrics.ClientRedirectionRequests.With(metricsHandler).Record(1)",
        "comment_created_at": "2025-05-02T17:25:55+00:00",
        "comment_author": "hehaifengcn",
        "comment_body": "I think it wouldn't hurt?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2091952366",
    "pr_number": 7773,
    "pr_file": "service/history/health_signal_aggregator.go",
    "created_at": "2025-05-15T20:59:36+00:00",
    "commented_code": "+package history\n+\n+import (\n+\t\"sync\"\n+\t\"sync/atomic\"\n+\t\"time\"\n+\n+\t\"go.temporal.io/api/serviceerror\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/aggregate\"\n+\t\"go.temporal.io/server/common/dynamicconfig\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+)\n+\n+const (\n+\temitMetricsInterval = 30 * time.Second\n+)\n+\n+var (\n+\tHistoryServiceRPS = metrics.NewTimerDef(\n+\t\t\"history_service_rps\",\n+\t\tmetrics.WithDescription(\"The number of requests per second for each history service method.\"),\n+\t)\n+)\n+\n+var NoopHealthSignalAggregator HealthSignalAggregator = newNoopSignalAggregator()\n+\n+type (\n+\t// HealthSignalAggregator interface for tracking RPC health signals\n+\tHealthSignalAggregator interface {\n+\t\tRecord(method string, latency time.Duration, err error)\n+\t\tAverageLatency() float64\n+\t\tErrorRatio() float64\n+\t\tStart()\n+\t\tStop()\n+\t}\n+\n+\t// HealthSignalAggregatorImpl implements HealthSignalAggregator\n+\tHealthSignalAggregatorImpl struct {\n+\t\tstatus     int32\n+\t\tshutdownCh chan struct{}\n+\n+\t\t// map of method -> request count\n+\t\trequestCounts map[string]int64\n+\t\trequestsLock  sync.Mutex\n+\n+\t\taggregationEnabled bool\n+\t\tlatencyAverage     aggregate.MovingWindowAverage\n+\t\terrorRatio         aggregate.MovingWindowAverage\n+\n+\t\tmetricsHandler          metrics.Handler\n+\t\temitMetricsTimer        *time.Ticker\n+\t\tperMethodRPSWarnLimit   dynamicconfig.IntPropertyFn\n+\t\tperMethodErrorWarnLimit dynamicconfig.FloatPropertyFn\n+\n+\t\tlogger log.Logger\n+\t}\n+\n+\tnoopSignalAggregator struct{}\n+)\n+\n+// NewHealthSignalAggregatorImpl creates a new instance of HealthSignalAggregatorImpl\n+func NewHealthSignalAggregatorImpl(\n+\taggregationEnabled bool,\n+\twindowSize time.Duration,\n+\tmaxBufferSize int,\n+\tmetricsHandler metrics.Handler,\n+\tperMethodRPSWarnLimit dynamicconfig.IntPropertyFn,\n+\tperMethodErrorWarnLimit dynamicconfig.FloatPropertyFn,\n+\tlogger log.Logger,\n+) *HealthSignalAggregatorImpl {\n+\tret := &HealthSignalAggregatorImpl{\n+\t\tstatus:                  common.DaemonStatusInitialized,\n+\t\tshutdownCh:              make(chan struct{}),\n+\t\trequestCounts:           make(map[string]int64),\n+\t\tmetricsHandler:          metricsHandler,\n+\t\temitMetricsTimer:        time.NewTicker(emitMetricsInterval),\n+\t\tperMethodRPSWarnLimit:   perMethodRPSWarnLimit,\n+\t\tperMethodErrorWarnLimit: perMethodErrorWarnLimit,\n+\t\tlogger:                  logger,\n+\t\taggregationEnabled:      aggregationEnabled,\n+\t}\n+\n+\tif aggregationEnabled {\n+\t\tret.latencyAverage = aggregate.NewMovingWindowAvgImpl(windowSize, maxBufferSize)\n+\t\tret.errorRatio = aggregate.NewMovingWindowAvgImpl(windowSize, maxBufferSize)\n+\t} else {\n+\t\tret.latencyAverage = aggregate.NoopMovingWindowAverage\n+\t\tret.errorRatio = aggregate.NoopMovingWindowAverage\n+\t}\n+\n+\treturn ret\n+}\n+\n+func (s *HealthSignalAggregatorImpl) Start() {\n+\tif !atomic.CompareAndSwapInt32(&s.status, common.DaemonStatusInitialized, common.DaemonStatusStarted) {\n+\t\treturn\n+\t}\n+\tgo s.emitMetricsLoop()\n+}\n+\n+func (s *HealthSignalAggregatorImpl) Stop() {\n+\tif !atomic.CompareAndSwapInt32(&s.status, common.DaemonStatusStarted, common.DaemonStatusStopped) {\n+\t\treturn\n+\t}\n+\tclose(s.shutdownCh)\n+\ts.emitMetricsTimer.Stop()\n+}\n+\n+func (s *HealthSignalAggregatorImpl) Record(method string, latency time.Duration, err error) {",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2091952366",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7773,
        "pr_file": "service/history/health_signal_aggregator.go",
        "discussion_id": "2091952366",
        "commented_code": "@@ -0,0 +1,198 @@\n+package history\n+\n+import (\n+\t\"sync\"\n+\t\"sync/atomic\"\n+\t\"time\"\n+\n+\t\"go.temporal.io/api/serviceerror\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/aggregate\"\n+\t\"go.temporal.io/server/common/dynamicconfig\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+)\n+\n+const (\n+\temitMetricsInterval = 30 * time.Second\n+)\n+\n+var (\n+\tHistoryServiceRPS = metrics.NewTimerDef(\n+\t\t\"history_service_rps\",\n+\t\tmetrics.WithDescription(\"The number of requests per second for each history service method.\"),\n+\t)\n+)\n+\n+var NoopHealthSignalAggregator HealthSignalAggregator = newNoopSignalAggregator()\n+\n+type (\n+\t// HealthSignalAggregator interface for tracking RPC health signals\n+\tHealthSignalAggregator interface {\n+\t\tRecord(method string, latency time.Duration, err error)\n+\t\tAverageLatency() float64\n+\t\tErrorRatio() float64\n+\t\tStart()\n+\t\tStop()\n+\t}\n+\n+\t// HealthSignalAggregatorImpl implements HealthSignalAggregator\n+\tHealthSignalAggregatorImpl struct {\n+\t\tstatus     int32\n+\t\tshutdownCh chan struct{}\n+\n+\t\t// map of method -> request count\n+\t\trequestCounts map[string]int64\n+\t\trequestsLock  sync.Mutex\n+\n+\t\taggregationEnabled bool\n+\t\tlatencyAverage     aggregate.MovingWindowAverage\n+\t\terrorRatio         aggregate.MovingWindowAverage\n+\n+\t\tmetricsHandler          metrics.Handler\n+\t\temitMetricsTimer        *time.Ticker\n+\t\tperMethodRPSWarnLimit   dynamicconfig.IntPropertyFn\n+\t\tperMethodErrorWarnLimit dynamicconfig.FloatPropertyFn\n+\n+\t\tlogger log.Logger\n+\t}\n+\n+\tnoopSignalAggregator struct{}\n+)\n+\n+// NewHealthSignalAggregatorImpl creates a new instance of HealthSignalAggregatorImpl\n+func NewHealthSignalAggregatorImpl(\n+\taggregationEnabled bool,\n+\twindowSize time.Duration,\n+\tmaxBufferSize int,\n+\tmetricsHandler metrics.Handler,\n+\tperMethodRPSWarnLimit dynamicconfig.IntPropertyFn,\n+\tperMethodErrorWarnLimit dynamicconfig.FloatPropertyFn,\n+\tlogger log.Logger,\n+) *HealthSignalAggregatorImpl {\n+\tret := &HealthSignalAggregatorImpl{\n+\t\tstatus:                  common.DaemonStatusInitialized,\n+\t\tshutdownCh:              make(chan struct{}),\n+\t\trequestCounts:           make(map[string]int64),\n+\t\tmetricsHandler:          metricsHandler,\n+\t\temitMetricsTimer:        time.NewTicker(emitMetricsInterval),\n+\t\tperMethodRPSWarnLimit:   perMethodRPSWarnLimit,\n+\t\tperMethodErrorWarnLimit: perMethodErrorWarnLimit,\n+\t\tlogger:                  logger,\n+\t\taggregationEnabled:      aggregationEnabled,\n+\t}\n+\n+\tif aggregationEnabled {\n+\t\tret.latencyAverage = aggregate.NewMovingWindowAvgImpl(windowSize, maxBufferSize)\n+\t\tret.errorRatio = aggregate.NewMovingWindowAvgImpl(windowSize, maxBufferSize)\n+\t} else {\n+\t\tret.latencyAverage = aggregate.NoopMovingWindowAverage\n+\t\tret.errorRatio = aggregate.NoopMovingWindowAverage\n+\t}\n+\n+\treturn ret\n+}\n+\n+func (s *HealthSignalAggregatorImpl) Start() {\n+\tif !atomic.CompareAndSwapInt32(&s.status, common.DaemonStatusInitialized, common.DaemonStatusStarted) {\n+\t\treturn\n+\t}\n+\tgo s.emitMetricsLoop()\n+}\n+\n+func (s *HealthSignalAggregatorImpl) Stop() {\n+\tif !atomic.CompareAndSwapInt32(&s.status, common.DaemonStatusStarted, common.DaemonStatusStopped) {\n+\t\treturn\n+\t}\n+\tclose(s.shutdownCh)\n+\ts.emitMetricsTimer.Stop()\n+}\n+\n+func (s *HealthSignalAggregatorImpl) Record(method string, latency time.Duration, err error) {",
        "comment_created_at": "2025-05-15T20:59:36+00:00",
        "comment_author": "prathyushpv",
        "comment_body": "I see that we are tracking latency for each method. Do we need that?\r\nFor persistence, we are emitting per-shard latency. That is because it is not possible to emit shard-id tag in metrics. That will increase the cardinality of that metric beyond what we can handle. So we log that every few seconds.\r\n\r\nWe are already emitting per method metrics. So we might not have to run emitMetricsLoop() for history rpc health tracking. \r\n\r\nIt would be better if we reuse persistence.HealthSignalAggregator here. We can pass a boolean value to it's constructor to disable emitMetricsLoop(). If we don't care about shard-id and namespace, we can pass empty values to it while calling Record()\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2093650649",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7773,
        "pr_file": "service/history/health_signal_aggregator.go",
        "discussion_id": "2091952366",
        "commented_code": "@@ -0,0 +1,198 @@\n+package history\n+\n+import (\n+\t\"sync\"\n+\t\"sync/atomic\"\n+\t\"time\"\n+\n+\t\"go.temporal.io/api/serviceerror\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/aggregate\"\n+\t\"go.temporal.io/server/common/dynamicconfig\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+)\n+\n+const (\n+\temitMetricsInterval = 30 * time.Second\n+)\n+\n+var (\n+\tHistoryServiceRPS = metrics.NewTimerDef(\n+\t\t\"history_service_rps\",\n+\t\tmetrics.WithDescription(\"The number of requests per second for each history service method.\"),\n+\t)\n+)\n+\n+var NoopHealthSignalAggregator HealthSignalAggregator = newNoopSignalAggregator()\n+\n+type (\n+\t// HealthSignalAggregator interface for tracking RPC health signals\n+\tHealthSignalAggregator interface {\n+\t\tRecord(method string, latency time.Duration, err error)\n+\t\tAverageLatency() float64\n+\t\tErrorRatio() float64\n+\t\tStart()\n+\t\tStop()\n+\t}\n+\n+\t// HealthSignalAggregatorImpl implements HealthSignalAggregator\n+\tHealthSignalAggregatorImpl struct {\n+\t\tstatus     int32\n+\t\tshutdownCh chan struct{}\n+\n+\t\t// map of method -> request count\n+\t\trequestCounts map[string]int64\n+\t\trequestsLock  sync.Mutex\n+\n+\t\taggregationEnabled bool\n+\t\tlatencyAverage     aggregate.MovingWindowAverage\n+\t\terrorRatio         aggregate.MovingWindowAverage\n+\n+\t\tmetricsHandler          metrics.Handler\n+\t\temitMetricsTimer        *time.Ticker\n+\t\tperMethodRPSWarnLimit   dynamicconfig.IntPropertyFn\n+\t\tperMethodErrorWarnLimit dynamicconfig.FloatPropertyFn\n+\n+\t\tlogger log.Logger\n+\t}\n+\n+\tnoopSignalAggregator struct{}\n+)\n+\n+// NewHealthSignalAggregatorImpl creates a new instance of HealthSignalAggregatorImpl\n+func NewHealthSignalAggregatorImpl(\n+\taggregationEnabled bool,\n+\twindowSize time.Duration,\n+\tmaxBufferSize int,\n+\tmetricsHandler metrics.Handler,\n+\tperMethodRPSWarnLimit dynamicconfig.IntPropertyFn,\n+\tperMethodErrorWarnLimit dynamicconfig.FloatPropertyFn,\n+\tlogger log.Logger,\n+) *HealthSignalAggregatorImpl {\n+\tret := &HealthSignalAggregatorImpl{\n+\t\tstatus:                  common.DaemonStatusInitialized,\n+\t\tshutdownCh:              make(chan struct{}),\n+\t\trequestCounts:           make(map[string]int64),\n+\t\tmetricsHandler:          metricsHandler,\n+\t\temitMetricsTimer:        time.NewTicker(emitMetricsInterval),\n+\t\tperMethodRPSWarnLimit:   perMethodRPSWarnLimit,\n+\t\tperMethodErrorWarnLimit: perMethodErrorWarnLimit,\n+\t\tlogger:                  logger,\n+\t\taggregationEnabled:      aggregationEnabled,\n+\t}\n+\n+\tif aggregationEnabled {\n+\t\tret.latencyAverage = aggregate.NewMovingWindowAvgImpl(windowSize, maxBufferSize)\n+\t\tret.errorRatio = aggregate.NewMovingWindowAvgImpl(windowSize, maxBufferSize)\n+\t} else {\n+\t\tret.latencyAverage = aggregate.NoopMovingWindowAverage\n+\t\tret.errorRatio = aggregate.NoopMovingWindowAverage\n+\t}\n+\n+\treturn ret\n+}\n+\n+func (s *HealthSignalAggregatorImpl) Start() {\n+\tif !atomic.CompareAndSwapInt32(&s.status, common.DaemonStatusInitialized, common.DaemonStatusStarted) {\n+\t\treturn\n+\t}\n+\tgo s.emitMetricsLoop()\n+}\n+\n+func (s *HealthSignalAggregatorImpl) Stop() {\n+\tif !atomic.CompareAndSwapInt32(&s.status, common.DaemonStatusStarted, common.DaemonStatusStopped) {\n+\t\treturn\n+\t}\n+\tclose(s.shutdownCh)\n+\ts.emitMetricsTimer.Stop()\n+}\n+\n+func (s *HealthSignalAggregatorImpl) Record(method string, latency time.Duration, err error) {",
        "comment_created_at": "2025-05-16T20:37:36+00:00",
        "comment_author": "laniehei",
        "comment_body": "It looks like we're tracking latency for each method, but the actual aggregation and then subsequent Latency number produced is looking at all of them, regardless of method. \r\n\r\nI've converted the emitMetricsLoop() to be a noop -- you're right on us not needing to emit the metrics because we're already doing this elsewhere where we can consume them. \r\n\r\nI would prefer not to reuse the persistence HealthSignalAggregator, because this is a bit confusing with naming/package placement. Will keep separate for now until we find a good shared spot for this.   ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2091960535",
    "pr_number": 7773,
    "pr_file": "service/history/handler.go",
    "created_at": "2025-05-15T21:06:25+00:00",
    "commented_code": "return nil, h.convertError(err)\n \t}\n \n+\tstartTime := h.timeSource.Now()\n+\tdefer func() {",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2091960535",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7773,
        "pr_file": "service/history/handler.go",
        "discussion_id": "2091960535",
        "commented_code": "@@ -264,6 +286,11 @@ func (h *Handler) IsActivityTaskValid(ctx context.Context, request *historyservi\n \t\treturn nil, h.convertError(err)\n \t}\n \n+\tstartTime := h.timeSource.Now()\n+\tdefer func() {",
        "comment_created_at": "2025-05-15T21:06:25+00:00",
        "comment_author": "prathyushpv",
        "comment_body": "We can combine this and `metrics.CapturePanic(h.logger, h.metricsHandler, &retError)` above in a single deferred function. It is better to record the start time at the beginning of the function. We can treat the total time spent in this function as the request latency.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2093456944",
    "pr_number": 7773,
    "pr_file": "service/history/handler.go",
    "created_at": "2025-05-16T17:44:16+00:00",
    "commented_code": "return nil, h.convertError(err)\n \t}\n \n+\tstartTime := h.timeSource.Now()\n+\tdefer func() {\n+\t\th.healthSignalAggregator.Record(\"IsWorkflowTaskValid\", time.Since(startTime), retError)\n+\t}()",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2093456944",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7773,
        "pr_file": "service/history/handler.go",
        "discussion_id": "2093456944",
        "commented_code": "@@ -237,6 +254,11 @@ func (h *Handler) IsWorkflowTaskValid(ctx context.Context, request *historyservi\n \t\treturn nil, h.convertError(err)\n \t}\n \n+\tstartTime := h.timeSource.Now()\n+\tdefer func() {\n+\t\th.healthSignalAggregator.Record(\"IsWorkflowTaskValid\", time.Since(startTime), retError)\n+\t}()",
        "comment_created_at": "2025-05-16T17:44:16+00:00",
        "comment_author": "yux0",
        "comment_body": "Is it possible to have it as an interceptor?",
        "pr_file_module": null
      },
      {
        "comment_id": "2093656325",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7773,
        "pr_file": "service/history/handler.go",
        "discussion_id": "2093456944",
        "commented_code": "@@ -237,6 +254,11 @@ func (h *Handler) IsWorkflowTaskValid(ctx context.Context, request *historyservi\n \t\treturn nil, h.convertError(err)\n \t}\n \n+\tstartTime := h.timeSource.Now()\n+\tdefer func() {\n+\t\th.healthSignalAggregator.Record(\"IsWorkflowTaskValid\", time.Since(startTime), retError)\n+\t}()",
        "comment_created_at": "2025-05-16T20:43:22+00:00",
        "comment_author": "laniehei",
        "comment_body": "We would have to pull from prometheus and we risk it being down. Let me know if I'm misunderstanding the suggestion",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2112920472",
    "pr_number": 7825,
    "pr_file": "service/history/workflow/mutable_state_impl.go",
    "created_at": "2025-05-28T23:28:16+00:00",
    "commented_code": "attachCompletionCallbacks,\n \t\tlinks,\n \t)\n+\tprevEffectiveVersioningBehavior := ms.GetEffectiveVersioningBehavior()\n \tif err := ms.ApplyWorkflowExecutionOptionsUpdatedEvent(event); err != nil {\n \t\treturn nil, err\n \t}\n+\n+\t// Versioning Override set via UpdateWorkflowExecutionOptionsRequest\n+\tif versioningOverride != nil {",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2112920472",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7825,
        "pr_file": "service/history/workflow/mutable_state_impl.go",
        "discussion_id": "2112920472",
        "commented_code": "@@ -4804,9 +4816,23 @@ func (ms *MutableStateImpl) AddWorkflowExecutionOptionsUpdatedEvent(\n \t\tattachCompletionCallbacks,\n \t\tlinks,\n \t)\n+\tprevEffectiveVersioningBehavior := ms.GetEffectiveVersioningBehavior()\n \tif err := ms.ApplyWorkflowExecutionOptionsUpdatedEvent(event); err != nil {\n \t\treturn nil, err\n \t}\n+\n+\t// Versioning Override set via UpdateWorkflowExecutionOptionsRequest\n+\tif versioningOverride != nil {",
        "comment_created_at": "2025-05-28T23:28:16+00:00",
        "comment_author": "ShahabT",
        "comment_body": "the API may be called for other things too. let's only emit the metric if effective behavior OR effective version changes.",
        "pr_file_module": null
      }
    ]
  }
]