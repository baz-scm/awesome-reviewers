[
  {
    "discussion_id": "2165168432",
    "pr_number": 7963,
    "pr_file": "service/matching/workers/worker_query_engine.go",
    "created_at": "2025-06-25T00:07:01+00:00",
    "commented_code": "+package workers\n+\n+import (\n+\t\"fmt\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"github.com/temporalio/sqlparser\"\n+\tworkerpb \"go.temporal.io/api/worker/v1\"\n+\t\"go.temporal.io/server/common/sqlquery\"\n+)\n+\n+const (\n+\tworkerInstanceKeyColName    = \"WorkerInstanceKey\"\n+\tworkerIdentityColName       = \"WorkerIdentity\"\n+\tworkerHostNameColName       = \"HostName\"\n+\tworkerTaskQueueColName      = \"TaskQueue\"\n+\tworkerDeploymentNameColName = \"DeploymentName\"\n+\tworkerSdkNameColName        = \"SdkName\"\n+\tworkerSdkVersionColName     = \"SdkVersion\"\n+\tworkerStartTimeColName      = \"StartTime\"\n+\tworkerHeartbeatTimeColName  = \"HeartbeatTime\"\n+\tworkerStatusColName         = \"WorkerStatus\"\n+)\n+\n+/*\n+FilterWorkers filters the list of per-namespace worker heartbeats against the provided query.\n+The query should be a valid SQL query with WHERE clause.\n+Query is used to filter workers based on worker heartbeat info.\n+The following worker status attributes are expected are supported as part of the query:\n+* WorkerInstanceKey\n+* WorkerIdentity\n+* HostName\n+* TaskQueue\n+* DeploymentName\n+* BuildId\n+* SdkName\n+* SdkVersion\n+* StartTime\n+* LastHeartbeatTime\n+* Status\n+Currently metrics are not supported as a part of ListWorkers query.\n+\n+Field names are case-sensitive.\n+\n+The query can have multiple conditions combined with AND/OR.\n+The query can have conditions on multiple fields.\n+Date time fields should be in RFC3339 format.\n+Example query:\n+\n+\t\"WHERE TaskQueue = 'my_task_queue' AND LastHeartbeatTime < '2023-10-27T10:30:00Z' \"\n+\n+Different fields can support different operators.\n+  - string fields (e.g., WorkerIdentity, HostName, TaskQueue, DeploymentName, SdkName, SdkVersion):\n+\t\tstarts_with, not starts_with\n+  - time fields (e.g., StartTime, LastHeartbeatTime):\n+\t\t =, !=, >, >=, <, <=, between\n+  - metric fields (e.g., total_sticky_cache_hit):\n+\t\t=, !=, >, >=, <, <=\n+\n+Returns the list of workers for which the query matches the worker heartbeat, or an error,\n+Errors are:\n+ - the query is invalid.\n+ - the query is not supported.\n+ - the provided namespace doesn't exist.\n+*/\n+\n+func newWorkerQueryEngine(nsID string, query string) *workerQueryEngine {\n+\treturn &workerQueryEngine{\n+\t\tnsID:  nsID,\n+\t\tquery: query,\n+\t}\n+}\n+\n+type WorkerHeartbeatPropertyFunc func(*workerpb.WorkerHeartbeat) string\n+\n+var (\n+\tpropertyMapFuncs = map[string]WorkerHeartbeatPropertyFunc{\n+\t\tworkerInstanceKeyColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.WorkerInstanceKey\n+\t\t},\n+\t\tworkerIdentityColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.WorkerIdentity\n+\t\t},\n+\t\tworkerHostNameColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.HostInfo.HostName\n+\t\t},\n+\t\tworkerTaskQueueColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.TaskQueue\n+\t\t},\n+\t\tworkerDeploymentNameColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.DeploymentVersion.DeploymentName\n+\t\t},\n+\t\tworkerSdkNameColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.SdkName\n+\t\t},\n+\t\tworkerSdkVersionColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.SdkVersion\n+\t\t},\n+\t\tworkerStatusColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.Status.String()\n+\t\t},\n+\t}\n+)\n+\n+type workerQueryEngine struct {\n+\tnsID          string // Namespace ID\n+\tquery         string\n+\tcurrentWorker *workerpb.WorkerHeartbeat // Current worker heartbeat being evaluated\n+}\n+\n+func (w *workerQueryEngine) EvaluateWorker(hb *workerpb.WorkerHeartbeat) (bool, error) {\n+\tquery, err := prepareQuery(w.query)\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\n+\twhereCause, err := getWhereCause(query)\n+\tif err != nil {\n+\t\treturn false, err\n+\t}",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2165168432",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7963,
        "pr_file": "service/matching/workers/worker_query_engine.go",
        "discussion_id": "2165168432",
        "commented_code": "@@ -0,0 +1,380 @@\n+package workers\n+\n+import (\n+\t\"fmt\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"github.com/temporalio/sqlparser\"\n+\tworkerpb \"go.temporal.io/api/worker/v1\"\n+\t\"go.temporal.io/server/common/sqlquery\"\n+)\n+\n+const (\n+\tworkerInstanceKeyColName    = \"WorkerInstanceKey\"\n+\tworkerIdentityColName       = \"WorkerIdentity\"\n+\tworkerHostNameColName       = \"HostName\"\n+\tworkerTaskQueueColName      = \"TaskQueue\"\n+\tworkerDeploymentNameColName = \"DeploymentName\"\n+\tworkerSdkNameColName        = \"SdkName\"\n+\tworkerSdkVersionColName     = \"SdkVersion\"\n+\tworkerStartTimeColName      = \"StartTime\"\n+\tworkerHeartbeatTimeColName  = \"HeartbeatTime\"\n+\tworkerStatusColName         = \"WorkerStatus\"\n+)\n+\n+/*\n+FilterWorkers filters the list of per-namespace worker heartbeats against the provided query.\n+The query should be a valid SQL query with WHERE clause.\n+Query is used to filter workers based on worker heartbeat info.\n+The following worker status attributes are expected are supported as part of the query:\n+* WorkerInstanceKey\n+* WorkerIdentity\n+* HostName\n+* TaskQueue\n+* DeploymentName\n+* BuildId\n+* SdkName\n+* SdkVersion\n+* StartTime\n+* LastHeartbeatTime\n+* Status\n+Currently metrics are not supported as a part of ListWorkers query.\n+\n+Field names are case-sensitive.\n+\n+The query can have multiple conditions combined with AND/OR.\n+The query can have conditions on multiple fields.\n+Date time fields should be in RFC3339 format.\n+Example query:\n+\n+\t\"WHERE TaskQueue = 'my_task_queue' AND LastHeartbeatTime < '2023-10-27T10:30:00Z' \"\n+\n+Different fields can support different operators.\n+  - string fields (e.g., WorkerIdentity, HostName, TaskQueue, DeploymentName, SdkName, SdkVersion):\n+\t\tstarts_with, not starts_with\n+  - time fields (e.g., StartTime, LastHeartbeatTime):\n+\t\t =, !=, >, >=, <, <=, between\n+  - metric fields (e.g., total_sticky_cache_hit):\n+\t\t=, !=, >, >=, <, <=\n+\n+Returns the list of workers for which the query matches the worker heartbeat, or an error,\n+Errors are:\n+ - the query is invalid.\n+ - the query is not supported.\n+ - the provided namespace doesn't exist.\n+*/\n+\n+func newWorkerQueryEngine(nsID string, query string) *workerQueryEngine {\n+\treturn &workerQueryEngine{\n+\t\tnsID:  nsID,\n+\t\tquery: query,\n+\t}\n+}\n+\n+type WorkerHeartbeatPropertyFunc func(*workerpb.WorkerHeartbeat) string\n+\n+var (\n+\tpropertyMapFuncs = map[string]WorkerHeartbeatPropertyFunc{\n+\t\tworkerInstanceKeyColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.WorkerInstanceKey\n+\t\t},\n+\t\tworkerIdentityColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.WorkerIdentity\n+\t\t},\n+\t\tworkerHostNameColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.HostInfo.HostName\n+\t\t},\n+\t\tworkerTaskQueueColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.TaskQueue\n+\t\t},\n+\t\tworkerDeploymentNameColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.DeploymentVersion.DeploymentName\n+\t\t},\n+\t\tworkerSdkNameColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.SdkName\n+\t\t},\n+\t\tworkerSdkVersionColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.SdkVersion\n+\t\t},\n+\t\tworkerStatusColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.Status.String()\n+\t\t},\n+\t}\n+)\n+\n+type workerQueryEngine struct {\n+\tnsID          string // Namespace ID\n+\tquery         string\n+\tcurrentWorker *workerpb.WorkerHeartbeat // Current worker heartbeat being evaluated\n+}\n+\n+func (w *workerQueryEngine) EvaluateWorker(hb *workerpb.WorkerHeartbeat) (bool, error) {\n+\tquery, err := prepareQuery(w.query)\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\n+\twhereCause, err := getWhereCause(query)\n+\tif err != nil {\n+\t\treturn false, err\n+\t}",
        "comment_created_at": "2025-06-25T00:07:01+00:00",
        "comment_author": "dnr",
        "comment_body": "this is parsing the query on every evaluation? shouldn't that happen upfront in newWorkerQueryEngine?",
        "pr_file_module": null
      },
      {
        "comment_id": "2167738211",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7963,
        "pr_file": "service/matching/workers/worker_query_engine.go",
        "discussion_id": "2165168432",
        "commented_code": "@@ -0,0 +1,380 @@\n+package workers\n+\n+import (\n+\t\"fmt\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"github.com/temporalio/sqlparser\"\n+\tworkerpb \"go.temporal.io/api/worker/v1\"\n+\t\"go.temporal.io/server/common/sqlquery\"\n+)\n+\n+const (\n+\tworkerInstanceKeyColName    = \"WorkerInstanceKey\"\n+\tworkerIdentityColName       = \"WorkerIdentity\"\n+\tworkerHostNameColName       = \"HostName\"\n+\tworkerTaskQueueColName      = \"TaskQueue\"\n+\tworkerDeploymentNameColName = \"DeploymentName\"\n+\tworkerSdkNameColName        = \"SdkName\"\n+\tworkerSdkVersionColName     = \"SdkVersion\"\n+\tworkerStartTimeColName      = \"StartTime\"\n+\tworkerHeartbeatTimeColName  = \"HeartbeatTime\"\n+\tworkerStatusColName         = \"WorkerStatus\"\n+)\n+\n+/*\n+FilterWorkers filters the list of per-namespace worker heartbeats against the provided query.\n+The query should be a valid SQL query with WHERE clause.\n+Query is used to filter workers based on worker heartbeat info.\n+The following worker status attributes are expected are supported as part of the query:\n+* WorkerInstanceKey\n+* WorkerIdentity\n+* HostName\n+* TaskQueue\n+* DeploymentName\n+* BuildId\n+* SdkName\n+* SdkVersion\n+* StartTime\n+* LastHeartbeatTime\n+* Status\n+Currently metrics are not supported as a part of ListWorkers query.\n+\n+Field names are case-sensitive.\n+\n+The query can have multiple conditions combined with AND/OR.\n+The query can have conditions on multiple fields.\n+Date time fields should be in RFC3339 format.\n+Example query:\n+\n+\t\"WHERE TaskQueue = 'my_task_queue' AND LastHeartbeatTime < '2023-10-27T10:30:00Z' \"\n+\n+Different fields can support different operators.\n+  - string fields (e.g., WorkerIdentity, HostName, TaskQueue, DeploymentName, SdkName, SdkVersion):\n+\t\tstarts_with, not starts_with\n+  - time fields (e.g., StartTime, LastHeartbeatTime):\n+\t\t =, !=, >, >=, <, <=, between\n+  - metric fields (e.g., total_sticky_cache_hit):\n+\t\t=, !=, >, >=, <, <=\n+\n+Returns the list of workers for which the query matches the worker heartbeat, or an error,\n+Errors are:\n+ - the query is invalid.\n+ - the query is not supported.\n+ - the provided namespace doesn't exist.\n+*/\n+\n+func newWorkerQueryEngine(nsID string, query string) *workerQueryEngine {\n+\treturn &workerQueryEngine{\n+\t\tnsID:  nsID,\n+\t\tquery: query,\n+\t}\n+}\n+\n+type WorkerHeartbeatPropertyFunc func(*workerpb.WorkerHeartbeat) string\n+\n+var (\n+\tpropertyMapFuncs = map[string]WorkerHeartbeatPropertyFunc{\n+\t\tworkerInstanceKeyColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.WorkerInstanceKey\n+\t\t},\n+\t\tworkerIdentityColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.WorkerIdentity\n+\t\t},\n+\t\tworkerHostNameColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.HostInfo.HostName\n+\t\t},\n+\t\tworkerTaskQueueColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.TaskQueue\n+\t\t},\n+\t\tworkerDeploymentNameColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.DeploymentVersion.DeploymentName\n+\t\t},\n+\t\tworkerSdkNameColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.SdkName\n+\t\t},\n+\t\tworkerSdkVersionColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.SdkVersion\n+\t\t},\n+\t\tworkerStatusColName: func(hb *workerpb.WorkerHeartbeat) string {\n+\t\t\treturn hb.Status.String()\n+\t\t},\n+\t}\n+)\n+\n+type workerQueryEngine struct {\n+\tnsID          string // Namespace ID\n+\tquery         string\n+\tcurrentWorker *workerpb.WorkerHeartbeat // Current worker heartbeat being evaluated\n+}\n+\n+func (w *workerQueryEngine) EvaluateWorker(hb *workerpb.WorkerHeartbeat) (bool, error) {\n+\tquery, err := prepareQuery(w.query)\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\n+\twhereCause, err := getWhereCause(query)\n+\tif err != nil {\n+\t\treturn false, err\n+\t}",
        "comment_created_at": "2025-06-25T22:20:11+00:00",
        "comment_author": "ychebotarev",
        "comment_body": "you right, good catch",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2133076443",
    "pr_number": 7729,
    "pr_file": "chasm/tree.go",
    "created_at": "2025-06-06T23:39:42+00:00",
    "commented_code": "return nil\n }\n+\n+// ValidateSideEffectTask runs a side effect task's associated validator,\n+// returning the deserialized task instance if the task is valid. Intended for\n+// use by standby executors.\n+//\n+// If validation succeeds but the task is invalid, nil is returned to signify the\n+// task can be skipped/deleted.\n+//\n+// If validation fails, that error is returned.\n+func (n *Node) ValidateSideEffectTask(\n+\tctx context.Context,\n+\tregistry *Registry,\n+\ttaskInfo *persistencespb.ChasmTaskInfo,\n+) (any, error) {\n+\ttaskType := taskInfo.Type\n+\tregistrableTask, ok := registry.task(taskType)\n+\tif !ok {\n+\t\treturn nil, serviceerror.NewInternalf(\"unknown task type '%s'\", taskType)\n+\t}\n+\n+\tif registrableTask.isPureTask {\n+\t\treturn nil, serviceerror.NewInternalf(\"ValidateSideEffectTask called on a Pure task '%s'\", taskType)\n+\t}\n+\n+\ttaskValue, err := deserializeTask(registrableTask, taskInfo.Data)",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2133076443",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7729,
        "pr_file": "chasm/tree.go",
        "discussion_id": "2133076443",
        "commented_code": "@@ -2025,3 +2028,155 @@ func (n *Node) ExecutePureTask(baseCtx context.Context, taskInstance any) error\n \n \treturn nil\n }\n+\n+// ValidateSideEffectTask runs a side effect task's associated validator,\n+// returning the deserialized task instance if the task is valid. Intended for\n+// use by standby executors.\n+//\n+// If validation succeeds but the task is invalid, nil is returned to signify the\n+// task can be skipped/deleted.\n+//\n+// If validation fails, that error is returned.\n+func (n *Node) ValidateSideEffectTask(\n+\tctx context.Context,\n+\tregistry *Registry,\n+\ttaskInfo *persistencespb.ChasmTaskInfo,\n+) (any, error) {\n+\ttaskType := taskInfo.Type\n+\tregistrableTask, ok := registry.task(taskType)\n+\tif !ok {\n+\t\treturn nil, serviceerror.NewInternalf(\"unknown task type '%s'\", taskType)\n+\t}\n+\n+\tif registrableTask.isPureTask {\n+\t\treturn nil, serviceerror.NewInternalf(\"ValidateSideEffectTask called on a Pure task '%s'\", taskType)\n+\t}\n+\n+\ttaskValue, err := deserializeTask(registrableTask, taskInfo.Data)",
        "comment_created_at": "2025-06-06T23:39:42+00:00",
        "comment_author": "yycptt",
        "comment_body": "Should ideally cache the deserialized task I think. maybe add a TODO here.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2069613555",
    "pr_number": 7052,
    "pr_file": "common/dynamicconfig/collection.go",
    "created_at": "2025-04-30T22:55:42+00:00",
    "commented_code": "return errCount < errCountLogThreshold || errCount%errCountLogThreshold == 0\n }\n \n-func findMatch[T any](cvs []ConstrainedValue, defaultCVs []TypedConstrainedValue[T], precedence []Constraints) (any, error) {\n-\tif len(cvs)+len(defaultCVs) == 0 {\n+func findMatch(cvs []ConstrainedValue, precedence []Constraints) (*ConstrainedValue, error) {\n+\tif len(cvs) == 0 {\n \t\treturn nil, errKeyNotPresent\n \t}\n \tfor _, m := range precedence {\n-\t\tfor _, cv := range cvs {\n-\t\t\tif m == cv.Constraints {\n-\t\t\t\treturn cv.Value, nil\n-\t\t\t}\n-\t\t}\n-\t\tfor _, cv := range defaultCVs {\n+\t\tfor idx, cv := range cvs {\n \t\t\tif m == cv.Constraints {\n-\t\t\t\treturn cv.Value, nil\n+\t\t\t\treturn &cvs[idx], nil",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2069613555",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7052,
        "pr_file": "common/dynamicconfig/collection.go",
        "discussion_id": "2069613555",
        "commented_code": "@@ -225,19 +237,14 @@ func (c *Collection) throttleLog() bool {\n \treturn errCount < errCountLogThreshold || errCount%errCountLogThreshold == 0\n }\n \n-func findMatch[T any](cvs []ConstrainedValue, defaultCVs []TypedConstrainedValue[T], precedence []Constraints) (any, error) {\n-\tif len(cvs)+len(defaultCVs) == 0 {\n+func findMatch(cvs []ConstrainedValue, precedence []Constraints) (*ConstrainedValue, error) {\n+\tif len(cvs) == 0 {\n \t\treturn nil, errKeyNotPresent\n \t}\n \tfor _, m := range precedence {\n-\t\tfor _, cv := range cvs {\n-\t\t\tif m == cv.Constraints {\n-\t\t\t\treturn cv.Value, nil\n-\t\t\t}\n-\t\t}\n-\t\tfor _, cv := range defaultCVs {\n+\t\tfor idx, cv := range cvs {\n \t\t\tif m == cv.Constraints {\n-\t\t\t\treturn cv.Value, nil\n+\t\t\t\treturn &cvs[idx], nil",
        "comment_created_at": "2025-04-30T22:55:42+00:00",
        "comment_author": "bergundy",
        "comment_body": "Is this different from `&cv`? If there's a reason, please put a comment here.",
        "pr_file_module": null
      },
      {
        "comment_id": "2070587758",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7052,
        "pr_file": "common/dynamicconfig/collection.go",
        "discussion_id": "2069613555",
        "commented_code": "@@ -225,19 +237,14 @@ func (c *Collection) throttleLog() bool {\n \treturn errCount < errCountLogThreshold || errCount%errCountLogThreshold == 0\n }\n \n-func findMatch[T any](cvs []ConstrainedValue, defaultCVs []TypedConstrainedValue[T], precedence []Constraints) (any, error) {\n-\tif len(cvs)+len(defaultCVs) == 0 {\n+func findMatch(cvs []ConstrainedValue, precedence []Constraints) (*ConstrainedValue, error) {\n+\tif len(cvs) == 0 {\n \t\treturn nil, errKeyNotPresent\n \t}\n \tfor _, m := range precedence {\n-\t\tfor _, cv := range cvs {\n-\t\t\tif m == cv.Constraints {\n-\t\t\t\treturn cv.Value, nil\n-\t\t\t}\n-\t\t}\n-\t\tfor _, cv := range defaultCVs {\n+\t\tfor idx, cv := range cvs {\n \t\t\tif m == cv.Constraints {\n-\t\t\t\treturn cv.Value, nil\n+\t\t\t\treturn &cvs[idx], nil",
        "comment_created_at": "2025-05-01T17:57:33+00:00",
        "comment_author": "dnr",
        "comment_body": "Yes, `&cv` would allocate a new ConstrainedValue, which would defeat the caching. This returns a pointer into the given slice, which comes directly from the Client. I'll add a comment",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2078302114",
    "pr_number": 7718,
    "pr_file": "common/rpc/interceptor/slow_request_logger.go",
    "created_at": "2025-05-07T18:57:01+00:00",
    "commented_code": "+package interceptor\n+\n+import (\n+\t\"context\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"go.temporal.io/server/common/dynamicconfig\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/rpc/interceptor/logtags\"\n+\t\"go.temporal.io/server/common/tasktoken\"\n+\t\"google.golang.org/grpc\"\n+)\n+\n+// Certain types of methods are ignored as a rule.\n+var ignoredMethodSubstrings = []string{\"Poll\", \"GetWorkflowExecutionHistory\"}\n+\n+type SlowRequestLoggerInterceptor struct {\n+\tlogger       log.Logger\n+\tworkflowTags *logtags.WorkflowTags\n+\tdc           *dynamicconfig.Collection\n+}\n+\n+func NewSlowRequestLoggerInterceptor(\n+\tlogger log.Logger,\n+\tdc *dynamicconfig.Collection,\n+) *SlowRequestLoggerInterceptor {\n+\treturn &SlowRequestLoggerInterceptor{\n+\t\tlogger:       logger,\n+\t\tworkflowTags: logtags.NewWorkflowTags(tasktoken.NewSerializer(), logger),\n+\t\tdc:           dc,\n+\t}\n+}\n+\n+func (i *SlowRequestLoggerInterceptor) Intercept(\n+\tctx context.Context,\n+\trequest interface{},\n+\tinfo *grpc.UnaryServerInfo,\n+\thandler grpc.UnaryHandler,\n+) (interface{}, error) {\n+\tthreshold := dynamicconfig.SlowRequestLoggingThreshold.Get(i.dc)()",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2078302114",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7718,
        "pr_file": "common/rpc/interceptor/slow_request_logger.go",
        "discussion_id": "2078302114",
        "commented_code": "@@ -0,0 +1,74 @@\n+package interceptor\n+\n+import (\n+\t\"context\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"go.temporal.io/server/common/dynamicconfig\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/rpc/interceptor/logtags\"\n+\t\"go.temporal.io/server/common/tasktoken\"\n+\t\"google.golang.org/grpc\"\n+)\n+\n+// Certain types of methods are ignored as a rule.\n+var ignoredMethodSubstrings = []string{\"Poll\", \"GetWorkflowExecutionHistory\"}\n+\n+type SlowRequestLoggerInterceptor struct {\n+\tlogger       log.Logger\n+\tworkflowTags *logtags.WorkflowTags\n+\tdc           *dynamicconfig.Collection\n+}\n+\n+func NewSlowRequestLoggerInterceptor(\n+\tlogger log.Logger,\n+\tdc *dynamicconfig.Collection,\n+) *SlowRequestLoggerInterceptor {\n+\treturn &SlowRequestLoggerInterceptor{\n+\t\tlogger:       logger,\n+\t\tworkflowTags: logtags.NewWorkflowTags(tasktoken.NewSerializer(), logger),\n+\t\tdc:           dc,\n+\t}\n+}\n+\n+func (i *SlowRequestLoggerInterceptor) Intercept(\n+\tctx context.Context,\n+\trequest interface{},\n+\tinfo *grpc.UnaryServerInfo,\n+\thandler grpc.UnaryHandler,\n+) (interface{}, error) {\n+\tthreshold := dynamicconfig.SlowRequestLoggingThreshold.Get(i.dc)()",
        "comment_created_at": "2025-05-07T18:57:01+00:00",
        "comment_author": "dnr",
        "comment_body": "Ideally we would do the exclusion before the call to get the dynamic config, to avoid that call if we're excluding anyway",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2013500654",
    "pr_number": 7152,
    "pr_file": "components/scheduler/invoker_executors.go",
    "created_at": "2025-03-26T06:59:21+00:00",
    "commented_code": "+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\t\"go.temporal.io/api/serviceerror\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tInvokerTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\tinvokerTaskExecutor struct {\n+\t\tInvokerTaskExecutorOptions\n+\t}\n+\n+\trateLimitedError struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tdelay time.Duration\n+\t}\n+)\n+\n+const (\n+\t// Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMinDeadline = 5 * time.Second\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = 1 * time.Hour\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tInvokerMaxStartAttempts = 10 // TODO - dial this up/remove it\n+)\n+\n+var (\n+\terrRetryLimitExceeded       = errors.New(\"retry limit exceeded\")\n+\t_                     error = &rateLimitedError{}\n+)\n+\n+func RegisterInvokerExecutors(registry *hsm.Registry, options InvokerTaskExecutorOptions) error {\n+\te := invokerTaskExecutor{\n+\t\tInvokerTaskExecutorOptions: options,\n+\t}\n+\n+\terr := hsm.RegisterTimerExecutor(registry, e.executeProcessBufferTask)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn hsm.RegisterImmediateExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e invokerTaskExecutor) executeExecuteTask(\n+\tctx context.Context,\n+\tenv hsm.Environment,\n+\tref hsm.Ref,\n+\ttask ExecuteTask,\n+) error {\n+\tvar scheduler *Scheduler\n+\tvar invoker *Invoker\n+\tvar result executeResult\n+\n+\t// Load Scheduler and Invoker's current states.\n+\terr := env.Access(ctx, ref, hsm.AccessRead, func(node *hsm.Node) error {\n+\t\ts, err := loadScheduler(node.Parent)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tscheduler = &s\n+\n+\t\ti, err := e.loadInvoker(node)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tinvoker = &i\n+\n+\t\treturn nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, *scheduler)\n+\n+\t// If we have nothing to do, we can return without any additional writes.\n+\teligibleStarts := invoker.getEligibleBufferedStarts()\n+\tif len(invoker.GetTerminateWorkflows())+\n+\t\tlen(invoker.GetCancelWorkflows())+\n+\t\tlen(eligibleStarts) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\t// Terminate, cancel, and start workflows. The result struct contains the\n+\t// complete outcome of all requests executed in a single batch.\n+\tactionsTaken := 0\n+\tresult = result.Append(e.terminateWorkflows(ctx, logger, *scheduler, &actionsTaken, invoker.GetTerminateWorkflows()))\n+\tresult = result.Append(e.cancelWorkflows(ctx, logger, *scheduler, &actionsTaken, invoker.GetCancelWorkflows()))\n+\tsres, startResults := e.startWorkflows(ctx, logger, env, *scheduler, &actionsTaken, eligibleStarts)\n+\tresult = result.Append(sres)\n+\n+\t// Write results.\n+\treturn env.Access(ctx, ref, hsm.AccessWrite, func(node *hsm.Node) error {\n+\t\t// Record completed executions on the Invoker.\n+\t\terr := hsm.MachineTransition(node, func(i Invoker) (hsm.TransitionOutput, error) {\n+\t\t\treturn TransitionRecordExecution.Apply(i, EventRecordExecution{\n+\t\t\t\tNode:          node,\n+\t\t\t\texecuteResult: result,\n+\t\t\t})\n+\t\t})\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\n+\t\t// Record action results on the Scheduler.\n+\t\treturn hsm.MachineTransition(node.Parent, func(s Scheduler) (hsm.TransitionOutput, error) {\n+\t\t\treturn TransitionRecordAction.Apply(s, EventRecordAction{\n+\t\t\t\tNode:        node,\n+\t\t\t\tActionCount: int64(len(startResults)),\n+\t\t\t\tResults:     startResults,\n+\t\t\t})\n+\t\t})\n+\t})\n+}\n+\n+// shouldYield returns true when the immediate task should complete/spawn a new task.\n+func (e invokerTaskExecutor) shouldYield(scheduler Scheduler, actionsTaken int) bool {\n+\ttweakables := e.Config.Tweakables(scheduler.Namespace)",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2013500654",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7152,
        "pr_file": "components/scheduler/invoker_executors.go",
        "discussion_id": "2013500654",
        "commented_code": "@@ -0,0 +1,613 @@\n+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\t\"go.temporal.io/api/serviceerror\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tInvokerTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\tinvokerTaskExecutor struct {\n+\t\tInvokerTaskExecutorOptions\n+\t}\n+\n+\trateLimitedError struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tdelay time.Duration\n+\t}\n+)\n+\n+const (\n+\t// Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMinDeadline = 5 * time.Second\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = 1 * time.Hour\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tInvokerMaxStartAttempts = 10 // TODO - dial this up/remove it\n+)\n+\n+var (\n+\terrRetryLimitExceeded       = errors.New(\"retry limit exceeded\")\n+\t_                     error = &rateLimitedError{}\n+)\n+\n+func RegisterInvokerExecutors(registry *hsm.Registry, options InvokerTaskExecutorOptions) error {\n+\te := invokerTaskExecutor{\n+\t\tInvokerTaskExecutorOptions: options,\n+\t}\n+\n+\terr := hsm.RegisterTimerExecutor(registry, e.executeProcessBufferTask)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn hsm.RegisterImmediateExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e invokerTaskExecutor) executeExecuteTask(\n+\tctx context.Context,\n+\tenv hsm.Environment,\n+\tref hsm.Ref,\n+\ttask ExecuteTask,\n+) error {\n+\tvar scheduler *Scheduler\n+\tvar invoker *Invoker\n+\tvar result executeResult\n+\n+\t// Load Scheduler and Invoker's current states.\n+\terr := env.Access(ctx, ref, hsm.AccessRead, func(node *hsm.Node) error {\n+\t\ts, err := loadScheduler(node.Parent)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tscheduler = &s\n+\n+\t\ti, err := e.loadInvoker(node)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tinvoker = &i\n+\n+\t\treturn nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, *scheduler)\n+\n+\t// If we have nothing to do, we can return without any additional writes.\n+\teligibleStarts := invoker.getEligibleBufferedStarts()\n+\tif len(invoker.GetTerminateWorkflows())+\n+\t\tlen(invoker.GetCancelWorkflows())+\n+\t\tlen(eligibleStarts) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\t// Terminate, cancel, and start workflows. The result struct contains the\n+\t// complete outcome of all requests executed in a single batch.\n+\tactionsTaken := 0\n+\tresult = result.Append(e.terminateWorkflows(ctx, logger, *scheduler, &actionsTaken, invoker.GetTerminateWorkflows()))\n+\tresult = result.Append(e.cancelWorkflows(ctx, logger, *scheduler, &actionsTaken, invoker.GetCancelWorkflows()))\n+\tsres, startResults := e.startWorkflows(ctx, logger, env, *scheduler, &actionsTaken, eligibleStarts)\n+\tresult = result.Append(sres)\n+\n+\t// Write results.\n+\treturn env.Access(ctx, ref, hsm.AccessWrite, func(node *hsm.Node) error {\n+\t\t// Record completed executions on the Invoker.\n+\t\terr := hsm.MachineTransition(node, func(i Invoker) (hsm.TransitionOutput, error) {\n+\t\t\treturn TransitionRecordExecution.Apply(i, EventRecordExecution{\n+\t\t\t\tNode:          node,\n+\t\t\t\texecuteResult: result,\n+\t\t\t})\n+\t\t})\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\n+\t\t// Record action results on the Scheduler.\n+\t\treturn hsm.MachineTransition(node.Parent, func(s Scheduler) (hsm.TransitionOutput, error) {\n+\t\t\treturn TransitionRecordAction.Apply(s, EventRecordAction{\n+\t\t\t\tNode:        node,\n+\t\t\t\tActionCount: int64(len(startResults)),\n+\t\t\t\tResults:     startResults,\n+\t\t\t})\n+\t\t})\n+\t})\n+}\n+\n+// shouldYield returns true when the immediate task should complete/spawn a new task.\n+func (e invokerTaskExecutor) shouldYield(scheduler Scheduler, actionsTaken int) bool {\n+\ttweakables := e.Config.Tweakables(scheduler.Namespace)",
        "comment_created_at": "2025-03-26T06:59:21+00:00",
        "comment_author": "yycptt",
        "comment_body": "nit: not sure how expensive this is. May worth considering evaluating maxActions only once per task.",
        "pr_file_module": null
      },
      {
        "comment_id": "2017562758",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7152,
        "pr_file": "components/scheduler/invoker_executors.go",
        "discussion_id": "2013500654",
        "commented_code": "@@ -0,0 +1,613 @@\n+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\t\"go.temporal.io/api/serviceerror\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tInvokerTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\tinvokerTaskExecutor struct {\n+\t\tInvokerTaskExecutorOptions\n+\t}\n+\n+\trateLimitedError struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tdelay time.Duration\n+\t}\n+)\n+\n+const (\n+\t// Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMinDeadline = 5 * time.Second\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = 1 * time.Hour\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tInvokerMaxStartAttempts = 10 // TODO - dial this up/remove it\n+)\n+\n+var (\n+\terrRetryLimitExceeded       = errors.New(\"retry limit exceeded\")\n+\t_                     error = &rateLimitedError{}\n+)\n+\n+func RegisterInvokerExecutors(registry *hsm.Registry, options InvokerTaskExecutorOptions) error {\n+\te := invokerTaskExecutor{\n+\t\tInvokerTaskExecutorOptions: options,\n+\t}\n+\n+\terr := hsm.RegisterTimerExecutor(registry, e.executeProcessBufferTask)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn hsm.RegisterImmediateExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e invokerTaskExecutor) executeExecuteTask(\n+\tctx context.Context,\n+\tenv hsm.Environment,\n+\tref hsm.Ref,\n+\ttask ExecuteTask,\n+) error {\n+\tvar scheduler *Scheduler\n+\tvar invoker *Invoker\n+\tvar result executeResult\n+\n+\t// Load Scheduler and Invoker's current states.\n+\terr := env.Access(ctx, ref, hsm.AccessRead, func(node *hsm.Node) error {\n+\t\ts, err := loadScheduler(node.Parent)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tscheduler = &s\n+\n+\t\ti, err := e.loadInvoker(node)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tinvoker = &i\n+\n+\t\treturn nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, *scheduler)\n+\n+\t// If we have nothing to do, we can return without any additional writes.\n+\teligibleStarts := invoker.getEligibleBufferedStarts()\n+\tif len(invoker.GetTerminateWorkflows())+\n+\t\tlen(invoker.GetCancelWorkflows())+\n+\t\tlen(eligibleStarts) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\t// Terminate, cancel, and start workflows. The result struct contains the\n+\t// complete outcome of all requests executed in a single batch.\n+\tactionsTaken := 0\n+\tresult = result.Append(e.terminateWorkflows(ctx, logger, *scheduler, &actionsTaken, invoker.GetTerminateWorkflows()))\n+\tresult = result.Append(e.cancelWorkflows(ctx, logger, *scheduler, &actionsTaken, invoker.GetCancelWorkflows()))\n+\tsres, startResults := e.startWorkflows(ctx, logger, env, *scheduler, &actionsTaken, eligibleStarts)\n+\tresult = result.Append(sres)\n+\n+\t// Write results.\n+\treturn env.Access(ctx, ref, hsm.AccessWrite, func(node *hsm.Node) error {\n+\t\t// Record completed executions on the Invoker.\n+\t\terr := hsm.MachineTransition(node, func(i Invoker) (hsm.TransitionOutput, error) {\n+\t\t\treturn TransitionRecordExecution.Apply(i, EventRecordExecution{\n+\t\t\t\tNode:          node,\n+\t\t\t\texecuteResult: result,\n+\t\t\t})\n+\t\t})\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\n+\t\t// Record action results on the Scheduler.\n+\t\treturn hsm.MachineTransition(node.Parent, func(s Scheduler) (hsm.TransitionOutput, error) {\n+\t\t\treturn TransitionRecordAction.Apply(s, EventRecordAction{\n+\t\t\t\tNode:        node,\n+\t\t\t\tActionCount: int64(len(startResults)),\n+\t\t\t\tResults:     startResults,\n+\t\t\t})\n+\t\t})\n+\t})\n+}\n+\n+// shouldYield returns true when the immediate task should complete/spawn a new task.\n+func (e invokerTaskExecutor) shouldYield(scheduler Scheduler, actionsTaken int) bool {\n+\ttweakables := e.Config.Tweakables(scheduler.Namespace)",
        "comment_created_at": "2025-03-27T20:32:18+00:00",
        "comment_author": "lina-temporal",
        "comment_body": "Yeah, that's a fair point. I refactored this a little to instead pass along a wrapped context that includes `actionsTaken/maxActions` on it. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2017890135",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7152,
        "pr_file": "components/scheduler/invoker_executors.go",
        "discussion_id": "2013500654",
        "commented_code": "@@ -0,0 +1,613 @@\n+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\t\"go.temporal.io/api/serviceerror\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tInvokerTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\tinvokerTaskExecutor struct {\n+\t\tInvokerTaskExecutorOptions\n+\t}\n+\n+\trateLimitedError struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tdelay time.Duration\n+\t}\n+)\n+\n+const (\n+\t// Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMinDeadline = 5 * time.Second\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = 1 * time.Hour\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tInvokerMaxStartAttempts = 10 // TODO - dial this up/remove it\n+)\n+\n+var (\n+\terrRetryLimitExceeded       = errors.New(\"retry limit exceeded\")\n+\t_                     error = &rateLimitedError{}\n+)\n+\n+func RegisterInvokerExecutors(registry *hsm.Registry, options InvokerTaskExecutorOptions) error {\n+\te := invokerTaskExecutor{\n+\t\tInvokerTaskExecutorOptions: options,\n+\t}\n+\n+\terr := hsm.RegisterTimerExecutor(registry, e.executeProcessBufferTask)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn hsm.RegisterImmediateExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e invokerTaskExecutor) executeExecuteTask(\n+\tctx context.Context,\n+\tenv hsm.Environment,\n+\tref hsm.Ref,\n+\ttask ExecuteTask,\n+) error {\n+\tvar scheduler *Scheduler\n+\tvar invoker *Invoker\n+\tvar result executeResult\n+\n+\t// Load Scheduler and Invoker's current states.\n+\terr := env.Access(ctx, ref, hsm.AccessRead, func(node *hsm.Node) error {\n+\t\ts, err := loadScheduler(node.Parent)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tscheduler = &s\n+\n+\t\ti, err := e.loadInvoker(node)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tinvoker = &i\n+\n+\t\treturn nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, *scheduler)\n+\n+\t// If we have nothing to do, we can return without any additional writes.\n+\teligibleStarts := invoker.getEligibleBufferedStarts()\n+\tif len(invoker.GetTerminateWorkflows())+\n+\t\tlen(invoker.GetCancelWorkflows())+\n+\t\tlen(eligibleStarts) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\t// Terminate, cancel, and start workflows. The result struct contains the\n+\t// complete outcome of all requests executed in a single batch.\n+\tactionsTaken := 0\n+\tresult = result.Append(e.terminateWorkflows(ctx, logger, *scheduler, &actionsTaken, invoker.GetTerminateWorkflows()))\n+\tresult = result.Append(e.cancelWorkflows(ctx, logger, *scheduler, &actionsTaken, invoker.GetCancelWorkflows()))\n+\tsres, startResults := e.startWorkflows(ctx, logger, env, *scheduler, &actionsTaken, eligibleStarts)\n+\tresult = result.Append(sres)\n+\n+\t// Write results.\n+\treturn env.Access(ctx, ref, hsm.AccessWrite, func(node *hsm.Node) error {\n+\t\t// Record completed executions on the Invoker.\n+\t\terr := hsm.MachineTransition(node, func(i Invoker) (hsm.TransitionOutput, error) {\n+\t\t\treturn TransitionRecordExecution.Apply(i, EventRecordExecution{\n+\t\t\t\tNode:          node,\n+\t\t\t\texecuteResult: result,\n+\t\t\t})\n+\t\t})\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\n+\t\t// Record action results on the Scheduler.\n+\t\treturn hsm.MachineTransition(node.Parent, func(s Scheduler) (hsm.TransitionOutput, error) {\n+\t\t\treturn TransitionRecordAction.Apply(s, EventRecordAction{\n+\t\t\t\tNode:        node,\n+\t\t\t\tActionCount: int64(len(startResults)),\n+\t\t\t\tResults:     startResults,\n+\t\t\t})\n+\t\t})\n+\t})\n+}\n+\n+// shouldYield returns true when the immediate task should complete/spawn a new task.\n+func (e invokerTaskExecutor) shouldYield(scheduler Scheduler, actionsTaken int) bool {\n+\ttweakables := e.Config.Tweakables(scheduler.Namespace)",
        "comment_created_at": "2025-03-28T04:26:03+00:00",
        "comment_author": "dnr",
        "comment_body": "It'll be less expensive after https://github.com/temporalio/temporal/pull/7052 that caches conversions",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2013546204",
    "pr_number": 7152,
    "pr_file": "components/scheduler/invoker_executors.go",
    "created_at": "2025-03-26T07:39:54+00:00",
    "commented_code": "+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\t\"go.temporal.io/api/serviceerror\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tInvokerTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\tinvokerTaskExecutor struct {\n+\t\tInvokerTaskExecutorOptions\n+\t}\n+\n+\trateLimitedError struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tdelay time.Duration\n+\t}\n+)\n+\n+const (\n+\t// Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMinDeadline = 5 * time.Second\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = 1 * time.Hour\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tInvokerMaxStartAttempts = 10 // TODO - dial this up/remove it\n+)\n+\n+var (\n+\terrRetryLimitExceeded       = errors.New(\"retry limit exceeded\")\n+\t_                     error = &rateLimitedError{}\n+)\n+\n+func RegisterInvokerExecutors(registry *hsm.Registry, options InvokerTaskExecutorOptions) error {\n+\te := invokerTaskExecutor{\n+\t\tInvokerTaskExecutorOptions: options,\n+\t}\n+\n+\terr := hsm.RegisterTimerExecutor(registry, e.executeProcessBufferTask)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn hsm.RegisterImmediateExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e invokerTaskExecutor) executeExecuteTask(\n+\tctx context.Context,\n+\tenv hsm.Environment,\n+\tref hsm.Ref,\n+\ttask ExecuteTask,\n+) error {\n+\tvar scheduler *Scheduler\n+\tvar invoker *Invoker\n+\tvar result executeResult\n+\n+\t// Load Scheduler and Invoker's current states.\n+\terr := env.Access(ctx, ref, hsm.AccessRead, func(node *hsm.Node) error {\n+\t\ts, err := loadScheduler(node.Parent)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tscheduler = &s\n+\n+\t\ti, err := e.loadInvoker(node)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tinvoker = &i\n+\n+\t\treturn nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, *scheduler)\n+\n+\t// If we have nothing to do, we can return without any additional writes.\n+\teligibleStarts := invoker.getEligibleBufferedStarts()\n+\tif len(invoker.GetTerminateWorkflows())+\n+\t\tlen(invoker.GetCancelWorkflows())+\n+\t\tlen(eligibleStarts) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\t// Terminate, cancel, and start workflows. The result struct contains the\n+\t// complete outcome of all requests executed in a single batch.\n+\tactionsTaken := 0\n+\tresult = result.Append(e.terminateWorkflows(ctx, logger, *scheduler, &actionsTaken, invoker.GetTerminateWorkflows()))\n+\tresult = result.Append(e.cancelWorkflows(ctx, logger, *scheduler, &actionsTaken, invoker.GetCancelWorkflows()))\n+\tsres, startResults := e.startWorkflows(ctx, logger, env, *scheduler, &actionsTaken, eligibleStarts)\n+\tresult = result.Append(sres)\n+\n+\t// Write results.\n+\treturn env.Access(ctx, ref, hsm.AccessWrite, func(node *hsm.Node) error {\n+\t\t// Record completed executions on the Invoker.\n+\t\terr := hsm.MachineTransition(node, func(i Invoker) (hsm.TransitionOutput, error) {\n+\t\t\treturn TransitionRecordExecution.Apply(i, EventRecordExecution{\n+\t\t\t\tNode:          node,\n+\t\t\t\texecuteResult: result,\n+\t\t\t})\n+\t\t})\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\n+\t\t// Record action results on the Scheduler.\n+\t\treturn hsm.MachineTransition(node.Parent, func(s Scheduler) (hsm.TransitionOutput, error) {\n+\t\t\treturn TransitionRecordAction.Apply(s, EventRecordAction{\n+\t\t\t\tNode:        node,\n+\t\t\t\tActionCount: int64(len(startResults)),\n+\t\t\t\tResults:     startResults,\n+\t\t\t})\n+\t\t})\n+\t})\n+}\n+\n+// shouldYield returns true when the immediate task should complete/spawn a new task.\n+func (e invokerTaskExecutor) shouldYield(scheduler Scheduler, actionsTaken int) bool {\n+\ttweakables := e.Config.Tweakables(scheduler.Namespace)\n+\tmaxActions := tweakables.MaxActionsPerExecution\n+\treturn actionsTaken >= maxActions\n+}\n+\n+// cancelWorkflows does a best-effort attempt to cancel all workflow executions provided in targets.\n+func (e invokerTaskExecutor) cancelWorkflows(\n+\tctx context.Context,\n+\tlogger log.Logger,\n+\tscheduler Scheduler,\n+\tactionsTaken *int,\n+\ttargets []*commonpb.WorkflowExecution,\n+) (result executeResult) {\n+\tfor _, wf := range targets {\n+\t\tif e.shouldYield(scheduler, *actionsTaken) {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\terr := e.cancelWorkflow(ctx, scheduler, wf)\n+\t\t*actionsTaken++\n+\t\tif err != nil {\n+\t\t\tlogger.Error(\"Failed to cancel workflow\", tag.Error(err), tag.WorkflowID(wf.WorkflowId))\n+\t\t\te.MetricsHandler.Counter(metrics.ScheduleCancelWorkflowErrors.Name()).Record(1)\n+\t\t}\n+\n+\t\t// Cancels are only attempted once.\n+\t\tresult.CompletedCancels = append(result.CompletedCancels, wf)\n+\t}\n+\treturn\n+}\n+\n+// terminateWorkflows does a best-effort attempt to cancel all workflow executions provided in targets.\n+func (e invokerTaskExecutor) terminateWorkflows(\n+\tctx context.Context,\n+\tlogger log.Logger,\n+\tscheduler Scheduler,\n+\tactionsTaken *int,\n+\ttargets []*commonpb.WorkflowExecution,\n+) (result executeResult) {\n+\tfor _, wf := range targets {\n+\t\tif e.shouldYield(scheduler, *actionsTaken) {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\terr := e.terminateWorkflow(ctx, scheduler, wf)\n+\t\t*actionsTaken++\n+\t\tif err != nil {\n+\t\t\tlogger.Error(\"Failed to terminate workflow\", tag.Error(err), tag.WorkflowID(wf.WorkflowId))\n+\t\t\te.MetricsHandler.Counter(metrics.ScheduleTerminateWorkflowErrors.Name()).Record(1)\n+\t\t}\n+\n+\t\t// Terminates are only attempted once.\n+\t\tresult.CompletedTerminates = append(result.CompletedTerminates, wf)\n+\t}\n+\treturn\n+}\n+\n+// startWorkflows executes the provided list of starts, returning a result with their outcomes.\n+func (e invokerTaskExecutor) startWorkflows(\n+\tctx context.Context,\n+\tlogger log.Logger,\n+\tenv hsm.Environment,\n+\tscheduler Scheduler,\n+\tactionsTaken *int,\n+\tstarts []*schedulespb.BufferedStart,\n+) (result executeResult, startResults []*schedulepb.ScheduleActionResult) {\n+\tmetricsWithTag := e.MetricsHandler.WithTags(\n+\t\tmetrics.StringTag(metrics.ScheduleActionTypeTag, metrics.ScheduleActionStartWorkflow))\n+\n+\tfor _, start := range starts {\n+\t\t// Starts that haven't been executed yet will remain in `BufferedStarts`,\n+\t\t// without change, so another ExecuteTask will be immediately created to continue\n+\t\t// processing in a new task.\n+\t\tif e.shouldYield(scheduler, *actionsTaken) {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\tstartResult, err := e.startWorkflow(ctx, env, scheduler, start)\n+\t\t*actionsTaken++\n+\t\tif err != nil {\n+\t\t\tlogger.Error(\"Failed to start workflow\", tag.Error(err))\n+\n+\t\t\t// Don't count \"already started\" for the error metric or retry, as it is most likely\n+\t\t\t// due to misconfiguration.\n+\t\t\tif !isAlreadyStartedError(err) {\n+\t\t\t\tmetricsWithTag.Counter(metrics.ScheduleActionErrors.Name()).Record(1)\n+\t\t\t}\n+\n+\t\t\tif isRetryableError(err) {\n+\t\t\t\t// Apply backoff to start and retry.\n+\t\t\t\te.applyBackoff(env, start, err)\n+\t\t\t\tresult.RetryableStarts = append(result.RetryableStarts, start)\n+\t\t\t} else {\n+\t\t\t\t// Drop the start from the buffer.\n+\t\t\t\tresult.FailedStarts = append(result.FailedStarts, start)\n+\t\t\t}\n+\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tmetricsWithTag.Counter(metrics.ScheduleActionSuccess.Name()).Record(1)\n+\t\tresult.CompletedStarts = append(result.CompletedStarts, start)\n+\t\tstartResults = append(startResults, startResult)\n+\t}\n+\treturn\n+}\n+\n+func (e invokerTaskExecutor) executeProcessBufferTask(env hsm.Environment, node *hsm.Node, task ProcessBufferTask) error {\n+\tschedulerNode := node.Parent\n+\tscheduler, err := loadScheduler(schedulerNode)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\tinvoker, err := e.loadInvoker(node)",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2013546204",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7152,
        "pr_file": "components/scheduler/invoker_executors.go",
        "discussion_id": "2013546204",
        "commented_code": "@@ -0,0 +1,613 @@\n+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\t\"go.temporal.io/api/serviceerror\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tInvokerTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\tinvokerTaskExecutor struct {\n+\t\tInvokerTaskExecutorOptions\n+\t}\n+\n+\trateLimitedError struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tdelay time.Duration\n+\t}\n+)\n+\n+const (\n+\t// Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMinDeadline = 5 * time.Second\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = 1 * time.Hour\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tInvokerMaxStartAttempts = 10 // TODO - dial this up/remove it\n+)\n+\n+var (\n+\terrRetryLimitExceeded       = errors.New(\"retry limit exceeded\")\n+\t_                     error = &rateLimitedError{}\n+)\n+\n+func RegisterInvokerExecutors(registry *hsm.Registry, options InvokerTaskExecutorOptions) error {\n+\te := invokerTaskExecutor{\n+\t\tInvokerTaskExecutorOptions: options,\n+\t}\n+\n+\terr := hsm.RegisterTimerExecutor(registry, e.executeProcessBufferTask)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn hsm.RegisterImmediateExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e invokerTaskExecutor) executeExecuteTask(\n+\tctx context.Context,\n+\tenv hsm.Environment,\n+\tref hsm.Ref,\n+\ttask ExecuteTask,\n+) error {\n+\tvar scheduler *Scheduler\n+\tvar invoker *Invoker\n+\tvar result executeResult\n+\n+\t// Load Scheduler and Invoker's current states.\n+\terr := env.Access(ctx, ref, hsm.AccessRead, func(node *hsm.Node) error {\n+\t\ts, err := loadScheduler(node.Parent)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tscheduler = &s\n+\n+\t\ti, err := e.loadInvoker(node)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tinvoker = &i\n+\n+\t\treturn nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, *scheduler)\n+\n+\t// If we have nothing to do, we can return without any additional writes.\n+\teligibleStarts := invoker.getEligibleBufferedStarts()\n+\tif len(invoker.GetTerminateWorkflows())+\n+\t\tlen(invoker.GetCancelWorkflows())+\n+\t\tlen(eligibleStarts) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\t// Terminate, cancel, and start workflows. The result struct contains the\n+\t// complete outcome of all requests executed in a single batch.\n+\tactionsTaken := 0\n+\tresult = result.Append(e.terminateWorkflows(ctx, logger, *scheduler, &actionsTaken, invoker.GetTerminateWorkflows()))\n+\tresult = result.Append(e.cancelWorkflows(ctx, logger, *scheduler, &actionsTaken, invoker.GetCancelWorkflows()))\n+\tsres, startResults := e.startWorkflows(ctx, logger, env, *scheduler, &actionsTaken, eligibleStarts)\n+\tresult = result.Append(sres)\n+\n+\t// Write results.\n+\treturn env.Access(ctx, ref, hsm.AccessWrite, func(node *hsm.Node) error {\n+\t\t// Record completed executions on the Invoker.\n+\t\terr := hsm.MachineTransition(node, func(i Invoker) (hsm.TransitionOutput, error) {\n+\t\t\treturn TransitionRecordExecution.Apply(i, EventRecordExecution{\n+\t\t\t\tNode:          node,\n+\t\t\t\texecuteResult: result,\n+\t\t\t})\n+\t\t})\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\n+\t\t// Record action results on the Scheduler.\n+\t\treturn hsm.MachineTransition(node.Parent, func(s Scheduler) (hsm.TransitionOutput, error) {\n+\t\t\treturn TransitionRecordAction.Apply(s, EventRecordAction{\n+\t\t\t\tNode:        node,\n+\t\t\t\tActionCount: int64(len(startResults)),\n+\t\t\t\tResults:     startResults,\n+\t\t\t})\n+\t\t})\n+\t})\n+}\n+\n+// shouldYield returns true when the immediate task should complete/spawn a new task.\n+func (e invokerTaskExecutor) shouldYield(scheduler Scheduler, actionsTaken int) bool {\n+\ttweakables := e.Config.Tweakables(scheduler.Namespace)\n+\tmaxActions := tweakables.MaxActionsPerExecution\n+\treturn actionsTaken >= maxActions\n+}\n+\n+// cancelWorkflows does a best-effort attempt to cancel all workflow executions provided in targets.\n+func (e invokerTaskExecutor) cancelWorkflows(\n+\tctx context.Context,\n+\tlogger log.Logger,\n+\tscheduler Scheduler,\n+\tactionsTaken *int,\n+\ttargets []*commonpb.WorkflowExecution,\n+) (result executeResult) {\n+\tfor _, wf := range targets {\n+\t\tif e.shouldYield(scheduler, *actionsTaken) {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\terr := e.cancelWorkflow(ctx, scheduler, wf)\n+\t\t*actionsTaken++\n+\t\tif err != nil {\n+\t\t\tlogger.Error(\"Failed to cancel workflow\", tag.Error(err), tag.WorkflowID(wf.WorkflowId))\n+\t\t\te.MetricsHandler.Counter(metrics.ScheduleCancelWorkflowErrors.Name()).Record(1)\n+\t\t}\n+\n+\t\t// Cancels are only attempted once.\n+\t\tresult.CompletedCancels = append(result.CompletedCancels, wf)\n+\t}\n+\treturn\n+}\n+\n+// terminateWorkflows does a best-effort attempt to cancel all workflow executions provided in targets.\n+func (e invokerTaskExecutor) terminateWorkflows(\n+\tctx context.Context,\n+\tlogger log.Logger,\n+\tscheduler Scheduler,\n+\tactionsTaken *int,\n+\ttargets []*commonpb.WorkflowExecution,\n+) (result executeResult) {\n+\tfor _, wf := range targets {\n+\t\tif e.shouldYield(scheduler, *actionsTaken) {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\terr := e.terminateWorkflow(ctx, scheduler, wf)\n+\t\t*actionsTaken++\n+\t\tif err != nil {\n+\t\t\tlogger.Error(\"Failed to terminate workflow\", tag.Error(err), tag.WorkflowID(wf.WorkflowId))\n+\t\t\te.MetricsHandler.Counter(metrics.ScheduleTerminateWorkflowErrors.Name()).Record(1)\n+\t\t}\n+\n+\t\t// Terminates are only attempted once.\n+\t\tresult.CompletedTerminates = append(result.CompletedTerminates, wf)\n+\t}\n+\treturn\n+}\n+\n+// startWorkflows executes the provided list of starts, returning a result with their outcomes.\n+func (e invokerTaskExecutor) startWorkflows(\n+\tctx context.Context,\n+\tlogger log.Logger,\n+\tenv hsm.Environment,\n+\tscheduler Scheduler,\n+\tactionsTaken *int,\n+\tstarts []*schedulespb.BufferedStart,\n+) (result executeResult, startResults []*schedulepb.ScheduleActionResult) {\n+\tmetricsWithTag := e.MetricsHandler.WithTags(\n+\t\tmetrics.StringTag(metrics.ScheduleActionTypeTag, metrics.ScheduleActionStartWorkflow))\n+\n+\tfor _, start := range starts {\n+\t\t// Starts that haven't been executed yet will remain in `BufferedStarts`,\n+\t\t// without change, so another ExecuteTask will be immediately created to continue\n+\t\t// processing in a new task.\n+\t\tif e.shouldYield(scheduler, *actionsTaken) {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\tstartResult, err := e.startWorkflow(ctx, env, scheduler, start)\n+\t\t*actionsTaken++\n+\t\tif err != nil {\n+\t\t\tlogger.Error(\"Failed to start workflow\", tag.Error(err))\n+\n+\t\t\t// Don't count \"already started\" for the error metric or retry, as it is most likely\n+\t\t\t// due to misconfiguration.\n+\t\t\tif !isAlreadyStartedError(err) {\n+\t\t\t\tmetricsWithTag.Counter(metrics.ScheduleActionErrors.Name()).Record(1)\n+\t\t\t}\n+\n+\t\t\tif isRetryableError(err) {\n+\t\t\t\t// Apply backoff to start and retry.\n+\t\t\t\te.applyBackoff(env, start, err)\n+\t\t\t\tresult.RetryableStarts = append(result.RetryableStarts, start)\n+\t\t\t} else {\n+\t\t\t\t// Drop the start from the buffer.\n+\t\t\t\tresult.FailedStarts = append(result.FailedStarts, start)\n+\t\t\t}\n+\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tmetricsWithTag.Counter(metrics.ScheduleActionSuccess.Name()).Record(1)\n+\t\tresult.CompletedStarts = append(result.CompletedStarts, start)\n+\t\tstartResults = append(startResults, startResult)\n+\t}\n+\treturn\n+}\n+\n+func (e invokerTaskExecutor) executeProcessBufferTask(env hsm.Environment, node *hsm.Node, task ProcessBufferTask) error {\n+\tschedulerNode := node.Parent\n+\tscheduler, err := loadScheduler(schedulerNode)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\tinvoker, err := e.loadInvoker(node)",
        "comment_created_at": "2025-03-26T07:39:54+00:00",
        "comment_author": "yycptt",
        "comment_body": "it looks like this timer logic is executed with the Scheduler locked, so we don't really need the deep copy here.",
        "pr_file_module": null
      },
      {
        "comment_id": "2017599582",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7152,
        "pr_file": "components/scheduler/invoker_executors.go",
        "discussion_id": "2013546204",
        "commented_code": "@@ -0,0 +1,613 @@\n+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\t\"go.temporal.io/api/serviceerror\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tInvokerTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\tinvokerTaskExecutor struct {\n+\t\tInvokerTaskExecutorOptions\n+\t}\n+\n+\trateLimitedError struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tdelay time.Duration\n+\t}\n+)\n+\n+const (\n+\t// Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMinDeadline = 5 * time.Second\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = 1 * time.Hour\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tInvokerMaxStartAttempts = 10 // TODO - dial this up/remove it\n+)\n+\n+var (\n+\terrRetryLimitExceeded       = errors.New(\"retry limit exceeded\")\n+\t_                     error = &rateLimitedError{}\n+)\n+\n+func RegisterInvokerExecutors(registry *hsm.Registry, options InvokerTaskExecutorOptions) error {\n+\te := invokerTaskExecutor{\n+\t\tInvokerTaskExecutorOptions: options,\n+\t}\n+\n+\terr := hsm.RegisterTimerExecutor(registry, e.executeProcessBufferTask)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn hsm.RegisterImmediateExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e invokerTaskExecutor) executeExecuteTask(\n+\tctx context.Context,\n+\tenv hsm.Environment,\n+\tref hsm.Ref,\n+\ttask ExecuteTask,\n+) error {\n+\tvar scheduler *Scheduler\n+\tvar invoker *Invoker\n+\tvar result executeResult\n+\n+\t// Load Scheduler and Invoker's current states.\n+\terr := env.Access(ctx, ref, hsm.AccessRead, func(node *hsm.Node) error {\n+\t\ts, err := loadScheduler(node.Parent)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tscheduler = &s\n+\n+\t\ti, err := e.loadInvoker(node)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tinvoker = &i\n+\n+\t\treturn nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, *scheduler)\n+\n+\t// If we have nothing to do, we can return without any additional writes.\n+\teligibleStarts := invoker.getEligibleBufferedStarts()\n+\tif len(invoker.GetTerminateWorkflows())+\n+\t\tlen(invoker.GetCancelWorkflows())+\n+\t\tlen(eligibleStarts) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\t// Terminate, cancel, and start workflows. The result struct contains the\n+\t// complete outcome of all requests executed in a single batch.\n+\tactionsTaken := 0\n+\tresult = result.Append(e.terminateWorkflows(ctx, logger, *scheduler, &actionsTaken, invoker.GetTerminateWorkflows()))\n+\tresult = result.Append(e.cancelWorkflows(ctx, logger, *scheduler, &actionsTaken, invoker.GetCancelWorkflows()))\n+\tsres, startResults := e.startWorkflows(ctx, logger, env, *scheduler, &actionsTaken, eligibleStarts)\n+\tresult = result.Append(sres)\n+\n+\t// Write results.\n+\treturn env.Access(ctx, ref, hsm.AccessWrite, func(node *hsm.Node) error {\n+\t\t// Record completed executions on the Invoker.\n+\t\terr := hsm.MachineTransition(node, func(i Invoker) (hsm.TransitionOutput, error) {\n+\t\t\treturn TransitionRecordExecution.Apply(i, EventRecordExecution{\n+\t\t\t\tNode:          node,\n+\t\t\t\texecuteResult: result,\n+\t\t\t})\n+\t\t})\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\n+\t\t// Record action results on the Scheduler.\n+\t\treturn hsm.MachineTransition(node.Parent, func(s Scheduler) (hsm.TransitionOutput, error) {\n+\t\t\treturn TransitionRecordAction.Apply(s, EventRecordAction{\n+\t\t\t\tNode:        node,\n+\t\t\t\tActionCount: int64(len(startResults)),\n+\t\t\t\tResults:     startResults,\n+\t\t\t})\n+\t\t})\n+\t})\n+}\n+\n+// shouldYield returns true when the immediate task should complete/spawn a new task.\n+func (e invokerTaskExecutor) shouldYield(scheduler Scheduler, actionsTaken int) bool {\n+\ttweakables := e.Config.Tweakables(scheduler.Namespace)\n+\tmaxActions := tweakables.MaxActionsPerExecution\n+\treturn actionsTaken >= maxActions\n+}\n+\n+// cancelWorkflows does a best-effort attempt to cancel all workflow executions provided in targets.\n+func (e invokerTaskExecutor) cancelWorkflows(\n+\tctx context.Context,\n+\tlogger log.Logger,\n+\tscheduler Scheduler,\n+\tactionsTaken *int,\n+\ttargets []*commonpb.WorkflowExecution,\n+) (result executeResult) {\n+\tfor _, wf := range targets {\n+\t\tif e.shouldYield(scheduler, *actionsTaken) {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\terr := e.cancelWorkflow(ctx, scheduler, wf)\n+\t\t*actionsTaken++\n+\t\tif err != nil {\n+\t\t\tlogger.Error(\"Failed to cancel workflow\", tag.Error(err), tag.WorkflowID(wf.WorkflowId))\n+\t\t\te.MetricsHandler.Counter(metrics.ScheduleCancelWorkflowErrors.Name()).Record(1)\n+\t\t}\n+\n+\t\t// Cancels are only attempted once.\n+\t\tresult.CompletedCancels = append(result.CompletedCancels, wf)\n+\t}\n+\treturn\n+}\n+\n+// terminateWorkflows does a best-effort attempt to cancel all workflow executions provided in targets.\n+func (e invokerTaskExecutor) terminateWorkflows(\n+\tctx context.Context,\n+\tlogger log.Logger,\n+\tscheduler Scheduler,\n+\tactionsTaken *int,\n+\ttargets []*commonpb.WorkflowExecution,\n+) (result executeResult) {\n+\tfor _, wf := range targets {\n+\t\tif e.shouldYield(scheduler, *actionsTaken) {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\terr := e.terminateWorkflow(ctx, scheduler, wf)\n+\t\t*actionsTaken++\n+\t\tif err != nil {\n+\t\t\tlogger.Error(\"Failed to terminate workflow\", tag.Error(err), tag.WorkflowID(wf.WorkflowId))\n+\t\t\te.MetricsHandler.Counter(metrics.ScheduleTerminateWorkflowErrors.Name()).Record(1)\n+\t\t}\n+\n+\t\t// Terminates are only attempted once.\n+\t\tresult.CompletedTerminates = append(result.CompletedTerminates, wf)\n+\t}\n+\treturn\n+}\n+\n+// startWorkflows executes the provided list of starts, returning a result with their outcomes.\n+func (e invokerTaskExecutor) startWorkflows(\n+\tctx context.Context,\n+\tlogger log.Logger,\n+\tenv hsm.Environment,\n+\tscheduler Scheduler,\n+\tactionsTaken *int,\n+\tstarts []*schedulespb.BufferedStart,\n+) (result executeResult, startResults []*schedulepb.ScheduleActionResult) {\n+\tmetricsWithTag := e.MetricsHandler.WithTags(\n+\t\tmetrics.StringTag(metrics.ScheduleActionTypeTag, metrics.ScheduleActionStartWorkflow))\n+\n+\tfor _, start := range starts {\n+\t\t// Starts that haven't been executed yet will remain in `BufferedStarts`,\n+\t\t// without change, so another ExecuteTask will be immediately created to continue\n+\t\t// processing in a new task.\n+\t\tif e.shouldYield(scheduler, *actionsTaken) {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\tstartResult, err := e.startWorkflow(ctx, env, scheduler, start)\n+\t\t*actionsTaken++\n+\t\tif err != nil {\n+\t\t\tlogger.Error(\"Failed to start workflow\", tag.Error(err))\n+\n+\t\t\t// Don't count \"already started\" for the error metric or retry, as it is most likely\n+\t\t\t// due to misconfiguration.\n+\t\t\tif !isAlreadyStartedError(err) {\n+\t\t\t\tmetricsWithTag.Counter(metrics.ScheduleActionErrors.Name()).Record(1)\n+\t\t\t}\n+\n+\t\t\tif isRetryableError(err) {\n+\t\t\t\t// Apply backoff to start and retry.\n+\t\t\t\te.applyBackoff(env, start, err)\n+\t\t\t\tresult.RetryableStarts = append(result.RetryableStarts, start)\n+\t\t\t} else {\n+\t\t\t\t// Drop the start from the buffer.\n+\t\t\t\tresult.FailedStarts = append(result.FailedStarts, start)\n+\t\t\t}\n+\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tmetricsWithTag.Counter(metrics.ScheduleActionSuccess.Name()).Record(1)\n+\t\tresult.CompletedStarts = append(result.CompletedStarts, start)\n+\t\tstartResults = append(startResults, startResult)\n+\t}\n+\treturn\n+}\n+\n+func (e invokerTaskExecutor) executeProcessBufferTask(env hsm.Environment, node *hsm.Node, task ProcessBufferTask) error {\n+\tschedulerNode := node.Parent\n+\tscheduler, err := loadScheduler(schedulerNode)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\tinvoker, err := e.loadInvoker(node)",
        "comment_created_at": "2025-03-27T21:04:27+00:00",
        "comment_author": "lina-temporal",
        "comment_body": "Yeah, true. I'll update `loadInvoker` to take a `shouldClone` parameter since we also use it in the immediate task.",
        "pr_file_module": null
      }
    ]
  }
]