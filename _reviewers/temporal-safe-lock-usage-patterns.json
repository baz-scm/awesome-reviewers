[
  {
    "discussion_id": "2198430839",
    "pr_number": 8027,
    "pr_file": "service/matching/physical_task_queue_manager.go",
    "created_at": "2025-07-10T18:38:45+00:00",
    "commented_code": "pollerHistory               *pollerHistory\n \t\tcurrentPolls                atomic.Int64\n \t\ttaskValidator               taskValidator\n-\t\ttasksAddedInIntervals       *taskTracker\n-\t\ttasksDispatchedInIntervals  *taskTracker\n \t\tdeploymentRegistrationCh    chan struct{}\n \t\tdeploymentVersionRegistered bool\n \t\tpollerScalingRateLimiter    quotas.RateLimiter\n \n-\t\tfirstPoll time.Time\n+\t\ttaskTrackerMu              sync.RWMutex",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2198430839",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 8027,
        "pr_file": "service/matching/physical_task_queue_manager.go",
        "discussion_id": "2198430839",
        "commented_code": "@@ -82,13 +83,13 @@ type (\n \t\tpollerHistory               *pollerHistory\n \t\tcurrentPolls                atomic.Int64\n \t\ttaskValidator               taskValidator\n-\t\ttasksAddedInIntervals       *taskTracker\n-\t\ttasksDispatchedInIntervals  *taskTracker\n \t\tdeploymentRegistrationCh    chan struct{}\n \t\tdeploymentVersionRegistered bool\n \t\tpollerScalingRateLimiter    quotas.RateLimiter\n \n-\t\tfirstPoll time.Time\n+\t\ttaskTrackerMu              sync.RWMutex",
        "comment_created_at": "2025-07-10T18:38:45+00:00",
        "comment_author": "dnr",
        "comment_body": "we usually use \"lock\"\n```suggestion\n\t\ttaskTrackerLock            sync.RWMutex\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2132949725",
    "pr_number": 7877,
    "pr_file": "service/history/replication/stream_sender.go",
    "created_at": "2025-06-06T21:52:28+00:00",
    "commented_code": "return nil\n }\n \n+func (s *StreamSenderImpl) recvMonitor() {\n+\theartbeatTimeout := time.NewTimer(s.config.ReplicationStreamSyncStatusDuration() * SyncTaskIntervalMultiplier)\n+\tdefer heartbeatTimeout.Stop()\n+\n+\tfor !s.shutdownChan.IsShutdown() {\n+\t\tselect {\n+\t\tcase <-s.recvSignalChan:\n+\t\t\theartbeatTimeout.Stop()",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "2132949725",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7877,
        "pr_file": "service/history/replication/stream_sender.go",
        "discussion_id": "2132949725",
        "commented_code": "@@ -195,6 +206,23 @@ func (s *StreamSenderImpl) recvEventLoop() (retErr error) {\n \treturn nil\n }\n \n+func (s *StreamSenderImpl) recvMonitor() {\n+\theartbeatTimeout := time.NewTimer(s.config.ReplicationStreamSyncStatusDuration() * SyncTaskIntervalMultiplier)\n+\tdefer heartbeatTimeout.Stop()\n+\n+\tfor !s.shutdownChan.IsShutdown() {\n+\t\tselect {\n+\t\tcase <-s.recvSignalChan:\n+\t\t\theartbeatTimeout.Stop()",
        "comment_created_at": "2025-06-06T21:52:28+00:00",
        "comment_author": "xwduan",
        "comment_body": "If the timer is already fired, it may cause the go routine leak. Maybe do this:\r\n```\r\nif !timer.Stop() {\r\n\tselect {\r\n\tcase <-timer.C: // drain the channel if it fired\r\n\tdefault:\r\n\t}\r\n}\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2136071600",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7877,
        "pr_file": "service/history/replication/stream_sender.go",
        "discussion_id": "2132949725",
        "commented_code": "@@ -195,6 +206,23 @@ func (s *StreamSenderImpl) recvEventLoop() (retErr error) {\n \treturn nil\n }\n \n+func (s *StreamSenderImpl) recvMonitor() {\n+\theartbeatTimeout := time.NewTimer(s.config.ReplicationStreamSyncStatusDuration() * SyncTaskIntervalMultiplier)\n+\tdefer heartbeatTimeout.Stop()\n+\n+\tfor !s.shutdownChan.IsShutdown() {\n+\t\tselect {\n+\t\tcase <-s.recvSignalChan:\n+\t\t\theartbeatTimeout.Stop()",
        "comment_created_at": "2025-06-09T16:49:50+00:00",
        "comment_author": "yux0",
        "comment_body": "Updated in both places.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1927969896",
    "pr_number": 7152,
    "pr_file": "components/scheduler/executor_executors.go",
    "created_at": "2025-01-24T01:41:55+00:00",
    "commented_code": "+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\tworkflowpb \"go.temporal.io/api/workflow/v1\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/sdk/temporal\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tExecutorTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\texecutorTaskExecutor struct {\n+\t\tExecutorTaskExecutorOptions\n+\t}\n+\n+\trateLimitedDetails struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tDelay time.Duration\n+\t}\n+)\n+\n+const (\n+\tstartWorkflowMinDeadline = 5 * time.Second // Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMaxDeadline = 1 * time.Hour   // Upper bound for the deadline in which buffered actions are dropped.\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = startWorkflowMaxDeadline\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tExecutorMaxStartAttempts = 10\n+\n+\terrTypeRetryLimitExceeded = \"RetryLimitExceeded\"\n+\terrTypeRateLimited        = \"RateLimited\"\n+\terrTypeAlreadyStarted     = \"serviceerror.WorkflowExecutionAlreadyStarted\"\n+)\n+\n+func RegisterExecutorExecutors(registry *hsm.Registry, options ExecutorTaskExecutorOptions) error {\n+\te := executorTaskExecutor{\n+\t\tExecutorTaskExecutorOptions: options,\n+\t}\n+\treturn hsm.RegisterTimerExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e executorTaskExecutor) executeExecuteTask(env hsm.Environment, node *hsm.Node, task ExecuteTask) error {\n+\tschedulerNode := node.Parent\n+\tscheduler, err := loadScheduler(schedulerNode)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, scheduler)\n+\n+\texecutor, err := e.loadExecutor(node)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Make sure we have something to start. If not, we can clear the buffer and\n+\t// transition to the waiting state.\n+\texecutionInfo := scheduler.Schedule.Action.GetStartWorkflow()\n+\tif executionInfo == nil || len(executor.BufferedStarts) == 0 {\n+\t\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\t\te.ExecutorInternal.BufferedStarts = nil\n+\t\t\treturn TransitionWait.Apply(e, EventWait{\n+\t\t\t\tNode: node,\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t// Drain the Executor's BufferedStarts. drainBuffer will update Executor's\n+\t// BufferedStarts, as well as Scheduler metadata.\n+\te.drainBuffer(logger, env, executor, scheduler, executionInfo)\n+\n+\t// Update SchedulerInfo metadata.\n+\terr = hsm.MachineTransition(schedulerNode, func(s Scheduler) (hsm.TransitionOutput, error) {\n+\t\ts.Info = scheduler.Info\n+\t\treturn hsm.TransitionOutput{}, nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Find the earliest possible time a BufferedStart can be retried, if any, based\n+\t// on BackoffTime.\n+\tvar retryAt time.Time\n+\tfor _, start := range executor.BufferedStarts {\n+\t\tbackoffTime := start.BackoffTime.AsTime()\n+\n+\t\t// Zero-value protobuf timestamps deserialize to the UNIX epoch, skip if the\n+\t\t// backoffTime looks invalid/unset.\n+\t\tif backoffTime.Before(start.ActualTime.AsTime()) {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tif retryAt.IsZero() || backoffTime.Before(retryAt) {\n+\t\t\tretryAt = backoffTime\n+\t\t}\n+\t}\n+\n+\tif !retryAt.IsZero() {\n+\t\t// When retryAt is set, reschedule.\n+\t\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\t\te.ExecutorInternal.BufferedStarts = executor.BufferedStarts\n+\t\t\treturn TransitionExecute.Apply(e, EventExecute{\n+\t\t\t\tNode:     node,\n+\t\t\t\tDeadline: retryAt,\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t// No more buffered starts, or remaining starts are waiting for a workflow to be\n+\t// closed. We can transition to waiting.\n+\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\te.ExecutorInternal.BufferedStarts = executor.BufferedStarts\n+\t\treturn TransitionWait.Apply(e, EventWait{\n+\t\t\tNode: node,\n+\t\t})\n+\t})\n+}\n+\n+// drainBuffer uses ProcessBuffer to resolve the Executor's remaining buffered\n+// starts, and then carries out the returned action.\n+func (e executorTaskExecutor) drainBuffer(\n+\tlogger log.Logger,\n+\tenv hsm.Environment,\n+\texecutor Executor,\n+\tscheduler Scheduler,\n+\trequest *workflowpb.NewWorkflowExecutionInfo,\n+) {\n+\tmetricsWithTag := e.MetricsHandler.WithTags(\n+\t\tmetrics.StringTag(metrics.ScheduleActionTypeTag, metrics.ScheduleActionStartWorkflow))\n+\tisRunning := len(scheduler.Info.RunningWorkflows) > 0\n+\n+\t// Resolve overlap policies and prepare next workflows to start.\n+\taction := scheduler1.ProcessBuffer(executor.BufferedStarts, isRunning, scheduler.resolveOverlapPolicy)\n+\tnewBuffer := action.NewBuffer\n+\n+\t// Combine all available starts.\n+\tallStarts := action.OverlappingStarts\n+\tif action.NonOverlappingStart != nil {\n+\t\tallStarts = append(allStarts, action.NonOverlappingStart)\n+\t}\n+\n+\t// Start workflows.\n+\tfor _, start := range allStarts {\n+\t\t// Ensure we can take more actions. Manual actions are always allowed.\n+\t\tif !start.Manual && !scheduler.useScheduledAction(true) {\n+\t\t\t// Drop buffered automated actions while paused.\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tif env.Now().Before(start.BackoffTime.AsTime()) {\n+\t\t\t// BufferedStart is still backing off, push it back into the buffer.\n+\t\t\tnewBuffer = append(newBuffer, start)\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tif env.Now().After(e.startWorkflowDeadline(scheduler, start)) {\n+\t\t\t// Drop expired starts.\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tresult, err := e.startWorkflow(env, scheduler, start, request)",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "1927969896",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7152,
        "pr_file": "components/scheduler/executor_executors.go",
        "discussion_id": "1927969896",
        "commented_code": "@@ -0,0 +1,493 @@\n+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\tworkflowpb \"go.temporal.io/api/workflow/v1\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/sdk/temporal\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tExecutorTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\texecutorTaskExecutor struct {\n+\t\tExecutorTaskExecutorOptions\n+\t}\n+\n+\trateLimitedDetails struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tDelay time.Duration\n+\t}\n+)\n+\n+const (\n+\tstartWorkflowMinDeadline = 5 * time.Second // Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMaxDeadline = 1 * time.Hour   // Upper bound for the deadline in which buffered actions are dropped.\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = startWorkflowMaxDeadline\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tExecutorMaxStartAttempts = 10\n+\n+\terrTypeRetryLimitExceeded = \"RetryLimitExceeded\"\n+\terrTypeRateLimited        = \"RateLimited\"\n+\terrTypeAlreadyStarted     = \"serviceerror.WorkflowExecutionAlreadyStarted\"\n+)\n+\n+func RegisterExecutorExecutors(registry *hsm.Registry, options ExecutorTaskExecutorOptions) error {\n+\te := executorTaskExecutor{\n+\t\tExecutorTaskExecutorOptions: options,\n+\t}\n+\treturn hsm.RegisterTimerExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e executorTaskExecutor) executeExecuteTask(env hsm.Environment, node *hsm.Node, task ExecuteTask) error {\n+\tschedulerNode := node.Parent\n+\tscheduler, err := loadScheduler(schedulerNode)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, scheduler)\n+\n+\texecutor, err := e.loadExecutor(node)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Make sure we have something to start. If not, we can clear the buffer and\n+\t// transition to the waiting state.\n+\texecutionInfo := scheduler.Schedule.Action.GetStartWorkflow()\n+\tif executionInfo == nil || len(executor.BufferedStarts) == 0 {\n+\t\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\t\te.ExecutorInternal.BufferedStarts = nil\n+\t\t\treturn TransitionWait.Apply(e, EventWait{\n+\t\t\t\tNode: node,\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t// Drain the Executor's BufferedStarts. drainBuffer will update Executor's\n+\t// BufferedStarts, as well as Scheduler metadata.\n+\te.drainBuffer(logger, env, executor, scheduler, executionInfo)\n+\n+\t// Update SchedulerInfo metadata.\n+\terr = hsm.MachineTransition(schedulerNode, func(s Scheduler) (hsm.TransitionOutput, error) {\n+\t\ts.Info = scheduler.Info\n+\t\treturn hsm.TransitionOutput{}, nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Find the earliest possible time a BufferedStart can be retried, if any, based\n+\t// on BackoffTime.\n+\tvar retryAt time.Time\n+\tfor _, start := range executor.BufferedStarts {\n+\t\tbackoffTime := start.BackoffTime.AsTime()\n+\n+\t\t// Zero-value protobuf timestamps deserialize to the UNIX epoch, skip if the\n+\t\t// backoffTime looks invalid/unset.\n+\t\tif backoffTime.Before(start.ActualTime.AsTime()) {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tif retryAt.IsZero() || backoffTime.Before(retryAt) {\n+\t\t\tretryAt = backoffTime\n+\t\t}\n+\t}\n+\n+\tif !retryAt.IsZero() {\n+\t\t// When retryAt is set, reschedule.\n+\t\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\t\te.ExecutorInternal.BufferedStarts = executor.BufferedStarts\n+\t\t\treturn TransitionExecute.Apply(e, EventExecute{\n+\t\t\t\tNode:     node,\n+\t\t\t\tDeadline: retryAt,\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t// No more buffered starts, or remaining starts are waiting for a workflow to be\n+\t// closed. We can transition to waiting.\n+\treturn hsm.MachineTransition(node, func(e Executor) (hsm.TransitionOutput, error) {\n+\t\te.ExecutorInternal.BufferedStarts = executor.BufferedStarts\n+\t\treturn TransitionWait.Apply(e, EventWait{\n+\t\t\tNode: node,\n+\t\t})\n+\t})\n+}\n+\n+// drainBuffer uses ProcessBuffer to resolve the Executor's remaining buffered\n+// starts, and then carries out the returned action.\n+func (e executorTaskExecutor) drainBuffer(\n+\tlogger log.Logger,\n+\tenv hsm.Environment,\n+\texecutor Executor,\n+\tscheduler Scheduler,\n+\trequest *workflowpb.NewWorkflowExecutionInfo,\n+) {\n+\tmetricsWithTag := e.MetricsHandler.WithTags(\n+\t\tmetrics.StringTag(metrics.ScheduleActionTypeTag, metrics.ScheduleActionStartWorkflow))\n+\tisRunning := len(scheduler.Info.RunningWorkflows) > 0\n+\n+\t// Resolve overlap policies and prepare next workflows to start.\n+\taction := scheduler1.ProcessBuffer(executor.BufferedStarts, isRunning, scheduler.resolveOverlapPolicy)\n+\tnewBuffer := action.NewBuffer\n+\n+\t// Combine all available starts.\n+\tallStarts := action.OverlappingStarts\n+\tif action.NonOverlappingStart != nil {\n+\t\tallStarts = append(allStarts, action.NonOverlappingStart)\n+\t}\n+\n+\t// Start workflows.\n+\tfor _, start := range allStarts {\n+\t\t// Ensure we can take more actions. Manual actions are always allowed.\n+\t\tif !start.Manual && !scheduler.useScheduledAction(true) {\n+\t\t\t// Drop buffered automated actions while paused.\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tif env.Now().Before(start.BackoffTime.AsTime()) {\n+\t\t\t// BufferedStart is still backing off, push it back into the buffer.\n+\t\t\tnewBuffer = append(newBuffer, start)\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tif env.Now().After(e.startWorkflowDeadline(scheduler, start)) {\n+\t\t\t// Drop expired starts.\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tresult, err := e.startWorkflow(env, scheduler, start, request)",
        "comment_created_at": "2025-01-24T01:41:55+00:00",
        "comment_author": "bergundy",
        "comment_body": "You shouldn't do any IO while holding the lock, you should do this in an immediate task, not a timer task (since the HSM framework can only execute timer tasks under a lock).",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1960226363",
    "pr_number": 7152,
    "pr_file": "components/scheduler/invoker_executors.go",
    "created_at": "2025-02-18T17:47:14+00:00",
    "commented_code": "+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\t\"go.temporal.io/api/serviceerror\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/headers\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tInvokerTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\tinvokerTaskExecutor struct {\n+\t\tInvokerTaskExecutorOptions\n+\t}\n+\n+\trateLimitedError struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tdelay time.Duration\n+\t}\n+)\n+\n+const (\n+\t// Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMinDeadline = 5 * time.Second\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = 1 * time.Hour\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tExecutorMaxStartAttempts = 10 // TODO - dial this up/remove it\n+)\n+\n+var (\n+\terrRetryLimitExceeded       = errors.New(\"retry limit exceeded\")\n+\t_                     error = &rateLimitedError{}\n+)\n+\n+func RegisterInvokerExecutors(registry *hsm.Registry, options InvokerTaskExecutorOptions) error {\n+\te := invokerTaskExecutor{\n+\t\tInvokerTaskExecutorOptions: options,\n+\t}\n+\n+\terr := hsm.RegisterTimerExecutor(registry, e.executeProcessBufferTask)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn hsm.RegisterImmediateExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e invokerTaskExecutor) executeExecuteTask(\n+\tctx context.Context,\n+\tenv hsm.Environment,\n+\tref hsm.Ref,\n+\ttask ExecuteTask,\n+) error {\n+\tvar scheduler *Scheduler\n+\tvar invoker *Invoker\n+\tvar result executeResult\n+\n+\t// Load Scheduler and Invoker's current states.\n+\terr := env.Access(ctx, ref, hsm.AccessRead, func(node *hsm.Node) error {\n+\t\ts, err := loadScheduler(node.Parent)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tscheduler = &s\n+\n+\t\ti, err := e.loadInvoker(node)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tinvoker = &i\n+\n+\t\treturn nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, *scheduler)\n+\n+\t// If we have nothing to do, we can return without any additional writes.\n+\teligibleStarts := invoker.getEligibleBufferedStarts()\n+\tif len(invoker.GetTerminateWorkflows())+\n+\t\tlen(invoker.GetCancelWorkflows())+\n+\t\tlen(eligibleStarts) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\t// Terminate, cancel, and start workflows. The result struct contains the\n+\t// complete outcome of all requests executed in a single batch.\n+\tresult = result.Append(e.terminateWorkflows(logger, *scheduler, invoker.GetTerminateWorkflows()))\n+\tresult = result.Append(e.cancelWorkflows(logger, *scheduler, invoker.GetCancelWorkflows()))\n+\tsres, startResults := e.startWorkflows(logger, env, *scheduler, eligibleStarts)",
    "repo_full_name": "temporalio/temporal",
    "discussion_comments": [
      {
        "comment_id": "1960226363",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7152,
        "pr_file": "components/scheduler/invoker_executors.go",
        "discussion_id": "1960226363",
        "commented_code": "@@ -0,0 +1,582 @@\n+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\t\"go.temporal.io/api/serviceerror\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/headers\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tInvokerTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\tinvokerTaskExecutor struct {\n+\t\tInvokerTaskExecutorOptions\n+\t}\n+\n+\trateLimitedError struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tdelay time.Duration\n+\t}\n+)\n+\n+const (\n+\t// Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMinDeadline = 5 * time.Second\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = 1 * time.Hour\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tExecutorMaxStartAttempts = 10 // TODO - dial this up/remove it\n+)\n+\n+var (\n+\terrRetryLimitExceeded       = errors.New(\"retry limit exceeded\")\n+\t_                     error = &rateLimitedError{}\n+)\n+\n+func RegisterInvokerExecutors(registry *hsm.Registry, options InvokerTaskExecutorOptions) error {\n+\te := invokerTaskExecutor{\n+\t\tInvokerTaskExecutorOptions: options,\n+\t}\n+\n+\terr := hsm.RegisterTimerExecutor(registry, e.executeProcessBufferTask)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn hsm.RegisterImmediateExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e invokerTaskExecutor) executeExecuteTask(\n+\tctx context.Context,\n+\tenv hsm.Environment,\n+\tref hsm.Ref,\n+\ttask ExecuteTask,\n+) error {\n+\tvar scheduler *Scheduler\n+\tvar invoker *Invoker\n+\tvar result executeResult\n+\n+\t// Load Scheduler and Invoker's current states.\n+\terr := env.Access(ctx, ref, hsm.AccessRead, func(node *hsm.Node) error {\n+\t\ts, err := loadScheduler(node.Parent)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tscheduler = &s\n+\n+\t\ti, err := e.loadInvoker(node)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tinvoker = &i\n+\n+\t\treturn nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, *scheduler)\n+\n+\t// If we have nothing to do, we can return without any additional writes.\n+\teligibleStarts := invoker.getEligibleBufferedStarts()\n+\tif len(invoker.GetTerminateWorkflows())+\n+\t\tlen(invoker.GetCancelWorkflows())+\n+\t\tlen(eligibleStarts) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\t// Terminate, cancel, and start workflows. The result struct contains the\n+\t// complete outcome of all requests executed in a single batch.\n+\tresult = result.Append(e.terminateWorkflows(logger, *scheduler, invoker.GetTerminateWorkflows()))\n+\tresult = result.Append(e.cancelWorkflows(logger, *scheduler, invoker.GetCancelWorkflows()))\n+\tsres, startResults := e.startWorkflows(logger, env, *scheduler, eligibleStarts)",
        "comment_created_at": "2025-02-18T17:47:14+00:00",
        "comment_author": "bergundy",
        "comment_body": "You should propagate the context into these calls.",
        "pr_file_module": null
      },
      {
        "comment_id": "1960227517",
        "repo_full_name": "temporalio/temporal",
        "pr_number": 7152,
        "pr_file": "components/scheduler/invoker_executors.go",
        "discussion_id": "1960226363",
        "commented_code": "@@ -0,0 +1,582 @@\n+// The MIT License\n+//\n+// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n+//\n+// Copyright (c) 2020 Uber Technologies, Inc.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a copy\n+// of this software and associated documentation files (the \"Software\"), to deal\n+// in the Software without restriction, including without limitation the rights\n+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+// copies of the Software, and to permit persons to whom the Software is\n+// furnished to do so, subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included in\n+// all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n+// THE SOFTWARE.\n+\n+package scheduler\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\tcommonpb \"go.temporal.io/api/common/v1\"\n+\tenumspb \"go.temporal.io/api/enums/v1\"\n+\tschedulepb \"go.temporal.io/api/schedule/v1\"\n+\t\"go.temporal.io/api/serviceerror\"\n+\t\"go.temporal.io/api/workflowservice/v1\"\n+\t\"go.temporal.io/server/api/historyservice/v1\"\n+\tschedulespb \"go.temporal.io/server/api/schedule/v1\"\n+\t\"go.temporal.io/server/common\"\n+\t\"go.temporal.io/server/common/headers\"\n+\t\"go.temporal.io/server/common/log\"\n+\t\"go.temporal.io/server/common/log/tag\"\n+\t\"go.temporal.io/server/common/metrics\"\n+\t\"go.temporal.io/server/common/resource\"\n+\t\"go.temporal.io/server/common/util\"\n+\t\"go.temporal.io/server/service/history/hsm\"\n+\tscheduler1 \"go.temporal.io/server/service/worker/scheduler\"\n+\t\"go.uber.org/fx\"\n+\t\"google.golang.org/protobuf/types/known/timestamppb\"\n+)\n+\n+type (\n+\tInvokerTaskExecutorOptions struct {\n+\t\tfx.In\n+\n+\t\tConfig         *Config\n+\t\tMetricsHandler metrics.Handler\n+\t\tBaseLogger     log.Logger\n+\t\tHistoryClient  resource.HistoryClient\n+\t\tFrontendClient workflowservice.WorkflowServiceClient\n+\t}\n+\n+\tinvokerTaskExecutor struct {\n+\t\tInvokerTaskExecutorOptions\n+\t}\n+\n+\trateLimitedError struct {\n+\t\t// The requested interval to delay processing by rescheduilng.\n+\t\tdelay time.Duration\n+\t}\n+)\n+\n+const (\n+\t// Lower bound for the deadline in which buffered actions are dropped.\n+\tstartWorkflowMinDeadline = 5 * time.Second\n+\n+\t// Because the catchup window doesn't apply to a manual start, pick a custom\n+\t// execution deadline before timing out a start.\n+\tmanualStartExecutionDeadline = 1 * time.Hour\n+\n+\t// Upper bound on how many times starting an individual buffered action should be retried.\n+\tExecutorMaxStartAttempts = 10 // TODO - dial this up/remove it\n+)\n+\n+var (\n+\terrRetryLimitExceeded       = errors.New(\"retry limit exceeded\")\n+\t_                     error = &rateLimitedError{}\n+)\n+\n+func RegisterInvokerExecutors(registry *hsm.Registry, options InvokerTaskExecutorOptions) error {\n+\te := invokerTaskExecutor{\n+\t\tInvokerTaskExecutorOptions: options,\n+\t}\n+\n+\terr := hsm.RegisterTimerExecutor(registry, e.executeProcessBufferTask)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn hsm.RegisterImmediateExecutor(registry, e.executeExecuteTask)\n+}\n+\n+func (e invokerTaskExecutor) executeExecuteTask(\n+\tctx context.Context,\n+\tenv hsm.Environment,\n+\tref hsm.Ref,\n+\ttask ExecuteTask,\n+) error {\n+\tvar scheduler *Scheduler\n+\tvar invoker *Invoker\n+\tvar result executeResult\n+\n+\t// Load Scheduler and Invoker's current states.\n+\terr := env.Access(ctx, ref, hsm.AccessRead, func(node *hsm.Node) error {\n+\t\ts, err := loadScheduler(node.Parent)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tscheduler = &s\n+\n+\t\ti, err := e.loadInvoker(node)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tinvoker = &i\n+\n+\t\treturn nil\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tlogger := newTaggedLogger(e.BaseLogger, *scheduler)\n+\n+\t// If we have nothing to do, we can return without any additional writes.\n+\teligibleStarts := invoker.getEligibleBufferedStarts()\n+\tif len(invoker.GetTerminateWorkflows())+\n+\t\tlen(invoker.GetCancelWorkflows())+\n+\t\tlen(eligibleStarts) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\t// Terminate, cancel, and start workflows. The result struct contains the\n+\t// complete outcome of all requests executed in a single batch.\n+\tresult = result.Append(e.terminateWorkflows(logger, *scheduler, invoker.GetTerminateWorkflows()))\n+\tresult = result.Append(e.cancelWorkflows(logger, *scheduler, invoker.GetCancelWorkflows()))\n+\tsres, startResults := e.startWorkflows(logger, env, *scheduler, eligibleStarts)",
        "comment_created_at": "2025-02-18T17:47:48+00:00",
        "comment_author": "bergundy",
        "comment_body": "Also limit the number of \"actions\" (or overall duration) you take for a single task execution to avoid blocking the shared executor for too long.",
        "pr_file_module": null
      }
    ]
  }
]