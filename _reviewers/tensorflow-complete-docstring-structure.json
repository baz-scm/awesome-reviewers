[
  {
    "discussion_id": "1685746686",
    "pr_number": 69756,
    "pr_file": "tensorflow/lite/python/lite.py",
    "created_at": "2024-07-21T14:02:09+00:00",
    "commented_code": "output_tensors: List of output tensors.\n \n     Returns:\n-      The converted data in serialized format.\n+      The converted data is in serialized format.",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1685746686",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 69756,
        "pr_file": "tensorflow/lite/python/lite.py",
        "discussion_id": "1685746686",
        "commented_code": "@@ -1437,11 +1437,11 @@ def convert(self, graph_def, input_tensors, output_tensors):\n       output_tensors: List of output tensors.\n \n     Returns:\n-      The converted data in serialized format.\n+      The converted data is in serialized format.",
        "comment_created_at": "2024-07-21T14:02:09+00:00",
        "comment_author": "mihaimaruseac",
        "comment_body": "```suggestion\r\n      The converted data in serialized format.\r\n```\r\n\r\nPlease revert this edit. The format of `Returns:` is a description of the returned value, as a noun phrase. It should be read as \"this function returns the converted data in serialized format\", but with your edit it gets to \"this function returns the converted data is in serialized format\"",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1295215841",
    "pr_number": 60161,
    "pr_file": "tensorflow/python/platform/test.py",
    "created_at": "2023-08-15T22:43:56+00:00",
    "commented_code": "TensorFlow official binary is built with XLA.\n   \"\"\"\n   return _test_util.IsBuiltWithXLA()\n+\n+\n+@tf_export('test.is_cpu_target_available')\n+def is_cpu_target_available(target):\n+  \"\"\"Returns whether TensorFlow was built for the given CPU target.\n+\n+  This method should only be used in tests written with `tf.test.TestCase`. A",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1295215841",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 60161,
        "pr_file": "tensorflow/python/platform/test.py",
        "discussion_id": "1295215841",
        "commented_code": "@@ -192,3 +192,25 @@ def is_built_with_xla():\n   TensorFlow official binary is built with XLA.\n   \"\"\"\n   return _test_util.IsBuiltWithXLA()\n+\n+\n+@tf_export('test.is_cpu_target_available')\n+def is_cpu_target_available(target):\n+  \"\"\"Returns whether TensorFlow was built for the given CPU target.\n+\n+  This method should only be used in tests written with `tf.test.TestCase`. A",
        "comment_created_at": "2023-08-15T22:43:56+00:00",
        "comment_author": "jakeharmon8",
        "comment_body": "Add an \"Args\" section here to appease our linter (e.g. see line 82)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "973852168",
    "pr_number": 57591,
    "pr_file": "tensorflow/python/compiler/tensorrt/utils.py",
    "created_at": "2022-09-19T04:20:13+00:00",
    "commented_code": "print(\"  - `sudo apt install -y graphviz`\")\n   print(\"  - `dot -Tpng <input_filename>.dot -o <output_filename>.png`\")\n   print(\"===================================================================\n\")\n+\n+\n+@functools.lru_cache\n+def get_nvidia_gpu_desc_dict():\n+\n+  try:\n+    all_devices = device_lib.list_local_devices()\n+    cuda_devices = [d for d in all_devices if d.device_type == \"GPU\"]\n+\n+    local_devices = list()\n+\n+    for device in cuda_devices:\n+      gpu_info = gpu_util.compute_capability_from_device_desc(device)\n+      cc = gpu_info.compute_capability\n+\n+      if cc == (0, 0):\n+        continue\n+\n+      local_devices.append({\n+      \t\"name\": device.name,\n+      \t\"device\": device.physical_device_desc,\n+      \t\"compute_capability\": \".\".join([str(s) for s in cc])\n+      })\n+\n+    return sorted(local_devices, key=lambda x: x[\"name\"], reverse=False)\n+\n+  except errors_impl.NotFoundError as e:\n+    if not all(x in str(e) for x in [\"CUDA\", \"not find\"]):\n+      raise e\n+\n+    else:\n+      logging.error(str(e))\n+      return False\n+\n+\n+@functools.lru_cache\n+def is_platform_supported(precision, gpu_id=0):\n+  \"\"\" This function determines if the current NVIDIA GPU platform supports the",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "973852168",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 57591,
        "pr_file": "tensorflow/python/compiler/tensorrt/utils.py",
        "discussion_id": "973852168",
        "commented_code": "@@ -257,3 +261,79 @@ def draw_graphdef_as_graphviz(graphdef, dot_output_filename):\n   print(\"  - `sudo apt install -y graphviz`\")\n   print(\"  - `dot -Tpng <input_filename>.dot -o <output_filename>.png`\")\n   print(\"===================================================================\\n\")\n+\n+\n+@functools.lru_cache\n+def get_nvidia_gpu_desc_dict():\n+\n+  try:\n+    all_devices = device_lib.list_local_devices()\n+    cuda_devices = [d for d in all_devices if d.device_type == \"GPU\"]\n+\n+    local_devices = list()\n+\n+    for device in cuda_devices:\n+      gpu_info = gpu_util.compute_capability_from_device_desc(device)\n+      cc = gpu_info.compute_capability\n+\n+      if cc == (0, 0):\n+        continue\n+\n+      local_devices.append({\n+      \t\"name\": device.name,\n+      \t\"device\": device.physical_device_desc,\n+      \t\"compute_capability\": \".\".join([str(s) for s in cc])\n+      })\n+\n+    return sorted(local_devices, key=lambda x: x[\"name\"], reverse=False)\n+\n+  except errors_impl.NotFoundError as e:\n+    if not all(x in str(e) for x in [\"CUDA\", \"not find\"]):\n+      raise e\n+\n+    else:\n+      logging.error(str(e))\n+      return False\n+\n+\n+@functools.lru_cache\n+def is_platform_supported(precision, gpu_id=0):\n+  \"\"\" This function determines if the current NVIDIA GPU platform supports the",
        "comment_created_at": "2022-09-19T04:20:13+00:00",
        "comment_author": "bixia1",
        "comment_body": "A docstring should start with a one-line summary, see [here](https://google.github.io/styleguide/pyguide.html#381-docstrings)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "973852334",
    "pr_number": 57591,
    "pr_file": "tensorflow/python/compiler/tensorrt/utils.py",
    "created_at": "2022-09-19T04:20:55+00:00",
    "commented_code": "print(\"  - `sudo apt install -y graphviz`\")\n   print(\"  - `dot -Tpng <input_filename>.dot -o <output_filename>.png`\")\n   print(\"===================================================================\n\")\n+\n+\n+@functools.lru_cache\n+def get_nvidia_gpu_desc_dict():\n+\n+  try:\n+    all_devices = device_lib.list_local_devices()\n+    cuda_devices = [d for d in all_devices if d.device_type == \"GPU\"]\n+\n+    local_devices = list()\n+\n+    for device in cuda_devices:\n+      gpu_info = gpu_util.compute_capability_from_device_desc(device)\n+      cc = gpu_info.compute_capability\n+\n+      if cc == (0, 0):\n+        continue\n+\n+      local_devices.append({\n+      \t\"name\": device.name,\n+      \t\"device\": device.physical_device_desc,\n+      \t\"compute_capability\": \".\".join([str(s) for s in cc])\n+      })\n+\n+    return sorted(local_devices, key=lambda x: x[\"name\"], reverse=False)\n+\n+  except errors_impl.NotFoundError as e:\n+    if not all(x in str(e) for x in [\"CUDA\", \"not find\"]):\n+      raise e\n+\n+    else:\n+      logging.error(str(e))\n+      return False\n+\n+\n+@functools.lru_cache\n+def is_platform_supported(precision, gpu_id=0):\n+  \"\"\" This function determines if the current NVIDIA GPU platform supports the\n+  requested TensorRT compute precision.\n+\n+  Required Compute Capabilities:\n+  - FP16 = 5.3 || 6.0 || 6.2 || 7.0+\n+  - INT8 = 6.1 || 7.0 || 7.2+",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "973852334",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 57591,
        "pr_file": "tensorflow/python/compiler/tensorrt/utils.py",
        "discussion_id": "973852334",
        "commented_code": "@@ -257,3 +261,79 @@ def draw_graphdef_as_graphviz(graphdef, dot_output_filename):\n   print(\"  - `sudo apt install -y graphviz`\")\n   print(\"  - `dot -Tpng <input_filename>.dot -o <output_filename>.png`\")\n   print(\"===================================================================\\n\")\n+\n+\n+@functools.lru_cache\n+def get_nvidia_gpu_desc_dict():\n+\n+  try:\n+    all_devices = device_lib.list_local_devices()\n+    cuda_devices = [d for d in all_devices if d.device_type == \"GPU\"]\n+\n+    local_devices = list()\n+\n+    for device in cuda_devices:\n+      gpu_info = gpu_util.compute_capability_from_device_desc(device)\n+      cc = gpu_info.compute_capability\n+\n+      if cc == (0, 0):\n+        continue\n+\n+      local_devices.append({\n+      \t\"name\": device.name,\n+      \t\"device\": device.physical_device_desc,\n+      \t\"compute_capability\": \".\".join([str(s) for s in cc])\n+      })\n+\n+    return sorted(local_devices, key=lambda x: x[\"name\"], reverse=False)\n+\n+  except errors_impl.NotFoundError as e:\n+    if not all(x in str(e) for x in [\"CUDA\", \"not find\"]):\n+      raise e\n+\n+    else:\n+      logging.error(str(e))\n+      return False\n+\n+\n+@functools.lru_cache\n+def is_platform_supported(precision, gpu_id=0):\n+  \"\"\" This function determines if the current NVIDIA GPU platform supports the\n+  requested TensorRT compute precision.\n+\n+  Required Compute Capabilities:\n+  - FP16 = 5.3 || 6.0 || 6.2 || 7.0+\n+  - INT8 = 6.1 || 7.0 || 7.2+",
        "comment_created_at": "2022-09-19T04:20:55+00:00",
        "comment_author": "bixia1",
        "comment_body": "Please also document the argument and the exceptions.",
        "pr_file_module": null
      }
    ]
  }
]