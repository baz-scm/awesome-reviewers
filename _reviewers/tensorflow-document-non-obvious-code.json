[
  {
    "discussion_id": "1227015430",
    "pr_number": 60701,
    "pr_file": "tensorflow/compiler/mlir/tosa/transforms/convert_tfl_uint8.cc",
    "created_at": "2023-06-12T17:29:12+00:00",
    "commented_code": "}\n };\n \n+namespace {\n+\n+// returns true iff @a type is a shaped type with element type that is uint8\n+// if it is, then return the rescaled type, uint8_zp, and output_zp to use to\n+// rescale type to signed type with adjusted zero point.\n+bool IsShapedUint8Type(OpBuilder& builder, const Type type, Type& rescaled_type,\n+                       int32_t& uint8_zp, int32_t& output_zp) {\n+  auto uint8_type = type.dyn_cast<mlir::ShapedType>();\n+  if (!uint8_type) return false;\n+\n+  auto element_type = uint8_type.getElementType();\n+  auto uint8_element_quant_type =\n+      element_type.dyn_cast<mlir::quant::UniformQuantizedType>();\n+  bool is_uint8_element_quant_type =\n+      uint8_element_quant_type && !uint8_element_quant_type.isSigned() &&\n+      uint8_element_quant_type.getStorageTypeIntegralWidth() == 8;\n+  bool is_uint8_element_type = element_type.isUnsignedInteger(8);\n+  if (!is_uint8_element_quant_type && !is_uint8_element_type) return false;\n+\n+  // type has uint8 element type\n+  if (is_uint8_element_quant_type) {\n+    double type_range_min =\n+        static_cast<double>(uint8_element_quant_type.getStorageTypeMin() -\n+                            uint8_element_quant_type.getZeroPoint()) *\n+        uint8_element_quant_type.getScale();\n+    double type_range_max =\n+        static_cast<double>(uint8_element_quant_type.getStorageTypeMax() -\n+                            uint8_element_quant_type.getZeroPoint()) *\n+        uint8_element_quant_type.getScale();\n+    bool narrow_range =\n+        uint8_element_quant_type.getStorageTypeMin() == 1 ? true : false;\n+\n+    rescaled_type = uint8_type.clone(buildQTypeFromMinMax(\n+        builder, uint8_element_quant_type.getExpressedType(),\n+        builder.getF64FloatAttr(type_range_min),\n+        builder.getF64FloatAttr(type_range_max),\n+        builder.getI32IntegerAttr(\n+            uint8_element_quant_type.getStorageTypeIntegralWidth()),\n+        0, true /* signed */, builder.getBoolAttr(narrow_range)));",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1227015430",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 60701,
        "pr_file": "tensorflow/compiler/mlir/tosa/transforms/convert_tfl_uint8.cc",
        "discussion_id": "1227015430",
        "commented_code": "@@ -130,6 +130,63 @@ struct ConvertUint8QConstOp : public RewritePattern {\n   }\n };\n \n+namespace {\n+\n+// returns true iff @a type is a shaped type with element type that is uint8\n+// if it is, then return the rescaled type, uint8_zp, and output_zp to use to\n+// rescale type to signed type with adjusted zero point.\n+bool IsShapedUint8Type(OpBuilder& builder, const Type type, Type& rescaled_type,\n+                       int32_t& uint8_zp, int32_t& output_zp) {\n+  auto uint8_type = type.dyn_cast<mlir::ShapedType>();\n+  if (!uint8_type) return false;\n+\n+  auto element_type = uint8_type.getElementType();\n+  auto uint8_element_quant_type =\n+      element_type.dyn_cast<mlir::quant::UniformQuantizedType>();\n+  bool is_uint8_element_quant_type =\n+      uint8_element_quant_type && !uint8_element_quant_type.isSigned() &&\n+      uint8_element_quant_type.getStorageTypeIntegralWidth() == 8;\n+  bool is_uint8_element_type = element_type.isUnsignedInteger(8);\n+  if (!is_uint8_element_quant_type && !is_uint8_element_type) return false;\n+\n+  // type has uint8 element type\n+  if (is_uint8_element_quant_type) {\n+    double type_range_min =\n+        static_cast<double>(uint8_element_quant_type.getStorageTypeMin() -\n+                            uint8_element_quant_type.getZeroPoint()) *\n+        uint8_element_quant_type.getScale();\n+    double type_range_max =\n+        static_cast<double>(uint8_element_quant_type.getStorageTypeMax() -\n+                            uint8_element_quant_type.getZeroPoint()) *\n+        uint8_element_quant_type.getScale();\n+    bool narrow_range =\n+        uint8_element_quant_type.getStorageTypeMin() == 1 ? true : false;\n+\n+    rescaled_type = uint8_type.clone(buildQTypeFromMinMax(\n+        builder, uint8_element_quant_type.getExpressedType(),\n+        builder.getF64FloatAttr(type_range_min),\n+        builder.getF64FloatAttr(type_range_max),\n+        builder.getI32IntegerAttr(\n+            uint8_element_quant_type.getStorageTypeIntegralWidth()),\n+        0, true /* signed */, builder.getBoolAttr(narrow_range)));",
        "comment_created_at": "2023-06-12T17:29:12+00:00",
        "comment_author": "rsuderman",
        "comment_body": "This should include /*arg=*/ for any non obvious arg. Specifically the `0` and `true`  should both have it.",
        "pr_file_module": null
      },
      {
        "comment_id": "1236208801",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 60701,
        "pr_file": "tensorflow/compiler/mlir/tosa/transforms/convert_tfl_uint8.cc",
        "discussion_id": "1227015430",
        "commented_code": "@@ -130,6 +130,63 @@ struct ConvertUint8QConstOp : public RewritePattern {\n   }\n };\n \n+namespace {\n+\n+// returns true iff @a type is a shaped type with element type that is uint8\n+// if it is, then return the rescaled type, uint8_zp, and output_zp to use to\n+// rescale type to signed type with adjusted zero point.\n+bool IsShapedUint8Type(OpBuilder& builder, const Type type, Type& rescaled_type,\n+                       int32_t& uint8_zp, int32_t& output_zp) {\n+  auto uint8_type = type.dyn_cast<mlir::ShapedType>();\n+  if (!uint8_type) return false;\n+\n+  auto element_type = uint8_type.getElementType();\n+  auto uint8_element_quant_type =\n+      element_type.dyn_cast<mlir::quant::UniformQuantizedType>();\n+  bool is_uint8_element_quant_type =\n+      uint8_element_quant_type && !uint8_element_quant_type.isSigned() &&\n+      uint8_element_quant_type.getStorageTypeIntegralWidth() == 8;\n+  bool is_uint8_element_type = element_type.isUnsignedInteger(8);\n+  if (!is_uint8_element_quant_type && !is_uint8_element_type) return false;\n+\n+  // type has uint8 element type\n+  if (is_uint8_element_quant_type) {\n+    double type_range_min =\n+        static_cast<double>(uint8_element_quant_type.getStorageTypeMin() -\n+                            uint8_element_quant_type.getZeroPoint()) *\n+        uint8_element_quant_type.getScale();\n+    double type_range_max =\n+        static_cast<double>(uint8_element_quant_type.getStorageTypeMax() -\n+                            uint8_element_quant_type.getZeroPoint()) *\n+        uint8_element_quant_type.getScale();\n+    bool narrow_range =\n+        uint8_element_quant_type.getStorageTypeMin() == 1 ? true : false;\n+\n+    rescaled_type = uint8_type.clone(buildQTypeFromMinMax(\n+        builder, uint8_element_quant_type.getExpressedType(),\n+        builder.getF64FloatAttr(type_range_min),\n+        builder.getF64FloatAttr(type_range_max),\n+        builder.getI32IntegerAttr(\n+            uint8_element_quant_type.getStorageTypeIntegralWidth()),\n+        0, true /* signed */, builder.getBoolAttr(narrow_range)));",
        "comment_created_at": "2023-06-21T02:33:40+00:00",
        "comment_author": "Tai78641",
        "comment_body": "fixed",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1264223373",
    "pr_number": 60668,
    "pr_file": "third_party/gpus/cuda_configure.bzl",
    "created_at": "2023-07-14T22:40:10+00:00",
    "commented_code": ").stdout.strip() + \"/share\"\n         inc_dirs += \"\n\" + resource_dir\n \n-    return [\n+    compiler_includes = [\n         _normalize_include_path(repository_ctx, _cxx_inc_convert(p))\n         for p in inc_dirs.split(\"\n\")\n     ]\n \n+    # fix include path by also including paths where resolved symlink is replaced by original path",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1264223373",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 60668,
        "pr_file": "third_party/gpus/cuda_configure.bzl",
        "discussion_id": "1264223373",
        "commented_code": "@@ -316,11 +316,36 @@ def _get_cxx_inc_directories_impl(repository_ctx, cc, lang_is_cpp, tf_sysroot):\n         ).stdout.strip() + \"/share\"\n         inc_dirs += \"\\n\" + resource_dir\n \n-    return [\n+    compiler_includes = [\n         _normalize_include_path(repository_ctx, _cxx_inc_convert(p))\n         for p in inc_dirs.split(\"\\n\")\n     ]\n \n+    # fix include path by also including paths where resolved symlink is replaced by original path",
        "comment_created_at": "2023-07-14T22:40:10+00:00",
        "comment_author": "angerson",
        "comment_body": "This is really confusing. It's in line with the rest of this function, which is also really confusing, so that's not really a deal breaker.\r\n\r\nI think it would be ok to include this if you extend the comment with hard examples of what's going on, like the possible results that cc might return and what this chunk of code will do to them.\r\n\r\nAlso @perfinion may have some requests here.",
        "pr_file_module": null
      },
      {
        "comment_id": "1265071008",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 60668,
        "pr_file": "third_party/gpus/cuda_configure.bzl",
        "discussion_id": "1264223373",
        "commented_code": "@@ -316,11 +316,36 @@ def _get_cxx_inc_directories_impl(repository_ctx, cc, lang_is_cpp, tf_sysroot):\n         ).stdout.strip() + \"/share\"\n         inc_dirs += \"\\n\" + resource_dir\n \n-    return [\n+    compiler_includes = [\n         _normalize_include_path(repository_ctx, _cxx_inc_convert(p))\n         for p in inc_dirs.split(\"\\n\")\n     ]\n \n+    # fix include path by also including paths where resolved symlink is replaced by original path",
        "comment_created_at": "2023-07-17T09:02:17+00:00",
        "comment_author": "Flamefire",
        "comment_body": "> I think it would be ok to include this if you extend the comment with hard examples of what's going on, like the possible results that cc might return and what this chunk of code will do to them.\r\n\r\nI extended the introductory comment and added examples further below.\r\nIn anticipation of a potential objection about duplicate paths being added I added a check in the construction of `original_compiler_includes`\r\n\r\nDoes that suffice?",
        "pr_file_module": null
      },
      {
        "comment_id": "1267198166",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 60668,
        "pr_file": "third_party/gpus/cuda_configure.bzl",
        "discussion_id": "1264223373",
        "commented_code": "@@ -316,11 +316,36 @@ def _get_cxx_inc_directories_impl(repository_ctx, cc, lang_is_cpp, tf_sysroot):\n         ).stdout.strip() + \"/share\"\n         inc_dirs += \"\\n\" + resource_dir\n \n-    return [\n+    compiler_includes = [\n         _normalize_include_path(repository_ctx, _cxx_inc_convert(p))\n         for p in inc_dirs.split(\"\\n\")\n     ]\n \n+    # fix include path by also including paths where resolved symlink is replaced by original path",
        "comment_created_at": "2023-07-18T19:11:49+00:00",
        "comment_author": "angerson",
        "comment_body": "Thanks for adding more details. Does this work for clang as well?",
        "pr_file_module": null
      },
      {
        "comment_id": "1267971661",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 60668,
        "pr_file": "third_party/gpus/cuda_configure.bzl",
        "discussion_id": "1264223373",
        "commented_code": "@@ -316,11 +316,36 @@ def _get_cxx_inc_directories_impl(repository_ctx, cc, lang_is_cpp, tf_sysroot):\n         ).stdout.strip() + \"/share\"\n         inc_dirs += \"\\n\" + resource_dir\n \n-    return [\n+    compiler_includes = [\n         _normalize_include_path(repository_ctx, _cxx_inc_convert(p))\n         for p in inc_dirs.split(\"\\n\")\n     ]\n \n+    # fix include path by also including paths where resolved symlink is replaced by original path",
        "comment_created_at": "2023-07-19T12:00:43+00:00",
        "comment_author": "Flamefire",
        "comment_body": "I assume so yes, as when I checked the only difference was `COLLECT_GCC=` vs `InstalledDir: ` in the compiler output so the logic should work the same.",
        "pr_file_module": null
      }
    ]
  }
]