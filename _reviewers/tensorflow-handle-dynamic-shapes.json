[
  {
    "discussion_id": "547036176",
    "pr_number": 44950,
    "pr_file": "tensorflow/python/tools/optimize_for_inference_lib.py",
    "created_at": "2020-12-22T02:45:10+00:00",
    "commented_code": "result_graph_def.node.extend(new_ops)\n   return result_graph_def\n+\n+def fuse_decomposed_batch_norm(input_graph_def):\n+  \"\"\"Fuse individual ops in batch normalization to FusedBatchNorm.\n+\n+  In some models, the batch normalizatin is performed via a group of individual\n+  ops instead of using single FusedBatchNorm op. This function identifies a\n+  pattern of batch normalization subgraph which is made of multiple ops and\n+  transforms the graph by replacing those individual ops with FusedBatchNorm op.\n+  This will provide the opportunity to further fold the FusedBatchNorm with\n+  convolution ops to reduce the computation steps during inference.\n+  This function currently only recognizes batch normalization patterns described\n+  below, this could be extended if newer patterns are seen. Also, only target\n+  for NHWC formatted graph.\n+\n+  Computation function:\n+    (X * multiplier) + (Beta - Mean * multiplier)\n+      where multiplier = rsqrt (Variance + Epsilon) * Gamma\n+                    OR = rsqrt (Variance + Epsilon) when Gamma is 1\n+\n+  Subgraph:\n+  {\"Add\"\n+      {{\"Mul\"          // mul_0\n+          {{\"*\"},     // input to apply batch normalization\n+           {\"Mul\"      // mul_1, same op is used inside the Sub block\n+              {{\"Rsqrt\"\n+                  {\"Add\"\n+                      {{\"Const\"},  //Variance\n+                       {\"Const\"}   //Epsilon\n+                      }\n+                  }\n+                }, // end - Rsqrt\n+                {\"Const\"}  //Gamma\n+              }\n+            } // end - mul_1\n+          }\n+       }, // end - mul_0\n+       {\"Sub\"\n+          {{\"Const\"},   //Beta\n+           {\"Mul\"        // mul_3\n+              {{\"Const\"},  //Mean\n+               {\"Mul\"        //same mul_1 op as in previous block\n+                  {{\"Rsqrt\"\n+                      {\"Add\"\n+                          {{\"Const\"},  //Variance\n+                           {\"Const\"}   //Epsilon\n+                          }\n+                      }\n+                   }, // end - Rsqrt\n+                   {\"Const\"} //Gamma\n+                  }\n+                } //end - mul_1\n+              }\n+            } // end - mul_3\n+          }\n+        } //end - Sub\n+      }\n+  } //end - Add\n+\n+  Subgraph pattern when gamma value is 1 and the gamma scaling Mul is skipped\n+  {\"Add\"\n+      {{\"Mul\"           // mul_0\n+          {{\"*\"},       // input to apply batch normalization\n+           {\"Rsqrt\"     // same Rsqrt op used in Sub block\n+              {\"Add\"\n+                 {{\"Const\"},  //Variance\n+                  {\"Const\"}   //Epsilon\n+                 }\n+              }\n+            } // end - Rsqrt\n+          }\n+        }, // end - mul_0\n+        {\"Sub\"\n+          {{\"Const\"},    //Beta\n+           {\"Mul\"        // mul_1\n+              {{\"Const\"},  //Mean\n+               {\"Rsqrt\"    //same Rsqrt op as in previous mul_0 block\n+                  {\"Add\"\n+                    {{\"Const\"},  //Variance\n+                     {\"Const\"}   //Epsilon\n+                    }\n+                  }\n+                } // end - Rsqrt\n+              }\n+           } // end - mul_1\n+          }\n+        } // end - Sub\n+      }\n+  } // end - Add\n+\n+  Args:\n+    input_graph_def: A GraphDef containing a model.\n+\n+  Returns:\n+    Modified graph with individual ops that made up of batch normalization\n+    fused to FusedBatchNorm.\n+\n+  Raises:\n+    ValueError: If the graph is badly formed with duplicate node names.\n+  \"\"\"\n+  input_node_map = {}\n+  for node in input_graph_def.node:\n+    if node.name not in input_node_map:\n+      input_node_map[node.name] = node\n+    else:\n+      raise ValueError(\"Duplicate node names detected for \", node.name)\n+\n+  # Check for data format and only proceed for NHWC or format not set case\n+  data_format = None\n+  for node in input_graph_def.node:\n+    if \"data_format\" in node.attr.keys():\n+      data_format = node.attr[\"data_format\"]\n+      break",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "547036176",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 44950,
        "pr_file": "tensorflow/python/tools/optimize_for_inference_lib.py",
        "discussion_id": "547036176",
        "commented_code": "@@ -531,3 +534,246 @@ def fuse_resize_and_conv(input_graph_def, output_node_names):\n \n   result_graph_def.node.extend(new_ops)\n   return result_graph_def\n+\n+def fuse_decomposed_batch_norm(input_graph_def):\n+  \"\"\"Fuse individual ops in batch normalization to FusedBatchNorm.\n+\n+  In some models, the batch normalizatin is performed via a group of individual\n+  ops instead of using single FusedBatchNorm op. This function identifies a\n+  pattern of batch normalization subgraph which is made of multiple ops and\n+  transforms the graph by replacing those individual ops with FusedBatchNorm op.\n+  This will provide the opportunity to further fold the FusedBatchNorm with\n+  convolution ops to reduce the computation steps during inference.\n+  This function currently only recognizes batch normalization patterns described\n+  below, this could be extended if newer patterns are seen. Also, only target\n+  for NHWC formatted graph.\n+\n+  Computation function:\n+    (X * multiplier) + (Beta - Mean * multiplier)\n+      where multiplier = rsqrt (Variance + Epsilon) * Gamma\n+                    OR = rsqrt (Variance + Epsilon) when Gamma is 1\n+\n+  Subgraph:\n+  {\"Add\"\n+      {{\"Mul\"          // mul_0\n+          {{\"*\"},     // input to apply batch normalization\n+           {\"Mul\"      // mul_1, same op is used inside the Sub block\n+              {{\"Rsqrt\"\n+                  {\"Add\"\n+                      {{\"Const\"},  //Variance\n+                       {\"Const\"}   //Epsilon\n+                      }\n+                  }\n+                }, // end - Rsqrt\n+                {\"Const\"}  //Gamma\n+              }\n+            } // end - mul_1\n+          }\n+       }, // end - mul_0\n+       {\"Sub\"\n+          {{\"Const\"},   //Beta\n+           {\"Mul\"        // mul_3\n+              {{\"Const\"},  //Mean\n+               {\"Mul\"        //same mul_1 op as in previous block\n+                  {{\"Rsqrt\"\n+                      {\"Add\"\n+                          {{\"Const\"},  //Variance\n+                           {\"Const\"}   //Epsilon\n+                          }\n+                      }\n+                   }, // end - Rsqrt\n+                   {\"Const\"} //Gamma\n+                  }\n+                } //end - mul_1\n+              }\n+            } // end - mul_3\n+          }\n+        } //end - Sub\n+      }\n+  } //end - Add\n+\n+  Subgraph pattern when gamma value is 1 and the gamma scaling Mul is skipped\n+  {\"Add\"\n+      {{\"Mul\"           // mul_0\n+          {{\"*\"},       // input to apply batch normalization\n+           {\"Rsqrt\"     // same Rsqrt op used in Sub block\n+              {\"Add\"\n+                 {{\"Const\"},  //Variance\n+                  {\"Const\"}   //Epsilon\n+                 }\n+              }\n+            } // end - Rsqrt\n+          }\n+        }, // end - mul_0\n+        {\"Sub\"\n+          {{\"Const\"},    //Beta\n+           {\"Mul\"        // mul_1\n+              {{\"Const\"},  //Mean\n+               {\"Rsqrt\"    //same Rsqrt op as in previous mul_0 block\n+                  {\"Add\"\n+                    {{\"Const\"},  //Variance\n+                     {\"Const\"}   //Epsilon\n+                    }\n+                  }\n+                } // end - Rsqrt\n+              }\n+           } // end - mul_1\n+          }\n+        } // end - Sub\n+      }\n+  } // end - Add\n+\n+  Args:\n+    input_graph_def: A GraphDef containing a model.\n+\n+  Returns:\n+    Modified graph with individual ops that made up of batch normalization\n+    fused to FusedBatchNorm.\n+\n+  Raises:\n+    ValueError: If the graph is badly formed with duplicate node names.\n+  \"\"\"\n+  input_node_map = {}\n+  for node in input_graph_def.node:\n+    if node.name not in input_node_map:\n+      input_node_map[node.name] = node\n+    else:\n+      raise ValueError(\"Duplicate node names detected for \", node.name)\n+\n+  # Check for data format and only proceed for NHWC or format not set case\n+  data_format = None\n+  for node in input_graph_def.node:\n+    if \"data_format\" in node.attr.keys():\n+      data_format = node.attr[\"data_format\"]\n+      break",
        "comment_created_at": "2020-12-22T02:45:10+00:00",
        "comment_author": "penpornk",
        "comment_body": "Is it okay to assume all nodes have the same format? Shouldn't we check all the nodes that have this attribute?",
        "pr_file_module": null
      },
      {
        "comment_id": "551643364",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 44950,
        "pr_file": "tensorflow/python/tools/optimize_for_inference_lib.py",
        "discussion_id": "547036176",
        "commented_code": "@@ -531,3 +534,246 @@ def fuse_resize_and_conv(input_graph_def, output_node_names):\n \n   result_graph_def.node.extend(new_ops)\n   return result_graph_def\n+\n+def fuse_decomposed_batch_norm(input_graph_def):\n+  \"\"\"Fuse individual ops in batch normalization to FusedBatchNorm.\n+\n+  In some models, the batch normalizatin is performed via a group of individual\n+  ops instead of using single FusedBatchNorm op. This function identifies a\n+  pattern of batch normalization subgraph which is made of multiple ops and\n+  transforms the graph by replacing those individual ops with FusedBatchNorm op.\n+  This will provide the opportunity to further fold the FusedBatchNorm with\n+  convolution ops to reduce the computation steps during inference.\n+  This function currently only recognizes batch normalization patterns described\n+  below, this could be extended if newer patterns are seen. Also, only target\n+  for NHWC formatted graph.\n+\n+  Computation function:\n+    (X * multiplier) + (Beta - Mean * multiplier)\n+      where multiplier = rsqrt (Variance + Epsilon) * Gamma\n+                    OR = rsqrt (Variance + Epsilon) when Gamma is 1\n+\n+  Subgraph:\n+  {\"Add\"\n+      {{\"Mul\"          // mul_0\n+          {{\"*\"},     // input to apply batch normalization\n+           {\"Mul\"      // mul_1, same op is used inside the Sub block\n+              {{\"Rsqrt\"\n+                  {\"Add\"\n+                      {{\"Const\"},  //Variance\n+                       {\"Const\"}   //Epsilon\n+                      }\n+                  }\n+                }, // end - Rsqrt\n+                {\"Const\"}  //Gamma\n+              }\n+            } // end - mul_1\n+          }\n+       }, // end - mul_0\n+       {\"Sub\"\n+          {{\"Const\"},   //Beta\n+           {\"Mul\"        // mul_3\n+              {{\"Const\"},  //Mean\n+               {\"Mul\"        //same mul_1 op as in previous block\n+                  {{\"Rsqrt\"\n+                      {\"Add\"\n+                          {{\"Const\"},  //Variance\n+                           {\"Const\"}   //Epsilon\n+                          }\n+                      }\n+                   }, // end - Rsqrt\n+                   {\"Const\"} //Gamma\n+                  }\n+                } //end - mul_1\n+              }\n+            } // end - mul_3\n+          }\n+        } //end - Sub\n+      }\n+  } //end - Add\n+\n+  Subgraph pattern when gamma value is 1 and the gamma scaling Mul is skipped\n+  {\"Add\"\n+      {{\"Mul\"           // mul_0\n+          {{\"*\"},       // input to apply batch normalization\n+           {\"Rsqrt\"     // same Rsqrt op used in Sub block\n+              {\"Add\"\n+                 {{\"Const\"},  //Variance\n+                  {\"Const\"}   //Epsilon\n+                 }\n+              }\n+            } // end - Rsqrt\n+          }\n+        }, // end - mul_0\n+        {\"Sub\"\n+          {{\"Const\"},    //Beta\n+           {\"Mul\"        // mul_1\n+              {{\"Const\"},  //Mean\n+               {\"Rsqrt\"    //same Rsqrt op as in previous mul_0 block\n+                  {\"Add\"\n+                    {{\"Const\"},  //Variance\n+                     {\"Const\"}   //Epsilon\n+                    }\n+                  }\n+                } // end - Rsqrt\n+              }\n+           } // end - mul_1\n+          }\n+        } // end - Sub\n+      }\n+  } // end - Add\n+\n+  Args:\n+    input_graph_def: A GraphDef containing a model.\n+\n+  Returns:\n+    Modified graph with individual ops that made up of batch normalization\n+    fused to FusedBatchNorm.\n+\n+  Raises:\n+    ValueError: If the graph is badly formed with duplicate node names.\n+  \"\"\"\n+  input_node_map = {}\n+  for node in input_graph_def.node:\n+    if node.name not in input_node_map:\n+      input_node_map[node.name] = node\n+    else:\n+      raise ValueError(\"Duplicate node names detected for \", node.name)\n+\n+  # Check for data format and only proceed for NHWC or format not set case\n+  data_format = None\n+  for node in input_graph_def.node:\n+    if \"data_format\" in node.attr.keys():\n+      data_format = node.attr[\"data_format\"]\n+      break",
        "comment_created_at": "2021-01-05T00:11:46+00:00",
        "comment_author": "yimeisun123",
        "comment_body": "I think it is safe to assume the format is consistent in the input graph. I could let the format check run through all nodes in the graph with additional processing cost. Please let me know your call.",
        "pr_file_module": null
      },
      {
        "comment_id": "556199526",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 44950,
        "pr_file": "tensorflow/python/tools/optimize_for_inference_lib.py",
        "discussion_id": "547036176",
        "commented_code": "@@ -531,3 +534,246 @@ def fuse_resize_and_conv(input_graph_def, output_node_names):\n \n   result_graph_def.node.extend(new_ops)\n   return result_graph_def\n+\n+def fuse_decomposed_batch_norm(input_graph_def):\n+  \"\"\"Fuse individual ops in batch normalization to FusedBatchNorm.\n+\n+  In some models, the batch normalizatin is performed via a group of individual\n+  ops instead of using single FusedBatchNorm op. This function identifies a\n+  pattern of batch normalization subgraph which is made of multiple ops and\n+  transforms the graph by replacing those individual ops with FusedBatchNorm op.\n+  This will provide the opportunity to further fold the FusedBatchNorm with\n+  convolution ops to reduce the computation steps during inference.\n+  This function currently only recognizes batch normalization patterns described\n+  below, this could be extended if newer patterns are seen. Also, only target\n+  for NHWC formatted graph.\n+\n+  Computation function:\n+    (X * multiplier) + (Beta - Mean * multiplier)\n+      where multiplier = rsqrt (Variance + Epsilon) * Gamma\n+                    OR = rsqrt (Variance + Epsilon) when Gamma is 1\n+\n+  Subgraph:\n+  {\"Add\"\n+      {{\"Mul\"          // mul_0\n+          {{\"*\"},     // input to apply batch normalization\n+           {\"Mul\"      // mul_1, same op is used inside the Sub block\n+              {{\"Rsqrt\"\n+                  {\"Add\"\n+                      {{\"Const\"},  //Variance\n+                       {\"Const\"}   //Epsilon\n+                      }\n+                  }\n+                }, // end - Rsqrt\n+                {\"Const\"}  //Gamma\n+              }\n+            } // end - mul_1\n+          }\n+       }, // end - mul_0\n+       {\"Sub\"\n+          {{\"Const\"},   //Beta\n+           {\"Mul\"        // mul_3\n+              {{\"Const\"},  //Mean\n+               {\"Mul\"        //same mul_1 op as in previous block\n+                  {{\"Rsqrt\"\n+                      {\"Add\"\n+                          {{\"Const\"},  //Variance\n+                           {\"Const\"}   //Epsilon\n+                          }\n+                      }\n+                   }, // end - Rsqrt\n+                   {\"Const\"} //Gamma\n+                  }\n+                } //end - mul_1\n+              }\n+            } // end - mul_3\n+          }\n+        } //end - Sub\n+      }\n+  } //end - Add\n+\n+  Subgraph pattern when gamma value is 1 and the gamma scaling Mul is skipped\n+  {\"Add\"\n+      {{\"Mul\"           // mul_0\n+          {{\"*\"},       // input to apply batch normalization\n+           {\"Rsqrt\"     // same Rsqrt op used in Sub block\n+              {\"Add\"\n+                 {{\"Const\"},  //Variance\n+                  {\"Const\"}   //Epsilon\n+                 }\n+              }\n+            } // end - Rsqrt\n+          }\n+        }, // end - mul_0\n+        {\"Sub\"\n+          {{\"Const\"},    //Beta\n+           {\"Mul\"        // mul_1\n+              {{\"Const\"},  //Mean\n+               {\"Rsqrt\"    //same Rsqrt op as in previous mul_0 block\n+                  {\"Add\"\n+                    {{\"Const\"},  //Variance\n+                     {\"Const\"}   //Epsilon\n+                    }\n+                  }\n+                } // end - Rsqrt\n+              }\n+           } // end - mul_1\n+          }\n+        } // end - Sub\n+      }\n+  } // end - Add\n+\n+  Args:\n+    input_graph_def: A GraphDef containing a model.\n+\n+  Returns:\n+    Modified graph with individual ops that made up of batch normalization\n+    fused to FusedBatchNorm.\n+\n+  Raises:\n+    ValueError: If the graph is badly formed with duplicate node names.\n+  \"\"\"\n+  input_node_map = {}\n+  for node in input_graph_def.node:\n+    if node.name not in input_node_map:\n+      input_node_map[node.name] = node\n+    else:\n+      raise ValueError(\"Duplicate node names detected for \", node.name)\n+\n+  # Check for data format and only proceed for NHWC or format not set case\n+  data_format = None\n+  for node in input_graph_def.node:\n+    if \"data_format\" in node.attr.keys():\n+      data_format = node.attr[\"data_format\"]\n+      break",
        "comment_created_at": "2021-01-13T01:14:32+00:00",
        "comment_author": "yimeisun123",
        "comment_body": "I decided to scan through all nodes for format check. Please refer to the new commit.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1443932792",
    "pr_number": 62750,
    "pr_file": "tensorflow/python/ops/math_ops.py",
    "created_at": "2024-01-07T05:19:44+00:00",
    "commented_code": "name=name,\n     )\n \n+@tf_export(\"sparse.sampled_addmm\")\n+def sampled_addmm(indices,\n+      values,\n+      dense_shape,\n+      mat1,\n+      mat2,\n+      beta=1.0,\n+      alpha=1.0,\n+      output_type=dtypes.float32\n+):\n+  \"\"\"Multiplies matrix `mat1` by matrix `mat2` at the locations defined by `indices`.\n+  The product is scaled and added to `values`, \n+  producing `alpha` * (`mat1` @ `mat2`) * spy(`indices`) + `beta` * `values`\n+\n+  The function `spy(indices)` is the sparsity pattern matrix derived from `indices`.\n+\n+  The `mat1` and `mat2` inputs must be tensors of rank >= 2 where the inner 2\n+  dimensions specify valid matrix multiplication dimensions, and any further dimensions\n+  specify matching batch size.\n+\n+  The `indices`, `values`, and `dense_shape` inputs make up the components of a \n+  `SparseTensor` which defines the sparsity pattern of the output. The sparsity pattern\n+  has values of 1 at the positions defined by the `SparseTensor`, and 0 elsewhere.\n+\n+  The `alpha` and `beta` inputs are the scaling factors.\n+\n+  The supported types for `values`, `mat1`, and `mat2` are:\n+  `bfloat16`, `float16`, `float32`, `float64`.\n+\n+  A simple 2-D tensor operation:\n+\n+  >>> indices = tf.constant([0, 0, 1, 1], shape=[2, 2])\n+  >>> indices\n+  <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[0, 0],\n+         [1, 1]], dtype=int32)>\n+  >>> values = tf.constant([0.5, 0.3])\n+  >>> values\n+  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5, 0.3], dtype=float32)>\n+  >>> dense_shape = tf.constant([2, 2])\n+  >>> dense_shape\n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>\n+  >>> mat1 = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n+  >>> mat1\n+  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n+  array([[1, 2, 3],\n+         [4, 5, 6]], dtype=int32)>\n+  >>> mat2 = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n+  >>> mat2\n+  <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n+  array([[ 7,  8],\n+         [ 9, 10],\n+         [11, 12]], dtype=int32)>\n+  >>> tf.sparse.sampled_addmm(indices, values, dense_shape, mat1, mat2, alpha=0.75, beta=0.25)\n+  (<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[0, 0],\n+         [1, 1]], dtype=int32)>, \n+  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 43.625, 115.575], dtype=float32)>, \n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>)\n \n+  A batch operation:\n+\n+  >>> indices = tf.constant([0, 1, 1, 0, 0, 0, 1, 0], shape=[2, 2, 2])\n+  >>> indices\n+  <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n+  array([[[0, 1],\n+          [1, 0]],\n+         [[0, 0],\n+          [1, 0]]], dtype=int32)>\n+  >>> values = tf.constant([3, 5, 2, 7], shape=[2, 2])\n+  >>> values\n+  <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[3, 5],\n+         [2, 7]], dtype=int32)>\n+  >>> dense_shape = tf.constant([2, 2])\n+  >>> dense_shape\n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>\n+  >>> mat1 = tf.constant(np.arange(1, 13), shape=[2, 2, 3])\n+  >>> mat1\n+  <tf.Tensor: shape=(2, 2, 3), dtype=int64, numpy=\n+  array([[[ 1,  2,  3],\n+          [ 4,  5,  6]],\n+         [[ 7,  8,  9],\n+          [10, 11, 12]]])>\n+  >>> mat2 = tf.constant(np.arange(13, 25), shape=[2, 3, 2])\n+  >>> mat2\n+  <tf.Tensor: shape=(2, 3, 2), dtype=int64, numpy=\n+  array([[[13, 14],\n+          [15, 16],\n+          [17, 18]],\n+         [[19, 20],\n+          [21, 22],\n+          [23, 24]]])>\n+  >>> tf.sparse.sampled_addmm(indices, values, dense_shape, mat1, mat2, alpha=0.75, beta=0.25)\n+  (<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n+  array([[[0, 1],\n+          [1, 0]],\n+         [[0, 0],\n+          [1, 0]]], dtype=int32)>, <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n+  array([[ 75.75, 173.  ],\n+         [381.5 , 524.5 ]], dtype=float32)>, \n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>)\n+\n+  Args:\n+    indices: `tf.Tensor` containing coordinates for the rows and columns to be multiplied.\n+      Must have rank > 1.\n+    values: `tf.Tensor` containing the values to be scaled and added to the sampled dot product.\n+    dense_shape: `tf.Tensor` defining the dense shape of the output.\n+    mat1: `tf.Tensor` to be multiplied. Must have rank > 1.\n+    mat2: `tf.Tensor` to be multiplied. Must have rank > 1.\n+    beta: Number to be multipled with `values`. Defaults to 1.0.\n+    alpha: Number to be multiplied with the sampled dot product of `mat1` and `mat2`.\n+      Defaults to 1.0.\n+    output_type: the output datatype if needed. Defaults to float32.\n+\n+  Returns:\n+    Three `tf.Tensor` objects making up the components of a `SparseTensor` representing the result\n+    of the operation.\n+\n+    output = `alpha` * (`mat1` @ `mat2`) * spy(`indices`) + `beta` * `values`\n+\n+    The function `spy(indices)` is the sparsity pattern matrix derived from `indices`. \n+  \"\"\"\n+  if context.executing_eagerly():\n+    if not isinstance(indices, ops.EagerTensor):\n+      indices = ops.convert_to_tensor(indices)\n+    if not isinstance(values, ops.EagerTensor):\n+      values = ops.convert_to_tensor(values)\n+    if not isinstance(mat1, ops.EagerTensor):\n+      mat1 = ops.convert_to_tensor(mat1)\n+    if not isinstance(mat2, ops.EagerTensor):\n+      mat2 = ops.convert_to_tensor(mat2)\n+  else:\n+    if not isinstance(indices, tensor_lib.Tensor):\n+      indices = ops.convert_to_tensor(indices)\n+    if not isinstance(values, tensor_lib.Tensor):\n+      values = ops.convert_to_tensor(values)\n+    if not isinstance(mat1, tensor_lib.Tensor):\n+      mat1 = ops.convert_to_tensor(mat1)\n+    if not isinstance(mat2, tensor_lib.Tensor):\n+      mat2 = ops.convert_to_tensor(mat2)\n+\n+  if values.dtype != output_type:\n+    values = cast(values, output_type)\n+  if mat1.dtype != output_type:\n+    mat1 = cast(mat1, output_type)\n+  if mat2.dtype != output_type:\n+    mat2 = cast(mat2, output_type)\n+\n+  dense_rows = mat1.shape[-2]\n+  dense_cols = mat2.shape[-1]\n+\n+  # TODO(mattbahr): use dense_shape to validate the shapes of mat1 and mat2\n+  dense_shape = constant_op.constant([dense_rows, dense_cols])",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1443932792",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 62750,
        "pr_file": "tensorflow/python/ops/math_ops.py",
        "discussion_id": "1443932792",
        "commented_code": "@@ -4857,7 +4857,177 @@ def sparse_segment_sum(\n         name=name,\n     )\n \n+@tf_export(\"sparse.sampled_addmm\")\n+def sampled_addmm(indices,\n+      values,\n+      dense_shape,\n+      mat1,\n+      mat2,\n+      beta=1.0,\n+      alpha=1.0,\n+      output_type=dtypes.float32\n+):\n+  \"\"\"Multiplies matrix `mat1` by matrix `mat2` at the locations defined by `indices`.\n+  The product is scaled and added to `values`, \n+  producing `alpha` * (`mat1` @ `mat2`) * spy(`indices`) + `beta` * `values`\n+\n+  The function `spy(indices)` is the sparsity pattern matrix derived from `indices`.\n+\n+  The `mat1` and `mat2` inputs must be tensors of rank >= 2 where the inner 2\n+  dimensions specify valid matrix multiplication dimensions, and any further dimensions\n+  specify matching batch size.\n+\n+  The `indices`, `values`, and `dense_shape` inputs make up the components of a \n+  `SparseTensor` which defines the sparsity pattern of the output. The sparsity pattern\n+  has values of 1 at the positions defined by the `SparseTensor`, and 0 elsewhere.\n+\n+  The `alpha` and `beta` inputs are the scaling factors.\n+\n+  The supported types for `values`, `mat1`, and `mat2` are:\n+  `bfloat16`, `float16`, `float32`, `float64`.\n+\n+  A simple 2-D tensor operation:\n+\n+  >>> indices = tf.constant([0, 0, 1, 1], shape=[2, 2])\n+  >>> indices\n+  <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[0, 0],\n+         [1, 1]], dtype=int32)>\n+  >>> values = tf.constant([0.5, 0.3])\n+  >>> values\n+  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5, 0.3], dtype=float32)>\n+  >>> dense_shape = tf.constant([2, 2])\n+  >>> dense_shape\n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>\n+  >>> mat1 = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n+  >>> mat1\n+  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n+  array([[1, 2, 3],\n+         [4, 5, 6]], dtype=int32)>\n+  >>> mat2 = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n+  >>> mat2\n+  <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n+  array([[ 7,  8],\n+         [ 9, 10],\n+         [11, 12]], dtype=int32)>\n+  >>> tf.sparse.sampled_addmm(indices, values, dense_shape, mat1, mat2, alpha=0.75, beta=0.25)\n+  (<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[0, 0],\n+         [1, 1]], dtype=int32)>, \n+  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 43.625, 115.575], dtype=float32)>, \n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>)\n \n+  A batch operation:\n+\n+  >>> indices = tf.constant([0, 1, 1, 0, 0, 0, 1, 0], shape=[2, 2, 2])\n+  >>> indices\n+  <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n+  array([[[0, 1],\n+          [1, 0]],\n+         [[0, 0],\n+          [1, 0]]], dtype=int32)>\n+  >>> values = tf.constant([3, 5, 2, 7], shape=[2, 2])\n+  >>> values\n+  <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[3, 5],\n+         [2, 7]], dtype=int32)>\n+  >>> dense_shape = tf.constant([2, 2])\n+  >>> dense_shape\n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>\n+  >>> mat1 = tf.constant(np.arange(1, 13), shape=[2, 2, 3])\n+  >>> mat1\n+  <tf.Tensor: shape=(2, 2, 3), dtype=int64, numpy=\n+  array([[[ 1,  2,  3],\n+          [ 4,  5,  6]],\n+         [[ 7,  8,  9],\n+          [10, 11, 12]]])>\n+  >>> mat2 = tf.constant(np.arange(13, 25), shape=[2, 3, 2])\n+  >>> mat2\n+  <tf.Tensor: shape=(2, 3, 2), dtype=int64, numpy=\n+  array([[[13, 14],\n+          [15, 16],\n+          [17, 18]],\n+         [[19, 20],\n+          [21, 22],\n+          [23, 24]]])>\n+  >>> tf.sparse.sampled_addmm(indices, values, dense_shape, mat1, mat2, alpha=0.75, beta=0.25)\n+  (<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n+  array([[[0, 1],\n+          [1, 0]],\n+         [[0, 0],\n+          [1, 0]]], dtype=int32)>, <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n+  array([[ 75.75, 173.  ],\n+         [381.5 , 524.5 ]], dtype=float32)>, \n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>)\n+\n+  Args:\n+    indices: `tf.Tensor` containing coordinates for the rows and columns to be multiplied.\n+      Must have rank > 1.\n+    values: `tf.Tensor` containing the values to be scaled and added to the sampled dot product.\n+    dense_shape: `tf.Tensor` defining the dense shape of the output.\n+    mat1: `tf.Tensor` to be multiplied. Must have rank > 1.\n+    mat2: `tf.Tensor` to be multiplied. Must have rank > 1.\n+    beta: Number to be multipled with `values`. Defaults to 1.0.\n+    alpha: Number to be multiplied with the sampled dot product of `mat1` and `mat2`.\n+      Defaults to 1.0.\n+    output_type: the output datatype if needed. Defaults to float32.\n+\n+  Returns:\n+    Three `tf.Tensor` objects making up the components of a `SparseTensor` representing the result\n+    of the operation.\n+\n+    output = `alpha` * (`mat1` @ `mat2`) * spy(`indices`) + `beta` * `values`\n+\n+    The function `spy(indices)` is the sparsity pattern matrix derived from `indices`. \n+  \"\"\"\n+  if context.executing_eagerly():\n+    if not isinstance(indices, ops.EagerTensor):\n+      indices = ops.convert_to_tensor(indices)\n+    if not isinstance(values, ops.EagerTensor):\n+      values = ops.convert_to_tensor(values)\n+    if not isinstance(mat1, ops.EagerTensor):\n+      mat1 = ops.convert_to_tensor(mat1)\n+    if not isinstance(mat2, ops.EagerTensor):\n+      mat2 = ops.convert_to_tensor(mat2)\n+  else:\n+    if not isinstance(indices, tensor_lib.Tensor):\n+      indices = ops.convert_to_tensor(indices)\n+    if not isinstance(values, tensor_lib.Tensor):\n+      values = ops.convert_to_tensor(values)\n+    if not isinstance(mat1, tensor_lib.Tensor):\n+      mat1 = ops.convert_to_tensor(mat1)\n+    if not isinstance(mat2, tensor_lib.Tensor):\n+      mat2 = ops.convert_to_tensor(mat2)\n+\n+  if values.dtype != output_type:\n+    values = cast(values, output_type)\n+  if mat1.dtype != output_type:\n+    mat1 = cast(mat1, output_type)\n+  if mat2.dtype != output_type:\n+    mat2 = cast(mat2, output_type)\n+\n+  dense_rows = mat1.shape[-2]\n+  dense_cols = mat2.shape[-1]\n+\n+  # TODO(mattbahr): use dense_shape to validate the shapes of mat1 and mat2\n+  dense_shape = constant_op.constant([dense_rows, dense_cols])",
        "comment_created_at": "2024-01-07T05:19:44+00:00",
        "comment_author": "mattbahr",
        "comment_body": "Having issues accessing the values in dense_shape. Everything I've tried causes my tests to fail in graph mode.",
        "pr_file_module": null
      },
      {
        "comment_id": "1445237791",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 62750,
        "pr_file": "tensorflow/python/ops/math_ops.py",
        "discussion_id": "1443932792",
        "commented_code": "@@ -4857,7 +4857,177 @@ def sparse_segment_sum(\n         name=name,\n     )\n \n+@tf_export(\"sparse.sampled_addmm\")\n+def sampled_addmm(indices,\n+      values,\n+      dense_shape,\n+      mat1,\n+      mat2,\n+      beta=1.0,\n+      alpha=1.0,\n+      output_type=dtypes.float32\n+):\n+  \"\"\"Multiplies matrix `mat1` by matrix `mat2` at the locations defined by `indices`.\n+  The product is scaled and added to `values`, \n+  producing `alpha` * (`mat1` @ `mat2`) * spy(`indices`) + `beta` * `values`\n+\n+  The function `spy(indices)` is the sparsity pattern matrix derived from `indices`.\n+\n+  The `mat1` and `mat2` inputs must be tensors of rank >= 2 where the inner 2\n+  dimensions specify valid matrix multiplication dimensions, and any further dimensions\n+  specify matching batch size.\n+\n+  The `indices`, `values`, and `dense_shape` inputs make up the components of a \n+  `SparseTensor` which defines the sparsity pattern of the output. The sparsity pattern\n+  has values of 1 at the positions defined by the `SparseTensor`, and 0 elsewhere.\n+\n+  The `alpha` and `beta` inputs are the scaling factors.\n+\n+  The supported types for `values`, `mat1`, and `mat2` are:\n+  `bfloat16`, `float16`, `float32`, `float64`.\n+\n+  A simple 2-D tensor operation:\n+\n+  >>> indices = tf.constant([0, 0, 1, 1], shape=[2, 2])\n+  >>> indices\n+  <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[0, 0],\n+         [1, 1]], dtype=int32)>\n+  >>> values = tf.constant([0.5, 0.3])\n+  >>> values\n+  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5, 0.3], dtype=float32)>\n+  >>> dense_shape = tf.constant([2, 2])\n+  >>> dense_shape\n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>\n+  >>> mat1 = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n+  >>> mat1\n+  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n+  array([[1, 2, 3],\n+         [4, 5, 6]], dtype=int32)>\n+  >>> mat2 = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n+  >>> mat2\n+  <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n+  array([[ 7,  8],\n+         [ 9, 10],\n+         [11, 12]], dtype=int32)>\n+  >>> tf.sparse.sampled_addmm(indices, values, dense_shape, mat1, mat2, alpha=0.75, beta=0.25)\n+  (<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[0, 0],\n+         [1, 1]], dtype=int32)>, \n+  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 43.625, 115.575], dtype=float32)>, \n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>)\n \n+  A batch operation:\n+\n+  >>> indices = tf.constant([0, 1, 1, 0, 0, 0, 1, 0], shape=[2, 2, 2])\n+  >>> indices\n+  <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n+  array([[[0, 1],\n+          [1, 0]],\n+         [[0, 0],\n+          [1, 0]]], dtype=int32)>\n+  >>> values = tf.constant([3, 5, 2, 7], shape=[2, 2])\n+  >>> values\n+  <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[3, 5],\n+         [2, 7]], dtype=int32)>\n+  >>> dense_shape = tf.constant([2, 2])\n+  >>> dense_shape\n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>\n+  >>> mat1 = tf.constant(np.arange(1, 13), shape=[2, 2, 3])\n+  >>> mat1\n+  <tf.Tensor: shape=(2, 2, 3), dtype=int64, numpy=\n+  array([[[ 1,  2,  3],\n+          [ 4,  5,  6]],\n+         [[ 7,  8,  9],\n+          [10, 11, 12]]])>\n+  >>> mat2 = tf.constant(np.arange(13, 25), shape=[2, 3, 2])\n+  >>> mat2\n+  <tf.Tensor: shape=(2, 3, 2), dtype=int64, numpy=\n+  array([[[13, 14],\n+          [15, 16],\n+          [17, 18]],\n+         [[19, 20],\n+          [21, 22],\n+          [23, 24]]])>\n+  >>> tf.sparse.sampled_addmm(indices, values, dense_shape, mat1, mat2, alpha=0.75, beta=0.25)\n+  (<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n+  array([[[0, 1],\n+          [1, 0]],\n+         [[0, 0],\n+          [1, 0]]], dtype=int32)>, <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n+  array([[ 75.75, 173.  ],\n+         [381.5 , 524.5 ]], dtype=float32)>, \n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>)\n+\n+  Args:\n+    indices: `tf.Tensor` containing coordinates for the rows and columns to be multiplied.\n+      Must have rank > 1.\n+    values: `tf.Tensor` containing the values to be scaled and added to the sampled dot product.\n+    dense_shape: `tf.Tensor` defining the dense shape of the output.\n+    mat1: `tf.Tensor` to be multiplied. Must have rank > 1.\n+    mat2: `tf.Tensor` to be multiplied. Must have rank > 1.\n+    beta: Number to be multipled with `values`. Defaults to 1.0.\n+    alpha: Number to be multiplied with the sampled dot product of `mat1` and `mat2`.\n+      Defaults to 1.0.\n+    output_type: the output datatype if needed. Defaults to float32.\n+\n+  Returns:\n+    Three `tf.Tensor` objects making up the components of a `SparseTensor` representing the result\n+    of the operation.\n+\n+    output = `alpha` * (`mat1` @ `mat2`) * spy(`indices`) + `beta` * `values`\n+\n+    The function `spy(indices)` is the sparsity pattern matrix derived from `indices`. \n+  \"\"\"\n+  if context.executing_eagerly():\n+    if not isinstance(indices, ops.EagerTensor):\n+      indices = ops.convert_to_tensor(indices)\n+    if not isinstance(values, ops.EagerTensor):\n+      values = ops.convert_to_tensor(values)\n+    if not isinstance(mat1, ops.EagerTensor):\n+      mat1 = ops.convert_to_tensor(mat1)\n+    if not isinstance(mat2, ops.EagerTensor):\n+      mat2 = ops.convert_to_tensor(mat2)\n+  else:\n+    if not isinstance(indices, tensor_lib.Tensor):\n+      indices = ops.convert_to_tensor(indices)\n+    if not isinstance(values, tensor_lib.Tensor):\n+      values = ops.convert_to_tensor(values)\n+    if not isinstance(mat1, tensor_lib.Tensor):\n+      mat1 = ops.convert_to_tensor(mat1)\n+    if not isinstance(mat2, tensor_lib.Tensor):\n+      mat2 = ops.convert_to_tensor(mat2)\n+\n+  if values.dtype != output_type:\n+    values = cast(values, output_type)\n+  if mat1.dtype != output_type:\n+    mat1 = cast(mat1, output_type)\n+  if mat2.dtype != output_type:\n+    mat2 = cast(mat2, output_type)\n+\n+  dense_rows = mat1.shape[-2]\n+  dense_cols = mat2.shape[-1]\n+\n+  # TODO(mattbahr): use dense_shape to validate the shapes of mat1 and mat2\n+  dense_shape = constant_op.constant([dense_rows, dense_cols])",
        "comment_created_at": "2024-01-08T19:33:35+00:00",
        "comment_author": "cantonios",
        "comment_body": "Correct, calling any shape-related attribute on the tensor object might fail if the shape isn't fixed at graph-compile time (e.g. `.shape`, `.ndim`).  These will likely return `None` in graph mode.  You need to call specific tf functions to access them:\r\n```\r\ndense_rows = tf.shape(mat1, out_type=dense_shape.dtype)[-2]  # will return a 0-d tensor object\r\n```\r\nfor the assert, you could use something like\r\n```\r\ncheck_ops.assert_equal_v2(dense_shape[-2], dense_rows)\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1468364264",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 62750,
        "pr_file": "tensorflow/python/ops/math_ops.py",
        "discussion_id": "1443932792",
        "commented_code": "@@ -4857,7 +4857,177 @@ def sparse_segment_sum(\n         name=name,\n     )\n \n+@tf_export(\"sparse.sampled_addmm\")\n+def sampled_addmm(indices,\n+      values,\n+      dense_shape,\n+      mat1,\n+      mat2,\n+      beta=1.0,\n+      alpha=1.0,\n+      output_type=dtypes.float32\n+):\n+  \"\"\"Multiplies matrix `mat1` by matrix `mat2` at the locations defined by `indices`.\n+  The product is scaled and added to `values`, \n+  producing `alpha` * (`mat1` @ `mat2`) * spy(`indices`) + `beta` * `values`\n+\n+  The function `spy(indices)` is the sparsity pattern matrix derived from `indices`.\n+\n+  The `mat1` and `mat2` inputs must be tensors of rank >= 2 where the inner 2\n+  dimensions specify valid matrix multiplication dimensions, and any further dimensions\n+  specify matching batch size.\n+\n+  The `indices`, `values`, and `dense_shape` inputs make up the components of a \n+  `SparseTensor` which defines the sparsity pattern of the output. The sparsity pattern\n+  has values of 1 at the positions defined by the `SparseTensor`, and 0 elsewhere.\n+\n+  The `alpha` and `beta` inputs are the scaling factors.\n+\n+  The supported types for `values`, `mat1`, and `mat2` are:\n+  `bfloat16`, `float16`, `float32`, `float64`.\n+\n+  A simple 2-D tensor operation:\n+\n+  >>> indices = tf.constant([0, 0, 1, 1], shape=[2, 2])\n+  >>> indices\n+  <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[0, 0],\n+         [1, 1]], dtype=int32)>\n+  >>> values = tf.constant([0.5, 0.3])\n+  >>> values\n+  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5, 0.3], dtype=float32)>\n+  >>> dense_shape = tf.constant([2, 2])\n+  >>> dense_shape\n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>\n+  >>> mat1 = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n+  >>> mat1\n+  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n+  array([[1, 2, 3],\n+         [4, 5, 6]], dtype=int32)>\n+  >>> mat2 = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n+  >>> mat2\n+  <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n+  array([[ 7,  8],\n+         [ 9, 10],\n+         [11, 12]], dtype=int32)>\n+  >>> tf.sparse.sampled_addmm(indices, values, dense_shape, mat1, mat2, alpha=0.75, beta=0.25)\n+  (<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[0, 0],\n+         [1, 1]], dtype=int32)>, \n+  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 43.625, 115.575], dtype=float32)>, \n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>)\n \n+  A batch operation:\n+\n+  >>> indices = tf.constant([0, 1, 1, 0, 0, 0, 1, 0], shape=[2, 2, 2])\n+  >>> indices\n+  <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n+  array([[[0, 1],\n+          [1, 0]],\n+         [[0, 0],\n+          [1, 0]]], dtype=int32)>\n+  >>> values = tf.constant([3, 5, 2, 7], shape=[2, 2])\n+  >>> values\n+  <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[3, 5],\n+         [2, 7]], dtype=int32)>\n+  >>> dense_shape = tf.constant([2, 2])\n+  >>> dense_shape\n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>\n+  >>> mat1 = tf.constant(np.arange(1, 13), shape=[2, 2, 3])\n+  >>> mat1\n+  <tf.Tensor: shape=(2, 2, 3), dtype=int64, numpy=\n+  array([[[ 1,  2,  3],\n+          [ 4,  5,  6]],\n+         [[ 7,  8,  9],\n+          [10, 11, 12]]])>\n+  >>> mat2 = tf.constant(np.arange(13, 25), shape=[2, 3, 2])\n+  >>> mat2\n+  <tf.Tensor: shape=(2, 3, 2), dtype=int64, numpy=\n+  array([[[13, 14],\n+          [15, 16],\n+          [17, 18]],\n+         [[19, 20],\n+          [21, 22],\n+          [23, 24]]])>\n+  >>> tf.sparse.sampled_addmm(indices, values, dense_shape, mat1, mat2, alpha=0.75, beta=0.25)\n+  (<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n+  array([[[0, 1],\n+          [1, 0]],\n+         [[0, 0],\n+          [1, 0]]], dtype=int32)>, <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n+  array([[ 75.75, 173.  ],\n+         [381.5 , 524.5 ]], dtype=float32)>, \n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>)\n+\n+  Args:\n+    indices: `tf.Tensor` containing coordinates for the rows and columns to be multiplied.\n+      Must have rank > 1.\n+    values: `tf.Tensor` containing the values to be scaled and added to the sampled dot product.\n+    dense_shape: `tf.Tensor` defining the dense shape of the output.\n+    mat1: `tf.Tensor` to be multiplied. Must have rank > 1.\n+    mat2: `tf.Tensor` to be multiplied. Must have rank > 1.\n+    beta: Number to be multipled with `values`. Defaults to 1.0.\n+    alpha: Number to be multiplied with the sampled dot product of `mat1` and `mat2`.\n+      Defaults to 1.0.\n+    output_type: the output datatype if needed. Defaults to float32.\n+\n+  Returns:\n+    Three `tf.Tensor` objects making up the components of a `SparseTensor` representing the result\n+    of the operation.\n+\n+    output = `alpha` * (`mat1` @ `mat2`) * spy(`indices`) + `beta` * `values`\n+\n+    The function `spy(indices)` is the sparsity pattern matrix derived from `indices`. \n+  \"\"\"\n+  if context.executing_eagerly():\n+    if not isinstance(indices, ops.EagerTensor):\n+      indices = ops.convert_to_tensor(indices)\n+    if not isinstance(values, ops.EagerTensor):\n+      values = ops.convert_to_tensor(values)\n+    if not isinstance(mat1, ops.EagerTensor):\n+      mat1 = ops.convert_to_tensor(mat1)\n+    if not isinstance(mat2, ops.EagerTensor):\n+      mat2 = ops.convert_to_tensor(mat2)\n+  else:\n+    if not isinstance(indices, tensor_lib.Tensor):\n+      indices = ops.convert_to_tensor(indices)\n+    if not isinstance(values, tensor_lib.Tensor):\n+      values = ops.convert_to_tensor(values)\n+    if not isinstance(mat1, tensor_lib.Tensor):\n+      mat1 = ops.convert_to_tensor(mat1)\n+    if not isinstance(mat2, tensor_lib.Tensor):\n+      mat2 = ops.convert_to_tensor(mat2)\n+\n+  if values.dtype != output_type:\n+    values = cast(values, output_type)\n+  if mat1.dtype != output_type:\n+    mat1 = cast(mat1, output_type)\n+  if mat2.dtype != output_type:\n+    mat2 = cast(mat2, output_type)\n+\n+  dense_rows = mat1.shape[-2]\n+  dense_cols = mat2.shape[-1]\n+\n+  # TODO(mattbahr): use dense_shape to validate the shapes of mat1 and mat2\n+  dense_shape = constant_op.constant([dense_rows, dense_cols])",
        "comment_created_at": "2024-01-27T06:39:03+00:00",
        "comment_author": "mattbahr",
        "comment_body": "Importing `check_ops` introduces a circular dependency so I don't think we'll be able to handle the assertion that way. \r\n\r\nI tried using a regular `assert`, but I keep running into issues with the values not being fixed:\r\n```\r\nassert dense_shape[-2] == array_ops.shape(mat1, out_type=dense_shape.dtype)[-2]\r\n```\r\nI've also tried using `tf.equal` and `tf.reduce_all`, but have the same problems with that.",
        "pr_file_module": null
      },
      {
        "comment_id": "1476740096",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 62750,
        "pr_file": "tensorflow/python/ops/math_ops.py",
        "discussion_id": "1443932792",
        "commented_code": "@@ -4857,7 +4857,177 @@ def sparse_segment_sum(\n         name=name,\n     )\n \n+@tf_export(\"sparse.sampled_addmm\")\n+def sampled_addmm(indices,\n+      values,\n+      dense_shape,\n+      mat1,\n+      mat2,\n+      beta=1.0,\n+      alpha=1.0,\n+      output_type=dtypes.float32\n+):\n+  \"\"\"Multiplies matrix `mat1` by matrix `mat2` at the locations defined by `indices`.\n+  The product is scaled and added to `values`, \n+  producing `alpha` * (`mat1` @ `mat2`) * spy(`indices`) + `beta` * `values`\n+\n+  The function `spy(indices)` is the sparsity pattern matrix derived from `indices`.\n+\n+  The `mat1` and `mat2` inputs must be tensors of rank >= 2 where the inner 2\n+  dimensions specify valid matrix multiplication dimensions, and any further dimensions\n+  specify matching batch size.\n+\n+  The `indices`, `values`, and `dense_shape` inputs make up the components of a \n+  `SparseTensor` which defines the sparsity pattern of the output. The sparsity pattern\n+  has values of 1 at the positions defined by the `SparseTensor`, and 0 elsewhere.\n+\n+  The `alpha` and `beta` inputs are the scaling factors.\n+\n+  The supported types for `values`, `mat1`, and `mat2` are:\n+  `bfloat16`, `float16`, `float32`, `float64`.\n+\n+  A simple 2-D tensor operation:\n+\n+  >>> indices = tf.constant([0, 0, 1, 1], shape=[2, 2])\n+  >>> indices\n+  <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[0, 0],\n+         [1, 1]], dtype=int32)>\n+  >>> values = tf.constant([0.5, 0.3])\n+  >>> values\n+  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5, 0.3], dtype=float32)>\n+  >>> dense_shape = tf.constant([2, 2])\n+  >>> dense_shape\n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>\n+  >>> mat1 = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n+  >>> mat1\n+  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n+  array([[1, 2, 3],\n+         [4, 5, 6]], dtype=int32)>\n+  >>> mat2 = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n+  >>> mat2\n+  <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n+  array([[ 7,  8],\n+         [ 9, 10],\n+         [11, 12]], dtype=int32)>\n+  >>> tf.sparse.sampled_addmm(indices, values, dense_shape, mat1, mat2, alpha=0.75, beta=0.25)\n+  (<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[0, 0],\n+         [1, 1]], dtype=int32)>, \n+  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 43.625, 115.575], dtype=float32)>, \n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>)\n \n+  A batch operation:\n+\n+  >>> indices = tf.constant([0, 1, 1, 0, 0, 0, 1, 0], shape=[2, 2, 2])\n+  >>> indices\n+  <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n+  array([[[0, 1],\n+          [1, 0]],\n+         [[0, 0],\n+          [1, 0]]], dtype=int32)>\n+  >>> values = tf.constant([3, 5, 2, 7], shape=[2, 2])\n+  >>> values\n+  <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n+  array([[3, 5],\n+         [2, 7]], dtype=int32)>\n+  >>> dense_shape = tf.constant([2, 2])\n+  >>> dense_shape\n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>\n+  >>> mat1 = tf.constant(np.arange(1, 13), shape=[2, 2, 3])\n+  >>> mat1\n+  <tf.Tensor: shape=(2, 2, 3), dtype=int64, numpy=\n+  array([[[ 1,  2,  3],\n+          [ 4,  5,  6]],\n+         [[ 7,  8,  9],\n+          [10, 11, 12]]])>\n+  >>> mat2 = tf.constant(np.arange(13, 25), shape=[2, 3, 2])\n+  >>> mat2\n+  <tf.Tensor: shape=(2, 3, 2), dtype=int64, numpy=\n+  array([[[13, 14],\n+          [15, 16],\n+          [17, 18]],\n+         [[19, 20],\n+          [21, 22],\n+          [23, 24]]])>\n+  >>> tf.sparse.sampled_addmm(indices, values, dense_shape, mat1, mat2, alpha=0.75, beta=0.25)\n+  (<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n+  array([[[0, 1],\n+          [1, 0]],\n+         [[0, 0],\n+          [1, 0]]], dtype=int32)>, <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n+  array([[ 75.75, 173.  ],\n+         [381.5 , 524.5 ]], dtype=float32)>, \n+  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>)\n+\n+  Args:\n+    indices: `tf.Tensor` containing coordinates for the rows and columns to be multiplied.\n+      Must have rank > 1.\n+    values: `tf.Tensor` containing the values to be scaled and added to the sampled dot product.\n+    dense_shape: `tf.Tensor` defining the dense shape of the output.\n+    mat1: `tf.Tensor` to be multiplied. Must have rank > 1.\n+    mat2: `tf.Tensor` to be multiplied. Must have rank > 1.\n+    beta: Number to be multipled with `values`. Defaults to 1.0.\n+    alpha: Number to be multiplied with the sampled dot product of `mat1` and `mat2`.\n+      Defaults to 1.0.\n+    output_type: the output datatype if needed. Defaults to float32.\n+\n+  Returns:\n+    Three `tf.Tensor` objects making up the components of a `SparseTensor` representing the result\n+    of the operation.\n+\n+    output = `alpha` * (`mat1` @ `mat2`) * spy(`indices`) + `beta` * `values`\n+\n+    The function `spy(indices)` is the sparsity pattern matrix derived from `indices`. \n+  \"\"\"\n+  if context.executing_eagerly():\n+    if not isinstance(indices, ops.EagerTensor):\n+      indices = ops.convert_to_tensor(indices)\n+    if not isinstance(values, ops.EagerTensor):\n+      values = ops.convert_to_tensor(values)\n+    if not isinstance(mat1, ops.EagerTensor):\n+      mat1 = ops.convert_to_tensor(mat1)\n+    if not isinstance(mat2, ops.EagerTensor):\n+      mat2 = ops.convert_to_tensor(mat2)\n+  else:\n+    if not isinstance(indices, tensor_lib.Tensor):\n+      indices = ops.convert_to_tensor(indices)\n+    if not isinstance(values, tensor_lib.Tensor):\n+      values = ops.convert_to_tensor(values)\n+    if not isinstance(mat1, tensor_lib.Tensor):\n+      mat1 = ops.convert_to_tensor(mat1)\n+    if not isinstance(mat2, tensor_lib.Tensor):\n+      mat2 = ops.convert_to_tensor(mat2)\n+\n+  if values.dtype != output_type:\n+    values = cast(values, output_type)\n+  if mat1.dtype != output_type:\n+    mat1 = cast(mat1, output_type)\n+  if mat2.dtype != output_type:\n+    mat2 = cast(mat2, output_type)\n+\n+  dense_rows = mat1.shape[-2]\n+  dense_cols = mat2.shape[-1]\n+\n+  # TODO(mattbahr): use dense_shape to validate the shapes of mat1 and mat2\n+  dense_shape = constant_op.constant([dense_rows, dense_cols])",
        "comment_created_at": "2024-02-02T21:18:07+00:00",
        "comment_author": "mattbahr",
        "comment_body": "I ended up implementing the check myself and using `gen_logging_ops`. Added a new test case as well and all seems to work ok. If you think this is too much then let me know, and I can remove it. I'm wondering if maybe I'm putting logic in this file that shouldn't be there. For now I'm going to mark this convo resolved.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1385203492",
    "pr_number": 62321,
    "pr_file": "tensorflow/python/ops/nn_impl.py",
    "created_at": "2023-11-07T16:24:20+00:00",
    "commented_code": "filter: 4-D with shape\n       `[filter_height, filter_width, in_channels, channel_multiplier]`.\n     strides: 1-D of size 4.  The stride of the sliding window for each\n-      dimension of `input`. Must have `strides[0] = strides[3] = 1`, and for",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1385203492",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 62321,
        "pr_file": "tensorflow/python/ops/nn_impl.py",
        "discussion_id": "1385203492",
        "commented_code": "@@ -856,9 +856,7 @@ def depthwise_conv2d_v2(input,\n     filter: 4-D with shape\n       `[filter_height, filter_width, in_channels, channel_multiplier]`.\n     strides: 1-D of size 4.  The stride of the sliding window for each\n-      dimension of `input`. Must have `strides[0] = strides[3] = 1`, and for",
        "comment_created_at": "2023-11-07T16:24:20+00:00",
        "comment_author": "cantonios",
        "comment_body": "You'll need to re-add something here, but you need to explicitly mention that _only_ equal strides are supported (I'm guessing only if `strides[0]` and `strides[3]` are 1?).  Do some testing to confirm, and fix where it's mentioned above as well.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1440990001",
    "pr_number": 62731,
    "pr_file": "tensorflow/python/ops/math_ops.py",
    "created_at": "2024-01-03T21:46:41+00:00",
    "commented_code": "name=name,\n     )\n \n+@tf_export(\"sparse.sampled_addmm\")\n+def sampled_addmm(indices,\n+    values,\n+    dense_shape,\n+    mat1,\n+    mat2,\n+    beta=1.0,\n+    alpha=1.0\n+):\n+  sparse_rank = len(dense_shape)\n+  sparse_num_batches = dense_shape[0] if sparse_rank == 3 else 1\n+  sparse_num_rows = dense_shape[0 if sparse_rank == 2 else 1]\n+  sparse_num_cols = dense_shape[1 if sparse_rank == 2 else 2]\n+\n+  values_shape = values.get_shape()\n+  values_shape_list = values_shape.as_list()\n+  indices_shape = indices.get_shape()\n+  indices_shape_list = indices_shape.as_list()\n+\n+  mat1_shape = mat1.get_shape()\n+  mat1_shape_list = mat1_shape.as_list()\n+  mat2_shape = mat2.get_shape()\n+  mat_rank = mat1_shape.ndims\n+\n+  mat_num_batches = mat1_shape_list[0] if mat_rank == 3 else 1\n+  mat1_num_rows = mat1_shape_list[0 if mat_rank == 2 else 1]\n+  mat2_num_cols = mat2_shape.as_list()[1 if mat_rank == 2 else 2]\n+\n+  if values_shape.ndims != sparse_rank - 1:",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1440990001",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 62731,
        "pr_file": "tensorflow/python/ops/math_ops.py",
        "discussion_id": "1440990001",
        "commented_code": "@@ -4857,6 +4857,68 @@ def sparse_segment_sum(\n         name=name,\n     )\n \n+@tf_export(\"sparse.sampled_addmm\")\n+def sampled_addmm(indices,\n+    values,\n+    dense_shape,\n+    mat1,\n+    mat2,\n+    beta=1.0,\n+    alpha=1.0\n+):\n+  sparse_rank = len(dense_shape)\n+  sparse_num_batches = dense_shape[0] if sparse_rank == 3 else 1\n+  sparse_num_rows = dense_shape[0 if sparse_rank == 2 else 1]\n+  sparse_num_cols = dense_shape[1 if sparse_rank == 2 else 2]\n+\n+  values_shape = values.get_shape()\n+  values_shape_list = values_shape.as_list()\n+  indices_shape = indices.get_shape()\n+  indices_shape_list = indices_shape.as_list()\n+\n+  mat1_shape = mat1.get_shape()\n+  mat1_shape_list = mat1_shape.as_list()\n+  mat2_shape = mat2.get_shape()\n+  mat_rank = mat1_shape.ndims\n+\n+  mat_num_batches = mat1_shape_list[0] if mat_rank == 3 else 1\n+  mat1_num_rows = mat1_shape_list[0 if mat_rank == 2 else 1]\n+  mat2_num_cols = mat2_shape.as_list()[1 if mat_rank == 2 else 2]\n+\n+  if values_shape.ndims != sparse_rank - 1:",
        "comment_created_at": "2024-01-03T21:46:41+00:00",
        "comment_author": "cantonios",
        "comment_body": "These shape validation checks are probably best done by the kernel itself, or its shape function when registering the op (if ranks are known).  You could also let the kernel itself determine the number of batch dimensions, etc.\r\n\r\nThe python code in TF is mainly used to set up a graph of execution, and may not have full tensor size information at the time (e.g. if the compiled graph is meant to work for tensors of varying sizes).  But by the time the graph is executed and we enter the kernel, the full size of the inputs will be known.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1443350487",
    "pr_number": 62731,
    "pr_file": "tensorflow/python/ops/math_ops.py",
    "created_at": "2024-01-05T20:41:19+00:00",
    "commented_code": "name=name,\n     )\n \n+@tf_export(\"sparse.sampled_addmm\")\n+def sampled_addmm(indices,\n+    values,\n+    dense_shape,\n+    mat1,\n+    mat2,\n+    beta=1.0,\n+    alpha=1.0\n+):\n+#  res = gen_math_ops.sampled_addmm(\n+#      indices,\n+#      values,\n+#      dense_shape,\n+#      mat1,\n+#      mat2,\n+#      beta=beta,\n+#      alpha=alpha\n+#  )\n+  indices_dims = indices.get_shape().ndims",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1443350487",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 62731,
        "pr_file": "tensorflow/python/ops/math_ops.py",
        "discussion_id": "1443350487",
        "commented_code": "@@ -4857,6 +4857,42 @@ def sparse_segment_sum(\n         name=name,\n     )\n \n+@tf_export(\"sparse.sampled_addmm\")\n+def sampled_addmm(indices,\n+    values,\n+    dense_shape,\n+    mat1,\n+    mat2,\n+    beta=1.0,\n+    alpha=1.0\n+):\n+#  res = gen_math_ops.sampled_addmm(\n+#      indices,\n+#      values,\n+#      dense_shape,\n+#      mat1,\n+#      mat2,\n+#      beta=beta,\n+#      alpha=alpha\n+#  )\n+  indices_dims = indices.get_shape().ndims",
        "comment_created_at": "2024-01-05T20:41:19+00:00",
        "comment_author": "cantonios",
        "comment_body": "The main issue here and below is the use of numpy, when `indices_dims` may not be known at graph construction time (since it may come from a dynamic tensor, e.g. with dynamic number of batches).  So, you would need to use `tf.rank(...)` instead of `*.ndims`.  And you'll have trouble with creating slice indices using python array manipulation like that.\r\n\r\nI think this can be rewritten as:\r\n```python\r\ndef sampled_addmm(indices, values, dense_shape, mat_a, mat_b, beta=1.0, alpha=1.0):\r\n  batch_indices = indices[:, :-2]\r\n  row_indices = indices[:,:-1]\r\n  col_indices = tf.concat([batch_indices, indices[:, -1:]], axis=-1)\r\n  rows = tf.gather_nd(mat_a, row_indices)\r\n  cols = tf.gather_nd(tf.linalg.matrix_transpose(mat_b), col_indices)\r\n  dot = tf.reduce_sum(rows * cols, axis=-1)\r\n  return dot * alpha + values * beta\r\n```\r\nand a reference implementation for testing\r\n```python\r\ndef sampled_addmm_ref(indices, values, dense_shape, mat_a, mat_b, beta=1.0, alpha=1.0):\r\n  dense = tf.linalg.matmul(mat_a, mat_b)\r\n  dense_vals = tf.gather_nd(dense, indices)\r\n  return alpha * dense_vals + beta * values\r\n```\r\nThis _should_ work for arbitrary batch dimensions.",
        "pr_file_module": null
      },
      {
        "comment_id": "1443498870",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 62731,
        "pr_file": "tensorflow/python/ops/math_ops.py",
        "discussion_id": "1443350487",
        "commented_code": "@@ -4857,6 +4857,42 @@ def sparse_segment_sum(\n         name=name,\n     )\n \n+@tf_export(\"sparse.sampled_addmm\")\n+def sampled_addmm(indices,\n+    values,\n+    dense_shape,\n+    mat1,\n+    mat2,\n+    beta=1.0,\n+    alpha=1.0\n+):\n+#  res = gen_math_ops.sampled_addmm(\n+#      indices,\n+#      values,\n+#      dense_shape,\n+#      mat1,\n+#      mat2,\n+#      beta=beta,\n+#      alpha=alpha\n+#  )\n+  indices_dims = indices.get_shape().ndims",
        "comment_created_at": "2024-01-05T22:44:59+00:00",
        "comment_author": "mattbahr",
        "comment_body": "Ok, thanks for the explanation! I think I'll close this PR and cut a clean branch with this implementation",
        "pr_file_module": null
      }
    ]
  }
]