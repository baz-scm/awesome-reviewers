[
  {
    "discussion_id": "1960602993",
    "pr_number": 84932,
    "pr_file": "tensorflow/lite/experimental/litert/vendors/qualcomm/BUILD",
    "created_at": "2025-02-18T21:17:15+00:00",
    "commented_code": "default_visibility = [\"//tensorflow/lite/experimental/litert:__subpackages__\"],\n )\n \n+cc_library(\n+    name = \"QnnLiteRTDelegate\",",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1960602993",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 84932,
        "pr_file": "tensorflow/lite/experimental/litert/vendors/qualcomm/BUILD",
        "discussion_id": "1960602993",
        "commented_code": "@@ -20,6 +20,16 @@ package(\n     default_visibility = [\"//tensorflow/lite/experimental/litert:__subpackages__\"],\n )\n \n+cc_library(\n+    name = \"QnnLiteRTDelegate\",",
        "comment_created_at": "2025-02-18T21:17:15+00:00",
        "comment_author": "terryheo",
        "comment_body": "QnnLiteRtDelegate ?",
        "pr_file_module": null
      },
      {
        "comment_id": "1960837056",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 84932,
        "pr_file": "tensorflow/lite/experimental/litert/vendors/qualcomm/BUILD",
        "discussion_id": "1960602993",
        "commented_code": "@@ -20,6 +20,16 @@ package(\n     default_visibility = [\"//tensorflow/lite/experimental/litert:__subpackages__\"],\n )\n \n+cc_library(\n+    name = \"QnnLiteRTDelegate\",",
        "comment_created_at": "2025-02-19T01:55:56+00:00",
        "comment_author": "jiunkaiy",
        "comment_body": "Hi @terryheo \r\nDo you want the header file in qnn_litert_delegate, or does the term 'delegate' seem strange to you?\r\nWe can change `QnnLiteRtDelegate.h` to `litert_qnn.h` and also fix these names in `litert_qnn.h`.\r\n\r\n`TfLiteQnnDelegateLogLevel` -> `LiteRTQnnLogLevel`\r\n`TfLiteQnnDelegateHtpPerformanceMode` -> `LiteRTQnnHtpPerformanceMode`\r\n`TfLiteQnnDelegateHtpBackendOptions` -> `LiteRTQnnHtpBackendOptions`\r\n`kDispatchOptionQnnDelegateOptions`  -> `kDispatchOptionLiteRTQnnOptions`\r\n`TfLiteQnnDelegateOptions` -> `LiteRTQnnOptions`\r\n`QNN_DELEGATE_HTP_OPTION_INIT`-> `LITERT_QNN_HTP_OPTION_INIT`\r\n`QNN_DELEGATE_OPTION_INIT` -> `LITERT_QNN_HTP_OPTION_INIT`\r\n[tensorflow/lite/experimental/litert/vendors/qualcomm/QnnLiteRTDelegate.h](https://github.com/tensorflow/tensorflow/pull/84932/files#diff-0cefad55198abd42199b2ef665491b75271677ee9299ef9178c3cc12a25beda9)\r\n\r\nWhat do you think?  \ud83d\ude04 ",
        "pr_file_module": null
      },
      {
        "comment_id": "1968724026",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 84932,
        "pr_file": "tensorflow/lite/experimental/litert/vendors/qualcomm/BUILD",
        "discussion_id": "1960602993",
        "commented_code": "@@ -20,6 +20,16 @@ package(\n     default_visibility = [\"//tensorflow/lite/experimental/litert:__subpackages__\"],\n )\n \n+cc_library(\n+    name = \"QnnLiteRTDelegate\",",
        "comment_created_at": "2025-02-25T02:26:05+00:00",
        "comment_author": "terryheo",
        "comment_body": "I should have been more specific. Our naming standard for LiteRT is \"LiteRt\" not \"LiteRT\"  in C++. The second \"t\" should be lower case. It's the same as \"Qnn, Htp\" instead of \"QNN, HTP\"",
        "pr_file_module": null
      },
      {
        "comment_id": "1968729982",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 84932,
        "pr_file": "tensorflow/lite/experimental/litert/vendors/qualcomm/BUILD",
        "discussion_id": "1960602993",
        "commented_code": "@@ -20,6 +20,16 @@ package(\n     default_visibility = [\"//tensorflow/lite/experimental/litert:__subpackages__\"],\n )\n \n+cc_library(\n+    name = \"QnnLiteRTDelegate\",",
        "comment_created_at": "2025-02-25T02:33:00+00:00",
        "comment_author": "jiunkaiy",
        "comment_body": "Thanks I will change to LiteRt",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1227007497",
    "pr_number": 60701,
    "pr_file": "tensorflow/compiler/mlir/tosa/transforms/convert_tfl_uint8.cc",
    "created_at": "2023-06-12T17:23:33+00:00",
    "commented_code": "}\n };\n \n+namespace {\n+\n+// returns true iff @a type is a shaped type with element type that is uint8\n+// if it is, then return the rescaled type, uint8_zp, and output_zp to use to\n+// rescale type to signed type with adjusted zero point.\n+bool IsShapedUint8Type(OpBuilder& builder, const Type type, Type& rescaled_type,",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1227007497",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 60701,
        "pr_file": "tensorflow/compiler/mlir/tosa/transforms/convert_tfl_uint8.cc",
        "discussion_id": "1227007497",
        "commented_code": "@@ -130,6 +130,63 @@ struct ConvertUint8QConstOp : public RewritePattern {\n   }\n };\n \n+namespace {\n+\n+// returns true iff @a type is a shaped type with element type that is uint8\n+// if it is, then return the rescaled type, uint8_zp, and output_zp to use to\n+// rescale type to signed type with adjusted zero point.\n+bool IsShapedUint8Type(OpBuilder& builder, const Type type, Type& rescaled_type,",
        "comment_created_at": "2023-06-12T17:23:33+00:00",
        "comment_author": "rsuderman",
        "comment_body": "`Type` should be `ShapedType`. I would expect your lowering to require `ShapedType` for all of its lowering and would fail otherwise. Better to restrict the helper to only use expected cases.\r\n\r\nThe name is also bad, it should indicate that it is intended to extract type information. `IsShapedUint8Type` makes it seem like it only returns `true` or `false` when it actually extracts additional information.",
        "pr_file_module": null
      },
      {
        "comment_id": "1236208650",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 60701,
        "pr_file": "tensorflow/compiler/mlir/tosa/transforms/convert_tfl_uint8.cc",
        "discussion_id": "1227007497",
        "commented_code": "@@ -130,6 +130,63 @@ struct ConvertUint8QConstOp : public RewritePattern {\n   }\n };\n \n+namespace {\n+\n+// returns true iff @a type is a shaped type with element type that is uint8\n+// if it is, then return the rescaled type, uint8_zp, and output_zp to use to\n+// rescale type to signed type with adjusted zero point.\n+bool IsShapedUint8Type(OpBuilder& builder, const Type type, Type& rescaled_type,",
        "comment_created_at": "2023-06-21T02:33:13+00:00",
        "comment_author": "Tai78641",
        "comment_body": "fixed",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1764096688",
    "pr_number": 59528,
    "pr_file": "tensorflow/core/transforms/remapper/tests/contraction.mlir",
    "created_at": "2024-09-17T22:30:09+00:00",
    "commented_code": "%Tanh, %ctl_5 = Tanh(%BiasAdd) device(\"/device:CPU:0\") name(\"Tanh\") {T = f32} : (tensor<*xf32>) -> (tensor<*xf32>)\n   return\n }\n+\n+// -----\n+// CHECK-LABEL: tfg.func @conv2dsqueezebias_test\n+tfg.func @conv2dsqueezebias_test() {\n+  // CHECK: %[[PLACEHOLDER:.*]], {{.*}} name(\"input_tensor\")\n+  %Placeholder, %ctl = Placeholder device(\"/device:CPU:0\") name(\"input_tensor\") {dtype = f32, shape = #tf_type.shape<1x3x3x1>} : () -> (tensor<*xf32>)\n+  // CHECK: %[[FILTER:.*]], {{.*}} name(\"Const\")\n+  %Const, %ctl_1 = Const device(\"/device:CPU:0\") name(\"Const\") {dtype = f32, value = dense<[[[[1.11986792, -3.0272491]]]]> : tensor<1x1x1x2xf32>} : () -> (tensor<*xf32>)\n+  // CHECK: %[[BIAS:.*]], {{.*}} name(\"Const_1\")\n+  %Const_1, %ctl_3 = Const device(\"/device:CPU:0\") name(\"Const_1\") {dtype = f32, value = dense<[0.531091094, -0.719168067]> : tensor<2xf32>} : () -> (tensor<*xf32>)\n+  // CHECK: Squeeze({{.*}}) {{.*}} name(\"BiasAdd\")\n+  %Conv2D, %ctl_4 = Conv2D(%Placeholder, %Const) device(\"/device:CPU:0\") name(\"Conv2D\") {T = f32, data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<*xf32>, tensor<*xf32>) -> (tensor<*xf32>)\n+  %Squeeze, %ctl_5 = Squeeze(%Conv2D) device(\"/device:CPU:0\") name(\"Squeeze\") {T = f32, squeeze_dims = [2]} : (tensor<*xf32>) -> (tensor<*xf32>)\n+  // CHECK: _FusedConv2D(%[[PLACEHOLDER]], %[[FILTER]], %[[BIAS]]) {{.*}} name(\"Conv2D\") {{.*}} fused_ops = [\"BiasAdd\"]\n+  %BiasAdd, %ctl_6 = BiasAdd(%Squeeze, %Const_1) device(\"/device:CPU:0\") name(\"BiasAdd\") {T = f32, data_format = \"NHWC\"} : (tensor<*xf32>, tensor<*xf32>) -> (tensor<*xf32>)",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1764096688",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 59528,
        "pr_file": "tensorflow/core/transforms/remapper/tests/contraction.mlir",
        "discussion_id": "1764096688",
        "commented_code": "@@ -44,3 +44,20 @@ tfg.func @matmul_test() {\n   %Tanh, %ctl_5 = Tanh(%BiasAdd) device(\"/device:CPU:0\") name(\"Tanh\") {T = f32} : (tensor<*xf32>) -> (tensor<*xf32>)\n   return\n }\n+\n+// -----\n+// CHECK-LABEL: tfg.func @conv2dsqueezebias_test\n+tfg.func @conv2dsqueezebias_test() {\n+  // CHECK: %[[PLACEHOLDER:.*]], {{.*}} name(\"input_tensor\")\n+  %Placeholder, %ctl = Placeholder device(\"/device:CPU:0\") name(\"input_tensor\") {dtype = f32, shape = #tf_type.shape<1x3x3x1>} : () -> (tensor<*xf32>)\n+  // CHECK: %[[FILTER:.*]], {{.*}} name(\"Const\")\n+  %Const, %ctl_1 = Const device(\"/device:CPU:0\") name(\"Const\") {dtype = f32, value = dense<[[[[1.11986792, -3.0272491]]]]> : tensor<1x1x1x2xf32>} : () -> (tensor<*xf32>)\n+  // CHECK: %[[BIAS:.*]], {{.*}} name(\"Const_1\")\n+  %Const_1, %ctl_3 = Const device(\"/device:CPU:0\") name(\"Const_1\") {dtype = f32, value = dense<[0.531091094, -0.719168067]> : tensor<2xf32>} : () -> (tensor<*xf32>)\n+  // CHECK: Squeeze({{.*}}) {{.*}} name(\"BiasAdd\")\n+  %Conv2D, %ctl_4 = Conv2D(%Placeholder, %Const) device(\"/device:CPU:0\") name(\"Conv2D\") {T = f32, data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<*xf32>, tensor<*xf32>) -> (tensor<*xf32>)\n+  %Squeeze, %ctl_5 = Squeeze(%Conv2D) device(\"/device:CPU:0\") name(\"Squeeze\") {T = f32, squeeze_dims = [2]} : (tensor<*xf32>) -> (tensor<*xf32>)\n+  // CHECK: _FusedConv2D(%[[PLACEHOLDER]], %[[FILTER]], %[[BIAS]]) {{.*}} name(\"Conv2D\") {{.*}} fused_ops = [\"BiasAdd\"]\n+  %BiasAdd, %ctl_6 = BiasAdd(%Squeeze, %Const_1) device(\"/device:CPU:0\") name(\"BiasAdd\") {T = f32, data_format = \"NHWC\"} : (tensor<*xf32>, tensor<*xf32>) -> (tensor<*xf32>)",
        "comment_created_at": "2024-09-17T22:30:09+00:00",
        "comment_author": "sueannioanis",
        "comment_body": "```\r\n// -----\r\n// CHECK-LABEL: tfg.func @conv2dsqueezebias_test\r\ntfg.func @conv2dsqueezebias_test() {\r\n  // Define the input tensor placeholder\r\n  %input_tensor, %ctl_input = Placeholder device(\"/device:CPU:0\") name(\"input_tensor\") {dtype = f32, shape = #tf_type.shape<1x3x3x1>} : () -> (tensor<*xf32>)\r\n  \r\n  // Define the filter tensor\r\n  %filter, %ctl_filter = Const device(\"/device:CPU:0\") name(\"filter\") {dtype = f32, value = dense<[[[[1.11986792, -3.0272491]]]]> : tensor<1x1x1x2xf32>} : () -> (tensor<*xf32>)\r\n  \r\n  // Define the bias tensor\r\n  %bias, %ctl_bias = Const device(\"/device:CPU:0\") name(\"bias\") {dtype = f32, value = dense<[0.531091094, -0.719168067]> : tensor<2xf32>} : () -> (tensor<*xf32>)\r\n\r\n  // Perform convolution operation\r\n  %conv2d, %ctl_conv = Conv2D(%input_tensor, %filter) device(\"/device:CPU:0\") name(\"conv2d\") {T = f32, data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<*xf32>, tensor<*xf32>) -> (tensor<*xf32>)\r\n  \r\n  // Squeeze the output of the convolution\r\n  %squeezed, %ctl_squeeze = Squeeze(%conv2d) device(\"/device:CPU:0\") name(\"squeezed\") {T = f32, squeeze_dims = [2]} : (tensor<*xf32>) -> (tensor<*xf32>)\r\n  \r\n  // Add bias to the squeezed tensor\r\n  %bias_added, %ctl_bias_add = BiasAdd(%squeezed, %bias) device(\"/device:CPU:0\") name(\"bias_added\") {T = f32, data_format = \"NHWC\"} : (tensor<*xf32>, tensor<*xf32>) -> (tensor<*xf32>)\r\n  \r\n  return\r\n}\r\n```\r\n\r\nRenaming Operations:\r\n\r\n- Changed `%Placeholder` to `%input_tensor`, `%Const` to `%filter`, and `%Const_1` to `%bias` for clarity. Clear names help in understanding the role of each tensor.\r\n\r\nSimplifying Names:\r\n\r\n- Adjusted operation names (`Conv2D` to `conv2d`, `Squeeze` to `squeezed`, and `BiasAdd` to `bias_added`) to be more descriptive and aligned with common TensorFlow naming conventions.\r\n\r\nConsistent Naming:\r\n\r\n- Used consistent naming conventions for tensors and operations to enhance readability. For instance, `%ctl` names are now aligned with their corresponding tensors.\r\n\r\nMaintained Device and Type Annotations:\r\n\r\n- Ensured that the device and type annotations remain intact for clarity on the computational resource and data type.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "348682001",
    "pr_number": 34284,
    "pr_file": "tensorflow/compiler/xla/service/algebraic_simplifier.cc",
    "created_at": "2019-11-20T18:46:29+00:00",
    "commented_code": "}\n }\n \n+bool IsPositive(const HloInstruction* hlo,",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "348682001",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 34284,
        "pr_file": "tensorflow/compiler/xla/service/algebraic_simplifier.cc",
        "discussion_id": "348682001",
        "commented_code": "@@ -80,6 +80,30 @@ bool IsAll(const HloInstruction* op, int8 value) {\n   }\n }\n \n+bool IsPositive(const HloInstruction* hlo,",
        "comment_created_at": "2019-11-20T18:46:29+00:00",
        "comment_author": "bixia1",
        "comment_body": "NIT: calling it IsNonNegative is more proper, because x*x could be 0 or NAN. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1299431156",
    "pr_number": 61632,
    "pr_file": "tensorflow/core/protobuf/config.proto",
    "created_at": "2023-08-20T20:30:36+00:00",
    "commented_code": "// the overall host system performance.\n   bool force_gpu_compatible = 8;\n \n+  // Whether to merge streams in one stream group. Four types of streams will be\n+  // created within a stream group by default: one compute stream, one HtoD copy\n+  // stream, one DtoH copy stream, and several DtoD copy streams. Use the\n+  // STREAM_MERGE_OPTIONS to merge the copy streams into the compute stream. For\n+  // example, setting \"merge_h_to_d_stream = true\" to make the compute stream\n+  // responsible for both computation and HtoD memory copy. Stream merging helps\n+  // reduce the overhead caused by stream synchronization, especially when data\n+  // transfers are frequent.\n+  message STREAM_MERGE_OPTIONS {\n+    bool merge_h_to_d_stream = 1;",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1299431156",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 61632,
        "pr_file": "tensorflow/core/protobuf/config.proto",
        "discussion_id": "1299431156",
        "commented_code": "@@ -99,6 +99,22 @@ message GPUOptions {\n   // the overall host system performance.\n   bool force_gpu_compatible = 8;\n \n+  // Whether to merge streams in one stream group. Four types of streams will be\n+  // created within a stream group by default: one compute stream, one HtoD copy\n+  // stream, one DtoH copy stream, and several DtoD copy streams. Use the\n+  // STREAM_MERGE_OPTIONS to merge the copy streams into the compute stream. For\n+  // example, setting \"merge_h_to_d_stream = true\" to make the compute stream\n+  // responsible for both computation and HtoD memory copy. Stream merging helps\n+  // reduce the overhead caused by stream synchronization, especially when data\n+  // transfers are frequent.\n+  message STREAM_MERGE_OPTIONS {\n+    bool merge_h_to_d_stream = 1;",
        "comment_created_at": "2023-08-20T20:30:36+00:00",
        "comment_author": "changhuilin",
        "comment_body": "probably \"merge_host_to_device_stream\" for better readability. Could you also add a comment for each field?\r\n\r\nsame for the others",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1272761498",
    "pr_number": 61368,
    "pr_file": "tensorflow/compiler/mlir/tosa/transforms/legalize_common.cc",
    "created_at": "2023-07-24T21:12:35+00:00",
    "commented_code": "return res;\n }\n \n-static int64_t count_dynamic_dims(llvm::ArrayRef<int64_t> dims) {\n-  int64_t count = 0;\n-  for (auto dim : dims)\n-    if (ShapedType::isDynamic(dim)) ++count;\n-  return count;\n+static OpFoldResult multiply_dims(ArrayRef<OpFoldResult> dims,",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1272761498",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 61368,
        "pr_file": "tensorflow/compiler/mlir/tosa/transforms/legalize_common.cc",
        "discussion_id": "1272761498",
        "commented_code": "@@ -66,11 +69,74 @@ static int64_t multiply_dims(llvm::ArrayRef<int64_t> dims, int64_t res = 1) {\n   return res;\n }\n \n-static int64_t count_dynamic_dims(llvm::ArrayRef<int64_t> dims) {\n-  int64_t count = 0;\n-  for (auto dim : dims)\n-    if (ShapedType::isDynamic(dim)) ++count;\n-  return count;\n+static OpFoldResult multiply_dims(ArrayRef<OpFoldResult> dims,",
        "comment_created_at": "2023-07-24T21:12:35+00:00",
        "comment_author": "rsuderman",
        "comment_body": "Match name style for helpers. It should appear as `thisIsTheFunctionName`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1439605202",
    "pr_number": 61841,
    "pr_file": "tensorflow/lite/tools/evaluation/utils_test.cc",
    "created_at": "2024-01-02T16:18:38+00:00",
    "commented_code": "TEST(UtilsTest, ReadFileErrors) {\n   std::string correct_path(kLabelsPath);\n+  std::string empty_path(kEmptyFilePath);\n   std::string wrong_path(\"xyz.txt\");\n   std::vector<std::string> lines;\n   EXPECT_FALSE(ReadFileLines(correct_path, nullptr));\n+  EXPECT_FALSE(ReadFileLines(empty_path, nullptr));\n   EXPECT_FALSE(ReadFileLines(wrong_path, &lines));\n }\n \n TEST(UtilsTest, ReadFileCorrectly) {\n   std::string file_path(kLabelsPath);\n+  std::string empty_path(kEmptyFilePath);\n   std::vector<std::string> lines;\n-  EXPECT_TRUE(ReadFileLines(file_path, &lines));\n+  std::vector<std::string> emptyLines;",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1439605202",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 61841,
        "pr_file": "tensorflow/lite/tools/evaluation/utils_test.cc",
        "discussion_id": "1439605202",
        "commented_code": "@@ -44,20 +44,27 @@ TEST(UtilsTest, StripTrailingSlashesTest) {\n \n TEST(UtilsTest, ReadFileErrors) {\n   std::string correct_path(kLabelsPath);\n+  std::string empty_path(kEmptyFilePath);\n   std::string wrong_path(\"xyz.txt\");\n   std::vector<std::string> lines;\n   EXPECT_FALSE(ReadFileLines(correct_path, nullptr));\n+  EXPECT_FALSE(ReadFileLines(empty_path, nullptr));\n   EXPECT_FALSE(ReadFileLines(wrong_path, &lines));\n }\n \n TEST(UtilsTest, ReadFileCorrectly) {\n   std::string file_path(kLabelsPath);\n+  std::string empty_path(kEmptyFilePath);\n   std::vector<std::string> lines;\n-  EXPECT_TRUE(ReadFileLines(file_path, &lines));\n+  std::vector<std::string> emptyLines;",
        "comment_created_at": "2024-01-02T16:18:38+00:00",
        "comment_author": "rascani",
        "comment_body": "`emptyLines` should be `empty_lines`",
        "pr_file_module": null
      }
    ]
  }
]