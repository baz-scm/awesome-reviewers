[
  {
    "discussion_id": "992352433",
    "pr_number": 51527,
    "pr_file": "tensorflow/lite/kernels/internal/reference/integer_ops/conv3d.h",
    "created_at": "2022-10-11T13:48:43+00:00",
    "commented_code": "+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_CONV3D_H_\n+#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_CONV3D_H_\n+\n+#include \"tensorflow/lite/kernels/internal/common.h\"\n+#include \"tensorflow/lite/kernels/internal/types.h\"\n+\n+namespace tflite {\n+namespace reference_integer_ops {\n+\n+// Fixed-point per-channel-quantization convolution reference kernel.\n+template <typename InputType, typename BiasType>\n+inline void Conv3DPerChannel(\n+    const Conv3DParams& params, const int32_t* output_multiplier,\n+    const int* output_shift, const RuntimeShape& input_shape,\n+    const InputType* input_data, const RuntimeShape& filter_shape,\n+    const int8_t* filter_data, const RuntimeShape& bias_shape,\n+    const BiasType* bias_data, const RuntimeShape& output_shape,\n+    InputType* output_data) {\n+  using AccumulatorType = BiasType;\n+  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n+  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 5);\n+  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n+\n+  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n+  const int input_num_channels = MatchingDim(input_shape, 4, filter_shape, 3);\n+  const int output_num_channels = MatchingDim(filter_shape, 4, output_shape, 4);\n+  if (bias_data) {\n+    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_num_channels);\n+  }\n+  const int input_width = input_shape.Dims(3);\n+  const int input_height = input_shape.Dims(2);\n+  const int input_depth = input_shape.Dims(1);\n+  const int filter_width = filter_shape.Dims(2);\n+  const int filter_height = filter_shape.Dims(1);\n+  const int filter_depth = filter_shape.Dims(0);\n+  const int output_width = output_shape.Dims(3);\n+  const int output_height = output_shape.Dims(2);\n+  const int output_depth = output_shape.Dims(1);\n+  const int pad_width = params.padding_values.width;\n+  const int pad_height = params.padding_values.height;\n+  const int pad_depth = params.padding_values.depth;\n+  const int32_t input_offset = params.input_offset;\n+  const int32_t output_offset = params.output_offset;\n+  const int32_t output_activation_min = params.quantized_activation_min;\n+  const int32_t output_activation_max = params.quantized_activation_max;\n+  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n+  for (int batch = 0; batch < batches; ++batch) {\n+    for (int out_d = 0; out_d < output_depth; ++out_d) {\n+      const int in_d_origin = (out_d * params.stride_depth) - pad_depth;\n+      for (int out_y = 0; out_y < output_height; ++out_y) {\n+        const int in_y_origin = (out_y * params.stride_height) - pad_height;\n+        for (int out_x = 0; out_x < output_width; ++out_x) {\n+          const int in_x_origin = (out_x * params.stride_width) - pad_width;\n+          for (int out_channel = 0; out_channel < output_num_channels;\n+               ++out_channel) {\n+            AccumulatorType total = 0;\n+            for (int filter_d = 0; filter_d < filter_depth; ++filter_d) {\n+              const int in_d = in_d_origin + params.dilation_depth * filter_d;\n+              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n+                const int in_y =\n+                    in_y_origin + params.dilation_height * filter_y;\n+                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {\n+                  const int in_x =\n+                      in_x_origin + params.dilation_width * filter_x;\n+\n+                  // Zero padding by omitting the areas outside the image.\n+                  const bool is_point_inside_image =\n+                      (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&\n+                      (in_y < input_height) && (in_d >= 0) &&\n+                      (in_d < input_depth);\n+                  if (!is_point_inside_image) {\n+                    continue;\n+                  }\n+\n+                  for (int in_channel = 0; in_channel < input_num_channels;\n+                       ++in_channel) {\n+                    int32_t input_value = input_data[Offset(",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "992352433",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 51527,
        "pr_file": "tensorflow/lite/kernels/internal/reference/integer_ops/conv3d.h",
        "discussion_id": "992352433",
        "commented_code": "@@ -0,0 +1,124 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_CONV3D_H_\n+#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_CONV3D_H_\n+\n+#include \"tensorflow/lite/kernels/internal/common.h\"\n+#include \"tensorflow/lite/kernels/internal/types.h\"\n+\n+namespace tflite {\n+namespace reference_integer_ops {\n+\n+// Fixed-point per-channel-quantization convolution reference kernel.\n+template <typename InputType, typename BiasType>\n+inline void Conv3DPerChannel(\n+    const Conv3DParams& params, const int32_t* output_multiplier,\n+    const int* output_shift, const RuntimeShape& input_shape,\n+    const InputType* input_data, const RuntimeShape& filter_shape,\n+    const int8_t* filter_data, const RuntimeShape& bias_shape,\n+    const BiasType* bias_data, const RuntimeShape& output_shape,\n+    InputType* output_data) {\n+  using AccumulatorType = BiasType;\n+  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n+  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 5);\n+  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n+\n+  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n+  const int input_num_channels = MatchingDim(input_shape, 4, filter_shape, 3);\n+  const int output_num_channels = MatchingDim(filter_shape, 4, output_shape, 4);\n+  if (bias_data) {\n+    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_num_channels);\n+  }\n+  const int input_width = input_shape.Dims(3);\n+  const int input_height = input_shape.Dims(2);\n+  const int input_depth = input_shape.Dims(1);\n+  const int filter_width = filter_shape.Dims(2);\n+  const int filter_height = filter_shape.Dims(1);\n+  const int filter_depth = filter_shape.Dims(0);\n+  const int output_width = output_shape.Dims(3);\n+  const int output_height = output_shape.Dims(2);\n+  const int output_depth = output_shape.Dims(1);\n+  const int pad_width = params.padding_values.width;\n+  const int pad_height = params.padding_values.height;\n+  const int pad_depth = params.padding_values.depth;\n+  const int32_t input_offset = params.input_offset;\n+  const int32_t output_offset = params.output_offset;\n+  const int32_t output_activation_min = params.quantized_activation_min;\n+  const int32_t output_activation_max = params.quantized_activation_max;\n+  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n+  for (int batch = 0; batch < batches; ++batch) {\n+    for (int out_d = 0; out_d < output_depth; ++out_d) {\n+      const int in_d_origin = (out_d * params.stride_depth) - pad_depth;\n+      for (int out_y = 0; out_y < output_height; ++out_y) {\n+        const int in_y_origin = (out_y * params.stride_height) - pad_height;\n+        for (int out_x = 0; out_x < output_width; ++out_x) {\n+          const int in_x_origin = (out_x * params.stride_width) - pad_width;\n+          for (int out_channel = 0; out_channel < output_num_channels;\n+               ++out_channel) {\n+            AccumulatorType total = 0;\n+            for (int filter_d = 0; filter_d < filter_depth; ++filter_d) {\n+              const int in_d = in_d_origin + params.dilation_depth * filter_d;\n+              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n+                const int in_y =\n+                    in_y_origin + params.dilation_height * filter_y;\n+                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {\n+                  const int in_x =\n+                      in_x_origin + params.dilation_width * filter_x;\n+\n+                  // Zero padding by omitting the areas outside the image.\n+                  const bool is_point_inside_image =\n+                      (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&\n+                      (in_y < input_height) && (in_d >= 0) &&\n+                      (in_d < input_depth);\n+                  if (!is_point_inside_image) {\n+                    continue;\n+                  }\n+\n+                  for (int in_channel = 0; in_channel < input_num_channels;\n+                       ++in_channel) {\n+                    int32_t input_value = input_data[Offset(",
        "comment_created_at": "2022-10-11T13:48:43+00:00",
        "comment_author": "alankelly",
        "comment_body": "Please don't use offset in an inner loop (I know there are examples elsewhere of this but I am working on getting id of them). it is really expensive. See this for an example of how to minimize the calculation in the inner loop. https://github.com/tensorflow/tensorflow/blob/b429f6545a8be48b6439f7e1d0feb9896a9e9b5a/tensorflow/lite/kernels/internal/reference/binary_function.h#L49",
        "pr_file_module": null
      },
      {
        "comment_id": "997018763",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 51527,
        "pr_file": "tensorflow/lite/kernels/internal/reference/integer_ops/conv3d.h",
        "discussion_id": "992352433",
        "commented_code": "@@ -0,0 +1,124 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_CONV3D_H_\n+#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_CONV3D_H_\n+\n+#include \"tensorflow/lite/kernels/internal/common.h\"\n+#include \"tensorflow/lite/kernels/internal/types.h\"\n+\n+namespace tflite {\n+namespace reference_integer_ops {\n+\n+// Fixed-point per-channel-quantization convolution reference kernel.\n+template <typename InputType, typename BiasType>\n+inline void Conv3DPerChannel(\n+    const Conv3DParams& params, const int32_t* output_multiplier,\n+    const int* output_shift, const RuntimeShape& input_shape,\n+    const InputType* input_data, const RuntimeShape& filter_shape,\n+    const int8_t* filter_data, const RuntimeShape& bias_shape,\n+    const BiasType* bias_data, const RuntimeShape& output_shape,\n+    InputType* output_data) {\n+  using AccumulatorType = BiasType;\n+  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n+  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 5);\n+  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n+\n+  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n+  const int input_num_channels = MatchingDim(input_shape, 4, filter_shape, 3);\n+  const int output_num_channels = MatchingDim(filter_shape, 4, output_shape, 4);\n+  if (bias_data) {\n+    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_num_channels);\n+  }\n+  const int input_width = input_shape.Dims(3);\n+  const int input_height = input_shape.Dims(2);\n+  const int input_depth = input_shape.Dims(1);\n+  const int filter_width = filter_shape.Dims(2);\n+  const int filter_height = filter_shape.Dims(1);\n+  const int filter_depth = filter_shape.Dims(0);\n+  const int output_width = output_shape.Dims(3);\n+  const int output_height = output_shape.Dims(2);\n+  const int output_depth = output_shape.Dims(1);\n+  const int pad_width = params.padding_values.width;\n+  const int pad_height = params.padding_values.height;\n+  const int pad_depth = params.padding_values.depth;\n+  const int32_t input_offset = params.input_offset;\n+  const int32_t output_offset = params.output_offset;\n+  const int32_t output_activation_min = params.quantized_activation_min;\n+  const int32_t output_activation_max = params.quantized_activation_max;\n+  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n+  for (int batch = 0; batch < batches; ++batch) {\n+    for (int out_d = 0; out_d < output_depth; ++out_d) {\n+      const int in_d_origin = (out_d * params.stride_depth) - pad_depth;\n+      for (int out_y = 0; out_y < output_height; ++out_y) {\n+        const int in_y_origin = (out_y * params.stride_height) - pad_height;\n+        for (int out_x = 0; out_x < output_width; ++out_x) {\n+          const int in_x_origin = (out_x * params.stride_width) - pad_width;\n+          for (int out_channel = 0; out_channel < output_num_channels;\n+               ++out_channel) {\n+            AccumulatorType total = 0;\n+            for (int filter_d = 0; filter_d < filter_depth; ++filter_d) {\n+              const int in_d = in_d_origin + params.dilation_depth * filter_d;\n+              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n+                const int in_y =\n+                    in_y_origin + params.dilation_height * filter_y;\n+                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {\n+                  const int in_x =\n+                      in_x_origin + params.dilation_width * filter_x;\n+\n+                  // Zero padding by omitting the areas outside the image.\n+                  const bool is_point_inside_image =\n+                      (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&\n+                      (in_y < input_height) && (in_d >= 0) &&\n+                      (in_d < input_depth);\n+                  if (!is_point_inside_image) {\n+                    continue;\n+                  }\n+\n+                  for (int in_channel = 0; in_channel < input_num_channels;\n+                       ++in_channel) {\n+                    int32_t input_value = input_data[Offset(",
        "comment_created_at": "2022-10-17T12:49:13+00:00",
        "comment_author": "Tessil",
        "comment_body": "Yes, we used it in the same way as the float Conv3D reference kernel. I updated it, thanks!",
        "pr_file_module": null
      },
      {
        "comment_id": "997019349",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 51527,
        "pr_file": "tensorflow/lite/kernels/internal/reference/integer_ops/conv3d.h",
        "discussion_id": "992352433",
        "commented_code": "@@ -0,0 +1,124 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_CONV3D_H_\n+#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_CONV3D_H_\n+\n+#include \"tensorflow/lite/kernels/internal/common.h\"\n+#include \"tensorflow/lite/kernels/internal/types.h\"\n+\n+namespace tflite {\n+namespace reference_integer_ops {\n+\n+// Fixed-point per-channel-quantization convolution reference kernel.\n+template <typename InputType, typename BiasType>\n+inline void Conv3DPerChannel(\n+    const Conv3DParams& params, const int32_t* output_multiplier,\n+    const int* output_shift, const RuntimeShape& input_shape,\n+    const InputType* input_data, const RuntimeShape& filter_shape,\n+    const int8_t* filter_data, const RuntimeShape& bias_shape,\n+    const BiasType* bias_data, const RuntimeShape& output_shape,\n+    InputType* output_data) {\n+  using AccumulatorType = BiasType;\n+  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n+  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 5);\n+  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n+\n+  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n+  const int input_num_channels = MatchingDim(input_shape, 4, filter_shape, 3);\n+  const int output_num_channels = MatchingDim(filter_shape, 4, output_shape, 4);\n+  if (bias_data) {\n+    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_num_channels);\n+  }\n+  const int input_width = input_shape.Dims(3);\n+  const int input_height = input_shape.Dims(2);\n+  const int input_depth = input_shape.Dims(1);\n+  const int filter_width = filter_shape.Dims(2);\n+  const int filter_height = filter_shape.Dims(1);\n+  const int filter_depth = filter_shape.Dims(0);\n+  const int output_width = output_shape.Dims(3);\n+  const int output_height = output_shape.Dims(2);\n+  const int output_depth = output_shape.Dims(1);\n+  const int pad_width = params.padding_values.width;\n+  const int pad_height = params.padding_values.height;\n+  const int pad_depth = params.padding_values.depth;\n+  const int32_t input_offset = params.input_offset;\n+  const int32_t output_offset = params.output_offset;\n+  const int32_t output_activation_min = params.quantized_activation_min;\n+  const int32_t output_activation_max = params.quantized_activation_max;\n+  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n+  for (int batch = 0; batch < batches; ++batch) {\n+    for (int out_d = 0; out_d < output_depth; ++out_d) {\n+      const int in_d_origin = (out_d * params.stride_depth) - pad_depth;\n+      for (int out_y = 0; out_y < output_height; ++out_y) {\n+        const int in_y_origin = (out_y * params.stride_height) - pad_height;\n+        for (int out_x = 0; out_x < output_width; ++out_x) {\n+          const int in_x_origin = (out_x * params.stride_width) - pad_width;\n+          for (int out_channel = 0; out_channel < output_num_channels;\n+               ++out_channel) {\n+            AccumulatorType total = 0;\n+            for (int filter_d = 0; filter_d < filter_depth; ++filter_d) {\n+              const int in_d = in_d_origin + params.dilation_depth * filter_d;\n+              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n+                const int in_y =\n+                    in_y_origin + params.dilation_height * filter_y;\n+                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {\n+                  const int in_x =\n+                      in_x_origin + params.dilation_width * filter_x;\n+\n+                  // Zero padding by omitting the areas outside the image.\n+                  const bool is_point_inside_image =\n+                      (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&\n+                      (in_y < input_height) && (in_d >= 0) &&\n+                      (in_d < input_depth);\n+                  if (!is_point_inside_image) {\n+                    continue;\n+                  }\n+\n+                  for (int in_channel = 0; in_channel < input_num_channels;\n+                       ++in_channel) {\n+                    int32_t input_value = input_data[Offset(",
        "comment_created_at": "2022-10-17T12:49:47+00:00",
        "comment_author": "Tessil",
        "comment_body": "I also updated TransposeConv3D PR in the same way https://github.com/tensorflow/tensorflow/pull/51572",
        "pr_file_module": null
      },
      {
        "comment_id": "1003202991",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 51527,
        "pr_file": "tensorflow/lite/kernels/internal/reference/integer_ops/conv3d.h",
        "discussion_id": "992352433",
        "commented_code": "@@ -0,0 +1,124 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_CONV3D_H_\n+#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_INTEGER_OPS_CONV3D_H_\n+\n+#include \"tensorflow/lite/kernels/internal/common.h\"\n+#include \"tensorflow/lite/kernels/internal/types.h\"\n+\n+namespace tflite {\n+namespace reference_integer_ops {\n+\n+// Fixed-point per-channel-quantization convolution reference kernel.\n+template <typename InputType, typename BiasType>\n+inline void Conv3DPerChannel(\n+    const Conv3DParams& params, const int32_t* output_multiplier,\n+    const int* output_shift, const RuntimeShape& input_shape,\n+    const InputType* input_data, const RuntimeShape& filter_shape,\n+    const int8_t* filter_data, const RuntimeShape& bias_shape,\n+    const BiasType* bias_data, const RuntimeShape& output_shape,\n+    InputType* output_data) {\n+  using AccumulatorType = BiasType;\n+  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n+  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 5);\n+  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n+\n+  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n+  const int input_num_channels = MatchingDim(input_shape, 4, filter_shape, 3);\n+  const int output_num_channels = MatchingDim(filter_shape, 4, output_shape, 4);\n+  if (bias_data) {\n+    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_num_channels);\n+  }\n+  const int input_width = input_shape.Dims(3);\n+  const int input_height = input_shape.Dims(2);\n+  const int input_depth = input_shape.Dims(1);\n+  const int filter_width = filter_shape.Dims(2);\n+  const int filter_height = filter_shape.Dims(1);\n+  const int filter_depth = filter_shape.Dims(0);\n+  const int output_width = output_shape.Dims(3);\n+  const int output_height = output_shape.Dims(2);\n+  const int output_depth = output_shape.Dims(1);\n+  const int pad_width = params.padding_values.width;\n+  const int pad_height = params.padding_values.height;\n+  const int pad_depth = params.padding_values.depth;\n+  const int32_t input_offset = params.input_offset;\n+  const int32_t output_offset = params.output_offset;\n+  const int32_t output_activation_min = params.quantized_activation_min;\n+  const int32_t output_activation_max = params.quantized_activation_max;\n+  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n+  for (int batch = 0; batch < batches; ++batch) {\n+    for (int out_d = 0; out_d < output_depth; ++out_d) {\n+      const int in_d_origin = (out_d * params.stride_depth) - pad_depth;\n+      for (int out_y = 0; out_y < output_height; ++out_y) {\n+        const int in_y_origin = (out_y * params.stride_height) - pad_height;\n+        for (int out_x = 0; out_x < output_width; ++out_x) {\n+          const int in_x_origin = (out_x * params.stride_width) - pad_width;\n+          for (int out_channel = 0; out_channel < output_num_channels;\n+               ++out_channel) {\n+            AccumulatorType total = 0;\n+            for (int filter_d = 0; filter_d < filter_depth; ++filter_d) {\n+              const int in_d = in_d_origin + params.dilation_depth * filter_d;\n+              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n+                const int in_y =\n+                    in_y_origin + params.dilation_height * filter_y;\n+                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {\n+                  const int in_x =\n+                      in_x_origin + params.dilation_width * filter_x;\n+\n+                  // Zero padding by omitting the areas outside the image.\n+                  const bool is_point_inside_image =\n+                      (in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&\n+                      (in_y < input_height) && (in_d >= 0) &&\n+                      (in_d < input_depth);\n+                  if (!is_point_inside_image) {\n+                    continue;\n+                  }\n+\n+                  for (int in_channel = 0; in_channel < input_num_channels;\n+                       ++in_channel) {\n+                    int32_t input_value = input_data[Offset(",
        "comment_created_at": "2022-10-24T11:26:07+00:00",
        "comment_author": "alankelly",
        "comment_body": "Thanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1569049778",
    "pr_number": 65939,
    "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general.cc",
    "created_at": "2024-04-17T15:34:24+00:00",
    "commented_code": "+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+absl::Status CheckParameters(const Tensor& lhs, const Tensor& rhs,\n+                             const Tensor& lhs_batching_dimensions,\n+                             const Tensor& rhs_batching_dimensions,\n+                             const Tensor& lhs_contracting_dimensions,\n+                             const Tensor& rhs_contracting_dimensions,\n+                             Tensor& output) {\n+  const int32_t* lhsb = lhs_batching_dimensions.GetDataAs<DataType::kSI32>();\n+  const int32_t* rhsb = rhs_batching_dimensions.GetDataAs<DataType::kSI32>();\n+  const int32_t* lhsc = lhs_contracting_dimensions.GetDataAs<DataType::kSI32>();\n+  const int32_t* rhsc = rhs_contracting_dimensions.GetDataAs<DataType::kSI32>();\n+  const DimensionSize lhsb_size = lhs_batching_dimensions.NumElements();\n+  const DimensionSize rhsb_size = rhs_batching_dimensions.NumElements();\n+  const DimensionSize lhsc_size = lhs_contracting_dimensions.NumElements();\n+  const DimensionSize rhsc_size = rhs_contracting_dimensions.NumElements();\n+  const size_t lhs_rank = lhs.Rank();\n+  const size_t rhs_rank = rhs.Rank();\n+  const size_t output_rank = output.Rank();\n+  std::vector<size_t> lhs_result_dims;",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1569049778",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 65939,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general.cc",
        "discussion_id": "1569049778",
        "commented_code": "@@ -0,0 +1,441 @@\n+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+absl::Status CheckParameters(const Tensor& lhs, const Tensor& rhs,\n+                             const Tensor& lhs_batching_dimensions,\n+                             const Tensor& rhs_batching_dimensions,\n+                             const Tensor& lhs_contracting_dimensions,\n+                             const Tensor& rhs_contracting_dimensions,\n+                             Tensor& output) {\n+  const int32_t* lhsb = lhs_batching_dimensions.GetDataAs<DataType::kSI32>();\n+  const int32_t* rhsb = rhs_batching_dimensions.GetDataAs<DataType::kSI32>();\n+  const int32_t* lhsc = lhs_contracting_dimensions.GetDataAs<DataType::kSI32>();\n+  const int32_t* rhsc = rhs_contracting_dimensions.GetDataAs<DataType::kSI32>();\n+  const DimensionSize lhsb_size = lhs_batching_dimensions.NumElements();\n+  const DimensionSize rhsb_size = rhs_batching_dimensions.NumElements();\n+  const DimensionSize lhsc_size = lhs_contracting_dimensions.NumElements();\n+  const DimensionSize rhsc_size = rhs_contracting_dimensions.NumElements();\n+  const size_t lhs_rank = lhs.Rank();\n+  const size_t rhs_rank = rhs.Rank();\n+  const size_t output_rank = output.Rank();\n+  std::vector<size_t> lhs_result_dims;",
        "comment_created_at": "2024-04-17T15:34:24+00:00",
        "comment_author": "qukhan",
        "comment_body": "We try to avoid memory allocations. You can use `absl::InlinedVector<T, *REASONABLE_SIZE*>` to have a vector that uses small buffer optimisation.",
        "pr_file_module": null
      },
      {
        "comment_id": "1583360516",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 65939,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general.cc",
        "discussion_id": "1569049778",
        "commented_code": "@@ -0,0 +1,441 @@\n+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+absl::Status CheckParameters(const Tensor& lhs, const Tensor& rhs,\n+                             const Tensor& lhs_batching_dimensions,\n+                             const Tensor& rhs_batching_dimensions,\n+                             const Tensor& lhs_contracting_dimensions,\n+                             const Tensor& rhs_contracting_dimensions,\n+                             Tensor& output) {\n+  const int32_t* lhsb = lhs_batching_dimensions.GetDataAs<DataType::kSI32>();\n+  const int32_t* rhsb = rhs_batching_dimensions.GetDataAs<DataType::kSI32>();\n+  const int32_t* lhsc = lhs_contracting_dimensions.GetDataAs<DataType::kSI32>();\n+  const int32_t* rhsc = rhs_contracting_dimensions.GetDataAs<DataType::kSI32>();\n+  const DimensionSize lhsb_size = lhs_batching_dimensions.NumElements();\n+  const DimensionSize rhsb_size = rhs_batching_dimensions.NumElements();\n+  const DimensionSize lhsc_size = lhs_contracting_dimensions.NumElements();\n+  const DimensionSize rhsc_size = rhs_contracting_dimensions.NumElements();\n+  const size_t lhs_rank = lhs.Rank();\n+  const size_t rhs_rank = rhs.Rank();\n+  const size_t output_rank = output.Rank();\n+  std::vector<size_t> lhs_result_dims;",
        "comment_created_at": "2024-04-29T16:17:10+00:00",
        "comment_author": "nishantsarda-mcw",
        "comment_body": "used absl::InlinedVector vectors.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1578299493",
    "pr_number": 66299,
    "pr_file": "tensorflow/lite/experimental/shlo/ops/convolution.cc",
    "created_at": "2024-04-24T17:52:26+00:00",
    "commented_code": "+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/convolution.h\"\n+\n+#include <algorithm>\n+#include <cstddef>\n+#include <string>\n+#include <type_traits>\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/unary_elementwise.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+template <class T>\n+using DimVector = absl::InlinedVector<T, 6>;\n+\n+bool IsUnique(DimVector<int64_t>& vec) {\n+  std::sort(vec.begin(), vec.end());\n+  return std::unique(vec.begin(), vec.end()) == vec.end();\n+}\n+\n+bool IsInRange(DimVector<int64_t>& vec, size_t N) {\n+  for (int64_t dim : vec) {\n+    if (dim >= N || dim < 0) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+template <DataType storage_type>\n+absl::Status PrepareImpl(ConvolutionOp& op, const Tensor& lhs,\n+                         const Tensor& rhs, Tensor& output) {\n+  using StorageT = StorageType<storage_type>;\n+\n+  // Transpose prepare\n+  const int64_t* window_spacial_pointer =\n+      op.attributes.kernel_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  const int64_t* output_spacial_pointer =\n+      op.attributes.output_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  const int64_t* input_spacial_pointer =\n+      op.attributes.input_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+\n+  std::vector<StorageT> lhs_permutation_values(\n+      static_cast<int64_t>(lhs.Rank()));\n+  lhs_permutation_values[0] = op.attributes.input_batch_dimension;\n+  lhs_permutation_values[1] = op.attributes.input_feature_dimension;\n+  DimVector<DimensionSize> lhs_shape_dims(lhs.Rank());\n+  lhs_shape_dims[0] =\n+      lhs.shape().Dim(static_cast<size_t>(op.attributes.input_batch_dimension));\n+  lhs_shape_dims[1] = lhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.input_feature_dimension));\n+  for (size_t i = 0; i < lhs.Rank() - 2; ++i) {\n+    lhs_shape_dims[i + 2] =\n+        lhs.shape().Dim(static_cast<size_t>(input_spacial_pointer[i]));\n+    lhs_permutation_values[i + 2] = input_spacial_pointer[i];\n+  }\n+  // malloc is used to have the storage space available out of prepare function\n+  // scope and it's pointer is stored in class data member to\n+  // deallocate the memory in destructor.\n+  op.lhs_permutation_data =\n+      malloc(lhs_permutation_values.size() * sizeof(StorageT));\n+  memmove(op.lhs_permutation_data, lhs_permutation_values.data(),\n+          lhs_permutation_values.size() * sizeof(StorageT));\n+  const Shape lhs_permutation_shape({static_cast<int64_t>(lhs.Rank())});\n+  Tensor lhs_permutations{.type = TensorType{.shape = lhs_permutation_shape,\n+                                             .element_type = storage_type},\n+                          .data = op.lhs_permutation_data};\n+\n+  op.lhs_transposed_data = malloc(lhs.NumElements() * sizeof(StorageT));\n+  const Shape lhs_transposed_shape(lhs_shape_dims);\n+  Tensor lhs_transposed{.type = TensorType{.shape = lhs_transposed_shape,\n+                                           .element_type = storage_type},\n+                        .data = op.lhs_transposed_data};\n+\n+  std::vector<StorageT> rhs_permutation_values(\n+      static_cast<int64_t>(rhs.Rank()));\n+  rhs_permutation_values[0] = op.attributes.kernel_output_feature_dimension;\n+  rhs_permutation_values[1] = op.attributes.kernel_input_feature_dimension;\n+  DimVector<DimensionSize> rhs_shape_dims(rhs.Rank());\n+  rhs_shape_dims[0] = rhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.kernel_output_feature_dimension));\n+  rhs_shape_dims[1] = rhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.kernel_input_feature_dimension));\n+  for (size_t i = 0; i < rhs.Rank() - 2; ++i) {\n+    rhs_shape_dims[i + 2] =\n+        rhs.shape().Dim(static_cast<size_t>(window_spacial_pointer[i]));\n+    rhs_permutation_values[i + 2] = window_spacial_pointer[i];\n+  }\n+  op.rhs_permutation_data = malloc(rhs.Rank() * sizeof(StorageT));\n+  memmove(op.rhs_permutation_data, rhs_permutation_values.data(),\n+          rhs_permutation_values.size() * sizeof(StorageT));\n+  const Shape rhs_permutation_shape({static_cast<int64_t>(rhs.Rank())});\n+  Tensor rhs_permutations{.type = TensorType{.shape = rhs_permutation_shape,\n+                                             .element_type = storage_type},\n+                          .data = op.rhs_permutation_data};\n+\n+  op.rhs_transposed_data = malloc(rhs.NumElements() * sizeof(StorageT));\n+  const Shape rhs_transposed_shape(rhs_shape_dims);\n+  Tensor rhs_transposed{.type = TensorType{.shape = rhs_transposed_shape,\n+                                           .element_type = storage_type},\n+                        .data = op.rhs_transposed_data};\n+\n+  std::vector<StorageT> output_permutation_values(\n+      static_cast<int64_t>(output.Rank()));\n+  output_permutation_values[0] = op.attributes.output_batch_dimension;\n+  output_permutation_values[1] = op.attributes.output_feature_dimension;\n+  DimVector<DimensionSize> output_shape_dims(output.Rank());\n+  output_shape_dims[0] = output.shape().Dim(\n+      static_cast<size_t>(op.attributes.output_batch_dimension));\n+  output_shape_dims[1] = output.shape().Dim(\n+      static_cast<size_t>(op.attributes.output_feature_dimension));\n+  for (size_t i = 0; i < output.Rank() - 2; ++i) {\n+    output_shape_dims[i + 2] =\n+        output.shape().Dim(static_cast<size_t>(output_spacial_pointer[i]));\n+    output_permutation_values[i + 2] = output_spacial_pointer[i];\n+  }\n+  op.output_permutation_data = malloc(output.Rank() * sizeof(StorageT));\n+  memmove(op.output_permutation_data, output_permutation_values.data(),\n+          output_permutation_values.size() * sizeof(StorageT));\n+  const Shape output_permutation_shape({static_cast<int64_t>(output.Rank())});\n+  Tensor output_permutations{\n+      .type = TensorType{.shape = output_permutation_shape,\n+                         .element_type = storage_type},\n+      .data = op.output_permutation_data};\n+\n+  op.output_transposed_data = malloc(output.NumElements() * sizeof(StorageT));\n+  const Shape output_transposed_shape(output_shape_dims);\n+  Tensor output_transposed{.type = TensorType{.shape = output_transposed_shape,\n+                                              .element_type = storage_type},\n+                           .data = op.output_transposed_data};\n+  // transpose prepare end\n+\n+  // DotGeneral prepare\n+  DimVector<DimensionSize> dims(rhs_transposed.Rank());\n+  size_t rhs_transposed_tensor_size = 1;\n+  dims[0] = 1;\n+  for (size_t i = 1; i < rhs_transposed.Rank(); ++i) {\n+    dims[i] = rhs_transposed.shape().Dim(i);\n+    rhs_transposed_tensor_size *= rhs_transposed.shape().Dim(i);\n+  }\n+  const Shape rhs_dot_general_shape(dims);\n+  op.rhs_dot_general_data =\n+      malloc(rhs_transposed_tensor_size * sizeof(StorageT));\n+  Tensor rhs_dot_general{.type = TensorType{.shape = rhs_dot_general_shape,\n+                                            .element_type = storage_type},\n+                         .data = op.rhs_dot_general_data};\n+\n+  op.lhs_dot_general_data =\n+      malloc(rhs_transposed_tensor_size * sizeof(StorageT));\n+  Tensor lhs_dot_general{.type = TensorType{.shape = rhs_dot_general_shape,\n+                                            .element_type = storage_type},\n+                         .data = op.lhs_dot_general_data};\n+\n+  std::vector<typename Storage<DataType::kSI64>::Type>\n+      lhs_contracting_dimensions_values(lhs_transposed.Rank() - 1);\n+  for (size_t i = 0; i < lhs_transposed.Rank() - 1; ++i) {\n+    lhs_contracting_dimensions_values[i] = i + 1;\n+  }\n+  op.lhs_contracting_dimensions_data =\n+      malloc((lhs_transposed.Rank() - 1) * sizeof(int64_t));\n+  memmove(op.lhs_contracting_dimensions_data,\n+          lhs_contracting_dimensions_values.data(),\n+          lhs_contracting_dimensions_values.size() * sizeof(int64_t));\n+  const Shape lhs_contracting_dimensions_shape(\n+      {static_cast<int64_t>(lhs_transposed.Rank() - 1)});\n+  Tensor lhs_contracting_dimensions{\n+      .type = TensorType{.shape = lhs_contracting_dimensions_shape,\n+                         .element_type = DataType::kSI64},\n+      .data = op.lhs_contracting_dimensions_data};\n+\n+  std::vector<typename Storage<DataType::kSI64>::Type>\n+      rhs_contracting_dimensions_values(rhs_transposed.Rank() - 1);\n+  for (size_t i = 0; i < rhs_transposed.Rank() - 1; ++i) {\n+    rhs_contracting_dimensions_values[i] = i + 1;\n+  }\n+  op.rhs_contracting_dimensions_data =\n+      malloc((rhs_transposed.Rank() - 1) * sizeof(int64_t));\n+  memmove(op.rhs_contracting_dimensions_data,\n+          rhs_contracting_dimensions_values.data(),\n+          rhs_contracting_dimensions_values.size() * sizeof(int64_t));\n+  Tensor rhs_contracting_dimensions{\n+      .type = TensorType{.shape = lhs_contracting_dimensions_shape,\n+                         .element_type = DataType::kSI8},\n+      .data = op.rhs_contracting_dimensions_data};\n+\n+  std::vector<StorageT> dor_general_output_values(1);\n+  dor_general_output_values[0] = 0;\n+  op.output_dot_general_data = malloc(1 * sizeof(StorageT));\n+  memmove(op.output_dot_general_data, dor_general_output_values.data(),\n+          dor_general_output_values.size() * sizeof(StorageT));\n+  const Shape dor_general_output_shape{{1}};\n+  Tensor output_dot_general{\n+      .type = TensorType{.shape = dor_general_output_shape,\n+                         .element_type = storage_type},\n+      .data = op.output_dot_general_data};\n+\n+  Tensor lhs_batching_dimensions{\n+      .type = TensorType{.shape = Shape(), .element_type = DataType::kSI8},\n+      .data = {}};\n+  Tensor rhs_batching_dimensions{\n+      .type = TensorType{.shape = Shape(), .element_type = DataType::kSI8},\n+      .data = {}};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  op.dot_general_op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  auto state = Prepare(op.dot_general_op, lhs_dot_general, rhs_dot_general,\n+                       output_dot_general);\n+  // Dot general prepare end\n+\n+  // padding prepare\n+  const int64_t* lhs_dilation_buffer =\n+      op.attributes.lhs_dilation.GetDataAs<DataType::kSI64>();\n+  const int64_t* padding_buffer =\n+      op.attributes.padding.GetDataAs<DataType::kSI64>();\n+\n+  int lhs_padded_spacials[lhs_transposed.Rank() - 2];\n+  int lhs_padded_tensor_size = 1;\n+  for (size_t i = lhs_transposed.Rank() - 1; i > 1; --i) {\n+    lhs_padded_spacials[i - 2] =\n+        lhs_transposed.shape().Dim(i) +\n+        (lhs_dilation_buffer[i - 2] - 1) * (lhs_transposed.shape().Dim(i) - 1) +\n+        padding_buffer[2 * (i - 2)] + padding_buffer[(2 * (i - 2)) + 1];\n+    lhs_padded_tensor_size *= lhs_padded_spacials[i - 2];\n+  }\n+\n+  lhs_padded_tensor_size *=\n+      lhs_transposed.shape().Dim(0) * lhs_transposed.shape().Dim(1);\n+  op.lhs_padded_data = malloc(lhs_padded_tensor_size * sizeof(StorageT));\n+  DimVector<DimensionSize> lhs_padding_shape_dims(lhs_transposed.Rank());\n+  lhs_padding_shape_dims[0] = lhs_transposed.shape().Dim(0);\n+  lhs_padding_shape_dims[1] = lhs_transposed.shape().Dim(1);\n+  for (size_t i = 0; i < lhs_transposed.Rank() - 2; ++i) {\n+    lhs_padding_shape_dims[i + 2] =\n+        static_cast<int64_t>(lhs_padded_spacials[i]);\n+  }\n+  const Shape lhs_padding_shape(lhs_padding_shape_dims);\n+  Tensor lhs_padded{.type = TensorType{.shape = lhs_padding_shape,\n+                                       .element_type = storage_type},\n+                    .data = op.lhs_padded_data};\n+  // padding prepare end\n+\n+  // Split prepare\n+  int64_t num_splits =\n+      op.attributes.batch_group_count * op.attributes.feature_group_count;\n+  int64_t split_dimension = 0;\n+  for (int64_t i = 0; i < num_splits; ++i) {\n+    DimVector<DimensionSize> rhs_split_dims(rhs_transposed.Rank());\n+    for (size_t i = 0; i < rhs_transposed.Rank(); ++i) {\n+      if (i == split_dimension) {\n+        rhs_split_dims[i] = (rhs_transposed.shape().Dim(i) / num_splits);\n+      } else {\n+        rhs_split_dims[i] = rhs_transposed.shape().Dim(i);\n+      }\n+    }\n+    const Shape rhs_split_shape(rhs_split_dims);\n+    void* rhs_split_data =\n+        malloc((rhs_transposed.NumElements() / num_splits) * sizeof(StorageT));\n+    op.rhs_splits_data.push_back(rhs_split_data);\n+    Tensor rhs_split{.type = TensorType{.shape = rhs_split_shape,\n+                                        .element_type = storage_type},\n+                     .data = rhs_split_data};\n+    op.rhs_splits.push_back(rhs_split);\n+  }\n+\n+  if (op.attributes.feature_group_count > 1) {\n+    split_dimension = 1;\n+  }\n+\n+  for (int64_t i = 0; i < num_splits; ++i) {\n+    DimVector<DimensionSize> lhs_split_dims(lhs_padded.Rank());\n+    for (size_t i = 0; i < lhs_padded.Rank(); ++i) {\n+      if (i == split_dimension) {\n+        lhs_split_dims[i] = (lhs_padded.shape().Dim(i) / num_splits);\n+      } else {\n+        lhs_split_dims[i] = lhs_padded.shape().Dim(i);\n+      }\n+    }\n+    const Shape lhs_split_shape(lhs_split_dims);\n+    void* lhs_split_data =\n+        malloc((lhs_padded.NumElements() / num_splits) * sizeof(StorageT));\n+    op.lhs_splits_data.push_back(lhs_split_data);\n+    Tensor lhs_split{.type = TensorType{.shape = lhs_split_shape,\n+                                        .element_type = storage_type},\n+                     .data = lhs_split_data};\n+    op.lhs_splits.push_back(lhs_split);\n+  }\n+  // split prepare end\n+\n+  // quantized tensors prepare\n+  if (lhs.IsQuantized()) {\n+    op.lhs_dequantized_data = malloc(lhs.NumElements() * sizeof(StorageT));\n+    const Shape lhs_dequantized_shape = lhs.shape();\n+    Tensor lhs_dequantized{.type = TensorType{.shape = lhs_dequantized_shape,\n+                                              .element_type = storage_type},\n+                           .data = op.lhs_dequantized_data};\n+    op.rhs_dequantized_data = malloc(rhs.NumElements() * sizeof(StorageT));\n+    const Shape rhs_dequantized_shape = rhs.shape();\n+    Tensor rhs_dequantized{.type = TensorType{.shape = rhs_dequantized_shape,\n+                                              .element_type = storage_type},\n+                           .data = op.rhs_dequantized_data};\n+    op.output_dequantized_data =\n+        malloc(output.NumElements() * sizeof(StorageT));\n+    const Shape output_dequantized_shape = output.shape();\n+    Tensor output_dequantized{\n+        .type = TensorType{.shape = output_dequantized_shape,\n+                           .element_type = storage_type},\n+        .data = op.output_dequantized_data};\n+\n+    op.lhs_dequantized = std::move(lhs_dequantized);\n+    op.rhs_dequantized = std::move(rhs_dequantized);\n+    op.output_dequantized = std::move(output_dequantized);\n+  }\n+  // quantized tensors prepare end\n+\n+  op.lhs_permutations = std::move(lhs_permutations);\n+  op.lhs_transposed = std::move(lhs_transposed);\n+  op.rhs_permutations = std::move(rhs_permutations);\n+  op.rhs_transposed = std::move(rhs_transposed);\n+  op.output_permutations = std::move(output_permutations);\n+  op.output_transposed = std::move(output_transposed);\n+  op.lhs_dot_general = std::move(lhs_dot_general);\n+  op.rhs_dot_general = std::move(rhs_dot_general);\n+  op.output_dot_general = std::move(output_dot_general);\n+  op.lhs_padded = std::move(lhs_padded);\n+\n+  const int64_t* rhs_dilation_buffer =\n+      op.attributes.rhs_dilation.GetDataAs<DataType::kSI64>();\n+  const int64_t* window_strides_pointer =\n+      op.attributes.window_strides.GetDataAs<DataType::kSI64>();\n+\n+  // Constraints Check\n+  if (op.attributes.precision_configs.size() != 2) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.convolution: Size of precision_config must be two.\");\n+  }\n+  if (op.attributes.precision_configs[0] != PrecisionTypes::DEFAULT &&\n+      op.attributes.precision_configs[1] != PrecisionTypes::DEFAULT) {\n+    return absl::UnimplementedError(\n+        \"stablehlo.convolution: Currently the precision_config supports \"\n+        \"DEFAULT configuration only.\");\n+  }\n+  size_t rank = lhs.Rank();\n+  if (lhs.Rank() != rhs.Rank()) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: rank(lhs) == rank(rhs)\");\n+  } else if (output.Rank() != lhs.Rank()) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: rank(output) == lhs.Rank()\");\n+  }\n+  if (!lhs.IsQuantized()) {\n+    SHLO_REF_RETURN_ON_ERROR(\n+        CheckSameBaselineType(CheckCtx(\"Convolution\"), lhs, rhs));\n+    SHLO_REF_RETURN_ON_ERROR(\n+        CheckSameBaselineType(CheckCtx(\"Convolution\"), lhs, output));\n+  }\n+  if (op.attributes.window_strides.shape().Dim(0) != rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: size(windowStride)=rank-2\");\n+  }\n+\n+  const int64_t* check_buffer =\n+      op.attributes.window_strides.GetDataAs<DataType::kSI64>();\n+  bool is_greater_than_zero = true;\n+  size_t n = op.attributes.window_strides.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    if (check_buffer[i] == 0) {\n+      is_greater_than_zero = false;\n+      exit;\n+    }\n+  }\n+\n+  if (!is_greater_than_zero) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<windowStride\");\n+  } else if (op.attributes.padding.shape().Dim(0) != rank - 2 ||\n+             op.attributes.padding.shape().Dim(1) != 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: shape(padding)=[rank-2,2]\");\n+  } else if (op.attributes.lhs_dilation.shape().Dim(0) != rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Contraint violation: shape(lhs_dilation) == rank-2\");\n+  }\n+\n+  check_buffer = op.attributes.lhs_dilation.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.lhs_dilation.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    if (check_buffer[i] == 0) {\n+      is_greater_than_zero = false;\n+      exit;\n+    }\n+  }\n+\n+  if (!is_greater_than_zero) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<lhs_dilation\");\n+  } else if (op.attributes.rhs_dilation.shape().Dim(0) != rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: shape(rhs_dilation) == rank-2\");\n+  }\n+\n+  check_buffer = op.attributes.rhs_dilation.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.rhs_dilation.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    if (check_buffer[i] == 0) {\n+      is_greater_than_zero = false;\n+      exit;\n+    }\n+  }\n+\n+  if (!is_greater_than_zero) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<rhs_dilation\");\n+  } else if (op.attributes.window_reversal.shape().Dim(0) != rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: shape(window_reversal) == rank-2\");\n+  } else if (lhs.shape().Dim(\n+                 static_cast<size_t>(op.attributes.input_batch_dimension)) %\n+                 op.attributes.batch_group_count !=\n+             0) {\n+    return absl::FailedPreconditionError(\n+        \"Contraint violation: Dim(lhs,input_batch_dimension)%batch_group_count \"\n+        \"= \"\n+        \"0\");\n+  } else if (lhs.shape().Dim(\n+                 static_cast<size_t>(op.attributes.input_feature_dimension)) %\n+                 op.attributes.feature_group_count !=\n+             0) {\n+    return absl::FailedPreconditionError(\n+        \"Contraint violation: \"\n+        \"Dim(lhs,input_feature_dimension)%(feature_group_count) = 0\");\n+  } else if (op.attributes.input_spacial_dimensions.shape().Dim(0) !=\n+             rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constarint violation: size(input_spacial_dimensions) = rank-2\");\n+  }\n+\n+  DimVector<int64_t> vec;\n+  vec.push_back(op.attributes.input_batch_dimension);\n+  vec.push_back(op.attributes.input_feature_dimension);\n+  check_buffer =\n+      op.attributes.input_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.input_spacial_dimensions.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    vec.push_back(check_buffer[i]);\n+  }\n+\n+  if (!(IsUnique(vec))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: isUnique(inputDimensions)\");\n+  } else if (!(IsInRange(vec, rank))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<= inputDimensions < rank\");\n+  } else if (rhs.shape().Dim(static_cast<size_t>(\n+                 op.attributes.kernel_input_feature_dimension)) !=\n+             lhs.shape().Dim(\n+                 static_cast<size_t>(op.attributes.input_feature_dimension)) /\n+                 op.attributes.feature_group_count) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: Dim(rhs,kernel_input_feature_dimension) = \"\n+        \"Dim(lhs,input_feature_dimension)/feature_group_count\");\n+  } else if (rhs.shape().Dim(static_cast<size_t>(\n+                 op.attributes.kernel_output_feature_dimension)) %\n+                 op.attributes.batch_group_count !=\n+             0) {\n+    return absl::FailedPreconditionError(\n+        \"Constarint violation: \"\n+        \"Dim(rhs,kernel_output_feature_dimension)%batch_group_count=0\");\n+  } else if (rhs.shape().Dim(static_cast<size_t>(\n+                 op.attributes.kernel_output_feature_dimension)) %\n+                 op.attributes.feature_group_count !=\n+             0) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: \"\n+        \"Dim(rhs,kernel_output_feature_dimension)%(feature_group_count)=0\");\n+  } else if (op.attributes.kernel_spacial_dimensions.shape().Dim(0) !=\n+             rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: size(kernel_spacial_dimensions) = rank-2\");\n+  }\n+\n+  vec.clear();\n+  vec.push_back(op.attributes.kernel_input_feature_dimension);\n+  vec.push_back(op.attributes.kernel_output_feature_dimension);\n+  check_buffer =\n+      op.attributes.kernel_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.kernel_spacial_dimensions.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    vec.push_back(check_buffer[i]);\n+  }\n+\n+  if (!(IsUnique(vec))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: isUnique(kernelDimensions)\");\n+  } else if (!(IsInRange(vec, rank))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<= kernelDimensions < rank\");\n+  } else if (op.attributes.output_spacial_dimensions.shape().Dim(0) !=\n+             rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: size(output_spacial_dimensions) = rank-2\");\n+  }\n+\n+  vec.clear();\n+  vec.push_back(op.attributes.output_batch_dimension);\n+  vec.push_back(op.attributes.output_feature_dimension);\n+  check_buffer =\n+      op.attributes.output_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.output_spacial_dimensions.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    vec.push_back(check_buffer[i]);\n+  }\n+\n+  if (!(IsUnique(vec))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: isUnique(outputDimensions)\");\n+  } else if (!(IsInRange(vec, rank))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<= outputDimensions < rank\");\n+  } else if (op.attributes.feature_group_count <= 0) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<feature_group_count\");\n+  } else if (op.attributes.batch_group_count <= 0) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<batch_group_count\");\n+  } else if (op.attributes.batch_group_count != 1 &&\n+             op.attributes.feature_group_count != 1) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: batch_group_count == 1 or feature_group_count \"\n+        \"== 1\");\n+  } else if (output.shape().Dim(\n+                 static_cast<size_t>(op.attributes.output_batch_dimension)) !=\n+             lhs.shape().Dim(\n+                 static_cast<size_t>(op.attributes.input_batch_dimension)) /\n+                 op.attributes.batch_group_count) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: output.shape().Dim(output_batch_dimension) == \"\n+        \"lhs.shape().Dim(input_batch_dimension)/batch_group_count\");\n+  } else if (output.shape().Dim(\n+                 static_cast<size_t>(op.attributes.output_feature_dimension)) !=\n+             rhs.shape().Dim(static_cast<size_t>(\n+                 op.attributes.kernel_output_feature_dimension))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: output.shape().Dim(outputFeatureDimention) == \"\n+        \"rhs.shape().Dim(kernel_output_feature_dimension)\");\n+  }\n+\n+  for (int64_t i = 0; i < output.Rank() - 2; ++i) {\n+    if (output.shape().Dim(static_cast<size_t>(output_spacial_pointer[i])) !=\n+        std::floor(\n+            (((lhs_dilation_buffer[i] *\n+               (lhs.shape().Dim(static_cast<size_t>(input_spacial_pointer[i])) -\n+                1)) +\n+              1 + padding_buffer[2 * i] + padding_buffer[2 * i + 1] -\n+              ((rhs_dilation_buffer[i] * (rhs.shape().Dim(static_cast<size_t>(\n+                                              window_spacial_pointer[i])) -\n+                                          1)) +\n+               1)) /\n+             window_strides_pointer[i]) +\n+            1)) {\n+      return absl::FailedPreconditionError(\n+          \"Constraint violation: output.shape().Dim(spacial_dim) is not \"\n+          \"properly set\");\n+    }\n+  }\n+\n+  if (lhs.IsQuantized() || rhs.IsQuantized() || output.IsQuantized()) {\n+    if (!(lhs.IsQuantized() && rhs.IsQuantized() && output.IsQuantized())) {\n+      return absl::FailedPreconditionError(\n+          \"Constraint violation: lhs.IsQuantized() && rhs.IsQuantized() && \"\n+          \"output.IsQuantized()\");\n+    }\n+    if (rhs.IsPerTensorQuantized()) {\n+      if (!(output.IsPerTensorQuantized())) {\n+        return absl::FailedPreconditionError(\n+            \"Constraint violation: If is_per_tensor_quantized(rhs), then \"\n+            \"is_per_tensor_quantized(output)\");\n+      }\n+    }\n+    if (rhs.IsPerAxisQuantized()) {\n+      if (rhs.quantized_per_axis_element_type().QuantizedDimension() !=\n+          op.attributes.kernel_output_feature_dimension) {\n+        return absl::FailedPreconditionError(\n+            \"Constraint violation:  If is_per_axis_quantized(rhs), then \"\n+            \"quantization_dimension(rhs) = \"\n+            \"op.attributes.kernel_output_feature_dimension\");\n+      }\n+    }\n+    if (output.IsPerAxisQuantized()) {\n+      if (output.quantized_per_axis_element_type().QuantizedDimension() !=\n+          op.attributes.output_feature_dimension) {\n+        return absl::FailedPreconditionError(\n+            \"Constraint violation:  If is_per_axis_quantized(output), then \"\n+            \"quantization_dimension(output) = \"\n+            \"op.attributes.output_feature_dimension\");\n+      }\n+    }\n+  }\n+\n+  return absl::OkStatus();\n+}\n+\n+// Slice op basic implimentation in context of Convolution\n+template <DataType storage_type>\n+void EvalDynamicSliceOp(const Tensor& operand, int64_t num_outputs,\n+                        size_t start_indices, size_t inner_dimensions_size,\n+                        size_t outer_dimensions_size, int64_t dimension,\n+                        Tensor& output) {\n+  using StorageT = StorageType<storage_type>;\n+\n+  StorageT* output_buffer = output.GetDataAs<storage_type>();\n+  const StorageT* operand_buffer = operand.GetDataAs<storage_type>();\n+\n+  size_t i = start_indices;\n+  size_t k = 0;\n+  while (i < operand.NumElements()) {\n+    for (size_t j = 0;\n+         j < (output.shape().Dim(dimension) * inner_dimensions_size);",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1578299493",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 66299,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/convolution.cc",
        "discussion_id": "1578299493",
        "commented_code": "@@ -0,0 +1,1171 @@\n+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/convolution.h\"\n+\n+#include <algorithm>\n+#include <cstddef>\n+#include <string>\n+#include <type_traits>\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/unary_elementwise.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+template <class T>\n+using DimVector = absl::InlinedVector<T, 6>;\n+\n+bool IsUnique(DimVector<int64_t>& vec) {\n+  std::sort(vec.begin(), vec.end());\n+  return std::unique(vec.begin(), vec.end()) == vec.end();\n+}\n+\n+bool IsInRange(DimVector<int64_t>& vec, size_t N) {\n+  for (int64_t dim : vec) {\n+    if (dim >= N || dim < 0) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+template <DataType storage_type>\n+absl::Status PrepareImpl(ConvolutionOp& op, const Tensor& lhs,\n+                         const Tensor& rhs, Tensor& output) {\n+  using StorageT = StorageType<storage_type>;\n+\n+  // Transpose prepare\n+  const int64_t* window_spacial_pointer =\n+      op.attributes.kernel_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  const int64_t* output_spacial_pointer =\n+      op.attributes.output_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  const int64_t* input_spacial_pointer =\n+      op.attributes.input_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+\n+  std::vector<StorageT> lhs_permutation_values(\n+      static_cast<int64_t>(lhs.Rank()));\n+  lhs_permutation_values[0] = op.attributes.input_batch_dimension;\n+  lhs_permutation_values[1] = op.attributes.input_feature_dimension;\n+  DimVector<DimensionSize> lhs_shape_dims(lhs.Rank());\n+  lhs_shape_dims[0] =\n+      lhs.shape().Dim(static_cast<size_t>(op.attributes.input_batch_dimension));\n+  lhs_shape_dims[1] = lhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.input_feature_dimension));\n+  for (size_t i = 0; i < lhs.Rank() - 2; ++i) {\n+    lhs_shape_dims[i + 2] =\n+        lhs.shape().Dim(static_cast<size_t>(input_spacial_pointer[i]));\n+    lhs_permutation_values[i + 2] = input_spacial_pointer[i];\n+  }\n+  // malloc is used to have the storage space available out of prepare function\n+  // scope and it's pointer is stored in class data member to\n+  // deallocate the memory in destructor.\n+  op.lhs_permutation_data =\n+      malloc(lhs_permutation_values.size() * sizeof(StorageT));\n+  memmove(op.lhs_permutation_data, lhs_permutation_values.data(),\n+          lhs_permutation_values.size() * sizeof(StorageT));\n+  const Shape lhs_permutation_shape({static_cast<int64_t>(lhs.Rank())});\n+  Tensor lhs_permutations{.type = TensorType{.shape = lhs_permutation_shape,\n+                                             .element_type = storage_type},\n+                          .data = op.lhs_permutation_data};\n+\n+  op.lhs_transposed_data = malloc(lhs.NumElements() * sizeof(StorageT));\n+  const Shape lhs_transposed_shape(lhs_shape_dims);\n+  Tensor lhs_transposed{.type = TensorType{.shape = lhs_transposed_shape,\n+                                           .element_type = storage_type},\n+                        .data = op.lhs_transposed_data};\n+\n+  std::vector<StorageT> rhs_permutation_values(\n+      static_cast<int64_t>(rhs.Rank()));\n+  rhs_permutation_values[0] = op.attributes.kernel_output_feature_dimension;\n+  rhs_permutation_values[1] = op.attributes.kernel_input_feature_dimension;\n+  DimVector<DimensionSize> rhs_shape_dims(rhs.Rank());\n+  rhs_shape_dims[0] = rhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.kernel_output_feature_dimension));\n+  rhs_shape_dims[1] = rhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.kernel_input_feature_dimension));\n+  for (size_t i = 0; i < rhs.Rank() - 2; ++i) {\n+    rhs_shape_dims[i + 2] =\n+        rhs.shape().Dim(static_cast<size_t>(window_spacial_pointer[i]));\n+    rhs_permutation_values[i + 2] = window_spacial_pointer[i];\n+  }\n+  op.rhs_permutation_data = malloc(rhs.Rank() * sizeof(StorageT));\n+  memmove(op.rhs_permutation_data, rhs_permutation_values.data(),\n+          rhs_permutation_values.size() * sizeof(StorageT));\n+  const Shape rhs_permutation_shape({static_cast<int64_t>(rhs.Rank())});\n+  Tensor rhs_permutations{.type = TensorType{.shape = rhs_permutation_shape,\n+                                             .element_type = storage_type},\n+                          .data = op.rhs_permutation_data};\n+\n+  op.rhs_transposed_data = malloc(rhs.NumElements() * sizeof(StorageT));\n+  const Shape rhs_transposed_shape(rhs_shape_dims);\n+  Tensor rhs_transposed{.type = TensorType{.shape = rhs_transposed_shape,\n+                                           .element_type = storage_type},\n+                        .data = op.rhs_transposed_data};\n+\n+  std::vector<StorageT> output_permutation_values(\n+      static_cast<int64_t>(output.Rank()));\n+  output_permutation_values[0] = op.attributes.output_batch_dimension;\n+  output_permutation_values[1] = op.attributes.output_feature_dimension;\n+  DimVector<DimensionSize> output_shape_dims(output.Rank());\n+  output_shape_dims[0] = output.shape().Dim(\n+      static_cast<size_t>(op.attributes.output_batch_dimension));\n+  output_shape_dims[1] = output.shape().Dim(\n+      static_cast<size_t>(op.attributes.output_feature_dimension));\n+  for (size_t i = 0; i < output.Rank() - 2; ++i) {\n+    output_shape_dims[i + 2] =\n+        output.shape().Dim(static_cast<size_t>(output_spacial_pointer[i]));\n+    output_permutation_values[i + 2] = output_spacial_pointer[i];\n+  }\n+  op.output_permutation_data = malloc(output.Rank() * sizeof(StorageT));\n+  memmove(op.output_permutation_data, output_permutation_values.data(),\n+          output_permutation_values.size() * sizeof(StorageT));\n+  const Shape output_permutation_shape({static_cast<int64_t>(output.Rank())});\n+  Tensor output_permutations{\n+      .type = TensorType{.shape = output_permutation_shape,\n+                         .element_type = storage_type},\n+      .data = op.output_permutation_data};\n+\n+  op.output_transposed_data = malloc(output.NumElements() * sizeof(StorageT));\n+  const Shape output_transposed_shape(output_shape_dims);\n+  Tensor output_transposed{.type = TensorType{.shape = output_transposed_shape,\n+                                              .element_type = storage_type},\n+                           .data = op.output_transposed_data};\n+  // transpose prepare end\n+\n+  // DotGeneral prepare\n+  DimVector<DimensionSize> dims(rhs_transposed.Rank());\n+  size_t rhs_transposed_tensor_size = 1;\n+  dims[0] = 1;\n+  for (size_t i = 1; i < rhs_transposed.Rank(); ++i) {\n+    dims[i] = rhs_transposed.shape().Dim(i);\n+    rhs_transposed_tensor_size *= rhs_transposed.shape().Dim(i);\n+  }\n+  const Shape rhs_dot_general_shape(dims);\n+  op.rhs_dot_general_data =\n+      malloc(rhs_transposed_tensor_size * sizeof(StorageT));\n+  Tensor rhs_dot_general{.type = TensorType{.shape = rhs_dot_general_shape,\n+                                            .element_type = storage_type},\n+                         .data = op.rhs_dot_general_data};\n+\n+  op.lhs_dot_general_data =\n+      malloc(rhs_transposed_tensor_size * sizeof(StorageT));\n+  Tensor lhs_dot_general{.type = TensorType{.shape = rhs_dot_general_shape,\n+                                            .element_type = storage_type},\n+                         .data = op.lhs_dot_general_data};\n+\n+  std::vector<typename Storage<DataType::kSI64>::Type>\n+      lhs_contracting_dimensions_values(lhs_transposed.Rank() - 1);\n+  for (size_t i = 0; i < lhs_transposed.Rank() - 1; ++i) {\n+    lhs_contracting_dimensions_values[i] = i + 1;\n+  }\n+  op.lhs_contracting_dimensions_data =\n+      malloc((lhs_transposed.Rank() - 1) * sizeof(int64_t));\n+  memmove(op.lhs_contracting_dimensions_data,\n+          lhs_contracting_dimensions_values.data(),\n+          lhs_contracting_dimensions_values.size() * sizeof(int64_t));\n+  const Shape lhs_contracting_dimensions_shape(\n+      {static_cast<int64_t>(lhs_transposed.Rank() - 1)});\n+  Tensor lhs_contracting_dimensions{\n+      .type = TensorType{.shape = lhs_contracting_dimensions_shape,\n+                         .element_type = DataType::kSI64},\n+      .data = op.lhs_contracting_dimensions_data};\n+\n+  std::vector<typename Storage<DataType::kSI64>::Type>\n+      rhs_contracting_dimensions_values(rhs_transposed.Rank() - 1);\n+  for (size_t i = 0; i < rhs_transposed.Rank() - 1; ++i) {\n+    rhs_contracting_dimensions_values[i] = i + 1;\n+  }\n+  op.rhs_contracting_dimensions_data =\n+      malloc((rhs_transposed.Rank() - 1) * sizeof(int64_t));\n+  memmove(op.rhs_contracting_dimensions_data,\n+          rhs_contracting_dimensions_values.data(),\n+          rhs_contracting_dimensions_values.size() * sizeof(int64_t));\n+  Tensor rhs_contracting_dimensions{\n+      .type = TensorType{.shape = lhs_contracting_dimensions_shape,\n+                         .element_type = DataType::kSI8},\n+      .data = op.rhs_contracting_dimensions_data};\n+\n+  std::vector<StorageT> dor_general_output_values(1);\n+  dor_general_output_values[0] = 0;\n+  op.output_dot_general_data = malloc(1 * sizeof(StorageT));\n+  memmove(op.output_dot_general_data, dor_general_output_values.data(),\n+          dor_general_output_values.size() * sizeof(StorageT));\n+  const Shape dor_general_output_shape{{1}};\n+  Tensor output_dot_general{\n+      .type = TensorType{.shape = dor_general_output_shape,\n+                         .element_type = storage_type},\n+      .data = op.output_dot_general_data};\n+\n+  Tensor lhs_batching_dimensions{\n+      .type = TensorType{.shape = Shape(), .element_type = DataType::kSI8},\n+      .data = {}};\n+  Tensor rhs_batching_dimensions{\n+      .type = TensorType{.shape = Shape(), .element_type = DataType::kSI8},\n+      .data = {}};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  op.dot_general_op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  auto state = Prepare(op.dot_general_op, lhs_dot_general, rhs_dot_general,\n+                       output_dot_general);\n+  // Dot general prepare end\n+\n+  // padding prepare\n+  const int64_t* lhs_dilation_buffer =\n+      op.attributes.lhs_dilation.GetDataAs<DataType::kSI64>();\n+  const int64_t* padding_buffer =\n+      op.attributes.padding.GetDataAs<DataType::kSI64>();\n+\n+  int lhs_padded_spacials[lhs_transposed.Rank() - 2];\n+  int lhs_padded_tensor_size = 1;\n+  for (size_t i = lhs_transposed.Rank() - 1; i > 1; --i) {\n+    lhs_padded_spacials[i - 2] =\n+        lhs_transposed.shape().Dim(i) +\n+        (lhs_dilation_buffer[i - 2] - 1) * (lhs_transposed.shape().Dim(i) - 1) +\n+        padding_buffer[2 * (i - 2)] + padding_buffer[(2 * (i - 2)) + 1];\n+    lhs_padded_tensor_size *= lhs_padded_spacials[i - 2];\n+  }\n+\n+  lhs_padded_tensor_size *=\n+      lhs_transposed.shape().Dim(0) * lhs_transposed.shape().Dim(1);\n+  op.lhs_padded_data = malloc(lhs_padded_tensor_size * sizeof(StorageT));\n+  DimVector<DimensionSize> lhs_padding_shape_dims(lhs_transposed.Rank());\n+  lhs_padding_shape_dims[0] = lhs_transposed.shape().Dim(0);\n+  lhs_padding_shape_dims[1] = lhs_transposed.shape().Dim(1);\n+  for (size_t i = 0; i < lhs_transposed.Rank() - 2; ++i) {\n+    lhs_padding_shape_dims[i + 2] =\n+        static_cast<int64_t>(lhs_padded_spacials[i]);\n+  }\n+  const Shape lhs_padding_shape(lhs_padding_shape_dims);\n+  Tensor lhs_padded{.type = TensorType{.shape = lhs_padding_shape,\n+                                       .element_type = storage_type},\n+                    .data = op.lhs_padded_data};\n+  // padding prepare end\n+\n+  // Split prepare\n+  int64_t num_splits =\n+      op.attributes.batch_group_count * op.attributes.feature_group_count;\n+  int64_t split_dimension = 0;\n+  for (int64_t i = 0; i < num_splits; ++i) {\n+    DimVector<DimensionSize> rhs_split_dims(rhs_transposed.Rank());\n+    for (size_t i = 0; i < rhs_transposed.Rank(); ++i) {\n+      if (i == split_dimension) {\n+        rhs_split_dims[i] = (rhs_transposed.shape().Dim(i) / num_splits);\n+      } else {\n+        rhs_split_dims[i] = rhs_transposed.shape().Dim(i);\n+      }\n+    }\n+    const Shape rhs_split_shape(rhs_split_dims);\n+    void* rhs_split_data =\n+        malloc((rhs_transposed.NumElements() / num_splits) * sizeof(StorageT));\n+    op.rhs_splits_data.push_back(rhs_split_data);\n+    Tensor rhs_split{.type = TensorType{.shape = rhs_split_shape,\n+                                        .element_type = storage_type},\n+                     .data = rhs_split_data};\n+    op.rhs_splits.push_back(rhs_split);\n+  }\n+\n+  if (op.attributes.feature_group_count > 1) {\n+    split_dimension = 1;\n+  }\n+\n+  for (int64_t i = 0; i < num_splits; ++i) {\n+    DimVector<DimensionSize> lhs_split_dims(lhs_padded.Rank());\n+    for (size_t i = 0; i < lhs_padded.Rank(); ++i) {\n+      if (i == split_dimension) {\n+        lhs_split_dims[i] = (lhs_padded.shape().Dim(i) / num_splits);\n+      } else {\n+        lhs_split_dims[i] = lhs_padded.shape().Dim(i);\n+      }\n+    }\n+    const Shape lhs_split_shape(lhs_split_dims);\n+    void* lhs_split_data =\n+        malloc((lhs_padded.NumElements() / num_splits) * sizeof(StorageT));\n+    op.lhs_splits_data.push_back(lhs_split_data);\n+    Tensor lhs_split{.type = TensorType{.shape = lhs_split_shape,\n+                                        .element_type = storage_type},\n+                     .data = lhs_split_data};\n+    op.lhs_splits.push_back(lhs_split);\n+  }\n+  // split prepare end\n+\n+  // quantized tensors prepare\n+  if (lhs.IsQuantized()) {\n+    op.lhs_dequantized_data = malloc(lhs.NumElements() * sizeof(StorageT));\n+    const Shape lhs_dequantized_shape = lhs.shape();\n+    Tensor lhs_dequantized{.type = TensorType{.shape = lhs_dequantized_shape,\n+                                              .element_type = storage_type},\n+                           .data = op.lhs_dequantized_data};\n+    op.rhs_dequantized_data = malloc(rhs.NumElements() * sizeof(StorageT));\n+    const Shape rhs_dequantized_shape = rhs.shape();\n+    Tensor rhs_dequantized{.type = TensorType{.shape = rhs_dequantized_shape,\n+                                              .element_type = storage_type},\n+                           .data = op.rhs_dequantized_data};\n+    op.output_dequantized_data =\n+        malloc(output.NumElements() * sizeof(StorageT));\n+    const Shape output_dequantized_shape = output.shape();\n+    Tensor output_dequantized{\n+        .type = TensorType{.shape = output_dequantized_shape,\n+                           .element_type = storage_type},\n+        .data = op.output_dequantized_data};\n+\n+    op.lhs_dequantized = std::move(lhs_dequantized);\n+    op.rhs_dequantized = std::move(rhs_dequantized);\n+    op.output_dequantized = std::move(output_dequantized);\n+  }\n+  // quantized tensors prepare end\n+\n+  op.lhs_permutations = std::move(lhs_permutations);\n+  op.lhs_transposed = std::move(lhs_transposed);\n+  op.rhs_permutations = std::move(rhs_permutations);\n+  op.rhs_transposed = std::move(rhs_transposed);\n+  op.output_permutations = std::move(output_permutations);\n+  op.output_transposed = std::move(output_transposed);\n+  op.lhs_dot_general = std::move(lhs_dot_general);\n+  op.rhs_dot_general = std::move(rhs_dot_general);\n+  op.output_dot_general = std::move(output_dot_general);\n+  op.lhs_padded = std::move(lhs_padded);\n+\n+  const int64_t* rhs_dilation_buffer =\n+      op.attributes.rhs_dilation.GetDataAs<DataType::kSI64>();\n+  const int64_t* window_strides_pointer =\n+      op.attributes.window_strides.GetDataAs<DataType::kSI64>();\n+\n+  // Constraints Check\n+  if (op.attributes.precision_configs.size() != 2) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.convolution: Size of precision_config must be two.\");\n+  }\n+  if (op.attributes.precision_configs[0] != PrecisionTypes::DEFAULT &&\n+      op.attributes.precision_configs[1] != PrecisionTypes::DEFAULT) {\n+    return absl::UnimplementedError(\n+        \"stablehlo.convolution: Currently the precision_config supports \"\n+        \"DEFAULT configuration only.\");\n+  }\n+  size_t rank = lhs.Rank();\n+  if (lhs.Rank() != rhs.Rank()) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: rank(lhs) == rank(rhs)\");\n+  } else if (output.Rank() != lhs.Rank()) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: rank(output) == lhs.Rank()\");\n+  }\n+  if (!lhs.IsQuantized()) {\n+    SHLO_REF_RETURN_ON_ERROR(\n+        CheckSameBaselineType(CheckCtx(\"Convolution\"), lhs, rhs));\n+    SHLO_REF_RETURN_ON_ERROR(\n+        CheckSameBaselineType(CheckCtx(\"Convolution\"), lhs, output));\n+  }\n+  if (op.attributes.window_strides.shape().Dim(0) != rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: size(windowStride)=rank-2\");\n+  }\n+\n+  const int64_t* check_buffer =\n+      op.attributes.window_strides.GetDataAs<DataType::kSI64>();\n+  bool is_greater_than_zero = true;\n+  size_t n = op.attributes.window_strides.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    if (check_buffer[i] == 0) {\n+      is_greater_than_zero = false;\n+      exit;\n+    }\n+  }\n+\n+  if (!is_greater_than_zero) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<windowStride\");\n+  } else if (op.attributes.padding.shape().Dim(0) != rank - 2 ||\n+             op.attributes.padding.shape().Dim(1) != 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: shape(padding)=[rank-2,2]\");\n+  } else if (op.attributes.lhs_dilation.shape().Dim(0) != rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Contraint violation: shape(lhs_dilation) == rank-2\");\n+  }\n+\n+  check_buffer = op.attributes.lhs_dilation.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.lhs_dilation.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    if (check_buffer[i] == 0) {\n+      is_greater_than_zero = false;\n+      exit;\n+    }\n+  }\n+\n+  if (!is_greater_than_zero) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<lhs_dilation\");\n+  } else if (op.attributes.rhs_dilation.shape().Dim(0) != rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: shape(rhs_dilation) == rank-2\");\n+  }\n+\n+  check_buffer = op.attributes.rhs_dilation.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.rhs_dilation.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    if (check_buffer[i] == 0) {\n+      is_greater_than_zero = false;\n+      exit;\n+    }\n+  }\n+\n+  if (!is_greater_than_zero) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<rhs_dilation\");\n+  } else if (op.attributes.window_reversal.shape().Dim(0) != rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: shape(window_reversal) == rank-2\");\n+  } else if (lhs.shape().Dim(\n+                 static_cast<size_t>(op.attributes.input_batch_dimension)) %\n+                 op.attributes.batch_group_count !=\n+             0) {\n+    return absl::FailedPreconditionError(\n+        \"Contraint violation: Dim(lhs,input_batch_dimension)%batch_group_count \"\n+        \"= \"\n+        \"0\");\n+  } else if (lhs.shape().Dim(\n+                 static_cast<size_t>(op.attributes.input_feature_dimension)) %\n+                 op.attributes.feature_group_count !=\n+             0) {\n+    return absl::FailedPreconditionError(\n+        \"Contraint violation: \"\n+        \"Dim(lhs,input_feature_dimension)%(feature_group_count) = 0\");\n+  } else if (op.attributes.input_spacial_dimensions.shape().Dim(0) !=\n+             rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constarint violation: size(input_spacial_dimensions) = rank-2\");\n+  }\n+\n+  DimVector<int64_t> vec;\n+  vec.push_back(op.attributes.input_batch_dimension);\n+  vec.push_back(op.attributes.input_feature_dimension);\n+  check_buffer =\n+      op.attributes.input_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.input_spacial_dimensions.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    vec.push_back(check_buffer[i]);\n+  }\n+\n+  if (!(IsUnique(vec))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: isUnique(inputDimensions)\");\n+  } else if (!(IsInRange(vec, rank))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<= inputDimensions < rank\");\n+  } else if (rhs.shape().Dim(static_cast<size_t>(\n+                 op.attributes.kernel_input_feature_dimension)) !=\n+             lhs.shape().Dim(\n+                 static_cast<size_t>(op.attributes.input_feature_dimension)) /\n+                 op.attributes.feature_group_count) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: Dim(rhs,kernel_input_feature_dimension) = \"\n+        \"Dim(lhs,input_feature_dimension)/feature_group_count\");\n+  } else if (rhs.shape().Dim(static_cast<size_t>(\n+                 op.attributes.kernel_output_feature_dimension)) %\n+                 op.attributes.batch_group_count !=\n+             0) {\n+    return absl::FailedPreconditionError(\n+        \"Constarint violation: \"\n+        \"Dim(rhs,kernel_output_feature_dimension)%batch_group_count=0\");\n+  } else if (rhs.shape().Dim(static_cast<size_t>(\n+                 op.attributes.kernel_output_feature_dimension)) %\n+                 op.attributes.feature_group_count !=\n+             0) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: \"\n+        \"Dim(rhs,kernel_output_feature_dimension)%(feature_group_count)=0\");\n+  } else if (op.attributes.kernel_spacial_dimensions.shape().Dim(0) !=\n+             rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: size(kernel_spacial_dimensions) = rank-2\");\n+  }\n+\n+  vec.clear();\n+  vec.push_back(op.attributes.kernel_input_feature_dimension);\n+  vec.push_back(op.attributes.kernel_output_feature_dimension);\n+  check_buffer =\n+      op.attributes.kernel_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.kernel_spacial_dimensions.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    vec.push_back(check_buffer[i]);\n+  }\n+\n+  if (!(IsUnique(vec))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: isUnique(kernelDimensions)\");\n+  } else if (!(IsInRange(vec, rank))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<= kernelDimensions < rank\");\n+  } else if (op.attributes.output_spacial_dimensions.shape().Dim(0) !=\n+             rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: size(output_spacial_dimensions) = rank-2\");\n+  }\n+\n+  vec.clear();\n+  vec.push_back(op.attributes.output_batch_dimension);\n+  vec.push_back(op.attributes.output_feature_dimension);\n+  check_buffer =\n+      op.attributes.output_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.output_spacial_dimensions.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    vec.push_back(check_buffer[i]);\n+  }\n+\n+  if (!(IsUnique(vec))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: isUnique(outputDimensions)\");\n+  } else if (!(IsInRange(vec, rank))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<= outputDimensions < rank\");\n+  } else if (op.attributes.feature_group_count <= 0) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<feature_group_count\");\n+  } else if (op.attributes.batch_group_count <= 0) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<batch_group_count\");\n+  } else if (op.attributes.batch_group_count != 1 &&\n+             op.attributes.feature_group_count != 1) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: batch_group_count == 1 or feature_group_count \"\n+        \"== 1\");\n+  } else if (output.shape().Dim(\n+                 static_cast<size_t>(op.attributes.output_batch_dimension)) !=\n+             lhs.shape().Dim(\n+                 static_cast<size_t>(op.attributes.input_batch_dimension)) /\n+                 op.attributes.batch_group_count) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: output.shape().Dim(output_batch_dimension) == \"\n+        \"lhs.shape().Dim(input_batch_dimension)/batch_group_count\");\n+  } else if (output.shape().Dim(\n+                 static_cast<size_t>(op.attributes.output_feature_dimension)) !=\n+             rhs.shape().Dim(static_cast<size_t>(\n+                 op.attributes.kernel_output_feature_dimension))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: output.shape().Dim(outputFeatureDimention) == \"\n+        \"rhs.shape().Dim(kernel_output_feature_dimension)\");\n+  }\n+\n+  for (int64_t i = 0; i < output.Rank() - 2; ++i) {\n+    if (output.shape().Dim(static_cast<size_t>(output_spacial_pointer[i])) !=\n+        std::floor(\n+            (((lhs_dilation_buffer[i] *\n+               (lhs.shape().Dim(static_cast<size_t>(input_spacial_pointer[i])) -\n+                1)) +\n+              1 + padding_buffer[2 * i] + padding_buffer[2 * i + 1] -\n+              ((rhs_dilation_buffer[i] * (rhs.shape().Dim(static_cast<size_t>(\n+                                              window_spacial_pointer[i])) -\n+                                          1)) +\n+               1)) /\n+             window_strides_pointer[i]) +\n+            1)) {\n+      return absl::FailedPreconditionError(\n+          \"Constraint violation: output.shape().Dim(spacial_dim) is not \"\n+          \"properly set\");\n+    }\n+  }\n+\n+  if (lhs.IsQuantized() || rhs.IsQuantized() || output.IsQuantized()) {\n+    if (!(lhs.IsQuantized() && rhs.IsQuantized() && output.IsQuantized())) {\n+      return absl::FailedPreconditionError(\n+          \"Constraint violation: lhs.IsQuantized() && rhs.IsQuantized() && \"\n+          \"output.IsQuantized()\");\n+    }\n+    if (rhs.IsPerTensorQuantized()) {\n+      if (!(output.IsPerTensorQuantized())) {\n+        return absl::FailedPreconditionError(\n+            \"Constraint violation: If is_per_tensor_quantized(rhs), then \"\n+            \"is_per_tensor_quantized(output)\");\n+      }\n+    }\n+    if (rhs.IsPerAxisQuantized()) {\n+      if (rhs.quantized_per_axis_element_type().QuantizedDimension() !=\n+          op.attributes.kernel_output_feature_dimension) {\n+        return absl::FailedPreconditionError(\n+            \"Constraint violation:  If is_per_axis_quantized(rhs), then \"\n+            \"quantization_dimension(rhs) = \"\n+            \"op.attributes.kernel_output_feature_dimension\");\n+      }\n+    }\n+    if (output.IsPerAxisQuantized()) {\n+      if (output.quantized_per_axis_element_type().QuantizedDimension() !=\n+          op.attributes.output_feature_dimension) {\n+        return absl::FailedPreconditionError(\n+            \"Constraint violation:  If is_per_axis_quantized(output), then \"\n+            \"quantization_dimension(output) = \"\n+            \"op.attributes.output_feature_dimension\");\n+      }\n+    }\n+  }\n+\n+  return absl::OkStatus();\n+}\n+\n+// Slice op basic implimentation in context of Convolution\n+template <DataType storage_type>\n+void EvalDynamicSliceOp(const Tensor& operand, int64_t num_outputs,\n+                        size_t start_indices, size_t inner_dimensions_size,\n+                        size_t outer_dimensions_size, int64_t dimension,\n+                        Tensor& output) {\n+  using StorageT = StorageType<storage_type>;\n+\n+  StorageT* output_buffer = output.GetDataAs<storage_type>();\n+  const StorageT* operand_buffer = operand.GetDataAs<storage_type>();\n+\n+  size_t i = start_indices;\n+  size_t k = 0;\n+  while (i < operand.NumElements()) {\n+    for (size_t j = 0;\n+         j < (output.shape().Dim(dimension) * inner_dimensions_size);",
        "comment_created_at": "2024-04-24T17:52:26+00:00",
        "comment_author": "qukhan",
        "comment_body": "`(output.shape().Dim(dimension) * inner_dimensions_size)` can be extracted to a constant outside the loop. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1596710893",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 66299,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/convolution.cc",
        "discussion_id": "1578299493",
        "commented_code": "@@ -0,0 +1,1171 @@\n+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/convolution.h\"\n+\n+#include <algorithm>\n+#include <cstddef>\n+#include <string>\n+#include <type_traits>\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/unary_elementwise.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+template <class T>\n+using DimVector = absl::InlinedVector<T, 6>;\n+\n+bool IsUnique(DimVector<int64_t>& vec) {\n+  std::sort(vec.begin(), vec.end());\n+  return std::unique(vec.begin(), vec.end()) == vec.end();\n+}\n+\n+bool IsInRange(DimVector<int64_t>& vec, size_t N) {\n+  for (int64_t dim : vec) {\n+    if (dim >= N || dim < 0) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+template <DataType storage_type>\n+absl::Status PrepareImpl(ConvolutionOp& op, const Tensor& lhs,\n+                         const Tensor& rhs, Tensor& output) {\n+  using StorageT = StorageType<storage_type>;\n+\n+  // Transpose prepare\n+  const int64_t* window_spacial_pointer =\n+      op.attributes.kernel_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  const int64_t* output_spacial_pointer =\n+      op.attributes.output_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  const int64_t* input_spacial_pointer =\n+      op.attributes.input_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+\n+  std::vector<StorageT> lhs_permutation_values(\n+      static_cast<int64_t>(lhs.Rank()));\n+  lhs_permutation_values[0] = op.attributes.input_batch_dimension;\n+  lhs_permutation_values[1] = op.attributes.input_feature_dimension;\n+  DimVector<DimensionSize> lhs_shape_dims(lhs.Rank());\n+  lhs_shape_dims[0] =\n+      lhs.shape().Dim(static_cast<size_t>(op.attributes.input_batch_dimension));\n+  lhs_shape_dims[1] = lhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.input_feature_dimension));\n+  for (size_t i = 0; i < lhs.Rank() - 2; ++i) {\n+    lhs_shape_dims[i + 2] =\n+        lhs.shape().Dim(static_cast<size_t>(input_spacial_pointer[i]));\n+    lhs_permutation_values[i + 2] = input_spacial_pointer[i];\n+  }\n+  // malloc is used to have the storage space available out of prepare function\n+  // scope and it's pointer is stored in class data member to\n+  // deallocate the memory in destructor.\n+  op.lhs_permutation_data =\n+      malloc(lhs_permutation_values.size() * sizeof(StorageT));\n+  memmove(op.lhs_permutation_data, lhs_permutation_values.data(),\n+          lhs_permutation_values.size() * sizeof(StorageT));\n+  const Shape lhs_permutation_shape({static_cast<int64_t>(lhs.Rank())});\n+  Tensor lhs_permutations{.type = TensorType{.shape = lhs_permutation_shape,\n+                                             .element_type = storage_type},\n+                          .data = op.lhs_permutation_data};\n+\n+  op.lhs_transposed_data = malloc(lhs.NumElements() * sizeof(StorageT));\n+  const Shape lhs_transposed_shape(lhs_shape_dims);\n+  Tensor lhs_transposed{.type = TensorType{.shape = lhs_transposed_shape,\n+                                           .element_type = storage_type},\n+                        .data = op.lhs_transposed_data};\n+\n+  std::vector<StorageT> rhs_permutation_values(\n+      static_cast<int64_t>(rhs.Rank()));\n+  rhs_permutation_values[0] = op.attributes.kernel_output_feature_dimension;\n+  rhs_permutation_values[1] = op.attributes.kernel_input_feature_dimension;\n+  DimVector<DimensionSize> rhs_shape_dims(rhs.Rank());\n+  rhs_shape_dims[0] = rhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.kernel_output_feature_dimension));\n+  rhs_shape_dims[1] = rhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.kernel_input_feature_dimension));\n+  for (size_t i = 0; i < rhs.Rank() - 2; ++i) {\n+    rhs_shape_dims[i + 2] =\n+        rhs.shape().Dim(static_cast<size_t>(window_spacial_pointer[i]));\n+    rhs_permutation_values[i + 2] = window_spacial_pointer[i];\n+  }\n+  op.rhs_permutation_data = malloc(rhs.Rank() * sizeof(StorageT));\n+  memmove(op.rhs_permutation_data, rhs_permutation_values.data(),\n+          rhs_permutation_values.size() * sizeof(StorageT));\n+  const Shape rhs_permutation_shape({static_cast<int64_t>(rhs.Rank())});\n+  Tensor rhs_permutations{.type = TensorType{.shape = rhs_permutation_shape,\n+                                             .element_type = storage_type},\n+                          .data = op.rhs_permutation_data};\n+\n+  op.rhs_transposed_data = malloc(rhs.NumElements() * sizeof(StorageT));\n+  const Shape rhs_transposed_shape(rhs_shape_dims);\n+  Tensor rhs_transposed{.type = TensorType{.shape = rhs_transposed_shape,\n+                                           .element_type = storage_type},\n+                        .data = op.rhs_transposed_data};\n+\n+  std::vector<StorageT> output_permutation_values(\n+      static_cast<int64_t>(output.Rank()));\n+  output_permutation_values[0] = op.attributes.output_batch_dimension;\n+  output_permutation_values[1] = op.attributes.output_feature_dimension;\n+  DimVector<DimensionSize> output_shape_dims(output.Rank());\n+  output_shape_dims[0] = output.shape().Dim(\n+      static_cast<size_t>(op.attributes.output_batch_dimension));\n+  output_shape_dims[1] = output.shape().Dim(\n+      static_cast<size_t>(op.attributes.output_feature_dimension));\n+  for (size_t i = 0; i < output.Rank() - 2; ++i) {\n+    output_shape_dims[i + 2] =\n+        output.shape().Dim(static_cast<size_t>(output_spacial_pointer[i]));\n+    output_permutation_values[i + 2] = output_spacial_pointer[i];\n+  }\n+  op.output_permutation_data = malloc(output.Rank() * sizeof(StorageT));\n+  memmove(op.output_permutation_data, output_permutation_values.data(),\n+          output_permutation_values.size() * sizeof(StorageT));\n+  const Shape output_permutation_shape({static_cast<int64_t>(output.Rank())});\n+  Tensor output_permutations{\n+      .type = TensorType{.shape = output_permutation_shape,\n+                         .element_type = storage_type},\n+      .data = op.output_permutation_data};\n+\n+  op.output_transposed_data = malloc(output.NumElements() * sizeof(StorageT));\n+  const Shape output_transposed_shape(output_shape_dims);\n+  Tensor output_transposed{.type = TensorType{.shape = output_transposed_shape,\n+                                              .element_type = storage_type},\n+                           .data = op.output_transposed_data};\n+  // transpose prepare end\n+\n+  // DotGeneral prepare\n+  DimVector<DimensionSize> dims(rhs_transposed.Rank());\n+  size_t rhs_transposed_tensor_size = 1;\n+  dims[0] = 1;\n+  for (size_t i = 1; i < rhs_transposed.Rank(); ++i) {\n+    dims[i] = rhs_transposed.shape().Dim(i);\n+    rhs_transposed_tensor_size *= rhs_transposed.shape().Dim(i);\n+  }\n+  const Shape rhs_dot_general_shape(dims);\n+  op.rhs_dot_general_data =\n+      malloc(rhs_transposed_tensor_size * sizeof(StorageT));\n+  Tensor rhs_dot_general{.type = TensorType{.shape = rhs_dot_general_shape,\n+                                            .element_type = storage_type},\n+                         .data = op.rhs_dot_general_data};\n+\n+  op.lhs_dot_general_data =\n+      malloc(rhs_transposed_tensor_size * sizeof(StorageT));\n+  Tensor lhs_dot_general{.type = TensorType{.shape = rhs_dot_general_shape,\n+                                            .element_type = storage_type},\n+                         .data = op.lhs_dot_general_data};\n+\n+  std::vector<typename Storage<DataType::kSI64>::Type>\n+      lhs_contracting_dimensions_values(lhs_transposed.Rank() - 1);\n+  for (size_t i = 0; i < lhs_transposed.Rank() - 1; ++i) {\n+    lhs_contracting_dimensions_values[i] = i + 1;\n+  }\n+  op.lhs_contracting_dimensions_data =\n+      malloc((lhs_transposed.Rank() - 1) * sizeof(int64_t));\n+  memmove(op.lhs_contracting_dimensions_data,\n+          lhs_contracting_dimensions_values.data(),\n+          lhs_contracting_dimensions_values.size() * sizeof(int64_t));\n+  const Shape lhs_contracting_dimensions_shape(\n+      {static_cast<int64_t>(lhs_transposed.Rank() - 1)});\n+  Tensor lhs_contracting_dimensions{\n+      .type = TensorType{.shape = lhs_contracting_dimensions_shape,\n+                         .element_type = DataType::kSI64},\n+      .data = op.lhs_contracting_dimensions_data};\n+\n+  std::vector<typename Storage<DataType::kSI64>::Type>\n+      rhs_contracting_dimensions_values(rhs_transposed.Rank() - 1);\n+  for (size_t i = 0; i < rhs_transposed.Rank() - 1; ++i) {\n+    rhs_contracting_dimensions_values[i] = i + 1;\n+  }\n+  op.rhs_contracting_dimensions_data =\n+      malloc((rhs_transposed.Rank() - 1) * sizeof(int64_t));\n+  memmove(op.rhs_contracting_dimensions_data,\n+          rhs_contracting_dimensions_values.data(),\n+          rhs_contracting_dimensions_values.size() * sizeof(int64_t));\n+  Tensor rhs_contracting_dimensions{\n+      .type = TensorType{.shape = lhs_contracting_dimensions_shape,\n+                         .element_type = DataType::kSI8},\n+      .data = op.rhs_contracting_dimensions_data};\n+\n+  std::vector<StorageT> dor_general_output_values(1);\n+  dor_general_output_values[0] = 0;\n+  op.output_dot_general_data = malloc(1 * sizeof(StorageT));\n+  memmove(op.output_dot_general_data, dor_general_output_values.data(),\n+          dor_general_output_values.size() * sizeof(StorageT));\n+  const Shape dor_general_output_shape{{1}};\n+  Tensor output_dot_general{\n+      .type = TensorType{.shape = dor_general_output_shape,\n+                         .element_type = storage_type},\n+      .data = op.output_dot_general_data};\n+\n+  Tensor lhs_batching_dimensions{\n+      .type = TensorType{.shape = Shape(), .element_type = DataType::kSI8},\n+      .data = {}};\n+  Tensor rhs_batching_dimensions{\n+      .type = TensorType{.shape = Shape(), .element_type = DataType::kSI8},\n+      .data = {}};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  op.dot_general_op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  auto state = Prepare(op.dot_general_op, lhs_dot_general, rhs_dot_general,\n+                       output_dot_general);\n+  // Dot general prepare end\n+\n+  // padding prepare\n+  const int64_t* lhs_dilation_buffer =\n+      op.attributes.lhs_dilation.GetDataAs<DataType::kSI64>();\n+  const int64_t* padding_buffer =\n+      op.attributes.padding.GetDataAs<DataType::kSI64>();\n+\n+  int lhs_padded_spacials[lhs_transposed.Rank() - 2];\n+  int lhs_padded_tensor_size = 1;\n+  for (size_t i = lhs_transposed.Rank() - 1; i > 1; --i) {\n+    lhs_padded_spacials[i - 2] =\n+        lhs_transposed.shape().Dim(i) +\n+        (lhs_dilation_buffer[i - 2] - 1) * (lhs_transposed.shape().Dim(i) - 1) +\n+        padding_buffer[2 * (i - 2)] + padding_buffer[(2 * (i - 2)) + 1];\n+    lhs_padded_tensor_size *= lhs_padded_spacials[i - 2];\n+  }\n+\n+  lhs_padded_tensor_size *=\n+      lhs_transposed.shape().Dim(0) * lhs_transposed.shape().Dim(1);\n+  op.lhs_padded_data = malloc(lhs_padded_tensor_size * sizeof(StorageT));\n+  DimVector<DimensionSize> lhs_padding_shape_dims(lhs_transposed.Rank());\n+  lhs_padding_shape_dims[0] = lhs_transposed.shape().Dim(0);\n+  lhs_padding_shape_dims[1] = lhs_transposed.shape().Dim(1);\n+  for (size_t i = 0; i < lhs_transposed.Rank() - 2; ++i) {\n+    lhs_padding_shape_dims[i + 2] =\n+        static_cast<int64_t>(lhs_padded_spacials[i]);\n+  }\n+  const Shape lhs_padding_shape(lhs_padding_shape_dims);\n+  Tensor lhs_padded{.type = TensorType{.shape = lhs_padding_shape,\n+                                       .element_type = storage_type},\n+                    .data = op.lhs_padded_data};\n+  // padding prepare end\n+\n+  // Split prepare\n+  int64_t num_splits =\n+      op.attributes.batch_group_count * op.attributes.feature_group_count;\n+  int64_t split_dimension = 0;\n+  for (int64_t i = 0; i < num_splits; ++i) {\n+    DimVector<DimensionSize> rhs_split_dims(rhs_transposed.Rank());\n+    for (size_t i = 0; i < rhs_transposed.Rank(); ++i) {\n+      if (i == split_dimension) {\n+        rhs_split_dims[i] = (rhs_transposed.shape().Dim(i) / num_splits);\n+      } else {\n+        rhs_split_dims[i] = rhs_transposed.shape().Dim(i);\n+      }\n+    }\n+    const Shape rhs_split_shape(rhs_split_dims);\n+    void* rhs_split_data =\n+        malloc((rhs_transposed.NumElements() / num_splits) * sizeof(StorageT));\n+    op.rhs_splits_data.push_back(rhs_split_data);\n+    Tensor rhs_split{.type = TensorType{.shape = rhs_split_shape,\n+                                        .element_type = storage_type},\n+                     .data = rhs_split_data};\n+    op.rhs_splits.push_back(rhs_split);\n+  }\n+\n+  if (op.attributes.feature_group_count > 1) {\n+    split_dimension = 1;\n+  }\n+\n+  for (int64_t i = 0; i < num_splits; ++i) {\n+    DimVector<DimensionSize> lhs_split_dims(lhs_padded.Rank());\n+    for (size_t i = 0; i < lhs_padded.Rank(); ++i) {\n+      if (i == split_dimension) {\n+        lhs_split_dims[i] = (lhs_padded.shape().Dim(i) / num_splits);\n+      } else {\n+        lhs_split_dims[i] = lhs_padded.shape().Dim(i);\n+      }\n+    }\n+    const Shape lhs_split_shape(lhs_split_dims);\n+    void* lhs_split_data =\n+        malloc((lhs_padded.NumElements() / num_splits) * sizeof(StorageT));\n+    op.lhs_splits_data.push_back(lhs_split_data);\n+    Tensor lhs_split{.type = TensorType{.shape = lhs_split_shape,\n+                                        .element_type = storage_type},\n+                     .data = lhs_split_data};\n+    op.lhs_splits.push_back(lhs_split);\n+  }\n+  // split prepare end\n+\n+  // quantized tensors prepare\n+  if (lhs.IsQuantized()) {\n+    op.lhs_dequantized_data = malloc(lhs.NumElements() * sizeof(StorageT));\n+    const Shape lhs_dequantized_shape = lhs.shape();\n+    Tensor lhs_dequantized{.type = TensorType{.shape = lhs_dequantized_shape,\n+                                              .element_type = storage_type},\n+                           .data = op.lhs_dequantized_data};\n+    op.rhs_dequantized_data = malloc(rhs.NumElements() * sizeof(StorageT));\n+    const Shape rhs_dequantized_shape = rhs.shape();\n+    Tensor rhs_dequantized{.type = TensorType{.shape = rhs_dequantized_shape,\n+                                              .element_type = storage_type},\n+                           .data = op.rhs_dequantized_data};\n+    op.output_dequantized_data =\n+        malloc(output.NumElements() * sizeof(StorageT));\n+    const Shape output_dequantized_shape = output.shape();\n+    Tensor output_dequantized{\n+        .type = TensorType{.shape = output_dequantized_shape,\n+                           .element_type = storage_type},\n+        .data = op.output_dequantized_data};\n+\n+    op.lhs_dequantized = std::move(lhs_dequantized);\n+    op.rhs_dequantized = std::move(rhs_dequantized);\n+    op.output_dequantized = std::move(output_dequantized);\n+  }\n+  // quantized tensors prepare end\n+\n+  op.lhs_permutations = std::move(lhs_permutations);\n+  op.lhs_transposed = std::move(lhs_transposed);\n+  op.rhs_permutations = std::move(rhs_permutations);\n+  op.rhs_transposed = std::move(rhs_transposed);\n+  op.output_permutations = std::move(output_permutations);\n+  op.output_transposed = std::move(output_transposed);\n+  op.lhs_dot_general = std::move(lhs_dot_general);\n+  op.rhs_dot_general = std::move(rhs_dot_general);\n+  op.output_dot_general = std::move(output_dot_general);\n+  op.lhs_padded = std::move(lhs_padded);\n+\n+  const int64_t* rhs_dilation_buffer =\n+      op.attributes.rhs_dilation.GetDataAs<DataType::kSI64>();\n+  const int64_t* window_strides_pointer =\n+      op.attributes.window_strides.GetDataAs<DataType::kSI64>();\n+\n+  // Constraints Check\n+  if (op.attributes.precision_configs.size() != 2) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.convolution: Size of precision_config must be two.\");\n+  }\n+  if (op.attributes.precision_configs[0] != PrecisionTypes::DEFAULT &&\n+      op.attributes.precision_configs[1] != PrecisionTypes::DEFAULT) {\n+    return absl::UnimplementedError(\n+        \"stablehlo.convolution: Currently the precision_config supports \"\n+        \"DEFAULT configuration only.\");\n+  }\n+  size_t rank = lhs.Rank();\n+  if (lhs.Rank() != rhs.Rank()) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: rank(lhs) == rank(rhs)\");\n+  } else if (output.Rank() != lhs.Rank()) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: rank(output) == lhs.Rank()\");\n+  }\n+  if (!lhs.IsQuantized()) {\n+    SHLO_REF_RETURN_ON_ERROR(\n+        CheckSameBaselineType(CheckCtx(\"Convolution\"), lhs, rhs));\n+    SHLO_REF_RETURN_ON_ERROR(\n+        CheckSameBaselineType(CheckCtx(\"Convolution\"), lhs, output));\n+  }\n+  if (op.attributes.window_strides.shape().Dim(0) != rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: size(windowStride)=rank-2\");\n+  }\n+\n+  const int64_t* check_buffer =\n+      op.attributes.window_strides.GetDataAs<DataType::kSI64>();\n+  bool is_greater_than_zero = true;\n+  size_t n = op.attributes.window_strides.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    if (check_buffer[i] == 0) {\n+      is_greater_than_zero = false;\n+      exit;\n+    }\n+  }\n+\n+  if (!is_greater_than_zero) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<windowStride\");\n+  } else if (op.attributes.padding.shape().Dim(0) != rank - 2 ||\n+             op.attributes.padding.shape().Dim(1) != 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: shape(padding)=[rank-2,2]\");\n+  } else if (op.attributes.lhs_dilation.shape().Dim(0) != rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Contraint violation: shape(lhs_dilation) == rank-2\");\n+  }\n+\n+  check_buffer = op.attributes.lhs_dilation.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.lhs_dilation.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    if (check_buffer[i] == 0) {\n+      is_greater_than_zero = false;\n+      exit;\n+    }\n+  }\n+\n+  if (!is_greater_than_zero) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<lhs_dilation\");\n+  } else if (op.attributes.rhs_dilation.shape().Dim(0) != rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: shape(rhs_dilation) == rank-2\");\n+  }\n+\n+  check_buffer = op.attributes.rhs_dilation.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.rhs_dilation.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    if (check_buffer[i] == 0) {\n+      is_greater_than_zero = false;\n+      exit;\n+    }\n+  }\n+\n+  if (!is_greater_than_zero) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<rhs_dilation\");\n+  } else if (op.attributes.window_reversal.shape().Dim(0) != rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: shape(window_reversal) == rank-2\");\n+  } else if (lhs.shape().Dim(\n+                 static_cast<size_t>(op.attributes.input_batch_dimension)) %\n+                 op.attributes.batch_group_count !=\n+             0) {\n+    return absl::FailedPreconditionError(\n+        \"Contraint violation: Dim(lhs,input_batch_dimension)%batch_group_count \"\n+        \"= \"\n+        \"0\");\n+  } else if (lhs.shape().Dim(\n+                 static_cast<size_t>(op.attributes.input_feature_dimension)) %\n+                 op.attributes.feature_group_count !=\n+             0) {\n+    return absl::FailedPreconditionError(\n+        \"Contraint violation: \"\n+        \"Dim(lhs,input_feature_dimension)%(feature_group_count) = 0\");\n+  } else if (op.attributes.input_spacial_dimensions.shape().Dim(0) !=\n+             rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constarint violation: size(input_spacial_dimensions) = rank-2\");\n+  }\n+\n+  DimVector<int64_t> vec;\n+  vec.push_back(op.attributes.input_batch_dimension);\n+  vec.push_back(op.attributes.input_feature_dimension);\n+  check_buffer =\n+      op.attributes.input_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.input_spacial_dimensions.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    vec.push_back(check_buffer[i]);\n+  }\n+\n+  if (!(IsUnique(vec))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: isUnique(inputDimensions)\");\n+  } else if (!(IsInRange(vec, rank))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<= inputDimensions < rank\");\n+  } else if (rhs.shape().Dim(static_cast<size_t>(\n+                 op.attributes.kernel_input_feature_dimension)) !=\n+             lhs.shape().Dim(\n+                 static_cast<size_t>(op.attributes.input_feature_dimension)) /\n+                 op.attributes.feature_group_count) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: Dim(rhs,kernel_input_feature_dimension) = \"\n+        \"Dim(lhs,input_feature_dimension)/feature_group_count\");\n+  } else if (rhs.shape().Dim(static_cast<size_t>(\n+                 op.attributes.kernel_output_feature_dimension)) %\n+                 op.attributes.batch_group_count !=\n+             0) {\n+    return absl::FailedPreconditionError(\n+        \"Constarint violation: \"\n+        \"Dim(rhs,kernel_output_feature_dimension)%batch_group_count=0\");\n+  } else if (rhs.shape().Dim(static_cast<size_t>(\n+                 op.attributes.kernel_output_feature_dimension)) %\n+                 op.attributes.feature_group_count !=\n+             0) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: \"\n+        \"Dim(rhs,kernel_output_feature_dimension)%(feature_group_count)=0\");\n+  } else if (op.attributes.kernel_spacial_dimensions.shape().Dim(0) !=\n+             rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: size(kernel_spacial_dimensions) = rank-2\");\n+  }\n+\n+  vec.clear();\n+  vec.push_back(op.attributes.kernel_input_feature_dimension);\n+  vec.push_back(op.attributes.kernel_output_feature_dimension);\n+  check_buffer =\n+      op.attributes.kernel_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.kernel_spacial_dimensions.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    vec.push_back(check_buffer[i]);\n+  }\n+\n+  if (!(IsUnique(vec))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: isUnique(kernelDimensions)\");\n+  } else if (!(IsInRange(vec, rank))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<= kernelDimensions < rank\");\n+  } else if (op.attributes.output_spacial_dimensions.shape().Dim(0) !=\n+             rank - 2) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: size(output_spacial_dimensions) = rank-2\");\n+  }\n+\n+  vec.clear();\n+  vec.push_back(op.attributes.output_batch_dimension);\n+  vec.push_back(op.attributes.output_feature_dimension);\n+  check_buffer =\n+      op.attributes.output_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  n = op.attributes.output_spacial_dimensions.NumElements();\n+  for (size_t i = 0; i < n; ++i) {\n+    vec.push_back(check_buffer[i]);\n+  }\n+\n+  if (!(IsUnique(vec))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: isUnique(outputDimensions)\");\n+  } else if (!(IsInRange(vec, rank))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<= outputDimensions < rank\");\n+  } else if (op.attributes.feature_group_count <= 0) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<feature_group_count\");\n+  } else if (op.attributes.batch_group_count <= 0) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: 0<batch_group_count\");\n+  } else if (op.attributes.batch_group_count != 1 &&\n+             op.attributes.feature_group_count != 1) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: batch_group_count == 1 or feature_group_count \"\n+        \"== 1\");\n+  } else if (output.shape().Dim(\n+                 static_cast<size_t>(op.attributes.output_batch_dimension)) !=\n+             lhs.shape().Dim(\n+                 static_cast<size_t>(op.attributes.input_batch_dimension)) /\n+                 op.attributes.batch_group_count) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: output.shape().Dim(output_batch_dimension) == \"\n+        \"lhs.shape().Dim(input_batch_dimension)/batch_group_count\");\n+  } else if (output.shape().Dim(\n+                 static_cast<size_t>(op.attributes.output_feature_dimension)) !=\n+             rhs.shape().Dim(static_cast<size_t>(\n+                 op.attributes.kernel_output_feature_dimension))) {\n+    return absl::FailedPreconditionError(\n+        \"Constraint violation: output.shape().Dim(outputFeatureDimention) == \"\n+        \"rhs.shape().Dim(kernel_output_feature_dimension)\");\n+  }\n+\n+  for (int64_t i = 0; i < output.Rank() - 2; ++i) {\n+    if (output.shape().Dim(static_cast<size_t>(output_spacial_pointer[i])) !=\n+        std::floor(\n+            (((lhs_dilation_buffer[i] *\n+               (lhs.shape().Dim(static_cast<size_t>(input_spacial_pointer[i])) -\n+                1)) +\n+              1 + padding_buffer[2 * i] + padding_buffer[2 * i + 1] -\n+              ((rhs_dilation_buffer[i] * (rhs.shape().Dim(static_cast<size_t>(\n+                                              window_spacial_pointer[i])) -\n+                                          1)) +\n+               1)) /\n+             window_strides_pointer[i]) +\n+            1)) {\n+      return absl::FailedPreconditionError(\n+          \"Constraint violation: output.shape().Dim(spacial_dim) is not \"\n+          \"properly set\");\n+    }\n+  }\n+\n+  if (lhs.IsQuantized() || rhs.IsQuantized() || output.IsQuantized()) {\n+    if (!(lhs.IsQuantized() && rhs.IsQuantized() && output.IsQuantized())) {\n+      return absl::FailedPreconditionError(\n+          \"Constraint violation: lhs.IsQuantized() && rhs.IsQuantized() && \"\n+          \"output.IsQuantized()\");\n+    }\n+    if (rhs.IsPerTensorQuantized()) {\n+      if (!(output.IsPerTensorQuantized())) {\n+        return absl::FailedPreconditionError(\n+            \"Constraint violation: If is_per_tensor_quantized(rhs), then \"\n+            \"is_per_tensor_quantized(output)\");\n+      }\n+    }\n+    if (rhs.IsPerAxisQuantized()) {\n+      if (rhs.quantized_per_axis_element_type().QuantizedDimension() !=\n+          op.attributes.kernel_output_feature_dimension) {\n+        return absl::FailedPreconditionError(\n+            \"Constraint violation:  If is_per_axis_quantized(rhs), then \"\n+            \"quantization_dimension(rhs) = \"\n+            \"op.attributes.kernel_output_feature_dimension\");\n+      }\n+    }\n+    if (output.IsPerAxisQuantized()) {\n+      if (output.quantized_per_axis_element_type().QuantizedDimension() !=\n+          op.attributes.output_feature_dimension) {\n+        return absl::FailedPreconditionError(\n+            \"Constraint violation:  If is_per_axis_quantized(output), then \"\n+            \"quantization_dimension(output) = \"\n+            \"op.attributes.output_feature_dimension\");\n+      }\n+    }\n+  }\n+\n+  return absl::OkStatus();\n+}\n+\n+// Slice op basic implimentation in context of Convolution\n+template <DataType storage_type>\n+void EvalDynamicSliceOp(const Tensor& operand, int64_t num_outputs,\n+                        size_t start_indices, size_t inner_dimensions_size,\n+                        size_t outer_dimensions_size, int64_t dimension,\n+                        Tensor& output) {\n+  using StorageT = StorageType<storage_type>;\n+\n+  StorageT* output_buffer = output.GetDataAs<storage_type>();\n+  const StorageT* operand_buffer = operand.GetDataAs<storage_type>();\n+\n+  size_t i = start_indices;\n+  size_t k = 0;\n+  while (i < operand.NumElements()) {\n+    for (size_t j = 0;\n+         j < (output.shape().Dim(dimension) * inner_dimensions_size);",
        "comment_created_at": "2024-05-10T12:47:31+00:00",
        "comment_author": "LokeshReddyOVS-MCW",
        "comment_body": "changed accordingly",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1040690535",
    "pr_number": 53206,
    "pr_file": "tensorflow/lite/kernels/internal/reference/pooling3d.h",
    "created_at": "2022-12-06T09:12:40+00:00",
    "commented_code": "+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING3D_H_\n+#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING3D_H_\n+\n+#include \"tensorflow/lite/kernels/internal/types.h\"\n+\n+namespace tflite {\n+namespace reference_ops {\n+\n+template <typename T, typename ActivationT>\n+inline T RoundAndAverage(ActivationT sum, int count) {\n+  // Round to the closest integer value.\n+  return sum > 0 ? (sum + count / 2) / count : (sum - count / 2) / count;\n+}\n+\n+template <>\n+inline float RoundAndAverage(float sum, int count) {\n+  // No rounding for float type.\n+  return sum / count;\n+}\n+\n+template <typename T, typename ActivationT>\n+inline void AveragePool3D(const Pool3DParams& params,\n+                          const RuntimeShape& input_shape, const T* input_data,\n+                          const RuntimeShape& output_shape, T* output_data) {\n+  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n+  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n+\n+  ActivationT activation_min, activation_max;\n+  GetActivationParams(params, &activation_min, &activation_max);\n+  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n+  const int channels = MatchingDim(input_shape, 4, output_shape, 4);\n+\n+  const int in_spatial_dim_1 = input_shape.Dims(1);\n+  const int in_spatial_dim_2 = input_shape.Dims(2);\n+  const int in_spatial_dim_3 = input_shape.Dims(3);\n+  const int out_spatial_dim_1 = output_shape.Dims(1);\n+  const int out_spatial_dim_2 = output_shape.Dims(2);\n+  const int out_spatial_dim_3 = output_shape.Dims(3);\n+\n+  const int stride_spatial_dim_1 = params.stride_depth;\n+  const int stride_spatial_dim_2 = params.stride_height;\n+  const int stride_spatial_dim_3 = params.stride_width;\n+  const int filter_spatial_dim_1 = params.filter_depth;\n+  const int filter_spatial_dim_2 = params.filter_height;\n+  const int filter_spatial_dim_3 = params.filter_width;\n+  const int padding_spatial_dim_1 = params.padding_values.depth;\n+  const int padding_spatial_dim_2 = params.padding_values.height;\n+  const int padding_spatial_dim_3 = params.padding_values.width;\n+\n+  for (int batch = 0; batch < batches; ++batch) {\n+    for (int out_d1 = 0; out_d1 < out_spatial_dim_1; ++out_d1) {\n+      const int in_d1_origin =\n+          (out_d1 * stride_spatial_dim_1) - padding_spatial_dim_1;\n+      const int filter_d1_start = std::max(0, -in_d1_origin);\n+      const int filter_d1_end =\n+          std::min(filter_spatial_dim_1, in_spatial_dim_1 - in_d1_origin);\n+      for (int out_d2 = 0; out_d2 < out_spatial_dim_2; ++out_d2) {\n+        const int in_d2_origin =\n+            (out_d2 * stride_spatial_dim_2) - padding_spatial_dim_2;\n+        const int filter_d2_start = std::max(0, -in_d2_origin);\n+        const int filter_d2_end =\n+            std::min(filter_spatial_dim_2, in_spatial_dim_2 - in_d2_origin);\n+        for (int out_d3 = 0; out_d3 < out_spatial_dim_3; ++out_d3) {\n+          const int in_d3_origin =\n+              (out_d3 * stride_spatial_dim_3) - padding_spatial_dim_3;\n+          const int filter_d3_start = std::max(0, -in_d3_origin);\n+          const int filter_d3_end =\n+              std::min(filter_spatial_dim_3, in_spatial_dim_3 - in_d3_origin);\n+          for (int channel = 0; channel < channels; ++channel) {\n+            ActivationT total = 0;\n+            for (int filter_d1 = filter_d1_start; filter_d1 < filter_d1_end;\n+                 ++filter_d1) {\n+              const int in_d1 = in_d1_origin + filter_d1;\n+              for (int filter_d2 = filter_d2_start; filter_d2 < filter_d2_end;\n+                   ++filter_d2) {\n+                const int in_d2 = in_d2_origin + filter_d2;\n+                for (int filter_d3 = filter_d3_start; filter_d3 < filter_d3_end;\n+                     ++filter_d3) {\n+                  const int in_d3 = in_d3_origin + filter_d3;\n+                  total += input_data[Offset(input_shape, batch, in_d1, in_d2,",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1040690535",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 53206,
        "pr_file": "tensorflow/lite/kernels/internal/reference/pooling3d.h",
        "discussion_id": "1040690535",
        "commented_code": "@@ -0,0 +1,193 @@\n+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING3D_H_\n+#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING3D_H_\n+\n+#include \"tensorflow/lite/kernels/internal/types.h\"\n+\n+namespace tflite {\n+namespace reference_ops {\n+\n+template <typename T, typename ActivationT>\n+inline T RoundAndAverage(ActivationT sum, int count) {\n+  // Round to the closest integer value.\n+  return sum > 0 ? (sum + count / 2) / count : (sum - count / 2) / count;\n+}\n+\n+template <>\n+inline float RoundAndAverage(float sum, int count) {\n+  // No rounding for float type.\n+  return sum / count;\n+}\n+\n+template <typename T, typename ActivationT>\n+inline void AveragePool3D(const Pool3DParams& params,\n+                          const RuntimeShape& input_shape, const T* input_data,\n+                          const RuntimeShape& output_shape, T* output_data) {\n+  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n+  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n+\n+  ActivationT activation_min, activation_max;\n+  GetActivationParams(params, &activation_min, &activation_max);\n+  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n+  const int channels = MatchingDim(input_shape, 4, output_shape, 4);\n+\n+  const int in_spatial_dim_1 = input_shape.Dims(1);\n+  const int in_spatial_dim_2 = input_shape.Dims(2);\n+  const int in_spatial_dim_3 = input_shape.Dims(3);\n+  const int out_spatial_dim_1 = output_shape.Dims(1);\n+  const int out_spatial_dim_2 = output_shape.Dims(2);\n+  const int out_spatial_dim_3 = output_shape.Dims(3);\n+\n+  const int stride_spatial_dim_1 = params.stride_depth;\n+  const int stride_spatial_dim_2 = params.stride_height;\n+  const int stride_spatial_dim_3 = params.stride_width;\n+  const int filter_spatial_dim_1 = params.filter_depth;\n+  const int filter_spatial_dim_2 = params.filter_height;\n+  const int filter_spatial_dim_3 = params.filter_width;\n+  const int padding_spatial_dim_1 = params.padding_values.depth;\n+  const int padding_spatial_dim_2 = params.padding_values.height;\n+  const int padding_spatial_dim_3 = params.padding_values.width;\n+\n+  for (int batch = 0; batch < batches; ++batch) {\n+    for (int out_d1 = 0; out_d1 < out_spatial_dim_1; ++out_d1) {\n+      const int in_d1_origin =\n+          (out_d1 * stride_spatial_dim_1) - padding_spatial_dim_1;\n+      const int filter_d1_start = std::max(0, -in_d1_origin);\n+      const int filter_d1_end =\n+          std::min(filter_spatial_dim_1, in_spatial_dim_1 - in_d1_origin);\n+      for (int out_d2 = 0; out_d2 < out_spatial_dim_2; ++out_d2) {\n+        const int in_d2_origin =\n+            (out_d2 * stride_spatial_dim_2) - padding_spatial_dim_2;\n+        const int filter_d2_start = std::max(0, -in_d2_origin);\n+        const int filter_d2_end =\n+            std::min(filter_spatial_dim_2, in_spatial_dim_2 - in_d2_origin);\n+        for (int out_d3 = 0; out_d3 < out_spatial_dim_3; ++out_d3) {\n+          const int in_d3_origin =\n+              (out_d3 * stride_spatial_dim_3) - padding_spatial_dim_3;\n+          const int filter_d3_start = std::max(0, -in_d3_origin);\n+          const int filter_d3_end =\n+              std::min(filter_spatial_dim_3, in_spatial_dim_3 - in_d3_origin);\n+          for (int channel = 0; channel < channels; ++channel) {\n+            ActivationT total = 0;\n+            for (int filter_d1 = filter_d1_start; filter_d1 < filter_d1_end;\n+                 ++filter_d1) {\n+              const int in_d1 = in_d1_origin + filter_d1;\n+              for (int filter_d2 = filter_d2_start; filter_d2 < filter_d2_end;\n+                   ++filter_d2) {\n+                const int in_d2 = in_d2_origin + filter_d2;\n+                for (int filter_d3 = filter_d3_start; filter_d3 < filter_d3_end;\n+                     ++filter_d3) {\n+                  const int in_d3 = in_d3_origin + filter_d3;\n+                  total += input_data[Offset(input_shape, batch, in_d1, in_d2,",
        "comment_created_at": "2022-12-06T09:12:40+00:00",
        "comment_author": "alankelly",
        "comment_body": "Can you replace Offset with the partial calculation of the input index in each of the nested loops? Offset has terrible performance and I am working on getting rid of it. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1040813124",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 53206,
        "pr_file": "tensorflow/lite/kernels/internal/reference/pooling3d.h",
        "discussion_id": "1040690535",
        "commented_code": "@@ -0,0 +1,193 @@\n+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING3D_H_\n+#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING3D_H_\n+\n+#include \"tensorflow/lite/kernels/internal/types.h\"\n+\n+namespace tflite {\n+namespace reference_ops {\n+\n+template <typename T, typename ActivationT>\n+inline T RoundAndAverage(ActivationT sum, int count) {\n+  // Round to the closest integer value.\n+  return sum > 0 ? (sum + count / 2) / count : (sum - count / 2) / count;\n+}\n+\n+template <>\n+inline float RoundAndAverage(float sum, int count) {\n+  // No rounding for float type.\n+  return sum / count;\n+}\n+\n+template <typename T, typename ActivationT>\n+inline void AveragePool3D(const Pool3DParams& params,\n+                          const RuntimeShape& input_shape, const T* input_data,\n+                          const RuntimeShape& output_shape, T* output_data) {\n+  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n+  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n+\n+  ActivationT activation_min, activation_max;\n+  GetActivationParams(params, &activation_min, &activation_max);\n+  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n+  const int channels = MatchingDim(input_shape, 4, output_shape, 4);\n+\n+  const int in_spatial_dim_1 = input_shape.Dims(1);\n+  const int in_spatial_dim_2 = input_shape.Dims(2);\n+  const int in_spatial_dim_3 = input_shape.Dims(3);\n+  const int out_spatial_dim_1 = output_shape.Dims(1);\n+  const int out_spatial_dim_2 = output_shape.Dims(2);\n+  const int out_spatial_dim_3 = output_shape.Dims(3);\n+\n+  const int stride_spatial_dim_1 = params.stride_depth;\n+  const int stride_spatial_dim_2 = params.stride_height;\n+  const int stride_spatial_dim_3 = params.stride_width;\n+  const int filter_spatial_dim_1 = params.filter_depth;\n+  const int filter_spatial_dim_2 = params.filter_height;\n+  const int filter_spatial_dim_3 = params.filter_width;\n+  const int padding_spatial_dim_1 = params.padding_values.depth;\n+  const int padding_spatial_dim_2 = params.padding_values.height;\n+  const int padding_spatial_dim_3 = params.padding_values.width;\n+\n+  for (int batch = 0; batch < batches; ++batch) {\n+    for (int out_d1 = 0; out_d1 < out_spatial_dim_1; ++out_d1) {\n+      const int in_d1_origin =\n+          (out_d1 * stride_spatial_dim_1) - padding_spatial_dim_1;\n+      const int filter_d1_start = std::max(0, -in_d1_origin);\n+      const int filter_d1_end =\n+          std::min(filter_spatial_dim_1, in_spatial_dim_1 - in_d1_origin);\n+      for (int out_d2 = 0; out_d2 < out_spatial_dim_2; ++out_d2) {\n+        const int in_d2_origin =\n+            (out_d2 * stride_spatial_dim_2) - padding_spatial_dim_2;\n+        const int filter_d2_start = std::max(0, -in_d2_origin);\n+        const int filter_d2_end =\n+            std::min(filter_spatial_dim_2, in_spatial_dim_2 - in_d2_origin);\n+        for (int out_d3 = 0; out_d3 < out_spatial_dim_3; ++out_d3) {\n+          const int in_d3_origin =\n+              (out_d3 * stride_spatial_dim_3) - padding_spatial_dim_3;\n+          const int filter_d3_start = std::max(0, -in_d3_origin);\n+          const int filter_d3_end =\n+              std::min(filter_spatial_dim_3, in_spatial_dim_3 - in_d3_origin);\n+          for (int channel = 0; channel < channels; ++channel) {\n+            ActivationT total = 0;\n+            for (int filter_d1 = filter_d1_start; filter_d1 < filter_d1_end;\n+                 ++filter_d1) {\n+              const int in_d1 = in_d1_origin + filter_d1;\n+              for (int filter_d2 = filter_d2_start; filter_d2 < filter_d2_end;\n+                   ++filter_d2) {\n+                const int in_d2 = in_d2_origin + filter_d2;\n+                for (int filter_d3 = filter_d3_start; filter_d3 < filter_d3_end;\n+                     ++filter_d3) {\n+                  const int in_d3 = in_d3_origin + filter_d3;\n+                  total += input_data[Offset(input_shape, batch, in_d1, in_d2,",
        "comment_created_at": "2022-12-06T10:53:31+00:00",
        "comment_author": "Tessil",
        "comment_body": "I moved the implementations from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/pooling3d.cc to their own `pooling3d.h` file. \r\n\r\nAs the PR has been open since more than one year and it's an important op to have for us, can we do it in two steps to avoid complexifying this PR more than it's currently? Once merged, I can create an optimisation PR to remove the `Offset` usage if it's alright to you.\r\n\r\nThanks",
        "pr_file_module": null
      },
      {
        "comment_id": "1040857387",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 53206,
        "pr_file": "tensorflow/lite/kernels/internal/reference/pooling3d.h",
        "discussion_id": "1040690535",
        "commented_code": "@@ -0,0 +1,193 @@\n+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING3D_H_\n+#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_POOLING3D_H_\n+\n+#include \"tensorflow/lite/kernels/internal/types.h\"\n+\n+namespace tflite {\n+namespace reference_ops {\n+\n+template <typename T, typename ActivationT>\n+inline T RoundAndAverage(ActivationT sum, int count) {\n+  // Round to the closest integer value.\n+  return sum > 0 ? (sum + count / 2) / count : (sum - count / 2) / count;\n+}\n+\n+template <>\n+inline float RoundAndAverage(float sum, int count) {\n+  // No rounding for float type.\n+  return sum / count;\n+}\n+\n+template <typename T, typename ActivationT>\n+inline void AveragePool3D(const Pool3DParams& params,\n+                          const RuntimeShape& input_shape, const T* input_data,\n+                          const RuntimeShape& output_shape, T* output_data) {\n+  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n+  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n+\n+  ActivationT activation_min, activation_max;\n+  GetActivationParams(params, &activation_min, &activation_max);\n+  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n+  const int channels = MatchingDim(input_shape, 4, output_shape, 4);\n+\n+  const int in_spatial_dim_1 = input_shape.Dims(1);\n+  const int in_spatial_dim_2 = input_shape.Dims(2);\n+  const int in_spatial_dim_3 = input_shape.Dims(3);\n+  const int out_spatial_dim_1 = output_shape.Dims(1);\n+  const int out_spatial_dim_2 = output_shape.Dims(2);\n+  const int out_spatial_dim_3 = output_shape.Dims(3);\n+\n+  const int stride_spatial_dim_1 = params.stride_depth;\n+  const int stride_spatial_dim_2 = params.stride_height;\n+  const int stride_spatial_dim_3 = params.stride_width;\n+  const int filter_spatial_dim_1 = params.filter_depth;\n+  const int filter_spatial_dim_2 = params.filter_height;\n+  const int filter_spatial_dim_3 = params.filter_width;\n+  const int padding_spatial_dim_1 = params.padding_values.depth;\n+  const int padding_spatial_dim_2 = params.padding_values.height;\n+  const int padding_spatial_dim_3 = params.padding_values.width;\n+\n+  for (int batch = 0; batch < batches; ++batch) {\n+    for (int out_d1 = 0; out_d1 < out_spatial_dim_1; ++out_d1) {\n+      const int in_d1_origin =\n+          (out_d1 * stride_spatial_dim_1) - padding_spatial_dim_1;\n+      const int filter_d1_start = std::max(0, -in_d1_origin);\n+      const int filter_d1_end =\n+          std::min(filter_spatial_dim_1, in_spatial_dim_1 - in_d1_origin);\n+      for (int out_d2 = 0; out_d2 < out_spatial_dim_2; ++out_d2) {\n+        const int in_d2_origin =\n+            (out_d2 * stride_spatial_dim_2) - padding_spatial_dim_2;\n+        const int filter_d2_start = std::max(0, -in_d2_origin);\n+        const int filter_d2_end =\n+            std::min(filter_spatial_dim_2, in_spatial_dim_2 - in_d2_origin);\n+        for (int out_d3 = 0; out_d3 < out_spatial_dim_3; ++out_d3) {\n+          const int in_d3_origin =\n+              (out_d3 * stride_spatial_dim_3) - padding_spatial_dim_3;\n+          const int filter_d3_start = std::max(0, -in_d3_origin);\n+          const int filter_d3_end =\n+              std::min(filter_spatial_dim_3, in_spatial_dim_3 - in_d3_origin);\n+          for (int channel = 0; channel < channels; ++channel) {\n+            ActivationT total = 0;\n+            for (int filter_d1 = filter_d1_start; filter_d1 < filter_d1_end;\n+                 ++filter_d1) {\n+              const int in_d1 = in_d1_origin + filter_d1;\n+              for (int filter_d2 = filter_d2_start; filter_d2 < filter_d2_end;\n+                   ++filter_d2) {\n+                const int in_d2 = in_d2_origin + filter_d2;\n+                for (int filter_d3 = filter_d3_start; filter_d3 < filter_d3_end;\n+                     ++filter_d3) {\n+                  const int in_d3 = in_d3_origin + filter_d3;\n+                  total += input_data[Offset(input_shape, batch, in_d1, in_d2,",
        "comment_created_at": "2022-12-06T11:35:38+00:00",
        "comment_author": "alankelly",
        "comment_body": "OK for me. Thanks",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "525590618",
    "pr_number": 44851,
    "pr_file": "tensorflow/compiler/mlir/tosa/transforms/legalize_tfl.cc",
    "created_at": "2020-11-17T23:20:48+00:00",
    "commented_code": "+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// Legalize TensorFlow Lite to TOSA\n+\n+#include <climits>\n+#include <cstddef>\n+#include <cstdint>\n+#include <fstream>\n+#include <iterator>\n+#include <numeric>\n+#include <unordered_set>\n+\n+#include \"llvm/ADT/APInt.h\"\n+#include \"llvm/ADT/ArrayRef.h\"\n+#include \"llvm/ADT/Optional.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/StringSwitch.h\"\n+#include \"mlir/Dialect/Quant/FakeQuantSupport.h\"\n+#include \"mlir/Dialect/Quant/QuantTypes.h\"\n+#include \"mlir/Dialect/Quant/UniformSupport.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"\n+#include \"mlir/Dialect/Tosa/IR/TosaOps.h\"\n+#include \"mlir/Dialect/Tosa/Utils/QuantUtils.h\"\n+#include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Diagnostics.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/Module.h\"\n+#include \"mlir/IR/Operation.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/StandardTypes.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n+#include \"mlir/IR/Types.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"tensorflow/compiler/mlir/lite/ir/tfl_ops.h\"\n+#include \"tensorflow/compiler/mlir/tosa/transforms/legalize_common.h\"\n+#include \"tensorflow/compiler/mlir/tosa/transforms/legalize_utils.h\"\n+#include \"tensorflow/compiler/mlir/tosa/transforms/passes.h\"\n+\n+#define PASS_NAME \"tosa-legalize-tfl\"\n+#define DEBUG_TYPE PASS_NAME\n+#define HARDSWISH_EXPLICIT_RESCALING false\n+\n+// Conditionally avoid converting some TFLite ops to TOSA.\n+// By default, all conversions will be invoked.\n+//\n+// The denylist file lists patterns which are not legalized from TFLite to TOSA.\n+llvm::cl::opt<std::string> tfl_tosa_denylist(\n+    \"tfl-tosa-denylist\",\n+    llvm::cl::desc(\"<a list of patterns not legalized from TFLite to TOSA>\"),\n+    llvm::cl::init(\"transforms/tfl_tosa_denylist.txt\"),\n+    llvm::cl::value_desc(\"pattern name\"));\n+\n+namespace mlir {\n+\n+namespace tosa {\n+\n+namespace {\n+// Performs lowering to TOSA dialect.\n+class LegalizeTFL : public PassWrapper<LegalizeTFL, FunctionPass> {\n+ public:\n+  explicit LegalizeTFL() {}\n+  void runOnFunction() override;\n+};\n+\n+#include \"tensorflow/compiler/mlir/tosa/transforms/tfl_legalize_patterns.inc\"\n+\n+#define DECL_CONVERT_OP(tfl_op)                                              \\\n+  struct ConvertTFL##tfl_op##Op : public RewritePattern {                    \\\n+    explicit ConvertTFL##tfl_op##Op(MLIRContext* context)                    \\\n+        : RewritePattern(TFL::tfl_op##Op::getOperationName(), 1, context) {} \\\n+    LogicalResult matchAndRewrite(Operation* op,                             \\\n+                                  PatternRewriter& rewriter) const override; \\\n+    static const std::string& getName() {                                    \\\n+      static char name[100];                                                 \\\n+      snprintf(name, sizeof(name), \"ConvertTFL%sOp\", #tfl_op);               \\\n+      static std::string name_str = std::string(name);                       \\\n+      return name_str;                                                       \\\n+    }                                                                        \\\n+  }\n+DECL_CONVERT_OP(Relu);\n+DECL_CONVERT_OP(Relu6);\n+DECL_CONVERT_OP(Equal);\n+DECL_CONVERT_OP(NotEqual);\n+DECL_CONVERT_OP(Greater);\n+DECL_CONVERT_OP(GreaterEqual);\n+DECL_CONVERT_OP(Add);\n+DECL_CONVERT_OP(Sub);\n+DECL_CONVERT_OP(Mul);\n+DECL_CONVERT_OP(Square);\n+DECL_CONVERT_OP(SquaredDifference);\n+DECL_CONVERT_OP(Round);\n+DECL_CONVERT_OP(Div);\n+DECL_CONVERT_OP(Maximum);\n+DECL_CONVERT_OP(Minimum);\n+DECL_CONVERT_OP(FloorMod);\n+DECL_CONVERT_OP(FloorDiv);\n+DECL_CONVERT_OP(AddN);\n+DECL_CONVERT_OP(AveragePool2D);\n+DECL_CONVERT_OP(MaxPool2D);\n+DECL_CONVERT_OP(Concatenation);\n+DECL_CONVERT_OP(Reshape);\n+DECL_CONVERT_OP(Rank);\n+DECL_CONVERT_OP(Shape);\n+DECL_CONVERT_OP(ExpandDims);\n+DECL_CONVERT_OP(Squeeze);\n+DECL_CONVERT_OP(Fill);\n+DECL_CONVERT_OP(Elu);\n+DECL_CONVERT_OP(Softmax);\n+DECL_CONVERT_OP(LogSoftmax);\n+DECL_CONVERT_OP(ReduceAny);\n+DECL_CONVERT_OP(ReduceMax);\n+DECL_CONVERT_OP(ReduceMin);\n+DECL_CONVERT_OP(Mean);\n+DECL_CONVERT_OP(ReduceProd);\n+DECL_CONVERT_OP(Sum);\n+DECL_CONVERT_OP(Conv2D);\n+DECL_CONVERT_OP(TransposeConv);\n+DECL_CONVERT_OP(DepthwiseConv2D);\n+DECL_CONVERT_OP(FullyConnected);\n+DECL_CONVERT_OP(Split);\n+DECL_CONVERT_OP(SplitV);\n+DECL_CONVERT_OP(Pack);\n+DECL_CONVERT_OP(Unpack);\n+DECL_CONVERT_OP(Transpose);\n+DECL_CONVERT_OP(Tile);\n+DECL_CONVERT_OP(Slice);\n+DECL_CONVERT_OP(StridedSlice);\n+DECL_CONVERT_OP(HardSwish);\n+DECL_CONVERT_OP(ZerosLike);\n+DECL_CONVERT_OP(Less);\n+DECL_CONVERT_OP(LessEqual);\n+DECL_CONVERT_OP(Pad);\n+DECL_CONVERT_OP(ResizeBilinear);\n+DECL_CONVERT_OP(ResizeNearestNeighbor);\n+DECL_CONVERT_OP(Select);\n+DECL_CONVERT_OP(SelectV2);\n+DECL_CONVERT_OP(SpaceToBatchNd);\n+DECL_CONVERT_OP(BatchToSpaceNd);\n+DECL_CONVERT_OP(SpaceToDepth);\n+DECL_CONVERT_OP(DepthToSpace);\n+DECL_CONVERT_OP(Logistic);\n+DECL_CONVERT_OP(Tanh);\n+DECL_CONVERT_OP(PRelu);\n+DECL_CONVERT_OP(LeakyRelu);\n+DECL_CONVERT_OP(Neg);\n+DECL_CONVERT_OP(Yield);\n+DECL_CONVERT_OP(Custom);\n+DECL_CONVERT_OP(ReverseV2);\n+DECL_CONVERT_OP(Quantize);\n+DECL_CONVERT_OP(Dequantize);\n+DECL_CONVERT_OP(QConst);\n+#undef DECL_CONVERT_OP\n+\n+LogicalResult ConvertTFLReluOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_relu_op = cast<TFL::ReluOp>(op);\n+\n+  auto input_type = tfl_relu_op.x().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_relu_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type || !output_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLReluOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_qtype = input_type.getElementType()\n+                           .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    auto op1_rescale_in = buildRescaleToInt32(rewriter, op, tfl_relu_op.x(),\n+                                              1.0f, input_qtype.getZeroPoint());\n+    auto op2_relun_op1 = rewriter.create<tosa::ReluNOp>(\n+        op->getLoc(), rescale_type, op1_rescale_in,\n+        rewriter.getI64IntegerAttr(std::numeric_limits<int32_t>::max()),\n+        rewriter.getF32FloatAttr(0.0f));\n+    auto op3_rescale_op2 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op2_relun_op1.getResult(), 1.0f,\n+        output_qtype.getZeroPoint());\n+\n+    output = op3_rescale_op2;\n+  } else {\n+    auto op1_relun_in = rewriter.create<tosa::ReluNOp>(\n+        op->getLoc(), output_type, tfl_relu_op.x(),\n+        rewriter.getI64IntegerAttr(0),\n+        rewriter.getF32FloatAttr(std::numeric_limits<float>::max()));\n+\n+    output = op1_relun_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLRelu6Op::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_relu6_op = cast<TFL::Relu6Op>(op);\n+\n+  auto input_type = tfl_relu6_op.x().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_relu6_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type || !output_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLRelu6Op: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_qtype = input_type.getElementType()\n+                           .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    int64_t rescaled_6 = (int64_t)std::round(6.0f / input_qtype.getScale()) +\n+                         input_qtype.getZeroPoint();\n+\n+    auto op1_rescale_in = buildRescaleToInt32(rewriter, op, tfl_relu6_op.x(),\n+                                              1.0f, input_qtype.getZeroPoint());\n+    auto op2_relun_op1 = rewriter.create<tosa::ReluNOp>(\n+        op->getLoc(), rescale_type, op1_rescale_in,\n+        rewriter.getI64IntegerAttr(rescaled_6), rewriter.getF32FloatAttr(0.0f));\n+    auto op3_rescale_op2 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op2_relun_op1.getResult(), 1.0f,\n+        output_qtype.getZeroPoint());\n+\n+    output = op3_rescale_op2;\n+  } else {\n+    auto op1_relun_in = rewriter.create<tosa::ReluNOp>(\n+        op->getLoc(), output_type, tfl_relu6_op.x(),\n+        rewriter.getI64IntegerAttr(0), rewriter.getF32FloatAttr(6.0f));\n+\n+    output = op1_relun_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLEqualOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_equal_op = cast<TFL::EqualOp>(op);\n+\n+  auto input_x_type = tfl_equal_op.x().getType().dyn_cast<RankedTensorType>();\n+  auto input_y_type = tfl_equal_op.y().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_equal_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_x_type || !input_y_type || !output_type) return failure();\n+\n+  bool input_x_is_qtype =\n+      input_x_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_y_is_qtype =\n+      input_y_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_x_is_qtype != output_is_qtype ||\n+      input_y_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLEqualOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_x_qtype = input_x_type.getElementType()\n+                             .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_y_qtype = input_y_type.getElementType()\n+                             .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    if (input_x_qtype.getScale() != input_y_qtype.getScale() ||\n+        input_x_qtype.getZeroPoint() != input_y_qtype.getZeroPoint()) {\n+      op->emitOpError(\n+          \"ConvertTFLEqualOp: input_x and input_y scale/zp \"\n+          \"must be the same\");\n+      return failure();\n+    }\n+\n+    auto op1_rescale_x = buildRescaleToInt32(\n+        rewriter, op, tfl_equal_op.x(), 1.0f, input_x_qtype.getZeroPoint());\n+    auto op2_rescale_y = buildRescaleToInt32(\n+        rewriter, op, tfl_equal_op.y(), 1.0f, input_y_qtype.getZeroPoint());\n+    auto op3_equal_op1_op2 = rewriter.create<tosa::EqualOp>(\n+        op->getLoc(), output_type, op1_rescale_x, op2_rescale_y);\n+\n+    output = op3_equal_op1_op2.getResult();\n+  } else {\n+    auto op1_equal_in = rewriter.create<tosa::EqualOp>(\n+        op->getLoc(), output_type, tfl_equal_op.x(), tfl_equal_op.y());\n+\n+    output = op1_equal_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLNotEqualOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_not_equal_op = cast<TFL::NotEqualOp>(op);\n+\n+  auto input_lhs_type =\n+      tfl_not_equal_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type =\n+      tfl_not_equal_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_not_equal_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLNotEqualOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    if (input_lhs_qtype.getScale() != input_rhs_qtype.getScale() ||\n+        input_lhs_qtype.getZeroPoint() != input_rhs_qtype.getZeroPoint()) {\n+      op->emitOpError(\n+          \"ConvertTFLNotEqualOp: input_x and input_y scale/zp \"\n+          \"must be the same\");\n+      return failure();\n+    }\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_not_equal_op.lhs(), 1.0f,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_not_equal_op.rhs(), 1.0f,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_equal_op1_op2 = rewriter.create<tosa::EqualOp>(\n+        op->getLoc(), output_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_not_op3 = rewriter.create<tosa::LogicalNotOp>(\n+        op->getLoc(), output_type, op3_equal_op1_op2.getResult());\n+\n+    output = op4_not_op3.getResult();\n+  } else {\n+    auto op1_equal_in = rewriter.create<tosa::EqualOp>(\n+        op->getLoc(), output_type, tfl_not_equal_op.lhs(),\n+        tfl_not_equal_op.rhs());\n+    auto op2_not_op1 = rewriter.create<tosa::LogicalNotOp>(\n+        op->getLoc(), output_type, op1_equal_in.getResult());\n+\n+    output = op2_not_op1.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLGreaterOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_greater_op = cast<TFL::GreaterOp>(op);\n+\n+  auto input_lhs_type =\n+      tfl_greater_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type =\n+      tfl_greater_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_greater_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLGreaterOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    if (input_lhs_qtype.getScale() != input_rhs_qtype.getScale() ||\n+        input_lhs_qtype.getZeroPoint() != input_rhs_qtype.getZeroPoint()) {\n+      op->emitOpError(\n+          \"ConvertTFLGreaterOp: input_x and input_y scale/zp \"\n+          \"must be the same\");\n+      return failure();\n+    }\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_greater_op.lhs(), 1.0f,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_greater_op.rhs(), 1.0f,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_greater_op1_op2 = rewriter.create<tosa::GreaterOp>(\n+        op->getLoc(), output_type, op1_rescale_lhs, op2_rescale_rhs);\n+\n+    output = op3_greater_op1_op2.getResult();\n+  } else {\n+    auto op1_greater_in = rewriter.create<tosa::GreaterOp>(\n+        op->getLoc(), output_type, tfl_greater_op.lhs(), tfl_greater_op.rhs());\n+\n+    output = op1_greater_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLGreaterEqualOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_greater_equal_op = cast<TFL::GreaterEqualOp>(op);\n+\n+  auto input_lhs_type =\n+      tfl_greater_equal_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type =\n+      tfl_greater_equal_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_greater_equal_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLGreaterEqualOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    if (input_lhs_qtype.getScale() != input_rhs_qtype.getScale() ||\n+        input_lhs_qtype.getZeroPoint() != input_rhs_qtype.getZeroPoint()) {\n+      op->emitOpError(\n+          \"ConvertTFLGreaterEqualOp: input_x and input_y scale/zp \"\n+          \"must be the same\");\n+      return failure();\n+    }\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_greater_equal_op.lhs(), 1.0f,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_greater_equal_op.rhs(), 1.0f,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_greater_equal_op1_op2 = rewriter.create<tosa::GreaterEqualOp>(\n+        op->getLoc(), output_type, op1_rescale_lhs, op2_rescale_rhs);\n+\n+    output = op3_greater_equal_op1_op2.getResult();\n+  } else {\n+    auto op1_greater_equal_in = rewriter.create<tosa::GreaterEqualOp>(\n+        op->getLoc(), output_type, tfl_greater_equal_op.lhs(),\n+        tfl_greater_equal_op.rhs());\n+\n+    output = op1_greater_equal_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLAddOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_add_op = cast<TFL::AddOp>(op);\n+\n+  auto input_lhs_type = tfl_add_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type = tfl_add_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_add_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLAddOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    double in_lhs_scale = input_lhs_qtype.getScale();\n+    double in_rhs_scale = input_rhs_qtype.getScale();\n+    double output_scale = output_qtype.getScale();\n+    double max_scale_2x = 2.0 * std::max(in_lhs_scale, in_rhs_scale);\n+\n+    int32_t input_shift = 20;\n+\n+    double lhs_rescale_scale =\n+        double(1 << input_shift) * in_lhs_scale / max_scale_2x;\n+    double rhs_rescale_scale =\n+        double(1 << input_shift) * in_rhs_scale / max_scale_2x;\n+    double output_rescale_scale =\n+        max_scale_2x / (output_scale * double(1 << input_shift));\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_add_op.lhs(), lhs_rescale_scale,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_add_op.rhs(), rhs_rescale_scale,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_add_op1_op2 = rewriter.create<tosa::AddOp>(\n+        op->getLoc(), rescale_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_rescale_op3 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op3_add_op1_op2.getResult(),\n+        output_rescale_scale, output_qtype.getZeroPoint());\n+    output = op4_rescale_op3;\n+  } else {\n+    auto op1_add_in = rewriter.create<tosa::AddOp>(\n+        op->getLoc(), output_type, tfl_add_op.lhs(), tfl_add_op.rhs());\n+\n+    output = op1_add_in.getResult();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_add_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op =\n+        convertFusedActivation(rewriter, op, output, fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLSubOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_sub_op = cast<TFL::SubOp>(op);\n+\n+  auto input_lhs_type = tfl_sub_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type = tfl_sub_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_sub_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLSubOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    double in_lhs_scale = input_lhs_qtype.getScale();\n+    double in_rhs_scale = input_rhs_qtype.getScale();\n+    double output_scale = output_qtype.getScale();\n+    double max_scale_2x = 2.0 * std::max(in_lhs_scale, in_rhs_scale);\n+\n+    int32_t input_shift = 20;\n+\n+    double lhs_rescale_scale =\n+        double(1 << input_shift) * in_lhs_scale / max_scale_2x;\n+    double rhs_rescale_scale =\n+        double(1 << input_shift) * in_rhs_scale / max_scale_2x;\n+    double output_rescale_scale =\n+        max_scale_2x / (output_scale * double(1 << input_shift));\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_sub_op.lhs(), lhs_rescale_scale,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_sub_op.rhs(), rhs_rescale_scale,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_sub_op1_op2 = rewriter.create<tosa::SubOp>(\n+        op->getLoc(), rescale_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_rescale_op3 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op3_sub_op1_op2.getResult(),\n+        output_rescale_scale, output_qtype.getZeroPoint());\n+    output = op4_rescale_op3;\n+  } else {\n+    auto op1_sub_in = rewriter.create<tosa::SubOp>(\n+        op->getLoc(), output_type, tfl_sub_op.lhs(), tfl_sub_op.rhs());\n+\n+    output = op1_sub_in.getResult();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_sub_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op =\n+        convertFusedActivation(rewriter, op, output, fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLMulOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_mul_op = cast<TFL::MulOp>(op);\n+\n+  auto lowered_op = convertMultiplyOp(rewriter, op, tfl_mul_op.getResult(),\n+                                      tfl_mul_op.lhs(), tfl_mul_op.rhs());\n+\n+  if (!lowered_op) {\n+    return failure();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_mul_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, lowered_op->getResult(0), fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {lowered_op->getResult(0)});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLSquareOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_square_op = cast<TFL::SquareOp>(op);\n+\n+  auto lowered_op = convertMultiplyOp(rewriter, op, tfl_square_op.getResult(),\n+                                      tfl_square_op.x(), tfl_square_op.x());\n+\n+  if (!lowered_op) {\n+    return failure();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_square_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, lowered_op->getResult(0), fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {lowered_op->getResult(0)});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLSquaredDifferenceOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_squared_op = cast<TFL::SquaredDifferenceOp>(op);\n+\n+  auto lowered_op =\n+      convertSquaredDifferenceOp(rewriter, op, tfl_squared_op.getResult(),\n+                                 tfl_squared_op.lhs(), tfl_squared_op.rhs());\n+\n+  TOSA_REPLACE_LOWERED_OP(rewriter, op, lowered_op);\n+}\n+\n+LogicalResult ConvertTFLRoundOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_round_op = cast<TFL::RoundOp>(op);\n+\n+  auto input_type = tfl_round_op.x().getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) {\n+    op->emitOpError(\"Round: input not ranked tensor type\");\n+    return failure();\n+  }\n+\n+  if (input_type.getElementType().isa<FloatType>()) {\n+    auto lowered_op = convertRoundOp(rewriter, op, tfl_round_op.getResult(),\n+                                     tfl_round_op.x());\n+\n+    TOSA_REPLACE_LOWERED_OP(rewriter, op, lowered_op);\n+\n+  } else {\n+    // Round on int is nonsensical. Instead, replace uses of result with the\n+    // input.\n+    tfl_round_op.replaceAllUsesWith(tfl_round_op.x());\n+  }\n+}\n+\n+LogicalResult ConvertTFLDivOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_div_op = cast<TFL::DivOp>(op);\n+\n+  auto output_type =\n+      tfl_div_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!output_type) return failure();\n+\n+  auto fused_activation_fn =\n+      tfl_div_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  auto reciprocal_op = rewriter.create<tosa::ReciprocalOp>(\n+      op->getLoc(), output_type, tfl_div_op.rhs());\n+  auto mul_op =\n+      rewriter.create<tosa::MulOp>(op->getLoc(), output_type, tfl_div_op.lhs(),\n+                                   reciprocal_op.getResult(), 0);\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, mul_op.getResult(), fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {mul_op.getResult()});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLMaximumOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_max_op = cast<TFL::MaximumOp>(op);\n+\n+  auto input_lhs_type = tfl_max_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type = tfl_max_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_max_op.getResult().getType().dyn_cast<RankedTensorType>();\n+\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLMaximumOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_max_op.lhs(), 1.0f, 0);\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_max_op.rhs(), 1.0f, 0);\n+    auto op3_max_op1_op2 = rewriter.create<tosa::MaximumOp>(\n+        op->getLoc(), rescale_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_rescale_op3 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op3_max_op1_op2.getResult(), 1.0f, 0);\n+\n+    output = op4_rescale_op3;\n+  } else {\n+    auto op1_max_in = rewriter.create<tosa::MaximumOp>(\n+        op->getLoc(), output_type, tfl_max_op.lhs(), tfl_max_op.rhs());\n+\n+    output = op1_max_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLMinimumOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_min_op = cast<TFL::MinimumOp>(op);\n+\n+  auto input_lhs_type = tfl_min_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type = tfl_min_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_min_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLMinimumOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_min_op.lhs(), 1.0f, 0);\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_min_op.rhs(), 1.0f, 0);\n+    auto op3_min_op1_op2 = rewriter.create<tosa::MinimumOp>(\n+        op->getLoc(), rescale_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_rescale_op3 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op3_min_op1_op2.getResult(), 1.0f, 0);\n+\n+    output = op4_rescale_op3;\n+  } else {\n+    auto op1_min_in = rewriter.create<tosa::MinimumOp>(\n+        op->getLoc(), output_type, tfl_min_op.lhs(), tfl_min_op.rhs());\n+\n+    output = op1_min_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLFloorDivOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_floordiv_op = cast<TFL::FloorDivOp>(op);\n+\n+  auto lowered_op =\n+      convertFloorDivOp(rewriter, op, tfl_floordiv_op.getResult(),\n+                        tfl_floordiv_op.lhs(), tfl_floordiv_op.rhs());\n+\n+  TOSA_REPLACE_LOWERED_OP(rewriter, op, lowered_op);\n+}\n+\n+LogicalResult ConvertTFLFloorModOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_floormod_op = cast<TFL::FloorModOp>(op);\n+\n+  auto lowered_op =\n+      convertFloorModOp(rewriter, op, tfl_floormod_op.getResult(),\n+                        tfl_floormod_op.lhs(), tfl_floormod_op.rhs());\n+\n+  TOSA_REPLACE_LOWERED_OP(rewriter, op, lowered_op);\n+}\n+\n+LogicalResult ConvertTFLAddNOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_addn_op = cast<TFL::AddNOp>(op);\n+\n+  auto output_type =\n+      tfl_addn_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!output_type) return failure();\n+\n+  SmallVector<Value, 4> inputs(tfl_addn_op.inputs());\n+\n+  assert(inputs.size() >= 2);\n+\n+  auto newOp = rewriter.create<tosa::AddOp>(op->getLoc(), output_type,\n+                                            inputs[0], inputs[1]);\n+  for (int i = 2; i < inputs.size(); i++) {\n+    newOp = rewriter.create<tosa::AddOp>(op->getLoc(), output_type, inputs[i],\n+                                         newOp.getResult());\n+  }\n+\n+  rewriter.replaceOp(op, {newOp.getResult()});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLAveragePool2DOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_avgpool_op = cast<TFL::AveragePool2DOp>(op);\n+\n+  auto input_type =\n+      tfl_avgpool_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_avgpool_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!output_type) return failure();\n+\n+  auto tmpAttr = tfl_avgpool_op.getAttrOfType<StringAttr>(\"data_format\");\n+  if (tmpAttr && tmpAttr.getValue().str() != \"NHWC\") return failure();\n+\n+  // Kernels and strides are dimensionally ordered\n+  SmallVector<int64_t, 4> i64array({1, 1, 1, 1});\n+  ArrayAttr kernel_size;\n+  ArrayAttr stride;\n+  ArrayAttr pad;\n+  {\n+    int64_t kernel_h = tfl_avgpool_op.filter_height();\n+    int64_t kernel_w = tfl_avgpool_op.filter_width();\n+    kernel_size = rewriter.getI64ArrayAttr({kernel_h, kernel_w});\n+    // i64array is formatted as NHWC now\n+    i64array[1] = kernel_h;\n+    i64array[2] = kernel_w;\n+  }\n+  {\n+    int64_t stride_h = tfl_avgpool_op.stride_h();\n+    int64_t stride_w = tfl_avgpool_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_avgpool_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    // Pooling has no non-unit dilation\n+    ArrayAttr dilation = rewriter.getI64ArrayAttr({1, 1});\n+\n+    auto filter_type = RankedTensorType::get(\n+        llvm::makeArrayRef<int64_t>(i64array), rewriter.getIntegerType(64));\n+\n+    // TFLite doesn't support explicit padding\n+    if (!getPaddingValuesFromPadType(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, stride, dilation, rewriter, pad))\n+      return failure();\n+  }\n+\n+  rewriter.replaceOpWithNewOp<tosa::AvgPool2dOp>(\n+      op, output_type, tfl_avgpool_op.input(), kernel_size, stride, pad);\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLMaxPool2DOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_maxpool_op = cast<TFL::MaxPool2DOp>(op);\n+\n+  auto input_type =\n+      tfl_maxpool_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_maxpool_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!output_type) return failure();\n+\n+  auto tmpAttr = tfl_maxpool_op.getAttrOfType<StringAttr>(\"data_format\");\n+  if (tmpAttr && tmpAttr.getValue().str() != \"NHWC\") return failure();\n+\n+  // Kernels and strides are dimensionally ordered\n+  SmallVector<int64_t, 4> i64array({1, 1, 1, 1});\n+  ArrayAttr kernel_size;\n+  ArrayAttr stride;\n+  ArrayAttr pad;\n+  {\n+    int64_t kernel_h = tfl_maxpool_op.filter_height();\n+    int64_t kernel_w = tfl_maxpool_op.filter_width();\n+    kernel_size = rewriter.getI64ArrayAttr({kernel_h, kernel_w});\n+    // i64array is formatted as NHWC now\n+    i64array[1] = kernel_h;\n+    i64array[2] = kernel_w;\n+  }\n+  {\n+    int64_t stride_h = tfl_maxpool_op.stride_h();\n+    int64_t stride_w = tfl_maxpool_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_maxpool_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    // Pooling has no non-unit dilation\n+    ArrayAttr dilation = rewriter.getI64ArrayAttr({1, 1});\n+\n+    auto filter_type = RankedTensorType::get(\n+        llvm::makeArrayRef<int64_t>(i64array), rewriter.getIntegerType(64));\n+\n+    // TFLite doesn't support explicit padding\n+    if (!getPaddingValuesFromPadType(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, stride, dilation, rewriter, pad))\n+      return failure();\n+  }\n+\n+  rewriter.replaceOpWithNewOp<tosa::MaxPool2dOp>(\n+      op, output_type, tfl_maxpool_op.input(), kernel_size, stride, pad);\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLConv2DOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_conv2d_op = cast<TFL::Conv2DOp>(op);\n+\n+  auto input_type =\n+      tfl_conv2d_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto filter_type =\n+      tfl_conv2d_op.filter().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_conv2d_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type) return failure();\n+  if (!output_type) return failure();\n+  if (!filter_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool filter_is_qtype =\n+      filter_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::QuantizedType>();\n+\n+  if ((input_is_qtype != filter_is_qtype) ||\n+      (input_is_qtype != output_is_qtype)) {\n+    op->emitOpError(\n+        \"ConvertTFLConv2DOp: input/filter/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  auto tmpAttr = tfl_conv2d_op.getAttrOfType<StringAttr>(\"data_format\");\n+  if (tmpAttr && tmpAttr.getValue().str() != \"NHWC\") return failure();\n+\n+  ArrayAttr pad;\n+  ArrayAttr stride;\n+  ArrayAttr dilation;\n+  StringAttr activation_function;\n+  {\n+    int64_t stride_h = tfl_conv2d_op.stride_h();\n+    int64_t stride_w = tfl_conv2d_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+  {\n+    int64_t dilation_h = tfl_conv2d_op.dilation_h_factor();\n+    int64_t dilation_w = tfl_conv2d_op.dilation_w_factor();\n+    dilation = rewriter.getI64ArrayAttr({dilation_h, dilation_w});\n+  }\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_conv2d_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    // TFLite doesn't support explicit padding\n+    if (!getPaddingValuesFromPadType(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, stride, dilation, rewriter, pad))\n+      return failure();\n+  }\n+\n+  Value unquantized_bias =\n+      getUnquantizedBias(rewriter, op, tfl_conv2d_op.bias());\n+\n+  auto a1_conv2d_op = rewriter.create<tosa::Conv2DOp>(\n+      op->getLoc(), output_type, tfl_conv2d_op.input(), tfl_conv2d_op.filter(),\n+      unquantized_bias, pad, stride, dilation);\n+\n+  Value conv2d_output;\n+  if (input_is_qtype) {\n+    conv2d_output =\n+        buildRescaleOpConvOutput(rewriter, op, a1_conv2d_op.getResult(),\n+                                 input_type, filter_type, output_type);\n+  } else {\n+    conv2d_output = a1_conv2d_op.getResult();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_conv2d_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, conv2d_output, fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {conv2d_output});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLTransposeConvOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_conv_op = cast<TFL::TransposeConvOp>(op);\n+\n+  auto input_type = tfl_conv_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto filter_type =\n+      tfl_conv_op.weights().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_conv_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type) return failure();\n+  if (!output_type) return failure();\n+  if (!filter_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool filter_is_qtype =\n+      filter_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::QuantizedType>();\n+\n+  if ((input_is_qtype != filter_is_qtype) ||\n+      (input_is_qtype != output_is_qtype)) {\n+    op->emitOpError(\n+        \"ConvertTFLConv2DOp: input/filter/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  ArrayAttr stride;\n+  ArrayAttr dilation;\n+  ArrayAttr outpad;\n+  ArrayAttr output_shape;\n+  StringAttr activation_function;\n+  {\n+    int64_t stride_h = tfl_conv_op.stride_h();\n+    int64_t stride_w = tfl_conv_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+\n+  // tfl.transpose_conv doesn't support dilations\n+  dilation = rewriter.getI64ArrayAttr({1, 1});\n+\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_conv_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    if (!getTransposeConv2dPaddingValues(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, output_type, stride, dilation, rewriter,\n+            outpad))\n+      return failure();\n+  }\n+  {\n+    ElementsAttr output_shape_elems;\n+    // Match from input_size tensor first\n+    if (matchPattern(tfl_conv_op.output_shape(),\n+                     m_Constant(&output_shape_elems))) {\n+      llvm::SmallVector<int64_t, 4> shape_vec;\n+      for (int i = 0; i < output_shape_elems.getNumElements(); i++)\n+        shape_vec.push_back(\n+            output_shape_elems.getValue<IntegerAttr>(i).getInt());\n+      output_shape = rewriter.getI64ArrayAttr(shape_vec);\n+    }\n+    // Use output tensor's shape otherwise\n+    else {\n+      output_shape = rewriter.getI64ArrayAttr(output_type.getShape());\n+    }\n+  }\n+\n+  Value zero_bias;\n+  if (input_is_qtype) {\n+    uint32_t input_bits = input_type.getElementType()\n+                              .dyn_cast<mlir::quant::QuantizedType>()\n+                              .getStorageTypeIntegralWidth();\n+    uint32_t weight_bits = filter_type.getElementType()\n+                               .dyn_cast<mlir::quant::QuantizedType>()\n+                               .getStorageTypeIntegralWidth();\n+\n+    if (input_bits == 16 && weight_bits == 8) {\n+      SmallVector<int64_t, 8> zero_bias_vec(output_type.getShape()[3], 0);\n+      zero_bias = get1DConstTensorInt48(rewriter, op, zero_bias_vec);\n+    } else {\n+      SmallVector<int32_t, 8> zero_bias_vec(output_type.getShape()[3], 0);\n+      zero_bias =\n+          get1DConstTensor<tosa::ConstOp, int32_t>(rewriter, op, zero_bias_vec);\n+    }\n+  } else {\n+    SmallVector<float, 8> zero_bias_vec(output_type.getShape()[3], 0.0f);\n+    zero_bias =\n+        get1DConstTensor<tosa::ConstOp, float>(rewriter, op, zero_bias_vec);\n+  }\n+\n+  auto a1_conv2d_op = rewriter.create<tosa::TransposeConv2DOp>(\n+      op->getLoc(), output_type, tfl_conv_op.input(), tfl_conv_op.weights(),\n+      zero_bias, outpad, stride, dilation, output_shape);\n+\n+  Value conv2d_output;\n+  if (input_is_qtype) {\n+    conv2d_output =\n+        buildRescaleOpConvOutput(rewriter, op, a1_conv2d_op.getResult(),\n+                                 input_type, filter_type, output_type);\n+  } else {\n+    conv2d_output = a1_conv2d_op.getResult();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_conv_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, conv2d_output, fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {conv2d_output});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLDepthwiseConv2DOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_conv2d_op = cast<TFL::DepthwiseConv2DOp>(op);\n+\n+  auto input_type =\n+      tfl_conv2d_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto filter_type =\n+      tfl_conv2d_op.filter().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_conv2d_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type) return failure();\n+  if (!output_type) return failure();\n+  if (!filter_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool filter_is_qtype =\n+      filter_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::QuantizedType>();\n+\n+  if ((input_is_qtype != filter_is_qtype) ||\n+      (input_is_qtype != output_is_qtype)) {\n+    op->emitOpError(\n+        \"ConvertTFLConv2DOp: input/filter/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  auto filter_shape = filter_type.getShape();\n+  // Operator depthwiseConv2D\n+  // TFLite orders the depthwiseConv2D filter in IHWO, while TOSA orders\n+  // filter in HWIO\n+  //\n+  // The lowering reorders the filter.\n+  //\n+  // a1_transpose = tosa.transpose(filter, {1, 2, 3, 0})   // HWIO\n+  // a2_reshape = tosa.reshape(filter, H, W, depth_multiplier, I /\n+  // depth_multiplier)\n+  // a3_transpose_conv2d = tosa.transpose_conv2d(input, a2_reshape, padding,\n+  // stride, dilation)\n+\n+  auto tmpAttr = tfl_conv2d_op.getAttrOfType<StringAttr>(\"data_format\");\n+  if (tmpAttr && tmpAttr.getValue().str() != \"NHWC\") return failure();\n+\n+  ArrayAttr pad;\n+  ArrayAttr stride;\n+  ArrayAttr dilation;\n+  StringAttr activation_function;\n+\n+  auto depth_multiplier =\n+      tfl_conv2d_op.getAttrOfType<IntegerAttr>(\"depth_multiplier\");\n+\n+  {\n+    int64_t stride_h = tfl_conv2d_op.stride_h();\n+    int64_t stride_w = tfl_conv2d_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+  {\n+    int64_t dilation_h = tfl_conv2d_op.dilation_h_factor();\n+    int64_t dilation_w = tfl_conv2d_op.dilation_w_factor();\n+    dilation = rewriter.getI64ArrayAttr({dilation_h, dilation_w});\n+  }\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_conv2d_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    if (!getPaddingValuesFromPadType(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, stride, dilation, rewriter, pad))\n+      return failure();\n+  }\n+\n+  std::vector<int64_t> a1_transpose_dims;",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "525590618",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 44851,
        "pr_file": "tensorflow/compiler/mlir/tosa/transforms/legalize_tfl.cc",
        "discussion_id": "525590618",
        "commented_code": "@@ -0,0 +1,2918 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// Legalize TensorFlow Lite to TOSA\n+\n+#include <climits>\n+#include <cstddef>\n+#include <cstdint>\n+#include <fstream>\n+#include <iterator>\n+#include <numeric>\n+#include <unordered_set>\n+\n+#include \"llvm/ADT/APInt.h\"\n+#include \"llvm/ADT/ArrayRef.h\"\n+#include \"llvm/ADT/Optional.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/StringSwitch.h\"\n+#include \"mlir/Dialect/Quant/FakeQuantSupport.h\"\n+#include \"mlir/Dialect/Quant/QuantTypes.h\"\n+#include \"mlir/Dialect/Quant/UniformSupport.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"\n+#include \"mlir/Dialect/Tosa/IR/TosaOps.h\"\n+#include \"mlir/Dialect/Tosa/Utils/QuantUtils.h\"\n+#include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Diagnostics.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/Module.h\"\n+#include \"mlir/IR/Operation.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/StandardTypes.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n+#include \"mlir/IR/Types.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"tensorflow/compiler/mlir/lite/ir/tfl_ops.h\"\n+#include \"tensorflow/compiler/mlir/tosa/transforms/legalize_common.h\"\n+#include \"tensorflow/compiler/mlir/tosa/transforms/legalize_utils.h\"\n+#include \"tensorflow/compiler/mlir/tosa/transforms/passes.h\"\n+\n+#define PASS_NAME \"tosa-legalize-tfl\"\n+#define DEBUG_TYPE PASS_NAME\n+#define HARDSWISH_EXPLICIT_RESCALING false\n+\n+// Conditionally avoid converting some TFLite ops to TOSA.\n+// By default, all conversions will be invoked.\n+//\n+// The denylist file lists patterns which are not legalized from TFLite to TOSA.\n+llvm::cl::opt<std::string> tfl_tosa_denylist(\n+    \"tfl-tosa-denylist\",\n+    llvm::cl::desc(\"<a list of patterns not legalized from TFLite to TOSA>\"),\n+    llvm::cl::init(\"transforms/tfl_tosa_denylist.txt\"),\n+    llvm::cl::value_desc(\"pattern name\"));\n+\n+namespace mlir {\n+\n+namespace tosa {\n+\n+namespace {\n+// Performs lowering to TOSA dialect.\n+class LegalizeTFL : public PassWrapper<LegalizeTFL, FunctionPass> {\n+ public:\n+  explicit LegalizeTFL() {}\n+  void runOnFunction() override;\n+};\n+\n+#include \"tensorflow/compiler/mlir/tosa/transforms/tfl_legalize_patterns.inc\"\n+\n+#define DECL_CONVERT_OP(tfl_op)                                              \\\n+  struct ConvertTFL##tfl_op##Op : public RewritePattern {                    \\\n+    explicit ConvertTFL##tfl_op##Op(MLIRContext* context)                    \\\n+        : RewritePattern(TFL::tfl_op##Op::getOperationName(), 1, context) {} \\\n+    LogicalResult matchAndRewrite(Operation* op,                             \\\n+                                  PatternRewriter& rewriter) const override; \\\n+    static const std::string& getName() {                                    \\\n+      static char name[100];                                                 \\\n+      snprintf(name, sizeof(name), \"ConvertTFL%sOp\", #tfl_op);               \\\n+      static std::string name_str = std::string(name);                       \\\n+      return name_str;                                                       \\\n+    }                                                                        \\\n+  }\n+DECL_CONVERT_OP(Relu);\n+DECL_CONVERT_OP(Relu6);\n+DECL_CONVERT_OP(Equal);\n+DECL_CONVERT_OP(NotEqual);\n+DECL_CONVERT_OP(Greater);\n+DECL_CONVERT_OP(GreaterEqual);\n+DECL_CONVERT_OP(Add);\n+DECL_CONVERT_OP(Sub);\n+DECL_CONVERT_OP(Mul);\n+DECL_CONVERT_OP(Square);\n+DECL_CONVERT_OP(SquaredDifference);\n+DECL_CONVERT_OP(Round);\n+DECL_CONVERT_OP(Div);\n+DECL_CONVERT_OP(Maximum);\n+DECL_CONVERT_OP(Minimum);\n+DECL_CONVERT_OP(FloorMod);\n+DECL_CONVERT_OP(FloorDiv);\n+DECL_CONVERT_OP(AddN);\n+DECL_CONVERT_OP(AveragePool2D);\n+DECL_CONVERT_OP(MaxPool2D);\n+DECL_CONVERT_OP(Concatenation);\n+DECL_CONVERT_OP(Reshape);\n+DECL_CONVERT_OP(Rank);\n+DECL_CONVERT_OP(Shape);\n+DECL_CONVERT_OP(ExpandDims);\n+DECL_CONVERT_OP(Squeeze);\n+DECL_CONVERT_OP(Fill);\n+DECL_CONVERT_OP(Elu);\n+DECL_CONVERT_OP(Softmax);\n+DECL_CONVERT_OP(LogSoftmax);\n+DECL_CONVERT_OP(ReduceAny);\n+DECL_CONVERT_OP(ReduceMax);\n+DECL_CONVERT_OP(ReduceMin);\n+DECL_CONVERT_OP(Mean);\n+DECL_CONVERT_OP(ReduceProd);\n+DECL_CONVERT_OP(Sum);\n+DECL_CONVERT_OP(Conv2D);\n+DECL_CONVERT_OP(TransposeConv);\n+DECL_CONVERT_OP(DepthwiseConv2D);\n+DECL_CONVERT_OP(FullyConnected);\n+DECL_CONVERT_OP(Split);\n+DECL_CONVERT_OP(SplitV);\n+DECL_CONVERT_OP(Pack);\n+DECL_CONVERT_OP(Unpack);\n+DECL_CONVERT_OP(Transpose);\n+DECL_CONVERT_OP(Tile);\n+DECL_CONVERT_OP(Slice);\n+DECL_CONVERT_OP(StridedSlice);\n+DECL_CONVERT_OP(HardSwish);\n+DECL_CONVERT_OP(ZerosLike);\n+DECL_CONVERT_OP(Less);\n+DECL_CONVERT_OP(LessEqual);\n+DECL_CONVERT_OP(Pad);\n+DECL_CONVERT_OP(ResizeBilinear);\n+DECL_CONVERT_OP(ResizeNearestNeighbor);\n+DECL_CONVERT_OP(Select);\n+DECL_CONVERT_OP(SelectV2);\n+DECL_CONVERT_OP(SpaceToBatchNd);\n+DECL_CONVERT_OP(BatchToSpaceNd);\n+DECL_CONVERT_OP(SpaceToDepth);\n+DECL_CONVERT_OP(DepthToSpace);\n+DECL_CONVERT_OP(Logistic);\n+DECL_CONVERT_OP(Tanh);\n+DECL_CONVERT_OP(PRelu);\n+DECL_CONVERT_OP(LeakyRelu);\n+DECL_CONVERT_OP(Neg);\n+DECL_CONVERT_OP(Yield);\n+DECL_CONVERT_OP(Custom);\n+DECL_CONVERT_OP(ReverseV2);\n+DECL_CONVERT_OP(Quantize);\n+DECL_CONVERT_OP(Dequantize);\n+DECL_CONVERT_OP(QConst);\n+#undef DECL_CONVERT_OP\n+\n+LogicalResult ConvertTFLReluOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_relu_op = cast<TFL::ReluOp>(op);\n+\n+  auto input_type = tfl_relu_op.x().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_relu_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type || !output_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLReluOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_qtype = input_type.getElementType()\n+                           .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    auto op1_rescale_in = buildRescaleToInt32(rewriter, op, tfl_relu_op.x(),\n+                                              1.0f, input_qtype.getZeroPoint());\n+    auto op2_relun_op1 = rewriter.create<tosa::ReluNOp>(\n+        op->getLoc(), rescale_type, op1_rescale_in,\n+        rewriter.getI64IntegerAttr(std::numeric_limits<int32_t>::max()),\n+        rewriter.getF32FloatAttr(0.0f));\n+    auto op3_rescale_op2 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op2_relun_op1.getResult(), 1.0f,\n+        output_qtype.getZeroPoint());\n+\n+    output = op3_rescale_op2;\n+  } else {\n+    auto op1_relun_in = rewriter.create<tosa::ReluNOp>(\n+        op->getLoc(), output_type, tfl_relu_op.x(),\n+        rewriter.getI64IntegerAttr(0),\n+        rewriter.getF32FloatAttr(std::numeric_limits<float>::max()));\n+\n+    output = op1_relun_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLRelu6Op::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_relu6_op = cast<TFL::Relu6Op>(op);\n+\n+  auto input_type = tfl_relu6_op.x().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_relu6_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type || !output_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLRelu6Op: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_qtype = input_type.getElementType()\n+                           .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    int64_t rescaled_6 = (int64_t)std::round(6.0f / input_qtype.getScale()) +\n+                         input_qtype.getZeroPoint();\n+\n+    auto op1_rescale_in = buildRescaleToInt32(rewriter, op, tfl_relu6_op.x(),\n+                                              1.0f, input_qtype.getZeroPoint());\n+    auto op2_relun_op1 = rewriter.create<tosa::ReluNOp>(\n+        op->getLoc(), rescale_type, op1_rescale_in,\n+        rewriter.getI64IntegerAttr(rescaled_6), rewriter.getF32FloatAttr(0.0f));\n+    auto op3_rescale_op2 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op2_relun_op1.getResult(), 1.0f,\n+        output_qtype.getZeroPoint());\n+\n+    output = op3_rescale_op2;\n+  } else {\n+    auto op1_relun_in = rewriter.create<tosa::ReluNOp>(\n+        op->getLoc(), output_type, tfl_relu6_op.x(),\n+        rewriter.getI64IntegerAttr(0), rewriter.getF32FloatAttr(6.0f));\n+\n+    output = op1_relun_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLEqualOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_equal_op = cast<TFL::EqualOp>(op);\n+\n+  auto input_x_type = tfl_equal_op.x().getType().dyn_cast<RankedTensorType>();\n+  auto input_y_type = tfl_equal_op.y().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_equal_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_x_type || !input_y_type || !output_type) return failure();\n+\n+  bool input_x_is_qtype =\n+      input_x_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_y_is_qtype =\n+      input_y_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_x_is_qtype != output_is_qtype ||\n+      input_y_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLEqualOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_x_qtype = input_x_type.getElementType()\n+                             .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_y_qtype = input_y_type.getElementType()\n+                             .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    if (input_x_qtype.getScale() != input_y_qtype.getScale() ||\n+        input_x_qtype.getZeroPoint() != input_y_qtype.getZeroPoint()) {\n+      op->emitOpError(\n+          \"ConvertTFLEqualOp: input_x and input_y scale/zp \"\n+          \"must be the same\");\n+      return failure();\n+    }\n+\n+    auto op1_rescale_x = buildRescaleToInt32(\n+        rewriter, op, tfl_equal_op.x(), 1.0f, input_x_qtype.getZeroPoint());\n+    auto op2_rescale_y = buildRescaleToInt32(\n+        rewriter, op, tfl_equal_op.y(), 1.0f, input_y_qtype.getZeroPoint());\n+    auto op3_equal_op1_op2 = rewriter.create<tosa::EqualOp>(\n+        op->getLoc(), output_type, op1_rescale_x, op2_rescale_y);\n+\n+    output = op3_equal_op1_op2.getResult();\n+  } else {\n+    auto op1_equal_in = rewriter.create<tosa::EqualOp>(\n+        op->getLoc(), output_type, tfl_equal_op.x(), tfl_equal_op.y());\n+\n+    output = op1_equal_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLNotEqualOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_not_equal_op = cast<TFL::NotEqualOp>(op);\n+\n+  auto input_lhs_type =\n+      tfl_not_equal_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type =\n+      tfl_not_equal_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_not_equal_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLNotEqualOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    if (input_lhs_qtype.getScale() != input_rhs_qtype.getScale() ||\n+        input_lhs_qtype.getZeroPoint() != input_rhs_qtype.getZeroPoint()) {\n+      op->emitOpError(\n+          \"ConvertTFLNotEqualOp: input_x and input_y scale/zp \"\n+          \"must be the same\");\n+      return failure();\n+    }\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_not_equal_op.lhs(), 1.0f,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_not_equal_op.rhs(), 1.0f,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_equal_op1_op2 = rewriter.create<tosa::EqualOp>(\n+        op->getLoc(), output_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_not_op3 = rewriter.create<tosa::LogicalNotOp>(\n+        op->getLoc(), output_type, op3_equal_op1_op2.getResult());\n+\n+    output = op4_not_op3.getResult();\n+  } else {\n+    auto op1_equal_in = rewriter.create<tosa::EqualOp>(\n+        op->getLoc(), output_type, tfl_not_equal_op.lhs(),\n+        tfl_not_equal_op.rhs());\n+    auto op2_not_op1 = rewriter.create<tosa::LogicalNotOp>(\n+        op->getLoc(), output_type, op1_equal_in.getResult());\n+\n+    output = op2_not_op1.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLGreaterOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_greater_op = cast<TFL::GreaterOp>(op);\n+\n+  auto input_lhs_type =\n+      tfl_greater_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type =\n+      tfl_greater_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_greater_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLGreaterOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    if (input_lhs_qtype.getScale() != input_rhs_qtype.getScale() ||\n+        input_lhs_qtype.getZeroPoint() != input_rhs_qtype.getZeroPoint()) {\n+      op->emitOpError(\n+          \"ConvertTFLGreaterOp: input_x and input_y scale/zp \"\n+          \"must be the same\");\n+      return failure();\n+    }\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_greater_op.lhs(), 1.0f,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_greater_op.rhs(), 1.0f,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_greater_op1_op2 = rewriter.create<tosa::GreaterOp>(\n+        op->getLoc(), output_type, op1_rescale_lhs, op2_rescale_rhs);\n+\n+    output = op3_greater_op1_op2.getResult();\n+  } else {\n+    auto op1_greater_in = rewriter.create<tosa::GreaterOp>(\n+        op->getLoc(), output_type, tfl_greater_op.lhs(), tfl_greater_op.rhs());\n+\n+    output = op1_greater_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLGreaterEqualOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_greater_equal_op = cast<TFL::GreaterEqualOp>(op);\n+\n+  auto input_lhs_type =\n+      tfl_greater_equal_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type =\n+      tfl_greater_equal_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_greater_equal_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLGreaterEqualOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    if (input_lhs_qtype.getScale() != input_rhs_qtype.getScale() ||\n+        input_lhs_qtype.getZeroPoint() != input_rhs_qtype.getZeroPoint()) {\n+      op->emitOpError(\n+          \"ConvertTFLGreaterEqualOp: input_x and input_y scale/zp \"\n+          \"must be the same\");\n+      return failure();\n+    }\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_greater_equal_op.lhs(), 1.0f,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_greater_equal_op.rhs(), 1.0f,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_greater_equal_op1_op2 = rewriter.create<tosa::GreaterEqualOp>(\n+        op->getLoc(), output_type, op1_rescale_lhs, op2_rescale_rhs);\n+\n+    output = op3_greater_equal_op1_op2.getResult();\n+  } else {\n+    auto op1_greater_equal_in = rewriter.create<tosa::GreaterEqualOp>(\n+        op->getLoc(), output_type, tfl_greater_equal_op.lhs(),\n+        tfl_greater_equal_op.rhs());\n+\n+    output = op1_greater_equal_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLAddOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_add_op = cast<TFL::AddOp>(op);\n+\n+  auto input_lhs_type = tfl_add_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type = tfl_add_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_add_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLAddOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    double in_lhs_scale = input_lhs_qtype.getScale();\n+    double in_rhs_scale = input_rhs_qtype.getScale();\n+    double output_scale = output_qtype.getScale();\n+    double max_scale_2x = 2.0 * std::max(in_lhs_scale, in_rhs_scale);\n+\n+    int32_t input_shift = 20;\n+\n+    double lhs_rescale_scale =\n+        double(1 << input_shift) * in_lhs_scale / max_scale_2x;\n+    double rhs_rescale_scale =\n+        double(1 << input_shift) * in_rhs_scale / max_scale_2x;\n+    double output_rescale_scale =\n+        max_scale_2x / (output_scale * double(1 << input_shift));\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_add_op.lhs(), lhs_rescale_scale,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_add_op.rhs(), rhs_rescale_scale,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_add_op1_op2 = rewriter.create<tosa::AddOp>(\n+        op->getLoc(), rescale_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_rescale_op3 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op3_add_op1_op2.getResult(),\n+        output_rescale_scale, output_qtype.getZeroPoint());\n+    output = op4_rescale_op3;\n+  } else {\n+    auto op1_add_in = rewriter.create<tosa::AddOp>(\n+        op->getLoc(), output_type, tfl_add_op.lhs(), tfl_add_op.rhs());\n+\n+    output = op1_add_in.getResult();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_add_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op =\n+        convertFusedActivation(rewriter, op, output, fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLSubOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_sub_op = cast<TFL::SubOp>(op);\n+\n+  auto input_lhs_type = tfl_sub_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type = tfl_sub_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_sub_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLSubOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    double in_lhs_scale = input_lhs_qtype.getScale();\n+    double in_rhs_scale = input_rhs_qtype.getScale();\n+    double output_scale = output_qtype.getScale();\n+    double max_scale_2x = 2.0 * std::max(in_lhs_scale, in_rhs_scale);\n+\n+    int32_t input_shift = 20;\n+\n+    double lhs_rescale_scale =\n+        double(1 << input_shift) * in_lhs_scale / max_scale_2x;\n+    double rhs_rescale_scale =\n+        double(1 << input_shift) * in_rhs_scale / max_scale_2x;\n+    double output_rescale_scale =\n+        max_scale_2x / (output_scale * double(1 << input_shift));\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_sub_op.lhs(), lhs_rescale_scale,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_sub_op.rhs(), rhs_rescale_scale,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_sub_op1_op2 = rewriter.create<tosa::SubOp>(\n+        op->getLoc(), rescale_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_rescale_op3 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op3_sub_op1_op2.getResult(),\n+        output_rescale_scale, output_qtype.getZeroPoint());\n+    output = op4_rescale_op3;\n+  } else {\n+    auto op1_sub_in = rewriter.create<tosa::SubOp>(\n+        op->getLoc(), output_type, tfl_sub_op.lhs(), tfl_sub_op.rhs());\n+\n+    output = op1_sub_in.getResult();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_sub_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op =\n+        convertFusedActivation(rewriter, op, output, fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLMulOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_mul_op = cast<TFL::MulOp>(op);\n+\n+  auto lowered_op = convertMultiplyOp(rewriter, op, tfl_mul_op.getResult(),\n+                                      tfl_mul_op.lhs(), tfl_mul_op.rhs());\n+\n+  if (!lowered_op) {\n+    return failure();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_mul_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, lowered_op->getResult(0), fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {lowered_op->getResult(0)});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLSquareOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_square_op = cast<TFL::SquareOp>(op);\n+\n+  auto lowered_op = convertMultiplyOp(rewriter, op, tfl_square_op.getResult(),\n+                                      tfl_square_op.x(), tfl_square_op.x());\n+\n+  if (!lowered_op) {\n+    return failure();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_square_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, lowered_op->getResult(0), fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {lowered_op->getResult(0)});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLSquaredDifferenceOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_squared_op = cast<TFL::SquaredDifferenceOp>(op);\n+\n+  auto lowered_op =\n+      convertSquaredDifferenceOp(rewriter, op, tfl_squared_op.getResult(),\n+                                 tfl_squared_op.lhs(), tfl_squared_op.rhs());\n+\n+  TOSA_REPLACE_LOWERED_OP(rewriter, op, lowered_op);\n+}\n+\n+LogicalResult ConvertTFLRoundOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_round_op = cast<TFL::RoundOp>(op);\n+\n+  auto input_type = tfl_round_op.x().getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) {\n+    op->emitOpError(\"Round: input not ranked tensor type\");\n+    return failure();\n+  }\n+\n+  if (input_type.getElementType().isa<FloatType>()) {\n+    auto lowered_op = convertRoundOp(rewriter, op, tfl_round_op.getResult(),\n+                                     tfl_round_op.x());\n+\n+    TOSA_REPLACE_LOWERED_OP(rewriter, op, lowered_op);\n+\n+  } else {\n+    // Round on int is nonsensical. Instead, replace uses of result with the\n+    // input.\n+    tfl_round_op.replaceAllUsesWith(tfl_round_op.x());\n+  }\n+}\n+\n+LogicalResult ConvertTFLDivOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_div_op = cast<TFL::DivOp>(op);\n+\n+  auto output_type =\n+      tfl_div_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!output_type) return failure();\n+\n+  auto fused_activation_fn =\n+      tfl_div_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  auto reciprocal_op = rewriter.create<tosa::ReciprocalOp>(\n+      op->getLoc(), output_type, tfl_div_op.rhs());\n+  auto mul_op =\n+      rewriter.create<tosa::MulOp>(op->getLoc(), output_type, tfl_div_op.lhs(),\n+                                   reciprocal_op.getResult(), 0);\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, mul_op.getResult(), fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {mul_op.getResult()});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLMaximumOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_max_op = cast<TFL::MaximumOp>(op);\n+\n+  auto input_lhs_type = tfl_max_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type = tfl_max_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_max_op.getResult().getType().dyn_cast<RankedTensorType>();\n+\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLMaximumOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_max_op.lhs(), 1.0f, 0);\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_max_op.rhs(), 1.0f, 0);\n+    auto op3_max_op1_op2 = rewriter.create<tosa::MaximumOp>(\n+        op->getLoc(), rescale_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_rescale_op3 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op3_max_op1_op2.getResult(), 1.0f, 0);\n+\n+    output = op4_rescale_op3;\n+  } else {\n+    auto op1_max_in = rewriter.create<tosa::MaximumOp>(\n+        op->getLoc(), output_type, tfl_max_op.lhs(), tfl_max_op.rhs());\n+\n+    output = op1_max_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLMinimumOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_min_op = cast<TFL::MinimumOp>(op);\n+\n+  auto input_lhs_type = tfl_min_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type = tfl_min_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_min_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLMinimumOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_min_op.lhs(), 1.0f, 0);\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_min_op.rhs(), 1.0f, 0);\n+    auto op3_min_op1_op2 = rewriter.create<tosa::MinimumOp>(\n+        op->getLoc(), rescale_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_rescale_op3 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op3_min_op1_op2.getResult(), 1.0f, 0);\n+\n+    output = op4_rescale_op3;\n+  } else {\n+    auto op1_min_in = rewriter.create<tosa::MinimumOp>(\n+        op->getLoc(), output_type, tfl_min_op.lhs(), tfl_min_op.rhs());\n+\n+    output = op1_min_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLFloorDivOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_floordiv_op = cast<TFL::FloorDivOp>(op);\n+\n+  auto lowered_op =\n+      convertFloorDivOp(rewriter, op, tfl_floordiv_op.getResult(),\n+                        tfl_floordiv_op.lhs(), tfl_floordiv_op.rhs());\n+\n+  TOSA_REPLACE_LOWERED_OP(rewriter, op, lowered_op);\n+}\n+\n+LogicalResult ConvertTFLFloorModOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_floormod_op = cast<TFL::FloorModOp>(op);\n+\n+  auto lowered_op =\n+      convertFloorModOp(rewriter, op, tfl_floormod_op.getResult(),\n+                        tfl_floormod_op.lhs(), tfl_floormod_op.rhs());\n+\n+  TOSA_REPLACE_LOWERED_OP(rewriter, op, lowered_op);\n+}\n+\n+LogicalResult ConvertTFLAddNOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_addn_op = cast<TFL::AddNOp>(op);\n+\n+  auto output_type =\n+      tfl_addn_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!output_type) return failure();\n+\n+  SmallVector<Value, 4> inputs(tfl_addn_op.inputs());\n+\n+  assert(inputs.size() >= 2);\n+\n+  auto newOp = rewriter.create<tosa::AddOp>(op->getLoc(), output_type,\n+                                            inputs[0], inputs[1]);\n+  for (int i = 2; i < inputs.size(); i++) {\n+    newOp = rewriter.create<tosa::AddOp>(op->getLoc(), output_type, inputs[i],\n+                                         newOp.getResult());\n+  }\n+\n+  rewriter.replaceOp(op, {newOp.getResult()});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLAveragePool2DOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_avgpool_op = cast<TFL::AveragePool2DOp>(op);\n+\n+  auto input_type =\n+      tfl_avgpool_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_avgpool_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!output_type) return failure();\n+\n+  auto tmpAttr = tfl_avgpool_op.getAttrOfType<StringAttr>(\"data_format\");\n+  if (tmpAttr && tmpAttr.getValue().str() != \"NHWC\") return failure();\n+\n+  // Kernels and strides are dimensionally ordered\n+  SmallVector<int64_t, 4> i64array({1, 1, 1, 1});\n+  ArrayAttr kernel_size;\n+  ArrayAttr stride;\n+  ArrayAttr pad;\n+  {\n+    int64_t kernel_h = tfl_avgpool_op.filter_height();\n+    int64_t kernel_w = tfl_avgpool_op.filter_width();\n+    kernel_size = rewriter.getI64ArrayAttr({kernel_h, kernel_w});\n+    // i64array is formatted as NHWC now\n+    i64array[1] = kernel_h;\n+    i64array[2] = kernel_w;\n+  }\n+  {\n+    int64_t stride_h = tfl_avgpool_op.stride_h();\n+    int64_t stride_w = tfl_avgpool_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_avgpool_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    // Pooling has no non-unit dilation\n+    ArrayAttr dilation = rewriter.getI64ArrayAttr({1, 1});\n+\n+    auto filter_type = RankedTensorType::get(\n+        llvm::makeArrayRef<int64_t>(i64array), rewriter.getIntegerType(64));\n+\n+    // TFLite doesn't support explicit padding\n+    if (!getPaddingValuesFromPadType(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, stride, dilation, rewriter, pad))\n+      return failure();\n+  }\n+\n+  rewriter.replaceOpWithNewOp<tosa::AvgPool2dOp>(\n+      op, output_type, tfl_avgpool_op.input(), kernel_size, stride, pad);\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLMaxPool2DOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_maxpool_op = cast<TFL::MaxPool2DOp>(op);\n+\n+  auto input_type =\n+      tfl_maxpool_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_maxpool_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!output_type) return failure();\n+\n+  auto tmpAttr = tfl_maxpool_op.getAttrOfType<StringAttr>(\"data_format\");\n+  if (tmpAttr && tmpAttr.getValue().str() != \"NHWC\") return failure();\n+\n+  // Kernels and strides are dimensionally ordered\n+  SmallVector<int64_t, 4> i64array({1, 1, 1, 1});\n+  ArrayAttr kernel_size;\n+  ArrayAttr stride;\n+  ArrayAttr pad;\n+  {\n+    int64_t kernel_h = tfl_maxpool_op.filter_height();\n+    int64_t kernel_w = tfl_maxpool_op.filter_width();\n+    kernel_size = rewriter.getI64ArrayAttr({kernel_h, kernel_w});\n+    // i64array is formatted as NHWC now\n+    i64array[1] = kernel_h;\n+    i64array[2] = kernel_w;\n+  }\n+  {\n+    int64_t stride_h = tfl_maxpool_op.stride_h();\n+    int64_t stride_w = tfl_maxpool_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_maxpool_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    // Pooling has no non-unit dilation\n+    ArrayAttr dilation = rewriter.getI64ArrayAttr({1, 1});\n+\n+    auto filter_type = RankedTensorType::get(\n+        llvm::makeArrayRef<int64_t>(i64array), rewriter.getIntegerType(64));\n+\n+    // TFLite doesn't support explicit padding\n+    if (!getPaddingValuesFromPadType(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, stride, dilation, rewriter, pad))\n+      return failure();\n+  }\n+\n+  rewriter.replaceOpWithNewOp<tosa::MaxPool2dOp>(\n+      op, output_type, tfl_maxpool_op.input(), kernel_size, stride, pad);\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLConv2DOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_conv2d_op = cast<TFL::Conv2DOp>(op);\n+\n+  auto input_type =\n+      tfl_conv2d_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto filter_type =\n+      tfl_conv2d_op.filter().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_conv2d_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type) return failure();\n+  if (!output_type) return failure();\n+  if (!filter_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool filter_is_qtype =\n+      filter_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::QuantizedType>();\n+\n+  if ((input_is_qtype != filter_is_qtype) ||\n+      (input_is_qtype != output_is_qtype)) {\n+    op->emitOpError(\n+        \"ConvertTFLConv2DOp: input/filter/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  auto tmpAttr = tfl_conv2d_op.getAttrOfType<StringAttr>(\"data_format\");\n+  if (tmpAttr && tmpAttr.getValue().str() != \"NHWC\") return failure();\n+\n+  ArrayAttr pad;\n+  ArrayAttr stride;\n+  ArrayAttr dilation;\n+  StringAttr activation_function;\n+  {\n+    int64_t stride_h = tfl_conv2d_op.stride_h();\n+    int64_t stride_w = tfl_conv2d_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+  {\n+    int64_t dilation_h = tfl_conv2d_op.dilation_h_factor();\n+    int64_t dilation_w = tfl_conv2d_op.dilation_w_factor();\n+    dilation = rewriter.getI64ArrayAttr({dilation_h, dilation_w});\n+  }\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_conv2d_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    // TFLite doesn't support explicit padding\n+    if (!getPaddingValuesFromPadType(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, stride, dilation, rewriter, pad))\n+      return failure();\n+  }\n+\n+  Value unquantized_bias =\n+      getUnquantizedBias(rewriter, op, tfl_conv2d_op.bias());\n+\n+  auto a1_conv2d_op = rewriter.create<tosa::Conv2DOp>(\n+      op->getLoc(), output_type, tfl_conv2d_op.input(), tfl_conv2d_op.filter(),\n+      unquantized_bias, pad, stride, dilation);\n+\n+  Value conv2d_output;\n+  if (input_is_qtype) {\n+    conv2d_output =\n+        buildRescaleOpConvOutput(rewriter, op, a1_conv2d_op.getResult(),\n+                                 input_type, filter_type, output_type);\n+  } else {\n+    conv2d_output = a1_conv2d_op.getResult();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_conv2d_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, conv2d_output, fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {conv2d_output});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLTransposeConvOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_conv_op = cast<TFL::TransposeConvOp>(op);\n+\n+  auto input_type = tfl_conv_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto filter_type =\n+      tfl_conv_op.weights().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_conv_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type) return failure();\n+  if (!output_type) return failure();\n+  if (!filter_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool filter_is_qtype =\n+      filter_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::QuantizedType>();\n+\n+  if ((input_is_qtype != filter_is_qtype) ||\n+      (input_is_qtype != output_is_qtype)) {\n+    op->emitOpError(\n+        \"ConvertTFLConv2DOp: input/filter/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  ArrayAttr stride;\n+  ArrayAttr dilation;\n+  ArrayAttr outpad;\n+  ArrayAttr output_shape;\n+  StringAttr activation_function;\n+  {\n+    int64_t stride_h = tfl_conv_op.stride_h();\n+    int64_t stride_w = tfl_conv_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+\n+  // tfl.transpose_conv doesn't support dilations\n+  dilation = rewriter.getI64ArrayAttr({1, 1});\n+\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_conv_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    if (!getTransposeConv2dPaddingValues(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, output_type, stride, dilation, rewriter,\n+            outpad))\n+      return failure();\n+  }\n+  {\n+    ElementsAttr output_shape_elems;\n+    // Match from input_size tensor first\n+    if (matchPattern(tfl_conv_op.output_shape(),\n+                     m_Constant(&output_shape_elems))) {\n+      llvm::SmallVector<int64_t, 4> shape_vec;\n+      for (int i = 0; i < output_shape_elems.getNumElements(); i++)\n+        shape_vec.push_back(\n+            output_shape_elems.getValue<IntegerAttr>(i).getInt());\n+      output_shape = rewriter.getI64ArrayAttr(shape_vec);\n+    }\n+    // Use output tensor's shape otherwise\n+    else {\n+      output_shape = rewriter.getI64ArrayAttr(output_type.getShape());\n+    }\n+  }\n+\n+  Value zero_bias;\n+  if (input_is_qtype) {\n+    uint32_t input_bits = input_type.getElementType()\n+                              .dyn_cast<mlir::quant::QuantizedType>()\n+                              .getStorageTypeIntegralWidth();\n+    uint32_t weight_bits = filter_type.getElementType()\n+                               .dyn_cast<mlir::quant::QuantizedType>()\n+                               .getStorageTypeIntegralWidth();\n+\n+    if (input_bits == 16 && weight_bits == 8) {\n+      SmallVector<int64_t, 8> zero_bias_vec(output_type.getShape()[3], 0);\n+      zero_bias = get1DConstTensorInt48(rewriter, op, zero_bias_vec);\n+    } else {\n+      SmallVector<int32_t, 8> zero_bias_vec(output_type.getShape()[3], 0);\n+      zero_bias =\n+          get1DConstTensor<tosa::ConstOp, int32_t>(rewriter, op, zero_bias_vec);\n+    }\n+  } else {\n+    SmallVector<float, 8> zero_bias_vec(output_type.getShape()[3], 0.0f);\n+    zero_bias =\n+        get1DConstTensor<tosa::ConstOp, float>(rewriter, op, zero_bias_vec);\n+  }\n+\n+  auto a1_conv2d_op = rewriter.create<tosa::TransposeConv2DOp>(\n+      op->getLoc(), output_type, tfl_conv_op.input(), tfl_conv_op.weights(),\n+      zero_bias, outpad, stride, dilation, output_shape);\n+\n+  Value conv2d_output;\n+  if (input_is_qtype) {\n+    conv2d_output =\n+        buildRescaleOpConvOutput(rewriter, op, a1_conv2d_op.getResult(),\n+                                 input_type, filter_type, output_type);\n+  } else {\n+    conv2d_output = a1_conv2d_op.getResult();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_conv_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, conv2d_output, fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {conv2d_output});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLDepthwiseConv2DOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_conv2d_op = cast<TFL::DepthwiseConv2DOp>(op);\n+\n+  auto input_type =\n+      tfl_conv2d_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto filter_type =\n+      tfl_conv2d_op.filter().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_conv2d_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type) return failure();\n+  if (!output_type) return failure();\n+  if (!filter_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool filter_is_qtype =\n+      filter_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::QuantizedType>();\n+\n+  if ((input_is_qtype != filter_is_qtype) ||\n+      (input_is_qtype != output_is_qtype)) {\n+    op->emitOpError(\n+        \"ConvertTFLConv2DOp: input/filter/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  auto filter_shape = filter_type.getShape();\n+  // Operator depthwiseConv2D\n+  // TFLite orders the depthwiseConv2D filter in IHWO, while TOSA orders\n+  // filter in HWIO\n+  //\n+  // The lowering reorders the filter.\n+  //\n+  // a1_transpose = tosa.transpose(filter, {1, 2, 3, 0})   // HWIO\n+  // a2_reshape = tosa.reshape(filter, H, W, depth_multiplier, I /\n+  // depth_multiplier)\n+  // a3_transpose_conv2d = tosa.transpose_conv2d(input, a2_reshape, padding,\n+  // stride, dilation)\n+\n+  auto tmpAttr = tfl_conv2d_op.getAttrOfType<StringAttr>(\"data_format\");\n+  if (tmpAttr && tmpAttr.getValue().str() != \"NHWC\") return failure();\n+\n+  ArrayAttr pad;\n+  ArrayAttr stride;\n+  ArrayAttr dilation;\n+  StringAttr activation_function;\n+\n+  auto depth_multiplier =\n+      tfl_conv2d_op.getAttrOfType<IntegerAttr>(\"depth_multiplier\");\n+\n+  {\n+    int64_t stride_h = tfl_conv2d_op.stride_h();\n+    int64_t stride_w = tfl_conv2d_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+  {\n+    int64_t dilation_h = tfl_conv2d_op.dilation_h_factor();\n+    int64_t dilation_w = tfl_conv2d_op.dilation_w_factor();\n+    dilation = rewriter.getI64ArrayAttr({dilation_h, dilation_w});\n+  }\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_conv2d_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    if (!getPaddingValuesFromPadType(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, stride, dilation, rewriter, pad))\n+      return failure();\n+  }\n+\n+  std::vector<int64_t> a1_transpose_dims;",
        "comment_created_at": "2020-11-17T23:20:48+00:00",
        "comment_author": "stellaraccident",
        "comment_body": "Here and elsewhere that you are using std::vector for dims: change to `llvm::SmallVector<int64_t, 4>`. For this kind of thing, you can also use `auto a1_transpose_dimes = llvm::to_vector<4>(a, b, c, d)`. There are a bunch of these it would be good to cleanup (not going to comment more).",
        "pr_file_module": null
      },
      {
        "comment_id": "526311247",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 44851,
        "pr_file": "tensorflow/compiler/mlir/tosa/transforms/legalize_tfl.cc",
        "discussion_id": "525590618",
        "commented_code": "@@ -0,0 +1,2918 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// Legalize TensorFlow Lite to TOSA\n+\n+#include <climits>\n+#include <cstddef>\n+#include <cstdint>\n+#include <fstream>\n+#include <iterator>\n+#include <numeric>\n+#include <unordered_set>\n+\n+#include \"llvm/ADT/APInt.h\"\n+#include \"llvm/ADT/ArrayRef.h\"\n+#include \"llvm/ADT/Optional.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/StringSwitch.h\"\n+#include \"mlir/Dialect/Quant/FakeQuantSupport.h\"\n+#include \"mlir/Dialect/Quant/QuantTypes.h\"\n+#include \"mlir/Dialect/Quant/UniformSupport.h\"\n+#include \"mlir/Dialect/StandardOps/IR/Ops.h\"\n+#include \"mlir/Dialect/Tosa/IR/TosaOps.h\"\n+#include \"mlir/Dialect/Tosa/Utils/QuantUtils.h\"\n+#include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Diagnostics.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/Module.h\"\n+#include \"mlir/IR/Operation.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/StandardTypes.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n+#include \"mlir/IR/Types.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"tensorflow/compiler/mlir/lite/ir/tfl_ops.h\"\n+#include \"tensorflow/compiler/mlir/tosa/transforms/legalize_common.h\"\n+#include \"tensorflow/compiler/mlir/tosa/transforms/legalize_utils.h\"\n+#include \"tensorflow/compiler/mlir/tosa/transforms/passes.h\"\n+\n+#define PASS_NAME \"tosa-legalize-tfl\"\n+#define DEBUG_TYPE PASS_NAME\n+#define HARDSWISH_EXPLICIT_RESCALING false\n+\n+// Conditionally avoid converting some TFLite ops to TOSA.\n+// By default, all conversions will be invoked.\n+//\n+// The denylist file lists patterns which are not legalized from TFLite to TOSA.\n+llvm::cl::opt<std::string> tfl_tosa_denylist(\n+    \"tfl-tosa-denylist\",\n+    llvm::cl::desc(\"<a list of patterns not legalized from TFLite to TOSA>\"),\n+    llvm::cl::init(\"transforms/tfl_tosa_denylist.txt\"),\n+    llvm::cl::value_desc(\"pattern name\"));\n+\n+namespace mlir {\n+\n+namespace tosa {\n+\n+namespace {\n+// Performs lowering to TOSA dialect.\n+class LegalizeTFL : public PassWrapper<LegalizeTFL, FunctionPass> {\n+ public:\n+  explicit LegalizeTFL() {}\n+  void runOnFunction() override;\n+};\n+\n+#include \"tensorflow/compiler/mlir/tosa/transforms/tfl_legalize_patterns.inc\"\n+\n+#define DECL_CONVERT_OP(tfl_op)                                              \\\n+  struct ConvertTFL##tfl_op##Op : public RewritePattern {                    \\\n+    explicit ConvertTFL##tfl_op##Op(MLIRContext* context)                    \\\n+        : RewritePattern(TFL::tfl_op##Op::getOperationName(), 1, context) {} \\\n+    LogicalResult matchAndRewrite(Operation* op,                             \\\n+                                  PatternRewriter& rewriter) const override; \\\n+    static const std::string& getName() {                                    \\\n+      static char name[100];                                                 \\\n+      snprintf(name, sizeof(name), \"ConvertTFL%sOp\", #tfl_op);               \\\n+      static std::string name_str = std::string(name);                       \\\n+      return name_str;                                                       \\\n+    }                                                                        \\\n+  }\n+DECL_CONVERT_OP(Relu);\n+DECL_CONVERT_OP(Relu6);\n+DECL_CONVERT_OP(Equal);\n+DECL_CONVERT_OP(NotEqual);\n+DECL_CONVERT_OP(Greater);\n+DECL_CONVERT_OP(GreaterEqual);\n+DECL_CONVERT_OP(Add);\n+DECL_CONVERT_OP(Sub);\n+DECL_CONVERT_OP(Mul);\n+DECL_CONVERT_OP(Square);\n+DECL_CONVERT_OP(SquaredDifference);\n+DECL_CONVERT_OP(Round);\n+DECL_CONVERT_OP(Div);\n+DECL_CONVERT_OP(Maximum);\n+DECL_CONVERT_OP(Minimum);\n+DECL_CONVERT_OP(FloorMod);\n+DECL_CONVERT_OP(FloorDiv);\n+DECL_CONVERT_OP(AddN);\n+DECL_CONVERT_OP(AveragePool2D);\n+DECL_CONVERT_OP(MaxPool2D);\n+DECL_CONVERT_OP(Concatenation);\n+DECL_CONVERT_OP(Reshape);\n+DECL_CONVERT_OP(Rank);\n+DECL_CONVERT_OP(Shape);\n+DECL_CONVERT_OP(ExpandDims);\n+DECL_CONVERT_OP(Squeeze);\n+DECL_CONVERT_OP(Fill);\n+DECL_CONVERT_OP(Elu);\n+DECL_CONVERT_OP(Softmax);\n+DECL_CONVERT_OP(LogSoftmax);\n+DECL_CONVERT_OP(ReduceAny);\n+DECL_CONVERT_OP(ReduceMax);\n+DECL_CONVERT_OP(ReduceMin);\n+DECL_CONVERT_OP(Mean);\n+DECL_CONVERT_OP(ReduceProd);\n+DECL_CONVERT_OP(Sum);\n+DECL_CONVERT_OP(Conv2D);\n+DECL_CONVERT_OP(TransposeConv);\n+DECL_CONVERT_OP(DepthwiseConv2D);\n+DECL_CONVERT_OP(FullyConnected);\n+DECL_CONVERT_OP(Split);\n+DECL_CONVERT_OP(SplitV);\n+DECL_CONVERT_OP(Pack);\n+DECL_CONVERT_OP(Unpack);\n+DECL_CONVERT_OP(Transpose);\n+DECL_CONVERT_OP(Tile);\n+DECL_CONVERT_OP(Slice);\n+DECL_CONVERT_OP(StridedSlice);\n+DECL_CONVERT_OP(HardSwish);\n+DECL_CONVERT_OP(ZerosLike);\n+DECL_CONVERT_OP(Less);\n+DECL_CONVERT_OP(LessEqual);\n+DECL_CONVERT_OP(Pad);\n+DECL_CONVERT_OP(ResizeBilinear);\n+DECL_CONVERT_OP(ResizeNearestNeighbor);\n+DECL_CONVERT_OP(Select);\n+DECL_CONVERT_OP(SelectV2);\n+DECL_CONVERT_OP(SpaceToBatchNd);\n+DECL_CONVERT_OP(BatchToSpaceNd);\n+DECL_CONVERT_OP(SpaceToDepth);\n+DECL_CONVERT_OP(DepthToSpace);\n+DECL_CONVERT_OP(Logistic);\n+DECL_CONVERT_OP(Tanh);\n+DECL_CONVERT_OP(PRelu);\n+DECL_CONVERT_OP(LeakyRelu);\n+DECL_CONVERT_OP(Neg);\n+DECL_CONVERT_OP(Yield);\n+DECL_CONVERT_OP(Custom);\n+DECL_CONVERT_OP(ReverseV2);\n+DECL_CONVERT_OP(Quantize);\n+DECL_CONVERT_OP(Dequantize);\n+DECL_CONVERT_OP(QConst);\n+#undef DECL_CONVERT_OP\n+\n+LogicalResult ConvertTFLReluOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_relu_op = cast<TFL::ReluOp>(op);\n+\n+  auto input_type = tfl_relu_op.x().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_relu_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type || !output_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLReluOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_qtype = input_type.getElementType()\n+                           .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    auto op1_rescale_in = buildRescaleToInt32(rewriter, op, tfl_relu_op.x(),\n+                                              1.0f, input_qtype.getZeroPoint());\n+    auto op2_relun_op1 = rewriter.create<tosa::ReluNOp>(\n+        op->getLoc(), rescale_type, op1_rescale_in,\n+        rewriter.getI64IntegerAttr(std::numeric_limits<int32_t>::max()),\n+        rewriter.getF32FloatAttr(0.0f));\n+    auto op3_rescale_op2 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op2_relun_op1.getResult(), 1.0f,\n+        output_qtype.getZeroPoint());\n+\n+    output = op3_rescale_op2;\n+  } else {\n+    auto op1_relun_in = rewriter.create<tosa::ReluNOp>(\n+        op->getLoc(), output_type, tfl_relu_op.x(),\n+        rewriter.getI64IntegerAttr(0),\n+        rewriter.getF32FloatAttr(std::numeric_limits<float>::max()));\n+\n+    output = op1_relun_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLRelu6Op::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_relu6_op = cast<TFL::Relu6Op>(op);\n+\n+  auto input_type = tfl_relu6_op.x().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_relu6_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type || !output_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLRelu6Op: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_qtype = input_type.getElementType()\n+                           .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    int64_t rescaled_6 = (int64_t)std::round(6.0f / input_qtype.getScale()) +\n+                         input_qtype.getZeroPoint();\n+\n+    auto op1_rescale_in = buildRescaleToInt32(rewriter, op, tfl_relu6_op.x(),\n+                                              1.0f, input_qtype.getZeroPoint());\n+    auto op2_relun_op1 = rewriter.create<tosa::ReluNOp>(\n+        op->getLoc(), rescale_type, op1_rescale_in,\n+        rewriter.getI64IntegerAttr(rescaled_6), rewriter.getF32FloatAttr(0.0f));\n+    auto op3_rescale_op2 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op2_relun_op1.getResult(), 1.0f,\n+        output_qtype.getZeroPoint());\n+\n+    output = op3_rescale_op2;\n+  } else {\n+    auto op1_relun_in = rewriter.create<tosa::ReluNOp>(\n+        op->getLoc(), output_type, tfl_relu6_op.x(),\n+        rewriter.getI64IntegerAttr(0), rewriter.getF32FloatAttr(6.0f));\n+\n+    output = op1_relun_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLEqualOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_equal_op = cast<TFL::EqualOp>(op);\n+\n+  auto input_x_type = tfl_equal_op.x().getType().dyn_cast<RankedTensorType>();\n+  auto input_y_type = tfl_equal_op.y().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_equal_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_x_type || !input_y_type || !output_type) return failure();\n+\n+  bool input_x_is_qtype =\n+      input_x_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_y_is_qtype =\n+      input_y_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_x_is_qtype != output_is_qtype ||\n+      input_y_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLEqualOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_x_qtype = input_x_type.getElementType()\n+                             .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_y_qtype = input_y_type.getElementType()\n+                             .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    if (input_x_qtype.getScale() != input_y_qtype.getScale() ||\n+        input_x_qtype.getZeroPoint() != input_y_qtype.getZeroPoint()) {\n+      op->emitOpError(\n+          \"ConvertTFLEqualOp: input_x and input_y scale/zp \"\n+          \"must be the same\");\n+      return failure();\n+    }\n+\n+    auto op1_rescale_x = buildRescaleToInt32(\n+        rewriter, op, tfl_equal_op.x(), 1.0f, input_x_qtype.getZeroPoint());\n+    auto op2_rescale_y = buildRescaleToInt32(\n+        rewriter, op, tfl_equal_op.y(), 1.0f, input_y_qtype.getZeroPoint());\n+    auto op3_equal_op1_op2 = rewriter.create<tosa::EqualOp>(\n+        op->getLoc(), output_type, op1_rescale_x, op2_rescale_y);\n+\n+    output = op3_equal_op1_op2.getResult();\n+  } else {\n+    auto op1_equal_in = rewriter.create<tosa::EqualOp>(\n+        op->getLoc(), output_type, tfl_equal_op.x(), tfl_equal_op.y());\n+\n+    output = op1_equal_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLNotEqualOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_not_equal_op = cast<TFL::NotEqualOp>(op);\n+\n+  auto input_lhs_type =\n+      tfl_not_equal_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type =\n+      tfl_not_equal_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_not_equal_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLNotEqualOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    if (input_lhs_qtype.getScale() != input_rhs_qtype.getScale() ||\n+        input_lhs_qtype.getZeroPoint() != input_rhs_qtype.getZeroPoint()) {\n+      op->emitOpError(\n+          \"ConvertTFLNotEqualOp: input_x and input_y scale/zp \"\n+          \"must be the same\");\n+      return failure();\n+    }\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_not_equal_op.lhs(), 1.0f,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_not_equal_op.rhs(), 1.0f,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_equal_op1_op2 = rewriter.create<tosa::EqualOp>(\n+        op->getLoc(), output_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_not_op3 = rewriter.create<tosa::LogicalNotOp>(\n+        op->getLoc(), output_type, op3_equal_op1_op2.getResult());\n+\n+    output = op4_not_op3.getResult();\n+  } else {\n+    auto op1_equal_in = rewriter.create<tosa::EqualOp>(\n+        op->getLoc(), output_type, tfl_not_equal_op.lhs(),\n+        tfl_not_equal_op.rhs());\n+    auto op2_not_op1 = rewriter.create<tosa::LogicalNotOp>(\n+        op->getLoc(), output_type, op1_equal_in.getResult());\n+\n+    output = op2_not_op1.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLGreaterOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_greater_op = cast<TFL::GreaterOp>(op);\n+\n+  auto input_lhs_type =\n+      tfl_greater_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type =\n+      tfl_greater_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_greater_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLGreaterOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    if (input_lhs_qtype.getScale() != input_rhs_qtype.getScale() ||\n+        input_lhs_qtype.getZeroPoint() != input_rhs_qtype.getZeroPoint()) {\n+      op->emitOpError(\n+          \"ConvertTFLGreaterOp: input_x and input_y scale/zp \"\n+          \"must be the same\");\n+      return failure();\n+    }\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_greater_op.lhs(), 1.0f,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_greater_op.rhs(), 1.0f,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_greater_op1_op2 = rewriter.create<tosa::GreaterOp>(\n+        op->getLoc(), output_type, op1_rescale_lhs, op2_rescale_rhs);\n+\n+    output = op3_greater_op1_op2.getResult();\n+  } else {\n+    auto op1_greater_in = rewriter.create<tosa::GreaterOp>(\n+        op->getLoc(), output_type, tfl_greater_op.lhs(), tfl_greater_op.rhs());\n+\n+    output = op1_greater_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLGreaterEqualOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_greater_equal_op = cast<TFL::GreaterEqualOp>(op);\n+\n+  auto input_lhs_type =\n+      tfl_greater_equal_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type =\n+      tfl_greater_equal_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_greater_equal_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLGreaterEqualOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    if (input_lhs_qtype.getScale() != input_rhs_qtype.getScale() ||\n+        input_lhs_qtype.getZeroPoint() != input_rhs_qtype.getZeroPoint()) {\n+      op->emitOpError(\n+          \"ConvertTFLGreaterEqualOp: input_x and input_y scale/zp \"\n+          \"must be the same\");\n+      return failure();\n+    }\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_greater_equal_op.lhs(), 1.0f,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_greater_equal_op.rhs(), 1.0f,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_greater_equal_op1_op2 = rewriter.create<tosa::GreaterEqualOp>(\n+        op->getLoc(), output_type, op1_rescale_lhs, op2_rescale_rhs);\n+\n+    output = op3_greater_equal_op1_op2.getResult();\n+  } else {\n+    auto op1_greater_equal_in = rewriter.create<tosa::GreaterEqualOp>(\n+        op->getLoc(), output_type, tfl_greater_equal_op.lhs(),\n+        tfl_greater_equal_op.rhs());\n+\n+    output = op1_greater_equal_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLAddOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_add_op = cast<TFL::AddOp>(op);\n+\n+  auto input_lhs_type = tfl_add_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type = tfl_add_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_add_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLAddOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    double in_lhs_scale = input_lhs_qtype.getScale();\n+    double in_rhs_scale = input_rhs_qtype.getScale();\n+    double output_scale = output_qtype.getScale();\n+    double max_scale_2x = 2.0 * std::max(in_lhs_scale, in_rhs_scale);\n+\n+    int32_t input_shift = 20;\n+\n+    double lhs_rescale_scale =\n+        double(1 << input_shift) * in_lhs_scale / max_scale_2x;\n+    double rhs_rescale_scale =\n+        double(1 << input_shift) * in_rhs_scale / max_scale_2x;\n+    double output_rescale_scale =\n+        max_scale_2x / (output_scale * double(1 << input_shift));\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_add_op.lhs(), lhs_rescale_scale,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_add_op.rhs(), rhs_rescale_scale,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_add_op1_op2 = rewriter.create<tosa::AddOp>(\n+        op->getLoc(), rescale_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_rescale_op3 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op3_add_op1_op2.getResult(),\n+        output_rescale_scale, output_qtype.getZeroPoint());\n+    output = op4_rescale_op3;\n+  } else {\n+    auto op1_add_in = rewriter.create<tosa::AddOp>(\n+        op->getLoc(), output_type, tfl_add_op.lhs(), tfl_add_op.rhs());\n+\n+    output = op1_add_in.getResult();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_add_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op =\n+        convertFusedActivation(rewriter, op, output, fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLSubOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_sub_op = cast<TFL::SubOp>(op);\n+\n+  auto input_lhs_type = tfl_sub_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type = tfl_sub_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_sub_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLSubOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    double in_lhs_scale = input_lhs_qtype.getScale();\n+    double in_rhs_scale = input_rhs_qtype.getScale();\n+    double output_scale = output_qtype.getScale();\n+    double max_scale_2x = 2.0 * std::max(in_lhs_scale, in_rhs_scale);\n+\n+    int32_t input_shift = 20;\n+\n+    double lhs_rescale_scale =\n+        double(1 << input_shift) * in_lhs_scale / max_scale_2x;\n+    double rhs_rescale_scale =\n+        double(1 << input_shift) * in_rhs_scale / max_scale_2x;\n+    double output_rescale_scale =\n+        max_scale_2x / (output_scale * double(1 << input_shift));\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_sub_op.lhs(), lhs_rescale_scale,\n+                            input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_sub_op.rhs(), rhs_rescale_scale,\n+                            input_rhs_qtype.getZeroPoint());\n+    auto op3_sub_op1_op2 = rewriter.create<tosa::SubOp>(\n+        op->getLoc(), rescale_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_rescale_op3 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op3_sub_op1_op2.getResult(),\n+        output_rescale_scale, output_qtype.getZeroPoint());\n+    output = op4_rescale_op3;\n+  } else {\n+    auto op1_sub_in = rewriter.create<tosa::SubOp>(\n+        op->getLoc(), output_type, tfl_sub_op.lhs(), tfl_sub_op.rhs());\n+\n+    output = op1_sub_in.getResult();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_sub_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op =\n+        convertFusedActivation(rewriter, op, output, fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLMulOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_mul_op = cast<TFL::MulOp>(op);\n+\n+  auto lowered_op = convertMultiplyOp(rewriter, op, tfl_mul_op.getResult(),\n+                                      tfl_mul_op.lhs(), tfl_mul_op.rhs());\n+\n+  if (!lowered_op) {\n+    return failure();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_mul_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, lowered_op->getResult(0), fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {lowered_op->getResult(0)});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLSquareOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_square_op = cast<TFL::SquareOp>(op);\n+\n+  auto lowered_op = convertMultiplyOp(rewriter, op, tfl_square_op.getResult(),\n+                                      tfl_square_op.x(), tfl_square_op.x());\n+\n+  if (!lowered_op) {\n+    return failure();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_square_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, lowered_op->getResult(0), fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {lowered_op->getResult(0)});\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLSquaredDifferenceOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_squared_op = cast<TFL::SquaredDifferenceOp>(op);\n+\n+  auto lowered_op =\n+      convertSquaredDifferenceOp(rewriter, op, tfl_squared_op.getResult(),\n+                                 tfl_squared_op.lhs(), tfl_squared_op.rhs());\n+\n+  TOSA_REPLACE_LOWERED_OP(rewriter, op, lowered_op);\n+}\n+\n+LogicalResult ConvertTFLRoundOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_round_op = cast<TFL::RoundOp>(op);\n+\n+  auto input_type = tfl_round_op.x().getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) {\n+    op->emitOpError(\"Round: input not ranked tensor type\");\n+    return failure();\n+  }\n+\n+  if (input_type.getElementType().isa<FloatType>()) {\n+    auto lowered_op = convertRoundOp(rewriter, op, tfl_round_op.getResult(),\n+                                     tfl_round_op.x());\n+\n+    TOSA_REPLACE_LOWERED_OP(rewriter, op, lowered_op);\n+\n+  } else {\n+    // Round on int is nonsensical. Instead, replace uses of result with the\n+    // input.\n+    tfl_round_op.replaceAllUsesWith(tfl_round_op.x());\n+  }\n+}\n+\n+LogicalResult ConvertTFLDivOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_div_op = cast<TFL::DivOp>(op);\n+\n+  auto output_type =\n+      tfl_div_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!output_type) return failure();\n+\n+  auto fused_activation_fn =\n+      tfl_div_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  auto reciprocal_op = rewriter.create<tosa::ReciprocalOp>(\n+      op->getLoc(), output_type, tfl_div_op.rhs());\n+  auto mul_op =\n+      rewriter.create<tosa::MulOp>(op->getLoc(), output_type, tfl_div_op.lhs(),\n+                                   reciprocal_op.getResult(), 0);\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, mul_op.getResult(), fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {mul_op.getResult()});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLMaximumOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_max_op = cast<TFL::MaximumOp>(op);\n+\n+  auto input_lhs_type = tfl_max_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type = tfl_max_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_max_op.getResult().getType().dyn_cast<RankedTensorType>();\n+\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLMaximumOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_max_op.lhs(), 1.0f, 0);\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_max_op.rhs(), 1.0f, 0);\n+    auto op3_max_op1_op2 = rewriter.create<tosa::MaximumOp>(\n+        op->getLoc(), rescale_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_rescale_op3 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op3_max_op1_op2.getResult(), 1.0f, 0);\n+\n+    output = op4_rescale_op3;\n+  } else {\n+    auto op1_max_in = rewriter.create<tosa::MaximumOp>(\n+        op->getLoc(), output_type, tfl_max_op.lhs(), tfl_max_op.rhs());\n+\n+    output = op1_max_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLMinimumOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_min_op = cast<TFL::MinimumOp>(op);\n+\n+  auto input_lhs_type = tfl_min_op.lhs().getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type = tfl_min_op.rhs().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_min_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return failure();\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertTFLMinimumOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+\n+    auto op1_rescale_lhs =\n+        buildRescaleToInt32(rewriter, op, tfl_min_op.lhs(), 1.0f, 0);\n+    auto op2_rescale_rhs =\n+        buildRescaleToInt32(rewriter, op, tfl_min_op.rhs(), 1.0f, 0);\n+    auto op3_min_op1_op2 = rewriter.create<tosa::MinimumOp>(\n+        op->getLoc(), rescale_type, op1_rescale_lhs, op2_rescale_rhs);\n+    auto op4_rescale_op3 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op3_min_op1_op2.getResult(), 1.0f, 0);\n+\n+    output = op4_rescale_op3;\n+  } else {\n+    auto op1_min_in = rewriter.create<tosa::MinimumOp>(\n+        op->getLoc(), output_type, tfl_min_op.lhs(), tfl_min_op.rhs());\n+\n+    output = op1_min_in.getResult();\n+  }\n+\n+  rewriter.replaceOp(op, {output});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLFloorDivOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_floordiv_op = cast<TFL::FloorDivOp>(op);\n+\n+  auto lowered_op =\n+      convertFloorDivOp(rewriter, op, tfl_floordiv_op.getResult(),\n+                        tfl_floordiv_op.lhs(), tfl_floordiv_op.rhs());\n+\n+  TOSA_REPLACE_LOWERED_OP(rewriter, op, lowered_op);\n+}\n+\n+LogicalResult ConvertTFLFloorModOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_floormod_op = cast<TFL::FloorModOp>(op);\n+\n+  auto lowered_op =\n+      convertFloorModOp(rewriter, op, tfl_floormod_op.getResult(),\n+                        tfl_floormod_op.lhs(), tfl_floormod_op.rhs());\n+\n+  TOSA_REPLACE_LOWERED_OP(rewriter, op, lowered_op);\n+}\n+\n+LogicalResult ConvertTFLAddNOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_addn_op = cast<TFL::AddNOp>(op);\n+\n+  auto output_type =\n+      tfl_addn_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!output_type) return failure();\n+\n+  SmallVector<Value, 4> inputs(tfl_addn_op.inputs());\n+\n+  assert(inputs.size() >= 2);\n+\n+  auto newOp = rewriter.create<tosa::AddOp>(op->getLoc(), output_type,\n+                                            inputs[0], inputs[1]);\n+  for (int i = 2; i < inputs.size(); i++) {\n+    newOp = rewriter.create<tosa::AddOp>(op->getLoc(), output_type, inputs[i],\n+                                         newOp.getResult());\n+  }\n+\n+  rewriter.replaceOp(op, {newOp.getResult()});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLAveragePool2DOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_avgpool_op = cast<TFL::AveragePool2DOp>(op);\n+\n+  auto input_type =\n+      tfl_avgpool_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_avgpool_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!output_type) return failure();\n+\n+  auto tmpAttr = tfl_avgpool_op.getAttrOfType<StringAttr>(\"data_format\");\n+  if (tmpAttr && tmpAttr.getValue().str() != \"NHWC\") return failure();\n+\n+  // Kernels and strides are dimensionally ordered\n+  SmallVector<int64_t, 4> i64array({1, 1, 1, 1});\n+  ArrayAttr kernel_size;\n+  ArrayAttr stride;\n+  ArrayAttr pad;\n+  {\n+    int64_t kernel_h = tfl_avgpool_op.filter_height();\n+    int64_t kernel_w = tfl_avgpool_op.filter_width();\n+    kernel_size = rewriter.getI64ArrayAttr({kernel_h, kernel_w});\n+    // i64array is formatted as NHWC now\n+    i64array[1] = kernel_h;\n+    i64array[2] = kernel_w;\n+  }\n+  {\n+    int64_t stride_h = tfl_avgpool_op.stride_h();\n+    int64_t stride_w = tfl_avgpool_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_avgpool_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    // Pooling has no non-unit dilation\n+    ArrayAttr dilation = rewriter.getI64ArrayAttr({1, 1});\n+\n+    auto filter_type = RankedTensorType::get(\n+        llvm::makeArrayRef<int64_t>(i64array), rewriter.getIntegerType(64));\n+\n+    // TFLite doesn't support explicit padding\n+    if (!getPaddingValuesFromPadType(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, stride, dilation, rewriter, pad))\n+      return failure();\n+  }\n+\n+  rewriter.replaceOpWithNewOp<tosa::AvgPool2dOp>(\n+      op, output_type, tfl_avgpool_op.input(), kernel_size, stride, pad);\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLMaxPool2DOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_maxpool_op = cast<TFL::MaxPool2DOp>(op);\n+\n+  auto input_type =\n+      tfl_maxpool_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_maxpool_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!output_type) return failure();\n+\n+  auto tmpAttr = tfl_maxpool_op.getAttrOfType<StringAttr>(\"data_format\");\n+  if (tmpAttr && tmpAttr.getValue().str() != \"NHWC\") return failure();\n+\n+  // Kernels and strides are dimensionally ordered\n+  SmallVector<int64_t, 4> i64array({1, 1, 1, 1});\n+  ArrayAttr kernel_size;\n+  ArrayAttr stride;\n+  ArrayAttr pad;\n+  {\n+    int64_t kernel_h = tfl_maxpool_op.filter_height();\n+    int64_t kernel_w = tfl_maxpool_op.filter_width();\n+    kernel_size = rewriter.getI64ArrayAttr({kernel_h, kernel_w});\n+    // i64array is formatted as NHWC now\n+    i64array[1] = kernel_h;\n+    i64array[2] = kernel_w;\n+  }\n+  {\n+    int64_t stride_h = tfl_maxpool_op.stride_h();\n+    int64_t stride_w = tfl_maxpool_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_maxpool_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    // Pooling has no non-unit dilation\n+    ArrayAttr dilation = rewriter.getI64ArrayAttr({1, 1});\n+\n+    auto filter_type = RankedTensorType::get(\n+        llvm::makeArrayRef<int64_t>(i64array), rewriter.getIntegerType(64));\n+\n+    // TFLite doesn't support explicit padding\n+    if (!getPaddingValuesFromPadType(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, stride, dilation, rewriter, pad))\n+      return failure();\n+  }\n+\n+  rewriter.replaceOpWithNewOp<tosa::MaxPool2dOp>(\n+      op, output_type, tfl_maxpool_op.input(), kernel_size, stride, pad);\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLConv2DOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_conv2d_op = cast<TFL::Conv2DOp>(op);\n+\n+  auto input_type =\n+      tfl_conv2d_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto filter_type =\n+      tfl_conv2d_op.filter().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_conv2d_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type) return failure();\n+  if (!output_type) return failure();\n+  if (!filter_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool filter_is_qtype =\n+      filter_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::QuantizedType>();\n+\n+  if ((input_is_qtype != filter_is_qtype) ||\n+      (input_is_qtype != output_is_qtype)) {\n+    op->emitOpError(\n+        \"ConvertTFLConv2DOp: input/filter/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  auto tmpAttr = tfl_conv2d_op.getAttrOfType<StringAttr>(\"data_format\");\n+  if (tmpAttr && tmpAttr.getValue().str() != \"NHWC\") return failure();\n+\n+  ArrayAttr pad;\n+  ArrayAttr stride;\n+  ArrayAttr dilation;\n+  StringAttr activation_function;\n+  {\n+    int64_t stride_h = tfl_conv2d_op.stride_h();\n+    int64_t stride_w = tfl_conv2d_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+  {\n+    int64_t dilation_h = tfl_conv2d_op.dilation_h_factor();\n+    int64_t dilation_w = tfl_conv2d_op.dilation_w_factor();\n+    dilation = rewriter.getI64ArrayAttr({dilation_h, dilation_w});\n+  }\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_conv2d_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    // TFLite doesn't support explicit padding\n+    if (!getPaddingValuesFromPadType(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, stride, dilation, rewriter, pad))\n+      return failure();\n+  }\n+\n+  Value unquantized_bias =\n+      getUnquantizedBias(rewriter, op, tfl_conv2d_op.bias());\n+\n+  auto a1_conv2d_op = rewriter.create<tosa::Conv2DOp>(\n+      op->getLoc(), output_type, tfl_conv2d_op.input(), tfl_conv2d_op.filter(),\n+      unquantized_bias, pad, stride, dilation);\n+\n+  Value conv2d_output;\n+  if (input_is_qtype) {\n+    conv2d_output =\n+        buildRescaleOpConvOutput(rewriter, op, a1_conv2d_op.getResult(),\n+                                 input_type, filter_type, output_type);\n+  } else {\n+    conv2d_output = a1_conv2d_op.getResult();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_conv2d_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, conv2d_output, fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {conv2d_output});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLTransposeConvOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_conv_op = cast<TFL::TransposeConvOp>(op);\n+\n+  auto input_type = tfl_conv_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto filter_type =\n+      tfl_conv_op.weights().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_conv_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type) return failure();\n+  if (!output_type) return failure();\n+  if (!filter_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool filter_is_qtype =\n+      filter_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::QuantizedType>();\n+\n+  if ((input_is_qtype != filter_is_qtype) ||\n+      (input_is_qtype != output_is_qtype)) {\n+    op->emitOpError(\n+        \"ConvertTFLConv2DOp: input/filter/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  ArrayAttr stride;\n+  ArrayAttr dilation;\n+  ArrayAttr outpad;\n+  ArrayAttr output_shape;\n+  StringAttr activation_function;\n+  {\n+    int64_t stride_h = tfl_conv_op.stride_h();\n+    int64_t stride_w = tfl_conv_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+\n+  // tfl.transpose_conv doesn't support dilations\n+  dilation = rewriter.getI64ArrayAttr({1, 1});\n+\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_conv_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    if (!getTransposeConv2dPaddingValues(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, output_type, stride, dilation, rewriter,\n+            outpad))\n+      return failure();\n+  }\n+  {\n+    ElementsAttr output_shape_elems;\n+    // Match from input_size tensor first\n+    if (matchPattern(tfl_conv_op.output_shape(),\n+                     m_Constant(&output_shape_elems))) {\n+      llvm::SmallVector<int64_t, 4> shape_vec;\n+      for (int i = 0; i < output_shape_elems.getNumElements(); i++)\n+        shape_vec.push_back(\n+            output_shape_elems.getValue<IntegerAttr>(i).getInt());\n+      output_shape = rewriter.getI64ArrayAttr(shape_vec);\n+    }\n+    // Use output tensor's shape otherwise\n+    else {\n+      output_shape = rewriter.getI64ArrayAttr(output_type.getShape());\n+    }\n+  }\n+\n+  Value zero_bias;\n+  if (input_is_qtype) {\n+    uint32_t input_bits = input_type.getElementType()\n+                              .dyn_cast<mlir::quant::QuantizedType>()\n+                              .getStorageTypeIntegralWidth();\n+    uint32_t weight_bits = filter_type.getElementType()\n+                               .dyn_cast<mlir::quant::QuantizedType>()\n+                               .getStorageTypeIntegralWidth();\n+\n+    if (input_bits == 16 && weight_bits == 8) {\n+      SmallVector<int64_t, 8> zero_bias_vec(output_type.getShape()[3], 0);\n+      zero_bias = get1DConstTensorInt48(rewriter, op, zero_bias_vec);\n+    } else {\n+      SmallVector<int32_t, 8> zero_bias_vec(output_type.getShape()[3], 0);\n+      zero_bias =\n+          get1DConstTensor<tosa::ConstOp, int32_t>(rewriter, op, zero_bias_vec);\n+    }\n+  } else {\n+    SmallVector<float, 8> zero_bias_vec(output_type.getShape()[3], 0.0f);\n+    zero_bias =\n+        get1DConstTensor<tosa::ConstOp, float>(rewriter, op, zero_bias_vec);\n+  }\n+\n+  auto a1_conv2d_op = rewriter.create<tosa::TransposeConv2DOp>(\n+      op->getLoc(), output_type, tfl_conv_op.input(), tfl_conv_op.weights(),\n+      zero_bias, outpad, stride, dilation, output_shape);\n+\n+  Value conv2d_output;\n+  if (input_is_qtype) {\n+    conv2d_output =\n+        buildRescaleOpConvOutput(rewriter, op, a1_conv2d_op.getResult(),\n+                                 input_type, filter_type, output_type);\n+  } else {\n+    conv2d_output = a1_conv2d_op.getResult();\n+  }\n+\n+  auto fused_activation_fn =\n+      tfl_conv_op.getAttrOfType<StringAttr>(\"fused_activation_function\");\n+\n+  if (fused_activation_fn) {\n+    auto fused_activation_op = convertFusedActivation(\n+        rewriter, op, conv2d_output, fused_activation_fn);\n+\n+    if (fused_activation_op) {\n+      TOSA_REPLACE_LOWERED_OP(rewriter, op, fused_activation_op);\n+    }\n+  }\n+\n+  rewriter.replaceOp(op, {conv2d_output});\n+\n+  return success();\n+}\n+\n+LogicalResult ConvertTFLDepthwiseConv2DOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  auto tfl_conv2d_op = cast<TFL::DepthwiseConv2DOp>(op);\n+\n+  auto input_type =\n+      tfl_conv2d_op.input().getType().dyn_cast<RankedTensorType>();\n+  auto filter_type =\n+      tfl_conv2d_op.filter().getType().dyn_cast<RankedTensorType>();\n+  auto output_type =\n+      tfl_conv2d_op.getResult().getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_type) return failure();\n+  if (!output_type) return failure();\n+  if (!filter_type) return failure();\n+\n+  bool input_is_qtype =\n+      input_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool filter_is_qtype =\n+      filter_type.getElementType().isa<mlir::quant::QuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::QuantizedType>();\n+\n+  if ((input_is_qtype != filter_is_qtype) ||\n+      (input_is_qtype != output_is_qtype)) {\n+    op->emitOpError(\n+        \"ConvertTFLConv2DOp: input/filter/output tensor should \"\n+        \"be all quantized or all native\");\n+    return failure();\n+  }\n+\n+  auto filter_shape = filter_type.getShape();\n+  // Operator depthwiseConv2D\n+  // TFLite orders the depthwiseConv2D filter in IHWO, while TOSA orders\n+  // filter in HWIO\n+  //\n+  // The lowering reorders the filter.\n+  //\n+  // a1_transpose = tosa.transpose(filter, {1, 2, 3, 0})   // HWIO\n+  // a2_reshape = tosa.reshape(filter, H, W, depth_multiplier, I /\n+  // depth_multiplier)\n+  // a3_transpose_conv2d = tosa.transpose_conv2d(input, a2_reshape, padding,\n+  // stride, dilation)\n+\n+  auto tmpAttr = tfl_conv2d_op.getAttrOfType<StringAttr>(\"data_format\");\n+  if (tmpAttr && tmpAttr.getValue().str() != \"NHWC\") return failure();\n+\n+  ArrayAttr pad;\n+  ArrayAttr stride;\n+  ArrayAttr dilation;\n+  StringAttr activation_function;\n+\n+  auto depth_multiplier =\n+      tfl_conv2d_op.getAttrOfType<IntegerAttr>(\"depth_multiplier\");\n+\n+  {\n+    int64_t stride_h = tfl_conv2d_op.stride_h();\n+    int64_t stride_w = tfl_conv2d_op.stride_w();\n+    stride = rewriter.getI64ArrayAttr({stride_h, stride_w});\n+  }\n+  {\n+    int64_t dilation_h = tfl_conv2d_op.dilation_h_factor();\n+    int64_t dilation_w = tfl_conv2d_op.dilation_w_factor();\n+    dilation = rewriter.getI64ArrayAttr({dilation_h, dilation_w});\n+  }\n+  {\n+    tensorflow::Padding tf_pad;\n+    if (!GetPaddingFromString(tfl_conv2d_op.padding().str(), &tf_pad).ok())\n+      return failure();\n+\n+    if (!getPaddingValuesFromPadType(\n+            tf_pad,\n+            tensorflow::FORMAT_NHWC,  // TFLite only supports this\n+            1,                        // tensorflow::FORMAT_OHWI,\n+            input_type, filter_type, stride, dilation, rewriter, pad))\n+      return failure();\n+  }\n+\n+  std::vector<int64_t> a1_transpose_dims;",
        "comment_created_at": "2020-11-18T18:06:18+00:00",
        "comment_author": "sjarus",
        "comment_body": "Replaced with SmallVector",
        "pr_file_module": null
      }
    ]
  }
]