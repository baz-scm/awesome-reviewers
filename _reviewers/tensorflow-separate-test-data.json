[
  {
    "discussion_id": "350875781",
    "pr_number": 34284,
    "pr_file": "tensorflow/compiler/xla/service/algebraic_simplifier_test.cc",
    "created_at": "2019-11-26T17:14:57+00:00",
    "commented_code": "GmockMatch(m::Parameter(1)));\n }\n \n+TEST_F(AlgebraicSimplifierTest, SqrtOfSelfMultiply) {\n+  const char* kModuleStr = R\"(\n+    HloModule m\n+    test {\n+      %p0 = f32[32]{0} parameter(0)\n+      %multiply = f32[32]{0} multiply(f32[32]{0} %p0, f32[32]{0} %p0)\n+      ROOT %sqrt = f32[32]{0} sqrt(f32[32]{0} %multiply)\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(kModuleStr));\n+  ASSERT_TRUE(AlgebraicSimplifier(default_options_).Run(m.get()).ValueOrDie());\n+  EXPECT_THAT(m->entry_computation()->root_instruction(),\n+              GmockMatch(m::Abs(m::Parameter(0))));\n+}\n+\n+TEST_F(AlgebraicSimplifierTest, RsqrtOfRPower) {",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "350875781",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 34284,
        "pr_file": "tensorflow/compiler/xla/service/algebraic_simplifier_test.cc",
        "discussion_id": "350875781",
        "commented_code": "@@ -5809,5 +5810,235 @@ TEST_F(AlgebraicSimplifierTest, SliceOfConcat) {\n               GmockMatch(m::Parameter(1)));\n }\n \n+TEST_F(AlgebraicSimplifierTest, SqrtOfSelfMultiply) {\n+  const char* kModuleStr = R\"(\n+    HloModule m\n+    test {\n+      %p0 = f32[32]{0} parameter(0)\n+      %multiply = f32[32]{0} multiply(f32[32]{0} %p0, f32[32]{0} %p0)\n+      ROOT %sqrt = f32[32]{0} sqrt(f32[32]{0} %multiply)\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(kModuleStr));\n+  ASSERT_TRUE(AlgebraicSimplifier(default_options_).Run(m.get()).ValueOrDie());\n+  EXPECT_THAT(m->entry_computation()->root_instruction(),\n+              GmockMatch(m::Abs(m::Parameter(0))));\n+}\n+\n+TEST_F(AlgebraicSimplifierTest, RsqrtOfRPower) {",
        "comment_created_at": "2019-11-26T17:14:57+00:00",
        "comment_author": "bixia1",
        "comment_body": "For a few tests you added that use the meta data, I think we need to add corresponding negative tests. The negative tests may not set the meta data, it may also just use a non-batch-norm related HLO value.",
        "pr_file_module": null
      },
      {
        "comment_id": "350974868",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 34284,
        "pr_file": "tensorflow/compiler/xla/service/algebraic_simplifier_test.cc",
        "discussion_id": "350875781",
        "commented_code": "@@ -5809,5 +5810,235 @@ TEST_F(AlgebraicSimplifierTest, SliceOfConcat) {\n               GmockMatch(m::Parameter(1)));\n }\n \n+TEST_F(AlgebraicSimplifierTest, SqrtOfSelfMultiply) {\n+  const char* kModuleStr = R\"(\n+    HloModule m\n+    test {\n+      %p0 = f32[32]{0} parameter(0)\n+      %multiply = f32[32]{0} multiply(f32[32]{0} %p0, f32[32]{0} %p0)\n+      ROOT %sqrt = f32[32]{0} sqrt(f32[32]{0} %multiply)\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(kModuleStr));\n+  ASSERT_TRUE(AlgebraicSimplifier(default_options_).Run(m.get()).ValueOrDie());\n+  EXPECT_THAT(m->entry_computation()->root_instruction(),\n+              GmockMatch(m::Abs(m::Parameter(0))));\n+}\n+\n+TEST_F(AlgebraicSimplifierTest, RsqrtOfRPower) {",
        "comment_created_at": "2019-11-26T20:54:10+00:00",
        "comment_author": "AyanmoI",
        "comment_body": "I agree. Will do that.",
        "pr_file_module": null
      },
      {
        "comment_id": "351458281",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 34284,
        "pr_file": "tensorflow/compiler/xla/service/algebraic_simplifier_test.cc",
        "discussion_id": "350875781",
        "commented_code": "@@ -5809,5 +5810,235 @@ TEST_F(AlgebraicSimplifierTest, SliceOfConcat) {\n               GmockMatch(m::Parameter(1)));\n }\n \n+TEST_F(AlgebraicSimplifierTest, SqrtOfSelfMultiply) {\n+  const char* kModuleStr = R\"(\n+    HloModule m\n+    test {\n+      %p0 = f32[32]{0} parameter(0)\n+      %multiply = f32[32]{0} multiply(f32[32]{0} %p0, f32[32]{0} %p0)\n+      ROOT %sqrt = f32[32]{0} sqrt(f32[32]{0} %multiply)\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(kModuleStr));\n+  ASSERT_TRUE(AlgebraicSimplifier(default_options_).Run(m.get()).ValueOrDie());\n+  EXPECT_THAT(m->entry_computation()->root_instruction(),\n+              GmockMatch(m::Abs(m::Parameter(0))));\n+}\n+\n+TEST_F(AlgebraicSimplifierTest, RsqrtOfRPower) {",
        "comment_created_at": "2019-11-27T19:14:50+00:00",
        "comment_author": "AyanmoI",
        "comment_body": "Btw do you want negative tests for all these tests? or would adding one or two cases suffice?",
        "pr_file_module": null
      },
      {
        "comment_id": "351487613",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 34284,
        "pr_file": "tensorflow/compiler/xla/service/algebraic_simplifier_test.cc",
        "discussion_id": "350875781",
        "commented_code": "@@ -5809,5 +5810,235 @@ TEST_F(AlgebraicSimplifierTest, SliceOfConcat) {\n               GmockMatch(m::Parameter(1)));\n }\n \n+TEST_F(AlgebraicSimplifierTest, SqrtOfSelfMultiply) {\n+  const char* kModuleStr = R\"(\n+    HloModule m\n+    test {\n+      %p0 = f32[32]{0} parameter(0)\n+      %multiply = f32[32]{0} multiply(f32[32]{0} %p0, f32[32]{0} %p0)\n+      ROOT %sqrt = f32[32]{0} sqrt(f32[32]{0} %multiply)\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(kModuleStr));\n+  ASSERT_TRUE(AlgebraicSimplifier(default_options_).Run(m.get()).ValueOrDie());\n+  EXPECT_THAT(m->entry_computation()->root_instruction(),\n+              GmockMatch(m::Abs(m::Parameter(0))));\n+}\n+\n+TEST_F(AlgebraicSimplifierTest, RsqrtOfRPower) {",
        "comment_created_at": "2019-11-27T20:35:14+00:00",
        "comment_author": "bixia1",
        "comment_body": "I think two is good enough.",
        "pr_file_module": null
      },
      {
        "comment_id": "351503539",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 34284,
        "pr_file": "tensorflow/compiler/xla/service/algebraic_simplifier_test.cc",
        "discussion_id": "350875781",
        "commented_code": "@@ -5809,5 +5810,235 @@ TEST_F(AlgebraicSimplifierTest, SliceOfConcat) {\n               GmockMatch(m::Parameter(1)));\n }\n \n+TEST_F(AlgebraicSimplifierTest, SqrtOfSelfMultiply) {\n+  const char* kModuleStr = R\"(\n+    HloModule m\n+    test {\n+      %p0 = f32[32]{0} parameter(0)\n+      %multiply = f32[32]{0} multiply(f32[32]{0} %p0, f32[32]{0} %p0)\n+      ROOT %sqrt = f32[32]{0} sqrt(f32[32]{0} %multiply)\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(kModuleStr));\n+  ASSERT_TRUE(AlgebraicSimplifier(default_options_).Run(m.get()).ValueOrDie());\n+  EXPECT_THAT(m->entry_computation()->root_instruction(),\n+              GmockMatch(m::Abs(m::Parameter(0))));\n+}\n+\n+TEST_F(AlgebraicSimplifierTest, RsqrtOfRPower) {",
        "comment_created_at": "2019-11-27T21:24:38+00:00",
        "comment_author": "AyanmoI",
        "comment_body": "The negative tests I added may a too detailed. Please let me know if this is what you expect or would just asserting that AlgebraicSimplifier change hasn't kicked in is enough?(`ASSERT_FALSE(AlgebraicSimplifier(default_options_).Run(m.get()).ValueOrDie());`)",
        "pr_file_module": null
      },
      {
        "comment_id": "351509854",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 34284,
        "pr_file": "tensorflow/compiler/xla/service/algebraic_simplifier_test.cc",
        "discussion_id": "350875781",
        "commented_code": "@@ -5809,5 +5810,235 @@ TEST_F(AlgebraicSimplifierTest, SliceOfConcat) {\n               GmockMatch(m::Parameter(1)));\n }\n \n+TEST_F(AlgebraicSimplifierTest, SqrtOfSelfMultiply) {\n+  const char* kModuleStr = R\"(\n+    HloModule m\n+    test {\n+      %p0 = f32[32]{0} parameter(0)\n+      %multiply = f32[32]{0} multiply(f32[32]{0} %p0, f32[32]{0} %p0)\n+      ROOT %sqrt = f32[32]{0} sqrt(f32[32]{0} %multiply)\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(kModuleStr));\n+  ASSERT_TRUE(AlgebraicSimplifier(default_options_).Run(m.get()).ValueOrDie());\n+  EXPECT_THAT(m->entry_computation()->root_instruction(),\n+              GmockMatch(m::Abs(m::Parameter(0))));\n+}\n+\n+TEST_F(AlgebraicSimplifierTest, RsqrtOfRPower) {",
        "comment_created_at": "2019-11-27T21:45:47+00:00",
        "comment_author": "bixia1",
        "comment_body": "Checking the pass return false is good enough here.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1586915825",
    "pr_number": 65939,
    "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general_test.cc",
    "created_at": "2024-05-01T23:21:57+00:00",
    "commented_code": "+/*Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/bf16.h\"\n+#include \"tensorflow/lite/experimental/shlo/f16.h\"\n+#include \"tensorflow/lite/experimental/shlo/i4.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/test_util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/status_matcher.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+using testing::Eq;\n+using testing::FloatEq;\n+using testing::FloatNear;\n+using testing::Pointwise;\n+namespace shlo_ref {\n+\n+namespace {\n+\n+template <class T>\n+struct NonQuantizedBoolDotGeneralTest : ::testing::Test {};\n+TYPED_TEST_SUITE(NonQuantizedBoolDotGeneralTest, BoolTestType, TestParamNames);\n+\n+TYPED_TEST(NonQuantizedBoolDotGeneralTest, BoolTestTypesTensorsWork1) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({7, 3, 4});\n+  const Shape shape_rhs({7, 4});\n+  const Shape shape_r({7, 3});\n+\n+  Vector<StorageT> lhs_data = {\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true};\n+  Vector<StorageT> rhs_data = {true, true, true, true, true, true, true,\n+                               true, true, true, true, true, true, true,\n+                               true, true, true, true, true, true, true,\n+                               true, true, true, true, true, true, true};\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{0};\n+  Vector<int64_t> rhsb_dim{0};\n+  Vector<int64_t> lhsc_dim{2};\n+  Vector<int64_t> rhsc_dim{1};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data = {true, true, true, true, true, true, true,\n+                                    true, true, true, true, true, true, true,\n+                                    true, true, true, true, true, true, true};\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedBoolDotGeneralTest, BoolTestTypesTensorsWork2) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({3, 4});\n+  const Shape shape_rhs({4, 2});\n+  const Shape shape_r({3, 2});\n+\n+  Vector<StorageT> lhs_data = {true, true,  true,  false, true, true,\n+                               true, false, false, true,  true, true};\n+  Vector<StorageT> rhs_data = {true, true,  true, false,\n+                               true, false, true, false};\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{};\n+  Vector<int64_t> rhsb_dim{};\n+  Vector<int64_t> lhsc_dim{1};\n+  Vector<int64_t> rhsc_dim{0};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data = {true, true, true, true, true, false};\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+template <class T>\n+struct NonQuantizedIntDotGeneralTest : ::testing::Test {};\n+TYPED_TEST_SUITE(NonQuantizedIntDotGeneralTest, IntTestTypes, TestParamNames);\n+\n+TYPED_TEST(NonQuantizedIntDotGeneralTest, IntTestTypesTensorsWork1) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({2, 2});\n+  const Shape shape_rhs({2, 2});\n+  const Shape shape_r({2, 2, 2, 2});\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 4};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{1, 0, 0, 1};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{};\n+  Vector<int64_t> rhsb_dim{};\n+  Vector<int64_t> lhsc_dim{};\n+  Vector<int64_t> rhsc_dim{};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{1, 0, 0, 1, 2, 0, 0, 2,\n+                                    3, 0, 0, 3, 4, 0, 0, 4};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedIntDotGeneralTest, IntTestTypesTensorsWork2) {\n+  using StorageT = typename TypeParam::StorageT;\n+  const Shape shape_lhs({7, 3, 4});\n+  const Shape shape_rhs({7, 4});\n+  const Shape shape_r({7, 3});\n+\n+  Vector<int64_t> lhs_data_int{\n+      0,  1,  4,  1,  -2, -3, 0, 0, 6,  -1, 0,  0,  1,  0,  -2, 0,  1,\n+      3,  4,  -6, 2,  4,  4,  0, 0, -2, -1, 1,  -2, -3, 0,  2,  -3, 0,\n+      0,  -2, 4,  -7, 2,  2,  0, 4, 2,  0,  -6, 1,  1,  2,  -2, -2, 0,\n+      -1, -4, -1, 0,  -1, 1,  3, 1, 1,  -4, 0,  0,  1,  -1, 0,  4,  -2,\n+      0,  5,  0,  -1, 0,  2,  1, 2, -1, 1,  -3, -2, -6, -3, -1, -3};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{2,  0, -1, 4,  -4, 0,  2,  -1, 0, 6,\n+                               8,  0, -1, -3, -1, -1, -3, 0,  5, 0,\n+                               -3, 0, 3,  -1, 2,  1,  -2, -3};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{0};\n+  Vector<int64_t> rhsb_dim{0};\n+  Vector<int64_t> lhsc_dim{2};\n+  Vector<int64_t> rhsc_dim{1};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{0,   -4, 12, -8,  10, 0,  -20,\n+                                    -18, 0,  13, -14, 0,  6,  12,\n+                                    2,   11, 17, 1,   -6, 11, -4};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedIntDotGeneralTest, IntTestTypesTensorsWork3) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({4, 2});\n+  const Shape shape_rhs({4, 2});\n+  const Shape shape_r({4, 4});\n+\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 4, 5, 6, 7, 8};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{2, 1, 1, 2, 2, 2, 1, 1};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{};\n+  Vector<int64_t> rhsb_dim{};\n+  Vector<int64_t> lhsc_dim{1};\n+  Vector<int64_t> rhsc_dim{1};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{4,  5,  6,  3,  10, 11, 14, 7,\n+                                    16, 17, 22, 11, 22, 23, 30, 15};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedIntDotGeneralTest, IntTestTypesTensorsWork4) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({1, 3, 4});\n+  const Shape shape_rhs({1, 4, 3});\n+  const Shape shape_r({1});\n+\n+  Vector<int64_t> lhs_data_int{2, 0, 0, 0, 5, -3, 0, 4, -1, 0, 0, -1};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{0, 4, 2, 3, 3, 3, -6, -2, 1, -1, 1, 0};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{0};\n+  Vector<int64_t> rhsb_dim{0};\n+  Vector<int64_t> lhsc_dim{2, 1};\n+  Vector<int64_t> rhsc_dim{1, 2};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{13};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+using kF32TestTypes = ::testing::Types<TestParam<DataType::kF32>>;\n+template <class T>\n+struct NonQuantizedkF32DotGeneralTest : ::testing::Test {};\n+\n+TYPED_TEST_SUITE(NonQuantizedkF32DotGeneralTest, kF32TestTypes, TestParamNames);\n+\n+TYPED_TEST(NonQuantizedkF32DotGeneralTest, kF32TestTypesTensorsWork1) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({4});\n+  const Shape shape_rhs({4});\n+  const Shape shape_r({1});\n+\n+  Vector<StorageT> lhs_data{-1.73818827, 6.32115507, 2.81545162, -1.37914991};\n+  Vector<StorageT> rhs_data{-4.02553225, -2.70646834, 3.14252234, 1.59961236};\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{};\n+  Vector<int64_t> rhsb_dim{};\n+  Vector<int64_t> lhsc_dim{0};\n+  Vector<int64_t> rhsc_dim{0};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data = {-3.46935892};\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(FloatNear(0.00001), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedkF32DotGeneralTest, kF32TestTypesTensorsWork2) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({8, 4, 3, 3, 4});\n+  const Shape shape_rhs({4, 8, 3, 4, 2});\n+  const Shape shape_r({8, 4, 3, 2});\n+\n+  Vector<StorageT> lhs_data{",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1586915825",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 65939,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general_test.cc",
        "discussion_id": "1586915825",
        "commented_code": "@@ -0,0 +1,1894 @@\n+/*Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/bf16.h\"\n+#include \"tensorflow/lite/experimental/shlo/f16.h\"\n+#include \"tensorflow/lite/experimental/shlo/i4.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/test_util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/status_matcher.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+using testing::Eq;\n+using testing::FloatEq;\n+using testing::FloatNear;\n+using testing::Pointwise;\n+namespace shlo_ref {\n+\n+namespace {\n+\n+template <class T>\n+struct NonQuantizedBoolDotGeneralTest : ::testing::Test {};\n+TYPED_TEST_SUITE(NonQuantizedBoolDotGeneralTest, BoolTestType, TestParamNames);\n+\n+TYPED_TEST(NonQuantizedBoolDotGeneralTest, BoolTestTypesTensorsWork1) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({7, 3, 4});\n+  const Shape shape_rhs({7, 4});\n+  const Shape shape_r({7, 3});\n+\n+  Vector<StorageT> lhs_data = {\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true};\n+  Vector<StorageT> rhs_data = {true, true, true, true, true, true, true,\n+                               true, true, true, true, true, true, true,\n+                               true, true, true, true, true, true, true,\n+                               true, true, true, true, true, true, true};\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{0};\n+  Vector<int64_t> rhsb_dim{0};\n+  Vector<int64_t> lhsc_dim{2};\n+  Vector<int64_t> rhsc_dim{1};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data = {true, true, true, true, true, true, true,\n+                                    true, true, true, true, true, true, true,\n+                                    true, true, true, true, true, true, true};\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedBoolDotGeneralTest, BoolTestTypesTensorsWork2) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({3, 4});\n+  const Shape shape_rhs({4, 2});\n+  const Shape shape_r({3, 2});\n+\n+  Vector<StorageT> lhs_data = {true, true,  true,  false, true, true,\n+                               true, false, false, true,  true, true};\n+  Vector<StorageT> rhs_data = {true, true,  true, false,\n+                               true, false, true, false};\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{};\n+  Vector<int64_t> rhsb_dim{};\n+  Vector<int64_t> lhsc_dim{1};\n+  Vector<int64_t> rhsc_dim{0};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data = {true, true, true, true, true, false};\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+template <class T>\n+struct NonQuantizedIntDotGeneralTest : ::testing::Test {};\n+TYPED_TEST_SUITE(NonQuantizedIntDotGeneralTest, IntTestTypes, TestParamNames);\n+\n+TYPED_TEST(NonQuantizedIntDotGeneralTest, IntTestTypesTensorsWork1) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({2, 2});\n+  const Shape shape_rhs({2, 2});\n+  const Shape shape_r({2, 2, 2, 2});\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 4};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{1, 0, 0, 1};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{};\n+  Vector<int64_t> rhsb_dim{};\n+  Vector<int64_t> lhsc_dim{};\n+  Vector<int64_t> rhsc_dim{};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{1, 0, 0, 1, 2, 0, 0, 2,\n+                                    3, 0, 0, 3, 4, 0, 0, 4};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedIntDotGeneralTest, IntTestTypesTensorsWork2) {\n+  using StorageT = typename TypeParam::StorageT;\n+  const Shape shape_lhs({7, 3, 4});\n+  const Shape shape_rhs({7, 4});\n+  const Shape shape_r({7, 3});\n+\n+  Vector<int64_t> lhs_data_int{\n+      0,  1,  4,  1,  -2, -3, 0, 0, 6,  -1, 0,  0,  1,  0,  -2, 0,  1,\n+      3,  4,  -6, 2,  4,  4,  0, 0, -2, -1, 1,  -2, -3, 0,  2,  -3, 0,\n+      0,  -2, 4,  -7, 2,  2,  0, 4, 2,  0,  -6, 1,  1,  2,  -2, -2, 0,\n+      -1, -4, -1, 0,  -1, 1,  3, 1, 1,  -4, 0,  0,  1,  -1, 0,  4,  -2,\n+      0,  5,  0,  -1, 0,  2,  1, 2, -1, 1,  -3, -2, -6, -3, -1, -3};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{2,  0, -1, 4,  -4, 0,  2,  -1, 0, 6,\n+                               8,  0, -1, -3, -1, -1, -3, 0,  5, 0,\n+                               -3, 0, 3,  -1, 2,  1,  -2, -3};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{0};\n+  Vector<int64_t> rhsb_dim{0};\n+  Vector<int64_t> lhsc_dim{2};\n+  Vector<int64_t> rhsc_dim{1};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{0,   -4, 12, -8,  10, 0,  -20,\n+                                    -18, 0,  13, -14, 0,  6,  12,\n+                                    2,   11, 17, 1,   -6, 11, -4};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedIntDotGeneralTest, IntTestTypesTensorsWork3) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({4, 2});\n+  const Shape shape_rhs({4, 2});\n+  const Shape shape_r({4, 4});\n+\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 4, 5, 6, 7, 8};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{2, 1, 1, 2, 2, 2, 1, 1};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{};\n+  Vector<int64_t> rhsb_dim{};\n+  Vector<int64_t> lhsc_dim{1};\n+  Vector<int64_t> rhsc_dim{1};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{4,  5,  6,  3,  10, 11, 14, 7,\n+                                    16, 17, 22, 11, 22, 23, 30, 15};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedIntDotGeneralTest, IntTestTypesTensorsWork4) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({1, 3, 4});\n+  const Shape shape_rhs({1, 4, 3});\n+  const Shape shape_r({1});\n+\n+  Vector<int64_t> lhs_data_int{2, 0, 0, 0, 5, -3, 0, 4, -1, 0, 0, -1};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{0, 4, 2, 3, 3, 3, -6, -2, 1, -1, 1, 0};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{0};\n+  Vector<int64_t> rhsb_dim{0};\n+  Vector<int64_t> lhsc_dim{2, 1};\n+  Vector<int64_t> rhsc_dim{1, 2};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{13};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+using kF32TestTypes = ::testing::Types<TestParam<DataType::kF32>>;\n+template <class T>\n+struct NonQuantizedkF32DotGeneralTest : ::testing::Test {};\n+\n+TYPED_TEST_SUITE(NonQuantizedkF32DotGeneralTest, kF32TestTypes, TestParamNames);\n+\n+TYPED_TEST(NonQuantizedkF32DotGeneralTest, kF32TestTypesTensorsWork1) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({4});\n+  const Shape shape_rhs({4});\n+  const Shape shape_r({1});\n+\n+  Vector<StorageT> lhs_data{-1.73818827, 6.32115507, 2.81545162, -1.37914991};\n+  Vector<StorageT> rhs_data{-4.02553225, -2.70646834, 3.14252234, 1.59961236};\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{};\n+  Vector<int64_t> rhsb_dim{};\n+  Vector<int64_t> lhsc_dim{0};\n+  Vector<int64_t> rhsc_dim{0};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data = {-3.46935892};\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(FloatNear(0.00001), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedkF32DotGeneralTest, kF32TestTypesTensorsWork2) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({8, 4, 3, 3, 4});\n+  const Shape shape_rhs({4, 8, 3, 4, 2});\n+  const Shape shape_r({8, 4, 3, 2});\n+\n+  Vector<StorageT> lhs_data{",
        "comment_created_at": "2024-05-01T23:21:57+00:00",
        "comment_author": "rascani",
        "comment_body": "I'd generally suggest moving the test data to a separate file with inline constexpr arrays that can be referenced from the tests. The large initialization blocks become distracting from reading the test bodies themselves.",
        "pr_file_module": null
      },
      {
        "comment_id": "1596645729",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 65939,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general_test.cc",
        "discussion_id": "1586915825",
        "commented_code": "@@ -0,0 +1,1894 @@\n+/*Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/bf16.h\"\n+#include \"tensorflow/lite/experimental/shlo/f16.h\"\n+#include \"tensorflow/lite/experimental/shlo/i4.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/test_util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/status_matcher.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+using testing::Eq;\n+using testing::FloatEq;\n+using testing::FloatNear;\n+using testing::Pointwise;\n+namespace shlo_ref {\n+\n+namespace {\n+\n+template <class T>\n+struct NonQuantizedBoolDotGeneralTest : ::testing::Test {};\n+TYPED_TEST_SUITE(NonQuantizedBoolDotGeneralTest, BoolTestType, TestParamNames);\n+\n+TYPED_TEST(NonQuantizedBoolDotGeneralTest, BoolTestTypesTensorsWork1) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({7, 3, 4});\n+  const Shape shape_rhs({7, 4});\n+  const Shape shape_r({7, 3});\n+\n+  Vector<StorageT> lhs_data = {\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true,\n+      true, true, true, true, true, true, true, true, true, true, true, true};\n+  Vector<StorageT> rhs_data = {true, true, true, true, true, true, true,\n+                               true, true, true, true, true, true, true,\n+                               true, true, true, true, true, true, true,\n+                               true, true, true, true, true, true, true};\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{0};\n+  Vector<int64_t> rhsb_dim{0};\n+  Vector<int64_t> lhsc_dim{2};\n+  Vector<int64_t> rhsc_dim{1};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data = {true, true, true, true, true, true, true,\n+                                    true, true, true, true, true, true, true,\n+                                    true, true, true, true, true, true, true};\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedBoolDotGeneralTest, BoolTestTypesTensorsWork2) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({3, 4});\n+  const Shape shape_rhs({4, 2});\n+  const Shape shape_r({3, 2});\n+\n+  Vector<StorageT> lhs_data = {true, true,  true,  false, true, true,\n+                               true, false, false, true,  true, true};\n+  Vector<StorageT> rhs_data = {true, true,  true, false,\n+                               true, false, true, false};\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{};\n+  Vector<int64_t> rhsb_dim{};\n+  Vector<int64_t> lhsc_dim{1};\n+  Vector<int64_t> rhsc_dim{0};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data = {true, true, true, true, true, false};\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+template <class T>\n+struct NonQuantizedIntDotGeneralTest : ::testing::Test {};\n+TYPED_TEST_SUITE(NonQuantizedIntDotGeneralTest, IntTestTypes, TestParamNames);\n+\n+TYPED_TEST(NonQuantizedIntDotGeneralTest, IntTestTypesTensorsWork1) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({2, 2});\n+  const Shape shape_rhs({2, 2});\n+  const Shape shape_r({2, 2, 2, 2});\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 4};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{1, 0, 0, 1};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{};\n+  Vector<int64_t> rhsb_dim{};\n+  Vector<int64_t> lhsc_dim{};\n+  Vector<int64_t> rhsc_dim{};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{1, 0, 0, 1, 2, 0, 0, 2,\n+                                    3, 0, 0, 3, 4, 0, 0, 4};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedIntDotGeneralTest, IntTestTypesTensorsWork2) {\n+  using StorageT = typename TypeParam::StorageT;\n+  const Shape shape_lhs({7, 3, 4});\n+  const Shape shape_rhs({7, 4});\n+  const Shape shape_r({7, 3});\n+\n+  Vector<int64_t> lhs_data_int{\n+      0,  1,  4,  1,  -2, -3, 0, 0, 6,  -1, 0,  0,  1,  0,  -2, 0,  1,\n+      3,  4,  -6, 2,  4,  4,  0, 0, -2, -1, 1,  -2, -3, 0,  2,  -3, 0,\n+      0,  -2, 4,  -7, 2,  2,  0, 4, 2,  0,  -6, 1,  1,  2,  -2, -2, 0,\n+      -1, -4, -1, 0,  -1, 1,  3, 1, 1,  -4, 0,  0,  1,  -1, 0,  4,  -2,\n+      0,  5,  0,  -1, 0,  2,  1, 2, -1, 1,  -3, -2, -6, -3, -1, -3};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{2,  0, -1, 4,  -4, 0,  2,  -1, 0, 6,\n+                               8,  0, -1, -3, -1, -1, -3, 0,  5, 0,\n+                               -3, 0, 3,  -1, 2,  1,  -2, -3};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{0};\n+  Vector<int64_t> rhsb_dim{0};\n+  Vector<int64_t> lhsc_dim{2};\n+  Vector<int64_t> rhsc_dim{1};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{0,   -4, 12, -8,  10, 0,  -20,\n+                                    -18, 0,  13, -14, 0,  6,  12,\n+                                    2,   11, 17, 1,   -6, 11, -4};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedIntDotGeneralTest, IntTestTypesTensorsWork3) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({4, 2});\n+  const Shape shape_rhs({4, 2});\n+  const Shape shape_r({4, 4});\n+\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 4, 5, 6, 7, 8};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{2, 1, 1, 2, 2, 2, 1, 1};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{};\n+  Vector<int64_t> rhsb_dim{};\n+  Vector<int64_t> lhsc_dim{1};\n+  Vector<int64_t> rhsc_dim{1};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{4,  5,  6,  3,  10, 11, 14, 7,\n+                                    16, 17, 22, 11, 22, 23, 30, 15};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedIntDotGeneralTest, IntTestTypesTensorsWork4) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({1, 3, 4});\n+  const Shape shape_rhs({1, 4, 3});\n+  const Shape shape_r({1});\n+\n+  Vector<int64_t> lhs_data_int{2, 0, 0, 0, 5, -3, 0, 4, -1, 0, 0, -1};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{0, 4, 2, 3, 3, 3, -6, -2, 1, -1, 1, 0};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{0};\n+  Vector<int64_t> rhsb_dim{0};\n+  Vector<int64_t> lhsc_dim{2, 1};\n+  Vector<int64_t> rhsc_dim{1, 2};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{13};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(Eq(), expected_data));\n+}\n+\n+using kF32TestTypes = ::testing::Types<TestParam<DataType::kF32>>;\n+template <class T>\n+struct NonQuantizedkF32DotGeneralTest : ::testing::Test {};\n+\n+TYPED_TEST_SUITE(NonQuantizedkF32DotGeneralTest, kF32TestTypes, TestParamNames);\n+\n+TYPED_TEST(NonQuantizedkF32DotGeneralTest, kF32TestTypesTensorsWork1) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({4});\n+  const Shape shape_rhs({4});\n+  const Shape shape_r({1});\n+\n+  Vector<StorageT> lhs_data{-1.73818827, 6.32115507, 2.81545162, -1.37914991};\n+  Vector<StorageT> rhs_data{-4.02553225, -2.70646834, 3.14252234, 1.59961236};\n+  Vector<StorageT> output_data(shape_r.NumElements());\n+  Vector<int64_t> lhsb_dim{};\n+  Vector<int64_t> rhsb_dim{};\n+  Vector<int64_t> lhsc_dim{0};\n+  Vector<int64_t> rhsc_dim{0};\n+  absl::Span<int64_t> lhs_batching_dimensions(lhsb_dim);\n+  absl::Span<int64_t> rhs_batching_dimensions(rhsb_dim);\n+  absl::Span<int64_t> lhs_contracting_dimensions(lhsc_dim);\n+  absl::Span<int64_t> rhs_contracting_dimensions(rhsc_dim);\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor output_tensor{\n+      .type = TensorType{.shape = shape_r, .element_type = TypeParam::kStorage},\n+      .data = output_data.data()};\n+  std::array<PrecisionTypes, 2> precision_configs = {PrecisionTypes::DEFAULT,\n+                                                     PrecisionTypes::DEFAULT};\n+\n+  auto op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data = {-3.46935892};\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(FloatNear(0.00001), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedkF32DotGeneralTest, kF32TestTypesTensorsWork2) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({8, 4, 3, 3, 4});\n+  const Shape shape_rhs({4, 8, 3, 4, 2});\n+  const Shape shape_r({8, 4, 3, 2});\n+\n+  Vector<StorageT> lhs_data{",
        "comment_created_at": "2024-05-10T11:37:04+00:00",
        "comment_author": "nishantsarda-mcw",
        "comment_body": "We have made the required changes, will update the same in the next commit.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1577014057",
    "pr_number": 66299,
    "pr_file": "tensorflow/lite/experimental/shlo/ops/convolution_test.cc",
    "created_at": "2024-04-23T23:26:27+00:00",
    "commented_code": "+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/convolution.h\"\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include <cmath>\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/bf16.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/f16.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/test_util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/status_matcher.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+using shlo_ref::testing::StatusIs;\n+using testing::FloatEq;\n+using testing::Pointwise;\n+namespace shlo_ref {\n+\n+namespace {\n+template <class T>\n+struct NonQuantizedFloatConvolutionTest : ::testing::Test {};\n+\n+TYPED_TEST_SUITE(NonQuantizedFloatConvolutionTest, FloatTestTypes,\n+                 TestParamNames);\n+\n+TYPED_TEST(NonQuantizedFloatConvolutionTest, FloatTestTypesTensorsWork) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({1, 1, 2, 2});\n+  const Shape shape_rhs({1, 1, 1, 1});\n+  const Shape shape_padding({2, 2});\n+  const Shape shape_parametrs({2});\n+  const Shape shape_result({1, 1, 2, 2});\n+\n+  Vector<float> lhs_data_float{1.16, 2.43, 3.81, 4.77};\n+  Vector<StorageT> lhs_data(lhs_data_float.begin(), lhs_data_float.end());\n+  Vector<float> rhs_data_float{2.21};\n+  Vector<StorageT> rhs_data(rhs_data_float.begin(), rhs_data_float.end());\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({1, 1});\n+  Vector<int64_t> padding_values({0, 0, 0, 0});\n+  Vector<int64_t> lhs_dilation_values({1, 1});\n+  Vector<int64_t> rhs_dilation_values({1, 1});\n+  Vector<bool> window_reversal_values({false, false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2, 3});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2, 3});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2, 3});\n+  int64_t feature_group_count = 1;\n+  int64_t batch_group_count = 1;\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+\n+  Tensor output_tensor{.type = TensorType{.shape = shape_result,\n+                                          .element_type = TypeParam::kStorage},\n+                       .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data;\n+  if (std::is_same<StorageT, BF16>::value) {\n+    Vector<float> expected_data_float = {2.54688, 5.375, 8.375, 10.5625};\n+    expected_data.assign(expected_data_float.begin(),\n+                         expected_data_float.end());\n+  } else if (std::is_same<StorageT, F16>::value) {\n+    Vector<float> expected_data_float = {2.56445, 5.37109, 8.42188, 10.5469};\n+    expected_data.assign(expected_data_float.begin(),\n+                         expected_data_float.end());\n+  } else {\n+    Vector<float> expected_data_float = {2.5636, 5.3703, 8.4201, 10.5417};\n+    expected_data.assign(expected_data_float.begin(),\n+                         expected_data_float.end());\n+  }\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(FloatEq(), expected_data));\n+}\n+\n+template <class T>\n+struct NonQuantizedIntConvolutionTest : ::testing::Test {};\n+\n+TYPED_TEST_SUITE(NonQuantizedIntConvolutionTest, IntTestTypes, TestParamNames);\n+\n+TYPED_TEST(NonQuantizedIntConvolutionTest, IntTestTypesTensorsWork) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({1, 4, 4, 1});\n+  const Shape shape_rhs({4, 2, 1, 1});\n+  const Shape shape_padding({2, 2});\n+  const Shape shape_parametrs({2});\n+  const Shape shape_result({1, 4, 2, 1});\n+\n+  Vector<int64_t> lhs_data_int{1, 3, 10, 12, 2, 4, 11, 13,\n+                               5, 7, 14, 16, 6, 8, 15, 17};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{1, 1, 1, 1, 1, 1, 1, 1};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({4, 4});\n+  Vector<int64_t> padding_values({0, 0, 0, 0});\n+  Vector<int64_t> lhs_dilation_values({2, 2});\n+  Vector<int64_t> rhs_dilation_values({1, 1});\n+  Vector<bool> window_reversal_values({false, false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2, 3});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2, 3});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2, 3});\n+  int64_t feature_group_count = 2;\n+  int64_t batch_group_count = 1;\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+\n+  Tensor output_tensor{.type = TensorType{.shape = shape_result,\n+                                          .element_type = TypeParam::kStorage},\n+                       .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{3, 21, 3, 21, 11, 29, 11, 29};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(FloatEq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedIntConvolutionTest, IntTestTypesTensorsWork1) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({1, 1, 10});\n+  const Shape shape_rhs({1, 1, 1});\n+  const Shape shape_padding({1, 2});\n+  const Shape shape_parametrs({1});\n+  const Shape shape_result({1, 1, 10});\n+\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 4, 5, 6, 7, 9, 4, 2};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{5};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({1});\n+  Vector<int64_t> padding_values({0, 0});\n+  Vector<int64_t> lhs_dilation_values({1});\n+  Vector<int64_t> rhs_dilation_values({1});\n+  Vector<bool> window_reversal_values({false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2});\n+  int64_t feature_group_count = 1;\n+  int64_t batch_group_count = 1;\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+\n+  Tensor output_tensor{.type = TensorType{.shape = shape_result,\n+                                          .element_type = TypeParam::kStorage},\n+                       .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{5, 10, 15, 20, 25, 30, 35, 45, 20, 10};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(FloatEq(), expected_data));\n+}\n+\n+template <class T>\n+struct QuantizedIntConvolutionTest : ::testing::Test {};\n+TYPED_TEST_SUITE(QuantizedIntConvolutionTest, QuantizedTestTypes,\n+                 TestParamNames);\n+\n+TYPED_TEST(QuantizedIntConvolutionTest, PerTensorsRaiseAnError) {\n+  using StorageT = typename TypeParam::StorageT;\n+  using ExpressedT = typename TypeParam::ExpressedT;\n+\n+  const Shape shape_lhs({1, 1, 10});\n+  const Shape shape_rhs({1, 1, 1});\n+  const Shape shape_padding({1, 2});\n+  const Shape shape_parametrs({1});\n+  const Shape shape_result({1, 1, 10});\n+\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 1, 2, 3, 1, 2, 3, 2};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<StorageT> rhs_data = Vector<StorageT>{1};\n+\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({1});\n+  Vector<int64_t> padding_values({0, 0});\n+  Vector<int64_t> lhs_dilation_values({1});\n+  Vector<int64_t> rhs_dilation_values({1});\n+  Vector<bool> window_reversal_values({false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2});\n+  int64_t feature_group_count = 1;\n+  int64_t batch_group_count = 1;\n+\n+  const ExpressedT scale = static_cast<ExpressedT>(1);\n+  const StorageT zero_point = static_cast<StorageT>(0);\n+\n+  const QuantizedElementTypePerTensor tensor_type =\n+      QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,\n+                                    TypeParam::kExpressed, scale);\n+\n+  Tensor lhs{.type = QuantizedPerTensorTensorType{.shape = shape_lhs,\n+                                                  .element_type = tensor_type},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = QuantizedPerTensorTensorType{.shape = shape_rhs,\n+                                                  .element_type = tensor_type},\n+             .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+  Tensor output_tensor{\n+      .type = QuantizedPerTensorTensorType{.shape = shape_result,\n+                                           .element_type = tensor_type},\n+      .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+  Vector<StorageT> expected_data =\n+      Vector<StorageT>{1, 2, 3, 1, 2, 3, 1, 2, 3, 2};\n+\n+  Vector<StorageT> expected_data_quantized(shape_result.NumElements());\n+  std::transform(expected_data.begin(), expected_data.end(),\n+                 expected_data_quantized.begin(), [&](StorageT val) {\n+                   return Quantize<TypeParam::kStorage, TypeParam::kExpressed>(\n+                       static_cast<ExpressedT>(val), zero_point,\n+                       static_cast<ExpressedT>(1.0) / scale);\n+                 });\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+\n+  constexpr double kEpsilon = 0.1;\n+  EXPECT_THAT(output_data, Pointwise(FloatEq(), expected_data));\n+}\n+\n+TYPED_TEST(QuantizedIntConvolutionTest, PerAxisRaiseAnError) {\n+  using StorageT = typename TypeParam::StorageT;\n+  using ExpressedT = typename TypeParam::ExpressedT;\n+\n+  const Shape shape_lhs({1, 1, 10});\n+  const Shape shape_rhs({1, 1, 1});\n+  const Shape shape_padding({1, 2});\n+  const Shape shape_parametrs({1});\n+  const Shape shape_result({1, 1, 10});\n+\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 4, 5, 1, 2, 3, 4, -5};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+\n+  Vector<StorageT> rhs_data = Vector<StorageT>{5};\n+\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({1});\n+  Vector<int64_t> padding_values({0, 0});\n+  Vector<int64_t> lhs_dilation_values({1});\n+  Vector<int64_t> rhs_dilation_values({1});\n+  Vector<bool> window_reversal_values({false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2});\n+  int64_t feature_group_count = 1;\n+  int64_t batch_group_count = 1;\n+\n+  std::initializer_list<float> zero_points = {0, 0, 0};\n+  std::initializer_list<float> scales = {1.7, 1.6, 1.5};\n+\n+  const ExpressedT scale = static_cast<ExpressedT>(1);\n+  const StorageT zero_point = static_cast<StorageT>(0);\n+\n+  QuantizedElementTypePerTensor tensor_type = QuantizedElementTypePerTensor(\n+      TypeParam::kStorage, zero_point, TypeParam::kExpressed, scale);\n+\n+  QuantizedElementTypePerAxis tensor_type_axis(\n+      TypeParam::kStorage, zero_points, TypeParam::kExpressed, scales, 0);\n+\n+  QuantizedElementTypePerAxis tensor_type_axis_res(\n+      TypeParam::kStorage, zero_points, TypeParam::kExpressed, scales, 1);\n+\n+  Tensor lhs{.type = QuantizedPerTensorTensorType{.shape = shape_lhs,\n+                                                  .element_type = tensor_type},\n+             .data = lhs_data.data()};\n+  Tensor rhs{\n+      .type = QuantizedPerAxisTensorType{.shape = shape_rhs,\n+                                         .element_type = tensor_type_axis},\n+      .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+  Tensor output_tensor{\n+      .type = QuantizedPerAxisTensorType{.shape = shape_result,\n+                                         .element_type = tensor_type_axis_res},\n+      .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data =\n+      Vector<StorageT>{5, 10, 15, 20, 25, 5, 10, 15, 20, -25};\n+  ;\n+  if (std::is_same<StorageT, I4>::value) {",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1577014057",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 66299,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/convolution_test.cc",
        "discussion_id": "1577014057",
        "commented_code": "@@ -0,0 +1,615 @@\n+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/convolution.h\"\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include <cmath>\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/bf16.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/f16.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/test_util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/status_matcher.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+using shlo_ref::testing::StatusIs;\n+using testing::FloatEq;\n+using testing::Pointwise;\n+namespace shlo_ref {\n+\n+namespace {\n+template <class T>\n+struct NonQuantizedFloatConvolutionTest : ::testing::Test {};\n+\n+TYPED_TEST_SUITE(NonQuantizedFloatConvolutionTest, FloatTestTypes,\n+                 TestParamNames);\n+\n+TYPED_TEST(NonQuantizedFloatConvolutionTest, FloatTestTypesTensorsWork) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({1, 1, 2, 2});\n+  const Shape shape_rhs({1, 1, 1, 1});\n+  const Shape shape_padding({2, 2});\n+  const Shape shape_parametrs({2});\n+  const Shape shape_result({1, 1, 2, 2});\n+\n+  Vector<float> lhs_data_float{1.16, 2.43, 3.81, 4.77};\n+  Vector<StorageT> lhs_data(lhs_data_float.begin(), lhs_data_float.end());\n+  Vector<float> rhs_data_float{2.21};\n+  Vector<StorageT> rhs_data(rhs_data_float.begin(), rhs_data_float.end());\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({1, 1});\n+  Vector<int64_t> padding_values({0, 0, 0, 0});\n+  Vector<int64_t> lhs_dilation_values({1, 1});\n+  Vector<int64_t> rhs_dilation_values({1, 1});\n+  Vector<bool> window_reversal_values({false, false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2, 3});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2, 3});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2, 3});\n+  int64_t feature_group_count = 1;\n+  int64_t batch_group_count = 1;\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+\n+  Tensor output_tensor{.type = TensorType{.shape = shape_result,\n+                                          .element_type = TypeParam::kStorage},\n+                       .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data;\n+  if (std::is_same<StorageT, BF16>::value) {\n+    Vector<float> expected_data_float = {2.54688, 5.375, 8.375, 10.5625};\n+    expected_data.assign(expected_data_float.begin(),\n+                         expected_data_float.end());\n+  } else if (std::is_same<StorageT, F16>::value) {\n+    Vector<float> expected_data_float = {2.56445, 5.37109, 8.42188, 10.5469};\n+    expected_data.assign(expected_data_float.begin(),\n+                         expected_data_float.end());\n+  } else {\n+    Vector<float> expected_data_float = {2.5636, 5.3703, 8.4201, 10.5417};\n+    expected_data.assign(expected_data_float.begin(),\n+                         expected_data_float.end());\n+  }\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(FloatEq(), expected_data));\n+}\n+\n+template <class T>\n+struct NonQuantizedIntConvolutionTest : ::testing::Test {};\n+\n+TYPED_TEST_SUITE(NonQuantizedIntConvolutionTest, IntTestTypes, TestParamNames);\n+\n+TYPED_TEST(NonQuantizedIntConvolutionTest, IntTestTypesTensorsWork) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({1, 4, 4, 1});\n+  const Shape shape_rhs({4, 2, 1, 1});\n+  const Shape shape_padding({2, 2});\n+  const Shape shape_parametrs({2});\n+  const Shape shape_result({1, 4, 2, 1});\n+\n+  Vector<int64_t> lhs_data_int{1, 3, 10, 12, 2, 4, 11, 13,\n+                               5, 7, 14, 16, 6, 8, 15, 17};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{1, 1, 1, 1, 1, 1, 1, 1};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({4, 4});\n+  Vector<int64_t> padding_values({0, 0, 0, 0});\n+  Vector<int64_t> lhs_dilation_values({2, 2});\n+  Vector<int64_t> rhs_dilation_values({1, 1});\n+  Vector<bool> window_reversal_values({false, false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2, 3});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2, 3});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2, 3});\n+  int64_t feature_group_count = 2;\n+  int64_t batch_group_count = 1;\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+\n+  Tensor output_tensor{.type = TensorType{.shape = shape_result,\n+                                          .element_type = TypeParam::kStorage},\n+                       .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{3, 21, 3, 21, 11, 29, 11, 29};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(FloatEq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedIntConvolutionTest, IntTestTypesTensorsWork1) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({1, 1, 10});\n+  const Shape shape_rhs({1, 1, 1});\n+  const Shape shape_padding({1, 2});\n+  const Shape shape_parametrs({1});\n+  const Shape shape_result({1, 1, 10});\n+\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 4, 5, 6, 7, 9, 4, 2};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{5};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({1});\n+  Vector<int64_t> padding_values({0, 0});\n+  Vector<int64_t> lhs_dilation_values({1});\n+  Vector<int64_t> rhs_dilation_values({1});\n+  Vector<bool> window_reversal_values({false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2});\n+  int64_t feature_group_count = 1;\n+  int64_t batch_group_count = 1;\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+\n+  Tensor output_tensor{.type = TensorType{.shape = shape_result,\n+                                          .element_type = TypeParam::kStorage},\n+                       .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{5, 10, 15, 20, 25, 30, 35, 45, 20, 10};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(FloatEq(), expected_data));\n+}\n+\n+template <class T>\n+struct QuantizedIntConvolutionTest : ::testing::Test {};\n+TYPED_TEST_SUITE(QuantizedIntConvolutionTest, QuantizedTestTypes,\n+                 TestParamNames);\n+\n+TYPED_TEST(QuantizedIntConvolutionTest, PerTensorsRaiseAnError) {\n+  using StorageT = typename TypeParam::StorageT;\n+  using ExpressedT = typename TypeParam::ExpressedT;\n+\n+  const Shape shape_lhs({1, 1, 10});\n+  const Shape shape_rhs({1, 1, 1});\n+  const Shape shape_padding({1, 2});\n+  const Shape shape_parametrs({1});\n+  const Shape shape_result({1, 1, 10});\n+\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 1, 2, 3, 1, 2, 3, 2};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<StorageT> rhs_data = Vector<StorageT>{1};\n+\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({1});\n+  Vector<int64_t> padding_values({0, 0});\n+  Vector<int64_t> lhs_dilation_values({1});\n+  Vector<int64_t> rhs_dilation_values({1});\n+  Vector<bool> window_reversal_values({false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2});\n+  int64_t feature_group_count = 1;\n+  int64_t batch_group_count = 1;\n+\n+  const ExpressedT scale = static_cast<ExpressedT>(1);\n+  const StorageT zero_point = static_cast<StorageT>(0);\n+\n+  const QuantizedElementTypePerTensor tensor_type =\n+      QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,\n+                                    TypeParam::kExpressed, scale);\n+\n+  Tensor lhs{.type = QuantizedPerTensorTensorType{.shape = shape_lhs,\n+                                                  .element_type = tensor_type},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = QuantizedPerTensorTensorType{.shape = shape_rhs,\n+                                                  .element_type = tensor_type},\n+             .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+  Tensor output_tensor{\n+      .type = QuantizedPerTensorTensorType{.shape = shape_result,\n+                                           .element_type = tensor_type},\n+      .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+  Vector<StorageT> expected_data =\n+      Vector<StorageT>{1, 2, 3, 1, 2, 3, 1, 2, 3, 2};\n+\n+  Vector<StorageT> expected_data_quantized(shape_result.NumElements());\n+  std::transform(expected_data.begin(), expected_data.end(),\n+                 expected_data_quantized.begin(), [&](StorageT val) {\n+                   return Quantize<TypeParam::kStorage, TypeParam::kExpressed>(\n+                       static_cast<ExpressedT>(val), zero_point,\n+                       static_cast<ExpressedT>(1.0) / scale);\n+                 });\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+\n+  constexpr double kEpsilon = 0.1;\n+  EXPECT_THAT(output_data, Pointwise(FloatEq(), expected_data));\n+}\n+\n+TYPED_TEST(QuantizedIntConvolutionTest, PerAxisRaiseAnError) {\n+  using StorageT = typename TypeParam::StorageT;\n+  using ExpressedT = typename TypeParam::ExpressedT;\n+\n+  const Shape shape_lhs({1, 1, 10});\n+  const Shape shape_rhs({1, 1, 1});\n+  const Shape shape_padding({1, 2});\n+  const Shape shape_parametrs({1});\n+  const Shape shape_result({1, 1, 10});\n+\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 4, 5, 1, 2, 3, 4, -5};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+\n+  Vector<StorageT> rhs_data = Vector<StorageT>{5};\n+\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({1});\n+  Vector<int64_t> padding_values({0, 0});\n+  Vector<int64_t> lhs_dilation_values({1});\n+  Vector<int64_t> rhs_dilation_values({1});\n+  Vector<bool> window_reversal_values({false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2});\n+  int64_t feature_group_count = 1;\n+  int64_t batch_group_count = 1;\n+\n+  std::initializer_list<float> zero_points = {0, 0, 0};\n+  std::initializer_list<float> scales = {1.7, 1.6, 1.5};\n+\n+  const ExpressedT scale = static_cast<ExpressedT>(1);\n+  const StorageT zero_point = static_cast<StorageT>(0);\n+\n+  QuantizedElementTypePerTensor tensor_type = QuantizedElementTypePerTensor(\n+      TypeParam::kStorage, zero_point, TypeParam::kExpressed, scale);\n+\n+  QuantizedElementTypePerAxis tensor_type_axis(\n+      TypeParam::kStorage, zero_points, TypeParam::kExpressed, scales, 0);\n+\n+  QuantizedElementTypePerAxis tensor_type_axis_res(\n+      TypeParam::kStorage, zero_points, TypeParam::kExpressed, scales, 1);\n+\n+  Tensor lhs{.type = QuantizedPerTensorTensorType{.shape = shape_lhs,\n+                                                  .element_type = tensor_type},\n+             .data = lhs_data.data()};\n+  Tensor rhs{\n+      .type = QuantizedPerAxisTensorType{.shape = shape_rhs,\n+                                         .element_type = tensor_type_axis},\n+      .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+  Tensor output_tensor{\n+      .type = QuantizedPerAxisTensorType{.shape = shape_result,\n+                                         .element_type = tensor_type_axis_res},\n+      .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data =\n+      Vector<StorageT>{5, 10, 15, 20, 25, 5, 10, 15, 20, -25};\n+  ;\n+  if (std::is_same<StorageT, I4>::value) {",
        "comment_created_at": "2024-04-23T23:26:27+00:00",
        "comment_author": "rascani",
        "comment_body": "I'd suggest splitting out a separate test case for i4 where we can ensure the input and expected data are within the i4 range without having to clamp most of the values.",
        "pr_file_module": null
      },
      {
        "comment_id": "1596709108",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 66299,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/convolution_test.cc",
        "discussion_id": "1577014057",
        "commented_code": "@@ -0,0 +1,615 @@\n+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/convolution.h\"\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include <cmath>\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/bf16.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/f16.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/test_util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/status_matcher.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+using shlo_ref::testing::StatusIs;\n+using testing::FloatEq;\n+using testing::Pointwise;\n+namespace shlo_ref {\n+\n+namespace {\n+template <class T>\n+struct NonQuantizedFloatConvolutionTest : ::testing::Test {};\n+\n+TYPED_TEST_SUITE(NonQuantizedFloatConvolutionTest, FloatTestTypes,\n+                 TestParamNames);\n+\n+TYPED_TEST(NonQuantizedFloatConvolutionTest, FloatTestTypesTensorsWork) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({1, 1, 2, 2});\n+  const Shape shape_rhs({1, 1, 1, 1});\n+  const Shape shape_padding({2, 2});\n+  const Shape shape_parametrs({2});\n+  const Shape shape_result({1, 1, 2, 2});\n+\n+  Vector<float> lhs_data_float{1.16, 2.43, 3.81, 4.77};\n+  Vector<StorageT> lhs_data(lhs_data_float.begin(), lhs_data_float.end());\n+  Vector<float> rhs_data_float{2.21};\n+  Vector<StorageT> rhs_data(rhs_data_float.begin(), rhs_data_float.end());\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({1, 1});\n+  Vector<int64_t> padding_values({0, 0, 0, 0});\n+  Vector<int64_t> lhs_dilation_values({1, 1});\n+  Vector<int64_t> rhs_dilation_values({1, 1});\n+  Vector<bool> window_reversal_values({false, false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2, 3});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2, 3});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2, 3});\n+  int64_t feature_group_count = 1;\n+  int64_t batch_group_count = 1;\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+\n+  Tensor output_tensor{.type = TensorType{.shape = shape_result,\n+                                          .element_type = TypeParam::kStorage},\n+                       .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data;\n+  if (std::is_same<StorageT, BF16>::value) {\n+    Vector<float> expected_data_float = {2.54688, 5.375, 8.375, 10.5625};\n+    expected_data.assign(expected_data_float.begin(),\n+                         expected_data_float.end());\n+  } else if (std::is_same<StorageT, F16>::value) {\n+    Vector<float> expected_data_float = {2.56445, 5.37109, 8.42188, 10.5469};\n+    expected_data.assign(expected_data_float.begin(),\n+                         expected_data_float.end());\n+  } else {\n+    Vector<float> expected_data_float = {2.5636, 5.3703, 8.4201, 10.5417};\n+    expected_data.assign(expected_data_float.begin(),\n+                         expected_data_float.end());\n+  }\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(FloatEq(), expected_data));\n+}\n+\n+template <class T>\n+struct NonQuantizedIntConvolutionTest : ::testing::Test {};\n+\n+TYPED_TEST_SUITE(NonQuantizedIntConvolutionTest, IntTestTypes, TestParamNames);\n+\n+TYPED_TEST(NonQuantizedIntConvolutionTest, IntTestTypesTensorsWork) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({1, 4, 4, 1});\n+  const Shape shape_rhs({4, 2, 1, 1});\n+  const Shape shape_padding({2, 2});\n+  const Shape shape_parametrs({2});\n+  const Shape shape_result({1, 4, 2, 1});\n+\n+  Vector<int64_t> lhs_data_int{1, 3, 10, 12, 2, 4, 11, 13,\n+                               5, 7, 14, 16, 6, 8, 15, 17};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{1, 1, 1, 1, 1, 1, 1, 1};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({4, 4});\n+  Vector<int64_t> padding_values({0, 0, 0, 0});\n+  Vector<int64_t> lhs_dilation_values({2, 2});\n+  Vector<int64_t> rhs_dilation_values({1, 1});\n+  Vector<bool> window_reversal_values({false, false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2, 3});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2, 3});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2, 3});\n+  int64_t feature_group_count = 2;\n+  int64_t batch_group_count = 1;\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+\n+  Tensor output_tensor{.type = TensorType{.shape = shape_result,\n+                                          .element_type = TypeParam::kStorage},\n+                       .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{3, 21, 3, 21, 11, 29, 11, 29};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(FloatEq(), expected_data));\n+}\n+\n+TYPED_TEST(NonQuantizedIntConvolutionTest, IntTestTypesTensorsWork1) {\n+  using StorageT = typename TypeParam::StorageT;\n+\n+  const Shape shape_lhs({1, 1, 10});\n+  const Shape shape_rhs({1, 1, 1});\n+  const Shape shape_padding({1, 2});\n+  const Shape shape_parametrs({1});\n+  const Shape shape_result({1, 1, 10});\n+\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 4, 5, 6, 7, 9, 4, 2};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<int64_t> rhs_data_int{5};\n+  Vector<StorageT> rhs_data(rhs_data_int.begin(), rhs_data_int.end());\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({1});\n+  Vector<int64_t> padding_values({0, 0});\n+  Vector<int64_t> lhs_dilation_values({1});\n+  Vector<int64_t> rhs_dilation_values({1});\n+  Vector<bool> window_reversal_values({false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2});\n+  int64_t feature_group_count = 1;\n+  int64_t batch_group_count = 1;\n+\n+  Tensor lhs{.type = TensorType{.shape = shape_lhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = TensorType{.shape = shape_rhs,\n+                                .element_type = TypeParam::kStorage},\n+             .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+\n+  Tensor output_tensor{.type = TensorType{.shape = shape_result,\n+                                          .element_type = TypeParam::kStorage},\n+                       .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+\n+  Vector<int64_t> expected_data_int{5, 10, 15, 20, 25, 30, 35, 45, 20, 10};\n+  Vector<StorageT> expected_data(expected_data_int.begin(),\n+                                 expected_data_int.end());\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+  EXPECT_THAT(output_data, Pointwise(FloatEq(), expected_data));\n+}\n+\n+template <class T>\n+struct QuantizedIntConvolutionTest : ::testing::Test {};\n+TYPED_TEST_SUITE(QuantizedIntConvolutionTest, QuantizedTestTypes,\n+                 TestParamNames);\n+\n+TYPED_TEST(QuantizedIntConvolutionTest, PerTensorsRaiseAnError) {\n+  using StorageT = typename TypeParam::StorageT;\n+  using ExpressedT = typename TypeParam::ExpressedT;\n+\n+  const Shape shape_lhs({1, 1, 10});\n+  const Shape shape_rhs({1, 1, 1});\n+  const Shape shape_padding({1, 2});\n+  const Shape shape_parametrs({1});\n+  const Shape shape_result({1, 1, 10});\n+\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 1, 2, 3, 1, 2, 3, 2};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+  Vector<StorageT> rhs_data = Vector<StorageT>{1};\n+\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({1});\n+  Vector<int64_t> padding_values({0, 0});\n+  Vector<int64_t> lhs_dilation_values({1});\n+  Vector<int64_t> rhs_dilation_values({1});\n+  Vector<bool> window_reversal_values({false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2});\n+  int64_t feature_group_count = 1;\n+  int64_t batch_group_count = 1;\n+\n+  const ExpressedT scale = static_cast<ExpressedT>(1);\n+  const StorageT zero_point = static_cast<StorageT>(0);\n+\n+  const QuantizedElementTypePerTensor tensor_type =\n+      QuantizedElementTypePerTensor(TypeParam::kStorage, zero_point,\n+                                    TypeParam::kExpressed, scale);\n+\n+  Tensor lhs{.type = QuantizedPerTensorTensorType{.shape = shape_lhs,\n+                                                  .element_type = tensor_type},\n+             .data = lhs_data.data()};\n+  Tensor rhs{.type = QuantizedPerTensorTensorType{.shape = shape_rhs,\n+                                                  .element_type = tensor_type},\n+             .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+  Tensor output_tensor{\n+      .type = QuantizedPerTensorTensorType{.shape = shape_result,\n+                                           .element_type = tensor_type},\n+      .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+  Vector<StorageT> expected_data =\n+      Vector<StorageT>{1, 2, 3, 1, 2, 3, 1, 2, 3, 2};\n+\n+  Vector<StorageT> expected_data_quantized(shape_result.NumElements());\n+  std::transform(expected_data.begin(), expected_data.end(),\n+                 expected_data_quantized.begin(), [&](StorageT val) {\n+                   return Quantize<TypeParam::kStorage, TypeParam::kExpressed>(\n+                       static_cast<ExpressedT>(val), zero_point,\n+                       static_cast<ExpressedT>(1.0) / scale);\n+                 });\n+\n+  ASSERT_OK(Prepare(op, lhs, rhs, output_tensor));\n+  ASSERT_OK(Evaluate(op, lhs, rhs, output_tensor));\n+\n+  constexpr double kEpsilon = 0.1;\n+  EXPECT_THAT(output_data, Pointwise(FloatEq(), expected_data));\n+}\n+\n+TYPED_TEST(QuantizedIntConvolutionTest, PerAxisRaiseAnError) {\n+  using StorageT = typename TypeParam::StorageT;\n+  using ExpressedT = typename TypeParam::ExpressedT;\n+\n+  const Shape shape_lhs({1, 1, 10});\n+  const Shape shape_rhs({1, 1, 1});\n+  const Shape shape_padding({1, 2});\n+  const Shape shape_parametrs({1});\n+  const Shape shape_result({1, 1, 10});\n+\n+  Vector<int64_t> lhs_data_int{1, 2, 3, 4, 5, 1, 2, 3, 4, -5};\n+  Vector<StorageT> lhs_data(lhs_data_int.begin(), lhs_data_int.end());\n+\n+  Vector<StorageT> rhs_data = Vector<StorageT>{5};\n+\n+  Vector<StorageT> output_data(shape_result.NumElements());\n+  Vector<int64_t> window_stride_values({1});\n+  Vector<int64_t> padding_values({0, 0});\n+  Vector<int64_t> lhs_dilation_values({1});\n+  Vector<int64_t> rhs_dilation_values({1});\n+  Vector<bool> window_reversal_values({false});\n+  int64_t input_batch_dimension = 0;\n+  int64_t input_feature_dimension = 1;\n+  Vector<int64_t> inputSpatialDimensions_values({2});\n+  int64_t kernel_input_feature_dimension = 1;\n+  int64_t kernel_output_feature_dimension = 0;\n+  Vector<int64_t> kernel_spatial_dimensions_values({2});\n+  int64_t output_batch_dimension = 0;\n+  int64_t output_feature_dimension = 1;\n+  Vector<int64_t> output_spatial_dimensions_values({2});\n+  int64_t feature_group_count = 1;\n+  int64_t batch_group_count = 1;\n+\n+  std::initializer_list<float> zero_points = {0, 0, 0};\n+  std::initializer_list<float> scales = {1.7, 1.6, 1.5};\n+\n+  const ExpressedT scale = static_cast<ExpressedT>(1);\n+  const StorageT zero_point = static_cast<StorageT>(0);\n+\n+  QuantizedElementTypePerTensor tensor_type = QuantizedElementTypePerTensor(\n+      TypeParam::kStorage, zero_point, TypeParam::kExpressed, scale);\n+\n+  QuantizedElementTypePerAxis tensor_type_axis(\n+      TypeParam::kStorage, zero_points, TypeParam::kExpressed, scales, 0);\n+\n+  QuantizedElementTypePerAxis tensor_type_axis_res(\n+      TypeParam::kStorage, zero_points, TypeParam::kExpressed, scales, 1);\n+\n+  Tensor lhs{.type = QuantizedPerTensorTensorType{.shape = shape_lhs,\n+                                                  .element_type = tensor_type},\n+             .data = lhs_data.data()};\n+  Tensor rhs{\n+      .type = QuantizedPerAxisTensorType{.shape = shape_rhs,\n+                                         .element_type = tensor_type_axis},\n+      .data = rhs_data.data()};\n+  Tensor window_stride{.type = TensorType{.shape = shape_parametrs,\n+                                          .element_type = DataType::kSI64},\n+                       .data = window_stride_values.data()};\n+  Tensor padding{.type = TensorType{.shape = shape_padding,\n+                                    .element_type = DataType::kSI64},\n+                 .data = padding_values.data()};\n+  Tensor lhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = lhs_dilation_values.data()};\n+  Tensor rhs_dilation{.type = TensorType{.shape = shape_parametrs,\n+                                         .element_type = DataType::kSI64},\n+                      .data = rhs_dilation_values.data()};\n+  Tensor window_reversal{.type = TensorType{.shape = shape_parametrs,\n+                                            .element_type = DataType::kI1},\n+                         .data = window_reversal_values.data()};\n+  Tensor inputSpatialDimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = inputSpatialDimensions_values.data()};\n+  Tensor kernel_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = kernel_spatial_dimensions_values.data()};\n+  Tensor output_spatial_dimensions{\n+      .type =\n+          TensorType{.shape = shape_parametrs, .element_type = DataType::kSI64},\n+      .data = output_spatial_dimensions_values.data()};\n+  Tensor output_tensor{\n+      .type = QuantizedPerAxisTensorType{.shape = shape_result,\n+                                         .element_type = tensor_type_axis_res},\n+      .data = output_data.data()};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  auto op = Create(ConvolutionOp::Attributes{\n+      .window_strides = window_stride,\n+      .padding = padding,\n+      .lhs_dilation = lhs_dilation,\n+      .rhs_dilation = rhs_dilation,\n+      .window_reversal = window_reversal,\n+      .input_batch_dimension = input_batch_dimension,\n+      .input_feature_dimension = input_feature_dimension,\n+      .input_spacial_dimensions = inputSpatialDimensions,\n+      .kernel_input_feature_dimension = kernel_input_feature_dimension,\n+      .kernel_output_feature_dimension = kernel_output_feature_dimension,\n+      .kernel_spacial_dimensions = kernel_spatial_dimensions,\n+      .output_batch_dimension = output_batch_dimension,\n+      .output_feature_dimension = output_feature_dimension,\n+      .output_spacial_dimensions = output_spatial_dimensions,\n+      .feature_group_count = feature_group_count,\n+      .batch_group_count = batch_group_count,\n+      .precision_configs = precision_configs});\n+\n+  Vector<StorageT> expected_data =\n+      Vector<StorageT>{5, 10, 15, 20, 25, 5, 10, 15, 20, -25};\n+  ;\n+  if (std::is_same<StorageT, I4>::value) {",
        "comment_created_at": "2024-05-10T12:45:56+00:00",
        "comment_author": "LokeshReddyOVS-MCW",
        "comment_body": "resolved by calling quantize function ,now works for all int types without clamping",
        "pr_file_module": null
      }
    ]
  }
]