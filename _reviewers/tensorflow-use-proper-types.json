[
  {
    "discussion_id": "1506336262",
    "pr_number": 63064,
    "pr_file": "tensorflow/core/kernels/transpose_op.cc",
    "created_at": "2024-02-28T17:34:19+00:00",
    "commented_code": "OP_REQUIRES(ctx, TensorShapeUtils::IsVector(perm.shape()),\n               errors::InvalidArgument(\"perm must be rank 1, got shape \",\n                                       perm.shape().DebugString()));\n-\n+  // Validation for negative values inside `perm`.\n+  auto perm_vector = perm.vec<int>();\n+  for (int i = 0; i < perm.dim_size(0); i++) {",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1506336262",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 63064,
        "pr_file": "tensorflow/core/kernels/transpose_op.cc",
        "discussion_id": "1506336262",
        "commented_code": "@@ -133,7 +133,16 @@ void TransposeOp::Compute(OpKernelContext* ctx) {\n   OP_REQUIRES(ctx, TensorShapeUtils::IsVector(perm.shape()),\n               errors::InvalidArgument(\"perm must be rank 1, got shape \",\n                                       perm.shape().DebugString()));\n-\n+  // Validation for negative values inside `perm`.\n+  auto perm_vector = perm.vec<int>();\n+  for (int i = 0; i < perm.dim_size(0); i++) {",
        "comment_created_at": "2024-02-28T17:34:19+00:00",
        "comment_author": "sagunb",
        "comment_body": "Thanks for the changes! Some minor comments to adhere to the c++ style guide.\r\nPlease use the prefix form (++i) - https://google.github.io/styleguide/cppguide.html#Preincrement_and_Predecrement",
        "pr_file_module": null
      },
      {
        "comment_id": "1506352095",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 63064,
        "pr_file": "tensorflow/core/kernels/transpose_op.cc",
        "discussion_id": "1506336262",
        "commented_code": "@@ -133,7 +133,16 @@ void TransposeOp::Compute(OpKernelContext* ctx) {\n   OP_REQUIRES(ctx, TensorShapeUtils::IsVector(perm.shape()),\n               errors::InvalidArgument(\"perm must be rank 1, got shape \",\n                                       perm.shape().DebugString()));\n-\n+  // Validation for negative values inside `perm`.\n+  auto perm_vector = perm.vec<int>();\n+  for (int i = 0; i < perm.dim_size(0); i++) {",
        "comment_created_at": "2024-02-28T17:47:29+00:00",
        "comment_author": "SuryanarayanaY",
        "comment_body": "Sure.Thanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1995133990",
    "pr_number": 89205,
    "pr_file": "tensorflow/lite/experimental/litert/vendors/qualcomm/core/schema/soc_table.h",
    "created_at": "2025-03-14T08:49:42+00:00",
    "commented_code": "+// Copyright (c) Qualcomm Innovation Center, Inc. All Rights Reserved.\n+// SPDX-License-Identifier: Apache-2.0\n+\n+#ifndef TENSORFLOW_LITE_EXPERIMENTAL_LITERT_VENDORS_QUALCOMM_CORE_SCHEMA_SOC_TABLE_H_\n+#define TENSORFLOW_LITE_EXPERIMENTAL_LITERT_VENDORS_QUALCOMM_CORE_SCHEMA_SOC_TABLE_H_\n+\n+#include <string>\n+#include <vector>\n+\n+namespace qnn {\n+enum class SnapdragonModel {\n+  UNKNOWN_SDM = 0,\n+  SA8295 = 39,\n+  SM8350 = 30,\n+  SM8450 = 36,\n+  SM8550 = 43,\n+  SA8255 = 52,\n+  SM8650 = 57,\n+  SM8750 = 69\n+};\n+\n+enum class DspArch {\n+  NONE = 0,\n+  V65 = 65,\n+  V66 = 66,\n+  V68 = 68,  // HTP supported device\n+  V69 = 69,  // HTP supported device\n+  V73 = 73,  // HTP supported device\n+  V75 = 75,  // HTP supported device\n+  V79 = 79,  // HTP supported device\n+};\n+\n+struct SocInfo {\n+  const char* soc_name;\n+  SnapdragonModel soc_model;\n+  DspArch dsp_arch;\n+  std::size_t vtcm_size_in_mb;\n+\n+  constexpr SocInfo(const char* soc_name, SnapdragonModel soc_model,\n+                    DspArch dsp_arch, std::size_t vtcm_size_in_mb)",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1995133990",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 89205,
        "pr_file": "tensorflow/lite/experimental/litert/vendors/qualcomm/core/schema/soc_table.h",
        "discussion_id": "1995133990",
        "commented_code": "@@ -0,0 +1,74 @@\n+// Copyright (c) Qualcomm Innovation Center, Inc. All Rights Reserved.\n+// SPDX-License-Identifier: Apache-2.0\n+\n+#ifndef TENSORFLOW_LITE_EXPERIMENTAL_LITERT_VENDORS_QUALCOMM_CORE_SCHEMA_SOC_TABLE_H_\n+#define TENSORFLOW_LITE_EXPERIMENTAL_LITERT_VENDORS_QUALCOMM_CORE_SCHEMA_SOC_TABLE_H_\n+\n+#include <string>\n+#include <vector>\n+\n+namespace qnn {\n+enum class SnapdragonModel {\n+  UNKNOWN_SDM = 0,\n+  SA8295 = 39,\n+  SM8350 = 30,\n+  SM8450 = 36,\n+  SM8550 = 43,\n+  SA8255 = 52,\n+  SM8650 = 57,\n+  SM8750 = 69\n+};\n+\n+enum class DspArch {\n+  NONE = 0,\n+  V65 = 65,\n+  V66 = 66,\n+  V68 = 68,  // HTP supported device\n+  V69 = 69,  // HTP supported device\n+  V73 = 73,  // HTP supported device\n+  V75 = 75,  // HTP supported device\n+  V79 = 79,  // HTP supported device\n+};\n+\n+struct SocInfo {\n+  const char* soc_name;\n+  SnapdragonModel soc_model;\n+  DspArch dsp_arch;\n+  std::size_t vtcm_size_in_mb;\n+\n+  constexpr SocInfo(const char* soc_name, SnapdragonModel soc_model,\n+                    DspArch dsp_arch, std::size_t vtcm_size_in_mb)",
        "comment_created_at": "2025-03-14T08:49:42+00:00",
        "comment_author": "weilhuan-quic",
        "comment_body": "Nit: pass const parameter.\r\n```suggestion\r\n  constexpr SocInfo(const char* soc_name, const SnapdragonModel soc_model,\r\n                    const DspArch dsp_arch, const std::size_t vtcm_size_in_mb)\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2002948758",
    "pr_number": 89337,
    "pr_file": "tensorflow/lite/experimental/litert/vendors/qualcomm/core/utils/miscs.cc",
    "created_at": "2025-03-19T10:07:42+00:00",
    "commented_code": "+// Copyright (c) Qualcomm Innovation Center, Inc. All Rights Reserved.\n+// SPDX-License-Identifier: Apache-2.0\n+\n+#include \"tensorflow/lite/experimental/litert/vendors/qualcomm/core/utils/miscs.h\"\n+\n+#include \"absl/types/span.h\"\n+\n+namespace qnn {\n+void ConvertDataFromInt16toUInt16(absl::Span<const std::int16_t>& src,",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "2002948758",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 89337,
        "pr_file": "tensorflow/lite/experimental/litert/vendors/qualcomm/core/utils/miscs.cc",
        "discussion_id": "2002948758",
        "commented_code": "@@ -0,0 +1,27 @@\n+// Copyright (c) Qualcomm Innovation Center, Inc. All Rights Reserved.\n+// SPDX-License-Identifier: Apache-2.0\n+\n+#include \"tensorflow/lite/experimental/litert/vendors/qualcomm/core/utils/miscs.h\"\n+\n+#include \"absl/types/span.h\"\n+\n+namespace qnn {\n+void ConvertDataFromInt16toUInt16(absl::Span<const std::int16_t>& src,",
        "comment_created_at": "2025-03-19T10:07:42+00:00",
        "comment_author": "chunhsue",
        "comment_body": "Pass by value, span is already a lightweight container.\r\n```suggestion\r\nvoid ConvertDataFromInt16toUInt16(absl::Span<const std::int16_t> src,\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2003007683",
    "pr_number": 83717,
    "pr_file": "tensorflow/lite/kernels/strided_slice_test.cc",
    "created_at": "2025-03-19T10:41:46+00:00",
    "commented_code": "// NNAPI does not support graphs with all constant inputs.\n       continue;\n     }\n-    StridedSliceOpModel<TypeParam> m({4}, {1}, {1}, {1}, {1, 2, 3, 4}, {1}, {3},\n+    std::vector<TypeParam> input_data = CastVector<TypeParam>({1, 2, 3, 4});",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "2003007683",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 83717,
        "pr_file": "tensorflow/lite/kernels/strided_slice_test.cc",
        "discussion_id": "2003007683",
        "commented_code": "@@ -295,11 +321,12 @@ TYPED_TEST(StridedSliceOpTest, In1DConst) {\n       // NNAPI does not support graphs with all constant inputs.\n       continue;\n     }\n-    StridedSliceOpModel<TypeParam> m({4}, {1}, {1}, {1}, {1, 2, 3, 4}, {1}, {3},\n+    std::vector<TypeParam> input_data = CastVector<TypeParam>({1, 2, 3, 4});",
        "comment_created_at": "2025-03-19T10:41:46+00:00",
        "comment_author": "qukhan",
        "comment_body": "Can you make all of the helper `input_data` vectors `const`?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1583884677",
    "pr_number": 66085,
    "pr_file": "tensorflow/core/kernels/quantize_op_test.cc",
    "created_at": "2024-04-29T23:05:16+00:00",
    "commented_code": "return out;\n }\n \n-TEST_P(ParameterizedQuantizeOpTest, QuantizeV2Quint8Scaled) {\n-  const int axis = GetParam();\n+std::vector<bfloat16> ConvertFloatToBloat16(const std::vector<float>& data) {\n+  std::vector<bfloat16> out(data.size());\n+  size_t i = 0;\n+  for (auto itr = out.begin(); itr != out.end(); ++itr, ++i) {\n+    *itr = bfloat16(data[i]);",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1583884677",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 66085,
        "pr_file": "tensorflow/core/kernels/quantize_op_test.cc",
        "discussion_id": "1583884677",
        "commented_code": "@@ -82,10 +106,20 @@ std::vector<T> ScalePerSliceAlongAxis(std::vector<int64_t> dims, int axis,\n   return out;\n }\n \n-TEST_P(ParameterizedQuantizeOpTest, QuantizeV2Quint8Scaled) {\n-  const int axis = GetParam();\n+std::vector<bfloat16> ConvertFloatToBloat16(const std::vector<float>& data) {\n+  std::vector<bfloat16> out(data.size());\n+  size_t i = 0;\n+  for (auto itr = out.begin(); itr != out.end(); ++itr, ++i) {\n+    *itr = bfloat16(data[i]);",
        "comment_created_at": "2024-04-29T23:05:16+00:00",
        "comment_author": "cantonios",
        "comment_body": "nit: prefer c++-style casts, `static_cast<bfloat16>(...)`.",
        "pr_file_module": null
      },
      {
        "comment_id": "1585250770",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 66085,
        "pr_file": "tensorflow/core/kernels/quantize_op_test.cc",
        "discussion_id": "1583884677",
        "commented_code": "@@ -82,10 +106,20 @@ std::vector<T> ScalePerSliceAlongAxis(std::vector<int64_t> dims, int axis,\n   return out;\n }\n \n-TEST_P(ParameterizedQuantizeOpTest, QuantizeV2Quint8Scaled) {\n-  const int axis = GetParam();\n+std::vector<bfloat16> ConvertFloatToBloat16(const std::vector<float>& data) {\n+  std::vector<bfloat16> out(data.size());\n+  size_t i = 0;\n+  for (auto itr = out.begin(); itr != out.end(); ++itr, ++i) {\n+    *itr = bfloat16(data[i]);",
        "comment_created_at": "2024-04-30T17:36:09+00:00",
        "comment_author": "mdfaijul",
        "comment_body": "Fixed.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1506339363",
    "pr_number": 63071,
    "pr_file": "tensorflow/core/ops/image_ops.cc",
    "created_at": "2024-02-28T17:36:58+00:00",
    "commented_code": "const Tensor* crop_window = c->input_tensor(1);\n       if (crop_window != nullptr) {\n         auto crop_window_vec = crop_window->vec<int32>();\n+        if (0 > crop_window_vec(2) || 0 > crop_window_vec(3)){",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1506339363",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 63071,
        "pr_file": "tensorflow/core/ops/image_ops.cc",
        "discussion_id": "1506339363",
        "commented_code": "@@ -519,6 +519,11 @@ REGISTER_OP(\"DecodeAndCropJpeg\")\n       const Tensor* crop_window = c->input_tensor(1);\n       if (crop_window != nullptr) {\n         auto crop_window_vec = crop_window->vec<int32>();\n+        if (0 > crop_window_vec(2) || 0 > crop_window_vec(3)){",
        "comment_created_at": "2024-02-28T17:36:58+00:00",
        "comment_author": "sagunb",
        "comment_body": "Please put the variable ahead of the value being compared to.",
        "pr_file_module": null
      },
      {
        "comment_id": "1506373546",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 63071,
        "pr_file": "tensorflow/core/ops/image_ops.cc",
        "discussion_id": "1506339363",
        "commented_code": "@@ -519,6 +519,11 @@ REGISTER_OP(\"DecodeAndCropJpeg\")\n       const Tensor* crop_window = c->input_tensor(1);\n       if (crop_window != nullptr) {\n         auto crop_window_vec = crop_window->vec<int32>();\n+        if (0 > crop_window_vec(2) || 0 > crop_window_vec(3)){",
        "comment_created_at": "2024-02-28T18:04:49+00:00",
        "comment_author": "SuryanarayanaY",
        "comment_body": "@sagunb , Done the changes.Thanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1578280013",
    "pr_number": 66299,
    "pr_file": "tensorflow/lite/experimental/shlo/ops/convolution.cc",
    "created_at": "2024-04-24T17:38:46+00:00",
    "commented_code": "+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/convolution.h\"\n+\n+#include <algorithm>\n+#include <cstddef>\n+#include <string>\n+#include <type_traits>\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/unary_elementwise.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+template <class T>\n+using DimVector = absl::InlinedVector<T, 6>;\n+\n+bool IsUnique(DimVector<int64_t>& vec) {\n+  std::sort(vec.begin(), vec.end());\n+  return std::unique(vec.begin(), vec.end()) == vec.end();\n+}\n+\n+bool IsInRange(DimVector<int64_t>& vec, size_t N) {\n+  for (int64_t dim : vec) {\n+    if (dim >= N || dim < 0) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+template <DataType storage_type>\n+absl::Status PrepareImpl(ConvolutionOp& op, const Tensor& lhs,\n+                         const Tensor& rhs, Tensor& output) {\n+  using StorageT = StorageType<storage_type>;\n+\n+  // Transpose prepare\n+  const int64_t* window_spacial_pointer =\n+      op.attributes.kernel_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  const int64_t* output_spacial_pointer =\n+      op.attributes.output_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  const int64_t* input_spacial_pointer =\n+      op.attributes.input_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+\n+  std::vector<StorageT> lhs_permutation_values(\n+      static_cast<int64_t>(lhs.Rank()));\n+  lhs_permutation_values[0] = op.attributes.input_batch_dimension;\n+  lhs_permutation_values[1] = op.attributes.input_feature_dimension;\n+  DimVector<DimensionSize> lhs_shape_dims(lhs.Rank());\n+  lhs_shape_dims[0] =\n+      lhs.shape().Dim(static_cast<size_t>(op.attributes.input_batch_dimension));\n+  lhs_shape_dims[1] = lhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.input_feature_dimension));\n+  for (size_t i = 0; i < lhs.Rank() - 2; ++i) {\n+    lhs_shape_dims[i + 2] =\n+        lhs.shape().Dim(static_cast<size_t>(input_spacial_pointer[i]));\n+    lhs_permutation_values[i + 2] = input_spacial_pointer[i];\n+  }\n+  // malloc is used to have the storage space available out of prepare function\n+  // scope and it's pointer is stored in class data member to\n+  // deallocate the memory in destructor.\n+  op.lhs_permutation_data =\n+      malloc(lhs_permutation_values.size() * sizeof(StorageT));\n+  memmove(op.lhs_permutation_data, lhs_permutation_values.data(),\n+          lhs_permutation_values.size() * sizeof(StorageT));\n+  const Shape lhs_permutation_shape({static_cast<int64_t>(lhs.Rank())});\n+  Tensor lhs_permutations{.type = TensorType{.shape = lhs_permutation_shape,\n+                                             .element_type = storage_type},\n+                          .data = op.lhs_permutation_data};\n+\n+  op.lhs_transposed_data = malloc(lhs.NumElements() * sizeof(StorageT));\n+  const Shape lhs_transposed_shape(lhs_shape_dims);\n+  Tensor lhs_transposed{.type = TensorType{.shape = lhs_transposed_shape,\n+                                           .element_type = storage_type},\n+                        .data = op.lhs_transposed_data};\n+\n+  std::vector<StorageT> rhs_permutation_values(\n+      static_cast<int64_t>(rhs.Rank()));\n+  rhs_permutation_values[0] = op.attributes.kernel_output_feature_dimension;\n+  rhs_permutation_values[1] = op.attributes.kernel_input_feature_dimension;\n+  DimVector<DimensionSize> rhs_shape_dims(rhs.Rank());\n+  rhs_shape_dims[0] = rhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.kernel_output_feature_dimension));\n+  rhs_shape_dims[1] = rhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.kernel_input_feature_dimension));\n+  for (size_t i = 0; i < rhs.Rank() - 2; ++i) {\n+    rhs_shape_dims[i + 2] =\n+        rhs.shape().Dim(static_cast<size_t>(window_spacial_pointer[i]));\n+    rhs_permutation_values[i + 2] = window_spacial_pointer[i];\n+  }\n+  op.rhs_permutation_data = malloc(rhs.Rank() * sizeof(StorageT));\n+  memmove(op.rhs_permutation_data, rhs_permutation_values.data(),\n+          rhs_permutation_values.size() * sizeof(StorageT));\n+  const Shape rhs_permutation_shape({static_cast<int64_t>(rhs.Rank())});\n+  Tensor rhs_permutations{.type = TensorType{.shape = rhs_permutation_shape,\n+                                             .element_type = storage_type},\n+                          .data = op.rhs_permutation_data};\n+\n+  op.rhs_transposed_data = malloc(rhs.NumElements() * sizeof(StorageT));\n+  const Shape rhs_transposed_shape(rhs_shape_dims);\n+  Tensor rhs_transposed{.type = TensorType{.shape = rhs_transposed_shape,\n+                                           .element_type = storage_type},\n+                        .data = op.rhs_transposed_data};\n+\n+  std::vector<StorageT> output_permutation_values(\n+      static_cast<int64_t>(output.Rank()));\n+  output_permutation_values[0] = op.attributes.output_batch_dimension;\n+  output_permutation_values[1] = op.attributes.output_feature_dimension;\n+  DimVector<DimensionSize> output_shape_dims(output.Rank());\n+  output_shape_dims[0] = output.shape().Dim(\n+      static_cast<size_t>(op.attributes.output_batch_dimension));\n+  output_shape_dims[1] = output.shape().Dim(\n+      static_cast<size_t>(op.attributes.output_feature_dimension));\n+  for (size_t i = 0; i < output.Rank() - 2; ++i) {\n+    output_shape_dims[i + 2] =\n+        output.shape().Dim(static_cast<size_t>(output_spacial_pointer[i]));\n+    output_permutation_values[i + 2] = output_spacial_pointer[i];\n+  }\n+  op.output_permutation_data = malloc(output.Rank() * sizeof(StorageT));\n+  memmove(op.output_permutation_data, output_permutation_values.data(),\n+          output_permutation_values.size() * sizeof(StorageT));\n+  const Shape output_permutation_shape({static_cast<int64_t>(output.Rank())});\n+  Tensor output_permutations{\n+      .type = TensorType{.shape = output_permutation_shape,\n+                         .element_type = storage_type},\n+      .data = op.output_permutation_data};\n+\n+  op.output_transposed_data = malloc(output.NumElements() * sizeof(StorageT));\n+  const Shape output_transposed_shape(output_shape_dims);\n+  Tensor output_transposed{.type = TensorType{.shape = output_transposed_shape,\n+                                              .element_type = storage_type},\n+                           .data = op.output_transposed_data};\n+  // transpose prepare end\n+\n+  // DotGeneral prepare\n+  DimVector<DimensionSize> dims(rhs_transposed.Rank());\n+  size_t rhs_transposed_tensor_size = 1;\n+  dims[0] = 1;\n+  for (size_t i = 1; i < rhs_transposed.Rank(); ++i) {\n+    dims[i] = rhs_transposed.shape().Dim(i);\n+    rhs_transposed_tensor_size *= rhs_transposed.shape().Dim(i);\n+  }\n+  const Shape rhs_dot_general_shape(dims);\n+  op.rhs_dot_general_data =\n+      malloc(rhs_transposed_tensor_size * sizeof(StorageT));\n+  Tensor rhs_dot_general{.type = TensorType{.shape = rhs_dot_general_shape,\n+                                            .element_type = storage_type},\n+                         .data = op.rhs_dot_general_data};\n+\n+  op.lhs_dot_general_data =\n+      malloc(rhs_transposed_tensor_size * sizeof(StorageT));\n+  Tensor lhs_dot_general{.type = TensorType{.shape = rhs_dot_general_shape,\n+                                            .element_type = storage_type},\n+                         .data = op.lhs_dot_general_data};\n+\n+  std::vector<typename Storage<DataType::kSI64>::Type>\n+      lhs_contracting_dimensions_values(lhs_transposed.Rank() - 1);\n+  for (size_t i = 0; i < lhs_transposed.Rank() - 1; ++i) {\n+    lhs_contracting_dimensions_values[i] = i + 1;\n+  }\n+  op.lhs_contracting_dimensions_data =\n+      malloc((lhs_transposed.Rank() - 1) * sizeof(int64_t));\n+  memmove(op.lhs_contracting_dimensions_data,\n+          lhs_contracting_dimensions_values.data(),\n+          lhs_contracting_dimensions_values.size() * sizeof(int64_t));\n+  const Shape lhs_contracting_dimensions_shape(\n+      {static_cast<int64_t>(lhs_transposed.Rank() - 1)});\n+  Tensor lhs_contracting_dimensions{\n+      .type = TensorType{.shape = lhs_contracting_dimensions_shape,\n+                         .element_type = DataType::kSI64},\n+      .data = op.lhs_contracting_dimensions_data};\n+\n+  std::vector<typename Storage<DataType::kSI64>::Type>\n+      rhs_contracting_dimensions_values(rhs_transposed.Rank() - 1);\n+  for (size_t i = 0; i < rhs_transposed.Rank() - 1; ++i) {\n+    rhs_contracting_dimensions_values[i] = i + 1;\n+  }\n+  op.rhs_contracting_dimensions_data =\n+      malloc((rhs_transposed.Rank() - 1) * sizeof(int64_t));\n+  memmove(op.rhs_contracting_dimensions_data,\n+          rhs_contracting_dimensions_values.data(),\n+          rhs_contracting_dimensions_values.size() * sizeof(int64_t));\n+  Tensor rhs_contracting_dimensions{\n+      .type = TensorType{.shape = lhs_contracting_dimensions_shape,\n+                         .element_type = DataType::kSI8},\n+      .data = op.rhs_contracting_dimensions_data};\n+\n+  std::vector<StorageT> dor_general_output_values(1);\n+  dor_general_output_values[0] = 0;\n+  op.output_dot_general_data = malloc(1 * sizeof(StorageT));\n+  memmove(op.output_dot_general_data, dor_general_output_values.data(),\n+          dor_general_output_values.size() * sizeof(StorageT));\n+  const Shape dor_general_output_shape{{1}};\n+  Tensor output_dot_general{\n+      .type = TensorType{.shape = dor_general_output_shape,\n+                         .element_type = storage_type},\n+      .data = op.output_dot_general_data};\n+\n+  Tensor lhs_batching_dimensions{\n+      .type = TensorType{.shape = Shape(), .element_type = DataType::kSI8},\n+      .data = {}};\n+  Tensor rhs_batching_dimensions{\n+      .type = TensorType{.shape = Shape(), .element_type = DataType::kSI8},\n+      .data = {}};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  op.dot_general_op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  auto state = Prepare(op.dot_general_op, lhs_dot_general, rhs_dot_general,",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1578280013",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 66299,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/convolution.cc",
        "discussion_id": "1578280013",
        "commented_code": "@@ -0,0 +1,1171 @@\n+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/convolution.h\"\n+\n+#include <algorithm>\n+#include <cstddef>\n+#include <string>\n+#include <type_traits>\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/unary_elementwise.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+template <class T>\n+using DimVector = absl::InlinedVector<T, 6>;\n+\n+bool IsUnique(DimVector<int64_t>& vec) {\n+  std::sort(vec.begin(), vec.end());\n+  return std::unique(vec.begin(), vec.end()) == vec.end();\n+}\n+\n+bool IsInRange(DimVector<int64_t>& vec, size_t N) {\n+  for (int64_t dim : vec) {\n+    if (dim >= N || dim < 0) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+template <DataType storage_type>\n+absl::Status PrepareImpl(ConvolutionOp& op, const Tensor& lhs,\n+                         const Tensor& rhs, Tensor& output) {\n+  using StorageT = StorageType<storage_type>;\n+\n+  // Transpose prepare\n+  const int64_t* window_spacial_pointer =\n+      op.attributes.kernel_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  const int64_t* output_spacial_pointer =\n+      op.attributes.output_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  const int64_t* input_spacial_pointer =\n+      op.attributes.input_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+\n+  std::vector<StorageT> lhs_permutation_values(\n+      static_cast<int64_t>(lhs.Rank()));\n+  lhs_permutation_values[0] = op.attributes.input_batch_dimension;\n+  lhs_permutation_values[1] = op.attributes.input_feature_dimension;\n+  DimVector<DimensionSize> lhs_shape_dims(lhs.Rank());\n+  lhs_shape_dims[0] =\n+      lhs.shape().Dim(static_cast<size_t>(op.attributes.input_batch_dimension));\n+  lhs_shape_dims[1] = lhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.input_feature_dimension));\n+  for (size_t i = 0; i < lhs.Rank() - 2; ++i) {\n+    lhs_shape_dims[i + 2] =\n+        lhs.shape().Dim(static_cast<size_t>(input_spacial_pointer[i]));\n+    lhs_permutation_values[i + 2] = input_spacial_pointer[i];\n+  }\n+  // malloc is used to have the storage space available out of prepare function\n+  // scope and it's pointer is stored in class data member to\n+  // deallocate the memory in destructor.\n+  op.lhs_permutation_data =\n+      malloc(lhs_permutation_values.size() * sizeof(StorageT));\n+  memmove(op.lhs_permutation_data, lhs_permutation_values.data(),\n+          lhs_permutation_values.size() * sizeof(StorageT));\n+  const Shape lhs_permutation_shape({static_cast<int64_t>(lhs.Rank())});\n+  Tensor lhs_permutations{.type = TensorType{.shape = lhs_permutation_shape,\n+                                             .element_type = storage_type},\n+                          .data = op.lhs_permutation_data};\n+\n+  op.lhs_transposed_data = malloc(lhs.NumElements() * sizeof(StorageT));\n+  const Shape lhs_transposed_shape(lhs_shape_dims);\n+  Tensor lhs_transposed{.type = TensorType{.shape = lhs_transposed_shape,\n+                                           .element_type = storage_type},\n+                        .data = op.lhs_transposed_data};\n+\n+  std::vector<StorageT> rhs_permutation_values(\n+      static_cast<int64_t>(rhs.Rank()));\n+  rhs_permutation_values[0] = op.attributes.kernel_output_feature_dimension;\n+  rhs_permutation_values[1] = op.attributes.kernel_input_feature_dimension;\n+  DimVector<DimensionSize> rhs_shape_dims(rhs.Rank());\n+  rhs_shape_dims[0] = rhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.kernel_output_feature_dimension));\n+  rhs_shape_dims[1] = rhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.kernel_input_feature_dimension));\n+  for (size_t i = 0; i < rhs.Rank() - 2; ++i) {\n+    rhs_shape_dims[i + 2] =\n+        rhs.shape().Dim(static_cast<size_t>(window_spacial_pointer[i]));\n+    rhs_permutation_values[i + 2] = window_spacial_pointer[i];\n+  }\n+  op.rhs_permutation_data = malloc(rhs.Rank() * sizeof(StorageT));\n+  memmove(op.rhs_permutation_data, rhs_permutation_values.data(),\n+          rhs_permutation_values.size() * sizeof(StorageT));\n+  const Shape rhs_permutation_shape({static_cast<int64_t>(rhs.Rank())});\n+  Tensor rhs_permutations{.type = TensorType{.shape = rhs_permutation_shape,\n+                                             .element_type = storage_type},\n+                          .data = op.rhs_permutation_data};\n+\n+  op.rhs_transposed_data = malloc(rhs.NumElements() * sizeof(StorageT));\n+  const Shape rhs_transposed_shape(rhs_shape_dims);\n+  Tensor rhs_transposed{.type = TensorType{.shape = rhs_transposed_shape,\n+                                           .element_type = storage_type},\n+                        .data = op.rhs_transposed_data};\n+\n+  std::vector<StorageT> output_permutation_values(\n+      static_cast<int64_t>(output.Rank()));\n+  output_permutation_values[0] = op.attributes.output_batch_dimension;\n+  output_permutation_values[1] = op.attributes.output_feature_dimension;\n+  DimVector<DimensionSize> output_shape_dims(output.Rank());\n+  output_shape_dims[0] = output.shape().Dim(\n+      static_cast<size_t>(op.attributes.output_batch_dimension));\n+  output_shape_dims[1] = output.shape().Dim(\n+      static_cast<size_t>(op.attributes.output_feature_dimension));\n+  for (size_t i = 0; i < output.Rank() - 2; ++i) {\n+    output_shape_dims[i + 2] =\n+        output.shape().Dim(static_cast<size_t>(output_spacial_pointer[i]));\n+    output_permutation_values[i + 2] = output_spacial_pointer[i];\n+  }\n+  op.output_permutation_data = malloc(output.Rank() * sizeof(StorageT));\n+  memmove(op.output_permutation_data, output_permutation_values.data(),\n+          output_permutation_values.size() * sizeof(StorageT));\n+  const Shape output_permutation_shape({static_cast<int64_t>(output.Rank())});\n+  Tensor output_permutations{\n+      .type = TensorType{.shape = output_permutation_shape,\n+                         .element_type = storage_type},\n+      .data = op.output_permutation_data};\n+\n+  op.output_transposed_data = malloc(output.NumElements() * sizeof(StorageT));\n+  const Shape output_transposed_shape(output_shape_dims);\n+  Tensor output_transposed{.type = TensorType{.shape = output_transposed_shape,\n+                                              .element_type = storage_type},\n+                           .data = op.output_transposed_data};\n+  // transpose prepare end\n+\n+  // DotGeneral prepare\n+  DimVector<DimensionSize> dims(rhs_transposed.Rank());\n+  size_t rhs_transposed_tensor_size = 1;\n+  dims[0] = 1;\n+  for (size_t i = 1; i < rhs_transposed.Rank(); ++i) {\n+    dims[i] = rhs_transposed.shape().Dim(i);\n+    rhs_transposed_tensor_size *= rhs_transposed.shape().Dim(i);\n+  }\n+  const Shape rhs_dot_general_shape(dims);\n+  op.rhs_dot_general_data =\n+      malloc(rhs_transposed_tensor_size * sizeof(StorageT));\n+  Tensor rhs_dot_general{.type = TensorType{.shape = rhs_dot_general_shape,\n+                                            .element_type = storage_type},\n+                         .data = op.rhs_dot_general_data};\n+\n+  op.lhs_dot_general_data =\n+      malloc(rhs_transposed_tensor_size * sizeof(StorageT));\n+  Tensor lhs_dot_general{.type = TensorType{.shape = rhs_dot_general_shape,\n+                                            .element_type = storage_type},\n+                         .data = op.lhs_dot_general_data};\n+\n+  std::vector<typename Storage<DataType::kSI64>::Type>\n+      lhs_contracting_dimensions_values(lhs_transposed.Rank() - 1);\n+  for (size_t i = 0; i < lhs_transposed.Rank() - 1; ++i) {\n+    lhs_contracting_dimensions_values[i] = i + 1;\n+  }\n+  op.lhs_contracting_dimensions_data =\n+      malloc((lhs_transposed.Rank() - 1) * sizeof(int64_t));\n+  memmove(op.lhs_contracting_dimensions_data,\n+          lhs_contracting_dimensions_values.data(),\n+          lhs_contracting_dimensions_values.size() * sizeof(int64_t));\n+  const Shape lhs_contracting_dimensions_shape(\n+      {static_cast<int64_t>(lhs_transposed.Rank() - 1)});\n+  Tensor lhs_contracting_dimensions{\n+      .type = TensorType{.shape = lhs_contracting_dimensions_shape,\n+                         .element_type = DataType::kSI64},\n+      .data = op.lhs_contracting_dimensions_data};\n+\n+  std::vector<typename Storage<DataType::kSI64>::Type>\n+      rhs_contracting_dimensions_values(rhs_transposed.Rank() - 1);\n+  for (size_t i = 0; i < rhs_transposed.Rank() - 1; ++i) {\n+    rhs_contracting_dimensions_values[i] = i + 1;\n+  }\n+  op.rhs_contracting_dimensions_data =\n+      malloc((rhs_transposed.Rank() - 1) * sizeof(int64_t));\n+  memmove(op.rhs_contracting_dimensions_data,\n+          rhs_contracting_dimensions_values.data(),\n+          rhs_contracting_dimensions_values.size() * sizeof(int64_t));\n+  Tensor rhs_contracting_dimensions{\n+      .type = TensorType{.shape = lhs_contracting_dimensions_shape,\n+                         .element_type = DataType::kSI8},\n+      .data = op.rhs_contracting_dimensions_data};\n+\n+  std::vector<StorageT> dor_general_output_values(1);\n+  dor_general_output_values[0] = 0;\n+  op.output_dot_general_data = malloc(1 * sizeof(StorageT));\n+  memmove(op.output_dot_general_data, dor_general_output_values.data(),\n+          dor_general_output_values.size() * sizeof(StorageT));\n+  const Shape dor_general_output_shape{{1}};\n+  Tensor output_dot_general{\n+      .type = TensorType{.shape = dor_general_output_shape,\n+                         .element_type = storage_type},\n+      .data = op.output_dot_general_data};\n+\n+  Tensor lhs_batching_dimensions{\n+      .type = TensorType{.shape = Shape(), .element_type = DataType::kSI8},\n+      .data = {}};\n+  Tensor rhs_batching_dimensions{\n+      .type = TensorType{.shape = Shape(), .element_type = DataType::kSI8},\n+      .data = {}};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  op.dot_general_op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  auto state = Prepare(op.dot_general_op, lhs_dot_general, rhs_dot_general,",
        "comment_created_at": "2024-04-24T17:38:46+00:00",
        "comment_author": "qukhan",
        "comment_body": "Unless the type is ridiculously complicated, prefer explicit types.",
        "pr_file_module": null
      },
      {
        "comment_id": "1596710061",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 66299,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/convolution.cc",
        "discussion_id": "1578280013",
        "commented_code": "@@ -0,0 +1,1171 @@\n+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/convolution.h\"\n+\n+#include <algorithm>\n+#include <cstddef>\n+#include <string>\n+#include <type_traits>\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/unary_elementwise.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+template <class T>\n+using DimVector = absl::InlinedVector<T, 6>;\n+\n+bool IsUnique(DimVector<int64_t>& vec) {\n+  std::sort(vec.begin(), vec.end());\n+  return std::unique(vec.begin(), vec.end()) == vec.end();\n+}\n+\n+bool IsInRange(DimVector<int64_t>& vec, size_t N) {\n+  for (int64_t dim : vec) {\n+    if (dim >= N || dim < 0) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+template <DataType storage_type>\n+absl::Status PrepareImpl(ConvolutionOp& op, const Tensor& lhs,\n+                         const Tensor& rhs, Tensor& output) {\n+  using StorageT = StorageType<storage_type>;\n+\n+  // Transpose prepare\n+  const int64_t* window_spacial_pointer =\n+      op.attributes.kernel_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  const int64_t* output_spacial_pointer =\n+      op.attributes.output_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+  const int64_t* input_spacial_pointer =\n+      op.attributes.input_spacial_dimensions.GetDataAs<DataType::kSI64>();\n+\n+  std::vector<StorageT> lhs_permutation_values(\n+      static_cast<int64_t>(lhs.Rank()));\n+  lhs_permutation_values[0] = op.attributes.input_batch_dimension;\n+  lhs_permutation_values[1] = op.attributes.input_feature_dimension;\n+  DimVector<DimensionSize> lhs_shape_dims(lhs.Rank());\n+  lhs_shape_dims[0] =\n+      lhs.shape().Dim(static_cast<size_t>(op.attributes.input_batch_dimension));\n+  lhs_shape_dims[1] = lhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.input_feature_dimension));\n+  for (size_t i = 0; i < lhs.Rank() - 2; ++i) {\n+    lhs_shape_dims[i + 2] =\n+        lhs.shape().Dim(static_cast<size_t>(input_spacial_pointer[i]));\n+    lhs_permutation_values[i + 2] = input_spacial_pointer[i];\n+  }\n+  // malloc is used to have the storage space available out of prepare function\n+  // scope and it's pointer is stored in class data member to\n+  // deallocate the memory in destructor.\n+  op.lhs_permutation_data =\n+      malloc(lhs_permutation_values.size() * sizeof(StorageT));\n+  memmove(op.lhs_permutation_data, lhs_permutation_values.data(),\n+          lhs_permutation_values.size() * sizeof(StorageT));\n+  const Shape lhs_permutation_shape({static_cast<int64_t>(lhs.Rank())});\n+  Tensor lhs_permutations{.type = TensorType{.shape = lhs_permutation_shape,\n+                                             .element_type = storage_type},\n+                          .data = op.lhs_permutation_data};\n+\n+  op.lhs_transposed_data = malloc(lhs.NumElements() * sizeof(StorageT));\n+  const Shape lhs_transposed_shape(lhs_shape_dims);\n+  Tensor lhs_transposed{.type = TensorType{.shape = lhs_transposed_shape,\n+                                           .element_type = storage_type},\n+                        .data = op.lhs_transposed_data};\n+\n+  std::vector<StorageT> rhs_permutation_values(\n+      static_cast<int64_t>(rhs.Rank()));\n+  rhs_permutation_values[0] = op.attributes.kernel_output_feature_dimension;\n+  rhs_permutation_values[1] = op.attributes.kernel_input_feature_dimension;\n+  DimVector<DimensionSize> rhs_shape_dims(rhs.Rank());\n+  rhs_shape_dims[0] = rhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.kernel_output_feature_dimension));\n+  rhs_shape_dims[1] = rhs.shape().Dim(\n+      static_cast<size_t>(op.attributes.kernel_input_feature_dimension));\n+  for (size_t i = 0; i < rhs.Rank() - 2; ++i) {\n+    rhs_shape_dims[i + 2] =\n+        rhs.shape().Dim(static_cast<size_t>(window_spacial_pointer[i]));\n+    rhs_permutation_values[i + 2] = window_spacial_pointer[i];\n+  }\n+  op.rhs_permutation_data = malloc(rhs.Rank() * sizeof(StorageT));\n+  memmove(op.rhs_permutation_data, rhs_permutation_values.data(),\n+          rhs_permutation_values.size() * sizeof(StorageT));\n+  const Shape rhs_permutation_shape({static_cast<int64_t>(rhs.Rank())});\n+  Tensor rhs_permutations{.type = TensorType{.shape = rhs_permutation_shape,\n+                                             .element_type = storage_type},\n+                          .data = op.rhs_permutation_data};\n+\n+  op.rhs_transposed_data = malloc(rhs.NumElements() * sizeof(StorageT));\n+  const Shape rhs_transposed_shape(rhs_shape_dims);\n+  Tensor rhs_transposed{.type = TensorType{.shape = rhs_transposed_shape,\n+                                           .element_type = storage_type},\n+                        .data = op.rhs_transposed_data};\n+\n+  std::vector<StorageT> output_permutation_values(\n+      static_cast<int64_t>(output.Rank()));\n+  output_permutation_values[0] = op.attributes.output_batch_dimension;\n+  output_permutation_values[1] = op.attributes.output_feature_dimension;\n+  DimVector<DimensionSize> output_shape_dims(output.Rank());\n+  output_shape_dims[0] = output.shape().Dim(\n+      static_cast<size_t>(op.attributes.output_batch_dimension));\n+  output_shape_dims[1] = output.shape().Dim(\n+      static_cast<size_t>(op.attributes.output_feature_dimension));\n+  for (size_t i = 0; i < output.Rank() - 2; ++i) {\n+    output_shape_dims[i + 2] =\n+        output.shape().Dim(static_cast<size_t>(output_spacial_pointer[i]));\n+    output_permutation_values[i + 2] = output_spacial_pointer[i];\n+  }\n+  op.output_permutation_data = malloc(output.Rank() * sizeof(StorageT));\n+  memmove(op.output_permutation_data, output_permutation_values.data(),\n+          output_permutation_values.size() * sizeof(StorageT));\n+  const Shape output_permutation_shape({static_cast<int64_t>(output.Rank())});\n+  Tensor output_permutations{\n+      .type = TensorType{.shape = output_permutation_shape,\n+                         .element_type = storage_type},\n+      .data = op.output_permutation_data};\n+\n+  op.output_transposed_data = malloc(output.NumElements() * sizeof(StorageT));\n+  const Shape output_transposed_shape(output_shape_dims);\n+  Tensor output_transposed{.type = TensorType{.shape = output_transposed_shape,\n+                                              .element_type = storage_type},\n+                           .data = op.output_transposed_data};\n+  // transpose prepare end\n+\n+  // DotGeneral prepare\n+  DimVector<DimensionSize> dims(rhs_transposed.Rank());\n+  size_t rhs_transposed_tensor_size = 1;\n+  dims[0] = 1;\n+  for (size_t i = 1; i < rhs_transposed.Rank(); ++i) {\n+    dims[i] = rhs_transposed.shape().Dim(i);\n+    rhs_transposed_tensor_size *= rhs_transposed.shape().Dim(i);\n+  }\n+  const Shape rhs_dot_general_shape(dims);\n+  op.rhs_dot_general_data =\n+      malloc(rhs_transposed_tensor_size * sizeof(StorageT));\n+  Tensor rhs_dot_general{.type = TensorType{.shape = rhs_dot_general_shape,\n+                                            .element_type = storage_type},\n+                         .data = op.rhs_dot_general_data};\n+\n+  op.lhs_dot_general_data =\n+      malloc(rhs_transposed_tensor_size * sizeof(StorageT));\n+  Tensor lhs_dot_general{.type = TensorType{.shape = rhs_dot_general_shape,\n+                                            .element_type = storage_type},\n+                         .data = op.lhs_dot_general_data};\n+\n+  std::vector<typename Storage<DataType::kSI64>::Type>\n+      lhs_contracting_dimensions_values(lhs_transposed.Rank() - 1);\n+  for (size_t i = 0; i < lhs_transposed.Rank() - 1; ++i) {\n+    lhs_contracting_dimensions_values[i] = i + 1;\n+  }\n+  op.lhs_contracting_dimensions_data =\n+      malloc((lhs_transposed.Rank() - 1) * sizeof(int64_t));\n+  memmove(op.lhs_contracting_dimensions_data,\n+          lhs_contracting_dimensions_values.data(),\n+          lhs_contracting_dimensions_values.size() * sizeof(int64_t));\n+  const Shape lhs_contracting_dimensions_shape(\n+      {static_cast<int64_t>(lhs_transposed.Rank() - 1)});\n+  Tensor lhs_contracting_dimensions{\n+      .type = TensorType{.shape = lhs_contracting_dimensions_shape,\n+                         .element_type = DataType::kSI64},\n+      .data = op.lhs_contracting_dimensions_data};\n+\n+  std::vector<typename Storage<DataType::kSI64>::Type>\n+      rhs_contracting_dimensions_values(rhs_transposed.Rank() - 1);\n+  for (size_t i = 0; i < rhs_transposed.Rank() - 1; ++i) {\n+    rhs_contracting_dimensions_values[i] = i + 1;\n+  }\n+  op.rhs_contracting_dimensions_data =\n+      malloc((rhs_transposed.Rank() - 1) * sizeof(int64_t));\n+  memmove(op.rhs_contracting_dimensions_data,\n+          rhs_contracting_dimensions_values.data(),\n+          rhs_contracting_dimensions_values.size() * sizeof(int64_t));\n+  Tensor rhs_contracting_dimensions{\n+      .type = TensorType{.shape = lhs_contracting_dimensions_shape,\n+                         .element_type = DataType::kSI8},\n+      .data = op.rhs_contracting_dimensions_data};\n+\n+  std::vector<StorageT> dor_general_output_values(1);\n+  dor_general_output_values[0] = 0;\n+  op.output_dot_general_data = malloc(1 * sizeof(StorageT));\n+  memmove(op.output_dot_general_data, dor_general_output_values.data(),\n+          dor_general_output_values.size() * sizeof(StorageT));\n+  const Shape dor_general_output_shape{{1}};\n+  Tensor output_dot_general{\n+      .type = TensorType{.shape = dor_general_output_shape,\n+                         .element_type = storage_type},\n+      .data = op.output_dot_general_data};\n+\n+  Tensor lhs_batching_dimensions{\n+      .type = TensorType{.shape = Shape(), .element_type = DataType::kSI8},\n+      .data = {}};\n+  Tensor rhs_batching_dimensions{\n+      .type = TensorType{.shape = Shape(), .element_type = DataType::kSI8},\n+      .data = {}};\n+\n+  absl::InlinedVector<PrecisionTypes, 2> precision_configs = {\n+      PrecisionTypes::DEFAULT, PrecisionTypes::DEFAULT};\n+  op.dot_general_op = Create(DotGeneralOp::Attributes{\n+      .lhs_batching_dimensions = lhs_batching_dimensions,\n+      .rhs_batching_dimensions = rhs_batching_dimensions,\n+      .lhs_contracting_dimensions = lhs_contracting_dimensions,\n+      .rhs_contracting_dimensions = rhs_contracting_dimensions,\n+      .precision_configs = precision_configs});\n+\n+  auto state = Prepare(op.dot_general_op, lhs_dot_general, rhs_dot_general,",
        "comment_created_at": "2024-05-10T12:46:51+00:00",
        "comment_author": "LokeshReddyOVS-MCW",
        "comment_body": "stopped auto usage",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1458825530",
    "pr_number": 59465,
    "pr_file": "tensorflow/core/kernels/segment_reduction_ops_impl.h",
    "created_at": "2024-01-19T11:17:54+00:00",
    "commented_code": "}\n       }\n     };\n-\n+    auto reductionWorker1D = [&](int64_t begin, int64_t end) -> void {\n+      for (int64_t i = 0; i < N; i++) {\n+        Index j = internal::SubtleMustCopy(segment_ids(i));\n+        // If `j` is in work scope of this worker, do the reduction.\n+        if (j >= begin && j < end) {\n+            reduction(data_ptr[i], out_ptr[j]);\n+        }\n+      }\n+    };\n     // Reduction functors includes Sum, Max, Min, etc. Simply consider it\n     // will cost 5 cycles per operation.\n     const int64_t kAverTaskSize = num_real_segment / num_segments;\n     const int64_t compute_cycles = 5 * inner_dim * kAverTaskSize;\n     const int64_t input_bytes = sizeof(T) * inner_dim * kAverTaskSize;\n     const int64_t output_bytes = sizeof(T) * inner_dim * kAverTaskSize;\n     const Eigen::TensorOpCost cost(input_bytes, output_bytes, compute_cycles);\n-    cpu_device.parallelFor(num_segments, cost, reductionWorker);\n+    if(is_inner_dim_1d) {\n+        cpu_device.parallelFor(num_segments, cost, reductionWorker1D);\n+    }\n+    else {\n+        cpu_device.parallelFor(num_segments, cost, reductionWorker);\n+    }",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1458825530",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 59465,
        "pr_file": "tensorflow/core/kernels/segment_reduction_ops_impl.h",
        "discussion_id": "1458825530",
        "commented_code": "@@ -410,15 +414,29 @@ struct UnsortedSegmentFunctor<CPUDevice, T, Index, InitialValueF, ReductionF> {\n         }\n       }\n     };\n-\n+    auto reductionWorker1D = [&](int64_t begin, int64_t end) -> void {\n+      for (int64_t i = 0; i < N; i++) {\n+        Index j = internal::SubtleMustCopy(segment_ids(i));\n+        // If `j` is in work scope of this worker, do the reduction.\n+        if (j >= begin && j < end) {\n+            reduction(data_ptr[i], out_ptr[j]);\n+        }\n+      }\n+    };\n     // Reduction functors includes Sum, Max, Min, etc. Simply consider it\n     // will cost 5 cycles per operation.\n     const int64_t kAverTaskSize = num_real_segment / num_segments;\n     const int64_t compute_cycles = 5 * inner_dim * kAverTaskSize;\n     const int64_t input_bytes = sizeof(T) * inner_dim * kAverTaskSize;\n     const int64_t output_bytes = sizeof(T) * inner_dim * kAverTaskSize;\n     const Eigen::TensorOpCost cost(input_bytes, output_bytes, compute_cycles);\n-    cpu_device.parallelFor(num_segments, cost, reductionWorker);\n+    if(is_inner_dim_1d) {\n+        cpu_device.parallelFor(num_segments, cost, reductionWorker1D);\n+    }\n+    else {\n+        cpu_device.parallelFor(num_segments, cost, reductionWorker);\n+    }",
        "comment_created_at": "2024-01-19T11:17:54+00:00",
        "comment_author": "penpornk",
        "comment_body": "Nit: Please apply code formatter. The formatted code will probably look like:\r\n```suggestion\r\n    if (is_inner_dim_1d) {\r\n      cpu_device.parallelFor(num_segments, cost, reductionWorker1D);\r\n    } else {\r\n      cpu_device.parallelFor(num_segments, cost, reductionWorker);\r\n    }\r\n```\r\n\r\nAlternatively, we could also do\r\n```\r\n    cpu_device.parallelFor(\r\n        num_segments, cost,\r\n        is_inner_dim_1d ? reductionWorker1D : reductionWorker);\r\n```\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1459680536",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 59465,
        "pr_file": "tensorflow/core/kernels/segment_reduction_ops_impl.h",
        "discussion_id": "1458825530",
        "commented_code": "@@ -410,15 +414,29 @@ struct UnsortedSegmentFunctor<CPUDevice, T, Index, InitialValueF, ReductionF> {\n         }\n       }\n     };\n-\n+    auto reductionWorker1D = [&](int64_t begin, int64_t end) -> void {\n+      for (int64_t i = 0; i < N; i++) {\n+        Index j = internal::SubtleMustCopy(segment_ids(i));\n+        // If `j` is in work scope of this worker, do the reduction.\n+        if (j >= begin && j < end) {\n+            reduction(data_ptr[i], out_ptr[j]);\n+        }\n+      }\n+    };\n     // Reduction functors includes Sum, Max, Min, etc. Simply consider it\n     // will cost 5 cycles per operation.\n     const int64_t kAverTaskSize = num_real_segment / num_segments;\n     const int64_t compute_cycles = 5 * inner_dim * kAverTaskSize;\n     const int64_t input_bytes = sizeof(T) * inner_dim * kAverTaskSize;\n     const int64_t output_bytes = sizeof(T) * inner_dim * kAverTaskSize;\n     const Eigen::TensorOpCost cost(input_bytes, output_bytes, compute_cycles);\n-    cpu_device.parallelFor(num_segments, cost, reductionWorker);\n+    if(is_inner_dim_1d) {\n+        cpu_device.parallelFor(num_segments, cost, reductionWorker1D);\n+    }\n+    else {\n+        cpu_device.parallelFor(num_segments, cost, reductionWorker);\n+    }",
        "comment_created_at": "2024-01-19T20:06:54+00:00",
        "comment_author": "cantonios",
        "comment_body": "The types are different, so you can't combine them in a ternary expression.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "525552070",
    "pr_number": 44851,
    "pr_file": "tensorflow/compiler/mlir/tosa/transforms/legalize_common.cc",
    "created_at": "2020-11-17T21:58:17+00:00",
    "commented_code": "+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file contains legalizations common to mapping both TensorFlow and\n+// TensorFlow Lite to TOSA.\n+//\n+// Conversion functions return nullptr on a lowerization failure or a\n+// lowered operator on success.  Callers must check and return a\n+// LogicalResult failure on nullptr.  Helper macros are provided in\n+// legalize_common.h to canonicalize this handling.\n+\n+#include \"tensorflow/compiler/mlir/tosa/transforms/legalize_common.h\"\n+\n+#include <climits>\n+#include <cstddef>\n+#include <cstdint>\n+#include <iterator>\n+#include <numeric>\n+\n+#include \"tensorflow/compiler/mlir/tosa/transforms/legalize_utils.h\"\n+\n+namespace mlir {\n+namespace tosa {\n+\n+// Lowers the Pack operator to TOSA.\n+Operation* convertPackOp(PatternRewriter& rewriter, Operation* op,\n+                         Value result_value, SmallVector<Value, 8>& inputs,\n+                         int32_t axis) {\n+  //////////////////////////////////////////////////\n+  // Operator: output = Pack([values], axis) or output = Stack([values], axis)\n+  // Lowering:\n+  //\n+  // This operator is lowered into a series of pairwise tosa.concat()\n+  // operators and a reshape\n+  // Depending on the inputs, a tranpose operator is also generated:\n+  //\n+  // Step 1: concatenate the tensors\n+  // a1_concat = tosa.concat(input[0], input[1], axis)\n+  // for (i = 2; i < len(input); i++)\n+  //   a1_concat = tosa.concat(a1_concat, input[i], axis)\n+  //\n+  // Step 2: reshape to N+1 dimensions\n+  // a2_reshape = tosa.reshape(a1_concat, new_rank)\n+  //\n+  // Step 3: Transpose if a new dimension is being added:\n+  // if (axis == rank(values[0]):\n+  //   // perm will be [1, 2, 3, 0]\n+  //   a3_transpose = tosa.transpose(a2_reshape, perm)\n+\n+  // Sanity check 1: make sure all input tensors have the same shape\n+  // if input[0] has shape [A, B, C], input[1] to input[N-1] should also have\n+  // shape[A, B, C]\n+  auto result_type = result_value.getType().dyn_cast<RankedTensorType>();\n+\n+  // Check for ranked tensor type.\n+  if (!result_type) {\n+    op->emitOpError(\"PackOp: result type not ranked tensor\");\n+    return nullptr;\n+  }\n+\n+  // Valid axis in TF is [-rank(input), rank(input))\n+  // Valid axis in TOSA is [0, rank(input))\n+  // Plus rank(input) once if axis is negative.\n+  auto input_type = op->getOperand(0).getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) {\n+    op->emitOpError(\"PackOp: input type not ranked tensor\");\n+    return nullptr;\n+  }\n+\n+  auto input_rank = input_type.getShape().size();\n+  if (axis < 0) axis += input_rank;\n+\n+  input_type = inputs[0].getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) {\n+    op->emitOpError(\"Input 0 type not ranked tensor.\");\n+    return nullptr;\n+  }\n+  ArrayRef<int64_t> input0_tensor_shape = input_type.getShape();\n+  int input_tensor_rank = input0_tensor_shape.size();\n+\n+  for (int i = 1; i < inputs.size(); i++) {\n+    input_type = inputs[0].getType().dyn_cast<RankedTensorType>();\n+    if (!input_type) {\n+      op->emitOpError(llvm::formatv(\n+          \"reduce axis {} is not in valid range [-rank(input), rank(input))\",\n+          i));\n+      return nullptr;\n+    }\n+    ArrayRef<int64_t> next_tensor_shape = input_type.getShape();\n+    if (next_tensor_shape.size() != input_tensor_rank) {\n+      op->emitOpError(\"PackOp: input tensor rank mismatch.\");\n+      return nullptr;\n+    }\n+    for (int d = 0; d < input0_tensor_shape.size(); d++) {\n+      if (input0_tensor_shape[d] != next_tensor_shape[d]) {\n+        op->emitOpError(\"PackOp: input tensor shape mismatch.\");\n+        return nullptr;\n+      }\n+    }\n+  }\n+\n+  // If input tensors are rank 0, should reshape them to rank 1 size 1 before\n+  // performing concat.\n+  if (input_tensor_rank == 0) {\n+    SmallVector<int64_t, 8> reshape_rank1_size1_shape{1};\n+    auto reshape_rank1_size1_type =\n+        RankedTensorType::get(ArrayRef<int64_t>(reshape_rank1_size1_shape),\n+                              result_type.getElementType());\n+    ArrayAttr shape_rank1_size1_attr =\n+        rewriter.getI64ArrayAttr(reshape_rank1_size1_shape);\n+    for (int i = 0; i < inputs.size(); i++) {\n+      auto a0_reshape_op = rewriter.create<tosa::ReshapeOp>(\n+          op->getLoc(), reshape_rank1_size1_type, inputs[i],\n+          shape_rank1_size1_attr);\n+      inputs[i] = a0_reshape_op.getResult();\n+    }\n+  }\n+\n+  // Sanity check 2: axis can be from [0, rank(input)+1]\n+  // Where rank(input)+1 means create a new dimension\n+  // Negative values are also allowed up to -(rank(input)+1)\n+  // where the axis \"wraps around\".\n+  if (axis < 0) axis += input_rank;\n+\n+  if (axis > (input_tensor_rank + 1)) {\n+    op->emitOpError(\"PackOp: axis out of valid range.\");\n+    return nullptr;\n+  }\n+\n+  // Sanity check 2: if input shape is [A, B, C], output shape should be [N,\n+  // A, B, C]\n+  // 2.a check output is rank(input) + 1\n+  SmallVector<int64_t, 8> output_shape_vals(result_type.getShape().begin(),\n+                                            result_type.getShape().end());\n+  if (output_shape_vals.size() != (input_tensor_rank + 1)) {\n+    op->emitOpError(\"PackOp: output tensor rank mismatch.\");\n+    return nullptr;\n+  }\n+  // 2.b check output rank 0 is N\n+  if (output_shape_vals[axis] != inputs.size()) {\n+    op->emitOpError(\"PackOp: output tensor shape mismatch.\");\n+    return nullptr;\n+  }\n+  ArrayAttr output_shape_attr = rewriter.getI64ArrayAttr(output_shape_vals);\n+\n+  // Most of the cases when PackOp.axis() is within [0, rank(input) - 1].\n+  // We can directly concatenate along that axis and perform the reshape.\n+  // For example, stack N [A, B, C] input tensor ranks along axis = 1\n+  // after concatenation, output will be [A, N * B, C]\n+  // and then reshape it into [A, N, B, C]\n+  // a special case would be PackOp.axis() equal to rank(input), in which case\n+  // we can't directly concatenate along the PackOp.axis(), instead\n+  // we concat along axis=0, and reshape into [N, A, B, C]\n+  // and then we need an extra transpose to [A, B, C, N].\n+  int64_t concat_axis;\n+  SmallVector<int32_t, 8> perm;\n+  SmallVector<int64_t, 8> reshape_output_shape;\n+  if (axis == 0 && input_tensor_rank == 0) {\n+    concat_axis = 0;\n+    // Don't need reshape and perm, since we inputs are reshaped into rank 1\n+    // size 1.  Output will be rank 1 size N.\n+  } else if (axis == input_tensor_rank) {\n+    concat_axis = 0;\n+\n+    // A special case when stack axis is equal to input tensor rank:\n+    // Output shape is [A, B, C, N]\n+    // so reshape output will be [N, A, B, C]\n+    // and perm will be [1, 2, 3, 0].\n+    reshape_output_shape.push_back(output_shape_vals[axis]);\n+    for (int d = 0; d < input_tensor_rank; d++) {\n+      perm.push_back(d + 1);\n+      reshape_output_shape.push_back(output_shape_vals[d]);\n+    }\n+    perm.push_back(0);\n+  } else {\n+    // General case, doesn't need perm vector.\n+    concat_axis = axis;\n+    reshape_output_shape.assign(output_shape_vals.begin(),\n+                                output_shape_vals.end());\n+  }\n+  IntegerAttr concat_axis_attr = rewriter.getI64IntegerAttr(concat_axis);\n+  ArrayAttr shape_attr = rewriter.getI64ArrayAttr(reshape_output_shape);\n+\n+  // For each concat output, shape will be different.\n+  // If input shape is [A, B, C] and concat_axis = 0, 1st concat output will\n+  // be [2 * A, B, C].\n+  int orig_input_dim_on_axis;\n+  std::vector<int64_t> concat_output_shape;\n+  if (input_tensor_rank == 0) {\n+    concat_output_shape.push_back(1);\n+    orig_input_dim_on_axis = 1;\n+  } else {\n+    for (int i = 0; i < input_tensor_rank; i++) {\n+      concat_output_shape.push_back(input0_tensor_shape[i]);\n+    }\n+    orig_input_dim_on_axis = input0_tensor_shape[concat_axis];\n+  }\n+\n+  concat_output_shape[concat_axis] = orig_input_dim_on_axis * 2;\n+  auto concat_type = RankedTensorType::get(\n+      ArrayRef<int64_t>(concat_output_shape), result_type.getElementType());\n+  auto a1_concat_op = rewriter.create<tosa::ConcatOp>(\n+      op->getLoc(), concat_type, inputs[0], inputs[1], concat_axis_attr);\n+\n+  // K-th concat output will be [(k+1) * A, B, C], last output will be [N * A,\n+  // B, C].\n+  for (int i = 2; i < inputs.size(); i++) {\n+    concat_output_shape[concat_axis] = orig_input_dim_on_axis * (i + 1);\n+    concat_type = RankedTensorType::get(ArrayRef<int64_t>(concat_output_shape),\n+                                        result_type.getElementType());\n+    a1_concat_op = rewriter.create<tosa::ConcatOp>(op->getLoc(), concat_type,\n+                                                   a1_concat_op.getResult(),\n+                                                   inputs[i], concat_axis_attr);\n+  }\n+\n+  Operation* lowered_op = nullptr;\n+  // Doesn't need reshape or transpose if input tensor is rank 0, since inputs\n+  // are reshaped beforehand.\n+  if (input_tensor_rank == 0) {\n+    lowered_op = a1_concat_op;\n+  } else {\n+    // Reshape [N * A, B, C] to [N, A, B, C].\n+    auto reshape_output_type = RankedTensorType::get(\n+        ArrayRef<int64_t>(reshape_output_shape), result_type.getElementType());\n+\n+    auto a2_reshape_op =\n+        rewriter.create<tosa::ReshapeOp>(op->getLoc(), reshape_output_type,\n+                                         a1_concat_op.getResult(), shape_attr);\n+\n+    // If axis is equal to input tensor rank, then we need extra transpose\n+    // [N, A, B, C] to [A, B, C, N]\n+    if (axis == input_tensor_rank) {\n+      auto a3_transpose_perm =\n+          get1DConstTensor<tosa::ConstOp, int32_t>(rewriter, op, perm);\n+      auto a3_transpose_op = rewriter.create<tosa::TransposeOp>(\n+          op->getLoc(), result_type, a2_reshape_op.getResult(),\n+          a3_transpose_perm);\n+      lowered_op = a3_transpose_op;\n+    } else {\n+      lowered_op = a2_reshape_op;\n+    }\n+  }\n+\n+  return lowered_op;\n+}\n+\n+// Lowers the Unpack operator to TOSA\n+Operation* convertUnpackOp(PatternRewriter& rewriter, Operation* op,\n+                           Value input_value, int32_t axis) {\n+  auto input_type = input_value.getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) return nullptr;\n+\n+  auto input_shape = input_type.getShape();\n+  int64_t input_rank = input_shape.size();\n+\n+  SmallVector<Value, 4> results_vec;\n+\n+  // Negative axis allowed as long as it's within [-input_rank, input_rank).\n+  if (axis < 0) axis += input_rank;\n+\n+  assert(axis > 0 && axis < input_shape.size());\n+\n+  // A list of the output types for each slice op\n+  SmallVector<Type, 4> outs_type_vec;\n+\n+  // Step 1: transpose 'axis' to leftmost dimension.\n+  Value transposed_input_value;\n+  if (axis != 0) {\n+    SmallVector<int32_t, 8> perm_vec;\n+    SmallVector<int64_t, 2> a1_transpose_shape(input_rank);\n+\n+    perm_vec.push_back(axis);\n+    for (int i = 0; i < input_rank; i++) {\n+      if (i == axis) continue;\n+      perm_vec.push_back(i);\n+    }\n+\n+    auto a1_transpose_perm =\n+        get1DConstTensor<tosa::ConstOp, int32_t>(rewriter, op, perm_vec);\n+\n+    for (int i = 0; i < input_rank; i++) {\n+      a1_transpose_shape[i] = input_shape[perm_vec[i]];\n+    }\n+\n+    auto a1_transpose_op = rewriter.create<tosa::TransposeOp>(\n+        op->getLoc(),\n+        RankedTensorType::get(ArrayRef<int64_t>(a1_transpose_shape),\n+                              input_type.getElementType()),\n+        input_value, a1_transpose_perm);\n+\n+    transposed_input_value = a1_transpose_op.getResult();\n+  } else {\n+    // Do nothing if axis is already at leftmost dimension.\n+    transposed_input_value = input_value;\n+  }\n+\n+  // Step 2: slice [N, A, B, C] into N [A, B, C].\n+  auto transposed_input_type =\n+      transposed_input_value.getType().dyn_cast<RankedTensorType>();\n+  if (!transposed_input_type) return nullptr;\n+\n+  auto transposed_input_shape = transposed_input_type.getShape();\n+  int64_t transposed_input_rank = transposed_input_shape.size();\n+\n+  for (int i = 0; i < transposed_input_shape[0]; i++) {\n+    SmallVector<int64_t, 4> begin_vals, size_vals, shape_vals;\n+\n+    for (int j = 0; j < transposed_input_rank; j++) {\n+      if (j == 0) {\n+        begin_vals.push_back(i);\n+        size_vals.push_back(1);\n+      } else {\n+        begin_vals.push_back(0);\n+        size_vals.push_back(transposed_input_shape[j]);\n+        shape_vals.push_back(transposed_input_shape[j]);\n+      }\n+    }\n+\n+    ArrayAttr begin = rewriter.getI64ArrayAttr(begin_vals);\n+    ArrayAttr size = rewriter.getI64ArrayAttr(size_vals);\n+\n+    auto a2_slice_op = rewriter.create<tosa::SliceOp>(\n+        op->getLoc(),\n+        RankedTensorType::get(ArrayRef<int64_t>(size_vals),\n+                              transposed_input_type.getElementType()),\n+        transposed_input_value, begin, size);\n+\n+    auto a3_reshape_op = rewriter.create<tosa::ReshapeOp>(\n+        op->getLoc(),\n+        RankedTensorType::get(ArrayRef<int64_t>(shape_vals),\n+                              transposed_input_type.getElementType()),\n+        a2_slice_op.getResult(), rewriter.getI64ArrayAttr(shape_vals));\n+\n+    outs_type_vec.push_back(RankedTensorType::get(\n+        ArrayRef<int64_t>(shape_vals), transposed_input_type.getElementType()));\n+\n+    results_vec.push_back(a3_reshape_op.getResult());\n+  }\n+\n+  // Combine the sequence of tosa.slice() ops into a list\n+  // using the IdentityN operator.\n+  auto identityn_op = rewriter.create<tosa::IdentityNOp>(\n+      op->getLoc(), ArrayRef<Type>(outs_type_vec), results_vec);\n+\n+  return identityn_op;\n+}\n+\n+// Lowers the Select operator to TOSA.\n+Operation* convertSelectOp(PatternRewriter& rewriter, Operation* op,\n+                           Value result_value, Value condition_value,\n+                           Value x_value, Value y_value) {\n+  auto result_type = result_value.getType().dyn_cast<RankedTensorType>();\n+  auto condition_type = condition_value.getType().dyn_cast<RankedTensorType>();\n+  auto x_type = x_value.getType().dyn_cast<RankedTensorType>();\n+  auto y_type = y_value.getType().dyn_cast<RankedTensorType>();\n+\n+  Operation* result_op = nullptr;\n+\n+  if (!result_type || !condition_type || !x_type || !y_type) {\n+    op->emitOpError(\"Select: failed ranked tensor type check\");\n+    return nullptr;\n+  }\n+\n+  // First check whether we need to reshape the condition to match\n+  // the same rank as the then/else clauses.\n+  if (result_type.getRank() == condition_type.getRank()) {\n+    // Nothing to reshape.\n+    result_op = rewriter.create<tosa::SelectOp>(\n+        op->getLoc(), result_type, condition_value, x_value, y_value);\n+  } else {\n+    // Need to reshape the condition.\n+    SmallVector<int64_t, 8> new_cond_dims;\n+    for (int i = 0; i < (result_type.getRank() - condition_type.getRank());\n+         i++) {\n+      new_cond_dims.push_back(1);\n+    }\n+    for (int i = 0; i < condition_type.getRank(); i++) {\n+      new_cond_dims.push_back(condition_type.getShape()[i]);\n+    }\n+\n+    auto reshape_op = rewriter.create<tosa::ReshapeOp>(\n+        op->getLoc(),\n+        RankedTensorType::get(ArrayRef<int64_t>(new_cond_dims),\n+                              condition_type.getElementType()),\n+        condition_value, rewriter.getI64ArrayAttr(new_cond_dims));\n+\n+    auto new_select = rewriter.create<tosa::SelectOp>(\n+        op->getLoc(), result_type, reshape_op, x_value, y_value);\n+    result_op = new_select;\n+  }\n+\n+  return result_op;\n+}\n+\n+// Lowers the ZerosLike operator to TOSA by creating a constant\n+// of the desired type and shape.\n+Operation* convertZerosLikeOp(PatternRewriter& rewriter, Operation* op,\n+                              Value result, Value input) {\n+  auto result_type = result.getType().dyn_cast<RankedTensorType>();\n+  if (!result_type) {\n+    op->emitOpError(\"Zeroslike: result not ranked tensor type\");\n+    return nullptr;\n+  }\n+\n+  auto input_type = input.getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) {\n+    op->emitOpError(\"Zeroslike: input not ranked tensor type\");\n+    return nullptr;\n+  }\n+\n+  auto input_shape = input_type.getShape();\n+\n+  ShapedType zero_type =\n+      RankedTensorType::get(input_shape, input_type.getElementType());\n+  Attribute zero_attr = rewriter.getZeroAttr(zero_type);\n+\n+  auto const_op =\n+      rewriter.create<tosa::ConstOp>(op->getLoc(), zero_type, zero_attr);\n+\n+  return const_op;\n+}\n+\n+// Lowers the Mul operator to TOSA.  For quantized types, this requires\n+// inserting rescale operators before and after the operation.\n+Operation* convertMultiplyOp(PatternRewriter& rewriter, Operation* op,\n+                             Value output_val, Value input_lhs_val,\n+                             Value input_rhs_val) {\n+  auto input_lhs_type = input_lhs_val.getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type = input_rhs_val.getType().dyn_cast<RankedTensorType>();\n+  auto output_type = output_val.getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return nullptr;\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertMultiplyOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return nullptr;\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    double in_lhs_scale = input_lhs_qtype.getScale();\n+    double in_rhs_scale = input_rhs_qtype.getScale();\n+    double output_scale = output_qtype.getScale();\n+\n+    double output_rescale_scale = in_lhs_scale * in_rhs_scale / output_scale;\n+\n+    auto op1_rescale_lhs = buildRescaleToInt32(\n+        rewriter, op, input_lhs_val, 1.0f, input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs = buildRescaleToInt32(\n+        rewriter, op, input_rhs_val, 1.0f, input_rhs_qtype.getZeroPoint());\n+    auto op3_mul_op1_op2 = rewriter.create<tosa::MulOp>(\n+        op->getLoc(), rescale_type, op1_rescale_lhs, op2_rescale_rhs, 0);\n+    auto op4_rescale_op3 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op3_mul_op1_op2.getResult(),\n+        output_rescale_scale, output_qtype.getZeroPoint());\n+    output = op4_rescale_op3;\n+  } else {\n+    auto op1_mul_in = rewriter.create<tosa::MulOp>(\n+        op->getLoc(), output_type, input_lhs_val, input_rhs_val, 0);\n+\n+    output = op1_mul_in.getResult();\n+  }\n+\n+  return output.getDefiningOp();\n+}\n+\n+// Lowers the SquaredDifference operator to TOSA.\n+Operation* convertSquaredDifferenceOp(PatternRewriter& rewriter, Operation* op,\n+                                      Value result, Value x, Value y) {\n+  // Squared-difference is (x-y)*(x-y).\n+  // This lowering calculates the difference and multiplies.\n+  auto result_type = result.getType().dyn_cast<RankedTensorType>();\n+  if (!result_type) {\n+    op->emitOpError(\"SquaredDifference: result not ranked tensor type\");\n+    return nullptr;\n+  }\n+\n+  auto x_type = x.getType().dyn_cast<RankedTensorType>();\n+  auto y_type = y.getType().dyn_cast<RankedTensorType>();\n+  if (!x_type || !y_type) {\n+    op->emitOpError(\"SquaredDifference: inputs not ranked tensor type\");\n+    return nullptr;\n+  }\n+\n+  auto sub_op = rewriter.create<tosa::SubOp>(op->getLoc(), result_type, x, y);\n+  auto square_op = rewriter.create<tosa::MulOp>(\n+      op->getLoc(), result_type, sub_op.getResult(), sub_op.getResult(), 0);\n+  return square_op;\n+}\n+\n+// Lowers the Round operator to TOSA.\n+Operation* convertRoundOp(PatternRewriter& rewriter, Operation* op,\n+                          Value result, Value input) {\n+  // Implements banker's rounding by calculating floor(input + 0.5).\n+  auto result_type = result.getType().dyn_cast<RankedTensorType>();\n+  if (!result_type) {\n+    op->emitOpError(\"Round: result not ranked tensor type\");\n+    return nullptr;\n+  }\n+\n+  auto input_type = input.getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) {\n+    op->emitOpError(\"Round: input not ranked tensor type\");\n+    return nullptr;\n+  }\n+\n+  auto add_op = rewriter.create<tosa::AddOp>(\n+      op->getLoc(), result_type, input,\n+      getTosaConstTensorSingleF32(rewriter, op, 0.5));\n+  auto floor_op = rewriter.create<tosa::FloorOp>(op->getLoc(), result_type,\n+                                                 add_op.getResult());\n+\n+  return floor_op;\n+}\n+\n+// Lowers ConcatV2 to TOSA.\n+Operation* convertConcatV2Op(PatternRewriter& rewriter, Operation* op,\n+                             Value result_value, SmallVector<Value, 8>& values,\n+                             int32_t axis) {\n+  // ConcatV2 becomes a series of TOSA Concat operators that take pairs of\n+  // tensors as arguments.   Rank-0 tensors are reshaped to Rank-1,\n+  // shape (1,) tensors.\n+  auto result_type = result_value.getType().dyn_cast<RankedTensorType>();\n+  if (!result_type) {\n+    op->emitOpError(\"ConcatV2Op: result type not ranked tensor.\");\n+    return nullptr;\n+  }\n+\n+  // Valid axis in TF is [-rank(input), rank(input)).\n+  // Valid axis in TOSA is [0, rank(input)).\n+  // Plus rank(input) once if axis is negative.\n+  auto input_type = op->getOperand(0).getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) {\n+    op->emitOpError(\"ConcatV2Op: input type not ranked tensor.\");\n+    return nullptr;\n+  }\n+\n+  auto input_rank = input_type.getShape().size();\n+\n+  if (axis < 0) axis += input_rank;\n+\n+  assert(values.size() >= 2);\n+\n+  if (!values[0].getType().dyn_cast<RankedTensorType>() ||\n+      !values[1].getType().dyn_cast<RankedTensorType>()) {\n+    op->emitOpError(\"ConcatV2Op: value type not ranked tensor.\");\n+    return nullptr;\n+  }\n+\n+  Value lhs_val = values[0];\n+  Value rhs_val = values[1];\n+  auto lhs_type = lhs_val.getType().dyn_cast<RankedTensorType>();\n+  auto rhs_type = rhs_val.getType().dyn_cast<RankedTensorType>();\n+  ArrayRef<int64_t> lhs_tensor_shape = lhs_type.getShape();\n+  ArrayRef<int64_t> rhs_tensor_shape = rhs_type.getShape();\n+  int input_tensor_rank = lhs_tensor_shape.size();\n+\n+  // For each concat output, shape will be different.\n+  // If input tensors are rank 0, should reshape them to rank 1 size 1 before\n+  // performing concat. If not, most dimensions should have same size as input\n+  // except the concat'd axis.\n+  //\n+  // If input is [A0, B, C] and [A1, B, C] and axis = 0\n+  // this concat output will be [A0 + A1, B, C].\n+  std::vector<int64_t> concat_result_shape;\n+  if (input_tensor_rank == 0) {\n+    if (axis != 0) {\n+      op->emitOpError(\"ConcatV2Op: axis invalid.\");\n+      return nullptr;\n+    }\n+    SmallVector<int64_t, 8> reshape_rank1_size1_shape{1};\n+    auto reshape_rank1_size1_type =\n+        RankedTensorType::get(ArrayRef<int64_t>(reshape_rank1_size1_shape),\n+                              result_type.getElementType());\n+    ArrayAttr shape_rank1_size1_attr =\n+        rewriter.getI64ArrayAttr(reshape_rank1_size1_shape);\n+    for (int i = 0; i < values.size(); i++) {\n+      auto a0_reshape_op = rewriter.create<tosa::ReshapeOp>(\n+          op->getLoc(), reshape_rank1_size1_type, values[i],\n+          shape_rank1_size1_attr);\n+      values[i] = a0_reshape_op.getResult();\n+    }\n+    concat_result_shape.push_back(2);\n+  } else {\n+    if (axis < 0 || axis >= input_tensor_rank) {\n+      op->emitOpError(\"ConcatV2Op: axis invalid.\");\n+      return nullptr;\n+    }\n+    for (int i = 0; i < input_tensor_rank; i++) {\n+      concat_result_shape.push_back(lhs_tensor_shape[i]);\n+    }\n+    concat_result_shape[axis] = lhs_tensor_shape[axis] + rhs_tensor_shape[axis];\n+  }\n+\n+  auto concat_type = RankedTensorType::get(\n+      ArrayRef<int64_t>(concat_result_shape), result_type.getElementType());\n+\n+  mlir::quant::UniformQuantizedType lhs_quant_type =\n+      lhs_type.getElementType()\n+          .dyn_cast_or_null<mlir::quant::UniformQuantizedType>();\n+  mlir::quant::UniformQuantizedType rhs_quant_type =\n+      rhs_type.getElementType()\n+          .dyn_cast_or_null<mlir::quant::UniformQuantizedType>();\n+  mlir::quant::UniformQuantizedType result_quant_type =\n+      result_type.getElementType()\n+          .dyn_cast_or_null<mlir::quant::UniformQuantizedType>();\n+\n+  double lhs_scale, rhs_scale, result_scale;\n+  int32_t lhs_zeropoint, rhs_zeropoint, result_zeropoint;\n+  RankedTensorType const_type, i32_type;\n+  DenseElementsAttr const_attr;\n+\n+  // tfl.concat currently allows different scales for each input tensor, which\n+  // TFlite team will fix in:\n+  // https://github.com/tensorflow/tensorflow/issues/39658\n+  //\n+  // For backward compatibility, we still need to support this artifact by\n+  // scaling inputs to let them have the same scales.\n+  if (result_quant_type && lhs_quant_type && rhs_quant_type) {\n+    lhs_scale = (double)lhs_quant_type.getScale();",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "525552070",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 44851,
        "pr_file": "tensorflow/compiler/mlir/tosa/transforms/legalize_common.cc",
        "discussion_id": "525552070",
        "commented_code": "@@ -0,0 +1,2821 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file contains legalizations common to mapping both TensorFlow and\n+// TensorFlow Lite to TOSA.\n+//\n+// Conversion functions return nullptr on a lowerization failure or a\n+// lowered operator on success.  Callers must check and return a\n+// LogicalResult failure on nullptr.  Helper macros are provided in\n+// legalize_common.h to canonicalize this handling.\n+\n+#include \"tensorflow/compiler/mlir/tosa/transforms/legalize_common.h\"\n+\n+#include <climits>\n+#include <cstddef>\n+#include <cstdint>\n+#include <iterator>\n+#include <numeric>\n+\n+#include \"tensorflow/compiler/mlir/tosa/transforms/legalize_utils.h\"\n+\n+namespace mlir {\n+namespace tosa {\n+\n+// Lowers the Pack operator to TOSA.\n+Operation* convertPackOp(PatternRewriter& rewriter, Operation* op,\n+                         Value result_value, SmallVector<Value, 8>& inputs,\n+                         int32_t axis) {\n+  //////////////////////////////////////////////////\n+  // Operator: output = Pack([values], axis) or output = Stack([values], axis)\n+  // Lowering:\n+  //\n+  // This operator is lowered into a series of pairwise tosa.concat()\n+  // operators and a reshape\n+  // Depending on the inputs, a tranpose operator is also generated:\n+  //\n+  // Step 1: concatenate the tensors\n+  // a1_concat = tosa.concat(input[0], input[1], axis)\n+  // for (i = 2; i < len(input); i++)\n+  //   a1_concat = tosa.concat(a1_concat, input[i], axis)\n+  //\n+  // Step 2: reshape to N+1 dimensions\n+  // a2_reshape = tosa.reshape(a1_concat, new_rank)\n+  //\n+  // Step 3: Transpose if a new dimension is being added:\n+  // if (axis == rank(values[0]):\n+  //   // perm will be [1, 2, 3, 0]\n+  //   a3_transpose = tosa.transpose(a2_reshape, perm)\n+\n+  // Sanity check 1: make sure all input tensors have the same shape\n+  // if input[0] has shape [A, B, C], input[1] to input[N-1] should also have\n+  // shape[A, B, C]\n+  auto result_type = result_value.getType().dyn_cast<RankedTensorType>();\n+\n+  // Check for ranked tensor type.\n+  if (!result_type) {\n+    op->emitOpError(\"PackOp: result type not ranked tensor\");\n+    return nullptr;\n+  }\n+\n+  // Valid axis in TF is [-rank(input), rank(input))\n+  // Valid axis in TOSA is [0, rank(input))\n+  // Plus rank(input) once if axis is negative.\n+  auto input_type = op->getOperand(0).getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) {\n+    op->emitOpError(\"PackOp: input type not ranked tensor\");\n+    return nullptr;\n+  }\n+\n+  auto input_rank = input_type.getShape().size();\n+  if (axis < 0) axis += input_rank;\n+\n+  input_type = inputs[0].getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) {\n+    op->emitOpError(\"Input 0 type not ranked tensor.\");\n+    return nullptr;\n+  }\n+  ArrayRef<int64_t> input0_tensor_shape = input_type.getShape();\n+  int input_tensor_rank = input0_tensor_shape.size();\n+\n+  for (int i = 1; i < inputs.size(); i++) {\n+    input_type = inputs[0].getType().dyn_cast<RankedTensorType>();\n+    if (!input_type) {\n+      op->emitOpError(llvm::formatv(\n+          \"reduce axis {} is not in valid range [-rank(input), rank(input))\",\n+          i));\n+      return nullptr;\n+    }\n+    ArrayRef<int64_t> next_tensor_shape = input_type.getShape();\n+    if (next_tensor_shape.size() != input_tensor_rank) {\n+      op->emitOpError(\"PackOp: input tensor rank mismatch.\");\n+      return nullptr;\n+    }\n+    for (int d = 0; d < input0_tensor_shape.size(); d++) {\n+      if (input0_tensor_shape[d] != next_tensor_shape[d]) {\n+        op->emitOpError(\"PackOp: input tensor shape mismatch.\");\n+        return nullptr;\n+      }\n+    }\n+  }\n+\n+  // If input tensors are rank 0, should reshape them to rank 1 size 1 before\n+  // performing concat.\n+  if (input_tensor_rank == 0) {\n+    SmallVector<int64_t, 8> reshape_rank1_size1_shape{1};\n+    auto reshape_rank1_size1_type =\n+        RankedTensorType::get(ArrayRef<int64_t>(reshape_rank1_size1_shape),\n+                              result_type.getElementType());\n+    ArrayAttr shape_rank1_size1_attr =\n+        rewriter.getI64ArrayAttr(reshape_rank1_size1_shape);\n+    for (int i = 0; i < inputs.size(); i++) {\n+      auto a0_reshape_op = rewriter.create<tosa::ReshapeOp>(\n+          op->getLoc(), reshape_rank1_size1_type, inputs[i],\n+          shape_rank1_size1_attr);\n+      inputs[i] = a0_reshape_op.getResult();\n+    }\n+  }\n+\n+  // Sanity check 2: axis can be from [0, rank(input)+1]\n+  // Where rank(input)+1 means create a new dimension\n+  // Negative values are also allowed up to -(rank(input)+1)\n+  // where the axis \"wraps around\".\n+  if (axis < 0) axis += input_rank;\n+\n+  if (axis > (input_tensor_rank + 1)) {\n+    op->emitOpError(\"PackOp: axis out of valid range.\");\n+    return nullptr;\n+  }\n+\n+  // Sanity check 2: if input shape is [A, B, C], output shape should be [N,\n+  // A, B, C]\n+  // 2.a check output is rank(input) + 1\n+  SmallVector<int64_t, 8> output_shape_vals(result_type.getShape().begin(),\n+                                            result_type.getShape().end());\n+  if (output_shape_vals.size() != (input_tensor_rank + 1)) {\n+    op->emitOpError(\"PackOp: output tensor rank mismatch.\");\n+    return nullptr;\n+  }\n+  // 2.b check output rank 0 is N\n+  if (output_shape_vals[axis] != inputs.size()) {\n+    op->emitOpError(\"PackOp: output tensor shape mismatch.\");\n+    return nullptr;\n+  }\n+  ArrayAttr output_shape_attr = rewriter.getI64ArrayAttr(output_shape_vals);\n+\n+  // Most of the cases when PackOp.axis() is within [0, rank(input) - 1].\n+  // We can directly concatenate along that axis and perform the reshape.\n+  // For example, stack N [A, B, C] input tensor ranks along axis = 1\n+  // after concatenation, output will be [A, N * B, C]\n+  // and then reshape it into [A, N, B, C]\n+  // a special case would be PackOp.axis() equal to rank(input), in which case\n+  // we can't directly concatenate along the PackOp.axis(), instead\n+  // we concat along axis=0, and reshape into [N, A, B, C]\n+  // and then we need an extra transpose to [A, B, C, N].\n+  int64_t concat_axis;\n+  SmallVector<int32_t, 8> perm;\n+  SmallVector<int64_t, 8> reshape_output_shape;\n+  if (axis == 0 && input_tensor_rank == 0) {\n+    concat_axis = 0;\n+    // Don't need reshape and perm, since we inputs are reshaped into rank 1\n+    // size 1.  Output will be rank 1 size N.\n+  } else if (axis == input_tensor_rank) {\n+    concat_axis = 0;\n+\n+    // A special case when stack axis is equal to input tensor rank:\n+    // Output shape is [A, B, C, N]\n+    // so reshape output will be [N, A, B, C]\n+    // and perm will be [1, 2, 3, 0].\n+    reshape_output_shape.push_back(output_shape_vals[axis]);\n+    for (int d = 0; d < input_tensor_rank; d++) {\n+      perm.push_back(d + 1);\n+      reshape_output_shape.push_back(output_shape_vals[d]);\n+    }\n+    perm.push_back(0);\n+  } else {\n+    // General case, doesn't need perm vector.\n+    concat_axis = axis;\n+    reshape_output_shape.assign(output_shape_vals.begin(),\n+                                output_shape_vals.end());\n+  }\n+  IntegerAttr concat_axis_attr = rewriter.getI64IntegerAttr(concat_axis);\n+  ArrayAttr shape_attr = rewriter.getI64ArrayAttr(reshape_output_shape);\n+\n+  // For each concat output, shape will be different.\n+  // If input shape is [A, B, C] and concat_axis = 0, 1st concat output will\n+  // be [2 * A, B, C].\n+  int orig_input_dim_on_axis;\n+  std::vector<int64_t> concat_output_shape;\n+  if (input_tensor_rank == 0) {\n+    concat_output_shape.push_back(1);\n+    orig_input_dim_on_axis = 1;\n+  } else {\n+    for (int i = 0; i < input_tensor_rank; i++) {\n+      concat_output_shape.push_back(input0_tensor_shape[i]);\n+    }\n+    orig_input_dim_on_axis = input0_tensor_shape[concat_axis];\n+  }\n+\n+  concat_output_shape[concat_axis] = orig_input_dim_on_axis * 2;\n+  auto concat_type = RankedTensorType::get(\n+      ArrayRef<int64_t>(concat_output_shape), result_type.getElementType());\n+  auto a1_concat_op = rewriter.create<tosa::ConcatOp>(\n+      op->getLoc(), concat_type, inputs[0], inputs[1], concat_axis_attr);\n+\n+  // K-th concat output will be [(k+1) * A, B, C], last output will be [N * A,\n+  // B, C].\n+  for (int i = 2; i < inputs.size(); i++) {\n+    concat_output_shape[concat_axis] = orig_input_dim_on_axis * (i + 1);\n+    concat_type = RankedTensorType::get(ArrayRef<int64_t>(concat_output_shape),\n+                                        result_type.getElementType());\n+    a1_concat_op = rewriter.create<tosa::ConcatOp>(op->getLoc(), concat_type,\n+                                                   a1_concat_op.getResult(),\n+                                                   inputs[i], concat_axis_attr);\n+  }\n+\n+  Operation* lowered_op = nullptr;\n+  // Doesn't need reshape or transpose if input tensor is rank 0, since inputs\n+  // are reshaped beforehand.\n+  if (input_tensor_rank == 0) {\n+    lowered_op = a1_concat_op;\n+  } else {\n+    // Reshape [N * A, B, C] to [N, A, B, C].\n+    auto reshape_output_type = RankedTensorType::get(\n+        ArrayRef<int64_t>(reshape_output_shape), result_type.getElementType());\n+\n+    auto a2_reshape_op =\n+        rewriter.create<tosa::ReshapeOp>(op->getLoc(), reshape_output_type,\n+                                         a1_concat_op.getResult(), shape_attr);\n+\n+    // If axis is equal to input tensor rank, then we need extra transpose\n+    // [N, A, B, C] to [A, B, C, N]\n+    if (axis == input_tensor_rank) {\n+      auto a3_transpose_perm =\n+          get1DConstTensor<tosa::ConstOp, int32_t>(rewriter, op, perm);\n+      auto a3_transpose_op = rewriter.create<tosa::TransposeOp>(\n+          op->getLoc(), result_type, a2_reshape_op.getResult(),\n+          a3_transpose_perm);\n+      lowered_op = a3_transpose_op;\n+    } else {\n+      lowered_op = a2_reshape_op;\n+    }\n+  }\n+\n+  return lowered_op;\n+}\n+\n+// Lowers the Unpack operator to TOSA\n+Operation* convertUnpackOp(PatternRewriter& rewriter, Operation* op,\n+                           Value input_value, int32_t axis) {\n+  auto input_type = input_value.getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) return nullptr;\n+\n+  auto input_shape = input_type.getShape();\n+  int64_t input_rank = input_shape.size();\n+\n+  SmallVector<Value, 4> results_vec;\n+\n+  // Negative axis allowed as long as it's within [-input_rank, input_rank).\n+  if (axis < 0) axis += input_rank;\n+\n+  assert(axis > 0 && axis < input_shape.size());\n+\n+  // A list of the output types for each slice op\n+  SmallVector<Type, 4> outs_type_vec;\n+\n+  // Step 1: transpose 'axis' to leftmost dimension.\n+  Value transposed_input_value;\n+  if (axis != 0) {\n+    SmallVector<int32_t, 8> perm_vec;\n+    SmallVector<int64_t, 2> a1_transpose_shape(input_rank);\n+\n+    perm_vec.push_back(axis);\n+    for (int i = 0; i < input_rank; i++) {\n+      if (i == axis) continue;\n+      perm_vec.push_back(i);\n+    }\n+\n+    auto a1_transpose_perm =\n+        get1DConstTensor<tosa::ConstOp, int32_t>(rewriter, op, perm_vec);\n+\n+    for (int i = 0; i < input_rank; i++) {\n+      a1_transpose_shape[i] = input_shape[perm_vec[i]];\n+    }\n+\n+    auto a1_transpose_op = rewriter.create<tosa::TransposeOp>(\n+        op->getLoc(),\n+        RankedTensorType::get(ArrayRef<int64_t>(a1_transpose_shape),\n+                              input_type.getElementType()),\n+        input_value, a1_transpose_perm);\n+\n+    transposed_input_value = a1_transpose_op.getResult();\n+  } else {\n+    // Do nothing if axis is already at leftmost dimension.\n+    transposed_input_value = input_value;\n+  }\n+\n+  // Step 2: slice [N, A, B, C] into N [A, B, C].\n+  auto transposed_input_type =\n+      transposed_input_value.getType().dyn_cast<RankedTensorType>();\n+  if (!transposed_input_type) return nullptr;\n+\n+  auto transposed_input_shape = transposed_input_type.getShape();\n+  int64_t transposed_input_rank = transposed_input_shape.size();\n+\n+  for (int i = 0; i < transposed_input_shape[0]; i++) {\n+    SmallVector<int64_t, 4> begin_vals, size_vals, shape_vals;\n+\n+    for (int j = 0; j < transposed_input_rank; j++) {\n+      if (j == 0) {\n+        begin_vals.push_back(i);\n+        size_vals.push_back(1);\n+      } else {\n+        begin_vals.push_back(0);\n+        size_vals.push_back(transposed_input_shape[j]);\n+        shape_vals.push_back(transposed_input_shape[j]);\n+      }\n+    }\n+\n+    ArrayAttr begin = rewriter.getI64ArrayAttr(begin_vals);\n+    ArrayAttr size = rewriter.getI64ArrayAttr(size_vals);\n+\n+    auto a2_slice_op = rewriter.create<tosa::SliceOp>(\n+        op->getLoc(),\n+        RankedTensorType::get(ArrayRef<int64_t>(size_vals),\n+                              transposed_input_type.getElementType()),\n+        transposed_input_value, begin, size);\n+\n+    auto a3_reshape_op = rewriter.create<tosa::ReshapeOp>(\n+        op->getLoc(),\n+        RankedTensorType::get(ArrayRef<int64_t>(shape_vals),\n+                              transposed_input_type.getElementType()),\n+        a2_slice_op.getResult(), rewriter.getI64ArrayAttr(shape_vals));\n+\n+    outs_type_vec.push_back(RankedTensorType::get(\n+        ArrayRef<int64_t>(shape_vals), transposed_input_type.getElementType()));\n+\n+    results_vec.push_back(a3_reshape_op.getResult());\n+  }\n+\n+  // Combine the sequence of tosa.slice() ops into a list\n+  // using the IdentityN operator.\n+  auto identityn_op = rewriter.create<tosa::IdentityNOp>(\n+      op->getLoc(), ArrayRef<Type>(outs_type_vec), results_vec);\n+\n+  return identityn_op;\n+}\n+\n+// Lowers the Select operator to TOSA.\n+Operation* convertSelectOp(PatternRewriter& rewriter, Operation* op,\n+                           Value result_value, Value condition_value,\n+                           Value x_value, Value y_value) {\n+  auto result_type = result_value.getType().dyn_cast<RankedTensorType>();\n+  auto condition_type = condition_value.getType().dyn_cast<RankedTensorType>();\n+  auto x_type = x_value.getType().dyn_cast<RankedTensorType>();\n+  auto y_type = y_value.getType().dyn_cast<RankedTensorType>();\n+\n+  Operation* result_op = nullptr;\n+\n+  if (!result_type || !condition_type || !x_type || !y_type) {\n+    op->emitOpError(\"Select: failed ranked tensor type check\");\n+    return nullptr;\n+  }\n+\n+  // First check whether we need to reshape the condition to match\n+  // the same rank as the then/else clauses.\n+  if (result_type.getRank() == condition_type.getRank()) {\n+    // Nothing to reshape.\n+    result_op = rewriter.create<tosa::SelectOp>(\n+        op->getLoc(), result_type, condition_value, x_value, y_value);\n+  } else {\n+    // Need to reshape the condition.\n+    SmallVector<int64_t, 8> new_cond_dims;\n+    for (int i = 0; i < (result_type.getRank() - condition_type.getRank());\n+         i++) {\n+      new_cond_dims.push_back(1);\n+    }\n+    for (int i = 0; i < condition_type.getRank(); i++) {\n+      new_cond_dims.push_back(condition_type.getShape()[i]);\n+    }\n+\n+    auto reshape_op = rewriter.create<tosa::ReshapeOp>(\n+        op->getLoc(),\n+        RankedTensorType::get(ArrayRef<int64_t>(new_cond_dims),\n+                              condition_type.getElementType()),\n+        condition_value, rewriter.getI64ArrayAttr(new_cond_dims));\n+\n+    auto new_select = rewriter.create<tosa::SelectOp>(\n+        op->getLoc(), result_type, reshape_op, x_value, y_value);\n+    result_op = new_select;\n+  }\n+\n+  return result_op;\n+}\n+\n+// Lowers the ZerosLike operator to TOSA by creating a constant\n+// of the desired type and shape.\n+Operation* convertZerosLikeOp(PatternRewriter& rewriter, Operation* op,\n+                              Value result, Value input) {\n+  auto result_type = result.getType().dyn_cast<RankedTensorType>();\n+  if (!result_type) {\n+    op->emitOpError(\"Zeroslike: result not ranked tensor type\");\n+    return nullptr;\n+  }\n+\n+  auto input_type = input.getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) {\n+    op->emitOpError(\"Zeroslike: input not ranked tensor type\");\n+    return nullptr;\n+  }\n+\n+  auto input_shape = input_type.getShape();\n+\n+  ShapedType zero_type =\n+      RankedTensorType::get(input_shape, input_type.getElementType());\n+  Attribute zero_attr = rewriter.getZeroAttr(zero_type);\n+\n+  auto const_op =\n+      rewriter.create<tosa::ConstOp>(op->getLoc(), zero_type, zero_attr);\n+\n+  return const_op;\n+}\n+\n+// Lowers the Mul operator to TOSA.  For quantized types, this requires\n+// inserting rescale operators before and after the operation.\n+Operation* convertMultiplyOp(PatternRewriter& rewriter, Operation* op,\n+                             Value output_val, Value input_lhs_val,\n+                             Value input_rhs_val) {\n+  auto input_lhs_type = input_lhs_val.getType().dyn_cast<RankedTensorType>();\n+  auto input_rhs_type = input_rhs_val.getType().dyn_cast<RankedTensorType>();\n+  auto output_type = output_val.getType().dyn_cast<RankedTensorType>();\n+  // Not a ranked tensor output\n+  if (!input_lhs_type || !input_rhs_type || !output_type) return nullptr;\n+\n+  bool input_lhs_is_qtype =\n+      input_lhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool input_rhs_is_qtype =\n+      input_rhs_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+  bool output_is_qtype =\n+      output_type.getElementType().isa<mlir::quant::UniformQuantizedType>();\n+\n+  if (input_lhs_is_qtype != output_is_qtype ||\n+      input_rhs_is_qtype != output_is_qtype) {\n+    op->emitOpError(\n+        \"ConvertMultiplyOp: input/output tensor should \"\n+        \"be all quantized or all native\");\n+    return nullptr;\n+  }\n+\n+  Value output;\n+  if (output_is_qtype) {\n+    auto rescale_type =\n+        RankedTensorType::get(output_type.getShape(), rewriter.getI32Type());\n+    auto input_lhs_qtype = input_lhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto input_rhs_qtype = input_rhs_type.getElementType()\n+                               .dyn_cast<mlir::quant::UniformQuantizedType>();\n+    auto output_qtype = output_type.getElementType()\n+                            .dyn_cast<mlir::quant::UniformQuantizedType>();\n+\n+    double in_lhs_scale = input_lhs_qtype.getScale();\n+    double in_rhs_scale = input_rhs_qtype.getScale();\n+    double output_scale = output_qtype.getScale();\n+\n+    double output_rescale_scale = in_lhs_scale * in_rhs_scale / output_scale;\n+\n+    auto op1_rescale_lhs = buildRescaleToInt32(\n+        rewriter, op, input_lhs_val, 1.0f, input_lhs_qtype.getZeroPoint());\n+    auto op2_rescale_rhs = buildRescaleToInt32(\n+        rewriter, op, input_rhs_val, 1.0f, input_rhs_qtype.getZeroPoint());\n+    auto op3_mul_op1_op2 = rewriter.create<tosa::MulOp>(\n+        op->getLoc(), rescale_type, op1_rescale_lhs, op2_rescale_rhs, 0);\n+    auto op4_rescale_op3 = buildRescaleFromInt32(\n+        rewriter, op, output_type, op3_mul_op1_op2.getResult(),\n+        output_rescale_scale, output_qtype.getZeroPoint());\n+    output = op4_rescale_op3;\n+  } else {\n+    auto op1_mul_in = rewriter.create<tosa::MulOp>(\n+        op->getLoc(), output_type, input_lhs_val, input_rhs_val, 0);\n+\n+    output = op1_mul_in.getResult();\n+  }\n+\n+  return output.getDefiningOp();\n+}\n+\n+// Lowers the SquaredDifference operator to TOSA.\n+Operation* convertSquaredDifferenceOp(PatternRewriter& rewriter, Operation* op,\n+                                      Value result, Value x, Value y) {\n+  // Squared-difference is (x-y)*(x-y).\n+  // This lowering calculates the difference and multiplies.\n+  auto result_type = result.getType().dyn_cast<RankedTensorType>();\n+  if (!result_type) {\n+    op->emitOpError(\"SquaredDifference: result not ranked tensor type\");\n+    return nullptr;\n+  }\n+\n+  auto x_type = x.getType().dyn_cast<RankedTensorType>();\n+  auto y_type = y.getType().dyn_cast<RankedTensorType>();\n+  if (!x_type || !y_type) {\n+    op->emitOpError(\"SquaredDifference: inputs not ranked tensor type\");\n+    return nullptr;\n+  }\n+\n+  auto sub_op = rewriter.create<tosa::SubOp>(op->getLoc(), result_type, x, y);\n+  auto square_op = rewriter.create<tosa::MulOp>(\n+      op->getLoc(), result_type, sub_op.getResult(), sub_op.getResult(), 0);\n+  return square_op;\n+}\n+\n+// Lowers the Round operator to TOSA.\n+Operation* convertRoundOp(PatternRewriter& rewriter, Operation* op,\n+                          Value result, Value input) {\n+  // Implements banker's rounding by calculating floor(input + 0.5).\n+  auto result_type = result.getType().dyn_cast<RankedTensorType>();\n+  if (!result_type) {\n+    op->emitOpError(\"Round: result not ranked tensor type\");\n+    return nullptr;\n+  }\n+\n+  auto input_type = input.getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) {\n+    op->emitOpError(\"Round: input not ranked tensor type\");\n+    return nullptr;\n+  }\n+\n+  auto add_op = rewriter.create<tosa::AddOp>(\n+      op->getLoc(), result_type, input,\n+      getTosaConstTensorSingleF32(rewriter, op, 0.5));\n+  auto floor_op = rewriter.create<tosa::FloorOp>(op->getLoc(), result_type,\n+                                                 add_op.getResult());\n+\n+  return floor_op;\n+}\n+\n+// Lowers ConcatV2 to TOSA.\n+Operation* convertConcatV2Op(PatternRewriter& rewriter, Operation* op,\n+                             Value result_value, SmallVector<Value, 8>& values,\n+                             int32_t axis) {\n+  // ConcatV2 becomes a series of TOSA Concat operators that take pairs of\n+  // tensors as arguments.   Rank-0 tensors are reshaped to Rank-1,\n+  // shape (1,) tensors.\n+  auto result_type = result_value.getType().dyn_cast<RankedTensorType>();\n+  if (!result_type) {\n+    op->emitOpError(\"ConcatV2Op: result type not ranked tensor.\");\n+    return nullptr;\n+  }\n+\n+  // Valid axis in TF is [-rank(input), rank(input)).\n+  // Valid axis in TOSA is [0, rank(input)).\n+  // Plus rank(input) once if axis is negative.\n+  auto input_type = op->getOperand(0).getType().dyn_cast<RankedTensorType>();\n+  if (!input_type) {\n+    op->emitOpError(\"ConcatV2Op: input type not ranked tensor.\");\n+    return nullptr;\n+  }\n+\n+  auto input_rank = input_type.getShape().size();\n+\n+  if (axis < 0) axis += input_rank;\n+\n+  assert(values.size() >= 2);\n+\n+  if (!values[0].getType().dyn_cast<RankedTensorType>() ||\n+      !values[1].getType().dyn_cast<RankedTensorType>()) {\n+    op->emitOpError(\"ConcatV2Op: value type not ranked tensor.\");\n+    return nullptr;\n+  }\n+\n+  Value lhs_val = values[0];\n+  Value rhs_val = values[1];\n+  auto lhs_type = lhs_val.getType().dyn_cast<RankedTensorType>();\n+  auto rhs_type = rhs_val.getType().dyn_cast<RankedTensorType>();\n+  ArrayRef<int64_t> lhs_tensor_shape = lhs_type.getShape();\n+  ArrayRef<int64_t> rhs_tensor_shape = rhs_type.getShape();\n+  int input_tensor_rank = lhs_tensor_shape.size();\n+\n+  // For each concat output, shape will be different.\n+  // If input tensors are rank 0, should reshape them to rank 1 size 1 before\n+  // performing concat. If not, most dimensions should have same size as input\n+  // except the concat'd axis.\n+  //\n+  // If input is [A0, B, C] and [A1, B, C] and axis = 0\n+  // this concat output will be [A0 + A1, B, C].\n+  std::vector<int64_t> concat_result_shape;\n+  if (input_tensor_rank == 0) {\n+    if (axis != 0) {\n+      op->emitOpError(\"ConcatV2Op: axis invalid.\");\n+      return nullptr;\n+    }\n+    SmallVector<int64_t, 8> reshape_rank1_size1_shape{1};\n+    auto reshape_rank1_size1_type =\n+        RankedTensorType::get(ArrayRef<int64_t>(reshape_rank1_size1_shape),\n+                              result_type.getElementType());\n+    ArrayAttr shape_rank1_size1_attr =\n+        rewriter.getI64ArrayAttr(reshape_rank1_size1_shape);\n+    for (int i = 0; i < values.size(); i++) {\n+      auto a0_reshape_op = rewriter.create<tosa::ReshapeOp>(\n+          op->getLoc(), reshape_rank1_size1_type, values[i],\n+          shape_rank1_size1_attr);\n+      values[i] = a0_reshape_op.getResult();\n+    }\n+    concat_result_shape.push_back(2);\n+  } else {\n+    if (axis < 0 || axis >= input_tensor_rank) {\n+      op->emitOpError(\"ConcatV2Op: axis invalid.\");\n+      return nullptr;\n+    }\n+    for (int i = 0; i < input_tensor_rank; i++) {\n+      concat_result_shape.push_back(lhs_tensor_shape[i]);\n+    }\n+    concat_result_shape[axis] = lhs_tensor_shape[axis] + rhs_tensor_shape[axis];\n+  }\n+\n+  auto concat_type = RankedTensorType::get(\n+      ArrayRef<int64_t>(concat_result_shape), result_type.getElementType());\n+\n+  mlir::quant::UniformQuantizedType lhs_quant_type =\n+      lhs_type.getElementType()\n+          .dyn_cast_or_null<mlir::quant::UniformQuantizedType>();\n+  mlir::quant::UniformQuantizedType rhs_quant_type =\n+      rhs_type.getElementType()\n+          .dyn_cast_or_null<mlir::quant::UniformQuantizedType>();\n+  mlir::quant::UniformQuantizedType result_quant_type =\n+      result_type.getElementType()\n+          .dyn_cast_or_null<mlir::quant::UniformQuantizedType>();\n+\n+  double lhs_scale, rhs_scale, result_scale;\n+  int32_t lhs_zeropoint, rhs_zeropoint, result_zeropoint;\n+  RankedTensorType const_type, i32_type;\n+  DenseElementsAttr const_attr;\n+\n+  // tfl.concat currently allows different scales for each input tensor, which\n+  // TFlite team will fix in:\n+  // https://github.com/tensorflow/tensorflow/issues/39658\n+  //\n+  // For backward compatibility, we still need to support this artifact by\n+  // scaling inputs to let them have the same scales.\n+  if (result_quant_type && lhs_quant_type && rhs_quant_type) {\n+    lhs_scale = (double)lhs_quant_type.getScale();",
        "comment_created_at": "2020-11-17T21:58:17+00:00",
        "comment_author": "stellaraccident",
        "comment_body": "Prefer static_cast<double>(...) vs C-Style cast. (here and below)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1144996810",
    "pr_number": 59843,
    "pr_file": "tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.h",
    "created_at": "2023-03-22T15:26:16+00:00",
    "commented_code": "// Return the context associated to that ptr.\n   static CUcontext GetAnyContext(void* ptr) {\n-    absl::ReaderMutexLock lock(&mu_);\n-    int device_ordinal;\n-    CUresult result = cuPointerGetAttribute(static_cast<void*>(&device_ordinal),\n-                                            CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL,\n-                                            reinterpret_cast<CUdeviceptr>(ptr));\n+    static const auto use_cuda_malloc_async = [] {",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1144996810",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 59843,
        "pr_file": "tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.h",
        "discussion_id": "1144996810",
        "commented_code": "@@ -111,19 +129,27 @@ class CreatedContexts {\n \n   // Return the context associated to that ptr.\n   static CUcontext GetAnyContext(void* ptr) {\n-    absl::ReaderMutexLock lock(&mu_);\n-    int device_ordinal;\n-    CUresult result = cuPointerGetAttribute(static_cast<void*>(&device_ordinal),\n-                                            CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL,\n-                                            reinterpret_cast<CUdeviceptr>(ptr));\n+    static const auto use_cuda_malloc_async = [] {",
        "comment_created_at": "2023-03-22T15:26:16+00:00",
        "comment_author": "cantonios",
        "comment_body": "Type this as `bool` instead of `auto`.  When types are known and typing out the type doesn't interfere with readability, explicit types are preferred.",
        "pr_file_module": null
      }
    ]
  }
]