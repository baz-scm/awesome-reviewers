[
  {
    "discussion_id": "1040717676",
    "pr_number": 50424,
    "pr_file": "tensorflow/lite/kernels/tile.cc",
    "created_at": "2022-12-06T09:35:06+00:00",
    "commented_code": "return kTfLiteError;\n   }\n \n+  if (input->type == kTfLiteInt8 || input->type == kTfLiteInt16) {\n+    TF_LITE_ENSURE_EQ(context, input->params.scale, output->params.scale);\n+    TF_LITE_ENSURE_EQ(context, input->params.zero_point,\n+                      output->params.zero_point);\n+  }\n+\n+  if (input->type == kTfLiteInt16) {",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1040717676",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 50424,
        "pr_file": "tensorflow/lite/kernels/tile.cc",
        "discussion_id": "1040717676",
        "commented_code": "@@ -239,6 +239,17 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     return kTfLiteError;\n   }\n \n+  if (input->type == kTfLiteInt8 || input->type == kTfLiteInt16) {\n+    TF_LITE_ENSURE_EQ(context, input->params.scale, output->params.scale);\n+    TF_LITE_ENSURE_EQ(context, input->params.zero_point,\n+                      output->params.zero_point);\n+  }\n+\n+  if (input->type == kTfLiteInt16) {",
        "comment_created_at": "2022-12-06T09:35:06+00:00",
        "comment_author": "alankelly",
        "comment_body": "Do we care what the input and output zero points are once they are the same?",
        "pr_file_module": null
      },
      {
        "comment_id": "1040819883",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 50424,
        "pr_file": "tensorflow/lite/kernels/tile.cc",
        "discussion_id": "1040717676",
        "commented_code": "@@ -239,6 +239,17 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     return kTfLiteError;\n   }\n \n+  if (input->type == kTfLiteInt8 || input->type == kTfLiteInt16) {\n+    TF_LITE_ENSURE_EQ(context, input->params.scale, output->params.scale);\n+    TF_LITE_ENSURE_EQ(context, input->params.zero_point,\n+                      output->params.zero_point);\n+  }\n+\n+  if (input->type == kTfLiteInt16) {",
        "comment_created_at": "2022-12-06T10:59:34+00:00",
        "comment_author": "Tessil",
        "comment_body": "Yes, for int16 quantized operators, all zero-points should be equal to 0 and not just equal. It's part of the int16 quantization design and enforced by the quantizer. This check is an extra safety check (as done in nearly all other int16 ops).",
        "pr_file_module": null
      },
      {
        "comment_id": "1040826295",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 50424,
        "pr_file": "tensorflow/lite/kernels/tile.cc",
        "discussion_id": "1040717676",
        "commented_code": "@@ -239,6 +239,17 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     return kTfLiteError;\n   }\n \n+  if (input->type == kTfLiteInt8 || input->type == kTfLiteInt16) {\n+    TF_LITE_ENSURE_EQ(context, input->params.scale, output->params.scale);\n+    TF_LITE_ENSURE_EQ(context, input->params.zero_point,\n+                      output->params.zero_point);\n+  }\n+\n+  if (input->type == kTfLiteInt16) {",
        "comment_created_at": "2022-12-06T11:04:53+00:00",
        "comment_author": "alankelly",
        "comment_body": "Thanks, will resolve this and the other related open comments",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1585608669",
    "pr_number": 65939,
    "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general.cc",
    "created_at": "2024-04-30T22:24:22+00:00",
    "commented_code": "+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+absl::Status CheckParameters(\n+    const Tensor& lhs, const Tensor& rhs,\n+    const absl::Span<int64_t> lhs_batching_dimensions,\n+    const absl::Span<int64_t> rhs_batching_dimensions,\n+    const absl::Span<int64_t> lhs_contracting_dimensions,\n+    const absl::Span<int64_t> rhs_contracting_dimensions, Tensor& output,\n+    std::array<PrecisionTypes, 2>& precision_configs) {\n+  const DimensionSize lhsb_size = lhs_batching_dimensions.size();\n+  const DimensionSize rhsb_size = rhs_batching_dimensions.size();\n+  const DimensionSize lhsc_size = lhs_contracting_dimensions.size();\n+  const DimensionSize rhsc_size = rhs_contracting_dimensions.size();\n+  const size_t lhs_rank = lhs.Rank();\n+  const size_t rhs_rank = rhs.Rank();\n+  const size_t output_rank = output.Rank();\n+  absl::InlinedVector<size_t, 6> lhs_result_dims;\n+  absl::InlinedVector<size_t, 6> rhs_result_dims;\n+  absl::InlinedVector<size_t, 6> output_shape_check;\n+\n+  if (precision_configs.size() != 2) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of precision_config must be two.\");\n+  }\n+  if (lhsb_size != rhsb_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_batching_dimensions and \"\n+        \"rhs_batching_dimensions must be same.\");\n+  } else if (lhsc_size != rhsc_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_contracting_dimensions and \"\n+        \"rhs_contracting_dimensions must be same.\");\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < lhsc_size; ++j) {\n+      if (lhs_batching_dimensions[i] == lhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The lhs_batching_dimensions and \"\n+            \"lhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < rhsc_size; ++j) {\n+      if (rhs_batching_dimensions[i] == rhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The rhs_batching_dimensions and \"\n+            \"rhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs_batching_dimensions[i] >= lhs_rank ||\n+        lhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_batching_dimensions index.\");\n+    }\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_batching_dimensions[i]));\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs_contracting_dimensions[i] >= lhs_rank ||\n+        lhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    if (rhs_batching_dimensions[i] >= rhs_rank ||\n+        rhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_batching_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsc_size; ++i) {\n+    if (rhs_contracting_dimensions[i] >= rhs_rank ||\n+        rhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs.shape().Dim(lhs_batching_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_batching_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"batch dimension size.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs.shape().Dim(lhs_contracting_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_contracting_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"contracting dimension size.\");\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_rank; ++i) {\n+    if ((std::count(lhs_batching_dimensions.begin(),\n+                    lhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(lhs_contracting_dimensions.begin(),\n+                    lhs_contracting_dimensions.end(), i) == 0)) {\n+      lhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < rhs_rank; ++i) {\n+    if ((std::count(rhs_batching_dimensions.begin(),\n+                    rhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(rhs_contracting_dimensions.begin(),\n+                    rhs_contracting_dimensions.end(), i) == 0)) {\n+      rhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_result_dims[i]));\n+  }\n+  for (size_t i = 0; i < rhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(rhs.shape().Dim(rhs_result_dims[i]));\n+  }\n+  if (output_shape_check.size()) {\n+    for (size_t i = 0; i < output_rank; ++i) {\n+      if (output.shape().Dim(i) != output_shape_check[i]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: Invalid output shape.\");\n+      }\n+    }\n+  }\n+  if (lhs.IsPerAxisQuantized()) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: The lhs tensor cannot be per-axis quantized.\");\n+  }\n+  if (!lhs.IsPerTensorQuantized() && !rhs.IsQuantized()) {\n+    if (lhs.tensor_element_type() != rhs.tensor_element_type()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: For non-quantized tensors the element type \"\n+          \"of lhs and rhs must be the same.\");\n+    }\n+  }\n+  if (lhs.IsPerTensorQuantized()) {",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1585608669",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 65939,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general.cc",
        "discussion_id": "1585608669",
        "commented_code": "@@ -0,0 +1,638 @@\n+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+absl::Status CheckParameters(\n+    const Tensor& lhs, const Tensor& rhs,\n+    const absl::Span<int64_t> lhs_batching_dimensions,\n+    const absl::Span<int64_t> rhs_batching_dimensions,\n+    const absl::Span<int64_t> lhs_contracting_dimensions,\n+    const absl::Span<int64_t> rhs_contracting_dimensions, Tensor& output,\n+    std::array<PrecisionTypes, 2>& precision_configs) {\n+  const DimensionSize lhsb_size = lhs_batching_dimensions.size();\n+  const DimensionSize rhsb_size = rhs_batching_dimensions.size();\n+  const DimensionSize lhsc_size = lhs_contracting_dimensions.size();\n+  const DimensionSize rhsc_size = rhs_contracting_dimensions.size();\n+  const size_t lhs_rank = lhs.Rank();\n+  const size_t rhs_rank = rhs.Rank();\n+  const size_t output_rank = output.Rank();\n+  absl::InlinedVector<size_t, 6> lhs_result_dims;\n+  absl::InlinedVector<size_t, 6> rhs_result_dims;\n+  absl::InlinedVector<size_t, 6> output_shape_check;\n+\n+  if (precision_configs.size() != 2) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of precision_config must be two.\");\n+  }\n+  if (lhsb_size != rhsb_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_batching_dimensions and \"\n+        \"rhs_batching_dimensions must be same.\");\n+  } else if (lhsc_size != rhsc_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_contracting_dimensions and \"\n+        \"rhs_contracting_dimensions must be same.\");\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < lhsc_size; ++j) {\n+      if (lhs_batching_dimensions[i] == lhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The lhs_batching_dimensions and \"\n+            \"lhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < rhsc_size; ++j) {\n+      if (rhs_batching_dimensions[i] == rhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The rhs_batching_dimensions and \"\n+            \"rhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs_batching_dimensions[i] >= lhs_rank ||\n+        lhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_batching_dimensions index.\");\n+    }\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_batching_dimensions[i]));\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs_contracting_dimensions[i] >= lhs_rank ||\n+        lhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    if (rhs_batching_dimensions[i] >= rhs_rank ||\n+        rhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_batching_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsc_size; ++i) {\n+    if (rhs_contracting_dimensions[i] >= rhs_rank ||\n+        rhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs.shape().Dim(lhs_batching_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_batching_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"batch dimension size.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs.shape().Dim(lhs_contracting_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_contracting_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"contracting dimension size.\");\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_rank; ++i) {\n+    if ((std::count(lhs_batching_dimensions.begin(),\n+                    lhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(lhs_contracting_dimensions.begin(),\n+                    lhs_contracting_dimensions.end(), i) == 0)) {\n+      lhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < rhs_rank; ++i) {\n+    if ((std::count(rhs_batching_dimensions.begin(),\n+                    rhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(rhs_contracting_dimensions.begin(),\n+                    rhs_contracting_dimensions.end(), i) == 0)) {\n+      rhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_result_dims[i]));\n+  }\n+  for (size_t i = 0; i < rhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(rhs.shape().Dim(rhs_result_dims[i]));\n+  }\n+  if (output_shape_check.size()) {\n+    for (size_t i = 0; i < output_rank; ++i) {\n+      if (output.shape().Dim(i) != output_shape_check[i]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: Invalid output shape.\");\n+      }\n+    }\n+  }\n+  if (lhs.IsPerAxisQuantized()) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: The lhs tensor cannot be per-axis quantized.\");\n+  }\n+  if (!lhs.IsPerTensorQuantized() && !rhs.IsQuantized()) {\n+    if (lhs.tensor_element_type() != rhs.tensor_element_type()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: For non-quantized tensors the element type \"\n+          \"of lhs and rhs must be the same.\");\n+    }\n+  }\n+  if (lhs.IsPerTensorQuantized()) {",
        "comment_created_at": "2024-04-30T22:24:22+00:00",
        "comment_author": "rascani",
        "comment_body": "I'd also suggest using `IsQuantized` here, even though peraxis is disallowed for lhs.",
        "pr_file_module": null
      },
      {
        "comment_id": "1596316607",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 65939,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general.cc",
        "discussion_id": "1585608669",
        "commented_code": "@@ -0,0 +1,638 @@\n+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+absl::Status CheckParameters(\n+    const Tensor& lhs, const Tensor& rhs,\n+    const absl::Span<int64_t> lhs_batching_dimensions,\n+    const absl::Span<int64_t> rhs_batching_dimensions,\n+    const absl::Span<int64_t> lhs_contracting_dimensions,\n+    const absl::Span<int64_t> rhs_contracting_dimensions, Tensor& output,\n+    std::array<PrecisionTypes, 2>& precision_configs) {\n+  const DimensionSize lhsb_size = lhs_batching_dimensions.size();\n+  const DimensionSize rhsb_size = rhs_batching_dimensions.size();\n+  const DimensionSize lhsc_size = lhs_contracting_dimensions.size();\n+  const DimensionSize rhsc_size = rhs_contracting_dimensions.size();\n+  const size_t lhs_rank = lhs.Rank();\n+  const size_t rhs_rank = rhs.Rank();\n+  const size_t output_rank = output.Rank();\n+  absl::InlinedVector<size_t, 6> lhs_result_dims;\n+  absl::InlinedVector<size_t, 6> rhs_result_dims;\n+  absl::InlinedVector<size_t, 6> output_shape_check;\n+\n+  if (precision_configs.size() != 2) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of precision_config must be two.\");\n+  }\n+  if (lhsb_size != rhsb_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_batching_dimensions and \"\n+        \"rhs_batching_dimensions must be same.\");\n+  } else if (lhsc_size != rhsc_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_contracting_dimensions and \"\n+        \"rhs_contracting_dimensions must be same.\");\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < lhsc_size; ++j) {\n+      if (lhs_batching_dimensions[i] == lhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The lhs_batching_dimensions and \"\n+            \"lhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < rhsc_size; ++j) {\n+      if (rhs_batching_dimensions[i] == rhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The rhs_batching_dimensions and \"\n+            \"rhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs_batching_dimensions[i] >= lhs_rank ||\n+        lhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_batching_dimensions index.\");\n+    }\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_batching_dimensions[i]));\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs_contracting_dimensions[i] >= lhs_rank ||\n+        lhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    if (rhs_batching_dimensions[i] >= rhs_rank ||\n+        rhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_batching_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsc_size; ++i) {\n+    if (rhs_contracting_dimensions[i] >= rhs_rank ||\n+        rhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs.shape().Dim(lhs_batching_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_batching_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"batch dimension size.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs.shape().Dim(lhs_contracting_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_contracting_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"contracting dimension size.\");\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_rank; ++i) {\n+    if ((std::count(lhs_batching_dimensions.begin(),\n+                    lhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(lhs_contracting_dimensions.begin(),\n+                    lhs_contracting_dimensions.end(), i) == 0)) {\n+      lhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < rhs_rank; ++i) {\n+    if ((std::count(rhs_batching_dimensions.begin(),\n+                    rhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(rhs_contracting_dimensions.begin(),\n+                    rhs_contracting_dimensions.end(), i) == 0)) {\n+      rhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_result_dims[i]));\n+  }\n+  for (size_t i = 0; i < rhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(rhs.shape().Dim(rhs_result_dims[i]));\n+  }\n+  if (output_shape_check.size()) {\n+    for (size_t i = 0; i < output_rank; ++i) {\n+      if (output.shape().Dim(i) != output_shape_check[i]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: Invalid output shape.\");\n+      }\n+    }\n+  }\n+  if (lhs.IsPerAxisQuantized()) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: The lhs tensor cannot be per-axis quantized.\");\n+  }\n+  if (!lhs.IsPerTensorQuantized() && !rhs.IsQuantized()) {\n+    if (lhs.tensor_element_type() != rhs.tensor_element_type()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: For non-quantized tensors the element type \"\n+          \"of lhs and rhs must be the same.\");\n+    }\n+  }\n+  if (lhs.IsPerTensorQuantized()) {",
        "comment_created_at": "2024-05-10T06:28:01+00:00",
        "comment_author": "nishantsarda-mcw",
        "comment_body": "Changed as suggested.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1585609636",
    "pr_number": 65939,
    "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general.cc",
    "created_at": "2024-04-30T22:25:45+00:00",
    "commented_code": "+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+absl::Status CheckParameters(\n+    const Tensor& lhs, const Tensor& rhs,\n+    const absl::Span<int64_t> lhs_batching_dimensions,\n+    const absl::Span<int64_t> rhs_batching_dimensions,\n+    const absl::Span<int64_t> lhs_contracting_dimensions,\n+    const absl::Span<int64_t> rhs_contracting_dimensions, Tensor& output,\n+    std::array<PrecisionTypes, 2>& precision_configs) {\n+  const DimensionSize lhsb_size = lhs_batching_dimensions.size();\n+  const DimensionSize rhsb_size = rhs_batching_dimensions.size();\n+  const DimensionSize lhsc_size = lhs_contracting_dimensions.size();\n+  const DimensionSize rhsc_size = rhs_contracting_dimensions.size();\n+  const size_t lhs_rank = lhs.Rank();\n+  const size_t rhs_rank = rhs.Rank();\n+  const size_t output_rank = output.Rank();\n+  absl::InlinedVector<size_t, 6> lhs_result_dims;\n+  absl::InlinedVector<size_t, 6> rhs_result_dims;\n+  absl::InlinedVector<size_t, 6> output_shape_check;\n+\n+  if (precision_configs.size() != 2) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of precision_config must be two.\");\n+  }\n+  if (lhsb_size != rhsb_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_batching_dimensions and \"\n+        \"rhs_batching_dimensions must be same.\");\n+  } else if (lhsc_size != rhsc_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_contracting_dimensions and \"\n+        \"rhs_contracting_dimensions must be same.\");\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < lhsc_size; ++j) {\n+      if (lhs_batching_dimensions[i] == lhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The lhs_batching_dimensions and \"\n+            \"lhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < rhsc_size; ++j) {\n+      if (rhs_batching_dimensions[i] == rhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The rhs_batching_dimensions and \"\n+            \"rhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs_batching_dimensions[i] >= lhs_rank ||\n+        lhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_batching_dimensions index.\");\n+    }\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_batching_dimensions[i]));\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs_contracting_dimensions[i] >= lhs_rank ||\n+        lhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    if (rhs_batching_dimensions[i] >= rhs_rank ||\n+        rhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_batching_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsc_size; ++i) {\n+    if (rhs_contracting_dimensions[i] >= rhs_rank ||\n+        rhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs.shape().Dim(lhs_batching_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_batching_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"batch dimension size.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs.shape().Dim(lhs_contracting_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_contracting_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"contracting dimension size.\");\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_rank; ++i) {\n+    if ((std::count(lhs_batching_dimensions.begin(),\n+                    lhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(lhs_contracting_dimensions.begin(),\n+                    lhs_contracting_dimensions.end(), i) == 0)) {\n+      lhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < rhs_rank; ++i) {\n+    if ((std::count(rhs_batching_dimensions.begin(),\n+                    rhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(rhs_contracting_dimensions.begin(),\n+                    rhs_contracting_dimensions.end(), i) == 0)) {\n+      rhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_result_dims[i]));\n+  }\n+  for (size_t i = 0; i < rhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(rhs.shape().Dim(rhs_result_dims[i]));\n+  }\n+  if (output_shape_check.size()) {\n+    for (size_t i = 0; i < output_rank; ++i) {\n+      if (output.shape().Dim(i) != output_shape_check[i]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: Invalid output shape.\");\n+      }\n+    }\n+  }\n+  if (lhs.IsPerAxisQuantized()) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: The lhs tensor cannot be per-axis quantized.\");\n+  }\n+  if (!lhs.IsPerTensorQuantized() && !rhs.IsQuantized()) {\n+    if (lhs.tensor_element_type() != rhs.tensor_element_type()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: For non-quantized tensors the element type \"\n+          \"of lhs and rhs must be the same.\");\n+    }\n+  }\n+  if (lhs.IsPerTensorQuantized()) {\n+    if (rhs.IsQuantized() && !output.IsQuantized()) {",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1585609636",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 65939,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general.cc",
        "discussion_id": "1585609636",
        "commented_code": "@@ -0,0 +1,638 @@\n+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+absl::Status CheckParameters(\n+    const Tensor& lhs, const Tensor& rhs,\n+    const absl::Span<int64_t> lhs_batching_dimensions,\n+    const absl::Span<int64_t> rhs_batching_dimensions,\n+    const absl::Span<int64_t> lhs_contracting_dimensions,\n+    const absl::Span<int64_t> rhs_contracting_dimensions, Tensor& output,\n+    std::array<PrecisionTypes, 2>& precision_configs) {\n+  const DimensionSize lhsb_size = lhs_batching_dimensions.size();\n+  const DimensionSize rhsb_size = rhs_batching_dimensions.size();\n+  const DimensionSize lhsc_size = lhs_contracting_dimensions.size();\n+  const DimensionSize rhsc_size = rhs_contracting_dimensions.size();\n+  const size_t lhs_rank = lhs.Rank();\n+  const size_t rhs_rank = rhs.Rank();\n+  const size_t output_rank = output.Rank();\n+  absl::InlinedVector<size_t, 6> lhs_result_dims;\n+  absl::InlinedVector<size_t, 6> rhs_result_dims;\n+  absl::InlinedVector<size_t, 6> output_shape_check;\n+\n+  if (precision_configs.size() != 2) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of precision_config must be two.\");\n+  }\n+  if (lhsb_size != rhsb_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_batching_dimensions and \"\n+        \"rhs_batching_dimensions must be same.\");\n+  } else if (lhsc_size != rhsc_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_contracting_dimensions and \"\n+        \"rhs_contracting_dimensions must be same.\");\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < lhsc_size; ++j) {\n+      if (lhs_batching_dimensions[i] == lhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The lhs_batching_dimensions and \"\n+            \"lhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < rhsc_size; ++j) {\n+      if (rhs_batching_dimensions[i] == rhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The rhs_batching_dimensions and \"\n+            \"rhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs_batching_dimensions[i] >= lhs_rank ||\n+        lhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_batching_dimensions index.\");\n+    }\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_batching_dimensions[i]));\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs_contracting_dimensions[i] >= lhs_rank ||\n+        lhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    if (rhs_batching_dimensions[i] >= rhs_rank ||\n+        rhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_batching_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsc_size; ++i) {\n+    if (rhs_contracting_dimensions[i] >= rhs_rank ||\n+        rhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs.shape().Dim(lhs_batching_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_batching_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"batch dimension size.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs.shape().Dim(lhs_contracting_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_contracting_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"contracting dimension size.\");\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_rank; ++i) {\n+    if ((std::count(lhs_batching_dimensions.begin(),\n+                    lhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(lhs_contracting_dimensions.begin(),\n+                    lhs_contracting_dimensions.end(), i) == 0)) {\n+      lhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < rhs_rank; ++i) {\n+    if ((std::count(rhs_batching_dimensions.begin(),\n+                    rhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(rhs_contracting_dimensions.begin(),\n+                    rhs_contracting_dimensions.end(), i) == 0)) {\n+      rhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_result_dims[i]));\n+  }\n+  for (size_t i = 0; i < rhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(rhs.shape().Dim(rhs_result_dims[i]));\n+  }\n+  if (output_shape_check.size()) {\n+    for (size_t i = 0; i < output_rank; ++i) {\n+      if (output.shape().Dim(i) != output_shape_check[i]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: Invalid output shape.\");\n+      }\n+    }\n+  }\n+  if (lhs.IsPerAxisQuantized()) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: The lhs tensor cannot be per-axis quantized.\");\n+  }\n+  if (!lhs.IsPerTensorQuantized() && !rhs.IsQuantized()) {\n+    if (lhs.tensor_element_type() != rhs.tensor_element_type()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: For non-quantized tensors the element type \"\n+          \"of lhs and rhs must be the same.\");\n+    }\n+  }\n+  if (lhs.IsPerTensorQuantized()) {\n+    if (rhs.IsQuantized() && !output.IsQuantized()) {",
        "comment_created_at": "2024-04-30T22:25:45+00:00",
        "comment_author": "rascani",
        "comment_body": "This isn't quite right. If one tensor is quantized, they all must be, so you actually want:\r\n\r\n```\r\nif(!rhs.IsQuantized() || !output.IsQuantized())\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1596316977",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 65939,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general.cc",
        "discussion_id": "1585609636",
        "commented_code": "@@ -0,0 +1,638 @@\n+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+absl::Status CheckParameters(\n+    const Tensor& lhs, const Tensor& rhs,\n+    const absl::Span<int64_t> lhs_batching_dimensions,\n+    const absl::Span<int64_t> rhs_batching_dimensions,\n+    const absl::Span<int64_t> lhs_contracting_dimensions,\n+    const absl::Span<int64_t> rhs_contracting_dimensions, Tensor& output,\n+    std::array<PrecisionTypes, 2>& precision_configs) {\n+  const DimensionSize lhsb_size = lhs_batching_dimensions.size();\n+  const DimensionSize rhsb_size = rhs_batching_dimensions.size();\n+  const DimensionSize lhsc_size = lhs_contracting_dimensions.size();\n+  const DimensionSize rhsc_size = rhs_contracting_dimensions.size();\n+  const size_t lhs_rank = lhs.Rank();\n+  const size_t rhs_rank = rhs.Rank();\n+  const size_t output_rank = output.Rank();\n+  absl::InlinedVector<size_t, 6> lhs_result_dims;\n+  absl::InlinedVector<size_t, 6> rhs_result_dims;\n+  absl::InlinedVector<size_t, 6> output_shape_check;\n+\n+  if (precision_configs.size() != 2) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of precision_config must be two.\");\n+  }\n+  if (lhsb_size != rhsb_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_batching_dimensions and \"\n+        \"rhs_batching_dimensions must be same.\");\n+  } else if (lhsc_size != rhsc_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_contracting_dimensions and \"\n+        \"rhs_contracting_dimensions must be same.\");\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < lhsc_size; ++j) {\n+      if (lhs_batching_dimensions[i] == lhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The lhs_batching_dimensions and \"\n+            \"lhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < rhsc_size; ++j) {\n+      if (rhs_batching_dimensions[i] == rhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The rhs_batching_dimensions and \"\n+            \"rhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs_batching_dimensions[i] >= lhs_rank ||\n+        lhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_batching_dimensions index.\");\n+    }\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_batching_dimensions[i]));\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs_contracting_dimensions[i] >= lhs_rank ||\n+        lhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    if (rhs_batching_dimensions[i] >= rhs_rank ||\n+        rhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_batching_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsc_size; ++i) {\n+    if (rhs_contracting_dimensions[i] >= rhs_rank ||\n+        rhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs.shape().Dim(lhs_batching_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_batching_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"batch dimension size.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs.shape().Dim(lhs_contracting_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_contracting_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"contracting dimension size.\");\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_rank; ++i) {\n+    if ((std::count(lhs_batching_dimensions.begin(),\n+                    lhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(lhs_contracting_dimensions.begin(),\n+                    lhs_contracting_dimensions.end(), i) == 0)) {\n+      lhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < rhs_rank; ++i) {\n+    if ((std::count(rhs_batching_dimensions.begin(),\n+                    rhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(rhs_contracting_dimensions.begin(),\n+                    rhs_contracting_dimensions.end(), i) == 0)) {\n+      rhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_result_dims[i]));\n+  }\n+  for (size_t i = 0; i < rhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(rhs.shape().Dim(rhs_result_dims[i]));\n+  }\n+  if (output_shape_check.size()) {\n+    for (size_t i = 0; i < output_rank; ++i) {\n+      if (output.shape().Dim(i) != output_shape_check[i]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: Invalid output shape.\");\n+      }\n+    }\n+  }\n+  if (lhs.IsPerAxisQuantized()) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: The lhs tensor cannot be per-axis quantized.\");\n+  }\n+  if (!lhs.IsPerTensorQuantized() && !rhs.IsQuantized()) {\n+    if (lhs.tensor_element_type() != rhs.tensor_element_type()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: For non-quantized tensors the element type \"\n+          \"of lhs and rhs must be the same.\");\n+    }\n+  }\n+  if (lhs.IsPerTensorQuantized()) {\n+    if (rhs.IsQuantized() && !output.IsQuantized()) {",
        "comment_created_at": "2024-05-10T06:28:31+00:00",
        "comment_author": "nishantsarda-mcw",
        "comment_body": "Changed to the correct constraint as suggested",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1585633400",
    "pr_number": 65939,
    "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general.cc",
    "created_at": "2024-04-30T22:58:21+00:00",
    "commented_code": "+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+absl::Status CheckParameters(\n+    const Tensor& lhs, const Tensor& rhs,\n+    const absl::Span<int64_t> lhs_batching_dimensions,\n+    const absl::Span<int64_t> rhs_batching_dimensions,\n+    const absl::Span<int64_t> lhs_contracting_dimensions,\n+    const absl::Span<int64_t> rhs_contracting_dimensions, Tensor& output,\n+    std::array<PrecisionTypes, 2>& precision_configs) {\n+  const DimensionSize lhsb_size = lhs_batching_dimensions.size();\n+  const DimensionSize rhsb_size = rhs_batching_dimensions.size();\n+  const DimensionSize lhsc_size = lhs_contracting_dimensions.size();\n+  const DimensionSize rhsc_size = rhs_contracting_dimensions.size();\n+  const size_t lhs_rank = lhs.Rank();\n+  const size_t rhs_rank = rhs.Rank();\n+  const size_t output_rank = output.Rank();\n+  absl::InlinedVector<size_t, 6> lhs_result_dims;\n+  absl::InlinedVector<size_t, 6> rhs_result_dims;\n+  absl::InlinedVector<size_t, 6> output_shape_check;\n+\n+  if (precision_configs.size() != 2) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of precision_config must be two.\");\n+  }\n+  if (lhsb_size != rhsb_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_batching_dimensions and \"\n+        \"rhs_batching_dimensions must be same.\");\n+  } else if (lhsc_size != rhsc_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_contracting_dimensions and \"\n+        \"rhs_contracting_dimensions must be same.\");\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < lhsc_size; ++j) {\n+      if (lhs_batching_dimensions[i] == lhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The lhs_batching_dimensions and \"\n+            \"lhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < rhsc_size; ++j) {\n+      if (rhs_batching_dimensions[i] == rhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The rhs_batching_dimensions and \"\n+            \"rhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs_batching_dimensions[i] >= lhs_rank ||\n+        lhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_batching_dimensions index.\");\n+    }\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_batching_dimensions[i]));\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs_contracting_dimensions[i] >= lhs_rank ||\n+        lhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    if (rhs_batching_dimensions[i] >= rhs_rank ||\n+        rhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_batching_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsc_size; ++i) {\n+    if (rhs_contracting_dimensions[i] >= rhs_rank ||\n+        rhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs.shape().Dim(lhs_batching_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_batching_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"batch dimension size.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs.shape().Dim(lhs_contracting_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_contracting_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"contracting dimension size.\");\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_rank; ++i) {\n+    if ((std::count(lhs_batching_dimensions.begin(),\n+                    lhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(lhs_contracting_dimensions.begin(),\n+                    lhs_contracting_dimensions.end(), i) == 0)) {\n+      lhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < rhs_rank; ++i) {\n+    if ((std::count(rhs_batching_dimensions.begin(),\n+                    rhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(rhs_contracting_dimensions.begin(),\n+                    rhs_contracting_dimensions.end(), i) == 0)) {\n+      rhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_result_dims[i]));\n+  }\n+  for (size_t i = 0; i < rhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(rhs.shape().Dim(rhs_result_dims[i]));\n+  }\n+  if (output_shape_check.size()) {\n+    for (size_t i = 0; i < output_rank; ++i) {\n+      if (output.shape().Dim(i) != output_shape_check[i]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: Invalid output shape.\");\n+      }\n+    }\n+  }\n+  if (lhs.IsPerAxisQuantized()) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: The lhs tensor cannot be per-axis quantized.\");\n+  }\n+  if (!lhs.IsPerTensorQuantized() && !rhs.IsQuantized()) {\n+    if (lhs.tensor_element_type() != rhs.tensor_element_type()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: For non-quantized tensors the element type \"\n+          \"of lhs and rhs must be the same.\");\n+    }\n+  }\n+  if (lhs.IsPerTensorQuantized()) {\n+    if (rhs.IsQuantized() && !output.IsQuantized()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: If lhs and rhs are quantized tensors, than \"\n+          \"the output tensor should also be quantized.\");\n+    } else if (lhs.StorageType() != rhs.StorageType()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: If the lhs and rhs are quantized tensors, \"\n+          \"than they should have the same storage type.\");\n+    } else if (rhs.IsPerTensorQuantized()) {\n+      if (!output.IsPerTensorQuantized()) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: If lhs and rhs are per-tensor quantized \"\n+            \"than output should also be per-tensor quantized.\");\n+      }\n+      if (lhs.quantized_per_tensor_element_type().ExpressedType() ==",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "1585633400",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 65939,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general.cc",
        "discussion_id": "1585633400",
        "commented_code": "@@ -0,0 +1,638 @@\n+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+absl::Status CheckParameters(\n+    const Tensor& lhs, const Tensor& rhs,\n+    const absl::Span<int64_t> lhs_batching_dimensions,\n+    const absl::Span<int64_t> rhs_batching_dimensions,\n+    const absl::Span<int64_t> lhs_contracting_dimensions,\n+    const absl::Span<int64_t> rhs_contracting_dimensions, Tensor& output,\n+    std::array<PrecisionTypes, 2>& precision_configs) {\n+  const DimensionSize lhsb_size = lhs_batching_dimensions.size();\n+  const DimensionSize rhsb_size = rhs_batching_dimensions.size();\n+  const DimensionSize lhsc_size = lhs_contracting_dimensions.size();\n+  const DimensionSize rhsc_size = rhs_contracting_dimensions.size();\n+  const size_t lhs_rank = lhs.Rank();\n+  const size_t rhs_rank = rhs.Rank();\n+  const size_t output_rank = output.Rank();\n+  absl::InlinedVector<size_t, 6> lhs_result_dims;\n+  absl::InlinedVector<size_t, 6> rhs_result_dims;\n+  absl::InlinedVector<size_t, 6> output_shape_check;\n+\n+  if (precision_configs.size() != 2) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of precision_config must be two.\");\n+  }\n+  if (lhsb_size != rhsb_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_batching_dimensions and \"\n+        \"rhs_batching_dimensions must be same.\");\n+  } else if (lhsc_size != rhsc_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_contracting_dimensions and \"\n+        \"rhs_contracting_dimensions must be same.\");\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < lhsc_size; ++j) {\n+      if (lhs_batching_dimensions[i] == lhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The lhs_batching_dimensions and \"\n+            \"lhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < rhsc_size; ++j) {\n+      if (rhs_batching_dimensions[i] == rhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The rhs_batching_dimensions and \"\n+            \"rhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs_batching_dimensions[i] >= lhs_rank ||\n+        lhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_batching_dimensions index.\");\n+    }\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_batching_dimensions[i]));\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs_contracting_dimensions[i] >= lhs_rank ||\n+        lhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    if (rhs_batching_dimensions[i] >= rhs_rank ||\n+        rhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_batching_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsc_size; ++i) {\n+    if (rhs_contracting_dimensions[i] >= rhs_rank ||\n+        rhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs.shape().Dim(lhs_batching_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_batching_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"batch dimension size.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs.shape().Dim(lhs_contracting_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_contracting_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"contracting dimension size.\");\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_rank; ++i) {\n+    if ((std::count(lhs_batching_dimensions.begin(),\n+                    lhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(lhs_contracting_dimensions.begin(),\n+                    lhs_contracting_dimensions.end(), i) == 0)) {\n+      lhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < rhs_rank; ++i) {\n+    if ((std::count(rhs_batching_dimensions.begin(),\n+                    rhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(rhs_contracting_dimensions.begin(),\n+                    rhs_contracting_dimensions.end(), i) == 0)) {\n+      rhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_result_dims[i]));\n+  }\n+  for (size_t i = 0; i < rhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(rhs.shape().Dim(rhs_result_dims[i]));\n+  }\n+  if (output_shape_check.size()) {\n+    for (size_t i = 0; i < output_rank; ++i) {\n+      if (output.shape().Dim(i) != output_shape_check[i]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: Invalid output shape.\");\n+      }\n+    }\n+  }\n+  if (lhs.IsPerAxisQuantized()) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: The lhs tensor cannot be per-axis quantized.\");\n+  }\n+  if (!lhs.IsPerTensorQuantized() && !rhs.IsQuantized()) {\n+    if (lhs.tensor_element_type() != rhs.tensor_element_type()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: For non-quantized tensors the element type \"\n+          \"of lhs and rhs must be the same.\");\n+    }\n+  }\n+  if (lhs.IsPerTensorQuantized()) {\n+    if (rhs.IsQuantized() && !output.IsQuantized()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: If lhs and rhs are quantized tensors, than \"\n+          \"the output tensor should also be quantized.\");\n+    } else if (lhs.StorageType() != rhs.StorageType()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: If the lhs and rhs are quantized tensors, \"\n+          \"than they should have the same storage type.\");\n+    } else if (rhs.IsPerTensorQuantized()) {\n+      if (!output.IsPerTensorQuantized()) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: If lhs and rhs are per-tensor quantized \"\n+            \"than output should also be per-tensor quantized.\");\n+      }\n+      if (lhs.quantized_per_tensor_element_type().ExpressedType() ==",
        "comment_created_at": "2024-04-30T22:58:21+00:00",
        "comment_author": "rascani",
        "comment_body": "I'd be okay with adding a `Tensor::ExpressedType()` helper that has an `assert(IsQuantized())` for shorthand.\r\n\r\nI'd also suggest combining the two if statements here:\r\n```\r\nif ((lhs.quantized_per_tensor_element_type().ExpressedType() ==\r\n     rhs.quantized_per_tensor_element_type().ExpressedType()) &&\r\n    (lhs.quantized_per_tensor_element_type().ExpressedType() ==\r\n     output.quantized_per_tensor_element_type().ExpressedType)) {\r\n...\r\n}\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1598851450",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 65939,
        "pr_file": "tensorflow/lite/experimental/shlo/ops/dot_general.cc",
        "discussion_id": "1585633400",
        "commented_code": "@@ -0,0 +1,638 @@\n+/* Copyright 2024 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/lite/experimental/shlo/ops/dot_general.h\"\n+\n+#include \"absl/status/status.h\"\n+#include \"tensorflow/lite/experimental/shlo/data_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/dispatch.h\"\n+#include \"tensorflow/lite/experimental/shlo/ops/util.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantize.h\"\n+#include \"tensorflow/lite/experimental/shlo/quantized_tensor_element_type.h\"\n+#include \"tensorflow/lite/experimental/shlo/shape.h\"\n+#include \"tensorflow/lite/experimental/shlo/tensor.h\"\n+\n+namespace shlo_ref {\n+\n+absl::Status CheckParameters(\n+    const Tensor& lhs, const Tensor& rhs,\n+    const absl::Span<int64_t> lhs_batching_dimensions,\n+    const absl::Span<int64_t> rhs_batching_dimensions,\n+    const absl::Span<int64_t> lhs_contracting_dimensions,\n+    const absl::Span<int64_t> rhs_contracting_dimensions, Tensor& output,\n+    std::array<PrecisionTypes, 2>& precision_configs) {\n+  const DimensionSize lhsb_size = lhs_batching_dimensions.size();\n+  const DimensionSize rhsb_size = rhs_batching_dimensions.size();\n+  const DimensionSize lhsc_size = lhs_contracting_dimensions.size();\n+  const DimensionSize rhsc_size = rhs_contracting_dimensions.size();\n+  const size_t lhs_rank = lhs.Rank();\n+  const size_t rhs_rank = rhs.Rank();\n+  const size_t output_rank = output.Rank();\n+  absl::InlinedVector<size_t, 6> lhs_result_dims;\n+  absl::InlinedVector<size_t, 6> rhs_result_dims;\n+  absl::InlinedVector<size_t, 6> output_shape_check;\n+\n+  if (precision_configs.size() != 2) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of precision_config must be two.\");\n+  }\n+  if (lhsb_size != rhsb_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_batching_dimensions and \"\n+        \"rhs_batching_dimensions must be same.\");\n+  } else if (lhsc_size != rhsc_size) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: Size of lhs_contracting_dimensions and \"\n+        \"rhs_contracting_dimensions must be same.\");\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < lhsc_size; ++j) {\n+      if (lhs_batching_dimensions[i] == lhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The lhs_batching_dimensions and \"\n+            \"lhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    for (DimensionSize j = 0; j < rhsc_size; ++j) {\n+      if (rhs_batching_dimensions[i] == rhs_contracting_dimensions[j]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: The rhs_batching_dimensions and \"\n+            \"rhs_contracting_dimensions must be unique.\");\n+      }\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs_batching_dimensions[i] >= lhs_rank ||\n+        lhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_batching_dimensions index.\");\n+    }\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_batching_dimensions[i]));\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs_contracting_dimensions[i] >= lhs_rank ||\n+        lhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid lhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsb_size; ++i) {\n+    if (rhs_batching_dimensions[i] >= rhs_rank ||\n+        rhs_batching_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_batching_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < rhsc_size; ++i) {\n+    if (rhs_contracting_dimensions[i] >= rhs_rank ||\n+        rhs_contracting_dimensions[i] < 0) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: Invalid rhs_contracting_dimensions index.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsb_size; ++i) {\n+    if (lhs.shape().Dim(lhs_batching_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_batching_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"batch dimension size.\");\n+    }\n+  }\n+  for (DimensionSize i = 0; i < lhsc_size; ++i) {\n+    if (lhs.shape().Dim(lhs_contracting_dimensions[i]) !=\n+        rhs.shape().Dim(rhs_contracting_dimensions[i])) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: The lhs and rhs tensors should have same \"\n+          \"contracting dimension size.\");\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_rank; ++i) {\n+    if ((std::count(lhs_batching_dimensions.begin(),\n+                    lhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(lhs_contracting_dimensions.begin(),\n+                    lhs_contracting_dimensions.end(), i) == 0)) {\n+      lhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < rhs_rank; ++i) {\n+    if ((std::count(rhs_batching_dimensions.begin(),\n+                    rhs_batching_dimensions.end(), i) == 0) &&\n+        (std::count(rhs_contracting_dimensions.begin(),\n+                    rhs_contracting_dimensions.end(), i) == 0)) {\n+      rhs_result_dims.push_back(i);\n+    }\n+  }\n+  for (size_t i = 0; i < lhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(lhs.shape().Dim(lhs_result_dims[i]));\n+  }\n+  for (size_t i = 0; i < rhs_result_dims.size(); ++i) {\n+    output_shape_check.push_back(rhs.shape().Dim(rhs_result_dims[i]));\n+  }\n+  if (output_shape_check.size()) {\n+    for (size_t i = 0; i < output_rank; ++i) {\n+      if (output.shape().Dim(i) != output_shape_check[i]) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: Invalid output shape.\");\n+      }\n+    }\n+  }\n+  if (lhs.IsPerAxisQuantized()) {\n+    return absl::FailedPreconditionError(\n+        \"stablehlo.dot_general: The lhs tensor cannot be per-axis quantized.\");\n+  }\n+  if (!lhs.IsPerTensorQuantized() && !rhs.IsQuantized()) {\n+    if (lhs.tensor_element_type() != rhs.tensor_element_type()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: For non-quantized tensors the element type \"\n+          \"of lhs and rhs must be the same.\");\n+    }\n+  }\n+  if (lhs.IsPerTensorQuantized()) {\n+    if (rhs.IsQuantized() && !output.IsQuantized()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: If lhs and rhs are quantized tensors, than \"\n+          \"the output tensor should also be quantized.\");\n+    } else if (lhs.StorageType() != rhs.StorageType()) {\n+      return absl::FailedPreconditionError(\n+          \"stablehlo.dot_general: If the lhs and rhs are quantized tensors, \"\n+          \"than they should have the same storage type.\");\n+    } else if (rhs.IsPerTensorQuantized()) {\n+      if (!output.IsPerTensorQuantized()) {\n+        return absl::FailedPreconditionError(\n+            \"stablehlo.dot_general: If lhs and rhs are per-tensor quantized \"\n+            \"than output should also be per-tensor quantized.\");\n+      }\n+      if (lhs.quantized_per_tensor_element_type().ExpressedType() ==",
        "comment_created_at": "2024-05-13T18:00:52+00:00",
        "comment_author": "nishantsarda-mcw",
        "comment_body": "Added Tensor::ExpressedType() library method. \r\nAddressed in this commit- https://github.com/tensorflow/tensorflow/commit/42c3b0f21d91b8d856f160eb3a64ac1b4c5fdf97",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "525535818",
    "pr_number": 44851,
    "pr_file": "tensorflow/compiler/mlir/tosa/tests/tfl-to-tosa-pipeline.mlir",
    "created_at": "2020-11-17T21:27:53+00:00",
    "commented_code": "+// RUN: tf-opt --tfl-to-tosa-pipeline --verify-each %s | FileCheck %s\n+\n+// Operations for testing tfl-to-tosa-pipeline\n+\n+// -----\n+\n+// CHECK-LABEL: test_conv2d\n+// CHECK: tosa.const\n+// CHECK: tosa.conv2d\n+func @test_conv2d(%arg0: tensor<1x32x32x8xf32>, %cst_0: tensor<16x1x1x8xf32>) -> tensor<1x32x32x16xf32> {\n+  %cst = constant dense<0.000000e+00> : tensor<16xf32>\n+  %0 = \"tfl.conv_2d\"(%arg0, %cst_0, %cst)  {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}  : (tensor<1x32x32x8xf32>, tensor<16x1x1x8xf32>, tensor<16xf32>) -> tensor<1x32x32x16xf32>\n+  return %0 : tensor<1x32x32x16xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: test_conv2d_bias\n+// CHECK: tosa.conv2d\n+func @test_conv2d_bias(%arg0: tensor<1x32x32x8xf32>, %cst: tensor<16x1x1x8xf32>, %cst_0: tensor<16xf32>) -> tensor<1x32x32x16xf32> {\n+  %0 = \"tfl.conv_2d\"(%arg0, %cst, %cst_0)  {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}  : (tensor<1x32x32x8xf32>, tensor<16x1x1x8xf32>, tensor<16xf32>) -> tensor<1x32x32x16xf32>\n+  return %0 : tensor<1x32x32x16xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: test_transpose_conv2d\n+// CHECK: tosa.const\n+// CHECK: tosa.transpose_conv2d\n+func @test_transpose_conv2d(%arg0: tensor<1x32x32x8xf32>, %cst_0: tensor<16x1x1x8xf32>) -> tensor<1x32x32x16xf32> {\n+  %cst = constant dense<[1, 32, 32, 16]> : tensor<4xi32>\n+  %cst_1 = constant unit\n+  %0 = \"tfl.transpose_conv\"(%cst, %cst_0, %arg0, %cst_1)  {padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}  : (tensor<4xi32>, tensor<16x1x1x8xf32>, tensor<1x32x32x8xf32>, none) -> tensor<1x32x32x16xf32>\n+  return %0 : tensor<1x32x32x16xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: test_fakequant_conv2d\n+// CHECK: tosa.const\n+// CHECK: tosa.const\n+// CHECK: tosa.conv2d\n+// CHECK: tosa.rescale\n+func @test_fakequant_conv2d(%arg0: tensor<1x32x32x8x!quant.uniform<i8:f32, 0.015684768557548523>>) -> tensor<1x32x32x16x!quant.uniform<i8:f32, 0.078431375324726104>> {",
    "repo_full_name": "tensorflow/tensorflow",
    "discussion_comments": [
      {
        "comment_id": "525535818",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 44851,
        "pr_file": "tensorflow/compiler/mlir/tosa/tests/tfl-to-tosa-pipeline.mlir",
        "discussion_id": "525535818",
        "commented_code": "@@ -0,0 +1,920 @@\n+// RUN: tf-opt --tfl-to-tosa-pipeline --verify-each %s | FileCheck %s\n+\n+// Operations for testing tfl-to-tosa-pipeline\n+\n+// -----\n+\n+// CHECK-LABEL: test_conv2d\n+// CHECK: tosa.const\n+// CHECK: tosa.conv2d\n+func @test_conv2d(%arg0: tensor<1x32x32x8xf32>, %cst_0: tensor<16x1x1x8xf32>) -> tensor<1x32x32x16xf32> {\n+  %cst = constant dense<0.000000e+00> : tensor<16xf32>\n+  %0 = \"tfl.conv_2d\"(%arg0, %cst_0, %cst)  {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}  : (tensor<1x32x32x8xf32>, tensor<16x1x1x8xf32>, tensor<16xf32>) -> tensor<1x32x32x16xf32>\n+  return %0 : tensor<1x32x32x16xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: test_conv2d_bias\n+// CHECK: tosa.conv2d\n+func @test_conv2d_bias(%arg0: tensor<1x32x32x8xf32>, %cst: tensor<16x1x1x8xf32>, %cst_0: tensor<16xf32>) -> tensor<1x32x32x16xf32> {\n+  %0 = \"tfl.conv_2d\"(%arg0, %cst, %cst_0)  {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}  : (tensor<1x32x32x8xf32>, tensor<16x1x1x8xf32>, tensor<16xf32>) -> tensor<1x32x32x16xf32>\n+  return %0 : tensor<1x32x32x16xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: test_transpose_conv2d\n+// CHECK: tosa.const\n+// CHECK: tosa.transpose_conv2d\n+func @test_transpose_conv2d(%arg0: tensor<1x32x32x8xf32>, %cst_0: tensor<16x1x1x8xf32>) -> tensor<1x32x32x16xf32> {\n+  %cst = constant dense<[1, 32, 32, 16]> : tensor<4xi32>\n+  %cst_1 = constant unit\n+  %0 = \"tfl.transpose_conv\"(%cst, %cst_0, %arg0, %cst_1)  {padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}  : (tensor<4xi32>, tensor<16x1x1x8xf32>, tensor<1x32x32x8xf32>, none) -> tensor<1x32x32x16xf32>\n+  return %0 : tensor<1x32x32x16xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: test_fakequant_conv2d\n+// CHECK: tosa.const\n+// CHECK: tosa.const\n+// CHECK: tosa.conv2d\n+// CHECK: tosa.rescale\n+func @test_fakequant_conv2d(%arg0: tensor<1x32x32x8x!quant.uniform<i8:f32, 0.015684768557548523>>) -> tensor<1x32x32x16x!quant.uniform<i8:f32, 0.078431375324726104>> {",
        "comment_created_at": "2020-11-17T21:27:53+00:00",
        "comment_author": "stellaraccident",
        "comment_body": "Nit: For most of these, I am fine with these simple CHECK existence tests, but there are non-trivial calculations that happen in these quantized conversions. I'd feel better with some additional checks on the actual attribute values. Here and elsewhere. I am fine with this as a TODO (I know these were all generated by a tool that did correctness tests, and we may want to extend the tool vs hand editing).",
        "pr_file_module": null
      },
      {
        "comment_id": "526455883",
        "repo_full_name": "tensorflow/tensorflow",
        "pr_number": 44851,
        "pr_file": "tensorflow/compiler/mlir/tosa/tests/tfl-to-tosa-pipeline.mlir",
        "discussion_id": "525535818",
        "commented_code": "@@ -0,0 +1,920 @@\n+// RUN: tf-opt --tfl-to-tosa-pipeline --verify-each %s | FileCheck %s\n+\n+// Operations for testing tfl-to-tosa-pipeline\n+\n+// -----\n+\n+// CHECK-LABEL: test_conv2d\n+// CHECK: tosa.const\n+// CHECK: tosa.conv2d\n+func @test_conv2d(%arg0: tensor<1x32x32x8xf32>, %cst_0: tensor<16x1x1x8xf32>) -> tensor<1x32x32x16xf32> {\n+  %cst = constant dense<0.000000e+00> : tensor<16xf32>\n+  %0 = \"tfl.conv_2d\"(%arg0, %cst_0, %cst)  {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}  : (tensor<1x32x32x8xf32>, tensor<16x1x1x8xf32>, tensor<16xf32>) -> tensor<1x32x32x16xf32>\n+  return %0 : tensor<1x32x32x16xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: test_conv2d_bias\n+// CHECK: tosa.conv2d\n+func @test_conv2d_bias(%arg0: tensor<1x32x32x8xf32>, %cst: tensor<16x1x1x8xf32>, %cst_0: tensor<16xf32>) -> tensor<1x32x32x16xf32> {\n+  %0 = \"tfl.conv_2d\"(%arg0, %cst, %cst_0)  {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}  : (tensor<1x32x32x8xf32>, tensor<16x1x1x8xf32>, tensor<16xf32>) -> tensor<1x32x32x16xf32>\n+  return %0 : tensor<1x32x32x16xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: test_transpose_conv2d\n+// CHECK: tosa.const\n+// CHECK: tosa.transpose_conv2d\n+func @test_transpose_conv2d(%arg0: tensor<1x32x32x8xf32>, %cst_0: tensor<16x1x1x8xf32>) -> tensor<1x32x32x16xf32> {\n+  %cst = constant dense<[1, 32, 32, 16]> : tensor<4xi32>\n+  %cst_1 = constant unit\n+  %0 = \"tfl.transpose_conv\"(%cst, %cst_0, %arg0, %cst_1)  {padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}  : (tensor<4xi32>, tensor<16x1x1x8xf32>, tensor<1x32x32x8xf32>, none) -> tensor<1x32x32x16xf32>\n+  return %0 : tensor<1x32x32x16xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: test_fakequant_conv2d\n+// CHECK: tosa.const\n+// CHECK: tosa.const\n+// CHECK: tosa.conv2d\n+// CHECK: tosa.rescale\n+func @test_fakequant_conv2d(%arg0: tensor<1x32x32x8x!quant.uniform<i8:f32, 0.015684768557548523>>) -> tensor<1x32x32x16x!quant.uniform<i8:f32, 0.078431375324726104>> {",
        "comment_created_at": "2020-11-18T22:09:20+00:00",
        "comment_author": "jsmolens",
        "comment_body": "Will add a TODO covering all of the \"fakequant\" tests.  These all need some other TLC because the random double precision numbers are hard to look at (and create awfully long lines).",
        "pr_file_module": null
      }
    ]
  }
]