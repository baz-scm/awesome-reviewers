[
  {
    "discussion_id": "2162140118",
    "pr_number": 37173,
    "pr_file": "internal/terraform/node_resource_plan_instance_query.go",
    "created_at": "2025-06-23T17:44:52+00:00",
    "commented_code": "return diags.Append(resp.Diagnostics.InConfigBody(config.Config, n.Addr.String()))\n \t}\n \n+\t// If a path is specified, generate the config for the resource\n+\tvar generated string\n+\tif n.generateConfigPath != \"\" {\n+\t\tvar gDiags tfdiags.Diagnostics\n+\t\tgenerated, gDiags = n.generateListConfig(resp.Result, providerSchema.ResourceTypes[n.Config.Type])\n+\t\tdiags = diags.Append(gDiags)\n+\t\tif diags.HasErrors() {\n+\t\t\treturn diags\n+\t\t}\n+\t}\n+\n \tchange := &plans.ResourceInstanceChange{\n \t\tAddr:         n.Addr,\n+\t\tPrevRunAddr:  n.Addr,\n \t\tProviderAddr: n.ResolvedProvider,\n \t\tChange: plans.Change{\n-\t\t\tAction: plans.Read,\n-\t\t\tBefore: cty.DynamicVal,\n-\t\t\tAfter:  resp.Result,\n+\t\t\tAction:          plans.Read,\n+\t\t\tBefore:          cty.DynamicVal,\n+\t\t\tAfter:           resp.Result,\n+\t\t\tGeneratedConfig: generated,\n \t\t},\n \t\tDeposedKey: states.NotDeposed,\n \t}\n \n \tctx.Changes().AppendResourceInstanceChange(change)\n \treturn diags\n }\n+\n+func (n *NodePlannableResourceInstance) generateListConfig(obj cty.Value, resourceSchema providers.Schema) (generated string, diags tfdiags.Diagnostics) {",
    "repo_full_name": "hashicorp/terraform",
    "discussion_comments": [
      {
        "comment_id": "2162140118",
        "repo_full_name": "hashicorp/terraform",
        "pr_number": 37173,
        "pr_file": "internal/terraform/node_resource_plan_instance_query.go",
        "discussion_id": "2162140118",
        "commented_code": "@@ -88,17 +91,61 @@ func (n *NodePlannableResourceInstance) listResourceExecute(ctx EvalContext) (di\n \t\treturn diags.Append(resp.Diagnostics.InConfigBody(config.Config, n.Addr.String()))\n \t}\n \n+\t// If a path is specified, generate the config for the resource\n+\tvar generated string\n+\tif n.generateConfigPath != \"\" {\n+\t\tvar gDiags tfdiags.Diagnostics\n+\t\tgenerated, gDiags = n.generateListConfig(resp.Result, providerSchema.ResourceTypes[n.Config.Type])\n+\t\tdiags = diags.Append(gDiags)\n+\t\tif diags.HasErrors() {\n+\t\t\treturn diags\n+\t\t}\n+\t}\n+\n \tchange := &plans.ResourceInstanceChange{\n \t\tAddr:         n.Addr,\n+\t\tPrevRunAddr:  n.Addr,\n \t\tProviderAddr: n.ResolvedProvider,\n \t\tChange: plans.Change{\n-\t\t\tAction: plans.Read,\n-\t\t\tBefore: cty.DynamicVal,\n-\t\t\tAfter:  resp.Result,\n+\t\t\tAction:          plans.Read,\n+\t\t\tBefore:          cty.DynamicVal,\n+\t\t\tAfter:           resp.Result,\n+\t\t\tGeneratedConfig: generated,\n \t\t},\n \t\tDeposedKey: states.NotDeposed,\n \t}\n \n \tctx.Changes().AppendResourceInstanceChange(change)\n \treturn diags\n }\n+\n+func (n *NodePlannableResourceInstance) generateListConfig(obj cty.Value, resourceSchema providers.Schema) (generated string, diags tfdiags.Diagnostics) {",
        "comment_created_at": "2025-06-23T17:44:52+00:00",
        "comment_author": "jbardin",
        "comment_body": "This is almost entirely a copy of `generateHCLStringAttributes`, is there any way we can combine them?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2179301425",
    "pr_number": 37174,
    "pr_file": "internal/command/query.go",
    "created_at": "2025-07-02T07:13:05+00:00",
    "commented_code": "+// Copyright (c) HashiCorp, Inc.\n+// SPDX-License-Identifier: BUSL-1.1\n+\n+package command\n+\n+import (\n+\t\"fmt\"\n+\t\"strings\"\n+\n+\t\"github.com/hashicorp/terraform/internal/backend/backendrun\"\n+\t\"github.com/hashicorp/terraform/internal/command/arguments\"\n+\t\"github.com/hashicorp/terraform/internal/command/views\"\n+\t\"github.com/hashicorp/terraform/internal/tfdiags\"\n+)\n+\n+type QueryCommand struct {\n+\tMeta\n+}",
    "repo_full_name": "hashicorp/terraform",
    "discussion_comments": [
      {
        "comment_id": "2179301425",
        "repo_full_name": "hashicorp/terraform",
        "pr_number": 37174,
        "pr_file": "internal/command/query.go",
        "discussion_id": "2179301425",
        "commented_code": "@@ -0,0 +1,221 @@\n+// Copyright (c) HashiCorp, Inc.\n+// SPDX-License-Identifier: BUSL-1.1\n+\n+package command\n+\n+import (\n+\t\"fmt\"\n+\t\"strings\"\n+\n+\t\"github.com/hashicorp/terraform/internal/backend/backendrun\"\n+\t\"github.com/hashicorp/terraform/internal/command/arguments\"\n+\t\"github.com/hashicorp/terraform/internal/command/views\"\n+\t\"github.com/hashicorp/terraform/internal/tfdiags\"\n+)\n+\n+type QueryCommand struct {\n+\tMeta\n+}",
        "comment_created_at": "2025-07-02T07:13:05+00:00",
        "comment_author": "dsa0x",
        "comment_body": "Could we embed the `PlanCommand` in here, and use its methods PrepareBackend,  GatherVariables, OperationRequest? Query is kind of a Plan anyway.",
        "pr_file_module": null
      },
      {
        "comment_id": "2179441728",
        "repo_full_name": "hashicorp/terraform",
        "pr_number": 37174,
        "pr_file": "internal/command/query.go",
        "discussion_id": "2179301425",
        "commented_code": "@@ -0,0 +1,221 @@\n+// Copyright (c) HashiCorp, Inc.\n+// SPDX-License-Identifier: BUSL-1.1\n+\n+package command\n+\n+import (\n+\t\"fmt\"\n+\t\"strings\"\n+\n+\t\"github.com/hashicorp/terraform/internal/backend/backendrun\"\n+\t\"github.com/hashicorp/terraform/internal/command/arguments\"\n+\t\"github.com/hashicorp/terraform/internal/command/views\"\n+\t\"github.com/hashicorp/terraform/internal/tfdiags\"\n+)\n+\n+type QueryCommand struct {\n+\tMeta\n+}",
        "comment_created_at": "2025-07-02T08:24:19+00:00",
        "comment_author": "dbanck",
        "comment_body": "Interesting idea! Right now `GatherVariables` is the only method with the exact same body, the other two are similar, but different. I'll check if `GatherVariables` can stay as is for passing variables to the query command in a separate PR",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2058315377",
    "pr_number": 36904,
    "pr_file": "internal/terraform/node_resource_abstract_instance.go",
    "created_at": "2025-04-24T12:36:32+00:00",
    "commented_code": "return diags\n }\n \n+func (n *NodeAbstractResourceInstance) validateIdentityMatchesSchema(newIdentity cty.Value, identitySchema *configschema.Object) (diags tfdiags.Diagnostics) {",
    "repo_full_name": "hashicorp/terraform",
    "discussion_comments": [
      {
        "comment_id": "2058315377",
        "repo_full_name": "hashicorp/terraform",
        "pr_number": 36904,
        "pr_file": "internal/terraform/node_resource_abstract_instance.go",
        "discussion_id": "2058315377",
        "commented_code": "@@ -2935,6 +2938,62 @@ func (n *NodeAbstractResourceInstance) validateIdentity(newIdentity cty.Value) (\n \treturn diags\n }\n \n+func (n *NodeAbstractResourceInstance) validateIdentityMatchesSchema(newIdentity cty.Value, identitySchema *configschema.Object) (diags tfdiags.Diagnostics) {",
        "comment_created_at": "2025-04-24T12:36:32+00:00",
        "comment_author": "mildwonkey",
        "comment_body": "It's unclear to me why this is in a separate validate function. I feel like it makes sense to do this validation in the validateIdentity function, which currently only checks for marks, by adding the schema to the signature. Is there a reason you made this a separate call I'm not seeing? (it's not necessarily wrong in its own function, these are just closely related such that if they always happen together - let's just put it in the same validate) \r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2058466981",
        "repo_full_name": "hashicorp/terraform",
        "pr_number": 36904,
        "pr_file": "internal/terraform/node_resource_abstract_instance.go",
        "discussion_id": "2058315377",
        "commented_code": "@@ -2935,6 +2938,62 @@ func (n *NodeAbstractResourceInstance) validateIdentity(newIdentity cty.Value) (\n \treturn diags\n }\n \n+func (n *NodeAbstractResourceInstance) validateIdentityMatchesSchema(newIdentity cty.Value, identitySchema *configschema.Object) (diags tfdiags.Diagnostics) {",
        "comment_created_at": "2025-04-24T13:35:04+00:00",
        "comment_author": "dsa0x",
        "comment_body": "Good point. It could be part of the `validateIdentity` call. I think it just followed the convention that was implemented for other validate methods. Those however had different conditions that made them not fit for the validateidentity function. I will consolidate this one",
        "pr_file_module": null
      },
      {
        "comment_id": "2063591792",
        "repo_full_name": "hashicorp/terraform",
        "pr_number": 36904,
        "pr_file": "internal/terraform/node_resource_abstract_instance.go",
        "discussion_id": "2058315377",
        "commented_code": "@@ -2935,6 +2938,62 @@ func (n *NodeAbstractResourceInstance) validateIdentity(newIdentity cty.Value) (\n \treturn diags\n }\n \n+func (n *NodeAbstractResourceInstance) validateIdentityMatchesSchema(newIdentity cty.Value, identitySchema *configschema.Object) (diags tfdiags.Diagnostics) {",
        "comment_created_at": "2025-04-28T12:51:47+00:00",
        "comment_author": "mildwonkey",
        "comment_body": "lgtm!! much clearer to review <3 ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1942492215",
    "pr_number": 36300,
    "pr_file": "internal/backend/local/test.go",
    "created_at": "2025-02-05T09:09:05+00:00",
    "commented_code": "file.Status = file.Status.Merge(moduletest.Pass)\n \t}\n \n-\t// Now execute the runs.\n-\tfor _, run := range file.Runs {\n-\t\tif runner.Suite.Cancelled {\n-\t\t\t// This means a hard stop has been requested, in this case we don't\n-\t\t\t// even stop to mark future tests as having been skipped. They'll\n+\t// Build the graph for the file.\n+\tb := graph.TestGraphBuilder{File: file, GlobalVars: runner.EvalContext.VariableCaches.GlobalVariables}\n+\tgraph, diags := b.Build()\n+\tfile.Diagnostics = file.Diagnostics.Append(diags)\n+\tif diags.HasErrors() {\n+\t\tfile.Status = file.Status.Merge(moduletest.Error)\n+\t\treturn\n+\t}\n+\n+\t// walk and execute the graph\n+\tdiags = runner.walkGraph(graph)\n+\n+\t// If the graph walk was terminated, we don't want to add the diagnostics.\n+\t// The error the user receives will just be:\n+\t// \t\t\tFailure! 0 passed, 1 failed.\n+\t// \t\t\texit status 1\n+\tif runner.EvalContext.Cancelled() {\n+\t\tfile.UpdateStatus(moduletest.Error)\n+\t\tlog.Printf(\"[TRACE] TestFileRunner: graph walk terminated for %s\", file.Name)\n+\t\treturn\n+\t}\n+\n+\tfile.Diagnostics = file.Diagnostics.Append(diags)\n+}\n+\n+// walkGraph goes through the graph and execute each run it finds.\n+func (runner *TestFileRunner) walkGraph(g *terraform.Graph) tfdiags.Diagnostics {\n+\tsem := runner.Suite.semaphore\n+\n+\t// Walk the graph.\n+\twalkFn := func(v dag.Vertex) (diags tfdiags.Diagnostics) {\n+\t\tif runner.EvalContext.Cancelled() {\n+\t\t\t// If the graph walk has been cancelled, the node should just return immediately.\n+\t\t\t// For now, this means a hard stop has been requested, in this case we don't\n+\t\t\t// even stop to mark future test runs as having been skipped. They'll\n \t\t\t// just show up as pending in the printed summary. We will quickly\n \t\t\t// just mark the overall file status has having errored to indicate\n \t\t\t// it was interrupted.\n-\t\t\tfile.Status = file.Status.Merge(moduletest.Error)\n \t\t\treturn\n \t\t}\n \n+\t\t// the walkFn is called asynchronously, and needs to be recovered\n+\t\t// separately in the case of a panic.\n+\t\tdefer logging.PanicHandler()\n+\n+\t\tlog.Printf(\"[TRACE] vertex %q: starting visit (%T)\", dag.VertexName(v), v)\n+\n+\t\tdefer func() {\n+\t\t\tif r := recover(); r != nil {\n+\t\t\t\t// If the walkFn panics, we get confusing logs about how the\n+\t\t\t\t// visit was complete. To stop this, we'll catch the panic log\n+\t\t\t\t// that the vertex panicked without finishing and re-panic.\n+\t\t\t\tlog.Printf(\"[ERROR] vertex %q panicked\", dag.VertexName(v))\n+\t\t\t\tpanic(r) // re-panic\n+\t\t\t}\n+\n+\t\t\tif diags.HasErrors() {\n+\t\t\t\tfor _, diag := range diags {\n+\t\t\t\t\tif diag.Severity() == tfdiags.Error {\n+\t\t\t\t\t\tdesc := diag.Description()\n+\t\t\t\t\t\tlog.Printf(\"[ERROR] vertex %q error: %s\", dag.VertexName(v), desc.Summary)\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tlog.Printf(\"[TRACE] vertex %q: visit complete, with errors\", dag.VertexName(v))\n+\t\t\t} else {\n+\t\t\t\tlog.Printf(\"[TRACE] vertex %q: visit complete\", dag.VertexName(v))\n+\t\t\t}\n+\t\t}()\n+\n+\t\t// Acquire a lock on the semaphore\n+\t\tsem.Acquire()\n+\t\tdefer sem.Release()\n+\n+\t\tswitch v := v.(type) {\n+\t\tcase *graph.NodeTestRun:\n+\t\t\t// TODO: The execution of a NodeTestRun is currently split between\n+\t\t\t// its Execute method and the continuation of the walk callback.\n+\t\t\t// Eventually, we should move all the logic related to a test run into\n+\t\t\t// its Execute method, effectively ensuring that the Execute method is\n+\t\t\t// enough to execute a test run in the graph.\n+\t\t\tdiags = v.Execute(runner.EvalContext)\n+\t\t\tif diags.HasErrors() {\n+\t\t\t\treturn diags\n+\t\t\t}\n+\t\t\t// continue\n+\t\tcase graph.GraphNodeExecutable:\n+\t\t\tdiags = v.Execute(runner.EvalContext)\n+\t\t\treturn diags\n+\t\tdefault:\n+\t\t\t// If the vertex isn't a test run or executable, we'll just skip it.\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// We already know that the vertex is a test run\n+\t\trunNode := v.(*graph.NodeTestRun)\n+\n+\t\tfile := runNode.File()\n+\t\trun := runNode.Run()\n+\n \t\tif runner.Suite.Stopped {\n \t\t\t// Then the test was requested to be stopped, so we just mark each\n \t\t\t// following test as skipped, print the status, and move on.\n \t\t\trun.Status = moduletest.Skip\n \t\t\trunner.Suite.View.Run(run, file, moduletest.Complete, 0)\n-\t\t\tcontinue\n+\t\t\treturn\n \t\t}\n-\n+\t\tfile.Lock()\n \t\tif file.Status == moduletest.Error {\n \t\t\t// If the overall test file has errored, we don't keep trying to\n \t\t\t// execute tests. Instead, we mark all remaining run blocks as\n \t\t\t// skipped, print the status, and move on.\n \t\t\trun.Status = moduletest.Skip\n \t\t\trunner.Suite.View.Run(run, file, moduletest.Complete, 0)\n-\t\t\tcontinue\n+\t\t\tfile.Unlock()\n+\t\t\treturn\n \t\t}\n+\t\tfile.Unlock()",
    "repo_full_name": "hashicorp/terraform",
    "discussion_comments": [
      {
        "comment_id": "1942492215",
        "repo_full_name": "hashicorp/terraform",
        "pr_number": 36300,
        "pr_file": "internal/backend/local/test.go",
        "discussion_id": "1942492215",
        "commented_code": "@@ -309,43 +272,127 @@ func (runner *TestFileRunner) Test(file *moduletest.File) {\n \t\tfile.Status = file.Status.Merge(moduletest.Pass)\n \t}\n \n-\t// Now execute the runs.\n-\tfor _, run := range file.Runs {\n-\t\tif runner.Suite.Cancelled {\n-\t\t\t// This means a hard stop has been requested, in this case we don't\n-\t\t\t// even stop to mark future tests as having been skipped. They'll\n+\t// Build the graph for the file.\n+\tb := graph.TestGraphBuilder{File: file, GlobalVars: runner.EvalContext.VariableCaches.GlobalVariables}\n+\tgraph, diags := b.Build()\n+\tfile.Diagnostics = file.Diagnostics.Append(diags)\n+\tif diags.HasErrors() {\n+\t\tfile.Status = file.Status.Merge(moduletest.Error)\n+\t\treturn\n+\t}\n+\n+\t// walk and execute the graph\n+\tdiags = runner.walkGraph(graph)\n+\n+\t// If the graph walk was terminated, we don't want to add the diagnostics.\n+\t// The error the user receives will just be:\n+\t// \t\t\tFailure! 0 passed, 1 failed.\n+\t// \t\t\texit status 1\n+\tif runner.EvalContext.Cancelled() {\n+\t\tfile.UpdateStatus(moduletest.Error)\n+\t\tlog.Printf(\"[TRACE] TestFileRunner: graph walk terminated for %s\", file.Name)\n+\t\treturn\n+\t}\n+\n+\tfile.Diagnostics = file.Diagnostics.Append(diags)\n+}\n+\n+// walkGraph goes through the graph and execute each run it finds.\n+func (runner *TestFileRunner) walkGraph(g *terraform.Graph) tfdiags.Diagnostics {\n+\tsem := runner.Suite.semaphore\n+\n+\t// Walk the graph.\n+\twalkFn := func(v dag.Vertex) (diags tfdiags.Diagnostics) {\n+\t\tif runner.EvalContext.Cancelled() {\n+\t\t\t// If the graph walk has been cancelled, the node should just return immediately.\n+\t\t\t// For now, this means a hard stop has been requested, in this case we don't\n+\t\t\t// even stop to mark future test runs as having been skipped. They'll\n \t\t\t// just show up as pending in the printed summary. We will quickly\n \t\t\t// just mark the overall file status has having errored to indicate\n \t\t\t// it was interrupted.\n-\t\t\tfile.Status = file.Status.Merge(moduletest.Error)\n \t\t\treturn\n \t\t}\n \n+\t\t// the walkFn is called asynchronously, and needs to be recovered\n+\t\t// separately in the case of a panic.\n+\t\tdefer logging.PanicHandler()\n+\n+\t\tlog.Printf(\"[TRACE] vertex %q: starting visit (%T)\", dag.VertexName(v), v)\n+\n+\t\tdefer func() {\n+\t\t\tif r := recover(); r != nil {\n+\t\t\t\t// If the walkFn panics, we get confusing logs about how the\n+\t\t\t\t// visit was complete. To stop this, we'll catch the panic log\n+\t\t\t\t// that the vertex panicked without finishing and re-panic.\n+\t\t\t\tlog.Printf(\"[ERROR] vertex %q panicked\", dag.VertexName(v))\n+\t\t\t\tpanic(r) // re-panic\n+\t\t\t}\n+\n+\t\t\tif diags.HasErrors() {\n+\t\t\t\tfor _, diag := range diags {\n+\t\t\t\t\tif diag.Severity() == tfdiags.Error {\n+\t\t\t\t\t\tdesc := diag.Description()\n+\t\t\t\t\t\tlog.Printf(\"[ERROR] vertex %q error: %s\", dag.VertexName(v), desc.Summary)\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tlog.Printf(\"[TRACE] vertex %q: visit complete, with errors\", dag.VertexName(v))\n+\t\t\t} else {\n+\t\t\t\tlog.Printf(\"[TRACE] vertex %q: visit complete\", dag.VertexName(v))\n+\t\t\t}\n+\t\t}()\n+\n+\t\t// Acquire a lock on the semaphore\n+\t\tsem.Acquire()\n+\t\tdefer sem.Release()\n+\n+\t\tswitch v := v.(type) {\n+\t\tcase *graph.NodeTestRun:\n+\t\t\t// TODO: The execution of a NodeTestRun is currently split between\n+\t\t\t// its Execute method and the continuation of the walk callback.\n+\t\t\t// Eventually, we should move all the logic related to a test run into\n+\t\t\t// its Execute method, effectively ensuring that the Execute method is\n+\t\t\t// enough to execute a test run in the graph.\n+\t\t\tdiags = v.Execute(runner.EvalContext)\n+\t\t\tif diags.HasErrors() {\n+\t\t\t\treturn diags\n+\t\t\t}\n+\t\t\t// continue\n+\t\tcase graph.GraphNodeExecutable:\n+\t\t\tdiags = v.Execute(runner.EvalContext)\n+\t\t\treturn diags\n+\t\tdefault:\n+\t\t\t// If the vertex isn't a test run or executable, we'll just skip it.\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// We already know that the vertex is a test run\n+\t\trunNode := v.(*graph.NodeTestRun)\n+\n+\t\tfile := runNode.File()\n+\t\trun := runNode.Run()\n+\n \t\tif runner.Suite.Stopped {\n \t\t\t// Then the test was requested to be stopped, so we just mark each\n \t\t\t// following test as skipped, print the status, and move on.\n \t\t\trun.Status = moduletest.Skip\n \t\t\trunner.Suite.View.Run(run, file, moduletest.Complete, 0)\n-\t\t\tcontinue\n+\t\t\treturn\n \t\t}\n-\n+\t\tfile.Lock()\n \t\tif file.Status == moduletest.Error {\n \t\t\t// If the overall test file has errored, we don't keep trying to\n \t\t\t// execute tests. Instead, we mark all remaining run blocks as\n \t\t\t// skipped, print the status, and move on.\n \t\t\trun.Status = moduletest.Skip\n \t\t\trunner.Suite.View.Run(run, file, moduletest.Complete, 0)\n-\t\t\tcontinue\n+\t\t\tfile.Unlock()\n+\t\t\treturn\n \t\t}\n+\t\tfile.Unlock()",
        "comment_created_at": "2025-02-05T09:09:05+00:00",
        "comment_author": "liamcervante",
        "comment_body": "should we move this above the `.Execute()` calls above? Just so we check if we should execute before we do any of the work?",
        "pr_file_module": null
      },
      {
        "comment_id": "1942520011",
        "repo_full_name": "hashicorp/terraform",
        "pr_number": 36300,
        "pr_file": "internal/backend/local/test.go",
        "discussion_id": "1942492215",
        "commented_code": "@@ -309,43 +272,127 @@ func (runner *TestFileRunner) Test(file *moduletest.File) {\n \t\tfile.Status = file.Status.Merge(moduletest.Pass)\n \t}\n \n-\t// Now execute the runs.\n-\tfor _, run := range file.Runs {\n-\t\tif runner.Suite.Cancelled {\n-\t\t\t// This means a hard stop has been requested, in this case we don't\n-\t\t\t// even stop to mark future tests as having been skipped. They'll\n+\t// Build the graph for the file.\n+\tb := graph.TestGraphBuilder{File: file, GlobalVars: runner.EvalContext.VariableCaches.GlobalVariables}\n+\tgraph, diags := b.Build()\n+\tfile.Diagnostics = file.Diagnostics.Append(diags)\n+\tif diags.HasErrors() {\n+\t\tfile.Status = file.Status.Merge(moduletest.Error)\n+\t\treturn\n+\t}\n+\n+\t// walk and execute the graph\n+\tdiags = runner.walkGraph(graph)\n+\n+\t// If the graph walk was terminated, we don't want to add the diagnostics.\n+\t// The error the user receives will just be:\n+\t// \t\t\tFailure! 0 passed, 1 failed.\n+\t// \t\t\texit status 1\n+\tif runner.EvalContext.Cancelled() {\n+\t\tfile.UpdateStatus(moduletest.Error)\n+\t\tlog.Printf(\"[TRACE] TestFileRunner: graph walk terminated for %s\", file.Name)\n+\t\treturn\n+\t}\n+\n+\tfile.Diagnostics = file.Diagnostics.Append(diags)\n+}\n+\n+// walkGraph goes through the graph and execute each run it finds.\n+func (runner *TestFileRunner) walkGraph(g *terraform.Graph) tfdiags.Diagnostics {\n+\tsem := runner.Suite.semaphore\n+\n+\t// Walk the graph.\n+\twalkFn := func(v dag.Vertex) (diags tfdiags.Diagnostics) {\n+\t\tif runner.EvalContext.Cancelled() {\n+\t\t\t// If the graph walk has been cancelled, the node should just return immediately.\n+\t\t\t// For now, this means a hard stop has been requested, in this case we don't\n+\t\t\t// even stop to mark future test runs as having been skipped. They'll\n \t\t\t// just show up as pending in the printed summary. We will quickly\n \t\t\t// just mark the overall file status has having errored to indicate\n \t\t\t// it was interrupted.\n-\t\t\tfile.Status = file.Status.Merge(moduletest.Error)\n \t\t\treturn\n \t\t}\n \n+\t\t// the walkFn is called asynchronously, and needs to be recovered\n+\t\t// separately in the case of a panic.\n+\t\tdefer logging.PanicHandler()\n+\n+\t\tlog.Printf(\"[TRACE] vertex %q: starting visit (%T)\", dag.VertexName(v), v)\n+\n+\t\tdefer func() {\n+\t\t\tif r := recover(); r != nil {\n+\t\t\t\t// If the walkFn panics, we get confusing logs about how the\n+\t\t\t\t// visit was complete. To stop this, we'll catch the panic log\n+\t\t\t\t// that the vertex panicked without finishing and re-panic.\n+\t\t\t\tlog.Printf(\"[ERROR] vertex %q panicked\", dag.VertexName(v))\n+\t\t\t\tpanic(r) // re-panic\n+\t\t\t}\n+\n+\t\t\tif diags.HasErrors() {\n+\t\t\t\tfor _, diag := range diags {\n+\t\t\t\t\tif diag.Severity() == tfdiags.Error {\n+\t\t\t\t\t\tdesc := diag.Description()\n+\t\t\t\t\t\tlog.Printf(\"[ERROR] vertex %q error: %s\", dag.VertexName(v), desc.Summary)\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tlog.Printf(\"[TRACE] vertex %q: visit complete, with errors\", dag.VertexName(v))\n+\t\t\t} else {\n+\t\t\t\tlog.Printf(\"[TRACE] vertex %q: visit complete\", dag.VertexName(v))\n+\t\t\t}\n+\t\t}()\n+\n+\t\t// Acquire a lock on the semaphore\n+\t\tsem.Acquire()\n+\t\tdefer sem.Release()\n+\n+\t\tswitch v := v.(type) {\n+\t\tcase *graph.NodeTestRun:\n+\t\t\t// TODO: The execution of a NodeTestRun is currently split between\n+\t\t\t// its Execute method and the continuation of the walk callback.\n+\t\t\t// Eventually, we should move all the logic related to a test run into\n+\t\t\t// its Execute method, effectively ensuring that the Execute method is\n+\t\t\t// enough to execute a test run in the graph.\n+\t\t\tdiags = v.Execute(runner.EvalContext)\n+\t\t\tif diags.HasErrors() {\n+\t\t\t\treturn diags\n+\t\t\t}\n+\t\t\t// continue\n+\t\tcase graph.GraphNodeExecutable:\n+\t\t\tdiags = v.Execute(runner.EvalContext)\n+\t\t\treturn diags\n+\t\tdefault:\n+\t\t\t// If the vertex isn't a test run or executable, we'll just skip it.\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// We already know that the vertex is a test run\n+\t\trunNode := v.(*graph.NodeTestRun)\n+\n+\t\tfile := runNode.File()\n+\t\trun := runNode.Run()\n+\n \t\tif runner.Suite.Stopped {\n \t\t\t// Then the test was requested to be stopped, so we just mark each\n \t\t\t// following test as skipped, print the status, and move on.\n \t\t\trun.Status = moduletest.Skip\n \t\t\trunner.Suite.View.Run(run, file, moduletest.Complete, 0)\n-\t\t\tcontinue\n+\t\t\treturn\n \t\t}\n-\n+\t\tfile.Lock()\n \t\tif file.Status == moduletest.Error {\n \t\t\t// If the overall test file has errored, we don't keep trying to\n \t\t\t// execute tests. Instead, we mark all remaining run blocks as\n \t\t\t// skipped, print the status, and move on.\n \t\t\trun.Status = moduletest.Skip\n \t\t\trunner.Suite.View.Run(run, file, moduletest.Complete, 0)\n-\t\t\tcontinue\n+\t\t\tfile.Unlock()\n+\t\t\treturn\n \t\t}\n+\t\tfile.Unlock()",
        "comment_created_at": "2025-02-05T09:27:13+00:00",
        "comment_author": "dsa0x",
        "comment_body": "Sure. It was only okay to do this because most of the execution is still done in this file, but that may change in future, so will move 👍 \r\n=> [9555a29](https://github.com/hashicorp/terraform/pull/36300/commits/9555a29ce2c76c8ab3d55ef0380e18249e58f030)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1914379052",
    "pr_number": 36315,
    "pr_file": "internal/command/test.go",
    "created_at": "2025-01-14T07:46:53+00:00",
    "commented_code": "TestingDirectory:    args.TestDirectory,\n \t\t\tOpts:                opts,\n \t\t\tView:                view,\n-\t\t\tJUnit:               junitFile,\n \t\t\tStopped:             false,\n \t\t\tCancelled:           false,\n \t\t\tStoppedCtx:          stopCtx,\n \t\t\tCancelledCtx:        cancelCtx,\n \t\t\tFilter:              args.Filter,\n \t\t\tVerbose:             args.Verbose,\n \t\t}\n+\n+\t\tif junitFile != nil {",
    "repo_full_name": "hashicorp/terraform",
    "discussion_comments": [
      {
        "comment_id": "1914379052",
        "repo_full_name": "hashicorp/terraform",
        "pr_number": 36315,
        "pr_file": "internal/command/test.go",
        "discussion_id": "1914379052",
        "commented_code": "@@ -235,14 +235,20 @@ func (c *TestCommand) Run(rawArgs []string) int {\n \t\t\tTestingDirectory:    args.TestDirectory,\n \t\t\tOpts:                opts,\n \t\t\tView:                view,\n-\t\t\tJUnit:               junitFile,\n \t\t\tStopped:             false,\n \t\t\tCancelled:           false,\n \t\t\tStoppedCtx:          stopCtx,\n \t\t\tCancelledCtx:        cancelCtx,\n \t\t\tFilter:              args.Filter,\n \t\t\tVerbose:             args.Verbose,\n \t\t}\n+\n+\t\tif junitFile != nil {",
        "comment_created_at": "2025-01-14T07:46:53+00:00",
        "comment_author": "liamcervante",
        "comment_body": "If we were to delay the creation of the junit file until here (instead of creating it above) we could provide the runner in the constructor and avoid the need for the whole `SetTestSuiteRunner` function. Does that work?",
        "pr_file_module": null
      },
      {
        "comment_id": "1914610181",
        "repo_full_name": "hashicorp/terraform",
        "pr_number": 36315,
        "pr_file": "internal/command/test.go",
        "discussion_id": "1914379052",
        "commented_code": "@@ -235,14 +235,20 @@ func (c *TestCommand) Run(rawArgs []string) int {\n \t\t\tTestingDirectory:    args.TestDirectory,\n \t\t\tOpts:                opts,\n \t\t\tView:                view,\n-\t\t\tJUnit:               junitFile,\n \t\t\tStopped:             false,\n \t\t\tCancelled:           false,\n \t\t\tStoppedCtx:          stopCtx,\n \t\t\tCancelledCtx:        cancelCtx,\n \t\t\tFilter:              args.Filter,\n \t\t\tVerbose:             args.Verbose,\n \t\t}\n+\n+\t\tif junitFile != nil {",
        "comment_created_at": "2025-01-14T10:44:09+00:00",
        "comment_author": "SarahFrench",
        "comment_body": "Yeah that's much better - once the JUnit code was no-longer related to Views it didn't have any reason to remain in that previous position. Addressed in https://github.com/hashicorp/terraform/pull/36315/commits/0f1e8fde33cd56b9cafc116087ef9f284f12cbe5",
        "pr_file_module": null
      }
    ]
  }
]