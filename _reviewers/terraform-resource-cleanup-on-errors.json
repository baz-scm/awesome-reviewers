[
  {
    "discussion_id": "1995717208",
    "pr_number": 36643,
    "pr_file": "internal/communicator/ssh/communicator.go",
    "created_at": "2025-03-14T14:47:28+00:00",
    "commented_code": "return \"\", fmt.Errorf(\"Cannot quote shell command, target platform unknown: %s\", targetPlatform)\n \n }\n+\n+// ProxyCommandConnectFunc is a convenience method for returning a function\n+// that connects to a host using a proxy command.\n+func ProxyCommandConnectFunc(proxyCommand, addr string) func() (net.Conn, error) {\n+\treturn func() (net.Conn, error) {\n+\t\tlog.Printf(\"[DEBUG] Connecting to %s using proxy command: %s\", addr, proxyCommand)\n+\n+\t\t// Replace %h and %p in the proxy command with the host and port\n+\t\thost, port, err := net.SplitHostPort(addr)\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"Error parsing address: %s\", err)\n+\t\t}\n+\n+\t\tcommand := strings.Replace(proxyCommand, \"%h\", host, -1)\n+\t\tcommand = strings.Replace(command, \"%p\", port, -1)\n+\n+\t\t// Split the command into command and args\n+\t\tcmdParts := strings.Fields(command)\n+\t\tif len(cmdParts) == 0 {\n+\t\t\treturn nil, fmt.Errorf(\"Invalid proxy command: %s\", proxyCommand)\n+\t\t}\n+\n+\t\t// Create a buffer to capture stderr\n+\t\tstderrBuf := new(bytes.Buffer)\n+\n+\t\t// Create proxy command\n+\t\tcmd := execCommand(cmdParts[0], cmdParts[1:]...)\n+\t\tcmd.Stderr = stderrBuf\n+\n+\t\t// Set up the command to run in its own process group\n+\t\tcmd.SysProcAttr = &syscall.SysProcAttr{\n+\t\t\tSetpgid: true, // Create a new process group\n+\t\t}\n+\n+\t\t// Start the command with pipes\n+\t\tstdin, err := cmd.StdinPipe()\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"Error creating stdin pipe for proxy command: %s\", err)\n+\t\t}\n+\n+\t\tstdout, err := cmd.StdoutPipe()\n+\t\tif err != nil {\n+\t\t\tstdin.Close()\n+\t\t\treturn nil, fmt.Errorf(\"Error creating stdout pipe for proxy command: %s\", err)\n+\t\t}\n+\n+\t\tif err := cmd.Start(); err != nil {\n+\t\t\tstdin.Close()\n+\t\t\tstdout.Close()\n+\t\t\treturn nil, fmt.Errorf(\"Error starting proxy command: %s\", err)\n+\t\t}\n+\n+\t\t// Create a wrapper that manages the command and pipes\n+\t\tconn := &proxyCommandConn{\n+\t\t\tcmd:        cmd,\n+\t\t\tstderr:     stderrBuf,\n+\t\t\tstdinPipe:  stdin,\n+\t\t\tstdoutPipe: stdout,\n+\t\t}\n+\n+\t\treturn conn, nil\n+\t}\n+}\n+\n+type proxyCommandConn struct {\n+\tcmd        *exec.Cmd\n+\tstderr     *bytes.Buffer\n+\tstdinPipe  io.WriteCloser\n+\tstdoutPipe io.ReadCloser\n+\tclosed     bool\n+\tmutex      sync.Mutex\n+}\n+\n+// Read reads data from the connection (the command's stdout)\n+func (c *proxyCommandConn) Read(b []byte) (int, error) {\n+\tif c.closed {\n+\t\treturn 0, io.EOF\n+\t}\n+\treturn c.stdoutPipe.Read(b)\n+}\n+\n+// Write writes data to the connection (the command's stdin)\n+func (c *proxyCommandConn) Write(b []byte) (int, error) {\n+\tif c.closed {\n+\t\treturn 0, io.ErrClosedPipe\n+\t}\n+\treturn c.stdinPipe.Write(b)\n+}\n+\n+func (c *proxyCommandConn) Close() error {\n+\tc.mutex.Lock()\n+\tdefer c.mutex.Unlock()\n+\n+\tif c.closed {\n+\t\treturn nil\n+\t}\n+\tc.closed = true\n+\n+\tlog.Print(\"[DEBUG] Closing proxy command connection\")\n+\n+\t// Close pipes and hangup\n+\tif err := c.stdinPipe.Close(); err != nil {\n+\t\tlog.Printf(\"[ERROR] Error closing stdin pipe: %s\", err)\n+\t}\n+\tif err := c.stdoutPipe.Close(); err != nil {\n+\t\tlog.Printf(\"[ERROR] Error closing stdout pipe: %s\", err)\n+\t}\n+\n+\t// Send hangup the proxy command and any child processes\n+\tif c.cmd.Process != nil {\n+\t\tpgid, err := syscall.Getpgid(c.cmd.Process.Pid)\n+\t\tif err != nil {\n+\t\t\tlog.Printf(\"[ERROR] Error getting process group ID: %s\", err)\n+\t\t\t// Fall back to killing just the process if we can't get the process group\n+\t\t\tif err := c.cmd.Process.Signal(syscall.SIGHUP); err != nil {",
    "repo_full_name": "hashicorp/terraform",
    "discussion_comments": [
      {
        "comment_id": "1995717208",
        "repo_full_name": "hashicorp/terraform",
        "pr_number": 36643,
        "pr_file": "internal/communicator/ssh/communicator.go",
        "discussion_id": "1995717208",
        "commented_code": "@@ -897,3 +918,164 @@ func quoteShell(args []string, targetPlatform string) (string, error) {\n \treturn \"\", fmt.Errorf(\"Cannot quote shell command, target platform unknown: %s\", targetPlatform)\n \n }\n+\n+// ProxyCommandConnectFunc is a convenience method for returning a function\n+// that connects to a host using a proxy command.\n+func ProxyCommandConnectFunc(proxyCommand, addr string) func() (net.Conn, error) {\n+\treturn func() (net.Conn, error) {\n+\t\tlog.Printf(\"[DEBUG] Connecting to %s using proxy command: %s\", addr, proxyCommand)\n+\n+\t\t// Replace %h and %p in the proxy command with the host and port\n+\t\thost, port, err := net.SplitHostPort(addr)\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"Error parsing address: %s\", err)\n+\t\t}\n+\n+\t\tcommand := strings.Replace(proxyCommand, \"%h\", host, -1)\n+\t\tcommand = strings.Replace(command, \"%p\", port, -1)\n+\n+\t\t// Split the command into command and args\n+\t\tcmdParts := strings.Fields(command)\n+\t\tif len(cmdParts) == 0 {\n+\t\t\treturn nil, fmt.Errorf(\"Invalid proxy command: %s\", proxyCommand)\n+\t\t}\n+\n+\t\t// Create a buffer to capture stderr\n+\t\tstderrBuf := new(bytes.Buffer)\n+\n+\t\t// Create proxy command\n+\t\tcmd := execCommand(cmdParts[0], cmdParts[1:]...)\n+\t\tcmd.Stderr = stderrBuf\n+\n+\t\t// Set up the command to run in its own process group\n+\t\tcmd.SysProcAttr = &syscall.SysProcAttr{\n+\t\t\tSetpgid: true, // Create a new process group\n+\t\t}\n+\n+\t\t// Start the command with pipes\n+\t\tstdin, err := cmd.StdinPipe()\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"Error creating stdin pipe for proxy command: %s\", err)\n+\t\t}\n+\n+\t\tstdout, err := cmd.StdoutPipe()\n+\t\tif err != nil {\n+\t\t\tstdin.Close()\n+\t\t\treturn nil, fmt.Errorf(\"Error creating stdout pipe for proxy command: %s\", err)\n+\t\t}\n+\n+\t\tif err := cmd.Start(); err != nil {\n+\t\t\tstdin.Close()\n+\t\t\tstdout.Close()\n+\t\t\treturn nil, fmt.Errorf(\"Error starting proxy command: %s\", err)\n+\t\t}\n+\n+\t\t// Create a wrapper that manages the command and pipes\n+\t\tconn := &proxyCommandConn{\n+\t\t\tcmd:        cmd,\n+\t\t\tstderr:     stderrBuf,\n+\t\t\tstdinPipe:  stdin,\n+\t\t\tstdoutPipe: stdout,\n+\t\t}\n+\n+\t\treturn conn, nil\n+\t}\n+}\n+\n+type proxyCommandConn struct {\n+\tcmd        *exec.Cmd\n+\tstderr     *bytes.Buffer\n+\tstdinPipe  io.WriteCloser\n+\tstdoutPipe io.ReadCloser\n+\tclosed     bool\n+\tmutex      sync.Mutex\n+}\n+\n+// Read reads data from the connection (the command's stdout)\n+func (c *proxyCommandConn) Read(b []byte) (int, error) {\n+\tif c.closed {\n+\t\treturn 0, io.EOF\n+\t}\n+\treturn c.stdoutPipe.Read(b)\n+}\n+\n+// Write writes data to the connection (the command's stdin)\n+func (c *proxyCommandConn) Write(b []byte) (int, error) {\n+\tif c.closed {\n+\t\treturn 0, io.ErrClosedPipe\n+\t}\n+\treturn c.stdinPipe.Write(b)\n+}\n+\n+func (c *proxyCommandConn) Close() error {\n+\tc.mutex.Lock()\n+\tdefer c.mutex.Unlock()\n+\n+\tif c.closed {\n+\t\treturn nil\n+\t}\n+\tc.closed = true\n+\n+\tlog.Print(\"[DEBUG] Closing proxy command connection\")\n+\n+\t// Close pipes and hangup\n+\tif err := c.stdinPipe.Close(); err != nil {\n+\t\tlog.Printf(\"[ERROR] Error closing stdin pipe: %s\", err)\n+\t}\n+\tif err := c.stdoutPipe.Close(); err != nil {\n+\t\tlog.Printf(\"[ERROR] Error closing stdout pipe: %s\", err)\n+\t}\n+\n+\t// Send hangup the proxy command and any child processes\n+\tif c.cmd.Process != nil {\n+\t\tpgid, err := syscall.Getpgid(c.cmd.Process.Pid)\n+\t\tif err != nil {\n+\t\t\tlog.Printf(\"[ERROR] Error getting process group ID: %s\", err)\n+\t\t\t// Fall back to killing just the process if we can't get the process group\n+\t\t\tif err := c.cmd.Process.Signal(syscall.SIGHUP); err != nil {",
        "comment_created_at": "2025-03-14T14:47:28+00:00",
        "comment_author": "jbardin",
        "comment_body": "If the proxy command is meant to be generic, we can't count on it knowing what to do with a HUP, and windows has no HUP. Since this is a last resort it should probably use TERM/KILL signal. The entire process could also use `CommandContext`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2045692744",
    "pr_number": 36643,
    "pr_file": "internal/communicator/ssh/communicator.go",
    "created_at": "2025-04-15T23:17:44+00:00",
    "commented_code": "return \"\", fmt.Errorf(\"Cannot quote shell command, target platform unknown: %s\", targetPlatform)\n \n }\n+\n+// ProxyCommandConnectFunc is a convenience method for returning a function\n+// that connects to a host using a proxy command.\n+func ProxyCommandConnectFunc(proxyCommand, addr string) func() (net.Conn, error) {\n+\treturn func() (net.Conn, error) {\n+\t\tlog.Printf(\"[DEBUG] Connecting to %s using proxy command: %s\", addr, proxyCommand)\n+\n+\t\t// Split the command into command and args\n+\t\tcmdParts := strings.Fields(proxyCommand)\n+\t\tif len(cmdParts) == 0 {\n+\t\t\treturn nil, fmt.Errorf(\"Invalid proxy command: %s\", proxyCommand)\n+\t\t}\n+\n+\t\t// Create a buffer to capture stderr\n+\t\tstderrBuf := new(bytes.Buffer)\n+\n+\t\t// Create context for command lifecycle management\n+\t\tctx, cancel := context.WithCancel(context.Background())\n+\n+\t\t// Create proxy command with context\n+\t\tcmd := exec.CommandContext(ctx, cmdParts[0], cmdParts[1:]...)\n+\t\tcmd.Stderr = stderrBuf\n+\n+\t\t// Start the command with pipes\n+\t\tstdin, err := cmd.StdinPipe()\n+\t\tif err != nil {\n+\t\t\tcancel()\n+\t\t\treturn nil, fmt.Errorf(\"Error creating stdin pipe for proxy command: %s\", err)\n+\t\t}\n+\n+\t\tstdout, err := cmd.StdoutPipe()\n+\t\tif err != nil {\n+\t\t\tstdin.Close()\n+\t\t\tcancel()\n+\t\t\treturn nil, fmt.Errorf(\"Error creating stdout pipe for proxy command: %s\", err)\n+\t\t}\n+\n+\t\tif err := cmd.Start(); err != nil {\n+\t\t\tstdin.Close()\n+\t\t\tcancel()\n+\t\t\treturn nil, fmt.Errorf(\"Error starting proxy command: %s\", err)\n+\t\t}\n+\n+\t\t// Create a wrapper that manages the command and pipes\n+\t\tconn := &proxyCommandConn{\n+\t\t\tcmd:        cmd,\n+\t\t\tstderr:     stderrBuf,\n+\t\t\tstdinPipe:  stdin,\n+\t\t\tstdoutPipe: stdout,\n+\t\t\tctx:        ctx,\n+\t\t\tcancel:     cancel,\n+\t\t}\n+\n+\t\treturn conn, nil\n+\t}\n+}\n+\n+type proxyCommandConn struct {\n+\tcmd        *exec.Cmd\n+\tstderr     *bytes.Buffer\n+\tstdinPipe  io.WriteCloser\n+\tstdoutPipe io.ReadCloser\n+\tclosed     bool\n+\tmutex      sync.Mutex\n+\tctx        context.Context\n+\tcancel     context.CancelFunc\n+}\n+\n+// Read reads data from the connection (the command's stdout)\n+func (c *proxyCommandConn) Read(b []byte) (int, error) {\n+\tc.mutex.Lock()\n+\tif c.closed {\n+\t\tc.mutex.Unlock()\n+\t\treturn 0, io.EOF\n+\t}\n+\tc.mutex.Unlock()\n+\n+\treturn c.stdoutPipe.Read(b)\n+}\n+\n+// Write writes data to the connection (the command's stdin)\n+func (c *proxyCommandConn) Write(b []byte) (int, error) {\n+\tc.mutex.Lock()\n+\tif c.closed {",
    "repo_full_name": "hashicorp/terraform",
    "discussion_comments": [
      {
        "comment_id": "2045692744",
        "repo_full_name": "hashicorp/terraform",
        "pr_number": 36643,
        "pr_file": "internal/communicator/ssh/communicator.go",
        "discussion_id": "2045692744",
        "commented_code": "@@ -897,3 +914,140 @@ func quoteShell(args []string, targetPlatform string) (string, error) {\n \treturn \"\", fmt.Errorf(\"Cannot quote shell command, target platform unknown: %s\", targetPlatform)\n \n }\n+\n+// ProxyCommandConnectFunc is a convenience method for returning a function\n+// that connects to a host using a proxy command.\n+func ProxyCommandConnectFunc(proxyCommand, addr string) func() (net.Conn, error) {\n+\treturn func() (net.Conn, error) {\n+\t\tlog.Printf(\"[DEBUG] Connecting to %s using proxy command: %s\", addr, proxyCommand)\n+\n+\t\t// Split the command into command and args\n+\t\tcmdParts := strings.Fields(proxyCommand)\n+\t\tif len(cmdParts) == 0 {\n+\t\t\treturn nil, fmt.Errorf(\"Invalid proxy command: %s\", proxyCommand)\n+\t\t}\n+\n+\t\t// Create a buffer to capture stderr\n+\t\tstderrBuf := new(bytes.Buffer)\n+\n+\t\t// Create context for command lifecycle management\n+\t\tctx, cancel := context.WithCancel(context.Background())\n+\n+\t\t// Create proxy command with context\n+\t\tcmd := exec.CommandContext(ctx, cmdParts[0], cmdParts[1:]...)\n+\t\tcmd.Stderr = stderrBuf\n+\n+\t\t// Start the command with pipes\n+\t\tstdin, err := cmd.StdinPipe()\n+\t\tif err != nil {\n+\t\t\tcancel()\n+\t\t\treturn nil, fmt.Errorf(\"Error creating stdin pipe for proxy command: %s\", err)\n+\t\t}\n+\n+\t\tstdout, err := cmd.StdoutPipe()\n+\t\tif err != nil {\n+\t\t\tstdin.Close()\n+\t\t\tcancel()\n+\t\t\treturn nil, fmt.Errorf(\"Error creating stdout pipe for proxy command: %s\", err)\n+\t\t}\n+\n+\t\tif err := cmd.Start(); err != nil {\n+\t\t\tstdin.Close()\n+\t\t\tcancel()\n+\t\t\treturn nil, fmt.Errorf(\"Error starting proxy command: %s\", err)\n+\t\t}\n+\n+\t\t// Create a wrapper that manages the command and pipes\n+\t\tconn := &proxyCommandConn{\n+\t\t\tcmd:        cmd,\n+\t\t\tstderr:     stderrBuf,\n+\t\t\tstdinPipe:  stdin,\n+\t\t\tstdoutPipe: stdout,\n+\t\t\tctx:        ctx,\n+\t\t\tcancel:     cancel,\n+\t\t}\n+\n+\t\treturn conn, nil\n+\t}\n+}\n+\n+type proxyCommandConn struct {\n+\tcmd        *exec.Cmd\n+\tstderr     *bytes.Buffer\n+\tstdinPipe  io.WriteCloser\n+\tstdoutPipe io.ReadCloser\n+\tclosed     bool\n+\tmutex      sync.Mutex\n+\tctx        context.Context\n+\tcancel     context.CancelFunc\n+}\n+\n+// Read reads data from the connection (the command's stdout)\n+func (c *proxyCommandConn) Read(b []byte) (int, error) {\n+\tc.mutex.Lock()\n+\tif c.closed {\n+\t\tc.mutex.Unlock()\n+\t\treturn 0, io.EOF\n+\t}\n+\tc.mutex.Unlock()\n+\n+\treturn c.stdoutPipe.Read(b)\n+}\n+\n+// Write writes data to the connection (the command's stdin)\n+func (c *proxyCommandConn) Write(b []byte) (int, error) {\n+\tc.mutex.Lock()\n+\tif c.closed {",
        "comment_created_at": "2025-04-15T23:17:44+00:00",
        "comment_author": "jbardin",
        "comment_body": "With the simplified handling of io I don't think the `closed `flag is needed any longer (a flag like that is usually an indication of a problem anyway). we can just forward the call to `Write` and let the error pass through.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1794230682",
    "pr_number": 35661,
    "pr_file": "internal/backend/remote-state/s3/client.go",
    "created_at": "2024-10-09T21:06:36+00:00",
    "commented_code": "return nil\n }\n \n-func (c *RemoteClient) getLockInfo(ctx context.Context) (*statemgr.LockInfo, error) {\n+// getLockInfoWithFile retrieves and parses a lock file from an S3 bucket.\n+func (c *RemoteClient) getLockInfoWithFile(ctx context.Context) (*statemgr.LockInfo, error) {\n+\t// Attempt to retrieve the lock file from S3.\n+\tgetOutput, err := c.s3Client.GetObject(ctx, &s3.GetObjectInput{\n+\t\tBucket: aws.String(c.bucketName),\n+\t\tKey:    aws.String(c.lockFilePath),\n+\t})\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"unable to retrieve file from S3 bucket '%s' with key '%s': %w\", c.bucketName, c.lockFilePath, err)\n+\t}\n+\tdefer getOutput.Body.Close()",
    "repo_full_name": "hashicorp/terraform",
    "discussion_comments": [
      {
        "comment_id": "1794230682",
        "repo_full_name": "hashicorp/terraform",
        "pr_number": 35661,
        "pr_file": "internal/backend/remote-state/s3/client.go",
        "discussion_id": "1794230682",
        "commented_code": "@@ -459,7 +623,32 @@ func (c *RemoteClient) deleteMD5(ctx context.Context) error {\n \treturn nil\n }\n \n-func (c *RemoteClient) getLockInfo(ctx context.Context) (*statemgr.LockInfo, error) {\n+// getLockInfoWithFile retrieves and parses a lock file from an S3 bucket.\n+func (c *RemoteClient) getLockInfoWithFile(ctx context.Context) (*statemgr.LockInfo, error) {\n+\t// Attempt to retrieve the lock file from S3.\n+\tgetOutput, err := c.s3Client.GetObject(ctx, &s3.GetObjectInput{\n+\t\tBucket: aws.String(c.bucketName),\n+\t\tKey:    aws.String(c.lockFilePath),\n+\t})\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"unable to retrieve file from S3 bucket '%s' with key '%s': %w\", c.bucketName, c.lockFilePath, err)\n+\t}\n+\tdefer getOutput.Body.Close()",
        "comment_created_at": "2024-10-09T21:06:36+00:00",
        "comment_author": "YakDriver",
        "comment_body": "```suggestion\r\n\tdefer func() {\r\n\t\tif cerr := getOutput.Body.Close(); cerr != nil {\r\n\t\t\tlog.Printf(\"failed to close S3 object body: %v\", cerr)\r\n\t\t}\r\n\t}()\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1794241451",
    "pr_number": 35661,
    "pr_file": "internal/backend/remote-state/s3/client.go",
    "created_at": "2024-10-09T21:18:18+00:00",
    "commented_code": "ctx, baselog := baselogging.NewHcLogger(ctx, log)\n \tctx = baselogging.RegisterLogger(ctx, baselog)\n \n-\tif c.ddbTable == \"\" {\n+\tif !c.useLockFile && c.ddbTable == \"\" {\n \t\treturn nil\n \t}\n \n \tlockErr := &statemgr.LockError{}\n \n \tlog.Info(\"Unlocking remote state\")\n \n+\tif c.ddbTable != \"\" {\n+\t\terr := c.unlockWithDynamoDB(ctx, id, lockErr)\n+\t\tif err != nil {\n+\t\t\t// Release the file lock if attempting to unlock DynamoDB fails.\n+\t\t\tif c.useLockFile {\n+\t\t\t\tif fileErr := c.unlockWithFile(ctx, id, lockErr, log); fileErr != nil {\n+\t\t\t\t\terr = fmt.Errorf(\"error when attempting to clean up the file lock after failing to unlock DynamoDB: %v; original DynamoDB unlock error: %w\", fileErr, err)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tlockErr.Err = err\n+\t\t\treturn lockErr\n+\t\t}\n+\t}\n+\n+\tif c.useLockFile {\n+\t\terr := c.unlockWithFile(ctx, id, lockErr, log)\n+\t\tif err != nil {\n+\t\t\tlockErr.Err = err\n+\t\t\treturn lockErr\n+\t\t}\n+\t}\n+\n+\treturn nil\n+}\n+\n+// unlockWithFile attempts to unlock the remote state by deleting the lock file from Amazon S3.\n+//\n+// This method is used when the S3 native locking mechanism is in use, which uses a `.tflock` file\n+// to manage state locking. The function deletes the lock file to release the lock, allowing other\n+// Terraform clients to acquire the lock on the same state file.\n+func (c *RemoteClient) unlockWithFile(ctx context.Context, id string, lockErr *statemgr.LockError, log hclog.Logger) error {\n+\tgetInput := &s3.GetObjectInput{\n+\t\tBucket: aws.String(c.bucketName),\n+\t\tKey:    aws.String(c.lockFilePath),\n+\t}\n+\n+\tif c.serverSideEncryption && c.customerEncryptionKey != nil {\n+\t\tgetInput.SSECustomerKey = aws.String(base64.StdEncoding.EncodeToString(c.customerEncryptionKey))\n+\t\tgetInput.SSECustomerAlgorithm = aws.String(s3EncryptionAlgorithm)\n+\t\tgetInput.SSECustomerKeyMD5 = aws.String(c.getSSECustomerKeyMD5())\n+\t}\n+\n+\tgetOutput, err := c.s3Client.GetObject(ctx, getInput)\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"unable to retrieve file from S3 bucket '%s' with key '%s': %w\", c.bucketName, c.lockFilePath, err)\n+\t}\n+\tdefer getOutput.Body.Close()",
    "repo_full_name": "hashicorp/terraform",
    "discussion_comments": [
      {
        "comment_id": "1794241451",
        "repo_full_name": "hashicorp/terraform",
        "pr_number": 35661,
        "pr_file": "internal/backend/remote-state/s3/client.go",
        "discussion_id": "1794241451",
        "commented_code": "@@ -334,27 +421,105 @@ func (c *RemoteClient) Unlock(id string) error {\n \tctx, baselog := baselogging.NewHcLogger(ctx, log)\n \tctx = baselogging.RegisterLogger(ctx, baselog)\n \n-\tif c.ddbTable == \"\" {\n+\tif !c.useLockFile && c.ddbTable == \"\" {\n \t\treturn nil\n \t}\n \n \tlockErr := &statemgr.LockError{}\n \n \tlog.Info(\"Unlocking remote state\")\n \n+\tif c.ddbTable != \"\" {\n+\t\terr := c.unlockWithDynamoDB(ctx, id, lockErr)\n+\t\tif err != nil {\n+\t\t\t// Release the file lock if attempting to unlock DynamoDB fails.\n+\t\t\tif c.useLockFile {\n+\t\t\t\tif fileErr := c.unlockWithFile(ctx, id, lockErr, log); fileErr != nil {\n+\t\t\t\t\terr = fmt.Errorf(\"error when attempting to clean up the file lock after failing to unlock DynamoDB: %v; original DynamoDB unlock error: %w\", fileErr, err)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tlockErr.Err = err\n+\t\t\treturn lockErr\n+\t\t}\n+\t}\n+\n+\tif c.useLockFile {\n+\t\terr := c.unlockWithFile(ctx, id, lockErr, log)\n+\t\tif err != nil {\n+\t\t\tlockErr.Err = err\n+\t\t\treturn lockErr\n+\t\t}\n+\t}\n+\n+\treturn nil\n+}\n+\n+// unlockWithFile attempts to unlock the remote state by deleting the lock file from Amazon S3.\n+//\n+// This method is used when the S3 native locking mechanism is in use, which uses a `.tflock` file\n+// to manage state locking. The function deletes the lock file to release the lock, allowing other\n+// Terraform clients to acquire the lock on the same state file.\n+func (c *RemoteClient) unlockWithFile(ctx context.Context, id string, lockErr *statemgr.LockError, log hclog.Logger) error {\n+\tgetInput := &s3.GetObjectInput{\n+\t\tBucket: aws.String(c.bucketName),\n+\t\tKey:    aws.String(c.lockFilePath),\n+\t}\n+\n+\tif c.serverSideEncryption && c.customerEncryptionKey != nil {\n+\t\tgetInput.SSECustomerKey = aws.String(base64.StdEncoding.EncodeToString(c.customerEncryptionKey))\n+\t\tgetInput.SSECustomerAlgorithm = aws.String(s3EncryptionAlgorithm)\n+\t\tgetInput.SSECustomerKeyMD5 = aws.String(c.getSSECustomerKeyMD5())\n+\t}\n+\n+\tgetOutput, err := c.s3Client.GetObject(ctx, getInput)\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"unable to retrieve file from S3 bucket '%s' with key '%s': %w\", c.bucketName, c.lockFilePath, err)\n+\t}\n+\tdefer getOutput.Body.Close()",
        "comment_created_at": "2024-10-09T21:18:18+00:00",
        "comment_author": "YakDriver",
        "comment_body": "```suggestion\r\n\tdefer func() {\r\n\t\tif cerr := getOutput.Body.Close(); cerr != nil {\r\n\t\t\tlog.Warn(fmt.Sprintf(\"failed to close S3 object body: %v\", cerr))\r\n\t\t}\r\n\t}()\r\n```",
        "pr_file_module": null
      }
    ]
  }
]