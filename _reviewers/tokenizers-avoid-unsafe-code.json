[
  {
    "discussion_id": "762829732",
    "pr_number": 841,
    "pr_file": "tokenizers/src/tokenizer/encoding.rs",
    "created_at": "2021-12-06T09:25:43+00:00",
    "commented_code": "}\n }\n \n+#[inline]\n+fn split_off_back<T>(vec: &mut Vec<T>, at: usize) -> Vec<T> {\n+    assert!(vec.len() >= at);\n+    let mut other = Vec::with_capacity(at);\n+    let left_over_len = vec.len() - at;\n+    let at_isize = at.try_into().unwrap();\n+    unsafe {",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "762829732",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 841,
        "pr_file": "tokenizers/src/tokenizer/encoding.rs",
        "discussion_id": "762829732",
        "commented_code": "@@ -543,21 +603,51 @@ impl std::iter::FromIterator<(u32, String, (usize, usize), Option<u32>, u32)> fo\n     }\n }\n \n+#[inline]\n+fn split_off_back<T>(vec: &mut Vec<T>, at: usize) -> Vec<T> {\n+    assert!(vec.len() >= at);\n+    let mut other = Vec::with_capacity(at);\n+    let left_over_len = vec.len() - at;\n+    let at_isize = at.try_into().unwrap();\n+    unsafe {",
        "comment_created_at": "2021-12-06T09:25:43+00:00",
        "comment_author": "Narsil",
        "comment_body": "Ouch, not sure we need `unsafe` in this library :)",
        "pr_file_module": null
      },
      {
        "comment_id": "762869178",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 841,
        "pr_file": "tokenizers/src/tokenizer/encoding.rs",
        "discussion_id": "762829732",
        "commented_code": "@@ -543,21 +603,51 @@ impl std::iter::FromIterator<(u32, String, (usize, usize), Option<u32>, u32)> fo\n     }\n }\n \n+#[inline]\n+fn split_off_back<T>(vec: &mut Vec<T>, at: usize) -> Vec<T> {\n+    assert!(vec.len() >= at);\n+    let mut other = Vec::with_capacity(at);\n+    let left_over_len = vec.len() - at;\n+    let at_isize = at.try_into().unwrap();\n+    unsafe {",
        "comment_created_at": "2021-12-06T10:15:38+00:00",
        "comment_author": "McPatate",
        "comment_body": "I didn't find a method to `split_off` that modifies the vector in place to keep the end bit and return the left over memory.\r\n\r\nThe code is I believe safe to use as I check bounds. This makes the split efficient, but if you're against having `unsafe` there are other ways of doing this.",
        "pr_file_module": null
      },
      {
        "comment_id": "762903819",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 841,
        "pr_file": "tokenizers/src/tokenizer/encoding.rs",
        "discussion_id": "762829732",
        "commented_code": "@@ -543,21 +603,51 @@ impl std::iter::FromIterator<(u32, String, (usize, usize), Option<u32>, u32)> fo\n     }\n }\n \n+#[inline]\n+fn split_off_back<T>(vec: &mut Vec<T>, at: usize) -> Vec<T> {\n+    assert!(vec.len() >= at);\n+    let mut other = Vec::with_capacity(at);\n+    let left_over_len = vec.len() - at;\n+    let at_isize = at.try_into().unwrap();\n+    unsafe {",
        "comment_created_at": "2021-12-06T11:01:18+00:00",
        "comment_author": "Narsil",
        "comment_body": "I think staying away from unsafe is probably best, even if it means 1 clone. That's my opinion at least.",
        "pr_file_module": null
      },
      {
        "comment_id": "762924015",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 841,
        "pr_file": "tokenizers/src/tokenizer/encoding.rs",
        "discussion_id": "762829732",
        "commented_code": "@@ -543,21 +603,51 @@ impl std::iter::FromIterator<(u32, String, (usize, usize), Option<u32>, u32)> fo\n     }\n }\n \n+#[inline]\n+fn split_off_back<T>(vec: &mut Vec<T>, at: usize) -> Vec<T> {\n+    assert!(vec.len() >= at);\n+    let mut other = Vec::with_capacity(at);\n+    let left_over_len = vec.len() - at;\n+    let at_isize = at.try_into().unwrap();\n+    unsafe {",
        "comment_created_at": "2021-12-06T11:29:42+00:00",
        "comment_author": "McPatate",
        "comment_body": "Maybe @n1t0 can chime in :)",
        "pr_file_module": null
      }
    ]
  }
]