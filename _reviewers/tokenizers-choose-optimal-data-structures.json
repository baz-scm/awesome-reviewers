[
  {
    "discussion_id": "510693449",
    "pr_number": 480,
    "pr_file": "tokenizers/src/models/unigram/trainer.rs",
    "created_at": "2020-10-23T07:39:36+00:00",
    "commented_code": "let mut min_score_penalty = 0.0;\n         let min_score_penalty_delta = 0.0001;\n \n-        let mut pieces: HashMap<String, f64> = HashMap::new();\n+        let mut pieces: Vec<(String, f64)> = vec![];\n+        let mut inserted: HashSet<String> = HashSet::new();\n         let existing_pieces: HashMap<String, f64> = model.iter().cloned().collect();\n-        // XXX: Make sure bos, eos and unk exists and are ids 0, 1, 2\n-        pieces.insert(self.unk_token.clone(), 0.0);\n         for c in required_chars {\n             if let Some(t) = existing_pieces.get(&c) {\n-                pieces.insert(c, *t);\n+                inserted.insert(c.clone());",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "510693449",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 480,
        "pr_file": "tokenizers/src/models/unigram/trainer.rs",
        "discussion_id": "510693449",
        "commented_code": "@@ -94,39 +96,40 @@ impl UnigramTrainer {\n         let mut min_score_penalty = 0.0;\n         let min_score_penalty_delta = 0.0001;\n \n-        let mut pieces: HashMap<String, f64> = HashMap::new();\n+        let mut pieces: Vec<(String, f64)> = vec![];\n+        let mut inserted: HashSet<String> = HashSet::new();\n         let existing_pieces: HashMap<String, f64> = model.iter().cloned().collect();\n-        // XXX: Make sure bos, eos and unk exists and are ids 0, 1, 2\n-        pieces.insert(self.unk_token.clone(), 0.0);\n         for c in required_chars {\n             if let Some(t) = existing_pieces.get(&c) {\n-                pieces.insert(c, *t);\n+                inserted.insert(c.clone());",
        "comment_created_at": "2020-10-23T07:39:36+00:00",
        "comment_author": "Narsil",
        "comment_body": "Is the hashSet `inserted` really necessary ? `required_chars` is already a HashMap, so we shouldn't get duplicates anyway, no ?",
        "pr_file_module": null
      },
      {
        "comment_id": "510880467",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 480,
        "pr_file": "tokenizers/src/models/unigram/trainer.rs",
        "discussion_id": "510693449",
        "commented_code": "@@ -94,39 +96,40 @@ impl UnigramTrainer {\n         let mut min_score_penalty = 0.0;\n         let min_score_penalty_delta = 0.0001;\n \n-        let mut pieces: HashMap<String, f64> = HashMap::new();\n+        let mut pieces: Vec<(String, f64)> = vec![];\n+        let mut inserted: HashSet<String> = HashSet::new();\n         let existing_pieces: HashMap<String, f64> = model.iter().cloned().collect();\n-        // XXX: Make sure bos, eos and unk exists and are ids 0, 1, 2\n-        pieces.insert(self.unk_token.clone(), 0.0);\n         for c in required_chars {\n             if let Some(t) = existing_pieces.get(&c) {\n-                pieces.insert(c, *t);\n+                inserted.insert(c.clone());",
        "comment_created_at": "2020-10-23T13:23:43+00:00",
        "comment_author": "n1t0",
        "comment_body": "`inserted` has been added here because we are changing `pieces` to be a `Vec<(String, f64)>` instead of a `HashMap<String, f64>`, so we still need a way to check if a token has been inserted with `O(1)`",
        "pr_file_module": null
      },
      {
        "comment_id": "510894175",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 480,
        "pr_file": "tokenizers/src/models/unigram/trainer.rs",
        "discussion_id": "510693449",
        "commented_code": "@@ -94,39 +96,40 @@ impl UnigramTrainer {\n         let mut min_score_penalty = 0.0;\n         let min_score_penalty_delta = 0.0001;\n \n-        let mut pieces: HashMap<String, f64> = HashMap::new();\n+        let mut pieces: Vec<(String, f64)> = vec![];\n+        let mut inserted: HashSet<String> = HashSet::new();\n         let existing_pieces: HashMap<String, f64> = model.iter().cloned().collect();\n-        // XXX: Make sure bos, eos and unk exists and are ids 0, 1, 2\n-        pieces.insert(self.unk_token.clone(), 0.0);\n         for c in required_chars {\n             if let Some(t) = existing_pieces.get(&c) {\n-                pieces.insert(c, *t);\n+                inserted.insert(c.clone());",
        "comment_created_at": "2020-10-23T13:45:09+00:00",
        "comment_author": "Narsil",
        "comment_body": "My bad, did not see that change to `Vec`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "463220617",
    "pr_number": 360,
    "pr_file": "tokenizers/src/tokenizer/pattern.rs",
    "created_at": "2020-07-30T19:26:04+00:00",
    "commented_code": "+use crate::{Offsets, Result};\n+use regex::Regex;\n+\n+/// Pattern used to split a NormalizedString\n+pub trait Pattern {\n+    /// Slice the given string in a list of pattern match positions, with\n+    /// a boolean indicating whether this is a match or not.\n+    ///\n+    /// This method *must* cover the whole string in its outputs, with\n+    /// contiguous ordered slices.\n+    fn find_matches(&self, inside: &str) -> Result<Vec<(Offsets, bool)>>;\n+}\n+\n+impl Pattern for char {\n+    fn find_matches(&self, inside: &str) -> Result<Vec<(Offsets, bool)>> {\n+        let is_char = |c: char| -> bool { c == *self };\n+        is_char.find_matches(inside)\n+    }\n+}\n+\n+impl Pattern for &str {\n+    fn find_matches(&self, inside: &str) -> Result<Vec<(Offsets, bool)>> {\n+        let re = Regex::new(&regex::escape(self))?;\n+        (&re).find_matches(inside)\n+    }\n+}\n+\n+impl Pattern for &Regex {\n+    fn find_matches(&self, inside: &str) -> Result<Vec<(Offsets, bool)>> {\n+        if inside.is_empty() {\n+            return Ok(vec![((0, 0), false)]);\n+        }\n+\n+        // Find initial matches\n+        let matches = self\n+            .find_iter(inside)\n+            .map(|m| ((m.start(), m.end()), true))\n+            .collect::<Vec<_>>();\n+\n+        // Then add missing splits inbetween\n+        let mut start_offset = 0;\n+        let mut splits = matches\n+            .into_iter()\n+            .flat_map(|((start, end), flag)| {\n+                let mut splits = vec![];\n+                if start_offset < start {\n+                    splits.push(((start_offset, start), false));\n+                }\n+                splits.push(((start, end), flag));\n+                start_offset = end;\n+\n+                splits\n+            })\n+            .collect::<Vec<_>>();\n+\n+        if let Some(((_, end), _)) = splits.iter().last().copied() {\n+            if end < inside.len() {\n+                splits.push(((end, inside.len()), false));\n+            }\n+        }\n+\n+        Ok(splits)",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "463220617",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 360,
        "pr_file": "tokenizers/src/tokenizer/pattern.rs",
        "discussion_id": "463220617",
        "commented_code": "@@ -0,0 +1,181 @@\n+use crate::{Offsets, Result};\n+use regex::Regex;\n+\n+/// Pattern used to split a NormalizedString\n+pub trait Pattern {\n+    /// Slice the given string in a list of pattern match positions, with\n+    /// a boolean indicating whether this is a match or not.\n+    ///\n+    /// This method *must* cover the whole string in its outputs, with\n+    /// contiguous ordered slices.\n+    fn find_matches(&self, inside: &str) -> Result<Vec<(Offsets, bool)>>;\n+}\n+\n+impl Pattern for char {\n+    fn find_matches(&self, inside: &str) -> Result<Vec<(Offsets, bool)>> {\n+        let is_char = |c: char| -> bool { c == *self };\n+        is_char.find_matches(inside)\n+    }\n+}\n+\n+impl Pattern for &str {\n+    fn find_matches(&self, inside: &str) -> Result<Vec<(Offsets, bool)>> {\n+        let re = Regex::new(&regex::escape(self))?;\n+        (&re).find_matches(inside)\n+    }\n+}\n+\n+impl Pattern for &Regex {\n+    fn find_matches(&self, inside: &str) -> Result<Vec<(Offsets, bool)>> {\n+        if inside.is_empty() {\n+            return Ok(vec![((0, 0), false)]);\n+        }\n+\n+        // Find initial matches\n+        let matches = self\n+            .find_iter(inside)\n+            .map(|m| ((m.start(), m.end()), true))\n+            .collect::<Vec<_>>();\n+\n+        // Then add missing splits inbetween\n+        let mut start_offset = 0;\n+        let mut splits = matches\n+            .into_iter()\n+            .flat_map(|((start, end), flag)| {\n+                let mut splits = vec![];\n+                if start_offset < start {\n+                    splits.push(((start_offset, start), false));\n+                }\n+                splits.push(((start, end), flag));\n+                start_offset = end;\n+\n+                splits\n+            })\n+            .collect::<Vec<_>>();\n+\n+        if let Some(((_, end), _)) = splits.iter().last().copied() {\n+            if end < inside.len() {\n+                splits.push(((end, inside.len()), false));\n+            }\n+        }\n+\n+        Ok(splits)",
        "comment_created_at": "2020-07-30T19:26:04+00:00",
        "comment_author": "sebpuetz",
        "comment_body": "This could be done in a single pass over the matches\r\n\r\n```suggestion\r\n        let mut prev = 0;\r\n        let mut splits = Vec::with_capacity(inside.len());\r\n        for m in self.find_iter(inside) {\r\n            if prev != m.start() {\r\n                splits.push(((prev, m.start()), false));\r\n            }\r\n            splits.push(((m.start(), m.end()), true));\r\n            prev = m.end();\r\n        }\r\n        if prev != inside.len() {\r\n            splits.push(((prev, inside.len()), false))\r\n        }\r\n        Ok(splits)\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "503745445",
    "pr_number": 454,
    "pr_file": "tokenizers/src/tokenizer/normalizer.rs",
    "created_at": "2020-10-13T08:00:58+00:00",
    "commented_code": "None\n                 }\n             })\n+            .filter_map(|o| o)\n             .collect::<Vec<_>>();\n-        self.transform(\n-            filtered.into_iter().rev().filter_map(|o| o),\n-            removed as usize,\n-        );\n+        // We can't use double rev, it get optimized away and make the code wrong.\n+        // Re-iterating with rev will trigger clippy error.\n+        // Reversing here seems slow but should not be a problem as it should\n+        // help cache locality.",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "503745445",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 454,
        "pr_file": "tokenizers/src/tokenizer/normalizer.rs",
        "discussion_id": "503745445",
        "commented_code": "@@ -490,11 +490,14 @@ impl NormalizedString {\n                     None\n                 }\n             })\n+            .filter_map(|o| o)\n             .collect::<Vec<_>>();\n-        self.transform(\n-            filtered.into_iter().rev().filter_map(|o| o),\n-            removed as usize,\n-        );\n+        // We can't use double rev, it get optimized away and make the code wrong.\n+        // Re-iterating with rev will trigger clippy error.\n+        // Reversing here seems slow but should not be a problem as it should\n+        // help cache locality.",
        "comment_created_at": "2020-10-13T08:00:58+00:00",
        "comment_author": "sebpuetz",
        "comment_body": "Rust's iterators are evaluated lazily, double `rev()`ing without collecting cancels out the reverse: https://github.com/huggingface/tokenizers/pull/344#issuecomment-661054887\r\n\r\nReplacing the `into_iter().rev()` with the in-place `reverse()` is probably not optimal either since `into_iter().rev()` is essentially free as it just iterates from the end and in contrast to `reverse()` does not need to swap all elements of the vector.\r\n\r\nWas there a clippy lint complaining about the `into_iter().rev()` part?",
        "pr_file_module": null
      },
      {
        "comment_id": "503749144",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 454,
        "pr_file": "tokenizers/src/tokenizer/normalizer.rs",
        "discussion_id": "503745445",
        "commented_code": "@@ -490,11 +490,14 @@ impl NormalizedString {\n                     None\n                 }\n             })\n+            .filter_map(|o| o)\n             .collect::<Vec<_>>();\n-        self.transform(\n-            filtered.into_iter().rev().filter_map(|o| o),\n-            removed as usize,\n-        );\n+        // We can't use double rev, it get optimized away and make the code wrong.\n+        // Re-iterating with rev will trigger clippy error.\n+        // Reversing here seems slow but should not be a problem as it should\n+        // help cache locality.",
        "comment_created_at": "2020-10-13T08:07:00+00:00",
        "comment_author": "Narsil",
        "comment_body": "Yes. It complains that `needless_collect`. Basically that we collect something that we iterate over afterwards.\r\n\r\nWe could definitely use `[allow(needless_collect)]` But I think we should try to fix them anyway. We probably also could have only one forward pass for this one.\r\n\r\nBut how does `iter().rev()` handle cache locality though ? ",
        "pr_file_module": null
      },
      {
        "comment_id": "503823341",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 454,
        "pr_file": "tokenizers/src/tokenizer/normalizer.rs",
        "discussion_id": "503745445",
        "commented_code": "@@ -490,11 +490,14 @@ impl NormalizedString {\n                     None\n                 }\n             })\n+            .filter_map(|o| o)\n             .collect::<Vec<_>>();\n-        self.transform(\n-            filtered.into_iter().rev().filter_map(|o| o),\n-            removed as usize,\n-        );\n+        // We can't use double rev, it get optimized away and make the code wrong.\n+        // Re-iterating with rev will trigger clippy error.\n+        // Reversing here seems slow but should not be a problem as it should\n+        // help cache locality.",
        "comment_created_at": "2020-10-13T09:57:55+00:00",
        "comment_author": "Narsil",
        "comment_body": "I fixed it to only forward pass anyway, it's just better.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "463154989",
    "pr_number": 360,
    "pr_file": "tokenizers/src/tokenizer/encoding.rs",
    "created_at": "2020-07-30T17:25:08+00:00",
    "commented_code": "pub fn from_tokens(tokens: Vec<Token>, type_id: u32) -> Self {\n         let length = tokens.len();\n-        let (ids, tokens, offsets, words) = tokens.into_iter().fold(\n+        let (ids, tokens, offsets) = tokens.into_iter().fold(",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "463154989",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 360,
        "pr_file": "tokenizers/src/tokenizer/encoding.rs",
        "discussion_id": "463154989",
        "commented_code": "@@ -49,27 +49,25 @@ impl Encoding {\n \n     pub fn from_tokens(tokens: Vec<Token>, type_id: u32) -> Self {\n         let length = tokens.len();\n-        let (ids, tokens, offsets, words) = tokens.into_iter().fold(\n+        let (ids, tokens, offsets) = tokens.into_iter().fold(",
        "comment_created_at": "2020-07-30T17:25:08+00:00",
        "comment_author": "sebpuetz",
        "comment_body": "This could also be implemented in terms of an `Encoding::extend`, whether all these `reserve` calls make sense could be benched & the `Encoding::default()` call could probably be replaced if `Encoding::with_capacity()` is added.  Another option is `impl std::iter::FromIterator<Token> for Encoding`.\r\n\r\n```Rust\r\n    pub fn extend(&mut self, tokens: impl IntoIterator<Item=(Token, Option<u32>, u32)>) {\r\n        let tokens = tokens.into_iter();\r\n        let (lower, upper) = tokens.size_hint();\r\n        let length = upper.unwrap_or(lower);\r\n        self.ids.reserve(length);\r\n        self.tokens.reserve(length);\r\n        self.offsets.reserve(length);\r\n        self.type_ids.reserve(length);\r\n        self.attention_mask.reserve(length);\r\n        self.special_tokens_mask.reserve(length);\r\n        self.words.reserve(length);\r\n        for (token, word, type_id) in tokens {\r\n            self.ids.push(token.id);\r\n            self.tokens.push(token.value);\r\n            self.offsets.push(token.offsets);\r\n            self.type_ids.push(type_id);\r\n            self.attention_mask.push(1);\r\n            self.special_tokens_mask.push(0);\r\n            self.words.push(word);\r\n        }\r\n    }\r\n\r\n    pub fn from_tokens(tokens: Vec<Token>, type_id: u32) -> Self {\r\n        let mut enc = Encoding::default();\r\n        enc.extend(tokens.into_iter().map(|t| (t, None, type_id)));\r\n        return enc\r\n    }\r\n```",
        "pr_file_module": null
      }
    ]
  }
]