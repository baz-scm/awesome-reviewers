[
  {
    "discussion_id": "767781104",
    "pr_number": 841,
    "pr_file": "tokenizers/src/tokenizer/encoding.rs",
    "created_at": "2021-12-13T13:58:48+00:00",
    "commented_code": ")\n     }\n \n+    pub fn split_overflow(\n+        &mut self,\n+        max_len: usize,\n+        left: bool,\n+    ) -> (\n+        Vec<u32>,\n+        Vec<u32>,\n+        Vec<String>,\n+        Vec<Option<u32>>,\n+        Vec<(usize, usize)>,\n+        Vec<u32>,\n+        Vec<u32>,\n+    ) {\n+        if left {\n+            (\n+                self.ids.split_off(max_len),\n+                self.type_ids.split_off(max_len),\n+                self.tokens.split_off(max_len),\n+                self.words.split_off(max_len),\n+                self.offsets.split_off(max_len),\n+                self.special_tokens_mask.split_off(max_len),\n+                self.attention_mask.split_off(max_len),\n+            )\n+        } else {\n+            let at = self.ids.len() - max_len;\n+            (\n+                self.ids.drain(..at).collect(),\n+                self.type_ids.drain(..at).collect(),\n+                self.tokens.drain(..at).collect(),\n+                self.words.drain(..at).collect(),\n+                self.offsets.drain(..at).collect(),\n+                self.special_tokens_mask.drain(..at).collect(),\n+                self.attention_mask.drain(..at).collect(),\n+            )\n+        }\n+    }\n+\n     /// Truncate the current `Encoding`.\n     ///\n     /// Panic if `stride >= max_len`\n-    pub fn truncate(&mut self, max_len: usize, stride: usize) {\n+    pub fn truncate(&mut self, max_len: usize, stride: usize, left: bool) {",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "767781104",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 841,
        "pr_file": "tokenizers/src/tokenizer/encoding.rs",
        "discussion_id": "767781104",
        "commented_code": "@@ -292,10 +292,47 @@ impl Encoding {\n         )\n     }\n \n+    pub fn split_overflow(\n+        &mut self,\n+        max_len: usize,\n+        left: bool,\n+    ) -> (\n+        Vec<u32>,\n+        Vec<u32>,\n+        Vec<String>,\n+        Vec<Option<u32>>,\n+        Vec<(usize, usize)>,\n+        Vec<u32>,\n+        Vec<u32>,\n+    ) {\n+        if left {\n+            (\n+                self.ids.split_off(max_len),\n+                self.type_ids.split_off(max_len),\n+                self.tokens.split_off(max_len),\n+                self.words.split_off(max_len),\n+                self.offsets.split_off(max_len),\n+                self.special_tokens_mask.split_off(max_len),\n+                self.attention_mask.split_off(max_len),\n+            )\n+        } else {\n+            let at = self.ids.len() - max_len;\n+            (\n+                self.ids.drain(..at).collect(),\n+                self.type_ids.drain(..at).collect(),\n+                self.tokens.drain(..at).collect(),\n+                self.words.drain(..at).collect(),\n+                self.offsets.drain(..at).collect(),\n+                self.special_tokens_mask.drain(..at).collect(),\n+                self.attention_mask.drain(..at).collect(),\n+            )\n+        }\n+    }\n+\n     /// Truncate the current `Encoding`.\n     ///\n     /// Panic if `stride >= max_len`\n-    pub fn truncate(&mut self, max_len: usize, stride: usize) {\n+    pub fn truncate(&mut self, max_len: usize, stride: usize, left: bool) {",
        "comment_created_at": "2021-12-13T13:58:48+00:00",
        "comment_author": "Narsil",
        "comment_body": "The argument name is odd to me.\r\n\r\ntruncate_left([0, 1, 2], 2) -> [1, 2]\r\ntruncate_right([0, 1, 2], 2) -> [0, 1]\r\n\r\nIMO. I think from the tests that you're doing the opposite.\r\n\r\nrenaming `left` to `right` is enough as a first approximation.\r\n\r\nIn `transformers` there's actually a `direction` name which would be an enum might clarify things a bit.",
        "pr_file_module": null
      },
      {
        "comment_id": "768549191",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 841,
        "pr_file": "tokenizers/src/tokenizer/encoding.rs",
        "discussion_id": "767781104",
        "commented_code": "@@ -292,10 +292,47 @@ impl Encoding {\n         )\n     }\n \n+    pub fn split_overflow(\n+        &mut self,\n+        max_len: usize,\n+        left: bool,\n+    ) -> (\n+        Vec<u32>,\n+        Vec<u32>,\n+        Vec<String>,\n+        Vec<Option<u32>>,\n+        Vec<(usize, usize)>,\n+        Vec<u32>,\n+        Vec<u32>,\n+    ) {\n+        if left {\n+            (\n+                self.ids.split_off(max_len),\n+                self.type_ids.split_off(max_len),\n+                self.tokens.split_off(max_len),\n+                self.words.split_off(max_len),\n+                self.offsets.split_off(max_len),\n+                self.special_tokens_mask.split_off(max_len),\n+                self.attention_mask.split_off(max_len),\n+            )\n+        } else {\n+            let at = self.ids.len() - max_len;\n+            (\n+                self.ids.drain(..at).collect(),\n+                self.type_ids.drain(..at).collect(),\n+                self.tokens.drain(..at).collect(),\n+                self.words.drain(..at).collect(),\n+                self.offsets.drain(..at).collect(),\n+                self.special_tokens_mask.drain(..at).collect(),\n+                self.attention_mask.drain(..at).collect(),\n+            )\n+        }\n+    }\n+\n     /// Truncate the current `Encoding`.\n     ///\n     /// Panic if `stride >= max_len`\n-    pub fn truncate(&mut self, max_len: usize, stride: usize) {\n+    pub fn truncate(&mut self, max_len: usize, stride: usize, left: bool) {",
        "comment_created_at": "2021-12-14T10:57:11+00:00",
        "comment_author": "n1t0",
        "comment_body": "We can probably keep it as an argument, but I would go for an Enum too. Was thinking about the name, and I find `side` easier to understand maybe, but for consistency with padding, we might prefer `direction`. Wdyt?",
        "pr_file_module": null
      },
      {
        "comment_id": "768647540",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 841,
        "pr_file": "tokenizers/src/tokenizer/encoding.rs",
        "discussion_id": "767781104",
        "commented_code": "@@ -292,10 +292,47 @@ impl Encoding {\n         )\n     }\n \n+    pub fn split_overflow(\n+        &mut self,\n+        max_len: usize,\n+        left: bool,\n+    ) -> (\n+        Vec<u32>,\n+        Vec<u32>,\n+        Vec<String>,\n+        Vec<Option<u32>>,\n+        Vec<(usize, usize)>,\n+        Vec<u32>,\n+        Vec<u32>,\n+    ) {\n+        if left {\n+            (\n+                self.ids.split_off(max_len),\n+                self.type_ids.split_off(max_len),\n+                self.tokens.split_off(max_len),\n+                self.words.split_off(max_len),\n+                self.offsets.split_off(max_len),\n+                self.special_tokens_mask.split_off(max_len),\n+                self.attention_mask.split_off(max_len),\n+            )\n+        } else {\n+            let at = self.ids.len() - max_len;\n+            (\n+                self.ids.drain(..at).collect(),\n+                self.type_ids.drain(..at).collect(),\n+                self.tokens.drain(..at).collect(),\n+                self.words.drain(..at).collect(),\n+                self.offsets.drain(..at).collect(),\n+                self.special_tokens_mask.drain(..at).collect(),\n+                self.attention_mask.drain(..at).collect(),\n+            )\n+        }\n+    }\n+\n     /// Truncate the current `Encoding`.\n     ///\n     /// Panic if `stride >= max_len`\n-    pub fn truncate(&mut self, max_len: usize, stride: usize) {\n+    pub fn truncate(&mut self, max_len: usize, stride: usize, left: bool) {",
        "comment_created_at": "2021-12-14T13:09:22+00:00",
        "comment_author": "McPatate",
        "comment_body": "Direction sounds good imo",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "378973287",
    "pr_number": 146,
    "pr_file": "bindings/python/src/tokenizer.rs",
    "created_at": "2020-02-13T16:29:39+00:00",
    "commented_code": "}\n     }\n \n+    fn num_added_tokens(&self, is_pair: bool) -> PyResult<usize> {",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "378973287",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 146,
        "pr_file": "bindings/python/src/tokenizer.rs",
        "discussion_id": "378973287",
        "commented_code": "@@ -38,6 +38,13 @@ impl Tokenizer {\n         }\n     }\n \n+    fn num_added_tokens(&self, is_pair: bool) -> PyResult<usize> {",
        "comment_created_at": "2020-02-13T16:29:39+00:00",
        "comment_author": "n1t0",
        "comment_body": "I'm not sure it really makes sense directly exposed on the `Tokenizer`. I'd expect this to be the number of added tokens that gets returned by `get_vocab_size(with_added_tokens=True)`.\r\n\r\nThis can simply be retrieved by doing `tokenizer.post_processor.num_added_tokens(is_pair)`",
        "pr_file_module": null
      },
      {
        "comment_id": "379023907",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 146,
        "pr_file": "bindings/python/src/tokenizer.rs",
        "discussion_id": "378973287",
        "commented_code": "@@ -38,6 +38,13 @@ impl Tokenizer {\n         }\n     }\n \n+    fn num_added_tokens(&self, is_pair: bool) -> PyResult<usize> {",
        "comment_created_at": "2020-02-13T17:56:26+00:00",
        "comment_author": "mfuntowicz",
        "comment_body": "I exposed this method on Tokenizer for the case where there is no post_processor. \r\nIn this case, it requires to check if post_processor is not null before accessing the property and returning 0 if null.\r\n\r\nHaving this wrapped on the Rust side doesn't require such checking to be done in every bindings we would have.",
        "pr_file_module": null
      },
      {
        "comment_id": "379027483",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 146,
        "pr_file": "bindings/python/src/tokenizer.rs",
        "discussion_id": "378973287",
        "commented_code": "@@ -38,6 +38,13 @@ impl Tokenizer {\n         }\n     }\n \n+    fn num_added_tokens(&self, is_pair: bool) -> PyResult<usize> {",
        "comment_created_at": "2020-02-13T18:03:19+00:00",
        "comment_author": "n1t0",
        "comment_body": "I understand. My main concern is just about having `add_tokens` that does something, and `num_added_tokens` just right beside it, doing something completely different. If we find the right name, we can add it here!",
        "pr_file_module": null
      },
      {
        "comment_id": "379068413",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 146,
        "pr_file": "bindings/python/src/tokenizer.rs",
        "discussion_id": "378973287",
        "commented_code": "@@ -38,6 +38,13 @@ impl Tokenizer {\n         }\n     }\n \n+    fn num_added_tokens(&self, is_pair: bool) -> PyResult<usize> {",
        "comment_created_at": "2020-02-13T19:22:46+00:00",
        "comment_author": "mfuntowicz",
        "comment_body": "What about just keeping added_tokens everywhere ?",
        "pr_file_module": null
      },
      {
        "comment_id": "379110429",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 146,
        "pr_file": "bindings/python/src/tokenizer.rs",
        "discussion_id": "378973287",
        "commented_code": "@@ -38,6 +38,13 @@ impl Tokenizer {\n         }\n     }\n \n+    fn num_added_tokens(&self, is_pair: bool) -> PyResult<usize> {",
        "comment_created_at": "2020-02-13T20:48:50+00:00",
        "comment_author": "n1t0",
        "comment_body": "As discussed, let's use `num_special_tokens_to_add`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1392781197",
    "pr_number": 1357,
    "pr_file": "bindings/python/src/pre_tokenizers.rs",
    "created_at": "2023-11-14T15:26:49+00:00",
    "commented_code": "setter!(self_, Metaspace, add_prefix_space, add_prefix_space);\n     }\n \n+    #[getter]\n+    fn get_prepend_scheme(self_: PyRef<Self>) -> String {\n+        // Assuming Metaspace has a method to get the prepend_scheme as a string\n+        let scheme: PrependScheme = getter!(self_, Metaspace, get_prepend_scheme());\n+        match scheme {\n+            PrependScheme::First => \"First\",\n+            PrependScheme::Never => \"Never\",\n+            PrependScheme::Always => \"Always\",\n+        }\n+        .to_string()\n+    }\n+    const UNKNOWN_VARIANT_ERROR_MESSAGE: &str =\n+        \"is an unknown variant, should be one of ['First', 'Never', 'Always']\";\n+\n+    #[setter]\n+    fn set_prepend_scheme(self_: PyRef<Self>, prepend_scheme: &str) -> PyResult<()> {\n+        // Assuming Metaspace has a method to set the prepend_scheme from a string\n+        let scheme = match prepend_scheme {\n+            \"First\" => PrependScheme::First,\n+            \"Never\" => PrependScheme::Never,\n+            \"Always\" => PrependScheme::Always,\n+            _ => {\n+                return Err(exceptions::PyValueError::new_err(format!(\n+                    \"{} {}\",\n+                    prepend_scheme,\n+                    Self::UNKNOWN_VARIANT_ERROR_MESSAGE,\n+                )));\n+            }\n+        };\n+        setter!(self_, Metaspace, @set_prepend_scheme, scheme);\n+        Ok(())\n+    }\n+\n     #[new]\n-    #[pyo3(signature = (replacement = PyChar('\u2581'), add_prefix_space = true, **_kwargs), text_signature = \"(self, replacement=\\\"_\\\", add_prefix_space=True)\")]\n+    #[pyo3(signature = (replacement = PyChar('\u2581'), add_prefix_space = true, prepend_scheme=None, **_kwargs), text_signature = \"(self, replacement=\\\"_\\\", add_prefix_space=True)\")]\n     fn new(\n         replacement: PyChar,\n         add_prefix_space: bool,\n+        prepend_scheme: Option<String>,\n         _kwargs: Option<&PyDict>,\n-    ) -> (Self, PyPreTokenizer) {\n-        (\n-            PyMetaspace {},\n-            Metaspace::new(replacement.0, add_prefix_space).into(),\n-        )\n+    ) -> PyResult<(Self, PyPreTokenizer)> {\n+        // Create a new Metaspace instance\n+        let mut new_instance: Metaspace = Metaspace::new(replacement.0, add_prefix_space);\n+\n+        // If a prepend scheme is provided, set it\n+        if let Some(prepend_scheme) = prepend_scheme {\n+            let prepend_scheme_enum = match prepend_scheme.as_str() {\n+                \"First\" => PrependScheme::First,",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "1392781197",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1357,
        "pr_file": "bindings/python/src/pre_tokenizers.rs",
        "discussion_id": "1392781197",
        "commented_code": "@@ -489,17 +489,67 @@ impl PyMetaspace {\n         setter!(self_, Metaspace, add_prefix_space, add_prefix_space);\n     }\n \n+    #[getter]\n+    fn get_prepend_scheme(self_: PyRef<Self>) -> String {\n+        // Assuming Metaspace has a method to get the prepend_scheme as a string\n+        let scheme: PrependScheme = getter!(self_, Metaspace, get_prepend_scheme());\n+        match scheme {\n+            PrependScheme::First => \"First\",\n+            PrependScheme::Never => \"Never\",\n+            PrependScheme::Always => \"Always\",\n+        }\n+        .to_string()\n+    }\n+    const UNKNOWN_VARIANT_ERROR_MESSAGE: &str =\n+        \"is an unknown variant, should be one of ['First', 'Never', 'Always']\";\n+\n+    #[setter]\n+    fn set_prepend_scheme(self_: PyRef<Self>, prepend_scheme: &str) -> PyResult<()> {\n+        // Assuming Metaspace has a method to set the prepend_scheme from a string\n+        let scheme = match prepend_scheme {\n+            \"First\" => PrependScheme::First,\n+            \"Never\" => PrependScheme::Never,\n+            \"Always\" => PrependScheme::Always,\n+            _ => {\n+                return Err(exceptions::PyValueError::new_err(format!(\n+                    \"{} {}\",\n+                    prepend_scheme,\n+                    Self::UNKNOWN_VARIANT_ERROR_MESSAGE,\n+                )));\n+            }\n+        };\n+        setter!(self_, Metaspace, @set_prepend_scheme, scheme);\n+        Ok(())\n+    }\n+\n     #[new]\n-    #[pyo3(signature = (replacement = PyChar('\u2581'), add_prefix_space = true, **_kwargs), text_signature = \"(self, replacement=\\\"_\\\", add_prefix_space=True)\")]\n+    #[pyo3(signature = (replacement = PyChar('\u2581'), add_prefix_space = true, prepend_scheme=None, **_kwargs), text_signature = \"(self, replacement=\\\"_\\\", add_prefix_space=True)\")]\n     fn new(\n         replacement: PyChar,\n         add_prefix_space: bool,\n+        prepend_scheme: Option<String>,\n         _kwargs: Option<&PyDict>,\n-    ) -> (Self, PyPreTokenizer) {\n-        (\n-            PyMetaspace {},\n-            Metaspace::new(replacement.0, add_prefix_space).into(),\n-        )\n+    ) -> PyResult<(Self, PyPreTokenizer)> {\n+        // Create a new Metaspace instance\n+        let mut new_instance: Metaspace = Metaspace::new(replacement.0, add_prefix_space);\n+\n+        // If a prepend scheme is provided, set it\n+        if let Some(prepend_scheme) = prepend_scheme {\n+            let prepend_scheme_enum = match prepend_scheme.as_str() {\n+                \"First\" => PrependScheme::First,",
        "comment_created_at": "2023-11-14T15:26:49+00:00",
        "comment_author": "Narsil",
        "comment_body": "Please factor this code out ! :D. You shouldn't need to have this twice, Make it a simple function.\r\n\r\nAlso why change `\"First\"`  ? `\"first\"` is much more pythonic imo.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1392916499",
    "pr_number": 1357,
    "pr_file": "bindings/python/src/pre_tokenizers.rs",
    "created_at": "2023-11-14T16:50:48+00:00",
    "commented_code": "}\n }\n \n+fn _from_string(string: String) -> Result<PrependScheme, PyErr> {",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "1392916499",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1357,
        "pr_file": "bindings/python/src/pre_tokenizers.rs",
        "discussion_id": "1392916499",
        "commented_code": "@@ -452,6 +452,21 @@ impl PySequence {\n     }\n }\n \n+fn _from_string(string: String) -> Result<PrependScheme, PyErr> {",
        "comment_created_at": "2023-11-14T16:50:48+00:00",
        "comment_author": "Narsil",
        "comment_body": "```suggestion\r\nfn from_string(string: String) -> Result<PrependScheme, PyErr> {\r\n```\r\n\r\nFunctions are private by default. `pub` means public.",
        "pr_file_module": null
      }
    ]
  }
]