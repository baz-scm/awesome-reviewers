[
  {
    "discussion_id": "672173130",
    "pr_number": 762,
    "pr_file": "bindings/python/py_src/tokenizers/implementations/sentencepiece_unigram.py",
    "created_at": "2021-07-19T10:18:00+00:00",
    "commented_code": "vocab_size: int = 8000,\n         show_progress: bool = True,\n         special_tokens: List[Union[str, AddedToken]] = [],\n+        unk_token: Optional[str] = None,\n     ):\n-        \"\"\" Train the model using the given files \"\"\"\n+        \"\"\"\n+        Train the model using the given files\n+\n+        Args:\n+            files (:obj:`List[str]`):\n+                A list of path to the files that we should use for training\n+            vocab_size (:obj:`int`):\n+                The size of the final vocabulary, including all tokens and alphabet.\n+            show_progress (:obj:`bool`):\n+                Whether to show progress bars while training.\n+            special_tokens (:obj:`List[Union[str, AddedToken]]`):\n+                A list of special tokens the model should know of.\n+            unk_token (:obj:`str`, `optional`):\n+                The unknown token to be used by the model.",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "672173130",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 762,
        "pr_file": "bindings/python/py_src/tokenizers/implementations/sentencepiece_unigram.py",
        "discussion_id": "672173130",
        "commented_code": "@@ -49,13 +49,29 @@ def train(\n         vocab_size: int = 8000,\n         show_progress: bool = True,\n         special_tokens: List[Union[str, AddedToken]] = [],\n+        unk_token: Optional[str] = None,\n     ):\n-        \"\"\" Train the model using the given files \"\"\"\n+        \"\"\"\n+        Train the model using the given files\n+\n+        Args:\n+            files (:obj:`List[str]`):\n+                A list of path to the files that we should use for training\n+            vocab_size (:obj:`int`):\n+                The size of the final vocabulary, including all tokens and alphabet.\n+            show_progress (:obj:`bool`):\n+                Whether to show progress bars while training.\n+            special_tokens (:obj:`List[Union[str, AddedToken]]`):\n+                A list of special tokens the model should know of.\n+            unk_token (:obj:`str`, `optional`):\n+                The unknown token to be used by the model.",
        "comment_created_at": "2021-07-19T10:18:00+00:00",
        "comment_author": "SaulLu",
        "comment_body": "Not all implementations in the same folder describe method arguments (I think because these objects are not in the documentation). Nevertheless, here I thought it could be really interesting to have them so that the user can understand what the new added `unk_token` argument is for.",
        "pr_file_module": null
      },
      {
        "comment_id": "673852743",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 762,
        "pr_file": "bindings/python/py_src/tokenizers/implementations/sentencepiece_unigram.py",
        "discussion_id": "672173130",
        "commented_code": "@@ -49,13 +49,29 @@ def train(\n         vocab_size: int = 8000,\n         show_progress: bool = True,\n         special_tokens: List[Union[str, AddedToken]] = [],\n+        unk_token: Optional[str] = None,\n     ):\n-        \"\"\" Train the model using the given files \"\"\"\n+        \"\"\"\n+        Train the model using the given files\n+\n+        Args:\n+            files (:obj:`List[str]`):\n+                A list of path to the files that we should use for training\n+            vocab_size (:obj:`int`):\n+                The size of the final vocabulary, including all tokens and alphabet.\n+            show_progress (:obj:`bool`):\n+                Whether to show progress bars while training.\n+            special_tokens (:obj:`List[Union[str, AddedToken]]`):\n+                A list of special tokens the model should know of.\n+            unk_token (:obj:`str`, `optional`):\n+                The unknown token to be used by the model.",
        "comment_created_at": "2021-07-21T10:25:47+00:00",
        "comment_author": "Narsil",
        "comment_body": "Yes, totally fine. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "492510136",
    "pr_number": 427,
    "pr_file": "bindings/python/examples/custom_components.py",
    "created_at": "2020-09-22T06:55:37+00:00",
    "commented_code": "+import jieba\n+\n+from tokenizers import Tokenizer, Regex\n+from tokenizers.models import BPE\n+from tokenizers.pre_tokenizers import PreTokenizer\n+from tokenizers.normalizers import Normalizer\n+from tokenizers.decoders import Decoder\n+\n+\n+class JiebaPreTokenizer:\n+    def jieba_split(self, i, normalized):\n+        return [normalized[w[1] : w[2]] for w in jieba.tokenize(str(normalized))]",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "492510136",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 427,
        "pr_file": "bindings/python/examples/custom_components.py",
        "discussion_id": "492510136",
        "commented_code": "@@ -0,0 +1,47 @@\n+import jieba\n+\n+from tokenizers import Tokenizer, Regex\n+from tokenizers.models import BPE\n+from tokenizers.pre_tokenizers import PreTokenizer\n+from tokenizers.normalizers import Normalizer\n+from tokenizers.decoders import Decoder\n+\n+\n+class JiebaPreTokenizer:\n+    def jieba_split(self, i, normalized):\n+        return [normalized[w[1] : w[2]] for w in jieba.tokenize(str(normalized))]",
        "comment_created_at": "2020-09-22T06:55:37+00:00",
        "comment_author": "Narsil",
        "comment_body": "We probably should add a little comment here, jieba's behavior is not super straightforward here.\r\n\r\nAlso we should probably explain what this code does.\r\n\r\n```python\r\nclass JiebaPreTokenizer:\r\n    def jieba_split(self, pretoken_index, pretoken):\r\n        new_tokens = []\r\n        # Why do we need `str(normalized)?`\r\n        pretoken_string = str(pretoken)\r\n        for token, start, stop in jieba.tokenize(pretoken_string):\r\n            new_tokens.append(normalized[start:stop])\r\n        return new_tokens\r\n\r\n    def pre_tokenize(self, pretok):\r\n        # Let's call split on the PreTokenizedString to split using `self.split`\r\n        # pretok.split takes in a function, that receives `i` that is the current token index\r\n        # and `pretoken` that is a substring of th original string, that might have been normalized,\r\n        # and already cut up by previous pretokenizer. It should return a list of subtokens.\r\n        # `pretok.split` changes it inplace.\r\n\r\n        # Checkout X to see all available primitives to do a custom pretokenization\r\n        pretok.split(self.jieba_split)\r\n```",
        "pr_file_module": null
      }
    ]
  }
]