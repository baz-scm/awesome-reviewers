[
  {
    "discussion_id": "815697848",
    "pr_number": 921,
    "pr_file": "tokenizers/src/models/unigram/trainer.rs",
    "created_at": "2022-02-28T09:09:40+00:00",
    "commented_code": "}\n                 (objs, ntokens, expected)\n             })\n-            .collect();\n-\n-        let mut expected: Vec<f64> = vec![0.0; model.len()];\n-        let mut objs: f64 = 0.0;\n-        let mut ntokens: u32 = 0;\n-\n-        for (lobjs, lntokens, lexpected) in collected {\n-            objs += lobjs;\n-            ntokens += lntokens;\n-            expected\n-                .iter_mut()\n-                .zip(lexpected)\n-                .for_each(|(global_el, local_el)| *global_el += local_el)\n-        }\n+            .reduce(\n+                || (0.0, 0, vec![0.0; model.len()]),\n+                |(objs, ntokens, expected), (lobjs, lntokens, lexpected)| {\n+                    (\n+                        objs + lobjs,\n+                        ntokens + lntokens,\n+                        expected\n+                            .iter()\n+                            .zip(lexpected)\n+                            .map(|(global_el, local_el)| global_el + local_el)\n+                            .collect(),",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "815697848",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 921,
        "pr_file": "tokenizers/src/models/unigram/trainer.rs",
        "discussion_id": "815697848",
        "commented_code": "@@ -425,22 +454,22 @@ impl UnigramTrainer {\n                 }\n                 (objs, ntokens, expected)\n             })\n-            .collect();\n-\n-        let mut expected: Vec<f64> = vec![0.0; model.len()];\n-        let mut objs: f64 = 0.0;\n-        let mut ntokens: u32 = 0;\n-\n-        for (lobjs, lntokens, lexpected) in collected {\n-            objs += lobjs;\n-            ntokens += lntokens;\n-            expected\n-                .iter_mut()\n-                .zip(lexpected)\n-                .for_each(|(global_el, local_el)| *global_el += local_el)\n-        }\n+            .reduce(\n+                || (0.0, 0, vec![0.0; model.len()]),\n+                |(objs, ntokens, expected), (lobjs, lntokens, lexpected)| {\n+                    (\n+                        objs + lobjs,\n+                        ntokens + lntokens,\n+                        expected\n+                            .iter()\n+                            .zip(lexpected)\n+                            .map(|(global_el, local_el)| global_el + local_el)\n+                            .collect(),",
        "comment_created_at": "2022-02-28T09:09:40+00:00",
        "comment_author": "Narsil",
        "comment_body": "`iter_mut()` + `for_each` makes sure there are not reallocations.\r\n\r\n`.collect()` is like `.clone()`, avoid if possible.\r\n\r\nIt might be optimized away but I would rather not count on it.",
        "pr_file_module": null
      },
      {
        "comment_id": "815741441",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 921,
        "pr_file": "tokenizers/src/models/unigram/trainer.rs",
        "discussion_id": "815697848",
        "commented_code": "@@ -425,22 +454,22 @@ impl UnigramTrainer {\n                 }\n                 (objs, ntokens, expected)\n             })\n-            .collect();\n-\n-        let mut expected: Vec<f64> = vec![0.0; model.len()];\n-        let mut objs: f64 = 0.0;\n-        let mut ntokens: u32 = 0;\n-\n-        for (lobjs, lntokens, lexpected) in collected {\n-            objs += lobjs;\n-            ntokens += lntokens;\n-            expected\n-                .iter_mut()\n-                .zip(lexpected)\n-                .for_each(|(global_el, local_el)| *global_el += local_el)\n-        }\n+            .reduce(\n+                || (0.0, 0, vec![0.0; model.len()]),\n+                |(objs, ntokens, expected), (lobjs, lntokens, lexpected)| {\n+                    (\n+                        objs + lobjs,\n+                        ntokens + lntokens,\n+                        expected\n+                            .iter()\n+                            .zip(lexpected)\n+                            .map(|(global_el, local_el)| global_el + local_el)\n+                            .collect(),",
        "comment_created_at": "2022-02-28T10:05:26+00:00",
        "comment_author": "thomasw21",
        "comment_body": "Hum but you need to set that variable to `mut` no?",
        "pr_file_module": null
      },
      {
        "comment_id": "815757163",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 921,
        "pr_file": "tokenizers/src/models/unigram/trainer.rs",
        "discussion_id": "815697848",
        "commented_code": "@@ -425,22 +454,22 @@ impl UnigramTrainer {\n                 }\n                 (objs, ntokens, expected)\n             })\n-            .collect();\n-\n-        let mut expected: Vec<f64> = vec![0.0; model.len()];\n-        let mut objs: f64 = 0.0;\n-        let mut ntokens: u32 = 0;\n-\n-        for (lobjs, lntokens, lexpected) in collected {\n-            objs += lobjs;\n-            ntokens += lntokens;\n-            expected\n-                .iter_mut()\n-                .zip(lexpected)\n-                .for_each(|(global_el, local_el)| *global_el += local_el)\n-        }\n+            .reduce(\n+                || (0.0, 0, vec![0.0; model.len()]),\n+                |(objs, ntokens, expected), (lobjs, lntokens, lexpected)| {\n+                    (\n+                        objs + lobjs,\n+                        ntokens + lntokens,\n+                        expected\n+                            .iter()\n+                            .zip(lexpected)\n+                            .map(|(global_el, local_el)| global_el + local_el)\n+                            .collect(),",
        "comment_created_at": "2022-02-28T10:26:11+00:00",
        "comment_author": "Narsil",
        "comment_body": "Probably, but if the reduce is not itself parallel, then it's ok to have `global` be mutable.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1392676087",
    "pr_number": 1357,
    "pr_file": "tokenizers/src/pre_tokenizers/metaspace.rs",
    "created_at": "2023-11-14T14:29:46+00:00",
    "commented_code": "self.replacement = replacement;\n         self.str_rep = replacement.to_string();\n     }\n+\n+    pub fn get_prepend_scheme(&self) -> PrependScheme {\n+        self.prepend_scheme.clone()",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "1392676087",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1357,
        "pr_file": "tokenizers/src/pre_tokenizers/metaspace.rs",
        "discussion_id": "1392676087",
        "commented_code": "@@ -55,6 +92,14 @@ impl Metaspace {\n         self.replacement = replacement;\n         self.str_rep = replacement.to_string();\n     }\n+\n+    pub fn get_prepend_scheme(&self) -> PrependScheme {\n+        self.prepend_scheme.clone()",
        "comment_created_at": "2023-11-14T14:29:46+00:00",
        "comment_author": "Narsil",
        "comment_body": "Don't clone on behalf of users, never.\r\n\r\nEither return a reference `&PrependScheme`.\r\nOr make `PrependScheme`  ` Copy`.\r\n\r\nMaking something Copy, is something you should do only when the size makes it worthwile.\r\nA reference is a pointer of size `usize` so copying usize is usually much faster than copying an entire struct.\r\nFor an enum like PrependScheme, it' s only 3 possible values, encoded internally by rust as `u8` or `usize` (Don't remember which). In any case, it's prefereable to copy the value than to pass references around (which cost a pointer dereference)\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1226260315",
    "pr_number": 1095,
    "pr_file": "tokenizers/src/tokenizer/mod.rs",
    "created_at": "2023-06-12T08:15:46+00:00",
    "commented_code": "final_vocab\n     }\n \n+    /// Get the added tokens\n+    pub fn get_added_tokens(&self) -> Vec<String> {\n+        self.added_vocabulary.get_added_tokens()\n+    }",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "1226260315",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1095,
        "pr_file": "tokenizers/src/tokenizer/mod.rs",
        "discussion_id": "1226260315",
        "commented_code": "@@ -643,6 +643,11 @@ where\n         final_vocab\n     }\n \n+    /// Get the added tokens\n+    pub fn get_added_tokens(&self) -> Vec<String> {\n+        self.added_vocabulary.get_added_tokens()\n+    }",
        "comment_created_at": "2023-06-12T08:15:46+00:00",
        "comment_author": "Narsil",
        "comment_body": "Let's remove this. Vec allocs is costly, you don't want to do this.",
        "pr_file_module": null
      },
      {
        "comment_id": "1226668439",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1095,
        "pr_file": "tokenizers/src/tokenizer/mod.rs",
        "discussion_id": "1226260315",
        "commented_code": "@@ -643,6 +643,11 @@ where\n         final_vocab\n     }\n \n+    /// Get the added tokens\n+    pub fn get_added_tokens(&self) -> Vec<String> {\n+        self.added_vocabulary.get_added_tokens()\n+    }",
        "comment_created_at": "2023-06-12T13:28:11+00:00",
        "comment_author": "ArthurZucker",
        "comment_body": "yep",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "488035841",
    "pr_number": 401,
    "pr_file": "bindings/python/src/normalizers.rs",
    "created_at": "2020-09-14T15:44:14+00:00",
    "commented_code": "}\n }\n \n+#[pyclass(extends=PyNormalizer, module = \"tokenizers.normalizers\", name=Nmt)]\n+pub struct PyNmt {}\n+#[pymethods]\n+impl PyNmt {\n+    #[new]\n+    fn new() -> PyResult<(Self, PyNormalizer)> {\n+        Ok((PyNmt {}, Nmt.into()))\n+    }\n+}\n+\n+#[pyclass(extends=PyNormalizer, module = \"tokenizers.normalizers\", name=Precompiled)]\n+pub struct PyPrecompiled {}\n+#[pymethods]\n+impl PyPrecompiled {\n+    #[new]\n+    fn new(precompiled_charsmap: Vec<u8>) -> PyResult<(Self, PyNormalizer)> {",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "488035841",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 401,
        "pr_file": "bindings/python/src/normalizers.rs",
        "discussion_id": "488035841",
        "commented_code": "@@ -273,6 +279,36 @@ impl Normalizer for PyNormalizerWrapper {\n     }\n }\n \n+#[pyclass(extends=PyNormalizer, module = \"tokenizers.normalizers\", name=Nmt)]\n+pub struct PyNmt {}\n+#[pymethods]\n+impl PyNmt {\n+    #[new]\n+    fn new() -> PyResult<(Self, PyNormalizer)> {\n+        Ok((PyNmt {}, Nmt.into()))\n+    }\n+}\n+\n+#[pyclass(extends=PyNormalizer, module = \"tokenizers.normalizers\", name=Precompiled)]\n+pub struct PyPrecompiled {}\n+#[pymethods]\n+impl PyPrecompiled {\n+    #[new]\n+    fn new(precompiled_charsmap: Vec<u8>) -> PyResult<(Self, PyNormalizer)> {",
        "comment_created_at": "2020-09-14T15:44:14+00:00",
        "comment_author": "n1t0",
        "comment_body": "You can probably get `&PyBytes` and then a slice, to avoid allocating a `Vec` here.",
        "pr_file_module": null
      },
      {
        "comment_id": "488108405",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 401,
        "pr_file": "bindings/python/src/normalizers.rs",
        "discussion_id": "488035841",
        "commented_code": "@@ -273,6 +279,36 @@ impl Normalizer for PyNormalizerWrapper {\n     }\n }\n \n+#[pyclass(extends=PyNormalizer, module = \"tokenizers.normalizers\", name=Nmt)]\n+pub struct PyNmt {}\n+#[pymethods]\n+impl PyNmt {\n+    #[new]\n+    fn new() -> PyResult<(Self, PyNormalizer)> {\n+        Ok((PyNmt {}, Nmt.into()))\n+    }\n+}\n+\n+#[pyclass(extends=PyNormalizer, module = \"tokenizers.normalizers\", name=Precompiled)]\n+pub struct PyPrecompiled {}\n+#[pymethods]\n+impl PyPrecompiled {\n+    #[new]\n+    fn new(precompiled_charsmap: Vec<u8>) -> PyResult<(Self, PyNormalizer)> {",
        "comment_created_at": "2020-09-14T17:37:13+00:00",
        "comment_author": "Narsil",
        "comment_body": "Done.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "463152955",
    "pr_number": 360,
    "pr_file": "tokenizers/src/tokenizer/mod.rs",
    "created_at": "2020-07-30T17:21:39+00:00",
    "commented_code": "Ok(())\n     }\n \n+    /// Tokenization logic, makes the bridge between the pre-tokenization phase and the real\n+    /// tokenization phase, and converting offsets back to the original referential.\n+    fn do_tokenize<P: Into<PreTokenizedString>>(\n+        &self,\n+        pretokenized: P,\n+        original_offsets: Offsets,\n+        type_id: u32,\n+    ) -> Result<Encoding> {\n+        let pretokenized: PreTokenizedString = pretokenized.into();\n+\n+        let mut empty_words = 0;",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "463152955",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 360,
        "pr_file": "tokenizers/src/tokenizer/mod.rs",
        "discussion_id": "463152955",
        "commented_code": "@@ -671,20 +663,78 @@ impl Tokenizer {\n         Ok(())\n     }\n \n+    /// Tokenization logic, makes the bridge between the pre-tokenization phase and the real\n+    /// tokenization phase, and converting offsets back to the original referential.\n+    fn do_tokenize<P: Into<PreTokenizedString>>(\n+        &self,\n+        pretokenized: P,\n+        original_offsets: Offsets,\n+        type_id: u32,\n+    ) -> Result<Encoding> {\n+        let pretokenized: PreTokenizedString = pretokenized.into();\n+\n+        let mut empty_words = 0;",
        "comment_created_at": "2020-07-30T17:21:39+00:00",
        "comment_author": "sebpuetz",
        "comment_body": "This could be simplified quite a bit and you could save some allocations by adding an `Encoding::extend`:\r\n\r\n```Rust\r\n        let mut encoding = Encoding::default();\r\n        for (i, substr) in pretokenized.into_iter().filter(|substr| !substr.normalized.is_empty()).enumerate() {\r\n            let tokens = self.model.tokenize(substr.normalized.get())?;\r\n            // We convert the normalized offsets back to the original\r\n            encoding.extend(\r\n            tokens.into_iter().map(|mut token| {\r\n                let converted_offsets = substr\r\n                    .normalized\r\n                    .convert_offsets(Range::Normalized(token.offsets.0..token.offsets.1))\r\n                    .map_or(token.offsets, |range| {\r\n                        (\r\n                            original_offsets.0 + substr.original_offsets.0 + range.start,\r\n                            original_offsets.0 + substr.original_offsets.0 + range.end,\r\n                        )\r\n                    });\r\n\r\n                // And we update the token to these original offsets, applying the original offset\r\n                // of the sequence we just tokenized.\r\n                token.offsets = converted_offsets;\r\n                (token, Some(i as u32), type_id)\r\n            }));\r\n        }\r\n        Ok(encoding)\r\n```",
        "pr_file_module": null
      }
    ]
  }
]