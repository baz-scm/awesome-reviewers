[
  {
    "discussion_id": "1673617585",
    "pr_number": 1555,
    "pr_file": "tokenizers/src/pre_tokenizers/byte_level.rs",
    "created_at": "2024-07-11T08:27:29+00:00",
    "commented_code": "/// Converts bytes to unicode characters.\n /// See https://github.com/openai/gpt-2/blob/master/src/encoder.py#L9\n-fn bytes_char() -> HashMap<u8, char> {\n+pub fn bytes_char() -> HashMap<u8, char> {",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "1673617585",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1555,
        "pr_file": "tokenizers/src/pre_tokenizers/byte_level.rs",
        "discussion_id": "1673617585",
        "commented_code": "@@ -11,7 +11,7 @@ use crate::utils::macro_rules_attribute;\n \n /// Converts bytes to unicode characters.\n /// See https://github.com/openai/gpt-2/blob/master/src/encoder.py#L9\n-fn bytes_char() -> HashMap<u8, char> {\n+pub fn bytes_char() -> HashMap<u8, char> {",
        "comment_created_at": "2024-07-11T08:27:29+00:00",
        "comment_author": "McPatate",
        "comment_body": "```suggestion\r\npub(crate) fn bytes_char() -> HashMap<u8, char> {\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "481281977",
    "pr_number": 292,
    "pr_file": "tokenizers/src/models/unigram/trainer.rs",
    "created_at": "2020-09-01T16:35:45+00:00",
    "commented_code": "+use crate::models::unigram::{\n+    lattice::Lattice,\n+    model::Unigram,\n+    unicode::{get_script, Script},\n+};\n+use crate::tokenizer::{AddedToken, Result, Trainer};\n+use indicatif::{ProgressBar, ProgressStyle};\n+use std::cmp::Reverse;\n+use std::collections::{HashMap, HashSet};\n+use std::convert::TryInto;\n+\n+// A token and a score\n+type SentencePiece = (String, f64);\n+\n+// A full sentence or word + it's count within the dataset\n+type Sentence = (String, u32);\n+\n+fn digamma(mut x: f64) -> f64 {\n+    let mut result = 0.0;\n+    while x < 7.0 {\n+        result -= 1.0 / x;\n+        x += 1.0;\n+    }\n+    x -= 1.0 / 2.0;\n+    let xx = 1.0 / x;\n+    let xx2 = xx * xx;\n+    let xx4 = xx2 * xx2;\n+    result += x.ln() + (1.0 / 24.0) * xx2 - 7.0 / 960.0 * xx4 + (31.0 / 8064.0) * xx4 * xx2\n+        - (127.0 / 30720.0) * xx4 * xx4;\n+    result\n+}\n+\n+fn to_log_prob(pieces: &mut [SentencePiece]) {\n+    let sum: f64 = pieces.iter().map(|(_, score)| score).sum();\n+    let logsum = sum.ln();\n+    for (_, score) in pieces.iter_mut() {\n+        *score = score.ln() - logsum;\n+    }\n+}\n+\n+/// A `UnigramTrainer` can train a `Unigram` model from `word_counts`.\n+#[derive(Builder, Debug, Clone)]\n+pub struct UnigramTrainer {\n+    #[builder(default = \"true\")]\n+    show_progress: bool,\n+    #[builder(default = \"8000\")]\n+    vocab_size: u32,\n+    #[builder(default = \"2\")]\n+    n_sub_iterations: u32,\n+    #[builder(default = \"0.75\")]\n+    shrinking_factor: f64,\n+    #[builder(default = \"vec![]\")]\n+    special_tokens: Vec<AddedToken>,\n+\n+    #[builder(default = \"' '\")]\n+    space_char: char,\n+\n+    #[builder(default = \"String::from(\\\"<unk>\\\")\")]\n+    unk_token: String,\n+\n+    #[builder(default = \"false\")]\n+    treat_whitespace_as_suffix: bool,",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "481281977",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 292,
        "pr_file": "tokenizers/src/models/unigram/trainer.rs",
        "discussion_id": "481281977",
        "commented_code": "@@ -0,0 +1,685 @@\n+use crate::models::unigram::{\n+    lattice::Lattice,\n+    model::Unigram,\n+    unicode::{get_script, Script},\n+};\n+use crate::tokenizer::{AddedToken, Result, Trainer};\n+use indicatif::{ProgressBar, ProgressStyle};\n+use std::cmp::Reverse;\n+use std::collections::{HashMap, HashSet};\n+use std::convert::TryInto;\n+\n+// A token and a score\n+type SentencePiece = (String, f64);\n+\n+// A full sentence or word + it's count within the dataset\n+type Sentence = (String, u32);\n+\n+fn digamma(mut x: f64) -> f64 {\n+    let mut result = 0.0;\n+    while x < 7.0 {\n+        result -= 1.0 / x;\n+        x += 1.0;\n+    }\n+    x -= 1.0 / 2.0;\n+    let xx = 1.0 / x;\n+    let xx2 = xx * xx;\n+    let xx4 = xx2 * xx2;\n+    result += x.ln() + (1.0 / 24.0) * xx2 - 7.0 / 960.0 * xx4 + (31.0 / 8064.0) * xx4 * xx2\n+        - (127.0 / 30720.0) * xx4 * xx4;\n+    result\n+}\n+\n+fn to_log_prob(pieces: &mut [SentencePiece]) {\n+    let sum: f64 = pieces.iter().map(|(_, score)| score).sum();\n+    let logsum = sum.ln();\n+    for (_, score) in pieces.iter_mut() {\n+        *score = score.ln() - logsum;\n+    }\n+}\n+\n+/// A `UnigramTrainer` can train a `Unigram` model from `word_counts`.\n+#[derive(Builder, Debug, Clone)]\n+pub struct UnigramTrainer {\n+    #[builder(default = \"true\")]\n+    show_progress: bool,\n+    #[builder(default = \"8000\")]\n+    vocab_size: u32,\n+    #[builder(default = \"2\")]\n+    n_sub_iterations: u32,\n+    #[builder(default = \"0.75\")]\n+    shrinking_factor: f64,\n+    #[builder(default = \"vec![]\")]\n+    special_tokens: Vec<AddedToken>,\n+\n+    #[builder(default = \"' '\")]\n+    space_char: char,\n+\n+    #[builder(default = \"String::from(\\\"<unk>\\\")\")]\n+    unk_token: String,\n+\n+    #[builder(default = \"false\")]\n+    treat_whitespace_as_suffix: bool,",
        "comment_created_at": "2020-09-01T16:35:45+00:00",
        "comment_author": "n1t0",
        "comment_body": "As we discussed, a lot of these options might be better suited as standalone `Normalizer` or `PreTokenizer`, and removed from there. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "492514238",
    "pr_number": 427,
    "pr_file": "bindings/python/src/models.rs",
    "created_at": "2020-09-22T07:05:00+00:00",
    "commented_code": "}\n     }\n \n+    fn tokenize(&self, tokens: &str) -> PyResult<Vec<PyToken>> {",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "492514238",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 427,
        "pr_file": "bindings/python/src/models.rs",
        "discussion_id": "492514238",
        "commented_code": "@@ -105,6 +106,22 @@ impl PyModel {\n         }\n     }\n \n+    fn tokenize(&self, tokens: &str) -> PyResult<Vec<PyToken>> {",
        "comment_created_at": "2020-09-22T07:05:00+00:00",
        "comment_author": "Narsil",
        "comment_body": "Why to we need this ? `encode` is enough, no ?",
        "pr_file_module": null
      },
      {
        "comment_id": "492809954",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 427,
        "pr_file": "bindings/python/src/models.rs",
        "discussion_id": "492514238",
        "commented_code": "@@ -105,6 +106,22 @@ impl PyModel {\n         }\n     }\n \n+    fn tokenize(&self, tokens: &str) -> PyResult<Vec<PyToken>> {",
        "comment_created_at": "2020-09-22T15:04:29+00:00",
        "comment_author": "n1t0",
        "comment_body": "These serve different purpose. `PreTokenizedString::tokenize` is deep inside the `encode` pipeline, and some use-cases require customization.",
        "pr_file_module": null
      }
    ]
  }
]