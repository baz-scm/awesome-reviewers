[
  {
    "discussion_id": "2007409928",
    "pr_number": 1752,
    "pr_file": "tokenizers/src/models/wordlevel/mod.rs",
    "created_at": "2025-03-21T11:43:42+00:00",
    "commented_code": "/// Set the vocab (token -> ID) mapping.\n     #[must_use]\n-    pub fn vocab(mut self, vocab: HashMap<String, u32>) -> Self {\n-        self.config.vocab = vocab;\n+    pub fn vocab<S: BuildHasher>(mut self, vocab: HashMap<String, u32, S>) -> Self {",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "2007409928",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1752,
        "pr_file": "tokenizers/src/models/wordlevel/mod.rs",
        "discussion_id": "2007409928",
        "commented_code": "@@ -61,8 +64,8 @@ impl WordLevelBuilder {\n \n     /// Set the vocab (token -> ID) mapping.\n     #[must_use]\n-    pub fn vocab(mut self, vocab: HashMap<String, u32>) -> Self {\n-        self.config.vocab = vocab;\n+    pub fn vocab<S: BuildHasher>(mut self, vocab: HashMap<String, u32, S>) -> Self {",
        "comment_created_at": "2025-03-21T11:43:42+00:00",
        "comment_author": "McPatate",
        "comment_body": "why does this (`<S: BuildHasher>`) need to be specified?",
        "pr_file_module": null
      },
      {
        "comment_id": "2008977643",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1752,
        "pr_file": "tokenizers/src/models/wordlevel/mod.rs",
        "discussion_id": "2007409928",
        "commented_code": "@@ -61,8 +64,8 @@ impl WordLevelBuilder {\n \n     /// Set the vocab (token -> ID) mapping.\n     #[must_use]\n-    pub fn vocab(mut self, vocab: HashMap<String, u32>) -> Self {\n-        self.config.vocab = vocab;\n+    pub fn vocab<S: BuildHasher>(mut self, vocab: HashMap<String, u32, S>) -> Self {",
        "comment_created_at": "2025-03-23T00:13:20+00:00",
        "comment_author": "MeetThePatel",
        "comment_body": "This pattern of specifying a trait bound on `S` is to prevent the user from having to deal with the change in hashmap. For example, if the function signature is: `pub fn vocab(mut self, vocab: FxHashMap<String, u32>) -> Self`, the user of the library would need to convert to a FxHashMap on their end. This not only causes them to take another direct dependency, it would also cause breaking changes. I ran into this (I believe it is still in my other PR) where I would need to modify the tests to convert into a FxHashMap. Since FxHashMap is just a type-specified version of std::collections::HashMap using FxHash, this type signature lets the user interact with the library without any extra effort. I tried my best to make the least impact on the API, using the tests as an indicator.\r\n\r\nThis also explains my reasoning for not using the `use rustc_hash::FxHashMap as HashMap`, as in some files, such as this one, I need to specify using `std::collections::HashMap`. The reason for not using the import alias in the rest of the codebase is just for consistency. It would be harder to navigate the codebase if HashMap was defined as separate things in different files.",
        "pr_file_module": null
      },
      {
        "comment_id": "2010439358",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1752,
        "pr_file": "tokenizers/src/models/wordlevel/mod.rs",
        "discussion_id": "2007409928",
        "commented_code": "@@ -61,8 +64,8 @@ impl WordLevelBuilder {\n \n     /// Set the vocab (token -> ID) mapping.\n     #[must_use]\n-    pub fn vocab(mut self, vocab: HashMap<String, u32>) -> Self {\n-        self.config.vocab = vocab;\n+    pub fn vocab<S: BuildHasher>(mut self, vocab: HashMap<String, u32, S>) -> Self {",
        "comment_created_at": "2025-03-24T15:42:44+00:00",
        "comment_author": "McPatate",
        "comment_body": "Is the trait `BuildHasher` a requirement for `FxHashMap`?\r\n\r\nYou could also alias like so: `use std::collections::HashMap as StdHashMap` which shouldn't change anything API-wise. But I understand the rationale, sgtm.",
        "pr_file_module": null
      },
      {
        "comment_id": "2010898728",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1752,
        "pr_file": "tokenizers/src/models/wordlevel/mod.rs",
        "discussion_id": "2007409928",
        "commented_code": "@@ -61,8 +64,8 @@ impl WordLevelBuilder {\n \n     /// Set the vocab (token -> ID) mapping.\n     #[must_use]\n-    pub fn vocab(mut self, vocab: HashMap<String, u32>) -> Self {\n-        self.config.vocab = vocab;\n+    pub fn vocab<S: BuildHasher>(mut self, vocab: HashMap<String, u32, S>) -> Self {",
        "comment_created_at": "2025-03-24T20:43:32+00:00",
        "comment_author": "MeetThePatel",
        "comment_body": "No, `FxHashMap<K, V> := std::collections::HashMap<K, V, rustc_hash::FxBuildHasher>`, and `rustc_hash::FxBuildHasher` impls `BuildHasher`.\r\n\r\nThis way, the function accepts `rustc_hash::FxBuildHasher` as well as `std::hash::DefaultHasher` (which would be the default `std::collections::HashMap`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1392239704",
    "pr_number": 1357,
    "pr_file": "tokenizers/src/pre_tokenizers/metaspace.rs",
    "created_at": "2023-11-14T09:11:15+00:00",
    "commented_code": "self.replacement = replacement;\n         self.str_rep = replacement.to_string();\n     }\n+    pub fn set_prepend_scheme(&mut self, scheme: impl Into<PrependScheme>) {\n+        self.prepend_scheme = scheme.into();",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "1392239704",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1357,
        "pr_file": "tokenizers/src/pre_tokenizers/metaspace.rs",
        "discussion_id": "1392239704",
        "commented_code": "@@ -55,6 +120,9 @@ impl Metaspace {\n         self.replacement = replacement;\n         self.str_rep = replacement.to_string();\n     }\n+    pub fn set_prepend_scheme(&mut self, scheme: impl Into<PrependScheme>) {\n+        self.prepend_scheme = scheme.into();",
        "comment_created_at": "2023-11-14T09:11:15+00:00",
        "comment_author": "Narsil",
        "comment_body": "Just make that `PrependScheme`.\r\n\r\n`impl Into<T>` has an associated cost with it. Which is that there will be a concrete function for EVERY possible caller that might arise (here at least `PrependScheme`, `&str` and `String`. \r\n\r\nThis is nice to make an API easier to use when the concrete underlying type is relatively verbose to make, or there are many structs implementing the given trait that might be useful (something like `impl Response` in a web framework.\r\n\r\nHere everything is a simple `enum`. Importing the enum and using a variant makes the function  so that only 1 exists, prevents all kind of runtiem errors (all will become compile time errors). And since only 1 function exists, it can be inline more easily by the compiler.\r\n\r\nSimple is better here IMO",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "517935674",
    "pr_number": 506,
    "pr_file": "bindings/node/native/src/encoding.rs",
    "created_at": "2020-11-05T10:13:14+00:00",
    "commented_code": "}\n         }\n \n-        method tokenToChars(mut cx) {\n-            // tokenToChars(token: number): [number, number] | undefined\n+        method tokenToSequence(mut cx) {\n+            // tokenToSequence(token: number): number | undefined\n \n-            let token = cx.argument::<JsNumber>(0)?.value() as usize;\n+            let token = cx.extract::<usize>(0)?;\n \n             let this = cx.this();\n             let guard = cx.lock();\n \n             let res = this.borrow(&guard)\n                 .encoding.as_ref().expect(\"Uninitialized Encoding\")\n-                .token_to_chars(token);\n+                .token_to_sequence(token);\n \n-            if let Some(offsets) = res {\n-                Ok(neon_serde::to_value(&mut cx, &offsets)?)\n+            if let Some(seq) = res {\n+                Ok(neon_serde::to_value(&mut cx, &seq)?)\n+            } else {\n+                Ok(cx.undefined().upcast())\n+            }\n+        }\n+\n+        method tokenToChars(mut cx) {\n+            // tokenToChars(token: number): [number, number] [number, [number, number]] | undefined\n+\n+            let token = cx.extract::<usize>(0)?;\n+\n+            let this = cx.this();\n+            let guard = cx.lock();\n+\n+            let (res, n_seq) = {\n+                let borrowed = this.borrow(&guard);\n+                let encoding = borrowed.encoding.as_ref().expect(\"Uninitialized Encoding\");\n+\n+                let res = encoding.token_to_chars(token);\n+                let n_seq = encoding.n_sequences();\n+                (res, n_seq)\n+            };\n+\n+            if let Some((seq_id, offsets)) = res {\n+                if n_seq > 1 {\n+                    Ok(neon_serde::to_value(&mut cx, &(seq_id, offsets))?)\n+                } else {\n+                    Ok(neon_serde::to_value(&mut cx, &offsets)?)\n+                }\n             } else {\n                 Ok(cx.undefined().upcast())\n             }\n         }\n \n         method tokenToWord(mut cx) {\n-            // tokenToWord(token: number): number | undefined\n+            // tokenToWord(token: number): number | [number, number] | undefined\n \n             let token = cx.argument::<JsNumber>(0)?.value() as usize;\n \n             let this = cx.this();\n             let guard = cx.lock();\n-            let index = this.borrow(&guard)\n-                .encoding.as_ref().expect(\"Uninitialized Encoding\")\n-                .token_to_word(token);\n \n-            if let Some(index) = index {\n-                Ok(cx.number(index as f64).upcast())\n+            let (res, n_seq) = {\n+                let borrowed = this.borrow(&guard);\n+                let encoding = borrowed.encoding.as_ref().expect(\"Uninitialized Encoding\");\n+\n+                let res = encoding.token_to_word(token);\n+                let n_seq = encoding.n_sequences();\n+                (res, n_seq)\n+            };\n+\n+            if let Some((seq_id, index)) = res {\n+                if n_seq > 1 {\n+                    Ok(neon_serde::to_value(&mut cx, &(seq_id, index))?)",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "517935674",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 506,
        "pr_file": "bindings/node/native/src/encoding.rs",
        "discussion_id": "517935674",
        "commented_code": "@@ -182,53 +206,92 @@ declare_types! {\n             }\n         }\n \n-        method tokenToChars(mut cx) {\n-            // tokenToChars(token: number): [number, number] | undefined\n+        method tokenToSequence(mut cx) {\n+            // tokenToSequence(token: number): number | undefined\n \n-            let token = cx.argument::<JsNumber>(0)?.value() as usize;\n+            let token = cx.extract::<usize>(0)?;\n \n             let this = cx.this();\n             let guard = cx.lock();\n \n             let res = this.borrow(&guard)\n                 .encoding.as_ref().expect(\"Uninitialized Encoding\")\n-                .token_to_chars(token);\n+                .token_to_sequence(token);\n \n-            if let Some(offsets) = res {\n-                Ok(neon_serde::to_value(&mut cx, &offsets)?)\n+            if let Some(seq) = res {\n+                Ok(neon_serde::to_value(&mut cx, &seq)?)\n+            } else {\n+                Ok(cx.undefined().upcast())\n+            }\n+        }\n+\n+        method tokenToChars(mut cx) {\n+            // tokenToChars(token: number): [number, number] [number, [number, number]] | undefined\n+\n+            let token = cx.extract::<usize>(0)?;\n+\n+            let this = cx.this();\n+            let guard = cx.lock();\n+\n+            let (res, n_seq) = {\n+                let borrowed = this.borrow(&guard);\n+                let encoding = borrowed.encoding.as_ref().expect(\"Uninitialized Encoding\");\n+\n+                let res = encoding.token_to_chars(token);\n+                let n_seq = encoding.n_sequences();\n+                (res, n_seq)\n+            };\n+\n+            if let Some((seq_id, offsets)) = res {\n+                if n_seq > 1 {\n+                    Ok(neon_serde::to_value(&mut cx, &(seq_id, offsets))?)\n+                } else {\n+                    Ok(neon_serde::to_value(&mut cx, &offsets)?)\n+                }\n             } else {\n                 Ok(cx.undefined().upcast())\n             }\n         }\n \n         method tokenToWord(mut cx) {\n-            // tokenToWord(token: number): number | undefined\n+            // tokenToWord(token: number): number | [number, number] | undefined\n \n             let token = cx.argument::<JsNumber>(0)?.value() as usize;\n \n             let this = cx.this();\n             let guard = cx.lock();\n-            let index = this.borrow(&guard)\n-                .encoding.as_ref().expect(\"Uninitialized Encoding\")\n-                .token_to_word(token);\n \n-            if let Some(index) = index {\n-                Ok(cx.number(index as f64).upcast())\n+            let (res, n_seq) = {\n+                let borrowed = this.borrow(&guard);\n+                let encoding = borrowed.encoding.as_ref().expect(\"Uninitialized Encoding\");\n+\n+                let res = encoding.token_to_word(token);\n+                let n_seq = encoding.n_sequences();\n+                (res, n_seq)\n+            };\n+\n+            if let Some((seq_id, index)) = res {\n+                if n_seq > 1 {\n+                    Ok(neon_serde::to_value(&mut cx, &(seq_id, index))?)",
        "comment_created_at": "2020-11-05T10:13:14+00:00",
        "comment_author": "Narsil",
        "comment_body": "TBH variant return types always give me shivers. It's usually so much better to have 2 different functions instead. Saves so much headaches for users....\r\n\r\nIf we want to save backcompat:\r\n\r\n```python\r\ndef token_to_word(token) -> int:\r\n     if self.num_sequences() > 1:\r\n         raise Exception(\"Can't use this in multiple sequences encoding, use token_to_word_seq instead\")\r\n     word, seq = token_to_word_seq(token)\r\n     return word\r\n\r\ndef token_to_word_seq(token) -> Tuple[int, int]:\r\n    ....\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "518193888",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 506,
        "pr_file": "bindings/node/native/src/encoding.rs",
        "discussion_id": "517935674",
        "commented_code": "@@ -182,53 +206,92 @@ declare_types! {\n             }\n         }\n \n-        method tokenToChars(mut cx) {\n-            // tokenToChars(token: number): [number, number] | undefined\n+        method tokenToSequence(mut cx) {\n+            // tokenToSequence(token: number): number | undefined\n \n-            let token = cx.argument::<JsNumber>(0)?.value() as usize;\n+            let token = cx.extract::<usize>(0)?;\n \n             let this = cx.this();\n             let guard = cx.lock();\n \n             let res = this.borrow(&guard)\n                 .encoding.as_ref().expect(\"Uninitialized Encoding\")\n-                .token_to_chars(token);\n+                .token_to_sequence(token);\n \n-            if let Some(offsets) = res {\n-                Ok(neon_serde::to_value(&mut cx, &offsets)?)\n+            if let Some(seq) = res {\n+                Ok(neon_serde::to_value(&mut cx, &seq)?)\n+            } else {\n+                Ok(cx.undefined().upcast())\n+            }\n+        }\n+\n+        method tokenToChars(mut cx) {\n+            // tokenToChars(token: number): [number, number] [number, [number, number]] | undefined\n+\n+            let token = cx.extract::<usize>(0)?;\n+\n+            let this = cx.this();\n+            let guard = cx.lock();\n+\n+            let (res, n_seq) = {\n+                let borrowed = this.borrow(&guard);\n+                let encoding = borrowed.encoding.as_ref().expect(\"Uninitialized Encoding\");\n+\n+                let res = encoding.token_to_chars(token);\n+                let n_seq = encoding.n_sequences();\n+                (res, n_seq)\n+            };\n+\n+            if let Some((seq_id, offsets)) = res {\n+                if n_seq > 1 {\n+                    Ok(neon_serde::to_value(&mut cx, &(seq_id, offsets))?)\n+                } else {\n+                    Ok(neon_serde::to_value(&mut cx, &offsets)?)\n+                }\n             } else {\n                 Ok(cx.undefined().upcast())\n             }\n         }\n \n         method tokenToWord(mut cx) {\n-            // tokenToWord(token: number): number | undefined\n+            // tokenToWord(token: number): number | [number, number] | undefined\n \n             let token = cx.argument::<JsNumber>(0)?.value() as usize;\n \n             let this = cx.this();\n             let guard = cx.lock();\n-            let index = this.borrow(&guard)\n-                .encoding.as_ref().expect(\"Uninitialized Encoding\")\n-                .token_to_word(token);\n \n-            if let Some(index) = index {\n-                Ok(cx.number(index as f64).upcast())\n+            let (res, n_seq) = {\n+                let borrowed = this.borrow(&guard);\n+                let encoding = borrowed.encoding.as_ref().expect(\"Uninitialized Encoding\");\n+\n+                let res = encoding.token_to_word(token);\n+                let n_seq = encoding.n_sequences();\n+                (res, n_seq)\n+            };\n+\n+            if let Some((seq_id, index)) = res {\n+                if n_seq > 1 {\n+                    Ok(neon_serde::to_value(&mut cx, &(seq_id, index))?)",
        "comment_created_at": "2020-11-05T16:39:41+00:00",
        "comment_author": "n1t0",
        "comment_body": "As discussed, we'll remove the output with the sequence id, and will just keep the classic one. The user can call `token_to_sequence` when needed to determine it.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "362511611",
    "pr_number": 24,
    "pr_file": "tokenizers/src/models/bpe/model.rs",
    "created_at": "2020-01-02T15:24:20+00:00",
    "commented_code": "path::{Path, PathBuf},\n };\n \n+#[derive(Default)]\n+struct Config {\n+    vocab: Option<HashMap<String, u32>>,\n+    vocab_r: Option<HashMap<u32, String>>,\n+    merges: Option<HashMap<Pair, (u32, u32)>>,\n+    cache_capacity: Option<usize>,\n+    dropout: Option<f32>,\n+    unk_token: Option<u32>,\n+}\n+\n+/// A `BpeBuilder` can be used to create a `BPE` model with a custom configuration.\n+#[derive(Default)]\n+pub struct BpeBuilder {\n+    config: Config,\n+}\n+\n+impl BpeBuilder {\n+    /// Constructs a new `BpeBuilder`.\n+    pub fn new() -> Self {\n+        Self::default()\n+    }\n+\n+    /// Set the `token -> ID` vocab map.\n+    pub fn vocab(mut self, vocab: HashMap<String, u32>) -> Self {",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "362511611",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 24,
        "pr_file": "tokenizers/src/models/bpe/model.rs",
        "discussion_id": "362511611",
        "commented_code": "@@ -10,6 +10,99 @@ use std::{\n     path::{Path, PathBuf},\n };\n \n+#[derive(Default)]\n+struct Config {\n+    vocab: Option<HashMap<String, u32>>,\n+    vocab_r: Option<HashMap<u32, String>>,\n+    merges: Option<HashMap<Pair, (u32, u32)>>,\n+    cache_capacity: Option<usize>,\n+    dropout: Option<f32>,\n+    unk_token: Option<u32>,\n+}\n+\n+/// A `BpeBuilder` can be used to create a `BPE` model with a custom configuration.\n+#[derive(Default)]\n+pub struct BpeBuilder {\n+    config: Config,\n+}\n+\n+impl BpeBuilder {\n+    /// Constructs a new `BpeBuilder`.\n+    pub fn new() -> Self {\n+        Self::default()\n+    }\n+\n+    /// Set the `token -> ID` vocab map.\n+    pub fn vocab(mut self, vocab: HashMap<String, u32>) -> Self {",
        "comment_created_at": "2020-01-02T15:24:20+00:00",
        "comment_author": "n1t0",
        "comment_body": "I'm wondering if it makes sense to expose `vocab`, `vocab_r`, and `merges` separately. I think we should always provide either vocab/merges or nothing. What do you think?",
        "pr_file_module": null
      },
      {
        "comment_id": "362515773",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 24,
        "pr_file": "tokenizers/src/models/bpe/model.rs",
        "discussion_id": "362511611",
        "commented_code": "@@ -10,6 +10,99 @@ use std::{\n     path::{Path, PathBuf},\n };\n \n+#[derive(Default)]\n+struct Config {\n+    vocab: Option<HashMap<String, u32>>,\n+    vocab_r: Option<HashMap<u32, String>>,\n+    merges: Option<HashMap<Pair, (u32, u32)>>,\n+    cache_capacity: Option<usize>,\n+    dropout: Option<f32>,\n+    unk_token: Option<u32>,\n+}\n+\n+/// A `BpeBuilder` can be used to create a `BPE` model with a custom configuration.\n+#[derive(Default)]\n+pub struct BpeBuilder {\n+    config: Config,\n+}\n+\n+impl BpeBuilder {\n+    /// Constructs a new `BpeBuilder`.\n+    pub fn new() -> Self {\n+        Self::default()\n+    }\n+\n+    /// Set the `token -> ID` vocab map.\n+    pub fn vocab(mut self, vocab: HashMap<String, u32>) -> Self {",
        "comment_created_at": "2020-01-02T15:35:31+00:00",
        "comment_author": "n1t0",
        "comment_body": "(With the `vocab_r` being built from `vocab` anyway)",
        "pr_file_module": null
      },
      {
        "comment_id": "362526511",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 24,
        "pr_file": "tokenizers/src/models/bpe/model.rs",
        "discussion_id": "362511611",
        "commented_code": "@@ -10,6 +10,99 @@ use std::{\n     path::{Path, PathBuf},\n };\n \n+#[derive(Default)]\n+struct Config {\n+    vocab: Option<HashMap<String, u32>>,\n+    vocab_r: Option<HashMap<u32, String>>,\n+    merges: Option<HashMap<Pair, (u32, u32)>>,\n+    cache_capacity: Option<usize>,\n+    dropout: Option<f32>,\n+    unk_token: Option<u32>,\n+}\n+\n+/// A `BpeBuilder` can be used to create a `BPE` model with a custom configuration.\n+#[derive(Default)]\n+pub struct BpeBuilder {\n+    config: Config,\n+}\n+\n+impl BpeBuilder {\n+    /// Constructs a new `BpeBuilder`.\n+    pub fn new() -> Self {\n+        Self::default()\n+    }\n+\n+    /// Set the `token -> ID` vocab map.\n+    pub fn vocab(mut self, vocab: HashMap<String, u32>) -> Self {",
        "comment_created_at": "2020-01-02T16:03:05+00:00",
        "comment_author": "epwalsh",
        "comment_body": "That's a good point",
        "pr_file_module": null
      },
      {
        "comment_id": "362528259",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 24,
        "pr_file": "tokenizers/src/models/bpe/model.rs",
        "discussion_id": "362511611",
        "commented_code": "@@ -10,6 +10,99 @@ use std::{\n     path::{Path, PathBuf},\n };\n \n+#[derive(Default)]\n+struct Config {\n+    vocab: Option<HashMap<String, u32>>,\n+    vocab_r: Option<HashMap<u32, String>>,\n+    merges: Option<HashMap<Pair, (u32, u32)>>,\n+    cache_capacity: Option<usize>,\n+    dropout: Option<f32>,\n+    unk_token: Option<u32>,\n+}\n+\n+/// A `BpeBuilder` can be used to create a `BPE` model with a custom configuration.\n+#[derive(Default)]\n+pub struct BpeBuilder {\n+    config: Config,\n+}\n+\n+impl BpeBuilder {\n+    /// Constructs a new `BpeBuilder`.\n+    pub fn new() -> Self {\n+        Self::default()\n+    }\n+\n+    /// Set the `token -> ID` vocab map.\n+    pub fn vocab(mut self, vocab: HashMap<String, u32>) -> Self {",
        "comment_created_at": "2020-01-02T16:07:42+00:00",
        "comment_author": "epwalsh",
        "comment_body": "So I removed the `vocab`, `vocab_r`, and `merges` methods and replaced them with just a `vocab_and_merges` method. How does that sound?",
        "pr_file_module": null
      },
      {
        "comment_id": "362535709",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 24,
        "pr_file": "tokenizers/src/models/bpe/model.rs",
        "discussion_id": "362511611",
        "commented_code": "@@ -10,6 +10,99 @@ use std::{\n     path::{Path, PathBuf},\n };\n \n+#[derive(Default)]\n+struct Config {\n+    vocab: Option<HashMap<String, u32>>,\n+    vocab_r: Option<HashMap<u32, String>>,\n+    merges: Option<HashMap<Pair, (u32, u32)>>,\n+    cache_capacity: Option<usize>,\n+    dropout: Option<f32>,\n+    unk_token: Option<u32>,\n+}\n+\n+/// A `BpeBuilder` can be used to create a `BPE` model with a custom configuration.\n+#[derive(Default)]\n+pub struct BpeBuilder {\n+    config: Config,\n+}\n+\n+impl BpeBuilder {\n+    /// Constructs a new `BpeBuilder`.\n+    pub fn new() -> Self {\n+        Self::default()\n+    }\n+\n+    /// Set the `token -> ID` vocab map.\n+    pub fn vocab(mut self, vocab: HashMap<String, u32>) -> Self {",
        "comment_created_at": "2020-01-02T16:27:26+00:00",
        "comment_author": "n1t0",
        "comment_body": ":+1: ",
        "pr_file_module": null
      }
    ]
  }
]