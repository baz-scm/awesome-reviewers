[
  {
    "discussion_id": "1195555743",
    "pr_number": 1246,
    "pr_file": "bindings/node/examples/documentation/quicktour.test.ts",
    "created_at": "2023-05-16T18:37:19+00:00",
    "commented_code": "// END print_type_ids\n         expect(output.getTypeIds()).toEqual([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]);\n         // START encode_batch\n-        let encodeBatch = promisify(tokenizer.encodeBatch.bind(tokenizer));\n \n-        var output = await encodeBatch([\"Hello, y'all!\", \"How are you \ud83d\ude01 ?\"]);\n+        var output = await tokenizer.encodeBatch([\"Hello, y'all!\", \"How are you \ud83d\ude01 ?\"]);\n         // END encode_batch\n         // START encode_batch_pair\n-        var output = await encodeBatch(\n-            [[\"Hello, y'all!\", \"How are you \ud83d\ude01 ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n-        );\n+        // var output = await tokenizer.encodeBatch(\n+        //     [[\"Hello, y'all!\", \"How are you \ud83d\ude01 ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n+        // );",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "1195555743",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1246,
        "pr_file": "bindings/node/examples/documentation/quicktour.test.ts",
        "discussion_id": "1195555743",
        "commented_code": "@@ -155,20 +149,19 @@ describe(\"quicktourExample\", () => {\n         // END print_type_ids\n         expect(output.getTypeIds()).toEqual([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]);\n         // START encode_batch\n-        let encodeBatch = promisify(tokenizer.encodeBatch.bind(tokenizer));\n \n-        var output = await encodeBatch([\"Hello, y'all!\", \"How are you \ud83d\ude01 ?\"]);\n+        var output = await tokenizer.encodeBatch([\"Hello, y'all!\", \"How are you \ud83d\ude01 ?\"]);\n         // END encode_batch\n         // START encode_batch_pair\n-        var output = await encodeBatch(\n-            [[\"Hello, y'all!\", \"How are you \ud83d\ude01 ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n-        );\n+        // var output = await tokenizer.encodeBatch(\n+        //     [[\"Hello, y'all!\", \"How are you \ud83d\ude01 ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n+        // );",
        "comment_created_at": "2023-05-16T18:37:19+00:00",
        "comment_author": "McPatate",
        "comment_body": "Should this be commented?",
        "pr_file_module": null
      },
      {
        "comment_id": "1196603347",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1246,
        "pr_file": "bindings/node/examples/documentation/quicktour.test.ts",
        "discussion_id": "1195555743",
        "commented_code": "@@ -155,20 +149,19 @@ describe(\"quicktourExample\", () => {\n         // END print_type_ids\n         expect(output.getTypeIds()).toEqual([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]);\n         // START encode_batch\n-        let encodeBatch = promisify(tokenizer.encodeBatch.bind(tokenizer));\n \n-        var output = await encodeBatch([\"Hello, y'all!\", \"How are you \ud83d\ude01 ?\"]);\n+        var output = await tokenizer.encodeBatch([\"Hello, y'all!\", \"How are you \ud83d\ude01 ?\"]);\n         // END encode_batch\n         // START encode_batch_pair\n-        var output = await encodeBatch(\n-            [[\"Hello, y'all!\", \"How are you \ud83d\ude01 ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n-        );\n+        // var output = await tokenizer.encodeBatch(\n+        //     [[\"Hello, y'all!\", \"How are you \ud83d\ude01 ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n+        // );",
        "comment_created_at": "2023-05-17T14:22:13+00:00",
        "comment_author": "Narsil",
        "comment_body": "Yes I removed 1 feature:\r\n\r\nPreTokenized inputs. Which are presplitted strings (so list of strings).\r\n\r\nWhy did I remove it :\r\n\r\n- It bloats quite a lot the code (it would require rewriting the argument parsing since it doesn't work by default with the macro, or at least I wasn't able to).\r\n- It's a rarely used feature which have lots of caveats.\r\n- We can readd later, right now, running on more recent versions in reasonable time was the goal.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "520789198",
    "pr_number": 518,
    "pr_file": "bindings/node/lib/bindings/tokenizer.test.ts",
    "created_at": "2020-11-10T18:45:37+00:00",
    "commented_code": "const model = BPE.empty();\n       tokenizer = new Tokenizer(model);\n+      tokenizer.setPreTokenizer(whitespacePreTokenizer());",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "520789198",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 518,
        "pr_file": "bindings/node/lib/bindings/tokenizer.test.ts",
        "discussion_id": "520789198",
        "commented_code": "@@ -131,6 +132,7 @@ describe(\"Tokenizer\", () => {\n \n       const model = BPE.empty();\n       tokenizer = new Tokenizer(model);\n+      tokenizer.setPreTokenizer(whitespacePreTokenizer());",
        "comment_created_at": "2020-11-10T18:45:37+00:00",
        "comment_author": "n1t0",
        "comment_body": "I don't understand why you want to add these in all the tests. The pre-tokenizer won't get called anyway because all the tokens in the tests come are handled by the `AddedVocabulary`.",
        "pr_file_module": null
      },
      {
        "comment_id": "521220223",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 518,
        "pr_file": "bindings/node/lib/bindings/tokenizer.test.ts",
        "discussion_id": "520789198",
        "commented_code": "@@ -131,6 +132,7 @@ describe(\"Tokenizer\", () => {\n \n       const model = BPE.empty();\n       tokenizer = new Tokenizer(model);\n+      tokenizer.setPreTokenizer(whitespacePreTokenizer());",
        "comment_created_at": "2020-11-11T09:17:45+00:00",
        "comment_author": "Narsil",
        "comment_body": "That's not True, at least it was not in my case, all the tests were failing because the test string was not splitted through whitespace, so the added vocabulary was not handled, and those tests were failing because UNK_TOKEN was not defined.\r\n\r\nWe probably should really test the output of those methods in the full to make sure it's consistent, I'm pretty sure all those tests would have caught the missing unk tokens otherwise.",
        "pr_file_module": null
      },
      {
        "comment_id": "521385235",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 518,
        "pr_file": "bindings/node/lib/bindings/tokenizer.test.ts",
        "discussion_id": "520789198",
        "commented_code": "@@ -131,6 +132,7 @@ describe(\"Tokenizer\", () => {\n \n       const model = BPE.empty();\n       tokenizer = new Tokenizer(model);\n+      tokenizer.setPreTokenizer(whitespacePreTokenizer());",
        "comment_created_at": "2020-11-11T14:13:06+00:00",
        "comment_author": "n1t0",
        "comment_body": "The added vocabulary is always handled since it happens first, so no need for a pre-tokenizer. The tests actually started to fail because the BPE is receiving the whitespace between each word, and it doesn't have an `unk_token`. With the `Whitespace` pre-tokenizer, these whitespaces are just removed.\r\n\r\n> We probably should really test the output of those methods in the full to make sure it's consistent, I'm pretty sure all those tests would have caught the missing unk tokens otherwise.\r\n\r\nThe unk token was treated this way by design. When provided we use it, otherwise the unknown tokens just get ignored. This is not a bug we didn't catch. That being said, feel free to add all the tests you find necessary, we never have enough of them :slightly_smiling_face: ",
        "pr_file_module": null
      }
    ]
  }
]