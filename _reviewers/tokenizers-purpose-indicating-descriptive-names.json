[
  {
    "discussion_id": "488026169",
    "pr_number": 401,
    "pr_file": "bindings/python/py_src/tokenizers/implementations/sentencepiece_unigram.py",
    "created_at": "2020-09-14T15:30:37+00:00",
    "commented_code": "\"\"\"\n \n     def __init__(\n-        self, vocab: Optional[str] = None, replacement: str = \"▁\", add_prefix_space: bool = True,\n+        self,\n+        vocab: Optional[List[Tuple[str, float]]] = None,",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "488026169",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 401,
        "pr_file": "bindings/python/py_src/tokenizers/implementations/sentencepiece_unigram.py",
        "discussion_id": "488026169",
        "commented_code": "@@ -13,14 +19,19 @@ class SentencePieceUnigramTokenizer(BaseTokenizer):\n     \"\"\"\n \n     def __init__(\n-        self, vocab: Optional[str] = None, replacement: str = \"▁\", add_prefix_space: bool = True,\n+        self,\n+        vocab: Optional[List[Tuple[str, float]]] = None,",
        "comment_created_at": "2020-09-14T15:30:37+00:00",
        "comment_author": "n1t0",
        "comment_body": "For consistency with current `implementations`, the constructor should probably expect file by default, and provide another way to build with in-memory data.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "503976338",
    "pr_number": 459,
    "pr_file": "bindings/python/tests/utils.py",
    "created_at": "2020-10-13T14:00:06+00:00",
    "commented_code": "}\n \n \n+@pytest.fixture(scope=\"session\")\n+def precompiled_files(data_dir):",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "503976338",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 459,
        "pr_file": "bindings/python/tests/utils.py",
        "discussion_id": "503976338",
        "commented_code": "@@ -75,6 +75,16 @@ def train_files(data_dir):\n     }\n \n \n+@pytest.fixture(scope=\"session\")\n+def precompiled_files(data_dir):",
        "comment_created_at": "2020-10-13T14:00:06+00:00",
        "comment_author": "n1t0",
        "comment_body": "What about `albert_tokenizer` instead of `precompiled_files` which seems off? Also, we can probably return `albert_base` directly since its a single file in this case.",
        "pr_file_module": null
      },
      {
        "comment_id": "504044808",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 459,
        "pr_file": "bindings/python/tests/utils.py",
        "discussion_id": "503976338",
        "commented_code": "@@ -75,6 +75,16 @@ def train_files(data_dir):\n     }\n \n \n+@pytest.fixture(scope=\"session\")\n+def precompiled_files(data_dir):",
        "comment_created_at": "2020-10-13T15:26:13+00:00",
        "comment_author": "Narsil",
        "comment_body": "I'd like to keep the dictionary format, at least for uniformity within the tests fixtures that all use dictionaries.\r\n\r\nIn terms of naming, because we might add follow ups saved tokenizers, what about `serialized_files` it makes `serialized_files[\"albert_base\"]` more understandable. I'd rather add another tokenizer in the tests than naming specifically for albert IMO. (We probably should have a handful of serialized tokenizers in those tests to make sure will load the old ones, with BertNormalizer, BPE, and so on). What do you think ?",
        "pr_file_module": null
      },
      {
        "comment_id": "504075532",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 459,
        "pr_file": "bindings/python/tests/utils.py",
        "discussion_id": "503976338",
        "commented_code": "@@ -75,6 +75,16 @@ def train_files(data_dir):\n     }\n \n \n+@pytest.fixture(scope=\"session\")\n+def precompiled_files(data_dir):",
        "comment_created_at": "2020-10-13T16:05:30+00:00",
        "comment_author": "n1t0",
        "comment_body": "Yes I agree that we'll probably add more, but it might be better to keep each set of files separated though. When you require `albert-base` you don't necessarily want to download all the serialized files, and having logically separated fixtures let us do that.\r\nI don't mind keeping a dict, but I felt that with serialized tokenizers it would just add repetition (as opposed to the vocab/merges used for the others).",
        "pr_file_module": null
      },
      {
        "comment_id": "504078913",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 459,
        "pr_file": "bindings/python/tests/utils.py",
        "discussion_id": "503976338",
        "commented_code": "@@ -75,6 +75,16 @@ def train_files(data_dir):\n     }\n \n \n+@pytest.fixture(scope=\"session\")\n+def precompiled_files(data_dir):",
        "comment_created_at": "2020-10-13T16:10:30+00:00",
        "comment_author": "Narsil",
        "comment_body": "Good argument for downloading separately, and that changes the names, which makes your solution the best in the end.\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "364159442",
    "pr_number": 43,
    "pr_file": "bindings/python/tokenizers/implementations/base_tokenizer.py",
    "created_at": "2020-01-08T10:22:59+00:00",
    "commented_code": "+from ..tokenizers import Tokenizer, Encoding\n+\n+from typing import List, Union, Tuple, Optional\n+\n+class BaseTokenizer:\n+    _tokenizer: Tokenizer\n+\n+    def __init__(self, tokenizer: Tokenizer):\n+        self._tokenizer = tokenizer\n+\n+    def with_padding(self,\n+                     direction: Optional[str] = \"right\",\n+                     pad_id: Optional[int] = 0,\n+                     pad_type_id: Optional[int] = 0,\n+                     pad_token: Optional[str] = \"[PAD]\",\n+                     max_length: Optional[int] = None):\n+        \"\"\" Change the padding strategy\n+\n+        Args:\n+            direction: (`optional`) str:\n+                Can be one of: `right` or `left`\n+\n+            pad_id: (`optional`) unsigned int:\n+                The indice to be used when padding\n+\n+            pad_type_id: (`optional`) unsigned int:\n+                The type indice to be used when padding\n+\n+            pad_token: (`optional`) str:\n+                The pad token to be used when padding\n+\n+            max_length: (`optional`) unsigned int:\n+                If specified, the length at which to pad. If not specified\n+                we pad using the size of the longest sequence in a batch\n+        \"\"\"\n+        return self._tokenizer.with_padding(direction=direction,\n+                                            pad_id=pad_id,\n+                                            pad_type_id=pad_type_id,\n+                                            pad_token=pad_token,\n+                                            max_length=max_length)\n+\n+    def without_padding(self):\n+        \"\"\" Disable padding \"\"\"\n+        return self._tokenizer.without_padding()\n+\n+    def with_truncation(self,",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "364159442",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 43,
        "pr_file": "bindings/python/tokenizers/implementations/base_tokenizer.py",
        "discussion_id": "364159442",
        "commented_code": "@@ -0,0 +1,185 @@\n+from ..tokenizers import Tokenizer, Encoding\n+\n+from typing import List, Union, Tuple, Optional\n+\n+class BaseTokenizer:\n+    _tokenizer: Tokenizer\n+\n+    def __init__(self, tokenizer: Tokenizer):\n+        self._tokenizer = tokenizer\n+\n+    def with_padding(self,\n+                     direction: Optional[str] = \"right\",\n+                     pad_id: Optional[int] = 0,\n+                     pad_type_id: Optional[int] = 0,\n+                     pad_token: Optional[str] = \"[PAD]\",\n+                     max_length: Optional[int] = None):\n+        \"\"\" Change the padding strategy\n+\n+        Args:\n+            direction: (`optional`) str:\n+                Can be one of: `right` or `left`\n+\n+            pad_id: (`optional`) unsigned int:\n+                The indice to be used when padding\n+\n+            pad_type_id: (`optional`) unsigned int:\n+                The type indice to be used when padding\n+\n+            pad_token: (`optional`) str:\n+                The pad token to be used when padding\n+\n+            max_length: (`optional`) unsigned int:\n+                If specified, the length at which to pad. If not specified\n+                we pad using the size of the longest sequence in a batch\n+        \"\"\"\n+        return self._tokenizer.with_padding(direction=direction,\n+                                            pad_id=pad_id,\n+                                            pad_type_id=pad_type_id,\n+                                            pad_token=pad_token,\n+                                            max_length=max_length)\n+\n+    def without_padding(self):\n+        \"\"\" Disable padding \"\"\"\n+        return self._tokenizer.without_padding()\n+\n+    def with_truncation(self,",
        "comment_created_at": "2020-01-08T10:22:59+00:00",
        "comment_author": "thomwolf",
        "comment_body": "I feel like the `with_xxx` and `without_xxx` terms are a bit hard to understand because they are actually meant to set or unset the state of the tokenizer if I understand it well.\r\n\r\nMaybe we can choose `enable_xxx` and `no_xxx` (or `disable_xxx`) (like in pytorch https://pytorch.org/docs/stable/autograd.html#torch.autograd.enable_grad)",
        "pr_file_module": null
      }
    ]
  }
]