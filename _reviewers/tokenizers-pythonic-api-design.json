[
  {
    "discussion_id": "521231803",
    "pr_number": 519,
    "pr_file": "bindings/python/py_src/tokenizers/implementations/bert_wordpiece.py",
    "created_at": "2020-11-11T09:37:16+00:00",
    "commented_code": ")\n         if isinstance(files, str):\n             files = [files]\n-        self._tokenizer.train(trainer, files)\n+        self._tokenizer.train(files, trainer=trainer)",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "521231803",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 519,
        "pr_file": "bindings/python/py_src/tokenizers/implementations/bert_wordpiece.py",
        "discussion_id": "521231803",
        "commented_code": "@@ -115,4 +115,4 @@ def train(\n         )\n         if isinstance(files, str):\n             files = [files]\n-        self._tokenizer.train(trainer, files)\n+        self._tokenizer.train(files, trainer=trainer)",
        "comment_created_at": "2020-11-11T09:37:16+00:00",
        "comment_author": "Narsil",
        "comment_body": "As long as we're breaking signature, I would argue we have a different signature like\r\n\r\n`train(files, options=bpe_train_options)`, or `train(files, vocab_size=X, ....)` what do you think ?\r\n\r\nI like the second version better, the only trouble is the exact description of those options if going to get fuzzy pretty fast and error handling a bit hard. But it \"feels\" more pythonic, what do you think ?\r\n\r\nEither that, or if we keep the `trainer` concept, we should stick to something closer to Rust, with `trainer.train(tokenizer, files)` I actually like that last version better at this moment, the control flow feels more natural.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "492982051",
    "pr_number": 430,
    "pr_file": "bindings/python/py_src/tokenizers/implementations/char_level_bpe.py",
    "created_at": "2020-09-22T19:29:06+00:00",
    "commented_code": "from .. import Tokenizer, AddedToken, pre_tokenizers, decoders, trainers\n from ..models import BPE\n-from ..normalizers import Sequence, Lowercase, unicode_normalizer_from_str, BertNormalizer\n+from ..normalizers import (\n+    Sequence,\n+    Lowercase,\n+    unicode_normalizer_from_str,\n+    BertNormalizer,\n+)\n from .base_tokenizer import BaseTokenizer\n \n-from typing import Optional, List, Union\n+from typing import Optional, List, Union, Dict, Tuple\n \n \n class CharBPETokenizer(BaseTokenizer):\n-    \"\"\" Original BPE Tokenizer\n-\n-        Represents the BPE algorithm, as introduced by Rico Sennrich\n-        (https://arxiv.org/abs/1508.07909)\n-\n-        The defaults settings corresponds to OpenAI GPT BPE tokenizers and differs from the original\n-        Sennrich subword-nmt implementation by the following options that you can deactivate:\n-            - adding a normalizer to clean up the text (deactivate with `bert_normalizer=False`) by:\n-                * removing any control characters and replacing all whitespaces by the classic one.\n-                * handle chinese chars by putting spaces around them.\n-                * strip all accents.\n-            - spitting on punctuation in addition to whitespaces (deactivate it with\n-              `split_on_whitespace_only=True`)\n+    \"\"\"Original BPE Tokenizer\n+\n+    Represents the BPE algorithm, as introduced by Rico Sennrich\n+    (https://arxiv.org/abs/1508.07909)\n+\n+    The defaults settings corresponds to OpenAI GPT BPE tokenizers and differs from the original\n+    Sennrich subword-nmt implementation by the following options that you can deactivate:\n+        - adding a normalizer to clean up the text (deactivate with `bert_normalizer=False`) by:\n+            * removing any control characters and replacing all whitespaces by the classic one.\n+            * handle chinese chars by putting spaces around them.\n+            * strip all accents.\n+        - spitting on punctuation in addition to whitespaces (deactivate it with\n+          `split_on_whitespace_only=True`)\n     \"\"\"\n \n     def __init__(\n         self,\n-        vocab_file: Optional[str] = None,\n-        merges_file: Optional[str] = None,\n+        vocab: Optional[Union[str, Dict[str, int]]] = None,\n+        merges: Optional[Union[str, Dict[Tuple[int, int], Tuple[int, int]]]] = None,",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "492982051",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 430,
        "pr_file": "bindings/python/py_src/tokenizers/implementations/char_level_bpe.py",
        "discussion_id": "492982051",
        "commented_code": "@@ -1,31 +1,36 @@\n from .. import Tokenizer, AddedToken, pre_tokenizers, decoders, trainers\n from ..models import BPE\n-from ..normalizers import Sequence, Lowercase, unicode_normalizer_from_str, BertNormalizer\n+from ..normalizers import (\n+    Sequence,\n+    Lowercase,\n+    unicode_normalizer_from_str,\n+    BertNormalizer,\n+)\n from .base_tokenizer import BaseTokenizer\n \n-from typing import Optional, List, Union\n+from typing import Optional, List, Union, Dict, Tuple\n \n \n class CharBPETokenizer(BaseTokenizer):\n-    \"\"\" Original BPE Tokenizer\n-\n-        Represents the BPE algorithm, as introduced by Rico Sennrich\n-        (https://arxiv.org/abs/1508.07909)\n-\n-        The defaults settings corresponds to OpenAI GPT BPE tokenizers and differs from the original\n-        Sennrich subword-nmt implementation by the following options that you can deactivate:\n-            - adding a normalizer to clean up the text (deactivate with `bert_normalizer=False`) by:\n-                * removing any control characters and replacing all whitespaces by the classic one.\n-                * handle chinese chars by putting spaces around them.\n-                * strip all accents.\n-            - spitting on punctuation in addition to whitespaces (deactivate it with\n-              `split_on_whitespace_only=True`)\n+    \"\"\"Original BPE Tokenizer\n+\n+    Represents the BPE algorithm, as introduced by Rico Sennrich\n+    (https://arxiv.org/abs/1508.07909)\n+\n+    The defaults settings corresponds to OpenAI GPT BPE tokenizers and differs from the original\n+    Sennrich subword-nmt implementation by the following options that you can deactivate:\n+        - adding a normalizer to clean up the text (deactivate with `bert_normalizer=False`) by:\n+            * removing any control characters and replacing all whitespaces by the classic one.\n+            * handle chinese chars by putting spaces around them.\n+            * strip all accents.\n+        - spitting on punctuation in addition to whitespaces (deactivate it with\n+          `split_on_whitespace_only=True`)\n     \"\"\"\n \n     def __init__(\n         self,\n-        vocab_file: Optional[str] = None,\n-        merges_file: Optional[str] = None,\n+        vocab: Optional[Union[str, Dict[str, int]]] = None,\n+        merges: Optional[Union[str, Dict[Tuple[int, int], Tuple[int, int]]]] = None,",
        "comment_created_at": "2020-09-22T19:29:06+00:00",
        "comment_author": "n1t0",
        "comment_body": "The format for merges here seems overly complicated. I agree that we use this specific format internally, but it is probably unnecessary to expose this to the final user. `List[Tuple[str, str]]` is easier to construct from all the different files format that exist out there, and also let us construct the final `merges` easily. It is also probably compatible with a lot more languages.",
        "pr_file_module": null
      },
      {
        "comment_id": "493323787",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 430,
        "pr_file": "bindings/python/py_src/tokenizers/implementations/char_level_bpe.py",
        "discussion_id": "492982051",
        "commented_code": "@@ -1,31 +1,36 @@\n from .. import Tokenizer, AddedToken, pre_tokenizers, decoders, trainers\n from ..models import BPE\n-from ..normalizers import Sequence, Lowercase, unicode_normalizer_from_str, BertNormalizer\n+from ..normalizers import (\n+    Sequence,\n+    Lowercase,\n+    unicode_normalizer_from_str,\n+    BertNormalizer,\n+)\n from .base_tokenizer import BaseTokenizer\n \n-from typing import Optional, List, Union\n+from typing import Optional, List, Union, Dict, Tuple\n \n \n class CharBPETokenizer(BaseTokenizer):\n-    \"\"\" Original BPE Tokenizer\n-\n-        Represents the BPE algorithm, as introduced by Rico Sennrich\n-        (https://arxiv.org/abs/1508.07909)\n-\n-        The defaults settings corresponds to OpenAI GPT BPE tokenizers and differs from the original\n-        Sennrich subword-nmt implementation by the following options that you can deactivate:\n-            - adding a normalizer to clean up the text (deactivate with `bert_normalizer=False`) by:\n-                * removing any control characters and replacing all whitespaces by the classic one.\n-                * handle chinese chars by putting spaces around them.\n-                * strip all accents.\n-            - spitting on punctuation in addition to whitespaces (deactivate it with\n-              `split_on_whitespace_only=True`)\n+    \"\"\"Original BPE Tokenizer\n+\n+    Represents the BPE algorithm, as introduced by Rico Sennrich\n+    (https://arxiv.org/abs/1508.07909)\n+\n+    The defaults settings corresponds to OpenAI GPT BPE tokenizers and differs from the original\n+    Sennrich subword-nmt implementation by the following options that you can deactivate:\n+        - adding a normalizer to clean up the text (deactivate with `bert_normalizer=False`) by:\n+            * removing any control characters and replacing all whitespaces by the classic one.\n+            * handle chinese chars by putting spaces around them.\n+            * strip all accents.\n+        - spitting on punctuation in addition to whitespaces (deactivate it with\n+          `split_on_whitespace_only=True`)\n     \"\"\"\n \n     def __init__(\n         self,\n-        vocab_file: Optional[str] = None,\n-        merges_file: Optional[str] = None,\n+        vocab: Optional[Union[str, Dict[str, int]]] = None,\n+        merges: Optional[Union[str, Dict[Tuple[int, int], Tuple[int, int]]]] = None,",
        "comment_created_at": "2020-09-23T08:45:17+00:00",
        "comment_author": "Narsil",
        "comment_body": "I'm merely exposing the rust api which requires this format.\r\nIt's also what `BPE::read_files()` returns.\r\n\r\nFollowing what you said we should mostly expose the rust api, no ?\r\nShould I change the rust API ? (I don't think we should)",
        "pr_file_module": null
      },
      {
        "comment_id": "493685507",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 430,
        "pr_file": "bindings/python/py_src/tokenizers/implementations/char_level_bpe.py",
        "discussion_id": "492982051",
        "commented_code": "@@ -1,31 +1,36 @@\n from .. import Tokenizer, AddedToken, pre_tokenizers, decoders, trainers\n from ..models import BPE\n-from ..normalizers import Sequence, Lowercase, unicode_normalizer_from_str, BertNormalizer\n+from ..normalizers import (\n+    Sequence,\n+    Lowercase,\n+    unicode_normalizer_from_str,\n+    BertNormalizer,\n+)\n from .base_tokenizer import BaseTokenizer\n \n-from typing import Optional, List, Union\n+from typing import Optional, List, Union, Dict, Tuple\n \n \n class CharBPETokenizer(BaseTokenizer):\n-    \"\"\" Original BPE Tokenizer\n-\n-        Represents the BPE algorithm, as introduced by Rico Sennrich\n-        (https://arxiv.org/abs/1508.07909)\n-\n-        The defaults settings corresponds to OpenAI GPT BPE tokenizers and differs from the original\n-        Sennrich subword-nmt implementation by the following options that you can deactivate:\n-            - adding a normalizer to clean up the text (deactivate with `bert_normalizer=False`) by:\n-                * removing any control characters and replacing all whitespaces by the classic one.\n-                * handle chinese chars by putting spaces around them.\n-                * strip all accents.\n-            - spitting on punctuation in addition to whitespaces (deactivate it with\n-              `split_on_whitespace_only=True`)\n+    \"\"\"Original BPE Tokenizer\n+\n+    Represents the BPE algorithm, as introduced by Rico Sennrich\n+    (https://arxiv.org/abs/1508.07909)\n+\n+    The defaults settings corresponds to OpenAI GPT BPE tokenizers and differs from the original\n+    Sennrich subword-nmt implementation by the following options that you can deactivate:\n+        - adding a normalizer to clean up the text (deactivate with `bert_normalizer=False`) by:\n+            * removing any control characters and replacing all whitespaces by the classic one.\n+            * handle chinese chars by putting spaces around them.\n+            * strip all accents.\n+        - spitting on punctuation in addition to whitespaces (deactivate it with\n+          `split_on_whitespace_only=True`)\n     \"\"\"\n \n     def __init__(\n         self,\n-        vocab_file: Optional[str] = None,\n-        merges_file: Optional[str] = None,\n+        vocab: Optional[Union[str, Dict[str, int]]] = None,\n+        merges: Optional[Union[str, Dict[Tuple[int, int], Tuple[int, int]]]] = None,",
        "comment_created_at": "2020-09-23T15:27:03+00:00",
        "comment_author": "n1t0",
        "comment_body": "Sure I understand. The API on the Rust side has been thought with `from_file` as the main entry point though, so this aspect has to be taken care of now that we want to expose this.\r\nWe should change the Rust API accordingly, and let the BPE build the `HashMap<Pair, (u32, u32)>` by itself.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "372621411",
    "pr_number": 99,
    "pr_file": "bindings/python/tokenizers/implementations/base_tokenizer.py",
    "created_at": "2020-01-29T20:47:46+00:00",
    "commented_code": "self._tokenizer.get_vocab_size(),\n             ', '.join(k + '=' + str(v) for k, v in self._parameters.items()))\n \n+    def get_vocab_size(self, with_added_tokens: bool = True):\n+        \"\"\" Return the size of vocabulary, with or without added tokens.\n+\n+        Args:\n+            with_added_tokens: (`optional`) bool:\n+                Whether to count in added special tokens or not\n+\n+        Returns:\n+            Size of vocabulary\n+        \"\"\"\n+        return self._tokenizer.get_vocab_size(with_added_tokens)",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "372621411",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 99,
        "pr_file": "bindings/python/tokenizers/implementations/base_tokenizer.py",
        "discussion_id": "372621411",
        "commented_code": "@@ -13,6 +13,18 @@ def __repr__(self):\n             self._tokenizer.get_vocab_size(),\n             ', '.join(k + '=' + str(v) for k, v in self._parameters.items()))\n \n+    def get_vocab_size(self, with_added_tokens: bool = True):\n+        \"\"\" Return the size of vocabulary, with or without added tokens.\n+\n+        Args:\n+            with_added_tokens: (`optional`) bool:\n+                Whether to count in added special tokens or not\n+\n+        Returns:\n+            Size of vocabulary\n+        \"\"\"\n+        return self._tokenizer.get_vocab_size(with_added_tokens)",
        "comment_created_at": "2020-01-29T20:47:46+00:00",
        "comment_author": "n1t0",
        "comment_body": "Thank you for this PR @kdexd! If I remember correctly `with_added_tokens` should be a named argument and so I don't think it works if provided as a positional argument.",
        "pr_file_module": null
      },
      {
        "comment_id": "373929448",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 99,
        "pr_file": "bindings/python/tokenizers/implementations/base_tokenizer.py",
        "discussion_id": "372621411",
        "commented_code": "@@ -13,6 +13,18 @@ def __repr__(self):\n             self._tokenizer.get_vocab_size(),\n             ', '.join(k + '=' + str(v) for k, v in self._parameters.items()))\n \n+    def get_vocab_size(self, with_added_tokens: bool = True):\n+        \"\"\" Return the size of vocabulary, with or without added tokens.\n+\n+        Args:\n+            with_added_tokens: (`optional`) bool:\n+                Whether to count in added special tokens or not\n+\n+        Returns:\n+            Size of vocabulary\n+        \"\"\"\n+        return self._tokenizer.get_vocab_size(with_added_tokens)",
        "comment_created_at": "2020-02-03T05:42:47+00:00",
        "comment_author": "kdexd",
        "comment_body": "Thanks for pointing out @n1t0 ! I hadn't added this option in my local patched version, glad it didn't slip by.",
        "pr_file_module": null
      }
    ]
  }
]