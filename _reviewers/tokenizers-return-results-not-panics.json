[
  {
    "discussion_id": "1392241005",
    "pr_number": 1357,
    "pr_file": "tokenizers/src/pre_tokenizers/metaspace.rs",
    "created_at": "2023-11-14T09:12:13+00:00",
    "commented_code": "+use crate::tokenizer::{Decoder, PreTokenizedString, PreTokenizer, Result, SplitDelimiterBehavior};\n use serde::{Deserialize, Deserializer, Serialize};\n+use std::fmt;\n \n-use crate::tokenizer::{Decoder, PreTokenizedString, PreTokenizer, Result, SplitDelimiterBehavior};\n+/// Enum representing options for the metaspace prepending scheme.\n+#[derive(Debug, Clone, PartialEq, Serialize, Eq, Deserialize)]\n+#[serde(rename_all = \"snake_case\")]\n+pub enum PrependScheme {\n+    /// Specifies that the scheme should be prepended only once, on the first split.\n+    First,\n+    /// Specifies that the space should not be prepended.\n+    Never,\n+    /// Specifies that the scheme should always be prepended.\n+    Always,\n+}\n+\n+impl From<&str> for PrependScheme {\n+    fn from(s: &str) -> Self {\n+        match s.to_lowercase().as_str() {\n+            \"first\" => PrependScheme::First,\n+            \"never\" => PrependScheme::Never,\n+            \"always\" => PrependScheme::Always,\n+            _ => panic!(\"Invalid value for PrependScheme: {}\", s),",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "1392241005",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1357,
        "pr_file": "tokenizers/src/pre_tokenizers/metaspace.rs",
        "discussion_id": "1392241005",
        "commented_code": "@@ -1,6 +1,45 @@\n+use crate::tokenizer::{Decoder, PreTokenizedString, PreTokenizer, Result, SplitDelimiterBehavior};\n use serde::{Deserialize, Deserializer, Serialize};\n+use std::fmt;\n \n-use crate::tokenizer::{Decoder, PreTokenizedString, PreTokenizer, Result, SplitDelimiterBehavior};\n+/// Enum representing options for the metaspace prepending scheme.\n+#[derive(Debug, Clone, PartialEq, Serialize, Eq, Deserialize)]\n+#[serde(rename_all = \"snake_case\")]\n+pub enum PrependScheme {\n+    /// Specifies that the scheme should be prepended only once, on the first split.\n+    First,\n+    /// Specifies that the space should not be prepended.\n+    Never,\n+    /// Specifies that the scheme should always be prepended.\n+    Always,\n+}\n+\n+impl From<&str> for PrependScheme {\n+    fn from(s: &str) -> Self {\n+        match s.to_lowercase().as_str() {\n+            \"first\" => PrependScheme::First,\n+            \"never\" => PrependScheme::Never,\n+            \"always\" => PrependScheme::Always,\n+            _ => panic!(\"Invalid value for PrependScheme: {}\", s),",
        "comment_created_at": "2023-11-14T09:12:13+00:00",
        "comment_author": "Narsil",
        "comment_body": "Panicking is NOT okay in transformers. We should NEVER panic since we're a library.\r\n\r\nFor that there is `TryFrom` which returns `Result<T>`. However I don't think this should be done in the Rust layer but in the Python layer instead.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1236854188",
    "pr_number": 1217,
    "pr_file": "tokenizers/src/models/unigram/model.rs",
    "created_at": "2023-06-21T11:34:59+00:00",
    "commented_code": "let mut offset = 0;\n         let mut tokens = Vec::with_capacity(str_tokens.len());\n         for string in str_tokens {\n-            let id: u32 = match self.token_to_ids.get(&string) {\n-                Some(id) => *id,\n-                None => self.unk_id.ok_or(UnigramError::MissingUnkId)? as u32,\n+            let ids = if self.token_to_ids.contains_key(&string) {\n+                vec![*self.token_to_ids.get(&string).unwrap()]\n+            } else if self.byte_fallback {\n+                string\n+                    .bytes()\n+                    .map(|b| self.token_to_id(&byte_to_piece(b)).unwrap())\n+                    .collect()\n+            } else {\n+                vec![self.unk_id.ok_or(UnigramError::MissingUnkId)? as u32]\n             };\n-            let len = string.len();\n-            let offsets = (offset, offset + len);\n+            let len = string.len() - ids.len() + 1;\n+            for id in ids {\n+                let offsets = (offset, offset + len);\n+                tokens.push(Token::new(id, self.id_to_token(id).unwrap(), offsets));\n+            }",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "1236854188",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1217,
        "pr_file": "tokenizers/src/models/unigram/model.rs",
        "discussion_id": "1236854188",
        "commented_code": "@@ -407,14 +433,22 @@ impl Model for Unigram {\n         let mut offset = 0;\n         let mut tokens = Vec::with_capacity(str_tokens.len());\n         for string in str_tokens {\n-            let id: u32 = match self.token_to_ids.get(&string) {\n-                Some(id) => *id,\n-                None => self.unk_id.ok_or(UnigramError::MissingUnkId)? as u32,\n+            let ids = if self.token_to_ids.contains_key(&string) {\n+                vec![*self.token_to_ids.get(&string).unwrap()]\n+            } else if self.byte_fallback {\n+                string\n+                    .bytes()\n+                    .map(|b| self.token_to_id(&byte_to_piece(b)).unwrap())\n+                    .collect()\n+            } else {\n+                vec![self.unk_id.ok_or(UnigramError::MissingUnkId)? as u32]\n             };\n-            let len = string.len();\n-            let offsets = (offset, offset + len);\n+            let len = string.len() - ids.len() + 1;\n+            for id in ids {\n+                let offsets = (offset, offset + len);\n+                tokens.push(Token::new(id, self.id_to_token(id).unwrap(), offsets));\n+            }",
        "comment_created_at": "2023-06-21T11:34:59+00:00",
        "comment_author": "Narsil",
        "comment_body": "Remove every `unwrap` and every `vec`.\r\n\r\nThere is 1 `collect` tolerated ( I think it's done that way in BPE) and it's *only* to check that ALL bytes have a token id (you're allowed to use a single `vec` or `collect` in that branch, not in the others.\r\n\r\n:)",
        "pr_file_module": null
      },
      {
        "comment_id": "1236868627",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1217,
        "pr_file": "tokenizers/src/models/unigram/model.rs",
        "discussion_id": "1236854188",
        "commented_code": "@@ -407,14 +433,22 @@ impl Model for Unigram {\n         let mut offset = 0;\n         let mut tokens = Vec::with_capacity(str_tokens.len());\n         for string in str_tokens {\n-            let id: u32 = match self.token_to_ids.get(&string) {\n-                Some(id) => *id,\n-                None => self.unk_id.ok_or(UnigramError::MissingUnkId)? as u32,\n+            let ids = if self.token_to_ids.contains_key(&string) {\n+                vec![*self.token_to_ids.get(&string).unwrap()]\n+            } else if self.byte_fallback {\n+                string\n+                    .bytes()\n+                    .map(|b| self.token_to_id(&byte_to_piece(b)).unwrap())\n+                    .collect()\n+            } else {\n+                vec![self.unk_id.ok_or(UnigramError::MissingUnkId)? as u32]\n             };\n-            let len = string.len();\n-            let offsets = (offset, offset + len);\n+            let len = string.len() - ids.len() + 1;\n+            for id in ids {\n+                let offsets = (offset, offset + len);\n+                tokens.push(Token::new(id, self.id_to_token(id).unwrap(), offsets));\n+            }",
        "comment_created_at": "2023-06-21T11:46:06+00:00",
        "comment_author": "chris-ha458",
        "comment_body": "I understand the `unwrap` part since it implicitly implies that potential errors are not being handled.\r\n\r\nCan you share the reasoning regarding `vec`? is it because since this focuses on the bytefallback pieces, which should be known during compile time (256 of them) and therefore should be addressed with arrays?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "481247861",
    "pr_number": 292,
    "pr_file": "tokenizers/src/models/unigram/model.rs",
    "created_at": "2020-09-01T15:52:53+00:00",
    "commented_code": "+use crate::models::unigram::lattice::Lattice;\n+use crate::models::unigram::trie::{Trie, TrieBuilder};\n+use crate::tokenizer::{Model, Result, Token};\n+\n+use std::collections::HashMap;\n+use std::convert::TryInto;\n+use std::fs::File;\n+use std::io::{BufRead, BufReader};\n+use std::path::{Path, PathBuf};\n+\n+type TokenMap = HashMap<String, u32>;\n+type Vocab = Vec<String>;\n+\n+/// A `Unigram` model to encode sentences.\n+#[derive(Clone)]\n+pub struct Unigram {\n+    token_to_ids: TokenMap,\n+    pub(crate) vocab: Vocab,\n+    pub(super) scores: Vec<f64>,\n+    trie: Trie<char>,\n+    pub min_score: f64,\n+    pub(super) unk_id: usize,\n+    pub(super) bos_id: usize,\n+    pub(super) eos_id: usize,\n+\n+    fuse_unk: bool,\n+}\n+impl PartialEq for Unigram {\n+    fn eq(&self, other: &Self) -> bool {\n+        let vocab: Vec<(&String, &f64)> = self.vocab.iter().zip(self.scores.iter()).collect();\n+        let other_vocab: Vec<(&String, &f64)> =\n+            other.vocab.iter().zip(other.scores.iter()).collect();\n+        self.unk_id == other.unk_id && vocab == other_vocab\n+    }\n+}\n+\n+impl std::fmt::Debug for Unigram {\n+    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {\n+        fmt.debug_struct(\"BPE\")\n+            .field(\"vocab\", &self.vocab.len())\n+            .finish()\n+    }\n+}\n+\n+static K_UNK_PENALTY: f64 = 10.0;\n+\n+impl Default for Unigram {\n+    fn default() -> Self {\n+        let vocab = vec![(\"<unk>\".to_string(), 0.0)];\n+        Self::from(&vocab, 0)\n+    }\n+}\n+\n+impl Unigram {\n+    /// Create a `Unigram` model from a given vocabulary.\n+    /// Vocabulary are the various tokens and their associated score which is a sort of a logprob of\n+    /// their frequency, which will enable tokenization and sampling.\n+    /// unk_id, is the index within the vocabulary.\n+    /// For now `Unigram` *requires* at least `unk` because we might find a never seen char.\n+    /// Further versions might allow that part to be hidden.\n+    pub fn from(vocabulary: &[(String, f64)], unk_id: usize) -> Self {\n+        let n = vocabulary.len();\n+        let mut vocab: Vec<String> = Vec::with_capacity(n);\n+        let mut scores: Vec<f64> = Vec::with_capacity(n);\n+        let mut token_to_ids: TokenMap = HashMap::new();\n+        let mut builder = TrieBuilder::default();\n+\n+        assert!(n >= 1, \"We need at least unk in the vocabulary\");",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "481247861",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 292,
        "pr_file": "tokenizers/src/models/unigram/model.rs",
        "discussion_id": "481247861",
        "commented_code": "@@ -0,0 +1,433 @@\n+use crate::models::unigram::lattice::Lattice;\n+use crate::models::unigram::trie::{Trie, TrieBuilder};\n+use crate::tokenizer::{Model, Result, Token};\n+\n+use std::collections::HashMap;\n+use std::convert::TryInto;\n+use std::fs::File;\n+use std::io::{BufRead, BufReader};\n+use std::path::{Path, PathBuf};\n+\n+type TokenMap = HashMap<String, u32>;\n+type Vocab = Vec<String>;\n+\n+/// A `Unigram` model to encode sentences.\n+#[derive(Clone)]\n+pub struct Unigram {\n+    token_to_ids: TokenMap,\n+    pub(crate) vocab: Vocab,\n+    pub(super) scores: Vec<f64>,\n+    trie: Trie<char>,\n+    pub min_score: f64,\n+    pub(super) unk_id: usize,\n+    pub(super) bos_id: usize,\n+    pub(super) eos_id: usize,\n+\n+    fuse_unk: bool,\n+}\n+impl PartialEq for Unigram {\n+    fn eq(&self, other: &Self) -> bool {\n+        let vocab: Vec<(&String, &f64)> = self.vocab.iter().zip(self.scores.iter()).collect();\n+        let other_vocab: Vec<(&String, &f64)> =\n+            other.vocab.iter().zip(other.scores.iter()).collect();\n+        self.unk_id == other.unk_id && vocab == other_vocab\n+    }\n+}\n+\n+impl std::fmt::Debug for Unigram {\n+    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {\n+        fmt.debug_struct(\"BPE\")\n+            .field(\"vocab\", &self.vocab.len())\n+            .finish()\n+    }\n+}\n+\n+static K_UNK_PENALTY: f64 = 10.0;\n+\n+impl Default for Unigram {\n+    fn default() -> Self {\n+        let vocab = vec![(\"<unk>\".to_string(), 0.0)];\n+        Self::from(&vocab, 0)\n+    }\n+}\n+\n+impl Unigram {\n+    /// Create a `Unigram` model from a given vocabulary.\n+    /// Vocabulary are the various tokens and their associated score which is a sort of a logprob of\n+    /// their frequency, which will enable tokenization and sampling.\n+    /// unk_id, is the index within the vocabulary.\n+    /// For now `Unigram` *requires* at least `unk` because we might find a never seen char.\n+    /// Further versions might allow that part to be hidden.\n+    pub fn from(vocabulary: &[(String, f64)], unk_id: usize) -> Self {\n+        let n = vocabulary.len();\n+        let mut vocab: Vec<String> = Vec::with_capacity(n);\n+        let mut scores: Vec<f64> = Vec::with_capacity(n);\n+        let mut token_to_ids: TokenMap = HashMap::new();\n+        let mut builder = TrieBuilder::default();\n+\n+        assert!(n >= 1, \"We need at least unk in the vocabulary\");",
        "comment_created_at": "2020-09-01T15:52:53+00:00",
        "comment_author": "n1t0",
        "comment_body": "It probably would be better to make `from` return a `Result` and fail gracefully since this is a public API.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "813749801",
    "pr_number": 919,
    "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
    "created_at": "2022-02-24T10:33:13+00:00",
    "commented_code": "type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }\n+}\n+fn space_rightmost_at_start(sentence: &str) -> usize {\n+    if let Some(match_) = RIGHTMOST_SPACE_AT_START.find(sentence) {\n+        match_.end()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        0\n+    }\n+}",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "813749801",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 919,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "813749801",
        "commented_code": "@@ -90,6 +91,37 @@ impl std::cmp::Eq for AddedToken {}\n \n type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }\n+}\n+fn space_rightmost_at_start(sentence: &str) -> usize {\n+    if let Some(match_) = RIGHTMOST_SPACE_AT_START.find(sentence) {\n+        match_.end()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        0\n+    }\n+}",
        "comment_created_at": "2022-02-24T10:33:13+00:00",
        "comment_author": "McPatate",
        "comment_body": "Wouldn't it be better to return a `Result` here ?",
        "pr_file_module": null
      },
      {
        "comment_id": "813862919",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 919,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "813749801",
        "commented_code": "@@ -90,6 +91,37 @@ impl std::cmp::Eq for AddedToken {}\n \n type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }\n+}\n+fn space_rightmost_at_start(sentence: &str) -> usize {\n+    if let Some(match_) = RIGHTMOST_SPACE_AT_START.find(sentence) {\n+        match_.end()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        0\n+    }\n+}",
        "comment_created_at": "2022-02-24T13:11:42+00:00",
        "comment_author": "Narsil",
        "comment_body": "Well if the regexp doesn't match, it's strictly equivalent to matching nothing, right ?\r\n\r\nI do that to avoid adding and `unwrap` which could potential `panic!` which makes users pretty unhappy.\r\n\r\nI don't think the pathway should be taken, but if it ever is, then I think `0` is a valid default which prevents panicking and is still, correct. Wdyt ?",
        "pr_file_module": null
      },
      {
        "comment_id": "813875427",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 919,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "813749801",
        "commented_code": "@@ -90,6 +91,37 @@ impl std::cmp::Eq for AddedToken {}\n \n type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }\n+}\n+fn space_rightmost_at_start(sentence: &str) -> usize {\n+    if let Some(match_) = RIGHTMOST_SPACE_AT_START.find(sentence) {\n+        match_.end()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        0\n+    }\n+}",
        "comment_created_at": "2022-02-24T13:24:59+00:00",
        "comment_author": "McPatate",
        "comment_body": "I understand adding `Result`s all the way up to the public API is cumbersome but to me is the cleaner approach : notify the user there was an error / undefined / unexpected behavior and let him handle it.\r\n\r\nReturning 0 is hiding the fact that you've run into an unexpected case/behavior (aka silent error). Adding comments to clarify code is a smell imo, make it comprehensible in the first place, you almost always can !\r\n\r\nBut then again, it's more work.",
        "pr_file_module": null
      },
      {
        "comment_id": "813950306",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 919,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "813749801",
        "commented_code": "@@ -90,6 +91,37 @@ impl std::cmp::Eq for AddedToken {}\n \n type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }\n+}\n+fn space_rightmost_at_start(sentence: &str) -> usize {\n+    if let Some(match_) = RIGHTMOST_SPACE_AT_START.find(sentence) {\n+        match_.end()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        0\n+    }\n+}",
        "comment_created_at": "2022-02-24T14:45:52+00:00",
        "comment_author": "Narsil",
        "comment_body": "> I understand adding Results all the way up to the public API is cumbersome but to me is the cleaner approach : notify the user there was an error / undefined / unexpected behavior and let him handle it.\r\n\r\nIn that case it's not true. Finding 0 match is different than an error.\r\nI could very well change the regex to be `\\s+` and then the `else` branch would be expected and working as intended.\r\n\r\nIs adding a `warn!(\"AddedToken with `single_word` seems to have an issue, Please report this\")` in that code path OK ?\r\nIt it warning the user, but still prevent catastrophing failure ?\r\n\r\nWe don't expect the code path to be taken, but it's not preventing the algorithm from working.",
        "pr_file_module": null
      },
      {
        "comment_id": "814081398",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 919,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "813749801",
        "commented_code": "@@ -90,6 +91,37 @@ impl std::cmp::Eq for AddedToken {}\n \n type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }\n+}\n+fn space_rightmost_at_start(sentence: &str) -> usize {\n+    if let Some(match_) = RIGHTMOST_SPACE_AT_START.find(sentence) {\n+        match_.end()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        0\n+    }\n+}",
        "comment_created_at": "2022-02-24T16:57:10+00:00",
        "comment_author": "McPatate",
        "comment_body": "Am I correct in assuming that if we go through the `else` clause, it means the user has inputted a malformatted `.lstrip` or `.rstrip` token ?",
        "pr_file_module": null
      },
      {
        "comment_id": "814105702",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 919,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "813749801",
        "commented_code": "@@ -90,6 +91,37 @@ impl std::cmp::Eq for AddedToken {}\n \n type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }\n+}\n+fn space_rightmost_at_start(sentence: &str) -> usize {\n+    if let Some(match_) = RIGHTMOST_SPACE_AT_START.find(sentence) {\n+        match_.end()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        0\n+    }\n+}",
        "comment_created_at": "2022-02-24T17:26:34+00:00",
        "comment_author": "Narsil",
        "comment_body": "No, the else clause will trigger, when `REGEXP.find()` returns `None`.\r\n\r\nThis is normal when the REGEX fails to find any match in the submitted string.\r\nThis particular brand of REGEX should match all the time (since it should match the empty string being `^\\s*` and `\\s*$`).\r\nSo I don't expect it to not match. But if it doesn't match, it's roughly the same as saying no space where found.",
        "pr_file_module": null
      },
      {
        "comment_id": "814585528",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 919,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "813749801",
        "commented_code": "@@ -90,6 +91,37 @@ impl std::cmp::Eq for AddedToken {}\n \n type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }\n+}\n+fn space_rightmost_at_start(sentence: &str) -> usize {\n+    if let Some(match_) = RIGHTMOST_SPACE_AT_START.find(sentence) {\n+        match_.end()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        0\n+    }\n+}",
        "comment_created_at": "2022-02-25T08:56:18+00:00",
        "comment_author": "McPatate",
        "comment_body": "But that's what I don't understand, shouldn't there be a space ? ",
        "pr_file_module": null
      },
      {
        "comment_id": "814598004",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 919,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "813749801",
        "commented_code": "@@ -90,6 +91,37 @@ impl std::cmp::Eq for AddedToken {}\n \n type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }\n+}\n+fn space_rightmost_at_start(sentence: &str) -> usize {\n+    if let Some(match_) = RIGHTMOST_SPACE_AT_START.find(sentence) {\n+        match_.end()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        0\n+    }\n+}",
        "comment_created_at": "2022-02-25T09:13:55+00:00",
        "comment_author": "Narsil",
        "comment_body": "Oh no no necessarily.\r\n\r\n`AddedTokens(\"<mask>\", lstrip=True)` means you want it to match `\"Something <mask>  else\"` -> `['Something\", \" <mask>\", \" else\"]`. Meaning you are actually capturing the left spaces in addition to your token (effectively tripping it from being seen for your model).\r\n\r\nBut `\"Something<mask>else\"` will also capture `[\"Something\", \"<mask>\", \"else\"]`. no space have been deleted.\r\nIf you want this to NOT capture you need to activate `single_word=True` too.\r\n\r\nBut both options are orthogonal and don´t  necessarily need one another (although I think in practice they are probably reasonned about in conjunction)\r\n\r\n\r\nEdit: `<mask>` is a comment in GH markdown easy to miss :D\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "814625872",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 919,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "813749801",
        "commented_code": "@@ -90,6 +91,37 @@ impl std::cmp::Eq for AddedToken {}\n \n type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }\n+}\n+fn space_rightmost_at_start(sentence: &str) -> usize {\n+    if let Some(match_) = RIGHTMOST_SPACE_AT_START.find(sentence) {\n+        match_.end()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        0\n+    }\n+}",
        "comment_created_at": "2022-02-25T09:51:21+00:00",
        "comment_author": "McPatate",
        "comment_body": "Ok that makes sense. One thing tho : I would remove the comment, imho it's misleading !",
        "pr_file_module": null
      },
      {
        "comment_id": "814753043",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 919,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "813749801",
        "commented_code": "@@ -90,6 +91,37 @@ impl std::cmp::Eq for AddedToken {}\n \n type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }\n+}\n+fn space_rightmost_at_start(sentence: &str) -> usize {\n+    if let Some(match_) = RIGHTMOST_SPACE_AT_START.find(sentence) {\n+        match_.end()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        0\n+    }\n+}",
        "comment_created_at": "2022-02-25T13:08:03+00:00",
        "comment_author": "Narsil",
        "comment_body": "@McPatate I merged since I am starting the necessary work to release this before transformers's own release on thursday, but we can continue the discussion to make this clear.\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "813790604",
    "pr_number": 919,
    "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
    "created_at": "2022-02-24T11:27:51+00:00",
    "commented_code": "type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "813790604",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 919,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "813790604",
        "commented_code": "@@ -90,6 +91,37 @@ impl std::cmp::Eq for AddedToken {}\n \n type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }",
        "comment_created_at": "2022-02-24T11:27:51+00:00",
        "comment_author": "mishig25",
        "comment_body": "if it is `never happen`, can we just call `LEFTMOST_SPACE_AT_END.find(sentence).expect(\"LEFTMOST_SPACE_AT_END should always match but didnt match int this sentence: {sentence}\").`",
        "pr_file_module": null
      },
      {
        "comment_id": "813864945",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 919,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "813790604",
        "commented_code": "@@ -90,6 +91,37 @@ impl std::cmp::Eq for AddedToken {}\n \n type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }",
        "comment_created_at": "2022-02-24T13:13:53+00:00",
        "comment_author": "Narsil",
        "comment_body": "`.unwrap()` is something I tend to avoid within logic code at all if possible, since having the program panic, is never a great user experience.\r\n\r\nMy answer : https://github.com/huggingface/tokenizers/pull/919#discussion_r813862919 applies too. I think returning `Option` would be preferrable to panicking, but the result would be the same IMO, if the regex doesn't match, I just don't capture anything.",
        "pr_file_module": null
      },
      {
        "comment_id": "813968926",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 919,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "813790604",
        "commented_code": "@@ -90,6 +91,37 @@ impl std::cmp::Eq for AddedToken {}\n \n type MatchingSet = (AhoCorasick, Vec<u32>);\n \n+lazy_static! {\n+    static ref STARTS_WITH_WORD: Regex = Regex::new(r\"^\\w\").unwrap();\n+    static ref ENDS_WITH_WORD: Regex = Regex::new(r\"\\w$\").unwrap();\n+    static ref RIGHTMOST_SPACE_AT_START: Regex = Regex::new(r\"^\\s*\").unwrap();\n+    static ref LEFTMOST_SPACE_AT_END: Regex = Regex::new(r\"\\s*$\").unwrap();\n+}\n+\n+fn ends_with_word(sentence: &str) -> bool {\n+    ENDS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn starts_with_word(sentence: &str) -> bool {\n+    STARTS_WITH_WORD.is_match(sentence)\n+}\n+\n+fn space_leftmost_at_end(sentence: &str) -> usize {\n+    if let Some(match_) = LEFTMOST_SPACE_AT_END.find(sentence) {\n+        match_.start()\n+    } else {\n+        // This should never happen since the Regex should match all the time\n+        sentence.len()\n+    }",
        "comment_created_at": "2022-02-24T15:04:01+00:00",
        "comment_author": "mishig25",
        "comment_body": "oh I see 🙂",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "779142059",
    "pr_number": 857,
    "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
    "created_at": "2022-01-05T21:16:20+00:00",
    "commented_code": "fn find_matches<'a>(\n         &self,\n         sentence: &str,\n-        split_re: &'a MatchingSet,\n+        split_trie: &'a Trie<u8>,\n     ) -> Vec<(Option<u32>, Offsets)> {\n         if sentence.is_empty() {\n             return vec![(None, (0, 0))];\n         }\n \n-        let mut matches = split_re\n-            .0\n-            .matches(sentence)\n-            .into_iter()\n-            .flat_map(|idx| {\n-                regex::Regex::new(&split_re.0.patterns()[idx])\n-                    .unwrap()\n-                    .find_iter(sentence)\n-                    .map(|m| (idx, (m.start(), m.end())))\n-                    .collect::<Vec<_>>()\n-            })\n-            .collect::<Vec<_>>();\n-\n-        // We sort all the matches by their start and then by their pattern id\n-        matches.sort_by(\n-            |(idxa, (sa, _)), (idxb, (sb, _))| {\n-                if sa != sb {\n-                    sa.cmp(sb)\n-                } else {\n-                    idxa.cmp(idxb)\n+        let mut start_offset = 0;\n+        let mut splits = split_trie\n+            .matches(sentence.as_bytes())\n+            .flat_map(|(start, stop)| {\n+                let mut start = start;\n+                let mut stop = stop;\n+\n+                let match_ = &sentence[start..stop];\n+                let id = self.added_tokens_map.get(match_).unwrap_or_else(|| {\n+                    self.shadow_map\n+                        .get(match_)\n+                        .unwrap_or_else(|| panic!(\"Expected to find an id for {:?}\", match_))",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "779142059",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 857,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "779142059",
        "commented_code": "@@ -315,84 +277,63 @@ impl AddedVocabulary {\n     fn find_matches<'a>(\n         &self,\n         sentence: &str,\n-        split_re: &'a MatchingSet,\n+        split_trie: &'a Trie<u8>,\n     ) -> Vec<(Option<u32>, Offsets)> {\n         if sentence.is_empty() {\n             return vec![(None, (0, 0))];\n         }\n \n-        let mut matches = split_re\n-            .0\n-            .matches(sentence)\n-            .into_iter()\n-            .flat_map(|idx| {\n-                regex::Regex::new(&split_re.0.patterns()[idx])\n-                    .unwrap()\n-                    .find_iter(sentence)\n-                    .map(|m| (idx, (m.start(), m.end())))\n-                    .collect::<Vec<_>>()\n-            })\n-            .collect::<Vec<_>>();\n-\n-        // We sort all the matches by their start and then by their pattern id\n-        matches.sort_by(\n-            |(idxa, (sa, _)), (idxb, (sb, _))| {\n-                if sa != sb {\n-                    sa.cmp(sb)\n-                } else {\n-                    idxa.cmp(idxb)\n+        let mut start_offset = 0;\n+        let mut splits = split_trie\n+            .matches(sentence.as_bytes())\n+            .flat_map(|(start, stop)| {\n+                let mut start = start;\n+                let mut stop = stop;\n+\n+                let match_ = &sentence[start..stop];\n+                let id = self.added_tokens_map.get(match_).unwrap_or_else(|| {\n+                    self.shadow_map\n+                        .get(match_)\n+                        .unwrap_or_else(|| panic!(\"Expected to find an id for {:?}\", match_))",
        "comment_created_at": "2022-01-05T21:16:20+00:00",
        "comment_author": "McPatate",
        "comment_body": "Maybe make this function return a `Result<T>`? ",
        "pr_file_module": null
      },
      {
        "comment_id": "779414442",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 857,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "779142059",
        "commented_code": "@@ -315,84 +277,63 @@ impl AddedVocabulary {\n     fn find_matches<'a>(\n         &self,\n         sentence: &str,\n-        split_re: &'a MatchingSet,\n+        split_trie: &'a Trie<u8>,\n     ) -> Vec<(Option<u32>, Offsets)> {\n         if sentence.is_empty() {\n             return vec![(None, (0, 0))];\n         }\n \n-        let mut matches = split_re\n-            .0\n-            .matches(sentence)\n-            .into_iter()\n-            .flat_map(|idx| {\n-                regex::Regex::new(&split_re.0.patterns()[idx])\n-                    .unwrap()\n-                    .find_iter(sentence)\n-                    .map(|m| (idx, (m.start(), m.end())))\n-                    .collect::<Vec<_>>()\n-            })\n-            .collect::<Vec<_>>();\n-\n-        // We sort all the matches by their start and then by their pattern id\n-        matches.sort_by(\n-            |(idxa, (sa, _)), (idxb, (sb, _))| {\n-                if sa != sb {\n-                    sa.cmp(sb)\n-                } else {\n-                    idxa.cmp(idxb)\n+        let mut start_offset = 0;\n+        let mut splits = split_trie\n+            .matches(sentence.as_bytes())\n+            .flat_map(|(start, stop)| {\n+                let mut start = start;\n+                let mut stop = stop;\n+\n+                let match_ = &sentence[start..stop];\n+                let id = self.added_tokens_map.get(match_).unwrap_or_else(|| {\n+                    self.shadow_map\n+                        .get(match_)\n+                        .unwrap_or_else(|| panic!(\"Expected to find an id for {:?}\", match_))",
        "comment_created_at": "2022-01-06T09:34:04+00:00",
        "comment_author": "Narsil",
        "comment_body": "I think this would go away using `aho-corasick` (it returns the ID with the match).",
        "pr_file_module": null
      },
      {
        "comment_id": "779440288",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 857,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "779142059",
        "commented_code": "@@ -315,84 +277,63 @@ impl AddedVocabulary {\n     fn find_matches<'a>(\n         &self,\n         sentence: &str,\n-        split_re: &'a MatchingSet,\n+        split_trie: &'a Trie<u8>,\n     ) -> Vec<(Option<u32>, Offsets)> {\n         if sentence.is_empty() {\n             return vec![(None, (0, 0))];\n         }\n \n-        let mut matches = split_re\n-            .0\n-            .matches(sentence)\n-            .into_iter()\n-            .flat_map(|idx| {\n-                regex::Regex::new(&split_re.0.patterns()[idx])\n-                    .unwrap()\n-                    .find_iter(sentence)\n-                    .map(|m| (idx, (m.start(), m.end())))\n-                    .collect::<Vec<_>>()\n-            })\n-            .collect::<Vec<_>>();\n-\n-        // We sort all the matches by their start and then by their pattern id\n-        matches.sort_by(\n-            |(idxa, (sa, _)), (idxb, (sb, _))| {\n-                if sa != sb {\n-                    sa.cmp(sb)\n-                } else {\n-                    idxa.cmp(idxb)\n+        let mut start_offset = 0;\n+        let mut splits = split_trie\n+            .matches(sentence.as_bytes())\n+            .flat_map(|(start, stop)| {\n+                let mut start = start;\n+                let mut stop = stop;\n+\n+                let match_ = &sentence[start..stop];\n+                let id = self.added_tokens_map.get(match_).unwrap_or_else(|| {\n+                    self.shadow_map\n+                        .get(match_)\n+                        .unwrap_or_else(|| panic!(\"Expected to find an id for {:?}\", match_))",
        "comment_created_at": "2022-01-06T10:15:42+00:00",
        "comment_author": "Narsil",
        "comment_body": "Also there are other `unwrap` which would panic too.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "463122522",
    "pr_number": 360,
    "pr_file": "tokenizers/src/tokenizer/pre_tokenizer.rs",
    "created_at": "2020-07-30T16:30:02+00:00",
    "commented_code": "+use crate::{NormalizedString, Offsets, Result};\n+\n+/// Wrapper for a subpart of a `NormalizedString`.\n+///\n+/// This SubString contains the underlying `NormalizedString` as well as its offsets\n+/// in the original string. These offsets are in the `original` referential\n+#[derive(Debug)]\n+pub struct SubString {\n+    /// The underlying `NormalizedString`. Each SubString is represented by a `NormalizedString`\n+    /// and in the end we might be carrying a lot of SubString representing various parts of the\n+    /// original input string.\n+    pub normalized: NormalizedString,\n+    /// Offsets of the `NormalizedString` in the `original` input string. These are useful to find\n+    /// the `original` offsets in the input string, as opposed to the `original` offsets in the\n+    /// sub-part of the input string represented by `NormalizedString`\n+    pub original_offsets: Offsets,\n+}\n+\n+/// A `PreTokenizedString` takes care of splitting the input string in multiple\n+/// sub strings, while ensuring that they form a coherend group. This let us keep\n+/// track of the offsets during the whole normalization and pre-tokenization steps.\n+#[derive(Debug)]\n+pub struct PreTokenizedString {\n+    parts: Vec<SubString>,\n+}\n+\n+impl PreTokenizedString {\n+    /// Split the `PreTokenizedString` by providing a `split_fn` in charge of splitting\n+    /// each substring (`NormalizedString`) into multiple parts.\n+    ///\n+    /// `split_fn` takes a `NormalizedString` and is in charge of returning an iterator\n+    /// over the produced `NormalizedString`. `split_fn` is free of modifying these\n+    /// `NormalizedString` as relevant, as long as it respects the constraint stated below.\n+    ///\n+    /// There are only one constraint that *MUST* be respected:\n+    /// > The produced `NormalizedString`, if combined back together, must have the\n+    /// same `original` string as the original one given to `split_fn`. This concretely\n+    /// means that for the offset tracking to work as expected, `split_fn` must produce\n+    /// \"splits\" of the original string.\n+    pub fn split<F, U>(&mut self, mut split_fn: F) -> Result<()>\n+    where\n+        F: FnMut(usize, NormalizedString) -> Result<U>,\n+        U: IntoIterator<Item = NormalizedString>,\n+    {\n+        self.parts = self\n+            .parts\n+            .drain(..)\n+            .enumerate()\n+            .flat_map(|(i, sub)| {\n+                let original_len = sub.normalized.len_original();\n+                let original_offsets = sub.original_offsets;\n+\n+                let mut new_len = 0;\n+                let res = split_fn(i, sub.normalized);\n+                if let Err(e) = res {\n+                    return itertools::Either::Left(std::iter::once(Err(e)));\n+                }\n+\n+                let parts = res\n+                    .unwrap()\n+                    .into_iter()\n+                    .map(|normalized| {\n+                        let len = normalized.len_original();\n+                        let new_s = SubString {\n+                            normalized,\n+                            original_offsets: (\n+                                original_offsets.0 + new_len,\n+                                original_offsets.0 + new_len + len,\n+                            ),\n+                        };\n+                        new_len += len;\n+                        new_s\n+                    })\n+                    .collect::<Vec<_>>();\n+\n+                if new_len != original_len {\n+                    println!(\n+                        \"Original offsets: {:?}\nNew: {:?}\",\n+                        (0, original_len),\n+                        (0, new_len)\n+                    );\n+                    itertools::Either::Left(std::iter::once(Err(\n+                        \"Split pre-tokenized string must represent the entire original string\"\n+                            .into(),\n+                    )))\n+                } else {\n+                    itertools::Either::Right(parts.into_iter().map(Ok))\n+                }\n+            })\n+            .collect::<Result<Vec<_>>>()?;\n+\n+        Ok(())\n+    }\n+\n+    pub fn iter(&self) -> std::slice::Iter<SubString> {\n+        self.into_iter()\n+    }\n+\n+    /// Returns a list of normalized string and the associated offsets,\n+    /// either in original or normalized referential\n+    pub fn get_normalized(&self, original: bool) -> Vec<(&str, Offsets)> {\n+        let mut offset = 0;\n+        self.iter()\n+            .map(|sub| {\n+                let offsets = if original {\n+                    (\n+                        sub.original_offsets.0,\n+                        sub.original_offsets.0 + sub.normalized.len_original(),\n+                    )\n+                } else {\n+                    let len = sub.normalized.len();\n+                    offset += len;\n+                    (offset - len, offset)\n+                };\n+\n+                (sub.normalized.get(), offsets)\n+            })\n+            .collect()\n+    }\n+\n+    /// Merge back to a NormalizedString\n+    pub fn into_merged(self) -> NormalizedString {\n+        let offsets = (\n+            0,\n+            self.parts\n+                .iter()\n+                .last()\n+                .map_or(0, |sub| sub.original_offsets.1),\n+        );\n+        let normalized: NormalizedString = self.into_iter().map(|sub| sub.normalized).collect();\n+        assert_eq!(offsets, (0, normalized.len_original()));",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "463122522",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 360,
        "pr_file": "tokenizers/src/tokenizer/pre_tokenizer.rs",
        "discussion_id": "463122522",
        "commented_code": "@@ -0,0 +1,179 @@\n+use crate::{NormalizedString, Offsets, Result};\n+\n+/// Wrapper for a subpart of a `NormalizedString`.\n+///\n+/// This SubString contains the underlying `NormalizedString` as well as its offsets\n+/// in the original string. These offsets are in the `original` referential\n+#[derive(Debug)]\n+pub struct SubString {\n+    /// The underlying `NormalizedString`. Each SubString is represented by a `NormalizedString`\n+    /// and in the end we might be carrying a lot of SubString representing various parts of the\n+    /// original input string.\n+    pub normalized: NormalizedString,\n+    /// Offsets of the `NormalizedString` in the `original` input string. These are useful to find\n+    /// the `original` offsets in the input string, as opposed to the `original` offsets in the\n+    /// sub-part of the input string represented by `NormalizedString`\n+    pub original_offsets: Offsets,\n+}\n+\n+/// A `PreTokenizedString` takes care of splitting the input string in multiple\n+/// sub strings, while ensuring that they form a coherend group. This let us keep\n+/// track of the offsets during the whole normalization and pre-tokenization steps.\n+#[derive(Debug)]\n+pub struct PreTokenizedString {\n+    parts: Vec<SubString>,\n+}\n+\n+impl PreTokenizedString {\n+    /// Split the `PreTokenizedString` by providing a `split_fn` in charge of splitting\n+    /// each substring (`NormalizedString`) into multiple parts.\n+    ///\n+    /// `split_fn` takes a `NormalizedString` and is in charge of returning an iterator\n+    /// over the produced `NormalizedString`. `split_fn` is free of modifying these\n+    /// `NormalizedString` as relevant, as long as it respects the constraint stated below.\n+    ///\n+    /// There are only one constraint that *MUST* be respected:\n+    /// > The produced `NormalizedString`, if combined back together, must have the\n+    /// same `original` string as the original one given to `split_fn`. This concretely\n+    /// means that for the offset tracking to work as expected, `split_fn` must produce\n+    /// \"splits\" of the original string.\n+    pub fn split<F, U>(&mut self, mut split_fn: F) -> Result<()>\n+    where\n+        F: FnMut(usize, NormalizedString) -> Result<U>,\n+        U: IntoIterator<Item = NormalizedString>,\n+    {\n+        self.parts = self\n+            .parts\n+            .drain(..)\n+            .enumerate()\n+            .flat_map(|(i, sub)| {\n+                let original_len = sub.normalized.len_original();\n+                let original_offsets = sub.original_offsets;\n+\n+                let mut new_len = 0;\n+                let res = split_fn(i, sub.normalized);\n+                if let Err(e) = res {\n+                    return itertools::Either::Left(std::iter::once(Err(e)));\n+                }\n+\n+                let parts = res\n+                    .unwrap()\n+                    .into_iter()\n+                    .map(|normalized| {\n+                        let len = normalized.len_original();\n+                        let new_s = SubString {\n+                            normalized,\n+                            original_offsets: (\n+                                original_offsets.0 + new_len,\n+                                original_offsets.0 + new_len + len,\n+                            ),\n+                        };\n+                        new_len += len;\n+                        new_s\n+                    })\n+                    .collect::<Vec<_>>();\n+\n+                if new_len != original_len {\n+                    println!(\n+                        \"Original offsets: {:?}\\nNew: {:?}\",\n+                        (0, original_len),\n+                        (0, new_len)\n+                    );\n+                    itertools::Either::Left(std::iter::once(Err(\n+                        \"Split pre-tokenized string must represent the entire original string\"\n+                            .into(),\n+                    )))\n+                } else {\n+                    itertools::Either::Right(parts.into_iter().map(Ok))\n+                }\n+            })\n+            .collect::<Result<Vec<_>>>()?;\n+\n+        Ok(())\n+    }\n+\n+    pub fn iter(&self) -> std::slice::Iter<SubString> {\n+        self.into_iter()\n+    }\n+\n+    /// Returns a list of normalized string and the associated offsets,\n+    /// either in original or normalized referential\n+    pub fn get_normalized(&self, original: bool) -> Vec<(&str, Offsets)> {\n+        let mut offset = 0;\n+        self.iter()\n+            .map(|sub| {\n+                let offsets = if original {\n+                    (\n+                        sub.original_offsets.0,\n+                        sub.original_offsets.0 + sub.normalized.len_original(),\n+                    )\n+                } else {\n+                    let len = sub.normalized.len();\n+                    offset += len;\n+                    (offset - len, offset)\n+                };\n+\n+                (sub.normalized.get(), offsets)\n+            })\n+            .collect()\n+    }\n+\n+    /// Merge back to a NormalizedString\n+    pub fn into_merged(self) -> NormalizedString {\n+        let offsets = (\n+            0,\n+            self.parts\n+                .iter()\n+                .last()\n+                .map_or(0, |sub| sub.original_offsets.1),\n+        );\n+        let normalized: NormalizedString = self.into_iter().map(|sub| sub.normalized).collect();\n+        assert_eq!(offsets, (0, normalized.len_original()));",
        "comment_created_at": "2020-07-30T16:30:02+00:00",
        "comment_author": "sebpuetz",
        "comment_body": "What are the conditions that make this assertion necessary? If the assertion can be triggered in \"normal usage\", the method should return a `Result` instead. Otherwise the library user (and fwiw the users of the bindings) get their process killed, that's not the nicest UX.\r\n\r\nIf the assertion can be made superfluous by encapsulating the `SubString` fields and verifying in the modifying contexts, then I'd suggest removing this assertion here.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "463149896",
    "pr_number": 360,
    "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
    "created_at": "2020-07-30T17:16:26+00:00",
    "commented_code": "if start_offset < start {\n                     splits.push((None, (start_offset, start)));\n                 }\n-                splits.push((Some(idx), (start, end)));\n+                splits.push((Some(split_re.1[idx] as u32), (start, end)));\n                 start_offset = end;\n \n                 splits\n             })\n             .collect::<Vec<_>>();\n         if let Some((_, (_, end))) = splits.iter().last().copied() {\n-            if end < sentence.get().len() {\n-                splits.push((None, (end, sentence.get().len())));\n+            if end < sentence.len() {\n+                splits.push((None, (end, sentence.len())));\n             }\n         }\n-\n         if splits.is_empty() {\n-            vec![(sentence, None)]\n-        } else {\n-            splits\n-                .into_iter()\n-                .map(|(idx, (start, end))| {\n-                    let normalized = sentence\n-                        .slice_bytes(Range::Normalized(start..end))\n-                        .expect(\"Error while extracting normalized Range\");\n+            splits.push((None, (0, sentence.len())));\n+        }\n \n-                    // Find out the associated AddedToken, and its id\n-                    let id = idx.map(|idx| split_re.1[idx]);\n+        splits\n+    }\n \n-                    (normalized, id)\n-                })\n-                .collect()\n-        }\n+    /// Split the input sentence to extract anything we found from the `MatchingSet`, as well as\n+    /// the list of corresponding IDs\n+    /// The list of IDs have the exact same number of elements than the Iterator.\n+    fn split_with_indices(\n+        &self,\n+        sentence: NormalizedString,\n+        split_re: &MatchingSet,\n+    ) -> (Vec<Option<u32>>, impl Iterator<Item = NormalizedString>) {\n+        let (indices, offsets) = self\n+            .find_matches(sentence.get(), split_re)\n+            .into_iter()\n+            .unzip::<_, _, Vec<_>, Vec<_>>();\n+\n+        // Return the indices and the split normalized string\n+        (\n+            indices,\n+            offsets.into_iter().map(move |(start, end)| {\n+                sentence\n+                    .slice_bytes(Range::Normalized(start..end))\n+                    .expect(\"Error while extracting normalized Range\")",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "463149896",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 360,
        "pr_file": "tokenizers/src/tokenizer/added_vocabulary.rs",
        "discussion_id": "463149896",
        "commented_code": "@@ -376,35 +379,46 @@ impl AddedVocabulary {\n                 if start_offset < start {\n                     splits.push((None, (start_offset, start)));\n                 }\n-                splits.push((Some(idx), (start, end)));\n+                splits.push((Some(split_re.1[idx] as u32), (start, end)));\n                 start_offset = end;\n \n                 splits\n             })\n             .collect::<Vec<_>>();\n         if let Some((_, (_, end))) = splits.iter().last().copied() {\n-            if end < sentence.get().len() {\n-                splits.push((None, (end, sentence.get().len())));\n+            if end < sentence.len() {\n+                splits.push((None, (end, sentence.len())));\n             }\n         }\n-\n         if splits.is_empty() {\n-            vec![(sentence, None)]\n-        } else {\n-            splits\n-                .into_iter()\n-                .map(|(idx, (start, end))| {\n-                    let normalized = sentence\n-                        .slice_bytes(Range::Normalized(start..end))\n-                        .expect(\"Error while extracting normalized Range\");\n+            splits.push((None, (0, sentence.len())));\n+        }\n \n-                    // Find out the associated AddedToken, and its id\n-                    let id = idx.map(|idx| split_re.1[idx]);\n+        splits\n+    }\n \n-                    (normalized, id)\n-                })\n-                .collect()\n-        }\n+    /// Split the input sentence to extract anything we found from the `MatchingSet`, as well as\n+    /// the list of corresponding IDs\n+    /// The list of IDs have the exact same number of elements than the Iterator.\n+    fn split_with_indices(\n+        &self,\n+        sentence: NormalizedString,\n+        split_re: &MatchingSet,\n+    ) -> (Vec<Option<u32>>, impl Iterator<Item = NormalizedString>) {\n+        let (indices, offsets) = self\n+            .find_matches(sentence.get(), split_re)\n+            .into_iter()\n+            .unzip::<_, _, Vec<_>, Vec<_>>();\n+\n+        // Return the indices and the split normalized string\n+        (\n+            indices,\n+            offsets.into_iter().map(move |(start, end)| {\n+                sentence\n+                    .slice_bytes(Range::Normalized(start..end))\n+                    .expect(\"Error while extracting normalized Range\")",
        "comment_created_at": "2020-07-30T17:16:26+00:00",
        "comment_author": "sebpuetz",
        "comment_body": "I'm getting panics from this `expect` when running the batch-encode-bench, so there must be some bug hidden that's not covered by the tests.\r\n\r\nThe benchmark currently doesn't compile since `WhiteSpace` is not an empty struct anymore, so you need to change the constructor to `WhiteSpace::default()` over there.\r\n\r\nSimilar to the other part, if this can be encountered during \"normal\" usage, I'd prefer to see some error handling here since it's much nicer as a library user.",
        "pr_file_module": null
      }
    ]
  }
]