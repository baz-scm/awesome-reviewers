[
  {
    "discussion_id": "805869227",
    "pr_number": 904,
    "pr_file": "tokenizers/src/tokenizer/mod.rs",
    "created_at": "2022-02-14T13:59:36+00:00",
    "commented_code": "let tokenizer = serde_json::from_str(&content)?;\n         Ok(tokenizer)\n     }\n+    pub fn from_string(content: String) -> Result<Self> {\n+        let tokenizer = serde_json::from_str(&content)?;\n+        Ok(tokenizer)\n+    }",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "805869227",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 904,
        "pr_file": "tokenizers/src/tokenizer/mod.rs",
        "discussion_id": "805869227",
        "commented_code": "@@ -406,6 +406,10 @@ impl Tokenizer {\n         let tokenizer = serde_json::from_str(&content)?;\n         Ok(tokenizer)\n     }\n+    pub fn from_string(content: String) -> Result<Self> {\n+        let tokenizer = serde_json::from_str(&content)?;\n+        Ok(tokenizer)\n+    }",
        "comment_created_at": "2022-02-14T13:59:36+00:00",
        "comment_author": "Narsil",
        "comment_body": "```suggestion\r\n    pub fn from_string(content: &str) -> Result<Self> {\r\n        serde_json::from_str(content)\r\n    }\r\n```\r\n\r\nSeems simpler:\r\n\r\nNo need to pass owned data, no `?`+   `Ok`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "779547744",
    "pr_number": 870,
    "pr_file": "tokenizers/src/models/wordlevel/mod.rs",
    "created_at": "2022-01-06T13:31:31+00:00",
    "commented_code": ".get(&*token)\n                 .or_else(|| self.vocab.get(&*self.unk_token))\n                 .ok_or(Error::MissingUnkToken)?,\n-            value: token.to_owned(),\n+            value: self\n+                .vocab\n+                .get(&*token)\n+                .map_or(self.unk_token.clone(), |_| token.to_owned()),",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "779547744",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 870,
        "pr_file": "tokenizers/src/models/wordlevel/mod.rs",
        "discussion_id": "779547744",
        "commented_code": "@@ -177,7 +177,10 @@ impl Model for WordLevel {\n                 .get(&*token)\n                 .or_else(|| self.vocab.get(&*self.unk_token))\n                 .ok_or(Error::MissingUnkToken)?,\n-            value: token.to_owned(),\n+            value: self\n+                .vocab\n+                .get(&*token)\n+                .map_or(self.unk_token.clone(), |_| token.to_owned()),",
        "comment_created_at": "2022-01-06T13:31:31+00:00",
        "comment_author": "Narsil",
        "comment_body": "Other option which feel slightly cleaner IMO:\r\n\r\n```rust\r\n        if let Some(&id) = self.vocab.get(token) {\r\n            Ok(vec![Token {\r\n                id,\r\n                value: token.to_owned(),\r\n                offsets: (0, token.len()),\r\n            }])\r\n        } else if let Some(&unk_id) = self.vocab.get(&self.unk_token) {\r\n            Ok(vec![Token {\r\n                id: unk_id,\r\n                value: self.unk_token.to_owned(),\r\n                offsets: (0, token.len()),\r\n            }])\r\n        } else {\r\n            Err(Box::new(Error::MissingUnkToken))\r\n        }\r\n```\r\nIt's personal taste, probably gets compiled the exact same way.",
        "pr_file_module": null
      },
      {
        "comment_id": "779595984",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 870,
        "pr_file": "tokenizers/src/models/wordlevel/mod.rs",
        "discussion_id": "779547744",
        "commented_code": "@@ -177,7 +177,10 @@ impl Model for WordLevel {\n                 .get(&*token)\n                 .or_else(|| self.vocab.get(&*self.unk_token))\n                 .ok_or(Error::MissingUnkToken)?,\n-            value: token.to_owned(),\n+            value: self\n+                .vocab\n+                .get(&*token)\n+                .map_or(self.unk_token.clone(), |_| token.to_owned()),",
        "comment_created_at": "2022-01-06T14:43:22+00:00",
        "comment_author": "mishig25",
        "comment_body": "indeed, it looks cleaner\r\nin commit https://github.com/huggingface/tokenizers/pull/870/commits/a566cde962a47e331eeb7d8cff472c03784e1432,\r\n1. made the suggested change\r\n2. added a unit cases that tests the error",
        "pr_file_module": null
      },
      {
        "comment_id": "779611474",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 870,
        "pr_file": "tokenizers/src/models/wordlevel/mod.rs",
        "discussion_id": "779547744",
        "commented_code": "@@ -177,7 +177,10 @@ impl Model for WordLevel {\n                 .get(&*token)\n                 .or_else(|| self.vocab.get(&*self.unk_token))\n                 .ok_or(Error::MissingUnkToken)?,\n-            value: token.to_owned(),\n+            value: self\n+                .vocab\n+                .get(&*token)\n+                .map_or(self.unk_token.clone(), |_| token.to_owned()),",
        "comment_created_at": "2022-01-06T15:03:52+00:00",
        "comment_author": "Narsil",
        "comment_body": "Perfect, I'll go ahead an merge.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "774069760",
    "pr_number": 841,
    "pr_file": "bindings/node/native/src/encoding.rs",
    "created_at": "2021-12-22T17:54:54+00:00",
    "commented_code": "}\n \n         method truncate(mut cx) {\n-            // truncate(length: number, stride: number = 0)\n+            // truncate(length: number, stride: number = 0, direction: string = 'right')\n \n             let length = cx.extract::<usize>(0)?;\n             let stride = cx.extract_opt::<usize>(1)?.unwrap_or(0);\n+            let direction = cx.extract_opt::<String>(2)?.unwrap_or_else(|| String::from(\"right\"));\n+\n+            if direction != \"right\" && direction != \"left\" {\n+                panic!(\"Invalid truncation direction value : {}\", direction);\n+            }\n \n+            let tdir = if direction == \"right\" {\n+                TruncateDirection::Right\n+            } else {\n+                TruncateDirection::Left\n+            };",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "774069760",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 841,
        "pr_file": "bindings/node/native/src/encoding.rs",
        "discussion_id": "774069760",
        "commented_code": "@@ -340,16 +342,26 @@ declare_types! {\n         }\n \n         method truncate(mut cx) {\n-            // truncate(length: number, stride: number = 0)\n+            // truncate(length: number, stride: number = 0, direction: string = 'right')\n \n             let length = cx.extract::<usize>(0)?;\n             let stride = cx.extract_opt::<usize>(1)?.unwrap_or(0);\n+            let direction = cx.extract_opt::<String>(2)?.unwrap_or_else(|| String::from(\"right\"));\n+\n+            if direction != \"right\" && direction != \"left\" {\n+                panic!(\"Invalid truncation direction value : {}\", direction);\n+            }\n \n+            let tdir = if direction == \"right\" {\n+                TruncateDirection::Right\n+            } else {\n+                TruncateDirection::Left\n+            };",
        "comment_created_at": "2021-12-22T17:54:54+00:00",
        "comment_author": "Narsil",
        "comment_body": "```suggestion\r\n            let direction = match direction.as_str(){\r\n                \"left\"  => Truncate::Left,\r\n                \"right\" => Truncate::Right,\r\n                other => panic!(\"Invalid truncation direction value : {}\", other);\r\n            };\r\n```\r\nSeems slightly more rusty (and it should compile)",
        "pr_file_module": null
      },
      {
        "comment_id": "774093846",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 841,
        "pr_file": "bindings/node/native/src/encoding.rs",
        "discussion_id": "774069760",
        "commented_code": "@@ -340,16 +342,26 @@ declare_types! {\n         }\n \n         method truncate(mut cx) {\n-            // truncate(length: number, stride: number = 0)\n+            // truncate(length: number, stride: number = 0, direction: string = 'right')\n \n             let length = cx.extract::<usize>(0)?;\n             let stride = cx.extract_opt::<usize>(1)?.unwrap_or(0);\n+            let direction = cx.extract_opt::<String>(2)?.unwrap_or_else(|| String::from(\"right\"));\n+\n+            if direction != \"right\" && direction != \"left\" {\n+                panic!(\"Invalid truncation direction value : {}\", direction);\n+            }\n \n+            let tdir = if direction == \"right\" {\n+                TruncateDirection::Right\n+            } else {\n+                TruncateDirection::Left\n+            };",
        "comment_created_at": "2021-12-22T18:41:31+00:00",
        "comment_author": "McPatate",
        "comment_body": "Thanks for the review, it indeed looks much cleaner !",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "489404487",
    "pr_number": 417,
    "pr_file": "tokenizers/src/normalizers/replace.rs",
    "created_at": "2020-09-16T12:40:39+00:00",
    "commented_code": "+use crate::tokenizer::{NormalizedString, Normalizer, Result};\n+use serde::{Deserialize, Serialize};\n+\n+/// This normalizer will take a `pattern` (for now only a String)\n+/// and replace every occurrence with `content`.\n+#[derive(Deserialize, Serialize, Clone, Debug)]\n+#[serde(tag = \"type\")]\n+pub struct Replace {\n+    pattern: String,\n+    content: String,\n+}\n+\n+impl Replace {\n+    pub fn new(pattern: String, content: String) -> Self {\n+        Self { pattern, content }\n+    }\n+}\n+\n+impl Normalizer for Replace {\n+    fn normalize(&self, normalized: &mut NormalizedString) -> Result<()> {\n+        let pattern: &str = &self.pattern;\n+        normalized.replace(pattern, &self.content)?;",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "489404487",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 417,
        "pr_file": "tokenizers/src/normalizers/replace.rs",
        "discussion_id": "489404487",
        "commented_code": "@@ -0,0 +1,43 @@\n+use crate::tokenizer::{NormalizedString, Normalizer, Result};\n+use serde::{Deserialize, Serialize};\n+\n+/// This normalizer will take a `pattern` (for now only a String)\n+/// and replace every occurrence with `content`.\n+#[derive(Deserialize, Serialize, Clone, Debug)]\n+#[serde(tag = \"type\")]\n+pub struct Replace {\n+    pattern: String,\n+    content: String,\n+}\n+\n+impl Replace {\n+    pub fn new(pattern: String, content: String) -> Self {\n+        Self { pattern, content }\n+    }\n+}\n+\n+impl Normalizer for Replace {\n+    fn normalize(&self, normalized: &mut NormalizedString) -> Result<()> {\n+        let pattern: &str = &self.pattern;\n+        normalized.replace(pattern, &self.content)?;",
        "comment_created_at": "2020-09-16T12:40:39+00:00",
        "comment_author": "n1t0",
        "comment_body": "You can directly return `normalized.replace(...)` here.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "463117578",
    "pr_number": 360,
    "pr_file": "tokenizers/src/tokenizer/pre_tokenizer.rs",
    "created_at": "2020-07-30T16:22:19+00:00",
    "commented_code": "+use crate::{NormalizedString, Offsets, Result};\n+\n+/// Wrapper for a subpart of a `NormalizedString`.\n+///\n+/// This SubString contains the underlying `NormalizedString` as well as its offsets\n+/// in the original string. These offsets are in the `original` referential\n+#[derive(Debug)]\n+pub struct SubString {\n+    /// The underlying `NormalizedString`. Each SubString is represented by a `NormalizedString`\n+    /// and in the end we might be carrying a lot of SubString representing various parts of the\n+    /// original input string.\n+    pub normalized: NormalizedString,\n+    /// Offsets of the `NormalizedString` in the `original` input string. These are useful to find\n+    /// the `original` offsets in the input string, as opposed to the `original` offsets in the\n+    /// sub-part of the input string represented by `NormalizedString`\n+    pub original_offsets: Offsets,\n+}\n+\n+/// A `PreTokenizedString` takes care of splitting the input string in multiple\n+/// sub strings, while ensuring that they form a coherend group. This let us keep\n+/// track of the offsets during the whole normalization and pre-tokenization steps.\n+#[derive(Debug)]\n+pub struct PreTokenizedString {\n+    parts: Vec<SubString>,\n+}\n+\n+impl PreTokenizedString {\n+    /// Split the `PreTokenizedString` by providing a `split_fn` in charge of splitting\n+    /// each substring (`NormalizedString`) into multiple parts.\n+    ///\n+    /// `split_fn` takes a `NormalizedString` and is in charge of returning an iterator\n+    /// over the produced `NormalizedString`. `split_fn` is free of modifying these\n+    /// `NormalizedString` as relevant, as long as it respects the constraint stated below.\n+    ///\n+    /// There are only one constraint that *MUST* be respected:\n+    /// > The produced `NormalizedString`, if combined back together, must have the\n+    /// same `original` string as the original one given to `split_fn`. This concretely\n+    /// means that for the offset tracking to work as expected, `split_fn` must produce\n+    /// \"splits\" of the original string.\n+    pub fn split<F, U>(&mut self, mut split_fn: F) -> Result<()>\n+    where\n+        F: FnMut(usize, NormalizedString) -> Result<U>,\n+        U: IntoIterator<Item = NormalizedString>,\n+    {\n+        self.parts = self",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "463117578",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 360,
        "pr_file": "tokenizers/src/tokenizer/pre_tokenizer.rs",
        "discussion_id": "463117578",
        "commented_code": "@@ -0,0 +1,179 @@\n+use crate::{NormalizedString, Offsets, Result};\n+\n+/// Wrapper for a subpart of a `NormalizedString`.\n+///\n+/// This SubString contains the underlying `NormalizedString` as well as its offsets\n+/// in the original string. These offsets are in the `original` referential\n+#[derive(Debug)]\n+pub struct SubString {\n+    /// The underlying `NormalizedString`. Each SubString is represented by a `NormalizedString`\n+    /// and in the end we might be carrying a lot of SubString representing various parts of the\n+    /// original input string.\n+    pub normalized: NormalizedString,\n+    /// Offsets of the `NormalizedString` in the `original` input string. These are useful to find\n+    /// the `original` offsets in the input string, as opposed to the `original` offsets in the\n+    /// sub-part of the input string represented by `NormalizedString`\n+    pub original_offsets: Offsets,\n+}\n+\n+/// A `PreTokenizedString` takes care of splitting the input string in multiple\n+/// sub strings, while ensuring that they form a coherend group. This let us keep\n+/// track of the offsets during the whole normalization and pre-tokenization steps.\n+#[derive(Debug)]\n+pub struct PreTokenizedString {\n+    parts: Vec<SubString>,\n+}\n+\n+impl PreTokenizedString {\n+    /// Split the `PreTokenizedString` by providing a `split_fn` in charge of splitting\n+    /// each substring (`NormalizedString`) into multiple parts.\n+    ///\n+    /// `split_fn` takes a `NormalizedString` and is in charge of returning an iterator\n+    /// over the produced `NormalizedString`. `split_fn` is free of modifying these\n+    /// `NormalizedString` as relevant, as long as it respects the constraint stated below.\n+    ///\n+    /// There are only one constraint that *MUST* be respected:\n+    /// > The produced `NormalizedString`, if combined back together, must have the\n+    /// same `original` string as the original one given to `split_fn`. This concretely\n+    /// means that for the offset tracking to work as expected, `split_fn` must produce\n+    /// \"splits\" of the original string.\n+    pub fn split<F, U>(&mut self, mut split_fn: F) -> Result<()>\n+    where\n+        F: FnMut(usize, NormalizedString) -> Result<U>,\n+        U: IntoIterator<Item = NormalizedString>,\n+    {\n+        self.parts = self",
        "comment_created_at": "2020-07-30T16:22:19+00:00",
        "comment_author": "sebpuetz",
        "comment_body": "This could be more readable (and save some allocations) by writing the outter loop as a `for` loop and extending a new vec:\r\n\r\n```Rust\r\n        // new_parts is at least as big as self.parts\r\n        let mut new_parts = Vec::with_capacity(self.parts.len());\r\n        for (i, sub) in self.parts.drain(..).enumerate() {\r\n            let original_len = sub.normalized.len_original();\r\n            let original_offsets = sub.original_offsets;\r\n\r\n            let mut new_len = 0;\r\n            new_parts.extend(split_fn(i, sub.normalized)?.into_iter()\r\n                .map(|normalized| {\r\n                    let len = normalized.len_original();\r\n                    let start = original_offsets.0 + new_len;\r\n                    let end = original_offsets.0 + new_len + len;\r\n                    let new_s = SubString::new(normalized, (start, end));\r\n                    new_len += len;\r\n                    new_s\r\n            }));\r\n            if original_len != new_len {\r\n                println!(\r\n                    \"Original offsets: {:?}\\nNew: {:?}\",\r\n                    (0, original_len),\r\n                    (0, new_len)\r\n                );\r\n                return Err(\r\n                    \"Split pre-tokenized string must represent the entire original string\".into()\r\n                )\r\n            }\r\n        }\r\n        self.parts = new_parts;\r\n```\r\n\r\nThis assumes that `SubString` gets a `new()` constructor ;)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "463159413",
    "pr_number": 360,
    "pr_file": "tokenizers/src/tokenizer/mod.rs",
    "created_at": "2020-07-30T17:33:03+00:00",
    "commented_code": "InputSequence::Raw(seq) => (vec![seq], false),\n         };\n \n-        let mut sequence_encodings = vec![];\n-        for subseq in sequence {\n-            let results = self\n-                .added_vocabulary\n-                .extract_and_normalize(self.normalizer.as_deref(), &subseq)\n-                .into_iter()\n-                .map(\n-                    |(mut normalized, id)| -> Result<(Encoding, NormalizedString)> {\n-                        if let Some(id) = id {\n-                            Ok((\n-                                Encoding::new(\n-                                    vec![id],\n-                                    vec![type_id],\n-                                    vec![normalized.get().to_owned()],\n-                                    vec![Some(0)],\n-                                    vec![(0, normalized.len())],\n-                                    vec![0],\n-                                    vec![1],\n-                                    vec![],\n-                                ),\n-                                normalized,\n-                            ))\n-                        } else {\n-                            // 1. Pre tokenization\n-                            let pre_tokenized = self.pre_tokenize(&mut normalized)?;\n-                            // 2. Model\n-                            let tokens = self.model.tokenize(pre_tokenized)?;\n-                            let encoding = Encoding::from_tokens(tokens, type_id);\n-\n-                            Ok((encoding, normalized))\n+        sequence\n+            .into_iter()\n+            .enumerate()\n+            .map(|(subseq_idx, subseq)| {\n+                let encodings = self\n+                    .added_vocabulary\n+                    .extract_and_normalize(self.normalizer.as_deref(), &subseq)\n+                    .map(|(normalized, original_offsets, id)| match id {",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "463159413",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 360,
        "pr_file": "tokenizers/src/tokenizer/mod.rs",
        "discussion_id": "463159413",
        "commented_code": "@@ -414,72 +415,61 @@ impl Tokenizer {\n             InputSequence::Raw(seq) => (vec![seq], false),\n         };\n \n-        let mut sequence_encodings = vec![];\n-        for subseq in sequence {\n-            let results = self\n-                .added_vocabulary\n-                .extract_and_normalize(self.normalizer.as_deref(), &subseq)\n-                .into_iter()\n-                .map(\n-                    |(mut normalized, id)| -> Result<(Encoding, NormalizedString)> {\n-                        if let Some(id) = id {\n-                            Ok((\n-                                Encoding::new(\n-                                    vec![id],\n-                                    vec![type_id],\n-                                    vec![normalized.get().to_owned()],\n-                                    vec![Some(0)],\n-                                    vec![(0, normalized.len())],\n-                                    vec![0],\n-                                    vec![1],\n-                                    vec![],\n-                                ),\n-                                normalized,\n-                            ))\n-                        } else {\n-                            // 1. Pre tokenization\n-                            let pre_tokenized = self.pre_tokenize(&mut normalized)?;\n-                            // 2. Model\n-                            let tokens = self.model.tokenize(pre_tokenized)?;\n-                            let encoding = Encoding::from_tokens(tokens, type_id);\n-\n-                            Ok((encoding, normalized))\n+        sequence\n+            .into_iter()\n+            .enumerate()\n+            .map(|(subseq_idx, subseq)| {\n+                let encodings = self\n+                    .added_vocabulary\n+                    .extract_and_normalize(self.normalizer.as_deref(), &subseq)\n+                    .map(|(normalized, original_offsets, id)| match id {",
        "comment_created_at": "2020-07-30T17:33:03+00:00",
        "comment_author": "sebpuetz",
        "comment_body": "I think this could be made more readable by adding some intermediate variables. I also have a hard time understanding what the optional `id` refers to, the docs state that it signals whether it is an `additional token`. Maybe you can add a small comment on the `Some(id)` match arm?",
        "pr_file_module": null
      }
    ]
  }
]