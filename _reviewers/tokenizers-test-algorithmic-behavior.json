[
  {
    "discussion_id": "1923272737",
    "pr_number": 1713,
    "pr_file": "bindings/python/tests/bindings/test_pre_tokenizers.py",
    "created_at": "2025-01-21T08:29:57+00:00",
    "commented_code": "assert pretok.individual_digits == True\n \n \n+class TestFixedLength:\n+    def test_instantiate(self):\n+        assert FixedLength() is not None\n+        assert isinstance(FixedLength(), PreTokenizer)\n+        assert isinstance(FixedLength(), FixedLength)\n+        assert isinstance(pickle.loads(pickle.dumps(FixedLength())), FixedLength)\n+\n+    def test_can_modify(self):\n+        pretok = FixedLength(length=5)\n+        assert pretok.length == 5\n+\n+        pretok.length = 10\n+        assert pretok.length == 10\n+",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "1923272737",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1713,
        "pr_file": "bindings/python/tests/bindings/test_pre_tokenizers.py",
        "discussion_id": "1923272737",
        "commented_code": "@@ -195,6 +196,21 @@ def test_can_modify(self):\n         assert pretok.individual_digits == True\n \n \n+class TestFixedLength:\n+    def test_instantiate(self):\n+        assert FixedLength() is not None\n+        assert isinstance(FixedLength(), PreTokenizer)\n+        assert isinstance(FixedLength(), FixedLength)\n+        assert isinstance(pickle.loads(pickle.dumps(FixedLength())), FixedLength)\n+\n+    def test_can_modify(self):\n+        pretok = FixedLength(length=5)\n+        assert pretok.length == 5\n+\n+        pretok.length = 10\n+        assert pretok.length == 10\n+",
        "comment_created_at": "2025-01-21T08:29:57+00:00",
        "comment_author": "ArthurZucker",
        "comment_body": "we'd also want to make sure that it does it's job as a pretokenizer! so testing with the same string, that it splits in 5 then 10! ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "673859190",
    "pr_number": 762,
    "pr_file": "bindings/python/tests/implementations/test_sentencepiece.py",
    "created_at": "2021-07-21T10:35:48+00:00",
    "commented_code": "class TestSentencePieceUnigram:\n+    def test_train(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\nAnother sentence\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(files=str(p), show_progress=False)\n+\n+        output = tokenizer.encode(\"A sentence\")\n+        assert output.tokens == [\"\u2581A\", \"\u2581\", \"s\", \"en\", \"t\", \"en\", \"c\", \"e\"]\n+\n+    def test_train_with_unk_token(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\nAnother sentence\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(\n+            files=str(p), show_progress=False, special_tokens=[\"<unk>\"], unk_token=\"<unk>\"\n+        )\n+        output = tokenizer.encode(\"A sentence \ud83e\udd17\")\n+        assert output.ids[-1] == 0",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "673859190",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 762,
        "pr_file": "bindings/python/tests/implementations/test_sentencepiece.py",
        "discussion_id": "673859190",
        "commented_code": "@@ -14,10 +16,40 @@ def test_train_from_iterator(self):\n \n \n class TestSentencePieceUnigram:\n+    def test_train(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(files=str(p), show_progress=False)\n+\n+        output = tokenizer.encode(\"A sentence\")\n+        assert output.tokens == [\"\u2581A\", \"\u2581\", \"s\", \"en\", \"t\", \"en\", \"c\", \"e\"]\n+\n+    def test_train_with_unk_token(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(\n+            files=str(p), show_progress=False, special_tokens=[\"<unk>\"], unk_token=\"<unk>\"\n+        )\n+        output = tokenizer.encode(\"A sentence \ud83e\udd17\")\n+        assert output.ids[-1] == 0",
        "comment_created_at": "2021-07-21T10:35:48+00:00",
        "comment_author": "Narsil",
        "comment_body": "Could you assert the full list of `ids` and `tokens` (like the other test ?)",
        "pr_file_module": null
      },
      {
        "comment_id": "673923488",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 762,
        "pr_file": "bindings/python/tests/implementations/test_sentencepiece.py",
        "discussion_id": "673859190",
        "commented_code": "@@ -14,10 +16,40 @@ def test_train_from_iterator(self):\n \n \n class TestSentencePieceUnigram:\n+    def test_train(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(files=str(p), show_progress=False)\n+\n+        output = tokenizer.encode(\"A sentence\")\n+        assert output.tokens == [\"\u2581A\", \"\u2581\", \"s\", \"en\", \"t\", \"en\", \"c\", \"e\"]\n+\n+    def test_train_with_unk_token(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(\n+            files=str(p), show_progress=False, special_tokens=[\"<unk>\"], unk_token=\"<unk>\"\n+        )\n+        output = tokenizer.encode(\"A sentence \ud83e\udd17\")\n+        assert output.ids[-1] == 0",
        "comment_created_at": "2021-07-21T12:23:06+00:00",
        "comment_author": "SaulLu",
        "comment_body": "Regarding the `ids` assert, this is what I initially wanted to do but the algorithm doesn't always calculate exactly the same vocabulary: when I run this test several times one id in the id list changes ~~(between `7` and `8`)~~. \r\n\r\n~~Concretely, the output of `print(output.ids, output.tokens)` is sometimes:~~\r\n```\r\n[12, 1, 4, 5, 2, 5, 7, 3, 1, 0] ['\u2581A', '\u2581', 's', 'en', 't', 'en', 'c', 'e', '\u2581', '\ud83e\udd17']\r\n```\r\n~~and other times:~~\r\n```\r\n[12, 1, 4, 5, 2, 5, 8, 3, 1, 0] ['\u2581A', '\u2581', 's', 'en', 't', 'en', 'c', 'e', '\u2581', '\ud83e\udd17']\r\n```\r\n\r\n~~Would it be okay if I replaced `assert output.ids[-1] == 0` with `assert (output.ids == [12, 1, 4, 5, 2, 5, 7, 3, 1, 0] or output.ids == [12, 1, 4, 5, 2, 5, 8, 3, 1, 0])`?~~ :slightly_smiling_face: \r\n\r\nEDIT:\r\n\r\nWhile testing what I just proposed, I realized that the id that changed could not only take as value `7` or `8` but also other. How do you think it is better to manage this?\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "673934404",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 762,
        "pr_file": "bindings/python/tests/implementations/test_sentencepiece.py",
        "discussion_id": "673859190",
        "commented_code": "@@ -14,10 +16,40 @@ def test_train_from_iterator(self):\n \n \n class TestSentencePieceUnigram:\n+    def test_train(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(files=str(p), show_progress=False)\n+\n+        output = tokenizer.encode(\"A sentence\")\n+        assert output.tokens == [\"\u2581A\", \"\u2581\", \"s\", \"en\", \"t\", \"en\", \"c\", \"e\"]\n+\n+    def test_train_with_unk_token(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(\n+            files=str(p), show_progress=False, special_tokens=[\"<unk>\"], unk_token=\"<unk>\"\n+        )\n+        output = tokenizer.encode(\"A sentence \ud83e\udd17\")\n+        assert output.ids[-1] == 0",
        "comment_created_at": "2021-07-21T12:39:26+00:00",
        "comment_author": "SaulLu",
        "comment_body": "Regarding the assert on the token list, the following assert fails: \r\n``` python\r\nassert output.tokens == [\"\u2581A\", \"\u2581\", \"s\", \"en\", \"t\", \"en\", \"c\", \"e\", \"_\", \"<unk>\"]\r\n```\r\nwith the error:\r\n```\r\n>       assert output.tokens == [\"\u2581A\", \"\u2581\", \"s\", \"en\", \"t\", \"en\", \"c\", \"e\", \"_\", \"<unk>\"]\r\nE       AssertionError: assert ['\u2581A', '\u2581', '...t', 'en', ...] == ['\u2581A', '\u2581', '...t', 'en', ...]\r\nE         At index 8 diff: '\u2581' != '_'\r\nE         Full diff:\r\nE         - ['\u2581A', '\u2581', 's', 'en', 't', 'en', 'c', 'e', '_', '<unk>']\r\nE         ?                                              ^    ^^^^^\r\nE         + ['\u2581A', '\u2581', 's', 'en', 't', 'en', 'c', 'e', '\u2581', '\ud83e\udd17']\r\nE         ?                                              ^    ^\r\n```\r\nIs this indeed the expected behavior?",
        "pr_file_module": null
      },
      {
        "comment_id": "674076069",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 762,
        "pr_file": "bindings/python/tests/implementations/test_sentencepiece.py",
        "discussion_id": "673859190",
        "commented_code": "@@ -14,10 +16,40 @@ def test_train_from_iterator(self):\n \n \n class TestSentencePieceUnigram:\n+    def test_train(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(files=str(p), show_progress=False)\n+\n+        output = tokenizer.encode(\"A sentence\")\n+        assert output.tokens == [\"\u2581A\", \"\u2581\", \"s\", \"en\", \"t\", \"en\", \"c\", \"e\"]\n+\n+    def test_train_with_unk_token(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(\n+            files=str(p), show_progress=False, special_tokens=[\"<unk>\"], unk_token=\"<unk>\"\n+        )\n+        output = tokenizer.encode(\"A sentence \ud83e\udd17\")\n+        assert output.ids[-1] == 0",
        "comment_created_at": "2021-07-21T15:24:34+00:00",
        "comment_author": "Narsil",
        "comment_body": "So if the tests are not deterministic definitely let's not add them.\r\n\r\nHowever it's suprising that it's not determinstic though..... It shouldn't be ....\r\n\r\n\u2581 != _  \r\nfirst is a special unicode character (rarely used that's why google chose it)\r\nSecond is regular underscore.",
        "pr_file_module": null
      },
      {
        "comment_id": "674076573",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 762,
        "pr_file": "bindings/python/tests/implementations/test_sentencepiece.py",
        "discussion_id": "673859190",
        "commented_code": "@@ -14,10 +16,40 @@ def test_train_from_iterator(self):\n \n \n class TestSentencePieceUnigram:\n+    def test_train(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(files=str(p), show_progress=False)\n+\n+        output = tokenizer.encode(\"A sentence\")\n+        assert output.tokens == [\"\u2581A\", \"\u2581\", \"s\", \"en\", \"t\", \"en\", \"c\", \"e\"]\n+\n+    def test_train_with_unk_token(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(\n+            files=str(p), show_progress=False, special_tokens=[\"<unk>\"], unk_token=\"<unk>\"\n+        )\n+        output = tokenizer.encode(\"A sentence \ud83e\udd17\")\n+        assert output.ids[-1] == 0",
        "comment_created_at": "2021-07-21T15:25:02+00:00",
        "comment_author": "Narsil",
        "comment_body": "<unk> instead of \ud83e\udd17 is indeed expected",
        "pr_file_module": null
      },
      {
        "comment_id": "674817694",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 762,
        "pr_file": "bindings/python/tests/implementations/test_sentencepiece.py",
        "discussion_id": "673859190",
        "commented_code": "@@ -14,10 +16,40 @@ def test_train_from_iterator(self):\n \n \n class TestSentencePieceUnigram:\n+    def test_train(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(files=str(p), show_progress=False)\n+\n+        output = tokenizer.encode(\"A sentence\")\n+        assert output.tokens == [\"\u2581A\", \"\u2581\", \"s\", \"en\", \"t\", \"en\", \"c\", \"e\"]\n+\n+    def test_train_with_unk_token(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(\n+            files=str(p), show_progress=False, special_tokens=[\"<unk>\"], unk_token=\"<unk>\"\n+        )\n+        output = tokenizer.encode(\"A sentence \ud83e\udd17\")\n+        assert output.ids[-1] == 0",
        "comment_created_at": "2021-07-22T13:54:23+00:00",
        "comment_author": "SaulLu",
        "comment_body": "Thanks a lot for your answer. I did not make any change for the assert on the `ids` and included the change to put the assert on the `tokens`. Can you check to see if this corresponds to what you expected ? :blush: ",
        "pr_file_module": null
      },
      {
        "comment_id": "675346180",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 762,
        "pr_file": "bindings/python/tests/implementations/test_sentencepiece.py",
        "discussion_id": "673859190",
        "commented_code": "@@ -14,10 +16,40 @@ def test_train_from_iterator(self):\n \n \n class TestSentencePieceUnigram:\n+    def test_train(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(files=str(p), show_progress=False)\n+\n+        output = tokenizer.encode(\"A sentence\")\n+        assert output.tokens == [\"\u2581A\", \"\u2581\", \"s\", \"en\", \"t\", \"en\", \"c\", \"e\"]\n+\n+    def test_train_with_unk_token(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(\n+            files=str(p), show_progress=False, special_tokens=[\"<unk>\"], unk_token=\"<unk>\"\n+        )\n+        output = tokenizer.encode(\"A sentence \ud83e\udd17\")\n+        assert output.ids[-1] == 0",
        "comment_created_at": "2021-07-23T07:00:48+00:00",
        "comment_author": "Narsil",
        "comment_body": "That's better, and OK I understand better why IDs are not deterministic, they essentially have the same score, so no particular reason that ids should be in a particular order.",
        "pr_file_module": null
      },
      {
        "comment_id": "675433444",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 762,
        "pr_file": "bindings/python/tests/implementations/test_sentencepiece.py",
        "discussion_id": "673859190",
        "commented_code": "@@ -14,10 +16,40 @@ def test_train_from_iterator(self):\n \n \n class TestSentencePieceUnigram:\n+    def test_train(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(files=str(p), show_progress=False)\n+\n+        output = tokenizer.encode(\"A sentence\")\n+        assert output.tokens == [\"\u2581A\", \"\u2581\", \"s\", \"en\", \"t\", \"en\", \"c\", \"e\"]\n+\n+    def test_train_with_unk_token(self, tmpdir):\n+        p = tmpdir.mkdir(\"tmpdir\").join(\"file.txt\")\n+        p.write(\"A first sentence\\nAnother sentence\\nAnd a last one\")\n+\n+        tokenizer = SentencePieceUnigramTokenizer()\n+        tokenizer.train(\n+            files=str(p), show_progress=False, special_tokens=[\"<unk>\"], unk_token=\"<unk>\"\n+        )\n+        output = tokenizer.encode(\"A sentence \ud83e\udd17\")\n+        assert output.ids[-1] == 0",
        "comment_created_at": "2021-07-23T09:39:24+00:00",
        "comment_author": "SaulLu",
        "comment_body": "Thanks for your feedback and for investigating the change in ids. It makes sense!",
        "pr_file_module": null
      }
    ]
  }
]