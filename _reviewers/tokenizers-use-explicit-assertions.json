[
  {
    "discussion_id": "1185846549",
    "pr_number": 1228,
    "pr_file": "tokenizers/src/models/bpe/trainer.rs",
    "created_at": "2023-05-05T08:47:44+00:00",
    "commented_code": ".collect();\n         assert_eq!(model.merges, expected_merges);\n     }\n+    #[test]\n+    fn bpe_test_max_token_length_16() {\n+        let max_token_length = 16;\n+        let long_word_counts: HashMap<String, u32> = [\n+            (\"singlelongtokenwithoutcasechange\", 2),\n+            (\"singleLongTokenWithCamelCaseChange\", 2),\n+            (\"Longsingletokenwithpunctu@t!onwithin\", 2),\n+            (\"Anotherlongsingletokenwithnumberw1th1n\", 2),\n+            (\"짧은한글문자열짧은한\", 2),             // korean 10 char\n+            (\"긴한글문자열긴한글문자열긴한글문\", 2), // korean 16 char\n+            (\"短字符串短字符串短字\", 2),             //simplified chinese 10 char\n+            (\"长字符串长字符串长字符串长字符串\", 2), // simp. chinese 16 char\n+            (\"短い文字列短い文字列\", 2),             // japanese 10 char\n+            (\"長い文字列長い文字列長い文字列長\", 2), // japanese 16 char\n+            (\"so\", 2),\n+            (\"GPT-2\", 2),\n+        ]\n+        .iter()\n+        .map(|(key, value)| (key.to_string(), *value))\n+        .collect();\n+        let trainer = BpeTrainer::builder()\n+            .max_token_length(max_token_length)\n+            .show_progress(false)\n+            .min_frequency(0)\n+            .build();\n+        let mut model = BPE::default();\n+        trainer.do_train(&long_word_counts, &mut model).unwrap();\n+        let vocab = model.get_vocab();\n+        for token in vocab.keys() {\n+            assert!(\n+                token.chars().count() <= max_token_length,\n+                \"token too long : {} , chars().count() = {}\",\n+                token,\n+                token.chars().count()",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "1185846549",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1228,
        "pr_file": "tokenizers/src/models/bpe/trainer.rs",
        "discussion_id": "1185846549",
        "commented_code": "@@ -734,4 +735,41 @@ mod tests {\n         .collect();\n         assert_eq!(model.merges, expected_merges);\n     }\n+    #[test]\n+    fn bpe_test_max_token_length_16() {\n+        let max_token_length = 16;\n+        let long_word_counts: HashMap<String, u32> = [\n+            (\"singlelongtokenwithoutcasechange\", 2),\n+            (\"singleLongTokenWithCamelCaseChange\", 2),\n+            (\"Longsingletokenwithpunctu@t!onwithin\", 2),\n+            (\"Anotherlongsingletokenwithnumberw1th1n\", 2),\n+            (\"짧은한글문자열짧은한\", 2),             // korean 10 char\n+            (\"긴한글문자열긴한글문자열긴한글문\", 2), // korean 16 char\n+            (\"短字符串短字符串短字\", 2),             //simplified chinese 10 char\n+            (\"长字符串长字符串长字符串长字符串\", 2), // simp. chinese 16 char\n+            (\"短い文字列短い文字列\", 2),             // japanese 10 char\n+            (\"長い文字列長い文字列長い文字列長\", 2), // japanese 16 char\n+            (\"so\", 2),\n+            (\"GPT-2\", 2),\n+        ]\n+        .iter()\n+        .map(|(key, value)| (key.to_string(), *value))\n+        .collect();\n+        let trainer = BpeTrainer::builder()\n+            .max_token_length(max_token_length)\n+            .show_progress(false)\n+            .min_frequency(0)\n+            .build();\n+        let mut model = BPE::default();\n+        trainer.do_train(&long_word_counts, &mut model).unwrap();\n+        let vocab = model.get_vocab();\n+        for token in vocab.keys() {\n+            assert!(\n+                token.chars().count() <= max_token_length,\n+                \"token too long : {} , chars().count() = {}\",\n+                token,\n+                token.chars().count()",
        "comment_created_at": "2023-05-05T08:47:44+00:00",
        "comment_author": "Narsil",
        "comment_body": "Thats a great test ! Good idea on the UTF-8 intense examples ! \r\n\r\nCould you add the explicit vocab as a result ? It makes the test more readable and more robust.\r\nLet's keep the functional part too, but having the explicit values prevents \"under the radar\" bugs, where behavior is modified unintentionally.|\r\n\r\n\r\nYou could reduce the size of those strings and `max_token_length` in order to keep things readable maybe.",
        "pr_file_module": null
      },
      {
        "comment_id": "1185860466",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1228,
        "pr_file": "tokenizers/src/models/bpe/trainer.rs",
        "discussion_id": "1185846549",
        "commented_code": "@@ -734,4 +735,41 @@ mod tests {\n         .collect();\n         assert_eq!(model.merges, expected_merges);\n     }\n+    #[test]\n+    fn bpe_test_max_token_length_16() {\n+        let max_token_length = 16;\n+        let long_word_counts: HashMap<String, u32> = [\n+            (\"singlelongtokenwithoutcasechange\", 2),\n+            (\"singleLongTokenWithCamelCaseChange\", 2),\n+            (\"Longsingletokenwithpunctu@t!onwithin\", 2),\n+            (\"Anotherlongsingletokenwithnumberw1th1n\", 2),\n+            (\"짧은한글문자열짧은한\", 2),             // korean 10 char\n+            (\"긴한글문자열긴한글문자열긴한글문\", 2), // korean 16 char\n+            (\"短字符串短字符串短字\", 2),             //simplified chinese 10 char\n+            (\"长字符串长字符串长字符串长字符串\", 2), // simp. chinese 16 char\n+            (\"短い文字列短い文字列\", 2),             // japanese 10 char\n+            (\"長い文字列長い文字列長い文字列長\", 2), // japanese 16 char\n+            (\"so\", 2),\n+            (\"GPT-2\", 2),\n+        ]\n+        .iter()\n+        .map(|(key, value)| (key.to_string(), *value))\n+        .collect();\n+        let trainer = BpeTrainer::builder()\n+            .max_token_length(max_token_length)\n+            .show_progress(false)\n+            .min_frequency(0)\n+            .build();\n+        let mut model = BPE::default();\n+        trainer.do_train(&long_word_counts, &mut model).unwrap();\n+        let vocab = model.get_vocab();\n+        for token in vocab.keys() {\n+            assert!(\n+                token.chars().count() <= max_token_length,\n+                \"token too long : {} , chars().count() = {}\",\n+                token,\n+                token.chars().count()",
        "comment_created_at": "2023-05-05T09:03:10+00:00",
        "comment_author": "chris-ha458",
        "comment_body": "What do you mean by keep explicit vocabulary?\nDo you mean the tokens learned by the model under that setting?\n\nIn that case I propose we make separate tests.\n\nThe current test could serve as the basis for a robust test that checks across multiple max_token_length values and added vocabulary.\nI wanted to add Arabic but it renders differently depending on the IDE (due to  left to right script) but exactly because of such behavior it would be useful to have in the future.\n\nAnother test could be added that takes a look into individual vocabularies learned, but this would be more rigid since adding more words to be learned would also change the vocabs, so we'd need to have new known good tokens to compare with.\n ",
        "pr_file_module": null
      },
      {
        "comment_id": "1185861728",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1228,
        "pr_file": "tokenizers/src/models/bpe/trainer.rs",
        "discussion_id": "1185846549",
        "commented_code": "@@ -734,4 +735,41 @@ mod tests {\n         .collect();\n         assert_eq!(model.merges, expected_merges);\n     }\n+    #[test]\n+    fn bpe_test_max_token_length_16() {\n+        let max_token_length = 16;\n+        let long_word_counts: HashMap<String, u32> = [\n+            (\"singlelongtokenwithoutcasechange\", 2),\n+            (\"singleLongTokenWithCamelCaseChange\", 2),\n+            (\"Longsingletokenwithpunctu@t!onwithin\", 2),\n+            (\"Anotherlongsingletokenwithnumberw1th1n\", 2),\n+            (\"짧은한글문자열짧은한\", 2),             // korean 10 char\n+            (\"긴한글문자열긴한글문자열긴한글문\", 2), // korean 16 char\n+            (\"短字符串短字符串短字\", 2),             //simplified chinese 10 char\n+            (\"长字符串长字符串长字符串长字符串\", 2), // simp. chinese 16 char\n+            (\"短い文字列短い文字列\", 2),             // japanese 10 char\n+            (\"長い文字列長い文字列長い文字列長\", 2), // japanese 16 char\n+            (\"so\", 2),\n+            (\"GPT-2\", 2),\n+        ]\n+        .iter()\n+        .map(|(key, value)| (key.to_string(), *value))\n+        .collect();\n+        let trainer = BpeTrainer::builder()\n+            .max_token_length(max_token_length)\n+            .show_progress(false)\n+            .min_frequency(0)\n+            .build();\n+        let mut model = BPE::default();\n+        trainer.do_train(&long_word_counts, &mut model).unwrap();\n+        let vocab = model.get_vocab();\n+        for token in vocab.keys() {\n+            assert!(\n+                token.chars().count() <= max_token_length,\n+                \"token too long : {} , chars().count() = {}\",\n+                token,\n+                token.chars().count()",
        "comment_created_at": "2023-05-05T09:04:43+00:00",
        "comment_author": "chris-ha458",
        "comment_body": "Also if we go for the multiple test route, i  definitely think one test with max_token_length set to 16 would be useful since that's the value Sentencepiece uses as default for both unigram and bpe",
        "pr_file_module": null
      },
      {
        "comment_id": "1185874869",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1228,
        "pr_file": "tokenizers/src/models/bpe/trainer.rs",
        "discussion_id": "1185846549",
        "commented_code": "@@ -734,4 +735,41 @@ mod tests {\n         .collect();\n         assert_eq!(model.merges, expected_merges);\n     }\n+    #[test]\n+    fn bpe_test_max_token_length_16() {\n+        let max_token_length = 16;\n+        let long_word_counts: HashMap<String, u32> = [\n+            (\"singlelongtokenwithoutcasechange\", 2),\n+            (\"singleLongTokenWithCamelCaseChange\", 2),\n+            (\"Longsingletokenwithpunctu@t!onwithin\", 2),\n+            (\"Anotherlongsingletokenwithnumberw1th1n\", 2),\n+            (\"짧은한글문자열짧은한\", 2),             // korean 10 char\n+            (\"긴한글문자열긴한글문자열긴한글문\", 2), // korean 16 char\n+            (\"短字符串短字符串短字\", 2),             //simplified chinese 10 char\n+            (\"长字符串长字符串长字符串长字符串\", 2), // simp. chinese 16 char\n+            (\"短い文字列短い文字列\", 2),             // japanese 10 char\n+            (\"長い文字列長い文字列長い文字列長\", 2), // japanese 16 char\n+            (\"so\", 2),\n+            (\"GPT-2\", 2),\n+        ]\n+        .iter()\n+        .map(|(key, value)| (key.to_string(), *value))\n+        .collect();\n+        let trainer = BpeTrainer::builder()\n+            .max_token_length(max_token_length)\n+            .show_progress(false)\n+            .min_frequency(0)\n+            .build();\n+        let mut model = BPE::default();\n+        trainer.do_train(&long_word_counts, &mut model).unwrap();\n+        let vocab = model.get_vocab();\n+        for token in vocab.keys() {\n+            assert!(\n+                token.chars().count() <= max_token_length,\n+                \"token too long : {} , chars().count() = {}\",\n+                token,\n+                token.chars().count()",
        "comment_created_at": "2023-05-05T09:19:30+00:00",
        "comment_author": "Narsil",
        "comment_body": "No 1 test, but explicit assert.\r\n\r\n`assert_eq!(tokenizer.get_vocab(), HashMapFrom([....]))`\r\n\r\nIt's really much better imo. Tests don't change that often, and I have seen many bugs be silent for too long for this lack of explicit value testing.",
        "pr_file_module": null
      },
      {
        "comment_id": "1186593609",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 1228,
        "pr_file": "tokenizers/src/models/bpe/trainer.rs",
        "discussion_id": "1185846549",
        "commented_code": "@@ -734,4 +735,41 @@ mod tests {\n         .collect();\n         assert_eq!(model.merges, expected_merges);\n     }\n+    #[test]\n+    fn bpe_test_max_token_length_16() {\n+        let max_token_length = 16;\n+        let long_word_counts: HashMap<String, u32> = [\n+            (\"singlelongtokenwithoutcasechange\", 2),\n+            (\"singleLongTokenWithCamelCaseChange\", 2),\n+            (\"Longsingletokenwithpunctu@t!onwithin\", 2),\n+            (\"Anotherlongsingletokenwithnumberw1th1n\", 2),\n+            (\"짧은한글문자열짧은한\", 2),             // korean 10 char\n+            (\"긴한글문자열긴한글문자열긴한글문\", 2), // korean 16 char\n+            (\"短字符串短字符串短字\", 2),             //simplified chinese 10 char\n+            (\"长字符串长字符串长字符串长字符串\", 2), // simp. chinese 16 char\n+            (\"短い文字列短い文字列\", 2),             // japanese 10 char\n+            (\"長い文字列長い文字列長い文字列長\", 2), // japanese 16 char\n+            (\"so\", 2),\n+            (\"GPT-2\", 2),\n+        ]\n+        .iter()\n+        .map(|(key, value)| (key.to_string(), *value))\n+        .collect();\n+        let trainer = BpeTrainer::builder()\n+            .max_token_length(max_token_length)\n+            .show_progress(false)\n+            .min_frequency(0)\n+            .build();\n+        let mut model = BPE::default();\n+        trainer.do_train(&long_word_counts, &mut model).unwrap();\n+        let vocab = model.get_vocab();\n+        for token in vocab.keys() {\n+            assert!(\n+                token.chars().count() <= max_token_length,\n+                \"token too long : {} , chars().count() = {}\",\n+                token,\n+                token.chars().count()",
        "comment_created_at": "2023-05-06T02:09:09+00:00",
        "comment_author": "chris-ha458",
        "comment_body": "Hmm so you feel like the test already has enough unicode coverage?\n\n\nWhat about,\n\nLeave current test (#1)\n\n change current test with explicit assert compare (#2)\n\nThis way we could add any new unicode we want on #1 and #2 can serve as explicit coverage.\n\nIs this too many tests for this feature?\nConsidering the breadth of this feature i think this warrants at least two.(hopefully three) But if you want it to be one, I'll try to pack as much into it",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "516086197",
    "pr_number": 494,
    "pr_file": "tokenizers/tests/documentation.rs",
    "created_at": "2020-11-02T16:16:56+00:00",
    "commented_code": "+use tokenizers::models::bpe::{BpeTrainer, BpeTrainerBuilder, BPE};\n+use tokenizers::normalizers::{Sequence, Strip, NFC};\n+use tokenizers::pre_tokenizers::byte_level::ByteLevel;\n+use tokenizers::{AddedToken, TokenizerBuilder};\n+use tokenizers::{DecoderWrapper, NormalizerWrapper, PostProcessorWrapper, PreTokenizerWrapper};\n+use tokenizers::{Tokenizer, TokenizerImpl};\n+\n+#[test]\n+fn train_tokenizer() {\n+    let vocab_size: usize = 100;\n+    let tokenizer = TokenizerBuilder::new()\n+        .with_model(BPE::default())\n+        .with_normalizer(Some(Sequence::new(vec![\n+            Strip::new(true, true).into(),\n+            NFC.into(),\n+        ])))\n+        .with_pre_tokenizer(Some(ByteLevel::default()))\n+        .with_post_processor(Some(ByteLevel::default()))\n+        .with_decoder(Some(ByteLevel::default()))\n+        .build()\n+        .unwrap();\n+\n+    let trainer = BpeTrainerBuilder::new()\n+        .show_progress(false)\n+        .vocab_size(vocab_size)\n+        .min_frequency(0)\n+        .special_tokens(vec![\n+            AddedToken::from(String::from(\"<s>\"), true),\n+            AddedToken::from(String::from(\"<pad>\"), true),\n+            AddedToken::from(String::from(\"</s>\"), true),\n+            AddedToken::from(String::from(\"<unk>\"), true),\n+            AddedToken::from(String::from(\"<mask>\"), true),\n+        ])\n+        .build();\n+\n+    let pretty = true;\n+    tokenizer\n+        .train(&trainer, vec![\"data/small.txt\".to_string()])\n+        .unwrap()\n+        .save(\"data/tokenizer.json\", pretty)\n+        .unwrap();\n+}\n+\n+#[test]\n+fn load_tokenizer() {\n+    let tokenizer = Tokenizer::from_file(\"data/roberta.json\").unwrap();\n+\n+    let example = \"This is an example\";\n+    let ids = vec![713, 16, 41, 1246];\n+    let tokens = vec![\"This\", \"Ġis\", \"Ġan\", \"Ġexample\"];\n+\n+    let encodings = tokenizer.encode(example, false).unwrap();\n+\n+    assert_eq!(encodings.get_ids(), ids);\n+    assert_eq!(encodings.get_tokens(), tokens);\n+\n+    let decoded = tokenizer.decode(ids, false).unwrap();\n+    assert_eq!(decoded, example);\n+}\n+\n+#[test]\n+#[ignore]\n+fn quicktour_slow_train() -> tokenizers::Result<()> {\n+    let (mut tokenizer, trainer) = quicktour_get_tokenizer_trainer()?;\n+\n+    // START quicktour_train\n+    let files = [\"test\", \"train\", \"valid\"]\n+        .iter()\n+        .map(|split| format!(\"data/wikitext-103-raw/wiki.{}.raw\", split))\n+        .collect::<Vec<_>>();\n+    tokenizer.train_and_replace(&trainer, files)?;\n+    // END quicktour_train\n+    // START quicktour_reload_model\n+    use std::path::Path;\n+    use tokenizers::Model;\n+\n+    let saved_files = tokenizer\n+        .get_model()\n+        .save(&Path::new(\"data\"), Some(\"wiki\"))?;\n+    tokenizer.with_model(\n+        BPE::from_file(\n+            saved_files[0].to_str().unwrap(),\n+            &saved_files[1].to_str().unwrap(),\n+        )\n+        .unk_token(\"[UNK]\".to_string())\n+        .build()?,\n+    );\n+    // END quicktour_reload_model\n+    // START quicktour_save\n+    tokenizer.save(\"data/tokenizer-wiki.json\", false)?;\n+    // END quicktour_save\n+\n+    Ok(())\n+}\n+\n+#[allow(unused_imports, clippy::type_complexity)]\n+fn quicktour_get_tokenizer_trainer() -> tokenizers::Result<(\n+    TokenizerImpl<\n+        BPE,\n+        NormalizerWrapper,\n+        PreTokenizerWrapper,\n+        PostProcessorWrapper,\n+        DecoderWrapper,\n+    >,\n+    BpeTrainer,\n+)> {\n+    // START quicktour_init_tokenizer\n+    use tokenizers::models::bpe::BPE;\n+    use tokenizers::TokenizerBuilder;\n+\n+    let mut tokenizer: TokenizerImpl<\n+        BPE,\n+        NormalizerWrapper,\n+        PreTokenizerWrapper,\n+        PostProcessorWrapper,\n+        DecoderWrapper,\n+    > = TokenizerImpl::new(BPE::default());\n+    // END quicktour_init_tokenizer\n+    // START quicktour_init_trainer\n+    use tokenizers::models::bpe::BpeTrainer;\n+\n+    let trainer = BpeTrainer::builder()\n+        .special_tokens(vec![\n+            AddedToken::from(\"[UNK]\", true),\n+            AddedToken::from(\"[CLS]\", true),\n+            AddedToken::from(\"[SEP]\", true),\n+            AddedToken::from(\"[PAD]\", true),\n+            AddedToken::from(\"[MASK]\", true),\n+        ])\n+        .build();\n+    // END quicktour_init_trainer\n+    // START quicktour_init_pretok\n+    use tokenizers::pre_tokenizers::whitespace::Whitespace;\n+\n+    tokenizer.with_pre_tokenizer(Whitespace::default());\n+    // END quicktour_init_pretok\n+\n+    Ok((tokenizer, trainer))\n+}\n+\n+#[test]\n+fn quicktour() -> tokenizers::Result<()> {\n+    // START quicktour_reload_tokenizer\n+    let mut tokenizer = Tokenizer::from_file(\"data/tokenizer-wiki.json\")?;\n+    // END quicktour_reload_tokenizer\n+    // START quicktour_encode\n+    let output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\", true)?;\n+    // END quicktour_encode\n+    // START quicktour_print_tokens\n+    println!(\"{:?}\", output.get_tokens());\n+    // [\"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\",]\n+    // END quicktour_print_tokens\n+    assert_eq!(\n+        output.get_tokens(),\n+        [\"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\",]\n+    );\n+    // START quicktour_print_ids\n+    println!(\"{:?}\", output.get_ids());\n+    // [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]\n+    // END quicktour_print_ids\n+    assert_eq!(\n+        output.get_ids(),\n+        [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]\n+    );\n+    // START quicktour_print_offsets\n+    println!(\"{:?}\", output.get_offsets()[9]);\n+    // (26, 30)\n+    // END quicktour_print_offsets\n+    assert_eq!(output.get_offsets()[9], (26, 30));\n+    // START quicktour_use_offsets\n+    let sentence = \"Hello, y'all! How are you 😁 ?\";\n+    println!(\"{}\", &sentence[26..30]);\n+    // \"😁\"\n+    // END quicktour_use_offsets\n+    // START quicktour_check_sep\n+    println!(\"{}\", tokenizer.token_to_id(\"[SEP]\").unwrap());\n+    // 2\n+    // END quicktour_check_sep\n+    assert_eq!(tokenizer.token_to_id(\"[SEP]\"), Some(2));\n+    // START quicktour_init_template_processing\n+    use tokenizers::processors::template::TemplateProcessing;\n+\n+    let special_tokens = vec![\n+        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\").unwrap()),\n+        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\").unwrap()),\n+    ];\n+    tokenizer.with_post_processor(\n+        TemplateProcessing::builder()\n+            .try_single(\"[CLS] $A [SEP]\")\n+            .unwrap()\n+            .try_pair(\"[CLS] $A [SEP] $B:1 [SEP]:1\")\n+            .unwrap()\n+            .special_tokens(special_tokens)\n+            .build()?,\n+    );\n+    // END quicktour_init_template_processing\n+    // START quicktour_print_special_tokens\n+    let output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\", true)?;\n+    println!(\"{:?}\", output.get_tokens());\n+    // [\"[CLS]\", \"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\"]\n+    // END quicktour_print_special_tokens\n+    assert_eq!(\n+        output.get_tokens(),\n+        [\"[CLS]\", \"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\"]\n+    );\n+    // START quicktour_print_special_tokens_pair\n+    let output = tokenizer.encode((\"Hello, y'all!\", \"How are you 😁 ?\"), true)?;\n+    println!(\"{:?}\", output.get_tokens());\n+    // [\"[CLS]\", \"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"[SEP]\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\"]\n+    // END quicktour_print_special_tokens_pair\n+    assert_eq!(\n+        output.get_tokens(),\n+        [\n+            \"[CLS]\", \"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"[SEP]\", \"How\", \"are\", \"you\", \"[UNK]\",\n+            \"?\", \"[SEP]\"\n+        ]\n+    );\n+    // START quicktour_print_type_ids\n+    println!(\"{:?}\", output.get_type_ids());\n+    // [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n+    // END quicktour_print_type_ids\n+    assert_eq!(\n+        output.get_type_ids(),\n+        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n+    );\n+    // START quicktour_encode_batch\n+    let output = tokenizer.encode_batch(vec![\"Hello, y'all!\", \"How are you 😁 ?\"], true)?;\n+    // END quicktour_encode_batch\n+    println!(\"{:?}\", output);\n+    // START quicktour_encode_batch_pair\n+    let output = tokenizer.encode_batch(\n+        vec![\n+            (\"Hello, y'all!\", \"How are you 😁 ?\"),\n+            (\"Hello to you too!\", \"I'm fine, thank you!\"),\n+        ],\n+        true,\n+    )?;\n+    // END quicktour_encode_batch_pair\n+    println!(\"{:?}\", output);\n+    // START quicktour_enable_padding\n+    use tokenizers::PaddingParams;\n+\n+    tokenizer.with_padding(Some(PaddingParams {\n+        pad_id: 3,\n+        pad_token: \"[PAD]\".to_string(),\n+        ..PaddingParams::default()\n+    }));\n+    // END quicktour_enable_padding\n+    // START quicktour_print_batch_tokens\n+    let output = tokenizer.encode_batch(vec![\"Hello, y'all!\", \"How are you 😁 ?\"], true)?;\n+    println!(\"{:?}\", output[1].get_tokens());\n+    // [\"[CLS]\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\", \"[PAD]\"]\n+    // END quicktour_print_batch_tokens\n+    assert_eq!(\n+        output[1].get_tokens(),\n+        [\"[CLS]\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\", \"[PAD]\"]\n+    );\n+    // START quicktour_print_attention_mask\n+    println!(\"{:?}\", output[1].get_attention_mask());\n+    // [1, 1, 1, 1, 1, 1, 1, 0]\n+    // END quicktour_print_attention_mask\n+    assert_eq!(output[1].get_attention_mask(), [1, 1, 1, 1, 1, 1, 1, 0]);\n+    Ok(())\n+}\n+\n+#[test]\n+fn pipeline() -> tokenizers::Result<()> {\n+    // START pipeline_reload_tokenizer\n+    use tokenizers::Tokenizer;\n+\n+    let mut tokenizer = Tokenizer::from_file(\"data/tokenizer-wiki.json\")?;\n+    // END pipeline_reload_tokenizer\n+    // START pipeline_setup_normalizer\n+    use tokenizers::normalizers::{\n+        strip::StripAccents, unicode::NFD, utils::Sequence as NormalizerSequence,\n+    };\n+\n+    let normalizer = NormalizerSequence::new(vec![NFD.into(), StripAccents.into()]);\n+    // END pipeline_setup_normalizer\n+    // START pipeline_test_normalizer\n+    use tokenizers::{NormalizedString, Normalizer};\n+\n+    let mut normalized = NormalizedString::from(\"Héllò hôw are ü?\");\n+    normalizer.normalize(&mut normalized)?;\n+\n+    println!(\"{}\", normalized.get());\n+    // \"Hello how are u?\"\n+    // END pipeline_test_normalizer\n+    assert_eq!(normalized.get(), \"Hello how are u?\");\n+    // START pipeline_replace_normalizer\n+    tokenizer.with_normalizer(normalizer);\n+    // END pipeline_replace_normalizer\n+    // START pipeline_setup_pre_tokenizer\n+    use tokenizers::pre_tokenizers::whitespace::Whitespace;\n+    use tokenizers::{OffsetReferential, OffsetType, PreTokenizedString, PreTokenizer};\n+\n+    let pre_tokenizer = Whitespace::default();\n+    let mut pre_tokenized = PreTokenizedString::from(\"Hello! How are you? I'm fine, thank you.\");\n+\n+    pre_tokenizer.pre_tokenize(&mut pre_tokenized)?;\n+\n+    println!(\n+        \"{:?}\",\n+        pre_tokenized.get_splits(OffsetReferential::Original, OffsetType::Byte)\n+    );\n+    // [(\"Hello\", (0, 5), None), (\"!\", (5, 6), None), (\"How\", (7, 10), None),\n+    //  (\"are\", (11, 14), None), (\"you\", (15, 18), None), (\"?\", (18, 19), None),\n+    //  (\"I\", (20, 21), None), (\"\\'\", (21, 22), None), (\"m\", (22, 23), None),\n+    //  (\"fine\", (24, 28), None), (\",\", (28, 29), None), (\"thank\", (30, 35), None),\n+    //  (\"you\", (36, 39), None), (\".\", (39, 40), None)]\n+    // END pipeline_setup_pre_tokenizer\n+    assert_eq!(\n+        pre_tokenized.get_splits(OffsetReferential::Original, OffsetType::Byte),\n+        vec![\n+            (\"Hello\", (0, 5), &None),\n+            (\"!\", (5, 6), &None),\n+            (\"How\", (7, 10), &None),\n+            (\"are\", (11, 14), &None),\n+            (\"you\", (15, 18), &None),\n+            (\"?\", (18, 19), &None),\n+            (\"I\", (20, 21), &None),\n+            (\"\\'\", (21, 22), &None),\n+            (\"m\", (22, 23), &None),\n+            (\"fine\", (24, 28), &None),\n+            (\",\", (28, 29), &None),\n+            (\"thank\", (30, 35), &None),\n+            (\"you\", (36, 39), &None),\n+            (\".\", (39, 40), &None)\n+        ]\n+    );\n+    // START pipeline_combine_pre_tokenizer\n+    use tokenizers::pre_tokenizers::{digits::Digits, sequence::Sequence};\n+\n+    let pre_tokenizer = Sequence::new(vec![Whitespace::default().into(), Digits::new(true).into()]);\n+    let mut pre_tokenized = PreTokenizedString::from(\"Call 911!\");\n+\n+    pre_tokenizer.pre_tokenize(&mut pre_tokenized)?;\n+\n+    println!(\n+        \"{:?}\",\n+        pre_tokenized.get_splits(OffsetReferential::Original, OffsetType::Byte)\n+    );\n+    // END pipeline_combine_pre_tokenizer\n+    assert_eq!(\n+        pre_tokenized.get_splits(OffsetReferential::Original, OffsetType::Byte),\n+        vec![\n+            (\"Call\", (0, 4), &None),\n+            (\"9\", (5, 6), &None),\n+            (\"1\", (6, 7), &None),\n+            (\"1\", (7, 8), &None),\n+            (\"!\", (8, 9), &None)\n+        ]\n+    );\n+    // START pipeline_replace_pre_tokenizer\n+    tokenizer.with_pre_tokenizer(pre_tokenizer);\n+    // END pipeline_replace_pre_tokenizer\n+    // START pipeline_setup_processor\n+    use tokenizers::processors::template::TemplateProcessing;\n+\n+    tokenizer.with_post_processor(\n+        TemplateProcessing::builder()\n+            .try_single(\"[CLS] $A [SEP]\")\n+            .unwrap()\n+            .try_pair(\"[CLS] $A [SEP] $B:1 [SEP]:1\")\n+            .unwrap()\n+            .special_tokens(vec![(\"[CLS]\", 1), (\"[SEP]\", 2)])\n+            .build()\n+            .unwrap(),\n+    );\n+    // END pipeline_setup_processor\n+    // START pipeline_test_decoding\n+    let output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\", true)?;\n+    println!(\"{:?}\", output.get_ids());\n+    // [1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2]\n+\n+    let decoded = tokenizer.decode(\n+        vec![1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2],\n+        true,\n+    )?;\n+    println!(\"{}\", decoded);\n+    // \"Hello , y ' all ! How are you ?\"\n+    // END pipeline_test_decoding\n+\n+    Ok(())\n+}\n+\n+#[test]\n+#[ignore]\n+fn pipeline_bert() -> tokenizers::Result<()> {\n+    // START bert_setup_tokenizer\n+    use tokenizers::models::wordpiece::WordPiece;\n+    use tokenizers::Tokenizer;\n+\n+    let mut bert_tokenizer = Tokenizer::new(WordPiece::default());\n+    // END bert_setup_tokenizer\n+    // START bert_setup_normalizer\n+    use tokenizers::normalizers::utils::Sequence as NormalizerSequence;\n+    use tokenizers::normalizers::{strip::StripAccents, unicode::NFD, utils::Lowercase};\n+\n+    bert_tokenizer.with_normalizer(NormalizerSequence::new(vec![\n+        NFD.into(),\n+        Lowercase.into(),\n+        StripAccents.into(),\n+    ]));\n+    // END bert_setup_normalizer\n+    // START bert_setup_pre_tokenizer\n+    use tokenizers::pre_tokenizers::whitespace::Whitespace;\n+\n+    bert_tokenizer.with_pre_tokenizer(Whitespace::default());\n+    // END bert_setup_pre_tokenizer\n+    // START bert_setup_processor\n+    use tokenizers::processors::template::TemplateProcessing;\n+\n+    bert_tokenizer.with_post_processor(\n+        TemplateProcessing::builder()\n+            .try_single(\"[CLS] $A [SEP]\")\n+            .unwrap()\n+            .try_pair(\"[CLS] $A [SEP] $B:1 [SEP]:1\")\n+            .unwrap()\n+            .special_tokens(vec![(\"[CLS]\", 1), (\"[SEP]\", 2)])\n+            .build()\n+            .unwrap(),\n+    );\n+    // END bert_setup_processor\n+    // START bert_train_tokenizer\n+    use std::path::Path;\n+    use tokenizers::models::{wordpiece::WordPieceTrainer, TrainerWrapper};\n+    use tokenizers::Model;\n+\n+    let trainer: TrainerWrapper = WordPieceTrainer::builder()\n+        .vocab_size(30_522)\n+        .special_tokens(vec![\n+            AddedToken::from(\"[UNK]\", true),\n+            AddedToken::from(\"[CLS]\", true),\n+            AddedToken::from(\"[SEP]\", true),\n+            AddedToken::from(\"[PAD]\", true),\n+            AddedToken::from(\"[MASK]\", true),\n+        ])\n+        .build()\n+        .into();\n+    let files = [\"test\", \"train\", \"valid\"]\n+        .iter()\n+        .map(|split| format!(\"data/wikitext-103-raw/wiki.{}.raw\", split))\n+        .collect::<Vec<_>>();\n+    bert_tokenizer.train_and_replace(&trainer, files)?;\n+\n+    let model_files = bert_tokenizer\n+        .get_model()\n+        .save(&Path::new(\"data\"), Some(\"bert-wiki\"))?;\n+    bert_tokenizer.with_model(\n+        WordPiece::from_file(model_files[0].to_str().unwrap())\n+            .unk_token(\"[UNK]\".to_string())\n+            .build()\n+            .unwrap(),\n+    );\n+\n+    bert_tokenizer.save(\"data/bert-wiki.json\", false)?;\n+    // END bert_train_tokenizer\n+    // START bert_test_decoding\n+    let output = bert_tokenizer.encode(\"Welcome to the 🤗 Tokenizers library.\", true)?;\n+    println!(\"{:?}\", output.get_tokens());",
    "repo_full_name": "huggingface/tokenizers",
    "discussion_comments": [
      {
        "comment_id": "516086197",
        "repo_full_name": "huggingface/tokenizers",
        "pr_number": 494,
        "pr_file": "tokenizers/tests/documentation.rs",
        "discussion_id": "516086197",
        "commented_code": "@@ -0,0 +1,477 @@\n+use tokenizers::models::bpe::{BpeTrainer, BpeTrainerBuilder, BPE};\n+use tokenizers::normalizers::{Sequence, Strip, NFC};\n+use tokenizers::pre_tokenizers::byte_level::ByteLevel;\n+use tokenizers::{AddedToken, TokenizerBuilder};\n+use tokenizers::{DecoderWrapper, NormalizerWrapper, PostProcessorWrapper, PreTokenizerWrapper};\n+use tokenizers::{Tokenizer, TokenizerImpl};\n+\n+#[test]\n+fn train_tokenizer() {\n+    let vocab_size: usize = 100;\n+    let tokenizer = TokenizerBuilder::new()\n+        .with_model(BPE::default())\n+        .with_normalizer(Some(Sequence::new(vec![\n+            Strip::new(true, true).into(),\n+            NFC.into(),\n+        ])))\n+        .with_pre_tokenizer(Some(ByteLevel::default()))\n+        .with_post_processor(Some(ByteLevel::default()))\n+        .with_decoder(Some(ByteLevel::default()))\n+        .build()\n+        .unwrap();\n+\n+    let trainer = BpeTrainerBuilder::new()\n+        .show_progress(false)\n+        .vocab_size(vocab_size)\n+        .min_frequency(0)\n+        .special_tokens(vec![\n+            AddedToken::from(String::from(\"<s>\"), true),\n+            AddedToken::from(String::from(\"<pad>\"), true),\n+            AddedToken::from(String::from(\"</s>\"), true),\n+            AddedToken::from(String::from(\"<unk>\"), true),\n+            AddedToken::from(String::from(\"<mask>\"), true),\n+        ])\n+        .build();\n+\n+    let pretty = true;\n+    tokenizer\n+        .train(&trainer, vec![\"data/small.txt\".to_string()])\n+        .unwrap()\n+        .save(\"data/tokenizer.json\", pretty)\n+        .unwrap();\n+}\n+\n+#[test]\n+fn load_tokenizer() {\n+    let tokenizer = Tokenizer::from_file(\"data/roberta.json\").unwrap();\n+\n+    let example = \"This is an example\";\n+    let ids = vec![713, 16, 41, 1246];\n+    let tokens = vec![\"This\", \"Ġis\", \"Ġan\", \"Ġexample\"];\n+\n+    let encodings = tokenizer.encode(example, false).unwrap();\n+\n+    assert_eq!(encodings.get_ids(), ids);\n+    assert_eq!(encodings.get_tokens(), tokens);\n+\n+    let decoded = tokenizer.decode(ids, false).unwrap();\n+    assert_eq!(decoded, example);\n+}\n+\n+#[test]\n+#[ignore]\n+fn quicktour_slow_train() -> tokenizers::Result<()> {\n+    let (mut tokenizer, trainer) = quicktour_get_tokenizer_trainer()?;\n+\n+    // START quicktour_train\n+    let files = [\"test\", \"train\", \"valid\"]\n+        .iter()\n+        .map(|split| format!(\"data/wikitext-103-raw/wiki.{}.raw\", split))\n+        .collect::<Vec<_>>();\n+    tokenizer.train_and_replace(&trainer, files)?;\n+    // END quicktour_train\n+    // START quicktour_reload_model\n+    use std::path::Path;\n+    use tokenizers::Model;\n+\n+    let saved_files = tokenizer\n+        .get_model()\n+        .save(&Path::new(\"data\"), Some(\"wiki\"))?;\n+    tokenizer.with_model(\n+        BPE::from_file(\n+            saved_files[0].to_str().unwrap(),\n+            &saved_files[1].to_str().unwrap(),\n+        )\n+        .unk_token(\"[UNK]\".to_string())\n+        .build()?,\n+    );\n+    // END quicktour_reload_model\n+    // START quicktour_save\n+    tokenizer.save(\"data/tokenizer-wiki.json\", false)?;\n+    // END quicktour_save\n+\n+    Ok(())\n+}\n+\n+#[allow(unused_imports, clippy::type_complexity)]\n+fn quicktour_get_tokenizer_trainer() -> tokenizers::Result<(\n+    TokenizerImpl<\n+        BPE,\n+        NormalizerWrapper,\n+        PreTokenizerWrapper,\n+        PostProcessorWrapper,\n+        DecoderWrapper,\n+    >,\n+    BpeTrainer,\n+)> {\n+    // START quicktour_init_tokenizer\n+    use tokenizers::models::bpe::BPE;\n+    use tokenizers::TokenizerBuilder;\n+\n+    let mut tokenizer: TokenizerImpl<\n+        BPE,\n+        NormalizerWrapper,\n+        PreTokenizerWrapper,\n+        PostProcessorWrapper,\n+        DecoderWrapper,\n+    > = TokenizerImpl::new(BPE::default());\n+    // END quicktour_init_tokenizer\n+    // START quicktour_init_trainer\n+    use tokenizers::models::bpe::BpeTrainer;\n+\n+    let trainer = BpeTrainer::builder()\n+        .special_tokens(vec![\n+            AddedToken::from(\"[UNK]\", true),\n+            AddedToken::from(\"[CLS]\", true),\n+            AddedToken::from(\"[SEP]\", true),\n+            AddedToken::from(\"[PAD]\", true),\n+            AddedToken::from(\"[MASK]\", true),\n+        ])\n+        .build();\n+    // END quicktour_init_trainer\n+    // START quicktour_init_pretok\n+    use tokenizers::pre_tokenizers::whitespace::Whitespace;\n+\n+    tokenizer.with_pre_tokenizer(Whitespace::default());\n+    // END quicktour_init_pretok\n+\n+    Ok((tokenizer, trainer))\n+}\n+\n+#[test]\n+fn quicktour() -> tokenizers::Result<()> {\n+    // START quicktour_reload_tokenizer\n+    let mut tokenizer = Tokenizer::from_file(\"data/tokenizer-wiki.json\")?;\n+    // END quicktour_reload_tokenizer\n+    // START quicktour_encode\n+    let output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\", true)?;\n+    // END quicktour_encode\n+    // START quicktour_print_tokens\n+    println!(\"{:?}\", output.get_tokens());\n+    // [\"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\",]\n+    // END quicktour_print_tokens\n+    assert_eq!(\n+        output.get_tokens(),\n+        [\"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\",]\n+    );\n+    // START quicktour_print_ids\n+    println!(\"{:?}\", output.get_ids());\n+    // [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]\n+    // END quicktour_print_ids\n+    assert_eq!(\n+        output.get_ids(),\n+        [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]\n+    );\n+    // START quicktour_print_offsets\n+    println!(\"{:?}\", output.get_offsets()[9]);\n+    // (26, 30)\n+    // END quicktour_print_offsets\n+    assert_eq!(output.get_offsets()[9], (26, 30));\n+    // START quicktour_use_offsets\n+    let sentence = \"Hello, y'all! How are you 😁 ?\";\n+    println!(\"{}\", &sentence[26..30]);\n+    // \"😁\"\n+    // END quicktour_use_offsets\n+    // START quicktour_check_sep\n+    println!(\"{}\", tokenizer.token_to_id(\"[SEP]\").unwrap());\n+    // 2\n+    // END quicktour_check_sep\n+    assert_eq!(tokenizer.token_to_id(\"[SEP]\"), Some(2));\n+    // START quicktour_init_template_processing\n+    use tokenizers::processors::template::TemplateProcessing;\n+\n+    let special_tokens = vec![\n+        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\").unwrap()),\n+        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\").unwrap()),\n+    ];\n+    tokenizer.with_post_processor(\n+        TemplateProcessing::builder()\n+            .try_single(\"[CLS] $A [SEP]\")\n+            .unwrap()\n+            .try_pair(\"[CLS] $A [SEP] $B:1 [SEP]:1\")\n+            .unwrap()\n+            .special_tokens(special_tokens)\n+            .build()?,\n+    );\n+    // END quicktour_init_template_processing\n+    // START quicktour_print_special_tokens\n+    let output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\", true)?;\n+    println!(\"{:?}\", output.get_tokens());\n+    // [\"[CLS]\", \"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\"]\n+    // END quicktour_print_special_tokens\n+    assert_eq!(\n+        output.get_tokens(),\n+        [\"[CLS]\", \"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\"]\n+    );\n+    // START quicktour_print_special_tokens_pair\n+    let output = tokenizer.encode((\"Hello, y'all!\", \"How are you 😁 ?\"), true)?;\n+    println!(\"{:?}\", output.get_tokens());\n+    // [\"[CLS]\", \"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"[SEP]\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\"]\n+    // END quicktour_print_special_tokens_pair\n+    assert_eq!(\n+        output.get_tokens(),\n+        [\n+            \"[CLS]\", \"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"[SEP]\", \"How\", \"are\", \"you\", \"[UNK]\",\n+            \"?\", \"[SEP]\"\n+        ]\n+    );\n+    // START quicktour_print_type_ids\n+    println!(\"{:?}\", output.get_type_ids());\n+    // [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n+    // END quicktour_print_type_ids\n+    assert_eq!(\n+        output.get_type_ids(),\n+        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n+    );\n+    // START quicktour_encode_batch\n+    let output = tokenizer.encode_batch(vec![\"Hello, y'all!\", \"How are you 😁 ?\"], true)?;\n+    // END quicktour_encode_batch\n+    println!(\"{:?}\", output);\n+    // START quicktour_encode_batch_pair\n+    let output = tokenizer.encode_batch(\n+        vec![\n+            (\"Hello, y'all!\", \"How are you 😁 ?\"),\n+            (\"Hello to you too!\", \"I'm fine, thank you!\"),\n+        ],\n+        true,\n+    )?;\n+    // END quicktour_encode_batch_pair\n+    println!(\"{:?}\", output);\n+    // START quicktour_enable_padding\n+    use tokenizers::PaddingParams;\n+\n+    tokenizer.with_padding(Some(PaddingParams {\n+        pad_id: 3,\n+        pad_token: \"[PAD]\".to_string(),\n+        ..PaddingParams::default()\n+    }));\n+    // END quicktour_enable_padding\n+    // START quicktour_print_batch_tokens\n+    let output = tokenizer.encode_batch(vec![\"Hello, y'all!\", \"How are you 😁 ?\"], true)?;\n+    println!(\"{:?}\", output[1].get_tokens());\n+    // [\"[CLS]\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\", \"[PAD]\"]\n+    // END quicktour_print_batch_tokens\n+    assert_eq!(\n+        output[1].get_tokens(),\n+        [\"[CLS]\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\", \"[PAD]\"]\n+    );\n+    // START quicktour_print_attention_mask\n+    println!(\"{:?}\", output[1].get_attention_mask());\n+    // [1, 1, 1, 1, 1, 1, 1, 0]\n+    // END quicktour_print_attention_mask\n+    assert_eq!(output[1].get_attention_mask(), [1, 1, 1, 1, 1, 1, 1, 0]);\n+    Ok(())\n+}\n+\n+#[test]\n+fn pipeline() -> tokenizers::Result<()> {\n+    // START pipeline_reload_tokenizer\n+    use tokenizers::Tokenizer;\n+\n+    let mut tokenizer = Tokenizer::from_file(\"data/tokenizer-wiki.json\")?;\n+    // END pipeline_reload_tokenizer\n+    // START pipeline_setup_normalizer\n+    use tokenizers::normalizers::{\n+        strip::StripAccents, unicode::NFD, utils::Sequence as NormalizerSequence,\n+    };\n+\n+    let normalizer = NormalizerSequence::new(vec![NFD.into(), StripAccents.into()]);\n+    // END pipeline_setup_normalizer\n+    // START pipeline_test_normalizer\n+    use tokenizers::{NormalizedString, Normalizer};\n+\n+    let mut normalized = NormalizedString::from(\"Héllò hôw are ü?\");\n+    normalizer.normalize(&mut normalized)?;\n+\n+    println!(\"{}\", normalized.get());\n+    // \"Hello how are u?\"\n+    // END pipeline_test_normalizer\n+    assert_eq!(normalized.get(), \"Hello how are u?\");\n+    // START pipeline_replace_normalizer\n+    tokenizer.with_normalizer(normalizer);\n+    // END pipeline_replace_normalizer\n+    // START pipeline_setup_pre_tokenizer\n+    use tokenizers::pre_tokenizers::whitespace::Whitespace;\n+    use tokenizers::{OffsetReferential, OffsetType, PreTokenizedString, PreTokenizer};\n+\n+    let pre_tokenizer = Whitespace::default();\n+    let mut pre_tokenized = PreTokenizedString::from(\"Hello! How are you? I'm fine, thank you.\");\n+\n+    pre_tokenizer.pre_tokenize(&mut pre_tokenized)?;\n+\n+    println!(\n+        \"{:?}\",\n+        pre_tokenized.get_splits(OffsetReferential::Original, OffsetType::Byte)\n+    );\n+    // [(\"Hello\", (0, 5), None), (\"!\", (5, 6), None), (\"How\", (7, 10), None),\n+    //  (\"are\", (11, 14), None), (\"you\", (15, 18), None), (\"?\", (18, 19), None),\n+    //  (\"I\", (20, 21), None), (\"\\'\", (21, 22), None), (\"m\", (22, 23), None),\n+    //  (\"fine\", (24, 28), None), (\",\", (28, 29), None), (\"thank\", (30, 35), None),\n+    //  (\"you\", (36, 39), None), (\".\", (39, 40), None)]\n+    // END pipeline_setup_pre_tokenizer\n+    assert_eq!(\n+        pre_tokenized.get_splits(OffsetReferential::Original, OffsetType::Byte),\n+        vec![\n+            (\"Hello\", (0, 5), &None),\n+            (\"!\", (5, 6), &None),\n+            (\"How\", (7, 10), &None),\n+            (\"are\", (11, 14), &None),\n+            (\"you\", (15, 18), &None),\n+            (\"?\", (18, 19), &None),\n+            (\"I\", (20, 21), &None),\n+            (\"\\'\", (21, 22), &None),\n+            (\"m\", (22, 23), &None),\n+            (\"fine\", (24, 28), &None),\n+            (\",\", (28, 29), &None),\n+            (\"thank\", (30, 35), &None),\n+            (\"you\", (36, 39), &None),\n+            (\".\", (39, 40), &None)\n+        ]\n+    );\n+    // START pipeline_combine_pre_tokenizer\n+    use tokenizers::pre_tokenizers::{digits::Digits, sequence::Sequence};\n+\n+    let pre_tokenizer = Sequence::new(vec![Whitespace::default().into(), Digits::new(true).into()]);\n+    let mut pre_tokenized = PreTokenizedString::from(\"Call 911!\");\n+\n+    pre_tokenizer.pre_tokenize(&mut pre_tokenized)?;\n+\n+    println!(\n+        \"{:?}\",\n+        pre_tokenized.get_splits(OffsetReferential::Original, OffsetType::Byte)\n+    );\n+    // END pipeline_combine_pre_tokenizer\n+    assert_eq!(\n+        pre_tokenized.get_splits(OffsetReferential::Original, OffsetType::Byte),\n+        vec![\n+            (\"Call\", (0, 4), &None),\n+            (\"9\", (5, 6), &None),\n+            (\"1\", (6, 7), &None),\n+            (\"1\", (7, 8), &None),\n+            (\"!\", (8, 9), &None)\n+        ]\n+    );\n+    // START pipeline_replace_pre_tokenizer\n+    tokenizer.with_pre_tokenizer(pre_tokenizer);\n+    // END pipeline_replace_pre_tokenizer\n+    // START pipeline_setup_processor\n+    use tokenizers::processors::template::TemplateProcessing;\n+\n+    tokenizer.with_post_processor(\n+        TemplateProcessing::builder()\n+            .try_single(\"[CLS] $A [SEP]\")\n+            .unwrap()\n+            .try_pair(\"[CLS] $A [SEP] $B:1 [SEP]:1\")\n+            .unwrap()\n+            .special_tokens(vec![(\"[CLS]\", 1), (\"[SEP]\", 2)])\n+            .build()\n+            .unwrap(),\n+    );\n+    // END pipeline_setup_processor\n+    // START pipeline_test_decoding\n+    let output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\", true)?;\n+    println!(\"{:?}\", output.get_ids());\n+    // [1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2]\n+\n+    let decoded = tokenizer.decode(\n+        vec![1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2],\n+        true,\n+    )?;\n+    println!(\"{}\", decoded);\n+    // \"Hello , y ' all ! How are you ?\"\n+    // END pipeline_test_decoding\n+\n+    Ok(())\n+}\n+\n+#[test]\n+#[ignore]\n+fn pipeline_bert() -> tokenizers::Result<()> {\n+    // START bert_setup_tokenizer\n+    use tokenizers::models::wordpiece::WordPiece;\n+    use tokenizers::Tokenizer;\n+\n+    let mut bert_tokenizer = Tokenizer::new(WordPiece::default());\n+    // END bert_setup_tokenizer\n+    // START bert_setup_normalizer\n+    use tokenizers::normalizers::utils::Sequence as NormalizerSequence;\n+    use tokenizers::normalizers::{strip::StripAccents, unicode::NFD, utils::Lowercase};\n+\n+    bert_tokenizer.with_normalizer(NormalizerSequence::new(vec![\n+        NFD.into(),\n+        Lowercase.into(),\n+        StripAccents.into(),\n+    ]));\n+    // END bert_setup_normalizer\n+    // START bert_setup_pre_tokenizer\n+    use tokenizers::pre_tokenizers::whitespace::Whitespace;\n+\n+    bert_tokenizer.with_pre_tokenizer(Whitespace::default());\n+    // END bert_setup_pre_tokenizer\n+    // START bert_setup_processor\n+    use tokenizers::processors::template::TemplateProcessing;\n+\n+    bert_tokenizer.with_post_processor(\n+        TemplateProcessing::builder()\n+            .try_single(\"[CLS] $A [SEP]\")\n+            .unwrap()\n+            .try_pair(\"[CLS] $A [SEP] $B:1 [SEP]:1\")\n+            .unwrap()\n+            .special_tokens(vec![(\"[CLS]\", 1), (\"[SEP]\", 2)])\n+            .build()\n+            .unwrap(),\n+    );\n+    // END bert_setup_processor\n+    // START bert_train_tokenizer\n+    use std::path::Path;\n+    use tokenizers::models::{wordpiece::WordPieceTrainer, TrainerWrapper};\n+    use tokenizers::Model;\n+\n+    let trainer: TrainerWrapper = WordPieceTrainer::builder()\n+        .vocab_size(30_522)\n+        .special_tokens(vec![\n+            AddedToken::from(\"[UNK]\", true),\n+            AddedToken::from(\"[CLS]\", true),\n+            AddedToken::from(\"[SEP]\", true),\n+            AddedToken::from(\"[PAD]\", true),\n+            AddedToken::from(\"[MASK]\", true),\n+        ])\n+        .build()\n+        .into();\n+    let files = [\"test\", \"train\", \"valid\"]\n+        .iter()\n+        .map(|split| format!(\"data/wikitext-103-raw/wiki.{}.raw\", split))\n+        .collect::<Vec<_>>();\n+    bert_tokenizer.train_and_replace(&trainer, files)?;\n+\n+    let model_files = bert_tokenizer\n+        .get_model()\n+        .save(&Path::new(\"data\"), Some(\"bert-wiki\"))?;\n+    bert_tokenizer.with_model(\n+        WordPiece::from_file(model_files[0].to_str().unwrap())\n+            .unk_token(\"[UNK]\".to_string())\n+            .build()\n+            .unwrap(),\n+    );\n+\n+    bert_tokenizer.save(\"data/bert-wiki.json\", false)?;\n+    // END bert_train_tokenizer\n+    // START bert_test_decoding\n+    let output = bert_tokenizer.encode(\"Welcome to the 🤗 Tokenizers library.\", true)?;\n+    println!(\"{:?}\", output.get_tokens());",
        "comment_created_at": "2020-11-02T16:16:56+00:00",
        "comment_author": "Narsil",
        "comment_body": "Going along with the `#ignore` if we keep this `print` we 're not checking anything actually during tests. If possible/compatible with a fast testing iteration, we probably should actually change those into real asserts.\r\n\r\nIt hinders readability only by a slight margin I feel, but it impacts forward compat by quite a bit.",
        "pr_file_module": null
      }
    ]
  }
]