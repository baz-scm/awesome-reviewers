[
  {
    "discussion_id": "2352407708",
    "pr_number": 5100,
    "pr_file": "hubble/src/indexer/consumer.rs",
    "created_at": "2025-09-16T12:51:08+00:00",
    "commented_code": "WalletMutationEntry => false,\n         CreateWrappedToken => false,\n         CreateWrappedTokenRelation => false,\n+        CreateProxyAccount => false,\n+        Bond => false,\n+        Unbond => false,\n         // ignore enriched records\n         PacketSendDecoded => false,\n         PacketSendTransfers => false,\n         PacketSendInstructionsSearch => false,\n+        PacketSendBond => false,\n+        PacketSendUnbond => false,\n     }\n }",
    "repo_full_name": "unionlabs/union",
    "discussion_comments": [
      {
        "comment_id": "2352407708",
        "repo_full_name": "unionlabs/union",
        "pr_number": 5100,
        "pr_file": "hubble/src/indexer/consumer.rs",
        "discussion_id": "2352407708",
        "commented_code": "@@ -308,10 +308,15 @@ fn should_trigger_enrich_reset(kind: RecordKind) -> bool {\n         WalletMutationEntry => false,\n         CreateWrappedToken => false,\n         CreateWrappedTokenRelation => false,\n+        CreateProxyAccount => false,\n+        Bond => false,\n+        Unbond => false,\n         // ignore enriched records\n         PacketSendDecoded => false,\n         PacketSendTransfers => false,\n         PacketSendInstructionsSearch => false,\n+        PacketSendBond => false,\n+        PacketSendUnbond => false,\n     }\n }",
        "comment_created_at": "2025-09-16T12:51:08+00:00",
        "comment_author": "cor",
        "comment_body": "Suggestion: we make this more readable by just providing a set of RecordKinds that _should_ enrich reset, and then just do an inclusion check",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2251878887",
    "pr_number": 4938,
    "pr_file": "cosmwasm/osmosis-tokenfactory-token-minter/src/contract.rs",
    "created_at": "2025-08-04T15:46:36+00:00",
    "commented_code": "let subdenom = deconstruct_factory_denom(&env, &denom)?;\n \n-    Ok(Response::new().add_messages(vec![\n-        CosmosMsg::Custom(TokenFactoryMsg::CreateDenom {\n-            subdenom: subdenom.to_owned(),\n-            metadata: None,\n-        }),\n-        // We are using stargate for now instead of `Any` to be safe in case we would want to\n-        // deploy on < wasmvm 2 chain that uses Osmosis' Token Factory\n-        #[allow(deprecated)]\n-        CosmosMsg::Stargate {\n-            type_url: MsgSetDenomMetadata::type_url(),\n-            value: MsgSetDenomMetadata {\n-                sender: env.contract.address.to_string(),\n-                metadata: Some(new_proto_metadata(denom.clone(), metadata)?),\n-            }\n-            .encode_to_vec()\n-            .into(),\n-        },\n-    ]))\n+    Ok(Response::new()\n+        .add_messages(vec![\n+            CosmosMsg::Custom(TokenFactoryMsg::CreateDenom {\n+                subdenom: subdenom.to_owned(),\n+                metadata: None,\n+            }),\n+            // We are using stargate for now instead of `Any` to be safe in case we would want to\n+            // deploy on < wasmvm 2 chain that uses Osmosis' Token Factory\n+            #[allow(deprecated)]\n+            CosmosMsg::Stargate {\n+                type_url: MsgSetDenomMetadata::type_url(),\n+                value: MsgSetDenomMetadata {\n+                    sender: env.contract.address.to_string(),\n+                    metadata: Some(new_proto_metadata(denom.clone(), metadata)?),\n+                }\n+                .encode_to_vec()\n+                .into(),\n+            },\n+        ])\n+        .add_event(\n+            Event::new(\"new_secure_wrapped_token\")",
    "repo_full_name": "unionlabs/union",
    "discussion_comments": [
      {
        "comment_id": "2251878887",
        "repo_full_name": "unionlabs/union",
        "pr_number": 4938,
        "pr_file": "cosmwasm/osmosis-tokenfactory-token-minter/src/contract.rs",
        "discussion_id": "2251878887",
        "commented_code": "@@ -234,30 +265,44 @@ fn wrapped_create_denom(\n \n     let subdenom = deconstruct_factory_denom(&env, &denom)?;\n \n-    Ok(Response::new().add_messages(vec![\n-        CosmosMsg::Custom(TokenFactoryMsg::CreateDenom {\n-            subdenom: subdenom.to_owned(),\n-            metadata: None,\n-        }),\n-        // We are using stargate for now instead of `Any` to be safe in case we would want to\n-        // deploy on < wasmvm 2 chain that uses Osmosis' Token Factory\n-        #[allow(deprecated)]\n-        CosmosMsg::Stargate {\n-            type_url: MsgSetDenomMetadata::type_url(),\n-            value: MsgSetDenomMetadata {\n-                sender: env.contract.address.to_string(),\n-                metadata: Some(new_proto_metadata(denom.clone(), metadata)?),\n-            }\n-            .encode_to_vec()\n-            .into(),\n-        },\n-    ]))\n+    Ok(Response::new()\n+        .add_messages(vec![\n+            CosmosMsg::Custom(TokenFactoryMsg::CreateDenom {\n+                subdenom: subdenom.to_owned(),\n+                metadata: None,\n+            }),\n+            // We are using stargate for now instead of `Any` to be safe in case we would want to\n+            // deploy on < wasmvm 2 chain that uses Osmosis' Token Factory\n+            #[allow(deprecated)]\n+            CosmosMsg::Stargate {\n+                type_url: MsgSetDenomMetadata::type_url(),\n+                value: MsgSetDenomMetadata {\n+                    sender: env.contract.address.to_string(),\n+                    metadata: Some(new_proto_metadata(denom.clone(), metadata)?),\n+                }\n+                .encode_to_vec()\n+                .into(),\n+            },\n+        ])\n+        .add_event(\n+            Event::new(\"new_secure_wrapped_token\")",
        "comment_created_at": "2025-08-04T15:46:36+00:00",
        "comment_author": "benluelo",
        "comment_body": "we should export these as consts from the lib crate",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2033133834",
    "pr_number": 4202,
    "pr_file": "lib/trusted-mpt-light-client-types/src/signed_data.rs",
    "created_at": "2025-04-08T12:55:54+00:00",
    "commented_code": "+use unionlabs::{\n+    encoding::{Encode, Encoding},\n+    primitives::{H256, H512},\n+};\n+\n+#[derive(Debug, Clone, PartialEq)]\n+#[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n+#[cfg_attr(feature = \"bincode\", derive(bincode::Encode, bincode::Decode))]\n+pub struct SignedData<T> {\n+    data: T,\n+    signature: H512,\n+}\n+\n+pub struct SignatureVerificationFailure;\n+\n+impl<T: Clone> SignedData<T> {\n+    pub fn sign<E, V: Fn(&[u8]) -> H512>(data: T, sign: V) -> Self\n+    where\n+        E: Encoding,\n+        T: Encode<E>,\n+    {\n+        SignedData {\n+            signature: sign(&data.clone().encode()),\n+            data,\n+        }\n+    }\n+\n+    pub fn unwrap_verified<E, V: Fn(&[u8], H512, H256) -> Option<bool>>(",
    "repo_full_name": "unionlabs/union",
    "discussion_comments": [
      {
        "comment_id": "2033133834",
        "repo_full_name": "unionlabs/union",
        "pr_number": 4202,
        "pr_file": "lib/trusted-mpt-light-client-types/src/signed_data.rs",
        "discussion_id": "2033133834",
        "commented_code": "@@ -0,0 +1,45 @@\n+use unionlabs::{\n+    encoding::{Encode, Encoding},\n+    primitives::{H256, H512},\n+};\n+\n+#[derive(Debug, Clone, PartialEq)]\n+#[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n+#[cfg_attr(feature = \"bincode\", derive(bincode::Encode, bincode::Decode))]\n+pub struct SignedData<T> {\n+    data: T,\n+    signature: H512,\n+}\n+\n+pub struct SignatureVerificationFailure;\n+\n+impl<T: Clone> SignedData<T> {\n+    pub fn sign<E, V: Fn(&[u8]) -> H512>(data: T, sign: V) -> Self\n+    where\n+        E: Encoding,\n+        T: Encode<E>,\n+    {\n+        SignedData {\n+            signature: sign(&data.clone().encode()),\n+            data,\n+        }\n+    }\n+\n+    pub fn unwrap_verified<E, V: Fn(&[u8], H512, H256) -> Option<bool>>(",
        "comment_created_at": "2025-04-08T12:55:54+00:00",
        "comment_author": "benluelo",
        "comment_body": "nit: either move the E bound to the params list or all bounds to the where clause",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2023998669",
    "pr_number": 4150,
    "pr_file": "voyager/plugins/client-update/bob/src/main.rs",
    "created_at": "2025-04-02T03:04:01+00:00",
    "commented_code": "+// #![warn(clippy::unwrap_used)]\n+\n+use std::collections::VecDeque;\n+\n+use alloy::{\n+    network::AnyNetwork,\n+    providers::{DynProvider, Provider, ProviderBuilder},\n+};\n+use bob_client::output_index_of_l2_block_on_l1_block;\n+use bob_light_client_types::{\n+    header::{L2Header, OutputRootProof},\n+    Header,\n+};\n+use bob_types::L2_OUTPUTS_SLOT;\n+use ethereum_light_client_types::{AccountProof, StorageProof};\n+use hex_literal::hex;\n+use ibc_union_spec::{ClientId, IbcUnion};\n+use jsonrpsee::{\n+    core::{async_trait, RpcResult},\n+    types::ErrorObject,\n+    Extensions,\n+};\n+use serde::{Deserialize, Serialize};\n+use tracing::{debug, instrument};\n+use unionlabs::{\n+    ibc::core::client::height::Height,\n+    primitives::{H160, H256, U256},\n+    ErrorReporter,\n+};\n+use voyager_message::{\n+    call::{Call, FetchUpdateHeaders, WaitForTrustedHeight},\n+    callback::AggregateSubmitTxFromOrderedHeaders,\n+    core::{ChainId, ClientType, IbcSpec, QueryHeight},\n+    data::{Data, DecodedHeaderMeta, OrderedHeaders},\n+    hook::UpdateHook,\n+    into_value,\n+    module::{PluginInfo, PluginServer},\n+    DefaultCmd, ExtensionsExt, Plugin, PluginMessage, RawClientId, VoyagerClient, VoyagerMessage,\n+};\n+use voyager_vm::{call, conc, data, pass::PassResult, promise, seq, BoxDynError, Op, Visit};\n+\n+use crate::{\n+    call::{FetchUpdate, ModuleCall},\n+    callback::ModuleCallback,\n+};\n+\n+pub mod call;\n+pub mod callback;\n+pub mod data;\n+\n+#[tokio::main(flavor = \"multi_thread\")]\n+async fn main() {\n+    Module::run().await\n+}\n+\n+#[derive(Debug, Clone)]\n+pub struct Module {\n+    pub chain_id: ChainId,\n+\n+    pub l1_client_id: ClientId,\n+\n+    pub l2_oracle_address: H160,\n+\n+    pub l1_provider: DynProvider,\n+    pub l2_provider: DynProvider<AnyNetwork>,\n+\n+    /// The address of the `IBCHandler` smart contract.\n+    pub ibc_handler_address: H160,\n+}\n+\n+#[derive(Debug, Clone, Serialize, Deserialize)]\n+#[serde(deny_unknown_fields)]\n+pub struct Config {\n+    pub l2_chain_id: ChainId,\n+\n+    /// The client ID of the L1 chain.\n+    pub l1_client_id: ClientId,\n+\n+    /// The L2 oracle contract on the L1.\n+    pub l2_oracle_address: H160,\n+\n+    /// The RPC endpoint for the settlement (L1) execution chain.\n+    pub l1_rpc_url: String,\n+\n+    /// The RPC endpoint for the main (L2) execution chain.\n+    pub l2_rpc_url: String,\n+\n+    /// The address of the `IBCHandler` smart contract.\n+    pub ibc_handler_address: H160,\n+\n+    #[serde(default)]\n+    pub max_cache_size: u32,\n+}\n+\n+fn plugin_name(chain_id: &ChainId) -> String {\n+    pub const PLUGIN_NAME: &str = env!(\"CARGO_PKG_NAME\");\n+\n+    format!(\"{PLUGIN_NAME}/{}\", chain_id)\n+}\n+\n+impl Module {\n+    fn plugin_name(&self) -> String {\n+        plugin_name(&self.chain_id)\n+    }\n+\n+    #[instrument(\n+        skip_all,\n+        fields(\n+            %block_number,\n+            ibc_handler_address = %self.ibc_handler_address\n+        )\n+    )]\n+    pub async fn fetch_oracle_account_proof(&self, block_number: u64) -> RpcResult<AccountProof> {\n+        let account_update = self\n+            .l2_provider\n+            .get_proof(self.l2_oracle_address.into(), vec![])\n+            .block_id(block_number.into())\n+            .await\n+            .map_err(|e| {\n+                ErrorObject::owned(\n+                    -1,\n+                    ErrorReporter(e).with_message(\"error fetching account update\"),\n+                    None::<()>,\n+                )\n+            })?;\n+\n+        debug!(storage_hash = %account_update.storage_hash, \"fetched account update\");\n+\n+        Ok(AccountProof {\n+            storage_root: account_update.storage_hash.into(),\n+            proof: account_update\n+                .account_proof\n+                .into_iter()\n+                .map(|x| x.into())\n+                .collect(),\n+        })\n+    }\n+\n+    pub async fn fetch_output_proposal_proof(\n+        &self,\n+        output_index: u32,\n+        height: u64,\n+    ) -> RpcResult<StorageProof> {\n+        let [proof]: [_; 1] = self\n+            .l1_provider\n+            .get_proof(\n+                self.l2_oracle_address.into(),\n+                vec![bob_verifier::compute_output_proposal_slot(\n+                    L2_OUTPUTS_SLOT.into(),\n+                    output_index,\n+                )\n+                .to_be_bytes()\n+                .into()],\n+            )\n+            .block_id(height.into())\n+            .await\n+            .map_err(|e| {\n+                ErrorObject::owned(\n+                    -1,\n+                    format!(\"error fetching output proposal proof: {}\", ErrorReporter(e)),\n+                    None::<()>,\n+                )\n+            })?\n+            .storage_proof\n+            .try_into()\n+            .unwrap();\n+        Ok(StorageProof {\n+            key: U256::from_be_bytes(proof.key.as_b256().0),\n+            value: U256::from_be_bytes(proof.value.to_be_bytes()),\n+            proof: proof.proof.into_iter().map(|bytes| bytes.into()).collect(),\n+        })\n+    }\n+}\n+\n+impl Plugin for Module {\n+    type Call = ModuleCall;\n+    type Callback = ModuleCallback;\n+\n+    type Config = Config;\n+    type Cmd = DefaultCmd;\n+\n+    async fn new(config: Self::Config) -> Result<Self, BoxDynError> {\n+        let l1_provider =\n+            DynProvider::new(ProviderBuilder::new().connect(&config.l1_rpc_url).await?);\n+\n+        let l2_provider = DynProvider::new(\n+            ProviderBuilder::new()\n+                .network::<AnyNetwork>()\n+                .connect(&config.l2_rpc_url)\n+                .await?,\n+        );\n+\n+        let l2_chain_id = ChainId::new(l2_provider.get_chain_id().await?.to_string());\n+\n+        assert_eq!(l2_chain_id, config.l2_chain_id);\n+\n+        Ok(Self {\n+            chain_id: l2_chain_id,\n+            l1_client_id: config.l1_client_id,\n+            l2_oracle_address: config.l2_oracle_address,\n+            l1_provider,\n+            l2_provider,\n+            ibc_handler_address: config.ibc_handler_address,\n+        })\n+    }\n+\n+    fn info(config: Self::Config) -> PluginInfo {\n+        PluginInfo {\n+            name: plugin_name(&config.l2_chain_id),\n+            interest_filter: UpdateHook::filter(\n+                &config.l2_chain_id,\n+                &ClientType::new(ClientType::BOB),\n+            ),\n+        }\n+    }\n+\n+    async fn cmd(_config: Self::Config, cmd: Self::Cmd) {\n+        match cmd {}\n+    }\n+}\n+\n+#[async_trait]\n+impl PluginServer<ModuleCall, ModuleCallback> for Module {\n+    #[instrument(skip_all, fields(chain_id = %self.chain_id))]\n+    async fn run_pass(\n+        &self,\n+        _: &Extensions,\n+        msgs: Vec<Op<VoyagerMessage>>,\n+    ) -> RpcResult<PassResult<VoyagerMessage>> {\n+        Ok(PassResult {\n+            optimize_further: vec![],\n+            ready: msgs\n+                .into_iter()\n+                .map(|mut op| {\n+                    UpdateHook::new(\n+                        &self.chain_id,\n+                        &ClientType::new(ClientType::ETHEREUM),\n+                        |fetch| {\n+                            Call::Plugin(PluginMessage::new(\n+                                self.plugin_name(),\n+                                ModuleCall::from(FetchUpdate {\n+                                    from_height: fetch.update_from,\n+                                    to_height: fetch.update_to,\n+                                    counterparty_chain_id: fetch.counterparty_chain_id.clone(),\n+                                    client_id: fetch.client_id.clone(),\n+                                }),\n+                            ))\n+                        },\n+                    )\n+                    .visit_op(&mut op);\n+\n+                    op\n+                })\n+                .enumerate()\n+                .map(|(i, op)| (vec![i], op))\n+                .collect(),\n+        })\n+    }\n+\n+    #[instrument(skip_all, fields(chain_id = %self.chain_id))]\n+    async fn call(&self, ext: &Extensions, msg: ModuleCall) -> RpcResult<Op<VoyagerMessage>> {\n+        match msg {\n+            ModuleCall::FetchUpdate(FetchUpdate {\n+                from_height,\n+                to_height,\n+                counterparty_chain_id,\n+                client_id,\n+            }) => self\n+                .fetch_update(\n+                    ext,\n+                    from_height,\n+                    to_height,\n+                    counterparty_chain_id,\n+                    client_id,\n+                )\n+                .await\n+                .map_err(|e| {\n+                    ErrorObject::owned(\n+                        -1,\n+                        format!(\"error fetching update: {}\", ErrorReporter(&*e)),\n+                        None::<()>,\n+                    )\n+                }),\n+        }\n+    }\n+\n+    #[instrument(skip_all, fields(chain_id = %self.chain_id))]\n+    async fn callback(\n+        &self,\n+        _: &Extensions,\n+        cb: ModuleCallback,\n+        _data: VecDeque<Data>,\n+    ) -> RpcResult<Op<VoyagerMessage>> {\n+        match cb {}\n+    }\n+}\n+\n+impl Module {\n+    async fn fetch_ibc_contract_root_proof(&self, height: u64) -> RpcResult<AccountProof> {\n+        let proof = self\n+            .l2_provider\n+            .get_proof(self.ibc_handler_address.into(), vec![])\n+            .block_id(height.into())\n+            .await\n+            .map_err(|e| {\n+                ErrorObject::owned(\n+                    -1,\n+                    format!(\"error fetching output proposal proof: {}\", ErrorReporter(e)),\n+                    None::<()>,\n+                )\n+            })?;\n+        Ok(AccountProof {\n+            storage_root: proof.storage_hash.into(),\n+            proof: proof.account_proof.into_iter().map(|x| x.into()).collect(),\n+        })\n+    }\n+\n+    async fn fetch_output_root_proof(&self, height: u64) -> RpcResult<OutputRootProof> {\n+        let l2_block = self\n+            .l2_provider\n+            .get_block(height.into())\n+            .await\n+            .map_err(|e| {\n+                ErrorObject::owned(\n+                    -1,\n+                    format!(\"error fetching output proposal proof: {}\", ErrorReporter(e)),\n+                    None::<()>,\n+                )\n+            })?\n+            .unwrap();\n+        // Opstack L2ToL1MessagePasser address, fixed.\n+        let l2_to_l1_message_passer = hex!(\"4200000000000000000000000000000000000016\");",
    "repo_full_name": "unionlabs/union",
    "discussion_comments": [
      {
        "comment_id": "2023998669",
        "repo_full_name": "unionlabs/union",
        "pr_number": 4150,
        "pr_file": "voyager/plugins/client-update/bob/src/main.rs",
        "discussion_id": "2023998669",
        "commented_code": "@@ -0,0 +1,493 @@\n+// #![warn(clippy::unwrap_used)]\n+\n+use std::collections::VecDeque;\n+\n+use alloy::{\n+    network::AnyNetwork,\n+    providers::{DynProvider, Provider, ProviderBuilder},\n+};\n+use bob_client::output_index_of_l2_block_on_l1_block;\n+use bob_light_client_types::{\n+    header::{L2Header, OutputRootProof},\n+    Header,\n+};\n+use bob_types::L2_OUTPUTS_SLOT;\n+use ethereum_light_client_types::{AccountProof, StorageProof};\n+use hex_literal::hex;\n+use ibc_union_spec::{ClientId, IbcUnion};\n+use jsonrpsee::{\n+    core::{async_trait, RpcResult},\n+    types::ErrorObject,\n+    Extensions,\n+};\n+use serde::{Deserialize, Serialize};\n+use tracing::{debug, instrument};\n+use unionlabs::{\n+    ibc::core::client::height::Height,\n+    primitives::{H160, H256, U256},\n+    ErrorReporter,\n+};\n+use voyager_message::{\n+    call::{Call, FetchUpdateHeaders, WaitForTrustedHeight},\n+    callback::AggregateSubmitTxFromOrderedHeaders,\n+    core::{ChainId, ClientType, IbcSpec, QueryHeight},\n+    data::{Data, DecodedHeaderMeta, OrderedHeaders},\n+    hook::UpdateHook,\n+    into_value,\n+    module::{PluginInfo, PluginServer},\n+    DefaultCmd, ExtensionsExt, Plugin, PluginMessage, RawClientId, VoyagerClient, VoyagerMessage,\n+};\n+use voyager_vm::{call, conc, data, pass::PassResult, promise, seq, BoxDynError, Op, Visit};\n+\n+use crate::{\n+    call::{FetchUpdate, ModuleCall},\n+    callback::ModuleCallback,\n+};\n+\n+pub mod call;\n+pub mod callback;\n+pub mod data;\n+\n+#[tokio::main(flavor = \"multi_thread\")]\n+async fn main() {\n+    Module::run().await\n+}\n+\n+#[derive(Debug, Clone)]\n+pub struct Module {\n+    pub chain_id: ChainId,\n+\n+    pub l1_client_id: ClientId,\n+\n+    pub l2_oracle_address: H160,\n+\n+    pub l1_provider: DynProvider,\n+    pub l2_provider: DynProvider<AnyNetwork>,\n+\n+    /// The address of the `IBCHandler` smart contract.\n+    pub ibc_handler_address: H160,\n+}\n+\n+#[derive(Debug, Clone, Serialize, Deserialize)]\n+#[serde(deny_unknown_fields)]\n+pub struct Config {\n+    pub l2_chain_id: ChainId,\n+\n+    /// The client ID of the L1 chain.\n+    pub l1_client_id: ClientId,\n+\n+    /// The L2 oracle contract on the L1.\n+    pub l2_oracle_address: H160,\n+\n+    /// The RPC endpoint for the settlement (L1) execution chain.\n+    pub l1_rpc_url: String,\n+\n+    /// The RPC endpoint for the main (L2) execution chain.\n+    pub l2_rpc_url: String,\n+\n+    /// The address of the `IBCHandler` smart contract.\n+    pub ibc_handler_address: H160,\n+\n+    #[serde(default)]\n+    pub max_cache_size: u32,\n+}\n+\n+fn plugin_name(chain_id: &ChainId) -> String {\n+    pub const PLUGIN_NAME: &str = env!(\"CARGO_PKG_NAME\");\n+\n+    format!(\"{PLUGIN_NAME}/{}\", chain_id)\n+}\n+\n+impl Module {\n+    fn plugin_name(&self) -> String {\n+        plugin_name(&self.chain_id)\n+    }\n+\n+    #[instrument(\n+        skip_all,\n+        fields(\n+            %block_number,\n+            ibc_handler_address = %self.ibc_handler_address\n+        )\n+    )]\n+    pub async fn fetch_oracle_account_proof(&self, block_number: u64) -> RpcResult<AccountProof> {\n+        let account_update = self\n+            .l2_provider\n+            .get_proof(self.l2_oracle_address.into(), vec![])\n+            .block_id(block_number.into())\n+            .await\n+            .map_err(|e| {\n+                ErrorObject::owned(\n+                    -1,\n+                    ErrorReporter(e).with_message(\"error fetching account update\"),\n+                    None::<()>,\n+                )\n+            })?;\n+\n+        debug!(storage_hash = %account_update.storage_hash, \"fetched account update\");\n+\n+        Ok(AccountProof {\n+            storage_root: account_update.storage_hash.into(),\n+            proof: account_update\n+                .account_proof\n+                .into_iter()\n+                .map(|x| x.into())\n+                .collect(),\n+        })\n+    }\n+\n+    pub async fn fetch_output_proposal_proof(\n+        &self,\n+        output_index: u32,\n+        height: u64,\n+    ) -> RpcResult<StorageProof> {\n+        let [proof]: [_; 1] = self\n+            .l1_provider\n+            .get_proof(\n+                self.l2_oracle_address.into(),\n+                vec![bob_verifier::compute_output_proposal_slot(\n+                    L2_OUTPUTS_SLOT.into(),\n+                    output_index,\n+                )\n+                .to_be_bytes()\n+                .into()],\n+            )\n+            .block_id(height.into())\n+            .await\n+            .map_err(|e| {\n+                ErrorObject::owned(\n+                    -1,\n+                    format!(\"error fetching output proposal proof: {}\", ErrorReporter(e)),\n+                    None::<()>,\n+                )\n+            })?\n+            .storage_proof\n+            .try_into()\n+            .unwrap();\n+        Ok(StorageProof {\n+            key: U256::from_be_bytes(proof.key.as_b256().0),\n+            value: U256::from_be_bytes(proof.value.to_be_bytes()),\n+            proof: proof.proof.into_iter().map(|bytes| bytes.into()).collect(),\n+        })\n+    }\n+}\n+\n+impl Plugin for Module {\n+    type Call = ModuleCall;\n+    type Callback = ModuleCallback;\n+\n+    type Config = Config;\n+    type Cmd = DefaultCmd;\n+\n+    async fn new(config: Self::Config) -> Result<Self, BoxDynError> {\n+        let l1_provider =\n+            DynProvider::new(ProviderBuilder::new().connect(&config.l1_rpc_url).await?);\n+\n+        let l2_provider = DynProvider::new(\n+            ProviderBuilder::new()\n+                .network::<AnyNetwork>()\n+                .connect(&config.l2_rpc_url)\n+                .await?,\n+        );\n+\n+        let l2_chain_id = ChainId::new(l2_provider.get_chain_id().await?.to_string());\n+\n+        assert_eq!(l2_chain_id, config.l2_chain_id);\n+\n+        Ok(Self {\n+            chain_id: l2_chain_id,\n+            l1_client_id: config.l1_client_id,\n+            l2_oracle_address: config.l2_oracle_address,\n+            l1_provider,\n+            l2_provider,\n+            ibc_handler_address: config.ibc_handler_address,\n+        })\n+    }\n+\n+    fn info(config: Self::Config) -> PluginInfo {\n+        PluginInfo {\n+            name: plugin_name(&config.l2_chain_id),\n+            interest_filter: UpdateHook::filter(\n+                &config.l2_chain_id,\n+                &ClientType::new(ClientType::BOB),\n+            ),\n+        }\n+    }\n+\n+    async fn cmd(_config: Self::Config, cmd: Self::Cmd) {\n+        match cmd {}\n+    }\n+}\n+\n+#[async_trait]\n+impl PluginServer<ModuleCall, ModuleCallback> for Module {\n+    #[instrument(skip_all, fields(chain_id = %self.chain_id))]\n+    async fn run_pass(\n+        &self,\n+        _: &Extensions,\n+        msgs: Vec<Op<VoyagerMessage>>,\n+    ) -> RpcResult<PassResult<VoyagerMessage>> {\n+        Ok(PassResult {\n+            optimize_further: vec![],\n+            ready: msgs\n+                .into_iter()\n+                .map(|mut op| {\n+                    UpdateHook::new(\n+                        &self.chain_id,\n+                        &ClientType::new(ClientType::ETHEREUM),\n+                        |fetch| {\n+                            Call::Plugin(PluginMessage::new(\n+                                self.plugin_name(),\n+                                ModuleCall::from(FetchUpdate {\n+                                    from_height: fetch.update_from,\n+                                    to_height: fetch.update_to,\n+                                    counterparty_chain_id: fetch.counterparty_chain_id.clone(),\n+                                    client_id: fetch.client_id.clone(),\n+                                }),\n+                            ))\n+                        },\n+                    )\n+                    .visit_op(&mut op);\n+\n+                    op\n+                })\n+                .enumerate()\n+                .map(|(i, op)| (vec![i], op))\n+                .collect(),\n+        })\n+    }\n+\n+    #[instrument(skip_all, fields(chain_id = %self.chain_id))]\n+    async fn call(&self, ext: &Extensions, msg: ModuleCall) -> RpcResult<Op<VoyagerMessage>> {\n+        match msg {\n+            ModuleCall::FetchUpdate(FetchUpdate {\n+                from_height,\n+                to_height,\n+                counterparty_chain_id,\n+                client_id,\n+            }) => self\n+                .fetch_update(\n+                    ext,\n+                    from_height,\n+                    to_height,\n+                    counterparty_chain_id,\n+                    client_id,\n+                )\n+                .await\n+                .map_err(|e| {\n+                    ErrorObject::owned(\n+                        -1,\n+                        format!(\"error fetching update: {}\", ErrorReporter(&*e)),\n+                        None::<()>,\n+                    )\n+                }),\n+        }\n+    }\n+\n+    #[instrument(skip_all, fields(chain_id = %self.chain_id))]\n+    async fn callback(\n+        &self,\n+        _: &Extensions,\n+        cb: ModuleCallback,\n+        _data: VecDeque<Data>,\n+    ) -> RpcResult<Op<VoyagerMessage>> {\n+        match cb {}\n+    }\n+}\n+\n+impl Module {\n+    async fn fetch_ibc_contract_root_proof(&self, height: u64) -> RpcResult<AccountProof> {\n+        let proof = self\n+            .l2_provider\n+            .get_proof(self.ibc_handler_address.into(), vec![])\n+            .block_id(height.into())\n+            .await\n+            .map_err(|e| {\n+                ErrorObject::owned(\n+                    -1,\n+                    format!(\"error fetching output proposal proof: {}\", ErrorReporter(e)),\n+                    None::<()>,\n+                )\n+            })?;\n+        Ok(AccountProof {\n+            storage_root: proof.storage_hash.into(),\n+            proof: proof.account_proof.into_iter().map(|x| x.into()).collect(),\n+        })\n+    }\n+\n+    async fn fetch_output_root_proof(&self, height: u64) -> RpcResult<OutputRootProof> {\n+        let l2_block = self\n+            .l2_provider\n+            .get_block(height.into())\n+            .await\n+            .map_err(|e| {\n+                ErrorObject::owned(\n+                    -1,\n+                    format!(\"error fetching output proposal proof: {}\", ErrorReporter(e)),\n+                    None::<()>,\n+                )\n+            })?\n+            .unwrap();\n+        // Opstack L2ToL1MessagePasser address, fixed.\n+        let l2_to_l1_message_passer = hex!(\"4200000000000000000000000000000000000016\");",
        "comment_created_at": "2025-04-02T03:04:01+00:00",
        "comment_author": "benluelo",
        "comment_body": "should probably extract this to a constant",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1991513362",
    "pr_number": 4023,
    "pr_file": "cosmwasm/ibc-union/app/ucs03-zkgm/src/contract.rs",
    "created_at": "2025-03-12T13:32:18+00:00",
    "commented_code": "}\n     }\n }\n+\n+/// Increases the outstanding balance for a (channel, path, token) combination.\n+/// This is used when escrowing tokens to track how many tokens can be unescrowed later.\n+/// The balance is used to prevent double-spending and ensure token conservation across chains.\n+fn increase_channel_balance(\n+    deps: DepsMut,\n+    channel_id: u32,\n+    path: U256,\n+    base_token: String,\n+    base_amount: Uint256,\n+) -> Result<(), ContractError> {\n+    CHANNEL_BALANCE.update(\n+        deps.storage,\n+        (channel_id, path.to_be_bytes::<32>().to_vec(), base_token),\n+        |balance| match balance {\n+            Some(value) => value\n+                .checked_add(base_amount)\n+                .map_err(|_| ContractError::InvalidChannelBalance),\n+            None => Ok(base_amount),\n+        },\n+    )?;\n+    Ok(())\n+}\n+\n+/// Decrease the outstanding balance of a (channel, path, token) combination.\n+/// This is used when unwrapping tokens to ensure we don't unescrow more tokens than were originally escrowed.\n+fn decrease_channel_balance(\n+    deps: DepsMut,\n+    channel_id: u32,\n+    path: U256,\n+    token: String,\n+    amount: Uint256,\n+) -> Result<(), ContractError> {\n+    CHANNEL_BALANCE.update(\n+        deps.storage,\n+        (channel_id, path.to_be_bytes::<32>().to_vec(), token),\n+        |balance| match balance {\n+            Some(value) => value\n+                .checked_sub(amount)\n+                .map_err(|_| ContractError::InvalidChannelBalance),\n+            None => Err(ContractError::InvalidChannelBalance),\n+        },\n+    )?;\n+    Ok(())\n+}\n+\n+pub fn derive_batch_salt(index: U256, salt: H256) -> H256 {\n+    keccak256((index, salt.get()).abi_encode())\n+}\n+\n+pub fn dequeue_channel_from_path(path: U256) -> (U256, u32) {\n+    if path == U256::ZERO {\n+        (U256::ZERO, 0)\n+    } else {\n+        (\n+            path >> 32,\n+            u32::try_from(path & U256::from(u32::MAX)).expect(\"impossible\"),\n+        )\n+    }\n+}\n+\n+/// Extract the last channel from a path and return the base path without it\n+pub fn pop_channel_from_path(path: U256) -> (U256, u32) {\n+    if path == U256::ZERO {\n+        return (U256::ZERO, 0);\n+    }\n+    // Find the highest non-zero 32-bit chunk (leftmost)\n+    let highest_index = (256 - path.leading_zeros() - 1) / 32;\n+    // Extract the channel ID from the highest non-zero slot\n+    let channel_id =\n+        u32::try_from((path >> (highest_index * 32)) & U256::from(u32::MAX)).expect(\"impossible\");\n+    // Clear that slot in the path\n+    let mask = !(U256::from(u32::MAX) << (highest_index * 32));\n+    let base_path = path & mask;\n+    (base_path, channel_id)\n+}\n+\n+pub fn update_channel_path(path: U256, next_channel_id: u32) -> Result<U256, ContractError> {\n+    if path == U256::ZERO {\n+        Ok(U256::from(next_channel_id))\n+    } else {\n+        let next_hop_index = (256 - path.leading_zeros()) / 32 + 1;\n+        if next_hop_index > 7 {\n+            return Err(ContractError::ChannelPathIsFull {\n+                path,\n+                next_hop_index,\n+            });\n+        }\n+        Ok((U256::from(next_channel_id) << (32 * next_hop_index)) | path)\n+    }\n+}\n+\n+pub fn reverse_channel_path(path: U256) -> U256 {\n+    U256::from(u32::try_from(path & U256::from(u32::MAX)).expect(\"impossible\")) << 224\n+        | U256::from(u32::try_from((path >> 32) & U256::from(u32::MAX)).expect(\"impossible\")) << 192\n+        | U256::from(u32::try_from((path >> 64) & U256::from(u32::MAX)).expect(\"impossible\")) << 160\n+        | U256::from(u32::try_from((path >> 96) & U256::from(u32::MAX)).expect(\"impossible\")) << 128\n+        | U256::from(u32::try_from((path >> 128) & U256::from(u32::MAX)).expect(\"impossible\")) << 96\n+        | U256::from(u32::try_from((path >> 160) & U256::from(u32::MAX)).expect(\"impossible\")) << 64\n+        | U256::from(u32::try_from((path >> 192) & U256::from(u32::MAX)).expect(\"impossible\")) << 32\n+        | U256::from(u32::try_from((path >> 224) & U256::from(u32::MAX)).expect(\"impossible\"))\n+}",
    "repo_full_name": "unionlabs/union",
    "discussion_comments": [
      {
        "comment_id": "1991513362",
        "repo_full_name": "unionlabs/union",
        "pr_number": 4023,
        "pr_file": "cosmwasm/ibc-union/app/ucs03-zkgm/src/contract.rs",
        "discussion_id": "1991513362",
        "commented_code": "@@ -1199,3 +1895,127 @@ pub fn query(deps: Deps, _: Env, msg: QueryMsg) -> Result<Binary, ContractError>\n         }\n     }\n }\n+\n+/// Increases the outstanding balance for a (channel, path, token) combination.\n+/// This is used when escrowing tokens to track how many tokens can be unescrowed later.\n+/// The balance is used to prevent double-spending and ensure token conservation across chains.\n+fn increase_channel_balance(\n+    deps: DepsMut,\n+    channel_id: u32,\n+    path: U256,\n+    base_token: String,\n+    base_amount: Uint256,\n+) -> Result<(), ContractError> {\n+    CHANNEL_BALANCE.update(\n+        deps.storage,\n+        (channel_id, path.to_be_bytes::<32>().to_vec(), base_token),\n+        |balance| match balance {\n+            Some(value) => value\n+                .checked_add(base_amount)\n+                .map_err(|_| ContractError::InvalidChannelBalance),\n+            None => Ok(base_amount),\n+        },\n+    )?;\n+    Ok(())\n+}\n+\n+/// Decrease the outstanding balance of a (channel, path, token) combination.\n+/// This is used when unwrapping tokens to ensure we don't unescrow more tokens than were originally escrowed.\n+fn decrease_channel_balance(\n+    deps: DepsMut,\n+    channel_id: u32,\n+    path: U256,\n+    token: String,\n+    amount: Uint256,\n+) -> Result<(), ContractError> {\n+    CHANNEL_BALANCE.update(\n+        deps.storage,\n+        (channel_id, path.to_be_bytes::<32>().to_vec(), token),\n+        |balance| match balance {\n+            Some(value) => value\n+                .checked_sub(amount)\n+                .map_err(|_| ContractError::InvalidChannelBalance),\n+            None => Err(ContractError::InvalidChannelBalance),\n+        },\n+    )?;\n+    Ok(())\n+}\n+\n+pub fn derive_batch_salt(index: U256, salt: H256) -> H256 {\n+    keccak256((index, salt.get()).abi_encode())\n+}\n+\n+pub fn dequeue_channel_from_path(path: U256) -> (U256, u32) {\n+    if path == U256::ZERO {\n+        (U256::ZERO, 0)\n+    } else {\n+        (\n+            path >> 32,\n+            u32::try_from(path & U256::from(u32::MAX)).expect(\"impossible\"),\n+        )\n+    }\n+}\n+\n+/// Extract the last channel from a path and return the base path without it\n+pub fn pop_channel_from_path(path: U256) -> (U256, u32) {\n+    if path == U256::ZERO {\n+        return (U256::ZERO, 0);\n+    }\n+    // Find the highest non-zero 32-bit chunk (leftmost)\n+    let highest_index = (256 - path.leading_zeros() - 1) / 32;\n+    // Extract the channel ID from the highest non-zero slot\n+    let channel_id =\n+        u32::try_from((path >> (highest_index * 32)) & U256::from(u32::MAX)).expect(\"impossible\");\n+    // Clear that slot in the path\n+    let mask = !(U256::from(u32::MAX) << (highest_index * 32));\n+    let base_path = path & mask;\n+    (base_path, channel_id)\n+}\n+\n+pub fn update_channel_path(path: U256, next_channel_id: u32) -> Result<U256, ContractError> {\n+    if path == U256::ZERO {\n+        Ok(U256::from(next_channel_id))\n+    } else {\n+        let next_hop_index = (256 - path.leading_zeros()) / 32 + 1;\n+        if next_hop_index > 7 {\n+            return Err(ContractError::ChannelPathIsFull {\n+                path,\n+                next_hop_index,\n+            });\n+        }\n+        Ok((U256::from(next_channel_id) << (32 * next_hop_index)) | path)\n+    }\n+}\n+\n+pub fn reverse_channel_path(path: U256) -> U256 {\n+    U256::from(u32::try_from(path & U256::from(u32::MAX)).expect(\"impossible\")) << 224\n+        | U256::from(u32::try_from((path >> 32) & U256::from(u32::MAX)).expect(\"impossible\")) << 192\n+        | U256::from(u32::try_from((path >> 64) & U256::from(u32::MAX)).expect(\"impossible\")) << 160\n+        | U256::from(u32::try_from((path >> 96) & U256::from(u32::MAX)).expect(\"impossible\")) << 128\n+        | U256::from(u32::try_from((path >> 128) & U256::from(u32::MAX)).expect(\"impossible\")) << 96\n+        | U256::from(u32::try_from((path >> 160) & U256::from(u32::MAX)).expect(\"impossible\")) << 64\n+        | U256::from(u32::try_from((path >> 192) & U256::from(u32::MAX)).expect(\"impossible\")) << 32\n+        | U256::from(u32::try_from((path >> 224) & U256::from(u32::MAX)).expect(\"impossible\"))\n+}",
        "comment_created_at": "2025-03-12T13:32:18+00:00",
        "comment_author": "benluelo",
        "comment_body": "small nit: a helper fn to extract the u32 at an index would drastically help readability",
        "pr_file_module": null
      },
      {
        "comment_id": "1991848458",
        "repo_full_name": "unionlabs/union",
        "pr_number": 4023,
        "pr_file": "cosmwasm/ibc-union/app/ucs03-zkgm/src/contract.rs",
        "discussion_id": "1991513362",
        "commented_code": "@@ -1199,3 +1895,127 @@ pub fn query(deps: Deps, _: Env, msg: QueryMsg) -> Result<Binary, ContractError>\n         }\n     }\n }\n+\n+/// Increases the outstanding balance for a (channel, path, token) combination.\n+/// This is used when escrowing tokens to track how many tokens can be unescrowed later.\n+/// The balance is used to prevent double-spending and ensure token conservation across chains.\n+fn increase_channel_balance(\n+    deps: DepsMut,\n+    channel_id: u32,\n+    path: U256,\n+    base_token: String,\n+    base_amount: Uint256,\n+) -> Result<(), ContractError> {\n+    CHANNEL_BALANCE.update(\n+        deps.storage,\n+        (channel_id, path.to_be_bytes::<32>().to_vec(), base_token),\n+        |balance| match balance {\n+            Some(value) => value\n+                .checked_add(base_amount)\n+                .map_err(|_| ContractError::InvalidChannelBalance),\n+            None => Ok(base_amount),\n+        },\n+    )?;\n+    Ok(())\n+}\n+\n+/// Decrease the outstanding balance of a (channel, path, token) combination.\n+/// This is used when unwrapping tokens to ensure we don't unescrow more tokens than were originally escrowed.\n+fn decrease_channel_balance(\n+    deps: DepsMut,\n+    channel_id: u32,\n+    path: U256,\n+    token: String,\n+    amount: Uint256,\n+) -> Result<(), ContractError> {\n+    CHANNEL_BALANCE.update(\n+        deps.storage,\n+        (channel_id, path.to_be_bytes::<32>().to_vec(), token),\n+        |balance| match balance {\n+            Some(value) => value\n+                .checked_sub(amount)\n+                .map_err(|_| ContractError::InvalidChannelBalance),\n+            None => Err(ContractError::InvalidChannelBalance),\n+        },\n+    )?;\n+    Ok(())\n+}\n+\n+pub fn derive_batch_salt(index: U256, salt: H256) -> H256 {\n+    keccak256((index, salt.get()).abi_encode())\n+}\n+\n+pub fn dequeue_channel_from_path(path: U256) -> (U256, u32) {\n+    if path == U256::ZERO {\n+        (U256::ZERO, 0)\n+    } else {\n+        (\n+            path >> 32,\n+            u32::try_from(path & U256::from(u32::MAX)).expect(\"impossible\"),\n+        )\n+    }\n+}\n+\n+/// Extract the last channel from a path and return the base path without it\n+pub fn pop_channel_from_path(path: U256) -> (U256, u32) {\n+    if path == U256::ZERO {\n+        return (U256::ZERO, 0);\n+    }\n+    // Find the highest non-zero 32-bit chunk (leftmost)\n+    let highest_index = (256 - path.leading_zeros() - 1) / 32;\n+    // Extract the channel ID from the highest non-zero slot\n+    let channel_id =\n+        u32::try_from((path >> (highest_index * 32)) & U256::from(u32::MAX)).expect(\"impossible\");\n+    // Clear that slot in the path\n+    let mask = !(U256::from(u32::MAX) << (highest_index * 32));\n+    let base_path = path & mask;\n+    (base_path, channel_id)\n+}\n+\n+pub fn update_channel_path(path: U256, next_channel_id: u32) -> Result<U256, ContractError> {\n+    if path == U256::ZERO {\n+        Ok(U256::from(next_channel_id))\n+    } else {\n+        let next_hop_index = (256 - path.leading_zeros()) / 32 + 1;\n+        if next_hop_index > 7 {\n+            return Err(ContractError::ChannelPathIsFull {\n+                path,\n+                next_hop_index,\n+            });\n+        }\n+        Ok((U256::from(next_channel_id) << (32 * next_hop_index)) | path)\n+    }\n+}\n+\n+pub fn reverse_channel_path(path: U256) -> U256 {\n+    U256::from(u32::try_from(path & U256::from(u32::MAX)).expect(\"impossible\")) << 224\n+        | U256::from(u32::try_from((path >> 32) & U256::from(u32::MAX)).expect(\"impossible\")) << 192\n+        | U256::from(u32::try_from((path >> 64) & U256::from(u32::MAX)).expect(\"impossible\")) << 160\n+        | U256::from(u32::try_from((path >> 96) & U256::from(u32::MAX)).expect(\"impossible\")) << 128\n+        | U256::from(u32::try_from((path >> 128) & U256::from(u32::MAX)).expect(\"impossible\")) << 96\n+        | U256::from(u32::try_from((path >> 160) & U256::from(u32::MAX)).expect(\"impossible\")) << 64\n+        | U256::from(u32::try_from((path >> 192) & U256::from(u32::MAX)).expect(\"impossible\")) << 32\n+        | U256::from(u32::try_from((path >> 224) & U256::from(u32::MAX)).expect(\"impossible\"))\n+}",
        "comment_created_at": "2025-03-12T16:15:41+00:00",
        "comment_author": "hussein-aitlahcen",
        "comment_body": "yep, done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1933633258",
    "pr_number": 3639,
    "pr_file": "tools/slot-calculator/src/main.rs",
    "created_at": "2025-01-29T10:34:11+00:00",
    "commented_code": "+use clap::{Arg, Command};\n+use proc_macro2::TokenStream;\n+use slotlib::{MappingKey, Slot};\n+use std::{collections::VecDeque, str::FromStr};\n+use syn_solidity::{parse2, Item, Type};\n+use typed_arena::Arena;\n+use unionlabs::primitives::{H256, U256};\n+\n+// Examples:\n+// mapping(uint256 => uint256)\n+// uint256[] => [\"uint256[]\"]\n+// mapping(uint256 => uint256[])\n+// mapping(uint256 => mapping(uint256 => uint256))\n+// mapping(uint256 => mapping(uint256 => mapping(uint256 => uint256)[])[])\n+// mapping(uint256 => mapping(uint256 => mapping(uint256 => mapping(uint256 => uint256)[]))[])\n+// mapping(uint256 => mapping(uint256 => uint256)[])\n+fn parse_layout(layout: &mut String) -> Result<Type, String> {\n+    // Check if the layout already includes a visibility modifier and a variable name\n+    if !layout.contains(\"public\") && !layout.contains(\"internal\") && !layout.contains(\"private\") {\n+        layout.push_str(\" public dummyName;\");\n+    }\n+\n+    let parsed_layout = layout\n+        .parse::<TokenStream>()\n+        .map_err(|_| \"Failed to parse layout\".to_string())?;\n+    let parsed_layout = parse2(parsed_layout).map_err(|e| e.to_string())?;\n+\n+    match &parsed_layout.items[0] {\n+        Item::Variable(var) => match &var.ty {\n+            Type::Mapping(map) => Ok(Type::Mapping(map.clone())),\n+            Type::Array(arr) => Ok(Type::Array(arr.clone())),\n+            _ => return Err(\"Unsupported type\".to_string()),\n+        },\n+        _ => return Err(\"Unsupported item\".to_string()),\n+    }\n+}\n+\n+fn parse_mapping_key<'a>(key_type: &'a Type, key: &'a str) -> MappingKey<'a> {\n+    match key_type {\n+        Type::Uint(_, size) => {\n+            let size = size.and_then(|s| Some(s.get())).unwrap_or(256);",
    "repo_full_name": "unionlabs/union",
    "discussion_comments": [
      {
        "comment_id": "1933633258",
        "repo_full_name": "unionlabs/union",
        "pr_number": 3639,
        "pr_file": "tools/slot-calculator/src/main.rs",
        "discussion_id": "1933633258",
        "commented_code": "@@ -0,0 +1,152 @@\n+use clap::{Arg, Command};\n+use proc_macro2::TokenStream;\n+use slotlib::{MappingKey, Slot};\n+use std::{collections::VecDeque, str::FromStr};\n+use syn_solidity::{parse2, Item, Type};\n+use typed_arena::Arena;\n+use unionlabs::primitives::{H256, U256};\n+\n+// Examples:\n+// mapping(uint256 => uint256)\n+// uint256[] => [\"uint256[]\"]\n+// mapping(uint256 => uint256[])\n+// mapping(uint256 => mapping(uint256 => uint256))\n+// mapping(uint256 => mapping(uint256 => mapping(uint256 => uint256)[])[])\n+// mapping(uint256 => mapping(uint256 => mapping(uint256 => mapping(uint256 => uint256)[]))[])\n+// mapping(uint256 => mapping(uint256 => uint256)[])\n+fn parse_layout(layout: &mut String) -> Result<Type, String> {\n+    // Check if the layout already includes a visibility modifier and a variable name\n+    if !layout.contains(\"public\") && !layout.contains(\"internal\") && !layout.contains(\"private\") {\n+        layout.push_str(\" public dummyName;\");\n+    }\n+\n+    let parsed_layout = layout\n+        .parse::<TokenStream>()\n+        .map_err(|_| \"Failed to parse layout\".to_string())?;\n+    let parsed_layout = parse2(parsed_layout).map_err(|e| e.to_string())?;\n+\n+    match &parsed_layout.items[0] {\n+        Item::Variable(var) => match &var.ty {\n+            Type::Mapping(map) => Ok(Type::Mapping(map.clone())),\n+            Type::Array(arr) => Ok(Type::Array(arr.clone())),\n+            _ => return Err(\"Unsupported type\".to_string()),\n+        },\n+        _ => return Err(\"Unsupported item\".to_string()),\n+    }\n+}\n+\n+fn parse_mapping_key<'a>(key_type: &'a Type, key: &'a str) -> MappingKey<'a> {\n+    match key_type {\n+        Type::Uint(_, size) => {\n+            let size = size.and_then(|s| Some(s.get())).unwrap_or(256);",
        "comment_created_at": "2025-01-29T10:34:11+00:00",
        "comment_author": "benluelo",
        "comment_body": "instead of .and_then with Some(), just use .map",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1862575897",
    "pr_number": 3309,
    "pr_file": "hubble/src/indexer/tm/fetcher_client.rs",
    "created_at": "2024-11-28T18:19:45+00:00",
    "commented_code": "let pg_transactions = transactions_response\n             .into_iter()\n-            .filter(|tx| tx.tx_result.code.is_ok())\n+            .filter(|tx| tx.tx_result.code == 0) // 0 == OK",
    "repo_full_name": "unionlabs/union",
    "discussion_comments": [
      {
        "comment_id": "1862575897",
        "repo_full_name": "unionlabs/union",
        "pr_number": 3309,
        "pr_file": "hubble/src/indexer/tm/fetcher_client.rs",
        "discussion_id": "1862575897",
        "commented_code": "@@ -221,7 +224,7 @@ impl TmFetcherClient {\n \n         let pg_transactions = transactions_response\n             .into_iter()\n-            .filter(|tx| tx.tx_result.code.is_ok())\n+            .filter(|tx| tx.tx_result.code == 0) // 0 == OK",
        "comment_created_at": "2024-11-28T18:19:45+00:00",
        "comment_author": "KaiserKarel",
        "comment_body": "perhaps use a const: `CODE_OK = 0`, although that is a nit tbh.",
        "pr_file_module": null
      },
      {
        "comment_id": "1863242471",
        "repo_full_name": "unionlabs/union",
        "pr_number": 3309,
        "pr_file": "hubble/src/indexer/tm/fetcher_client.rs",
        "discussion_id": "1862575897",
        "commented_code": "@@ -221,7 +224,7 @@ impl TmFetcherClient {\n \n         let pg_transactions = transactions_response\n             .into_iter()\n-            .filter(|tx| tx.tx_result.code.is_ok())\n+            .filter(|tx| tx.tx_result.code == 0) // 0 == OK",
        "comment_created_at": "2024-11-29T09:42:31+00:00",
        "comment_author": "qlp",
        "comment_body": "fixed in f40e13a0c013fb1bd8061e281ae6cd5dda72f610",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1768594650",
    "pr_number": 2579,
    "pr_file": "mpc/client/src/main.rs",
    "created_at": "2024-09-20T13:12:11+00:00",
    "commented_code": "+mod types;\n+mod ui;\n+\n+use std::{\n+    io,\n+    net::SocketAddr,\n+    os::unix::fs::MetadataExt,\n+    str::FromStr,\n+    sync::{\n+        atomic::{AtomicBool, Ordering},\n+        Arc,\n+    },\n+    time::{Duration, Instant, UNIX_EPOCH},\n+};\n+\n+use async_sqlite::{rusqlite::OpenFlags, JournalMode, PoolBuilder};\n+use base64::{prelude::BASE64_STANDARD, Engine};\n+use crossterm::{cursor::Show, event, execute};\n+use http_body_util::{BodyExt, Full};\n+use httpdate::parse_http_date;\n+use hyper::{\n+    body::{Buf, Bytes},\n+    service::service_fn,\n+    Method,\n+};\n+use hyper_util::{rt::TokioIo, server::graceful::GracefulShutdown};\n+use mpc_shared::{phase2_contribute, signed_message, supabase::SupabaseMPCApi, CONTRIBUTION_SIZE};\n+use pgp::{\n+    cleartext::CleartextSignedMessage,\n+    crypto::{hash::HashAlgorithm, sym::SymmetricKeyAlgorithm},\n+    types::SecretKeyTrait,\n+    ArmorOptions, Deserializable, KeyType, SecretKeyParamsBuilder, SignedSecretKey,\n+};\n+use ratatui::{backend::CrosstermBackend, Terminal, Viewport};\n+use reqwest::{header::LOCATION, Body};\n+use serde::Deserialize;\n+use tokio::{\n+    net::TcpListener,\n+    sync::{\n+        broadcast::{self, Receiver, Sender},\n+        mpsc, oneshot, RwLock,\n+    },\n+};\n+use tokio_util::sync::CancellationToken;\n+use types::Status;\n+\n+const CONTRIBUTE_ENDPOINT: &str = \"/contribute\";\n+const SK_ENDPOINT: &str = \"/secret_key\";\n+\n+const ZKGM_DIR: &str = \"zkgm\";\n+const CONTRIB_SK_PATH: &str = \"zkgm/contrib_key.sk.asc\";\n+const SUCCESSFUL_PATH: &str = \".zkgm_successful\";\n+\n+#[derive(PartialEq, Eq, Debug, Clone, Deserialize)]\n+#[serde(rename_all = \"camelCase\")]\n+struct Contribute {\n+    supabase_project: String,\n+    bucket: String,\n+    jwt: String,\n+    api_key: String,\n+    contributor_id: String,\n+    payload_id: String,\n+    user_email: Option<String>,\n+}\n+\n+#[derive(thiserror::Error, Debug, Clone)]\n+enum Error {\n+    #[error(\"we are not the current contributor.\")]\n+    NotCurrentContributor,\n+    #[error(\"couldn't find expected header: {0}\")]\n+    HeaderNotFound(String),\n+    #[error(\"current contributor not found.\")]\n+    ContributorNotFound,\n+    #[error(\"current payload not found.\")]\n+    PayloadNotFound,\n+    #[error(transparent)]\n+    Phase2ContributionFailed(#[from] mpc_shared::Phase2ContributionError),\n+    #[error(transparent)]\n+    Phase2VerificationFailed(#[from] mpc_shared::Phase2VerificationError),\n+    #[error(\"pgp key couldn't be found\")]\n+    PGPKeyNotFound,\n+}\n+\n+type BoxBody = http_body_util::combinators::BoxBody<Bytes, hyper::Error>;\n+\n+type DynError = Box<dyn std::error::Error + Send + Sync>;\n+\n+fn temp_file(payload_id: &str) -> String {\n+    format!(\"{ZKGM_DIR}/{payload_id}\")\n+}\n+\n+fn generate_pgp_key(email: String) -> SignedSecretKey {\n+    let mut key_params = SecretKeyParamsBuilder::default();\n+    key_params\n+        .key_type(KeyType::EdDSA)\n+        .can_certify(false)\n+        .can_sign(true)\n+        .can_encrypt(false)\n+        .primary_user_id(email)\n+        .preferred_symmetric_algorithms(\n+            [SymmetricKeyAlgorithm::AES256].to_vec().try_into().unwrap(),\n+        )\n+        .preferred_hash_algorithms([HashAlgorithm::None].to_vec().try_into().unwrap());\n+    let secret_key_params = key_params.build().expect(\"impossible\");\n+    let secret_key = secret_key_params.generate().expect(\"impossible\");\n+    let passwd_fn = || String::new();\n+    let signed_secret_key = secret_key.sign(passwd_fn).expect(\"impossible\");\n+    signed_secret_key\n+}\n+\n+async fn is_already_successful() -> bool {\n+    tokio::fs::metadata(SUCCESSFUL_PATH).await.is_ok()\n+}\n+\n+async fn wait_successful(tx_status: Sender<Status>) {\n+    loop {\n+        if is_already_successful().await {\n+            tx_status.send(Status::Successful).expect(\"impossible\");\n+            tokio::time::sleep(tokio::time::Duration::from_millis(2000)).await;\n+            break;\n+        }\n+    }\n+}\n+\n+async fn contribute(\n+    tx_status: Sender<Status>,\n+    Contribute {\n+        supabase_project,\n+        bucket,\n+        jwt,\n+        api_key,\n+        contributor_id,\n+        payload_id,\n+        ..\n+    }: Contribute,\n+) -> Result<(), DynError> {\n+    if is_already_successful().await {\n+        return Ok(());\n+    }\n+    let mut secret_key = if let Ok(_) = tokio::fs::metadata(CONTRIB_SK_PATH).await {",
    "repo_full_name": "unionlabs/union",
    "discussion_comments": [
      {
        "comment_id": "1768594650",
        "repo_full_name": "unionlabs/union",
        "pr_number": 2579,
        "pr_file": "mpc/client/src/main.rs",
        "discussion_id": "1768594650",
        "commented_code": "@@ -0,0 +1,628 @@\n+mod types;\n+mod ui;\n+\n+use std::{\n+    io,\n+    net::SocketAddr,\n+    os::unix::fs::MetadataExt,\n+    str::FromStr,\n+    sync::{\n+        atomic::{AtomicBool, Ordering},\n+        Arc,\n+    },\n+    time::{Duration, Instant, UNIX_EPOCH},\n+};\n+\n+use async_sqlite::{rusqlite::OpenFlags, JournalMode, PoolBuilder};\n+use base64::{prelude::BASE64_STANDARD, Engine};\n+use crossterm::{cursor::Show, event, execute};\n+use http_body_util::{BodyExt, Full};\n+use httpdate::parse_http_date;\n+use hyper::{\n+    body::{Buf, Bytes},\n+    service::service_fn,\n+    Method,\n+};\n+use hyper_util::{rt::TokioIo, server::graceful::GracefulShutdown};\n+use mpc_shared::{phase2_contribute, signed_message, supabase::SupabaseMPCApi, CONTRIBUTION_SIZE};\n+use pgp::{\n+    cleartext::CleartextSignedMessage,\n+    crypto::{hash::HashAlgorithm, sym::SymmetricKeyAlgorithm},\n+    types::SecretKeyTrait,\n+    ArmorOptions, Deserializable, KeyType, SecretKeyParamsBuilder, SignedSecretKey,\n+};\n+use ratatui::{backend::CrosstermBackend, Terminal, Viewport};\n+use reqwest::{header::LOCATION, Body};\n+use serde::Deserialize;\n+use tokio::{\n+    net::TcpListener,\n+    sync::{\n+        broadcast::{self, Receiver, Sender},\n+        mpsc, oneshot, RwLock,\n+    },\n+};\n+use tokio_util::sync::CancellationToken;\n+use types::Status;\n+\n+const CONTRIBUTE_ENDPOINT: &str = \"/contribute\";\n+const SK_ENDPOINT: &str = \"/secret_key\";\n+\n+const ZKGM_DIR: &str = \"zkgm\";\n+const CONTRIB_SK_PATH: &str = \"zkgm/contrib_key.sk.asc\";\n+const SUCCESSFUL_PATH: &str = \".zkgm_successful\";\n+\n+#[derive(PartialEq, Eq, Debug, Clone, Deserialize)]\n+#[serde(rename_all = \"camelCase\")]\n+struct Contribute {\n+    supabase_project: String,\n+    bucket: String,\n+    jwt: String,\n+    api_key: String,\n+    contributor_id: String,\n+    payload_id: String,\n+    user_email: Option<String>,\n+}\n+\n+#[derive(thiserror::Error, Debug, Clone)]\n+enum Error {\n+    #[error(\"we are not the current contributor.\")]\n+    NotCurrentContributor,\n+    #[error(\"couldn't find expected header: {0}\")]\n+    HeaderNotFound(String),\n+    #[error(\"current contributor not found.\")]\n+    ContributorNotFound,\n+    #[error(\"current payload not found.\")]\n+    PayloadNotFound,\n+    #[error(transparent)]\n+    Phase2ContributionFailed(#[from] mpc_shared::Phase2ContributionError),\n+    #[error(transparent)]\n+    Phase2VerificationFailed(#[from] mpc_shared::Phase2VerificationError),\n+    #[error(\"pgp key couldn't be found\")]\n+    PGPKeyNotFound,\n+}\n+\n+type BoxBody = http_body_util::combinators::BoxBody<Bytes, hyper::Error>;\n+\n+type DynError = Box<dyn std::error::Error + Send + Sync>;\n+\n+fn temp_file(payload_id: &str) -> String {\n+    format!(\"{ZKGM_DIR}/{payload_id}\")\n+}\n+\n+fn generate_pgp_key(email: String) -> SignedSecretKey {\n+    let mut key_params = SecretKeyParamsBuilder::default();\n+    key_params\n+        .key_type(KeyType::EdDSA)\n+        .can_certify(false)\n+        .can_sign(true)\n+        .can_encrypt(false)\n+        .primary_user_id(email)\n+        .preferred_symmetric_algorithms(\n+            [SymmetricKeyAlgorithm::AES256].to_vec().try_into().unwrap(),\n+        )\n+        .preferred_hash_algorithms([HashAlgorithm::None].to_vec().try_into().unwrap());\n+    let secret_key_params = key_params.build().expect(\"impossible\");\n+    let secret_key = secret_key_params.generate().expect(\"impossible\");\n+    let passwd_fn = || String::new();\n+    let signed_secret_key = secret_key.sign(passwd_fn).expect(\"impossible\");\n+    signed_secret_key\n+}\n+\n+async fn is_already_successful() -> bool {\n+    tokio::fs::metadata(SUCCESSFUL_PATH).await.is_ok()\n+}\n+\n+async fn wait_successful(tx_status: Sender<Status>) {\n+    loop {\n+        if is_already_successful().await {\n+            tx_status.send(Status::Successful).expect(\"impossible\");\n+            tokio::time::sleep(tokio::time::Duration::from_millis(2000)).await;\n+            break;\n+        }\n+    }\n+}\n+\n+async fn contribute(\n+    tx_status: Sender<Status>,\n+    Contribute {\n+        supabase_project,\n+        bucket,\n+        jwt,\n+        api_key,\n+        contributor_id,\n+        payload_id,\n+        ..\n+    }: Contribute,\n+) -> Result<(), DynError> {\n+    if is_already_successful().await {\n+        return Ok(());\n+    }\n+    let mut secret_key = if let Ok(_) = tokio::fs::metadata(CONTRIB_SK_PATH).await {",
        "comment_created_at": "2024-09-20T13:12:11+00:00",
        "comment_author": "benluelo",
        "comment_body": "clippy will probably tell you to use `.is_ok()` here",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1768598809",
    "pr_number": 2579,
    "pr_file": "mpc/client/src/main.rs",
    "created_at": "2024-09-20T13:15:23+00:00",
    "commented_code": "+mod types;\n+mod ui;\n+\n+use std::{\n+    io,\n+    net::SocketAddr,\n+    os::unix::fs::MetadataExt,\n+    str::FromStr,\n+    sync::{\n+        atomic::{AtomicBool, Ordering},\n+        Arc,\n+    },\n+    time::{Duration, Instant, UNIX_EPOCH},\n+};\n+\n+use async_sqlite::{rusqlite::OpenFlags, JournalMode, PoolBuilder};\n+use base64::{prelude::BASE64_STANDARD, Engine};\n+use crossterm::{cursor::Show, event, execute};\n+use http_body_util::{BodyExt, Full};\n+use httpdate::parse_http_date;\n+use hyper::{\n+    body::{Buf, Bytes},\n+    service::service_fn,\n+    Method,\n+};\n+use hyper_util::{rt::TokioIo, server::graceful::GracefulShutdown};\n+use mpc_shared::{phase2_contribute, signed_message, supabase::SupabaseMPCApi, CONTRIBUTION_SIZE};\n+use pgp::{\n+    cleartext::CleartextSignedMessage,\n+    crypto::{hash::HashAlgorithm, sym::SymmetricKeyAlgorithm},\n+    types::SecretKeyTrait,\n+    ArmorOptions, Deserializable, KeyType, SecretKeyParamsBuilder, SignedSecretKey,\n+};\n+use ratatui::{backend::CrosstermBackend, Terminal, Viewport};\n+use reqwest::{header::LOCATION, Body};\n+use serde::Deserialize;\n+use tokio::{\n+    net::TcpListener,\n+    sync::{\n+        broadcast::{self, Receiver, Sender},\n+        mpsc, oneshot, RwLock,\n+    },\n+};\n+use tokio_util::sync::CancellationToken;\n+use types::Status;\n+\n+const CONTRIBUTE_ENDPOINT: &str = \"/contribute\";\n+const SK_ENDPOINT: &str = \"/secret_key\";\n+\n+const ZKGM_DIR: &str = \"zkgm\";\n+const CONTRIB_SK_PATH: &str = \"zkgm/contrib_key.sk.asc\";\n+const SUCCESSFUL_PATH: &str = \".zkgm_successful\";\n+\n+#[derive(PartialEq, Eq, Debug, Clone, Deserialize)]\n+#[serde(rename_all = \"camelCase\")]\n+struct Contribute {\n+    supabase_project: String,\n+    bucket: String,\n+    jwt: String,\n+    api_key: String,\n+    contributor_id: String,\n+    payload_id: String,\n+    user_email: Option<String>,\n+}\n+\n+#[derive(thiserror::Error, Debug, Clone)]\n+enum Error {\n+    #[error(\"we are not the current contributor.\")]\n+    NotCurrentContributor,\n+    #[error(\"couldn't find expected header: {0}\")]\n+    HeaderNotFound(String),\n+    #[error(\"current contributor not found.\")]\n+    ContributorNotFound,\n+    #[error(\"current payload not found.\")]\n+    PayloadNotFound,\n+    #[error(transparent)]\n+    Phase2ContributionFailed(#[from] mpc_shared::Phase2ContributionError),\n+    #[error(transparent)]\n+    Phase2VerificationFailed(#[from] mpc_shared::Phase2VerificationError),\n+    #[error(\"pgp key couldn't be found\")]\n+    PGPKeyNotFound,\n+}\n+\n+type BoxBody = http_body_util::combinators::BoxBody<Bytes, hyper::Error>;\n+\n+type DynError = Box<dyn std::error::Error + Send + Sync>;\n+\n+fn temp_file(payload_id: &str) -> String {\n+    format!(\"{ZKGM_DIR}/{payload_id}\")\n+}\n+\n+fn generate_pgp_key(email: String) -> SignedSecretKey {\n+    let mut key_params = SecretKeyParamsBuilder::default();\n+    key_params\n+        .key_type(KeyType::EdDSA)\n+        .can_certify(false)\n+        .can_sign(true)\n+        .can_encrypt(false)\n+        .primary_user_id(email)\n+        .preferred_symmetric_algorithms(\n+            [SymmetricKeyAlgorithm::AES256].to_vec().try_into().unwrap(),\n+        )\n+        .preferred_hash_algorithms([HashAlgorithm::None].to_vec().try_into().unwrap());\n+    let secret_key_params = key_params.build().expect(\"impossible\");\n+    let secret_key = secret_key_params.generate().expect(\"impossible\");\n+    let passwd_fn = || String::new();\n+    let signed_secret_key = secret_key.sign(passwd_fn).expect(\"impossible\");\n+    signed_secret_key\n+}\n+\n+async fn is_already_successful() -> bool {\n+    tokio::fs::metadata(SUCCESSFUL_PATH).await.is_ok()\n+}\n+\n+async fn wait_successful(tx_status: Sender<Status>) {\n+    loop {\n+        if is_already_successful().await {\n+            tx_status.send(Status::Successful).expect(\"impossible\");\n+            tokio::time::sleep(tokio::time::Duration::from_millis(2000)).await;\n+            break;\n+        }\n+    }\n+}\n+\n+async fn contribute(\n+    tx_status: Sender<Status>,\n+    Contribute {\n+        supabase_project,\n+        bucket,\n+        jwt,\n+        api_key,\n+        contributor_id,\n+        payload_id,\n+        ..\n+    }: Contribute,\n+) -> Result<(), DynError> {\n+    if is_already_successful().await {\n+        return Ok(());\n+    }\n+    let mut secret_key = if let Ok(_) = tokio::fs::metadata(CONTRIB_SK_PATH).await {\n+        SignedSecretKey::from_armor_single::<&[u8]>(\n+            tokio::fs::read(CONTRIB_SK_PATH).await?.as_ref(),\n+        )\n+        .expect(\"impossible\")\n+        .0\n+    } else {\n+        return Err(Error::PGPKeyNotFound.into());\n+    };\n+    let client = SupabaseMPCApi::new(supabase_project.clone(), api_key, jwt);\n+    let current_contributor = client\n+        .current_contributor()\n+        .await?\n+        .ok_or(Error::ContributorNotFound)?;\n+    if current_contributor.id != contributor_id {\n+        return Err(Error::NotCurrentContributor.into());\n+    }\n+    let current_payload = client\n+        .current_payload()\n+        .await?\n+        .ok_or(Error::PayloadNotFound)?;\n+    tx_status\n+        .send(Status::DownloadStarted(current_payload.id.clone()))\n+        .expect(\"impossible\");\n+    let payload = client\n+        .download_payload(&current_payload.id, &current_payload.id, |percent| {\n+            let tx_status = tx_status.clone();\n+            let current_payload_clone = current_payload.id.clone();\n+            async move {\n+                tx_status\n+                    .send(Status::Downloading(current_payload_clone, percent as u8))\n+                    .expect(\"impossible\");\n+            }\n+        })\n+        .await?;\n+    tx_status\n+        .send(Status::DownloadEnded(current_payload.id.clone()))\n+        .expect(\"impossible\");\n+    let phase2_contribution = if let Ok(true) = tokio::fs::metadata(temp_file(&payload_id))\n+        .await\n+        .map(|meta| meta.size() as usize == CONTRIBUTION_SIZE)\n+    {\n+        tokio::fs::read(temp_file(&payload_id)).await?\n+    } else {\n+        tx_status\n+            .send(Status::ContributionStarted)\n+            .expect(\"impossible\");\n+        let (tx_contrib, rx_contrib) = oneshot::channel();\n+        let handle = tokio::task::spawn_blocking(move || {\n+            tx_contrib\n+                .send(phase2_contribute(&payload))\n+                .expect(\"impossible\");\n+        });\n+        let phase2_contribution = rx_contrib.await??;\n+        handle.await?;\n+        tx_status\n+            .send(Status::ContributionEnded)\n+            .expect(\"impossible\");\n+        tokio::fs::write(temp_file(&payload_id), &phase2_contribution).await?;\n+        phase2_contribution\n+    };\n+\n+    // ------------------------\n+    // Sign and submits the sig\n+    // Gnark phase2 contribution appends the sha256 hash at the end\n+    let phase2_contribution_hash = &phase2_contribution[phase2_contribution.len() - 32..];\n+    let signature = CleartextSignedMessage::sign(\n+        &signed_message(\n+            &current_payload.id,\n+            &payload_id,\n+            &hex::encode(phase2_contribution_hash),\n+        ),\n+        &mut secret_key,\n+        || String::new(),\n+    )\n+    .expect(\"impossible\");\n+    let public_key = secret_key\n+        .public_key()\n+        .sign(&secret_key, || String::new())\n+        .expect(\"impossible\")\n+        .to_armored_bytes(ArmorOptions::default())\n+        .expect(\"impossible\");\n+    client\n+        .insert_contribution_signature(\n+            current_contributor.id,\n+            public_key,\n+            signature\n+                .to_armored_bytes(ArmorOptions::default())\n+                .expect(\"impossible\"),\n+        )\n+        .await?;\n+    let pool = PoolBuilder::new()\n+        .path(temp_file(\"state.sqlite3\"))\n+        .flags(\n+            OpenFlags::SQLITE_OPEN_READ_WRITE\n+                | OpenFlags::SQLITE_OPEN_CREATE\n+                | OpenFlags::SQLITE_OPEN_FULL_MUTEX\n+                | OpenFlags::SQLITE_OPEN_URI,\n+        )\n+        .journal_mode(JournalMode::Wal)\n+        .num_conns(1)\n+        .open()\n+        .await?;\n+    pool.conn(|conn| {\n+        conn.execute(\n+            \"CREATE TABLE IF NOT EXISTS resumable_upload (\n+                         location TEXT PRIMARY KEY NOT NULL,\n+                         create_at TIMESTAMPTZ NOT NULL DEFAULT(unixepoch()),\n+                         expire TIMSTAMPTZ NOT NULL\n+                     )\",\n+            (), // empty list of parameters.\n+        )?;\n+        Ok(())\n+    })\n+    .await?;\n+    let mut upload_location = pool\n+        .conn(move |conn| {\n+            let mut stmt = conn.prepare(\n+                \"SELECT location FROM resumable_upload WHERE expire > unixepoch() LIMIT 1\",\n+            )?;\n+            let mut rows = stmt.query(())?;\n+            if let Some(row) = rows.next()? {\n+                Ok(Some(row.get::<_, String>(0)?))\n+            } else {\n+                Ok(None)\n+            }\n+        })\n+        .await?;\n+    let upload_client = client.new_reqwest_builder()?.build()?;\n+    if let Some(ref location) = upload_location {\n+        if upload_client\n+            .head(location)\n+            .header(\"Tus-Resumable\", \"1.0.0\")\n+            .send()\n+            .await?\n+            .error_for_status()\n+            .is_err()\n+        {\n+            upload_location = None;\n+        }\n+    }\n+    let upload_location = match upload_location {\n+        Some(location) => location,\n+        None => {\n+            // =====================================================\n+            // https://tus.io/protocols/resumable-upload#creation ==\n+            // =====================================================\n+            let response = upload_client\n+                .post(format!(\"{supabase_project}/storage/v1/upload/resumable\"))\n+                .header(\"Tus-Resumable\", \"1.0.0\")\n+                .header(\"Upload-Length\", CONTRIBUTION_SIZE.to_string())\n+                .header(\n+                    \"Upload-Metadata\",\n+                    format!(\n+                        \"bucketName {},objectName {}\",\n+                        BASE64_STANDARD.encode(&bucket),\n+                        BASE64_STANDARD.encode(&payload_id)\n+                    ),\n+                )\n+                .send()\n+                .await?\n+                .error_for_status()?;\n+            let location = response\n+                .headers()\n+                .get(LOCATION)\n+                .ok_or(Error::HeaderNotFound(LOCATION.as_str().into()))?\n+                .to_str()?\n+                .to_string();\n+            let expire = response\n+                .headers()\n+                .get(\"Upload-Expires\")\n+                .ok_or(Error::HeaderNotFound(\"Upload-Expires\".into()))?\n+                .to_str()?\n+                .into();\n+            let expire = parse_http_date(expire)?;\n+            let expire_timestamp = expire.duration_since(UNIX_EPOCH)?.as_secs();\n+            let location_clone = location.clone();\n+            pool.conn(move |conn| {\n+                let mut stmt =\n+                    conn.prepare(\"INSERT INTO resumable_upload (location, expire) VALUES (?, ?)\")?;\n+                let r = stmt.execute((location_clone, expire_timestamp))?;\n+                assert!(r == 1);\n+                Ok(())\n+            })\n+            .await?;\n+            location\n+        }\n+    };\n+    // =================================================\n+    // https://tus.io/protocols/resumable-upload#head ==\n+    // =================================================\n+    let response = upload_client\n+        .head(&upload_location)\n+        .header(\"Tus-Resumable\", \"1.0.0\")\n+        .send()\n+        .await?\n+        .error_for_status()?;\n+    let upload_length = usize::from_str(\n+        response\n+            .headers()\n+            .get(\"Upload-Length\")\n+            .ok_or(Error::HeaderNotFound(\"Upload-Length\".into()))?\n+            .to_str()?,\n+    )?;\n+    let upload_offset = usize::from_str(\n+        response\n+            .headers()\n+            .get(\"Upload-Offset\")\n+            .ok_or(Error::HeaderNotFound(\"Upload-Offset\".into()))?\n+            .to_str()?,\n+    )?;\n+    assert!(upload_length == CONTRIBUTION_SIZE, \"invalid upload-length.\");\n+    if upload_offset < upload_length {\n+        tx_status\n+            .send(Status::UploadStarted(payload_id.clone()))\n+            .expect(\"impossible\");\n+        // ==================================================\n+        // https://tus.io/protocols/resumable-upload#patch ==\n+        // ==================================================\n+        let chunks = phase2_contribution\n+            .into_iter()\n+            .skip(upload_offset)\n+            .collect::<Vec<_>>()\n+            // 1mb\n+            .chunks(1024 * 1024)\n+            .map(|x| Ok::<_, std::io::Error>(x.to_vec()))\n+            .collect::<Vec<_>>();\n+        upload_client\n+            .patch(&upload_location)\n+            .header(\"Tus-Resumable\", \"1.0.0\")\n+            .header(\"Content-Type\", \"application/offset+octet-stream\")\n+            .header(\"Upload-Offset\", upload_offset.to_string())\n+            .body(Body::wrap_stream(futures_util::stream::iter(chunks)))\n+            .send()\n+            .await?\n+            .error_for_status()?;\n+        tx_status\n+            .send(Status::UploadEnded(payload_id.clone()))\n+            .expect(\"impossible\");\n+    }\n+    pool.close().await?;\n+    Ok(())\n+}\n+\n+fn full<T: Into<Bytes>>(chunk: T) -> BoxBody {\n+    Full::new(chunk.into())\n+        .map_err(|never| match never {})\n+        .boxed()\n+}\n+\n+async fn handle(\n+    lock: Arc<AtomicBool>,\n+    tx_status: Sender<Status>,\n+    latest_status: Arc<RwLock<Status>>,\n+    req: hyper::Request<hyper::body::Incoming>,\n+) -> Result<hyper::Response<BoxBody>, DynError> {\n+    let response = |status, body| {\n+        Ok(hyper::Response::builder()\n+            .header(hyper::header::ACCESS_CONTROL_ALLOW_ORIGIN, \"*\")\n+            .header(hyper::header::CONTENT_TYPE, \"application/json\")\n+            .status(status)\n+            .body(body)\n+            .unwrap())\n+    };\n+    let file_response = |status, body| {\n+        Ok(hyper::Response::builder()\n+            .header(hyper::header::ACCESS_CONTROL_ALLOW_ORIGIN, \"*\")\n+            .header(hyper::header::CONTENT_TYPE, \"application/octet-stream\")\n+            .header(\n+                hyper::header::CONTENT_DISPOSITION,\n+                format!(\"attachment; filename={CONTRIB_SK_PATH}\"),\n+            )\n+            .status(status)\n+            .body(body)\n+            .unwrap())\n+    };\n+    let response_empty = |status| response(status, BoxBody::default());\n+    match (req.method(), req.uri().path()) {\n+        (&Method::POST, SK_ENDPOINT) => {\n+            let whole_body = req.collect().await?.aggregate();\n+            let email = serde_json::from_reader(whole_body.reader())?;\n+            let guard = latest_status.write().await;\n+            let result = {\n+                if let Err(_) = tokio::fs::metadata(CONTRIB_SK_PATH).await {\n+                    let secret_key = generate_pgp_key(email);\n+                    let secret_key_serialized = secret_key\n+                        .to_armored_bytes(ArmorOptions::default())\n+                        .expect(\"impossible\");\n+                    tokio::fs::write(CONTRIB_SK_PATH, &secret_key_serialized).await?;\n+                    response_empty(hyper::StatusCode::CREATED)\n+                } else {\n+                    response_empty(hyper::StatusCode::OK)\n+                }\n+            };\n+            drop(guard);\n+            result\n+        }\n+        (&Method::GET, SK_ENDPOINT) => {\n+            if let Err(_) = tokio::fs::metadata(CONTRIB_SK_PATH).await {\n+                response_empty(hyper::StatusCode::NOT_FOUND)\n+            } else {\n+                let content = tokio::fs::read(CONTRIB_SK_PATH).await?;\n+                file_response(hyper::StatusCode::OK, full(content))\n+            }\n+        }\n+        (&Method::POST, CONTRIBUTE_ENDPOINT)\n+            if lock\n+                .compare_exchange(false, true, Ordering::SeqCst, Ordering::SeqCst)\n+                .is_ok() =>",
    "repo_full_name": "unionlabs/union",
    "discussion_comments": [
      {
        "comment_id": "1768598809",
        "repo_full_name": "unionlabs/union",
        "pr_number": 2579,
        "pr_file": "mpc/client/src/main.rs",
        "discussion_id": "1768598809",
        "commented_code": "@@ -0,0 +1,628 @@\n+mod types;\n+mod ui;\n+\n+use std::{\n+    io,\n+    net::SocketAddr,\n+    os::unix::fs::MetadataExt,\n+    str::FromStr,\n+    sync::{\n+        atomic::{AtomicBool, Ordering},\n+        Arc,\n+    },\n+    time::{Duration, Instant, UNIX_EPOCH},\n+};\n+\n+use async_sqlite::{rusqlite::OpenFlags, JournalMode, PoolBuilder};\n+use base64::{prelude::BASE64_STANDARD, Engine};\n+use crossterm::{cursor::Show, event, execute};\n+use http_body_util::{BodyExt, Full};\n+use httpdate::parse_http_date;\n+use hyper::{\n+    body::{Buf, Bytes},\n+    service::service_fn,\n+    Method,\n+};\n+use hyper_util::{rt::TokioIo, server::graceful::GracefulShutdown};\n+use mpc_shared::{phase2_contribute, signed_message, supabase::SupabaseMPCApi, CONTRIBUTION_SIZE};\n+use pgp::{\n+    cleartext::CleartextSignedMessage,\n+    crypto::{hash::HashAlgorithm, sym::SymmetricKeyAlgorithm},\n+    types::SecretKeyTrait,\n+    ArmorOptions, Deserializable, KeyType, SecretKeyParamsBuilder, SignedSecretKey,\n+};\n+use ratatui::{backend::CrosstermBackend, Terminal, Viewport};\n+use reqwest::{header::LOCATION, Body};\n+use serde::Deserialize;\n+use tokio::{\n+    net::TcpListener,\n+    sync::{\n+        broadcast::{self, Receiver, Sender},\n+        mpsc, oneshot, RwLock,\n+    },\n+};\n+use tokio_util::sync::CancellationToken;\n+use types::Status;\n+\n+const CONTRIBUTE_ENDPOINT: &str = \"/contribute\";\n+const SK_ENDPOINT: &str = \"/secret_key\";\n+\n+const ZKGM_DIR: &str = \"zkgm\";\n+const CONTRIB_SK_PATH: &str = \"zkgm/contrib_key.sk.asc\";\n+const SUCCESSFUL_PATH: &str = \".zkgm_successful\";\n+\n+#[derive(PartialEq, Eq, Debug, Clone, Deserialize)]\n+#[serde(rename_all = \"camelCase\")]\n+struct Contribute {\n+    supabase_project: String,\n+    bucket: String,\n+    jwt: String,\n+    api_key: String,\n+    contributor_id: String,\n+    payload_id: String,\n+    user_email: Option<String>,\n+}\n+\n+#[derive(thiserror::Error, Debug, Clone)]\n+enum Error {\n+    #[error(\"we are not the current contributor.\")]\n+    NotCurrentContributor,\n+    #[error(\"couldn't find expected header: {0}\")]\n+    HeaderNotFound(String),\n+    #[error(\"current contributor not found.\")]\n+    ContributorNotFound,\n+    #[error(\"current payload not found.\")]\n+    PayloadNotFound,\n+    #[error(transparent)]\n+    Phase2ContributionFailed(#[from] mpc_shared::Phase2ContributionError),\n+    #[error(transparent)]\n+    Phase2VerificationFailed(#[from] mpc_shared::Phase2VerificationError),\n+    #[error(\"pgp key couldn't be found\")]\n+    PGPKeyNotFound,\n+}\n+\n+type BoxBody = http_body_util::combinators::BoxBody<Bytes, hyper::Error>;\n+\n+type DynError = Box<dyn std::error::Error + Send + Sync>;\n+\n+fn temp_file(payload_id: &str) -> String {\n+    format!(\"{ZKGM_DIR}/{payload_id}\")\n+}\n+\n+fn generate_pgp_key(email: String) -> SignedSecretKey {\n+    let mut key_params = SecretKeyParamsBuilder::default();\n+    key_params\n+        .key_type(KeyType::EdDSA)\n+        .can_certify(false)\n+        .can_sign(true)\n+        .can_encrypt(false)\n+        .primary_user_id(email)\n+        .preferred_symmetric_algorithms(\n+            [SymmetricKeyAlgorithm::AES256].to_vec().try_into().unwrap(),\n+        )\n+        .preferred_hash_algorithms([HashAlgorithm::None].to_vec().try_into().unwrap());\n+    let secret_key_params = key_params.build().expect(\"impossible\");\n+    let secret_key = secret_key_params.generate().expect(\"impossible\");\n+    let passwd_fn = || String::new();\n+    let signed_secret_key = secret_key.sign(passwd_fn).expect(\"impossible\");\n+    signed_secret_key\n+}\n+\n+async fn is_already_successful() -> bool {\n+    tokio::fs::metadata(SUCCESSFUL_PATH).await.is_ok()\n+}\n+\n+async fn wait_successful(tx_status: Sender<Status>) {\n+    loop {\n+        if is_already_successful().await {\n+            tx_status.send(Status::Successful).expect(\"impossible\");\n+            tokio::time::sleep(tokio::time::Duration::from_millis(2000)).await;\n+            break;\n+        }\n+    }\n+}\n+\n+async fn contribute(\n+    tx_status: Sender<Status>,\n+    Contribute {\n+        supabase_project,\n+        bucket,\n+        jwt,\n+        api_key,\n+        contributor_id,\n+        payload_id,\n+        ..\n+    }: Contribute,\n+) -> Result<(), DynError> {\n+    if is_already_successful().await {\n+        return Ok(());\n+    }\n+    let mut secret_key = if let Ok(_) = tokio::fs::metadata(CONTRIB_SK_PATH).await {\n+        SignedSecretKey::from_armor_single::<&[u8]>(\n+            tokio::fs::read(CONTRIB_SK_PATH).await?.as_ref(),\n+        )\n+        .expect(\"impossible\")\n+        .0\n+    } else {\n+        return Err(Error::PGPKeyNotFound.into());\n+    };\n+    let client = SupabaseMPCApi::new(supabase_project.clone(), api_key, jwt);\n+    let current_contributor = client\n+        .current_contributor()\n+        .await?\n+        .ok_or(Error::ContributorNotFound)?;\n+    if current_contributor.id != contributor_id {\n+        return Err(Error::NotCurrentContributor.into());\n+    }\n+    let current_payload = client\n+        .current_payload()\n+        .await?\n+        .ok_or(Error::PayloadNotFound)?;\n+    tx_status\n+        .send(Status::DownloadStarted(current_payload.id.clone()))\n+        .expect(\"impossible\");\n+    let payload = client\n+        .download_payload(&current_payload.id, &current_payload.id, |percent| {\n+            let tx_status = tx_status.clone();\n+            let current_payload_clone = current_payload.id.clone();\n+            async move {\n+                tx_status\n+                    .send(Status::Downloading(current_payload_clone, percent as u8))\n+                    .expect(\"impossible\");\n+            }\n+        })\n+        .await?;\n+    tx_status\n+        .send(Status::DownloadEnded(current_payload.id.clone()))\n+        .expect(\"impossible\");\n+    let phase2_contribution = if let Ok(true) = tokio::fs::metadata(temp_file(&payload_id))\n+        .await\n+        .map(|meta| meta.size() as usize == CONTRIBUTION_SIZE)\n+    {\n+        tokio::fs::read(temp_file(&payload_id)).await?\n+    } else {\n+        tx_status\n+            .send(Status::ContributionStarted)\n+            .expect(\"impossible\");\n+        let (tx_contrib, rx_contrib) = oneshot::channel();\n+        let handle = tokio::task::spawn_blocking(move || {\n+            tx_contrib\n+                .send(phase2_contribute(&payload))\n+                .expect(\"impossible\");\n+        });\n+        let phase2_contribution = rx_contrib.await??;\n+        handle.await?;\n+        tx_status\n+            .send(Status::ContributionEnded)\n+            .expect(\"impossible\");\n+        tokio::fs::write(temp_file(&payload_id), &phase2_contribution).await?;\n+        phase2_contribution\n+    };\n+\n+    // ------------------------\n+    // Sign and submits the sig\n+    // Gnark phase2 contribution appends the sha256 hash at the end\n+    let phase2_contribution_hash = &phase2_contribution[phase2_contribution.len() - 32..];\n+    let signature = CleartextSignedMessage::sign(\n+        &signed_message(\n+            &current_payload.id,\n+            &payload_id,\n+            &hex::encode(phase2_contribution_hash),\n+        ),\n+        &mut secret_key,\n+        || String::new(),\n+    )\n+    .expect(\"impossible\");\n+    let public_key = secret_key\n+        .public_key()\n+        .sign(&secret_key, || String::new())\n+        .expect(\"impossible\")\n+        .to_armored_bytes(ArmorOptions::default())\n+        .expect(\"impossible\");\n+    client\n+        .insert_contribution_signature(\n+            current_contributor.id,\n+            public_key,\n+            signature\n+                .to_armored_bytes(ArmorOptions::default())\n+                .expect(\"impossible\"),\n+        )\n+        .await?;\n+    let pool = PoolBuilder::new()\n+        .path(temp_file(\"state.sqlite3\"))\n+        .flags(\n+            OpenFlags::SQLITE_OPEN_READ_WRITE\n+                | OpenFlags::SQLITE_OPEN_CREATE\n+                | OpenFlags::SQLITE_OPEN_FULL_MUTEX\n+                | OpenFlags::SQLITE_OPEN_URI,\n+        )\n+        .journal_mode(JournalMode::Wal)\n+        .num_conns(1)\n+        .open()\n+        .await?;\n+    pool.conn(|conn| {\n+        conn.execute(\n+            \"CREATE TABLE IF NOT EXISTS resumable_upload (\n+                         location TEXT PRIMARY KEY NOT NULL,\n+                         create_at TIMESTAMPTZ NOT NULL DEFAULT(unixepoch()),\n+                         expire TIMSTAMPTZ NOT NULL\n+                     )\",\n+            (), // empty list of parameters.\n+        )?;\n+        Ok(())\n+    })\n+    .await?;\n+    let mut upload_location = pool\n+        .conn(move |conn| {\n+            let mut stmt = conn.prepare(\n+                \"SELECT location FROM resumable_upload WHERE expire > unixepoch() LIMIT 1\",\n+            )?;\n+            let mut rows = stmt.query(())?;\n+            if let Some(row) = rows.next()? {\n+                Ok(Some(row.get::<_, String>(0)?))\n+            } else {\n+                Ok(None)\n+            }\n+        })\n+        .await?;\n+    let upload_client = client.new_reqwest_builder()?.build()?;\n+    if let Some(ref location) = upload_location {\n+        if upload_client\n+            .head(location)\n+            .header(\"Tus-Resumable\", \"1.0.0\")\n+            .send()\n+            .await?\n+            .error_for_status()\n+            .is_err()\n+        {\n+            upload_location = None;\n+        }\n+    }\n+    let upload_location = match upload_location {\n+        Some(location) => location,\n+        None => {\n+            // =====================================================\n+            // https://tus.io/protocols/resumable-upload#creation ==\n+            // =====================================================\n+            let response = upload_client\n+                .post(format!(\"{supabase_project}/storage/v1/upload/resumable\"))\n+                .header(\"Tus-Resumable\", \"1.0.0\")\n+                .header(\"Upload-Length\", CONTRIBUTION_SIZE.to_string())\n+                .header(\n+                    \"Upload-Metadata\",\n+                    format!(\n+                        \"bucketName {},objectName {}\",\n+                        BASE64_STANDARD.encode(&bucket),\n+                        BASE64_STANDARD.encode(&payload_id)\n+                    ),\n+                )\n+                .send()\n+                .await?\n+                .error_for_status()?;\n+            let location = response\n+                .headers()\n+                .get(LOCATION)\n+                .ok_or(Error::HeaderNotFound(LOCATION.as_str().into()))?\n+                .to_str()?\n+                .to_string();\n+            let expire = response\n+                .headers()\n+                .get(\"Upload-Expires\")\n+                .ok_or(Error::HeaderNotFound(\"Upload-Expires\".into()))?\n+                .to_str()?\n+                .into();\n+            let expire = parse_http_date(expire)?;\n+            let expire_timestamp = expire.duration_since(UNIX_EPOCH)?.as_secs();\n+            let location_clone = location.clone();\n+            pool.conn(move |conn| {\n+                let mut stmt =\n+                    conn.prepare(\"INSERT INTO resumable_upload (location, expire) VALUES (?, ?)\")?;\n+                let r = stmt.execute((location_clone, expire_timestamp))?;\n+                assert!(r == 1);\n+                Ok(())\n+            })\n+            .await?;\n+            location\n+        }\n+    };\n+    // =================================================\n+    // https://tus.io/protocols/resumable-upload#head ==\n+    // =================================================\n+    let response = upload_client\n+        .head(&upload_location)\n+        .header(\"Tus-Resumable\", \"1.0.0\")\n+        .send()\n+        .await?\n+        .error_for_status()?;\n+    let upload_length = usize::from_str(\n+        response\n+            .headers()\n+            .get(\"Upload-Length\")\n+            .ok_or(Error::HeaderNotFound(\"Upload-Length\".into()))?\n+            .to_str()?,\n+    )?;\n+    let upload_offset = usize::from_str(\n+        response\n+            .headers()\n+            .get(\"Upload-Offset\")\n+            .ok_or(Error::HeaderNotFound(\"Upload-Offset\".into()))?\n+            .to_str()?,\n+    )?;\n+    assert!(upload_length == CONTRIBUTION_SIZE, \"invalid upload-length.\");\n+    if upload_offset < upload_length {\n+        tx_status\n+            .send(Status::UploadStarted(payload_id.clone()))\n+            .expect(\"impossible\");\n+        // ==================================================\n+        // https://tus.io/protocols/resumable-upload#patch ==\n+        // ==================================================\n+        let chunks = phase2_contribution\n+            .into_iter()\n+            .skip(upload_offset)\n+            .collect::<Vec<_>>()\n+            // 1mb\n+            .chunks(1024 * 1024)\n+            .map(|x| Ok::<_, std::io::Error>(x.to_vec()))\n+            .collect::<Vec<_>>();\n+        upload_client\n+            .patch(&upload_location)\n+            .header(\"Tus-Resumable\", \"1.0.0\")\n+            .header(\"Content-Type\", \"application/offset+octet-stream\")\n+            .header(\"Upload-Offset\", upload_offset.to_string())\n+            .body(Body::wrap_stream(futures_util::stream::iter(chunks)))\n+            .send()\n+            .await?\n+            .error_for_status()?;\n+        tx_status\n+            .send(Status::UploadEnded(payload_id.clone()))\n+            .expect(\"impossible\");\n+    }\n+    pool.close().await?;\n+    Ok(())\n+}\n+\n+fn full<T: Into<Bytes>>(chunk: T) -> BoxBody {\n+    Full::new(chunk.into())\n+        .map_err(|never| match never {})\n+        .boxed()\n+}\n+\n+async fn handle(\n+    lock: Arc<AtomicBool>,\n+    tx_status: Sender<Status>,\n+    latest_status: Arc<RwLock<Status>>,\n+    req: hyper::Request<hyper::body::Incoming>,\n+) -> Result<hyper::Response<BoxBody>, DynError> {\n+    let response = |status, body| {\n+        Ok(hyper::Response::builder()\n+            .header(hyper::header::ACCESS_CONTROL_ALLOW_ORIGIN, \"*\")\n+            .header(hyper::header::CONTENT_TYPE, \"application/json\")\n+            .status(status)\n+            .body(body)\n+            .unwrap())\n+    };\n+    let file_response = |status, body| {\n+        Ok(hyper::Response::builder()\n+            .header(hyper::header::ACCESS_CONTROL_ALLOW_ORIGIN, \"*\")\n+            .header(hyper::header::CONTENT_TYPE, \"application/octet-stream\")\n+            .header(\n+                hyper::header::CONTENT_DISPOSITION,\n+                format!(\"attachment; filename={CONTRIB_SK_PATH}\"),\n+            )\n+            .status(status)\n+            .body(body)\n+            .unwrap())\n+    };\n+    let response_empty = |status| response(status, BoxBody::default());\n+    match (req.method(), req.uri().path()) {\n+        (&Method::POST, SK_ENDPOINT) => {\n+            let whole_body = req.collect().await?.aggregate();\n+            let email = serde_json::from_reader(whole_body.reader())?;\n+            let guard = latest_status.write().await;\n+            let result = {\n+                if let Err(_) = tokio::fs::metadata(CONTRIB_SK_PATH).await {\n+                    let secret_key = generate_pgp_key(email);\n+                    let secret_key_serialized = secret_key\n+                        .to_armored_bytes(ArmorOptions::default())\n+                        .expect(\"impossible\");\n+                    tokio::fs::write(CONTRIB_SK_PATH, &secret_key_serialized).await?;\n+                    response_empty(hyper::StatusCode::CREATED)\n+                } else {\n+                    response_empty(hyper::StatusCode::OK)\n+                }\n+            };\n+            drop(guard);\n+            result\n+        }\n+        (&Method::GET, SK_ENDPOINT) => {\n+            if let Err(_) = tokio::fs::metadata(CONTRIB_SK_PATH).await {\n+                response_empty(hyper::StatusCode::NOT_FOUND)\n+            } else {\n+                let content = tokio::fs::read(CONTRIB_SK_PATH).await?;\n+                file_response(hyper::StatusCode::OK, full(content))\n+            }\n+        }\n+        (&Method::POST, CONTRIBUTE_ENDPOINT)\n+            if lock\n+                .compare_exchange(false, true, Ordering::SeqCst, Ordering::SeqCst)\n+                .is_ok() =>",
        "comment_created_at": "2024-09-20T13:15:23+00:00",
        "comment_author": "benluelo",
        "comment_body": "writing to an atomic bool in an if guard is interesting, i would recommend pulling this out into the arm expression for clarity/ least surprise",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1719971901",
    "pr_number": 2693,
    "pr_file": "hubble/src/arb.rs",
    "created_at": "2024-08-16T15:05:12+00:00",
    "commented_code": ".rollup_finalization_config\n             .l1_next_node_num_slot_offset_bytes\n             .inner() as usize;\n-\n         let raw_slot = self\n             .l1_client\n             .get_storage_at(\n-                ethers::types::H160::from(self.rollup_finalization_config.l1_contract_address),\n-                ethers::types::H256(\n+                alloy::primitives::Address::new(\n+                    (self.rollup_finalization_config.l1_contract_address).0,\n+                ),\n+                alloy::primitives::Uint::from_be_bytes(\n                     self.rollup_finalization_config\n                         .l1_latest_confirmed_slot\n                         .to_be_bytes(),\n                 ),\n-                Some(ethers::types::BlockNumber::Number(l1_height.into()).into()),\n             )\n-            .await\n-            .unwrap();\n-\n-        debug!(raw_slot = %H256::from(raw_slot));\n+            .await?;\n \n+        let raw_slot: B256 = raw_slot.into();",
    "repo_full_name": "unionlabs/union",
    "discussion_comments": [
      {
        "comment_id": "1719971901",
        "repo_full_name": "unionlabs/union",
        "pr_number": 2693,
        "pr_file": "hubble/src/arb.rs",
        "discussion_id": "1719971901",
        "commented_code": "@@ -141,31 +153,28 @@ impl Arb {\n             .rollup_finalization_config\n             .l1_next_node_num_slot_offset_bytes\n             .inner() as usize;\n-\n         let raw_slot = self\n             .l1_client\n             .get_storage_at(\n-                ethers::types::H160::from(self.rollup_finalization_config.l1_contract_address),\n-                ethers::types::H256(\n+                alloy::primitives::Address::new(\n+                    (self.rollup_finalization_config.l1_contract_address).0,\n+                ),\n+                alloy::primitives::Uint::from_be_bytes(\n                     self.rollup_finalization_config\n                         .l1_latest_confirmed_slot\n                         .to_be_bytes(),\n                 ),\n-                Some(ethers::types::BlockNumber::Number(l1_height.into()).into()),\n             )\n-            .await\n-            .unwrap();\n-\n-        debug!(raw_slot = %H256::from(raw_slot));\n+            .await?;\n \n+        let raw_slot: B256 = raw_slot.into();",
        "comment_created_at": "2024-08-16T15:05:12+00:00",
        "comment_author": "rohankokkulabito",
        "comment_body": "<div>\n\n<strong>Bito Code Review Agent Run</strong> <a href=https://github.com/unionlabs/union/pull/2693#issuecomment-2293655644>#ef6b49</a><span> - 08/16/2024, 03:05 pm</span>\n<br>\n\n<div><small>\ud83d\udd34</small> High importance</div>\n\n<div id=\"suggestion\">\n<div id=\"issue\">\n<b>Issue:</b> The conversion of 'raw_slot' to 'B256' using '.into()' might not be explicit enough, leading to potential confusion or misuse. It is better to use 'B256::from(raw_slot)' for clarity.\n</div>\n<div id=\"fix\">\n<b>Fix:</b> Use 'B256::from(raw_slot)' instead of 'raw_slot.into()' to make the conversion explicit and clear.\n</div>\n<details>\n<summary>\n<b>Code suggestion</b>\n</summary>\n<div id=\"code\">\n\n```\n @@ -170,1 +170,1 @@\n -        let raw_slot: B256 = raw_slot.into();\n +        let raw_slot: B256 = B256::from(raw_slot);\n```\n</div>\n</details>\n</div>\n<br>\n\n\n</div>\n\nIs this a valid issue, or was it incorrectly flagged by the Agent?\n- [ ] it was incorrectly flagged",
        "pr_file_module": null
      }
    ]
  }
]