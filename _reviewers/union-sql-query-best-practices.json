[
  {
    "discussion_id": "1674136202",
    "pr_number": 2434,
    "pr_file": "drip/src/main.rs",
    "created_at": "2024-07-11T14:41:46+00:00",
    "commented_code": "}\n }\n \n+#[derive(SimpleObject)]\n+struct Request {\n+    id: i64,\n+    address: String,\n+    time: String,\n+    tx_hash: Option<String>,\n+}\n+\n struct Query;\n \n #[Object]\n impl Query {\n     async fn howdy(&self) -> &'static str {\n         \"partner\"\n     }\n+\n+    async fn unhandled_transfers<'ctx>(&self, ctx: &Context<'ctx>,limit: Option<i32>,offset: Option<i32>,) -> FieldResult<Vec<Request>> {\n+        let db = ctx.data::<Pool>().unwrap();\n+        let limit = limit.unwrap_or(10);\n+        let offset = offset.unwrap_or(0);\n+        let requests: Vec<Request> = db        \n+            .conn(move |conn| {\n+                let mut stmt = conn.prepare(\"SELECT id, address, time, tx_hash FROM requests WHERE tx_hash IS NULL LIMIT ?1 OFFSET ?2\",",
    "repo_full_name": "unionlabs/union",
    "discussion_comments": [
      {
        "comment_id": "1674136202",
        "repo_full_name": "unionlabs/union",
        "pr_number": 2434,
        "pr_file": "drip/src/main.rs",
        "discussion_id": "1674136202",
        "commented_code": "@@ -467,13 +471,105 @@ impl Mutation {\n     }\n }\n \n+#[derive(SimpleObject)]\n+struct Request {\n+    id: i64,\n+    address: String,\n+    time: String,\n+    tx_hash: Option<String>,\n+}\n+\n struct Query;\n \n #[Object]\n impl Query {\n     async fn howdy(&self) -> &'static str {\n         \"partner\"\n     }\n+\n+    async fn unhandled_transfers<'ctx>(&self, ctx: &Context<'ctx>,limit: Option<i32>,offset: Option<i32>,) -> FieldResult<Vec<Request>> {\n+        let db = ctx.data::<Pool>().unwrap();\n+        let limit = limit.unwrap_or(10);\n+        let offset = offset.unwrap_or(0);\n+        let requests: Vec<Request> = db        \n+            .conn(move |conn| {\n+                let mut stmt = conn.prepare(\"SELECT id, address, time, tx_hash FROM requests WHERE tx_hash IS NULL LIMIT ?1 OFFSET ?2\",",
        "comment_created_at": "2024-07-11T14:41:46+00:00",
        "comment_author": "KaiserKarel",
        "comment_body": "This query is not correct for consistent pagination. It needs an ORDER BY clause, otherwise we'll get different results each time. We also run into the issue that if new unhandled_transfers are added while a caller is paginating, our cursor will become invalid. It's better to let the client pass an OffsetDatetime to paginate.\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1674148387",
    "pr_number": 2434,
    "pr_file": "drip/src/main.rs",
    "created_at": "2024-07-11T14:49:31+00:00",
    "commented_code": "}\n }\n \n+#[derive(SimpleObject)]\n+struct Request {\n+    id: i64,\n+    address: String,\n+    time: String,\n+    tx_hash: Option<String>,\n+}\n+\n struct Query;\n \n #[Object]\n impl Query {\n     async fn howdy(&self) -> &'static str {\n         \"partner\"\n     }\n+\n+    async fn unhandled_transfers<'ctx>(&self, ctx: &Context<'ctx>,limit: Option<i32>,offset: Option<i32>,) -> FieldResult<Vec<Request>> {\n+        let db = ctx.data::<Pool>().unwrap();\n+        let limit = limit.unwrap_or(10);\n+        let offset = offset.unwrap_or(0);\n+        let requests: Vec<Request> = db        \n+            .conn(move |conn| {\n+                let mut stmt = conn.prepare(\"SELECT id, address, time, tx_hash FROM requests WHERE tx_hash IS NULL LIMIT ?1 OFFSET ?2\",\n+            )?;\n+                let rows = stmt.query_map(params![limit, offset], |row| {\n+                    Ok(Request {\n+                        id: row.get(0)?,\n+                        address: row.get(1)?,\n+                        time: row.get(2)?,\n+                        tx_hash: row.get(3)?,\n+                    })\n+                })?;\n+                let requests: Result<Vec<_>, _> = rows.collect();\n+                requests\n+            })\n+            .await\n+            .map_err(|e| e.to_string())?;\n+\n+        Ok(requests)\n+    }\n+\n+\n+    async fn handled_transfers<'ctx>(&self, ctx: &Context<'ctx>, limit: Option<i32>, offset: Option<i32>) -> FieldResult<Vec<Request>> {\n+        let db = ctx.data::<Pool>().unwrap();\n+        let limit = limit.unwrap_or(10);\n+        let offset = offset.unwrap_or(0);\n+        let requests: Vec<Request> = db        \n+            .conn(move |conn| {\n+                let mut stmt = conn.prepare(\n+                    \"SELECT id, address, time, tx_hash",
    "repo_full_name": "unionlabs/union",
    "discussion_comments": [
      {
        "comment_id": "1674148387",
        "repo_full_name": "unionlabs/union",
        "pr_number": 2434,
        "pr_file": "drip/src/main.rs",
        "discussion_id": "1674148387",
        "commented_code": "@@ -467,13 +471,105 @@ impl Mutation {\n     }\n }\n \n+#[derive(SimpleObject)]\n+struct Request {\n+    id: i64,\n+    address: String,\n+    time: String,\n+    tx_hash: Option<String>,\n+}\n+\n struct Query;\n \n #[Object]\n impl Query {\n     async fn howdy(&self) -> &'static str {\n         \"partner\"\n     }\n+\n+    async fn unhandled_transfers<'ctx>(&self, ctx: &Context<'ctx>,limit: Option<i32>,offset: Option<i32>,) -> FieldResult<Vec<Request>> {\n+        let db = ctx.data::<Pool>().unwrap();\n+        let limit = limit.unwrap_or(10);\n+        let offset = offset.unwrap_or(0);\n+        let requests: Vec<Request> = db        \n+            .conn(move |conn| {\n+                let mut stmt = conn.prepare(\"SELECT id, address, time, tx_hash FROM requests WHERE tx_hash IS NULL LIMIT ?1 OFFSET ?2\",\n+            )?;\n+                let rows = stmt.query_map(params![limit, offset], |row| {\n+                    Ok(Request {\n+                        id: row.get(0)?,\n+                        address: row.get(1)?,\n+                        time: row.get(2)?,\n+                        tx_hash: row.get(3)?,\n+                    })\n+                })?;\n+                let requests: Result<Vec<_>, _> = rows.collect();\n+                requests\n+            })\n+            .await\n+            .map_err(|e| e.to_string())?;\n+\n+        Ok(requests)\n+    }\n+\n+\n+    async fn handled_transfers<'ctx>(&self, ctx: &Context<'ctx>, limit: Option<i32>, offset: Option<i32>) -> FieldResult<Vec<Request>> {\n+        let db = ctx.data::<Pool>().unwrap();\n+        let limit = limit.unwrap_or(10);\n+        let offset = offset.unwrap_or(0);\n+        let requests: Vec<Request> = db        \n+            .conn(move |conn| {\n+                let mut stmt = conn.prepare(\n+                    \"SELECT id, address, time, tx_hash ",
        "comment_created_at": "2024-07-11T14:49:31+00:00",
        "comment_author": "KaiserKarel",
        "comment_body": "Needs an ORDER BY clause",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1633449821",
    "pr_number": 2084,
    "pr_file": "transfer-test-service/src/sql_helper.rs",
    "created_at": "2024-06-10T15:30:15+00:00",
    "commented_code": "+use sqlx::PgPool;\n+\n+use crate::config::PacketStatus;\n+\n+pub async fn insert_or_update_packet_status(\n+    pool: &PgPool,\n+    packet_status: PacketStatus,\n+) -> Result<(), sqlx::Error> {\n+    sqlx\n+        ::query(\n+            r#\"\n+        INSERT INTO packet_statuses (source_chain_id, target_chain_id, sequence_number, send_packet, recv_packet, write_ack, acknowledge_packet, last_update)\n+        VALUES ($1, $2, $3, $4, $5, $6, $7, $8)\n+        ON CONFLICT (source_chain_id, target_chain_id, sequence_number)\n+        DO UPDATE SET\n+            send_packet = EXCLUDED.send_packet,\n+            recv_packet = EXCLUDED.recv_packet,\n+            write_ack = EXCLUDED.write_ack,\n+            acknowledge_packet = EXCLUDED.acknowledge_packet,\n+            last_update = EXCLUDED.last_update\n+        \"#\n+        )\n+        .bind(packet_status.source_chain_id)\n+        .bind(packet_status.target_chain_id)\n+        .bind(packet_status.sequence_number as i64) // Convert u64 to i64\n+        .bind(serde_json::to_value(&packet_status.send_packet).unwrap())\n+        .bind(serde_json::to_value(&packet_status.recv_packet).unwrap())\n+        .bind(serde_json::to_value(&packet_status.write_ack).unwrap())\n+        .bind(serde_json::to_value(&packet_status.acknowledge_packet).unwrap())\n+        .bind(packet_status.last_update)\n+        .execute(pool).await?;\n+\n+    Ok(())\n+}\n+pub async fn create_table_if_not_exists(pool: &PgPool) -> Result<(), sqlx::Error> {\n+    sqlx::query(\n+        r#\"\n+        CREATE TABLE IF NOT EXISTS packet_statuses (\n+            source_chain_id INT NOT NULL,\n+            target_chain_id INT NOT NULL,\n+            sequence_number BIGINT NOT NULL,\n+            send_packet JSONB,\n+            recv_packet JSONB,\n+            write_ack JSONB,\n+            acknowledge_packet JSONB,\n+            last_update TIMESTAMPTZ,\n+            PRIMARY KEY (source_chain_id, target_chain_id, sequence_number)\n+        );\n+        \"#,\n+    )\n+    .execute(pool)\n+    .await?;\n+    Ok(())\n+}\n+\n+pub async fn get_packet_statuses(\n+    pool: &PgPool,\n+    source_chain_id: i32,\n+    target_chain_id: i32,\n+) -> Result<Vec<PacketStatus>, sqlx::Error> {\n+    let statuses = sqlx::query_as::<_, PacketStatus>(\n+        r#\"\n+        SELECT * FROM packet_statuses\n+        WHERE source_chain_id = $1 AND target_chain_id = $2\n+        \"#,\n+    )\n+    .bind(source_chain_id)\n+    .bind(target_chain_id)\n+    .fetch_all(pool)\n+    .await?;\n+\n+    Ok(statuses)\n+}\n+\n+pub async fn table_exists(pool: &PgPool, table_name: &str) -> Result<bool, sqlx::Error> {\n+    let result: (i64,) = sqlx::query_as(\n+        r#\"\n+        SELECT COUNT(*)\n+        FROM information_schema.tables\n+        WHERE table_schema = 'public' AND table_name = $1\n+        \"#,\n+    )\n+    .bind(table_name)\n+    .fetch_one(pool)\n+    .await?;\n+\n+    Ok(result.0 > 0)\n+}\n+\n+pub async fn delete_packet_status(\n+    pool: &PgPool,\n+    source_chain_id: i32,\n+    target_chain_id: i32,\n+    sequence_number: i64,\n+) -> Result<(), sqlx::Error> {\n+    sqlx::query(\n+        r#\"\n+        DELETE FROM packet_statuses\n+        WHERE source_chain_id = $1 AND target_chain_id = $2 AND sequence_number = $3\n+        \"#,\n+    )\n+    .bind(source_chain_id)\n+    .bind(target_chain_id)\n+    .bind(sequence_number)",
    "repo_full_name": "unionlabs/union",
    "discussion_comments": [
      {
        "comment_id": "1633449821",
        "repo_full_name": "unionlabs/union",
        "pr_number": 2084,
        "pr_file": "transfer-test-service/src/sql_helper.rs",
        "discussion_id": "1633449821",
        "commented_code": "@@ -0,0 +1,109 @@\n+use sqlx::PgPool;\n+\n+use crate::config::PacketStatus;\n+\n+pub async fn insert_or_update_packet_status(\n+    pool: &PgPool,\n+    packet_status: PacketStatus,\n+) -> Result<(), sqlx::Error> {\n+    sqlx\n+        ::query(\n+            r#\"\n+        INSERT INTO packet_statuses (source_chain_id, target_chain_id, sequence_number, send_packet, recv_packet, write_ack, acknowledge_packet, last_update)\n+        VALUES ($1, $2, $3, $4, $5, $6, $7, $8)\n+        ON CONFLICT (source_chain_id, target_chain_id, sequence_number)\n+        DO UPDATE SET\n+            send_packet = EXCLUDED.send_packet,\n+            recv_packet = EXCLUDED.recv_packet,\n+            write_ack = EXCLUDED.write_ack,\n+            acknowledge_packet = EXCLUDED.acknowledge_packet,\n+            last_update = EXCLUDED.last_update\n+        \"#\n+        )\n+        .bind(packet_status.source_chain_id)\n+        .bind(packet_status.target_chain_id)\n+        .bind(packet_status.sequence_number as i64) // Convert u64 to i64\n+        .bind(serde_json::to_value(&packet_status.send_packet).unwrap())\n+        .bind(serde_json::to_value(&packet_status.recv_packet).unwrap())\n+        .bind(serde_json::to_value(&packet_status.write_ack).unwrap())\n+        .bind(serde_json::to_value(&packet_status.acknowledge_packet).unwrap())\n+        .bind(packet_status.last_update)\n+        .execute(pool).await?;\n+\n+    Ok(())\n+}\n+pub async fn create_table_if_not_exists(pool: &PgPool) -> Result<(), sqlx::Error> {\n+    sqlx::query(\n+        r#\"\n+        CREATE TABLE IF NOT EXISTS packet_statuses (\n+            source_chain_id INT NOT NULL,\n+            target_chain_id INT NOT NULL,\n+            sequence_number BIGINT NOT NULL,\n+            send_packet JSONB,\n+            recv_packet JSONB,\n+            write_ack JSONB,\n+            acknowledge_packet JSONB,\n+            last_update TIMESTAMPTZ,\n+            PRIMARY KEY (source_chain_id, target_chain_id, sequence_number)\n+        );\n+        \"#,\n+    )\n+    .execute(pool)\n+    .await?;\n+    Ok(())\n+}\n+\n+pub async fn get_packet_statuses(\n+    pool: &PgPool,\n+    source_chain_id: i32,\n+    target_chain_id: i32,\n+) -> Result<Vec<PacketStatus>, sqlx::Error> {\n+    let statuses = sqlx::query_as::<_, PacketStatus>(\n+        r#\"\n+        SELECT * FROM packet_statuses\n+        WHERE source_chain_id = $1 AND target_chain_id = $2\n+        \"#,\n+    )\n+    .bind(source_chain_id)\n+    .bind(target_chain_id)\n+    .fetch_all(pool)\n+    .await?;\n+\n+    Ok(statuses)\n+}\n+\n+pub async fn table_exists(pool: &PgPool, table_name: &str) -> Result<bool, sqlx::Error> {\n+    let result: (i64,) = sqlx::query_as(\n+        r#\"\n+        SELECT COUNT(*)\n+        FROM information_schema.tables\n+        WHERE table_schema = 'public' AND table_name = $1\n+        \"#,\n+    )\n+    .bind(table_name)\n+    .fetch_one(pool)\n+    .await?;\n+\n+    Ok(result.0 > 0)\n+}\n+\n+pub async fn delete_packet_status(\n+    pool: &PgPool,\n+    source_chain_id: i32,\n+    target_chain_id: i32,\n+    sequence_number: i64,\n+) -> Result<(), sqlx::Error> {\n+    sqlx::query(\n+        r#\"\n+        DELETE FROM packet_statuses\n+        WHERE source_chain_id = $1 AND target_chain_id = $2 AND sequence_number = $3\n+        \"#,\n+    )\n+    .bind(source_chain_id)\n+    .bind(target_chain_id)\n+    .bind(sequence_number)",
        "comment_created_at": "2024-06-10T15:30:15+00:00",
        "comment_author": "benluelo",
        "comment_body": "general tip - you can use the `query!` macro to have compile time verification of your queryies: https://docs.rs/sqlx/latest/sqlx/macro.query.html",
        "pr_file_module": null
      }
    ]
  }
]