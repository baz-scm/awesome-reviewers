[
  {
    "discussion_id": "1664213789",
    "pr_number": 3344,
    "pr_file": "unstructured/ingest/v2/processes/connectors/salesforce.py",
    "created_at": "2024-07-03T13:40:13+00:00",
    "commented_code": "+\"\"\"\n+Salesforce Connector\n+Able to download Account, Case, Campaign, EmailMessage, Lead\n+Salesforce returns everything as a list of json.\n+This saves each entry as a separate file to be partitioned.\n+Using JWT authorization\n+https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_auth_key_and_cert.htm\n+https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_auth_connected_app.htm\n+\"\"\"\n+\n+import json\n+import typing as t\n+from collections import OrderedDict\n+from dataclasses import dataclass, field\n+from email.utils import formatdate\n+from pathlib import Path\n+from string import Template\n+from textwrap import dedent\n+from typing import TYPE_CHECKING, Any, Generator\n+\n+from dateutil import parser\n+\n+from unstructured.documents.elements import DataSourceMetadata\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import SourceConnectionNetworkError\n+from unstructured.ingest.v2.interfaces import (\n+    AccessConfig,\n+    ConnectionConfig,\n+    Downloader,\n+    DownloaderConfig,\n+    DownloadResponse,\n+    FileData,\n+    Indexer,\n+    IndexerConfig,\n+    SourceIdentifiers,\n+)\n+from unstructured.ingest.v2.logger import logger\n+from unstructured.ingest.v2.processes.connector_registry import (\n+    SourceRegistryEntry,\n+    add_source_entry,\n+)\n+from unstructured.utils import requires_dependencies\n+\n+\n+class MissingCategoryError(Exception):\n+    \"\"\"There are no categories with that name.\"\"\"\n+\n+\n+CONNECTOR_TYPE = \"salesforce\"\n+\n+if TYPE_CHECKING:\n+    pass\n+\n+SALESFORCE_API_VERSION = \"57.0\"\n+\n+ACCEPTED_CATEGORIES = [\"Account\", \"Case\", \"Campaign\", \"EmailMessage\", \"Lead\"]\n+\n+EMAIL_TEMPLATE = Template(\n+    \"\"\"MIME-Version: 1.0\n+Date: $date\n+Message-ID: $message_identifier\n+Subject: $subject\n+From: $from_email\n+To: $to_email\n+Content-Type: multipart/alternative; boundary=\"00000000000095c9b205eff92630\"\n+--00000000000095c9b205eff92630\n+Content-Type: text/plain; charset=\"UTF-8\"\n+$textbody\n+--00000000000095c9b205eff92630\n+Content-Type: text/html; charset=\"UTF-8\"\n+$htmlbody\n+--00000000000095c9b205eff92630--\n+\"\"\",\n+)\n+\n+\n+@dataclass\n+class SalesforceAccessConfig(AccessConfig):\n+    consumer_key: str\n+    private_key: str\n+\n+    @requires_dependencies([\"cryptography\"])\n+    def get_private_key_value_and_type(self) -> t.Tuple[str, t.Type]:\n+        from cryptography.hazmat.primitives import serialization\n+\n+        try:\n+            serialization.load_pem_private_key(data=self.private_key.encode(\"utf-8\"), password=None)\n+        except ValueError:\n+            pass\n+        else:\n+            return self.private_key, str\n+\n+        if Path(self.private_key).is_file():\n+            return self.private_key, Path\n+\n+        raise ValueError(\"private_key does not contain PEM private key or path\")\n+\n+\n+@dataclass\n+class SalesforceConnectionConfig(ConnectionConfig):\n+    username: str\n+    access_config: SalesforceAccessConfig = enhanced_field(sensitive=True)\n+\n+    @requires_dependencies([\"simple_salesforce\"], extras=\"salesforce\")\n+    def get_client(self):\n+        from simple_salesforce import Salesforce\n+\n+        pkey_value, pkey_type = self.access_config.get_private_key_value_and_type()\n+\n+        return Salesforce(\n+            username=self.username,\n+            consumer_key=self.access_config.consumer_key,\n+            privatekey_file=pkey_value if pkey_type is Path else None,\n+            privatekey=pkey_value if pkey_type is str else None,\n+            version=SALESFORCE_API_VERSION,\n+        )\n+\n+\n+@dataclass\n+class SalesforceIndexerConfig(IndexerConfig):\n+    categories: t.List[str]\n+\n+\n+@dataclass\n+class SalesforceIndexer(Indexer):\n+    connection_config: SalesforceConnectionConfig\n+    index_config: SalesforceIndexerConfig\n+\n+    @requires_dependencies([\"simple_salesforce\"], extras=\"salesforce\")\n+    def list_files(self) -> t.List[FileData]:\n+        \"\"\"Get Salesforce Ids for the records.\n+        Send them to next phase where each doc gets downloaded into the\n+        appropriate format for partitioning.\n+        \"\"\"\n+        from simple_salesforce.exceptions import SalesforceMalformedRequest\n+\n+        client = self.connection_config.get_client()\n+\n+        files_list = []\n+        for record_type in self.index_config.categories:\n+            if record_type not in ACCEPTED_CATEGORIES:\n+                raise ValueError(f\"{record_type} not currently an accepted Salesforce category\")\n+\n+            try:\n+                # Get ids from Salesforce\n+                records = client.query_all_iter(\n+                    f\"select Id, SystemModstamp, CreatedDate, LastModifiedDate from {record_type}\",\n+                )\n+                for record in records:\n+                    files_list.append(\n+                        FileData(\n+                            connector_type=CONNECTOR_TYPE,\n+                            identifier=record[\"Id\"],\n+                            source_identifiers=SourceIdentifiers(\n+                                filename=record[\"Id\"],\n+                                fullpath=f\"{record['attributes']['type']}/{record['Id']}\",\n+                            ),\n+                            metadata=DataSourceMetadata(\n+                                url=record[\"attributes\"][\"url\"],\n+                                version=record[\"SystemModstamp\"],\n+                                date_created=record[\"CreatedDate\"],\n+                                date_modified=record[\"LastModifiedDate\"],",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1664213789",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3344,
        "pr_file": "unstructured/ingest/v2/processes/connectors/salesforce.py",
        "discussion_id": "1664213789",
        "commented_code": "@@ -0,0 +1,293 @@\n+\"\"\"\n+Salesforce Connector\n+Able to download Account, Case, Campaign, EmailMessage, Lead\n+Salesforce returns everything as a list of json.\n+This saves each entry as a separate file to be partitioned.\n+Using JWT authorization\n+https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_auth_key_and_cert.htm\n+https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_auth_connected_app.htm\n+\"\"\"\n+\n+import json\n+import typing as t\n+from collections import OrderedDict\n+from dataclasses import dataclass, field\n+from email.utils import formatdate\n+from pathlib import Path\n+from string import Template\n+from textwrap import dedent\n+from typing import TYPE_CHECKING, Any, Generator\n+\n+from dateutil import parser\n+\n+from unstructured.documents.elements import DataSourceMetadata\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import SourceConnectionNetworkError\n+from unstructured.ingest.v2.interfaces import (\n+    AccessConfig,\n+    ConnectionConfig,\n+    Downloader,\n+    DownloaderConfig,\n+    DownloadResponse,\n+    FileData,\n+    Indexer,\n+    IndexerConfig,\n+    SourceIdentifiers,\n+)\n+from unstructured.ingest.v2.logger import logger\n+from unstructured.ingest.v2.processes.connector_registry import (\n+    SourceRegistryEntry,\n+    add_source_entry,\n+)\n+from unstructured.utils import requires_dependencies\n+\n+\n+class MissingCategoryError(Exception):\n+    \"\"\"There are no categories with that name.\"\"\"\n+\n+\n+CONNECTOR_TYPE = \"salesforce\"\n+\n+if TYPE_CHECKING:\n+    pass\n+\n+SALESFORCE_API_VERSION = \"57.0\"\n+\n+ACCEPTED_CATEGORIES = [\"Account\", \"Case\", \"Campaign\", \"EmailMessage\", \"Lead\"]\n+\n+EMAIL_TEMPLATE = Template(\n+    \"\"\"MIME-Version: 1.0\n+Date: $date\n+Message-ID: $message_identifier\n+Subject: $subject\n+From: $from_email\n+To: $to_email\n+Content-Type: multipart/alternative; boundary=\"00000000000095c9b205eff92630\"\n+--00000000000095c9b205eff92630\n+Content-Type: text/plain; charset=\"UTF-8\"\n+$textbody\n+--00000000000095c9b205eff92630\n+Content-Type: text/html; charset=\"UTF-8\"\n+$htmlbody\n+--00000000000095c9b205eff92630--\n+\"\"\",\n+)\n+\n+\n+@dataclass\n+class SalesforceAccessConfig(AccessConfig):\n+    consumer_key: str\n+    private_key: str\n+\n+    @requires_dependencies([\"cryptography\"])\n+    def get_private_key_value_and_type(self) -> t.Tuple[str, t.Type]:\n+        from cryptography.hazmat.primitives import serialization\n+\n+        try:\n+            serialization.load_pem_private_key(data=self.private_key.encode(\"utf-8\"), password=None)\n+        except ValueError:\n+            pass\n+        else:\n+            return self.private_key, str\n+\n+        if Path(self.private_key).is_file():\n+            return self.private_key, Path\n+\n+        raise ValueError(\"private_key does not contain PEM private key or path\")\n+\n+\n+@dataclass\n+class SalesforceConnectionConfig(ConnectionConfig):\n+    username: str\n+    access_config: SalesforceAccessConfig = enhanced_field(sensitive=True)\n+\n+    @requires_dependencies([\"simple_salesforce\"], extras=\"salesforce\")\n+    def get_client(self):\n+        from simple_salesforce import Salesforce\n+\n+        pkey_value, pkey_type = self.access_config.get_private_key_value_and_type()\n+\n+        return Salesforce(\n+            username=self.username,\n+            consumer_key=self.access_config.consumer_key,\n+            privatekey_file=pkey_value if pkey_type is Path else None,\n+            privatekey=pkey_value if pkey_type is str else None,\n+            version=SALESFORCE_API_VERSION,\n+        )\n+\n+\n+@dataclass\n+class SalesforceIndexerConfig(IndexerConfig):\n+    categories: t.List[str]\n+\n+\n+@dataclass\n+class SalesforceIndexer(Indexer):\n+    connection_config: SalesforceConnectionConfig\n+    index_config: SalesforceIndexerConfig\n+\n+    @requires_dependencies([\"simple_salesforce\"], extras=\"salesforce\")\n+    def list_files(self) -> t.List[FileData]:\n+        \"\"\"Get Salesforce Ids for the records.\n+        Send them to next phase where each doc gets downloaded into the\n+        appropriate format for partitioning.\n+        \"\"\"\n+        from simple_salesforce.exceptions import SalesforceMalformedRequest\n+\n+        client = self.connection_config.get_client()\n+\n+        files_list = []\n+        for record_type in self.index_config.categories:\n+            if record_type not in ACCEPTED_CATEGORIES:\n+                raise ValueError(f\"{record_type} not currently an accepted Salesforce category\")\n+\n+            try:\n+                # Get ids from Salesforce\n+                records = client.query_all_iter(\n+                    f\"select Id, SystemModstamp, CreatedDate, LastModifiedDate from {record_type}\",\n+                )\n+                for record in records:\n+                    files_list.append(\n+                        FileData(\n+                            connector_type=CONNECTOR_TYPE,\n+                            identifier=record[\"Id\"],\n+                            source_identifiers=SourceIdentifiers(\n+                                filename=record[\"Id\"],\n+                                fullpath=f\"{record['attributes']['type']}/{record['Id']}\",\n+                            ),\n+                            metadata=DataSourceMetadata(\n+                                url=record[\"attributes\"][\"url\"],\n+                                version=record[\"SystemModstamp\"],\n+                                date_created=record[\"CreatedDate\"],\n+                                date_modified=record[\"LastModifiedDate\"],",
        "comment_created_at": "2024-07-03T13:40:13+00:00",
        "comment_author": "rbiseck3",
        "comment_body": "Can we convert all the time based fields into a timestamp (int) so it can be used to set the local file modified time when the content is downloaded? ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1498413264",
    "pr_number": 2571,
    "pr_file": "test_unstructured_ingest/python/test-ingest-astra-output.py",
    "created_at": "2024-02-21T22:56:29+00:00",
    "commented_code": "+import random\n+\n+import click\n+from astrapy.db import AstraDB\n+\n+\n+@click.command()\n+@click.option(\"--token\", type=str)\n+@click.option(\"--api-endpoint\", type=str)\n+@click.option(\"--collection-name\", type=str, default=\"collection_test\")\n+@click.option(\"--embedding-dimension\", type=int, default=384)\n+def run_check(token, api_endpoint, collection_name, embedding_dimension):\n+    print(f\"Checking contents of Astra DB collection: {collection_name}\")\n+\n+    # Initialize our vector db\n+    astra_db = AstraDB(token=token, api_endpoint=api_endpoint)\n+    astra_db_collection = astra_db.collection(collection_name)\n+\n+    # Tally up the embeddings\n+    docs_count = astra_db_collection.count_documents()\n+    number_of_embeddings = docs_count[\"status\"][\"count\"]\n+\n+    # Print the results\n+    expected_embeddings = 3\n+    print(\n+        f\"# of embeddings in collection vs expected: {number_of_embeddings}/{expected_embeddings}\"\n+    )\n+\n+    # Check that the assertion is true\n+    assert number_of_embeddings == expected_embeddings, (\n+        f\"Number of rows in generated table ({number_of_embeddings})\"\n+        f\"doesn't match expected value: {expected_embeddings}\"\n+    )\n+\n+    # Generate a random embedding of the appropriate length\n+    random_vector = [round(random.uniform(0, 1), 1) for _ in range(embedding_dimension)]\n+\n+    find_one = astra_db_collection.find_one()\n+    random_vector = find_one[\"data\"][\"document\"][\"vector\"]\n+    random_text = find_one[\"data\"][\"document\"][\"content\"]\n+    print(random_text)\n+\n+    # Perform a similarity search\n+    find_result = astra_db_collection.vector_find(random_vector, limit=1)\n+    print(find_result)",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1498413264",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2571,
        "pr_file": "test_unstructured_ingest/python/test-ingest-astra-output.py",
        "discussion_id": "1498413264",
        "commented_code": "@@ -0,0 +1,57 @@\n+import random\n+\n+import click\n+from astrapy.db import AstraDB\n+\n+\n+@click.command()\n+@click.option(\"--token\", type=str)\n+@click.option(\"--api-endpoint\", type=str)\n+@click.option(\"--collection-name\", type=str, default=\"collection_test\")\n+@click.option(\"--embedding-dimension\", type=int, default=384)\n+def run_check(token, api_endpoint, collection_name, embedding_dimension):\n+    print(f\"Checking contents of Astra DB collection: {collection_name}\")\n+\n+    # Initialize our vector db\n+    astra_db = AstraDB(token=token, api_endpoint=api_endpoint)\n+    astra_db_collection = astra_db.collection(collection_name)\n+\n+    # Tally up the embeddings\n+    docs_count = astra_db_collection.count_documents()\n+    number_of_embeddings = docs_count[\"status\"][\"count\"]\n+\n+    # Print the results\n+    expected_embeddings = 3\n+    print(\n+        f\"# of embeddings in collection vs expected: {number_of_embeddings}/{expected_embeddings}\"\n+    )\n+\n+    # Check that the assertion is true\n+    assert number_of_embeddings == expected_embeddings, (\n+        f\"Number of rows in generated table ({number_of_embeddings})\"\n+        f\"doesn't match expected value: {expected_embeddings}\"\n+    )\n+\n+    # Generate a random embedding of the appropriate length\n+    random_vector = [round(random.uniform(0, 1), 1) for _ in range(embedding_dimension)]\n+\n+    find_one = astra_db_collection.find_one()\n+    random_vector = find_one[\"data\"][\"document\"][\"vector\"]\n+    random_text = find_one[\"data\"][\"document\"][\"content\"]\n+    print(random_text)\n+\n+    # Perform a similarity search\n+    find_result = astra_db_collection.vector_find(random_vector, limit=1)\n+    print(find_result)",
        "comment_created_at": "2024-02-21T22:56:29+00:00",
        "comment_author": "potter-potter",
        "comment_body": "@erichare Do you know why this is not finding any results? I'm essentially searching with the same vector I just pulled from the database. It should just pull up the same record. But the result is always blank.",
        "pr_file_module": null
      },
      {
        "comment_id": "1498428487",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2571,
        "pr_file": "test_unstructured_ingest/python/test-ingest-astra-output.py",
        "discussion_id": "1498413264",
        "commented_code": "@@ -0,0 +1,57 @@\n+import random\n+\n+import click\n+from astrapy.db import AstraDB\n+\n+\n+@click.command()\n+@click.option(\"--token\", type=str)\n+@click.option(\"--api-endpoint\", type=str)\n+@click.option(\"--collection-name\", type=str, default=\"collection_test\")\n+@click.option(\"--embedding-dimension\", type=int, default=384)\n+def run_check(token, api_endpoint, collection_name, embedding_dimension):\n+    print(f\"Checking contents of Astra DB collection: {collection_name}\")\n+\n+    # Initialize our vector db\n+    astra_db = AstraDB(token=token, api_endpoint=api_endpoint)\n+    astra_db_collection = astra_db.collection(collection_name)\n+\n+    # Tally up the embeddings\n+    docs_count = astra_db_collection.count_documents()\n+    number_of_embeddings = docs_count[\"status\"][\"count\"]\n+\n+    # Print the results\n+    expected_embeddings = 3\n+    print(\n+        f\"# of embeddings in collection vs expected: {number_of_embeddings}/{expected_embeddings}\"\n+    )\n+\n+    # Check that the assertion is true\n+    assert number_of_embeddings == expected_embeddings, (\n+        f\"Number of rows in generated table ({number_of_embeddings})\"\n+        f\"doesn't match expected value: {expected_embeddings}\"\n+    )\n+\n+    # Generate a random embedding of the appropriate length\n+    random_vector = [round(random.uniform(0, 1), 1) for _ in range(embedding_dimension)]\n+\n+    find_one = astra_db_collection.find_one()\n+    random_vector = find_one[\"data\"][\"document\"][\"vector\"]\n+    random_text = find_one[\"data\"][\"document\"][\"content\"]\n+    print(random_text)\n+\n+    # Perform a similarity search\n+    find_result = astra_db_collection.vector_find(random_vector, limit=1)\n+    print(find_result)",
        "comment_created_at": "2024-02-21T23:17:50+00:00",
        "comment_author": "erichare",
        "comment_body": "@potter-potter So, i'm looking at the test.... internally, the vector / embeddings column in astra is always named `$vector`, i.e., i would expect line 39 to look like.\r\n\r\n```python\r\nrandom_vector = find_one[\"data\"][\"document\"][\"$vector\"]\r\n```\r\n\r\nThe `find` and `vector_find` methods at the API level are going to look for that field. I'm wondering if we need to make a small change to the integration as a result? ",
        "pr_file_module": null
      },
      {
        "comment_id": "1498429890",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2571,
        "pr_file": "test_unstructured_ingest/python/test-ingest-astra-output.py",
        "discussion_id": "1498413264",
        "commented_code": "@@ -0,0 +1,57 @@\n+import random\n+\n+import click\n+from astrapy.db import AstraDB\n+\n+\n+@click.command()\n+@click.option(\"--token\", type=str)\n+@click.option(\"--api-endpoint\", type=str)\n+@click.option(\"--collection-name\", type=str, default=\"collection_test\")\n+@click.option(\"--embedding-dimension\", type=int, default=384)\n+def run_check(token, api_endpoint, collection_name, embedding_dimension):\n+    print(f\"Checking contents of Astra DB collection: {collection_name}\")\n+\n+    # Initialize our vector db\n+    astra_db = AstraDB(token=token, api_endpoint=api_endpoint)\n+    astra_db_collection = astra_db.collection(collection_name)\n+\n+    # Tally up the embeddings\n+    docs_count = astra_db_collection.count_documents()\n+    number_of_embeddings = docs_count[\"status\"][\"count\"]\n+\n+    # Print the results\n+    expected_embeddings = 3\n+    print(\n+        f\"# of embeddings in collection vs expected: {number_of_embeddings}/{expected_embeddings}\"\n+    )\n+\n+    # Check that the assertion is true\n+    assert number_of_embeddings == expected_embeddings, (\n+        f\"Number of rows in generated table ({number_of_embeddings})\"\n+        f\"doesn't match expected value: {expected_embeddings}\"\n+    )\n+\n+    # Generate a random embedding of the appropriate length\n+    random_vector = [round(random.uniform(0, 1), 1) for _ in range(embedding_dimension)]\n+\n+    find_one = astra_db_collection.find_one()\n+    random_vector = find_one[\"data\"][\"document\"][\"vector\"]\n+    random_text = find_one[\"data\"][\"document\"][\"content\"]\n+    print(random_text)\n+\n+    # Perform a similarity search\n+    find_result = astra_db_collection.vector_find(random_vector, limit=1)\n+    print(find_result)",
        "comment_created_at": "2024-02-21T23:20:00+00:00",
        "comment_author": "erichare",
        "comment_body": "My guess would have been the `normalize_dict()` method but you'd probably know better? At the time i thought that was normalization for purposes of unstructured, but if thats what is being sent to Astra, with `write_dict`, i think that needs to be changed... that's my guess anyway. `vector` -> `$vector`",
        "pr_file_module": null
      },
      {
        "comment_id": "1498435906",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2571,
        "pr_file": "test_unstructured_ingest/python/test-ingest-astra-output.py",
        "discussion_id": "1498413264",
        "commented_code": "@@ -0,0 +1,57 @@\n+import random\n+\n+import click\n+from astrapy.db import AstraDB\n+\n+\n+@click.command()\n+@click.option(\"--token\", type=str)\n+@click.option(\"--api-endpoint\", type=str)\n+@click.option(\"--collection-name\", type=str, default=\"collection_test\")\n+@click.option(\"--embedding-dimension\", type=int, default=384)\n+def run_check(token, api_endpoint, collection_name, embedding_dimension):\n+    print(f\"Checking contents of Astra DB collection: {collection_name}\")\n+\n+    # Initialize our vector db\n+    astra_db = AstraDB(token=token, api_endpoint=api_endpoint)\n+    astra_db_collection = astra_db.collection(collection_name)\n+\n+    # Tally up the embeddings\n+    docs_count = astra_db_collection.count_documents()\n+    number_of_embeddings = docs_count[\"status\"][\"count\"]\n+\n+    # Print the results\n+    expected_embeddings = 3\n+    print(\n+        f\"# of embeddings in collection vs expected: {number_of_embeddings}/{expected_embeddings}\"\n+    )\n+\n+    # Check that the assertion is true\n+    assert number_of_embeddings == expected_embeddings, (\n+        f\"Number of rows in generated table ({number_of_embeddings})\"\n+        f\"doesn't match expected value: {expected_embeddings}\"\n+    )\n+\n+    # Generate a random embedding of the appropriate length\n+    random_vector = [round(random.uniform(0, 1), 1) for _ in range(embedding_dimension)]\n+\n+    find_one = astra_db_collection.find_one()\n+    random_vector = find_one[\"data\"][\"document\"][\"vector\"]\n+    random_text = find_one[\"data\"][\"document\"][\"content\"]\n+    print(random_text)\n+\n+    # Perform a similarity search\n+    find_result = astra_db_collection.vector_find(random_vector, limit=1)\n+    print(find_result)",
        "comment_created_at": "2024-02-21T23:28:24+00:00",
        "comment_author": "erichare",
        "comment_body": "Left a couple comments that I *think* will fix it... this was my oversight, if i'm correct about it.... thinking that the normalization of the dictionary was for the return to Unstructured rather than for insertion into Astra.\r\n\r\nI think once these changes are made, `vector_find` should work properly.... as of now i think its failing because its not actually seeing the \"proper\" `$vector` field in the documents.",
        "pr_file_module": null
      },
      {
        "comment_id": "1498439405",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2571,
        "pr_file": "test_unstructured_ingest/python/test-ingest-astra-output.py",
        "discussion_id": "1498413264",
        "commented_code": "@@ -0,0 +1,57 @@\n+import random\n+\n+import click\n+from astrapy.db import AstraDB\n+\n+\n+@click.command()\n+@click.option(\"--token\", type=str)\n+@click.option(\"--api-endpoint\", type=str)\n+@click.option(\"--collection-name\", type=str, default=\"collection_test\")\n+@click.option(\"--embedding-dimension\", type=int, default=384)\n+def run_check(token, api_endpoint, collection_name, embedding_dimension):\n+    print(f\"Checking contents of Astra DB collection: {collection_name}\")\n+\n+    # Initialize our vector db\n+    astra_db = AstraDB(token=token, api_endpoint=api_endpoint)\n+    astra_db_collection = astra_db.collection(collection_name)\n+\n+    # Tally up the embeddings\n+    docs_count = astra_db_collection.count_documents()\n+    number_of_embeddings = docs_count[\"status\"][\"count\"]\n+\n+    # Print the results\n+    expected_embeddings = 3\n+    print(\n+        f\"# of embeddings in collection vs expected: {number_of_embeddings}/{expected_embeddings}\"\n+    )\n+\n+    # Check that the assertion is true\n+    assert number_of_embeddings == expected_embeddings, (\n+        f\"Number of rows in generated table ({number_of_embeddings})\"\n+        f\"doesn't match expected value: {expected_embeddings}\"\n+    )\n+\n+    # Generate a random embedding of the appropriate length\n+    random_vector = [round(random.uniform(0, 1), 1) for _ in range(embedding_dimension)]\n+\n+    find_one = astra_db_collection.find_one()\n+    random_vector = find_one[\"data\"][\"document\"][\"vector\"]\n+    random_text = find_one[\"data\"][\"document\"][\"content\"]\n+    print(random_text)\n+\n+    # Perform a similarity search\n+    find_result = astra_db_collection.vector_find(random_vector, limit=1)\n+    print(find_result)",
        "comment_created_at": "2024-02-21T23:34:14+00:00",
        "comment_author": "potter-potter",
        "comment_body": "Does the free account not allow vector search via api?\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1498440866",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2571,
        "pr_file": "test_unstructured_ingest/python/test-ingest-astra-output.py",
        "discussion_id": "1498413264",
        "commented_code": "@@ -0,0 +1,57 @@\n+import random\n+\n+import click\n+from astrapy.db import AstraDB\n+\n+\n+@click.command()\n+@click.option(\"--token\", type=str)\n+@click.option(\"--api-endpoint\", type=str)\n+@click.option(\"--collection-name\", type=str, default=\"collection_test\")\n+@click.option(\"--embedding-dimension\", type=int, default=384)\n+def run_check(token, api_endpoint, collection_name, embedding_dimension):\n+    print(f\"Checking contents of Astra DB collection: {collection_name}\")\n+\n+    # Initialize our vector db\n+    astra_db = AstraDB(token=token, api_endpoint=api_endpoint)\n+    astra_db_collection = astra_db.collection(collection_name)\n+\n+    # Tally up the embeddings\n+    docs_count = astra_db_collection.count_documents()\n+    number_of_embeddings = docs_count[\"status\"][\"count\"]\n+\n+    # Print the results\n+    expected_embeddings = 3\n+    print(\n+        f\"# of embeddings in collection vs expected: {number_of_embeddings}/{expected_embeddings}\"\n+    )\n+\n+    # Check that the assertion is true\n+    assert number_of_embeddings == expected_embeddings, (\n+        f\"Number of rows in generated table ({number_of_embeddings})\"\n+        f\"doesn't match expected value: {expected_embeddings}\"\n+    )\n+\n+    # Generate a random embedding of the appropriate length\n+    random_vector = [round(random.uniform(0, 1), 1) for _ in range(embedding_dimension)]\n+\n+    find_one = astra_db_collection.find_one()\n+    random_vector = find_one[\"data\"][\"document\"][\"vector\"]\n+    random_text = find_one[\"data\"][\"document\"][\"content\"]\n+    print(random_text)\n+\n+    # Perform a similarity search\n+    find_result = astra_db_collection.vector_find(random_vector, limit=1)\n+    print(find_result)",
        "comment_created_at": "2024-02-21T23:36:50+00:00",
        "comment_author": "erichare",
        "comment_body": "It should allow you to! If you login to `astra.datastax.com` and click your Database, and the Data Explorer tab shows a collection with N dimensions (plus >= 1 records) then `vector_find` should work just fine!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1498433926",
    "pr_number": 2571,
    "pr_file": "unstructured/ingest/connector/astra.py",
    "created_at": "2024-02-21T23:26:07+00:00",
    "commented_code": "+import copy\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.enhanced_dataclass.core import _asdict\n+from unstructured.ingest.error import DestinationConnectionError, SourceConnectionNetworkError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.staging.base import flatten_dict\n+from unstructured.utils import requires_dependencies\n+\n+if t.TYPE_CHECKING:\n+    from astrapy.db import AstraDB, AstraDBCollection\n+\n+NON_INDEXED_FIELDS = [\"metadata._node_content\", \"content\"]\n+\n+\n+@dataclass\n+class AstraDBAccessConfig(AccessConfig):\n+    token: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+    api_endpoint: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+\n+\n+@dataclass\n+class SimpleAstraDBConfig(BaseConnectorConfig):\n+    access_config: AstraDBAccessConfig\n+    collection_name: str\n+    embedding_dimension: int\n+\n+\n+@dataclass\n+class AstraDBWriteConfig(WriteConfig):\n+    batch_size: int = 20\n+\n+\n+@dataclass\n+class AstraDBDestinationConnector(BaseDestinationConnector):\n+    write_config: AstraDBWriteConfig\n+    connector_config: SimpleAstraDBConfig\n+    _astra_db: t.Optional[\"AstraDB\"] = field(init=False, default=None)\n+    _astra_db_collection: t.Optional[\"AstraDBCollection\"] = field(init=False, default=None)\n+\n+    def to_dict(self, **kwargs):\n+        \"\"\"\n+        The _astra_db_collection variable in this dataclass breaks deepcopy due to:\n+        TypeError: cannot pickle '_thread.lock' object\n+        When serializing, remove it, meaning client data will need to be reinitialized\n+        when deserialized\n+        \"\"\"\n+        self_cp = copy.copy(self)\n+\n+        if hasattr(self_cp, \"_astra_db_collection\"):\n+            setattr(self_cp, \"_astra_db_collection\", None)\n+\n+        return _asdict(self_cp, **kwargs)\n+\n+    @property\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def astra_db_collection(self) -> \"AstraDBCollection\":\n+        if self._astra_db_collection is None:\n+            from astrapy.db import AstraDB\n+\n+            # Build the Astra DB object\n+            self._astra_db = AstraDB(\n+                api_endpoint=self.connector_config.access_config.api_endpoint,\n+                token=self.connector_config.access_config.token,\n+            )\n+\n+            # Create and connect to the newly created collection\n+            self._astra_db_collection = self._astra_db.create_collection(\n+                collection_name=self.connector_config.collection_name,\n+                dimension=self.connector_config.embedding_dimension,\n+                options={\"indexing\": {\"deny\": NON_INDEXED_FIELDS}},\n+            )\n+        return self._astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    @DestinationConnectionError.wrap\n+    def initialize(self):\n+        _ = self.astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def check_connection(self):\n+        try:\n+            _ = self.astra_db_collection\n+        except Exception as e:\n+            logger.error(f\"Failed to validate connection {e}\", exc_info=True)\n+            raise SourceConnectionNetworkError(f\"failed to validate connection: {e}\")\n+\n+    def write_dict(self, *args, elements_dict: t.List[t.Dict[str, t.Any]], **kwargs) -> None:\n+        logger.info(f\"Inserting / updating {len(elements_dict)} documents to AstraDB.\")\n+\n+        astra_batch_size = self.write_config.batch_size\n+\n+        for chunk in chunk_generator(elements_dict, astra_batch_size):\n+            self._astra_db_collection.insert_many(chunk)\n+\n+    def normalize_dict(self, element_dict: dict) -> dict:\n+        return {\n+            \"id\": str(uuid.uuid4()),",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1498433926",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2571,
        "pr_file": "unstructured/ingest/connector/astra.py",
        "discussion_id": "1498433926",
        "commented_code": "@@ -0,0 +1,114 @@\n+import copy\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.enhanced_dataclass.core import _asdict\n+from unstructured.ingest.error import DestinationConnectionError, SourceConnectionNetworkError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.staging.base import flatten_dict\n+from unstructured.utils import requires_dependencies\n+\n+if t.TYPE_CHECKING:\n+    from astrapy.db import AstraDB, AstraDBCollection\n+\n+NON_INDEXED_FIELDS = [\"metadata._node_content\", \"content\"]\n+\n+\n+@dataclass\n+class AstraDBAccessConfig(AccessConfig):\n+    token: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+    api_endpoint: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+\n+\n+@dataclass\n+class SimpleAstraDBConfig(BaseConnectorConfig):\n+    access_config: AstraDBAccessConfig\n+    collection_name: str\n+    embedding_dimension: int\n+\n+\n+@dataclass\n+class AstraDBWriteConfig(WriteConfig):\n+    batch_size: int = 20\n+\n+\n+@dataclass\n+class AstraDBDestinationConnector(BaseDestinationConnector):\n+    write_config: AstraDBWriteConfig\n+    connector_config: SimpleAstraDBConfig\n+    _astra_db: t.Optional[\"AstraDB\"] = field(init=False, default=None)\n+    _astra_db_collection: t.Optional[\"AstraDBCollection\"] = field(init=False, default=None)\n+\n+    def to_dict(self, **kwargs):\n+        \"\"\"\n+        The _astra_db_collection variable in this dataclass breaks deepcopy due to:\n+        TypeError: cannot pickle '_thread.lock' object\n+        When serializing, remove it, meaning client data will need to be reinitialized\n+        when deserialized\n+        \"\"\"\n+        self_cp = copy.copy(self)\n+\n+        if hasattr(self_cp, \"_astra_db_collection\"):\n+            setattr(self_cp, \"_astra_db_collection\", None)\n+\n+        return _asdict(self_cp, **kwargs)\n+\n+    @property\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def astra_db_collection(self) -> \"AstraDBCollection\":\n+        if self._astra_db_collection is None:\n+            from astrapy.db import AstraDB\n+\n+            # Build the Astra DB object\n+            self._astra_db = AstraDB(\n+                api_endpoint=self.connector_config.access_config.api_endpoint,\n+                token=self.connector_config.access_config.token,\n+            )\n+\n+            # Create and connect to the newly created collection\n+            self._astra_db_collection = self._astra_db.create_collection(\n+                collection_name=self.connector_config.collection_name,\n+                dimension=self.connector_config.embedding_dimension,\n+                options={\"indexing\": {\"deny\": NON_INDEXED_FIELDS}},\n+            )\n+        return self._astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    @DestinationConnectionError.wrap\n+    def initialize(self):\n+        _ = self.astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def check_connection(self):\n+        try:\n+            _ = self.astra_db_collection\n+        except Exception as e:\n+            logger.error(f\"Failed to validate connection {e}\", exc_info=True)\n+            raise SourceConnectionNetworkError(f\"failed to validate connection: {e}\")\n+\n+    def write_dict(self, *args, elements_dict: t.List[t.Dict[str, t.Any]], **kwargs) -> None:\n+        logger.info(f\"Inserting / updating {len(elements_dict)} documents to AstraDB.\")\n+\n+        astra_batch_size = self.write_config.batch_size\n+\n+        for chunk in chunk_generator(elements_dict, astra_batch_size):\n+            self._astra_db_collection.insert_many(chunk)\n+\n+    def normalize_dict(self, element_dict: dict) -> dict:\n+        return {\n+            \"id\": str(uuid.uuid4()),",
        "comment_created_at": "2024-02-21T23:26:07+00:00",
        "comment_author": "erichare",
        "comment_body": "Assuming i'm correct that this is the literal document that gets sent to Astra DB, i think let's change `id` to `_id`",
        "pr_file_module": null
      },
      {
        "comment_id": "1498441330",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2571,
        "pr_file": "unstructured/ingest/connector/astra.py",
        "discussion_id": "1498433926",
        "commented_code": "@@ -0,0 +1,114 @@\n+import copy\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.enhanced_dataclass.core import _asdict\n+from unstructured.ingest.error import DestinationConnectionError, SourceConnectionNetworkError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.staging.base import flatten_dict\n+from unstructured.utils import requires_dependencies\n+\n+if t.TYPE_CHECKING:\n+    from astrapy.db import AstraDB, AstraDBCollection\n+\n+NON_INDEXED_FIELDS = [\"metadata._node_content\", \"content\"]\n+\n+\n+@dataclass\n+class AstraDBAccessConfig(AccessConfig):\n+    token: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+    api_endpoint: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+\n+\n+@dataclass\n+class SimpleAstraDBConfig(BaseConnectorConfig):\n+    access_config: AstraDBAccessConfig\n+    collection_name: str\n+    embedding_dimension: int\n+\n+\n+@dataclass\n+class AstraDBWriteConfig(WriteConfig):\n+    batch_size: int = 20\n+\n+\n+@dataclass\n+class AstraDBDestinationConnector(BaseDestinationConnector):\n+    write_config: AstraDBWriteConfig\n+    connector_config: SimpleAstraDBConfig\n+    _astra_db: t.Optional[\"AstraDB\"] = field(init=False, default=None)\n+    _astra_db_collection: t.Optional[\"AstraDBCollection\"] = field(init=False, default=None)\n+\n+    def to_dict(self, **kwargs):\n+        \"\"\"\n+        The _astra_db_collection variable in this dataclass breaks deepcopy due to:\n+        TypeError: cannot pickle '_thread.lock' object\n+        When serializing, remove it, meaning client data will need to be reinitialized\n+        when deserialized\n+        \"\"\"\n+        self_cp = copy.copy(self)\n+\n+        if hasattr(self_cp, \"_astra_db_collection\"):\n+            setattr(self_cp, \"_astra_db_collection\", None)\n+\n+        return _asdict(self_cp, **kwargs)\n+\n+    @property\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def astra_db_collection(self) -> \"AstraDBCollection\":\n+        if self._astra_db_collection is None:\n+            from astrapy.db import AstraDB\n+\n+            # Build the Astra DB object\n+            self._astra_db = AstraDB(\n+                api_endpoint=self.connector_config.access_config.api_endpoint,\n+                token=self.connector_config.access_config.token,\n+            )\n+\n+            # Create and connect to the newly created collection\n+            self._astra_db_collection = self._astra_db.create_collection(\n+                collection_name=self.connector_config.collection_name,\n+                dimension=self.connector_config.embedding_dimension,\n+                options={\"indexing\": {\"deny\": NON_INDEXED_FIELDS}},\n+            )\n+        return self._astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    @DestinationConnectionError.wrap\n+    def initialize(self):\n+        _ = self.astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def check_connection(self):\n+        try:\n+            _ = self.astra_db_collection\n+        except Exception as e:\n+            logger.error(f\"Failed to validate connection {e}\", exc_info=True)\n+            raise SourceConnectionNetworkError(f\"failed to validate connection: {e}\")\n+\n+    def write_dict(self, *args, elements_dict: t.List[t.Dict[str, t.Any]], **kwargs) -> None:\n+        logger.info(f\"Inserting / updating {len(elements_dict)} documents to AstraDB.\")\n+\n+        astra_batch_size = self.write_config.batch_size\n+\n+        for chunk in chunk_generator(elements_dict, astra_batch_size):\n+            self._astra_db_collection.insert_many(chunk)\n+\n+    def normalize_dict(self, element_dict: dict) -> dict:\n+        return {\n+            \"id\": str(uuid.uuid4()),",
        "comment_created_at": "2024-02-21T23:37:41+00:00",
        "comment_author": "potter-potter",
        "comment_body": "Oh. Is it not saving it in the native `id` of AstraDB? What is the actual schema of AstraDB?",
        "pr_file_module": null
      },
      {
        "comment_id": "1498443251",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2571,
        "pr_file": "unstructured/ingest/connector/astra.py",
        "discussion_id": "1498433926",
        "commented_code": "@@ -0,0 +1,114 @@\n+import copy\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.enhanced_dataclass.core import _asdict\n+from unstructured.ingest.error import DestinationConnectionError, SourceConnectionNetworkError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.staging.base import flatten_dict\n+from unstructured.utils import requires_dependencies\n+\n+if t.TYPE_CHECKING:\n+    from astrapy.db import AstraDB, AstraDBCollection\n+\n+NON_INDEXED_FIELDS = [\"metadata._node_content\", \"content\"]\n+\n+\n+@dataclass\n+class AstraDBAccessConfig(AccessConfig):\n+    token: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+    api_endpoint: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+\n+\n+@dataclass\n+class SimpleAstraDBConfig(BaseConnectorConfig):\n+    access_config: AstraDBAccessConfig\n+    collection_name: str\n+    embedding_dimension: int\n+\n+\n+@dataclass\n+class AstraDBWriteConfig(WriteConfig):\n+    batch_size: int = 20\n+\n+\n+@dataclass\n+class AstraDBDestinationConnector(BaseDestinationConnector):\n+    write_config: AstraDBWriteConfig\n+    connector_config: SimpleAstraDBConfig\n+    _astra_db: t.Optional[\"AstraDB\"] = field(init=False, default=None)\n+    _astra_db_collection: t.Optional[\"AstraDBCollection\"] = field(init=False, default=None)\n+\n+    def to_dict(self, **kwargs):\n+        \"\"\"\n+        The _astra_db_collection variable in this dataclass breaks deepcopy due to:\n+        TypeError: cannot pickle '_thread.lock' object\n+        When serializing, remove it, meaning client data will need to be reinitialized\n+        when deserialized\n+        \"\"\"\n+        self_cp = copy.copy(self)\n+\n+        if hasattr(self_cp, \"_astra_db_collection\"):\n+            setattr(self_cp, \"_astra_db_collection\", None)\n+\n+        return _asdict(self_cp, **kwargs)\n+\n+    @property\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def astra_db_collection(self) -> \"AstraDBCollection\":\n+        if self._astra_db_collection is None:\n+            from astrapy.db import AstraDB\n+\n+            # Build the Astra DB object\n+            self._astra_db = AstraDB(\n+                api_endpoint=self.connector_config.access_config.api_endpoint,\n+                token=self.connector_config.access_config.token,\n+            )\n+\n+            # Create and connect to the newly created collection\n+            self._astra_db_collection = self._astra_db.create_collection(\n+                collection_name=self.connector_config.collection_name,\n+                dimension=self.connector_config.embedding_dimension,\n+                options={\"indexing\": {\"deny\": NON_INDEXED_FIELDS}},\n+            )\n+        return self._astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    @DestinationConnectionError.wrap\n+    def initialize(self):\n+        _ = self.astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def check_connection(self):\n+        try:\n+            _ = self.astra_db_collection\n+        except Exception as e:\n+            logger.error(f\"Failed to validate connection {e}\", exc_info=True)\n+            raise SourceConnectionNetworkError(f\"failed to validate connection: {e}\")\n+\n+    def write_dict(self, *args, elements_dict: t.List[t.Dict[str, t.Any]], **kwargs) -> None:\n+        logger.info(f\"Inserting / updating {len(elements_dict)} documents to AstraDB.\")\n+\n+        astra_batch_size = self.write_config.batch_size\n+\n+        for chunk in chunk_generator(elements_dict, astra_batch_size):\n+            self._astra_db_collection.insert_many(chunk)\n+\n+    def normalize_dict(self, element_dict: dict) -> dict:\n+        return {\n+            \"id\": str(uuid.uuid4()),",
        "comment_created_at": "2024-02-21T23:40:26+00:00",
        "comment_author": "erichare",
        "comment_body": "It's effectively schemaless, but there are a few internal fields... `_id` for a unique identifier of the record, `$vector` for the embedding, and then a few internal fields that can be returned such as `$similarity` for a similarity score when performing a vector search.\r\n\r\nMy apologies that this normalization wasn't done right (assuming that i'm not off track about that). Hopefully that works, and happy to do some live testing with you (or if you want to let me know what steps you're using to run the test on this PR, i'd be happy to play with it too)",
        "pr_file_module": null
      },
      {
        "comment_id": "1498452287",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2571,
        "pr_file": "unstructured/ingest/connector/astra.py",
        "discussion_id": "1498433926",
        "commented_code": "@@ -0,0 +1,114 @@\n+import copy\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.enhanced_dataclass.core import _asdict\n+from unstructured.ingest.error import DestinationConnectionError, SourceConnectionNetworkError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.staging.base import flatten_dict\n+from unstructured.utils import requires_dependencies\n+\n+if t.TYPE_CHECKING:\n+    from astrapy.db import AstraDB, AstraDBCollection\n+\n+NON_INDEXED_FIELDS = [\"metadata._node_content\", \"content\"]\n+\n+\n+@dataclass\n+class AstraDBAccessConfig(AccessConfig):\n+    token: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+    api_endpoint: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+\n+\n+@dataclass\n+class SimpleAstraDBConfig(BaseConnectorConfig):\n+    access_config: AstraDBAccessConfig\n+    collection_name: str\n+    embedding_dimension: int\n+\n+\n+@dataclass\n+class AstraDBWriteConfig(WriteConfig):\n+    batch_size: int = 20\n+\n+\n+@dataclass\n+class AstraDBDestinationConnector(BaseDestinationConnector):\n+    write_config: AstraDBWriteConfig\n+    connector_config: SimpleAstraDBConfig\n+    _astra_db: t.Optional[\"AstraDB\"] = field(init=False, default=None)\n+    _astra_db_collection: t.Optional[\"AstraDBCollection\"] = field(init=False, default=None)\n+\n+    def to_dict(self, **kwargs):\n+        \"\"\"\n+        The _astra_db_collection variable in this dataclass breaks deepcopy due to:\n+        TypeError: cannot pickle '_thread.lock' object\n+        When serializing, remove it, meaning client data will need to be reinitialized\n+        when deserialized\n+        \"\"\"\n+        self_cp = copy.copy(self)\n+\n+        if hasattr(self_cp, \"_astra_db_collection\"):\n+            setattr(self_cp, \"_astra_db_collection\", None)\n+\n+        return _asdict(self_cp, **kwargs)\n+\n+    @property\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def astra_db_collection(self) -> \"AstraDBCollection\":\n+        if self._astra_db_collection is None:\n+            from astrapy.db import AstraDB\n+\n+            # Build the Astra DB object\n+            self._astra_db = AstraDB(\n+                api_endpoint=self.connector_config.access_config.api_endpoint,\n+                token=self.connector_config.access_config.token,\n+            )\n+\n+            # Create and connect to the newly created collection\n+            self._astra_db_collection = self._astra_db.create_collection(\n+                collection_name=self.connector_config.collection_name,\n+                dimension=self.connector_config.embedding_dimension,\n+                options={\"indexing\": {\"deny\": NON_INDEXED_FIELDS}},\n+            )\n+        return self._astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    @DestinationConnectionError.wrap\n+    def initialize(self):\n+        _ = self.astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def check_connection(self):\n+        try:\n+            _ = self.astra_db_collection\n+        except Exception as e:\n+            logger.error(f\"Failed to validate connection {e}\", exc_info=True)\n+            raise SourceConnectionNetworkError(f\"failed to validate connection: {e}\")\n+\n+    def write_dict(self, *args, elements_dict: t.List[t.Dict[str, t.Any]], **kwargs) -> None:\n+        logger.info(f\"Inserting / updating {len(elements_dict)} documents to AstraDB.\")\n+\n+        astra_batch_size = self.write_config.batch_size\n+\n+        for chunk in chunk_generator(elements_dict, astra_batch_size):\n+            self._astra_db_collection.insert_many(chunk)\n+\n+    def normalize_dict(self, element_dict: dict) -> dict:\n+        return {\n+            \"id\": str(uuid.uuid4()),",
        "comment_created_at": "2024-02-21T23:55:38+00:00",
        "comment_author": "potter-potter",
        "comment_body": "Got ya. Ideally we are putting the vector in the right place. We could completely remove `_id` if Astra DB makes it's own id.\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1498453479",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2571,
        "pr_file": "unstructured/ingest/connector/astra.py",
        "discussion_id": "1498433926",
        "commented_code": "@@ -0,0 +1,114 @@\n+import copy\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.enhanced_dataclass.core import _asdict\n+from unstructured.ingest.error import DestinationConnectionError, SourceConnectionNetworkError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.staging.base import flatten_dict\n+from unstructured.utils import requires_dependencies\n+\n+if t.TYPE_CHECKING:\n+    from astrapy.db import AstraDB, AstraDBCollection\n+\n+NON_INDEXED_FIELDS = [\"metadata._node_content\", \"content\"]\n+\n+\n+@dataclass\n+class AstraDBAccessConfig(AccessConfig):\n+    token: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+    api_endpoint: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+\n+\n+@dataclass\n+class SimpleAstraDBConfig(BaseConnectorConfig):\n+    access_config: AstraDBAccessConfig\n+    collection_name: str\n+    embedding_dimension: int\n+\n+\n+@dataclass\n+class AstraDBWriteConfig(WriteConfig):\n+    batch_size: int = 20\n+\n+\n+@dataclass\n+class AstraDBDestinationConnector(BaseDestinationConnector):\n+    write_config: AstraDBWriteConfig\n+    connector_config: SimpleAstraDBConfig\n+    _astra_db: t.Optional[\"AstraDB\"] = field(init=False, default=None)\n+    _astra_db_collection: t.Optional[\"AstraDBCollection\"] = field(init=False, default=None)\n+\n+    def to_dict(self, **kwargs):\n+        \"\"\"\n+        The _astra_db_collection variable in this dataclass breaks deepcopy due to:\n+        TypeError: cannot pickle '_thread.lock' object\n+        When serializing, remove it, meaning client data will need to be reinitialized\n+        when deserialized\n+        \"\"\"\n+        self_cp = copy.copy(self)\n+\n+        if hasattr(self_cp, \"_astra_db_collection\"):\n+            setattr(self_cp, \"_astra_db_collection\", None)\n+\n+        return _asdict(self_cp, **kwargs)\n+\n+    @property\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def astra_db_collection(self) -> \"AstraDBCollection\":\n+        if self._astra_db_collection is None:\n+            from astrapy.db import AstraDB\n+\n+            # Build the Astra DB object\n+            self._astra_db = AstraDB(\n+                api_endpoint=self.connector_config.access_config.api_endpoint,\n+                token=self.connector_config.access_config.token,\n+            )\n+\n+            # Create and connect to the newly created collection\n+            self._astra_db_collection = self._astra_db.create_collection(\n+                collection_name=self.connector_config.collection_name,\n+                dimension=self.connector_config.embedding_dimension,\n+                options={\"indexing\": {\"deny\": NON_INDEXED_FIELDS}},\n+            )\n+        return self._astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    @DestinationConnectionError.wrap\n+    def initialize(self):\n+        _ = self.astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def check_connection(self):\n+        try:\n+            _ = self.astra_db_collection\n+        except Exception as e:\n+            logger.error(f\"Failed to validate connection {e}\", exc_info=True)\n+            raise SourceConnectionNetworkError(f\"failed to validate connection: {e}\")\n+\n+    def write_dict(self, *args, elements_dict: t.List[t.Dict[str, t.Any]], **kwargs) -> None:\n+        logger.info(f\"Inserting / updating {len(elements_dict)} documents to AstraDB.\")\n+\n+        astra_batch_size = self.write_config.batch_size\n+\n+        for chunk in chunk_generator(elements_dict, astra_batch_size):\n+            self._astra_db_collection.insert_many(chunk)\n+\n+    def normalize_dict(self, element_dict: dict) -> dict:\n+        return {\n+            \"id\": str(uuid.uuid4()),",
        "comment_created_at": "2024-02-21T23:57:26+00:00",
        "comment_author": "potter-potter",
        "comment_body": "so where is it storing the text and metadata? We could name those whatever we want? And then the user is responsible for getting the content back? How would it know what to summarize if we just stored it as `blabla`",
        "pr_file_module": null
      },
      {
        "comment_id": "1498455646",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2571,
        "pr_file": "unstructured/ingest/connector/astra.py",
        "discussion_id": "1498433926",
        "commented_code": "@@ -0,0 +1,114 @@\n+import copy\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.enhanced_dataclass.core import _asdict\n+from unstructured.ingest.error import DestinationConnectionError, SourceConnectionNetworkError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.staging.base import flatten_dict\n+from unstructured.utils import requires_dependencies\n+\n+if t.TYPE_CHECKING:\n+    from astrapy.db import AstraDB, AstraDBCollection\n+\n+NON_INDEXED_FIELDS = [\"metadata._node_content\", \"content\"]\n+\n+\n+@dataclass\n+class AstraDBAccessConfig(AccessConfig):\n+    token: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+    api_endpoint: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+\n+\n+@dataclass\n+class SimpleAstraDBConfig(BaseConnectorConfig):\n+    access_config: AstraDBAccessConfig\n+    collection_name: str\n+    embedding_dimension: int\n+\n+\n+@dataclass\n+class AstraDBWriteConfig(WriteConfig):\n+    batch_size: int = 20\n+\n+\n+@dataclass\n+class AstraDBDestinationConnector(BaseDestinationConnector):\n+    write_config: AstraDBWriteConfig\n+    connector_config: SimpleAstraDBConfig\n+    _astra_db: t.Optional[\"AstraDB\"] = field(init=False, default=None)\n+    _astra_db_collection: t.Optional[\"AstraDBCollection\"] = field(init=False, default=None)\n+\n+    def to_dict(self, **kwargs):\n+        \"\"\"\n+        The _astra_db_collection variable in this dataclass breaks deepcopy due to:\n+        TypeError: cannot pickle '_thread.lock' object\n+        When serializing, remove it, meaning client data will need to be reinitialized\n+        when deserialized\n+        \"\"\"\n+        self_cp = copy.copy(self)\n+\n+        if hasattr(self_cp, \"_astra_db_collection\"):\n+            setattr(self_cp, \"_astra_db_collection\", None)\n+\n+        return _asdict(self_cp, **kwargs)\n+\n+    @property\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def astra_db_collection(self) -> \"AstraDBCollection\":\n+        if self._astra_db_collection is None:\n+            from astrapy.db import AstraDB\n+\n+            # Build the Astra DB object\n+            self._astra_db = AstraDB(\n+                api_endpoint=self.connector_config.access_config.api_endpoint,\n+                token=self.connector_config.access_config.token,\n+            )\n+\n+            # Create and connect to the newly created collection\n+            self._astra_db_collection = self._astra_db.create_collection(\n+                collection_name=self.connector_config.collection_name,\n+                dimension=self.connector_config.embedding_dimension,\n+                options={\"indexing\": {\"deny\": NON_INDEXED_FIELDS}},\n+            )\n+        return self._astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    @DestinationConnectionError.wrap\n+    def initialize(self):\n+        _ = self.astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def check_connection(self):\n+        try:\n+            _ = self.astra_db_collection\n+        except Exception as e:\n+            logger.error(f\"Failed to validate connection {e}\", exc_info=True)\n+            raise SourceConnectionNetworkError(f\"failed to validate connection: {e}\")\n+\n+    def write_dict(self, *args, elements_dict: t.List[t.Dict[str, t.Any]], **kwargs) -> None:\n+        logger.info(f\"Inserting / updating {len(elements_dict)} documents to AstraDB.\")\n+\n+        astra_batch_size = self.write_config.batch_size\n+\n+        for chunk in chunk_generator(elements_dict, astra_batch_size):\n+            self._astra_db_collection.insert_many(chunk)\n+\n+    def normalize_dict(self, element_dict: dict) -> dict:\n+        return {\n+            \"id\": str(uuid.uuid4()),",
        "comment_created_at": "2024-02-22T00:01:10+00:00",
        "comment_author": "erichare",
        "comment_body": "Correct yep! The only one with the special name is `$vector` if the embeddings have been pre-computed (we're exploring computing them at insert time as an option, but right now that's not in place and we tend to rely on integrations like this to handle that.)\r\n\r\nI'm going to be honest i'm a bit less sure about `_id` - but just testing it, it does seem like Astra generates that field automatically with a random uuid if not provided. so i think its safe to remove it if desired.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1498450191",
    "pr_number": 2571,
    "pr_file": "unstructured/ingest/connector/astra.py",
    "created_at": "2024-02-21T23:52:15+00:00",
    "commented_code": "+import copy\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.enhanced_dataclass.core import _asdict\n+from unstructured.ingest.error import DestinationConnectionError, SourceConnectionNetworkError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.staging.base import flatten_dict\n+from unstructured.utils import requires_dependencies\n+\n+if t.TYPE_CHECKING:\n+    from astrapy.db import AstraDB, AstraDBCollection\n+\n+NON_INDEXED_FIELDS = [\"metadata._node_content\", \"content\"]\n+\n+\n+@dataclass\n+class AstraDBAccessConfig(AccessConfig):\n+    token: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+    api_endpoint: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+\n+\n+@dataclass\n+class SimpleAstraDBConfig(BaseConnectorConfig):\n+    access_config: AstraDBAccessConfig\n+    collection_name: str\n+    embedding_dimension: int\n+\n+\n+@dataclass\n+class AstraDBWriteConfig(WriteConfig):\n+    batch_size: int = 20\n+\n+\n+@dataclass\n+class AstraDBDestinationConnector(BaseDestinationConnector):\n+    write_config: AstraDBWriteConfig\n+    connector_config: SimpleAstraDBConfig\n+    _astra_db: t.Optional[\"AstraDB\"] = field(init=False, default=None)\n+    _astra_db_collection: t.Optional[\"AstraDBCollection\"] = field(init=False, default=None)\n+\n+    def to_dict(self, **kwargs):\n+        \"\"\"\n+        The _astra_db_collection variable in this dataclass breaks deepcopy due to:\n+        TypeError: cannot pickle '_thread.lock' object\n+        When serializing, remove it, meaning client data will need to be reinitialized\n+        when deserialized\n+        \"\"\"\n+        self_cp = copy.copy(self)\n+\n+        if hasattr(self_cp, \"_astra_db_collection\"):\n+            setattr(self_cp, \"_astra_db_collection\", None)\n+\n+        return _asdict(self_cp, **kwargs)\n+\n+    @property\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def astra_db_collection(self) -> \"AstraDBCollection\":\n+        if self._astra_db_collection is None:\n+            from astrapy.db import AstraDB\n+\n+            # Build the Astra DB object\n+            self._astra_db = AstraDB(\n+                api_endpoint=self.connector_config.access_config.api_endpoint,\n+                token=self.connector_config.access_config.token,\n+            )\n+\n+            # Create and connect to the newly created collection\n+            self._astra_db_collection = self._astra_db.create_collection(\n+                collection_name=self.connector_config.collection_name,\n+                dimension=self.connector_config.embedding_dimension,\n+                options={\"indexing\": {\"deny\": NON_INDEXED_FIELDS}},\n+            )\n+        return self._astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    @DestinationConnectionError.wrap\n+    def initialize(self):\n+        _ = self.astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def check_connection(self):\n+        try:\n+            _ = self.astra_db_collection\n+        except Exception as e:\n+            logger.error(f\"Failed to validate connection {e}\", exc_info=True)\n+            raise SourceConnectionNetworkError(f\"failed to validate connection: {e}\")\n+\n+    def write_dict(self, *args, elements_dict: t.List[t.Dict[str, t.Any]], **kwargs) -> None:\n+        logger.info(f\"Inserting / updating {len(elements_dict)} documents to AstraDB.\")\n+\n+        astra_batch_size = self.write_config.batch_size\n+\n+        for chunk in chunk_generator(elements_dict, astra_batch_size):\n+            self._astra_db_collection.insert_many(chunk)\n+\n+    def normalize_dict(self, element_dict: dict) -> dict:\n+        return {\n+            \"_id\": str(uuid.uuid4()),\n+            \"$vector\": element_dict.pop(\"embeddings\", None),\n+            \"content\": element_dict.pop(\"text\", None),",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1498450191",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2571,
        "pr_file": "unstructured/ingest/connector/astra.py",
        "discussion_id": "1498450191",
        "commented_code": "@@ -0,0 +1,114 @@\n+import copy\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.enhanced_dataclass.core import _asdict\n+from unstructured.ingest.error import DestinationConnectionError, SourceConnectionNetworkError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.staging.base import flatten_dict\n+from unstructured.utils import requires_dependencies\n+\n+if t.TYPE_CHECKING:\n+    from astrapy.db import AstraDB, AstraDBCollection\n+\n+NON_INDEXED_FIELDS = [\"metadata._node_content\", \"content\"]\n+\n+\n+@dataclass\n+class AstraDBAccessConfig(AccessConfig):\n+    token: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+    api_endpoint: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+\n+\n+@dataclass\n+class SimpleAstraDBConfig(BaseConnectorConfig):\n+    access_config: AstraDBAccessConfig\n+    collection_name: str\n+    embedding_dimension: int\n+\n+\n+@dataclass\n+class AstraDBWriteConfig(WriteConfig):\n+    batch_size: int = 20\n+\n+\n+@dataclass\n+class AstraDBDestinationConnector(BaseDestinationConnector):\n+    write_config: AstraDBWriteConfig\n+    connector_config: SimpleAstraDBConfig\n+    _astra_db: t.Optional[\"AstraDB\"] = field(init=False, default=None)\n+    _astra_db_collection: t.Optional[\"AstraDBCollection\"] = field(init=False, default=None)\n+\n+    def to_dict(self, **kwargs):\n+        \"\"\"\n+        The _astra_db_collection variable in this dataclass breaks deepcopy due to:\n+        TypeError: cannot pickle '_thread.lock' object\n+        When serializing, remove it, meaning client data will need to be reinitialized\n+        when deserialized\n+        \"\"\"\n+        self_cp = copy.copy(self)\n+\n+        if hasattr(self_cp, \"_astra_db_collection\"):\n+            setattr(self_cp, \"_astra_db_collection\", None)\n+\n+        return _asdict(self_cp, **kwargs)\n+\n+    @property\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def astra_db_collection(self) -> \"AstraDBCollection\":\n+        if self._astra_db_collection is None:\n+            from astrapy.db import AstraDB\n+\n+            # Build the Astra DB object\n+            self._astra_db = AstraDB(\n+                api_endpoint=self.connector_config.access_config.api_endpoint,\n+                token=self.connector_config.access_config.token,\n+            )\n+\n+            # Create and connect to the newly created collection\n+            self._astra_db_collection = self._astra_db.create_collection(\n+                collection_name=self.connector_config.collection_name,\n+                dimension=self.connector_config.embedding_dimension,\n+                options={\"indexing\": {\"deny\": NON_INDEXED_FIELDS}},\n+            )\n+        return self._astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    @DestinationConnectionError.wrap\n+    def initialize(self):\n+        _ = self.astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def check_connection(self):\n+        try:\n+            _ = self.astra_db_collection\n+        except Exception as e:\n+            logger.error(f\"Failed to validate connection {e}\", exc_info=True)\n+            raise SourceConnectionNetworkError(f\"failed to validate connection: {e}\")\n+\n+    def write_dict(self, *args, elements_dict: t.List[t.Dict[str, t.Any]], **kwargs) -> None:\n+        logger.info(f\"Inserting / updating {len(elements_dict)} documents to AstraDB.\")\n+\n+        astra_batch_size = self.write_config.batch_size\n+\n+        for chunk in chunk_generator(elements_dict, astra_batch_size):\n+            self._astra_db_collection.insert_many(chunk)\n+\n+    def normalize_dict(self, element_dict: dict) -> dict:\n+        return {\n+            \"_id\": str(uuid.uuid4()),\n+            \"$vector\": element_dict.pop(\"embeddings\", None),\n+            \"content\": element_dict.pop(\"text\", None),",
        "comment_created_at": "2024-02-21T23:52:15+00:00",
        "comment_author": "potter-potter",
        "comment_body": "@erichare can you confirm that `content` and `metadata` are the correct column names to be stored in the database. (like id and vector had to be adjusted)",
        "pr_file_module": null
      },
      {
        "comment_id": "1498453726",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2571,
        "pr_file": "unstructured/ingest/connector/astra.py",
        "discussion_id": "1498450191",
        "commented_code": "@@ -0,0 +1,114 @@\n+import copy\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.enhanced_dataclass.core import _asdict\n+from unstructured.ingest.error import DestinationConnectionError, SourceConnectionNetworkError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.staging.base import flatten_dict\n+from unstructured.utils import requires_dependencies\n+\n+if t.TYPE_CHECKING:\n+    from astrapy.db import AstraDB, AstraDBCollection\n+\n+NON_INDEXED_FIELDS = [\"metadata._node_content\", \"content\"]\n+\n+\n+@dataclass\n+class AstraDBAccessConfig(AccessConfig):\n+    token: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+    api_endpoint: t.Optional[str] = enhanced_field(default=None, sensitive=True)\n+\n+\n+@dataclass\n+class SimpleAstraDBConfig(BaseConnectorConfig):\n+    access_config: AstraDBAccessConfig\n+    collection_name: str\n+    embedding_dimension: int\n+\n+\n+@dataclass\n+class AstraDBWriteConfig(WriteConfig):\n+    batch_size: int = 20\n+\n+\n+@dataclass\n+class AstraDBDestinationConnector(BaseDestinationConnector):\n+    write_config: AstraDBWriteConfig\n+    connector_config: SimpleAstraDBConfig\n+    _astra_db: t.Optional[\"AstraDB\"] = field(init=False, default=None)\n+    _astra_db_collection: t.Optional[\"AstraDBCollection\"] = field(init=False, default=None)\n+\n+    def to_dict(self, **kwargs):\n+        \"\"\"\n+        The _astra_db_collection variable in this dataclass breaks deepcopy due to:\n+        TypeError: cannot pickle '_thread.lock' object\n+        When serializing, remove it, meaning client data will need to be reinitialized\n+        when deserialized\n+        \"\"\"\n+        self_cp = copy.copy(self)\n+\n+        if hasattr(self_cp, \"_astra_db_collection\"):\n+            setattr(self_cp, \"_astra_db_collection\", None)\n+\n+        return _asdict(self_cp, **kwargs)\n+\n+    @property\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def astra_db_collection(self) -> \"AstraDBCollection\":\n+        if self._astra_db_collection is None:\n+            from astrapy.db import AstraDB\n+\n+            # Build the Astra DB object\n+            self._astra_db = AstraDB(\n+                api_endpoint=self.connector_config.access_config.api_endpoint,\n+                token=self.connector_config.access_config.token,\n+            )\n+\n+            # Create and connect to the newly created collection\n+            self._astra_db_collection = self._astra_db.create_collection(\n+                collection_name=self.connector_config.collection_name,\n+                dimension=self.connector_config.embedding_dimension,\n+                options={\"indexing\": {\"deny\": NON_INDEXED_FIELDS}},\n+            )\n+        return self._astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    @DestinationConnectionError.wrap\n+    def initialize(self):\n+        _ = self.astra_db_collection\n+\n+    @requires_dependencies([\"astrapy\"], extras=\"astra\")\n+    def check_connection(self):\n+        try:\n+            _ = self.astra_db_collection\n+        except Exception as e:\n+            logger.error(f\"Failed to validate connection {e}\", exc_info=True)\n+            raise SourceConnectionNetworkError(f\"failed to validate connection: {e}\")\n+\n+    def write_dict(self, *args, elements_dict: t.List[t.Dict[str, t.Any]], **kwargs) -> None:\n+        logger.info(f\"Inserting / updating {len(elements_dict)} documents to AstraDB.\")\n+\n+        astra_batch_size = self.write_config.batch_size\n+\n+        for chunk in chunk_generator(elements_dict, astra_batch_size):\n+            self._astra_db_collection.insert_many(chunk)\n+\n+    def normalize_dict(self, element_dict: dict) -> dict:\n+        return {\n+            \"_id\": str(uuid.uuid4()),\n+            \"$vector\": element_dict.pop(\"embeddings\", None),\n+            \"content\": element_dict.pop(\"text\", None),",
        "comment_created_at": "2024-02-21T23:57:50+00:00",
        "comment_author": "erichare",
        "comment_body": "Yep I can confirm. these are totally fine. And actually if you have a preference for a different name, that's fine too, these fields have no special meaning in Astra DB... just that in some other integrations they are what is used",
        "pr_file_module": null
      }
    ]
  }
]