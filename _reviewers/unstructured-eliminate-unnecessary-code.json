[
  {
    "discussion_id": "2132445318",
    "pr_number": 4012,
    "pr_file": "unstructured/documents/mappings.py",
    "created_at": "2025-06-06T15:51:17+00:00",
    "commented_code": "for tag in element_type().allowed_tags\n }\n CSS_CLASS_TO_ELEMENT_TYPE_MAP: Dict[str, Type[ontology.OntologyElement]] = {\n-    element_type().css_class_name: element_type\n-    for element_type in ALL_ONTOLOGY_ELEMENT_TYPES\n-    for tag in element_type().allowed_tags",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "2132445318",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 4012,
        "pr_file": "unstructured/documents/mappings.py",
        "discussion_id": "2132445318",
        "commented_code": "@@ -134,9 +136,7 @@ def get_ontology_to_unstructured_type_mapping() -> dict[str, Element]:\n     for tag in element_type().allowed_tags\n }\n CSS_CLASS_TO_ELEMENT_TYPE_MAP: Dict[str, Type[ontology.OntologyElement]] = {\n-    element_type().css_class_name: element_type\n-    for element_type in ALL_ONTOLOGY_ELEMENT_TYPES\n-    for tag in element_type().allowed_tags",
        "comment_created_at": "2025-06-06T15:51:17+00:00",
        "comment_author": "badGarnet",
        "comment_body": "how was this line of code working before?",
        "pr_file_module": null
      },
      {
        "comment_id": "2135543654",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 4012,
        "pr_file": "unstructured/documents/mappings.py",
        "discussion_id": "2132445318",
        "commented_code": "@@ -134,9 +136,7 @@ def get_ontology_to_unstructured_type_mapping() -> dict[str, Element]:\n     for tag in element_type().allowed_tags\n }\n CSS_CLASS_TO_ELEMENT_TYPE_MAP: Dict[str, Type[ontology.OntologyElement]] = {\n-    element_type().css_class_name: element_type\n-    for element_type in ALL_ONTOLOGY_ELEMENT_TYPES\n-    for tag in element_type().allowed_tags",
        "comment_created_at": "2025-06-09T11:20:54+00:00",
        "comment_author": "plutasnyy",
        "comment_body": "This was copy-paste form the mapping above (which is more specific), here we just did more loops, but the 'tag' is not used anywhere. After the change, the mapping is the same, just unnecessary loops are removed:\r\n```python\r\nCSS_CLASS_TO_ELEMENT_TYPE_MAP_BEFORE: Dict[str, Type[ontology.OntologyElement]] = {\r\n    element_type().css_class_name: element_type\r\n    for element_type in ALL_ONTOLOGY_ELEMENT_TYPES\r\n    for tag in element_type().allowed_tags\r\n}\r\n\r\nCSS_CLASS_TO_ELEMENT_TYPE_MAP_AFTER: Dict[str, Type[ontology.OntologyElement]] = {\r\n    element_type().css_class_name: element_type for element_type in ALL_ONTOLOGY_ELEMENT_TYPES\r\n}\r\nprint(CSS_CLASS_TO_ELEMENT_TYPE_MAP_BEFORE == CSS_CLASS_TO_ELEMENT_TYPE_MAP_AFTER)\r\n# Output: True\r\n```\r\n\r\nThis is because some CSS classes can have multiple HTML tags assigned eg.\r\n```python\r\nclass Heading(OntologyElement):\r\n    description: str = Field(\"Section headings (levels 1-6)\", frozen=True)\r\n    elementType: ElementTypeEnum = Field(ElementTypeEnum.text, frozen=True)\r\n    allowed_tags: List[str] = Field([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"], frozen=True)\r\n```\r\nIn this case, we were creating a 6 times entry: `\"Heading\": OntologyElement.Heading`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1927559741",
    "pr_number": 3886,
    "pr_file": "unstructured/file_utils/ndjson.py",
    "created_at": "2025-01-23T19:16:32+00:00",
    "commented_code": "+import json\n+\n+\n+def dumps(obj: list[dict], **kwargs) -> str:\n+    lines = []\n+    for each in obj:\n+        lines.append(json.dumps(each, **kwargs))\n+    return \"\n\".join(lines)",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1927559741",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3886,
        "pr_file": "unstructured/file_utils/ndjson.py",
        "discussion_id": "1927559741",
        "commented_code": "@@ -0,0 +1,24 @@\n+import json\n+\n+\n+def dumps(obj: list[dict], **kwargs) -> str:\n+    lines = []\n+    for each in obj:\n+        lines.append(json.dumps(each, **kwargs))\n+    return \"\\n\".join(lines)",
        "comment_created_at": "2025-01-23T19:16:32+00:00",
        "comment_author": "scanny",
        "comment_body": "A matter of style, but this can be:\r\n```python\r\nreturn \"\\n\".join(json.dumps(each, **kwargs) for each in obj)\r\n```\r\n\r\nWhich is both more concise and more efficient.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1553711494",
    "pr_number": 2861,
    "pr_file": "setup.py",
    "created_at": "2024-04-05T13:59:23+00:00",
    "commented_code": "file_list = [file_list]\n     requirements: List[str] = []\n     for file in file_list:\n+        path = Path(file)\n+        file_dir = path.parent.resolve()\n         with open(file, encoding=\"utf-8\") as f:\n-            requirements.extend(f.readlines())\n-    requirements = [\n-        req for req in requirements if not req.startswith(\"#\") and not req.startswith(\"-\")\n-    ]\n+            raw = f.read().splitlines()\n+            requirements.extend([r for r in raw if not r.startswith(\"#\") and not r.startswith(\"-\")])\n+            recursive_reqs = [r for r in raw if r.startswith(\"-r\")]\n+            if recursive_reqs:\n+                filenames = []\n+                for recursive_req in recursive_reqs:\n+                    file_spec = recursive_req.split()[-1]\n+                    file_path = Path(file_dir) / file_spec\n+                    filenames.append(str(file_path.resolve()))\n+                requirements.extend(load_requirements(file_list=filenames))",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1553711494",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2861,
        "pr_file": "setup.py",
        "discussion_id": "1553711494",
        "commented_code": "@@ -32,11 +33,19 @@ def load_requirements(file_list: Optional[Union[str, List[str]]] = None) -> List\n         file_list = [file_list]\n     requirements: List[str] = []\n     for file in file_list:\n+        path = Path(file)\n+        file_dir = path.parent.resolve()\n         with open(file, encoding=\"utf-8\") as f:\n-            requirements.extend(f.readlines())\n-    requirements = [\n-        req for req in requirements if not req.startswith(\"#\") and not req.startswith(\"-\")\n-    ]\n+            raw = f.read().splitlines()\n+            requirements.extend([r for r in raw if not r.startswith(\"#\") and not r.startswith(\"-\")])\n+            recursive_reqs = [r for r in raw if r.startswith(\"-r\")]\n+            if recursive_reqs:\n+                filenames = []\n+                for recursive_req in recursive_reqs:\n+                    file_spec = recursive_req.split()[-1]\n+                    file_path = Path(file_dir) / file_spec\n+                    filenames.append(str(file_path.resolve()))\n+                requirements.extend(load_requirements(file_list=filenames))",
        "comment_created_at": "2024-04-05T13:59:23+00:00",
        "comment_author": "qued",
        "comment_body": "We don't need to keep the file open after we've read it, so this block can be moved out of the `with` clause.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1553735803",
    "pr_number": 2861,
    "pr_file": "setup.py",
    "created_at": "2024-04-05T14:14:11+00:00",
    "commented_code": "file_list = [file_list]\n     requirements: List[str] = []\n     for file in file_list:\n+        path = Path(file)\n+        file_dir = path.parent.resolve()\n         with open(file, encoding=\"utf-8\") as f:\n-            requirements.extend(f.readlines())\n-    requirements = [\n-        req for req in requirements if not req.startswith(\"#\") and not req.startswith(\"-\")\n-    ]\n+            raw = f.read().splitlines()\n+            requirements.extend([r for r in raw if not r.startswith(\"#\") and not r.startswith(\"-\")])\n+            recursive_reqs = [r for r in raw if r.startswith(\"-r\")]\n+            if recursive_reqs:",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1553735803",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2861,
        "pr_file": "setup.py",
        "discussion_id": "1553735803",
        "commented_code": "@@ -32,11 +33,19 @@ def load_requirements(file_list: Optional[Union[str, List[str]]] = None) -> List\n         file_list = [file_list]\n     requirements: List[str] = []\n     for file in file_list:\n+        path = Path(file)\n+        file_dir = path.parent.resolve()\n         with open(file, encoding=\"utf-8\") as f:\n-            requirements.extend(f.readlines())\n-    requirements = [\n-        req for req in requirements if not req.startswith(\"#\") and not req.startswith(\"-\")\n-    ]\n+            raw = f.read().splitlines()\n+            requirements.extend([r for r in raw if not r.startswith(\"#\") and not r.startswith(\"-\")])\n+            recursive_reqs = [r for r in raw if r.startswith(\"-r\")]\n+            if recursive_reqs:",
        "comment_created_at": "2024-04-05T14:14:11+00:00",
        "comment_author": "qued",
        "comment_body": "Small nit: the code iterates over `recursive_reqs`, so if `recursive_reqs` is empty all the code within this `if` block is a noop anyway. In such cases I think it's cleaner and more readable to not include the `if` statement.",
        "pr_file_module": null
      },
      {
        "comment_id": "1553750912",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2861,
        "pr_file": "setup.py",
        "discussion_id": "1553735803",
        "commented_code": "@@ -32,11 +33,19 @@ def load_requirements(file_list: Optional[Union[str, List[str]]] = None) -> List\n         file_list = [file_list]\n     requirements: List[str] = []\n     for file in file_list:\n+        path = Path(file)\n+        file_dir = path.parent.resolve()\n         with open(file, encoding=\"utf-8\") as f:\n-            requirements.extend(f.readlines())\n-    requirements = [\n-        req for req in requirements if not req.startswith(\"#\") and not req.startswith(\"-\")\n-    ]\n+            raw = f.read().splitlines()\n+            requirements.extend([r for r in raw if not r.startswith(\"#\") and not r.startswith(\"-\")])\n+            recursive_reqs = [r for r in raw if r.startswith(\"-r\")]\n+            if recursive_reqs:",
        "comment_created_at": "2024-04-05T14:24:21+00:00",
        "comment_author": "rbiseck3",
        "comment_body": "I actually did this for readability because I only instantiate the `filenames = []` if there are any recursive ones to look through. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1553752330",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2861,
        "pr_file": "setup.py",
        "discussion_id": "1553735803",
        "commented_code": "@@ -32,11 +33,19 @@ def load_requirements(file_list: Optional[Union[str, List[str]]] = None) -> List\n         file_list = [file_list]\n     requirements: List[str] = []\n     for file in file_list:\n+        path = Path(file)\n+        file_dir = path.parent.resolve()\n         with open(file, encoding=\"utf-8\") as f:\n-            requirements.extend(f.readlines())\n-    requirements = [\n-        req for req in requirements if not req.startswith(\"#\") and not req.startswith(\"-\")\n-    ]\n+            raw = f.read().splitlines()\n+            requirements.extend([r for r in raw if not r.startswith(\"#\") and not r.startswith(\"-\")])\n+            recursive_reqs = [r for r in raw if r.startswith(\"-r\")]\n+            if recursive_reqs:",
        "comment_created_at": "2024-04-05T14:25:21+00:00",
        "comment_author": "rbiseck3",
        "comment_body": "But I could just have it call it one at a time. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1700510615",
    "pr_number": 3444,
    "pr_file": "test_unstructured/partition/test_email.py",
    "created_at": "2024-08-01T16:51:25+00:00",
    "commented_code": "def test_partition_email_from_filename():\n-    filename = os.path.join(EXAMPLE_DOCS_DIRECTORY, \"fake-email.eml\")\n-    elements = partition_email(filename=filename)\n+    elements = partition_email(filename=example_doc_path(\"eml/fake-email.eml\"))",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1700510615",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3444,
        "pr_file": "test_unstructured/partition/test_email.py",
        "discussion_id": "1700510615",
        "commented_code": "@@ -110,24 +119,17 @@\n \n \n def test_partition_email_from_filename():\n-    filename = os.path.join(EXAMPLE_DOCS_DIRECTORY, \"fake-email.eml\")\n-    elements = partition_email(filename=filename)\n+    elements = partition_email(filename=example_doc_path(\"eml/fake-email.eml\"))",
        "comment_created_at": "2024-08-01T16:51:25+00:00",
        "comment_author": "scanny",
        "comment_body": "Just for future reference, these are more compact and easier to scan if you remove the `filename=` bit. `filename` is always the first parameter and can be provided positionally. Since this form appears so often in tests I think it improves readability if it's reduced to \"partition(this)\" :)",
        "pr_file_module": null
      },
      {
        "comment_id": "1700578183",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3444,
        "pr_file": "test_unstructured/partition/test_email.py",
        "discussion_id": "1700510615",
        "commented_code": "@@ -110,24 +119,17 @@\n \n \n def test_partition_email_from_filename():\n-    filename = os.path.join(EXAMPLE_DOCS_DIRECTORY, \"fake-email.eml\")\n-    elements = partition_email(filename=filename)\n+    elements = partition_email(filename=example_doc_path(\"eml/fake-email.eml\"))",
        "comment_created_at": "2024-08-01T17:48:43+00:00",
        "comment_author": "Coniferish",
        "comment_body": "Thanks. Didn't think about that, but will try to remember to do so :)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1688061423",
    "pr_number": 3431,
    "pr_file": "unstructured/errors.py",
    "created_at": "2024-07-23T13:25:15+00:00",
    "commented_code": "+class PdfMaxPagesExceededError(ValueError):\n+    \"\"\"Error raised, when number of PDF pages exceeds max_pages limit\n+    and HI_RES strategy is chosen.\n+    \"\"\"\n+\n+    def __init__(self, message):\n+        self.message = message",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1688061423",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3431,
        "pr_file": "unstructured/errors.py",
        "discussion_id": "1688061423",
        "commented_code": "@@ -0,0 +1,8 @@\n+class PdfMaxPagesExceededError(ValueError):\n+    \"\"\"Error raised, when number of PDF pages exceeds max_pages limit\n+    and HI_RES strategy is chosen.\n+    \"\"\"\n+\n+    def __init__(self, message):\n+        self.message = message",
        "comment_created_at": "2024-07-23T13:25:15+00:00",
        "comment_author": "pawel-kmiecik",
        "comment_body": "we don't need keeping the message in this class. So the whole `__init__` method is not needed.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1690541361",
    "pr_number": 3437,
    "pr_file": "unstructured/file_utils/filetype.py",
    "created_at": "2024-07-24T22:45:30+00:00",
    "commented_code": "A `str` return value is always in lower-case.\n         \"\"\"\n+        file_path = self.file_path",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1690541361",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3437,
        "pr_file": "unstructured/file_utils/filetype.py",
        "discussion_id": "1690541361",
        "commented_code": "@@ -351,19 +358,19 @@ def mime_type(self) -> str | None:\n \n         A `str` return value is always in lower-case.\n         \"\"\"\n+        file_path = self.file_path",
        "comment_created_at": "2024-07-24T22:45:30+00:00",
        "comment_author": "Coniferish",
        "comment_body": "why declare this instead of using `self.file_path` down below?",
        "pr_file_module": null
      },
      {
        "comment_id": "1690641574",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3437,
        "pr_file": "unstructured/file_utils/filetype.py",
        "discussion_id": "1690541361",
        "commented_code": "@@ -351,19 +358,19 @@ def mime_type(self) -> str | None:\n \n         A `str` return value is always in lower-case.\n         \"\"\"\n+        file_path = self.file_path",
        "comment_created_at": "2024-07-25T01:04:38+00:00",
        "comment_author": "scanny",
        "comment_body": "Just efficiency, a little more compact too. It's referred to four times in the function. I think more the latter in this case since only two of those dereferencings will happen for any given file.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1651407465",
    "pr_number": 3264,
    "pr_file": "unstructured/ingest/v2/pipeline/steps/upload.py",
    "created_at": "2024-06-24T17:47:11+00:00",
    "commented_code": "def process_whole(self, iterable: iterable_input):\n         self.run(contents=iterable)\n \n-    async def _process_async(self, iterable: iterable_input):\n-        return await asyncio.gather(*[self.run_async(**i) for i in iterable])\n-\n-    def process_async(self, iterable: iterable_input):\n-        logger.info(\"processing content async\")\n-        return asyncio.run(self._process_async(iterable=iterable))\n-\n+    @timed\n     def __call__(self, iterable: iterable_input):\n         logger.info(\n             f\"Calling {self.__class__.__name__} \" f\"with {len(iterable)} docs\",  # type: ignore\n         )\n-        if self.process.is_async():\n+        if self.process.is_async() and self.process.is_async():",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1651407465",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3264,
        "pr_file": "unstructured/ingest/v2/pipeline/steps/upload.py",
        "discussion_id": "1651407465",
        "commented_code": "@@ -44,18 +44,12 @@ def __post_init__(self):\n     def process_whole(self, iterable: iterable_input):\n         self.run(contents=iterable)\n \n-    async def _process_async(self, iterable: iterable_input):\n-        return await asyncio.gather(*[self.run_async(**i) for i in iterable])\n-\n-    def process_async(self, iterable: iterable_input):\n-        logger.info(\"processing content async\")\n-        return asyncio.run(self._process_async(iterable=iterable))\n-\n+    @timed\n     def __call__(self, iterable: iterable_input):\n         logger.info(\n             f\"Calling {self.__class__.__name__} \" f\"with {len(iterable)} docs\",  # type: ignore\n         )\n-        if self.process.is_async():\n+        if self.process.is_async() and self.process.is_async():",
        "comment_created_at": "2024-06-24T17:47:11+00:00",
        "comment_author": "potter-potter",
        "comment_body": "This seems strange. why test for is_async twice?",
        "pr_file_module": null
      },
      {
        "comment_id": "1651411531",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3264,
        "pr_file": "unstructured/ingest/v2/pipeline/steps/upload.py",
        "discussion_id": "1651407465",
        "commented_code": "@@ -44,18 +44,12 @@ def __post_init__(self):\n     def process_whole(self, iterable: iterable_input):\n         self.run(contents=iterable)\n \n-    async def _process_async(self, iterable: iterable_input):\n-        return await asyncio.gather(*[self.run_async(**i) for i in iterable])\n-\n-    def process_async(self, iterable: iterable_input):\n-        logger.info(\"processing content async\")\n-        return asyncio.run(self._process_async(iterable=iterable))\n-\n+    @timed\n     def __call__(self, iterable: iterable_input):\n         logger.info(\n             f\"Calling {self.__class__.__name__} \" f\"with {len(iterable)} docs\",  # type: ignore\n         )\n-        if self.process.is_async():\n+        if self.process.is_async() and self.process.is_async():",
        "comment_created_at": "2024-06-24T17:51:02+00:00",
        "comment_author": "rbiseck3",
        "comment_body": "Good catch, must have been a copy-paste error. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1602667885",
    "pr_number": 3014,
    "pr_file": "unstructured/metrics/evaluate.py",
    "created_at": "2024-05-16T06:06:01+00:00",
    "commented_code": "@abstractmethod\n     def _process_document(self, doc: Path) -> list:\n         \"\"\"Should return all metadata and metrics for a single document.\"\"\"\n-        pass",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1602667885",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3014,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1602667885",
        "commented_code": "@@ -160,7 +160,6 @@ def _try_process_document(self, doc: Path) -> Optional[list]:\n     @abstractmethod\n     def _process_document(self, doc: Path) -> list:\n         \"\"\"Should return all metadata and metrics for a single document.\"\"\"\n-        pass",
        "comment_created_at": "2024-05-16T06:06:01+00:00",
        "comment_author": "cragwolfe",
        "comment_body": "why removed?",
        "pr_file_module": null
      },
      {
        "comment_id": "1602744761",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3014,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1602667885",
        "commented_code": "@@ -160,7 +160,6 @@ def _try_process_document(self, doc: Path) -> Optional[list]:\n     @abstractmethod\n     def _process_document(self, doc: Path) -> list:\n         \"\"\"Should return all metadata and metrics for a single document.\"\"\"\n-        pass",
        "comment_created_at": "2024-05-16T07:15:54+00:00",
        "comment_author": "amadeusz-ds",
        "comment_body": "It was removed by the linter",
        "pr_file_module": null
      },
      {
        "comment_id": "1603304471",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3014,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1602667885",
        "commented_code": "@@ -160,7 +160,6 @@ def _try_process_document(self, doc: Path) -> Optional[list]:\n     @abstractmethod\n     def _process_document(self, doc: Path) -> list:\n         \"\"\"Should return all metadata and metrics for a single document.\"\"\"\n-        pass",
        "comment_created_at": "2024-05-16T13:02:42+00:00",
        "comment_author": "pawel-kmiecik",
        "comment_body": "If a docstring is added, the `pass` keyword is optional for functions.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1556302420",
    "pr_number": 2673,
    "pr_file": "test_unstructured/documents/test_elements.py",
    "created_at": "2024-04-08T18:48:57+00:00",
    "commented_code": "RegexMetadata,\n     Text,\n     Title,\n+    assign_and_map_hash_ids,\n )\n \n \n-def test_text_id():\n-    text_element = Text(text=\"hello there!\")\n-    assert text_element.id == \"c69509590d81db2f37f9d75480c8efed\"\n-\n-\n-def test_text_uuid():\n-    text_element = Text(text=\"hello there!\", element_id=UUID())\n-\n-    id = text_element.id\n-\n-    assert isinstance(id, str)\n-    assert len(id) == 36\n-    assert id.count(\"-\") == 4\n-    # -- Test that the element is JSON serializable. This shold run without an error --\n-    json.dumps(text_element.to_dict())\n+@pytest.mark.parametrize(\n+    \"element\",\n+    [\n+        Element(),\n+        Text(text=\"\"),\n+        CheckBox(),\n+    ],\n+)",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1556302420",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2673,
        "pr_file": "test_unstructured/documents/test_elements.py",
        "discussion_id": "1556302420",
        "commented_code": "@@ -29,34 +30,48 @@\n     RegexMetadata,\n     Text,\n     Title,\n+    assign_and_map_hash_ids,\n )\n \n \n-def test_text_id():\n-    text_element = Text(text=\"hello there!\")\n-    assert text_element.id == \"c69509590d81db2f37f9d75480c8efed\"\n-\n-\n-def test_text_uuid():\n-    text_element = Text(text=\"hello there!\", element_id=UUID())\n-\n-    id = text_element.id\n-\n-    assert isinstance(id, str)\n-    assert len(id) == 36\n-    assert id.count(\"-\") == 4\n-    # -- Test that the element is JSON serializable. This shold run without an error --\n-    json.dumps(text_element.to_dict())\n+@pytest.mark.parametrize(\n+    \"element\",\n+    [\n+        Element(),\n+        Text(text=\"\"),\n+        CheckBox(),\n+    ],\n+)",
        "comment_created_at": "2024-04-08T18:48:57+00:00",
        "comment_author": "scanny",
        "comment_body": "This can all fit on one line. Remove the comma after `CheckBox` and the closing `]` otherwise Black will spread this out vertically like this.",
        "pr_file_module": null
      },
      {
        "comment_id": "1557529120",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2673,
        "pr_file": "test_unstructured/documents/test_elements.py",
        "discussion_id": "1556302420",
        "commented_code": "@@ -29,34 +30,48 @@\n     RegexMetadata,\n     Text,\n     Title,\n+    assign_and_map_hash_ids,\n )\n \n \n-def test_text_id():\n-    text_element = Text(text=\"hello there!\")\n-    assert text_element.id == \"c69509590d81db2f37f9d75480c8efed\"\n-\n-\n-def test_text_uuid():\n-    text_element = Text(text=\"hello there!\", element_id=UUID())\n-\n-    id = text_element.id\n-\n-    assert isinstance(id, str)\n-    assert len(id) == 36\n-    assert id.count(\"-\") == 4\n-    # -- Test that the element is JSON serializable. This shold run without an error --\n-    json.dumps(text_element.to_dict())\n+@pytest.mark.parametrize(\n+    \"element\",\n+    [\n+        Element(),\n+        Text(text=\"\"),\n+        CheckBox(),\n+    ],\n+)",
        "comment_created_at": "2024-04-09T12:09:58+00:00",
        "comment_author": "micmarty-deepsense",
        "comment_body": "![image](https://github.com/Unstructured-IO/unstructured/assets/64484917/cae48fd5-e3a8-4dfc-8b58-53ca9363faaa)\r\n\r\ndone",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1556322778",
    "pr_number": 2673,
    "pr_file": "test_unstructured/documents/test_email_elements.py",
    "created_at": "2024-04-08T18:52:44+00:00",
    "commented_code": "-import uuid\n from functools import partial\n \n import pytest\n \n from unstructured.cleaners.core import clean_prefix\n from unstructured.cleaners.translate import translate_text\n-from unstructured.documents.email_elements import UUID, EmailElement, Name, NoID\n+from unstructured.documents.email_elements import EmailElement, Name, NoID, Subject\n \n \n-def test_text_id():\n-    name_element = Name(name=\"Example\", text=\"hello there!\")\n-    assert name_element.id == \"c69509590d81db2f37f9d75480c8efed\"\n+@pytest.mark.parametrize(\n+    \"element\",\n+    [\n+        EmailElement(text=\"\"),\n+        Name(text=\"\", name=\"\"),\n+        Subject(text=\"\"),\n+    ],\n+)",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1556322778",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2673,
        "pr_file": "test_unstructured/documents/test_email_elements.py",
        "discussion_id": "1556322778",
        "commented_code": "@@ -1,26 +1,50 @@\n-import uuid\n from functools import partial\n \n import pytest\n \n from unstructured.cleaners.core import clean_prefix\n from unstructured.cleaners.translate import translate_text\n-from unstructured.documents.email_elements import UUID, EmailElement, Name, NoID\n+from unstructured.documents.email_elements import EmailElement, Name, NoID, Subject\n \n \n-def test_text_id():\n-    name_element = Name(name=\"Example\", text=\"hello there!\")\n-    assert name_element.id == \"c69509590d81db2f37f9d75480c8efed\"\n+@pytest.mark.parametrize(\n+    \"element\",\n+    [\n+        EmailElement(text=\"\"),\n+        Name(text=\"\", name=\"\"),\n+        Subject(text=\"\"),\n+    ],\n+)",
        "comment_created_at": "2024-04-08T18:52:44+00:00",
        "comment_author": "scanny",
        "comment_body": "I think these will fit on one line, won't they? Try to make things as vertically compact as possible (within reason) so scrolling is minimized and whole functions can fit on the same screen-full of lines.",
        "pr_file_module": null
      },
      {
        "comment_id": "1557529418",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2673,
        "pr_file": "test_unstructured/documents/test_email_elements.py",
        "discussion_id": "1556322778",
        "commented_code": "@@ -1,26 +1,50 @@\n-import uuid\n from functools import partial\n \n import pytest\n \n from unstructured.cleaners.core import clean_prefix\n from unstructured.cleaners.translate import translate_text\n-from unstructured.documents.email_elements import UUID, EmailElement, Name, NoID\n+from unstructured.documents.email_elements import EmailElement, Name, NoID, Subject\n \n \n-def test_text_id():\n-    name_element = Name(name=\"Example\", text=\"hello there!\")\n-    assert name_element.id == \"c69509590d81db2f37f9d75480c8efed\"\n+@pytest.mark.parametrize(\n+    \"element\",\n+    [\n+        EmailElement(text=\"\"),\n+        Name(text=\"\", name=\"\"),\n+        Subject(text=\"\"),\n+    ],\n+)",
        "comment_created_at": "2024-04-09T12:10:15+00:00",
        "comment_author": "micmarty-deepsense",
        "comment_body": "![image](https://github.com/Unstructured-IO/unstructured/assets/64484917/0a9c110f-caef-4dac-9dad-cf91d87a40a4)\r\ndone",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1571750149",
    "pr_number": 2905,
    "pr_file": "test_unstructured_ingest/unit/pipeline/reformat/test_chunking.py",
    "created_at": "2024-04-19T04:04:43+00:00",
    "commented_code": "+from __future__ import annotations\n+\n+import os\n+import tempfile\n+\n+import pytest\n+from _pytest.logging import LogCaptureFixture\n+\n+from test_unstructured.unit_utils import (\n+    FixtureRequest,\n+    Mock,\n+    example_doc_path,\n+    function_mock,\n+    method_mock,\n+)\n+from unstructured.documents.elements import CompositeElement\n+from unstructured.ingest.interfaces import ChunkingConfig, PartitionConfig\n+from unstructured.ingest.pipeline.interfaces import PipelineContext\n+from unstructured.ingest.pipeline.reformat.chunking import Chunker\n+\n+ELEMENTS_JSON_FILE = example_doc_path(\n+    \"test_evaluate_files/unstructured_output/Bank Good Credit Loan.pptx.json\"\n+)\n+\n+\n+class DescribeChunker:\n+    \"\"\"Unit tests for ingest.pipeline.reformat.chunking.Chunker\"\"\"\n+\n+    # -- Chunker.run() -----------------------------------------------------------------------------\n+\n+    # -- integration test --\n+    def it_creates_json(self, _ingest_docs_map_: Mock):\n+        chunking_config = ChunkingConfig(chunking_strategy=\"by_title\")\n+        pipeline_context = PipelineContext()\n+        partition_config = PartitionConfig()",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1571750149",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2905,
        "pr_file": "test_unstructured_ingest/unit/pipeline/reformat/test_chunking.py",
        "discussion_id": "1571750149",
        "commented_code": "@@ -0,0 +1,144 @@\n+from __future__ import annotations\n+\n+import os\n+import tempfile\n+\n+import pytest\n+from _pytest.logging import LogCaptureFixture\n+\n+from test_unstructured.unit_utils import (\n+    FixtureRequest,\n+    Mock,\n+    example_doc_path,\n+    function_mock,\n+    method_mock,\n+)\n+from unstructured.documents.elements import CompositeElement\n+from unstructured.ingest.interfaces import ChunkingConfig, PartitionConfig\n+from unstructured.ingest.pipeline.interfaces import PipelineContext\n+from unstructured.ingest.pipeline.reformat.chunking import Chunker\n+\n+ELEMENTS_JSON_FILE = example_doc_path(\n+    \"test_evaluate_files/unstructured_output/Bank Good Credit Loan.pptx.json\"\n+)\n+\n+\n+class DescribeChunker:\n+    \"\"\"Unit tests for ingest.pipeline.reformat.chunking.Chunker\"\"\"\n+\n+    # -- Chunker.run() -----------------------------------------------------------------------------\n+\n+    # -- integration test --\n+    def it_creates_json(self, _ingest_docs_map_: Mock):\n+        chunking_config = ChunkingConfig(chunking_strategy=\"by_title\")\n+        pipeline_context = PipelineContext()\n+        partition_config = PartitionConfig()",
        "comment_created_at": "2024-04-19T04:04:43+00:00",
        "comment_author": "scanny",
        "comment_body": "These are all short and can be inlined in the `Chunker` call immediately below. In general, the only reason you want to introduce a local variable for something like this is if you need access to the object later in the test.\r\n\r\nBesides compactness, placing them inline avoids implying to the reader that these need to be accessed later. Like the time I spent discovering they aren't used anywhere else would be unnecessary.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1496435605",
    "pr_number": 2554,
    "pr_file": "unstructured/utils.py",
    "created_at": "2024-02-20T20:15:05+00:00",
    "commented_code": "first_string: str,\n     second_string: str,\n     n: int,\n-) -> (float, List):\n+) -> tuple[float, set[tuple[str, ...]]]:\n     \"\"\"Calculate the percentage of common_ngrams between string_A and string_B\n     with reference to the total number of ngrams in string_A\"\"\"\n \n     if not n:\n-        return 0, {}\n+        return 0, set()\n     first_string_ngrams = ngrams(first_string.split(), n)\n     second_string_ngrams = ngrams(second_string.split(), n)\n \n     if not first_string_ngrams:\n-        return 0\n+        return 0, set()\n \n     common_ngrams = set(first_string_ngrams) & set(second_string_ngrams)\n     percentage = (len(common_ngrams) / len(first_string_ngrams)) * 100\n     return percentage, common_ngrams\n \n \n-def calculate_largest_ngram_percentage(first_string: str, second_string: str) -> (float, List, str):\n+def calculate_largest_ngram_percentage(\n+    first_string: str,\n+    second_string: str,",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1496435605",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2554,
        "pr_file": "unstructured/utils.py",
        "discussion_id": "1496435605",
        "commented_code": "@@ -334,28 +352,37 @@ def calculate_shared_ngram_percentage(\n     first_string: str,\n     second_string: str,\n     n: int,\n-) -> (float, List):\n+) -> tuple[float, set[tuple[str, ...]]]:\n     \"\"\"Calculate the percentage of common_ngrams between string_A and string_B\n     with reference to the total number of ngrams in string_A\"\"\"\n \n     if not n:\n-        return 0, {}\n+        return 0, set()\n     first_string_ngrams = ngrams(first_string.split(), n)\n     second_string_ngrams = ngrams(second_string.split(), n)\n \n     if not first_string_ngrams:\n-        return 0\n+        return 0, set()\n \n     common_ngrams = set(first_string_ngrams) & set(second_string_ngrams)\n     percentage = (len(common_ngrams) / len(first_string_ngrams)) * 100\n     return percentage, common_ngrams\n \n \n-def calculate_largest_ngram_percentage(first_string: str, second_string: str) -> (float, List, str):\n+def calculate_largest_ngram_percentage(\n+    first_string: str,\n+    second_string: str,",
        "comment_created_at": "2024-02-20T20:15:05+00:00",
        "comment_author": "scanny",
        "comment_body": "these will fit on one line. That's that pesky Black magic comma thing.",
        "pr_file_module": null
      },
      {
        "comment_id": "1496523592",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2554,
        "pr_file": "unstructured/utils.py",
        "discussion_id": "1496435605",
        "commented_code": "@@ -334,28 +352,37 @@ def calculate_shared_ngram_percentage(\n     first_string: str,\n     second_string: str,\n     n: int,\n-) -> (float, List):\n+) -> tuple[float, set[tuple[str, ...]]]:\n     \"\"\"Calculate the percentage of common_ngrams between string_A and string_B\n     with reference to the total number of ngrams in string_A\"\"\"\n \n     if not n:\n-        return 0, {}\n+        return 0, set()\n     first_string_ngrams = ngrams(first_string.split(), n)\n     second_string_ngrams = ngrams(second_string.split(), n)\n \n     if not first_string_ngrams:\n-        return 0\n+        return 0, set()\n \n     common_ngrams = set(first_string_ngrams) & set(second_string_ngrams)\n     percentage = (len(common_ngrams) / len(first_string_ngrams)) * 100\n     return percentage, common_ngrams\n \n \n-def calculate_largest_ngram_percentage(first_string: str, second_string: str) -> (float, List, str):\n+def calculate_largest_ngram_percentage(\n+    first_string: str,\n+    second_string: str,",
        "comment_created_at": "2024-02-20T21:17:42+00:00",
        "comment_author": "Coniferish",
        "comment_body": "Is this what the fixed version should look like?\r\n```\r\ndef calculate_largest_ngram_percentage(first_string: str, second_string: str,\r\n) -> tuple[float, set[tuple[str, ...]], str]:\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1496615330",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2554,
        "pr_file": "unstructured/utils.py",
        "discussion_id": "1496435605",
        "commented_code": "@@ -334,28 +352,37 @@ def calculate_shared_ngram_percentage(\n     first_string: str,\n     second_string: str,\n     n: int,\n-) -> (float, List):\n+) -> tuple[float, set[tuple[str, ...]]]:\n     \"\"\"Calculate the percentage of common_ngrams between string_A and string_B\n     with reference to the total number of ngrams in string_A\"\"\"\n \n     if not n:\n-        return 0, {}\n+        return 0, set()\n     first_string_ngrams = ngrams(first_string.split(), n)\n     second_string_ngrams = ngrams(second_string.split(), n)\n \n     if not first_string_ngrams:\n-        return 0\n+        return 0, set()\n \n     common_ngrams = set(first_string_ngrams) & set(second_string_ngrams)\n     percentage = (len(common_ngrams) / len(first_string_ngrams)) * 100\n     return percentage, common_ngrams\n \n \n-def calculate_largest_ngram_percentage(first_string: str, second_string: str) -> (float, List, str):\n+def calculate_largest_ngram_percentage(\n+    first_string: str,\n+    second_string: str,",
        "comment_created_at": "2024-02-20T22:25:41+00:00",
        "comment_author": "scanny",
        "comment_body": "No. Both parameters will fit on one line, like this:\r\n```python\r\ndef calculate_largest_ngram_percentage(\r\n    first_string: str, second_string: str\r\n) -> tuple[float, set[tuple[str, ...]], str]:\r\n```\r\n\r\nNote the lack of a trailing comma after `second_string`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1498188692",
    "pr_number": 2572,
    "pr_file": "unstructured/metrics/evaluate.py",
    "created_at": "2024-02-21T19:33:11+00:00",
    "commented_code": "_write_to_file(export_dir, \"all-docs-element-type-frequency.tsv\", df)\n     _write_to_file(export_dir, \"aggregate-scores-element-type.tsv\", agg_df)\n     _display(agg_df)\n+\n+\n+def group_text_extraction_acc(\n+    grouping: str, df: pd.DataFrame, export_filename: str\n+) -> Tuple[pd.DataFrame, str]:\n+    \"\"\"Aggregate the evaluation score on `grouping` type.\"\"\"\n+    if grouping in [\"doctype\", \"connector\"]:\n+        grouped_acc = (\n+            df.groupby(grouping)\n+            .agg({\"cct-accuracy\": [_mean, _stdev, \"count\"]})\n+            .rename(columns={\"_mean\": \"mean\", \"_stdev\": \"stdev\"})\n+        )\n+        grouped_miss = (\n+            df.groupby(grouping)\n+            .agg({\"cct-%missing\": [_mean, _stdev, \"count\"]})\n+            .rename(columns={\"_mean\": \"mean\", \"_stdev\": \"stdev\"})\n+        )\n+        df = _format_grouping_output(grouped_acc, grouped_miss)\n+        export_filename = f\"all-{grouping}-agg-cct\"\n+    else:\n+        print(\"No field to group by. Returning a non-group evaluation.\")\n+    return df, export_filename",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1498188692",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2572,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1498188692",
        "commented_code": "@@ -187,3 +173,25 @@ def measure_element_type_accuracy(\n     _write_to_file(export_dir, \"all-docs-element-type-frequency.tsv\", df)\n     _write_to_file(export_dir, \"aggregate-scores-element-type.tsv\", agg_df)\n     _display(agg_df)\n+\n+\n+def group_text_extraction_acc(\n+    grouping: str, df: pd.DataFrame, export_filename: str\n+) -> Tuple[pd.DataFrame, str]:\n+    \"\"\"Aggregate the evaluation score on `grouping` type.\"\"\"\n+    if grouping in [\"doctype\", \"connector\"]:\n+        grouped_acc = (\n+            df.groupby(grouping)\n+            .agg({\"cct-accuracy\": [_mean, _stdev, \"count\"]})\n+            .rename(columns={\"_mean\": \"mean\", \"_stdev\": \"stdev\"})\n+        )\n+        grouped_miss = (\n+            df.groupby(grouping)\n+            .agg({\"cct-%missing\": [_mean, _stdev, \"count\"]})\n+            .rename(columns={\"_mean\": \"mean\", \"_stdev\": \"stdev\"})\n+        )\n+        df = _format_grouping_output(grouped_acc, grouped_miss)\n+        export_filename = f\"all-{grouping}-agg-cct\"\n+    else:\n+        print(\"No field to group by. Returning a non-group evaluation.\")\n+    return df, export_filename",
        "comment_created_at": "2024-02-21T19:33:11+00:00",
        "comment_author": "scanny",
        "comment_body": "In general, avoiding nesting improves readability. In this case it could be accomplished with a \"guard\", like:\r\n```python\r\n    # -- {{explanation of why other grouping types don't count}} --\r\n    if grouping not in (\"doctype\", \"connector\"):\r\n        print(\"No field to group by. Returning a non-group evaluation.\")\r\n        return df, export_filename\r\n\r\n    grouped_acc = (\r\n        df.groupby(grouping)\r\n        .agg({\"cct-accuracy\": [_mean, _stdev, \"count\"]})\r\n        .rename(columns={\"_mean\": \"mean\", \"_stdev\": \"stdev\"})\r\n    )\r\n    grouped_miss = (\r\n        df.groupby(grouping)\r\n        .agg({\"cct-%missing\": [_mean, _stdev, \"count\"]})\r\n        .rename(columns={\"_mean\": \"mean\", \"_stdev\": \"stdev\"})\r\n    )\r\n    df = _format_grouping_output(grouped_acc, grouped_miss)\r\n    export_filename = f\"all-{grouping}-agg-cct\"\r\n    return df, export_filename\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1498191562",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2572,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1498188692",
        "commented_code": "@@ -187,3 +173,25 @@ def measure_element_type_accuracy(\n     _write_to_file(export_dir, \"all-docs-element-type-frequency.tsv\", df)\n     _write_to_file(export_dir, \"aggregate-scores-element-type.tsv\", agg_df)\n     _display(agg_df)\n+\n+\n+def group_text_extraction_acc(\n+    grouping: str, df: pd.DataFrame, export_filename: str\n+) -> Tuple[pd.DataFrame, str]:\n+    \"\"\"Aggregate the evaluation score on `grouping` type.\"\"\"\n+    if grouping in [\"doctype\", \"connector\"]:\n+        grouped_acc = (\n+            df.groupby(grouping)\n+            .agg({\"cct-accuracy\": [_mean, _stdev, \"count\"]})\n+            .rename(columns={\"_mean\": \"mean\", \"_stdev\": \"stdev\"})\n+        )\n+        grouped_miss = (\n+            df.groupby(grouping)\n+            .agg({\"cct-%missing\": [_mean, _stdev, \"count\"]})\n+            .rename(columns={\"_mean\": \"mean\", \"_stdev\": \"stdev\"})\n+        )\n+        df = _format_grouping_output(grouped_acc, grouped_miss)\n+        export_filename = f\"all-{grouping}-agg-cct\"\n+    else:\n+        print(\"No field to group by. Returning a non-group evaluation.\")\n+    return df, export_filename",
        "comment_created_at": "2024-02-21T19:35:13+00:00",
        "comment_author": "scanny",
        "comment_body": "Also, why is there a `print()` call? I'm not familiar with this part of the code, so maybe it's part of a CLI or something, but in general a library should not have print statements.",
        "pr_file_module": null
      },
      {
        "comment_id": "1498681678",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2572,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1498188692",
        "commented_code": "@@ -187,3 +173,25 @@ def measure_element_type_accuracy(\n     _write_to_file(export_dir, \"all-docs-element-type-frequency.tsv\", df)\n     _write_to_file(export_dir, \"aggregate-scores-element-type.tsv\", agg_df)\n     _display(agg_df)\n+\n+\n+def group_text_extraction_acc(\n+    grouping: str, df: pd.DataFrame, export_filename: str\n+) -> Tuple[pd.DataFrame, str]:\n+    \"\"\"Aggregate the evaluation score on `grouping` type.\"\"\"\n+    if grouping in [\"doctype\", \"connector\"]:\n+        grouped_acc = (\n+            df.groupby(grouping)\n+            .agg({\"cct-accuracy\": [_mean, _stdev, \"count\"]})\n+            .rename(columns={\"_mean\": \"mean\", \"_stdev\": \"stdev\"})\n+        )\n+        grouped_miss = (\n+            df.groupby(grouping)\n+            .agg({\"cct-%missing\": [_mean, _stdev, \"count\"]})\n+            .rename(columns={\"_mean\": \"mean\", \"_stdev\": \"stdev\"})\n+        )\n+        df = _format_grouping_output(grouped_acc, grouped_miss)\n+        export_filename = f\"all-{grouping}-agg-cct\"\n+    else:\n+        print(\"No field to group by. Returning a non-group evaluation.\")\n+    return df, export_filename",
        "comment_created_at": "2024-02-22T05:52:19+00:00",
        "comment_author": "Klaijan",
        "comment_body": "The print call is to be shown on CLI terminal if no grouping is done on the df. I'll think of ways to move this elsewhere if possible.",
        "pr_file_module": null
      },
      {
        "comment_id": "1499643886",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2572,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1498188692",
        "commented_code": "@@ -187,3 +173,25 @@ def measure_element_type_accuracy(\n     _write_to_file(export_dir, \"all-docs-element-type-frequency.tsv\", df)\n     _write_to_file(export_dir, \"aggregate-scores-element-type.tsv\", agg_df)\n     _display(agg_df)\n+\n+\n+def group_text_extraction_acc(\n+    grouping: str, df: pd.DataFrame, export_filename: str\n+) -> Tuple[pd.DataFrame, str]:\n+    \"\"\"Aggregate the evaluation score on `grouping` type.\"\"\"\n+    if grouping in [\"doctype\", \"connector\"]:\n+        grouped_acc = (\n+            df.groupby(grouping)\n+            .agg({\"cct-accuracy\": [_mean, _stdev, \"count\"]})\n+            .rename(columns={\"_mean\": \"mean\", \"_stdev\": \"stdev\"})\n+        )\n+        grouped_miss = (\n+            df.groupby(grouping)\n+            .agg({\"cct-%missing\": [_mean, _stdev, \"count\"]})\n+            .rename(columns={\"_mean\": \"mean\", \"_stdev\": \"stdev\"})\n+        )\n+        df = _format_grouping_output(grouped_acc, grouped_miss)\n+        export_filename = f\"all-{grouping}-agg-cct\"\n+    else:\n+        print(\"No field to group by. Returning a non-group evaluation.\")\n+    return df, export_filename",
        "comment_created_at": "2024-02-22T17:50:08+00:00",
        "comment_author": "scanny",
        "comment_body": "It's fine if this is a CLI, I just didn't realize we had that in the library :)",
        "pr_file_module": null
      }
    ]
  }
]