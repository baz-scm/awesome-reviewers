[
  {
    "discussion_id": "1797347097",
    "pr_number": 3595,
    "pr_file": "CHANGELOG.md",
    "created_at": "2024-10-11T19:25:06+00:00",
    "commented_code": "+## 0.16.0\n+\n+### Enhancements\n+* **Remove ingest implementation**",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1797347097",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3595,
        "pr_file": "CHANGELOG.md",
        "discussion_id": "1797347097",
        "commented_code": "@@ -1,3 +1,8 @@\n+## 0.16.0\n+\n+### Enhancements\n+* **Remove ingest implementation**",
        "comment_created_at": "2024-10-11T19:25:06+00:00",
        "comment_author": "badGarnet",
        "comment_body": "Since this is such a big breaking change lets do a more detailed changelog:\r\n- advise on where to look for the new ingest library\r\n- list all extras that are deprecated as a result (so easier for users who have setup to use certain extras to know things are changed)\r\n- high level on what functionalities are removed exactly (e.g., which submodules are now gone)",
        "pr_file_module": null
      },
      {
        "comment_id": "1797349709",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3595,
        "pr_file": "CHANGELOG.md",
        "discussion_id": "1797347097",
        "commented_code": "@@ -1,3 +1,8 @@\n+## 0.16.0\n+\n+### Enhancements\n+* **Remove ingest implementation**",
        "comment_created_at": "2024-10-11T19:28:16+00:00",
        "comment_author": "badGarnet",
        "comment_body": "additionally please also add to changelog that the implementation for embed submodule is updated with things like `SecretStr` to represent secrets",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1605330474",
    "pr_number": 3009,
    "pr_file": "unstructured/ingest/v2/README.md",
    "created_at": "2024-05-17T17:07:44+00:00",
    "commented_code": "+# Batch Processing Documents\n+\n+## The unstructured-ingest CLI\n+\n+The unstructured library includes a CLI to batch ingest documents from various sources, storing structured outputs locally on the filesystem.\n+\n+For example, the following command processes all the documents in S3 in the\n+`utic-dev-tech-fixtures` bucket with a prefix of `small-pdf-set/`.\n+\n+    unstructured-ingest \\\n+       s3 \\\n+       --remote-url s3://utic-dev-tech-fixtures/small-pdf-set/ \\\n+       --anonymous \\\n+       --output-dir s3-small-batch-output \\\n+       --num-processes 2\n+\n+Naturally, --num-processes may be adjusted for better instance utilization with multiprocessing.\n+\n+Installation note: make sure to install the following extras when installing unstructured, needed for the above command:\n+\n+    pip install \"unstructured[s3,local-inference]\"\n+\n+See the [Quick Start](https://github.com/Unstructured-IO/unstructured#eight_pointed_black_star-quick-start) which documents how to pip install `dectectron2` and other OS dependencies, necessary for the parsing of .PDF files.\n+\n+# Developers' Guide\n+\n+## Local testing\n+\n+When testing from a local checkout rather than a pip-installed version of `unstructured`,\n+just execute `unstructured/ingest/main.py`, e.g.:\n+\n+    PYTHONPATH=. ./unstructured/ingest/v2/main.py \\\n+       s3 \\\n+       --remote-url s3://utic-dev-tech-fixtures/small-pdf-set/ \\\n+       --anonymous \\\n+       --output-dir s3-small-batch-output \\\n+       --num-processes 2\n+\n+## Adding Source Data Connectors\n+\n+To add a source connector, refer to [local.py](unstructured/ingest/v2/processes/connectors/local.py) as an example that implements the two relevant abstract base classes with their associated configs.\n+\n+If the connector has an available `fsspec` implementation, then refer to [s3.py](unstructured/ingest/v2/processes/connectors/fsspec/s3.py).\n+\n+Make sure to update the source registry via `add_source_entry` using a unique key for the source type. This will expose it as an available connector.\n+\n+\n+Create at least one folder [examples/ingest](examples/ingest) with an easily reproducible\n+script that shows the new connector in action.\n+\n+Finally, to ensure the connector remains stable, add a new script test_unstructured_ingest/test-ingest-\\<the-new-data-source\\>.sh similar to [test_unstructured_ingest/test-ingest-s3.sh](test_unstructured_ingest/test-ingest-s3.sh), and append a line invoking the new script in [test_unstructured_ingest/test-ingest.sh](test_unstructured_ingest/test-ingest.sh).\n+\n+You'll notice that the unstructured outputs for the new documents are expected\n+to be checked into CI under test_unstructured_ingest/expected-structured-output/\\<folder-name-relevant-to-your-dataset\\>. So, you'll need to `git add` those json outputs so that `test-ingest.sh` passes in CI.\n+\n+## Adding Destination Data Connectors\n+\n+To add a source connector, refer to [local.py](unstructured/ingest/v2/processes/connectors/local.py) as an example that implements the the uploader abstract base classes with the associated configs.\n+\n+If the connector has an available `fsspec` implementation, then refer to [s3.py](unstructured/ingest/v2/processes/connectors/fsspec/s3.py).\n+\n+Make sure to update the destination registry via `add_source_entry` using a unique key for the source type. This will expose it as an available connector.\n+\n+Similar tests and examples should be added to demonstrate/validate the use of the destination connector similar to the steps laid out for a source connector.\n+\n+### The checklist:\n+\n+In checklist form, the above steps are summarized as:\n+\n+- [ ] Create a new file under [connectors/](unstructured/ingest/v2/processes/connectors/) implementing the the base classes required depending on if it's a new source or destination connector.\n+  - [ ] If the IngestDoc relies on a connection or session that could be reused, the subclass of `BaseConnectorConfig` implements a session handle to manage connections. The ConnectorConfig subclass should also inherit from `ConfigSessionHandleMixin` and the IngestDoc subclass should also inherit from `IngestDocSessionHandleMixin`. Check [here](https://github.com/Unstructured-IO/unstructured/pull/1058/files#diff-dae96d30f58cffe1b348c036d006b48bdc7e2e47fbd7c8ec1c45d63face1542d) for a detailed example.\n+  - [ ] Indexer should fetch appropriate metadata from the source that can be used to reference the doc in the pipeline and detect if there are any changes from what might already exist locally.\n+  - [ ] Add the relevant decorators from `unstructured.ingest.error` on top of relevant methods to handle errors such as a source connection error, destination connection error, or a partition error.\n+  - [ ] Register the required information via `add_source_entry` or `add_source_entry` to expose the new connectors.\n+- [ ] Update the CLI to expose the new connectors via CLI params\n+  - [ ] Add a new file under [cmds](unstructured/ingest/v2/cli/cmds)\n+  - [ ] Add the command base classes from the file above in the [__init__.py](unstructured/ingest/v2/cli/cmds/__init__.py). This will expose it in the CLI.\n+- [ ] Update [unstructured/ingest/cli](unstructured/ingest/cli) with support for the new connector.\n+- [ ] Create a folder under [examples/ingest](examples/ingest) that includes at least one well documented script.\n+- [ ] Add a script test_unstructured_ingest/[src|dest\\/test-ingest-\\<the-new-data-source\\>.sh. It's json output files should have a total of no more than 100K.\n+- [ ] Git add the expected outputs under test_unstructured_ingest/expected-structured-output/\\<folder-name-relevant-to-your-dataset\\> so the above test passes in CI.\n+- [ ] Add a line to [test_unstructured_ingest/test-ingest.sh](test_unstructured_ingest/test-ingest.sh) invoking the new test script.\n+- [ ] Make sure the tests for the connector are running and not skipped by reviewing the logs in CI.\n+- [ ] If additional python dependencies are needed for the new connector:\n+  - [ ] Add them as an extra to [setup.py](unstructured/setup.py).\n+  - [ ] Update the Makefile, adding a target for `install-ingest-<name>` and adding another `pip-compile` line to the `pip-compile` make target. See [this commit](https://github.com/Unstructured-IO/unstructured/commit/ab542ca3c6274f96b431142262d47d727f309e37) for a reference.\n+  - [ ] The added dependencies should be imported at runtime when the new connector is invoked, rather than as top-level imports.\n+  - [ ] Add the decorator `unstructured.utils.requires_dependencies` on top of each class instance or function that uses those connector-specific dependencies e.g. for `GitHubConnector` should look like `@requires_dependencies(dependencies=[\"github\"], extras=\"github\")`\n+  - [ ] Run `make tidy` and `make check` to ensure linting checks pass.\n+- [ ] Update ingest documentation [here](https://github.com/Unstructured-IO/docs)\n+- [ ] For team members that are developing in the original repository:\n+  - [ ] If there are secret variables created for the connector tests, make sure to:\n+    - [ ] add the secrets into Github (contact someone with access)\n+    - [ ] include the secret variables in [`ci.yml`](https://github.com/Unstructured-IO/unstructured/blob/main/.github/workflows/ci.yml) and [`ingest-test-fixtures-update-pr.yml`](https://github.com/Unstructured-IO/unstructured/blob/main/.github/workflows/ingest-test-fixtures-update-pr.yml)\n+    - [ ] add a make install line in the workflow configurations to be able to provide the workflow machine with the required dependencies on the connector while testing\n+    - [ ] Whenever necessary, use the [ingest update test fixtures](https://github.com/Unstructured-IO/unstructured/actions/workflows/ingest-test-fixtures-update-pr.yml) workflow to update the test fixtures.\n+\n+## Design References\n+\n+The ingest flow is similar to an ETL pipeline that gets defined at runtime based on user input:\n+\n+![unstructured ingest cli diagram](assets/pipeline.png)\n+\n+\n+\n+### Steps\n+* `Indexer`: This is responsible for reaching out to the source location and pulling in metadata for each document that will need to be downloaded and processed\n+* `Downloader`: Using the information generated from the indexer, download the content as files on the local file system for processing. This may require manipulation of the data to prep it for partitioning.\n+* `Uncompressor`: If enabled, this will look for any supported compressed files (tar and zip are currently supported) and expands those.\n+* `Partitioner`: Generated the structured enriched content from the local files that have been pulled down. Both local and api-based partitioning is supported, with api-based partitioning set to run async while local set to run via multiprocessing.\n+* `Chunker`: Optionally chunk the partitioned content. Can also be run locally or via the api, with async/multiprocessing set in the same approach as the partitioner.\n+* `Embedder`: Create vector embeddings for each element in the structured output. Most of these are via an API call (i.e. AWS Bedrock) and therefor run async but can also use a local huggingface model which will run via multiprocessing.\n+* `Stager`: This is an optional step that won't apply for most pipelines. If the data needs to be modified from the existing structure to better support the upload, such as converting it to a csv for tabular-based destinations (sql dbs).\n+* `Uploader`: Write the local content to the destination. If none if provided, the local one will be used which writes the final result to a location on the local filesystem. If batch uploads are needed, this will run in a single process with access to all docs. If batch is not supported, all docs can be uploaded concurrently using the async approach.\n+\n+\n+\n+### Parallel Execution/Concurrency",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1605330474",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3009,
        "pr_file": "unstructured/ingest/v2/README.md",
        "discussion_id": "1605330474",
        "commented_code": "@@ -0,0 +1,119 @@\n+# Batch Processing Documents\n+\n+## The unstructured-ingest CLI\n+\n+The unstructured library includes a CLI to batch ingest documents from various sources, storing structured outputs locally on the filesystem.\n+\n+For example, the following command processes all the documents in S3 in the\n+`utic-dev-tech-fixtures` bucket with a prefix of `small-pdf-set/`.\n+\n+    unstructured-ingest \\\n+       s3 \\\n+       --remote-url s3://utic-dev-tech-fixtures/small-pdf-set/ \\\n+       --anonymous \\\n+       --output-dir s3-small-batch-output \\\n+       --num-processes 2\n+\n+Naturally, --num-processes may be adjusted for better instance utilization with multiprocessing.\n+\n+Installation note: make sure to install the following extras when installing unstructured, needed for the above command:\n+\n+    pip install \"unstructured[s3,local-inference]\"\n+\n+See the [Quick Start](https://github.com/Unstructured-IO/unstructured#eight_pointed_black_star-quick-start) which documents how to pip install `dectectron2` and other OS dependencies, necessary for the parsing of .PDF files.\n+\n+# Developers' Guide\n+\n+## Local testing\n+\n+When testing from a local checkout rather than a pip-installed version of `unstructured`,\n+just execute `unstructured/ingest/main.py`, e.g.:\n+\n+    PYTHONPATH=. ./unstructured/ingest/v2/main.py \\\n+       s3 \\\n+       --remote-url s3://utic-dev-tech-fixtures/small-pdf-set/ \\\n+       --anonymous \\\n+       --output-dir s3-small-batch-output \\\n+       --num-processes 2\n+\n+## Adding Source Data Connectors\n+\n+To add a source connector, refer to [local.py](unstructured/ingest/v2/processes/connectors/local.py) as an example that implements the two relevant abstract base classes with their associated configs.\n+\n+If the connector has an available `fsspec` implementation, then refer to [s3.py](unstructured/ingest/v2/processes/connectors/fsspec/s3.py).\n+\n+Make sure to update the source registry via `add_source_entry` using a unique key for the source type. This will expose it as an available connector.\n+\n+\n+Create at least one folder [examples/ingest](examples/ingest) with an easily reproducible\n+script that shows the new connector in action.\n+\n+Finally, to ensure the connector remains stable, add a new script test_unstructured_ingest/test-ingest-\\<the-new-data-source\\>.sh similar to [test_unstructured_ingest/test-ingest-s3.sh](test_unstructured_ingest/test-ingest-s3.sh), and append a line invoking the new script in [test_unstructured_ingest/test-ingest.sh](test_unstructured_ingest/test-ingest.sh).\n+\n+You'll notice that the unstructured outputs for the new documents are expected\n+to be checked into CI under test_unstructured_ingest/expected-structured-output/\\<folder-name-relevant-to-your-dataset\\>. So, you'll need to `git add` those json outputs so that `test-ingest.sh` passes in CI.\n+\n+## Adding Destination Data Connectors\n+\n+To add a source connector, refer to [local.py](unstructured/ingest/v2/processes/connectors/local.py) as an example that implements the the uploader abstract base classes with the associated configs.\n+\n+If the connector has an available `fsspec` implementation, then refer to [s3.py](unstructured/ingest/v2/processes/connectors/fsspec/s3.py).\n+\n+Make sure to update the destination registry via `add_source_entry` using a unique key for the source type. This will expose it as an available connector.\n+\n+Similar tests and examples should be added to demonstrate/validate the use of the destination connector similar to the steps laid out for a source connector.\n+\n+### The checklist:\n+\n+In checklist form, the above steps are summarized as:\n+\n+- [ ] Create a new file under [connectors/](unstructured/ingest/v2/processes/connectors/) implementing the the base classes required depending on if it's a new source or destination connector.\n+  - [ ] If the IngestDoc relies on a connection or session that could be reused, the subclass of `BaseConnectorConfig` implements a session handle to manage connections. The ConnectorConfig subclass should also inherit from `ConfigSessionHandleMixin` and the IngestDoc subclass should also inherit from `IngestDocSessionHandleMixin`. Check [here](https://github.com/Unstructured-IO/unstructured/pull/1058/files#diff-dae96d30f58cffe1b348c036d006b48bdc7e2e47fbd7c8ec1c45d63face1542d) for a detailed example.\n+  - [ ] Indexer should fetch appropriate metadata from the source that can be used to reference the doc in the pipeline and detect if there are any changes from what might already exist locally.\n+  - [ ] Add the relevant decorators from `unstructured.ingest.error` on top of relevant methods to handle errors such as a source connection error, destination connection error, or a partition error.\n+  - [ ] Register the required information via `add_source_entry` or `add_source_entry` to expose the new connectors.\n+- [ ] Update the CLI to expose the new connectors via CLI params\n+  - [ ] Add a new file under [cmds](unstructured/ingest/v2/cli/cmds)\n+  - [ ] Add the command base classes from the file above in the [__init__.py](unstructured/ingest/v2/cli/cmds/__init__.py). This will expose it in the CLI.\n+- [ ] Update [unstructured/ingest/cli](unstructured/ingest/cli) with support for the new connector.\n+- [ ] Create a folder under [examples/ingest](examples/ingest) that includes at least one well documented script.\n+- [ ] Add a script test_unstructured_ingest/[src|dest\\/test-ingest-\\<the-new-data-source\\>.sh. It's json output files should have a total of no more than 100K.\n+- [ ] Git add the expected outputs under test_unstructured_ingest/expected-structured-output/\\<folder-name-relevant-to-your-dataset\\> so the above test passes in CI.\n+- [ ] Add a line to [test_unstructured_ingest/test-ingest.sh](test_unstructured_ingest/test-ingest.sh) invoking the new test script.\n+- [ ] Make sure the tests for the connector are running and not skipped by reviewing the logs in CI.\n+- [ ] If additional python dependencies are needed for the new connector:\n+  - [ ] Add them as an extra to [setup.py](unstructured/setup.py).\n+  - [ ] Update the Makefile, adding a target for `install-ingest-<name>` and adding another `pip-compile` line to the `pip-compile` make target. See [this commit](https://github.com/Unstructured-IO/unstructured/commit/ab542ca3c6274f96b431142262d47d727f309e37) for a reference.\n+  - [ ] The added dependencies should be imported at runtime when the new connector is invoked, rather than as top-level imports.\n+  - [ ] Add the decorator `unstructured.utils.requires_dependencies` on top of each class instance or function that uses those connector-specific dependencies e.g. for `GitHubConnector` should look like `@requires_dependencies(dependencies=[\"github\"], extras=\"github\")`\n+  - [ ] Run `make tidy` and `make check` to ensure linting checks pass.\n+- [ ] Update ingest documentation [here](https://github.com/Unstructured-IO/docs)\n+- [ ] For team members that are developing in the original repository:\n+  - [ ] If there are secret variables created for the connector tests, make sure to:\n+    - [ ] add the secrets into Github (contact someone with access)\n+    - [ ] include the secret variables in [`ci.yml`](https://github.com/Unstructured-IO/unstructured/blob/main/.github/workflows/ci.yml) and [`ingest-test-fixtures-update-pr.yml`](https://github.com/Unstructured-IO/unstructured/blob/main/.github/workflows/ingest-test-fixtures-update-pr.yml)\n+    - [ ] add a make install line in the workflow configurations to be able to provide the workflow machine with the required dependencies on the connector while testing\n+    - [ ] Whenever necessary, use the [ingest update test fixtures](https://github.com/Unstructured-IO/unstructured/actions/workflows/ingest-test-fixtures-update-pr.yml) workflow to update the test fixtures.\n+\n+## Design References\n+\n+The ingest flow is similar to an ETL pipeline that gets defined at runtime based on user input:\n+\n+![unstructured ingest cli diagram](assets/pipeline.png)\n+\n+\n+\n+### Steps\n+* `Indexer`: This is responsible for reaching out to the source location and pulling in metadata for each document that will need to be downloaded and processed\n+* `Downloader`: Using the information generated from the indexer, download the content as files on the local file system for processing. This may require manipulation of the data to prep it for partitioning.\n+* `Uncompressor`: If enabled, this will look for any supported compressed files (tar and zip are currently supported) and expands those.\n+* `Partitioner`: Generated the structured enriched content from the local files that have been pulled down. Both local and api-based partitioning is supported, with api-based partitioning set to run async while local set to run via multiprocessing.\n+* `Chunker`: Optionally chunk the partitioned content. Can also be run locally or via the api, with async/multiprocessing set in the same approach as the partitioner.\n+* `Embedder`: Create vector embeddings for each element in the structured output. Most of these are via an API call (i.e. AWS Bedrock) and therefor run async but can also use a local huggingface model which will run via multiprocessing.\n+* `Stager`: This is an optional step that won't apply for most pipelines. If the data needs to be modified from the existing structure to better support the upload, such as converting it to a csv for tabular-based destinations (sql dbs).\n+* `Uploader`: Write the local content to the destination. If none if provided, the local one will be used which writes the final result to a location on the local filesystem. If batch uploads are needed, this will run in a single process with access to all docs. If batch is not supported, all docs can be uploaded concurrently using the async approach.\n+\n+\n+\n+### Parallel Execution/Concurrency",
        "comment_created_at": "2024-05-17T17:07:44+00:00",
        "comment_author": "ryannikolaidis",
        "comment_body": "The above is a good birds eye view; I think what would be a really helpful addition below is to diagram and explain the entire journey in depth. i.e. trace granularly (like function level) incoming request -> constructing command -> building pipeline -> executing pipeline so that a developer can drop into code an understand exactly where it fits in and how to debug.",
        "pr_file_module": null
      }
    ]
  }
]