[
  {
    "discussion_id": "2208887759",
    "pr_number": 4060,
    "pr_file": "unstructured/file_utils/encoding.py",
    "created_at": "2025-07-15T23:25:49+00:00",
    "commented_code": "else:\n         raise FileNotFoundError(\"No filename nor file were specified\")\n \n-    result = chardet.detect(byte_data)\n+    result = detect(byte_data)\n     encoding = result[\"encoding\"]\n     confidence = result[\"confidence\"]\n \n-    if encoding is None or confidence < ENCODE_REC_THRESHOLD:\n+    if encoding is None or (confidence is not None and confidence < ENCODE_REC_THRESHOLD):",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "2208887759",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 4060,
        "pr_file": "unstructured/file_utils/encoding.py",
        "discussion_id": "2208887759",
        "commented_code": "@@ -70,11 +70,11 @@ def detect_file_encoding(\n     else:\n         raise FileNotFoundError(\"No filename nor file were specified\")\n \n-    result = chardet.detect(byte_data)\n+    result = detect(byte_data)\n     encoding = result[\"encoding\"]\n     confidence = result[\"confidence\"]\n \n-    if encoding is None or confidence < ENCODE_REC_THRESHOLD:\n+    if encoding is None or (confidence is not None and confidence < ENCODE_REC_THRESHOLD):",
        "comment_created_at": "2025-07-15T23:25:49+00:00",
        "comment_author": "ahmetmeleq",
        "comment_body": "```suggestion\r\n    if encoding is None or confidence is None or confidence < ENCODE_REC_THRESHOLD:\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2208888378",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 4060,
        "pr_file": "unstructured/file_utils/encoding.py",
        "discussion_id": "2208887759",
        "commented_code": "@@ -70,11 +70,11 @@ def detect_file_encoding(\n     else:\n         raise FileNotFoundError(\"No filename nor file were specified\")\n \n-    result = chardet.detect(byte_data)\n+    result = detect(byte_data)\n     encoding = result[\"encoding\"]\n     confidence = result[\"confidence\"]\n \n-    if encoding is None or confidence < ENCODE_REC_THRESHOLD:\n+    if encoding is None or (confidence is not None and confidence < ENCODE_REC_THRESHOLD):",
        "comment_created_at": "2025-07-15T23:26:31+00:00",
        "comment_author": "ahmetmeleq",
        "comment_body": "We might want to fallback when confidence is `None` too",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2209090805",
    "pr_number": 4062,
    "pr_file": "unstructured/partition/common/lang.py",
    "created_at": "2025-07-16T02:40:45+00:00",
    "commented_code": "else:\n         for e in elements:\n             if hasattr(e, \"text\"):\n-                e.metadata.languages = detect_languages(e.text)\n+                e.metadata.languages = detect_languages(e.text or \"\")",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "2209090805",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 4062,
        "pr_file": "unstructured/partition/common/lang.py",
        "discussion_id": "2209090805",
        "commented_code": "@@ -501,7 +501,7 @@ def apply_lang_metadata(\n     else:\n         for e in elements:\n             if hasattr(e, \"text\"):\n-                e.metadata.languages = detect_languages(e.text)\n+                e.metadata.languages = detect_languages(e.text or \"\")",
        "comment_created_at": "2025-07-16T02:40:45+00:00",
        "comment_author": "yuming-long",
        "comment_body": "do we need to call ` detect_languages(e.text or \"\")`  -> should be no lanugaga for empty str imo",
        "pr_file_module": null
      },
      {
        "comment_id": "2211589786",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 4062,
        "pr_file": "unstructured/partition/common/lang.py",
        "discussion_id": "2209090805",
        "commented_code": "@@ -501,7 +501,7 @@ def apply_lang_metadata(\n     else:\n         for e in elements:\n             if hasattr(e, \"text\"):\n-                e.metadata.languages = detect_languages(e.text)\n+                e.metadata.languages = detect_languages(e.text or \"\")",
        "comment_created_at": "2025-07-16T20:54:11+00:00",
        "comment_author": "shreyanid",
        "comment_body": "if e.text is not a string (ex. a numerical value) or a None value, we'll use an empty string to safely trigger the logic that already exists in the helper function to set the language field accordingly. I'll add some string casting",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2209091872",
    "pr_number": 4062,
    "pr_file": "unstructured/partition/common/lang.py",
    "created_at": "2025-07-16T02:41:53+00:00",
    "commented_code": "if not isinstance(elements, list):\n         elements = list(elements)\n \n-    full_text = \" \".join(e.text for e in elements if hasattr(e, \"text\"))\n+    full_text = \" \".join(e.text or \"\" for e in elements if hasattr(e, \"text\"))",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "2209091872",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 4062,
        "pr_file": "unstructured/partition/common/lang.py",
        "discussion_id": "2209091872",
        "commented_code": "@@ -487,7 +487,7 @@ def apply_lang_metadata(\n     if not isinstance(elements, list):\n         elements = list(elements)\n \n-    full_text = \" \".join(e.text for e in elements if hasattr(e, \"text\"))\n+    full_text = \" \".join(e.text or \"\" for e in elements if hasattr(e, \"text\"))",
        "comment_created_at": "2025-07-16T02:41:53+00:00",
        "comment_author": "yuming-long",
        "comment_body": " full_text = \" \".join(e.text  for e in elements if (hasattr(e, \"text\") and e.text))\r\n maybe this is easier to read?",
        "pr_file_module": null
      },
      {
        "comment_id": "2211596647",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 4062,
        "pr_file": "unstructured/partition/common/lang.py",
        "discussion_id": "2209091872",
        "commented_code": "@@ -487,7 +487,7 @@ def apply_lang_metadata(\n     if not isinstance(elements, list):\n         elements = list(elements)\n \n-    full_text = \" \".join(e.text for e in elements if hasattr(e, \"text\"))\n+    full_text = \" \".join(e.text or \"\" for e in elements if hasattr(e, \"text\"))",
        "comment_created_at": "2025-07-16T20:57:42+00:00",
        "comment_author": "shreyanid",
        "comment_body": "Other falsy values like 0 could get caught into the e.text condition, but for the purpose of language detection that doesn't really matter :) I can do something like this!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1920087988",
    "pr_number": 3870,
    "pr_file": "unstructured/partition/auto.py",
    "created_at": "2025-01-17T11:57:00+00:00",
    "commented_code": "if file is not None:\n         file.seek(0)\n \n-    infer_table_structure = decide_table_extraction(\n-        file_type,\n-        skip_infer_table_types,\n-        pdf_infer_table_structure,\n+    # avoid double specification of infer_table_structure; this can happen when the kwarg passed\n+    # into a partition function, e.g., partition_email is reused to partition sub-elements, e.g.,\n+    # partition an image attachment buy calling partition with the kwargs. In that case here kwargs\n+    # would have a infer_table_structure already\n+    infer_table_structure = kwargs.pop(",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1920087988",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3870,
        "pr_file": "unstructured/partition/auto.py",
        "discussion_id": "1920087988",
        "commented_code": "@@ -174,10 +174,17 @@ def partition(\n     if file is not None:\n         file.seek(0)\n \n-    infer_table_structure = decide_table_extraction(\n-        file_type,\n-        skip_infer_table_types,\n-        pdf_infer_table_structure,\n+    # avoid double specification of infer_table_structure; this can happen when the kwarg passed\n+    # into a partition function, e.g., partition_email is reused to partition sub-elements, e.g.,\n+    # partition an image attachment buy calling partition with the kwargs. In that case here kwargs\n+    # would have a infer_table_structure already\n+    infer_table_structure = kwargs.pop(",
        "comment_created_at": "2025-01-17T11:57:00+00:00",
        "comment_author": "plutasnyy",
        "comment_body": "Looking at the failing test I think we introduced a regression, and I believe this is because the below behaviour \r\n```python\r\n>>> x={'a': None, 'b': 2}\r\n>>> x\r\n{'a': None, 'b': 2}\r\n>>> a=x.pop('a', 5)\r\n>>> b=x.pop('b', 5)\r\n>>> a\r\n>>> b\r\n2\r\n```\r\n`general_partition` methods configure all parameters regardless of whether they are passed or not, so I assume it passed some Nones.\r\n\r\nI modified that and added test",
        "pr_file_module": null
      },
      {
        "comment_id": "1920186963",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3870,
        "pr_file": "unstructured/partition/auto.py",
        "discussion_id": "1920087988",
        "commented_code": "@@ -174,10 +174,17 @@ def partition(\n     if file is not None:\n         file.seek(0)\n \n-    infer_table_structure = decide_table_extraction(\n-        file_type,\n-        skip_infer_table_types,\n-        pdf_infer_table_structure,\n+    # avoid double specification of infer_table_structure; this can happen when the kwarg passed\n+    # into a partition function, e.g., partition_email is reused to partition sub-elements, e.g.,\n+    # partition an image attachment buy calling partition with the kwargs. In that case here kwargs\n+    # would have a infer_table_structure already\n+    infer_table_structure = kwargs.pop(",
        "comment_created_at": "2025-01-17T13:23:31+00:00",
        "comment_author": "plutasnyy",
        "comment_body": "It didn't change the ingest test, although maybe it is safer this way",
        "pr_file_module": null
      },
      {
        "comment_id": "1920396268",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3870,
        "pr_file": "unstructured/partition/auto.py",
        "discussion_id": "1920087988",
        "commented_code": "@@ -174,10 +174,17 @@ def partition(\n     if file is not None:\n         file.seek(0)\n \n-    infer_table_structure = decide_table_extraction(\n-        file_type,\n-        skip_infer_table_types,\n-        pdf_infer_table_structure,\n+    # avoid double specification of infer_table_structure; this can happen when the kwarg passed\n+    # into a partition function, e.g., partition_email is reused to partition sub-elements, e.g.,\n+    # partition an image attachment buy calling partition with the kwargs. In that case here kwargs\n+    # would have a infer_table_structure already\n+    infer_table_structure = kwargs.pop(",
        "comment_created_at": "2025-01-17T15:55:19+00:00",
        "comment_author": "badGarnet",
        "comment_body": "thanks for the catch",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1711779308",
    "pr_number": 3495,
    "pr_file": "unstructured/ingest/v2/processes/connectors/onedrive.py",
    "created_at": "2024-08-09T16:37:05+00:00",
    "commented_code": "server_path = file_path + \"/\" + filename\n         rel_path = server_path.replace(self.index_config.path, \"\").lstrip(\"/\")\n         date_modified_dt = (\n-            parser.parse(drive_item.last_modified_datetime)\n+            parser.parse(str(drive_item.last_modified_datetime))",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1711779308",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3495,
        "pr_file": "unstructured/ingest/v2/processes/connectors/onedrive.py",
        "discussion_id": "1711779308",
        "commented_code": "@@ -123,12 +123,12 @@ def drive_item_to_file_data(self, drive_item: \"DriveItem\") -> FileData:\n         server_path = file_path + \"/\" + filename\n         rel_path = server_path.replace(self.index_config.path, \"\").lstrip(\"/\")\n         date_modified_dt = (\n-            parser.parse(drive_item.last_modified_datetime)\n+            parser.parse(str(drive_item.last_modified_datetime))",
        "comment_created_at": "2024-08-09T16:37:05+00:00",
        "comment_author": "Coniferish",
        "comment_body": "This was raising `TypeError: Parser must be a string or character stream, not datetime` when trying to use `parser.parse(drive_item.last_modified_datetime)`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1669119459",
    "pr_number": 3361,
    "pr_file": "unstructured/nlp/tokenize.py",
    "created_at": "2024-07-08T18:50:10+00:00",
    "commented_code": "CACHE_MAX_SIZE: Final[int] = 128\n \n+NLTK_DATA_URL = \"https://utic-public-cf.s3.amazonaws.com/nltk_data.tgz\"\n+NLTK_DATA_SHA256 = \"126faf671cd255a062c436b3d0f2d311dfeefcd92ffa43f7c3ab677309404d61\"\n+\n+\n+def _raise_on_nltk_download(*args, **kwargs):\n+    raise ValueError(\"NLTK download disabled. See CVE-2024-39705\")\n+\n+\n+nltk.download = _raise_on_nltk_download\n+\n+\n+# NOTE(robinson) - mimic default dir logic from NLTK\n+# https://github.com/nltk/nltk/\n+# \tblob/8c233dc585b91c7a0c58f96a9d99244a379740d5/nltk/downloader.py#L1046\n+def get_nltk_data_dir():\n+    # Check if we are on GAE where we cannot write into filesystem.\n+    if \"APPENGINE_RUNTIME\" in os.environ:\n+        return\n+\n+    # Check if we have sufficient permissions to install in a\n+    # variety of system-wide locations.\n+    for nltkdir in nltk.data.path:\n+        if os.path.exists(nltkdir) and nltk.internals.is_writable(nltkdir):\n+            return nltkdir\n+\n+    # On Windows, use %APPDATA%\n+    if sys.platform == \"win32\" and \"APPDATA\" in os.environ:\n+        homedir = os.environ[\"APPDATA\"]\n+\n+    # Otherwise, install in the user's home directory.\n+    else:\n+        homedir = os.path.expanduser(\"~/\")\n+        if homedir == \"~/\":\n+            raise ValueError(\"Could not find a default download directory\")\n+\n+    # NOTE(robinson) - NLTK appends nltk_data to the homedir. That's already\n+    # present in the tar file so we don't have to do that here.\n+    return homedir\n+\n+\n+def download_nltk_packages():\n+    nltk_data_dir = get_nltk_data_dir()\n+\n+    def sha256_checksum(filename, block_size=65536):\n+        sha256 = hashlib.sha256()\n+        with open(filename, \"rb\") as f:\n+            for block in iter(lambda: f.read(block_size), b\"\"):\n+                sha256.update(block)\n+        return sha256.hexdigest()\n+\n+    with tempfile.NamedTemporaryFile() as tmp_file:\n+        tgz_file = tmp_file.name\n+        urllib.request.urlretrieve(NLTK_DATA_URL, tgz_file)\n+\n+        file_hash = sha256_checksum(tgz_file)\n+        if file_hash != NLTK_DATA_SHA256:\n+            os.remove(tgz_file)\n+            raise ValueError(f\"SHA-256 mismatch: expected {NLTK_DATA_SHA256}, got {file_hash}\")\n+\n+        # Extract the contents\n+        if not os.path.exists(nltk_data_dir):\n+            os.makedirs(nltk_data_dir)\n+\n+        with tarfile.open(tgz_file, \"r:gz\") as tar:\n+            tar.extractall(path=nltk_data_dir)",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1669119459",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3361,
        "pr_file": "unstructured/nlp/tokenize.py",
        "discussion_id": "1669119459",
        "commented_code": "@@ -14,37 +19,121 @@\n \n CACHE_MAX_SIZE: Final[int] = 128\n \n+NLTK_DATA_URL = \"https://utic-public-cf.s3.amazonaws.com/nltk_data.tgz\"\n+NLTK_DATA_SHA256 = \"126faf671cd255a062c436b3d0f2d311dfeefcd92ffa43f7c3ab677309404d61\"\n+\n+\n+def _raise_on_nltk_download(*args, **kwargs):\n+    raise ValueError(\"NLTK download disabled. See CVE-2024-39705\")\n+\n+\n+nltk.download = _raise_on_nltk_download\n+\n+\n+# NOTE(robinson) - mimic default dir logic from NLTK\n+# https://github.com/nltk/nltk/\n+# \tblob/8c233dc585b91c7a0c58f96a9d99244a379740d5/nltk/downloader.py#L1046\n+def get_nltk_data_dir():\n+    # Check if we are on GAE where we cannot write into filesystem.\n+    if \"APPENGINE_RUNTIME\" in os.environ:\n+        return\n+\n+    # Check if we have sufficient permissions to install in a\n+    # variety of system-wide locations.\n+    for nltkdir in nltk.data.path:\n+        if os.path.exists(nltkdir) and nltk.internals.is_writable(nltkdir):\n+            return nltkdir\n+\n+    # On Windows, use %APPDATA%\n+    if sys.platform == \"win32\" and \"APPDATA\" in os.environ:\n+        homedir = os.environ[\"APPDATA\"]\n+\n+    # Otherwise, install in the user's home directory.\n+    else:\n+        homedir = os.path.expanduser(\"~/\")\n+        if homedir == \"~/\":\n+            raise ValueError(\"Could not find a default download directory\")\n+\n+    # NOTE(robinson) - NLTK appends nltk_data to the homedir. That's already\n+    # present in the tar file so we don't have to do that here.\n+    return homedir\n+\n+\n+def download_nltk_packages():\n+    nltk_data_dir = get_nltk_data_dir()\n+\n+    def sha256_checksum(filename, block_size=65536):\n+        sha256 = hashlib.sha256()\n+        with open(filename, \"rb\") as f:\n+            for block in iter(lambda: f.read(block_size), b\"\"):\n+                sha256.update(block)\n+        return sha256.hexdigest()\n+\n+    with tempfile.NamedTemporaryFile() as tmp_file:\n+        tgz_file = tmp_file.name\n+        urllib.request.urlretrieve(NLTK_DATA_URL, tgz_file)\n+\n+        file_hash = sha256_checksum(tgz_file)\n+        if file_hash != NLTK_DATA_SHA256:\n+            os.remove(tgz_file)\n+            raise ValueError(f\"SHA-256 mismatch: expected {NLTK_DATA_SHA256}, got {file_hash}\")\n+\n+        # Extract the contents\n+        if not os.path.exists(nltk_data_dir):\n+            os.makedirs(nltk_data_dir)\n+\n+        with tarfile.open(tgz_file, \"r:gz\") as tar:\n+            tar.extractall(path=nltk_data_dir)",
        "comment_created_at": "2024-07-08T18:50:10+00:00",
        "comment_author": "scanny",
        "comment_body": "A TypeError will be raised here (L82) whenever `get_nltk_data_dir()` returns `None`, such as when running on GAE.\r\n\r\nNeed a code path for when `nltk_data_dir` is `None`.\r\n\r\nSee scratch PR #3362 for changes that allowed the type-checker to reveal this, basically cleaning up typing in this module.",
        "pr_file_module": null
      },
      {
        "comment_id": "1669167211",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3361,
        "pr_file": "unstructured/nlp/tokenize.py",
        "discussion_id": "1669119459",
        "commented_code": "@@ -14,37 +19,121 @@\n \n CACHE_MAX_SIZE: Final[int] = 128\n \n+NLTK_DATA_URL = \"https://utic-public-cf.s3.amazonaws.com/nltk_data.tgz\"\n+NLTK_DATA_SHA256 = \"126faf671cd255a062c436b3d0f2d311dfeefcd92ffa43f7c3ab677309404d61\"\n+\n+\n+def _raise_on_nltk_download(*args, **kwargs):\n+    raise ValueError(\"NLTK download disabled. See CVE-2024-39705\")\n+\n+\n+nltk.download = _raise_on_nltk_download\n+\n+\n+# NOTE(robinson) - mimic default dir logic from NLTK\n+# https://github.com/nltk/nltk/\n+# \tblob/8c233dc585b91c7a0c58f96a9d99244a379740d5/nltk/downloader.py#L1046\n+def get_nltk_data_dir():\n+    # Check if we are on GAE where we cannot write into filesystem.\n+    if \"APPENGINE_RUNTIME\" in os.environ:\n+        return\n+\n+    # Check if we have sufficient permissions to install in a\n+    # variety of system-wide locations.\n+    for nltkdir in nltk.data.path:\n+        if os.path.exists(nltkdir) and nltk.internals.is_writable(nltkdir):\n+            return nltkdir\n+\n+    # On Windows, use %APPDATA%\n+    if sys.platform == \"win32\" and \"APPDATA\" in os.environ:\n+        homedir = os.environ[\"APPDATA\"]\n+\n+    # Otherwise, install in the user's home directory.\n+    else:\n+        homedir = os.path.expanduser(\"~/\")\n+        if homedir == \"~/\":\n+            raise ValueError(\"Could not find a default download directory\")\n+\n+    # NOTE(robinson) - NLTK appends nltk_data to the homedir. That's already\n+    # present in the tar file so we don't have to do that here.\n+    return homedir\n+\n+\n+def download_nltk_packages():\n+    nltk_data_dir = get_nltk_data_dir()\n+\n+    def sha256_checksum(filename, block_size=65536):\n+        sha256 = hashlib.sha256()\n+        with open(filename, \"rb\") as f:\n+            for block in iter(lambda: f.read(block_size), b\"\"):\n+                sha256.update(block)\n+        return sha256.hexdigest()\n+\n+    with tempfile.NamedTemporaryFile() as tmp_file:\n+        tgz_file = tmp_file.name\n+        urllib.request.urlretrieve(NLTK_DATA_URL, tgz_file)\n+\n+        file_hash = sha256_checksum(tgz_file)\n+        if file_hash != NLTK_DATA_SHA256:\n+            os.remove(tgz_file)\n+            raise ValueError(f\"SHA-256 mismatch: expected {NLTK_DATA_SHA256}, got {file_hash}\")\n+\n+        # Extract the contents\n+        if not os.path.exists(nltk_data_dir):\n+            os.makedirs(nltk_data_dir)\n+\n+        with tarfile.open(tgz_file, \"r:gz\") as tar:\n+            tar.extractall(path=nltk_data_dir)",
        "comment_created_at": "2024-07-08T19:21:47+00:00",
        "comment_author": "MthwRobinson",
        "comment_body": "Thanks, added.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1655403465",
    "pr_number": 3306,
    "pr_file": "unstructured/partition/docx.py",
    "created_at": "2024-06-26T19:16:36+00:00",
    "commented_code": "self._file.seek(0)\n             return io.BytesIO(self._file.read())\n \n-        if self._file:\n-            return self._file\n+        assert self._file is not None  # -- assured by `._validate()` --",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1655403465",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3306,
        "pr_file": "unstructured/partition/docx.py",
        "discussion_id": "1655403465",
        "commented_code": "@@ -358,12 +365,26 @@ def _docx_file(self) -> str | IO[bytes]:\n             self._file.seek(0)\n             return io.BytesIO(self._file.read())\n \n-        if self._file:\n-            return self._file\n+        assert self._file is not None  # -- assured by `._validate()` --",
        "comment_created_at": "2024-06-26T19:16:36+00:00",
        "comment_author": "MthwRobinson",
        "comment_body": "Would we want a more informative error that the assertion error here? ",
        "pr_file_module": null
      },
      {
        "comment_id": "1655404500",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3306,
        "pr_file": "unstructured/partition/docx.py",
        "discussion_id": "1655403465",
        "commented_code": "@@ -358,12 +365,26 @@ def _docx_file(self) -> str | IO[bytes]:\n             self._file.seek(0)\n             return io.BytesIO(self._file.read())\n \n-        if self._file:\n-            return self._file\n+        assert self._file is not None  # -- assured by `._validate()` --",
        "comment_created_at": "2024-06-26T19:17:39+00:00",
        "comment_author": "MthwRobinson",
        "comment_body": "I think we're okay here based on `_validate`",
        "pr_file_module": null
      },
      {
        "comment_id": "1655417854",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3306,
        "pr_file": "unstructured/partition/docx.py",
        "discussion_id": "1655403465",
        "commented_code": "@@ -358,12 +365,26 @@ def _docx_file(self) -> str | IO[bytes]:\n             self._file.seek(0)\n             return io.BytesIO(self._file.read())\n \n-        if self._file:\n-            return self._file\n+        assert self._file is not None  # -- assured by `._validate()` --",
        "comment_created_at": "2024-06-26T19:31:13+00:00",
        "comment_author": "scanny",
        "comment_body": "We should never get an error here (outside of tests maybe). This assert is to indicate to the type-checker we have other ways of knowing this value is not None here, even though type-wise it is `IO[bytes] | None`.\r\n\r\nUsed as expected, a new `opts` object is constructed using `opts = DocxPartitionerOptions.load(...)`, which runs `._validate()` which would raise the appropriate detailed exception before we ever got here.\r\n\r\nThis is what the `# -- assured by ._validate() --` comment is getting at.\r\n\r\n`partition_docx()` is the only code that constructes a new `DocxPartitionerOptions` object outside of tests and it uses `.load()` to do it (as should any other callers who do so in future), so that's why that assert would never fail during operation. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1577204381",
    "pr_number": 2673,
    "pr_file": "unstructured/documents/elements.py",
    "created_at": "2024-04-24T03:36:56+00:00",
    "commented_code": "return new_coordinates\n \n-    @property\n-    def id(self):\n-        if self._element_id is None:\n-            self._element_id = str(uuid.uuid4())\n-        return self._element_id\n-\n-    def id_to_hash(self) -> str:\n+    def id_to_hash(self, sequence_number: int) -> str:\n         \"\"\"Calculates and assigns a deterministic hash as an ID.\n \n-        The hash ID is based on element's text.\n+        The hash ID is based on element's text, sequence number on page,\n+        page number and its filename.\n \n-        Returns:\n-            The first 32 characters of the SHA256 hash of the concatenated input parameters.\n+        Args:\n+            sequence_number: index on page\n+\n+        Returns: new ID value\n         \"\"\"\n-        self._element_id = hashlib.sha256(self.text.encode()).hexdigest()[:32]\n+        data = f\"{self.metadata.filename}{self.text}{self.metadata.page_number}{sequence_number}\"",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1577204381",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2673,
        "pr_file": "unstructured/documents/elements.py",
        "discussion_id": "1577204381",
        "commented_code": "@@ -737,23 +749,27 @@ def convert_coordinates_to_new_system(\n \n         return new_coordinates\n \n-    @property\n-    def id(self):\n-        if self._element_id is None:\n-            self._element_id = str(uuid.uuid4())\n-        return self._element_id\n-\n-    def id_to_hash(self) -> str:\n+    def id_to_hash(self, sequence_number: int) -> str:\n         \"\"\"Calculates and assigns a deterministic hash as an ID.\n \n-        The hash ID is based on element's text.\n+        The hash ID is based on element's text, sequence number on page,\n+        page number and its filename.\n \n-        Returns:\n-            The first 32 characters of the SHA256 hash of the concatenated input parameters.\n+        Args:\n+            sequence_number: index on page\n+\n+        Returns: new ID value\n         \"\"\"\n-        self._element_id = hashlib.sha256(self.text.encode()).hexdigest()[:32]\n+        data = f\"{self.metadata.filename}{self.text}{self.metadata.page_number}{sequence_number}\"",
        "comment_created_at": "2024-04-24T03:36:56+00:00",
        "comment_author": "scanny",
        "comment_body": "I think we're going to need some similar solution here, aren't we? `self.metadata.page_number` can be `None` still, right? And I think we said it should be 1 in that case, but I think it actually needs to be a little more sophisticated to deal with the \"occasional None\" case; otherwise page_number could go ... 6, 6, 1, 6, 7, 8, 1, ...\r\n\r\nI'm inclined to think this method needs to take `page_number` as well as `sequence_number` (which should probably be named `page_sequence_number` btw). The nested function proposed above could just be elaborated to `yield e, current_page_number, sequence_number` and maybe be named a little differently since it's generating triples now instead of pairs.\r\n\r\nUnless we've somehow established that all elements always have `.metadata.page_number` (and I don't think we have).",
        "pr_file_module": null
      },
      {
        "comment_id": "1577261514",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2673,
        "pr_file": "unstructured/documents/elements.py",
        "discussion_id": "1577204381",
        "commented_code": "@@ -737,23 +749,27 @@ def convert_coordinates_to_new_system(\n \n         return new_coordinates\n \n-    @property\n-    def id(self):\n-        if self._element_id is None:\n-            self._element_id = str(uuid.uuid4())\n-        return self._element_id\n-\n-    def id_to_hash(self) -> str:\n+    def id_to_hash(self, sequence_number: int) -> str:\n         \"\"\"Calculates and assigns a deterministic hash as an ID.\n \n-        The hash ID is based on element's text.\n+        The hash ID is based on element's text, sequence number on page,\n+        page number and its filename.\n \n-        Returns:\n-            The first 32 characters of the SHA256 hash of the concatenated input parameters.\n+        Args:\n+            sequence_number: index on page\n+\n+        Returns: new ID value\n         \"\"\"\n-        self._element_id = hashlib.sha256(self.text.encode()).hexdigest()[:32]\n+        data = f\"{self.metadata.filename}{self.text}{self.metadata.page_number}{sequence_number}\"",
        "comment_created_at": "2024-04-24T05:09:01+00:00",
        "comment_author": "cragwolfe",
        "comment_body": "are you sure we need to solve this?\r\nwe should not be writing `self.metadata.page_number` for this PR, only using it for hash calculations? and if it is None, just assume 1?\r\nEDIT: It shouldn't matter for the purpose of the hash.",
        "pr_file_module": null
      },
      {
        "comment_id": "1578370185",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2673,
        "pr_file": "unstructured/documents/elements.py",
        "discussion_id": "1577204381",
        "commented_code": "@@ -737,23 +749,27 @@ def convert_coordinates_to_new_system(\n \n         return new_coordinates\n \n-    @property\n-    def id(self):\n-        if self._element_id is None:\n-            self._element_id = str(uuid.uuid4())\n-        return self._element_id\n-\n-    def id_to_hash(self) -> str:\n+    def id_to_hash(self, sequence_number: int) -> str:\n         \"\"\"Calculates and assigns a deterministic hash as an ID.\n \n-        The hash ID is based on element's text.\n+        The hash ID is based on element's text, sequence number on page,\n+        page number and its filename.\n \n-        Returns:\n-            The first 32 characters of the SHA256 hash of the concatenated input parameters.\n+        Args:\n+            sequence_number: index on page\n+\n+        Returns: new ID value\n         \"\"\"\n-        self._element_id = hashlib.sha256(self.text.encode()).hexdigest()[:32]\n+        data = f\"{self.metadata.filename}{self.text}{self.metadata.page_number}{sequence_number}\"",
        "comment_created_at": "2024-04-24T18:55:55+00:00",
        "comment_author": "scanny",
        "comment_body": "@Crag: A couple things:\r\n- This will be using \"None\" for the page-number, not \"1\".\r\n- Therefore the \"document-unique\" promise is not guaranteed.\r\n\r\nI had understood that the \"document-unique\" guarantee was a hard commit. But if we're satisfied with \"usually mostly document-unique\" then I suppose this will get it done :)",
        "pr_file_module": null
      },
      {
        "comment_id": "1578484675",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2673,
        "pr_file": "unstructured/documents/elements.py",
        "discussion_id": "1577204381",
        "commented_code": "@@ -737,23 +749,27 @@ def convert_coordinates_to_new_system(\n \n         return new_coordinates\n \n-    @property\n-    def id(self):\n-        if self._element_id is None:\n-            self._element_id = str(uuid.uuid4())\n-        return self._element_id\n-\n-    def id_to_hash(self) -> str:\n+    def id_to_hash(self, sequence_number: int) -> str:\n         \"\"\"Calculates and assigns a deterministic hash as an ID.\n \n-        The hash ID is based on element's text.\n+        The hash ID is based on element's text, sequence number on page,\n+        page number and its filename.\n \n-        Returns:\n-            The first 32 characters of the SHA256 hash of the concatenated input parameters.\n+        Args:\n+            sequence_number: index on page\n+\n+        Returns: new ID value\n         \"\"\"\n-        self._element_id = hashlib.sha256(self.text.encode()).hexdigest()[:32]\n+        data = f\"{self.metadata.filename}{self.text}{self.metadata.page_number}{sequence_number}\"",
        "comment_created_at": "2024-04-24T20:27:34+00:00",
        "comment_author": "cragwolfe",
        "comment_body": "if there are None page_number elements in a PDF, that sounds like a bug.\r\nthe biggest concern is PDF's. what case are you worried about?",
        "pr_file_module": null
      },
      {
        "comment_id": "1578614675",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2673,
        "pr_file": "unstructured/documents/elements.py",
        "discussion_id": "1577204381",
        "commented_code": "@@ -737,23 +749,27 @@ def convert_coordinates_to_new_system(\n \n         return new_coordinates\n \n-    @property\n-    def id(self):\n-        if self._element_id is None:\n-            self._element_id = str(uuid.uuid4())\n-        return self._element_id\n-\n-    def id_to_hash(self) -> str:\n+    def id_to_hash(self, sequence_number: int) -> str:\n         \"\"\"Calculates and assigns a deterministic hash as an ID.\n \n-        The hash ID is based on element's text.\n+        The hash ID is based on element's text, sequence number on page,\n+        page number and its filename.\n \n-        Returns:\n-            The first 32 characters of the SHA256 hash of the concatenated input parameters.\n+        Args:\n+            sequence_number: index on page\n+\n+        Returns: new ID value\n         \"\"\"\n-        self._element_id = hashlib.sha256(self.text.encode()).hexdigest()[:32]\n+        data = f\"{self.metadata.filename}{self.text}{self.metadata.page_number}{sequence_number}\"",
        "comment_created_at": "2024-04-24T23:05:52+00:00",
        "comment_author": "scanny",
        "comment_body": "This code will be executed for all file-types, not just PDF, TIFF, and PPTX that \"guarantee\" page-numbers.",
        "pr_file_module": null
      },
      {
        "comment_id": "1578616821",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2673,
        "pr_file": "unstructured/documents/elements.py",
        "discussion_id": "1577204381",
        "commented_code": "@@ -737,23 +749,27 @@ def convert_coordinates_to_new_system(\n \n         return new_coordinates\n \n-    @property\n-    def id(self):\n-        if self._element_id is None:\n-            self._element_id = str(uuid.uuid4())\n-        return self._element_id\n-\n-    def id_to_hash(self) -> str:\n+    def id_to_hash(self, sequence_number: int) -> str:\n         \"\"\"Calculates and assigns a deterministic hash as an ID.\n \n-        The hash ID is based on element's text.\n+        The hash ID is based on element's text, sequence number on page,\n+        page number and its filename.\n \n-        Returns:\n-            The first 32 characters of the SHA256 hash of the concatenated input parameters.\n+        Args:\n+            sequence_number: index on page\n+\n+        Returns: new ID value\n         \"\"\"\n-        self._element_id = hashlib.sha256(self.text.encode()).hexdigest()[:32]\n+        data = f\"{self.metadata.filename}{self.text}{self.metadata.page_number}{sequence_number}\"",
        "comment_created_at": "2024-04-24T23:09:17+00:00",
        "comment_author": "scanny",
        "comment_body": "So there are two problematic cases across file-types:\r\n- No page numbers\r\n- Occasional omitted page numbers.\r\n\r\nNo-elements-have-page-numbers happens in formats like TXT and I'm sure several others. Occasional-omitted-page-numbers are like `PageBreak` and perhaps `PageNumber` elements are not assigned page-number metadata.\r\n\r\nThese are the cases that the [proposed generator above](https://github.com/Unstructured-IO/unstructured/pull/2673#discussion_r1577185967) addresses. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1578934480",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2673,
        "pr_file": "unstructured/documents/elements.py",
        "discussion_id": "1577204381",
        "commented_code": "@@ -737,23 +749,27 @@ def convert_coordinates_to_new_system(\n \n         return new_coordinates\n \n-    @property\n-    def id(self):\n-        if self._element_id is None:\n-            self._element_id = str(uuid.uuid4())\n-        return self._element_id\n-\n-    def id_to_hash(self) -> str:\n+    def id_to_hash(self, sequence_number: int) -> str:\n         \"\"\"Calculates and assigns a deterministic hash as an ID.\n \n-        The hash ID is based on element's text.\n+        The hash ID is based on element's text, sequence number on page,\n+        page number and its filename.\n \n-        Returns:\n-            The first 32 characters of the SHA256 hash of the concatenated input parameters.\n+        Args:\n+            sequence_number: index on page\n+\n+        Returns: new ID value\n         \"\"\"\n-        self._element_id = hashlib.sha256(self.text.encode()).hexdigest()[:32]\n+        data = f\"{self.metadata.filename}{self.text}{self.metadata.page_number}{sequence_number}\"",
        "comment_created_at": "2024-04-25T06:25:56+00:00",
        "comment_author": "cragwolfe",
        "comment_body": "> No-elements-have-page-numbers happens in formats like TXT\r\n\r\nbut then each element should have a different sequence number, as if it was one giant page\r\n\r\n> Occasional-omitted-page-numbers are like PageBreak\r\n\r\nif a PageBreak with no page number appears in a PDF or PPTX (or other \"real page numbers\" doc), imo that is a bug. the page number should be the page it is marking the end of.",
        "pr_file_module": null
      },
      {
        "comment_id": "1579827588",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2673,
        "pr_file": "unstructured/documents/elements.py",
        "discussion_id": "1577204381",
        "commented_code": "@@ -737,23 +749,27 @@ def convert_coordinates_to_new_system(\n \n         return new_coordinates\n \n-    @property\n-    def id(self):\n-        if self._element_id is None:\n-            self._element_id = str(uuid.uuid4())\n-        return self._element_id\n-\n-    def id_to_hash(self) -> str:\n+    def id_to_hash(self, sequence_number: int) -> str:\n         \"\"\"Calculates and assigns a deterministic hash as an ID.\n \n-        The hash ID is based on element's text.\n+        The hash ID is based on element's text, sequence number on page,\n+        page number and its filename.\n \n-        Returns:\n-            The first 32 characters of the SHA256 hash of the concatenated input parameters.\n+        Args:\n+            sequence_number: index on page\n+\n+        Returns: new ID value\n         \"\"\"\n-        self._element_id = hashlib.sha256(self.text.encode()).hexdigest()[:32]\n+        data = f\"{self.metadata.filename}{self.text}{self.metadata.page_number}{sequence_number}\"",
        "comment_created_at": "2024-04-25T16:50:39+00:00",
        "comment_author": "scanny",
        "comment_body": ">> No-elements-have-page-numbers happens in formats like TXT\r\n\r\n> but then each element should have a different sequence number, as if it was one giant page\r\n\r\nHmm, yes, good point. I suppose \"None\" is as good as \"1\" in that case, as long as we're consistent. I agree, that handles that case well enough since we aren't page-splitting those formats.\r\n\r\n>> Occasional-omitted-page-numbers are like PageBreak\r\n\r\n> if a PageBreak with no page number appears in a PDF or PPTX (or other \"real page numbers\" doc), imo that is a bug.\r\n\r\nOkay, that's fair enough. We can check those formats and make sure they assign at least page-number metadata to those elements.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1571760364",
    "pr_number": 2905,
    "pr_file": "test_unstructured_ingest/unit/pipeline/reformat/test_chunking.py",
    "created_at": "2024-04-19T04:15:21+00:00",
    "commented_code": "+from __future__ import annotations\n+\n+import os\n+import tempfile\n+\n+import pytest\n+from _pytest.logging import LogCaptureFixture\n+\n+from test_unstructured.unit_utils import (\n+    FixtureRequest,\n+    Mock,\n+    example_doc_path,\n+    function_mock,\n+    method_mock,\n+)\n+from unstructured.documents.elements import CompositeElement\n+from unstructured.ingest.interfaces import ChunkingConfig, PartitionConfig\n+from unstructured.ingest.pipeline.interfaces import PipelineContext\n+from unstructured.ingest.pipeline.reformat.chunking import Chunker\n+\n+ELEMENTS_JSON_FILE = example_doc_path(\n+    \"test_evaluate_files/unstructured_output/Bank Good Credit Loan.pptx.json\"\n+)\n+\n+\n+class DescribeChunker:\n+    \"\"\"Unit tests for ingest.pipeline.reformat.chunking.Chunker\"\"\"\n+\n+    # -- Chunker.run() -----------------------------------------------------------------------------\n+\n+    # -- integration test --\n+    def it_creates_json(self, _ingest_docs_map_: Mock):\n+        chunking_config = ChunkingConfig(chunking_strategy=\"by_title\")\n+        pipeline_context = PipelineContext()\n+        partition_config = PartitionConfig()\n+        chunker = Chunker(\n+            chunking_config=chunking_config,\n+            pipeline_context=pipeline_context,\n+            partition_config=partition_config,\n+        )\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            # -- `Chunker.chunk()` defaults to writing to \"{work_dir}/chunked\", which is located in\n+            # -- \"/.cache\" of a user's profile.\n+            # -- Define `work_dir` add the \"/chunked\" subdirectory to it:\n+            chunker.pipeline_context.work_dir = tmpdir\n+            os.makedirs(os.path.join(tmpdir, \"chunked\"), exist_ok=True)\n+\n+            filename = chunker.run(ELEMENTS_JSON_FILE)\n+            head, tail = os.path.split(filename if filename else \"\")",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1571760364",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2905,
        "pr_file": "test_unstructured_ingest/unit/pipeline/reformat/test_chunking.py",
        "discussion_id": "1571760364",
        "commented_code": "@@ -0,0 +1,144 @@\n+from __future__ import annotations\n+\n+import os\n+import tempfile\n+\n+import pytest\n+from _pytest.logging import LogCaptureFixture\n+\n+from test_unstructured.unit_utils import (\n+    FixtureRequest,\n+    Mock,\n+    example_doc_path,\n+    function_mock,\n+    method_mock,\n+)\n+from unstructured.documents.elements import CompositeElement\n+from unstructured.ingest.interfaces import ChunkingConfig, PartitionConfig\n+from unstructured.ingest.pipeline.interfaces import PipelineContext\n+from unstructured.ingest.pipeline.reformat.chunking import Chunker\n+\n+ELEMENTS_JSON_FILE = example_doc_path(\n+    \"test_evaluate_files/unstructured_output/Bank Good Credit Loan.pptx.json\"\n+)\n+\n+\n+class DescribeChunker:\n+    \"\"\"Unit tests for ingest.pipeline.reformat.chunking.Chunker\"\"\"\n+\n+    # -- Chunker.run() -----------------------------------------------------------------------------\n+\n+    # -- integration test --\n+    def it_creates_json(self, _ingest_docs_map_: Mock):\n+        chunking_config = ChunkingConfig(chunking_strategy=\"by_title\")\n+        pipeline_context = PipelineContext()\n+        partition_config = PartitionConfig()\n+        chunker = Chunker(\n+            chunking_config=chunking_config,\n+            pipeline_context=pipeline_context,\n+            partition_config=partition_config,\n+        )\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            # -- `Chunker.chunk()` defaults to writing to \"{work_dir}/chunked\", which is located in\n+            # -- \"/.cache\" of a user's profile.\n+            # -- Define `work_dir` add the \"/chunked\" subdirectory to it:\n+            chunker.pipeline_context.work_dir = tmpdir\n+            os.makedirs(os.path.join(tmpdir, \"chunked\"), exist_ok=True)\n+\n+            filename = chunker.run(ELEMENTS_JSON_FILE)\n+            head, tail = os.path.split(filename if filename else \"\")",
        "comment_created_at": "2024-04-19T04:15:21+00:00",
        "comment_author": "scanny",
        "comment_body": "Under what circumstances would `filename` be `None`? Seems like worth a separate assert, like:\r\n```python\r\nassert filename is not None, \"message explaining what this means and why it's a failure\"\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1572503846",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2905,
        "pr_file": "test_unstructured_ingest/unit/pipeline/reformat/test_chunking.py",
        "discussion_id": "1571760364",
        "commented_code": "@@ -0,0 +1,144 @@\n+from __future__ import annotations\n+\n+import os\n+import tempfile\n+\n+import pytest\n+from _pytest.logging import LogCaptureFixture\n+\n+from test_unstructured.unit_utils import (\n+    FixtureRequest,\n+    Mock,\n+    example_doc_path,\n+    function_mock,\n+    method_mock,\n+)\n+from unstructured.documents.elements import CompositeElement\n+from unstructured.ingest.interfaces import ChunkingConfig, PartitionConfig\n+from unstructured.ingest.pipeline.interfaces import PipelineContext\n+from unstructured.ingest.pipeline.reformat.chunking import Chunker\n+\n+ELEMENTS_JSON_FILE = example_doc_path(\n+    \"test_evaluate_files/unstructured_output/Bank Good Credit Loan.pptx.json\"\n+)\n+\n+\n+class DescribeChunker:\n+    \"\"\"Unit tests for ingest.pipeline.reformat.chunking.Chunker\"\"\"\n+\n+    # -- Chunker.run() -----------------------------------------------------------------------------\n+\n+    # -- integration test --\n+    def it_creates_json(self, _ingest_docs_map_: Mock):\n+        chunking_config = ChunkingConfig(chunking_strategy=\"by_title\")\n+        pipeline_context = PipelineContext()\n+        partition_config = PartitionConfig()\n+        chunker = Chunker(\n+            chunking_config=chunking_config,\n+            pipeline_context=pipeline_context,\n+            partition_config=partition_config,\n+        )\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            # -- `Chunker.chunk()` defaults to writing to \"{work_dir}/chunked\", which is located in\n+            # -- \"/.cache\" of a user's profile.\n+            # -- Define `work_dir` add the \"/chunked\" subdirectory to it:\n+            chunker.pipeline_context.work_dir = tmpdir\n+            os.makedirs(os.path.join(tmpdir, \"chunked\"), exist_ok=True)\n+\n+            filename = chunker.run(ELEMENTS_JSON_FILE)\n+            head, tail = os.path.split(filename if filename else \"\")",
        "comment_created_at": "2024-04-19T14:55:40+00:00",
        "comment_author": "Coniferish",
        "comment_body": "`None` is returned by `Chunker.run()` if that is somehow called without defining `chunking_strategy`. A message is logged, so I'll add a test for that.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1446282065",
    "pr_number": 2357,
    "pr_file": "unstructured/ingest/connector/vectara.py",
    "created_at": "2024-01-09T15:58:15+00:00",
    "commented_code": "+import datetime\n+import json\n+import traceback\n+import typing as t\n+import uuid\n+from dataclasses import dataclass\n+\n+import requests\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import DestinationConnectionError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    BaseIngestDoc,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.staging.base import flatten_dict\n+\n+\n+@dataclass\n+class VectaraAccessConfig(AccessConfig):\n+    oauth_client_id: str = enhanced_field(sensitive=True)\n+    oauth_secret: str = enhanced_field(sensitive=True)\n+\n+\n+@dataclass\n+class SimpleVectaraConfig(BaseConnectorConfig):\n+    access_config: VectaraAccessConfig\n+    customer_id: t.AnyStr = None",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1446282065",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2357,
        "pr_file": "unstructured/ingest/connector/vectara.py",
        "discussion_id": "1446282065",
        "commented_code": "@@ -0,0 +1,246 @@\n+import datetime\n+import json\n+import traceback\n+import typing as t\n+import uuid\n+from dataclasses import dataclass\n+\n+import requests\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import DestinationConnectionError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    BaseIngestDoc,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.staging.base import flatten_dict\n+\n+\n+@dataclass\n+class VectaraAccessConfig(AccessConfig):\n+    oauth_client_id: str = enhanced_field(sensitive=True)\n+    oauth_secret: str = enhanced_field(sensitive=True)\n+\n+\n+@dataclass\n+class SimpleVectaraConfig(BaseConnectorConfig):\n+    access_config: VectaraAccessConfig\n+    customer_id: t.AnyStr = None",
        "comment_created_at": "2024-01-09T15:58:15+00:00",
        "comment_author": "rbiseck3",
        "comment_body": "If we're setting this to a default of `None`, the typing should be `t.Optional[]`. But should this be optional? Or are these required? Also why are we supporting `bytes`? Which is part of the `AnyStr` constraint. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1446761704",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2357,
        "pr_file": "unstructured/ingest/connector/vectara.py",
        "discussion_id": "1446282065",
        "commented_code": "@@ -0,0 +1,246 @@\n+import datetime\n+import json\n+import traceback\n+import typing as t\n+import uuid\n+from dataclasses import dataclass\n+\n+import requests\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import DestinationConnectionError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    BaseIngestDoc,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.staging.base import flatten_dict\n+\n+\n+@dataclass\n+class VectaraAccessConfig(AccessConfig):\n+    oauth_client_id: str = enhanced_field(sensitive=True)\n+    oauth_secret: str = enhanced_field(sensitive=True)\n+\n+\n+@dataclass\n+class SimpleVectaraConfig(BaseConnectorConfig):\n+    access_config: VectaraAccessConfig\n+    customer_id: t.AnyStr = None",
        "comment_created_at": "2024-01-10T00:30:15+00:00",
        "comment_author": "potter-potter",
        "comment_body": "customer_id should not be optional. corpus_name and corpus_id should be. I thought AnyStr was interesting. I saw that it included bytes. But I think we just need `str`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1466689489",
    "pr_number": 2435,
    "pr_file": "unstructured/partition/lang.py",
    "created_at": "2024-01-25T17:11:13+00:00",
    "commented_code": "return TESSERACT_LANGUAGES_SPLITTER.join(converted_languages)\n \n \n-def check_languages(languages: Optional[List[str]], ocr_languages: Optional[str]):\n-    \"\"\"Handle `ocr_languages` and `languages`, defining `languages` to ['eng'] as default and\n-    converting `ocr_languages` if needed\"\"\"\n-    if languages is None:\n-        languages = [\"eng\"]\n+def check_language_args(\n+    languages: Optional[List[str]], ocr_languages: Optional[str]\n+) -> Union[list[str], None]:",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1466689489",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2435,
        "pr_file": "unstructured/partition/lang.py",
        "discussion_id": "1466689489",
        "commented_code": "@@ -167,34 +167,68 @@ def prepare_languages_for_tesseract(languages: Optional[List[str]] = [\"eng\"]):\n     return TESSERACT_LANGUAGES_SPLITTER.join(converted_languages)\n \n \n-def check_languages(languages: Optional[List[str]], ocr_languages: Optional[str]):\n-    \"\"\"Handle `ocr_languages` and `languages`, defining `languages` to ['eng'] as default and\n-    converting `ocr_languages` if needed\"\"\"\n-    if languages is None:\n-        languages = [\"eng\"]\n+def check_language_args(\n+    languages: Optional[List[str]], ocr_languages: Optional[str]\n+) -> Union[list[str], None]:",
        "comment_created_at": "2024-01-25T17:11:13+00:00",
        "comment_author": "scanny",
        "comment_body": "The return type should be `Optional[list[str]]`. If you were going to use `Union` notation for this, which is perfectly valid, just not conventional for this code base, then it would be `list[str] | None`. Both the lowercase version of `List` and the `|` version of `Union` require a `from __future__ import annotations` import (which all our files should have until Python 3.13 or later).",
        "pr_file_module": null
      },
      {
        "comment_id": "1466706889",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2435,
        "pr_file": "unstructured/partition/lang.py",
        "discussion_id": "1466689489",
        "commented_code": "@@ -167,34 +167,68 @@ def prepare_languages_for_tesseract(languages: Optional[List[str]] = [\"eng\"]):\n     return TESSERACT_LANGUAGES_SPLITTER.join(converted_languages)\n \n \n-def check_languages(languages: Optional[List[str]], ocr_languages: Optional[str]):\n-    \"\"\"Handle `ocr_languages` and `languages`, defining `languages` to ['eng'] as default and\n-    converting `ocr_languages` if needed\"\"\"\n-    if languages is None:\n-        languages = [\"eng\"]\n+def check_language_args(\n+    languages: Optional[List[str]], ocr_languages: Optional[str]\n+) -> Union[list[str], None]:",
        "comment_created_at": "2024-01-25T17:25:25+00:00",
        "comment_author": "scanny",
        "comment_body": "I would be strongly inclined to cut off optionality of `languages` at the interface function and use `[]` as the zero value for `languages` everywhere else. Rationale:\r\n\r\n1. The option to provide None is a convenience to an end-user who is specifying `languages` as a kwarg (dict item) where it it more work to omit the argument. If a default is applied to replace an omitted value, it's the interface function's job to do that substitution.\r\n2. `None` and `[]` have the same semantics (as far as I know). Using both complicates any other non-interface function that uses that value. In general, `Optional` should be used sparingly outside of interface functions and methods.\r\n\r\nThe general principle here is to identify flexibilities we afford an end-user in calling an interface function and isolate those flexibilities to that interface function. Otherwise we have internal and helper functions attending to peculiarities of calling flexibility which should not be their responsibility (and makes interface changes like this tend to involve much deeper parts of the code than we'd like because they're unnecessarily coupled to the interface particulars.",
        "pr_file_module": null
      }
    ]
  }
]