[
  {
    "discussion_id": "1809947719",
    "pr_number": 3732,
    "pr_file": "test_unstructured/documents/test_ontology_to_unstructured_parsing.py",
    "created_at": "2024-10-22T05:03:44+00:00",
    "commented_code": "+from pathlib import Path\n+\n+import pytest\n+\n+from unstructured.chunking.basic import chunk_elements\n+from unstructured.chunking.title import chunk_by_title\n+from unstructured.documents.ontology import Column, Document, Page, Paragraph\n+from unstructured.documents.transformations import (\n+    ontology_to_unstructured_elements,\n+    parse_html_to_ontology,\n+)\n+from unstructured.embed.openai import OpenAIEmbeddingConfig, OpenAIEmbeddingEncoder\n+from unstructured.staging.base import elements_from_json\n+\n+\n+def test_page_number_is_passed_correctly():\n+    ontology = Document(\n+        children=[\n+            Page(\n+                children=[Paragraph(text=\"Paragraph1\")],\n+                additional_attributes={\"data-page-number\": \"1\"},\n+            ),\n+            Page(\n+                children=[Paragraph(text=\"Paragraph2\")],\n+                additional_attributes={\"data-page-number\": \"2\"},\n+            ),\n+        ]\n+    )\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    body, page1, p1, page2, p2 = unstructured_elements\n+    assert p1.metadata.page_number == 1\n+    assert p2.metadata.page_number == 2\n+\n+\n+def test_invalid_page_number_is_not_passed():\n+    ontology = Document(\n+        children=[\n+            Page(\n+                children=[Paragraph(text=\"Paragraph1\")],\n+                additional_attributes={\"data-page-number\": \"invalid\"},\n+            )\n+        ]\n+    )\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    body, page1, p1 = unstructured_elements\n+    assert not p1.metadata.page_number\n+\n+\n+def test_depth_is_passed_correctly():\n+    ontology = Document(\n+        children=[\n+            Page(children=[Paragraph(text=\"Paragraph1\")]),\n+            Page(\n+                children=[\n+                    Column(children=[Paragraph(text=\"Paragraph2\")]),\n+                    Column(children=[Paragraph(text=\"Paragraph3\")]),\n+                ]\n+            ),\n+        ]\n+    )\n+\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    body, page1, p1, page2, c1, p2, c2, p3 = unstructured_elements\n+    assert body.metadata.category_depth == 0\n+\n+    assert page1.metadata.category_depth == 1\n+    assert page2.metadata.category_depth == 1\n+\n+    assert p1.metadata.category_depth == 2\n+\n+    assert c2.metadata.category_depth == 2\n+    assert c1.metadata.category_depth == 2\n+\n+    assert p2.metadata.category_depth == 3\n+    assert p3.metadata.category_depth == 3\n+\n+\n+def test_chunking_is_applied_on_elements():\n+    ontology = Document(\n+        children=[\n+            Page(children=[Paragraph(text=\"Paragraph1\")]),\n+            Page(\n+                children=[\n+                    Column(children=[Paragraph(text=\"Paragraph2\")]),\n+                    Column(children=[Paragraph(text=\"Paragraph3\")]),\n+                ]\n+            ),\n+        ]\n+    )\n+\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+\n+    chunked_basic = chunk_elements(unstructured_elements)\n+    assert str(chunked_basic[0]) == \"Paragraph1\n\nParagraph2\n\nParagraph3\"\n+    chunked_by_title = chunk_by_title(unstructured_elements)\n+    assert str(chunked_by_title[0]) == \"Paragraph1\n\nParagraph2\n\nParagraph3\"\n+\n+\n+def test_embeddings_are_applied_on_elements(mocker):\n+    ontology = Document(\n+        children=[\n+            Page(children=[Paragraph(text=\"Paragraph1\")]),\n+            Page(\n+                children=[\n+                    Column(children=[Paragraph(text=\"Paragraph2\")]),\n+                    Column(children=[Paragraph(text=\"Paragraph3\")]),\n+                ]\n+            ),\n+        ]\n+    )\n+\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    # Mocked client with the desired behavior for embed_documents\n+    mock_client = mocker.MagicMock()\n+    mock_client.embed_documents.return_value = [1, 2, 3, 4, 5, 6, 7, 8]\n+\n+    # Mock get_client to return our mock_client\n+    mocker.patch.object(OpenAIEmbeddingConfig, \"get_client\", return_value=mock_client)\n+\n+    encoder = OpenAIEmbeddingEncoder(config=OpenAIEmbeddingConfig(api_key=\"api_key\"))\n+    elements = encoder.embed_documents(\n+        elements=unstructured_elements,\n+    )\n+\n+    assert len(elements) == 8\n+\n+    document, page1, p1, page2, c1, p2, c2, p3 = elements\n+\n+    assert p1.embeddings == 3\n+    assert p2.embeddings == 6\n+    assert p3.embeddings == 8\n+\n+\n+@pytest.mark.parametrize(\n+    (\"html_file_path\", \"json_file_path\"),\n+    [\n+        (\"html_files/example.html\", \"structured_jsons/example.json\"),\n+    ],\n+)\n+def test_ingest(html_file_path, json_file_path):\n+    html_code = Path(html_file_path).read_text()\n+    expected_json_elements = elements_from_json(str(Path(json_file_path)))\n+\n+    ontology = parse_html_to_ontology(html_code)\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1809947719",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3732,
        "pr_file": "test_unstructured/documents/test_ontology_to_unstructured_parsing.py",
        "discussion_id": "1809947719",
        "commented_code": "@@ -0,0 +1,146 @@\n+from pathlib import Path\n+\n+import pytest\n+\n+from unstructured.chunking.basic import chunk_elements\n+from unstructured.chunking.title import chunk_by_title\n+from unstructured.documents.ontology import Column, Document, Page, Paragraph\n+from unstructured.documents.transformations import (\n+    ontology_to_unstructured_elements,\n+    parse_html_to_ontology,\n+)\n+from unstructured.embed.openai import OpenAIEmbeddingConfig, OpenAIEmbeddingEncoder\n+from unstructured.staging.base import elements_from_json\n+\n+\n+def test_page_number_is_passed_correctly():\n+    ontology = Document(\n+        children=[\n+            Page(\n+                children=[Paragraph(text=\"Paragraph1\")],\n+                additional_attributes={\"data-page-number\": \"1\"},\n+            ),\n+            Page(\n+                children=[Paragraph(text=\"Paragraph2\")],\n+                additional_attributes={\"data-page-number\": \"2\"},\n+            ),\n+        ]\n+    )\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    body, page1, p1, page2, p2 = unstructured_elements\n+    assert p1.metadata.page_number == 1\n+    assert p2.metadata.page_number == 2\n+\n+\n+def test_invalid_page_number_is_not_passed():\n+    ontology = Document(\n+        children=[\n+            Page(\n+                children=[Paragraph(text=\"Paragraph1\")],\n+                additional_attributes={\"data-page-number\": \"invalid\"},\n+            )\n+        ]\n+    )\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    body, page1, p1 = unstructured_elements\n+    assert not p1.metadata.page_number\n+\n+\n+def test_depth_is_passed_correctly():\n+    ontology = Document(\n+        children=[\n+            Page(children=[Paragraph(text=\"Paragraph1\")]),\n+            Page(\n+                children=[\n+                    Column(children=[Paragraph(text=\"Paragraph2\")]),\n+                    Column(children=[Paragraph(text=\"Paragraph3\")]),\n+                ]\n+            ),\n+        ]\n+    )\n+\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    body, page1, p1, page2, c1, p2, c2, p3 = unstructured_elements\n+    assert body.metadata.category_depth == 0\n+\n+    assert page1.metadata.category_depth == 1\n+    assert page2.metadata.category_depth == 1\n+\n+    assert p1.metadata.category_depth == 2\n+\n+    assert c2.metadata.category_depth == 2\n+    assert c1.metadata.category_depth == 2\n+\n+    assert p2.metadata.category_depth == 3\n+    assert p3.metadata.category_depth == 3\n+\n+\n+def test_chunking_is_applied_on_elements():\n+    ontology = Document(\n+        children=[\n+            Page(children=[Paragraph(text=\"Paragraph1\")]),\n+            Page(\n+                children=[\n+                    Column(children=[Paragraph(text=\"Paragraph2\")]),\n+                    Column(children=[Paragraph(text=\"Paragraph3\")]),\n+                ]\n+            ),\n+        ]\n+    )\n+\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+\n+    chunked_basic = chunk_elements(unstructured_elements)\n+    assert str(chunked_basic[0]) == \"Paragraph1\\n\\nParagraph2\\n\\nParagraph3\"\n+    chunked_by_title = chunk_by_title(unstructured_elements)\n+    assert str(chunked_by_title[0]) == \"Paragraph1\\n\\nParagraph2\\n\\nParagraph3\"\n+\n+\n+def test_embeddings_are_applied_on_elements(mocker):\n+    ontology = Document(\n+        children=[\n+            Page(children=[Paragraph(text=\"Paragraph1\")]),\n+            Page(\n+                children=[\n+                    Column(children=[Paragraph(text=\"Paragraph2\")]),\n+                    Column(children=[Paragraph(text=\"Paragraph3\")]),\n+                ]\n+            ),\n+        ]\n+    )\n+\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    # Mocked client with the desired behavior for embed_documents\n+    mock_client = mocker.MagicMock()\n+    mock_client.embed_documents.return_value = [1, 2, 3, 4, 5, 6, 7, 8]\n+\n+    # Mock get_client to return our mock_client\n+    mocker.patch.object(OpenAIEmbeddingConfig, \"get_client\", return_value=mock_client)\n+\n+    encoder = OpenAIEmbeddingEncoder(config=OpenAIEmbeddingConfig(api_key=\"api_key\"))\n+    elements = encoder.embed_documents(\n+        elements=unstructured_elements,\n+    )\n+\n+    assert len(elements) == 8\n+\n+    document, page1, p1, page2, c1, p2, c2, p3 = elements\n+\n+    assert p1.embeddings == 3\n+    assert p2.embeddings == 6\n+    assert p3.embeddings == 8\n+\n+\n+@pytest.mark.parametrize(\n+    (\"html_file_path\", \"json_file_path\"),\n+    [\n+        (\"html_files/example.html\", \"structured_jsons/example.json\"),\n+    ],\n+)\n+def test_ingest(html_file_path, json_file_path):\n+    html_code = Path(html_file_path).read_text()\n+    expected_json_elements = elements_from_json(str(Path(json_file_path)))\n+\n+    ontology = parse_html_to_ontology(html_code)\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)",
        "comment_created_at": "2024-10-22T05:03:44+00:00",
        "comment_author": "cragwolfe",
        "comment_body": "for consumers that just want unstructured elements, these two functions calls combined `parse_html_to_ontology()` and `ontology_to_unstructured_elements()`, i.e. html -> unstructured elements should accessible to consumers using [paritition_html()](https://github.com/Unstructured-IO/unstructured/blob/a11ad22609ca016f8080646c20e6da9ad5e7dc08/unstructured/partition/html/partition.py#L23) (or plain old [partition()](https://github.com/Unstructured-IO/unstructured/blob/a11ad22609ca016f8080646c20e6da9ad5e7dc08/unstructured/partition/auto.py#L25) which may call `partition_html()`)  with an additional flag that indicates whether to parse with the prior or new parsing method. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1810767802",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3732,
        "pr_file": "test_unstructured/documents/test_ontology_to_unstructured_parsing.py",
        "discussion_id": "1809947719",
        "commented_code": "@@ -0,0 +1,146 @@\n+from pathlib import Path\n+\n+import pytest\n+\n+from unstructured.chunking.basic import chunk_elements\n+from unstructured.chunking.title import chunk_by_title\n+from unstructured.documents.ontology import Column, Document, Page, Paragraph\n+from unstructured.documents.transformations import (\n+    ontology_to_unstructured_elements,\n+    parse_html_to_ontology,\n+)\n+from unstructured.embed.openai import OpenAIEmbeddingConfig, OpenAIEmbeddingEncoder\n+from unstructured.staging.base import elements_from_json\n+\n+\n+def test_page_number_is_passed_correctly():\n+    ontology = Document(\n+        children=[\n+            Page(\n+                children=[Paragraph(text=\"Paragraph1\")],\n+                additional_attributes={\"data-page-number\": \"1\"},\n+            ),\n+            Page(\n+                children=[Paragraph(text=\"Paragraph2\")],\n+                additional_attributes={\"data-page-number\": \"2\"},\n+            ),\n+        ]\n+    )\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    body, page1, p1, page2, p2 = unstructured_elements\n+    assert p1.metadata.page_number == 1\n+    assert p2.metadata.page_number == 2\n+\n+\n+def test_invalid_page_number_is_not_passed():\n+    ontology = Document(\n+        children=[\n+            Page(\n+                children=[Paragraph(text=\"Paragraph1\")],\n+                additional_attributes={\"data-page-number\": \"invalid\"},\n+            )\n+        ]\n+    )\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    body, page1, p1 = unstructured_elements\n+    assert not p1.metadata.page_number\n+\n+\n+def test_depth_is_passed_correctly():\n+    ontology = Document(\n+        children=[\n+            Page(children=[Paragraph(text=\"Paragraph1\")]),\n+            Page(\n+                children=[\n+                    Column(children=[Paragraph(text=\"Paragraph2\")]),\n+                    Column(children=[Paragraph(text=\"Paragraph3\")]),\n+                ]\n+            ),\n+        ]\n+    )\n+\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    body, page1, p1, page2, c1, p2, c2, p3 = unstructured_elements\n+    assert body.metadata.category_depth == 0\n+\n+    assert page1.metadata.category_depth == 1\n+    assert page2.metadata.category_depth == 1\n+\n+    assert p1.metadata.category_depth == 2\n+\n+    assert c2.metadata.category_depth == 2\n+    assert c1.metadata.category_depth == 2\n+\n+    assert p2.metadata.category_depth == 3\n+    assert p3.metadata.category_depth == 3\n+\n+\n+def test_chunking_is_applied_on_elements():\n+    ontology = Document(\n+        children=[\n+            Page(children=[Paragraph(text=\"Paragraph1\")]),\n+            Page(\n+                children=[\n+                    Column(children=[Paragraph(text=\"Paragraph2\")]),\n+                    Column(children=[Paragraph(text=\"Paragraph3\")]),\n+                ]\n+            ),\n+        ]\n+    )\n+\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+\n+    chunked_basic = chunk_elements(unstructured_elements)\n+    assert str(chunked_basic[0]) == \"Paragraph1\\n\\nParagraph2\\n\\nParagraph3\"\n+    chunked_by_title = chunk_by_title(unstructured_elements)\n+    assert str(chunked_by_title[0]) == \"Paragraph1\\n\\nParagraph2\\n\\nParagraph3\"\n+\n+\n+def test_embeddings_are_applied_on_elements(mocker):\n+    ontology = Document(\n+        children=[\n+            Page(children=[Paragraph(text=\"Paragraph1\")]),\n+            Page(\n+                children=[\n+                    Column(children=[Paragraph(text=\"Paragraph2\")]),\n+                    Column(children=[Paragraph(text=\"Paragraph3\")]),\n+                ]\n+            ),\n+        ]\n+    )\n+\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    # Mocked client with the desired behavior for embed_documents\n+    mock_client = mocker.MagicMock()\n+    mock_client.embed_documents.return_value = [1, 2, 3, 4, 5, 6, 7, 8]\n+\n+    # Mock get_client to return our mock_client\n+    mocker.patch.object(OpenAIEmbeddingConfig, \"get_client\", return_value=mock_client)\n+\n+    encoder = OpenAIEmbeddingEncoder(config=OpenAIEmbeddingConfig(api_key=\"api_key\"))\n+    elements = encoder.embed_documents(\n+        elements=unstructured_elements,\n+    )\n+\n+    assert len(elements) == 8\n+\n+    document, page1, p1, page2, c1, p2, c2, p3 = elements\n+\n+    assert p1.embeddings == 3\n+    assert p2.embeddings == 6\n+    assert p3.embeddings == 8\n+\n+\n+@pytest.mark.parametrize(\n+    (\"html_file_path\", \"json_file_path\"),\n+    [\n+        (\"html_files/example.html\", \"structured_jsons/example.json\"),\n+    ],\n+)\n+def test_ingest(html_file_path, json_file_path):\n+    html_code = Path(html_file_path).read_text()\n+    expected_json_elements = elements_from_json(str(Path(json_file_path)))\n+\n+    ontology = parse_html_to_ontology(html_code)\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)",
        "comment_created_at": "2024-10-22T13:48:42+00:00",
        "comment_author": "plutasnyy",
        "comment_body": "I have added another strategy for partition_html; please double check that. Also, should we add contains_element_ontology to partition.auto? I guess we will leverage here **kwargs?",
        "pr_file_module": null
      },
      {
        "comment_id": "1810768487",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3732,
        "pr_file": "test_unstructured/documents/test_ontology_to_unstructured_parsing.py",
        "discussion_id": "1809947719",
        "commented_code": "@@ -0,0 +1,146 @@\n+from pathlib import Path\n+\n+import pytest\n+\n+from unstructured.chunking.basic import chunk_elements\n+from unstructured.chunking.title import chunk_by_title\n+from unstructured.documents.ontology import Column, Document, Page, Paragraph\n+from unstructured.documents.transformations import (\n+    ontology_to_unstructured_elements,\n+    parse_html_to_ontology,\n+)\n+from unstructured.embed.openai import OpenAIEmbeddingConfig, OpenAIEmbeddingEncoder\n+from unstructured.staging.base import elements_from_json\n+\n+\n+def test_page_number_is_passed_correctly():\n+    ontology = Document(\n+        children=[\n+            Page(\n+                children=[Paragraph(text=\"Paragraph1\")],\n+                additional_attributes={\"data-page-number\": \"1\"},\n+            ),\n+            Page(\n+                children=[Paragraph(text=\"Paragraph2\")],\n+                additional_attributes={\"data-page-number\": \"2\"},\n+            ),\n+        ]\n+    )\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    body, page1, p1, page2, p2 = unstructured_elements\n+    assert p1.metadata.page_number == 1\n+    assert p2.metadata.page_number == 2\n+\n+\n+def test_invalid_page_number_is_not_passed():\n+    ontology = Document(\n+        children=[\n+            Page(\n+                children=[Paragraph(text=\"Paragraph1\")],\n+                additional_attributes={\"data-page-number\": \"invalid\"},\n+            )\n+        ]\n+    )\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    body, page1, p1 = unstructured_elements\n+    assert not p1.metadata.page_number\n+\n+\n+def test_depth_is_passed_correctly():\n+    ontology = Document(\n+        children=[\n+            Page(children=[Paragraph(text=\"Paragraph1\")]),\n+            Page(\n+                children=[\n+                    Column(children=[Paragraph(text=\"Paragraph2\")]),\n+                    Column(children=[Paragraph(text=\"Paragraph3\")]),\n+                ]\n+            ),\n+        ]\n+    )\n+\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    body, page1, p1, page2, c1, p2, c2, p3 = unstructured_elements\n+    assert body.metadata.category_depth == 0\n+\n+    assert page1.metadata.category_depth == 1\n+    assert page2.metadata.category_depth == 1\n+\n+    assert p1.metadata.category_depth == 2\n+\n+    assert c2.metadata.category_depth == 2\n+    assert c1.metadata.category_depth == 2\n+\n+    assert p2.metadata.category_depth == 3\n+    assert p3.metadata.category_depth == 3\n+\n+\n+def test_chunking_is_applied_on_elements():\n+    ontology = Document(\n+        children=[\n+            Page(children=[Paragraph(text=\"Paragraph1\")]),\n+            Page(\n+                children=[\n+                    Column(children=[Paragraph(text=\"Paragraph2\")]),\n+                    Column(children=[Paragraph(text=\"Paragraph3\")]),\n+                ]\n+            ),\n+        ]\n+    )\n+\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+\n+    chunked_basic = chunk_elements(unstructured_elements)\n+    assert str(chunked_basic[0]) == \"Paragraph1\\n\\nParagraph2\\n\\nParagraph3\"\n+    chunked_by_title = chunk_by_title(unstructured_elements)\n+    assert str(chunked_by_title[0]) == \"Paragraph1\\n\\nParagraph2\\n\\nParagraph3\"\n+\n+\n+def test_embeddings_are_applied_on_elements(mocker):\n+    ontology = Document(\n+        children=[\n+            Page(children=[Paragraph(text=\"Paragraph1\")]),\n+            Page(\n+                children=[\n+                    Column(children=[Paragraph(text=\"Paragraph2\")]),\n+                    Column(children=[Paragraph(text=\"Paragraph3\")]),\n+                ]\n+            ),\n+        ]\n+    )\n+\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)\n+    # Mocked client with the desired behavior for embed_documents\n+    mock_client = mocker.MagicMock()\n+    mock_client.embed_documents.return_value = [1, 2, 3, 4, 5, 6, 7, 8]\n+\n+    # Mock get_client to return our mock_client\n+    mocker.patch.object(OpenAIEmbeddingConfig, \"get_client\", return_value=mock_client)\n+\n+    encoder = OpenAIEmbeddingEncoder(config=OpenAIEmbeddingConfig(api_key=\"api_key\"))\n+    elements = encoder.embed_documents(\n+        elements=unstructured_elements,\n+    )\n+\n+    assert len(elements) == 8\n+\n+    document, page1, p1, page2, c1, p2, c2, p3 = elements\n+\n+    assert p1.embeddings == 3\n+    assert p2.embeddings == 6\n+    assert p3.embeddings == 8\n+\n+\n+@pytest.mark.parametrize(\n+    (\"html_file_path\", \"json_file_path\"),\n+    [\n+        (\"html_files/example.html\", \"structured_jsons/example.json\"),\n+    ],\n+)\n+def test_ingest(html_file_path, json_file_path):\n+    html_code = Path(html_file_path).read_text()\n+    expected_json_elements = elements_from_json(str(Path(json_file_path)))\n+\n+    ontology = parse_html_to_ontology(html_code)\n+    unstructured_elements = ontology_to_unstructured_elements(ontology)",
        "comment_created_at": "2024-10-22T13:49:04+00:00",
        "comment_author": "plutasnyy",
        "comment_body": "https://github.com/Unstructured-IO/unstructured/pull/3732/commits/fdf176a3e76b6cbf727e5797f837310b7e7fcbb5",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1811838579",
    "pr_number": 3732,
    "pr_file": "unstructured/partition/html/partition.py",
    "created_at": "2024-10-23T04:56:10+00:00",
    "commented_code": "ssl_verify: bool = True,\n     skip_headers_and_footers: bool = False,\n     detection_origin: Optional[str] = None,\n+    contains_ontology_schema: bool = False,",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1811838579",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3732,
        "pr_file": "unstructured/partition/html/partition.py",
        "discussion_id": "1811838579",
        "commented_code": "@@ -31,6 +35,7 @@ def partition_html(\n     ssl_verify: bool = True,\n     skip_headers_and_footers: bool = False,\n     detection_origin: Optional[str] = None,\n+    contains_ontology_schema: bool = False,",
        "comment_created_at": "2024-10-23T04:56:10+00:00",
        "comment_author": "cragwolfe",
        "comment_body": "let's rename this parameter, and instead a boolean let's allow a string argument where None (or v1?) is the \"classic\" html parser. this will allow for additional \"parsers\" in the future without having to introduce a new parameter.\r\n\r\nso to support the new parsing method, how about:\r\n\r\n```\r\nhtml_parser=v2\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1650949729",
    "pr_number": 3278,
    "pr_file": "test_unstructured/partition/test_auto.py",
    "created_at": "2024-06-24T12:28:32+00:00",
    "commented_code": "PartitionStrategy.OCR_ONLY,\n     ],\n )\n-def test_partition_forwards_strategy_arg_to_partition_docx(request: FixtureRequest, strategy: str):\n+def test_partition_forwards_strategy_arg_to_partition_docx_and_its_brokers(",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1650949729",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3278,
        "pr_file": "test_unstructured/partition/test_auto.py",
        "discussion_id": "1650949729",
        "commented_code": "@@ -187,7 +188,17 @@ def test_auto_partition_doc_with_file(mock_docx_document, expected_docx_elements\n         PartitionStrategy.OCR_ONLY,\n     ],\n )\n-def test_partition_forwards_strategy_arg_to_partition_docx(request: FixtureRequest, strategy: str):\n+def test_partition_forwards_strategy_arg_to_partition_docx_and_its_brokers(",
        "comment_created_at": "2024-06-24T12:28:32+00:00",
        "comment_author": "MthwRobinson",
        "comment_body": "Is there a test we can add that tests this for all brokering partitioners? So if something isn't passed we're able to catch it.",
        "pr_file_module": null
      },
      {
        "comment_id": "1651692547",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3278,
        "pr_file": "test_unstructured/partition/test_auto.py",
        "discussion_id": "1650949729",
        "commented_code": "@@ -187,7 +188,17 @@ def test_auto_partition_doc_with_file(mock_docx_document, expected_docx_elements\n         PartitionStrategy.OCR_ONLY,\n     ],\n )\n-def test_partition_forwards_strategy_arg_to_partition_docx(request: FixtureRequest, strategy: str):\n+def test_partition_forwards_strategy_arg_to_partition_docx_and_its_brokers(",
        "comment_created_at": "2024-06-24T22:03:00+00:00",
        "comment_author": "scanny",
        "comment_body": "Maybe, but I think the solution is not being selective about what parameters we pass along from `partition()` to delegate partitioners. I go into more details in the write-up I sent, but there shouldn't be a good reason for us to be explicitly calling out which parameters get forwarded. We can do something like:\r\n```python\r\n        elements = _partition_doc(filename, file=file, **kwargs)\r\n```\r\ninstead of what we're doing now which looks like this:\r\n```python\r\n        elements = _partition_doc(\r\n            filename=filename,\r\n            file=file,\r\n            infer_table_structure=infer_table_structure,\r\n            languages=languages,\r\n            detect_language_per_element=detect_language_per_element,\r\n            starting_page_number=starting_page_number,\r\n            strategy=strategy,\r\n            **kwargs,\r\n        )\r\n```\r\n\r\nThere's some work to get there, which I went into in my write-up, but the result is basically maintenance-free.\r\n\r\nAll partitioners get all arguments from the call, they pick out which ones they want and ignore the rest.\r\n\r\nAny new parameter that a partitioner starts using will already be in its `kwargs`. All it has to do is create a parameter definition for that one and it will get populated on each call.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1617402443",
    "pr_number": 3096,
    "pr_file": "unstructured/partition/docx.py",
    "created_at": "2024-05-28T14:51:58+00:00",
    "commented_code": "metadata_last_modified: Optional[str],\n         starting_page_number: int = 1,\n         strategy: str | None = None,\n+        languages: Optional[list[str]] = None,",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1617402443",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3096,
        "pr_file": "unstructured/partition/docx.py",
        "discussion_id": "1617402443",
        "commented_code": "@@ -154,6 +157,7 @@ def __init__(\n         metadata_last_modified: Optional[str],\n         starting_page_number: int = 1,\n         strategy: str | None = None,\n+        languages: Optional[list[str]] = None,",
        "comment_created_at": "2024-05-28T14:51:58+00:00",
        "comment_author": "MthwRobinson",
        "comment_body": "@scanny - Would you prefer `**kwargs` here?",
        "pr_file_module": null
      },
      {
        "comment_id": "1618119761",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3096,
        "pr_file": "unstructured/partition/docx.py",
        "discussion_id": "1617402443",
        "commented_code": "@@ -154,6 +157,7 @@ def __init__(\n         metadata_last_modified: Optional[str],\n         starting_page_number: int = 1,\n         strategy: str | None = None,\n+        languages: Optional[list[str]] = None,",
        "comment_created_at": "2024-05-29T03:03:23+00:00",
        "comment_author": "JIAQIA",
        "comment_body": "OK, it will pass languages by kwargs now.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1459237257",
    "pr_number": 2428,
    "pr_file": "unstructured/partition/auto.py",
    "created_at": "2024-01-19T15:50:58+00:00",
    "commented_code": "detect_language_per_element=detect_language_per_element,\n             **kwargs,\n         )\n+    elif filetype in CODE_FILETYPES:",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1459237257",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2428,
        "pr_file": "unstructured/partition/auto.py",
        "discussion_id": "1459237257",
        "commented_code": "@@ -524,6 +541,13 @@ def partition(\n             detect_language_per_element=detect_language_per_element,\n             **kwargs,\n         )\n+    elif filetype in CODE_FILETYPES:",
        "comment_created_at": "2024-01-19T15:50:58+00:00",
        "comment_author": "MthwRobinson",
        "comment_body": "Could we update this logic to fallback to `partition_text` if `tree_sitter` doesn't exist? That way people who a processing these as text files can still do so if they don't have the code dependencies installed.",
        "pr_file_module": null
      },
      {
        "comment_id": "1460222115",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2428,
        "pr_file": "unstructured/partition/auto.py",
        "discussion_id": "1459237257",
        "commented_code": "@@ -524,6 +541,13 @@ def partition(\n             detect_language_per_element=detect_language_per_element,\n             **kwargs,\n         )\n+    elif filetype in CODE_FILETYPES:",
        "comment_created_at": "2024-01-20T05:17:16+00:00",
        "comment_author": "Plemeur",
        "comment_body": "Sure, \r\nWe can either :\r\n- define `partition_code = partition_text` when checking for the dependancy\r\n- Add a global switch (or recheck for dependancy) and change the filetype from any code filetype to TXT\r\n- Add a global switch (or recheck for dependancy) and add a nested if in the `if filetype in CODE_FILETYPES:`\r\n\r\nI'm fine with any of those",
        "pr_file_module": null
      },
      {
        "comment_id": "1461552679",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2428,
        "pr_file": "unstructured/partition/auto.py",
        "discussion_id": "1459237257",
        "commented_code": "@@ -524,6 +541,13 @@ def partition(\n             detect_language_per_element=detect_language_per_element,\n             **kwargs,\n         )\n+    elif filetype in CODE_FILETYPES:",
        "comment_created_at": "2024-01-22T09:18:00+00:00",
        "comment_author": "Plemeur",
        "comment_body": "Added tests, \r\nThere are some gotcha that I will try to fix such as javascript files detected as cpp by libmagic, or go file detected as plain text\r\n\r\nOn top of that, I found that it's tricky to make the text actually fit both the minimum and the maximum size properly. I went with enforcing the minimum because it means less elements so potentially better search in the vector database\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1463730482",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2428,
        "pr_file": "unstructured/partition/auto.py",
        "discussion_id": "1459237257",
        "commented_code": "@@ -524,6 +541,13 @@ def partition(\n             detect_language_per_element=detect_language_per_element,\n             **kwargs,\n         )\n+    elif filetype in CODE_FILETYPES:",
        "comment_created_at": "2024-01-23T18:04:59+00:00",
        "comment_author": "MthwRobinson",
        "comment_body": "I'm good with either of these. I'd say avoid `partition_code = partition_text`, just so it's clear later in the code that `partition_text` is being applied and not `partition_code`.\r\n- Add a global switch (or recheck for dependancy) and change the filetype from any code filetype to TXT\r\n- Add a global switch (or recheck for dependancy) and add a nested if in the if filetype in CODE_FILETYPES:\r\n\r\nAre the MIME type discrepancies when they're checked as files rather than by filename? If think that's okay and you can mock the correct MIME type in the test. Hopefully most of those files come in with the file extension.\r\n\r\nThe enforcing minimum logic makes sense to me.",
        "pr_file_module": null
      },
      {
        "comment_id": "1464212110",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2428,
        "pr_file": "unstructured/partition/auto.py",
        "discussion_id": "1459237257",
        "commented_code": "@@ -524,6 +541,13 @@ def partition(\n             detect_language_per_element=detect_language_per_element,\n             **kwargs,\n         )\n+    elif filetype in CODE_FILETYPES:",
        "comment_created_at": "2024-01-24T02:08:33+00:00",
        "comment_author": "Plemeur",
        "comment_body": "Yes, MIME types are not always correctly detected.\r\nJust to make sure people have the option, I'm adding the `programming_language` parameters, and adding some test for the auto partition \r\n\r\nWe could try all the available parsers for file that are not detected as code at all, but for the ones where the MIME is mixed up, there's nothing we can really do",
        "pr_file_module": null
      },
      {
        "comment_id": "1464334575",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2428,
        "pr_file": "unstructured/partition/auto.py",
        "discussion_id": "1459237257",
        "commented_code": "@@ -524,6 +541,13 @@ def partition(\n             detect_language_per_element=detect_language_per_element,\n             **kwargs,\n         )\n+    elif filetype in CODE_FILETYPES:",
        "comment_created_at": "2024-01-24T05:44:32+00:00",
        "comment_author": "Plemeur",
        "comment_body": "After looking up a bit, it seems that most code files do not have a MIME type...\r\nHere is a list of available ones https://www.iana.org/assignments/media-types/media-types.xhtml\r\n\r\nSince it is required for the auto partition, I made custom ones, it looks like to me that the common way of doing so is `text/x-{language}` so I went for that ",
        "pr_file_module": null
      }
    ]
  }
]