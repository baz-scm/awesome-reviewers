[
  {
    "discussion_id": "1946463588",
    "pr_number": 3900,
    "pr_file": "unstructured/partition/pdf_image/pdfminer_processing.py",
    "created_at": "2025-02-07T12:36:28+00:00",
    "commented_code": "return all(x is not None for x in bbox) and (bbox[2] - bbox[0] > 0) and (bbox[3] - bbox[1] > 0)\n \n \n+def _minimum_containing_coords(*regions: TextRegions) -> np.ndarray:\n+    # TODO: refactor to just use np array as input\n+    return np.vstack(\n+        (\n+            np.min([region.x1 for region in regions], axis=0),\n+            np.min([region.y1 for region in regions], axis=0),\n+            np.max([region.x2 for region in regions], axis=0),\n+            np.max([region.y2 for region in regions], axis=0),\n+        )\n+    ).T\n+\n+\n+def _inferred_is_elementtype(\n+    inferred_layout: LayoutElements, etypes: Iterable[ElementType]\n+) -> np.ndarry:\n+    inferred_text_idx = [\n+        idx\n+        for idx, class_name in inferred_layout.element_class_id_map.items()\n+        if class_name in etypes\n+    ]\n+    inferred_is_etypes = np.zeros((len(inferred_layout),)).astype(bool)\n+    for idx in inferred_text_idx:\n+        inferred_is_etypes = np.logical_or(\n+            inferred_is_etypes, inferred_layout.element_class_ids == idx\n+        )\n+    return inferred_is_etypes\n+\n+\n+def _inferred_is_text(inferred_layout: LayoutElements) -> np.ndarry:\n+    return ~_inferred_is_elementtype(\n+        inferred_layout,\n+        etypes=(\n+            ElementType.FIGURE,\n+            ElementType.IMAGE,\n+            # NOTE (yao): PICTURE is not in the loop version of the logic in inference library\n+            # ElementType.PICTURE,\n+            ElementType.PAGE_BREAK,\n+            ElementType.TABLE,\n+        ),\n+    )\n+\n+\n+def _merge_extracted_into_inferred_when_almost_the_same(\n+    extracted_layout: LayoutElements,\n+    inferred_layout: LayoutElements,\n+    same_region_threshold: float,\n+) -> tuple[np.ndarray, np.ndarray]:\n+\n+    if len(inferred_layout) == 0:\n+        return np.array([False] * len(extracted_layout))\n+    if len(extracted_layout) == 0:\n+        return np.array([])\n+\n+    boxes_almost_same = boxes_iou(\n+        extracted_layout.element_coords,\n+        inferred_layout.element_coords,\n+        threshold=same_region_threshold,\n+    )\n+    extracted_almost_the_same_as_inferred = boxes_almost_same.sum(axis=1).astype(bool)\n+    # NOTE: if a row is full of False the argmax returns first index; we use the mask above to\n+    # distinguish those (they would be False in the mask)\n+    first_match = np.argmax(boxes_almost_same, axis=1)\n+    inferred_indices_to_update = first_match[extracted_almost_the_same_as_inferred]\n+    extracted_to_remove = extracted_layout.slice(extracted_almost_the_same_as_inferred)\n+    # copy here in case we change the extracted layout later\n+    inferred_layout.texts[inferred_indices_to_update] = extracted_to_remove.texts.copy()\n+    # use coords that can bound BOTH the inferred and extracted region as final bounding box coords\n+    inferred_layout.element_coords[inferred_indices_to_update] = _minimum_containing_coords(\n+        inferred_layout.slice(inferred_indices_to_update),\n+        extracted_to_remove,\n+    )\n+    return extracted_almost_the_same_as_inferred\n+\n+\n+def _merge_extracted_that_are_subregion_of_inferred_text(\n+    extracted_layout,\n+    inferred_layout,\n+    extracted_is_subregion_of_inferred,\n+    extracted_to_proc,\n+    inferred_to_proc,\n+):\n+    # in theory one extracted __should__ only match at most one inferred region, given inferred\n+    # region can not overlap; so first match here __should__ also be the only match\n+    inferred_to_iter = inferred_to_proc[inferred_to_proc]\n+    extracted_to_iter = extracted_to_proc[extracted_to_proc]\n+    for inferred_index, inferred_row in enumerate(extracted_is_subregion_of_inferred.T):\n+        matches = np.where(inferred_row)[0]\n+        if not matches.size:\n+            continue\n+        # Technically those two lines below can be vectorized but this loop would still run anyway;\n+        # it is not clear which one is overall faster so might worth profiling in the future\n+        extracted_to_iter[matches] = False\n+        inferred_to_iter[inferred_index] = False\n+        # then expand inferred box by all the extracted boxes\n+        # FIXME (yao): this part is broken at the moment\n+        inferred_layout.element_coords[[inferred_index]] = _minimum_containing_coords(\n+            inferred_layout.slice([inferred_index]),\n+            *[extracted_layout.slice([match]) for match in matches],\n+        )\n+    inferred_to_proc[inferred_to_proc] = inferred_to_iter\n+    extracted_to_proc[extracted_to_proc] = extracted_to_iter\n+    return inferred_layout\n+\n+\n+def _mark_non_table_inferred_for_removal_if_has_subregion_relationship(\n+    extracted_layout,\n+    inferred_layout,\n+    inferred_to_keep,\n+    subregion_threshold,\n+):\n+    inferred_is_subregion_of_extracted = bboxes1_is_almost_subregion_of_bboxes2(\n+        inferred_layout.element_coords,\n+        extracted_layout.element_coords,\n+        threshold=subregion_threshold,\n+    )\n+    extracted_is_subregion_of_inferred = bboxes1_is_almost_subregion_of_bboxes2(\n+        extracted_layout.element_coords,\n+        inferred_layout.element_coords,\n+        threshold=subregion_threshold,\n+    )\n+    inferred_to_remove_mask = (\n+        np.logical_or(\n+            inferred_is_subregion_of_extracted,\n+            extracted_is_subregion_of_inferred.T,\n+        )\n+        .sum(axis=1)\n+        .astype(bool)\n+    )\n+    # NOTE (yao): maybe we should expand those matching extracted region to contain the inferred\n+    # regions it has subregion relationship with? like we did for inferred regions\n+    inferred_to_keep[inferred_to_remove_mask] = False\n+    return inferred_to_keep\n+\n+\n+@requires_dependencies(\"unstructured_inference\")\n+def array_merge_inferred_layout_with_extracted_layout(\n+    inferred_layout: LayoutElements,\n+    extracted_layout: LayoutElements,\n+    page_image_size: tuple,\n+    same_region_threshold: float = inference_config.LAYOUT_SAME_REGION_THRESHOLD,\n+    subregion_threshold: float = inference_config.LAYOUT_SUBREGION_THRESHOLD,\n+    max_rounds: int = 5,\n+) -> LayoutElements:\n+    \"\"\"merge elements using array data structures; it also returns LayoutElements instead of\n+    collection of LayoutElement\"\"\"\n+    from unstructured_inference.inference.layoutelement import LayoutElements\n+\n+    if len(extracted_layout) == 0:\n+        return inferred_layout\n+    if len(inferred_layout) == 0:\n+        return extracted_layout\n+\n+    w, h = page_image_size\n+    full_page_region = Rectangle(0, 0, w, h)\n+    # ==== RULE 0: Full page extracted images are ignored\n+    # non full page extracted image regions are kept, except when they match a non-text inferred\n+    # region then we use the common bounding boxes and keep just one of the two sets (see rules\n+    # below)\n+    image_indices_to_keep = np.where(extracted_layout.element_class_ids == 1)[0]\n+    if len(image_indices_to_keep):\n+        full_page_image_mask = (\n+            boxes_iou(\n+                extracted_layout.slice(image_indices_to_keep).element_coords,\n+                [full_page_region],\n+                threshold=FULL_PAGE_REGION_THRESHOLD,\n+            )\n+            .sum(axis=1)\n+            .astype(bool)\n+        )\n+        image_indices_to_keep = image_indices_to_keep[~full_page_image_mask]\n+\n+    # ==== RULE 1: any inferred box that is almost the same as an extracted image box, inferred is\n+    # removed\n+    # NOTE (yao): what if od model detects table but pdfminer says image -> we would lose the table\n+    boxes_almost_same = (\n+        boxes_iou(\n+            inferred_layout.element_coords,\n+            extracted_layout.slice(image_indices_to_keep).element_coords,\n+            threshold=same_region_threshold,\n+        )\n+        .sum(axis=1)\n+        .astype(bool)\n+    )\n+\n+    # drop off those matching inferred from processing\n+    inferred_layout_to_proc = inferred_layout.slice(~boxes_almost_same)\n+    inferred_to_keep = np.array([True] * len(inferred_layout_to_proc))\n+\n+    # TODO (yao): experiment with all regions, not just text region, being potential targets to be\n+    # merged into inferred elements\n+    text_element_indices = np.where(extracted_layout.element_class_ids == 0)[0]\n+\n+    if len(text_element_indices) == 0:\n+        return LayoutElements.concatenate(\n+            (\n+                inferred_layout_to_proc,\n+                extracted_layout.slice(image_indices_to_keep),\n+            )\n+        )\n+\n+    if len(inferred_layout_to_proc) == 0:\n+        return extracted_layout.slice(np.concatenate((image_indices_to_keep, text_element_indices)))\n+\n+    extracted_text_layouts = extracted_layout.slice(text_element_indices)\n+    # ==== RULE 2. if there is a inferred region almost the same as the extracted text-region ->\n+    # keep inferred and removed extracted region; here we put more trust in OD model more than\n+    # pdfminer for bounding box",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1946463588",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3900,
        "pr_file": "unstructured/partition/pdf_image/pdfminer_processing.py",
        "discussion_id": "1946463588",
        "commented_code": "@@ -50,6 +52,304 @@ def _validate_bbox(bbox: list[int | float]) -> bool:\n     return all(x is not None for x in bbox) and (bbox[2] - bbox[0] > 0) and (bbox[3] - bbox[1] > 0)\n \n \n+def _minimum_containing_coords(*regions: TextRegions) -> np.ndarray:\n+    # TODO: refactor to just use np array as input\n+    return np.vstack(\n+        (\n+            np.min([region.x1 for region in regions], axis=0),\n+            np.min([region.y1 for region in regions], axis=0),\n+            np.max([region.x2 for region in regions], axis=0),\n+            np.max([region.y2 for region in regions], axis=0),\n+        )\n+    ).T\n+\n+\n+def _inferred_is_elementtype(\n+    inferred_layout: LayoutElements, etypes: Iterable[ElementType]\n+) -> np.ndarry:\n+    inferred_text_idx = [\n+        idx\n+        for idx, class_name in inferred_layout.element_class_id_map.items()\n+        if class_name in etypes\n+    ]\n+    inferred_is_etypes = np.zeros((len(inferred_layout),)).astype(bool)\n+    for idx in inferred_text_idx:\n+        inferred_is_etypes = np.logical_or(\n+            inferred_is_etypes, inferred_layout.element_class_ids == idx\n+        )\n+    return inferred_is_etypes\n+\n+\n+def _inferred_is_text(inferred_layout: LayoutElements) -> np.ndarry:\n+    return ~_inferred_is_elementtype(\n+        inferred_layout,\n+        etypes=(\n+            ElementType.FIGURE,\n+            ElementType.IMAGE,\n+            # NOTE (yao): PICTURE is not in the loop version of the logic in inference library\n+            # ElementType.PICTURE,\n+            ElementType.PAGE_BREAK,\n+            ElementType.TABLE,\n+        ),\n+    )\n+\n+\n+def _merge_extracted_into_inferred_when_almost_the_same(\n+    extracted_layout: LayoutElements,\n+    inferred_layout: LayoutElements,\n+    same_region_threshold: float,\n+) -> tuple[np.ndarray, np.ndarray]:\n+\n+    if len(inferred_layout) == 0:\n+        return np.array([False] * len(extracted_layout))\n+    if len(extracted_layout) == 0:\n+        return np.array([])\n+\n+    boxes_almost_same = boxes_iou(\n+        extracted_layout.element_coords,\n+        inferred_layout.element_coords,\n+        threshold=same_region_threshold,\n+    )\n+    extracted_almost_the_same_as_inferred = boxes_almost_same.sum(axis=1).astype(bool)\n+    # NOTE: if a row is full of False the argmax returns first index; we use the mask above to\n+    # distinguish those (they would be False in the mask)\n+    first_match = np.argmax(boxes_almost_same, axis=1)\n+    inferred_indices_to_update = first_match[extracted_almost_the_same_as_inferred]\n+    extracted_to_remove = extracted_layout.slice(extracted_almost_the_same_as_inferred)\n+    # copy here in case we change the extracted layout later\n+    inferred_layout.texts[inferred_indices_to_update] = extracted_to_remove.texts.copy()\n+    # use coords that can bound BOTH the inferred and extracted region as final bounding box coords\n+    inferred_layout.element_coords[inferred_indices_to_update] = _minimum_containing_coords(\n+        inferred_layout.slice(inferred_indices_to_update),\n+        extracted_to_remove,\n+    )\n+    return extracted_almost_the_same_as_inferred\n+\n+\n+def _merge_extracted_that_are_subregion_of_inferred_text(\n+    extracted_layout,\n+    inferred_layout,\n+    extracted_is_subregion_of_inferred,\n+    extracted_to_proc,\n+    inferred_to_proc,\n+):\n+    # in theory one extracted __should__ only match at most one inferred region, given inferred\n+    # region can not overlap; so first match here __should__ also be the only match\n+    inferred_to_iter = inferred_to_proc[inferred_to_proc]\n+    extracted_to_iter = extracted_to_proc[extracted_to_proc]\n+    for inferred_index, inferred_row in enumerate(extracted_is_subregion_of_inferred.T):\n+        matches = np.where(inferred_row)[0]\n+        if not matches.size:\n+            continue\n+        # Technically those two lines below can be vectorized but this loop would still run anyway;\n+        # it is not clear which one is overall faster so might worth profiling in the future\n+        extracted_to_iter[matches] = False\n+        inferred_to_iter[inferred_index] = False\n+        # then expand inferred box by all the extracted boxes\n+        # FIXME (yao): this part is broken at the moment\n+        inferred_layout.element_coords[[inferred_index]] = _minimum_containing_coords(\n+            inferred_layout.slice([inferred_index]),\n+            *[extracted_layout.slice([match]) for match in matches],\n+        )\n+    inferred_to_proc[inferred_to_proc] = inferred_to_iter\n+    extracted_to_proc[extracted_to_proc] = extracted_to_iter\n+    return inferred_layout\n+\n+\n+def _mark_non_table_inferred_for_removal_if_has_subregion_relationship(\n+    extracted_layout,\n+    inferred_layout,\n+    inferred_to_keep,\n+    subregion_threshold,\n+):\n+    inferred_is_subregion_of_extracted = bboxes1_is_almost_subregion_of_bboxes2(\n+        inferred_layout.element_coords,\n+        extracted_layout.element_coords,\n+        threshold=subregion_threshold,\n+    )\n+    extracted_is_subregion_of_inferred = bboxes1_is_almost_subregion_of_bboxes2(\n+        extracted_layout.element_coords,\n+        inferred_layout.element_coords,\n+        threshold=subregion_threshold,\n+    )\n+    inferred_to_remove_mask = (\n+        np.logical_or(\n+            inferred_is_subregion_of_extracted,\n+            extracted_is_subregion_of_inferred.T,\n+        )\n+        .sum(axis=1)\n+        .astype(bool)\n+    )\n+    # NOTE (yao): maybe we should expand those matching extracted region to contain the inferred\n+    # regions it has subregion relationship with? like we did for inferred regions\n+    inferred_to_keep[inferred_to_remove_mask] = False\n+    return inferred_to_keep\n+\n+\n+@requires_dependencies(\"unstructured_inference\")\n+def array_merge_inferred_layout_with_extracted_layout(\n+    inferred_layout: LayoutElements,\n+    extracted_layout: LayoutElements,\n+    page_image_size: tuple,\n+    same_region_threshold: float = inference_config.LAYOUT_SAME_REGION_THRESHOLD,\n+    subregion_threshold: float = inference_config.LAYOUT_SUBREGION_THRESHOLD,\n+    max_rounds: int = 5,\n+) -> LayoutElements:\n+    \"\"\"merge elements using array data structures; it also returns LayoutElements instead of\n+    collection of LayoutElement\"\"\"\n+    from unstructured_inference.inference.layoutelement import LayoutElements\n+\n+    if len(extracted_layout) == 0:\n+        return inferred_layout\n+    if len(inferred_layout) == 0:\n+        return extracted_layout\n+\n+    w, h = page_image_size\n+    full_page_region = Rectangle(0, 0, w, h)\n+    # ==== RULE 0: Full page extracted images are ignored\n+    # non full page extracted image regions are kept, except when they match a non-text inferred\n+    # region then we use the common bounding boxes and keep just one of the two sets (see rules\n+    # below)\n+    image_indices_to_keep = np.where(extracted_layout.element_class_ids == 1)[0]\n+    if len(image_indices_to_keep):\n+        full_page_image_mask = (\n+            boxes_iou(\n+                extracted_layout.slice(image_indices_to_keep).element_coords,\n+                [full_page_region],\n+                threshold=FULL_PAGE_REGION_THRESHOLD,\n+            )\n+            .sum(axis=1)\n+            .astype(bool)\n+        )\n+        image_indices_to_keep = image_indices_to_keep[~full_page_image_mask]\n+\n+    # ==== RULE 1: any inferred box that is almost the same as an extracted image box, inferred is\n+    # removed\n+    # NOTE (yao): what if od model detects table but pdfminer says image -> we would lose the table\n+    boxes_almost_same = (\n+        boxes_iou(\n+            inferred_layout.element_coords,\n+            extracted_layout.slice(image_indices_to_keep).element_coords,\n+            threshold=same_region_threshold,\n+        )\n+        .sum(axis=1)\n+        .astype(bool)\n+    )\n+\n+    # drop off those matching inferred from processing\n+    inferred_layout_to_proc = inferred_layout.slice(~boxes_almost_same)\n+    inferred_to_keep = np.array([True] * len(inferred_layout_to_proc))\n+\n+    # TODO (yao): experiment with all regions, not just text region, being potential targets to be\n+    # merged into inferred elements\n+    text_element_indices = np.where(extracted_layout.element_class_ids == 0)[0]\n+\n+    if len(text_element_indices) == 0:\n+        return LayoutElements.concatenate(\n+            (\n+                inferred_layout_to_proc,\n+                extracted_layout.slice(image_indices_to_keep),\n+            )\n+        )\n+\n+    if len(inferred_layout_to_proc) == 0:\n+        return extracted_layout.slice(np.concatenate((image_indices_to_keep, text_element_indices)))\n+\n+    extracted_text_layouts = extracted_layout.slice(text_element_indices)\n+    # ==== RULE 2. if there is a inferred region almost the same as the extracted text-region ->\n+    # keep inferred and removed extracted region; here we put more trust in OD model more than\n+    # pdfminer for bounding box",
        "comment_created_at": "2025-02-07T12:36:28+00:00",
        "comment_author": "plutasnyy",
        "comment_body": "Was this decision made for any particular reason? Did we observe worse bboxes from pdfminer? \r\nFYI I know the scope of this change is purely code refactor ;)",
        "pr_file_module": null
      },
      {
        "comment_id": "1946726320",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3900,
        "pr_file": "unstructured/partition/pdf_image/pdfminer_processing.py",
        "discussion_id": "1946463588",
        "commented_code": "@@ -50,6 +52,304 @@ def _validate_bbox(bbox: list[int | float]) -> bool:\n     return all(x is not None for x in bbox) and (bbox[2] - bbox[0] > 0) and (bbox[3] - bbox[1] > 0)\n \n \n+def _minimum_containing_coords(*regions: TextRegions) -> np.ndarray:\n+    # TODO: refactor to just use np array as input\n+    return np.vstack(\n+        (\n+            np.min([region.x1 for region in regions], axis=0),\n+            np.min([region.y1 for region in regions], axis=0),\n+            np.max([region.x2 for region in regions], axis=0),\n+            np.max([region.y2 for region in regions], axis=0),\n+        )\n+    ).T\n+\n+\n+def _inferred_is_elementtype(\n+    inferred_layout: LayoutElements, etypes: Iterable[ElementType]\n+) -> np.ndarry:\n+    inferred_text_idx = [\n+        idx\n+        for idx, class_name in inferred_layout.element_class_id_map.items()\n+        if class_name in etypes\n+    ]\n+    inferred_is_etypes = np.zeros((len(inferred_layout),)).astype(bool)\n+    for idx in inferred_text_idx:\n+        inferred_is_etypes = np.logical_or(\n+            inferred_is_etypes, inferred_layout.element_class_ids == idx\n+        )\n+    return inferred_is_etypes\n+\n+\n+def _inferred_is_text(inferred_layout: LayoutElements) -> np.ndarry:\n+    return ~_inferred_is_elementtype(\n+        inferred_layout,\n+        etypes=(\n+            ElementType.FIGURE,\n+            ElementType.IMAGE,\n+            # NOTE (yao): PICTURE is not in the loop version of the logic in inference library\n+            # ElementType.PICTURE,\n+            ElementType.PAGE_BREAK,\n+            ElementType.TABLE,\n+        ),\n+    )\n+\n+\n+def _merge_extracted_into_inferred_when_almost_the_same(\n+    extracted_layout: LayoutElements,\n+    inferred_layout: LayoutElements,\n+    same_region_threshold: float,\n+) -> tuple[np.ndarray, np.ndarray]:\n+\n+    if len(inferred_layout) == 0:\n+        return np.array([False] * len(extracted_layout))\n+    if len(extracted_layout) == 0:\n+        return np.array([])\n+\n+    boxes_almost_same = boxes_iou(\n+        extracted_layout.element_coords,\n+        inferred_layout.element_coords,\n+        threshold=same_region_threshold,\n+    )\n+    extracted_almost_the_same_as_inferred = boxes_almost_same.sum(axis=1).astype(bool)\n+    # NOTE: if a row is full of False the argmax returns first index; we use the mask above to\n+    # distinguish those (they would be False in the mask)\n+    first_match = np.argmax(boxes_almost_same, axis=1)\n+    inferred_indices_to_update = first_match[extracted_almost_the_same_as_inferred]\n+    extracted_to_remove = extracted_layout.slice(extracted_almost_the_same_as_inferred)\n+    # copy here in case we change the extracted layout later\n+    inferred_layout.texts[inferred_indices_to_update] = extracted_to_remove.texts.copy()\n+    # use coords that can bound BOTH the inferred and extracted region as final bounding box coords\n+    inferred_layout.element_coords[inferred_indices_to_update] = _minimum_containing_coords(\n+        inferred_layout.slice(inferred_indices_to_update),\n+        extracted_to_remove,\n+    )\n+    return extracted_almost_the_same_as_inferred\n+\n+\n+def _merge_extracted_that_are_subregion_of_inferred_text(\n+    extracted_layout,\n+    inferred_layout,\n+    extracted_is_subregion_of_inferred,\n+    extracted_to_proc,\n+    inferred_to_proc,\n+):\n+    # in theory one extracted __should__ only match at most one inferred region, given inferred\n+    # region can not overlap; so first match here __should__ also be the only match\n+    inferred_to_iter = inferred_to_proc[inferred_to_proc]\n+    extracted_to_iter = extracted_to_proc[extracted_to_proc]\n+    for inferred_index, inferred_row in enumerate(extracted_is_subregion_of_inferred.T):\n+        matches = np.where(inferred_row)[0]\n+        if not matches.size:\n+            continue\n+        # Technically those two lines below can be vectorized but this loop would still run anyway;\n+        # it is not clear which one is overall faster so might worth profiling in the future\n+        extracted_to_iter[matches] = False\n+        inferred_to_iter[inferred_index] = False\n+        # then expand inferred box by all the extracted boxes\n+        # FIXME (yao): this part is broken at the moment\n+        inferred_layout.element_coords[[inferred_index]] = _minimum_containing_coords(\n+            inferred_layout.slice([inferred_index]),\n+            *[extracted_layout.slice([match]) for match in matches],\n+        )\n+    inferred_to_proc[inferred_to_proc] = inferred_to_iter\n+    extracted_to_proc[extracted_to_proc] = extracted_to_iter\n+    return inferred_layout\n+\n+\n+def _mark_non_table_inferred_for_removal_if_has_subregion_relationship(\n+    extracted_layout,\n+    inferred_layout,\n+    inferred_to_keep,\n+    subregion_threshold,\n+):\n+    inferred_is_subregion_of_extracted = bboxes1_is_almost_subregion_of_bboxes2(\n+        inferred_layout.element_coords,\n+        extracted_layout.element_coords,\n+        threshold=subregion_threshold,\n+    )\n+    extracted_is_subregion_of_inferred = bboxes1_is_almost_subregion_of_bboxes2(\n+        extracted_layout.element_coords,\n+        inferred_layout.element_coords,\n+        threshold=subregion_threshold,\n+    )\n+    inferred_to_remove_mask = (\n+        np.logical_or(\n+            inferred_is_subregion_of_extracted,\n+            extracted_is_subregion_of_inferred.T,\n+        )\n+        .sum(axis=1)\n+        .astype(bool)\n+    )\n+    # NOTE (yao): maybe we should expand those matching extracted region to contain the inferred\n+    # regions it has subregion relationship with? like we did for inferred regions\n+    inferred_to_keep[inferred_to_remove_mask] = False\n+    return inferred_to_keep\n+\n+\n+@requires_dependencies(\"unstructured_inference\")\n+def array_merge_inferred_layout_with_extracted_layout(\n+    inferred_layout: LayoutElements,\n+    extracted_layout: LayoutElements,\n+    page_image_size: tuple,\n+    same_region_threshold: float = inference_config.LAYOUT_SAME_REGION_THRESHOLD,\n+    subregion_threshold: float = inference_config.LAYOUT_SUBREGION_THRESHOLD,\n+    max_rounds: int = 5,\n+) -> LayoutElements:\n+    \"\"\"merge elements using array data structures; it also returns LayoutElements instead of\n+    collection of LayoutElement\"\"\"\n+    from unstructured_inference.inference.layoutelement import LayoutElements\n+\n+    if len(extracted_layout) == 0:\n+        return inferred_layout\n+    if len(inferred_layout) == 0:\n+        return extracted_layout\n+\n+    w, h = page_image_size\n+    full_page_region = Rectangle(0, 0, w, h)\n+    # ==== RULE 0: Full page extracted images are ignored\n+    # non full page extracted image regions are kept, except when they match a non-text inferred\n+    # region then we use the common bounding boxes and keep just one of the two sets (see rules\n+    # below)\n+    image_indices_to_keep = np.where(extracted_layout.element_class_ids == 1)[0]\n+    if len(image_indices_to_keep):\n+        full_page_image_mask = (\n+            boxes_iou(\n+                extracted_layout.slice(image_indices_to_keep).element_coords,\n+                [full_page_region],\n+                threshold=FULL_PAGE_REGION_THRESHOLD,\n+            )\n+            .sum(axis=1)\n+            .astype(bool)\n+        )\n+        image_indices_to_keep = image_indices_to_keep[~full_page_image_mask]\n+\n+    # ==== RULE 1: any inferred box that is almost the same as an extracted image box, inferred is\n+    # removed\n+    # NOTE (yao): what if od model detects table but pdfminer says image -> we would lose the table\n+    boxes_almost_same = (\n+        boxes_iou(\n+            inferred_layout.element_coords,\n+            extracted_layout.slice(image_indices_to_keep).element_coords,\n+            threshold=same_region_threshold,\n+        )\n+        .sum(axis=1)\n+        .astype(bool)\n+    )\n+\n+    # drop off those matching inferred from processing\n+    inferred_layout_to_proc = inferred_layout.slice(~boxes_almost_same)\n+    inferred_to_keep = np.array([True] * len(inferred_layout_to_proc))\n+\n+    # TODO (yao): experiment with all regions, not just text region, being potential targets to be\n+    # merged into inferred elements\n+    text_element_indices = np.where(extracted_layout.element_class_ids == 0)[0]\n+\n+    if len(text_element_indices) == 0:\n+        return LayoutElements.concatenate(\n+            (\n+                inferred_layout_to_proc,\n+                extracted_layout.slice(image_indices_to_keep),\n+            )\n+        )\n+\n+    if len(inferred_layout_to_proc) == 0:\n+        return extracted_layout.slice(np.concatenate((image_indices_to_keep, text_element_indices)))\n+\n+    extracted_text_layouts = extracted_layout.slice(text_element_indices)\n+    # ==== RULE 2. if there is a inferred region almost the same as the extracted text-region ->\n+    # keep inferred and removed extracted region; here we put more trust in OD model more than\n+    # pdfminer for bounding box",
        "comment_created_at": "2025-02-07T15:37:11+00:00",
        "comment_author": "badGarnet",
        "comment_body": "Not sure to be honest. For the moment we generally observe OD model has pretty good bound boxes and definitely richer ontology of element types so here it is more a statement that: \"we keep OD model's element type\"",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1664376492",
    "pr_number": 3311,
    "pr_file": "unstructured/ingest/v2/processes/connectors/azure_cognitive_search.py",
    "created_at": "2024-07-03T15:20:57+00:00",
    "commented_code": "+import json\n+import multiprocessing as mp\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+from pathlib import Path\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import DestinationConnectionError, WriteError\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.ingest.utils.string_and_date_utils import ensure_isoformat_datetime\n+from unstructured.ingest.v2.interfaces import (\n+    AccessConfig,\n+    ConnectionConfig,\n+    UploadContent,\n+    Uploader,\n+    UploaderConfig,\n+    UploadStager,\n+    UploadStagerConfig,\n+)\n+from unstructured.ingest.v2.logger import logger\n+from unstructured.ingest.v2.processes.connector_registry import (\n+    DestinationRegistryEntry,\n+    add_destination_entry,\n+)\n+from unstructured.utils import requires_dependencies\n+\n+if t.TYPE_CHECKING:\n+    from azure.search.documents import SearchClient\n+\n+\n+CONNECTOR_TYPE = \"azure_cognitive_search\"\n+\n+\n+@dataclass\n+class AzureCognitiveSearchAccessConfig(AccessConfig):\n+    key: t.Optional[str] = enhanced_field(default=None, overload_name=\"azure_cognitive_search_key\")\n+\n+\n+@dataclass\n+class AzureCognitiveSearchConnectionConfig(ConnectionConfig):\n+    endpoint: str\n+    index: str\n+    access_config: AzureCognitiveSearchAccessConfig = enhanced_field(sensitive=True)\n+\n+    @requires_dependencies([\"azure.search\", \"azure.core\"], extras=\"azure-cognitive-search\")\n+    def generate_client(self) -> \"SearchClient\":\n+        from azure.core.credentials import AzureKeyCredential\n+        from azure.search.documents import SearchClient\n+\n+        return SearchClient(\n+            endpoint=self.endpoint,\n+            index_name=self.index,\n+            credential=AzureKeyCredential(self.access_config.key),\n+        )\n+\n+\n+@dataclass\n+class AzureCognitiveSearchUploadStagerConfig(UploadStagerConfig):\n+    pass\n+\n+\n+@dataclass\n+class AzureCognitiveSearchUploaderConfig(UploaderConfig):\n+    batch_size: int = 100\n+    num_processes: int = 1\n+\n+\n+@dataclass\n+class AzureCognitiveSearchUploadStager(UploadStager):\n+    upload_stager_config: AzureCognitiveSearchUploadStagerConfig = field(\n+        default_factory=lambda: AzureCognitiveSearchUploadStagerConfig()\n+    )\n+\n+    @staticmethod\n+    def conform_dict(data: dict) -> dict:\n+        \"\"\"\n+        updates the dictionary that is from each Element being converted into a dict/json\n+        into a dictionary that conforms to the schema expected by the\n+        Azure Cognitive Search index\n+        \"\"\"\n+        from dateutil import parser\n+\n+        data[\"id\"] = str(uuid.uuid4())\n+\n+        if points := data.get(\"metadata\", {}).get(\"coordinates\", {}).get(\"points\"):\n+            data[\"metadata\"][\"coordinates\"][\"points\"] = json.dumps(points)\n+        if version := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"version\"):\n+            data[\"metadata\"][\"data_source\"][\"version\"] = str(version)\n+        if record_locator := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"record_locator\"):\n+            data[\"metadata\"][\"data_source\"][\"record_locator\"] = json.dumps(record_locator)\n+        if permissions_data := (\n+            data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"permissions_data\")\n+        ):\n+            data[\"metadata\"][\"data_source\"][\"permissions_data\"] = json.dumps(permissions_data)\n+        if links := data.get(\"metadata\", {}).get(\"links\"):\n+            data[\"metadata\"][\"links\"] = [json.dumps(link) for link in links]\n+        if last_modified := data.get(\"metadata\", {}).get(\"last_modified\"):\n+            data[\"metadata\"][\"last_modified\"] = parser.parse(",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1664376492",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3311,
        "pr_file": "unstructured/ingest/v2/processes/connectors/azure_cognitive_search.py",
        "discussion_id": "1664376492",
        "commented_code": "@@ -0,0 +1,225 @@\n+import json\n+import multiprocessing as mp\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+from pathlib import Path\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import DestinationConnectionError, WriteError\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.ingest.utils.string_and_date_utils import ensure_isoformat_datetime\n+from unstructured.ingest.v2.interfaces import (\n+    AccessConfig,\n+    ConnectionConfig,\n+    UploadContent,\n+    Uploader,\n+    UploaderConfig,\n+    UploadStager,\n+    UploadStagerConfig,\n+)\n+from unstructured.ingest.v2.logger import logger\n+from unstructured.ingest.v2.processes.connector_registry import (\n+    DestinationRegistryEntry,\n+    add_destination_entry,\n+)\n+from unstructured.utils import requires_dependencies\n+\n+if t.TYPE_CHECKING:\n+    from azure.search.documents import SearchClient\n+\n+\n+CONNECTOR_TYPE = \"azure_cognitive_search\"\n+\n+\n+@dataclass\n+class AzureCognitiveSearchAccessConfig(AccessConfig):\n+    key: t.Optional[str] = enhanced_field(default=None, overload_name=\"azure_cognitive_search_key\")\n+\n+\n+@dataclass\n+class AzureCognitiveSearchConnectionConfig(ConnectionConfig):\n+    endpoint: str\n+    index: str\n+    access_config: AzureCognitiveSearchAccessConfig = enhanced_field(sensitive=True)\n+\n+    @requires_dependencies([\"azure.search\", \"azure.core\"], extras=\"azure-cognitive-search\")\n+    def generate_client(self) -> \"SearchClient\":\n+        from azure.core.credentials import AzureKeyCredential\n+        from azure.search.documents import SearchClient\n+\n+        return SearchClient(\n+            endpoint=self.endpoint,\n+            index_name=self.index,\n+            credential=AzureKeyCredential(self.access_config.key),\n+        )\n+\n+\n+@dataclass\n+class AzureCognitiveSearchUploadStagerConfig(UploadStagerConfig):\n+    pass\n+\n+\n+@dataclass\n+class AzureCognitiveSearchUploaderConfig(UploaderConfig):\n+    batch_size: int = 100\n+    num_processes: int = 1\n+\n+\n+@dataclass\n+class AzureCognitiveSearchUploadStager(UploadStager):\n+    upload_stager_config: AzureCognitiveSearchUploadStagerConfig = field(\n+        default_factory=lambda: AzureCognitiveSearchUploadStagerConfig()\n+    )\n+\n+    @staticmethod\n+    def conform_dict(data: dict) -> dict:\n+        \"\"\"\n+        updates the dictionary that is from each Element being converted into a dict/json\n+        into a dictionary that conforms to the schema expected by the\n+        Azure Cognitive Search index\n+        \"\"\"\n+        from dateutil import parser\n+\n+        data[\"id\"] = str(uuid.uuid4())\n+\n+        if points := data.get(\"metadata\", {}).get(\"coordinates\", {}).get(\"points\"):\n+            data[\"metadata\"][\"coordinates\"][\"points\"] = json.dumps(points)\n+        if version := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"version\"):\n+            data[\"metadata\"][\"data_source\"][\"version\"] = str(version)\n+        if record_locator := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"record_locator\"):\n+            data[\"metadata\"][\"data_source\"][\"record_locator\"] = json.dumps(record_locator)\n+        if permissions_data := (\n+            data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"permissions_data\")\n+        ):\n+            data[\"metadata\"][\"data_source\"][\"permissions_data\"] = json.dumps(permissions_data)\n+        if links := data.get(\"metadata\", {}).get(\"links\"):\n+            data[\"metadata\"][\"links\"] = [json.dumps(link) for link in links]\n+        if last_modified := data.get(\"metadata\", {}).get(\"last_modified\"):\n+            data[\"metadata\"][\"last_modified\"] = parser.parse(",
        "comment_created_at": "2024-07-03T15:20:57+00:00",
        "comment_author": "vangheem",
        "comment_body": "I feel like these sets of datetime formats are a bit awkward.\r\n\r\nThe ensure_isoformat_datetime potential uses the dateutil parse as well, then we convert it back to string and again parse.\r\n\r\nMaybe once this PR is merged: https://github.com/Unstructured-IO/unstructured/pull/3314/files#diff-903a1fa78fa219cd00b0ca836301f9bcaeef3db6e57e30606f687e36bccf17eb\r\n\r\nYou can refactor these date handlers to something like `parse_datetime(last_modified).strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")`",
        "pr_file_module": null
      },
      {
        "comment_id": "1664921833",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3311,
        "pr_file": "unstructured/ingest/v2/processes/connectors/azure_cognitive_search.py",
        "discussion_id": "1664376492",
        "commented_code": "@@ -0,0 +1,225 @@\n+import json\n+import multiprocessing as mp\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+from pathlib import Path\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import DestinationConnectionError, WriteError\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.ingest.utils.string_and_date_utils import ensure_isoformat_datetime\n+from unstructured.ingest.v2.interfaces import (\n+    AccessConfig,\n+    ConnectionConfig,\n+    UploadContent,\n+    Uploader,\n+    UploaderConfig,\n+    UploadStager,\n+    UploadStagerConfig,\n+)\n+from unstructured.ingest.v2.logger import logger\n+from unstructured.ingest.v2.processes.connector_registry import (\n+    DestinationRegistryEntry,\n+    add_destination_entry,\n+)\n+from unstructured.utils import requires_dependencies\n+\n+if t.TYPE_CHECKING:\n+    from azure.search.documents import SearchClient\n+\n+\n+CONNECTOR_TYPE = \"azure_cognitive_search\"\n+\n+\n+@dataclass\n+class AzureCognitiveSearchAccessConfig(AccessConfig):\n+    key: t.Optional[str] = enhanced_field(default=None, overload_name=\"azure_cognitive_search_key\")\n+\n+\n+@dataclass\n+class AzureCognitiveSearchConnectionConfig(ConnectionConfig):\n+    endpoint: str\n+    index: str\n+    access_config: AzureCognitiveSearchAccessConfig = enhanced_field(sensitive=True)\n+\n+    @requires_dependencies([\"azure.search\", \"azure.core\"], extras=\"azure-cognitive-search\")\n+    def generate_client(self) -> \"SearchClient\":\n+        from azure.core.credentials import AzureKeyCredential\n+        from azure.search.documents import SearchClient\n+\n+        return SearchClient(\n+            endpoint=self.endpoint,\n+            index_name=self.index,\n+            credential=AzureKeyCredential(self.access_config.key),\n+        )\n+\n+\n+@dataclass\n+class AzureCognitiveSearchUploadStagerConfig(UploadStagerConfig):\n+    pass\n+\n+\n+@dataclass\n+class AzureCognitiveSearchUploaderConfig(UploaderConfig):\n+    batch_size: int = 100\n+    num_processes: int = 1\n+\n+\n+@dataclass\n+class AzureCognitiveSearchUploadStager(UploadStager):\n+    upload_stager_config: AzureCognitiveSearchUploadStagerConfig = field(\n+        default_factory=lambda: AzureCognitiveSearchUploadStagerConfig()\n+    )\n+\n+    @staticmethod\n+    def conform_dict(data: dict) -> dict:\n+        \"\"\"\n+        updates the dictionary that is from each Element being converted into a dict/json\n+        into a dictionary that conforms to the schema expected by the\n+        Azure Cognitive Search index\n+        \"\"\"\n+        from dateutil import parser\n+\n+        data[\"id\"] = str(uuid.uuid4())\n+\n+        if points := data.get(\"metadata\", {}).get(\"coordinates\", {}).get(\"points\"):\n+            data[\"metadata\"][\"coordinates\"][\"points\"] = json.dumps(points)\n+        if version := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"version\"):\n+            data[\"metadata\"][\"data_source\"][\"version\"] = str(version)\n+        if record_locator := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"record_locator\"):\n+            data[\"metadata\"][\"data_source\"][\"record_locator\"] = json.dumps(record_locator)\n+        if permissions_data := (\n+            data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"permissions_data\")\n+        ):\n+            data[\"metadata\"][\"data_source\"][\"permissions_data\"] = json.dumps(permissions_data)\n+        if links := data.get(\"metadata\", {}).get(\"links\"):\n+            data[\"metadata\"][\"links\"] = [json.dumps(link) for link in links]\n+        if last_modified := data.get(\"metadata\", {}).get(\"last_modified\"):\n+            data[\"metadata\"][\"last_modified\"] = parser.parse(",
        "comment_created_at": "2024-07-04T00:04:55+00:00",
        "comment_author": "ahmetmeleq",
        "comment_body": "These should be addressing our review points! (and some unspoken topics)\r\n\r\n- [duplicate date parsing logic within pull](https://github.com/Unstructured-IO/unstructured/pull/3311/commits/6743634507c6a362b247d8c440eebac272135a16)\r\n- [revert added date parsing logic within string_and_date_utils](https://github.com/Unstructured-IO/unstructured/pull/3311/commits/9cfd0669c77ff54a8f821aa44beb06b1dc1cc244)\r\n- [update batching function name](https://github.com/Unstructured-IO/unstructured/pull/3311/commits/543a76c3346eb3d3f279eebdcf678b6f116eb307)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1742687280",
    "pr_number": 3593,
    "pr_file": "unstructured/partition/pdf_image/pdfminer_processing.py",
    "created_at": "2024-09-03T21:00:46+00:00",
    "commented_code": "\"\"\"\n \n     for page in document.pages:\n-        table_boxes = [e.bbox for e in page.elements if e.type == ElementType.TABLE]\n+        non_pdfminer_element_boxes = [e.bbox for e in page.elements if e.source != Source.PDFMINER]",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1742687280",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3593,
        "pr_file": "unstructured/partition/pdf_image/pdfminer_processing.py",
        "discussion_id": "1742687280",
        "commented_code": "@@ -223,7 +223,7 @@ def clean_pdfminer_inner_elements(document: \"DocumentLayout\") -> \"DocumentLayout\n     \"\"\"\n \n     for page in document.pages:\n-        table_boxes = [e.bbox for e in page.elements if e.type == ElementType.TABLE]\n+        non_pdfminer_element_boxes = [e.bbox for e in page.elements if e.source != Source.PDFMINER]",
        "comment_created_at": "2024-09-03T21:00:46+00:00",
        "comment_author": "MthwRobinson",
        "comment_body": "Is there any way to avoid adding the PDF miner elements to the list in the first place instead of filtering them after the fact? If that's complicated, definitely okay with moving forward with filtering for the bug fix and circling back on that later.",
        "pr_file_module": null
      },
      {
        "comment_id": "1742706606",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3593,
        "pr_file": "unstructured/partition/pdf_image/pdfminer_processing.py",
        "discussion_id": "1742687280",
        "commented_code": "@@ -223,7 +223,7 @@ def clean_pdfminer_inner_elements(document: \"DocumentLayout\") -> \"DocumentLayout\n     \"\"\"\n \n     for page in document.pages:\n-        table_boxes = [e.bbox for e in page.elements if e.type == ElementType.TABLE]\n+        non_pdfminer_element_boxes = [e.bbox for e in page.elements if e.source != Source.PDFMINER]",
        "comment_created_at": "2024-09-03T21:22:51+00:00",
        "comment_author": "christinestraub",
        "comment_body": "I think we can move forward with filtering option right now and then circle back on this in a separate PR? \r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1743648323",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3593,
        "pr_file": "unstructured/partition/pdf_image/pdfminer_processing.py",
        "discussion_id": "1742687280",
        "commented_code": "@@ -223,7 +223,7 @@ def clean_pdfminer_inner_elements(document: \"DocumentLayout\") -> \"DocumentLayout\n     \"\"\"\n \n     for page in document.pages:\n-        table_boxes = [e.bbox for e in page.elements if e.type == ElementType.TABLE]\n+        non_pdfminer_element_boxes = [e.bbox for e in page.elements if e.source != Source.PDFMINER]",
        "comment_created_at": "2024-09-04T12:02:06+00:00",
        "comment_author": "MthwRobinson",
        "comment_body": "Yep sounds good!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1594629036",
    "pr_number": 2977,
    "pr_file": "unstructured/metrics/evaluate.py",
    "created_at": "2024-05-08T20:51:17+00:00",
    "commented_code": "except Exception:\n                 # if any of the output/source file is unable to open, skip the loop\n                 continue\n-            accuracy = round(calculate_accuracy(output_cct, source_cct, weights), 3)\n+            # NOTE(amadeusz): Levenshtein distance calculation takes too long\n+            # skip it if file sizes differ wildly\n+            if 0.5 < len(output_cct.encode()) / len(source_cct.encode()) < 2.0:\n+                accuracy = round(calculate_accuracy(output_cct, source_cct, weights), 3)\n+            else:\n+                # 0.01 to distinguish it was set manually\n+                accuracy = 0.01\n             percent_missing = round(calculate_percent_missing_text(output_cct, source_cct), 3)",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1594629036",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2977,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1594629036",
        "commented_code": "@@ -116,7 +116,13 @@ def measure_text_extraction_accuracy(\n             except Exception:\n                 # if any of the output/source file is unable to open, skip the loop\n                 continue\n-            accuracy = round(calculate_accuracy(output_cct, source_cct, weights), 3)\n+            # NOTE(amadeusz): Levenshtein distance calculation takes too long\n+            # skip it if file sizes differ wildly\n+            if 0.5 < len(output_cct.encode()) / len(source_cct.encode()) < 2.0:\n+                accuracy = round(calculate_accuracy(output_cct, source_cct, weights), 3)\n+            else:\n+                # 0.01 to distinguish it was set manually\n+                accuracy = 0.01\n             percent_missing = round(calculate_percent_missing_text(output_cct, source_cct), 3)",
        "comment_created_at": "2024-05-08T20:51:17+00:00",
        "comment_author": "Klaijan",
        "comment_body": "Do we want to calculate `percent_missing` still, if we are skipping `accuracy`?",
        "pr_file_module": null
      },
      {
        "comment_id": "1594650878",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2977,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1594629036",
        "commented_code": "@@ -116,7 +116,13 @@ def measure_text_extraction_accuracy(\n             except Exception:\n                 # if any of the output/source file is unable to open, skip the loop\n                 continue\n-            accuracy = round(calculate_accuracy(output_cct, source_cct, weights), 3)\n+            # NOTE(amadeusz): Levenshtein distance calculation takes too long\n+            # skip it if file sizes differ wildly\n+            if 0.5 < len(output_cct.encode()) / len(source_cct.encode()) < 2.0:\n+                accuracy = round(calculate_accuracy(output_cct, source_cct, weights), 3)\n+            else:\n+                # 0.01 to distinguish it was set manually\n+                accuracy = 0.01\n             percent_missing = round(calculate_percent_missing_text(output_cct, source_cct), 3)",
        "comment_created_at": "2024-05-08T20:56:57+00:00",
        "comment_author": "amadeusz-ds",
        "comment_body": "We can but for those examples accuracy was always `0.0` but percent_missing wasn't `1.0` so it would change current results.\r\nAnd it's quicker to calculate than accuracy.",
        "pr_file_module": null
      },
      {
        "comment_id": "1594893109",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2977,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1594629036",
        "commented_code": "@@ -116,7 +116,13 @@ def measure_text_extraction_accuracy(\n             except Exception:\n                 # if any of the output/source file is unable to open, skip the loop\n                 continue\n-            accuracy = round(calculate_accuracy(output_cct, source_cct, weights), 3)\n+            # NOTE(amadeusz): Levenshtein distance calculation takes too long\n+            # skip it if file sizes differ wildly\n+            if 0.5 < len(output_cct.encode()) / len(source_cct.encode()) < 2.0:\n+                accuracy = round(calculate_accuracy(output_cct, source_cct, weights), 3)\n+            else:\n+                # 0.01 to distinguish it was set manually\n+                accuracy = 0.01\n             percent_missing = round(calculate_percent_missing_text(output_cct, source_cct), 3)",
        "comment_created_at": "2024-05-09T02:34:49+00:00",
        "comment_author": "Klaijan",
        "comment_body": "Maybe a question for @cragwolfe ?",
        "pr_file_module": null
      },
      {
        "comment_id": "1594894162",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2977,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1594629036",
        "commented_code": "@@ -116,7 +116,13 @@ def measure_text_extraction_accuracy(\n             except Exception:\n                 # if any of the output/source file is unable to open, skip the loop\n                 continue\n-            accuracy = round(calculate_accuracy(output_cct, source_cct, weights), 3)\n+            # NOTE(amadeusz): Levenshtein distance calculation takes too long\n+            # skip it if file sizes differ wildly\n+            if 0.5 < len(output_cct.encode()) / len(source_cct.encode()) < 2.0:\n+                accuracy = round(calculate_accuracy(output_cct, source_cct, weights), 3)\n+            else:\n+                # 0.01 to distinguish it was set manually\n+                accuracy = 0.01\n             percent_missing = round(calculate_percent_missing_text(output_cct, source_cct), 3)",
        "comment_created_at": "2024-05-09T02:37:24+00:00",
        "comment_author": "cragwolfe",
        "comment_body": "yeah, assuming it is much quicker to calculate than accuracy, i'm good to leave `percent_missing` as is",
        "pr_file_module": null
      },
      {
        "comment_id": "1594894782",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2977,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1594629036",
        "commented_code": "@@ -116,7 +116,13 @@ def measure_text_extraction_accuracy(\n             except Exception:\n                 # if any of the output/source file is unable to open, skip the loop\n                 continue\n-            accuracy = round(calculate_accuracy(output_cct, source_cct, weights), 3)\n+            # NOTE(amadeusz): Levenshtein distance calculation takes too long\n+            # skip it if file sizes differ wildly\n+            if 0.5 < len(output_cct.encode()) / len(source_cct.encode()) < 2.0:\n+                accuracy = round(calculate_accuracy(output_cct, source_cct, weights), 3)\n+            else:\n+                # 0.01 to distinguish it was set manually\n+                accuracy = 0.01\n             percent_missing = round(calculate_percent_missing_text(output_cct, source_cct), 3)",
        "comment_created_at": "2024-05-09T02:38:36+00:00",
        "comment_author": "Klaijan",
        "comment_body": "Thank you. Then this looks good to me!",
        "pr_file_module": null
      }
    ]
  }
]