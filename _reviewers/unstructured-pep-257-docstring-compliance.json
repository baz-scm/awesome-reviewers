[
  {
    "discussion_id": "1550464943",
    "pr_number": 2700,
    "pr_file": "unstructured/documents/elements.py",
    "created_at": "2024-04-03T20:48:59+00:00",
    "commented_code": "UNCATEGORIZED_TEXT = \"UncategorizedText\"\n     NARRATIVE_TEXT = \"NarrativeText\"\n     BULLETED_TEXT = \"BulletedText\"\n+    PARAGRAPH = \"Paragraph\"",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1550464943",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2700,
        "pr_file": "unstructured/documents/elements.py",
        "discussion_id": "1550464943",
        "commented_code": "@@ -597,6 +597,7 @@ class ElementType:\n     UNCATEGORIZED_TEXT = \"UncategorizedText\"\n     NARRATIVE_TEXT = \"NarrativeText\"\n     BULLETED_TEXT = \"BulletedText\"\n+    PARAGRAPH = \"Paragraph\"",
        "comment_created_at": "2024-04-03T20:48:59+00:00",
        "comment_author": "scanny",
        "comment_body": "Can we add a docstring for `ElementType`? I know you didn't add it in this PR, but can we just state what its semantics are? A couple questions I have:\r\n- What scope of \"elements\" does this \"typing\" apply to? Is it `Element` and its subtypes? Does it include `LayoutElement` objects?\r\n- What is this mapping intended to be used for?\r\n- Who decides what `ElementType` an \"element\" has?\r\n- I see this used in tests as roughly an enumeration of possible values for `Element.category`. What is the difference between this and `Element.category`? Is this really `ElementCategories` with a different name?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1927555514",
    "pr_number": 3886,
    "pr_file": "unstructured/file_utils/ndjson.py",
    "created_at": "2025-01-23T19:12:17+00:00",
    "commented_code": "+import json",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1927555514",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3886,
        "pr_file": "unstructured/file_utils/ndjson.py",
        "discussion_id": "1927555514",
        "commented_code": "@@ -0,0 +1,24 @@\n+import json",
        "comment_created_at": "2025-01-23T19:12:17+00:00",
        "comment_author": "scanny",
        "comment_body": "In my view every module should have a docstring answering the question \"What would I want to know about this module if I were encountering for the first time?\"\r\n\r\nI can think of a few topics:\r\n- Why do we need `ndjson`?\r\n- Is it something we partition?\r\n- Who's using it now?\r\n- How robust is this?\r\n- Why didn't we use a standard library for this?\r\n\r\nPerhaps some others will occur to you.\r\n\r\nIt's up to each of us to keep the code-base understandable for all those who end up here and using prose to expose the non-obvious aspects is an important part of that in my view.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1809905348",
    "pr_number": 3732,
    "pr_file": "unstructured/documents/mappings.py",
    "created_at": "2024-10-22T04:36:53+00:00",
    "commented_code": "+from collections import defaultdict",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1809905348",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3732,
        "pr_file": "unstructured/documents/mappings.py",
        "discussion_id": "1809905348",
        "commented_code": "@@ -0,0 +1,152 @@\n+from collections import defaultdict",
        "comment_created_at": "2024-10-22T04:36:53+00:00",
        "comment_author": "cragwolfe",
        "comment_body": "please add docstring, what is this module for?",
        "pr_file_module": null
      },
      {
        "comment_id": "1810710128",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3732,
        "pr_file": "unstructured/documents/mappings.py",
        "discussion_id": "1809905348",
        "commented_code": "@@ -0,0 +1,152 @@\n+from collections import defaultdict",
        "comment_created_at": "2024-10-22T13:16:25+00:00",
        "comment_author": "plutasnyy",
        "comment_body": "done\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1669128286",
    "pr_number": 3361,
    "pr_file": "unstructured/nlp/tokenize.py",
    "created_at": "2024-07-08T18:54:52+00:00",
    "commented_code": "CACHE_MAX_SIZE: Final[int] = 128\n \n+NLTK_DATA_URL = \"https://utic-public-cf.s3.amazonaws.com/nltk_data.tgz\"\n+NLTK_DATA_SHA256 = \"126faf671cd255a062c436b3d0f2d311dfeefcd92ffa43f7c3ab677309404d61\"\n+\n+\n+def _raise_on_nltk_download(*args, **kwargs):\n+    raise ValueError(\"NLTK download disabled. See CVE-2024-39705\")\n+\n+\n+nltk.download = _raise_on_nltk_download\n+\n+\n+# NOTE(robinson) - mimic default dir logic from NLTK\n+# https://github.com/nltk/nltk/\n+# \tblob/8c233dc585b91c7a0c58f96a9d99244a379740d5/nltk/downloader.py#L1046\n+def get_nltk_data_dir():",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1669128286",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3361,
        "pr_file": "unstructured/nlp/tokenize.py",
        "discussion_id": "1669128286",
        "commented_code": "@@ -14,37 +19,121 @@\n \n CACHE_MAX_SIZE: Final[int] = 128\n \n+NLTK_DATA_URL = \"https://utic-public-cf.s3.amazonaws.com/nltk_data.tgz\"\n+NLTK_DATA_SHA256 = \"126faf671cd255a062c436b3d0f2d311dfeefcd92ffa43f7c3ab677309404d61\"\n+\n+\n+def _raise_on_nltk_download(*args, **kwargs):\n+    raise ValueError(\"NLTK download disabled. See CVE-2024-39705\")\n+\n+\n+nltk.download = _raise_on_nltk_download\n+\n+\n+# NOTE(robinson) - mimic default dir logic from NLTK\n+# https://github.com/nltk/nltk/\n+# \tblob/8c233dc585b91c7a0c58f96a9d99244a379740d5/nltk/downloader.py#L1046\n+def get_nltk_data_dir():",
        "comment_created_at": "2024-07-08T18:54:52+00:00",
        "comment_author": "scanny",
        "comment_body": "A docstring would be welcome here. Also type-annotation (`-> str | None`) as that's a critical detail for this function.\r\n\r\nA good docstring (in my view) states the _contract_ of the function (not how it fulfills it, like what it \"does\"), along with any caveats. Basically, as a \"black-box\", what does this function _promise_?\r\n\r\nSo in this case perhaps something like:\r\n```python\r\n    \"\"\"The directory where `nltk` resources are located.\r\n\r\n    `None` when the location is not writable.\r\n    \"\"\"\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1669164258",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3361,
        "pr_file": "unstructured/nlp/tokenize.py",
        "discussion_id": "1669128286",
        "commented_code": "@@ -14,37 +19,121 @@\n \n CACHE_MAX_SIZE: Final[int] = 128\n \n+NLTK_DATA_URL = \"https://utic-public-cf.s3.amazonaws.com/nltk_data.tgz\"\n+NLTK_DATA_SHA256 = \"126faf671cd255a062c436b3d0f2d311dfeefcd92ffa43f7c3ab677309404d61\"\n+\n+\n+def _raise_on_nltk_download(*args, **kwargs):\n+    raise ValueError(\"NLTK download disabled. See CVE-2024-39705\")\n+\n+\n+nltk.download = _raise_on_nltk_download\n+\n+\n+# NOTE(robinson) - mimic default dir logic from NLTK\n+# https://github.com/nltk/nltk/\n+# \tblob/8c233dc585b91c7a0c58f96a9d99244a379740d5/nltk/downloader.py#L1046\n+def get_nltk_data_dir():",
        "comment_created_at": "2024-07-08T19:20:04+00:00",
        "comment_author": "MthwRobinson",
        "comment_body": "Docstring added.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1603240668",
    "pr_number": 3034,
    "pr_file": "unstructured/documents/form_utils.py",
    "created_at": "2024-05-16T12:23:15+00:00",
    "commented_code": "+from __future__ import annotations\n+\n+import copy\n+\n+from unstructured.documents.elements import FormKeyValuePair\n+\n+\n+def _kvform_rehydrate_internal_elements(kv_pairs: list[dict]) -> list[FormKeyValuePair]:",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1603240668",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3034,
        "pr_file": "unstructured/documents/form_utils.py",
        "discussion_id": "1603240668",
        "commented_code": "@@ -0,0 +1,32 @@\n+from __future__ import annotations\n+\n+import copy\n+\n+from unstructured.documents.elements import FormKeyValuePair\n+\n+\n+def _kvform_rehydrate_internal_elements(kv_pairs: list[dict]) -> list[FormKeyValuePair]:",
        "comment_created_at": "2024-05-16T12:23:15+00:00",
        "comment_author": "MthwRobinson",
        "comment_body": "Could you add doc strings to this function explaining what it does and where it's used? Also, since it's an internal function and is only used in `element.py`, and reason not to put it in `elements.py` instead of splitting it into a new moduel?",
        "pr_file_module": null
      },
      {
        "comment_id": "1603258365",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3034,
        "pr_file": "unstructured/documents/form_utils.py",
        "discussion_id": "1603240668",
        "commented_code": "@@ -0,0 +1,32 @@\n+from __future__ import annotations\n+\n+import copy\n+\n+from unstructured.documents.elements import FormKeyValuePair\n+\n+\n+def _kvform_rehydrate_internal_elements(kv_pairs: list[dict]) -> list[FormKeyValuePair]:",
        "comment_created_at": "2024-05-16T12:31:02+00:00",
        "comment_author": "MillCheck",
        "comment_body": "I thought it made the already code-heavy elements.py into something even harder to navigate - otherwise no reason to have it in a separate file.",
        "pr_file_module": null
      },
      {
        "comment_id": "1603283109",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3034,
        "pr_file": "unstructured/documents/form_utils.py",
        "discussion_id": "1603240668",
        "commented_code": "@@ -0,0 +1,32 @@\n+from __future__ import annotations\n+\n+import copy\n+\n+from unstructured.documents.elements import FormKeyValuePair\n+\n+\n+def _kvform_rehydrate_internal_elements(kv_pairs: list[dict]) -> list[FormKeyValuePair]:",
        "comment_created_at": "2024-05-16T12:48:09+00:00",
        "comment_author": "MillCheck",
        "comment_body": "Moved back to elements.py",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1540159982",
    "pr_number": 2673,
    "pr_file": "unstructured/documents/elements.py",
    "created_at": "2024-03-26T22:00:05+00:00",
    "commented_code": "self.embeddings: Optional[list[float]] = embeddings\n \n         if isinstance(element_id, NoID):\n-            # NOTE(robinson) - Cut the SHA256 hex in half to get the first 128 bits\n-            element_id = hashlib.sha256(text.encode()).hexdigest()[:32]\n-\n+            self._id = self.calculate_hash(metadata)\n         elif isinstance(element_id, UUID):\n-            element_id = str(uuid.uuid4())\n+            self._id = uuid.uuid4()\n+        elif isinstance(element_id, str):\n+            self._id = element_id\n+        else:\n+            raise ValueError(\"ID must be a string, UUID, or NoID\")\n \n         super().__init__(\n-            element_id=element_id,\n+            element_id=self._id,\n             metadata=metadata,\n             coordinates=coordinates,\n             coordinate_system=coordinate_system,\n             detection_origin=detection_origin,\n         )\n \n+    def calculate_hash(\n+        self, metadata: Optional[ElementMetadata] = None, other: Any = None\n+    ) -> HashValue:\n+        \"\"\"Calculate the hash depending on what element data is available.\n+\n+        Args:\n+            metadata - if provided, it will be included in the hash calculation.\n+            other - any other data that should be included in the hash calculation.\n+                Must implement __str__.\n+\n+        Returns:\n+            HashValue - 128-bit hash value of the element.\n+        \"\"\"\n+        if metadata is not None:\n+            # Metadata supported by PDF and HTML files\n+            data = f\"{self.text}{metadata.page_number}{metadata.index_on_page}{other}\"\n+        else:\n+            data = f\"{self.text}{other}\"\n+\n+        return HashValue(hashlib.sha256(data.encode()).hexdigest()[:32])\n+\n+    @property\n+    def id(self) -> str:",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1540159982",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2673,
        "pr_file": "unstructured/documents/elements.py",
        "discussion_id": "1540159982",
        "commented_code": "@@ -772,20 +786,61 @@ def __init__(\n         self.embeddings: Optional[list[float]] = embeddings\n \n         if isinstance(element_id, NoID):\n-            # NOTE(robinson) - Cut the SHA256 hex in half to get the first 128 bits\n-            element_id = hashlib.sha256(text.encode()).hexdigest()[:32]\n-\n+            self._id = self.calculate_hash(metadata)\n         elif isinstance(element_id, UUID):\n-            element_id = str(uuid.uuid4())\n+            self._id = uuid.uuid4()\n+        elif isinstance(element_id, str):\n+            self._id = element_id\n+        else:\n+            raise ValueError(\"ID must be a string, UUID, or NoID\")\n \n         super().__init__(\n-            element_id=element_id,\n+            element_id=self._id,\n             metadata=metadata,\n             coordinates=coordinates,\n             coordinate_system=coordinate_system,\n             detection_origin=detection_origin,\n         )\n \n+    def calculate_hash(\n+        self, metadata: Optional[ElementMetadata] = None, other: Any = None\n+    ) -> HashValue:\n+        \"\"\"Calculate the hash depending on what element data is available.\n+\n+        Args:\n+            metadata - if provided, it will be included in the hash calculation.\n+            other - any other data that should be included in the hash calculation.\n+                Must implement __str__.\n+\n+        Returns:\n+            HashValue - 128-bit hash value of the element.\n+        \"\"\"\n+        if metadata is not None:\n+            # Metadata supported by PDF and HTML files\n+            data = f\"{self.text}{metadata.page_number}{metadata.index_on_page}{other}\"\n+        else:\n+            data = f\"{self.text}{other}\"\n+\n+        return HashValue(hashlib.sha256(data.encode()).hexdigest()[:32])\n+\n+    @property\n+    def id(self) -> str:",
        "comment_created_at": "2024-03-26T22:00:05+00:00",
        "comment_author": "scanny",
        "comment_body": "All methods require a docstring.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1571334757",
    "pr_number": 2897,
    "pr_file": "unstructured/partition/pdf_image/pdfminer_processing.py",
    "created_at": "2024-04-18T20:29:26+00:00",
    "commented_code": "inferred_page.elements[:] = elements\n \n     return inferred_document_layout\n+\n+\n+@requires_dependencies(\"unstructured_inference\")\n+def clean_pdfminer_inner_elements(document: \"DocumentLayout\") -> \"DocumentLayout\":\n+    \"\"\"Clean pdfminer elements from inside tables and stores them in extra_info dictionary\n+    with the table id as key\"\"\"",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1571334757",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2897,
        "pr_file": "unstructured/partition/pdf_image/pdfminer_processing.py",
        "discussion_id": "1571334757",
        "commented_code": "@@ -134,3 +137,55 @@ def merge_inferred_with_extracted_layout(\n         inferred_page.elements[:] = elements\n \n     return inferred_document_layout\n+\n+\n+@requires_dependencies(\"unstructured_inference\")\n+def clean_pdfminer_inner_elements(document: \"DocumentLayout\") -> \"DocumentLayout\":\n+    \"\"\"Clean pdfminer elements from inside tables and stores them in extra_info dictionary\n+    with the table id as key\"\"\"",
        "comment_created_at": "2024-04-18T20:29:26+00:00",
        "comment_author": "scanny",
        "comment_body": "We adhere to PEP 257 through the rest of the codebase, at least any part that I or John touch for some reason. I think it's a good convention and it's widely adopted (probably because it's \"official\").\r\n\r\nThat would make this docstring something like:\r\n```python\r\n\"\"\"Move pdfminer elements from inside tables and to the extra_info dictionary.\r\n\r\nEach element appears in the `extra_info` dictionary using its table id as the key.\r\n\"\"\"\r\n```\r\n\r\nThe tldr; on PEP 257 is:\r\n- always triple double-quotes\r\n- first line must fit on one line, with trailing triple-quotes if the docstring is a one-liner; should be complete sentence. This is what appears in IDE when you hover.\r\n- When there is more, it is separated by a blank line from the summary line.\r\n- Trailing triple-quotes go on a separate line when the docstring has multiple/\"further-details\" lines.\r\n\r\nYou can see a lot of examples around this location: https://github.com/Unstructured-IO/unstructured/blob/main/unstructured/chunking/base.py#L118",
        "pr_file_module": null
      },
      {
        "comment_id": "1571405981",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2897,
        "pr_file": "unstructured/partition/pdf_image/pdfminer_processing.py",
        "discussion_id": "1571334757",
        "commented_code": "@@ -134,3 +137,55 @@ def merge_inferred_with_extracted_layout(\n         inferred_page.elements[:] = elements\n \n     return inferred_document_layout\n+\n+\n+@requires_dependencies(\"unstructured_inference\")\n+def clean_pdfminer_inner_elements(document: \"DocumentLayout\") -> \"DocumentLayout\":\n+    \"\"\"Clean pdfminer elements from inside tables and stores them in extra_info dictionary\n+    with the table id as key\"\"\"",
        "comment_created_at": "2024-04-18T21:30:16+00:00",
        "comment_author": "cragwolfe",
        "comment_body": "could this be a lint thing rather than enforced-by-human? i think this pattern exists all over the place in the repo but sure, this one could be updated. [imo, not a high prioirity thing]",
        "pr_file_module": null
      },
      {
        "comment_id": "1571463080",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2897,
        "pr_file": "unstructured/partition/pdf_image/pdfminer_processing.py",
        "discussion_id": "1571334757",
        "commented_code": "@@ -134,3 +137,55 @@ def merge_inferred_with_extracted_layout(\n         inferred_page.elements[:] = elements\n \n     return inferred_document_layout\n+\n+\n+@requires_dependencies(\"unstructured_inference\")\n+def clean_pdfminer_inner_elements(document: \"DocumentLayout\") -> \"DocumentLayout\":\n+    \"\"\"Clean pdfminer elements from inside tables and stores them in extra_info dictionary\n+    with the table id as key\"\"\"",
        "comment_created_at": "2024-04-18T22:35:30+00:00",
        "comment_author": "scanny",
        "comment_body": "Doesn't work well as a lint thing unfortunately. Docstrings are prose so such a linter that was really effective would need a language model and no one has come up with that yet as far as I know.\r\n\r\nAs far as priority, I just change ones I happen to be working with and always use PEP 257 in new code. I don't try to do any wholesale updating. That satisfies me. Boy scout rule basically (always leave the code cleaner than you found it) :)\r\n\r\nI mentioned it here because this is new code.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1496431434",
    "pr_number": 2554,
    "pr_file": "unstructured/utils.py",
    "created_at": "2024-02-20T20:11:55+00:00",
    "commented_code": "return True\n \n \n-# Copied from unstructured/ingest/connector/biomed.py\n-def validate_date_args(date: Optional[str] = None):\n+def validate_date_args(date: Optional[str] = None) -> bool:\n+    \"\"\"Validate whether the provided date string satisfies any of the supported date formats.",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1496431434",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2554,
        "pr_file": "unstructured/utils.py",
        "discussion_id": "1496431434",
        "commented_code": "@@ -228,8 +235,19 @@ def dependency_exists(dependency: str):\n     return True\n \n \n-# Copied from unstructured/ingest/connector/biomed.py\n-def validate_date_args(date: Optional[str] = None):\n+def validate_date_args(date: Optional[str] = None) -> bool:\n+    \"\"\"Validate whether the provided date string satisfies any of the supported date formats.",
        "comment_created_at": "2024-02-20T20:11:55+00:00",
        "comment_author": "scanny",
        "comment_body": "PEP 257 specifies a blank line between first line and any continuation text.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1496439809",
    "pr_number": 2554,
    "pr_file": "unstructured/utils.py",
    "created_at": "2024-02-20T20:18:03+00:00",
    "commented_code": "first_string: str,\n     second_string: str,\n     n: int,\n-) -> (float, List):\n+) -> tuple[float, set[tuple[str, ...]]]:\n     \"\"\"Calculate the percentage of common_ngrams between string_A and string_B\n     with reference to the total number of ngrams in string_A\"\"\"\n \n     if not n:\n-        return 0, {}\n+        return 0, set()\n     first_string_ngrams = ngrams(first_string.split(), n)\n     second_string_ngrams = ngrams(second_string.split(), n)\n \n     if not first_string_ngrams:\n-        return 0\n+        return 0, set()\n \n     common_ngrams = set(first_string_ngrams) & set(second_string_ngrams)\n     percentage = (len(common_ngrams) / len(first_string_ngrams)) * 100\n     return percentage, common_ngrams\n \n \n-def calculate_largest_ngram_percentage(first_string: str, second_string: str) -> (float, List, str):\n+def calculate_largest_ngram_percentage(\n+    first_string: str,\n+    second_string: str,\n+) -> tuple[float, set[tuple[str, ...]], str]:\n     \"\"\"Iteratively calculate_shared_ngram_percentage starting from the biggest\n-    ngram possible until is >0.0%\"\"\"\n+    ngram possible until is >0.0%",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1496439809",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2554,
        "pr_file": "unstructured/utils.py",
        "discussion_id": "1496439809",
        "commented_code": "@@ -334,28 +352,37 @@ def calculate_shared_ngram_percentage(\n     first_string: str,\n     second_string: str,\n     n: int,\n-) -> (float, List):\n+) -> tuple[float, set[tuple[str, ...]]]:\n     \"\"\"Calculate the percentage of common_ngrams between string_A and string_B\n     with reference to the total number of ngrams in string_A\"\"\"\n \n     if not n:\n-        return 0, {}\n+        return 0, set()\n     first_string_ngrams = ngrams(first_string.split(), n)\n     second_string_ngrams = ngrams(second_string.split(), n)\n \n     if not first_string_ngrams:\n-        return 0\n+        return 0, set()\n \n     common_ngrams = set(first_string_ngrams) & set(second_string_ngrams)\n     percentage = (len(common_ngrams) / len(first_string_ngrams)) * 100\n     return percentage, common_ngrams\n \n \n-def calculate_largest_ngram_percentage(first_string: str, second_string: str) -> (float, List, str):\n+def calculate_largest_ngram_percentage(\n+    first_string: str,\n+    second_string: str,\n+) -> tuple[float, set[tuple[str, ...]], str]:\n     \"\"\"Iteratively calculate_shared_ngram_percentage starting from the biggest\n-    ngram possible until is >0.0%\"\"\"\n+    ngram possible until is >0.0%",
        "comment_created_at": "2024-02-20T20:18:03+00:00",
        "comment_author": "scanny",
        "comment_body": "remove text from the sentence until it fits. \"Iteratively\" is a good first candidate, then \"calculate\" since that's obvious.\r\n\r\nThe docstring should describe what to expect and how to use the function, now how it's implemented. So describe the result, like \"All shared-ngram percentages from biggest to ...\".\r\n\r\nWhat is the `>0.0%' bit? Aren't all percentages greater than zero?\r\n\r\nIf you can't fit a truly descriptive first sentence, make it an overview like \"n-gram profile for ...\" or something and let the continuation text do the heavy lifting. Just don't fall into the trap of restating the function name in longer words :)",
        "pr_file_module": null
      },
      {
        "comment_id": "1496548927",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2554,
        "pr_file": "unstructured/utils.py",
        "discussion_id": "1496439809",
        "commented_code": "@@ -334,28 +352,37 @@ def calculate_shared_ngram_percentage(\n     first_string: str,\n     second_string: str,\n     n: int,\n-) -> (float, List):\n+) -> tuple[float, set[tuple[str, ...]]]:\n     \"\"\"Calculate the percentage of common_ngrams between string_A and string_B\n     with reference to the total number of ngrams in string_A\"\"\"\n \n     if not n:\n-        return 0, {}\n+        return 0, set()\n     first_string_ngrams = ngrams(first_string.split(), n)\n     second_string_ngrams = ngrams(second_string.split(), n)\n \n     if not first_string_ngrams:\n-        return 0\n+        return 0, set()\n \n     common_ngrams = set(first_string_ngrams) & set(second_string_ngrams)\n     percentage = (len(common_ngrams) / len(first_string_ngrams)) * 100\n     return percentage, common_ngrams\n \n \n-def calculate_largest_ngram_percentage(first_string: str, second_string: str) -> (float, List, str):\n+def calculate_largest_ngram_percentage(\n+    first_string: str,\n+    second_string: str,\n+) -> tuple[float, set[tuple[str, ...]], str]:\n     \"\"\"Iteratively calculate_shared_ngram_percentage starting from the biggest\n-    ngram possible until is >0.0%\"\"\"\n+    ngram possible until is >0.0%",
        "comment_created_at": "2024-02-20T21:36:22+00:00",
        "comment_author": "Coniferish",
        "comment_body": "> What is the `>0.0%' bit? Aren't all percentages greater than zero?\r\n\r\nIt looks like `n` is initialized as the number of words in the longer of the two strings and decremented until it reaches 0 or there's a common word/n-gram (and the percentage is greater than 0).\r\n\r\nI updated the first sentence and added the implementation explanation as a comment",
        "pr_file_module": null
      },
      {
        "comment_id": "1496617321",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2554,
        "pr_file": "unstructured/utils.py",
        "discussion_id": "1496439809",
        "commented_code": "@@ -334,28 +352,37 @@ def calculate_shared_ngram_percentage(\n     first_string: str,\n     second_string: str,\n     n: int,\n-) -> (float, List):\n+) -> tuple[float, set[tuple[str, ...]]]:\n     \"\"\"Calculate the percentage of common_ngrams between string_A and string_B\n     with reference to the total number of ngrams in string_A\"\"\"\n \n     if not n:\n-        return 0, {}\n+        return 0, set()\n     first_string_ngrams = ngrams(first_string.split(), n)\n     second_string_ngrams = ngrams(second_string.split(), n)\n \n     if not first_string_ngrams:\n-        return 0\n+        return 0, set()\n \n     common_ngrams = set(first_string_ngrams) & set(second_string_ngrams)\n     percentage = (len(common_ngrams) / len(first_string_ngrams)) * 100\n     return percentage, common_ngrams\n \n \n-def calculate_largest_ngram_percentage(first_string: str, second_string: str) -> (float, List, str):\n+def calculate_largest_ngram_percentage(\n+    first_string: str,\n+    second_string: str,\n+) -> tuple[float, set[tuple[str, ...]], str]:\n     \"\"\"Iteratively calculate_shared_ngram_percentage starting from the biggest\n-    ngram possible until is >0.0%\"\"\"\n+    ngram possible until is >0.0%",
        "comment_created_at": "2024-02-20T22:27:07+00:00",
        "comment_author": "scanny",
        "comment_body": "k, sounds like the right move :)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1496922570",
    "pr_number": 2554,
    "pr_file": "unstructured/utils.py",
    "created_at": "2024-02-21T05:14:47+00:00",
    "commented_code": "def only(it: Iterable[Any]) -> Any:\n-    \"\"\"Returns the only element from a singleton iterable. Raises an error if the iterable is not a\n-    singleton.\"\"\"\n+    \"\"\"Returns the only element from a singleton iterable.\n+\n+    Raises an error if the iterable is not a singleton.\"\"\"",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1496922570",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2554,
        "pr_file": "unstructured/utils.py",
        "discussion_id": "1496922570",
        "commented_code": "@@ -283,8 +282,9 @@ def first(it: Iterable[Any]) -> Any:\n \n \n def only(it: Iterable[Any]) -> Any:\n-    \"\"\"Returns the only element from a singleton iterable. Raises an error if the iterable is not a\n-    singleton.\"\"\"\n+    \"\"\"Returns the only element from a singleton iterable.\n+\n+    Raises an error if the iterable is not a singleton.\"\"\"",
        "comment_created_at": "2024-02-21T05:14:47+00:00",
        "comment_author": "scanny",
        "comment_body": "PEP 257 specifies last \"\"\" goes on new line in the multi-line form. There are a few more of these below.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1499686331",
    "pr_number": 2572,
    "pr_file": "unstructured/metrics/evaluate.py",
    "created_at": "2024-02-22T18:03:11+00:00",
    "commented_code": "_write_to_file(export_dir, \"all-docs-element-type-frequency.tsv\", df)\n     _write_to_file(export_dir, \"aggregate-scores-element-type.tsv\", agg_df)\n     _display(agg_df)\n+\n+\n+def group_text_extraction_accuracy(\n+    grouping: str, data_input: Union[pd.DataFrame, str], export_dir: str\n+) -> None:\n+    \"\"\"\n+    Aggregates accuracy and missing metrics by 'doctype' or 'connector', exporting to TSV.\n+\n+    Args:\n+        grouping (str): Grouping category ('doctype' or 'connector').\n+        data_input (Union[pd.DataFrame, str]): DataFrame or path to a CSV/TSV file.\n+        export_dir (str): Directory for the exported TSV file.\n+    \"\"\"",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1499686331",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2572,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1499686331",
        "commented_code": "@@ -187,3 +177,46 @@ def measure_element_type_accuracy(\n     _write_to_file(export_dir, \"all-docs-element-type-frequency.tsv\", df)\n     _write_to_file(export_dir, \"aggregate-scores-element-type.tsv\", agg_df)\n     _display(agg_df)\n+\n+\n+def group_text_extraction_accuracy(\n+    grouping: str, data_input: Union[pd.DataFrame, str], export_dir: str\n+) -> None:\n+    \"\"\"\n+    Aggregates accuracy and missing metrics by 'doctype' or 'connector', exporting to TSV.\n+\n+    Args:\n+        grouping (str): Grouping category ('doctype' or 'connector').\n+        data_input (Union[pd.DataFrame, str]): DataFrame or path to a CSV/TSV file.\n+        export_dir (str): Directory for the exported TSV file.\n+    \"\"\"",
        "comment_created_at": "2024-02-22T18:03:11+00:00",
        "comment_author": "scanny",
        "comment_body": "Docstrings should be [PEP 257](https://peps.python.org/pep-0257/) compliant, like this in this case:\r\n```python\r\n    \"\"\"Aggregates accuracy and missing metrics by 'doctype' or 'connector', exporting to TSV.\r\n\r\n    Args:\r\n        grouping (str): Grouping category ('doctype' or 'connector').\r\n        data_input (Union[pd.DataFrame, str]): DataFrame or path to a CSV/TSV file.\r\n        export_dir (str): Directory for the exported TSV file.\r\n    \"\"\"\r\n```\r\n\r\nI encourage you to read the PEP, but the basic idea is:\r\n- first line begins immediately after the \"\"\" and extends for no more than one line.\r\n- second line is blank\r\n- remaining lines as needed\r\n- closing \"\"\" is on separate line, aligned at same indent level as first \"\"\"\r\n\r\nIf only a single line docstring is needed it must fit on one line, like:\r\n\r\n```python\r\n    \"\"\"Aggregate accuracy and missing metrics by 'doctype' or 'connector'.\"\"\"\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1500192808",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2572,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1499686331",
        "commented_code": "@@ -187,3 +177,46 @@ def measure_element_type_accuracy(\n     _write_to_file(export_dir, \"all-docs-element-type-frequency.tsv\", df)\n     _write_to_file(export_dir, \"aggregate-scores-element-type.tsv\", agg_df)\n     _display(agg_df)\n+\n+\n+def group_text_extraction_accuracy(\n+    grouping: str, data_input: Union[pd.DataFrame, str], export_dir: str\n+) -> None:\n+    \"\"\"\n+    Aggregates accuracy and missing metrics by 'doctype' or 'connector', exporting to TSV.\n+\n+    Args:\n+        grouping (str): Grouping category ('doctype' or 'connector').\n+        data_input (Union[pd.DataFrame, str]): DataFrame or path to a CSV/TSV file.\n+        export_dir (str): Directory for the exported TSV file.\n+    \"\"\"",
        "comment_created_at": "2024-02-23T04:08:20+00:00",
        "comment_author": "Klaijan",
        "comment_body": "Thank you!",
        "pr_file_module": null
      }
    ]
  }
]