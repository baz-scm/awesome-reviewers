[
  {
    "discussion_id": "2231952525",
    "pr_number": 4071,
    "pr_file": "unstructured/file_utils/encoding.py",
    "created_at": "2025-07-25T20:27:58+00:00",
    "commented_code": "except (UnicodeDecodeError, UnicodeError):\n                 continue\n         else:\n-            raise UnicodeDecodeError(\n-                \"Unable to determine the encoding of the file or match it with any \"\n-                \"of the specified encodings.\",\n-                byte_data,\n-                0,\n-                len(byte_data),\n-                \"Invalid encoding\",\n-            )\n+            # NOTE: Use UnprocessableEntityError instead of UnicodeDecodeError to avoid\n+            # logging the entire file content. UnicodeDecodeError automatically stores\n+            # the complete input data, which can be problematic for large files.\n+            raise UnprocessableEntityError(\n+                \"Unable to determine file encoding after trying all common encodings. \"\n+                \"File may be corrupted or in an unsupported format.\"\n+            ) from None\n \n     else:\n-        file_text = byte_data.decode(encoding)\n+        # NOTE: Catch UnicodeDecodeError to avoid logging the entire file content.\n+        # UnicodeDecodeError automatically stores the complete input data in its\n+        # 'object' attribute, which can cause issues with large files in logging\n+        # and error reporting systems.\n+        try:\n+            file_text = byte_data.decode(encoding)\n+        except (UnicodeDecodeError, UnicodeError):\n+            raise UnprocessableEntityError(\n+                f\"File encoding detection failed: detected '{encoding}' but decode failed. \"\n+                f\"File may be corrupted or in an unsupported format.\"\n+            ) from None",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "2231952525",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 4071,
        "pr_file": "unstructured/file_utils/encoding.py",
        "discussion_id": "2231952525",
        "commented_code": "@@ -88,17 +89,26 @@ def detect_file_encoding(\n             except (UnicodeDecodeError, UnicodeError):\n                 continue\n         else:\n-            raise UnicodeDecodeError(\n-                \"Unable to determine the encoding of the file or match it with any \"\n-                \"of the specified encodings.\",\n-                byte_data,\n-                0,\n-                len(byte_data),\n-                \"Invalid encoding\",\n-            )\n+            # NOTE: Use UnprocessableEntityError instead of UnicodeDecodeError to avoid\n+            # logging the entire file content. UnicodeDecodeError automatically stores\n+            # the complete input data, which can be problematic for large files.\n+            raise UnprocessableEntityError(\n+                \"Unable to determine file encoding after trying all common encodings. \"\n+                \"File may be corrupted or in an unsupported format.\"\n+            ) from None\n \n     else:\n-        file_text = byte_data.decode(encoding)\n+        # NOTE: Catch UnicodeDecodeError to avoid logging the entire file content.\n+        # UnicodeDecodeError automatically stores the complete input data in its\n+        # 'object' attribute, which can cause issues with large files in logging\n+        # and error reporting systems.\n+        try:\n+            file_text = byte_data.decode(encoding)\n+        except (UnicodeDecodeError, UnicodeError):\n+            raise UnprocessableEntityError(\n+                f\"File encoding detection failed: detected '{encoding}' but decode failed. \"\n+                f\"File may be corrupted or in an unsupported format.\"\n+            ) from None",
        "comment_created_at": "2025-07-25T20:27:58+00:00",
        "comment_author": "yuming-long",
        "comment_body": "why from None",
        "pr_file_module": null
      },
      {
        "comment_id": "2231960453",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 4071,
        "pr_file": "unstructured/file_utils/encoding.py",
        "discussion_id": "2231952525",
        "commented_code": "@@ -88,17 +89,26 @@ def detect_file_encoding(\n             except (UnicodeDecodeError, UnicodeError):\n                 continue\n         else:\n-            raise UnicodeDecodeError(\n-                \"Unable to determine the encoding of the file or match it with any \"\n-                \"of the specified encodings.\",\n-                byte_data,\n-                0,\n-                len(byte_data),\n-                \"Invalid encoding\",\n-            )\n+            # NOTE: Use UnprocessableEntityError instead of UnicodeDecodeError to avoid\n+            # logging the entire file content. UnicodeDecodeError automatically stores\n+            # the complete input data, which can be problematic for large files.\n+            raise UnprocessableEntityError(\n+                \"Unable to determine file encoding after trying all common encodings. \"\n+                \"File may be corrupted or in an unsupported format.\"\n+            ) from None\n \n     else:\n-        file_text = byte_data.decode(encoding)\n+        # NOTE: Catch UnicodeDecodeError to avoid logging the entire file content.\n+        # UnicodeDecodeError automatically stores the complete input data in its\n+        # 'object' attribute, which can cause issues with large files in logging\n+        # and error reporting systems.\n+        try:\n+            file_text = byte_data.decode(encoding)\n+        except (UnicodeDecodeError, UnicodeError):\n+            raise UnprocessableEntityError(\n+                f\"File encoding detection failed: detected '{encoding}' but decode failed. \"\n+                f\"File may be corrupted or in an unsupported format.\"\n+            ) from None",
        "comment_created_at": "2025-07-25T20:33:38+00:00",
        "comment_author": "CyMule",
        "comment_body": "APM would end up capturing the original exception which would contain the full bytes we tried to decode. The `None` prevents the exception chaining and importantly prevents the bytes from being passed to APM",
        "pr_file_module": null
      },
      {
        "comment_id": "2231994145",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 4071,
        "pr_file": "unstructured/file_utils/encoding.py",
        "discussion_id": "2231952525",
        "commented_code": "@@ -88,17 +89,26 @@ def detect_file_encoding(\n             except (UnicodeDecodeError, UnicodeError):\n                 continue\n         else:\n-            raise UnicodeDecodeError(\n-                \"Unable to determine the encoding of the file or match it with any \"\n-                \"of the specified encodings.\",\n-                byte_data,\n-                0,\n-                len(byte_data),\n-                \"Invalid encoding\",\n-            )\n+            # NOTE: Use UnprocessableEntityError instead of UnicodeDecodeError to avoid\n+            # logging the entire file content. UnicodeDecodeError automatically stores\n+            # the complete input data, which can be problematic for large files.\n+            raise UnprocessableEntityError(\n+                \"Unable to determine file encoding after trying all common encodings. \"\n+                \"File may be corrupted or in an unsupported format.\"\n+            ) from None\n \n     else:\n-        file_text = byte_data.decode(encoding)\n+        # NOTE: Catch UnicodeDecodeError to avoid logging the entire file content.\n+        # UnicodeDecodeError automatically stores the complete input data in its\n+        # 'object' attribute, which can cause issues with large files in logging\n+        # and error reporting systems.\n+        try:\n+            file_text = byte_data.decode(encoding)\n+        except (UnicodeDecodeError, UnicodeError):\n+            raise UnprocessableEntityError(\n+                f\"File encoding detection failed: detected '{encoding}' but decode failed. \"\n+                f\"File may be corrupted or in an unsupported format.\"\n+            ) from None",
        "comment_created_at": "2025-07-25T20:56:42+00:00",
        "comment_author": "yuming-long",
        "comment_body": "gotcha thanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1664221776",
    "pr_number": 3344,
    "pr_file": "unstructured/ingest/v2/processes/connectors/salesforce.py",
    "created_at": "2024-07-03T13:45:08+00:00",
    "commented_code": "+\"\"\"\n+Salesforce Connector\n+Able to download Account, Case, Campaign, EmailMessage, Lead\n+Salesforce returns everything as a list of json.\n+This saves each entry as a separate file to be partitioned.\n+Using JWT authorization\n+https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_auth_key_and_cert.htm\n+https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_auth_connected_app.htm\n+\"\"\"\n+\n+import json\n+import typing as t\n+from collections import OrderedDict\n+from dataclasses import dataclass, field\n+from email.utils import formatdate\n+from pathlib import Path\n+from string import Template\n+from textwrap import dedent\n+from typing import TYPE_CHECKING, Any, Generator\n+\n+from dateutil import parser\n+\n+from unstructured.documents.elements import DataSourceMetadata\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import SourceConnectionNetworkError\n+from unstructured.ingest.v2.interfaces import (\n+    AccessConfig,\n+    ConnectionConfig,\n+    Downloader,\n+    DownloaderConfig,\n+    DownloadResponse,\n+    FileData,\n+    Indexer,\n+    IndexerConfig,\n+    SourceIdentifiers,\n+)\n+from unstructured.ingest.v2.logger import logger\n+from unstructured.ingest.v2.processes.connector_registry import (\n+    SourceRegistryEntry,\n+    add_source_entry,\n+)\n+from unstructured.utils import requires_dependencies\n+\n+\n+class MissingCategoryError(Exception):\n+    \"\"\"There are no categories with that name.\"\"\"\n+\n+\n+CONNECTOR_TYPE = \"salesforce\"\n+\n+if TYPE_CHECKING:\n+    pass\n+\n+SALESFORCE_API_VERSION = \"57.0\"\n+\n+ACCEPTED_CATEGORIES = [\"Account\", \"Case\", \"Campaign\", \"EmailMessage\", \"Lead\"]\n+\n+EMAIL_TEMPLATE = Template(\n+    \"\"\"MIME-Version: 1.0\n+Date: $date\n+Message-ID: $message_identifier\n+Subject: $subject\n+From: $from_email\n+To: $to_email\n+Content-Type: multipart/alternative; boundary=\"00000000000095c9b205eff92630\"\n+--00000000000095c9b205eff92630\n+Content-Type: text/plain; charset=\"UTF-8\"\n+$textbody\n+--00000000000095c9b205eff92630\n+Content-Type: text/html; charset=\"UTF-8\"\n+$htmlbody\n+--00000000000095c9b205eff92630--\n+\"\"\",\n+)\n+\n+\n+@dataclass\n+class SalesforceAccessConfig(AccessConfig):\n+    consumer_key: str\n+    private_key: str\n+\n+    @requires_dependencies([\"cryptography\"])\n+    def get_private_key_value_and_type(self) -> t.Tuple[str, t.Type]:\n+        from cryptography.hazmat.primitives import serialization\n+\n+        try:\n+            serialization.load_pem_private_key(data=self.private_key.encode(\"utf-8\"), password=None)\n+        except ValueError:\n+            pass\n+        else:\n+            return self.private_key, str\n+\n+        if Path(self.private_key).is_file():\n+            return self.private_key, Path\n+\n+        raise ValueError(\"private_key does not contain PEM private key or path\")\n+\n+\n+@dataclass\n+class SalesforceConnectionConfig(ConnectionConfig):\n+    username: str\n+    access_config: SalesforceAccessConfig = enhanced_field(sensitive=True)\n+\n+    @requires_dependencies([\"simple_salesforce\"], extras=\"salesforce\")\n+    def get_client(self):\n+        from simple_salesforce import Salesforce\n+\n+        pkey_value, pkey_type = self.access_config.get_private_key_value_and_type()\n+\n+        return Salesforce(\n+            username=self.username,\n+            consumer_key=self.access_config.consumer_key,\n+            privatekey_file=pkey_value if pkey_type is Path else None,\n+            privatekey=pkey_value if pkey_type is str else None,\n+            version=SALESFORCE_API_VERSION,\n+        )\n+\n+\n+@dataclass\n+class SalesforceIndexerConfig(IndexerConfig):\n+    categories: t.List[str]\n+\n+\n+@dataclass\n+class SalesforceIndexer(Indexer):\n+    connection_config: SalesforceConnectionConfig\n+    index_config: SalesforceIndexerConfig\n+\n+    @requires_dependencies([\"simple_salesforce\"], extras=\"salesforce\")\n+    def list_files(self) -> t.List[FileData]:\n+        \"\"\"Get Salesforce Ids for the records.\n+        Send them to next phase where each doc gets downloaded into the\n+        appropriate format for partitioning.\n+        \"\"\"\n+        from simple_salesforce.exceptions import SalesforceMalformedRequest\n+\n+        client = self.connection_config.get_client()\n+\n+        files_list = []\n+        for record_type in self.index_config.categories:\n+            if record_type not in ACCEPTED_CATEGORIES:\n+                raise ValueError(f\"{record_type} not currently an accepted Salesforce category\")\n+\n+            try:\n+                # Get ids from Salesforce\n+                records = client.query_all_iter(\n+                    f\"select Id, SystemModstamp, CreatedDate, LastModifiedDate from {record_type}\",\n+                )\n+                for record in records:\n+                    files_list.append(\n+                        FileData(\n+                            connector_type=CONNECTOR_TYPE,\n+                            identifier=record[\"Id\"],\n+                            source_identifiers=SourceIdentifiers(\n+                                filename=record[\"Id\"],\n+                                fullpath=f\"{record['attributes']['type']}/{record['Id']}\",\n+                            ),\n+                            metadata=DataSourceMetadata(\n+                                url=record[\"attributes\"][\"url\"],\n+                                version=record[\"SystemModstamp\"],\n+                                date_created=record[\"CreatedDate\"],\n+                                date_modified=record[\"LastModifiedDate\"],\n+                                record_locator={\"id\": record[\"Id\"]},\n+                            ),\n+                            additional_metadata={\"record_type\": record[\"attributes\"][\"type\"]},\n+                        )\n+                    )\n+            except SalesforceMalformedRequest as e:\n+                raise SalesforceMalformedRequest(f\"Problem with Salesforce query: {e}\")\n+\n+        return files_list\n+\n+    def run(self, **kwargs: Any) -> Generator[FileData, None, None]:\n+        for f in self.list_files():\n+            yield f\n+\n+\n+@dataclass\n+class SalesforceDownloaderConfig(DownloaderConfig):\n+    pass\n+\n+\n+@dataclass\n+class SalesforceDownloader(Downloader):\n+    connection_config: SalesforceConnectionConfig\n+    download_config: SalesforceDownloaderConfig = field(\n+        default_factory=lambda: SalesforceDownloaderConfig()\n+    )\n+    connector_type: str = CONNECTOR_TYPE\n+\n+    def get_file_extension(self, record_type) -> str:\n+        if record_type == \"EmailMessage\":\n+            extension = \".eml\"\n+        elif record_type in [\"Account\", \"Lead\", \"Case\", \"Campaign\"]:\n+            extension = \".xml\"\n+        else:\n+            raise MissingCategoryError(\n+                f\"There are no categories with the name: {record_type}\",\n+            )\n+        return extension\n+\n+    def get_download_path(self, file_data: FileData) -> Path:\n+        record_file = file_data.identifier + self.get_file_extension(\n+            file_data.additional_metadata[\"record_type\"]\n+        )\n+        return Path(self.download_dir) / file_data.additional_metadata[\"record_type\"] / record_file\n+\n+    def _xml_for_record(self, record: OrderedDict) -> str:\n+        \"\"\"Creates partitionable xml file from a record\"\"\"\n+        import xml.etree.ElementTree as ET\n+\n+        def flatten_dict(data, parent, prefix=\"\"):\n+            for key, value in data.items():\n+                if isinstance(value, OrderedDict):\n+                    flatten_dict(value, parent, prefix=f\"{prefix}{key}.\")\n+                else:\n+                    item = ET.Element(\"item\")\n+                    item.text = f\"{prefix}{key}: {value}\"\n+                    parent.append(item)\n+\n+        root = ET.Element(\"root\")\n+        flatten_dict(record, root)\n+        xml_string = ET.tostring(root, encoding=\"utf-8\", xml_declaration=True).decode()\n+        return xml_string\n+\n+    def _eml_for_record(self, email_json: t.Dict[str, t.Any]) -> str:\n+        \"\"\"Recreates standard expected .eml format using template.\"\"\"\n+        eml = EMAIL_TEMPLATE.substitute(\n+            date=formatdate(parser.parse(email_json.get(\"MessageDate\")).timestamp()),\n+            message_identifier=email_json.get(\"MessageIdentifier\"),\n+            subject=email_json.get(\"Subject\"),\n+            from_email=email_json.get(\"FromAddress\"),\n+            to_email=email_json.get(\"ToAddress\"),\n+            textbody=email_json.get(\"TextBody\"),\n+            htmlbody=email_json.get(\"HtmlBody\"),\n+        )\n+        return dedent(eml)\n+\n+    @SourceConnectionNetworkError.wrap\n+    def _get_response(self, file_data: FileData):\n+        client = self.connection_config.get_client()\n+        return client.query(\n+            f\"select FIELDS(STANDARD) from {file_data.additional_metadata['record_type']} where Id='{file_data.identifier}'\",  # noqa: E501\n+        )\n+\n+    def get_record(self, file_data: FileData) -> OrderedDict:\n+        # Get record from Salesforce based on id\n+        response = self._get_response(file_data)\n+        logger.debug(f\"response was returned for salesforce record id: {file_data.identifier}\")\n+        records = response[\"records\"]\n+        if not records:\n+            raise ValueError(\n+                f\"No record found with record id {file_data.identifier}: {json.dumps(response)}\"\n+            )\n+        record_json = records[0]\n+        return record_json\n+\n+    def run(self, file_data: FileData, **kwargs: Any) -> DownloadResponse:\n+        record = self.get_record(file_data)\n+\n+        try:\n+            if file_data.additional_metadata[\"record_type\"] == \"EmailMessage\":\n+                document = self._eml_for_record(record)\n+            else:\n+                document = self._xml_for_record(record)\n+            download_path = self.get_download_path(file_data=file_data)\n+            download_path.parent.mkdir(parents=True, exist_ok=True)\n+\n+            with open(download_path, \"w\") as page_file:\n+                page_file.write(document)\n+\n+            return DownloadResponse(\n+                file_data=file_data,\n+                path=Path(download_path),\n+            )\n+\n+        except Exception as e:\n+            logger.error(\n+                f\"Error while downloading and saving file: {file_data.identifier}.\",\n+            )\n+            logger.error(e)",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1664221776",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3344,
        "pr_file": "unstructured/ingest/v2/processes/connectors/salesforce.py",
        "discussion_id": "1664221776",
        "commented_code": "@@ -0,0 +1,293 @@\n+\"\"\"\n+Salesforce Connector\n+Able to download Account, Case, Campaign, EmailMessage, Lead\n+Salesforce returns everything as a list of json.\n+This saves each entry as a separate file to be partitioned.\n+Using JWT authorization\n+https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_auth_key_and_cert.htm\n+https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_auth_connected_app.htm\n+\"\"\"\n+\n+import json\n+import typing as t\n+from collections import OrderedDict\n+from dataclasses import dataclass, field\n+from email.utils import formatdate\n+from pathlib import Path\n+from string import Template\n+from textwrap import dedent\n+from typing import TYPE_CHECKING, Any, Generator\n+\n+from dateutil import parser\n+\n+from unstructured.documents.elements import DataSourceMetadata\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import SourceConnectionNetworkError\n+from unstructured.ingest.v2.interfaces import (\n+    AccessConfig,\n+    ConnectionConfig,\n+    Downloader,\n+    DownloaderConfig,\n+    DownloadResponse,\n+    FileData,\n+    Indexer,\n+    IndexerConfig,\n+    SourceIdentifiers,\n+)\n+from unstructured.ingest.v2.logger import logger\n+from unstructured.ingest.v2.processes.connector_registry import (\n+    SourceRegistryEntry,\n+    add_source_entry,\n+)\n+from unstructured.utils import requires_dependencies\n+\n+\n+class MissingCategoryError(Exception):\n+    \"\"\"There are no categories with that name.\"\"\"\n+\n+\n+CONNECTOR_TYPE = \"salesforce\"\n+\n+if TYPE_CHECKING:\n+    pass\n+\n+SALESFORCE_API_VERSION = \"57.0\"\n+\n+ACCEPTED_CATEGORIES = [\"Account\", \"Case\", \"Campaign\", \"EmailMessage\", \"Lead\"]\n+\n+EMAIL_TEMPLATE = Template(\n+    \"\"\"MIME-Version: 1.0\n+Date: $date\n+Message-ID: $message_identifier\n+Subject: $subject\n+From: $from_email\n+To: $to_email\n+Content-Type: multipart/alternative; boundary=\"00000000000095c9b205eff92630\"\n+--00000000000095c9b205eff92630\n+Content-Type: text/plain; charset=\"UTF-8\"\n+$textbody\n+--00000000000095c9b205eff92630\n+Content-Type: text/html; charset=\"UTF-8\"\n+$htmlbody\n+--00000000000095c9b205eff92630--\n+\"\"\",\n+)\n+\n+\n+@dataclass\n+class SalesforceAccessConfig(AccessConfig):\n+    consumer_key: str\n+    private_key: str\n+\n+    @requires_dependencies([\"cryptography\"])\n+    def get_private_key_value_and_type(self) -> t.Tuple[str, t.Type]:\n+        from cryptography.hazmat.primitives import serialization\n+\n+        try:\n+            serialization.load_pem_private_key(data=self.private_key.encode(\"utf-8\"), password=None)\n+        except ValueError:\n+            pass\n+        else:\n+            return self.private_key, str\n+\n+        if Path(self.private_key).is_file():\n+            return self.private_key, Path\n+\n+        raise ValueError(\"private_key does not contain PEM private key or path\")\n+\n+\n+@dataclass\n+class SalesforceConnectionConfig(ConnectionConfig):\n+    username: str\n+    access_config: SalesforceAccessConfig = enhanced_field(sensitive=True)\n+\n+    @requires_dependencies([\"simple_salesforce\"], extras=\"salesforce\")\n+    def get_client(self):\n+        from simple_salesforce import Salesforce\n+\n+        pkey_value, pkey_type = self.access_config.get_private_key_value_and_type()\n+\n+        return Salesforce(\n+            username=self.username,\n+            consumer_key=self.access_config.consumer_key,\n+            privatekey_file=pkey_value if pkey_type is Path else None,\n+            privatekey=pkey_value if pkey_type is str else None,\n+            version=SALESFORCE_API_VERSION,\n+        )\n+\n+\n+@dataclass\n+class SalesforceIndexerConfig(IndexerConfig):\n+    categories: t.List[str]\n+\n+\n+@dataclass\n+class SalesforceIndexer(Indexer):\n+    connection_config: SalesforceConnectionConfig\n+    index_config: SalesforceIndexerConfig\n+\n+    @requires_dependencies([\"simple_salesforce\"], extras=\"salesforce\")\n+    def list_files(self) -> t.List[FileData]:\n+        \"\"\"Get Salesforce Ids for the records.\n+        Send them to next phase where each doc gets downloaded into the\n+        appropriate format for partitioning.\n+        \"\"\"\n+        from simple_salesforce.exceptions import SalesforceMalformedRequest\n+\n+        client = self.connection_config.get_client()\n+\n+        files_list = []\n+        for record_type in self.index_config.categories:\n+            if record_type not in ACCEPTED_CATEGORIES:\n+                raise ValueError(f\"{record_type} not currently an accepted Salesforce category\")\n+\n+            try:\n+                # Get ids from Salesforce\n+                records = client.query_all_iter(\n+                    f\"select Id, SystemModstamp, CreatedDate, LastModifiedDate from {record_type}\",\n+                )\n+                for record in records:\n+                    files_list.append(\n+                        FileData(\n+                            connector_type=CONNECTOR_TYPE,\n+                            identifier=record[\"Id\"],\n+                            source_identifiers=SourceIdentifiers(\n+                                filename=record[\"Id\"],\n+                                fullpath=f\"{record['attributes']['type']}/{record['Id']}\",\n+                            ),\n+                            metadata=DataSourceMetadata(\n+                                url=record[\"attributes\"][\"url\"],\n+                                version=record[\"SystemModstamp\"],\n+                                date_created=record[\"CreatedDate\"],\n+                                date_modified=record[\"LastModifiedDate\"],\n+                                record_locator={\"id\": record[\"Id\"]},\n+                            ),\n+                            additional_metadata={\"record_type\": record[\"attributes\"][\"type\"]},\n+                        )\n+                    )\n+            except SalesforceMalformedRequest as e:\n+                raise SalesforceMalformedRequest(f\"Problem with Salesforce query: {e}\")\n+\n+        return files_list\n+\n+    def run(self, **kwargs: Any) -> Generator[FileData, None, None]:\n+        for f in self.list_files():\n+            yield f\n+\n+\n+@dataclass\n+class SalesforceDownloaderConfig(DownloaderConfig):\n+    pass\n+\n+\n+@dataclass\n+class SalesforceDownloader(Downloader):\n+    connection_config: SalesforceConnectionConfig\n+    download_config: SalesforceDownloaderConfig = field(\n+        default_factory=lambda: SalesforceDownloaderConfig()\n+    )\n+    connector_type: str = CONNECTOR_TYPE\n+\n+    def get_file_extension(self, record_type) -> str:\n+        if record_type == \"EmailMessage\":\n+            extension = \".eml\"\n+        elif record_type in [\"Account\", \"Lead\", \"Case\", \"Campaign\"]:\n+            extension = \".xml\"\n+        else:\n+            raise MissingCategoryError(\n+                f\"There are no categories with the name: {record_type}\",\n+            )\n+        return extension\n+\n+    def get_download_path(self, file_data: FileData) -> Path:\n+        record_file = file_data.identifier + self.get_file_extension(\n+            file_data.additional_metadata[\"record_type\"]\n+        )\n+        return Path(self.download_dir) / file_data.additional_metadata[\"record_type\"] / record_file\n+\n+    def _xml_for_record(self, record: OrderedDict) -> str:\n+        \"\"\"Creates partitionable xml file from a record\"\"\"\n+        import xml.etree.ElementTree as ET\n+\n+        def flatten_dict(data, parent, prefix=\"\"):\n+            for key, value in data.items():\n+                if isinstance(value, OrderedDict):\n+                    flatten_dict(value, parent, prefix=f\"{prefix}{key}.\")\n+                else:\n+                    item = ET.Element(\"item\")\n+                    item.text = f\"{prefix}{key}: {value}\"\n+                    parent.append(item)\n+\n+        root = ET.Element(\"root\")\n+        flatten_dict(record, root)\n+        xml_string = ET.tostring(root, encoding=\"utf-8\", xml_declaration=True).decode()\n+        return xml_string\n+\n+    def _eml_for_record(self, email_json: t.Dict[str, t.Any]) -> str:\n+        \"\"\"Recreates standard expected .eml format using template.\"\"\"\n+        eml = EMAIL_TEMPLATE.substitute(\n+            date=formatdate(parser.parse(email_json.get(\"MessageDate\")).timestamp()),\n+            message_identifier=email_json.get(\"MessageIdentifier\"),\n+            subject=email_json.get(\"Subject\"),\n+            from_email=email_json.get(\"FromAddress\"),\n+            to_email=email_json.get(\"ToAddress\"),\n+            textbody=email_json.get(\"TextBody\"),\n+            htmlbody=email_json.get(\"HtmlBody\"),\n+        )\n+        return dedent(eml)\n+\n+    @SourceConnectionNetworkError.wrap\n+    def _get_response(self, file_data: FileData):\n+        client = self.connection_config.get_client()\n+        return client.query(\n+            f\"select FIELDS(STANDARD) from {file_data.additional_metadata['record_type']} where Id='{file_data.identifier}'\",  # noqa: E501\n+        )\n+\n+    def get_record(self, file_data: FileData) -> OrderedDict:\n+        # Get record from Salesforce based on id\n+        response = self._get_response(file_data)\n+        logger.debug(f\"response was returned for salesforce record id: {file_data.identifier}\")\n+        records = response[\"records\"]\n+        if not records:\n+            raise ValueError(\n+                f\"No record found with record id {file_data.identifier}: {json.dumps(response)}\"\n+            )\n+        record_json = records[0]\n+        return record_json\n+\n+    def run(self, file_data: FileData, **kwargs: Any) -> DownloadResponse:\n+        record = self.get_record(file_data)\n+\n+        try:\n+            if file_data.additional_metadata[\"record_type\"] == \"EmailMessage\":\n+                document = self._eml_for_record(record)\n+            else:\n+                document = self._xml_for_record(record)\n+            download_path = self.get_download_path(file_data=file_data)\n+            download_path.parent.mkdir(parents=True, exist_ok=True)\n+\n+            with open(download_path, \"w\") as page_file:\n+                page_file.write(document)\n+\n+            return DownloadResponse(\n+                file_data=file_data,\n+                path=Path(download_path),\n+            )\n+\n+        except Exception as e:\n+            logger.error(\n+                f\"Error while downloading and saving file: {file_data.identifier}.\",\n+            )\n+            logger.error(e)",
        "comment_created_at": "2024-07-03T13:45:08+00:00",
        "comment_author": "rbiseck3",
        "comment_body": "We should reraise something here so that the error gets caught intentionally rather than simply logging it and letting the process swallow it up. \r\n\r\n```suggestion\r\n            logger.error(\r\n                f\"Error while downloading and saving file {file_data.identifier}: {e}\",\r\n            )\r\n            raise e\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1658874486",
    "pr_number": 3286,
    "pr_file": "unstructured/ingest/v2/processes/connectors/pinecone.py",
    "created_at": "2024-06-28T14:50:53+00:00",
    "commented_code": "+import json\n+import multiprocessing as mp\n+import uuid\n+from dataclasses import dataclass, field\n+from pathlib import Path\n+from typing import TYPE_CHECKING, Any, Optional\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import DestinationConnectionError, WriteError\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.ingest.v2.interfaces import (\n+    AccessConfig,\n+    ConnectionConfig,\n+    UploadContent,\n+    Uploader,\n+    UploaderConfig,\n+    UploadStager,\n+    UploadStagerConfig,\n+)\n+from unstructured.ingest.v2.logger import logger\n+from unstructured.ingest.v2.processes.connector_registry import (\n+    DestinationRegistryEntry,\n+    add_destination_entry,\n+)\n+from unstructured.staging.base import flatten_dict\n+from unstructured.utils import requires_dependencies\n+\n+if TYPE_CHECKING:\n+    from pinecone import Index as PineconeIndex\n+\n+\n+CONNECTOR_TYPE = \"pinecone\"\n+\n+\n+@dataclass\n+class PineconeAccessConfig(AccessConfig):\n+    api_key: Optional[str] = enhanced_field(default=None, overload_name=\"pinecone_api_key\")\n+\n+\n+@dataclass\n+class PineconeConnectionConfig(ConnectionConfig):\n+    index_name: str\n+    environment: str\n+    access_config: PineconeAccessConfig = enhanced_field(sensitive=True)\n+\n+    @requires_dependencies([\"pinecone\"], extras=\"pinecone\")\n+    def create_index(self) -> \"PineconeIndex\":\n+        from pinecone import Pinecone\n+\n+        from unstructured import __version__ as unstructured_version\n+\n+        pc = Pinecone(\n+            api_key=self.access_config.api_key,\n+            source_tag=f\"unstructured=={unstructured_version}\",\n+        )\n+\n+        index = pc.Index(self.index_name)\n+        logger.debug(f\"Connected to index: {pc.describe_index(self.index_name)}\")\n+        return index\n+\n+\n+@dataclass\n+class PineconeUploadStagerConfig(UploadStagerConfig):\n+    pass\n+\n+\n+@dataclass\n+class PineconeUploaderConfig(UploaderConfig):\n+    batch_size: int = 100\n+    num_of_processes: int = 4\n+\n+\n+@dataclass\n+class PineconeUploadStager(UploadStager):\n+    upload_stager_config: PineconeUploadStagerConfig = field(\n+        default_factory=lambda: PineconeUploadStagerConfig()\n+    )\n+\n+    @staticmethod\n+    def conform_dict(element_dict: dict) -> dict:\n+        # While flatten_dict enables indexing on various fields,\n+        # element_serialized enables easily reloading the element object to memory.\n+        # element_serialized is formed without text/embeddings to avoid data bloating.\n+        return {\n+            \"id\": str(uuid.uuid4()),\n+            \"values\": element_dict.pop(\"embeddings\", None),\n+            \"metadata\": {\n+                \"text\": element_dict.pop(\"text\", None),\n+                \"element_serialized\": json.dumps(element_dict),\n+                **flatten_dict(\n+                    element_dict,\n+                    separator=\"-\",\n+                    flatten_lists=True,\n+                    remove_none=True,\n+                ),\n+            },\n+        }\n+\n+    def run(\n+        self,\n+        elements_filepath: Path,\n+        output_dir: Path,\n+        output_filename: str,\n+        **kwargs: Any,\n+    ) -> Path:\n+        with open(elements_filepath) as elements_file:\n+            elements_contents = json.load(elements_file)\n+\n+        conformed_elements = [\n+            self.conform_dict(element_dict=element) for element in elements_contents\n+        ]\n+\n+        output_path = Path(output_dir) / Path(f\"{output_filename}.json\")\n+        with open(output_path, \"w\") as output_file:\n+            json.dump(conformed_elements, output_file)\n+        return output_path\n+\n+\n+@dataclass\n+class PineconeUploader(Uploader):\n+    upload_config: PineconeUploaderConfig\n+    connection_config: PineconeConnectionConfig\n+    connector_type: str = CONNECTOR_TYPE\n+\n+    @DestinationConnectionError.wrap\n+    def check_connection(self):\n+        _ = self.connection_config.create_index()\n+\n+    @DestinationConnectionError.wrap\n+    @requires_dependencies([\"pinecone\"], extras=\"pinecone\")\n+    def upsert_batch(self, batch):\n+        import pinecone.core.client.exceptions\n+\n+        try:\n+            index = self.connection_config.create_index()\n+            response = index.upsert(batch)\n+        except pinecone.core.client.exceptions.PineconeApiException as api_error:\n+            raise WriteError(f\"http error: {api_error}\") from api_error\n+        logger.debug(f\"results: {response}\")",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1658874486",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3286,
        "pr_file": "unstructured/ingest/v2/processes/connectors/pinecone.py",
        "discussion_id": "1658874486",
        "commented_code": "@@ -0,0 +1,181 @@\n+import json\n+import multiprocessing as mp\n+import uuid\n+from dataclasses import dataclass, field\n+from pathlib import Path\n+from typing import TYPE_CHECKING, Any, Optional\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import DestinationConnectionError, WriteError\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.ingest.v2.interfaces import (\n+    AccessConfig,\n+    ConnectionConfig,\n+    UploadContent,\n+    Uploader,\n+    UploaderConfig,\n+    UploadStager,\n+    UploadStagerConfig,\n+)\n+from unstructured.ingest.v2.logger import logger\n+from unstructured.ingest.v2.processes.connector_registry import (\n+    DestinationRegistryEntry,\n+    add_destination_entry,\n+)\n+from unstructured.staging.base import flatten_dict\n+from unstructured.utils import requires_dependencies\n+\n+if TYPE_CHECKING:\n+    from pinecone import Index as PineconeIndex\n+\n+\n+CONNECTOR_TYPE = \"pinecone\"\n+\n+\n+@dataclass\n+class PineconeAccessConfig(AccessConfig):\n+    api_key: Optional[str] = enhanced_field(default=None, overload_name=\"pinecone_api_key\")\n+\n+\n+@dataclass\n+class PineconeConnectionConfig(ConnectionConfig):\n+    index_name: str\n+    environment: str\n+    access_config: PineconeAccessConfig = enhanced_field(sensitive=True)\n+\n+    @requires_dependencies([\"pinecone\"], extras=\"pinecone\")\n+    def create_index(self) -> \"PineconeIndex\":\n+        from pinecone import Pinecone\n+\n+        from unstructured import __version__ as unstructured_version\n+\n+        pc = Pinecone(\n+            api_key=self.access_config.api_key,\n+            source_tag=f\"unstructured=={unstructured_version}\",\n+        )\n+\n+        index = pc.Index(self.index_name)\n+        logger.debug(f\"Connected to index: {pc.describe_index(self.index_name)}\")\n+        return index\n+\n+\n+@dataclass\n+class PineconeUploadStagerConfig(UploadStagerConfig):\n+    pass\n+\n+\n+@dataclass\n+class PineconeUploaderConfig(UploaderConfig):\n+    batch_size: int = 100\n+    num_of_processes: int = 4\n+\n+\n+@dataclass\n+class PineconeUploadStager(UploadStager):\n+    upload_stager_config: PineconeUploadStagerConfig = field(\n+        default_factory=lambda: PineconeUploadStagerConfig()\n+    )\n+\n+    @staticmethod\n+    def conform_dict(element_dict: dict) -> dict:\n+        # While flatten_dict enables indexing on various fields,\n+        # element_serialized enables easily reloading the element object to memory.\n+        # element_serialized is formed without text/embeddings to avoid data bloating.\n+        return {\n+            \"id\": str(uuid.uuid4()),\n+            \"values\": element_dict.pop(\"embeddings\", None),\n+            \"metadata\": {\n+                \"text\": element_dict.pop(\"text\", None),\n+                \"element_serialized\": json.dumps(element_dict),\n+                **flatten_dict(\n+                    element_dict,\n+                    separator=\"-\",\n+                    flatten_lists=True,\n+                    remove_none=True,\n+                ),\n+            },\n+        }\n+\n+    def run(\n+        self,\n+        elements_filepath: Path,\n+        output_dir: Path,\n+        output_filename: str,\n+        **kwargs: Any,\n+    ) -> Path:\n+        with open(elements_filepath) as elements_file:\n+            elements_contents = json.load(elements_file)\n+\n+        conformed_elements = [\n+            self.conform_dict(element_dict=element) for element in elements_contents\n+        ]\n+\n+        output_path = Path(output_dir) / Path(f\"{output_filename}.json\")\n+        with open(output_path, \"w\") as output_file:\n+            json.dump(conformed_elements, output_file)\n+        return output_path\n+\n+\n+@dataclass\n+class PineconeUploader(Uploader):\n+    upload_config: PineconeUploaderConfig\n+    connection_config: PineconeConnectionConfig\n+    connector_type: str = CONNECTOR_TYPE\n+\n+    @DestinationConnectionError.wrap\n+    def check_connection(self):\n+        _ = self.connection_config.create_index()\n+\n+    @DestinationConnectionError.wrap\n+    @requires_dependencies([\"pinecone\"], extras=\"pinecone\")\n+    def upsert_batch(self, batch):\n+        import pinecone.core.client.exceptions\n+\n+        try:\n+            index = self.connection_config.create_index()\n+            response = index.upsert(batch)\n+        except pinecone.core.client.exceptions.PineconeApiException as api_error:\n+            raise WriteError(f\"http error: {api_error}\") from api_error\n+        logger.debug(f\"results: {response}\")",
        "comment_created_at": "2024-06-28T14:50:53+00:00",
        "comment_author": "rbiseck3",
        "comment_body": "Couple comments:\r\n* Import the exception being caught directly\r\n* If caught, raise `DestinationConnectionError` directly rather than wrapping it\r\n\r\n```suggestion\r\n    @requires_dependencies([\"pinecone\"], extras=\"pinecone\")\r\n    def upsert_batch(self, batch):\r\n        from pinecone.core.client.exceptions import PineconeApiException\r\n\r\n        try:\r\n            index = self.connection_config.create_index()\r\n            response = index.upsert(batch)\r\n        except PineconeApiException as api_error:\r\n            raise DestinationConnectionError(f\"http error: {api_error}\") from api_error\r\n        logger.debug(f\"results: {response}\")\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1661095421",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3286,
        "pr_file": "unstructured/ingest/v2/processes/connectors/pinecone.py",
        "discussion_id": "1658874486",
        "commented_code": "@@ -0,0 +1,181 @@\n+import json\n+import multiprocessing as mp\n+import uuid\n+from dataclasses import dataclass, field\n+from pathlib import Path\n+from typing import TYPE_CHECKING, Any, Optional\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import DestinationConnectionError, WriteError\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.ingest.v2.interfaces import (\n+    AccessConfig,\n+    ConnectionConfig,\n+    UploadContent,\n+    Uploader,\n+    UploaderConfig,\n+    UploadStager,\n+    UploadStagerConfig,\n+)\n+from unstructured.ingest.v2.logger import logger\n+from unstructured.ingest.v2.processes.connector_registry import (\n+    DestinationRegistryEntry,\n+    add_destination_entry,\n+)\n+from unstructured.staging.base import flatten_dict\n+from unstructured.utils import requires_dependencies\n+\n+if TYPE_CHECKING:\n+    from pinecone import Index as PineconeIndex\n+\n+\n+CONNECTOR_TYPE = \"pinecone\"\n+\n+\n+@dataclass\n+class PineconeAccessConfig(AccessConfig):\n+    api_key: Optional[str] = enhanced_field(default=None, overload_name=\"pinecone_api_key\")\n+\n+\n+@dataclass\n+class PineconeConnectionConfig(ConnectionConfig):\n+    index_name: str\n+    environment: str\n+    access_config: PineconeAccessConfig = enhanced_field(sensitive=True)\n+\n+    @requires_dependencies([\"pinecone\"], extras=\"pinecone\")\n+    def create_index(self) -> \"PineconeIndex\":\n+        from pinecone import Pinecone\n+\n+        from unstructured import __version__ as unstructured_version\n+\n+        pc = Pinecone(\n+            api_key=self.access_config.api_key,\n+            source_tag=f\"unstructured=={unstructured_version}\",\n+        )\n+\n+        index = pc.Index(self.index_name)\n+        logger.debug(f\"Connected to index: {pc.describe_index(self.index_name)}\")\n+        return index\n+\n+\n+@dataclass\n+class PineconeUploadStagerConfig(UploadStagerConfig):\n+    pass\n+\n+\n+@dataclass\n+class PineconeUploaderConfig(UploaderConfig):\n+    batch_size: int = 100\n+    num_of_processes: int = 4\n+\n+\n+@dataclass\n+class PineconeUploadStager(UploadStager):\n+    upload_stager_config: PineconeUploadStagerConfig = field(\n+        default_factory=lambda: PineconeUploadStagerConfig()\n+    )\n+\n+    @staticmethod\n+    def conform_dict(element_dict: dict) -> dict:\n+        # While flatten_dict enables indexing on various fields,\n+        # element_serialized enables easily reloading the element object to memory.\n+        # element_serialized is formed without text/embeddings to avoid data bloating.\n+        return {\n+            \"id\": str(uuid.uuid4()),\n+            \"values\": element_dict.pop(\"embeddings\", None),\n+            \"metadata\": {\n+                \"text\": element_dict.pop(\"text\", None),\n+                \"element_serialized\": json.dumps(element_dict),\n+                **flatten_dict(\n+                    element_dict,\n+                    separator=\"-\",\n+                    flatten_lists=True,\n+                    remove_none=True,\n+                ),\n+            },\n+        }\n+\n+    def run(\n+        self,\n+        elements_filepath: Path,\n+        output_dir: Path,\n+        output_filename: str,\n+        **kwargs: Any,\n+    ) -> Path:\n+        with open(elements_filepath) as elements_file:\n+            elements_contents = json.load(elements_file)\n+\n+        conformed_elements = [\n+            self.conform_dict(element_dict=element) for element in elements_contents\n+        ]\n+\n+        output_path = Path(output_dir) / Path(f\"{output_filename}.json\")\n+        with open(output_path, \"w\") as output_file:\n+            json.dump(conformed_elements, output_file)\n+        return output_path\n+\n+\n+@dataclass\n+class PineconeUploader(Uploader):\n+    upload_config: PineconeUploaderConfig\n+    connection_config: PineconeConnectionConfig\n+    connector_type: str = CONNECTOR_TYPE\n+\n+    @DestinationConnectionError.wrap\n+    def check_connection(self):\n+        _ = self.connection_config.create_index()\n+\n+    @DestinationConnectionError.wrap\n+    @requires_dependencies([\"pinecone\"], extras=\"pinecone\")\n+    def upsert_batch(self, batch):\n+        import pinecone.core.client.exceptions\n+\n+        try:\n+            index = self.connection_config.create_index()\n+            response = index.upsert(batch)\n+        except pinecone.core.client.exceptions.PineconeApiException as api_error:\n+            raise WriteError(f\"http error: {api_error}\") from api_error\n+        logger.debug(f\"results: {response}\")",
        "comment_created_at": "2024-07-01T13:53:47+00:00",
        "comment_author": "ahmetmeleq",
        "comment_body": "[import exception directly, raise exception directly rather than wrapp\u2026](https://github.com/Unstructured-IO/unstructured/pull/3286/commits/e89dcf60ffcfd4a3f0ce218e0ef45020073264d8)",
        "pr_file_module": null
      },
      {
        "comment_id": "1661096637",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3286,
        "pr_file": "unstructured/ingest/v2/processes/connectors/pinecone.py",
        "discussion_id": "1658874486",
        "commented_code": "@@ -0,0 +1,181 @@\n+import json\n+import multiprocessing as mp\n+import uuid\n+from dataclasses import dataclass, field\n+from pathlib import Path\n+from typing import TYPE_CHECKING, Any, Optional\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import DestinationConnectionError, WriteError\n+from unstructured.ingest.utils.data_prep import chunk_generator\n+from unstructured.ingest.v2.interfaces import (\n+    AccessConfig,\n+    ConnectionConfig,\n+    UploadContent,\n+    Uploader,\n+    UploaderConfig,\n+    UploadStager,\n+    UploadStagerConfig,\n+)\n+from unstructured.ingest.v2.logger import logger\n+from unstructured.ingest.v2.processes.connector_registry import (\n+    DestinationRegistryEntry,\n+    add_destination_entry,\n+)\n+from unstructured.staging.base import flatten_dict\n+from unstructured.utils import requires_dependencies\n+\n+if TYPE_CHECKING:\n+    from pinecone import Index as PineconeIndex\n+\n+\n+CONNECTOR_TYPE = \"pinecone\"\n+\n+\n+@dataclass\n+class PineconeAccessConfig(AccessConfig):\n+    api_key: Optional[str] = enhanced_field(default=None, overload_name=\"pinecone_api_key\")\n+\n+\n+@dataclass\n+class PineconeConnectionConfig(ConnectionConfig):\n+    index_name: str\n+    environment: str\n+    access_config: PineconeAccessConfig = enhanced_field(sensitive=True)\n+\n+    @requires_dependencies([\"pinecone\"], extras=\"pinecone\")\n+    def create_index(self) -> \"PineconeIndex\":\n+        from pinecone import Pinecone\n+\n+        from unstructured import __version__ as unstructured_version\n+\n+        pc = Pinecone(\n+            api_key=self.access_config.api_key,\n+            source_tag=f\"unstructured=={unstructured_version}\",\n+        )\n+\n+        index = pc.Index(self.index_name)\n+        logger.debug(f\"Connected to index: {pc.describe_index(self.index_name)}\")\n+        return index\n+\n+\n+@dataclass\n+class PineconeUploadStagerConfig(UploadStagerConfig):\n+    pass\n+\n+\n+@dataclass\n+class PineconeUploaderConfig(UploaderConfig):\n+    batch_size: int = 100\n+    num_of_processes: int = 4\n+\n+\n+@dataclass\n+class PineconeUploadStager(UploadStager):\n+    upload_stager_config: PineconeUploadStagerConfig = field(\n+        default_factory=lambda: PineconeUploadStagerConfig()\n+    )\n+\n+    @staticmethod\n+    def conform_dict(element_dict: dict) -> dict:\n+        # While flatten_dict enables indexing on various fields,\n+        # element_serialized enables easily reloading the element object to memory.\n+        # element_serialized is formed without text/embeddings to avoid data bloating.\n+        return {\n+            \"id\": str(uuid.uuid4()),\n+            \"values\": element_dict.pop(\"embeddings\", None),\n+            \"metadata\": {\n+                \"text\": element_dict.pop(\"text\", None),\n+                \"element_serialized\": json.dumps(element_dict),\n+                **flatten_dict(\n+                    element_dict,\n+                    separator=\"-\",\n+                    flatten_lists=True,\n+                    remove_none=True,\n+                ),\n+            },\n+        }\n+\n+    def run(\n+        self,\n+        elements_filepath: Path,\n+        output_dir: Path,\n+        output_filename: str,\n+        **kwargs: Any,\n+    ) -> Path:\n+        with open(elements_filepath) as elements_file:\n+            elements_contents = json.load(elements_file)\n+\n+        conformed_elements = [\n+            self.conform_dict(element_dict=element) for element in elements_contents\n+        ]\n+\n+        output_path = Path(output_dir) / Path(f\"{output_filename}.json\")\n+        with open(output_path, \"w\") as output_file:\n+            json.dump(conformed_elements, output_file)\n+        return output_path\n+\n+\n+@dataclass\n+class PineconeUploader(Uploader):\n+    upload_config: PineconeUploaderConfig\n+    connection_config: PineconeConnectionConfig\n+    connector_type: str = CONNECTOR_TYPE\n+\n+    @DestinationConnectionError.wrap\n+    def check_connection(self):\n+        _ = self.connection_config.create_index()\n+\n+    @DestinationConnectionError.wrap\n+    @requires_dependencies([\"pinecone\"], extras=\"pinecone\")\n+    def upsert_batch(self, batch):\n+        import pinecone.core.client.exceptions\n+\n+        try:\n+            index = self.connection_config.create_index()\n+            response = index.upsert(batch)\n+        except pinecone.core.client.exceptions.PineconeApiException as api_error:\n+            raise WriteError(f\"http error: {api_error}\") from api_error\n+        logger.debug(f\"results: {response}\")",
        "comment_created_at": "2024-07-01T13:54:33+00:00",
        "comment_author": "ahmetmeleq",
        "comment_body": "Btw, why do we prefer raising directly in this case?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1446285912",
    "pr_number": 2357,
    "pr_file": "unstructured/ingest/connector/vectara.py",
    "created_at": "2024-01-09T16:01:11+00:00",
    "commented_code": "+import datetime\n+import json\n+import traceback\n+import typing as t\n+import uuid\n+from dataclasses import dataclass\n+\n+import requests\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import DestinationConnectionError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    BaseIngestDoc,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.staging.base import flatten_dict\n+\n+\n+@dataclass\n+class VectaraAccessConfig(AccessConfig):\n+    oauth_client_id: str = enhanced_field(sensitive=True)\n+    oauth_secret: str = enhanced_field(sensitive=True)\n+\n+\n+@dataclass\n+class SimpleVectaraConfig(BaseConnectorConfig):\n+    access_config: VectaraAccessConfig\n+    customer_id: t.AnyStr = None\n+    corpus_name: t.AnyStr = \"vectara-unstructured\"\n+    corpus_id: t.AnyStr = None\n+\n+\n+@dataclass\n+class VectaraWriteConfig(WriteConfig):\n+    batch_size: int = 100\n+\n+\n+@dataclass\n+class VectaraDestinationConnector(BaseDestinationConnector):\n+    write_config: VectaraWriteConfig\n+    connector_config: SimpleVectaraConfig\n+\n+    BASE_URL = \"https://api.vectara.io/v1\"\n+\n+    @DestinationConnectionError.wrap\n+    def vectara(self):\n+        \"\"\"\n+        Check the connection for Vectara and validate corpus exists.\n+        - If more than one exists - then return a message\n+        - If exactly one exists with this name - use it.\n+        - If does not exist - create it.\n+        \"\"\"\n+        try:\n+            jwt_token = self._get_jwt_token()\n+            if not jwt_token:\n+                return \"Unable to get JWT Token. Confirm your Client ID and Client Secret.\"\n+\n+            list_corpora_response = self._request(\n+                endpoint=\"list-corpora\",\n+                data={\"numResults\": 100, \"filter\": self.connector_config.corpus_name},\n+            )\n+\n+            possible_corpora_ids_names_map = {\n+                corpus.get(\"id\"): corpus.get(\"name\")\n+                for corpus in list_corpora_response.get(\"corpus\")\n+                if corpus.get(\"name\") == self.connector_config.corpus_name\n+            }\n+\n+            if len(possible_corpora_ids_names_map) > 1:\n+                return f\"Multiple Corpora exist with name {self.connector_config.corpus_name}\"\n+            if len(possible_corpora_ids_names_map) == 1:\n+                self.connector_config.corpus_id = list(possible_corpora_ids_names_map.keys())[0]\n+            else:\n+                data = {\n+                    \"corpus\": {\n+                        \"name\": self.connector_config.corpus_name,\n+                    }\n+                }\n+                create_corpus_response = self._request(endpoint=\"create-corpus\", data=data)\n+                self.connector_config.corpus_id = create_corpus_response.get(\"corpusId\")\n+\n+        except Exception as e:\n+            return str(e) + \"\n\" + \"\".join(traceback.TracebackException.from_exception(e).format())",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1446285912",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2357,
        "pr_file": "unstructured/ingest/connector/vectara.py",
        "discussion_id": "1446285912",
        "commented_code": "@@ -0,0 +1,246 @@\n+import datetime\n+import json\n+import traceback\n+import typing as t\n+import uuid\n+from dataclasses import dataclass\n+\n+import requests\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import DestinationConnectionError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    BaseIngestDoc,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.staging.base import flatten_dict\n+\n+\n+@dataclass\n+class VectaraAccessConfig(AccessConfig):\n+    oauth_client_id: str = enhanced_field(sensitive=True)\n+    oauth_secret: str = enhanced_field(sensitive=True)\n+\n+\n+@dataclass\n+class SimpleVectaraConfig(BaseConnectorConfig):\n+    access_config: VectaraAccessConfig\n+    customer_id: t.AnyStr = None\n+    corpus_name: t.AnyStr = \"vectara-unstructured\"\n+    corpus_id: t.AnyStr = None\n+\n+\n+@dataclass\n+class VectaraWriteConfig(WriteConfig):\n+    batch_size: int = 100\n+\n+\n+@dataclass\n+class VectaraDestinationConnector(BaseDestinationConnector):\n+    write_config: VectaraWriteConfig\n+    connector_config: SimpleVectaraConfig\n+\n+    BASE_URL = \"https://api.vectara.io/v1\"\n+\n+    @DestinationConnectionError.wrap\n+    def vectara(self):\n+        \"\"\"\n+        Check the connection for Vectara and validate corpus exists.\n+        - If more than one exists - then return a message\n+        - If exactly one exists with this name - use it.\n+        - If does not exist - create it.\n+        \"\"\"\n+        try:\n+            jwt_token = self._get_jwt_token()\n+            if not jwt_token:\n+                return \"Unable to get JWT Token. Confirm your Client ID and Client Secret.\"\n+\n+            list_corpora_response = self._request(\n+                endpoint=\"list-corpora\",\n+                data={\"numResults\": 100, \"filter\": self.connector_config.corpus_name},\n+            )\n+\n+            possible_corpora_ids_names_map = {\n+                corpus.get(\"id\"): corpus.get(\"name\")\n+                for corpus in list_corpora_response.get(\"corpus\")\n+                if corpus.get(\"name\") == self.connector_config.corpus_name\n+            }\n+\n+            if len(possible_corpora_ids_names_map) > 1:\n+                return f\"Multiple Corpora exist with name {self.connector_config.corpus_name}\"\n+            if len(possible_corpora_ids_names_map) == 1:\n+                self.connector_config.corpus_id = list(possible_corpora_ids_names_map.keys())[0]\n+            else:\n+                data = {\n+                    \"corpus\": {\n+                        \"name\": self.connector_config.corpus_name,\n+                    }\n+                }\n+                create_corpus_response = self._request(endpoint=\"create-corpus\", data=data)\n+                self.connector_config.corpus_id = create_corpus_response.get(\"corpusId\")\n+\n+        except Exception as e:\n+            return str(e) + \"\\n\" + \"\".join(traceback.TracebackException.from_exception(e).format())",
        "comment_created_at": "2024-01-09T16:01:11+00:00",
        "comment_author": "rbiseck3",
        "comment_body": "Whys is this returning a string on error? This can reraise a new error based on the first one if needed, but otherwise won't this just be swallowed up? Also it's returning a string, but that string is never used since this gets called without an output in initialize:\r\n```python\r\n    def initialize(self):\r\n        self.vectara()\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1446764209",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2357,
        "pr_file": "unstructured/ingest/connector/vectara.py",
        "discussion_id": "1446285912",
        "commented_code": "@@ -0,0 +1,246 @@\n+import datetime\n+import json\n+import traceback\n+import typing as t\n+import uuid\n+from dataclasses import dataclass\n+\n+import requests\n+\n+from unstructured.ingest.enhanced_dataclass import enhanced_field\n+from unstructured.ingest.error import DestinationConnectionError\n+from unstructured.ingest.interfaces import (\n+    AccessConfig,\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    BaseIngestDoc,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.staging.base import flatten_dict\n+\n+\n+@dataclass\n+class VectaraAccessConfig(AccessConfig):\n+    oauth_client_id: str = enhanced_field(sensitive=True)\n+    oauth_secret: str = enhanced_field(sensitive=True)\n+\n+\n+@dataclass\n+class SimpleVectaraConfig(BaseConnectorConfig):\n+    access_config: VectaraAccessConfig\n+    customer_id: t.AnyStr = None\n+    corpus_name: t.AnyStr = \"vectara-unstructured\"\n+    corpus_id: t.AnyStr = None\n+\n+\n+@dataclass\n+class VectaraWriteConfig(WriteConfig):\n+    batch_size: int = 100\n+\n+\n+@dataclass\n+class VectaraDestinationConnector(BaseDestinationConnector):\n+    write_config: VectaraWriteConfig\n+    connector_config: SimpleVectaraConfig\n+\n+    BASE_URL = \"https://api.vectara.io/v1\"\n+\n+    @DestinationConnectionError.wrap\n+    def vectara(self):\n+        \"\"\"\n+        Check the connection for Vectara and validate corpus exists.\n+        - If more than one exists - then return a message\n+        - If exactly one exists with this name - use it.\n+        - If does not exist - create it.\n+        \"\"\"\n+        try:\n+            jwt_token = self._get_jwt_token()\n+            if not jwt_token:\n+                return \"Unable to get JWT Token. Confirm your Client ID and Client Secret.\"\n+\n+            list_corpora_response = self._request(\n+                endpoint=\"list-corpora\",\n+                data={\"numResults\": 100, \"filter\": self.connector_config.corpus_name},\n+            )\n+\n+            possible_corpora_ids_names_map = {\n+                corpus.get(\"id\"): corpus.get(\"name\")\n+                for corpus in list_corpora_response.get(\"corpus\")\n+                if corpus.get(\"name\") == self.connector_config.corpus_name\n+            }\n+\n+            if len(possible_corpora_ids_names_map) > 1:\n+                return f\"Multiple Corpora exist with name {self.connector_config.corpus_name}\"\n+            if len(possible_corpora_ids_names_map) == 1:\n+                self.connector_config.corpus_id = list(possible_corpora_ids_names_map.keys())[0]\n+            else:\n+                data = {\n+                    \"corpus\": {\n+                        \"name\": self.connector_config.corpus_name,\n+                    }\n+                }\n+                create_corpus_response = self._request(endpoint=\"create-corpus\", data=data)\n+                self.connector_config.corpus_id = create_corpus_response.get(\"corpusId\")\n+\n+        except Exception as e:\n+            return str(e) + \"\\n\" + \"\".join(traceback.TracebackException.from_exception(e).format())",
        "comment_created_at": "2024-01-10T00:35:38+00:00",
        "comment_author": "potter-potter",
        "comment_body": "Yep. My mistake. Leftover. No need to be a string. Should just raise.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1433261395",
    "pr_number": 2299,
    "pr_file": "unstructured/metrics/evaluate.py",
    "created_at": "2023-12-20T22:58:21+00:00",
    "commented_code": "fn_txt = fn + \".txt\"\n \n         if fn_txt in source_list:  # type: ignore\n-            output_cct = elements_to_text(elements_from_json(os.path.join(output_dir, doc)))\n-            source_cct = _read_text(os.path.join(source_dir, fn_txt))\n+            try:\n+                output_cct = _prepare_output_cct(os.path.join(output_dir, doc), output_type)\n+                source_cct = _read_text(os.path.join(source_dir, fn_txt))\n+            except Exception:\n+                pass",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1433261395",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2299,
        "pr_file": "unstructured/metrics/evaluate.py",
        "discussion_id": "1433261395",
        "commented_code": "@@ -77,11 +88,13 @@ def measure_text_extraction_accuracy(\n             fn_txt = fn + \".txt\"\n \n         if fn_txt in source_list:  # type: ignore\n-            output_cct = elements_to_text(elements_from_json(os.path.join(output_dir, doc)))\n-            source_cct = _read_text(os.path.join(source_dir, fn_txt))\n+            try:\n+                output_cct = _prepare_output_cct(os.path.join(output_dir, doc), output_type)\n+                source_cct = _read_text(os.path.join(source_dir, fn_txt))\n+            except Exception:\n+                pass",
        "comment_created_at": "2023-12-20T22:58:21+00:00",
        "comment_author": "badGarnet",
        "comment_body": "this might be too generous; we at least should bring the exception message up so the log have a record on if any doc has issues with processing",
        "pr_file_module": null
      }
    ]
  }
]