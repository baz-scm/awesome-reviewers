[
  {
    "discussion_id": "1907366755",
    "pr_number": 3860,
    "pr_file": "unstructured/partition/utils/config.py",
    "created_at": "2025-01-08T15:23:40+00:00",
    "commented_code": "\"\"\"optimum text height for tesseract OCR\"\"\"\n         return self._get_int(\"TESSERACT_OPTIMUM_TEXT_HEIGHT\", 20)\n \n+    @property\n+    def TESSERACT_CHARACTER_CONFIDENCE_THRESHOLD(self) -> int:\n+        \"\"\"Tesseract predictions with confidence below this threshold are ignored\"\"\"\n+        return self._get_float(\"TESSERACT_CHARACTER_CONFIDENCE_THRESHOLD\", 0.0)",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1907366755",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3860,
        "pr_file": "unstructured/partition/utils/config.py",
        "discussion_id": "1907366755",
        "commented_code": "@@ -96,6 +96,11 @@ def TESSERACT_OPTIMUM_TEXT_HEIGHT(self) -> int:\n         \"\"\"optimum text height for tesseract OCR\"\"\"\n         return self._get_int(\"TESSERACT_OPTIMUM_TEXT_HEIGHT\", 20)\n \n+    @property\n+    def TESSERACT_CHARACTER_CONFIDENCE_THRESHOLD(self) -> int:\n+        \"\"\"Tesseract predictions with confidence below this threshold are ignored\"\"\"\n+        return self._get_float(\"TESSERACT_CHARACTER_CONFIDENCE_THRESHOLD\", 0.0)",
        "comment_created_at": "2025-01-08T15:23:40+00:00",
        "comment_author": "MaksOpp",
        "comment_body": "I wonder, maybe we'd like to have some really low default threshold, i.e. 0.1, just to filter out complete garbage chars?",
        "pr_file_module": null
      },
      {
        "comment_id": "1910692783",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3860,
        "pr_file": "unstructured/partition/utils/config.py",
        "discussion_id": "1907366755",
        "commented_code": "@@ -96,6 +96,11 @@ def TESSERACT_OPTIMUM_TEXT_HEIGHT(self) -> int:\n         \"\"\"optimum text height for tesseract OCR\"\"\"\n         return self._get_int(\"TESSERACT_OPTIMUM_TEXT_HEIGHT\", 20)\n \n+    @property\n+    def TESSERACT_CHARACTER_CONFIDENCE_THRESHOLD(self) -> int:\n+        \"\"\"Tesseract predictions with confidence below this threshold are ignored\"\"\"\n+        return self._get_float(\"TESSERACT_CHARACTER_CONFIDENCE_THRESHOLD\", 0.0)",
        "comment_created_at": "2025-01-10T17:24:45+00:00",
        "comment_author": "badGarnet",
        "comment_body": "I am ok with 0; the default behavior is no filter at all so this PR should just keep that for now. We can use followups to change this value.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1907369021",
    "pr_number": 3860,
    "pr_file": "unstructured/partition/utils/ocr_models/tesseract_ocr.py",
    "created_at": "2025-01-08T15:25:13+00:00",
    "commented_code": "np.round(env_config.TESSERACT_OPTIMUM_TEXT_HEIGHT / text_height, 1),\n                 max_zoom,\n             )\n-            ocr_df = unstructured_pytesseract.image_to_data(\n+            ocr_df = self.image_to_data_with_character_confidence_filter(\n                 np.array(zoom_image(image, zoom)),\n                 lang=self.language,\n-                output_type=Output.DATAFRAME,\n+                character_confidence_threshold=env_config.TESSERACT_CHARACTER_CONFIDENCE_THRESHOLD,\n             )\n             ocr_df = ocr_df.dropna()\n-\n         ocr_regions = self.parse_data(ocr_df, zoom=zoom)\n \n         return ocr_regions\n \n+    def image_to_data_with_character_confidence_filter(\n+        self,\n+        image: np.ndarray,\n+        lang: str = \"eng\",\n+        config: str = \"\",\n+        character_confidence_threshold: float = 0.5,",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1907369021",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3860,
        "pr_file": "unstructured/partition/utils/ocr_models/tesseract_ocr.py",
        "discussion_id": "1907369021",
        "commented_code": "@@ -76,17 +77,89 @@ def get_layout_from_image(self, image: PILImage.Image) -> List[TextRegion]:\n                 np.round(env_config.TESSERACT_OPTIMUM_TEXT_HEIGHT / text_height, 1),\n                 max_zoom,\n             )\n-            ocr_df = unstructured_pytesseract.image_to_data(\n+            ocr_df = self.image_to_data_with_character_confidence_filter(\n                 np.array(zoom_image(image, zoom)),\n                 lang=self.language,\n-                output_type=Output.DATAFRAME,\n+                character_confidence_threshold=env_config.TESSERACT_CHARACTER_CONFIDENCE_THRESHOLD,\n             )\n             ocr_df = ocr_df.dropna()\n-\n         ocr_regions = self.parse_data(ocr_df, zoom=zoom)\n \n         return ocr_regions\n \n+    def image_to_data_with_character_confidence_filter(\n+        self,\n+        image: np.ndarray,\n+        lang: str = \"eng\",\n+        config: str = \"\",\n+        character_confidence_threshold: float = 0.5,",
        "comment_created_at": "2025-01-08T15:25:13+00:00",
        "comment_author": "MaksOpp",
        "comment_body": "Here we are adding some default, so maybe let's also keep it in config?",
        "pr_file_module": null
      },
      {
        "comment_id": "1907371134",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3860,
        "pr_file": "unstructured/partition/utils/ocr_models/tesseract_ocr.py",
        "discussion_id": "1907369021",
        "commented_code": "@@ -76,17 +77,89 @@ def get_layout_from_image(self, image: PILImage.Image) -> List[TextRegion]:\n                 np.round(env_config.TESSERACT_OPTIMUM_TEXT_HEIGHT / text_height, 1),\n                 max_zoom,\n             )\n-            ocr_df = unstructured_pytesseract.image_to_data(\n+            ocr_df = self.image_to_data_with_character_confidence_filter(\n                 np.array(zoom_image(image, zoom)),\n                 lang=self.language,\n-                output_type=Output.DATAFRAME,\n+                character_confidence_threshold=env_config.TESSERACT_CHARACTER_CONFIDENCE_THRESHOLD,\n             )\n             ocr_df = ocr_df.dropna()\n-\n         ocr_regions = self.parse_data(ocr_df, zoom=zoom)\n \n         return ocr_regions\n \n+    def image_to_data_with_character_confidence_filter(\n+        self,\n+        image: np.ndarray,\n+        lang: str = \"eng\",\n+        config: str = \"\",\n+        character_confidence_threshold: float = 0.5,",
        "comment_created_at": "2025-01-08T15:26:44+00:00",
        "comment_author": "MaksOpp",
        "comment_body": "I see below we again have 0.5 as a default in `hocr_to_dataframe`, so either way, I would unify those ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1804846540",
    "pr_number": 3724,
    "pr_file": "unstructured/partition/api.py",
    "created_at": "2024-10-17T14:00:58+00:00",
    "commented_code": ")\n \n \n+def get_retries_config(\n+    retries_connection_errors: Optional[bool],\n+    retries_exponent: Optional[float],\n+    retries_initial_interval: Optional[int],\n+    retries_max_elapsed_time: Optional[int],\n+    retries_max_interval: Optional[int],\n+    sdk: UnstructuredClient,\n+) -> Optional[retries.RetryConfig]:\n+    \"\"\"Constructs a RetryConfig object from the provided parameters. If any of the parameters\n+    are None, the default values are taken from the SDK configuration or the default constants.\n+\n+    If all parameters are None, returns None (and the SDK-managed defaults are used within the\n+    client)\n+\n+    The solution is not perfect as the RetryConfig object does not include the defaults by\n+    itself so we might need to construct it basing on our defaults.\n+\n+    Parameters\n+    ----------\n+    retries_connection_errors\n+        Defines whether to retry on connection errors.\n+    retries_exponent\n+        Defines the exponential factor to increase the interval between retries.\n+    retries_initial_interval\n+        Defines the time interval to wait before the first retry in case of a request failure.\n+    retries_max_elapsed_time\n+        Defines the maximum time to wait for retries. If exceeded, the original exception is raised.\n+    retries_max_interval\n+        Defines the maximum time interval to wait between retries.\n+    sdk\n+        The UnstructuredClient object to take the default values from.\n+    \"\"\"\n+    retries_config = None\n+    default_retries_config = sdk.sdk_configuration.retry_config\n+    if any(\n+        setting is not None\n+        for setting in (\n+            retries_initial_interval,\n+            retries_max_interval,\n+            retries_exponent,\n+            retries_max_elapsed_time,\n+            retries_connection_errors,\n+        )\n+    ):\n+        default_retries_initial_interval = (\n+            default_retries_config.backoff.initial_interval\n+            if default_retries_config and default_retries_config.backoff.initial_interval\n+            else DEFAULT_RETRIES_INITIAL_INTERVAL_SEC\n+        )",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1804846540",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3724,
        "pr_file": "unstructured/partition/api.py",
        "discussion_id": "1804846540",
        "commented_code": "@@ -97,6 +136,94 @@ def partition_via_api(\n         )\n \n \n+def get_retries_config(\n+    retries_connection_errors: Optional[bool],\n+    retries_exponent: Optional[float],\n+    retries_initial_interval: Optional[int],\n+    retries_max_elapsed_time: Optional[int],\n+    retries_max_interval: Optional[int],\n+    sdk: UnstructuredClient,\n+) -> Optional[retries.RetryConfig]:\n+    \"\"\"Constructs a RetryConfig object from the provided parameters. If any of the parameters\n+    are None, the default values are taken from the SDK configuration or the default constants.\n+\n+    If all parameters are None, returns None (and the SDK-managed defaults are used within the\n+    client)\n+\n+    The solution is not perfect as the RetryConfig object does not include the defaults by\n+    itself so we might need to construct it basing on our defaults.\n+\n+    Parameters\n+    ----------\n+    retries_connection_errors\n+        Defines whether to retry on connection errors.\n+    retries_exponent\n+        Defines the exponential factor to increase the interval between retries.\n+    retries_initial_interval\n+        Defines the time interval to wait before the first retry in case of a request failure.\n+    retries_max_elapsed_time\n+        Defines the maximum time to wait for retries. If exceeded, the original exception is raised.\n+    retries_max_interval\n+        Defines the maximum time interval to wait between retries.\n+    sdk\n+        The UnstructuredClient object to take the default values from.\n+    \"\"\"\n+    retries_config = None\n+    default_retries_config = sdk.sdk_configuration.retry_config\n+    if any(\n+        setting is not None\n+        for setting in (\n+            retries_initial_interval,\n+            retries_max_interval,\n+            retries_exponent,\n+            retries_max_elapsed_time,\n+            retries_connection_errors,\n+        )\n+    ):\n+        default_retries_initial_interval = (\n+            default_retries_config.backoff.initial_interval\n+            if default_retries_config and default_retries_config.backoff.initial_interval\n+            else DEFAULT_RETRIES_INITIAL_INTERVAL_SEC\n+        )",
        "comment_created_at": "2024-10-17T14:00:58+00:00",
        "comment_author": "badGarnet",
        "comment_body": "small nit; this pattern repeated enough times for us to consider refactor like\r\n```suggestion\r\n        def _config_or_default(attr: str, default_val: int | float):\r\n            return (getattr(default_retries_config.backoff, attr) if default_retries_config else None) or default_val\r\n        default_retries_initial_interval = _config_or_default(\"initial_interval\", DEFAULT_RETRIES_INITIAL_INTERVAL_SEC)\r\n        # and those below\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1804849817",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3724,
        "pr_file": "unstructured/partition/api.py",
        "discussion_id": "1804846540",
        "commented_code": "@@ -97,6 +136,94 @@ def partition_via_api(\n         )\n \n \n+def get_retries_config(\n+    retries_connection_errors: Optional[bool],\n+    retries_exponent: Optional[float],\n+    retries_initial_interval: Optional[int],\n+    retries_max_elapsed_time: Optional[int],\n+    retries_max_interval: Optional[int],\n+    sdk: UnstructuredClient,\n+) -> Optional[retries.RetryConfig]:\n+    \"\"\"Constructs a RetryConfig object from the provided parameters. If any of the parameters\n+    are None, the default values are taken from the SDK configuration or the default constants.\n+\n+    If all parameters are None, returns None (and the SDK-managed defaults are used within the\n+    client)\n+\n+    The solution is not perfect as the RetryConfig object does not include the defaults by\n+    itself so we might need to construct it basing on our defaults.\n+\n+    Parameters\n+    ----------\n+    retries_connection_errors\n+        Defines whether to retry on connection errors.\n+    retries_exponent\n+        Defines the exponential factor to increase the interval between retries.\n+    retries_initial_interval\n+        Defines the time interval to wait before the first retry in case of a request failure.\n+    retries_max_elapsed_time\n+        Defines the maximum time to wait for retries. If exceeded, the original exception is raised.\n+    retries_max_interval\n+        Defines the maximum time interval to wait between retries.\n+    sdk\n+        The UnstructuredClient object to take the default values from.\n+    \"\"\"\n+    retries_config = None\n+    default_retries_config = sdk.sdk_configuration.retry_config\n+    if any(\n+        setting is not None\n+        for setting in (\n+            retries_initial_interval,\n+            retries_max_interval,\n+            retries_exponent,\n+            retries_max_elapsed_time,\n+            retries_connection_errors,\n+        )\n+    ):\n+        default_retries_initial_interval = (\n+            default_retries_config.backoff.initial_interval\n+            if default_retries_config and default_retries_config.backoff.initial_interval\n+            else DEFAULT_RETRIES_INITIAL_INTERVAL_SEC\n+        )",
        "comment_created_at": "2024-10-17T14:02:54+00:00",
        "comment_author": "badGarnet",
        "comment_body": "and one more note this logic means if the config value is 0 we will be using default; is that intended or should we change the second condition to `default_retries_config.backoff.initial_interval is not None` (I got burned by this recently \ud83d\ude2c )",
        "pr_file_module": null
      },
      {
        "comment_id": "1806155382",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3724,
        "pr_file": "unstructured/partition/api.py",
        "discussion_id": "1804846540",
        "commented_code": "@@ -97,6 +136,94 @@ def partition_via_api(\n         )\n \n \n+def get_retries_config(\n+    retries_connection_errors: Optional[bool],\n+    retries_exponent: Optional[float],\n+    retries_initial_interval: Optional[int],\n+    retries_max_elapsed_time: Optional[int],\n+    retries_max_interval: Optional[int],\n+    sdk: UnstructuredClient,\n+) -> Optional[retries.RetryConfig]:\n+    \"\"\"Constructs a RetryConfig object from the provided parameters. If any of the parameters\n+    are None, the default values are taken from the SDK configuration or the default constants.\n+\n+    If all parameters are None, returns None (and the SDK-managed defaults are used within the\n+    client)\n+\n+    The solution is not perfect as the RetryConfig object does not include the defaults by\n+    itself so we might need to construct it basing on our defaults.\n+\n+    Parameters\n+    ----------\n+    retries_connection_errors\n+        Defines whether to retry on connection errors.\n+    retries_exponent\n+        Defines the exponential factor to increase the interval between retries.\n+    retries_initial_interval\n+        Defines the time interval to wait before the first retry in case of a request failure.\n+    retries_max_elapsed_time\n+        Defines the maximum time to wait for retries. If exceeded, the original exception is raised.\n+    retries_max_interval\n+        Defines the maximum time interval to wait between retries.\n+    sdk\n+        The UnstructuredClient object to take the default values from.\n+    \"\"\"\n+    retries_config = None\n+    default_retries_config = sdk.sdk_configuration.retry_config\n+    if any(\n+        setting is not None\n+        for setting in (\n+            retries_initial_interval,\n+            retries_max_interval,\n+            retries_exponent,\n+            retries_max_elapsed_time,\n+            retries_connection_errors,\n+        )\n+    ):\n+        default_retries_initial_interval = (\n+            default_retries_config.backoff.initial_interval\n+            if default_retries_config and default_retries_config.backoff.initial_interval\n+            else DEFAULT_RETRIES_INITIAL_INTERVAL_SEC\n+        )",
        "comment_created_at": "2024-10-18T09:05:07+00:00",
        "comment_author": "pawel-kmiecik",
        "comment_body": "I've started the implementation to check for `None` (as you suggested) but in this specific case all these settings should have values greater than 0 (zero is incorrect) - so this simplified check should be ok.\r\nI'll mention it in the comments.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1651423342",
    "pr_number": 3264,
    "pr_file": "unstructured/ingest/v2/processes/partitioner.py",
    "created_at": "2024-06-24T18:02:22+00:00",
    "commented_code": "from unstructured_client.models.shared import Files, PartitionParameters\n \n         partition_request = self.config.to_partition_kwargs()\n+        possible_fields = [f.name for f in fields(PartitionParameters)]\n+        filtered_partition_request = {\n+            k: v for k, v in partition_request.items() if k in possible_fields\n+        }\n+        if len(filtered_partition_request) == len(partition_request):",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1651423342",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3264,
        "pr_file": "unstructured/ingest/v2/processes/partitioner.py",
        "discussion_id": "1651423342",
        "commented_code": "@@ -116,14 +116,25 @@ def create_partition_parameters(self, filename: Path) -> \"PartitionParameters\":\n         from unstructured_client.models.shared import Files, PartitionParameters\n \n         partition_request = self.config.to_partition_kwargs()\n+        possible_fields = [f.name for f in fields(PartitionParameters)]\n+        filtered_partition_request = {\n+            k: v for k, v in partition_request.items() if k in possible_fields\n+        }\n+        if len(filtered_partition_request) == len(partition_request):",
        "comment_created_at": "2024-06-24T18:02:22+00:00",
        "comment_author": "potter-potter",
        "comment_body": "I don't understand this. I ran a test with \r\n--strategy fast \\\r\n--partition-by-api \\\r\n\r\nAnd got this:\r\n`Following fields were omitted due to not being supported by the currently used unstructured client: `\r\nwith nothing past the `:`",
        "pr_file_module": null
      },
      {
        "comment_id": "1651440475",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3264,
        "pr_file": "unstructured/ingest/v2/processes/partitioner.py",
        "discussion_id": "1651423342",
        "commented_code": "@@ -116,14 +116,25 @@ def create_partition_parameters(self, filename: Path) -> \"PartitionParameters\":\n         from unstructured_client.models.shared import Files, PartitionParameters\n \n         partition_request = self.config.to_partition_kwargs()\n+        possible_fields = [f.name for f in fields(PartitionParameters)]\n+        filtered_partition_request = {\n+            k: v for k, v in partition_request.items() if k in possible_fields\n+        }\n+        if len(filtered_partition_request) == len(partition_request):",
        "comment_created_at": "2024-06-24T18:12:21+00:00",
        "comment_author": "rbiseck3",
        "comment_body": "Had the boolean logic wrong, updated it from `==` to `!=` so this shouldn't show up anymore if no fields were omitted. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1604064061",
    "pr_number": 3035,
    "pr_file": "unstructured/partition/auto.py",
    "created_at": "2024-05-16T21:32:01+00:00",
    "commented_code": "encoding: Optional[str] = None,\n     paragraph_grouper: Optional[Callable[[str], str]] | Literal[False] = None,\n     headers: dict[str, str] = {},\n-    skip_infer_table_types: list[str] = [],\n+    skip_infer_table_types: list[str] = [\"pdf\", \"jpg\", \"png\", \"xls\", \"xlsx\", \"heic\"],",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1604064061",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3035,
        "pr_file": "unstructured/partition/auto.py",
        "discussion_id": "1604064061",
        "commented_code": "@@ -141,12 +141,12 @@ def partition(\n     encoding: Optional[str] = None,\n     paragraph_grouper: Optional[Callable[[str], str]] | Literal[False] = None,\n     headers: dict[str, str] = {},\n-    skip_infer_table_types: list[str] = [],\n+    skip_infer_table_types: list[str] = [\"pdf\", \"jpg\", \"png\", \"xls\", \"xlsx\", \"heic\"],",
        "comment_created_at": "2024-05-16T21:32:01+00:00",
        "comment_author": "scanny",
        "comment_body": "I recommend we only include PDF and image types in this list, not `xls` or `xlsx`.\r\n\r\n### Rationale\r\nThe \"infer_table\" behavior is historically somewhat confused when it comes to non-PDF/Image file-types.\r\n\r\nThe `infer_table_structure` argument (predecessor to `skip_infer_table_types`) controlled two things and we often want those two controlled separately for non-PDF/Image types.\r\n\r\n1. For PDF/Image file-types, `infer_table_types` controlled partitioning \"expense\", both money and time, by controlling whether the extra compute of using a table-inference model was engaged. This was important (and still is) for PDF/Image because doing this can take a lot more CPU and elapsed-time.\r\n2. It _also_ determined whether `.metadata.text_as_html` was added to `Table` elements. For PDF/Image this was obvious enough because if you didn't infer tables there was no `Table` element to put the `.metadata.text_as_html` on.\r\n3. However, for non-PDF/Image file-types, no inference was ever required or used to detect tables. So `Table` elements always appear for those file-types when they occur in the document.\r\n4. In that non-PDF/Image case, `infer_table_structure` came to mean: \"include `.metadata.text_as_html` on `Table` elements\".\r\n### My POV\r\n- In my opinion, any \"infer_table\" option, regardless of how it is specified, should be limited to PDF/Image types.\r\n- In addition, we can have a `suppress_metadata_fields: list[str] = []` parameter that is applicable to all partitioners and handled in post-partitioning processing (implemented in one of the decorators), and that removes any metadata the user prefers not to appear in the payload.\r\n- In the meantime, I believe only PDF/Image partitioners are actually sensitive to the value of `skip_infer_table_types`, so probably not a big deal, but still, better to remove `xls` and `xlsx` from the list I think.",
        "pr_file_module": null
      },
      {
        "comment_id": "1604068857",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3035,
        "pr_file": "unstructured/partition/auto.py",
        "discussion_id": "1604064061",
        "commented_code": "@@ -141,12 +141,12 @@ def partition(\n     encoding: Optional[str] = None,\n     paragraph_grouper: Optional[Callable[[str], str]] | Literal[False] = None,\n     headers: dict[str, str] = {},\n-    skip_infer_table_types: list[str] = [],\n+    skip_infer_table_types: list[str] = [\"pdf\", \"jpg\", \"png\", \"xls\", \"xlsx\", \"heic\"],",
        "comment_created_at": "2024-05-16T21:38:22+00:00",
        "comment_author": "qued",
        "comment_body": "Agree with all this.",
        "pr_file_module": null
      },
      {
        "comment_id": "1604096843",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 3035,
        "pr_file": "unstructured/partition/auto.py",
        "discussion_id": "1604064061",
        "commented_code": "@@ -141,12 +141,12 @@ def partition(\n     encoding: Optional[str] = None,\n     paragraph_grouper: Optional[Callable[[str], str]] | Literal[False] = None,\n     headers: dict[str, str] = {},\n-    skip_infer_table_types: list[str] = [],\n+    skip_infer_table_types: list[str] = [\"pdf\", \"jpg\", \"png\", \"xls\", \"xlsx\", \"heic\"],",
        "comment_created_at": "2024-05-16T21:54:55+00:00",
        "comment_author": "MthwRobinson",
        "comment_body": "Updated!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1446278726",
    "pr_number": 2357,
    "pr_file": "unstructured/ingest/cli/cmds/vectara.py",
    "created_at": "2024-01-09T15:55:44+00:00",
    "commented_code": "+import typing as t\n+from dataclasses import dataclass\n+\n+import click\n+\n+from unstructured.ingest.cli.interfaces import CliConfig\n+from unstructured.ingest.connector.vectara import SimpleVectaraConfig, VectaraWriteConfig\n+\n+\n+@dataclass\n+class VectaraCliWriteConfig(SimpleVectaraConfig, CliConfig):\n+    @staticmethod\n+    def get_cli_options() -> t.List[click.Option]:\n+        options = [\n+            click.Option(\n+                [\"--customer-id\"],\n+                required=True,\n+                type=str,\n+                help=\"The Vectara customer-id.\",\n+                envvar=\"VECTARA_CUSTOMER_ID\",\n+                show_envvar=True,\n+            ),\n+            click.Option(\n+                [\"--oauth-client-id\"],\n+                required=True,\n+                type=str,\n+                help=\"Vectara OAuth2 client ID.\",\n+                envvar=\"VECTARA_OAUTH_CLIENT_ID\",\n+                show_envvar=True,\n+            ),\n+            click.Option(\n+                [\"--oauth-secret\"],\n+                required=True,\n+                type=str,\n+                help=\"Vectara OAuth2 secret.\",\n+                envvar=\"VECTARA_OAUTH_SECRET\",\n+                show_envvar=True,\n+            ),\n+            click.Option(\n+                [\"--corpus-name\"],\n+                required=False,\n+                default=\"vectara-unstructured\",",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1446278726",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2357,
        "pr_file": "unstructured/ingest/cli/cmds/vectara.py",
        "discussion_id": "1446278726",
        "commented_code": "@@ -0,0 +1,59 @@\n+import typing as t\n+from dataclasses import dataclass\n+\n+import click\n+\n+from unstructured.ingest.cli.interfaces import CliConfig\n+from unstructured.ingest.connector.vectara import SimpleVectaraConfig, VectaraWriteConfig\n+\n+\n+@dataclass\n+class VectaraCliWriteConfig(SimpleVectaraConfig, CliConfig):\n+    @staticmethod\n+    def get_cli_options() -> t.List[click.Option]:\n+        options = [\n+            click.Option(\n+                [\"--customer-id\"],\n+                required=True,\n+                type=str,\n+                help=\"The Vectara customer-id.\",\n+                envvar=\"VECTARA_CUSTOMER_ID\",\n+                show_envvar=True,\n+            ),\n+            click.Option(\n+                [\"--oauth-client-id\"],\n+                required=True,\n+                type=str,\n+                help=\"Vectara OAuth2 client ID.\",\n+                envvar=\"VECTARA_OAUTH_CLIENT_ID\",\n+                show_envvar=True,\n+            ),\n+            click.Option(\n+                [\"--oauth-secret\"],\n+                required=True,\n+                type=str,\n+                help=\"Vectara OAuth2 secret.\",\n+                envvar=\"VECTARA_OAUTH_SECRET\",\n+                show_envvar=True,\n+            ),\n+            click.Option(\n+                [\"--corpus-name\"],\n+                required=False,\n+                default=\"vectara-unstructured\",",
        "comment_created_at": "2024-01-09T15:55:44+00:00",
        "comment_author": "rbiseck3",
        "comment_body": "Rather than use an `unstructured` default, let's make this required and have no default. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1446691331",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2357,
        "pr_file": "unstructured/ingest/cli/cmds/vectara.py",
        "discussion_id": "1446278726",
        "commented_code": "@@ -0,0 +1,59 @@\n+import typing as t\n+from dataclasses import dataclass\n+\n+import click\n+\n+from unstructured.ingest.cli.interfaces import CliConfig\n+from unstructured.ingest.connector.vectara import SimpleVectaraConfig, VectaraWriteConfig\n+\n+\n+@dataclass\n+class VectaraCliWriteConfig(SimpleVectaraConfig, CliConfig):\n+    @staticmethod\n+    def get_cli_options() -> t.List[click.Option]:\n+        options = [\n+            click.Option(\n+                [\"--customer-id\"],\n+                required=True,\n+                type=str,\n+                help=\"The Vectara customer-id.\",\n+                envvar=\"VECTARA_CUSTOMER_ID\",\n+                show_envvar=True,\n+            ),\n+            click.Option(\n+                [\"--oauth-client-id\"],\n+                required=True,\n+                type=str,\n+                help=\"Vectara OAuth2 client ID.\",\n+                envvar=\"VECTARA_OAUTH_CLIENT_ID\",\n+                show_envvar=True,\n+            ),\n+            click.Option(\n+                [\"--oauth-secret\"],\n+                required=True,\n+                type=str,\n+                help=\"Vectara OAuth2 secret.\",\n+                envvar=\"VECTARA_OAUTH_SECRET\",\n+                show_envvar=True,\n+            ),\n+            click.Option(\n+                [\"--corpus-name\"],\n+                required=False,\n+                default=\"vectara-unstructured\",",
        "comment_created_at": "2024-01-09T22:34:54+00:00",
        "comment_author": "potter-potter",
        "comment_body": "yep. The default doesn't make any sense...",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1466773846",
    "pr_number": 2435,
    "pr_file": "unstructured/partition/lang.py",
    "created_at": "2024-01-25T18:30:28+00:00",
    "commented_code": "return TESSERACT_LANGUAGES_SPLITTER.join(converted_languages)\n \n \n-def check_languages(languages: Optional[List[str]], ocr_languages: Optional[str]):\n-    \"\"\"Handle `ocr_languages` and `languages`, defining `languages` to ['eng'] as default and\n-    converting `ocr_languages` if needed\"\"\"\n-    if languages is None:\n-        languages = [\"eng\"]\n+def check_language_args(\n+    languages: Optional[List[str]], ocr_languages: Optional[str]\n+) -> Union[list[str], None]:\n+    \"\"\"Handle users defining both `ocr_languages` and `languages`, giving preference to `languages`\n+    and converting `ocr_languages` if needed, but defaulting to `None.\n+\n+    `ocr_languages` is only a parameter for `auto.partition`, `partition_image`, & `partition_pdf`.\n+    `ocr_languages` should not be defined as 'auto' since 'auto' is intended for language detection\n+    which is not supported by `partition_image` or `partition_pdf`.\"\"\"\n+    # --- Clean and update defaults\n+    if ocr_languages == \"auto\":\n+        raise ValueError(\n+            \"`ocr_languages` is deprecated but was used to extract text from pdfs and images. \"\n+            \"The 'auto' argument is only for language *detection* when it is assigned \"\n+            \"to `languages` and partitioning documents other than pdfs or images. \"\n+            \"Language detection is not currently supported in pdfs or images.\"\n+        )\n \n-    if not isinstance(languages, list):\n+    if ocr_languages:\n+        ocr_languages = _clean_ocr_languages_arg(ocr_languages)\n+        logger.warning(\n+            \"The ocr_languages kwarg will be deprecated in a future version of unstructured. \"\n+            \"Please use languages instead.\",\n+        )\n+\n+    # raise error if languages is not None of a list\n+    if not (isinstance(languages, list) or languages is None):\n         raise TypeError(\n             \"The language parameter must be a list of language codes as strings, ex. ['eng']\",\n         )\n \n-    if ocr_languages is not None:\n-        if languages != [\"eng\"]:\n-            raise ValueError(\n-                \"Only one of languages and ocr_languages should be specified. \"\n-                \"languages is preferred. ocr_languages is marked for deprecation.\",\n-            )\n+    # --- If `languages` is a null/default value and `ocr_languages` is defined, use `ocr_languages`\n+    if ocr_languages and (\n+        languages == [\"auto\"] or languages == [\"\"] or languages is None or not languages[0]\n+    ):\n+        languages = ocr_languages.split(TESSERACT_LANGUAGES_SPLITTER)\n+        logger.warning(\n+            \"Only one of languages and ocr_languages should be specified. \"\n+            \"languages is preferred. ocr_languages is marked for deprecation.\",\n+        )\n \n+    # --- Clean `languages`\n+    # If \"auto\" is included in the list of inputs, language detection will be triggered downstream.\n+    # The rest of the inputted languages are ignored.\n+    if languages:\n+        if \"auto\" not in languages:\n+            for i, lang in enumerate(languages):\n+                languages[i] = TESSERACT_LANGUAGES_AND_CODES.get(lang.lower(), lang)\n+\n+            str_languages = _clean_ocr_languages_arg(languages)\n+            languages = str_languages.split(TESSERACT_LANGUAGES_SPLITTER)\n+        # else, remove the extraneous languages.\n+        # NOTE (jennings): \"auto\" should only be used for partitioners OTHER THAN `_pdf` or `_image`\n         else:\n-            languages = convert_old_ocr_languages_to_languages(ocr_languages)\n-            logger.warning(\n-                \"The ocr_languages kwarg will be deprecated in a future version of unstructured. \"\n-                \"Please use languages instead.\",\n-            )\n-    return languages\n+            # define as 'auto' for language detection when partitioning non-pdfs or -images\n+            languages = [\"auto\"]\n+        return languages\n+\n+    return None",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1466773846",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2435,
        "pr_file": "unstructured/partition/lang.py",
        "discussion_id": "1466773846",
        "commented_code": "@@ -167,34 +167,68 @@ def prepare_languages_for_tesseract(languages: Optional[List[str]] = [\"eng\"]):\n     return TESSERACT_LANGUAGES_SPLITTER.join(converted_languages)\n \n \n-def check_languages(languages: Optional[List[str]], ocr_languages: Optional[str]):\n-    \"\"\"Handle `ocr_languages` and `languages`, defining `languages` to ['eng'] as default and\n-    converting `ocr_languages` if needed\"\"\"\n-    if languages is None:\n-        languages = [\"eng\"]\n+def check_language_args(\n+    languages: Optional[List[str]], ocr_languages: Optional[str]\n+) -> Union[list[str], None]:\n+    \"\"\"Handle users defining both `ocr_languages` and `languages`, giving preference to `languages`\n+    and converting `ocr_languages` if needed, but defaulting to `None.\n+\n+    `ocr_languages` is only a parameter for `auto.partition`, `partition_image`, & `partition_pdf`.\n+    `ocr_languages` should not be defined as 'auto' since 'auto' is intended for language detection\n+    which is not supported by `partition_image` or `partition_pdf`.\"\"\"\n+    # --- Clean and update defaults\n+    if ocr_languages == \"auto\":\n+        raise ValueError(\n+            \"`ocr_languages` is deprecated but was used to extract text from pdfs and images. \"\n+            \"The 'auto' argument is only for language *detection* when it is assigned \"\n+            \"to `languages` and partitioning documents other than pdfs or images. \"\n+            \"Language detection is not currently supported in pdfs or images.\"\n+        )\n \n-    if not isinstance(languages, list):\n+    if ocr_languages:\n+        ocr_languages = _clean_ocr_languages_arg(ocr_languages)\n+        logger.warning(\n+            \"The ocr_languages kwarg will be deprecated in a future version of unstructured. \"\n+            \"Please use languages instead.\",\n+        )\n+\n+    # raise error if languages is not None of a list\n+    if not (isinstance(languages, list) or languages is None):\n         raise TypeError(\n             \"The language parameter must be a list of language codes as strings, ex. ['eng']\",\n         )\n \n-    if ocr_languages is not None:\n-        if languages != [\"eng\"]:\n-            raise ValueError(\n-                \"Only one of languages and ocr_languages should be specified. \"\n-                \"languages is preferred. ocr_languages is marked for deprecation.\",\n-            )\n+    # --- If `languages` is a null/default value and `ocr_languages` is defined, use `ocr_languages`\n+    if ocr_languages and (\n+        languages == [\"auto\"] or languages == [\"\"] or languages is None or not languages[0]\n+    ):\n+        languages = ocr_languages.split(TESSERACT_LANGUAGES_SPLITTER)\n+        logger.warning(\n+            \"Only one of languages and ocr_languages should be specified. \"\n+            \"languages is preferred. ocr_languages is marked for deprecation.\",\n+        )\n \n+    # --- Clean `languages`\n+    # If \"auto\" is included in the list of inputs, language detection will be triggered downstream.\n+    # The rest of the inputted languages are ignored.\n+    if languages:\n+        if \"auto\" not in languages:\n+            for i, lang in enumerate(languages):\n+                languages[i] = TESSERACT_LANGUAGES_AND_CODES.get(lang.lower(), lang)\n+\n+            str_languages = _clean_ocr_languages_arg(languages)\n+            languages = str_languages.split(TESSERACT_LANGUAGES_SPLITTER)\n+        # else, remove the extraneous languages.\n+        # NOTE (jennings): \"auto\" should only be used for partitioners OTHER THAN `_pdf` or `_image`\n         else:\n-            languages = convert_old_ocr_languages_to_languages(ocr_languages)\n-            logger.warning(\n-                \"The ocr_languages kwarg will be deprecated in a future version of unstructured. \"\n-                \"Please use languages instead.\",\n-            )\n-    return languages\n+            # define as 'auto' for language detection when partitioning non-pdfs or -images\n+            languages = [\"auto\"]\n+        return languages\n+\n+    return None",
        "comment_created_at": "2024-01-25T18:30:28+00:00",
        "comment_author": "christinestraub",
        "comment_body": "What do you think about returning a default value (`[\"eng\"]`) instead of `None`, which would avoid setting the default value in `pdf.py` and `image.py`?",
        "pr_file_module": null
      },
      {
        "comment_id": "1466815219",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2435,
        "pr_file": "unstructured/partition/lang.py",
        "discussion_id": "1466773846",
        "commented_code": "@@ -167,34 +167,68 @@ def prepare_languages_for_tesseract(languages: Optional[List[str]] = [\"eng\"]):\n     return TESSERACT_LANGUAGES_SPLITTER.join(converted_languages)\n \n \n-def check_languages(languages: Optional[List[str]], ocr_languages: Optional[str]):\n-    \"\"\"Handle `ocr_languages` and `languages`, defining `languages` to ['eng'] as default and\n-    converting `ocr_languages` if needed\"\"\"\n-    if languages is None:\n-        languages = [\"eng\"]\n+def check_language_args(\n+    languages: Optional[List[str]], ocr_languages: Optional[str]\n+) -> Union[list[str], None]:\n+    \"\"\"Handle users defining both `ocr_languages` and `languages`, giving preference to `languages`\n+    and converting `ocr_languages` if needed, but defaulting to `None.\n+\n+    `ocr_languages` is only a parameter for `auto.partition`, `partition_image`, & `partition_pdf`.\n+    `ocr_languages` should not be defined as 'auto' since 'auto' is intended for language detection\n+    which is not supported by `partition_image` or `partition_pdf`.\"\"\"\n+    # --- Clean and update defaults\n+    if ocr_languages == \"auto\":\n+        raise ValueError(\n+            \"`ocr_languages` is deprecated but was used to extract text from pdfs and images. \"\n+            \"The 'auto' argument is only for language *detection* when it is assigned \"\n+            \"to `languages` and partitioning documents other than pdfs or images. \"\n+            \"Language detection is not currently supported in pdfs or images.\"\n+        )\n \n-    if not isinstance(languages, list):\n+    if ocr_languages:\n+        ocr_languages = _clean_ocr_languages_arg(ocr_languages)\n+        logger.warning(\n+            \"The ocr_languages kwarg will be deprecated in a future version of unstructured. \"\n+            \"Please use languages instead.\",\n+        )\n+\n+    # raise error if languages is not None of a list\n+    if not (isinstance(languages, list) or languages is None):\n         raise TypeError(\n             \"The language parameter must be a list of language codes as strings, ex. ['eng']\",\n         )\n \n-    if ocr_languages is not None:\n-        if languages != [\"eng\"]:\n-            raise ValueError(\n-                \"Only one of languages and ocr_languages should be specified. \"\n-                \"languages is preferred. ocr_languages is marked for deprecation.\",\n-            )\n+    # --- If `languages` is a null/default value and `ocr_languages` is defined, use `ocr_languages`\n+    if ocr_languages and (\n+        languages == [\"auto\"] or languages == [\"\"] or languages is None or not languages[0]\n+    ):\n+        languages = ocr_languages.split(TESSERACT_LANGUAGES_SPLITTER)\n+        logger.warning(\n+            \"Only one of languages and ocr_languages should be specified. \"\n+            \"languages is preferred. ocr_languages is marked for deprecation.\",\n+        )\n \n+    # --- Clean `languages`\n+    # If \"auto\" is included in the list of inputs, language detection will be triggered downstream.\n+    # The rest of the inputted languages are ignored.\n+    if languages:\n+        if \"auto\" not in languages:\n+            for i, lang in enumerate(languages):\n+                languages[i] = TESSERACT_LANGUAGES_AND_CODES.get(lang.lower(), lang)\n+\n+            str_languages = _clean_ocr_languages_arg(languages)\n+            languages = str_languages.split(TESSERACT_LANGUAGES_SPLITTER)\n+        # else, remove the extraneous languages.\n+        # NOTE (jennings): \"auto\" should only be used for partitioners OTHER THAN `_pdf` or `_image`\n         else:\n-            languages = convert_old_ocr_languages_to_languages(ocr_languages)\n-            logger.warning(\n-                \"The ocr_languages kwarg will be deprecated in a future version of unstructured. \"\n-                \"Please use languages instead.\",\n-            )\n-    return languages\n+            # define as 'auto' for language detection when partitioning non-pdfs or -images\n+            languages = [\"auto\"]\n+        return languages\n+\n+    return None",
        "comment_created_at": "2024-01-25T19:14:15+00:00",
        "comment_author": "Coniferish",
        "comment_body": "I don't think we can do that because of the other partitioners that default to \"auto\". `None` is used as the default because we have to be able to resolve the languages/ocr_languages (or just languages) at the auto `partition` level. Downstream, once the document type is determined and the document is passed to the appropriate partitioner, the default for `languages` can be either \"auto\" or \"eng\". \"eng\" for the other partitioners would skip language detection since it'd be interpreted as a user telling us they know the document is in English.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1438360537",
    "pr_number": 2327,
    "pr_file": "unstructured/ingest/cli/cmds/salesforce.py",
    "created_at": "2023-12-29T18:10:32+00:00",
    "commented_code": "help=\"For the Salesforce JWT auth. Found in Consumer Details.\",\n             ),\n             click.Option(\n-                [\"--private-key-path\"],\n+                [\"--private-key\"],\n                 required=True,\n-                type=click.Path(file_okay=True, exists=True, dir_okay=False),\n-                help=\"Path to the private key for the Salesforce JWT auth. \"\n-                \"Usually named server.key.\",\n+                type=FileOrJson(allow_raw_str=True),",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1438360537",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2327,
        "pr_file": "unstructured/ingest/cli/cmds/salesforce.py",
        "discussion_id": "1438360537",
        "commented_code": "@@ -31,11 +32,11 @@ def get_cli_options() -> t.List[click.Option]:\n                 help=\"For the Salesforce JWT auth. Found in Consumer Details.\",\n             ),\n             click.Option(\n-                [\"--private-key-path\"],\n+                [\"--private-key\"],\n                 required=True,\n-                type=click.Path(file_okay=True, exists=True, dir_okay=False),\n-                help=\"Path to the private key for the Salesforce JWT auth. \"\n-                \"Usually named server.key.\",\n+                type=FileOrJson(allow_raw_str=True),",
        "comment_created_at": "2023-12-29T18:10:32+00:00",
        "comment_author": "rbiseck3",
        "comment_body": "Will the contents of the private key always be a valid json? If so, lets use the default `allow_raw_str` to raise an error if the value passed in is not a valid json. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1444425180",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2327,
        "pr_file": "unstructured/ingest/cli/cmds/salesforce.py",
        "discussion_id": "1438360537",
        "commented_code": "@@ -31,11 +32,11 @@ def get_cli_options() -> t.List[click.Option]:\n                 help=\"For the Salesforce JWT auth. Found in Consumer Details.\",\n             ),\n             click.Option(\n-                [\"--private-key-path\"],\n+                [\"--private-key\"],\n                 required=True,\n-                type=click.Path(file_okay=True, exists=True, dir_okay=False),\n-                help=\"Path to the private key for the Salesforce JWT auth. \"\n-                \"Usually named server.key.\",\n+                type=FileOrJson(allow_raw_str=True),",
        "comment_created_at": "2024-01-08T10:36:33+00:00",
        "comment_author": "jakub-sandomierz-deepsense-ai",
        "comment_body": "I have changed type to `str`. This parameter should hold path or PEM key. Full validation is done in `SalesforceAccessConfig`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1393615523",
    "pr_number": 2005,
    "pr_file": "unstructured/ingest/connector/sql/connector.py",
    "created_at": "2023-11-15T04:17:47+00:00",
    "commented_code": "+import json\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+\n+from unstructured.ingest.interfaces import (\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    BaseIngestDoc,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.utils import requires_dependencies\n+\n+from .tables import (\n+    make_coordinates_table,\n+    make_data_source_table,\n+    make_elements_table,\n+    make_metadata_table,\n+)\n+\n+ELEMENTS_TABLE_NAME = \"elements\"\n+METADATA_TABLE_NAME = \"metadata\"\n+DATA_SOURCE_TABLE_NAME = \"data_source\"\n+COORDINATES_TABLE_NAME = \"coordinates\"\n+\n+\n+@dataclass\n+class SimpleSqlConfig(BaseConnectorConfig):\n+    drivername: t.Optional[str]\n+    username: t.Optional[str]\n+    password: t.Optional[str] = field(repr=False)\n+    host: t.Optional[str]\n+    database: t.Optional[str]\n+    database_url: t.Optional[str] = None\n+    port: t.Optional[int] = 5432\n+\n+    @property\n+    def db_url(self):\n+        from sqlalchemy.engine.url import URL\n+\n+        if self.database_url is not None:\n+            return self.database_url\n+\n+        return URL.create(\n+            drivername=self.drivername,\n+            username=self.username,\n+            password=self.password,\n+            host=self.host,\n+            port=self.port,\n+            database=self.database,\n+        )\n+\n+\n+@dataclass\n+class SqlWriteConfig(WriteConfig):\n+    mode: t.Literal[\"error\", \"append\", \"overwrite\"] = \"error\"\n+    table_name_mapping: t.Optional[t.Dict[str, str]] = None\n+\n+\n+@dataclass\n+class SqlDestinationConnector(BaseDestinationConnector):\n+    write_config: SqlWriteConfig\n+    connector_config: SimpleSqlConfig\n+\n+    @requires_dependencies([\"sqlalchemy\"], extras=\"sql\")\n+    def initialize(self):\n+        from sqlalchemy import create_engine\n+\n+        self.engine = create_engine(self.connector_config.db_url)\n+        if self.write_config.table_name_mapping is None:\n+            self.write_config.table_name_mapping = {}\n+\n+    def check_connection(self):\n+        pass\n+\n+    def conform_dict(self, data: dict) -> None:\n+        \"\"\"\n+        Updates the element dictionary to conform to the sql schema\n+        \"\"\"\n+        from datetime import datetime\n+\n+        data[\"id\"] = str(uuid.uuid4())\n+        data[\"metadata_id\"] = None\n+        if data.get(\"metadata\"):\n+            metadata_id = str(uuid.uuid4())\n+            data[\"metadata\"][\"id\"] = metadata_id\n+            data[\"metadata_id\"] = metadata_id\n+            data[\"data_source_id\"] = None\n+            data[\"coordinates_id\"] = None\n+\n+            if data.get(\"metadata\", {}).get(\"data_source\"):\n+                data_source_id = str(uuid.uuid4())\n+                data[\"metadata\"][\"data_source\"][\"id\"] = data_source_id\n+                data[\"data_source_id\"] = data_source_id\n+\n+            if data.get(\"metadata\", {}).get(\"coordinates\"):\n+                coordinates_id = str(uuid.uuid4())\n+                data[\"metadata\"][\"coordinates\"][\"id\"] = coordinates_id\n+                data[\"coordinates_id\"] = coordinates_id\n+\n+        # Dict as string formatting\n+        if record_locator := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"record_locator\"):\n+            # Explicit casting otherwise fails schema type checking\n+            data[\"metadata\"][\"data_source\"][\"record_locator\"] = str(json.dumps(record_locator))\n+\n+        # Array of items as string formatting\n+        if points := data.get(\"metadata\", {}).get(\"coordinates\", {}).get(\"points\"):\n+            data[\"metadata\"][\"coordinates\"][\"points\"] = str(json.dumps(points))\n+\n+        if links := data.get(\"metadata\", {}).get(\"links\", {}):\n+            data[\"metadata\"][\"links\"] = str(json.dumps(links))\n+\n+        if permissions_data := (\n+            data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"permissions_data\")\n+        ):\n+            data[\"metadata\"][\"data_source\"][\"permissions_data\"] = json.dumps(permissions_data)\n+\n+        # Datetime formatting\n+        if date_created := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"date_created\"):\n+            data[\"metadata\"][\"data_source\"][\"date_created\"] = datetime.fromisoformat(date_created)\n+\n+        if date_modified := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"date_modified\"):\n+            data[\"metadata\"][\"data_source\"][\"date_modified\"] = datetime.fromisoformat(date_modified)\n+\n+        if date_processed := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"date_processed\"):\n+            data[\"metadata\"][\"data_source\"][\"date_processed\"] = datetime.fromisoformat(\n+                date_processed\n+            )\n+\n+        if last_modified := data.get(\"metadata\", {}).get(\"last_modified\", {}):\n+            data[\"metadata\"][\"last_modified\"] = datetime.fromisoformat(last_modified)\n+\n+        # String casting\n+        if version := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"version\"):\n+            data[\"metadata\"][\"data_source\"][\"version\"] = str(version)\n+\n+        if page_number := data.get(\"metadata\", {}).get(\"page_number\"):\n+            data[\"metadata\"][\"page_number\"] = str(page_number)\n+\n+        if regex_metadata := data.get(\"metadata\", {}).get(\"regex_metadata\"):\n+            data[\"metadata\"][\"regex_metadata\"] = str(json.dumps(regex_metadata))\n+\n+        data_source = data.get(\"metadata\", {}).pop(\"data_source\", None)\n+        coordinates = data.get(\"metadata\", {}).pop(\"coordinates\", None)\n+        metadata = data.pop(\"metadata\", None)\n+\n+        return data, metadata, data_source, coordinates\n+\n+    # @DestinationConnectionError.wrap\n+    def write_dict(self, *args, json_list: t.List[t.Dict[str, t.Any]], **kwargs) -> None:\n+        from sqlalchemy import insert\n+\n+        logger.info(\n+            f\"writing {len(json_list)} objects to database url \" f\"{self.connector_config.db_url} \"\n+        )\n+\n+        elements = make_elements_table(\n+            self.write_config.table_name_mapping.get(ELEMENTS_TABLE_NAME, ELEMENTS_TABLE_NAME)",
    "repo_full_name": "Unstructured-IO/unstructured",
    "discussion_comments": [
      {
        "comment_id": "1393615523",
        "repo_full_name": "Unstructured-IO/unstructured",
        "pr_number": 2005,
        "pr_file": "unstructured/ingest/connector/sql/connector.py",
        "discussion_id": "1393615523",
        "commented_code": "@@ -0,0 +1,200 @@\n+import json\n+import typing as t\n+import uuid\n+from dataclasses import dataclass, field\n+\n+from unstructured.ingest.interfaces import (\n+    BaseConnectorConfig,\n+    BaseDestinationConnector,\n+    BaseIngestDoc,\n+    WriteConfig,\n+)\n+from unstructured.ingest.logger import logger\n+from unstructured.utils import requires_dependencies\n+\n+from .tables import (\n+    make_coordinates_table,\n+    make_data_source_table,\n+    make_elements_table,\n+    make_metadata_table,\n+)\n+\n+ELEMENTS_TABLE_NAME = \"elements\"\n+METADATA_TABLE_NAME = \"metadata\"\n+DATA_SOURCE_TABLE_NAME = \"data_source\"\n+COORDINATES_TABLE_NAME = \"coordinates\"\n+\n+\n+@dataclass\n+class SimpleSqlConfig(BaseConnectorConfig):\n+    drivername: t.Optional[str]\n+    username: t.Optional[str]\n+    password: t.Optional[str] = field(repr=False)\n+    host: t.Optional[str]\n+    database: t.Optional[str]\n+    database_url: t.Optional[str] = None\n+    port: t.Optional[int] = 5432\n+\n+    @property\n+    def db_url(self):\n+        from sqlalchemy.engine.url import URL\n+\n+        if self.database_url is not None:\n+            return self.database_url\n+\n+        return URL.create(\n+            drivername=self.drivername,\n+            username=self.username,\n+            password=self.password,\n+            host=self.host,\n+            port=self.port,\n+            database=self.database,\n+        )\n+\n+\n+@dataclass\n+class SqlWriteConfig(WriteConfig):\n+    mode: t.Literal[\"error\", \"append\", \"overwrite\"] = \"error\"\n+    table_name_mapping: t.Optional[t.Dict[str, str]] = None\n+\n+\n+@dataclass\n+class SqlDestinationConnector(BaseDestinationConnector):\n+    write_config: SqlWriteConfig\n+    connector_config: SimpleSqlConfig\n+\n+    @requires_dependencies([\"sqlalchemy\"], extras=\"sql\")\n+    def initialize(self):\n+        from sqlalchemy import create_engine\n+\n+        self.engine = create_engine(self.connector_config.db_url)\n+        if self.write_config.table_name_mapping is None:\n+            self.write_config.table_name_mapping = {}\n+\n+    def check_connection(self):\n+        pass\n+\n+    def conform_dict(self, data: dict) -> None:\n+        \"\"\"\n+        Updates the element dictionary to conform to the sql schema\n+        \"\"\"\n+        from datetime import datetime\n+\n+        data[\"id\"] = str(uuid.uuid4())\n+        data[\"metadata_id\"] = None\n+        if data.get(\"metadata\"):\n+            metadata_id = str(uuid.uuid4())\n+            data[\"metadata\"][\"id\"] = metadata_id\n+            data[\"metadata_id\"] = metadata_id\n+            data[\"data_source_id\"] = None\n+            data[\"coordinates_id\"] = None\n+\n+            if data.get(\"metadata\", {}).get(\"data_source\"):\n+                data_source_id = str(uuid.uuid4())\n+                data[\"metadata\"][\"data_source\"][\"id\"] = data_source_id\n+                data[\"data_source_id\"] = data_source_id\n+\n+            if data.get(\"metadata\", {}).get(\"coordinates\"):\n+                coordinates_id = str(uuid.uuid4())\n+                data[\"metadata\"][\"coordinates\"][\"id\"] = coordinates_id\n+                data[\"coordinates_id\"] = coordinates_id\n+\n+        # Dict as string formatting\n+        if record_locator := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"record_locator\"):\n+            # Explicit casting otherwise fails schema type checking\n+            data[\"metadata\"][\"data_source\"][\"record_locator\"] = str(json.dumps(record_locator))\n+\n+        # Array of items as string formatting\n+        if points := data.get(\"metadata\", {}).get(\"coordinates\", {}).get(\"points\"):\n+            data[\"metadata\"][\"coordinates\"][\"points\"] = str(json.dumps(points))\n+\n+        if links := data.get(\"metadata\", {}).get(\"links\", {}):\n+            data[\"metadata\"][\"links\"] = str(json.dumps(links))\n+\n+        if permissions_data := (\n+            data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"permissions_data\")\n+        ):\n+            data[\"metadata\"][\"data_source\"][\"permissions_data\"] = json.dumps(permissions_data)\n+\n+        # Datetime formatting\n+        if date_created := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"date_created\"):\n+            data[\"metadata\"][\"data_source\"][\"date_created\"] = datetime.fromisoformat(date_created)\n+\n+        if date_modified := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"date_modified\"):\n+            data[\"metadata\"][\"data_source\"][\"date_modified\"] = datetime.fromisoformat(date_modified)\n+\n+        if date_processed := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"date_processed\"):\n+            data[\"metadata\"][\"data_source\"][\"date_processed\"] = datetime.fromisoformat(\n+                date_processed\n+            )\n+\n+        if last_modified := data.get(\"metadata\", {}).get(\"last_modified\", {}):\n+            data[\"metadata\"][\"last_modified\"] = datetime.fromisoformat(last_modified)\n+\n+        # String casting\n+        if version := data.get(\"metadata\", {}).get(\"data_source\", {}).get(\"version\"):\n+            data[\"metadata\"][\"data_source\"][\"version\"] = str(version)\n+\n+        if page_number := data.get(\"metadata\", {}).get(\"page_number\"):\n+            data[\"metadata\"][\"page_number\"] = str(page_number)\n+\n+        if regex_metadata := data.get(\"metadata\", {}).get(\"regex_metadata\"):\n+            data[\"metadata\"][\"regex_metadata\"] = str(json.dumps(regex_metadata))\n+\n+        data_source = data.get(\"metadata\", {}).pop(\"data_source\", None)\n+        coordinates = data.get(\"metadata\", {}).pop(\"coordinates\", None)\n+        metadata = data.pop(\"metadata\", None)\n+\n+        return data, metadata, data_source, coordinates\n+\n+    # @DestinationConnectionError.wrap\n+    def write_dict(self, *args, json_list: t.List[t.Dict[str, t.Any]], **kwargs) -> None:\n+        from sqlalchemy import insert\n+\n+        logger.info(\n+            f\"writing {len(json_list)} objects to database url \" f\"{self.connector_config.db_url} \"\n+        )\n+\n+        elements = make_elements_table(\n+            self.write_config.table_name_mapping.get(ELEMENTS_TABLE_NAME, ELEMENTS_TABLE_NAME)",
        "comment_created_at": "2023-11-15T04:17:47+00:00",
        "comment_author": "potter-potter",
        "comment_body": "Is there a specific reason were using .get like this? (if it doesn't find ELEMENTS_TABLE_NAME, it use ELEMENTS_TABLE_NAME). Probably be better to just use [\"ELEMENTS_TABLE_NAME\"] and  hard fail if there is no ELEMENTS_TABLE_NAME.",
        "pr_file_module": null
      }
    ]
  }
]