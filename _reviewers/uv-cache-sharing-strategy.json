[
  {
    "discussion_id": "1590189557",
    "pr_number": 3372,
    "pr_file": "Dockerfile",
    "created_at": "2024-05-05T01:24:14+00:00",
    "commented_code": "-FROM --platform=$BUILDPLATFORM ubuntu as build\n+# syntax=docker/dockerfile:1\n+\n+FROM --platform=${BUILDPLATFORM} ubuntu:24.04 AS builder-base\n+# Configure the shell to exit early if any command fails, or when referencing unset variables.\n+# Additionally `-x` outputs each command run, this is helpful for troubleshooting failures.\n+SHELL [\"/bin/bash\", \"-eux\", \"-o\", \"pipefail\", \"-c\"]\n+\n+RUN \\\n+  --mount=target=/var/lib/apt/lists,type=cache,sharing=locked \\\n+  --mount=target=/var/cache/apt,type=cache,sharing=locked \\\n+  <<HEREDOC\n+    # https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/reference.md#example-cache-apt-packages\n+    # https://stackoverflow.com/questions/66808788/docker-can-you-cache-apt-get-package-installs#comment135104889_72851168\n+    rm -f /etc/apt/apt.conf.d/docker-clean\n+    echo 'Binary::apt::APT::Keep-Downloaded-Packages \"true\";' > /etc/apt/apt.conf.d/keep-cache\n+\n+    apt update && apt install -y --no-install-recommends \\\n+      build-essential \\\n+      curl \\\n+      python3-venv \\\n+      cmake\n+HEREDOC\n+\n ENV HOME=\"/root\"\n+ENV PATH=\"$HOME/.venv/bin:$PATH\"\n WORKDIR $HOME\n \n-RUN apt update \\\n-  && apt install -y --no-install-recommends \\\n-  build-essential \\\n-  curl \\\n-  python3-venv \\\n-  cmake \\\n-  && apt clean \\\n-  && rm -rf /var/lib/apt/lists/*\n-\n-# Setup zig as cross compiling linker\n-RUN python3 -m venv $HOME/.venv\n-RUN .venv/bin/pip install cargo-zigbuild\n-ENV PATH=\"$HOME/.venv/bin:$PATH\"\n+# Setup zig as cross compiling linker:\n+RUN <<HEREDOC\n+  python3 -m venv $HOME/.venv\n+  .venv/bin/pip install --no-cache-dir cargo-zigbuild\n+HEREDOC\n \n-# Install rust\n-ARG TARGETPLATFORM\n-RUN case \"$TARGETPLATFORM\" in \\\n-  \"linux/arm64\") echo \"aarch64-unknown-linux-musl\" > rust_target.txt ;; \\\n-  \"linux/amd64\") echo \"x86_64-unknown-linux-musl\" > rust_target.txt ;; \\\n-  *) exit 1 ;; \\\n-  esac\n-# Update rustup whenever we bump the rust version\n-COPY rust-toolchain.toml rust-toolchain.toml\n-RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --target $(cat rust_target.txt) --profile minimal --default-toolchain none\n+# Install rust:\n ENV PATH=\"$HOME/.cargo/bin:$PATH\"\n-# Installs the correct toolchain version from rust-toolchain.toml and then the musl target\n-RUN rustup target add $(cat rust_target.txt)\n-\n-# Build\n-COPY crates crates\n-COPY ./Cargo.toml Cargo.toml\n-COPY ./Cargo.lock Cargo.lock\n-RUN cargo zigbuild --bin uv --target $(cat rust_target.txt) --release\n-RUN cp target/$(cat rust_target.txt)/release/uv /uv\n-# TODO(konsti): Optimize binary size, with a version that also works when cross compiling\n-# RUN strip --strip-all /uv\n+COPY rust-toolchain.toml .\n+RUN <<HEREDOC\n+  # Install rustup, but skip installing a default toolchain as we only want the version from `rust-toolchain.toml`:\n+  curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --profile minimal --default-toolchain none\n+\n+  # When rustup installs the toolchain ensure it actually uses the minimal profile, avoiding excess layer weight:\n+  # https://github.com/rust-lang/rustup/issues/3805#issuecomment-2094066914\n+  echo 'profile = \"minimal\"' >> rust-toolchain.toml\n+  echo 'targets = [ \"aarch64-unknown-linux-musl\", \"x86_64-unknown-linux-musl\" ]' >> rust-toolchain.toml\n+  # Add the relevant musl target triples (for a building binary with static linking):\n+  # Workaround until `ensure` arrives: https://github.com/rust-lang/rustup/issues/2686#issuecomment-788825744\n+  rustup show\n+HEREDOC\n+\n+# Handle individual images differences for ARM64 / AMD64:\n+FROM builder-base AS builder-arm64\n+ENV CARGO_BUILD_TARGET=aarch64-unknown-linux-musl\n+\n+FROM builder-base AS builder-amd64\n+ENV CARGO_BUILD_TARGET=x86_64-unknown-linux-musl\n+\n+# Build app:\n+FROM builder-${TARGETARCH} AS builder-app\n+COPY crates/ crates/\n+COPY Cargo.toml Cargo.lock .\n+ARG APP_NAME=uv\n+ARG CARGO_HOME=/usr/local/cargo\n+ARG RUSTFLAGS=\"-C strip=symbols -C relocation-model=static -C target-feature=+crt-static -C opt-level=z\"\n+ARG TARGETPLATFORM\n+RUN \\\n+  --mount=type=cache,target=\"/root/.cache/zig\",id=\"zig-cache\" \\\n+  # Cache mounts (dirs for crates cache + build target):\n+  # https://doc.rust-lang.org/cargo/guide/cargo-home.html#caching-the-cargo-home-in-ci\n+  # CAUTION: As cargo uses multiple lock files (eg: `${CARGO_HOME}/{.global-cache,.package-cache,.package-cache-mutate}`), do not mount subdirs individually.\n+  --mount=type=cache,target=\"${CARGO_HOME}\",id=\"cargo-cache\" \\\n+  # This cache mount is specific enough that you may not have any concurrent builds needing to share it, communicate that expectation explicitly:\n+  --mount=type=cache,target=\"target/\",id=\"cargo-target-${APP_NAME}-${TARGETPLATFORM}\",sharing=locked \\\n+  # These are redundant as they're easily reconstructed from cache above. Use TMPFS mounts to exclude from cache mounts:\n+  # TMPFS mount is a better choice than `rm -rf` command (which is risky on a cache mount that is shared across concurrent builds).\n+  --mount=type=tmpfs,target=\"${CARGO_HOME}/registry/src\" \\\n+  --mount=type=tmpfs,target=\"${CARGO_HOME}/git/checkouts\" \\",
    "repo_full_name": "astral-sh/uv",
    "discussion_comments": [
      {
        "comment_id": "1590189557",
        "repo_full_name": "astral-sh/uv",
        "pr_number": 3372,
        "pr_file": "Dockerfile",
        "discussion_id": "1590189557",
        "commented_code": "@@ -1,45 +1,86 @@\n-FROM --platform=$BUILDPLATFORM ubuntu as build\n+# syntax=docker/dockerfile:1\n+\n+FROM --platform=${BUILDPLATFORM} ubuntu:24.04 AS builder-base\n+# Configure the shell to exit early if any command fails, or when referencing unset variables.\n+# Additionally `-x` outputs each command run, this is helpful for troubleshooting failures.\n+SHELL [\"/bin/bash\", \"-eux\", \"-o\", \"pipefail\", \"-c\"]\n+\n+RUN \\\n+  --mount=target=/var/lib/apt/lists,type=cache,sharing=locked \\\n+  --mount=target=/var/cache/apt,type=cache,sharing=locked \\\n+  <<HEREDOC\n+    # https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/reference.md#example-cache-apt-packages\n+    # https://stackoverflow.com/questions/66808788/docker-can-you-cache-apt-get-package-installs#comment135104889_72851168\n+    rm -f /etc/apt/apt.conf.d/docker-clean\n+    echo 'Binary::apt::APT::Keep-Downloaded-Packages \"true\";' > /etc/apt/apt.conf.d/keep-cache\n+\n+    apt update && apt install -y --no-install-recommends \\\n+      build-essential \\\n+      curl \\\n+      python3-venv \\\n+      cmake\n+HEREDOC\n+\n ENV HOME=\"/root\"\n+ENV PATH=\"$HOME/.venv/bin:$PATH\"\n WORKDIR $HOME\n \n-RUN apt update \\\n-  && apt install -y --no-install-recommends \\\n-  build-essential \\\n-  curl \\\n-  python3-venv \\\n-  cmake \\\n-  && apt clean \\\n-  && rm -rf /var/lib/apt/lists/*\n-\n-# Setup zig as cross compiling linker\n-RUN python3 -m venv $HOME/.venv\n-RUN .venv/bin/pip install cargo-zigbuild\n-ENV PATH=\"$HOME/.venv/bin:$PATH\"\n+# Setup zig as cross compiling linker:\n+RUN <<HEREDOC\n+  python3 -m venv $HOME/.venv\n+  .venv/bin/pip install --no-cache-dir cargo-zigbuild\n+HEREDOC\n \n-# Install rust\n-ARG TARGETPLATFORM\n-RUN case \"$TARGETPLATFORM\" in \\\n-  \"linux/arm64\") echo \"aarch64-unknown-linux-musl\" > rust_target.txt ;; \\\n-  \"linux/amd64\") echo \"x86_64-unknown-linux-musl\" > rust_target.txt ;; \\\n-  *) exit 1 ;; \\\n-  esac\n-# Update rustup whenever we bump the rust version\n-COPY rust-toolchain.toml rust-toolchain.toml\n-RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --target $(cat rust_target.txt) --profile minimal --default-toolchain none\n+# Install rust:\n ENV PATH=\"$HOME/.cargo/bin:$PATH\"\n-# Installs the correct toolchain version from rust-toolchain.toml and then the musl target\n-RUN rustup target add $(cat rust_target.txt)\n-\n-# Build\n-COPY crates crates\n-COPY ./Cargo.toml Cargo.toml\n-COPY ./Cargo.lock Cargo.lock\n-RUN cargo zigbuild --bin uv --target $(cat rust_target.txt) --release\n-RUN cp target/$(cat rust_target.txt)/release/uv /uv\n-# TODO(konsti): Optimize binary size, with a version that also works when cross compiling\n-# RUN strip --strip-all /uv\n+COPY rust-toolchain.toml .\n+RUN <<HEREDOC\n+  # Install rustup, but skip installing a default toolchain as we only want the version from `rust-toolchain.toml`:\n+  curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --profile minimal --default-toolchain none\n+\n+  # When rustup installs the toolchain ensure it actually uses the minimal profile, avoiding excess layer weight:\n+  # https://github.com/rust-lang/rustup/issues/3805#issuecomment-2094066914\n+  echo 'profile = \"minimal\"' >> rust-toolchain.toml\n+  echo 'targets = [ \"aarch64-unknown-linux-musl\", \"x86_64-unknown-linux-musl\" ]' >> rust-toolchain.toml\n+  # Add the relevant musl target triples (for a building binary with static linking):\n+  # Workaround until `ensure` arrives: https://github.com/rust-lang/rustup/issues/2686#issuecomment-788825744\n+  rustup show\n+HEREDOC\n+\n+# Handle individual images differences for ARM64 / AMD64:\n+FROM builder-base AS builder-arm64\n+ENV CARGO_BUILD_TARGET=aarch64-unknown-linux-musl\n+\n+FROM builder-base AS builder-amd64\n+ENV CARGO_BUILD_TARGET=x86_64-unknown-linux-musl\n+\n+# Build app:\n+FROM builder-${TARGETARCH} AS builder-app\n+COPY crates/ crates/\n+COPY Cargo.toml Cargo.lock .\n+ARG APP_NAME=uv\n+ARG CARGO_HOME=/usr/local/cargo\n+ARG RUSTFLAGS=\"-C strip=symbols -C relocation-model=static -C target-feature=+crt-static -C opt-level=z\"\n+ARG TARGETPLATFORM\n+RUN \\\n+  --mount=type=cache,target=\"/root/.cache/zig\",id=\"zig-cache\" \\\n+  # Cache mounts (dirs for crates cache + build target):\n+  # https://doc.rust-lang.org/cargo/guide/cargo-home.html#caching-the-cargo-home-in-ci\n+  # CAUTION: As cargo uses multiple lock files (eg: `${CARGO_HOME}/{.global-cache,.package-cache,.package-cache-mutate}`), do not mount subdirs individually.\n+  --mount=type=cache,target=\"${CARGO_HOME}\",id=\"cargo-cache\" \\\n+  # This cache mount is specific enough that you may not have any concurrent builds needing to share it, communicate that expectation explicitly:\n+  --mount=type=cache,target=\"target/\",id=\"cargo-target-${APP_NAME}-${TARGETPLATFORM}\",sharing=locked \\\n+  # These are redundant as they're easily reconstructed from cache above. Use TMPFS mounts to exclude from cache mounts:\n+  # TMPFS mount is a better choice than `rm -rf` command (which is risky on a cache mount that is shared across concurrent builds).\n+  --mount=type=tmpfs,target=\"${CARGO_HOME}/registry/src\" \\\n+  --mount=type=tmpfs,target=\"${CARGO_HOME}/git/checkouts\" \\",
        "comment_created_at": "2024-05-05T01:24:14+00:00",
        "comment_author": "polarathene",
        "comment_body": "The cache mounting here is a bit excessive, I've added some context with comments. You're not really going to need to maintain much here AFAIK, so it's mostly out of sight/mind in practice.\r\n\r\nIf you're not familiar with this feature, the default `sharing` type is `shared`, which allows other concurrent builds to share the same data for each mount by matching `id`, if no `id` is configured it implicitly uses the `target` path.\r\n\r\nWe have 3 caches here, `zig`, `cargo`, and the project specific `target/` directory.\r\n- `zig` I'm not familiar with it's cache system. I assume it's a global cache from the path and that it's fine for concurrent writers (_aka `sharing=shared`, the implicit default_).\r\n- `cargo` mounts the entire cargo home location as it doesn't yet have an isolated cache location.\r\n  - This does mean if any other `Dockerfile` used the `id` of `cargo-cache` they'd both be sharing the same contents, including whatever is installed in `bin/`.\r\n  - It'd normally not be safe for concurrent writers AFAIK, but due to the lock files cargo manages here, it's a non-issue.\r\n- The `target/` location has an `id` assigned per `TARGETPLATFORM`, and is also isolated by `APP_NAME` (`uv`) as this is not useful to share with other `Dockerfile` unrelated to `uv` project.\r\n  - While there shouldn't be any concurrent writers, if someone was to to do several parallel builds with `RUSTFLAGS` arg being changed, the `sharing=locked` will block them as they'd all want to compile the dependencies again (_making the cache a bit less useful, but not as wasteful as individual caches via `sharing=private` which aren't deterministic for repeat builds_).\r\n  - A previous commit did take a different approach, where the build process was part of the original builder stage and built both targets together, instead of relying on `TARGETPLATFORM`. If you don't mind always building for both targets, they could share a bit of a common `target/release/` cache contents AFAIK. The logic for the two commands below was complicated a bit more with a bash loop (_since static builds require and explicit `--target` AFAIK, even when multiple targets are configured via `rust-toolchain.toml` / `.cargo/config.toml`?_), so I opted for keeping the logic below simple to grok by using separate `TARGETPLATFORM` caches (_these can still build concurrently as I demo'd in the PR description_).\r\n\r\nThe last two `tmpfs` mounts are rather self-explanatory. There is no value in caching these to disk.\r\n\r\nIf you do want something that looks simpler, Earthly has tooling to simplify rust builds and cache management. However `Dockerfile` is more common knowledge to have and troubleshoot/maintain, adding another tool/abstraction didn't seem worthwhile to contribute.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2065430769",
    "pr_number": 11106,
    "pr_file": "Dockerfile",
    "created_at": "2025-04-29T04:43:12+00:00",
    "commented_code": "esac\n \n # Update rustup whenever we bump the rust version\n+ENV PATH=\"$CARGO_HOME/bin:$PATH\"\n COPY rust-toolchain.toml rust-toolchain.toml\n-RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --target $(cat rust_target.txt) --profile minimal --default-toolchain none\n-ENV PATH=\"$HOME/.cargo/bin:$PATH\"\n-# Installs the correct toolchain version from rust-toolchain.toml and then the musl target\n-RUN rustup target add $(cat rust_target.txt)\n+RUN \\\n+  --mount=type=cache,target=/buildkit-cache,id=\"tool-caches\" \\",
    "repo_full_name": "astral-sh/uv",
    "discussion_comments": [
      {
        "comment_id": "2065430769",
        "repo_full_name": "astral-sh/uv",
        "pr_number": 11106,
        "pr_file": "Dockerfile",
        "discussion_id": "2065430769",
        "commented_code": "@@ -25,21 +38,32 @@ RUN case \"$TARGETPLATFORM\" in \\\n   esac\n \n # Update rustup whenever we bump the rust version\n+ENV PATH=\"$CARGO_HOME/bin:$PATH\"\n COPY rust-toolchain.toml rust-toolchain.toml\n-RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --target $(cat rust_target.txt) --profile minimal --default-toolchain none\n-ENV PATH=\"$HOME/.cargo/bin:$PATH\"\n-# Installs the correct toolchain version from rust-toolchain.toml and then the musl target\n-RUN rustup target add $(cat rust_target.txt)\n+RUN \\\n+  --mount=type=cache,target=/buildkit-cache,id=\"tool-caches\" \\",
        "comment_created_at": "2025-04-29T04:43:12+00:00",
        "comment_author": "polarathene",
        "comment_body": "This `RUN` does not play well with concurrent writers when that `tool-caches` cache mount is used. Causing builds to fail:\r\n\r\n```\r\n1.499 info: downloading component 'cargo'\r\n1.790 error: component download failed for cargo-x86_64-unknown-linux-gnu: could not rename downloaded file from '/buildkit-cache/rustup/downloads/c5c1590f7e9246ad9f4f97cfe26ffa92707b52a769726596a9ef81565ebd908b.partial' to '/buildkit-cache/rustup/downloads/c5c1590f7e9246ad9f4f97cfe26ffa92707b52a769726596a9ef81565ebd908b': No such file or directory (os error 2)\r\n```\r\n\r\nWhile cargo might manage lock files to avoid this type of scenario, you need to be mindful of cache mount usage when it's not compatible with the default `sharing=shared` mount option.\r\n\r\n```bash\r\n# When using a Buildx container driver:\r\ndocker buildx create --name=container --driver=docker-container --use --bootstrap\r\n\r\n# You can now build for multiple platforms concurrently:\r\ndocker buildx build --builder=container --platform=linux/arm64,linux/amd64 --tag localhost/uv .\r\n```\r\n\r\nTo prevent this problem use `sharing=locked` to block another build from writing to the same cache mount id. That or running two separate build commands to build one platform at a time.\r\n\r\n---\r\n\r\nWhile on the topic of cache mounts. It's a non-issue for CI of a project where you only build a single `Dockerfile` your project maintains.\r\n\r\nHowever on user systems, AFAIK if that `id` is used in another project `Dockerfile`, it also shares that cache. Sometimes that's a non-issue, but be mindful of accidentally mixing/sharing with other projects that shouldn't share a cache mount due to concerns like invalidating each others storage, or like seen here conflicting write access, or with `sharing=locked` blocking a build of another project.",
        "pr_file_module": null
      },
      {
        "comment_id": "2065547600",
        "repo_full_name": "astral-sh/uv",
        "pr_number": 11106,
        "pr_file": "Dockerfile",
        "discussion_id": "2065430769",
        "commented_code": "@@ -25,21 +38,32 @@ RUN case \"$TARGETPLATFORM\" in \\\n   esac\n \n # Update rustup whenever we bump the rust version\n+ENV PATH=\"$CARGO_HOME/bin:$PATH\"\n COPY rust-toolchain.toml rust-toolchain.toml\n-RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --target $(cat rust_target.txt) --profile minimal --default-toolchain none\n-ENV PATH=\"$HOME/.cargo/bin:$PATH\"\n-# Installs the correct toolchain version from rust-toolchain.toml and then the musl target\n-RUN rustup target add $(cat rust_target.txt)\n+RUN \\\n+  --mount=type=cache,target=/buildkit-cache,id=\"tool-caches\" \\",
        "comment_created_at": "2025-04-29T05:57:13+00:00",
        "comment_author": "polarathene",
        "comment_body": "```suggestion\r\nRUN \\\r\n  --mount=type=cache,target=/buildkit-cache,id=\"tool-caches\",sharing=locked \\\r\n```\r\n\r\n**EDIT:** As per feedback in the next comment, I'm really not sure about the toolchain being stored in a cache mount as a good idea? Rather then apply this fix it may be better to just avoid the cache mount entirely (_you'd then have the ability to build the `build` stage and shell into it to troubleshoot building if need be too, actually maybe not due to `CARGO_HOME` if you need zigbuild_)",
        "pr_file_module": null
      },
      {
        "comment_id": "2065603073",
        "repo_full_name": "astral-sh/uv",
        "pr_number": 11106,
        "pr_file": "Dockerfile",
        "discussion_id": "2065430769",
        "commented_code": "@@ -25,21 +38,32 @@ RUN case \"$TARGETPLATFORM\" in \\\n   esac\n \n # Update rustup whenever we bump the rust version\n+ENV PATH=\"$CARGO_HOME/bin:$PATH\"\n COPY rust-toolchain.toml rust-toolchain.toml\n-RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --target $(cat rust_target.txt) --profile minimal --default-toolchain none\n-ENV PATH=\"$HOME/.cargo/bin:$PATH\"\n-# Installs the correct toolchain version from rust-toolchain.toml and then the musl target\n-RUN rustup target add $(cat rust_target.txt)\n+RUN \\\n+  --mount=type=cache,target=/buildkit-cache,id=\"tool-caches\" \\",
        "comment_created_at": "2025-04-29T06:42:19+00:00",
        "comment_author": "polarathene",
        "comment_body": "I am not sure about why the rust toolchain is stored in a cache mount, while Zig and other toolchains are left in the image layers? To pair an update of `rust-toolchain.toml` bumping the toolchain to trigger `rustup self update`?\r\n\r\nThe `COPY` for `rust-toolchain.toml` would invalidate the `RUN` layer, so it would be updated just the same no?\r\n- I could understand if you were sharing this cache mount with other `Dockerfile` without common base layer sharing, but if those projects were configured with different toolchains they likewise accumulate in cache storage? (_which is more prone to GC than an actively used layer_) Cleaning up unused layers is probably preferable, cache should really be used for actual cache (_I think it's possible for a cache mount to clear between `RUN`, not ideal for a toolchain_).\r\n- The other possibility being for CI image caching and wanting to minimize storage.\r\n  - The bulk of your build time with this `Dockerfile` is with the actual cargo build later on, so pulling from a CI cache blob or from the remote source (rustup, package manager, etc) are not likely to be that much faster. Regardless you're configuring persistence in CI via cache mounts, is that beneficial vs standard caching of image layers?\r\n  - If you lose CI time to the large cache import/export delays (eg: due to de/compression), it may be faster to just not cache that portion of the image at all and do a clean build of it. Cache only what's helpful.\r\n\r\nYou will however benefit from the cache mount when building multiple targets separately (_rather than multiple `cargo build` in the same `RUN`_):\r\n- This is only because of the earlier `ARG TARGETPLATFORM` introducing a divergence in layer cache (_1.3GB + 1.4GB to support without cache mount but actual diff is approx 200MB only_).\r\n- Since both targets build from the same build host arch, there's no concern about conflict there with the cache mount either \ud83d\udc4d\r\n- It should be rare for earlier layer cache invalidation to really matter, but that'd be a win for cache mounts. Personally I prefer the immutable/predictable layer content vs accumulating cache mount that if I'm not mistaken can be cleared during build between layers (_as cache is intended to be disposable_).\r\n\r\nThat concern is easily fixed as per my suggestion for avoiding divergence at this point. Both targets added are 354MB combined. Total layer weight with minimal profile is 930MB (_instead of 1.6GB_), be that layer cache or a cache mount.\r\n\r\n---\r\n\r\n**Breakdown:**\r\n\r\n```bash\r\n# Build (without `tool-caches` cache mount):\r\ndocker buildx build --builder=container --platform=linux/amd64 --tag localhost/uv --load .\r\n\r\n# Inspect:\r\ndocker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock ghcr.io/wagoodman/dive:latest localhost/uv\r\n```\r\n\r\n**Sizes (bolded is within a cache mount):**\r\n- **1.6GB** (_930MB minimal profile_) => Rustup toolchain `/buildkit-cache/rustup` (_also adds 19MB to sibling dir `cargo/`_):\r\n  - `lib/rustlib/aarch64-unknown-linux-musl/lib` (135MB) / `lib/rustlib/x86_64-unknown-linux-musl/lib` (219MB)\r\n  - `lib/rustlib/x86_64-unknown-linux-gnu/bin` (18MB) + `lib/rustlib/x86_64-unknown-linux-gnu/lib` (158MB)\r\n  - `lib/libLLVM.so.19.1-rust-1.86.0-stable` (174MB) + `lib/librustc_driver-ea2439778c0a32ac.so` (141MB)\r\n- **85MB** => Pip cache `/buildkit-cache/pip/http-v2`\r\n- **258MB** => Apt cache `/var/cache/apt` (220MB) + `/var/lib/apt` (48MB)\r\n- 310MB => Zig toolchain at `/root/.venv/lib/python3.12/site-packages/ziglang`\r\n- 527MB => Base image (78MB) + 13MB (python venv setup) + `/usr` (_base package layer adds 436MB_)\r\n\r\n**Image build time:**\r\n\r\nOn a budget VPS (_Fedora 42 at Vultr, 1vCPU + 2GB RAM with 3GB more via zram swap_):\r\n- `apt` layer built within 37s\r\n- `cargo-zigbuild` install 12s\r\n- `rustup` setup 32s\r\n- `cargo` release build (x86_64), **2 hours 25 minutes**.\r\n\r\nThe build took excessively long presumably due to single CPU and quite possibly RAM, I didn't investigate that too extensively. Changing from `lto=\"fat\"` to `lto=\"thin\"` brought that build time down to **43 minutes**, at the expense of being 25% larger (40MB => 50MB).\r\n\r\nYou're getting much better results reported for the build, but the bulk of the time is down to the actual build. I'd avoid wasting CI cache store (_causing evictions sooner than necessary for cache items that are actually helpful_) on the rust toolchain, saving a minute at best is not worth better using the cache to optimize the build time (_requires [`sccache`](https://doc.rust-lang.org/cargo/reference/build-cache.html#shared-cache) IIRC to be decent but is not without quirks_).\r\n\r\nThat said you can use the cache mounts in CI and not upload/restore them for minimizing the image layers cache, but presently there is very little benefit in caching image layers at all? You could instead just focus on the cache mount(s) for the `cargo` build itself.\r\n\r\nThe `cargo` target cache is 1GB alone when building this project, but [as mentioned](https://github.com/astral-sh/uv/pull/11106#issuecomment-2837804391) it's a bit of a hassle to actually leverage for the CI.\r\n\r\n---\r\n\r\n### After a build\r\n\r\nFor reference, the cargo and zig caches are decent in size, but a good portion of the cargo one isn't relevant, nor is the zigbuild cache mount worthwhile?\r\n\r\n```console\r\n# Cargo:\r\n$ du -shx /buildkit-cache/cargo\r\n298M    /buildkit-cache/cargo\r\n# Bulk is from registry dir:\r\n$ /buildkit-cache/cargo/registry/\r\n217M    /buildkit-cache/cargo/registry/src\r\n33M     /buildkit-cache/cargo/registry/cache\r\n26M     /buildkit-cache/cargo/registry/index\r\n275M    /buildkit-cache/cargo/registry/\r\n\r\n# Zig:\r\ndu -shx /buildkit-cache/zig\r\n164M    /buildkit-cache/zig\r\n\r\n# Zigbuild:\r\n# Nothing worthwhile to cache? (plus it created another folder for itself):\r\ndu -shx /buildkit-cache/cargo-zigbuild/cargo-zigbuild/0.20.0\r\n24K     /buildkit-cache/cargo-zigbuild/cargo-zigbuild/0.20.0\r\n\r\n# Rustup for reference (before optimization):\r\n$ du -hx --max-depth=1 /buildkit-cache/rustup\r\n4.0K    /buildkit-cache/rustup/tmp\r\n1.5G    /buildkit-cache/rustup/toolchains\r\n8.0K    /buildkit-cache/rustup/update-hashes\r\n4.0K    /buildkit-cache/rustup/downloads\r\n1.5G    /buildkit-cache/rustup\r\n# This was built without minimal profile applied + only x86_64 musl target:\r\n$ du -hx --max-depth=1 /buildkit-cache/rustup/toolchains/1.86-x86_64-unknown-linux-gnu\r\n20K     /buildkit-cache/rustup/toolchains/1.86-x86_64-unknown-linux-gnu/etc\r\n1.4M    /buildkit-cache/rustup/toolchains/1.86-x86_64-unknown-linux-gnu/libexec\r\n728M    /buildkit-cache/rustup/toolchains/1.86-x86_64-unknown-linux-gnu/share\r\n73M     /buildkit-cache/rustup/toolchains/1.86-x86_64-unknown-linux-gnu/bin\r\n679M    /buildkit-cache/rustup/toolchains/1.86-x86_64-unknown-linux-gnu/lib\r\n1.5G    /buildkit-cache/rustup/toolchains/1.86-x86_64-unknown-linux-gnu\r\n```\r\n\r\nAs per [my PR attempt](https://github.com/astral-sh/uv/pull/3372/files#r1590189557), the bulk of the cargo cache mount there is from data that is quick to generate/compute at build time, thus not worth persisting. I used two separate tmpfs cache mounts to filter those out:\r\n\r\n```Dockerfile\r\n  # These are redundant as they're easily reconstructed from cache above. Use TMPFS mounts to exclude from cache mounts:\r\n  # TMPFS mount is a better choice than `rm -rf` command (which is risky on a cache mount that is shared across concurrent builds).\r\n  --mount=type=tmpfs,target=\"${CARGO_HOME}/registry/src\" \\\r\n  --mount=type=tmpfs,target=\"${CARGO_HOME}/git/checkouts\" \\\r\n```\r\n\r\nOnly relevant if storage of the cache mount is a concern, which it may be for CI limits to keep tame, otherwise is overkill :)",
        "pr_file_module": null
      }
    ]
  }
]