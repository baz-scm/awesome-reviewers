[
  {
    "discussion_id": "2173380175",
    "pr_number": 20210,
    "pr_file": "docker-compose.yml-exemple",
    "created_at": "2025-06-28T15:53:31+00:00",
    "commented_code": "+services:\n+  vllm:\n+    image: vllm/vllm-openai:v0.9.1\n+    container_name: vllm\n+    runtime: nvidia\n+    deploy:\n+      resources:\n+        reservations:\n+          devices:\n+            - driver: nvidia\n+              count: all\n+              capabilities: [gpu]\n+    ipc: host\n+    ports:\n+      - \"8000:8000\"\n+    volumes:\n+      - models:/models",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2173380175",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20210,
        "pr_file": "docker-compose.yml-exemple",
        "discussion_id": "2173380175",
        "commented_code": "@@ -0,0 +1,30 @@\n+services:\n+  vllm:\n+    image: vllm/vllm-openai:v0.9.1\n+    container_name: vllm\n+    runtime: nvidia\n+    deploy:\n+      resources:\n+        reservations:\n+          devices:\n+            - driver: nvidia\n+              count: all\n+              capabilities: [gpu]\n+    ipc: host\n+    ports:\n+      - \"8000:8000\"\n+    volumes:\n+      - models:/models",
        "comment_created_at": "2025-06-28T15:53:31+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe current volume configuration mounts a volume at `/models`, but vLLM and the underlying Hugging Face libraries download models to `/root/.cache/huggingface` by default. This means models won't be persisted in the `models` volume as intended. To correctly cache the models, you should mount the volume to `/root/.cache/huggingface`.\n\n```\n      - models:/root/.cache/huggingface\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2173380179",
    "pr_number": 20210,
    "pr_file": "docker-compose.yml-exemple",
    "created_at": "2025-06-28T15:53:31+00:00",
    "commented_code": "+services:\n+  vllm:\n+    image: vllm/vllm-openai:v0.9.1\n+    container_name: vllm\n+    runtime: nvidia\n+    deploy:\n+      resources:\n+        reservations:\n+          devices:\n+            - driver: nvidia\n+              count: all\n+              capabilities: [gpu]\n+    ipc: host\n+    ports:\n+      - \"8000:8000\"\n+    volumes:\n+      - models:/models\n+    environment:\n+      HUGGING_FACE_HUB_TOKEN: \"YOUR TOKEN HERE\"",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2173380179",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20210,
        "pr_file": "docker-compose.yml-exemple",
        "discussion_id": "2173380179",
        "commented_code": "@@ -0,0 +1,30 @@\n+services:\n+  vllm:\n+    image: vllm/vllm-openai:v0.9.1\n+    container_name: vllm\n+    runtime: nvidia\n+    deploy:\n+      resources:\n+        reservations:\n+          devices:\n+            - driver: nvidia\n+              count: all\n+              capabilities: [gpu]\n+    ipc: host\n+    ports:\n+      - \"8000:8000\"\n+    volumes:\n+      - models:/models\n+    environment:\n+      HUGGING_FACE_HUB_TOKEN: \"YOUR TOKEN HERE\"",
        "comment_created_at": "2025-06-28T15:53:31+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nTo ensure that models are stored in the correctly mounted volume, you should explicitly set the `HF_HOME` environment variable. This directs the Hugging Face library to use the specified path for caching.\n\n```\n      HF_HOME: /root/.cache/huggingface\n      HUGGING_FACE_HUB_TOKEN: \"YOUR TOKEN HERE\"\n```",
        "pr_file_module": null
      }
    ]
  }
]