[
  {
    "discussion_id": "2185442532",
    "pr_number": 20466,
    "pr_file": "vllm/v1/attention/backends/flash_attn.py",
    "created_at": "2025-07-04T13:53:54+00:00",
    "commented_code": "self.aot_sliding_window: Optional[tuple[int, int]] = None\n \n     def build(\n-        self, common_prefix_len: int,\n-        common_attn_metadata: CommonAttentionMetadata\n+        self,\n+        common_prefix_len: int,\n+        common_attn_metadata: CommonAttentionMetadata,\n     ) -> FlashAttentionMetadata:\n         num_reqs = common_attn_metadata.num_reqs\n         num_actual_tokens = common_attn_metadata.num_actual_tokens\n         max_query_len = common_attn_metadata.max_query_len\n-\n-        max_seq_len = int(self.runner.seq_lens_np[:num_reqs].max())\n+        max_seq_len = int(common_attn_metadata.seq_lens_cpu.max())\n         query_start_loc = common_attn_metadata.query_start_loc\n+        query_start_loc_cpu = common_attn_metadata.query_start_loc_cpu\n         seq_lens = common_attn_metadata.seq_lens\n-        block_table = self.block_table\n-        block_table_tensor = block_table.get_device_tensor()[:num_reqs]\n-\n-        block_table.slot_mapping[:num_actual_tokens].copy_(\n-            block_table.slot_mapping_cpu[:num_actual_tokens],\n-            non_blocking=True)\n-        # Fill unused with -1. Needed for reshape_and_cache in full cuda graph\n-        # mode.\n-        block_table.slot_mapping[num_actual_tokens:].fill_(-1)\n-\n-        slot_mapping = block_table.slot_mapping[:num_actual_tokens]\n+        seq_lens_cpu = common_attn_metadata.seq_lens_cpu\n+        block_table_tensor = common_attn_metadata.block_table_tensor",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2185442532",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20466,
        "pr_file": "vllm/v1/attention/backends/flash_attn.py",
        "discussion_id": "2185442532",
        "commented_code": "@@ -197,27 +197,20 @@ def __init__(self, runner: \"GPUModelRunner\", kv_cache_spec: AttentionSpec,\n         self.aot_sliding_window: Optional[tuple[int, int]] = None\n \n     def build(\n-        self, common_prefix_len: int,\n-        common_attn_metadata: CommonAttentionMetadata\n+        self,\n+        common_prefix_len: int,\n+        common_attn_metadata: CommonAttentionMetadata,\n     ) -> FlashAttentionMetadata:\n         num_reqs = common_attn_metadata.num_reqs\n         num_actual_tokens = common_attn_metadata.num_actual_tokens\n         max_query_len = common_attn_metadata.max_query_len\n-\n-        max_seq_len = int(self.runner.seq_lens_np[:num_reqs].max())\n+        max_seq_len = int(common_attn_metadata.seq_lens_cpu.max())\n         query_start_loc = common_attn_metadata.query_start_loc\n+        query_start_loc_cpu = common_attn_metadata.query_start_loc_cpu\n         seq_lens = common_attn_metadata.seq_lens\n-        block_table = self.block_table\n-        block_table_tensor = block_table.get_device_tensor()[:num_reqs]\n-\n-        block_table.slot_mapping[:num_actual_tokens].copy_(\n-            block_table.slot_mapping_cpu[:num_actual_tokens],\n-            non_blocking=True)\n-        # Fill unused with -1. Needed for reshape_and_cache in full cuda graph\n-        # mode.\n-        block_table.slot_mapping[num_actual_tokens:].fill_(-1)\n-\n-        slot_mapping = block_table.slot_mapping[:num_actual_tokens]\n+        seq_lens_cpu = common_attn_metadata.seq_lens_cpu\n+        block_table_tensor = common_attn_metadata.block_table_tensor",
        "comment_created_at": "2025-07-04T13:53:54+00:00",
        "comment_author": "heheda12345",
        "comment_body": "I'm doubt about whether `block_table_tensor` and `slot_mapping` should be put into `common_attn_metadata`.\r\n\r\nFor models with sliding window + full attention, the block_table_tensor for the two types of layers are different as they need different number of slots. These two types of layers are put into different kv_cache_groups and thus have different attention backend and different `BlockTable` now.  ",
        "pr_file_module": null
      },
      {
        "comment_id": "2185525567",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20466,
        "pr_file": "vllm/v1/attention/backends/flash_attn.py",
        "discussion_id": "2185442532",
        "commented_code": "@@ -197,27 +197,20 @@ def __init__(self, runner: \"GPUModelRunner\", kv_cache_spec: AttentionSpec,\n         self.aot_sliding_window: Optional[tuple[int, int]] = None\n \n     def build(\n-        self, common_prefix_len: int,\n-        common_attn_metadata: CommonAttentionMetadata\n+        self,\n+        common_prefix_len: int,\n+        common_attn_metadata: CommonAttentionMetadata,\n     ) -> FlashAttentionMetadata:\n         num_reqs = common_attn_metadata.num_reqs\n         num_actual_tokens = common_attn_metadata.num_actual_tokens\n         max_query_len = common_attn_metadata.max_query_len\n-\n-        max_seq_len = int(self.runner.seq_lens_np[:num_reqs].max())\n+        max_seq_len = int(common_attn_metadata.seq_lens_cpu.max())\n         query_start_loc = common_attn_metadata.query_start_loc\n+        query_start_loc_cpu = common_attn_metadata.query_start_loc_cpu\n         seq_lens = common_attn_metadata.seq_lens\n-        block_table = self.block_table\n-        block_table_tensor = block_table.get_device_tensor()[:num_reqs]\n-\n-        block_table.slot_mapping[:num_actual_tokens].copy_(\n-            block_table.slot_mapping_cpu[:num_actual_tokens],\n-            non_blocking=True)\n-        # Fill unused with -1. Needed for reshape_and_cache in full cuda graph\n-        # mode.\n-        block_table.slot_mapping[num_actual_tokens:].fill_(-1)\n-\n-        slot_mapping = block_table.slot_mapping[:num_actual_tokens]\n+        seq_lens_cpu = common_attn_metadata.seq_lens_cpu\n+        block_table_tensor = common_attn_metadata.block_table_tensor",
        "comment_created_at": "2025-07-04T14:36:05+00:00",
        "comment_author": "LucasWilkinson",
        "comment_body": "Ya, this PR kinda redefines `CommonAttentionMetadata` from \"common inputs across KV-caches groups\" to a \"common interface for `AttentionMetadataBuilder.build` that we implement backend-agnostic attention schemes/features on\". But as a result there will be different `CommonAttentionMetadata` for each KV-cache group (this seems fine since they all reference the same underlying tensors anyways).\r\n\r\nBasically, the idea is that `AttentionMetadataBuilder.build` transforms `CommonAttentionMetadata` to a backend-specific metadata and `CommonAttentionMetadata` should be the minimal amount of things to do this.\r\n\r\nThe reason for adding `slot_mapping` and `block_table_tensor` to the `CommonAttentionMetadata` is two-fold:\r\n\r\n1) [This is the main motivation] There's some attention-related things that manipulate the `block_table_tensor` and/or `slot_mapping` in a way that is common across backends; e.g. local attention https://github.com/vllm-project/vllm/blob/5561681d0400469aba6abfee242be717db47ae0c/vllm/v1/attention/backends/utils.py#L199-L310 and attention slicing for micro-batching (dual-batch overlap) https://github.com/vllm-project/vllm/pull/18415. With this refactor, these could be pulled out of `AttentionMetadataBuilder` and instead operate on `CommonAttentionMetadata` before being passed into `AttentionMetadataBuilder.build`, making these features backend-independent.\r\n\r\n2) Break the dependency on model runner in the `AttentionMetadataBuilder`s. This makes unit-testing challenging as it's hard to know what parts need to be mocked.\r\n\r\nI was thinking about breaking `block_table_tensor` and `slot_mapping` into a separate data structure (adding a third input to `AttentionMetadataBuilder.build`) to preserve the semantics of `CommonAttentionMetadata` being \"common inputs across KV-caches groups,\" but I'm not convinced this is needed since all the `CommonAttentionMetadata` classes reference the same underlying tensors anyway.\r\n\r\nI could see arguments for passing `BlockTable` to `AttentionMetadataBuilder.build`; it's just that it makes things like local attention and microbatch slicing (mentioned in 1) more difficult since instead of just slicing/manipulating the tensors directly, they would have to construct a new sliced/manipulated `BlockTable` which would be messy.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2186467577",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20466,
        "pr_file": "vllm/v1/attention/backends/flash_attn.py",
        "discussion_id": "2185442532",
        "commented_code": "@@ -197,27 +197,20 @@ def __init__(self, runner: \"GPUModelRunner\", kv_cache_spec: AttentionSpec,\n         self.aot_sliding_window: Optional[tuple[int, int]] = None\n \n     def build(\n-        self, common_prefix_len: int,\n-        common_attn_metadata: CommonAttentionMetadata\n+        self,\n+        common_prefix_len: int,\n+        common_attn_metadata: CommonAttentionMetadata,\n     ) -> FlashAttentionMetadata:\n         num_reqs = common_attn_metadata.num_reqs\n         num_actual_tokens = common_attn_metadata.num_actual_tokens\n         max_query_len = common_attn_metadata.max_query_len\n-\n-        max_seq_len = int(self.runner.seq_lens_np[:num_reqs].max())\n+        max_seq_len = int(common_attn_metadata.seq_lens_cpu.max())\n         query_start_loc = common_attn_metadata.query_start_loc\n+        query_start_loc_cpu = common_attn_metadata.query_start_loc_cpu\n         seq_lens = common_attn_metadata.seq_lens\n-        block_table = self.block_table\n-        block_table_tensor = block_table.get_device_tensor()[:num_reqs]\n-\n-        block_table.slot_mapping[:num_actual_tokens].copy_(\n-            block_table.slot_mapping_cpu[:num_actual_tokens],\n-            non_blocking=True)\n-        # Fill unused with -1. Needed for reshape_and_cache in full cuda graph\n-        # mode.\n-        block_table.slot_mapping[num_actual_tokens:].fill_(-1)\n-\n-        slot_mapping = block_table.slot_mapping[:num_actual_tokens]\n+        seq_lens_cpu = common_attn_metadata.seq_lens_cpu\n+        block_table_tensor = common_attn_metadata.block_table_tensor",
        "comment_created_at": "2025-07-05T03:03:38+00:00",
        "comment_author": "heheda12345",
        "comment_body": "Thanks for the detailed reply. It make sense to me.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2166501336",
    "pr_number": 20072,
    "pr_file": "examples/offline_inference/prithvi_geospatial_mae.py",
    "created_at": "2025-06-25T11:37:30+00:00",
    "commented_code": "self.model = LLM(\n             model=os.path.join(os.path.dirname(__file__), \"./model\"),\n             skip_tokenizer_init=True,\n-            dtype=\"float32\",\n+            dtype=\"float16\",\n+            enforce_eager=True,",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2166501336",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20072,
        "pr_file": "examples/offline_inference/prithvi_geospatial_mae.py",
        "discussion_id": "2166501336",
        "commented_code": "@@ -143,7 +143,8 @@ def __init__(self):\n         self.model = LLM(\n             model=os.path.join(os.path.dirname(__file__), \"./model\"),\n             skip_tokenizer_init=True,\n-            dtype=\"float32\",\n+            dtype=\"float16\",\n+            enforce_eager=True,",
        "comment_created_at": "2025-06-25T11:37:30+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nUsing `float16` can reduce memory usage and potentially improve performance, but it's important to ensure that the model's numerical stability isn't compromised. Consider adding a comment explaining why `float16` is suitable here and any potential trade-offs.\n\n```suggestion\n            dtype=\"float16\",  # Using float16 to reduce memory footprint with minimal impact on accuracy\n            enforce_eager=True,\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2109750247",
    "pr_number": 18218,
    "pr_file": "tests/kernels/mamba/test_causal_conv1d.py",
    "created_at": "2025-05-27T17:17:05+00:00",
    "commented_code": "causal_conv1d_opcheck_fn(x.squeeze(0), weight, bias, cumsum.cuda(),\n                              padded_state_indices, has_initial_states,\n                              final_states, activation)\n+\n+\n+@pytest.mark.parametrize(\"itype\",\n+                         [torch.float32, torch.float16, torch.bfloat16])\n+@pytest.mark.parametrize(\"silu_activation\", [False, True])\n+@pytest.mark.parametrize(\"has_bias\", [False, True])\n+@pytest.mark.parametrize(\"seqlen\", [1])\n+@pytest.mark.parametrize(\"width\", [2, 3, 4])\n+@pytest.mark.parametrize(\"dim\", [2048, 2048 + 16, 4096])\n+# tests correctness in case subset of the sequences are padded\n+@pytest.mark.parametrize(\"with_padding\", [True, False])\n+@pytest.mark.parametrize(\"batch_size\", [3])\n+def test_causal_conv1d_update_with_batch_gather_vllm(batch_size, with_padding,\n+                                                     dim, width, seqlen,\n+                                                     has_bias, silu_activation,\n+                                                     itype):\n+    device = \"cuda\"\n+    rtol, atol = (3e-4, 1e-3) if itype == torch.float32 else (3e-3, 5e-3)\n+    if itype == torch.bfloat16:\n+        rtol, atol = 1e-2, 5e-2\n+\n+    # set seed\n+    current_platform.seed_everything(0)\n+\n+    padding = 5 if with_padding else 0\n+    padded_batch_size = batch_size + padding\n+    # total_entries = number of cache line\n+    total_entries = 10 * batch_size\n+\n+    channel_last = True\n+    if not channel_last:\n+        x = torch.randn(padded_batch_size,\n+                        dim,\n+                        seqlen,\n+                        device=device,\n+                        dtype=itype)\n+    else:\n+        # x will be (batch, dim, seqlen) with contiguous along dim-axis\n+        x = torch.randn(padded_batch_size,\n+                        seqlen,\n+                        dim,\n+                        device=device,\n+                        dtype=itype).transpose(1, 2)\n+\n+    x_ref = x.clone()\n+\n+    conv_state_indices = torch.randperm(total_entries)[:batch_size].to(\n+        dtype=torch.int32, device=device)\n+    unused_states_bool = torch.ones(total_entries,\n+                                    dtype=torch.bool,\n+                                    device=device)\n+    unused_states_bool[conv_state_indices] = False\n+    padded_state_indices = torch.concat([\n+        conv_state_indices,\n+        torch.as_tensor(\n+            [PAD_SLOT_ID] * padding, dtype=torch.int32, device=device)\n+    ],\n+                                        dim=0)\n+\n+    if not channel_last:\n+        conv_state = torch.randn(total_entries,\n+                                 dim,\n+                                 width - 1,\n+                                 device=device,\n+                                 dtype=itype)\n+    else:\n+        # conv_state will be (cache_lines, dim, state_len)\n+        # with contiguous along dim-axis\n+        conv_state = torch.randn(total_entries,\n+                                 width - 1,\n+                                 dim,\n+                                 device=device,\n+                                 dtype=itype).transpose(1, 2)\n+\n+    conv_state_for_padding_test = conv_state.clone()\n+\n+    weight = torch.randn(dim, width, device=device, dtype=itype)\n+    bias = torch.randn(dim, device=device, dtype=itype) if has_bias else None\n+    conv_state_ref = conv_state[conv_state_indices, :].detach().clone()\n+    activation = None if not silu_activation else \"silu\"\n+\n+    out = causal_conv1d_update_triton(x,\n+                                      conv_state,\n+                                      weight,\n+                                      bias,\n+                                      activation=activation,\n+                                      conv_state_indices=padded_state_indices,\n+                                      pad_slot_id=PAD_SLOT_ID)\n+    out_ref = causal_conv1d_update_ref(x_ref[:batch_size],\n+                                       conv_state_ref,\n+                                       weight,\n+                                       bias,\n+                                       activation=activation)\n+\n+    assert torch.equal(conv_state[conv_state_indices, :], conv_state_ref)\n+    assert torch.equal(conv_state[unused_states_bool],\n+                       conv_state_for_padding_test[unused_states_bool])\n+    assert torch.allclose(out[:batch_size], out_ref, rtol=rtol, atol=atol)\n+\n+\n+@pytest.mark.parametrize(\"itype\", [torch.bfloat16])\n+@pytest.mark.parametrize(\"silu_activation\", [True])\n+@pytest.mark.parametrize(\"has_bias\", [True])\n+@pytest.mark.parametrize(\"width\", [4])\n+@pytest.mark.parametrize('seqlen', [8, 16, 784, 1024, 2048, 2049, 4096])\n+@pytest.mark.parametrize('dim', [64, 4096])\n+@pytest.mark.parametrize('with_padding', [True, False])\n+@pytest.mark.parametrize('batch', [4])\n+def test_causal_conv1d_varlen_vllm(batch, with_padding, dim, seqlen, width,\n+                                   has_bias, silu_activation, itype):\n+    device = \"cuda\"\n+    torch.cuda.empty_cache()\n+    rtol, atol = (3e-4, 1e-3) if itype == torch.float32 else (3e-3, 5e-3)\n+    if itype == torch.bfloat16:\n+        rtol, atol = 1e-2, 5e-2\n+    # set seed\n+    current_platform.seed_everything(0)\n+    seqlens = []\n+    batch_size = batch\n+    padding = 3 if with_padding else 0\n+    padded_batch_size = batch_size + padding\n+    nsplits = padded_batch_size - 1\n+\n+    eos_pos = torch.randperm(seqlen - 1)[:nsplits].sort().values\n+\n+    seqlens.append(\n+        torch.diff(\n+            torch.cat(\n+                [torch.tensor([-1]), eos_pos,\n+                 torch.tensor([seqlen - 1])])).tolist())\n+    assert sum(seqlens[-1]) == seqlen\n+    assert all(s > 0 for s in seqlens[-1])\n+\n+    total_entries = batch_size * 10\n+    cumsum = torch.cumsum(torch.tensor(seqlens[0]), dim=0).to(torch.int32)\n+    cumsum = torch.concat([torch.tensor([0], dtype=torch.int32), cumsum],\n+                          dim=0)\n+    channel_last = True\n+    if not channel_last:\n+        x = torch.randn(1, 4096 + dim + 64, seqlen, device=device,\n+                        dtype=itype)[:, 4096:4096 + dim, :]\n+    else:\n+        x = rearrange(\n+            torch.randn(1, seqlen, 4096 + dim + 64, device=device,\n+                        dtype=itype), \"b s d -> b d s\")[:, 4096:4096 + dim, :]\n+\n+    weight = torch.randn(dim, width, device=device, dtype=itype)\n+\n+    bias = torch.randn(dim, device=device, dtype=itype) if has_bias else None\n+    x_ref = x.clone()\n+    weight_ref = weight.clone()\n+    bias_ref = bias.clone() if bias is not None else None\n+    activation = None if not silu_activation else \"silu\"\n+    if not channel_last:\n+        final_states = torch.randn(total_entries,\n+                                   dim,\n+                                   width - 1,\n+                                   device=x.device,\n+                                   dtype=x.dtype)\n+    else:\n+        final_states = torch.randn(total_entries,\n+                                   width - 1,\n+                                   dim,\n+                                   device=x.device,\n+                                   dtype=x.dtype).transpose(1, 2)\n+    final_states_ref = final_states.clone()\n+    has_initial_states = torch.randint(0,\n+                                       2, (cumsum.shape[0] - 1, ),\n+                                       dtype=torch.bool,\n+                                       device=x.device)\n+    state_indices = torch.randperm(total_entries,\n+                                   dtype=torch.int32,\n+                                   device=x.device)[:batch_size]\n+    padded_state_indices = torch.concat([\n+        state_indices,\n+        torch.as_tensor(\n+            [PAD_SLOT_ID] * padding, dtype=torch.int32, device=device),\n+    ],\n+                                        dim=-1)\n+    out = causal_conv1d_fn_triton(x.squeeze(0),\n+                                  weight,\n+                                  bias=bias,\n+                                  conv_states=final_states,\n+                                  query_start_loc=cumsum.cuda(),\n+                                  cache_indices=padded_state_indices,\n+                                  has_initial_states=has_initial_states,\n+                                  activation=activation,\n+                                  pad_slot_id=PAD_SLOT_ID)\n+\n+    out_ref = []\n+    out_ref_b = []\n+\n+    splits = [torch.split(var, seqlens[0], dim=-1) for var in (x_ref)]\n+    for i in range(len(seqlens[0])):\n+        x_s = [v[i].unsqueeze(0) for v in splits][0]\n+        if padded_state_indices[i] == PAD_SLOT_ID:\n+            continue\n+        out_ref_b.append(\n+            causal_conv1d_ref(\n+                x_s,\n+                weight_ref,\n+                bias_ref,\n+                activation=activation,\n+                return_final_states=True,\n+                final_states_out=final_states_ref[\n+                    padded_state_indices[i]].unsqueeze(0),\n+                initial_states=final_states_ref[padded_state_indices[i]].\n+                unsqueeze(0) if has_initial_states[i] else None))\n+    out_ref.append(torch.cat([t[0] for t in out_ref_b], dim=2))\n+    out_ref_tensor = torch.cat(out_ref, dim=0)\n+\n+    try:\n+        assert torch.allclose(final_states[state_indices],\n+                              final_states_ref[state_indices],\n+                              rtol=rtol,\n+                              atol=atol)\n+        print(\"Passed conv_state\")\n+    except Exception as e:\n+        print(\"FAILED conv_state\")\n+        raise e\n+    unpadded_out = out[:, :out_ref_tensor.shape[-1]]\n+    try:\n+        assert torch.allclose(unpadded_out,\n+                              out_ref_tensor,\n+                              rtol=rtol,\n+                              atol=atol)\n+    except Exception as e:\n+        input(\n+            \"Passed conv_state, but failed output: Press Enter to continue...\")",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2109750247",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 18218,
        "pr_file": "tests/kernels/mamba/test_causal_conv1d.py",
        "discussion_id": "2109750247",
        "commented_code": "@@ -435,3 +437,237 @@ def test_causal_conv1d_varlen(with_padding, dim, seqlen, width, has_bias,\n     causal_conv1d_opcheck_fn(x.squeeze(0), weight, bias, cumsum.cuda(),\n                              padded_state_indices, has_initial_states,\n                              final_states, activation)\n+\n+\n+@pytest.mark.parametrize(\"itype\",\n+                         [torch.float32, torch.float16, torch.bfloat16])\n+@pytest.mark.parametrize(\"silu_activation\", [False, True])\n+@pytest.mark.parametrize(\"has_bias\", [False, True])\n+@pytest.mark.parametrize(\"seqlen\", [1])\n+@pytest.mark.parametrize(\"width\", [2, 3, 4])\n+@pytest.mark.parametrize(\"dim\", [2048, 2048 + 16, 4096])\n+# tests correctness in case subset of the sequences are padded\n+@pytest.mark.parametrize(\"with_padding\", [True, False])\n+@pytest.mark.parametrize(\"batch_size\", [3])\n+def test_causal_conv1d_update_with_batch_gather_vllm(batch_size, with_padding,\n+                                                     dim, width, seqlen,\n+                                                     has_bias, silu_activation,\n+                                                     itype):\n+    device = \"cuda\"\n+    rtol, atol = (3e-4, 1e-3) if itype == torch.float32 else (3e-3, 5e-3)\n+    if itype == torch.bfloat16:\n+        rtol, atol = 1e-2, 5e-2\n+\n+    # set seed\n+    current_platform.seed_everything(0)\n+\n+    padding = 5 if with_padding else 0\n+    padded_batch_size = batch_size + padding\n+    # total_entries = number of cache line\n+    total_entries = 10 * batch_size\n+\n+    channel_last = True\n+    if not channel_last:\n+        x = torch.randn(padded_batch_size,\n+                        dim,\n+                        seqlen,\n+                        device=device,\n+                        dtype=itype)\n+    else:\n+        # x will be (batch, dim, seqlen) with contiguous along dim-axis\n+        x = torch.randn(padded_batch_size,\n+                        seqlen,\n+                        dim,\n+                        device=device,\n+                        dtype=itype).transpose(1, 2)\n+\n+    x_ref = x.clone()\n+\n+    conv_state_indices = torch.randperm(total_entries)[:batch_size].to(\n+        dtype=torch.int32, device=device)\n+    unused_states_bool = torch.ones(total_entries,\n+                                    dtype=torch.bool,\n+                                    device=device)\n+    unused_states_bool[conv_state_indices] = False\n+    padded_state_indices = torch.concat([\n+        conv_state_indices,\n+        torch.as_tensor(\n+            [PAD_SLOT_ID] * padding, dtype=torch.int32, device=device)\n+    ],\n+                                        dim=0)\n+\n+    if not channel_last:\n+        conv_state = torch.randn(total_entries,\n+                                 dim,\n+                                 width - 1,\n+                                 device=device,\n+                                 dtype=itype)\n+    else:\n+        # conv_state will be (cache_lines, dim, state_len)\n+        # with contiguous along dim-axis\n+        conv_state = torch.randn(total_entries,\n+                                 width - 1,\n+                                 dim,\n+                                 device=device,\n+                                 dtype=itype).transpose(1, 2)\n+\n+    conv_state_for_padding_test = conv_state.clone()\n+\n+    weight = torch.randn(dim, width, device=device, dtype=itype)\n+    bias = torch.randn(dim, device=device, dtype=itype) if has_bias else None\n+    conv_state_ref = conv_state[conv_state_indices, :].detach().clone()\n+    activation = None if not silu_activation else \"silu\"\n+\n+    out = causal_conv1d_update_triton(x,\n+                                      conv_state,\n+                                      weight,\n+                                      bias,\n+                                      activation=activation,\n+                                      conv_state_indices=padded_state_indices,\n+                                      pad_slot_id=PAD_SLOT_ID)\n+    out_ref = causal_conv1d_update_ref(x_ref[:batch_size],\n+                                       conv_state_ref,\n+                                       weight,\n+                                       bias,\n+                                       activation=activation)\n+\n+    assert torch.equal(conv_state[conv_state_indices, :], conv_state_ref)\n+    assert torch.equal(conv_state[unused_states_bool],\n+                       conv_state_for_padding_test[unused_states_bool])\n+    assert torch.allclose(out[:batch_size], out_ref, rtol=rtol, atol=atol)\n+\n+\n+@pytest.mark.parametrize(\"itype\", [torch.bfloat16])\n+@pytest.mark.parametrize(\"silu_activation\", [True])\n+@pytest.mark.parametrize(\"has_bias\", [True])\n+@pytest.mark.parametrize(\"width\", [4])\n+@pytest.mark.parametrize('seqlen', [8, 16, 784, 1024, 2048, 2049, 4096])\n+@pytest.mark.parametrize('dim', [64, 4096])\n+@pytest.mark.parametrize('with_padding', [True, False])\n+@pytest.mark.parametrize('batch', [4])\n+def test_causal_conv1d_varlen_vllm(batch, with_padding, dim, seqlen, width,\n+                                   has_bias, silu_activation, itype):\n+    device = \"cuda\"\n+    torch.cuda.empty_cache()\n+    rtol, atol = (3e-4, 1e-3) if itype == torch.float32 else (3e-3, 5e-3)\n+    if itype == torch.bfloat16:\n+        rtol, atol = 1e-2, 5e-2\n+    # set seed\n+    current_platform.seed_everything(0)\n+    seqlens = []\n+    batch_size = batch\n+    padding = 3 if with_padding else 0\n+    padded_batch_size = batch_size + padding\n+    nsplits = padded_batch_size - 1\n+\n+    eos_pos = torch.randperm(seqlen - 1)[:nsplits].sort().values\n+\n+    seqlens.append(\n+        torch.diff(\n+            torch.cat(\n+                [torch.tensor([-1]), eos_pos,\n+                 torch.tensor([seqlen - 1])])).tolist())\n+    assert sum(seqlens[-1]) == seqlen\n+    assert all(s > 0 for s in seqlens[-1])\n+\n+    total_entries = batch_size * 10\n+    cumsum = torch.cumsum(torch.tensor(seqlens[0]), dim=0).to(torch.int32)\n+    cumsum = torch.concat([torch.tensor([0], dtype=torch.int32), cumsum],\n+                          dim=0)\n+    channel_last = True\n+    if not channel_last:\n+        x = torch.randn(1, 4096 + dim + 64, seqlen, device=device,\n+                        dtype=itype)[:, 4096:4096 + dim, :]\n+    else:\n+        x = rearrange(\n+            torch.randn(1, seqlen, 4096 + dim + 64, device=device,\n+                        dtype=itype), \"b s d -> b d s\")[:, 4096:4096 + dim, :]\n+\n+    weight = torch.randn(dim, width, device=device, dtype=itype)\n+\n+    bias = torch.randn(dim, device=device, dtype=itype) if has_bias else None\n+    x_ref = x.clone()\n+    weight_ref = weight.clone()\n+    bias_ref = bias.clone() if bias is not None else None\n+    activation = None if not silu_activation else \"silu\"\n+    if not channel_last:\n+        final_states = torch.randn(total_entries,\n+                                   dim,\n+                                   width - 1,\n+                                   device=x.device,\n+                                   dtype=x.dtype)\n+    else:\n+        final_states = torch.randn(total_entries,\n+                                   width - 1,\n+                                   dim,\n+                                   device=x.device,\n+                                   dtype=x.dtype).transpose(1, 2)\n+    final_states_ref = final_states.clone()\n+    has_initial_states = torch.randint(0,\n+                                       2, (cumsum.shape[0] - 1, ),\n+                                       dtype=torch.bool,\n+                                       device=x.device)\n+    state_indices = torch.randperm(total_entries,\n+                                   dtype=torch.int32,\n+                                   device=x.device)[:batch_size]\n+    padded_state_indices = torch.concat([\n+        state_indices,\n+        torch.as_tensor(\n+            [PAD_SLOT_ID] * padding, dtype=torch.int32, device=device),\n+    ],\n+                                        dim=-1)\n+    out = causal_conv1d_fn_triton(x.squeeze(0),\n+                                  weight,\n+                                  bias=bias,\n+                                  conv_states=final_states,\n+                                  query_start_loc=cumsum.cuda(),\n+                                  cache_indices=padded_state_indices,\n+                                  has_initial_states=has_initial_states,\n+                                  activation=activation,\n+                                  pad_slot_id=PAD_SLOT_ID)\n+\n+    out_ref = []\n+    out_ref_b = []\n+\n+    splits = [torch.split(var, seqlens[0], dim=-1) for var in (x_ref)]\n+    for i in range(len(seqlens[0])):\n+        x_s = [v[i].unsqueeze(0) for v in splits][0]\n+        if padded_state_indices[i] == PAD_SLOT_ID:\n+            continue\n+        out_ref_b.append(\n+            causal_conv1d_ref(\n+                x_s,\n+                weight_ref,\n+                bias_ref,\n+                activation=activation,\n+                return_final_states=True,\n+                final_states_out=final_states_ref[\n+                    padded_state_indices[i]].unsqueeze(0),\n+                initial_states=final_states_ref[padded_state_indices[i]].\n+                unsqueeze(0) if has_initial_states[i] else None))\n+    out_ref.append(torch.cat([t[0] for t in out_ref_b], dim=2))\n+    out_ref_tensor = torch.cat(out_ref, dim=0)\n+\n+    try:\n+        assert torch.allclose(final_states[state_indices],\n+                              final_states_ref[state_indices],\n+                              rtol=rtol,\n+                              atol=atol)\n+        print(\"Passed conv_state\")\n+    except Exception as e:\n+        print(\"FAILED conv_state\")\n+        raise e\n+    unpadded_out = out[:, :out_ref_tensor.shape[-1]]\n+    try:\n+        assert torch.allclose(unpadded_out,\n+                              out_ref_tensor,\n+                              rtol=rtol,\n+                              atol=atol)\n+    except Exception as e:\n+        input(\n+            \"Passed conv_state, but failed output: Press Enter to continue...\")",
        "comment_created_at": "2025-05-27T17:17:05+00:00",
        "comment_author": "tlrmchlsmth",
        "comment_body": "I can see that doing this makes sense while debugging, but we should make this much simpler and noninteractive before landing.\r\n\r\n```suggestion\r\n    assert torch.allclose(final_states[state_indices],\r\n                          final_states_ref[state_indices],\r\n                          rtol=rtol,\r\n                          atol=atol)\r\n    unpadded_out = out[:, :out_ref_tensor.shape[-1]]\r\n    assert torch.allclose(unpadded_out,\r\n                          out_ref_tensor,\r\n                          rtol=rtol,\r\n                          atol=atol)\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2155794695",
    "pr_number": 19825,
    "pr_file": "vllm/v1/attention/backends/flashinfer.py",
    "created_at": "2025-06-19T00:53:41+00:00",
    "commented_code": "logits_soft_cap=self.global_hyperparameters.\n                     logits_soft_cap,\n                     q_data_type=attn_metadata.q_data_type,\n-                    kv_data_type=attn_metadata.data_type,\n+                    kv_data_type=attn_metadata.kv_data_type,\n                 )\n \n             if self._num_decodes > 0:\n                 attn_metadata.decode_wrapper = self._get_decode_wrapper()\n-                attn_metadata.decode_wrapper.plan(\n-                    attn_metadata.paged_kv_indptr[:self._num_decodes + 1],\n-                    attn_metadata.paged_kv_indices,\n-                    attn_metadata.paged_kv_last_page_len[:self._num_decodes],\n-                    attn_metadata.num_qo_heads,\n-                    attn_metadata.num_kv_heads,\n-                    attn_metadata.head_dim,\n-                    attn_metadata.page_size,\n-                    # Disable flashinfer's pos encoding and use vllm's rope.\n-                    pos_encoding_mode=\"NONE\",\n-                    sm_scale=self.global_hyperparameters.sm_scale,\n-                    window_left=self.global_hyperparameters.window_left,\n-                    logits_soft_cap=self.global_hyperparameters.\n-                    logits_soft_cap,\n-                    q_data_type=attn_metadata.q_data_type,\n-                    kv_data_type=attn_metadata.data_type,\n+                if not envs.VLLM_USE_TRTLLM_DECODE_ATTENTION:\n+                    attn_metadata.decode_wrapper.plan(\n+                        attn_metadata.paged_kv_indptr[:self._num_decodes + 1],\n+                        attn_metadata.paged_kv_indices,\n+                        attn_metadata.paged_kv_last_page_len[:self._num_decodes],\n+                        attn_metadata.num_qo_heads,\n+                        attn_metadata.num_kv_heads,\n+                        attn_metadata.head_dim,\n+                        attn_metadata.page_size,\n+                        # Disable flashinfer's pos encoding and use vllm's rope.\n+                        pos_encoding_mode=\"NONE\",\n+                        sm_scale=self.global_hyperparameters.sm_scale,\n+                        window_left=self.global_hyperparameters.window_left,\n+                        logits_soft_cap=self.global_hyperparameters.\n+                        logits_soft_cap,\n+                        q_data_type=attn_metadata.q_data_type,\n+                        kv_data_type=attn_metadata.kv_data_type,\n+                    )\n+                else:\n+                    # kv_cache_shape = (NUM_BLOCKS, 2, num_kv_heads, page_size, head_dim)\n+                    trtllm_batch_decode_with_kv_cache(\n+                    query=torch.zeros(self._num_decodes, attn_metadata.num_qo_heads, attn_metadata.head_dim, device=self.runner.device, dtype=attn_metadata.q_data_type).half(),\n+                    kv_cache=torch.zeros(2048, 2, attn_metadata.num_kv_heads, attn_metadata.head_dim, device=self.runner.device, dtype=attn_metadata.kv_data_type).half(),",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2155794695",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19825,
        "pr_file": "vllm/v1/attention/backends/flashinfer.py",
        "discussion_id": "2155794695",
        "commented_code": "@@ -376,27 +382,45 @@ def _plan(self, attn_metadata: FlashInferMetadata):\n                     logits_soft_cap=self.global_hyperparameters.\n                     logits_soft_cap,\n                     q_data_type=attn_metadata.q_data_type,\n-                    kv_data_type=attn_metadata.data_type,\n+                    kv_data_type=attn_metadata.kv_data_type,\n                 )\n \n             if self._num_decodes > 0:\n                 attn_metadata.decode_wrapper = self._get_decode_wrapper()\n-                attn_metadata.decode_wrapper.plan(\n-                    attn_metadata.paged_kv_indptr[:self._num_decodes + 1],\n-                    attn_metadata.paged_kv_indices,\n-                    attn_metadata.paged_kv_last_page_len[:self._num_decodes],\n-                    attn_metadata.num_qo_heads,\n-                    attn_metadata.num_kv_heads,\n-                    attn_metadata.head_dim,\n-                    attn_metadata.page_size,\n-                    # Disable flashinfer's pos encoding and use vllm's rope.\n-                    pos_encoding_mode=\"NONE\",\n-                    sm_scale=self.global_hyperparameters.sm_scale,\n-                    window_left=self.global_hyperparameters.window_left,\n-                    logits_soft_cap=self.global_hyperparameters.\n-                    logits_soft_cap,\n-                    q_data_type=attn_metadata.q_data_type,\n-                    kv_data_type=attn_metadata.data_type,\n+                if not envs.VLLM_USE_TRTLLM_DECODE_ATTENTION:\n+                    attn_metadata.decode_wrapper.plan(\n+                        attn_metadata.paged_kv_indptr[:self._num_decodes + 1],\n+                        attn_metadata.paged_kv_indices,\n+                        attn_metadata.paged_kv_last_page_len[:self._num_decodes],\n+                        attn_metadata.num_qo_heads,\n+                        attn_metadata.num_kv_heads,\n+                        attn_metadata.head_dim,\n+                        attn_metadata.page_size,\n+                        # Disable flashinfer's pos encoding and use vllm's rope.\n+                        pos_encoding_mode=\"NONE\",\n+                        sm_scale=self.global_hyperparameters.sm_scale,\n+                        window_left=self.global_hyperparameters.window_left,\n+                        logits_soft_cap=self.global_hyperparameters.\n+                        logits_soft_cap,\n+                        q_data_type=attn_metadata.q_data_type,\n+                        kv_data_type=attn_metadata.kv_data_type,\n+                    )\n+                else:\n+                    # kv_cache_shape = (NUM_BLOCKS, 2, num_kv_heads, page_size, head_dim)\n+                    trtllm_batch_decode_with_kv_cache(\n+                    query=torch.zeros(self._num_decodes, attn_metadata.num_qo_heads, attn_metadata.head_dim, device=self.runner.device, dtype=attn_metadata.q_data_type).half(),\n+                    kv_cache=torch.zeros(2048, 2, attn_metadata.num_kv_heads, attn_metadata.head_dim, device=self.runner.device, dtype=attn_metadata.kv_data_type).half(),",
        "comment_created_at": "2025-06-19T00:53:41+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe shape `(2048, 2, attn_metadata.num_kv_heads, attn_metadata.head_dim)` for `kv_cache` is hardcoded. It should be derived from `attn_metadata` or other relevant configuration parameters to ensure it's dynamically sized based on the actual model configuration. Otherwise, it may lead to incorrect behavior or errors when used with different model sizes.\n\n```suggestion\n                    kv_cache=torch.zeros(2048, 2, attn_metadata.num_kv_heads, attn_metadata.head_dim, device=self.runner.device, dtype=attn_metadata.kv_data_type).half(),\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2188464478",
    "pr_number": 20527,
    "pr_file": "vllm/transformers_utils/config.py",
    "created_at": "2025-07-06T16:47:31+00:00",
    "commented_code": "return None\n \n \n-def get_cross_encoder_activation_function(config: PretrainedConfig):\n+def get_classification_activation_function(config: PretrainedConfig):\n+    return nn.Sigmoid() if config.num_labels == 1 else nn.Identity()",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2188464478",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20527,
        "pr_file": "vllm/transformers_utils/config.py",
        "discussion_id": "2188464478",
        "commented_code": "@@ -866,24 +866,26 @@ def try_get_generation_config(\n             return None\n \n \n-def get_cross_encoder_activation_function(config: PretrainedConfig):\n+def get_classification_activation_function(config: PretrainedConfig):\n+    return nn.Sigmoid() if config.num_labels == 1 else nn.Identity()",
        "comment_created_at": "2025-07-06T16:47:31+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThis function returns `nn.Identity()` for multi-label classification (`config.num_labels > 1`), which is a change from the previous behavior that used `nn.Softmax()`. This is a regression for general-purpose classification models, as it would return raw logits instead of probabilities. Restore the `Softmax` activation for multi-label cases to maintain correct behavior for classification tasks. Note that `nn.Softmax()` without a `dim` argument is deprecated, so `dim=-1` should be specified.\n\n```suggestion\ndef get_classification_activation_function(config: PretrainedConfig):\n    return nn.Sigmoid() if config.num_labels == 1 else nn.Softmax(dim=-1)\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2188467099",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20527,
        "pr_file": "vllm/transformers_utils/config.py",
        "discussion_id": "2188464478",
        "commented_code": "@@ -866,24 +866,26 @@ def try_get_generation_config(\n             return None\n \n \n-def get_cross_encoder_activation_function(config: PretrainedConfig):\n+def get_classification_activation_function(config: PretrainedConfig):\n+    return nn.Sigmoid() if config.num_labels == 1 else nn.Identity()",
        "comment_created_at": "2025-07-06T16:50:41+00:00",
        "comment_author": "DarkLight1337",
        "comment_body": "Nice catch!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2164654274",
    "pr_number": 20034,
    "pr_file": "vllm/v1/attention/backends/mla/common.py",
    "created_at": "2025-06-24T18:30:48+00:00",
    "commented_code": ")\n         self.block_table = block_table\n \n+        # FI\n+        self._workspace_buffer = None\n+        self._prefill_wrapper = None  # Wrapper for prefill/append\n+        self._prefill_wrapper_ctx = []  # Wrapper for prefill/append\n+        self.global_hyperparameters = None\n+\n+    def _get_workspace_buffer(self):\n+        if self._workspace_buffer is None:\n+            self._workspace_buffer = torch.empty(\n+                FLASHINFER_WORKSPACE_BUFFER_SIZE,\n+                dtype=torch.uint8,\n+                device=self.runner.device)\n+        return self._workspace_buffer\n+\n+    def _get_prefill_wrapper(self):\n+        if self._prefill_wrapper is None:\n+            self._prefill_wrapper = BatchPrefillWithRaggedKVCacheWrapper(\n+                self._get_workspace_buffer(), \"NHD\")\n+\n+        return self._prefill_wrapper\n+\n+    def _get_prefill_wrapper_ctx(self, num_chunks):\n+        if len(self._prefill_wrapper_ctx) < num_chunks:\n+            for _ in range(len(self._prefill_wrapper_ctx), num_chunks):\n+                self._prefill_wrapper_ctx.append(\n+                    BatchPrefillWithRaggedKVCacheWrapper(\n+                        self._get_workspace_buffer(), \"NHD\"))\n+\n+        return self._prefill_wrapper_ctx\n+\n+    def _build_fi_prefill(self, common_attn_metadata: CommonAttentionMetadata,\n+                          attn_metadata: MLACommonMetadata):\n+        # print(\"INSIDE _build_fi_prefill\")\n+        if self.global_hyperparameters is None:\n+            self.global_hyperparameters = infer_global_hyperparameters(\n+                get_per_layer_parameters(self.runner.vllm_config))\n+\n+        assert attn_metadata.prefill is not None\n+        qo_indptr = attn_metadata.prefill.query_start_loc\n+\n+        # print(\"    qo_indptr.shape = {} qo_indptr = {}\".format(qo_indptr.shape, qo_indptr))\n+\n+        # slot_mapping = attn_metadata.slot_mapping\n+\n+        # num_reqs = common_attn_metadata.num_reqs\n+        # num_actual_tokens = common_attn_metadata.num_actual_tokens\n+        # print(\"    num_reqs = {} num_actual_tokens = {}\".format(num_reqs, num_actual_tokens))\n+\n+        # assert self._num_decodes + self._num_prefills == num_reqs\n+        # assert (self._num_decode_tokens +\n+        #         self._num_prefill_tokens == num_actual_tokens)\n+\n+        # page_size = self.kv_cache_spec.block_size\n+        # device = self.runner.device\n+        # print(\"    page_size = {}\".format(page_size))\n+\n+        # prefill_seq_lens = common_attn_metadata.seq_lens[self._num_decodes:]\n+        # print(\"    prefill_seq_lens.shape = {} prefill_seq_lens = {}\".format(prefill_seq_lens.shape, prefill_seq_lens))\n+\n+        # prefill_block_table_bounds = (prefill_seq_lens + page_size -\n+        #                               1) // page_size\n+        # print(\"    prefill_block_table_bounds.shape = {} prefill_block_table_bounds = {}\".format(prefill_block_table_bounds.shape, prefill_block_table_bounds))\n+\n+        # prefill_block_table = attn_metadata.prefill.block_table\n+        # print(\"    prefill_block_table.shape = {}\".format(prefill_block_table.shape))\n+\n+        # mask = (torch.arange(prefill_block_table.size(1),\n+        #                      dtype=prefill_block_table.dtype,\n+        #                      device=prefill_block_table.device).unsqueeze(0)\n+        #         < prefill_block_table_bounds.unsqueeze(1))\n+        # # print(\"    mask.shape = {} mask = {}\".format(mask.shape, mask))\n+\n+        # prefill_paged_kv_indices = prefill_block_table[mask]\n+        # # print(\"    prefill_paged_kv_indices.shape = {} prefill_paged_kv_indices = {}\".format(prefill_paged_kv_indices.shape, prefill_paged_kv_indices))\n+\n+        # prefill_paged_kv_indptr = torch.cat([\n+        #     torch.zeros(1,\n+        #                 dtype=prefill_block_table_bounds.dtype,\n+        #                 device=prefill_block_table_bounds.device),\n+        #     prefill_block_table_bounds.cumsum(dim=0, dtype=torch.int32)\n+        # ])\n+\n+        # # print(\"    prefill_paged_kv_indptr.shape = {} prefill_paged_kv_indptr = {}\".format(prefill_paged_kv_indptr.shape, prefill_paged_kv_indptr))\n+\n+        # prefill_paged_kv_last_page_len = prefill_seq_lens % page_size\n+        # prefill_paged_kv_last_page_len = torch.where(\n+        #     prefill_paged_kv_last_page_len == 0, page_size,\n+        #     prefill_paged_kv_last_page_len)\n+\n+        # print(\"    prefill_paged_kv_last_page_len.shape = {} prefill_paged_kv_last_page_len = {}\".format(prefill_paged_kv_last_page_len.shape, prefill_paged_kv_last_page_len))\n+\n+        prefill_wrapper = self._get_prefill_wrapper()\n+\n+        has_context = attn_metadata.prefill.chunked_context is not None\n+\n+        if has_context:\n+            num_chunks = attn_metadata.prefill.chunked_context.cu_seq_lens.shape[\n+                0]\n+            prefill_wrapper_ctx = self._get_prefill_wrapper_ctx(num_chunks)\n+        else:\n+            prefill_wrapper_ctx = []\n+        num_qo_heads = self.runner.num_query_heads\n+        num_kv_heads = self.kv_cache_spec.num_kv_heads\n+        head_dim_qk = self.kv_cache_spec.head_size\n+\n+        # print(\"num_qo_heads = {}\".format(num_qo_heads))\n+        # print(\"num_kv_heads = {}\".format(num_kv_heads))\n+        # print(\"head_dim_qk = {}\".format(head_dim_qk))\n+        # print(\"global_hyperparameters.sm_scale = {}\".format(self.global_hyperparameters.sm_scale))\n+        # print(\"global_hyperparameters.window_left = {}\".format(self.global_hyperparameters.window_left))\n+        # print(\"global_hyperparameters.logits_soft_cap = {}\".format(self.global_hyperparameters.logits_soft_cap))\n+\n+        kv_indptr = qo_indptr.clone()\n+\n+        prefill_wrapper.plan(\n+            qo_indptr,\n+            kv_indptr,\n+            num_qo_heads,\n+            num_kv_heads,\n+            192,  #head_dim_qk,\n+            causal=True,\n+            head_dim_vo=128,",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2164654274",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20034,
        "pr_file": "vllm/v1/attention/backends/mla/common.py",
        "discussion_id": "2164654274",
        "commented_code": "@@ -384,6 +527,204 @@\n             )\n         self.block_table = block_table\n \n+        # FI\n+        self._workspace_buffer = None\n+        self._prefill_wrapper = None  # Wrapper for prefill/append\n+        self._prefill_wrapper_ctx = []  # Wrapper for prefill/append\n+        self.global_hyperparameters = None\n+\n+    def _get_workspace_buffer(self):\n+        if self._workspace_buffer is None:\n+            self._workspace_buffer = torch.empty(\n+                FLASHINFER_WORKSPACE_BUFFER_SIZE,\n+                dtype=torch.uint8,\n+                device=self.runner.device)\n+        return self._workspace_buffer\n+\n+    def _get_prefill_wrapper(self):\n+        if self._prefill_wrapper is None:\n+            self._prefill_wrapper = BatchPrefillWithRaggedKVCacheWrapper(\n+                self._get_workspace_buffer(), \"NHD\")\n+\n+        return self._prefill_wrapper\n+\n+    def _get_prefill_wrapper_ctx(self, num_chunks):\n+        if len(self._prefill_wrapper_ctx) < num_chunks:\n+            for _ in range(len(self._prefill_wrapper_ctx), num_chunks):\n+                self._prefill_wrapper_ctx.append(\n+                    BatchPrefillWithRaggedKVCacheWrapper(\n+                        self._get_workspace_buffer(), \"NHD\"))\n+\n+        return self._prefill_wrapper_ctx\n+\n+    def _build_fi_prefill(self, common_attn_metadata: CommonAttentionMetadata,\n+                          attn_metadata: MLACommonMetadata):\n+        # print(\"INSIDE _build_fi_prefill\")\n+        if self.global_hyperparameters is None:\n+            self.global_hyperparameters = infer_global_hyperparameters(\n+                get_per_layer_parameters(self.runner.vllm_config))\n+\n+        assert attn_metadata.prefill is not None\n+        qo_indptr = attn_metadata.prefill.query_start_loc\n+\n+        # print(\"    qo_indptr.shape = {} qo_indptr = {}\".format(qo_indptr.shape, qo_indptr))\n+\n+        # slot_mapping = attn_metadata.slot_mapping\n+\n+        # num_reqs = common_attn_metadata.num_reqs\n+        # num_actual_tokens = common_attn_metadata.num_actual_tokens\n+        # print(\"    num_reqs = {} num_actual_tokens = {}\".format(num_reqs, num_actual_tokens))\n+\n+        # assert self._num_decodes + self._num_prefills == num_reqs\n+        # assert (self._num_decode_tokens +\n+        #         self._num_prefill_tokens == num_actual_tokens)\n+\n+        # page_size = self.kv_cache_spec.block_size\n+        # device = self.runner.device\n+        # print(\"    page_size = {}\".format(page_size))\n+\n+        # prefill_seq_lens = common_attn_metadata.seq_lens[self._num_decodes:]\n+        # print(\"    prefill_seq_lens.shape = {} prefill_seq_lens = {}\".format(prefill_seq_lens.shape, prefill_seq_lens))\n+\n+        # prefill_block_table_bounds = (prefill_seq_lens + page_size -\n+        #                               1) // page_size\n+        # print(\"    prefill_block_table_bounds.shape = {} prefill_block_table_bounds = {}\".format(prefill_block_table_bounds.shape, prefill_block_table_bounds))\n+\n+        # prefill_block_table = attn_metadata.prefill.block_table\n+        # print(\"    prefill_block_table.shape = {}\".format(prefill_block_table.shape))\n+\n+        # mask = (torch.arange(prefill_block_table.size(1),\n+        #                      dtype=prefill_block_table.dtype,\n+        #                      device=prefill_block_table.device).unsqueeze(0)\n+        #         < prefill_block_table_bounds.unsqueeze(1))\n+        # # print(\"    mask.shape = {} mask = {}\".format(mask.shape, mask))\n+\n+        # prefill_paged_kv_indices = prefill_block_table[mask]\n+        # # print(\"    prefill_paged_kv_indices.shape = {} prefill_paged_kv_indices = {}\".format(prefill_paged_kv_indices.shape, prefill_paged_kv_indices))\n+\n+        # prefill_paged_kv_indptr = torch.cat([\n+        #     torch.zeros(1,\n+        #                 dtype=prefill_block_table_bounds.dtype,\n+        #                 device=prefill_block_table_bounds.device),\n+        #     prefill_block_table_bounds.cumsum(dim=0, dtype=torch.int32)\n+        # ])\n+\n+        # # print(\"    prefill_paged_kv_indptr.shape = {} prefill_paged_kv_indptr = {}\".format(prefill_paged_kv_indptr.shape, prefill_paged_kv_indptr))\n+\n+        # prefill_paged_kv_last_page_len = prefill_seq_lens % page_size\n+        # prefill_paged_kv_last_page_len = torch.where(\n+        #     prefill_paged_kv_last_page_len == 0, page_size,\n+        #     prefill_paged_kv_last_page_len)\n+\n+        # print(\"    prefill_paged_kv_last_page_len.shape = {} prefill_paged_kv_last_page_len = {}\".format(prefill_paged_kv_last_page_len.shape, prefill_paged_kv_last_page_len))\n+\n+        prefill_wrapper = self._get_prefill_wrapper()\n+\n+        has_context = attn_metadata.prefill.chunked_context is not None\n+\n+        if has_context:\n+            num_chunks = attn_metadata.prefill.chunked_context.cu_seq_lens.shape[\n+                0]\n+            prefill_wrapper_ctx = self._get_prefill_wrapper_ctx(num_chunks)\n+        else:\n+            prefill_wrapper_ctx = []\n+        num_qo_heads = self.runner.num_query_heads\n+        num_kv_heads = self.kv_cache_spec.num_kv_heads\n+        head_dim_qk = self.kv_cache_spec.head_size\n+\n+        # print(\"num_qo_heads = {}\".format(num_qo_heads))\n+        # print(\"num_kv_heads = {}\".format(num_kv_heads))\n+        # print(\"head_dim_qk = {}\".format(head_dim_qk))\n+        # print(\"global_hyperparameters.sm_scale = {}\".format(self.global_hyperparameters.sm_scale))\n+        # print(\"global_hyperparameters.window_left = {}\".format(self.global_hyperparameters.window_left))\n+        # print(\"global_hyperparameters.logits_soft_cap = {}\".format(self.global_hyperparameters.logits_soft_cap))\n+\n+        kv_indptr = qo_indptr.clone()\n+\n+        prefill_wrapper.plan(\n+            qo_indptr,\n+            kv_indptr,\n+            num_qo_heads,\n+            num_kv_heads,\n+            192,  #head_dim_qk,\n+            causal=True,\n+            head_dim_vo=128,",
        "comment_created_at": "2025-06-24T18:30:48+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe head dimensions are hardcoded here. It would be better to use variables or constants defined elsewhere in the code to ensure consistency and avoid magic numbers.\n\n```python\n            head_dim_qk,  #head_dim_qk,\n            causal=True,\n            head_dim_vo=self.kv_cache_spec.head_size,\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2143102863",
    "pr_number": 19562,
    "pr_file": "vllm/model_executor/layers/quantization/modelopt.py",
    "created_at": "2025-06-12T15:45:38+00:00",
    "commented_code": "a1_gscale=layer.w13_input_scale_quant,\n                                a2_gscale=layer.w2_input_scale_quant,\n                                device=x.device).to(x.dtype)\n+\n+def flashinfer_cutlass_fp4_supported() -> bool:\n+    return cutlass_fp4_supported() and flashinfer_cutlass_fused_moe is not None\n+\n+\n+class FlashInferCutlassNvFp4FusedMoE(ModelOptNvFp4FusedMoE):\n+    \"\"\"\n+    MoE TRTLLM-CUTLASS Method for FP4 Quantization.\n+    Args: \n+        quant_config: NVFP4 CUTLASS Quant Config\n+    \"\"\"\n+\n+    def __init__(self, quant_config: ModelOptNvFp4Config):\n+        self.quant_config = quant_config\n+        self.flashinfer_cutlass_fp4_supported = flashinfer_cutlass_fp4_supported()\n+        self.use_marlin = False\n+\n+        if not self.flashinfer_cutlass_fp4_supported:\n+            raise ValueError(\"Current platform does not support NVFP4\"\n+                                \" quantization for flashinfer-cutlass implementation.\"\n+                                \" Please use Blackwell and\"\n+                                \" above.\")\n+\n+    @property\n+    def load_up_proj_weight_first(self) -> bool:\n+        return True\n+\n+    def apply(\n+        self,\n+        layer: torch.nn.Module,\n+        x: torch.Tensor,\n+        router_logits: torch.Tensor,\n+        top_k: int,\n+        renormalize: bool,\n+        use_grouped_topk: bool = False,\n+        topk_group: Optional[int] = None,\n+        num_expert_group: Optional[int] = None,\n+        global_num_experts: int = -1,\n+        expert_map: Optional[torch.Tensor] = None,\n+        custom_routing_function: Optional[Callable] = None,\n+        scoring_func: str = \"softmax\",\n+        e_score_correction_bias: Optional[torch.Tensor] = None,\n+        apply_router_weight_on_input: bool = False,\n+        activation: str = \"silu\",\n+        ep_rank: Optional[int] = None,\n+        ep_size: Optional[int] = None,\n+        tp_rank: Optional[int] = None,\n+        tp_size: Optional[int] = None,        \n+    ):\n+\n+        assert activation == \"silu\", \"Only SiLU activation is supported.\"\n+        assert not apply_router_weight_on_input, (\n+            \"Router weight on input is not \"\n+            \"supported for FlashInferCutlassFusedMoE.\")\n+        # assert expert_map is None, (\"Expert Parallelism / expert_map \"\n+        #                             \"is currently not supported for \"\n+        #                             \"FlashInferCutlassFusedMoE.\")\n+\n+        topk_weights, topk_ids = FusedMoEFlashinferCutlass.select_experts(\n+            hidden_states=x,\n+            router_logits=router_logits,\n+            use_grouped_topk=use_grouped_topk,\n+            top_k=top_k,\n+            renormalize=renormalize,\n+            topk_group=topk_group,\n+            num_expert_group=num_expert_group,\n+            custom_routing_function=custom_routing_function,\n+            scoring_func=scoring_func,\n+            e_score_correction_bias=e_score_correction_bias)\n+        \n+        a1_gs = torch.min(layer.w13_input_scale_quant)\n+        a2_gs = torch.min(layer.w2_input_scale_quant)\n+        w1_blockscale=layer.w13_blockscale_swizzled                                                                    \n+        w2_blockscale=layer.w2_blockscale_swizzled                                                                                          \n+        g1_alphas=layer.g1_alphas                                                                                      \n+        g2_alphas=layer.g2_alphas\n+        \n+        quant_scales=[\n+            a1_gs,\n+            w1_blockscale.view(torch.int32),\n+            g1_alphas,\n+            a2_gs,\n+            w2_blockscale.view(torch.int32),\n+            g2_alphas,",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2143102863",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19562,
        "pr_file": "vllm/model_executor/layers/quantization/modelopt.py",
        "discussion_id": "2143102863",
        "commented_code": "@@ -735,3 +743,108 @@ def apply(\n                                a1_gscale=layer.w13_input_scale_quant,\n                                a2_gscale=layer.w2_input_scale_quant,\n                                device=x.device).to(x.dtype)\n+\n+def flashinfer_cutlass_fp4_supported() -> bool:\n+    return cutlass_fp4_supported() and flashinfer_cutlass_fused_moe is not None\n+\n+\n+class FlashInferCutlassNvFp4FusedMoE(ModelOptNvFp4FusedMoE):\n+    \"\"\"\n+    MoE TRTLLM-CUTLASS Method for FP4 Quantization.\n+    Args: \n+        quant_config: NVFP4 CUTLASS Quant Config\n+    \"\"\"\n+\n+    def __init__(self, quant_config: ModelOptNvFp4Config):\n+        self.quant_config = quant_config\n+        self.flashinfer_cutlass_fp4_supported = flashinfer_cutlass_fp4_supported()\n+        self.use_marlin = False\n+\n+        if not self.flashinfer_cutlass_fp4_supported:\n+            raise ValueError(\"Current platform does not support NVFP4\"\n+                                \" quantization for flashinfer-cutlass implementation.\"\n+                                \" Please use Blackwell and\"\n+                                \" above.\")\n+\n+    @property\n+    def load_up_proj_weight_first(self) -> bool:\n+        return True\n+\n+    def apply(\n+        self,\n+        layer: torch.nn.Module,\n+        x: torch.Tensor,\n+        router_logits: torch.Tensor,\n+        top_k: int,\n+        renormalize: bool,\n+        use_grouped_topk: bool = False,\n+        topk_group: Optional[int] = None,\n+        num_expert_group: Optional[int] = None,\n+        global_num_experts: int = -1,\n+        expert_map: Optional[torch.Tensor] = None,\n+        custom_routing_function: Optional[Callable] = None,\n+        scoring_func: str = \"softmax\",\n+        e_score_correction_bias: Optional[torch.Tensor] = None,\n+        apply_router_weight_on_input: bool = False,\n+        activation: str = \"silu\",\n+        ep_rank: Optional[int] = None,\n+        ep_size: Optional[int] = None,\n+        tp_rank: Optional[int] = None,\n+        tp_size: Optional[int] = None,        \n+    ):\n+\n+        assert activation == \"silu\", \"Only SiLU activation is supported.\"\n+        assert not apply_router_weight_on_input, (\n+            \"Router weight on input is not \"\n+            \"supported for FlashInferCutlassFusedMoE.\")\n+        # assert expert_map is None, (\"Expert Parallelism / expert_map \"\n+        #                             \"is currently not supported for \"\n+        #                             \"FlashInferCutlassFusedMoE.\")\n+\n+        topk_weights, topk_ids = FusedMoEFlashinferCutlass.select_experts(\n+            hidden_states=x,\n+            router_logits=router_logits,\n+            use_grouped_topk=use_grouped_topk,\n+            top_k=top_k,\n+            renormalize=renormalize,\n+            topk_group=topk_group,\n+            num_expert_group=num_expert_group,\n+            custom_routing_function=custom_routing_function,\n+            scoring_func=scoring_func,\n+            e_score_correction_bias=e_score_correction_bias)\n+        \n+        a1_gs = torch.min(layer.w13_input_scale_quant)\n+        a2_gs = torch.min(layer.w2_input_scale_quant)\n+        w1_blockscale=layer.w13_blockscale_swizzled                                                                    \n+        w2_blockscale=layer.w2_blockscale_swizzled                                                                                          \n+        g1_alphas=layer.g1_alphas                                                                                      \n+        g2_alphas=layer.g2_alphas\n+        \n+        quant_scales=[\n+            a1_gs,\n+            w1_blockscale.view(torch.int32),\n+            g1_alphas,\n+            a2_gs,\n+            w2_blockscale.view(torch.int32),\n+            g2_alphas,",
        "comment_created_at": "2025-06-12T15:45:38+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nCasting `w2_blockscale` to `torch.int32` seems incorrect. Block scales are typically floating-point values. Please verify if this casting is necessary for the FlashInfer kernel and explain why if it is.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2171219109",
    "pr_number": 20172,
    "pr_file": "tests/quantization/test_bitsandbytes_hpu.py",
    "created_at": "2025-06-27T08:31:45+00:00",
    "commented_code": "+# SPDX-License-Identifier: Apache-2.0\n+\"\"\"Tests whether bitsandbytes computation is enabled correctly.\n+Run `pytest tests/quantization/test_bitsandbytes.py`.\n+\"\"\"\n+\n+import gc\n+\n+import pytest\n+import torch\n+from transformers import BitsAndBytesConfig\n+\n+from tests.quantization.utils import is_quant_method_supported\n+\n+from ..utils import compare_two_settings, create_new_process_for_each_test\n+\n+models_4bit_to_test = [\n+    (\n+        \"/mnt/weka/data/pytorch/mistral/Mistral-7B-Instruct-v0.3\",\n+        \"quantize_inflight_model_with_both_HF_and_Mistral_format_weights\",\n+    ),\n+    (\"meta-llama/Llama-3.2-1B\", \"quantize_llama_model_inflight\"),",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2171219109",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20172,
        "pr_file": "tests/quantization/test_bitsandbytes_hpu.py",
        "discussion_id": "2171219109",
        "commented_code": "@@ -0,0 +1,156 @@\n+# SPDX-License-Identifier: Apache-2.0\n+\"\"\"Tests whether bitsandbytes computation is enabled correctly.\n+Run `pytest tests/quantization/test_bitsandbytes.py`.\n+\"\"\"\n+\n+import gc\n+\n+import pytest\n+import torch\n+from transformers import BitsAndBytesConfig\n+\n+from tests.quantization.utils import is_quant_method_supported\n+\n+from ..utils import compare_two_settings, create_new_process_for_each_test\n+\n+models_4bit_to_test = [\n+    (\n+        \"/mnt/weka/data/pytorch/mistral/Mistral-7B-Instruct-v0.3\",\n+        \"quantize_inflight_model_with_both_HF_and_Mistral_format_weights\",\n+    ),\n+    (\"meta-llama/Llama-3.2-1B\", \"quantize_llama_model_inflight\"),",
        "comment_created_at": "2025-06-27T08:31:45+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nUsing hardcoded local paths like `/mnt/weka/data/pytorch/mistral/Mistral-7B-Instruct-v0.3` in tests can reduce portability and make it difficult to run tests in different environments. Consider using publicly available Hugging Face model names or a more flexible mechanism for model paths if local models are strictly necessary for specific test cases.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2188983443",
    "pr_number": 20543,
    "pr_file": "vllm/model_executor/models/transformers.py",
    "created_at": "2025-07-07T05:11:19+00:00",
    "commented_code": ")\n \n \n+# Copied from `accelerate`\n+@contextmanager\n+def init_on_device_without_buffers(device: torch.device):\n+    \"\"\"\n+    A context manager under which models are initialized with all\n+    parameters on the specified device. However buffers are not\n+    initialized on specified device.\n+\n+    Args:\n+        device (`torch.device`):\n+            Device to initialize all parameters on.\n+    \"\"\"\n+\n+    old_register_parameter = nn.Module.register_parameter\n+\n+    def register_empty_parameter(module, name, param):\n+        old_register_parameter(module, name, param)\n+        if param is not None:\n+            param_cls = type(module._parameters[name])\n+            kwargs = module._parameters[name].__dict__\n+            kwargs[\"requires_grad\"] = param.requires_grad\n+            module._parameters[name] = param_cls(\n+                module._parameters[name].to(device), **kwargs)\n+\n+    tensor_constructors_to_patch = {}\n+\n+    def patch_tensor_constructor(fn):\n+\n+        def wrapper(*args, **kwargs):\n+            kwargs[\"device\"] = device\n+            return fn(*args, **kwargs)\n+\n+        return wrapper\n+\n+    try:\n+        nn.Module.register_parameter = register_empty_parameter\n+        for torch_function_name in tensor_constructors_to_patch:\n+            setattr(\n+                torch, torch_function_name,\n+                patch_tensor_constructor(getattr(torch, torch_function_name)))\n+        yield\n+    finally:\n+        nn.Module.register_parameter = old_register_parameter\n+        for torch_function_name, old_torch_function in tensor_constructors_to_patch.items(\n+        ):\n+            setattr(torch, torch_function_name, old_torch_function)\n+\n+\n+class MultiModalProcessingInfo(BaseProcessingInfo):\n+\n+    def get_hf_config(self):\n+        return self.ctx.model_config.hf_config\n+\n+    def get_supported_mm_limits(self):\n+        return {\"image\": None}\n+\n+    def get_mm_max_tokens_per_item(self, seq_len, mm_counts):\n+        return {\"image\": self.get_max_image_tokens()}\n+\n+    def get_max_image_tokens(self) -> int:\n+        width, height = self.get_max_image_size()\n+        processor = self.get_hf_processor()\n+        mm_processor_kwargs = self.ctx.model_config.mm_processor_kwargs or {}\n+        mm_tokens = processor._get_num_multimodal_tokens(\n+            image_sizes=([height, width], ), **mm_processor_kwargs)\n+        image_tokens = mm_tokens[\"num_image_tokens\"][0]\n+        return image_tokens\n+\n+    def get_hf_processor(self):\n+        processor = cached_get_processor(self.ctx.model_config.model)\n+        return processor\n+\n+    def get_max_image_size(self):\n+        return 10_000, 10_000  # hardcode for arbitrary very large size\n+\n+\n+class MultiModalDummyInputsBuilder(BaseDummyInputsBuilder):\n+\n+    def get_dummy_text(self, mm_counts: Mapping[str, int]) -> str:\n+        num_images = mm_counts.get(\"image\", 0)\n+\n+        processor = self.info.get_hf_processor()\n+        if \"gemma3\" in processor.__class__.__name__.lower():\n+            image_token = processor.boi_token\n+        else:\n+            image_token = getattr(processor, \"image_token\", \"\")\n+        return image_token * num_images\n+\n+    def get_dummy_mm_data(\n+        self,\n+        seq_len: int,\n+        mm_counts: Mapping[str, int],\n+    ) -> MultiModalDataDict:\n+        num_images = mm_counts.get(\"image\", 0)\n+\n+        target_width, target_height = self.info.get_max_image_size()\n+\n+        return {\n+            \"image\":\n+            self._get_dummy_images(width=target_width,\n+                                   height=target_height,\n+                                   num_images=num_images),\n+        }\n+\n+\n+class MultiModalProcessor(BaseMultiModalProcessor):\n+\n+    def _get_prompt_updates(\n+        self,\n+        mm_items,\n+        hf_processor_mm_kwargs,\n+        out_mm_kwargs,\n+    ):\n+        \"\"\"\n+        Given the original multi-modal items for this modality\n+        and HF-processed data, output the updates to perform.\n+\n+        The information returned by this method is used to update token inputs\n+        which bypass the HF processor. It is also used to update the output of\n+        HF processor if the HF process does not apply prompt updates to text\n+        inputs.\n+\n+        Moreover, this information is critical to determine the token positions\n+        in order to construct  :class:`~vllm-multimodal.input.PlaceholderRange`\n+        for each multi-modal item.\n+        \"\"\"\n+        return None\n+\n+    def _get_mm_fields_config(\n+        self,\n+        hf_inputs,\n+        hf_processor_mm_kwargs,\n+        num_image_patches: torch.Tensor = None,\n+    ):\n+        # HF Processors always return a mask but vLLM doesn't need it\n+        hf_inputs.pop(\"attention_mask\", None)\n+        mm_fields = {\n+            key: MultiModalFieldConfig.flat_from_sizes(\"image\",\n+                                                       num_image_patches)\n+            for key in hf_inputs\n+        }\n+        mm_fields[\"image_embeds\"] = MultiModalFieldConfig.flat_from_sizes(\n+            \"image\", num_image_patches)\n+        mm_fields[\"num_image_patches\"] = MultiModalFieldConfig.batched(\"image\")\n+        return mm_fields\n+\n+    def _apply_hf_processor_text_mm(\n+        self,\n+        prompt_text,\n+        mm_items,\n+        hf_processor_mm_kwargs,\n+    ):\n+        \"\"\"\n+        Apply the HF processor on the prompt text and multi-modal data\n+        together.\n+\n+        In addition, return whether prompt replacements have been applied.\n+        \"\"\"\n+        processor_data, passthrough_data = self._get_hf_mm_data(mm_items)\n+        processor_data[\"return_mm_token_type_ids\"] = True\n+\n+        processed_data = self._call_hf_processor(\n+            prompt=prompt_text,\n+            mm_data=processor_data,\n+            mm_kwargs=hf_processor_mm_kwargs,\n+        )\n+        processed_data.update(passthrough_data)\n+\n+        prompt_ids, = processed_data.pop(\"input_ids\").tolist()\n+        mm_token_type_ids = processed_data.pop(\n+            \"mm_token_type_ids\"\n+        ) if \"mm_token_type_ids\" in processed_data else processed_data.pop(\n+            \"token_type_ids\")  # for gemma3 only\n+\n+        return prompt_ids, processed_data, mm_token_type_ids\n+\n+    def apply(\n+        self,\n+        prompt,\n+        mm_data,\n+        hf_processor_mm_kwargs,\n+        return_mm_hashes=False,\n+    ) -> MultiModalInputs:\n+        \"\"\"\n+        Process multi-modal inputs to be used in vLLM.\n+\n+        Apply HF Processor on prompt text and multi-modal data together,\n+        outputting token IDs and processed tensors.\n+        \"\"\"\n+        if return_mm_hashes:\n+            raise ValueError(\n+                \"TransformersForMultimodalLM doesn't support mm hashing yet! \"\n+                \"Probably you didn't set `disable_mm_preprocessor_cache=True`\")\n+\n+        mm_items = self._to_mm_items(mm_data)\n+        hf_processor = self.info.get_hf_processor(**hf_processor_mm_kwargs)\n+\n+        (prompt_ids, processed_data,\n+         mm_token_type_ids) = self._apply_hf_processor_text_mm(\n+             prompt_text=prompt,\n+             mm_items=mm_items,\n+             hf_processor_mm_kwargs=hf_processor_mm_kwargs,\n+         )\n+\n+        # HF processor will return `mm_token_type_ids` from which\n+        # we can infer mm_placeholders. Until then hardcode to make code run\n+        # Below tested on Llava. Prompts and `mm_token_type_ids` are always bs=1\n+        mm_positions = torch.where(mm_token_type_ids == 1)[1]",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2188983443",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20543,
        "pr_file": "vllm/model_executor/models/transformers.py",
        "discussion_id": "2188983443",
        "commented_code": "@@ -112,6 +121,260 @@ def replace_linear_class(\n     )\n \n \n+# Copied from `accelerate`\n+@contextmanager\n+def init_on_device_without_buffers(device: torch.device):\n+    \"\"\"\n+    A context manager under which models are initialized with all\n+    parameters on the specified device. However buffers are not\n+    initialized on specified device.\n+\n+    Args:\n+        device (`torch.device`):\n+            Device to initialize all parameters on.\n+    \"\"\"\n+\n+    old_register_parameter = nn.Module.register_parameter\n+\n+    def register_empty_parameter(module, name, param):\n+        old_register_parameter(module, name, param)\n+        if param is not None:\n+            param_cls = type(module._parameters[name])\n+            kwargs = module._parameters[name].__dict__\n+            kwargs[\"requires_grad\"] = param.requires_grad\n+            module._parameters[name] = param_cls(\n+                module._parameters[name].to(device), **kwargs)\n+\n+    tensor_constructors_to_patch = {}\n+\n+    def patch_tensor_constructor(fn):\n+\n+        def wrapper(*args, **kwargs):\n+            kwargs[\"device\"] = device\n+            return fn(*args, **kwargs)\n+\n+        return wrapper\n+\n+    try:\n+        nn.Module.register_parameter = register_empty_parameter\n+        for torch_function_name in tensor_constructors_to_patch:\n+            setattr(\n+                torch, torch_function_name,\n+                patch_tensor_constructor(getattr(torch, torch_function_name)))\n+        yield\n+    finally:\n+        nn.Module.register_parameter = old_register_parameter\n+        for torch_function_name, old_torch_function in tensor_constructors_to_patch.items(\n+        ):\n+            setattr(torch, torch_function_name, old_torch_function)\n+\n+\n+class MultiModalProcessingInfo(BaseProcessingInfo):\n+\n+    def get_hf_config(self):\n+        return self.ctx.model_config.hf_config\n+\n+    def get_supported_mm_limits(self):\n+        return {\"image\": None}\n+\n+    def get_mm_max_tokens_per_item(self, seq_len, mm_counts):\n+        return {\"image\": self.get_max_image_tokens()}\n+\n+    def get_max_image_tokens(self) -> int:\n+        width, height = self.get_max_image_size()\n+        processor = self.get_hf_processor()\n+        mm_processor_kwargs = self.ctx.model_config.mm_processor_kwargs or {}\n+        mm_tokens = processor._get_num_multimodal_tokens(\n+            image_sizes=([height, width], ), **mm_processor_kwargs)\n+        image_tokens = mm_tokens[\"num_image_tokens\"][0]\n+        return image_tokens\n+\n+    def get_hf_processor(self):\n+        processor = cached_get_processor(self.ctx.model_config.model)\n+        return processor\n+\n+    def get_max_image_size(self):\n+        return 10_000, 10_000  # hardcode for arbitrary very large size\n+\n+\n+class MultiModalDummyInputsBuilder(BaseDummyInputsBuilder):\n+\n+    def get_dummy_text(self, mm_counts: Mapping[str, int]) -> str:\n+        num_images = mm_counts.get(\"image\", 0)\n+\n+        processor = self.info.get_hf_processor()\n+        if \"gemma3\" in processor.__class__.__name__.lower():\n+            image_token = processor.boi_token\n+        else:\n+            image_token = getattr(processor, \"image_token\", \"\")\n+        return image_token * num_images\n+\n+    def get_dummy_mm_data(\n+        self,\n+        seq_len: int,\n+        mm_counts: Mapping[str, int],\n+    ) -> MultiModalDataDict:\n+        num_images = mm_counts.get(\"image\", 0)\n+\n+        target_width, target_height = self.info.get_max_image_size()\n+\n+        return {\n+            \"image\":\n+            self._get_dummy_images(width=target_width,\n+                                   height=target_height,\n+                                   num_images=num_images),\n+        }\n+\n+\n+class MultiModalProcessor(BaseMultiModalProcessor):\n+\n+    def _get_prompt_updates(\n+        self,\n+        mm_items,\n+        hf_processor_mm_kwargs,\n+        out_mm_kwargs,\n+    ):\n+        \"\"\"\n+        Given the original multi-modal items for this modality\n+        and HF-processed data, output the updates to perform.\n+\n+        The information returned by this method is used to update token inputs\n+        which bypass the HF processor. It is also used to update the output of\n+        HF processor if the HF process does not apply prompt updates to text\n+        inputs.\n+\n+        Moreover, this information is critical to determine the token positions\n+        in order to construct  :class:`~vllm-multimodal.input.PlaceholderRange`\n+        for each multi-modal item.\n+        \"\"\"\n+        return None\n+\n+    def _get_mm_fields_config(\n+        self,\n+        hf_inputs,\n+        hf_processor_mm_kwargs,\n+        num_image_patches: torch.Tensor = None,\n+    ):\n+        # HF Processors always return a mask but vLLM doesn't need it\n+        hf_inputs.pop(\"attention_mask\", None)\n+        mm_fields = {\n+            key: MultiModalFieldConfig.flat_from_sizes(\"image\",\n+                                                       num_image_patches)\n+            for key in hf_inputs\n+        }\n+        mm_fields[\"image_embeds\"] = MultiModalFieldConfig.flat_from_sizes(\n+            \"image\", num_image_patches)\n+        mm_fields[\"num_image_patches\"] = MultiModalFieldConfig.batched(\"image\")\n+        return mm_fields\n+\n+    def _apply_hf_processor_text_mm(\n+        self,\n+        prompt_text,\n+        mm_items,\n+        hf_processor_mm_kwargs,\n+    ):\n+        \"\"\"\n+        Apply the HF processor on the prompt text and multi-modal data\n+        together.\n+\n+        In addition, return whether prompt replacements have been applied.\n+        \"\"\"\n+        processor_data, passthrough_data = self._get_hf_mm_data(mm_items)\n+        processor_data[\"return_mm_token_type_ids\"] = True\n+\n+        processed_data = self._call_hf_processor(\n+            prompt=prompt_text,\n+            mm_data=processor_data,\n+            mm_kwargs=hf_processor_mm_kwargs,\n+        )\n+        processed_data.update(passthrough_data)\n+\n+        prompt_ids, = processed_data.pop(\"input_ids\").tolist()\n+        mm_token_type_ids = processed_data.pop(\n+            \"mm_token_type_ids\"\n+        ) if \"mm_token_type_ids\" in processed_data else processed_data.pop(\n+            \"token_type_ids\")  # for gemma3 only\n+\n+        return prompt_ids, processed_data, mm_token_type_ids\n+\n+    def apply(\n+        self,\n+        prompt,\n+        mm_data,\n+        hf_processor_mm_kwargs,\n+        return_mm_hashes=False,\n+    ) -> MultiModalInputs:\n+        \"\"\"\n+        Process multi-modal inputs to be used in vLLM.\n+\n+        Apply HF Processor on prompt text and multi-modal data together,\n+        outputting token IDs and processed tensors.\n+        \"\"\"\n+        if return_mm_hashes:\n+            raise ValueError(\n+                \"TransformersForMultimodalLM doesn't support mm hashing yet! \"\n+                \"Probably you didn't set `disable_mm_preprocessor_cache=True`\")\n+\n+        mm_items = self._to_mm_items(mm_data)\n+        hf_processor = self.info.get_hf_processor(**hf_processor_mm_kwargs)\n+\n+        (prompt_ids, processed_data,\n+         mm_token_type_ids) = self._apply_hf_processor_text_mm(\n+             prompt_text=prompt,\n+             mm_items=mm_items,\n+             hf_processor_mm_kwargs=hf_processor_mm_kwargs,\n+         )\n+\n+        # HF processor will return `mm_token_type_ids` from which\n+        # we can infer mm_placeholders. Until then hardcode to make code run\n+        # Below tested on Llava. Prompts and `mm_token_type_ids` are always bs=1\n+        mm_positions = torch.where(mm_token_type_ids == 1)[1]",
        "comment_created_at": "2025-07-07T05:11:19+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe comment here indicates that the logic for inferring `mm_placeholders` is hardcoded and has only been tested on Llava. This approach is brittle and may not generalize to other vision-language models, potentially leading to incorrect behavior or silent failures when new models are used with this backend.\n\nFor long-term maintainability and correctness, it would be more robust to implement a more generic mechanism or a model-specific registration system for this kind of logic. While this might be a temporary solution, it introduces significant technical debt.",
        "pr_file_module": null
      }
    ]
  }
]