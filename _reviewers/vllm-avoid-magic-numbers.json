[
  {
    "discussion_id": "2185442532",
    "pr_number": 20466,
    "pr_file": "vllm/v1/attention/backends/flash_attn.py",
    "created_at": "2025-07-04T13:53:54+00:00",
    "commented_code": "self.aot_sliding_window: Optional[tuple[int, int]] = None\n \n     def build(\n-        self, common_prefix_len: int,\n-        common_attn_metadata: CommonAttentionMetadata\n+        self,\n+        common_prefix_len: int,\n+        common_attn_metadata: CommonAttentionMetadata,\n     ) -> FlashAttentionMetadata:\n         num_reqs = common_attn_metadata.num_reqs\n         num_actual_tokens = common_attn_metadata.num_actual_tokens\n         max_query_len = common_attn_metadata.max_query_len\n-\n-        max_seq_len = int(self.runner.seq_lens_np[:num_reqs].max())\n+        max_seq_len = int(common_attn_metadata.seq_lens_cpu.max())\n         query_start_loc = common_attn_metadata.query_start_loc\n+        query_start_loc_cpu = common_attn_metadata.query_start_loc_cpu\n         seq_lens = common_attn_metadata.seq_lens\n-        block_table = self.block_table\n-        block_table_tensor = block_table.get_device_tensor()[:num_reqs]\n-\n-        block_table.slot_mapping[:num_actual_tokens].copy_(\n-            block_table.slot_mapping_cpu[:num_actual_tokens],\n-            non_blocking=True)\n-        # Fill unused with -1. Needed for reshape_and_cache in full cuda graph\n-        # mode.\n-        block_table.slot_mapping[num_actual_tokens:].fill_(-1)\n-\n-        slot_mapping = block_table.slot_mapping[:num_actual_tokens]\n+        seq_lens_cpu = common_attn_metadata.seq_lens_cpu\n+        block_table_tensor = common_attn_metadata.block_table_tensor",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2185442532",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20466,
        "pr_file": "vllm/v1/attention/backends/flash_attn.py",
        "discussion_id": "2185442532",
        "commented_code": "@@ -197,27 +197,20 @@ def __init__(self, runner: \"GPUModelRunner\", kv_cache_spec: AttentionSpec,\n         self.aot_sliding_window: Optional[tuple[int, int]] = None\n \n     def build(\n-        self, common_prefix_len: int,\n-        common_attn_metadata: CommonAttentionMetadata\n+        self,\n+        common_prefix_len: int,\n+        common_attn_metadata: CommonAttentionMetadata,\n     ) -> FlashAttentionMetadata:\n         num_reqs = common_attn_metadata.num_reqs\n         num_actual_tokens = common_attn_metadata.num_actual_tokens\n         max_query_len = common_attn_metadata.max_query_len\n-\n-        max_seq_len = int(self.runner.seq_lens_np[:num_reqs].max())\n+        max_seq_len = int(common_attn_metadata.seq_lens_cpu.max())\n         query_start_loc = common_attn_metadata.query_start_loc\n+        query_start_loc_cpu = common_attn_metadata.query_start_loc_cpu\n         seq_lens = common_attn_metadata.seq_lens\n-        block_table = self.block_table\n-        block_table_tensor = block_table.get_device_tensor()[:num_reqs]\n-\n-        block_table.slot_mapping[:num_actual_tokens].copy_(\n-            block_table.slot_mapping_cpu[:num_actual_tokens],\n-            non_blocking=True)\n-        # Fill unused with -1. Needed for reshape_and_cache in full cuda graph\n-        # mode.\n-        block_table.slot_mapping[num_actual_tokens:].fill_(-1)\n-\n-        slot_mapping = block_table.slot_mapping[:num_actual_tokens]\n+        seq_lens_cpu = common_attn_metadata.seq_lens_cpu\n+        block_table_tensor = common_attn_metadata.block_table_tensor",
        "comment_created_at": "2025-07-04T13:53:54+00:00",
        "comment_author": "heheda12345",
        "comment_body": "I'm doubt about whether `block_table_tensor` and `slot_mapping` should be put into `common_attn_metadata`.\r\n\r\nFor models with sliding window + full attention, the block_table_tensor for the two types of layers are different as they need different number of slots. These two types of layers are put into different kv_cache_groups and thus have different attention backend and different `BlockTable` now.  ",
        "pr_file_module": null
      },
      {
        "comment_id": "2185525567",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20466,
        "pr_file": "vllm/v1/attention/backends/flash_attn.py",
        "discussion_id": "2185442532",
        "commented_code": "@@ -197,27 +197,20 @@ def __init__(self, runner: \"GPUModelRunner\", kv_cache_spec: AttentionSpec,\n         self.aot_sliding_window: Optional[tuple[int, int]] = None\n \n     def build(\n-        self, common_prefix_len: int,\n-        common_attn_metadata: CommonAttentionMetadata\n+        self,\n+        common_prefix_len: int,\n+        common_attn_metadata: CommonAttentionMetadata,\n     ) -> FlashAttentionMetadata:\n         num_reqs = common_attn_metadata.num_reqs\n         num_actual_tokens = common_attn_metadata.num_actual_tokens\n         max_query_len = common_attn_metadata.max_query_len\n-\n-        max_seq_len = int(self.runner.seq_lens_np[:num_reqs].max())\n+        max_seq_len = int(common_attn_metadata.seq_lens_cpu.max())\n         query_start_loc = common_attn_metadata.query_start_loc\n+        query_start_loc_cpu = common_attn_metadata.query_start_loc_cpu\n         seq_lens = common_attn_metadata.seq_lens\n-        block_table = self.block_table\n-        block_table_tensor = block_table.get_device_tensor()[:num_reqs]\n-\n-        block_table.slot_mapping[:num_actual_tokens].copy_(\n-            block_table.slot_mapping_cpu[:num_actual_tokens],\n-            non_blocking=True)\n-        # Fill unused with -1. Needed for reshape_and_cache in full cuda graph\n-        # mode.\n-        block_table.slot_mapping[num_actual_tokens:].fill_(-1)\n-\n-        slot_mapping = block_table.slot_mapping[:num_actual_tokens]\n+        seq_lens_cpu = common_attn_metadata.seq_lens_cpu\n+        block_table_tensor = common_attn_metadata.block_table_tensor",
        "comment_created_at": "2025-07-04T14:36:05+00:00",
        "comment_author": "LucasWilkinson",
        "comment_body": "Ya, this PR kinda redefines `CommonAttentionMetadata` from \"common inputs across KV-caches groups\" to a \"common interface for `AttentionMetadataBuilder.build` that we implement backend-agnostic attention schemes/features on\". But as a result there will be different `CommonAttentionMetadata` for each KV-cache group (this seems fine since they all reference the same underlying tensors anyways).\r\n\r\nBasically, the idea is that `AttentionMetadataBuilder.build` transforms `CommonAttentionMetadata` to a backend-specific metadata and `CommonAttentionMetadata` should be the minimal amount of things to do this.\r\n\r\nThe reason for adding `slot_mapping` and `block_table_tensor` to the `CommonAttentionMetadata` is two-fold:\r\n\r\n1) [This is the main motivation] There's some attention-related things that manipulate the `block_table_tensor` and/or `slot_mapping` in a way that is common across backends; e.g. local attention https://github.com/vllm-project/vllm/blob/5561681d0400469aba6abfee242be717db47ae0c/vllm/v1/attention/backends/utils.py#L199-L310 and attention slicing for micro-batching (dual-batch overlap) https://github.com/vllm-project/vllm/pull/18415. With this refactor, these could be pulled out of `AttentionMetadataBuilder` and instead operate on `CommonAttentionMetadata` before being passed into `AttentionMetadataBuilder.build`, making these features backend-independent.\r\n\r\n2) Break the dependency on model runner in the `AttentionMetadataBuilder`s. This makes unit-testing challenging as it's hard to know what parts need to be mocked.\r\n\r\nI was thinking about breaking `block_table_tensor` and `slot_mapping` into a separate data structure (adding a third input to `AttentionMetadataBuilder.build`) to preserve the semantics of `CommonAttentionMetadata` being \"common inputs across KV-caches groups,\" but I'm not convinced this is needed since all the `CommonAttentionMetadata` classes reference the same underlying tensors anyway.\r\n\r\nI could see arguments for passing `BlockTable` to `AttentionMetadataBuilder.build`; it's just that it makes things like local attention and microbatch slicing (mentioned in 1) more difficult since instead of just slicing/manipulating the tensors directly, they would have to construct a new sliced/manipulated `BlockTable` which would be messy.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2186467577",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20466,
        "pr_file": "vllm/v1/attention/backends/flash_attn.py",
        "discussion_id": "2185442532",
        "commented_code": "@@ -197,27 +197,20 @@ def __init__(self, runner: \"GPUModelRunner\", kv_cache_spec: AttentionSpec,\n         self.aot_sliding_window: Optional[tuple[int, int]] = None\n \n     def build(\n-        self, common_prefix_len: int,\n-        common_attn_metadata: CommonAttentionMetadata\n+        self,\n+        common_prefix_len: int,\n+        common_attn_metadata: CommonAttentionMetadata,\n     ) -> FlashAttentionMetadata:\n         num_reqs = common_attn_metadata.num_reqs\n         num_actual_tokens = common_attn_metadata.num_actual_tokens\n         max_query_len = common_attn_metadata.max_query_len\n-\n-        max_seq_len = int(self.runner.seq_lens_np[:num_reqs].max())\n+        max_seq_len = int(common_attn_metadata.seq_lens_cpu.max())\n         query_start_loc = common_attn_metadata.query_start_loc\n+        query_start_loc_cpu = common_attn_metadata.query_start_loc_cpu\n         seq_lens = common_attn_metadata.seq_lens\n-        block_table = self.block_table\n-        block_table_tensor = block_table.get_device_tensor()[:num_reqs]\n-\n-        block_table.slot_mapping[:num_actual_tokens].copy_(\n-            block_table.slot_mapping_cpu[:num_actual_tokens],\n-            non_blocking=True)\n-        # Fill unused with -1. Needed for reshape_and_cache in full cuda graph\n-        # mode.\n-        block_table.slot_mapping[num_actual_tokens:].fill_(-1)\n-\n-        slot_mapping = block_table.slot_mapping[:num_actual_tokens]\n+        seq_lens_cpu = common_attn_metadata.seq_lens_cpu\n+        block_table_tensor = common_attn_metadata.block_table_tensor",
        "comment_created_at": "2025-07-05T03:03:38+00:00",
        "comment_author": "heheda12345",
        "comment_body": "Thanks for the detailed reply. It make sense to me.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2109750247",
    "pr_number": 18218,
    "pr_file": "tests/kernels/mamba/test_causal_conv1d.py",
    "created_at": "2025-05-27T17:17:05+00:00",
    "commented_code": "causal_conv1d_opcheck_fn(x.squeeze(0), weight, bias, cumsum.cuda(),\n                              padded_state_indices, has_initial_states,\n                              final_states, activation)\n+\n+\n+@pytest.mark.parametrize(\"itype\",\n+                         [torch.float32, torch.float16, torch.bfloat16])\n+@pytest.mark.parametrize(\"silu_activation\", [False, True])\n+@pytest.mark.parametrize(\"has_bias\", [False, True])\n+@pytest.mark.parametrize(\"seqlen\", [1])\n+@pytest.mark.parametrize(\"width\", [2, 3, 4])\n+@pytest.mark.parametrize(\"dim\", [2048, 2048 + 16, 4096])\n+# tests correctness in case subset of the sequences are padded\n+@pytest.mark.parametrize(\"with_padding\", [True, False])\n+@pytest.mark.parametrize(\"batch_size\", [3])\n+def test_causal_conv1d_update_with_batch_gather_vllm(batch_size, with_padding,\n+                                                     dim, width, seqlen,\n+                                                     has_bias, silu_activation,\n+                                                     itype):\n+    device = \"cuda\"\n+    rtol, atol = (3e-4, 1e-3) if itype == torch.float32 else (3e-3, 5e-3)\n+    if itype == torch.bfloat16:\n+        rtol, atol = 1e-2, 5e-2\n+\n+    # set seed\n+    current_platform.seed_everything(0)\n+\n+    padding = 5 if with_padding else 0\n+    padded_batch_size = batch_size + padding\n+    # total_entries = number of cache line\n+    total_entries = 10 * batch_size\n+\n+    channel_last = True\n+    if not channel_last:\n+        x = torch.randn(padded_batch_size,\n+                        dim,\n+                        seqlen,\n+                        device=device,\n+                        dtype=itype)\n+    else:\n+        # x will be (batch, dim, seqlen) with contiguous along dim-axis\n+        x = torch.randn(padded_batch_size,\n+                        seqlen,\n+                        dim,\n+                        device=device,\n+                        dtype=itype).transpose(1, 2)\n+\n+    x_ref = x.clone()\n+\n+    conv_state_indices = torch.randperm(total_entries)[:batch_size].to(\n+        dtype=torch.int32, device=device)\n+    unused_states_bool = torch.ones(total_entries,\n+                                    dtype=torch.bool,\n+                                    device=device)\n+    unused_states_bool[conv_state_indices] = False\n+    padded_state_indices = torch.concat([\n+        conv_state_indices,\n+        torch.as_tensor(\n+            [PAD_SLOT_ID] * padding, dtype=torch.int32, device=device)\n+    ],\n+                                        dim=0)\n+\n+    if not channel_last:\n+        conv_state = torch.randn(total_entries,\n+                                 dim,\n+                                 width - 1,\n+                                 device=device,\n+                                 dtype=itype)\n+    else:\n+        # conv_state will be (cache_lines, dim, state_len)\n+        # with contiguous along dim-axis\n+        conv_state = torch.randn(total_entries,\n+                                 width - 1,\n+                                 dim,\n+                                 device=device,\n+                                 dtype=itype).transpose(1, 2)\n+\n+    conv_state_for_padding_test = conv_state.clone()\n+\n+    weight = torch.randn(dim, width, device=device, dtype=itype)\n+    bias = torch.randn(dim, device=device, dtype=itype) if has_bias else None\n+    conv_state_ref = conv_state[conv_state_indices, :].detach().clone()\n+    activation = None if not silu_activation else \"silu\"\n+\n+    out = causal_conv1d_update_triton(x,\n+                                      conv_state,\n+                                      weight,\n+                                      bias,\n+                                      activation=activation,\n+                                      conv_state_indices=padded_state_indices,\n+                                      pad_slot_id=PAD_SLOT_ID)\n+    out_ref = causal_conv1d_update_ref(x_ref[:batch_size],\n+                                       conv_state_ref,\n+                                       weight,\n+                                       bias,\n+                                       activation=activation)\n+\n+    assert torch.equal(conv_state[conv_state_indices, :], conv_state_ref)\n+    assert torch.equal(conv_state[unused_states_bool],\n+                       conv_state_for_padding_test[unused_states_bool])\n+    assert torch.allclose(out[:batch_size], out_ref, rtol=rtol, atol=atol)\n+\n+\n+@pytest.mark.parametrize(\"itype\", [torch.bfloat16])\n+@pytest.mark.parametrize(\"silu_activation\", [True])\n+@pytest.mark.parametrize(\"has_bias\", [True])\n+@pytest.mark.parametrize(\"width\", [4])\n+@pytest.mark.parametrize('seqlen', [8, 16, 784, 1024, 2048, 2049, 4096])\n+@pytest.mark.parametrize('dim', [64, 4096])\n+@pytest.mark.parametrize('with_padding', [True, False])\n+@pytest.mark.parametrize('batch', [4])\n+def test_causal_conv1d_varlen_vllm(batch, with_padding, dim, seqlen, width,\n+                                   has_bias, silu_activation, itype):\n+    device = \"cuda\"\n+    torch.cuda.empty_cache()\n+    rtol, atol = (3e-4, 1e-3) if itype == torch.float32 else (3e-3, 5e-3)\n+    if itype == torch.bfloat16:\n+        rtol, atol = 1e-2, 5e-2\n+    # set seed\n+    current_platform.seed_everything(0)\n+    seqlens = []\n+    batch_size = batch\n+    padding = 3 if with_padding else 0\n+    padded_batch_size = batch_size + padding\n+    nsplits = padded_batch_size - 1\n+\n+    eos_pos = torch.randperm(seqlen - 1)[:nsplits].sort().values\n+\n+    seqlens.append(\n+        torch.diff(\n+            torch.cat(\n+                [torch.tensor([-1]), eos_pos,\n+                 torch.tensor([seqlen - 1])])).tolist())\n+    assert sum(seqlens[-1]) == seqlen\n+    assert all(s > 0 for s in seqlens[-1])\n+\n+    total_entries = batch_size * 10\n+    cumsum = torch.cumsum(torch.tensor(seqlens[0]), dim=0).to(torch.int32)\n+    cumsum = torch.concat([torch.tensor([0], dtype=torch.int32), cumsum],\n+                          dim=0)\n+    channel_last = True\n+    if not channel_last:\n+        x = torch.randn(1, 4096 + dim + 64, seqlen, device=device,\n+                        dtype=itype)[:, 4096:4096 + dim, :]\n+    else:\n+        x = rearrange(\n+            torch.randn(1, seqlen, 4096 + dim + 64, device=device,\n+                        dtype=itype), \"b s d -> b d s\")[:, 4096:4096 + dim, :]\n+\n+    weight = torch.randn(dim, width, device=device, dtype=itype)\n+\n+    bias = torch.randn(dim, device=device, dtype=itype) if has_bias else None\n+    x_ref = x.clone()\n+    weight_ref = weight.clone()\n+    bias_ref = bias.clone() if bias is not None else None\n+    activation = None if not silu_activation else \"silu\"\n+    if not channel_last:\n+        final_states = torch.randn(total_entries,\n+                                   dim,\n+                                   width - 1,\n+                                   device=x.device,\n+                                   dtype=x.dtype)\n+    else:\n+        final_states = torch.randn(total_entries,\n+                                   width - 1,\n+                                   dim,\n+                                   device=x.device,\n+                                   dtype=x.dtype).transpose(1, 2)\n+    final_states_ref = final_states.clone()\n+    has_initial_states = torch.randint(0,\n+                                       2, (cumsum.shape[0] - 1, ),\n+                                       dtype=torch.bool,\n+                                       device=x.device)\n+    state_indices = torch.randperm(total_entries,\n+                                   dtype=torch.int32,\n+                                   device=x.device)[:batch_size]\n+    padded_state_indices = torch.concat([\n+        state_indices,\n+        torch.as_tensor(\n+            [PAD_SLOT_ID] * padding, dtype=torch.int32, device=device),\n+    ],\n+                                        dim=-1)\n+    out = causal_conv1d_fn_triton(x.squeeze(0),\n+                                  weight,\n+                                  bias=bias,\n+                                  conv_states=final_states,\n+                                  query_start_loc=cumsum.cuda(),\n+                                  cache_indices=padded_state_indices,\n+                                  has_initial_states=has_initial_states,\n+                                  activation=activation,\n+                                  pad_slot_id=PAD_SLOT_ID)\n+\n+    out_ref = []\n+    out_ref_b = []\n+\n+    splits = [torch.split(var, seqlens[0], dim=-1) for var in (x_ref)]\n+    for i in range(len(seqlens[0])):\n+        x_s = [v[i].unsqueeze(0) for v in splits][0]\n+        if padded_state_indices[i] == PAD_SLOT_ID:\n+            continue\n+        out_ref_b.append(\n+            causal_conv1d_ref(\n+                x_s,\n+                weight_ref,\n+                bias_ref,\n+                activation=activation,\n+                return_final_states=True,\n+                final_states_out=final_states_ref[\n+                    padded_state_indices[i]].unsqueeze(0),\n+                initial_states=final_states_ref[padded_state_indices[i]].\n+                unsqueeze(0) if has_initial_states[i] else None))\n+    out_ref.append(torch.cat([t[0] for t in out_ref_b], dim=2))\n+    out_ref_tensor = torch.cat(out_ref, dim=0)\n+\n+    try:\n+        assert torch.allclose(final_states[state_indices],\n+                              final_states_ref[state_indices],\n+                              rtol=rtol,\n+                              atol=atol)\n+        print(\"Passed conv_state\")\n+    except Exception as e:\n+        print(\"FAILED conv_state\")\n+        raise e\n+    unpadded_out = out[:, :out_ref_tensor.shape[-1]]\n+    try:\n+        assert torch.allclose(unpadded_out,\n+                              out_ref_tensor,\n+                              rtol=rtol,\n+                              atol=atol)\n+    except Exception as e:\n+        input(\n+            \"Passed conv_state, but failed output: Press Enter to continue...\")",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2109750247",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 18218,
        "pr_file": "tests/kernels/mamba/test_causal_conv1d.py",
        "discussion_id": "2109750247",
        "commented_code": "@@ -435,3 +437,237 @@ def test_causal_conv1d_varlen(with_padding, dim, seqlen, width, has_bias,\n     causal_conv1d_opcheck_fn(x.squeeze(0), weight, bias, cumsum.cuda(),\n                              padded_state_indices, has_initial_states,\n                              final_states, activation)\n+\n+\n+@pytest.mark.parametrize(\"itype\",\n+                         [torch.float32, torch.float16, torch.bfloat16])\n+@pytest.mark.parametrize(\"silu_activation\", [False, True])\n+@pytest.mark.parametrize(\"has_bias\", [False, True])\n+@pytest.mark.parametrize(\"seqlen\", [1])\n+@pytest.mark.parametrize(\"width\", [2, 3, 4])\n+@pytest.mark.parametrize(\"dim\", [2048, 2048 + 16, 4096])\n+# tests correctness in case subset of the sequences are padded\n+@pytest.mark.parametrize(\"with_padding\", [True, False])\n+@pytest.mark.parametrize(\"batch_size\", [3])\n+def test_causal_conv1d_update_with_batch_gather_vllm(batch_size, with_padding,\n+                                                     dim, width, seqlen,\n+                                                     has_bias, silu_activation,\n+                                                     itype):\n+    device = \"cuda\"\n+    rtol, atol = (3e-4, 1e-3) if itype == torch.float32 else (3e-3, 5e-3)\n+    if itype == torch.bfloat16:\n+        rtol, atol = 1e-2, 5e-2\n+\n+    # set seed\n+    current_platform.seed_everything(0)\n+\n+    padding = 5 if with_padding else 0\n+    padded_batch_size = batch_size + padding\n+    # total_entries = number of cache line\n+    total_entries = 10 * batch_size\n+\n+    channel_last = True\n+    if not channel_last:\n+        x = torch.randn(padded_batch_size,\n+                        dim,\n+                        seqlen,\n+                        device=device,\n+                        dtype=itype)\n+    else:\n+        # x will be (batch, dim, seqlen) with contiguous along dim-axis\n+        x = torch.randn(padded_batch_size,\n+                        seqlen,\n+                        dim,\n+                        device=device,\n+                        dtype=itype).transpose(1, 2)\n+\n+    x_ref = x.clone()\n+\n+    conv_state_indices = torch.randperm(total_entries)[:batch_size].to(\n+        dtype=torch.int32, device=device)\n+    unused_states_bool = torch.ones(total_entries,\n+                                    dtype=torch.bool,\n+                                    device=device)\n+    unused_states_bool[conv_state_indices] = False\n+    padded_state_indices = torch.concat([\n+        conv_state_indices,\n+        torch.as_tensor(\n+            [PAD_SLOT_ID] * padding, dtype=torch.int32, device=device)\n+    ],\n+                                        dim=0)\n+\n+    if not channel_last:\n+        conv_state = torch.randn(total_entries,\n+                                 dim,\n+                                 width - 1,\n+                                 device=device,\n+                                 dtype=itype)\n+    else:\n+        # conv_state will be (cache_lines, dim, state_len)\n+        # with contiguous along dim-axis\n+        conv_state = torch.randn(total_entries,\n+                                 width - 1,\n+                                 dim,\n+                                 device=device,\n+                                 dtype=itype).transpose(1, 2)\n+\n+    conv_state_for_padding_test = conv_state.clone()\n+\n+    weight = torch.randn(dim, width, device=device, dtype=itype)\n+    bias = torch.randn(dim, device=device, dtype=itype) if has_bias else None\n+    conv_state_ref = conv_state[conv_state_indices, :].detach().clone()\n+    activation = None if not silu_activation else \"silu\"\n+\n+    out = causal_conv1d_update_triton(x,\n+                                      conv_state,\n+                                      weight,\n+                                      bias,\n+                                      activation=activation,\n+                                      conv_state_indices=padded_state_indices,\n+                                      pad_slot_id=PAD_SLOT_ID)\n+    out_ref = causal_conv1d_update_ref(x_ref[:batch_size],\n+                                       conv_state_ref,\n+                                       weight,\n+                                       bias,\n+                                       activation=activation)\n+\n+    assert torch.equal(conv_state[conv_state_indices, :], conv_state_ref)\n+    assert torch.equal(conv_state[unused_states_bool],\n+                       conv_state_for_padding_test[unused_states_bool])\n+    assert torch.allclose(out[:batch_size], out_ref, rtol=rtol, atol=atol)\n+\n+\n+@pytest.mark.parametrize(\"itype\", [torch.bfloat16])\n+@pytest.mark.parametrize(\"silu_activation\", [True])\n+@pytest.mark.parametrize(\"has_bias\", [True])\n+@pytest.mark.parametrize(\"width\", [4])\n+@pytest.mark.parametrize('seqlen', [8, 16, 784, 1024, 2048, 2049, 4096])\n+@pytest.mark.parametrize('dim', [64, 4096])\n+@pytest.mark.parametrize('with_padding', [True, False])\n+@pytest.mark.parametrize('batch', [4])\n+def test_causal_conv1d_varlen_vllm(batch, with_padding, dim, seqlen, width,\n+                                   has_bias, silu_activation, itype):\n+    device = \"cuda\"\n+    torch.cuda.empty_cache()\n+    rtol, atol = (3e-4, 1e-3) if itype == torch.float32 else (3e-3, 5e-3)\n+    if itype == torch.bfloat16:\n+        rtol, atol = 1e-2, 5e-2\n+    # set seed\n+    current_platform.seed_everything(0)\n+    seqlens = []\n+    batch_size = batch\n+    padding = 3 if with_padding else 0\n+    padded_batch_size = batch_size + padding\n+    nsplits = padded_batch_size - 1\n+\n+    eos_pos = torch.randperm(seqlen - 1)[:nsplits].sort().values\n+\n+    seqlens.append(\n+        torch.diff(\n+            torch.cat(\n+                [torch.tensor([-1]), eos_pos,\n+                 torch.tensor([seqlen - 1])])).tolist())\n+    assert sum(seqlens[-1]) == seqlen\n+    assert all(s > 0 for s in seqlens[-1])\n+\n+    total_entries = batch_size * 10\n+    cumsum = torch.cumsum(torch.tensor(seqlens[0]), dim=0).to(torch.int32)\n+    cumsum = torch.concat([torch.tensor([0], dtype=torch.int32), cumsum],\n+                          dim=0)\n+    channel_last = True\n+    if not channel_last:\n+        x = torch.randn(1, 4096 + dim + 64, seqlen, device=device,\n+                        dtype=itype)[:, 4096:4096 + dim, :]\n+    else:\n+        x = rearrange(\n+            torch.randn(1, seqlen, 4096 + dim + 64, device=device,\n+                        dtype=itype), \"b s d -> b d s\")[:, 4096:4096 + dim, :]\n+\n+    weight = torch.randn(dim, width, device=device, dtype=itype)\n+\n+    bias = torch.randn(dim, device=device, dtype=itype) if has_bias else None\n+    x_ref = x.clone()\n+    weight_ref = weight.clone()\n+    bias_ref = bias.clone() if bias is not None else None\n+    activation = None if not silu_activation else \"silu\"\n+    if not channel_last:\n+        final_states = torch.randn(total_entries,\n+                                   dim,\n+                                   width - 1,\n+                                   device=x.device,\n+                                   dtype=x.dtype)\n+    else:\n+        final_states = torch.randn(total_entries,\n+                                   width - 1,\n+                                   dim,\n+                                   device=x.device,\n+                                   dtype=x.dtype).transpose(1, 2)\n+    final_states_ref = final_states.clone()\n+    has_initial_states = torch.randint(0,\n+                                       2, (cumsum.shape[0] - 1, ),\n+                                       dtype=torch.bool,\n+                                       device=x.device)\n+    state_indices = torch.randperm(total_entries,\n+                                   dtype=torch.int32,\n+                                   device=x.device)[:batch_size]\n+    padded_state_indices = torch.concat([\n+        state_indices,\n+        torch.as_tensor(\n+            [PAD_SLOT_ID] * padding, dtype=torch.int32, device=device),\n+    ],\n+                                        dim=-1)\n+    out = causal_conv1d_fn_triton(x.squeeze(0),\n+                                  weight,\n+                                  bias=bias,\n+                                  conv_states=final_states,\n+                                  query_start_loc=cumsum.cuda(),\n+                                  cache_indices=padded_state_indices,\n+                                  has_initial_states=has_initial_states,\n+                                  activation=activation,\n+                                  pad_slot_id=PAD_SLOT_ID)\n+\n+    out_ref = []\n+    out_ref_b = []\n+\n+    splits = [torch.split(var, seqlens[0], dim=-1) for var in (x_ref)]\n+    for i in range(len(seqlens[0])):\n+        x_s = [v[i].unsqueeze(0) for v in splits][0]\n+        if padded_state_indices[i] == PAD_SLOT_ID:\n+            continue\n+        out_ref_b.append(\n+            causal_conv1d_ref(\n+                x_s,\n+                weight_ref,\n+                bias_ref,\n+                activation=activation,\n+                return_final_states=True,\n+                final_states_out=final_states_ref[\n+                    padded_state_indices[i]].unsqueeze(0),\n+                initial_states=final_states_ref[padded_state_indices[i]].\n+                unsqueeze(0) if has_initial_states[i] else None))\n+    out_ref.append(torch.cat([t[0] for t in out_ref_b], dim=2))\n+    out_ref_tensor = torch.cat(out_ref, dim=0)\n+\n+    try:\n+        assert torch.allclose(final_states[state_indices],\n+                              final_states_ref[state_indices],\n+                              rtol=rtol,\n+                              atol=atol)\n+        print(\"Passed conv_state\")\n+    except Exception as e:\n+        print(\"FAILED conv_state\")\n+        raise e\n+    unpadded_out = out[:, :out_ref_tensor.shape[-1]]\n+    try:\n+        assert torch.allclose(unpadded_out,\n+                              out_ref_tensor,\n+                              rtol=rtol,\n+                              atol=atol)\n+    except Exception as e:\n+        input(\n+            \"Passed conv_state, but failed output: Press Enter to continue...\")",
        "comment_created_at": "2025-05-27T17:17:05+00:00",
        "comment_author": "tlrmchlsmth",
        "comment_body": "I can see that doing this makes sense while debugging, but we should make this much simpler and noninteractive before landing.\r\n\r\n```suggestion\r\n    assert torch.allclose(final_states[state_indices],\r\n                          final_states_ref[state_indices],\r\n                          rtol=rtol,\r\n                          atol=atol)\r\n    unpadded_out = out[:, :out_ref_tensor.shape[-1]]\r\n    assert torch.allclose(unpadded_out,\r\n                          out_ref_tensor,\r\n                          rtol=rtol,\r\n                          atol=atol)\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2188464478",
    "pr_number": 20527,
    "pr_file": "vllm/transformers_utils/config.py",
    "created_at": "2025-07-06T16:47:31+00:00",
    "commented_code": "return None\n \n \n-def get_cross_encoder_activation_function(config: PretrainedConfig):\n+def get_classification_activation_function(config: PretrainedConfig):\n+    return nn.Sigmoid() if config.num_labels == 1 else nn.Identity()",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2188467099",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20527,
        "pr_file": "vllm/transformers_utils/config.py",
        "discussion_id": "2188464478",
        "commented_code": "@@ -866,24 +866,26 @@ def try_get_generation_config(\n             return None\n \n \n-def get_cross_encoder_activation_function(config: PretrainedConfig):\n+def get_classification_activation_function(config: PretrainedConfig):\n+    return nn.Sigmoid() if config.num_labels == 1 else nn.Identity()",
        "comment_created_at": "2025-07-06T16:50:41+00:00",
        "comment_author": "DarkLight1337",
        "comment_body": "Nice catch!",
        "pr_file_module": null
      }
    ]
  }
]