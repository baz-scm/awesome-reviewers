[
  {
    "discussion_id": "2191605845",
    "pr_number": 20610,
    "pr_file": "vllm/v1/engine/async_llm.py",
    "created_at": "2025-07-08T06:37:06+00:00",
    "commented_code": "raise\n \n         # Request validation error.\n-        except ValueError:\n+        except ValueError as e:\n             if self.log_requests:\n-                logger.info(\"Request %s failed (bad request).\", request_id)\n+                logger.info(\"Request %s failed (bad request): %s.\", request_id, e)\n             raise\n \n         # Unexpected error in the generate() task (possibly recoverable).\n         except Exception as e:\n             await self.abort(request_id)\n             if self.log_requests:\n-                logger.info(\"Request %s failed.\", request_id)\n+                try:\n+                    s = str(e)\n+                except:\n+                    s = e.__class__.__name__ + \" (unprintable)\"",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2191605845",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20610,
        "pr_file": "vllm/v1/engine/async_llm.py",
        "discussion_id": "2191605845",
        "commented_code": "@@ -346,16 +346,20 @@ async def generate(\n             raise\n \n         # Request validation error.\n-        except ValueError:\n+        except ValueError as e:\n             if self.log_requests:\n-                logger.info(\"Request %s failed (bad request).\", request_id)\n+                logger.info(\"Request %s failed (bad request): %s.\", request_id, e)\n             raise\n \n         # Unexpected error in the generate() task (possibly recoverable).\n         except Exception as e:\n             await self.abort(request_id)\n             if self.log_requests:\n-                logger.info(\"Request %s failed.\", request_id)\n+                try:\n+                    s = str(e)\n+                except:\n+                    s = e.__class__.__name__ + \" (unprintable)\"",
        "comment_created_at": "2025-07-08T06:37:06+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis is a great improvement for error logging!\n\nOne suggestion: it's better to avoid bare `except:` clauses. Per PEP 8, you should always specify what exception(s) to catch. A bare `except` can catch unexpected exceptions like `SystemExit` or `KeyboardInterrupt`, which can hide bugs or prevent graceful shutdown.\n\nYou can make this more robust by specifying `except Exception:`. This will still catch any standard exception that might occur when converting the error to a string, but won't swallow system-level exceptions.\n\nI've also taken the liberty to suggest using an f-string for slightly improved readability.\n\n```suggestion\n                try:\n                    s = str(e)\n                except Exception:\n                    s = f\"{e.__class__.__name__} (unprintable)\"\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2177977532",
    "pr_number": 20321,
    "pr_file": "vllm/attention/layer.py",
    "created_at": "2025-07-01T15:56:26+00:00",
    "commented_code": ").parallel_config.pipeline_parallel_size)\n         ]\n \n-        self.q_range = torch.tensor(envs.Q_SCALE_CONSTANT, dtype=torch.float32)\n-        self.k_range = torch.tensor(envs.K_SCALE_CONSTANT, dtype=torch.float32)\n-        self.v_range = torch.tensor(envs.V_SCALE_CONSTANT, dtype=torch.float32)\n+        try:\n+            self.q_range = torch.tensor(envs.Q_SCALE_CONSTANT, dtype=torch.float32)\n+            self.k_range = torch.tensor(envs.K_SCALE_CONSTANT, dtype=torch.float32)\n+            self.v_range = torch.tensor(envs.V_SCALE_CONSTANT, dtype=torch.float32)\n+        except Exception as e:\n+            if torch.cuda.is_available():\n+                # This helps to see how much memory is allocated when using CUDA\n+                logger.error(f\"Failed to initialize attention q/k/v range constants: {e}\")\n+                logger.debug(f\"CUDA device: {torch.cuda.current_device()}\")\n+                logger.debug(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n+                logger.debug(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n+            raise RuntimeError(\n+                \"Failed to initialize q/k/v range constants. \"\n+                \"This may be caused by insufficient memory to allocate kv cache.\"\n+            ) from e",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2177977532",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20321,
        "pr_file": "vllm/attention/layer.py",
        "discussion_id": "2177977532",
        "commented_code": "@@ -179,9 +182,21 @@ def __init__(\n             ).parallel_config.pipeline_parallel_size)\n         ]\n \n-        self.q_range = torch.tensor(envs.Q_SCALE_CONSTANT, dtype=torch.float32)\n-        self.k_range = torch.tensor(envs.K_SCALE_CONSTANT, dtype=torch.float32)\n-        self.v_range = torch.tensor(envs.V_SCALE_CONSTANT, dtype=torch.float32)\n+        try:\n+            self.q_range = torch.tensor(envs.Q_SCALE_CONSTANT, dtype=torch.float32)\n+            self.k_range = torch.tensor(envs.K_SCALE_CONSTANT, dtype=torch.float32)\n+            self.v_range = torch.tensor(envs.V_SCALE_CONSTANT, dtype=torch.float32)\n+        except Exception as e:\n+            if torch.cuda.is_available():\n+                # This helps to see how much memory is allocated when using CUDA\n+                logger.error(f\"Failed to initialize attention q/k/v range constants: {e}\")\n+                logger.debug(f\"CUDA device: {torch.cuda.current_device()}\")\n+                logger.debug(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n+                logger.debug(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n+            raise RuntimeError(\n+                \"Failed to initialize q/k/v range constants. \"\n+                \"This may be caused by insufficient memory to allocate kv cache.\"\n+            ) from e",
        "comment_created_at": "2025-07-01T15:56:26+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nCatching a generic `Exception` can hide other programming errors that are not related to memory. It is more precise to catch `RuntimeError`, which `torch.tensor` can raise for out-of-memory conditions, or `torch.cuda.OutOfMemoryError` if specifically targeting CUDA OOM.\n\n```suggestion\n        try:\n            self.q_range = torch.tensor(envs.Q_SCALE_CONSTANT, dtype=torch.float32)\n            self.k_range = torch.tensor(envs.K_SCALE_CONSTANT, dtype=torch.float32)\n            self.v_range = torch.tensor(envs.V_SCALE_CONSTANT, dtype=torch.float32)\n        except RuntimeError as e:\n            if torch.cuda.is_available():\n                # This helps to see how much memory is allocated when using CUDA\n                logger.error(f\"Failed to initialize attention q/k/v range constants: {e}\")\n                logger.debug(f\"CUDA device: {torch.cuda.current_device()}\")\n                logger.debug(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n                logger.debug(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n            raise RuntimeError(\n                \"Failed to initialize q/k/v range constants. \"\n                \"This may be caused by insufficient memory to allocate kv cache.\"\n            ) from e\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2177977540",
    "pr_number": 20321,
    "pr_file": "vllm/model_executor/layers/linear.py",
    "created_at": "2025-07-01T15:56:26+00:00",
    "commented_code": "output_partition_sizes: list[int], input_size: int,\n                        output_size: int, params_dtype: torch.dtype,\n                        **extra_weight_attrs):\n-        weight = Parameter(torch.empty(sum(output_partition_sizes),\n-                                       input_size_per_partition,\n-                                       dtype=params_dtype),\n-                           requires_grad=False)\n+        # This method creates unquantized linear weights.\n+        # The weights are not quantized, and they are not sharded.\n+        # The amount of memory allocated for the weights is\n+        # sum(output_partition_sizes) * input_size_per_partition.\n+        try:\n+            weight = Parameter(torch.empty(sum(output_partition_sizes),\n+                                        input_size_per_partition,\n+                                        dtype=params_dtype),\n+                            requires_grad=False)\n+        except RuntimeError as e:\n+            if torch.cuda.is_available():\n+                # This helps to see how much memory is allocated when using CUDA\n+                logger.error(f\"Failed to create unquantized linear weights: {e}\")\n+                logger.debug(f\"CUDA device: {torch.cuda.current_device()}\")\n+                logger.debug(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n+                logger.debug(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n+            raise RuntimeError(\n+                \"Failed to create unquantized linear weights. \"\n+                \"This may be caused by insufficient memory to allocate the weight.\"\n+            ) from e",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2177977540",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20321,
        "pr_file": "vllm/model_executor/layers/linear.py",
        "discussion_id": "2177977540",
        "commented_code": "@@ -189,10 +189,26 @@ def create_weights(self, layer: torch.nn.Module,\n                        output_partition_sizes: list[int], input_size: int,\n                        output_size: int, params_dtype: torch.dtype,\n                        **extra_weight_attrs):\n-        weight = Parameter(torch.empty(sum(output_partition_sizes),\n-                                       input_size_per_partition,\n-                                       dtype=params_dtype),\n-                           requires_grad=False)\n+        # This method creates unquantized linear weights.\n+        # The weights are not quantized, and they are not sharded.\n+        # The amount of memory allocated for the weights is\n+        # sum(output_partition_sizes) * input_size_per_partition.\n+        try:\n+            weight = Parameter(torch.empty(sum(output_partition_sizes),\n+                                        input_size_per_partition,\n+                                        dtype=params_dtype),\n+                            requires_grad=False)\n+        except RuntimeError as e:\n+            if torch.cuda.is_available():\n+                # This helps to see how much memory is allocated when using CUDA\n+                logger.error(f\"Failed to create unquantized linear weights: {e}\")\n+                logger.debug(f\"CUDA device: {torch.cuda.current_device()}\")\n+                logger.debug(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n+                logger.debug(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n+            raise RuntimeError(\n+                \"Failed to create unquantized linear weights. \"\n+                \"This may be caused by insufficient memory to allocate the weight.\"\n+            ) from e",
        "comment_created_at": "2025-07-01T15:56:26+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nCatching a generic `RuntimeError` can hide other programming errors that are not related to memory. It is more precise to catch `torch.cuda.OutOfMemoryError` if specifically targeting CUDA OOM.\n\n```suggestion\n        try:\n            weight = Parameter(torch.empty(sum(output_partition_sizes),\n                                        input_size_per_partition,\n                                        dtype=params_dtype),\n                            requires_grad=False)\n        except torch.cuda.OutOfMemoryError as e:\n            if torch.cuda.is_available():\n                # This helps to see how much memory is allocated when using CUDA\n                logger.error(f\"Failed to create unquantized linear weights: {e}\")\n                logger.debug(f\"CUDA device: {torch.cuda.current_device()}\")\n                logger.debug(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n                logger.debug(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n            raise RuntimeError(\n                \"Failed to create unquantized linear weights. \"\n                \"This may be caused by insufficient memory to allocate the weight.\"\n            ) from e\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2172376901",
    "pr_number": 20190,
    "pr_file": "examples/offline_inference/eagle.py",
    "created_at": "2025-06-27T16:21:07+00:00",
    "commented_code": "\"max_model_len\": max_model_len,\n         },\n         disable_log_stats=False,\n+        compilation_config=(\n+            json.loads(args.compilation_config) if args.compilation_config else None\n+        ),",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2172376901",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20190,
        "pr_file": "examples/offline_inference/eagle.py",
        "discussion_id": "2172376901",
        "commented_code": "@@ -94,6 +95,9 @@ def main():\n             \"max_model_len\": max_model_len,\n         },\n         disable_log_stats=False,\n+        compilation_config=(\n+            json.loads(args.compilation_config) if args.compilation_config else None\n+        ),",
        "comment_created_at": "2025-06-27T16:21:07+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe direct call to `json.loads` can cause the script to crash with a `json.JSONDecodeError` if an invalid JSON string is passed to the `--compilation_config` argument. Consider adding a try-except block to handle potential parsing errors gracefully.\n\n```python\ncompilation_config = None\nif args.compilation_config:\n    try:\n        compilation_config = json.loads(args.compilation_config)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid JSON for --compilation_config: {e}\") from e\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2144688617",
    "pr_number": 19608,
    "pr_file": "vllm/entrypoints/openai/api_server.py",
    "created_at": "2025-06-13T10:05:09+00:00",
    "commented_code": "return None\n \n \n+def _clean_content_for_logging(content: str) -> str:\n+    \"\"\"Clean content for safe logging by escaping special characters.\"\"\"\n+    if not content:\n+        return content\n+\n+    # Replace common special characters that can break log formatting\n+    clean_content = (\n+        content.replace('\n', '\\\n')  # Newlines\n+        .replace('\\r', '\\\\r')  # Carriage returns\n+        .replace('\\t', '\\\\t')  # Tabs\n+        .replace('\\b', '\\\\b')  # Backspaces\n+        .replace('\\f', '\\\\f')  # Form feeds\n+        .replace('\\v', '\\\\v')  # Vertical tabs\n+        .replace('\\0', '\\\\0')  # Null characters\n+        .replace('\"', '\\\\\"')  # Double quotes\n+        .replace('\\\\', '\\\\\\\\'))  # Backslashes (do this last)\n+\n+    return clean_content\n+\n+\n+def _extract_content_from_chunk(chunk_data: dict) -> str:\n+    \"\"\"Extract content from a streaming response chunk.\"\"\"\n+    try:\n+        from vllm.entrypoints.openai.protocol import (\n+            ChatCompletionStreamResponse, CompletionStreamResponse)\n+\n+        # Try using Completion types for type-safe parsing\n+        if chunk_data.get('object') == 'chat.completion.chunk':\n+            response = ChatCompletionStreamResponse.model_validate(chunk_data)\n+            if response.choices and response.choices[0].delta.content:\n+                return response.choices[0].delta.content\n+        elif chunk_data.get('object') == 'text_completion':\n+            response = CompletionStreamResponse.model_validate(chunk_data)\n+            if response.choices and response.choices[0].text:\n+                return response.choices[0].text\n+    except Exception:",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2144688617",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19608,
        "pr_file": "vllm/entrypoints/openai/api_server.py",
        "discussion_id": "2144688617",
        "commented_code": "@@ -1014,6 +1014,95 @@ def load_log_config(log_config_file: Optional[str]) -> Optional[dict]:\n         return None\n \n \n+def _clean_content_for_logging(content: str) -> str:\n+    \"\"\"Clean content for safe logging by escaping special characters.\"\"\"\n+    if not content:\n+        return content\n+\n+    # Replace common special characters that can break log formatting\n+    clean_content = (\n+        content.replace('\\n', '\\\\n')  # Newlines\n+        .replace('\\r', '\\\\r')  # Carriage returns\n+        .replace('\\t', '\\\\t')  # Tabs\n+        .replace('\\b', '\\\\b')  # Backspaces\n+        .replace('\\f', '\\\\f')  # Form feeds\n+        .replace('\\v', '\\\\v')  # Vertical tabs\n+        .replace('\\0', '\\\\0')  # Null characters\n+        .replace('\"', '\\\\\"')  # Double quotes\n+        .replace('\\\\', '\\\\\\\\'))  # Backslashes (do this last)\n+\n+    return clean_content\n+\n+\n+def _extract_content_from_chunk(chunk_data: dict) -> str:\n+    \"\"\"Extract content from a streaming response chunk.\"\"\"\n+    try:\n+        from vllm.entrypoints.openai.protocol import (\n+            ChatCompletionStreamResponse, CompletionStreamResponse)\n+\n+        # Try using Completion types for type-safe parsing\n+        if chunk_data.get('object') == 'chat.completion.chunk':\n+            response = ChatCompletionStreamResponse.model_validate(chunk_data)\n+            if response.choices and response.choices[0].delta.content:\n+                return response.choices[0].delta.content\n+        elif chunk_data.get('object') == 'text_completion':\n+            response = CompletionStreamResponse.model_validate(chunk_data)\n+            if response.choices and response.choices[0].text:\n+                return response.choices[0].text\n+    except Exception:",
        "comment_created_at": "2025-06-13T10:05:09+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nConsider catching more specific exceptions (e.g., `pydantic.ValidationError`, `KeyError`, `AttributeError`) instead of using a broad `except Exception:` clause. If a broad exception is necessary, logging the actual exception at a debug level could aid in diagnosing unexpected chunk formats.",
        "pr_file_module": null
      },
      {
        "comment_id": "2151655134",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19608,
        "pr_file": "vllm/entrypoints/openai/api_server.py",
        "discussion_id": "2144688617",
        "commented_code": "@@ -1014,6 +1014,95 @@ def load_log_config(log_config_file: Optional[str]) -> Optional[dict]:\n         return None\n \n \n+def _clean_content_for_logging(content: str) -> str:\n+    \"\"\"Clean content for safe logging by escaping special characters.\"\"\"\n+    if not content:\n+        return content\n+\n+    # Replace common special characters that can break log formatting\n+    clean_content = (\n+        content.replace('\\n', '\\\\n')  # Newlines\n+        .replace('\\r', '\\\\r')  # Carriage returns\n+        .replace('\\t', '\\\\t')  # Tabs\n+        .replace('\\b', '\\\\b')  # Backspaces\n+        .replace('\\f', '\\\\f')  # Form feeds\n+        .replace('\\v', '\\\\v')  # Vertical tabs\n+        .replace('\\0', '\\\\0')  # Null characters\n+        .replace('\"', '\\\\\"')  # Double quotes\n+        .replace('\\\\', '\\\\\\\\'))  # Backslashes (do this last)\n+\n+    return clean_content\n+\n+\n+def _extract_content_from_chunk(chunk_data: dict) -> str:\n+    \"\"\"Extract content from a streaming response chunk.\"\"\"\n+    try:\n+        from vllm.entrypoints.openai.protocol import (\n+            ChatCompletionStreamResponse, CompletionStreamResponse)\n+\n+        # Try using Completion types for type-safe parsing\n+        if chunk_data.get('object') == 'chat.completion.chunk':\n+            response = ChatCompletionStreamResponse.model_validate(chunk_data)\n+            if response.choices and response.choices[0].delta.content:\n+                return response.choices[0].delta.content\n+        elif chunk_data.get('object') == 'text_completion':\n+            response = CompletionStreamResponse.model_validate(chunk_data)\n+            if response.choices and response.choices[0].text:\n+                return response.choices[0].text\n+    except Exception:",
        "comment_created_at": "2025-06-17T08:27:41+00:00",
        "comment_author": "aarnphm",
        "comment_body": "@lengrongfu I for once agree with gemini here",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190283172",
    "pr_number": 20575,
    "pr_file": "vllm/entrypoints/openai/serving_tokenization.py",
    "created_at": "2025-07-07T14:33:00+00:00",
    "commented_code": "input_text = prompt_input[\"prompt\"]\n \n         return DetokenizeResponse(prompt=input_text)\n+\n+    async def get_tokenizer_info(\n+            self) -> Union[TokenizerInfoResponse, ErrorResponse]:\n+        \"\"\"Get comprehensive tokenizer information.\"\"\"\n+        try:\n+            tokenizer = await self.engine_client.get_tokenizer()\n+            info = TokenizerInfo(tokenizer, self.model_config,\n+                                 self.chat_template).to_dict()\n+            return TokenizerInfoResponse(**info)\n+        except Exception as e:\n+            return self.create_error_response(\n+                f\"Failed to get tokenizer info: {str(e)}\")",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2190283172",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20575,
        "pr_file": "vllm/entrypoints/openai/serving_tokenization.py",
        "discussion_id": "2190283172",
        "commented_code": "@@ -155,3 +160,54 @@ async def create_detokenize(\n         input_text = prompt_input[\"prompt\"]\n \n         return DetokenizeResponse(prompt=input_text)\n+\n+    async def get_tokenizer_info(\n+            self) -> Union[TokenizerInfoResponse, ErrorResponse]:\n+        \"\"\"Get comprehensive tokenizer information.\"\"\"\n+        try:\n+            tokenizer = await self.engine_client.get_tokenizer()\n+            info = TokenizerInfo(tokenizer, self.model_config,\n+                                 self.chat_template).to_dict()\n+            return TokenizerInfoResponse(**info)\n+        except Exception as e:\n+            return self.create_error_response(\n+                f\"Failed to get tokenizer info: {str(e)}\")",
        "comment_created_at": "2025-07-07T14:33:00+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nCatching a broad `Exception` can hide bugs and swallow important exceptions like `KeyboardInterrupt`. It's better to log the full traceback to aid in debugging when an unexpected error occurs.\n\n```suggestion\n        except Exception as e:\n            logger.exception(\"Failed to get tokenizer info.\")\n            return self.create_error_response(\n                f\"Failed to get tokenizer info: {e!s}\")\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190545431",
    "pr_number": 20583,
    "pr_file": "examples/offline_inference_tt.py",
    "created_at": "2025-07-07T16:22:22+00:00",
    "commented_code": "+# SPDX-License-Identifier: Apache-2.0\n+import argparse\n+import json\n+import os\n+import time\n+from pathlib import Path\n+\n+import numpy as np\n+import uvloop\n+from PIL import Image as PIL_Image\n+from pkg_resources import resource_filename\n+from tqdm import tqdm\n+from transformers import AutoTokenizer\n+\n+from vllm import LLM, ModelRegistry, SamplingParams\n+from vllm.engine.arg_utils import AsyncEngineArgs\n+from vllm.engine.multiprocessing.client import MQLLMEngineClient\n+from vllm.entrypoints.openai.api_server import (\n+    build_async_engine_client_from_engine_args)\n+from vllm.inputs.data import TokensPrompt\n+from vllm.utils import merge_async_iterators\n+\n+\n+def register_tt_models():\n+    llama_text_version = os.getenv(\"TT_LLAMA_TEXT_VER\", \"tt_transformers\")\n+    if llama_text_version == \"tt_transformers\":\n+        path_llama_text = \\\n+            \"models.tt_transformers.tt.generator_vllm:LlamaForCausalLM\"\n+    elif llama_text_version == \"llama3_subdevices\":\n+        path_llama_text = \\\n+            \"models.demos.llama3_subdevices.tt.generator_vllm:LlamaForCausalLM\"\n+    elif llama_text_version == \"llama2_70b\":\n+        path_llama_text = \\\n+            \"models.demos.t3000.llama2_70b.tt.generator_vllm:TtLlamaForCausalLM\"\n+    else:\n+        raise ValueError(\n+            f\"Unsupported TT Llama version: {llama_text_version}, \"\n+            \"pick one of [tt_transformers, llama3_subdevices, llama2_70b]\")",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2190545431",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20583,
        "pr_file": "examples/offline_inference_tt.py",
        "discussion_id": "2190545431",
        "commented_code": "@@ -0,0 +1,479 @@\n+# SPDX-License-Identifier: Apache-2.0\n+import argparse\n+import json\n+import os\n+import time\n+from pathlib import Path\n+\n+import numpy as np\n+import uvloop\n+from PIL import Image as PIL_Image\n+from pkg_resources import resource_filename\n+from tqdm import tqdm\n+from transformers import AutoTokenizer\n+\n+from vllm import LLM, ModelRegistry, SamplingParams\n+from vllm.engine.arg_utils import AsyncEngineArgs\n+from vllm.engine.multiprocessing.client import MQLLMEngineClient\n+from vllm.entrypoints.openai.api_server import (\n+    build_async_engine_client_from_engine_args)\n+from vllm.inputs.data import TokensPrompt\n+from vllm.utils import merge_async_iterators\n+\n+\n+def register_tt_models():\n+    llama_text_version = os.getenv(\"TT_LLAMA_TEXT_VER\", \"tt_transformers\")\n+    if llama_text_version == \"tt_transformers\":\n+        path_llama_text = \\\n+            \"models.tt_transformers.tt.generator_vllm:LlamaForCausalLM\"\n+    elif llama_text_version == \"llama3_subdevices\":\n+        path_llama_text = \\\n+            \"models.demos.llama3_subdevices.tt.generator_vllm:LlamaForCausalLM\"\n+    elif llama_text_version == \"llama2_70b\":\n+        path_llama_text = \\\n+            \"models.demos.t3000.llama2_70b.tt.generator_vllm:TtLlamaForCausalLM\"\n+    else:\n+        raise ValueError(\n+            f\"Unsupported TT Llama version: {llama_text_version}, \"\n+            \"pick one of [tt_transformers, llama3_subdevices, llama2_70b]\")",
        "comment_created_at": "2025-07-07T16:22:22+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nConsider using a more specific exception type than `ValueError`. A custom exception, or a more descriptive built-in exception like `NotImplementedError`, would provide better context about the error condition.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2186204050",
    "pr_number": 20504,
    "pr_file": "vllm/entrypoints/openai/serving_responses.py",
    "created_at": "2025-07-04T23:32:44+00:00",
    "commented_code": "+# SPDX-License-Identifier: Apache-2.0\n+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n+\n+import asyncio\n+import time\n+from collections.abc import AsyncGenerator, AsyncIterator\n+from typing import Callable, Final, Optional, Union\n+\n+import jinja2\n+from fastapi import Request\n+from openai.types.responses import ResponseOutputMessage, ResponseOutputText\n+\n+from vllm.config import ModelConfig\n+from vllm.engine.protocol import EngineClient\n+from vllm.entrypoints.chat_utils import ChatTemplateContentFormatOption\n+from vllm.entrypoints.logger import RequestLogger\n+from vllm.entrypoints.openai.protocol import (ErrorResponse,\n+                                              PromptTokenUsageInfo,\n+                                              RequestResponseMetadata,\n+                                              ResponseReasoningItem,\n+                                              ResponsesRequest,\n+                                              ResponsesResponse, UsageInfo)\n+from vllm.entrypoints.openai.serving_engine import OpenAIServing\n+from vllm.entrypoints.openai.serving_models import OpenAIServingModels\n+from vllm.logger import init_logger\n+from vllm.outputs import RequestOutput\n+from vllm.reasoning import ReasoningParser, ReasoningParserManager\n+from vllm.sampling_params import SamplingParams\n+from vllm.transformers_utils.tokenizer import AnyTokenizer\n+from vllm.utils import random_uuid\n+\n+logger = init_logger(__name__)\n+\n+\n+class OpenAIServingResponses(OpenAIServing):\n+\n+    def __init__(\n+        self,\n+        engine_client: EngineClient,\n+        model_config: ModelConfig,\n+        models: OpenAIServingModels,\n+        *,\n+        request_logger: Optional[RequestLogger],\n+        chat_template: Optional[str],\n+        chat_template_content_format: ChatTemplateContentFormatOption,\n+        return_tokens_as_token_ids: bool = False,\n+        reasoning_parser: str = \"\",\n+        enable_auto_tools: bool = False,\n+        expand_tools_even_if_tool_choice_none: bool = False,\n+        tool_parser: Optional[str] = None,\n+        enable_prompt_tokens_details: bool = False,\n+        enable_force_include_usage: bool = False,\n+    ) -> None:\n+        super().__init__(\n+            engine_client=engine_client,\n+            model_config=model_config,\n+            models=models,\n+            request_logger=request_logger,\n+            return_tokens_as_token_ids=return_tokens_as_token_ids,\n+            enable_force_include_usage=enable_force_include_usage,\n+        )\n+\n+        self.chat_template = chat_template\n+        self.chat_template_content_format: Final = chat_template_content_format\n+\n+        self.reasoning_parser: Optional[Callable[[AnyTokenizer],\n+                                                 ReasoningParser]] = None\n+        if reasoning_parser:\n+            try:\n+                self.reasoning_parser = (\n+                    ReasoningParserManager.get_reasoning_parser(\n+                        reasoning_parser))\n+                assert self.reasoning_parser is not None\n+            except Exception as e:\n+                raise TypeError(\n+                    f\"{reasoning_parser=} has not been registered\") from e\n+\n+        self.enable_prompt_tokens_details = enable_prompt_tokens_details\n+        self.enable_force_include_usage = enable_force_include_usage\n+        self.default_sampling_params = (\n+            self.model_config.get_diff_sampling_param())\n+        if self.default_sampling_params:\n+            source = self.model_config.generation_config\n+            source = \"model\" if source == \"auto\" else source\n+            logger.info(\"Using default chat sampling params from %s: %s\",\n+                        source, self.default_sampling_params)\n+\n+    async def create_responses(\n+        self,\n+        request: ResponsesRequest,\n+        raw_request: Optional[Request] = None,\n+    ) -> Union[AsyncGenerator[str, None], ResponsesResponse, ErrorResponse]:\n+        error_check_ret = await self._check_model(request)\n+        if error_check_ret is not None:\n+            logger.error(\"Error with model %s\", error_check_ret)\n+            return error_check_ret\n+\n+        # If the engine is dead, raise the engine's DEAD_ERROR.\n+        # This is required for the streaming case, where we return a\n+        # success status before we actually start generating text :).\n+        if self.engine_client.errored:\n+            raise self.engine_client.dead_error\n+\n+        try:\n+            (\n+                lora_request,\n+                prompt_adapter_request,\n+            ) = self._maybe_get_adapters(request)\n+            model_name = self._get_model_name(request.model, lora_request)\n+            tokenizer = await self.engine_client.get_tokenizer(lora_request)\n+\n+            # Reponses API supports simple text inputs without chat format.\n+            if isinstance(request.input, str):\n+                text_input = request.input\n+                request.input = [{\"role\": \"user\", \"content\": text_input}]\n+\n+            (\n+                conversation,\n+                request_prompts,\n+                engine_prompts,\n+            ) = await self._preprocess_chat(\n+                request,\n+                tokenizer,\n+                request.input,\n+                chat_template=self.chat_template,\n+                chat_template_content_format=self.chat_template_content_format,\n+            )\n+        except (ValueError, TypeError, RuntimeError,\n+                jinja2.TemplateError) as e:\n+            logger.exception(\"Error in preprocessing prompt inputs\")\n+            return self.create_error_response(f\"{e} {e.__cause__}\")\n+\n+        request_metadata = RequestResponseMetadata(\n+            request_id=request.request_id)\n+        if raw_request:\n+            raw_request.state.request_metadata = request_metadata\n+\n+        # Schedule the request and get the result generator.\n+        generators: list[AsyncGenerator[RequestOutput, None]] = []\n+        try:\n+            for i, engine_prompt in enumerate(engine_prompts):\n+                default_max_tokens = self.max_model_len - len(\n+                    engine_prompt[\"prompt_token_ids\"])\n+                sampling_params = request.to_sampling_params(\n+                    default_max_tokens, self.default_sampling_params)\n+\n+                self._log_inputs(request.request_id,\n+                                 request_prompts[i],\n+                                 params=sampling_params,\n+                                 lora_request=lora_request,\n+                                 prompt_adapter_request=prompt_adapter_request)\n+\n+                trace_headers = (None if raw_request is None else await\n+                                 self._get_trace_headers(raw_request.headers))\n+\n+                generator = self.engine_client.generate(\n+                    engine_prompt,\n+                    sampling_params,\n+                    request.request_id,\n+                    lora_request=lora_request,\n+                    trace_headers=trace_headers,\n+                    prompt_adapter_request=prompt_adapter_request,\n+                    priority=request.priority,\n+                )\n+\n+                generators.append(generator)\n+        except ValueError as e:\n+            # TODO: Use a vllm-specific Validation Error\n+            return self.create_error_response(str(e))\n+\n+        assert len(generators) == 1\n+        result_generator, = generators\n+\n+        if request.background:\n+            return await self.create_background_response(\n+                request,\n+                sampling_params,\n+                result_generator,\n+                model_name,\n+            )\n+\n+        if request.stream:\n+            raise NotImplementedError(\"Streaming responses are not supported\")\n+\n+        try:\n+            return await self.responses_full_generator(\n+                request,\n+                sampling_params,\n+                result_generator,\n+                model_name,\n+                tokenizer,\n+                request_metadata,\n+            )\n+        except Exception as e:\n+            return self.create_error_response(str(e))\n+\n+    async def responses_full_generator(\n+        self,\n+        request: ResponsesRequest,\n+        sampling_params: SamplingParams,\n+        result_generator: AsyncIterator[RequestOutput],\n+        model_name: str,\n+        tokenizer: AnyTokenizer,\n+        request_metadata: RequestResponseMetadata,\n+    ) -> Union[ErrorResponse, ResponsesResponse]:\n+        created_time = int(time.time())\n+        final_res: Optional[RequestOutput] = None\n+\n+        try:\n+            async for res in result_generator:\n+                final_res = res\n+        except asyncio.CancelledError:\n+            return self.create_error_response(\"Client disconnected\")\n+        except ValueError as e:\n+            # TODO: Use a vllm-specific Validation Error\n+            return self.create_error_response(str(e))\n+\n+        assert final_res is not None\n+        assert len(final_res.outputs) == 1\n+        final_output = final_res.outputs[0]\n+\n+        if self.reasoning_parser:\n+            try:\n+                reasoning_parser = self.reasoning_parser(tokenizer)\n+            except RuntimeError as e:\n+                logger.exception(\"Error in reasoning parser creation.\")\n+                return self.create_error_response(str(e))\n+\n+            reasoning_content, content = (\n+                reasoning_parser.extract_reasoning_content(final_output.text,\n+                                                           request=request))\n+        else:\n+            reasoning_content = None\n+            content = final_output.text\n+\n+        output = []\n+        if reasoning_content:\n+            reasoning_item = ResponseReasoningItem(\n+                text=reasoning_content,\n+                status=None,  # NOTE: Only the last output item has status.\n+            )\n+            output.append(reasoning_item)\n+        if content:\n+            output_text = ResponseOutputText(\n+                text=content,\n+                annotations=[],  # TODO\n+                type=\"output_text\",\n+                logprobs=None,  # TODO\n+            )\n+            message = ResponseOutputMessage(\n+                id=f\"msg_{random_uuid()}\",\n+                content=[output_text],\n+                role=\"assistant\",\n+                status=\"completed\",\n+                type=\"message\",\n+            )\n+            output.append(message)\n+\n+        # Calculate usage.\n+        assert final_res.prompt_token_ids is not None\n+        num_prompt_tokens = len(final_res.prompt_token_ids)\n+        num_generated_tokens = len(final_output.token_ids)\n+        usage = UsageInfo(prompt_tokens=num_prompt_tokens,\n+                          completion_tokens=num_generated_tokens,\n+                          total_tokens=num_prompt_tokens +\n+                          num_generated_tokens)\n+        if self.enable_prompt_tokens_details and final_res.num_cached_tokens:\n+            usage.prompt_tokens_details = PromptTokenUsageInfo(\n+                cached_tokens=final_res.num_cached_tokens)\n+        request_metadata.final_usage_info = usage\n+\n+        response = ResponsesResponse.from_request(\n+            request,\n+            sampling_params,\n+            model_name=model_name,\n+            created_time=created_time,\n+            output=output,\n+            status=\"completed\",\n+            usage=usage,\n+        )\n+        return response\n+\n+    async def create_background_response(\n+        self,\n+        request: ResponsesRequest,\n+        sampling_params: SamplingParams,\n+        result_generator: AsyncIterator[RequestOutput],\n+        model_name: str,\n+    ) -> ResponsesResponse:\n+        created_time = int(time.time())\n+        # Start the task but don't await it.\n+        asyncio.create_task(_drain_generator(result_generator))",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2186204050",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20504,
        "pr_file": "vllm/entrypoints/openai/serving_responses.py",
        "discussion_id": "2186204050",
        "commented_code": "@@ -0,0 +1,306 @@\n+# SPDX-License-Identifier: Apache-2.0\n+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n+\n+import asyncio\n+import time\n+from collections.abc import AsyncGenerator, AsyncIterator\n+from typing import Callable, Final, Optional, Union\n+\n+import jinja2\n+from fastapi import Request\n+from openai.types.responses import ResponseOutputMessage, ResponseOutputText\n+\n+from vllm.config import ModelConfig\n+from vllm.engine.protocol import EngineClient\n+from vllm.entrypoints.chat_utils import ChatTemplateContentFormatOption\n+from vllm.entrypoints.logger import RequestLogger\n+from vllm.entrypoints.openai.protocol import (ErrorResponse,\n+                                              PromptTokenUsageInfo,\n+                                              RequestResponseMetadata,\n+                                              ResponseReasoningItem,\n+                                              ResponsesRequest,\n+                                              ResponsesResponse, UsageInfo)\n+from vllm.entrypoints.openai.serving_engine import OpenAIServing\n+from vllm.entrypoints.openai.serving_models import OpenAIServingModels\n+from vllm.logger import init_logger\n+from vllm.outputs import RequestOutput\n+from vllm.reasoning import ReasoningParser, ReasoningParserManager\n+from vllm.sampling_params import SamplingParams\n+from vllm.transformers_utils.tokenizer import AnyTokenizer\n+from vllm.utils import random_uuid\n+\n+logger = init_logger(__name__)\n+\n+\n+class OpenAIServingResponses(OpenAIServing):\n+\n+    def __init__(\n+        self,\n+        engine_client: EngineClient,\n+        model_config: ModelConfig,\n+        models: OpenAIServingModels,\n+        *,\n+        request_logger: Optional[RequestLogger],\n+        chat_template: Optional[str],\n+        chat_template_content_format: ChatTemplateContentFormatOption,\n+        return_tokens_as_token_ids: bool = False,\n+        reasoning_parser: str = \"\",\n+        enable_auto_tools: bool = False,\n+        expand_tools_even_if_tool_choice_none: bool = False,\n+        tool_parser: Optional[str] = None,\n+        enable_prompt_tokens_details: bool = False,\n+        enable_force_include_usage: bool = False,\n+    ) -> None:\n+        super().__init__(\n+            engine_client=engine_client,\n+            model_config=model_config,\n+            models=models,\n+            request_logger=request_logger,\n+            return_tokens_as_token_ids=return_tokens_as_token_ids,\n+            enable_force_include_usage=enable_force_include_usage,\n+        )\n+\n+        self.chat_template = chat_template\n+        self.chat_template_content_format: Final = chat_template_content_format\n+\n+        self.reasoning_parser: Optional[Callable[[AnyTokenizer],\n+                                                 ReasoningParser]] = None\n+        if reasoning_parser:\n+            try:\n+                self.reasoning_parser = (\n+                    ReasoningParserManager.get_reasoning_parser(\n+                        reasoning_parser))\n+                assert self.reasoning_parser is not None\n+            except Exception as e:\n+                raise TypeError(\n+                    f\"{reasoning_parser=} has not been registered\") from e\n+\n+        self.enable_prompt_tokens_details = enable_prompt_tokens_details\n+        self.enable_force_include_usage = enable_force_include_usage\n+        self.default_sampling_params = (\n+            self.model_config.get_diff_sampling_param())\n+        if self.default_sampling_params:\n+            source = self.model_config.generation_config\n+            source = \"model\" if source == \"auto\" else source\n+            logger.info(\"Using default chat sampling params from %s: %s\",\n+                        source, self.default_sampling_params)\n+\n+    async def create_responses(\n+        self,\n+        request: ResponsesRequest,\n+        raw_request: Optional[Request] = None,\n+    ) -> Union[AsyncGenerator[str, None], ResponsesResponse, ErrorResponse]:\n+        error_check_ret = await self._check_model(request)\n+        if error_check_ret is not None:\n+            logger.error(\"Error with model %s\", error_check_ret)\n+            return error_check_ret\n+\n+        # If the engine is dead, raise the engine's DEAD_ERROR.\n+        # This is required for the streaming case, where we return a\n+        # success status before we actually start generating text :).\n+        if self.engine_client.errored:\n+            raise self.engine_client.dead_error\n+\n+        try:\n+            (\n+                lora_request,\n+                prompt_adapter_request,\n+            ) = self._maybe_get_adapters(request)\n+            model_name = self._get_model_name(request.model, lora_request)\n+            tokenizer = await self.engine_client.get_tokenizer(lora_request)\n+\n+            # Reponses API supports simple text inputs without chat format.\n+            if isinstance(request.input, str):\n+                text_input = request.input\n+                request.input = [{\"role\": \"user\", \"content\": text_input}]\n+\n+            (\n+                conversation,\n+                request_prompts,\n+                engine_prompts,\n+            ) = await self._preprocess_chat(\n+                request,\n+                tokenizer,\n+                request.input,\n+                chat_template=self.chat_template,\n+                chat_template_content_format=self.chat_template_content_format,\n+            )\n+        except (ValueError, TypeError, RuntimeError,\n+                jinja2.TemplateError) as e:\n+            logger.exception(\"Error in preprocessing prompt inputs\")\n+            return self.create_error_response(f\"{e} {e.__cause__}\")\n+\n+        request_metadata = RequestResponseMetadata(\n+            request_id=request.request_id)\n+        if raw_request:\n+            raw_request.state.request_metadata = request_metadata\n+\n+        # Schedule the request and get the result generator.\n+        generators: list[AsyncGenerator[RequestOutput, None]] = []\n+        try:\n+            for i, engine_prompt in enumerate(engine_prompts):\n+                default_max_tokens = self.max_model_len - len(\n+                    engine_prompt[\"prompt_token_ids\"])\n+                sampling_params = request.to_sampling_params(\n+                    default_max_tokens, self.default_sampling_params)\n+\n+                self._log_inputs(request.request_id,\n+                                 request_prompts[i],\n+                                 params=sampling_params,\n+                                 lora_request=lora_request,\n+                                 prompt_adapter_request=prompt_adapter_request)\n+\n+                trace_headers = (None if raw_request is None else await\n+                                 self._get_trace_headers(raw_request.headers))\n+\n+                generator = self.engine_client.generate(\n+                    engine_prompt,\n+                    sampling_params,\n+                    request.request_id,\n+                    lora_request=lora_request,\n+                    trace_headers=trace_headers,\n+                    prompt_adapter_request=prompt_adapter_request,\n+                    priority=request.priority,\n+                )\n+\n+                generators.append(generator)\n+        except ValueError as e:\n+            # TODO: Use a vllm-specific Validation Error\n+            return self.create_error_response(str(e))\n+\n+        assert len(generators) == 1\n+        result_generator, = generators\n+\n+        if request.background:\n+            return await self.create_background_response(\n+                request,\n+                sampling_params,\n+                result_generator,\n+                model_name,\n+            )\n+\n+        if request.stream:\n+            raise NotImplementedError(\"Streaming responses are not supported\")\n+\n+        try:\n+            return await self.responses_full_generator(\n+                request,\n+                sampling_params,\n+                result_generator,\n+                model_name,\n+                tokenizer,\n+                request_metadata,\n+            )\n+        except Exception as e:\n+            return self.create_error_response(str(e))\n+\n+    async def responses_full_generator(\n+        self,\n+        request: ResponsesRequest,\n+        sampling_params: SamplingParams,\n+        result_generator: AsyncIterator[RequestOutput],\n+        model_name: str,\n+        tokenizer: AnyTokenizer,\n+        request_metadata: RequestResponseMetadata,\n+    ) -> Union[ErrorResponse, ResponsesResponse]:\n+        created_time = int(time.time())\n+        final_res: Optional[RequestOutput] = None\n+\n+        try:\n+            async for res in result_generator:\n+                final_res = res\n+        except asyncio.CancelledError:\n+            return self.create_error_response(\"Client disconnected\")\n+        except ValueError as e:\n+            # TODO: Use a vllm-specific Validation Error\n+            return self.create_error_response(str(e))\n+\n+        assert final_res is not None\n+        assert len(final_res.outputs) == 1\n+        final_output = final_res.outputs[0]\n+\n+        if self.reasoning_parser:\n+            try:\n+                reasoning_parser = self.reasoning_parser(tokenizer)\n+            except RuntimeError as e:\n+                logger.exception(\"Error in reasoning parser creation.\")\n+                return self.create_error_response(str(e))\n+\n+            reasoning_content, content = (\n+                reasoning_parser.extract_reasoning_content(final_output.text,\n+                                                           request=request))\n+        else:\n+            reasoning_content = None\n+            content = final_output.text\n+\n+        output = []\n+        if reasoning_content:\n+            reasoning_item = ResponseReasoningItem(\n+                text=reasoning_content,\n+                status=None,  # NOTE: Only the last output item has status.\n+            )\n+            output.append(reasoning_item)\n+        if content:\n+            output_text = ResponseOutputText(\n+                text=content,\n+                annotations=[],  # TODO\n+                type=\"output_text\",\n+                logprobs=None,  # TODO\n+            )\n+            message = ResponseOutputMessage(\n+                id=f\"msg_{random_uuid()}\",\n+                content=[output_text],\n+                role=\"assistant\",\n+                status=\"completed\",\n+                type=\"message\",\n+            )\n+            output.append(message)\n+\n+        # Calculate usage.\n+        assert final_res.prompt_token_ids is not None\n+        num_prompt_tokens = len(final_res.prompt_token_ids)\n+        num_generated_tokens = len(final_output.token_ids)\n+        usage = UsageInfo(prompt_tokens=num_prompt_tokens,\n+                          completion_tokens=num_generated_tokens,\n+                          total_tokens=num_prompt_tokens +\n+                          num_generated_tokens)\n+        if self.enable_prompt_tokens_details and final_res.num_cached_tokens:\n+            usage.prompt_tokens_details = PromptTokenUsageInfo(\n+                cached_tokens=final_res.num_cached_tokens)\n+        request_metadata.final_usage_info = usage\n+\n+        response = ResponsesResponse.from_request(\n+            request,\n+            sampling_params,\n+            model_name=model_name,\n+            created_time=created_time,\n+            output=output,\n+            status=\"completed\",\n+            usage=usage,\n+        )\n+        return response\n+\n+    async def create_background_response(\n+        self,\n+        request: ResponsesRequest,\n+        sampling_params: SamplingParams,\n+        result_generator: AsyncIterator[RequestOutput],\n+        model_name: str,\n+    ) -> ResponsesResponse:\n+        created_time = int(time.time())\n+        # Start the task but don't await it.\n+        asyncio.create_task(_drain_generator(result_generator))",
        "comment_created_at": "2025-07-04T23:32:44+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nA background task is created here using `asyncio.create_task` to drain the generator. If an exception occurs within this task, it will be silently lost because the task is not awaited and has no \"done\" callback to handle exceptions. This can make debugging difficult. It's recommended to add a callback to log any potential exceptions.\n\n```python\n        task = asyncio.create_task(_drain_generator(result_generator))\n        task.add_done_callback(\n            lambda t: t.exception() and logger.error(\n                \"Error draining generator in background\", exc_info=t.exception()))\n```",
        "pr_file_module": null
      }
    ]
  }
]