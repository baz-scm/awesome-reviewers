[
  {
    "discussion_id": "2144688617",
    "pr_number": 19608,
    "pr_file": "vllm/entrypoints/openai/api_server.py",
    "created_at": "2025-06-13T10:05:09+00:00",
    "commented_code": "return None\n \n \n+def _clean_content_for_logging(content: str) -> str:\n+    \"\"\"Clean content for safe logging by escaping special characters.\"\"\"\n+    if not content:\n+        return content\n+\n+    # Replace common special characters that can break log formatting\n+    clean_content = (\n+        content.replace('\n', '\\\n')  # Newlines\n+        .replace('\\r', '\\\\r')  # Carriage returns\n+        .replace('\\t', '\\\\t')  # Tabs\n+        .replace('\\b', '\\\\b')  # Backspaces\n+        .replace('\\f', '\\\\f')  # Form feeds\n+        .replace('\\v', '\\\\v')  # Vertical tabs\n+        .replace('\\0', '\\\\0')  # Null characters\n+        .replace('\"', '\\\\\"')  # Double quotes\n+        .replace('\\\\', '\\\\\\\\'))  # Backslashes (do this last)\n+\n+    return clean_content\n+\n+\n+def _extract_content_from_chunk(chunk_data: dict) -> str:\n+    \"\"\"Extract content from a streaming response chunk.\"\"\"\n+    try:\n+        from vllm.entrypoints.openai.protocol import (\n+            ChatCompletionStreamResponse, CompletionStreamResponse)\n+\n+        # Try using Completion types for type-safe parsing\n+        if chunk_data.get('object') == 'chat.completion.chunk':\n+            response = ChatCompletionStreamResponse.model_validate(chunk_data)\n+            if response.choices and response.choices[0].delta.content:\n+                return response.choices[0].delta.content\n+        elif chunk_data.get('object') == 'text_completion':\n+            response = CompletionStreamResponse.model_validate(chunk_data)\n+            if response.choices and response.choices[0].text:\n+                return response.choices[0].text\n+    except Exception:",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2151655134",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19608,
        "pr_file": "vllm/entrypoints/openai/api_server.py",
        "discussion_id": "2144688617",
        "commented_code": "@@ -1014,6 +1014,95 @@ def load_log_config(log_config_file: Optional[str]) -> Optional[dict]:\n         return None\n \n \n+def _clean_content_for_logging(content: str) -> str:\n+    \"\"\"Clean content for safe logging by escaping special characters.\"\"\"\n+    if not content:\n+        return content\n+\n+    # Replace common special characters that can break log formatting\n+    clean_content = (\n+        content.replace('\\n', '\\\\n')  # Newlines\n+        .replace('\\r', '\\\\r')  # Carriage returns\n+        .replace('\\t', '\\\\t')  # Tabs\n+        .replace('\\b', '\\\\b')  # Backspaces\n+        .replace('\\f', '\\\\f')  # Form feeds\n+        .replace('\\v', '\\\\v')  # Vertical tabs\n+        .replace('\\0', '\\\\0')  # Null characters\n+        .replace('\"', '\\\\\"')  # Double quotes\n+        .replace('\\\\', '\\\\\\\\'))  # Backslashes (do this last)\n+\n+    return clean_content\n+\n+\n+def _extract_content_from_chunk(chunk_data: dict) -> str:\n+    \"\"\"Extract content from a streaming response chunk.\"\"\"\n+    try:\n+        from vllm.entrypoints.openai.protocol import (\n+            ChatCompletionStreamResponse, CompletionStreamResponse)\n+\n+        # Try using Completion types for type-safe parsing\n+        if chunk_data.get('object') == 'chat.completion.chunk':\n+            response = ChatCompletionStreamResponse.model_validate(chunk_data)\n+            if response.choices and response.choices[0].delta.content:\n+                return response.choices[0].delta.content\n+        elif chunk_data.get('object') == 'text_completion':\n+            response = CompletionStreamResponse.model_validate(chunk_data)\n+            if response.choices and response.choices[0].text:\n+                return response.choices[0].text\n+    except Exception:",
        "comment_created_at": "2025-06-17T08:27:41+00:00",
        "comment_author": "aarnphm",
        "comment_body": "@lengrongfu I for once agree with gemini here",
        "pr_file_module": null
      }
    ]
  }
]