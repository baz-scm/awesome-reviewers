[
  {
    "discussion_id": "2191550829",
    "pr_number": 20260,
    "pr_file": "docs/serving/openai_compatible_server.md",
    "created_at": "2025-07-08T05:57:27+00:00",
    "commented_code": "### Re-rank API\n \n Our Re-rank API can apply an embedding model or a cross-encoder model to predict relevant scores between a single query, and\n-each of a list of documents. Usually, the score for a sentence pair refers to the similarity between two sentences, on\n-a scale of 0 to 1.\n+each of a list of documents. Usually, the score for a sentence pair refers to the similarity between two sentences or multi-modal inputs(image, etc.), on a scale of 0 to 1.",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2191550829",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20260,
        "pr_file": "docs/serving/openai_compatible_server.md",
        "discussion_id": "2191550829",
        "commented_code": "@@ -698,8 +746,7 @@ The following extra parameters are supported:\n ### Re-rank API\n \n Our Re-rank API can apply an embedding model or a cross-encoder model to predict relevant scores between a single query, and\n-each of a list of documents. Usually, the score for a sentence pair refers to the similarity between two sentences, on\n-a scale of 0 to 1.\n+each of a list of documents. Usually, the score for a sentence pair refers to the similarity between two sentences or multi-modal inputs(image, etc.), on a scale of 0 to 1.",
        "comment_created_at": "2025-07-08T05:57:27+00:00",
        "comment_author": "DarkLight1337",
        "comment_body": "```suggestion\r\neach of a list of documents. Usually, the score for a sentence pair refers to the similarity between two sentences or multi-modal inputs (image, etc.), on a scale of 0 to 1.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2191551244",
    "pr_number": 20260,
    "pr_file": "docs/serving/openai_compatible_server.md",
    "created_at": "2025-07-08T05:57:48+00:00",
    "commented_code": "### Score API\n \n-Our Score API can apply a cross-encoder model or an embedding model to predict scores for sentence pairs. When using an embedding model the score corresponds to the cosine similarity between each embedding pair.\n+Our Score API can apply a cross-encoder model or an embedding model to predict scores for sentence pairs or multimodal pairs. When using an embedding model the score corresponds to the cosine similarity between each embedding pair.",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2191551244",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20260,
        "pr_file": "docs/serving/openai_compatible_server.md",
        "discussion_id": "2191551244",
        "commented_code": "@@ -540,7 +540,7 @@ The following extra parameters are supported:\n \n ### Score API\n \n-Our Score API can apply a cross-encoder model or an embedding model to predict scores for sentence pairs. When using an embedding model the score corresponds to the cosine similarity between each embedding pair.\n+Our Score API can apply a cross-encoder model or an embedding model to predict scores for sentence pairs or multimodal pairs. When using an embedding model the score corresponds to the cosine similarity between each embedding pair.",
        "comment_created_at": "2025-07-08T05:57:48+00:00",
        "comment_author": "DarkLight1337",
        "comment_body": "```suggestion\r\nOur Score API can apply a cross-encoder model or an embedding model to predict scores for sentence or multimodal pairs. When using an embedding model the score corresponds to the cosine similarity between each embedding pair.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2191040711",
    "pr_number": 20590,
    "pr_file": "docs/deployment/frameworks/anyscale.md",
    "created_at": "2025-07-07T21:30:58+00:00",
    "commented_code": "+---\n+title: Anyscale\n+---\n+[](){ #deployment-anyscale }\n+\n+[Anyscale](https://www.anyscale.com) is a managed, multi-cloud platform developed by the creators of Ray.\n+It hosts Ray clusters inside your own AWS, GCP, or Azure account, delivering the flexibility of open-source Ray\n+without the operational overhead of maintaining Kubernetes control planes, configuring autoscalers, or managing observability stacks.\n+When serving large language models with vLLM, Anyscale can rapidly provision [production-ready HTTPS endpoints](https://docs.anyscale.com/examples/deploy-ray-serve-llms) or [fault-tolerant batch inference jobs](https://docs.anyscale.com/examples/ray-data-llm).",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2191040711",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20590,
        "pr_file": "docs/deployment/frameworks/anyscale.md",
        "discussion_id": "2191040711",
        "commented_code": "@@ -0,0 +1,9 @@\n+---\n+title: Anyscale\n+---\n+[](){ #deployment-anyscale }\n+\n+[Anyscale](https://www.anyscale.com) is a managed, multi-cloud platform developed by the creators of Ray.\n+It hosts Ray clusters inside your own AWS, GCP, or Azure account, delivering the flexibility of open-source Ray\n+without the operational overhead of maintaining Kubernetes control planes, configuring autoscalers, or managing observability stacks.\n+When serving large language models with vLLM, Anyscale can rapidly provision [production-ready HTTPS endpoints](https://docs.anyscale.com/examples/deploy-ray-serve-llms) or [fault-tolerant batch inference jobs](https://docs.anyscale.com/examples/ray-data-llm).",
        "comment_created_at": "2025-07-07T21:30:58+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nConsider adding a minimal, complete code example to demonstrate deploying a vLLM model on Anyscale using Ray Serve. This would provide users with a practical starting point.\n\n```markdown\n## Example Usage\n\nYou can deploy a vLLM model on Anyscale using Ray Serve. Here is a minimal example of how to define a model for deployment:\n\n```python\nfrom ray import serve\nfrom ray.serve.llm import LLM, LLMConfig\n\n# Any model supported by vLLM can be served, including HuggingFace models,\n# fine-tuned models, and models with custom code.\n# The model can be a local path or a HuggingFace repository.\n# For a full list of options, see https://docs.vllm.ai/en/latest/models/engine_args.html\nllm = LLM(\n    \"meta-llama/Llama-2-7b-chat-hf\",\n    llm_config=LLMConfig(\n        # vLLM engine arguments\n        engine_kwargs={\n            \"tensor_parallel_size\": 1,\n            \"gpu_memory_utilization\": 0.9,\n        },\n        # Ray Serve deployment arguments\n        deployment_config={\n            \"autoscaling_config\": {\n                \"min_replicas\": 1,\n                \"max_replicas\": 2,\n            },\n        },\n    ),\n)\n\n# To deploy the model on Anyscale, you would typically use `serve.run(llm)`\n# within your Anyscale service definition.\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2191093665",
    "pr_number": 20595,
    "pr_file": "docs/serving/openai_compatible_server.md",
    "created_at": "2025-07-07T22:21:24+00:00",
    "commented_code": "```python\n --8<-- \"vllm/entrypoints/openai/protocol.py:rerank-extra-params\"\n ```\n+\n+## Ray Serve LLM\n+\n+Ray Serve LLM enables scalable, production-grade serving of the vLLM engine. It integrates tightly with vLLM and extends it with features such as auto-scaling, load balancing, and back-pressure.\n+\n+Key capabilities:\n+\n+- Exposes an OpenAI-compatible HTTP API as well as a Pythonic API.\n+- Scales from a single GPU to a multi-node cluster without code changes.\n+- Provides observability and autoscaling policies through Ray dashboards and metrics.\n+\n+The following example shows how to deploy DeepSeek models with Ray Serve LLM: <gh-file:examples/online_serving/ray_serve_deepseek.py>.",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2191093665",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20595,
        "pr_file": "docs/serving/openai_compatible_server.md",
        "discussion_id": "2191093665",
        "commented_code": "@@ -775,3 +775,17 @@ The following extra parameters are supported:\n ```python\n --8<-- \"vllm/entrypoints/openai/protocol.py:rerank-extra-params\"\n ```\n+\n+## Ray Serve LLM\n+\n+Ray Serve LLM enables scalable, production-grade serving of the vLLM engine. It integrates tightly with vLLM and extends it with features such as auto-scaling, load balancing, and back-pressure.\n+\n+Key capabilities:\n+\n+- Exposes an OpenAI-compatible HTTP API as well as a Pythonic API.\n+- Scales from a single GPU to a multi-node cluster without code changes.\n+- Provides observability and autoscaling policies through Ray dashboards and metrics.\n+\n+The following example shows how to deploy DeepSeek models with Ray Serve LLM: <gh-file:examples/online_serving/ray_serve_deepseek.py>.",
        "comment_created_at": "2025-07-07T22:21:24+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe current phrasing is a bit specific to DeepSeek models. To improve clarity and generality for this documentation section, I suggest rephrasing to frame this as an example for large models, for which Ray Serve is particularly well-suited, rather than being exclusively about DeepSeek.\n\n```suggestion\nThe following example shows how to deploy a large model like DeepSeek with Ray Serve LLM: <gh-file:examples/online_serving/ray_serve_deepseek.py>.\n```",
        "pr_file_module": null
      }
    ]
  }
]