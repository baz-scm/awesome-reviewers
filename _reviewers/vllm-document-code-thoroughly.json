[
  {
    "discussion_id": "2187446294",
    "pr_number": 20512,
    "pr_file": "vllm/core/block/block_table.py",
    "created_at": "2025-07-05T16:44:50+00:00",
    "commented_code": "if b is not null_block:\n                     self._allocator.free(b)\n                     self._blocks[idx] = null_block\n+            if end_block_idx > 0:\n+                self._blocks[end_block_idx].prev_block = None",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2187446294",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20512,
        "pr_file": "vllm/core/block/block_table.py",
        "discussion_id": "2187446294",
        "commented_code": "@@ -157,6 +157,9 @@ def append_token_ids(self,\n                 if b is not null_block:\n                     self._allocator.free(b)\n                     self._blocks[idx] = null_block\n+            if end_block_idx > 0:\n+                self._blocks[end_block_idx].prev_block = None",
        "comment_created_at": "2025-07-05T16:44:50+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nConsider adding a comment here to explain why `prev_block` is being set to `None`. This will help future developers understand the purpose of this line, referencing the issue this commit fixes.",
        "pr_file_module": null
      },
      {
        "comment_id": "2189638926",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20512,
        "pr_file": "vllm/core/block/block_table.py",
        "discussion_id": "2187446294",
        "commented_code": "@@ -157,6 +157,9 @@ def append_token_ids(self,\n                 if b is not null_block:\n                     self._allocator.free(b)\n                     self._blocks[idx] = null_block\n+            if end_block_idx > 0:\n+                self._blocks[end_block_idx].prev_block = None",
        "comment_created_at": "2025-07-07T10:34:36+00:00",
        "comment_author": "CLFutureX",
        "comment_body": "> ![medium](https://camo.githubusercontent.com/a33c7ba47779a2aabda978d90fc2665dd949d88da87f5ff554e65308de059f02/68747470733a2f2f7777772e677374617469632e636f6d2f636f64657265766965776167656e742f6d656469756d2d7072696f726974792e737667)\r\n> \r\n> Consider adding a comment here to explain why `prev_block` is being set to `None`. This will help future developers understand the purpose of this line, referencing the issue this commit fixes.\r\n\r\nadd",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2179372410",
    "pr_number": 20359,
    "pr_file": "vllm/v1/core/sched/scheduler.py",
    "created_at": "2025-07-02T07:50:18+00:00",
    "commented_code": "logger = init_logger(__name__)\n \n+class TokenBudget():",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2179372410",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20359,
        "pr_file": "vllm/v1/core/sched/scheduler.py",
        "discussion_id": "2179372410",
        "commented_code": "@@ -37,6 +39,89 @@\n \n logger = init_logger(__name__)\n \n+class TokenBudget():",
        "comment_created_at": "2025-07-02T07:50:18+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis new class `TokenBudget` is a key component of the token throttling logic. It would be beneficial for future maintenance to add a docstring explaining its purpose and how it manages token budgets for prefill and decode stages, especially when pipeline parallelism is enabled.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2175080248",
    "pr_number": 20260,
    "pr_file": "vllm/entrypoints/score_utils.py",
    "created_at": "2025-06-30T13:26:28+00:00",
    "commented_code": "def _validate_score_input_lens(\n-    texts_1: Union[list[str], list[dict]],\n-    texts_2: Union[list[str], list[dict]],\n+    data_1: Union[SingletonPrompt, Sequence[SingletonPrompt]],\n+    data_2: Union[SingletonPrompt, Sequence[SingletonPrompt]],\n ):\n-    if len(texts_1) > 1 and len(texts_1) != len(texts_2):\n+    if len(data_1) > 1 and len(data_1) != len(data_2):\n         raise ValueError(\"Input lengths must be either 1:1, 1:N or N:N\")\n-    if len(texts_1) == 0:\n+    if len(data_1) == 0:\n         raise ValueError(\"At least one text element must be given\")\n-    if len(texts_2) == 0:\n-        raise ValueError(\"At least one text_pair element must be given\")\n\\ No newline at end of file\n+    if len(data_2) == 0:\n+        raise ValueError(\"At least one text_pair element must be given\")\n+\n+\n+def formatting_prompts(\n+    model_arch: str,\n+    tokenizer: AnyTokenizer,\n+    tokenization_kwargs: dict[str, Any],\n+    query: SingletonPrompt,\n+    doc: SingletonPrompt,\n+    query_type: str = 'text',\n+    doc_type: str = 'text',\n+    prefix_str: str = '',\n+) -> TokensPrompt:",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2175080248",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20260,
        "pr_file": "vllm/entrypoints/score_utils.py",
        "discussion_id": "2175080248",
        "commented_code": "@@ -39,12 +42,71 @@ def _cosine_similarity(\n \n \n def _validate_score_input_lens(\n-    texts_1: Union[list[str], list[dict]],\n-    texts_2: Union[list[str], list[dict]],\n+    data_1: Union[SingletonPrompt, Sequence[SingletonPrompt]],\n+    data_2: Union[SingletonPrompt, Sequence[SingletonPrompt]],\n ):\n-    if len(texts_1) > 1 and len(texts_1) != len(texts_2):\n+    if len(data_1) > 1 and len(data_1) != len(data_2):\n         raise ValueError(\"Input lengths must be either 1:1, 1:N or N:N\")\n-    if len(texts_1) == 0:\n+    if len(data_1) == 0:\n         raise ValueError(\"At least one text element must be given\")\n-    if len(texts_2) == 0:\n-        raise ValueError(\"At least one text_pair element must be given\")\n\\ No newline at end of file\n+    if len(data_2) == 0:\n+        raise ValueError(\"At least one text_pair element must be given\")\n+\n+\n+def formatting_prompts(\n+    model_arch: str,\n+    tokenizer: AnyTokenizer,\n+    tokenization_kwargs: dict[str, Any],\n+    query: SingletonPrompt,\n+    doc: SingletonPrompt,\n+    query_type: str = 'text',\n+    doc_type: str = 'text',\n+    prefix_str: str = '',\n+) -> TokensPrompt:",
        "comment_created_at": "2025-06-30T13:26:28+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nConsider adding a docstring to explain the purpose of this function, its parameters, and what it returns. This will improve code maintainability and understanding for other developers. Also, consider using descriptive variable names for `model_arch`, `tokenizer`, `tokenization_kwargs`, `query`, `doc`, `query_type`, and `doc_type` to enhance readability.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190283174",
    "pr_number": 20575,
    "pr_file": "vllm/entrypoints/openai/serving_tokenization.py",
    "created_at": "2025-07-07T14:33:00+00:00",
    "commented_code": "input_text = prompt_input[\"prompt\"]\n \n         return DetokenizeResponse(prompt=input_text)\n+\n+    async def get_tokenizer_info(\n+            self) -> Union[TokenizerInfoResponse, ErrorResponse]:\n+        \"\"\"Get comprehensive tokenizer information.\"\"\"\n+        try:\n+            tokenizer = await self.engine_client.get_tokenizer()\n+            info = TokenizerInfo(tokenizer, self.model_config,\n+                                 self.chat_template).to_dict()\n+            return TokenizerInfoResponse(**info)\n+        except Exception as e:\n+            return self.create_error_response(\n+                f\"Failed to get tokenizer info: {str(e)}\")\n+\n+\n+class TokenizerInfo:\n+\n+    def __init__(self, tokenizer: AnyTokenizer, model_config: ModelConfig,\n+                 chat_template: Optional[str]):\n+        self.tokenizer = tokenizer\n+        self.model_config = model_config\n+        self.chat_template = chat_template\n+\n+    def to_dict(self) -> Dict[str, Any]:\n+        \"\"\"Return the tokenizer configuration.\"\"\"\n+        return self._get_tokenizer_config()\n+\n+    # Use the tokenizer's init_kwargs as the base (this contains the original config)\n+    def _get_tokenizer_config(self) -> Dict[str, Any]:\n+        \"\"\"Get tokenizer configuration directly from the tokenizer object.\"\"\"\n+        config = dict(self.tokenizer.init_kwargs) if hasattr(self.tokenizer, 'init_kwargs') and self.tokenizer.init_kwargs else {}\n+        \n+        # Remove file path fields\n+        config.pop('vocab_file', None)\n+        config.pop('merges_file', None)\n+        \n+        config = self._make_json_serializable(config)\n+        config['tokenizer_class'] = self.tokenizer.__class__.__bases__[0].__name__\n+        if self.chat_template:\n+            config['chat_template'] = self.chat_template\n+        return config\n+\n+    def _make_json_serializable(self, obj):\n+        \"\"\"Convert any non-JSON-serializable objects to serializable format.\"\"\"\n+        if hasattr(obj, 'content'): \n+            return obj.content\n+        elif isinstance(obj, dict):\n+            return {k: self._make_json_serializable(v) for k, v in obj.items()}\n+        elif isinstance(obj, list):\n+            return [self._make_json_serializable(item) for item in obj]\n+        else:\n+            return obj",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2190283174",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20575,
        "pr_file": "vllm/entrypoints/openai/serving_tokenization.py",
        "discussion_id": "2190283174",
        "commented_code": "@@ -155,3 +160,54 @@ async def create_detokenize(\n         input_text = prompt_input[\"prompt\"]\n \n         return DetokenizeResponse(prompt=input_text)\n+\n+    async def get_tokenizer_info(\n+            self) -> Union[TokenizerInfoResponse, ErrorResponse]:\n+        \"\"\"Get comprehensive tokenizer information.\"\"\"\n+        try:\n+            tokenizer = await self.engine_client.get_tokenizer()\n+            info = TokenizerInfo(tokenizer, self.model_config,\n+                                 self.chat_template).to_dict()\n+            return TokenizerInfoResponse(**info)\n+        except Exception as e:\n+            return self.create_error_response(\n+                f\"Failed to get tokenizer info: {str(e)}\")\n+\n+\n+class TokenizerInfo:\n+\n+    def __init__(self, tokenizer: AnyTokenizer, model_config: ModelConfig,\n+                 chat_template: Optional[str]):\n+        self.tokenizer = tokenizer\n+        self.model_config = model_config\n+        self.chat_template = chat_template\n+\n+    def to_dict(self) -> Dict[str, Any]:\n+        \"\"\"Return the tokenizer configuration.\"\"\"\n+        return self._get_tokenizer_config()\n+\n+    # Use the tokenizer's init_kwargs as the base (this contains the original config)\n+    def _get_tokenizer_config(self) -> Dict[str, Any]:\n+        \"\"\"Get tokenizer configuration directly from the tokenizer object.\"\"\"\n+        config = dict(self.tokenizer.init_kwargs) if hasattr(self.tokenizer, 'init_kwargs') and self.tokenizer.init_kwargs else {}\n+        \n+        # Remove file path fields\n+        config.pop('vocab_file', None)\n+        config.pop('merges_file', None)\n+        \n+        config = self._make_json_serializable(config)\n+        config['tokenizer_class'] = self.tokenizer.__class__.__bases__[0].__name__\n+        if self.chat_template:\n+            config['chat_template'] = self.chat_template\n+        return config\n+\n+    def _make_json_serializable(self, obj):\n+        \"\"\"Convert any non-JSON-serializable objects to serializable format.\"\"\"\n+        if hasattr(obj, 'content'): \n+            return obj.content\n+        elif isinstance(obj, dict):\n+            return {k: self._make_json_serializable(v) for k, v in obj.items()}\n+        elif isinstance(obj, list):\n+            return [self._make_json_serializable(item) for item in obj]\n+        else:\n+            return obj",
        "comment_created_at": "2025-07-07T14:33:00+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe `_make_json_serializable` method is a bit magical, especially the `if hasattr(obj, 'content')` check. Adding type hints and a comment explaining this special case (e.g., for handling `AddedToken` objects from the tokenizer) would improve readability and maintainability for future contributors.\n\n```suggestion\n    def _make_json_serializable(self, obj: Any) -> Any:\n        \"\"\"Convert any non-JSON-serializable objects to serializable format.\"\"\"\n        # This is a special case to handle AddedToken objects, which are not\n        # directly JSON-serializable but have a 'content' attribute.\n        if hasattr(obj, 'content'):\n            return obj.content\n        elif isinstance(obj, dict):\n            return {k: self._make_json_serializable(v) for k, v in obj.items()}\n        elif isinstance(obj, list):\n            return [self._make_json_serializable(item) for item in obj]\n        else:\n            return obj\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2177470767",
    "pr_number": 20319,
    "pr_file": "vllm/platforms/cpu.py",
    "created_at": "2025-07-01T12:22:46+00:00",
    "commented_code": "from vllm.utils import GiB_bytes\n         model_config = vllm_config.model_config\n \n-        model_config.disable_cascade_attn = True\n+        if model_config:\n+            model_config.disable_cascade_attn = True",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2177470767",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20319,
        "pr_file": "vllm/platforms/cpu.py",
        "discussion_id": "2177470767",
        "commented_code": "@@ -90,7 +90,8 @@ def check_and_update_config(cls, vllm_config: VllmConfig) -> None:\n         from vllm.utils import GiB_bytes\n         model_config = vllm_config.model_config\n \n-        model_config.disable_cascade_attn = True\n+        if model_config:\n+            model_config.disable_cascade_attn = True",
        "comment_created_at": "2025-07-01T12:22:46+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nConsider adding a code comment to clarify why `disable_cascade_attn` is being set to `True` specifically for the CPU platform. This will help future developers understand this platform-specific configuration choice.\n\n```suggestion\n        if model_config:\n            # Cascade attention is not supported on CPU.\n            model_config.disable_cascade_attn = True\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2188895704",
    "pr_number": 20540,
    "pr_file": "tests/lora/test_phi.py",
    "created_at": "2025-07-07T03:32:07+00:00",
    "commented_code": "return generated_texts\n \n \n-# Skipping for V1 for now as we are hitting,\n-# \"Head size 80 is not supported by FlashAttention.\" error.\n-@pytest.mark.skip(reason=\"Head size 80 is not supported by FlashAttention\")\n def test_phi2_lora(phi2_lora_files):",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2188895704",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20540,
        "pr_file": "tests/lora/test_phi.py",
        "discussion_id": "2188895704",
        "commented_code": "@@ -49,9 +49,6 @@ def do_sample(llm: vllm.LLM, lora_path: str, lora_id: int) -> list[str]:\n     return generated_texts\n \n \n-# Skipping for V1 for now as we are hitting,\n-# \"Head size 80 is not supported by FlashAttention.\" error.\n-@pytest.mark.skip(reason=\"Head size 80 is not supported by FlashAttention\")\n def test_phi2_lora(phi2_lora_files):",
        "comment_created_at": "2025-07-07T03:32:07+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nConsider adding a comment explaining why this test was previously skipped and the reason for enabling `enforce_eager=True`. This provides valuable context for future maintainers.\n\n```suggestion\n# The phi-2 LoRA test runs in eager mode to avoid a FlashAttention\n# incompatibility with head size 80.\ndef test_phi2_lora(phi2_lora_files):\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2170857951",
    "pr_number": 20168,
    "pr_file": "vllm/model_executor/models/registry.py",
    "created_at": "2025-06-27T05:54:13+00:00",
    "commented_code": "\"RobertaForSequenceClassification\"),\n     \"ModernBertForSequenceClassification\": (\"modernbert\",\n                                             \"ModernBertForSequenceClassification\"),\n+    # [Auto-converted (see adapters.py)]\n+    \"GemmaForSequenceClassification\": (\"gemma\", \"GemmaForSequenceClassification\"), # noqa: E501\n     \"Qwen3ForSequenceClassification\": (\"qwen3\", \"Qwen3ForSequenceClassification\"), # noqa: E501",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2170857951",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20168,
        "pr_file": "vllm/model_executor/models/registry.py",
        "discussion_id": "2170857951",
        "commented_code": "@@ -174,6 +174,8 @@\n                                             \"RobertaForSequenceClassification\"),\n     \"ModernBertForSequenceClassification\": (\"modernbert\",\n                                             \"ModernBertForSequenceClassification\"),\n+    # [Auto-converted (see adapters.py)]\n+    \"GemmaForSequenceClassification\": (\"gemma\", \"GemmaForSequenceClassification\"), # noqa: E501\n     \"Qwen3ForSequenceClassification\": (\"qwen3\", \"Qwen3ForSequenceClassification\"), # noqa: E501",
        "comment_created_at": "2025-06-27T05:54:13+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nConsider adding a comment explaining why these models are auto-converted and what the implications are for users. This will help users understand the purpose of this entry and any potential limitations or considerations when using these models.\n\n```python\n    # [Auto-converted (see adapters.py)]\n    # These models are automatically converted to sequence classification models using adapters.py.\n    \"GemmaForSequenceClassification\": (\"gemma\", \"GemmaForSequenceClassification\"), # noqa: E501\n    \"Qwen3ForSequenceClassification\": (\"qwen3\", \"Qwen3ForSequenceClassification\"), # noqa: E501\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2186962494",
    "pr_number": 20509,
    "pr_file": "vllm/model_executor/layers/fused_moe/cutlass_moe.py",
    "created_at": "2025-07-05T08:21:34+00:00",
    "commented_code": "topk_ids: torch.Tensor,\n     w1_scale: torch.Tensor,\n     w2_scale: torch.Tensor,\n-    per_act_token: bool,\n+    per_act_token: Optional[bool] = None,",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2186962494",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20509,
        "pr_file": "vllm/model_executor/layers/fused_moe/cutlass_moe.py",
        "discussion_id": "2186962494",
        "commented_code": "@@ -322,7 +322,7 @@ def cutlass_moe_fp8(\n     topk_ids: torch.Tensor,\n     w1_scale: torch.Tensor,\n     w2_scale: torch.Tensor,\n-    per_act_token: bool,\n+    per_act_token: Optional[bool] = None,",
        "comment_created_at": "2025-07-05T08:21:34+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nConsider adding a docstring to explain the purpose of the `per_act_token` parameter, especially now that it's optional. This will help users understand when and how to use this parameter.\n\n```python\nper_act_token: Optional[bool] = None,  \"\"\"Whether to use a different scale for each token.\"\"\"\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2186234474",
    "pr_number": 20505,
    "pr_file": "vllm/entrypoints/chat_utils.py",
    "created_at": "2025-07-04T23:57:46+00:00",
    "commented_code": "content: Union[Optional[str], list[dict[str, str]]]\n     \"\"\"The contents of the message\"\"\"\n \n+    reasoning_content: Optional[str]\n+    \"\"\"The reasoning content of the message\"\"\"",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2186234474",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20505,
        "pr_file": "vllm/entrypoints/chat_utils.py",
        "discussion_id": "2186234474",
        "commented_code": "@@ -169,6 +172,9 @@ class ConversationMessage(TypedDict, total=False):\n     content: Union[Optional[str], list[dict[str, str]]]\n     \"\"\"The contents of the message\"\"\"\n \n+    reasoning_content: Optional[str]\n+    \"\"\"The reasoning content of the message\"\"\"",
        "comment_created_at": "2025-07-04T23:57:46+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nConsider adding a docstring that briefly explains the purpose of the `reasoning_content` field.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2185982451",
    "pr_number": 20498,
    "pr_file": "vllm/model_executor/layers/fused_moe/layer.py",
    "created_at": "2025-07-04T19:17:26+00:00",
    "commented_code": "self.logical_to_physical_map = logical_to_physical_map[moe_layer_idx]\n         self.logical_replica_count = logical_replica_count[moe_layer_idx]\n \n+    @staticmethod\n+    def uniform_random_select_experts(\n+        hidden_states: torch.Tensor,\n+        router_logits: torch.Tensor,\n+        top_k: int,\n+        indices_type: Optional[torch.dtype] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2185982451",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20498,
        "pr_file": "vllm/model_executor/layers/fused_moe/layer.py",
        "discussion_id": "2185982451",
        "commented_code": "@@ -1154,6 +1154,41 @@ def set_eplb_state(\n         self.logical_to_physical_map = logical_to_physical_map[moe_layer_idx]\n         self.logical_replica_count = logical_replica_count[moe_layer_idx]\n \n+    @staticmethod\n+    def uniform_random_select_experts(\n+        hidden_states: torch.Tensor,\n+        router_logits: torch.Tensor,\n+        top_k: int,\n+        indices_type: Optional[torch.dtype] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:",
        "comment_created_at": "2025-07-04T19:17:26+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis new static method `uniform_random_select_experts` is missing a docstring. For maintainability and clarity, please add a docstring that explains what the function does, its parameters, and what it returns. This is consistent with other methods in the class like `select_experts`.",
        "pr_file_module": null
      }
    ]
  }
]