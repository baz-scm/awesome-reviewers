[
  {
    "discussion_id": "2109771080",
    "pr_number": 18218,
    "pr_file": "vllm/model_executor/layers/mamba/mamba_mixer2.py",
    "created_at": "2025-05-27T17:25:46+00:00",
    "commented_code": "self.norm = Mixer2RMSNormGated(intermediate_size,\n                                        n_groups,\n                                        eps=rms_norm_eps)\n+        self.conv_in_triton = self.is_conv_in_Triton()\n+\n+    def forward_cuda(self, *args, **kwargs):\n+        if self.conv_in_triton:\n+            return self.forward_triton(*args, **kwargs)\n+        else:\n+            return self.forward_cuda_split(*args, **kwargs)\n+\n+    def is_conv_in_Triton(self):\n+        import os\n+        path = os.environ.get(\"VLLM_USE_TRITON_CONV1D\", None)\n+        if path is not None:\n+            print(\"mamba_mixer2 - VLLM_USE_TRITON_CONV1D\")\n+            return True\n+        return False\n \n     def forward_native(self, hidden_states: torch.Tensor,\n                        conv_state: torch.Tensor, ssm_state: torch.Tensor):\n         pass\n \n-    def forward_cuda(\n+    def forward_triton(\n+        self,\n+        hidden_states: torch.Tensor,\n+        mamba_cache_params: MambaCacheParams,\n+        mamba2_metadata: Mamba2Metadata,\n+    ):",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2109771080",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 18218,
        "pr_file": "vllm/model_executor/layers/mamba/mamba_mixer2.py",
        "discussion_id": "2109771080",
        "commented_code": "@@ -374,12 +377,335 @@ def __init__(self,\n         self.norm = Mixer2RMSNormGated(intermediate_size,\n                                        n_groups,\n                                        eps=rms_norm_eps)\n+        self.conv_in_triton = self.is_conv_in_Triton()\n+\n+    def forward_cuda(self, *args, **kwargs):\n+        if self.conv_in_triton:\n+            return self.forward_triton(*args, **kwargs)\n+        else:\n+            return self.forward_cuda_split(*args, **kwargs)\n+\n+    def is_conv_in_Triton(self):\n+        import os\n+        path = os.environ.get(\"VLLM_USE_TRITON_CONV1D\", None)\n+        if path is not None:\n+            print(\"mamba_mixer2 - VLLM_USE_TRITON_CONV1D\")\n+            return True\n+        return False\n \n     def forward_native(self, hidden_states: torch.Tensor,\n                        conv_state: torch.Tensor, ssm_state: torch.Tensor):\n         pass\n \n-    def forward_cuda(\n+    def forward_triton(\n+        self,\n+        hidden_states: torch.Tensor,\n+        mamba_cache_params: MambaCacheParams,\n+        mamba2_metadata: Mamba2Metadata,\n+    ):",
        "comment_created_at": "2025-05-27T17:25:46+00:00",
        "comment_author": "tlrmchlsmth",
        "comment_body": "This is adding a lot of duplicated code and logic between `forward_cuda` and `forward_triton`. Could you please change it so that the dispatching between CUDA and triton implementations of `causal_conv1d` happens at a lower level?",
        "pr_file_module": null
      },
      {
        "comment_id": "2127486865",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 18218,
        "pr_file": "vllm/model_executor/layers/mamba/mamba_mixer2.py",
        "discussion_id": "2109771080",
        "commented_code": "@@ -374,12 +377,335 @@ def __init__(self,\n         self.norm = Mixer2RMSNormGated(intermediate_size,\n                                        n_groups,\n                                        eps=rms_norm_eps)\n+        self.conv_in_triton = self.is_conv_in_Triton()\n+\n+    def forward_cuda(self, *args, **kwargs):\n+        if self.conv_in_triton:\n+            return self.forward_triton(*args, **kwargs)\n+        else:\n+            return self.forward_cuda_split(*args, **kwargs)\n+\n+    def is_conv_in_Triton(self):\n+        import os\n+        path = os.environ.get(\"VLLM_USE_TRITON_CONV1D\", None)\n+        if path is not None:\n+            print(\"mamba_mixer2 - VLLM_USE_TRITON_CONV1D\")\n+            return True\n+        return False\n \n     def forward_native(self, hidden_states: torch.Tensor,\n                        conv_state: torch.Tensor, ssm_state: torch.Tensor):\n         pass\n \n-    def forward_cuda(\n+    def forward_triton(\n+        self,\n+        hidden_states: torch.Tensor,\n+        mamba_cache_params: MambaCacheParams,\n+        mamba2_metadata: Mamba2Metadata,\n+    ):",
        "comment_created_at": "2025-06-04T21:41:53+00:00",
        "comment_author": "tlrmchlsmth",
        "comment_body": "This comment still hasn't been addressed.",
        "pr_file_module": null
      },
      {
        "comment_id": "2127827849",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 18218,
        "pr_file": "vllm/model_executor/layers/mamba/mamba_mixer2.py",
        "discussion_id": "2109771080",
        "commented_code": "@@ -374,12 +377,335 @@ def __init__(self,\n         self.norm = Mixer2RMSNormGated(intermediate_size,\n                                        n_groups,\n                                        eps=rms_norm_eps)\n+        self.conv_in_triton = self.is_conv_in_Triton()\n+\n+    def forward_cuda(self, *args, **kwargs):\n+        if self.conv_in_triton:\n+            return self.forward_triton(*args, **kwargs)\n+        else:\n+            return self.forward_cuda_split(*args, **kwargs)\n+\n+    def is_conv_in_Triton(self):\n+        import os\n+        path = os.environ.get(\"VLLM_USE_TRITON_CONV1D\", None)\n+        if path is not None:\n+            print(\"mamba_mixer2 - VLLM_USE_TRITON_CONV1D\")\n+            return True\n+        return False\n \n     def forward_native(self, hidden_states: torch.Tensor,\n                        conv_state: torch.Tensor, ssm_state: torch.Tensor):\n         pass\n \n-    def forward_cuda(\n+    def forward_triton(\n+        self,\n+        hidden_states: torch.Tensor,\n+        mamba_cache_params: MambaCacheParams,\n+        mamba2_metadata: Mamba2Metadata,\n+    ):",
        "comment_created_at": "2025-06-05T02:36:55+00:00",
        "comment_author": "thoangtrvn",
        "comment_body": "ah sorry I missed this. I updated the code to merge into one and migrate some code to a lower level. @tlrmchlsmth ",
        "pr_file_module": null
      }
    ]
  }
]