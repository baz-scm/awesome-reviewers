[
  {
    "discussion_id": "2190335315",
    "pr_number": 20577,
    "pr_file": "vllm/model_executor/models/prithvi_geospatial_mae.py",
    "created_at": "2025-07-07T14:55:53+00:00",
    "commented_code": "if not isinstance(pixel_values, torch.Tensor):\n             raise ValueError(f\"Incorrect type of pixel_values. \"\n                              f\"Got type: {type(pixel_values)}\")\n-        pixel_values = torch.unbind(pixel_values, dim=0)[0]\n+        # pixel_values = torch.unbind(pixel_values, dim=0)[0]",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2190335315",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20577,
        "pr_file": "vllm/model_executor/models/prithvi_geospatial_mae.py",
        "discussion_id": "2190335315",
        "commented_code": "@@ -169,7 +184,7 @@ def _parse_and_validate_multimodal_data(\n         if not isinstance(pixel_values, torch.Tensor):\n             raise ValueError(f\"Incorrect type of pixel_values. \"\n                              f\"Got type: {type(pixel_values)}\")\n-        pixel_values = torch.unbind(pixel_values, dim=0)[0]\n+        # pixel_values = torch.unbind(pixel_values, dim=0)[0]",
        "comment_created_at": "2025-07-07T14:55:53+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis line is commented out. If it's no longer needed, consider removing it to reduce code clutter and improve readability. If it's temporarily disabled for debugging, add a comment explaining why and when it should be re-enabled.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2173093200",
    "pr_number": 20206,
    "pr_file": "vllm/utils.py",
    "created_at": "2025-06-28T02:40:39+00:00",
    "commented_code": "except Exception:\n         # Fallback to PKG-INFO to load the package info, needed by the doc gen.\n         return Version(importlib.metadata.version('torch')) >= Version(target)\n+\n+\n+# class ArgParserUtils:\n+    \n+#     @staticmethod\n+#     def parse_type(return_type: Callable[[str], T]) -> Callable[[str], T]:\n+\n+#         def _parse_type(val: str) -> T:\n+#             try:\n+#                 if return_type is json.loads and not re.match(\"^{.*}$\", val):\n+#                     return cast(T, nullable_kvs(val))\n+#                 return return_type(val)\n+#             except ValueError as e:\n+#                 raise argparse.ArgumentTypeError(\n+#                     f\"Value {val} cannot be converted to {return_type}.\") from e\n+\n+#         return _parse_type\n+\n+\n+#     @staticmethod\n+#     def optional_type(\n+#             return_type: Callable[[str], T]) -> Callable[[str], Optional[T]]:\n+\n+#         def _optional_type(val: str) -> Optional[T]:\n+#             if val == \"\" or val == \"None\":\n+#                 return None\n+#             return parse_type(return_type)(val)\n+\n+#         return _optional_type\n+\n+\n+#     @staticmethod\n+#     def union_dict_and_str(val: str) -> Optional[Union[str, dict[str, str]]]:\n+#         if not re.match(\"^{.*}$\", val):\n+#             return str(val)\n+#         return optional_type(json.loads)(val)\n+\n+\n+#     @deprecated(\n+#         \"Passing a JSON argument as a string containing comma separated key=value \"\n+#         \"pairs is deprecated. This will be removed in v0.10.0. Please use a JSON \"\n+#         \"string instead.\")\n+#     def nullable_kvs(val: str) -> dict[str, int]:\n+#         \"\"\"Parses a string containing comma separate key [str] to value [int]\n+#         pairs into a dictionary.\n+\n+#         Args:\n+#             val: String value to be parsed.\n+\n+#         Returns:\n+#             Dictionary with parsed values.\n+#         \"\"\"\n+#         out_dict: dict[str, int] = {}\n+#         for item in val.split(\",\"):\n+#             kv_parts = [part.lower().strip() for part in item.split(\"=\")]\n+#             if len(kv_parts) != 2:\n+#                 raise argparse.ArgumentTypeError(\n+#                     \"Each item should be in the form KEY=VALUE\")\n+#             key, value = kv_parts\n+\n+#             try:\n+#                 parsed_value = int(value)\n+#             except ValueError as exc:\n+#                 msg = f\"Failed to parse value of item {key}={value}\"\n+#                 raise argparse.ArgumentTypeError(msg) from exc\n+\n+#             if key in out_dict and out_dict[key] != parsed_value:\n+#                 raise argparse.ArgumentTypeError(\n+#                     f\"Conflicting values specified for key: {key}\")\n+#             out_dict[key] = parsed_value\n+\n+#         return out_dict\n+\n+\n+#     def is_type(type_hint: TypeHint, type: TypeHintT) -> TypeIs[TypeHintT]:\n+#         \"\"\"Check if the type hint is a specific type.\"\"\"\n+#         return type_hint is type or get_origin(type_hint) is type\n+\n+\n+#     def contains_type(type_hints: set[TypeHint], type: TypeHintT) -> bool:\n+#         \"\"\"Check if the type hints contain a specific type.\"\"\"\n+#         return any(is_type(type_hint, type) for type_hint in type_hints)\n+\n+\n+#     def get_type(type_hints: set[TypeHint], type: TypeHintT) -> TypeHintT:\n+#         \"\"\"Get the specific type from the type hints.\"\"\"\n+#         return next((th for th in type_hints if is_type(th, type)), None)\n+\n+\n+#     def literal_to_kwargs(type_hints: set[TypeHint]) -> dict[str, Any]:\n+#         \"\"\"Convert Literal type hints to argparse kwargs.\"\"\"\n+#         type_hint = get_type(type_hints, Literal)\n+#         choices = get_args(type_hint)\n+#         choice_type = type(choices[0])\n+#         if not all(isinstance(choice, choice_type) for choice in choices):\n+#             raise ValueError(\n+#                 \"All choices must be of the same type. \"\n+#                 f\"Got {choices} with types {[type(c) for c in choices]}\")\n+#         return {\"type\": choice_type, \"choices\": sorted(choices)}\n+\n+\n+#     def is_not_builtin(type_hint: TypeHint) -> bool:\n+#         \"\"\"Check if the class is not a built-in type.\"\"\"\n+#         return type_hint.__module__ != \"builtins\"\n+\n+\n+#     def get_type_hints(type_hint: TypeHint) -> set[TypeHint]:\n+#         \"\"\"Extract type hints from Annotated or Union type hints.\"\"\"\n+#         type_hints: set[TypeHint] = set()\n+#         origin = get_origin(type_hint)\n+#         args = get_args(type_hint)\n+\n+#         if origin is Annotated:\n+#             type_hints.update(get_type_hints(args[0]))\n+#         elif origin is Union:\n+#             for arg in args:\n+#                 type_hints.update(get_type_hints(arg))\n+#         else:\n+#             type_hints.add(type_hint)\n+\n+#         return type_hints\n+\n+\n+#     def get_kwargs(cls: ConfigType) -> dict[str, Any]:\n+#         cls_docs = get_attr_docs(cls)\n+#         kwargs = {}\n+#         for field in fields(cls):\n+#             # Get the set of possible types for the field\n+#             type_hints: set[TypeHint] = get_type_hints(field.type)\n+\n+#             # If the field is a dataclass, we can use the model_validate_json\n+#             generator = (th for th in type_hints if is_dataclass(th))\n+#             dataclass_cls = next(generator, None)\n+\n+#             # Get the default value of the field\n+#             if field.default is not MISSING:\n+#                 default = field.default\n+#             elif field.default_factory is not MISSING:\n+#                 default = field.default_factory()\n+\n+#             # Get the help text for the field\n+#             name = field.name\n+#             help = cls_docs[name].strip()\n+#             # Escape % for argparse\n+#             help = help.replace(\"%\", \"%%\")\n+\n+#             # Initialise the kwargs dictionary for the field\n+#             kwargs[name] = {\"default\": default, \"help\": help}\n+\n+#             # Set other kwargs based on the type hints\n+#             json_tip = \"\"\"\n\nShould either be a valid JSON string or JSON keys\n+#             passed individually. For example, the following sets of arguments are\n+#             equivalent:\n\n\n+#             - `--json-arg '{\"key1\": \"value1\", \"key2\": {\"key3\": \"value2\"}}'`\n\n+#             - `--json-arg.key1 value1 --json-arg.key2.key3 value2`\n\n\"\"\"\n+#             if dataclass_cls is not None:\n+\n+#                 def parse_dataclass(val: str, cls=dataclass_cls) -> Any:\n+#                     try:\n+#                         if hasattr(cls, \"from_cli\"):\n+#                             return cls.from_cli(val)\n+#                         return TypeAdapter(cls).validate_json(val)\n+#                     except ValidationError as e:\n+#                         raise argparse.ArgumentTypeError(repr(e)) from e\n+\n+#                 kwargs[name][\"type\"] = parse_dataclass\n+#                 kwargs[name][\"help\"] += json_tip\n+#             elif contains_type(type_hints, bool):\n+#                 # Creates --no-<name> and --<name> flags\n+#                 kwargs[name][\"action\"] = argparse.BooleanOptionalAction\n+#             elif contains_type(type_hints, Literal):\n+#                 kwargs[name].update(literal_to_kwargs(type_hints))\n+#             elif contains_type(type_hints, tuple):\n+#                 type_hint = get_type(type_hints, tuple)\n+#                 types = get_args(type_hint)\n+#                 tuple_type = types[0]\n+#                 assert all(t is tuple_type for t in types if t is not Ellipsis), (\n+#                     \"All non-Ellipsis tuple elements must be of the same \"\n+#                     f\"type. Got {types}.\")\n+#                 kwargs[name][\"type\"] = tuple_type\n+#                 kwargs[name][\"nargs\"] = \"+\" if Ellipsis in types else len(types)\n+#             elif contains_type(type_hints, list):\n+#                 type_hint = get_type(type_hints, list)\n+#                 types = get_args(type_hint)\n+#                 assert len(types) == 1, (\n+#                     \"List type must have exactly one type. Got \"\n+#                     f\"{type_hint} with types {types}\")\n+#                 kwargs[name][\"type\"] = types[0]\n+#                 kwargs[name][\"nargs\"] = \"+\"\n+#             elif contains_type(type_hints, int):\n+#                 kwargs[name][\"type\"] = int\n+#                 # Special case for large integers\n+#                 if name in {\"max_model_len\", \"max_num_batched_tokens\"}:\n+#                     kwargs[name][\"type\"] = human_readable_int\n+#             elif contains_type(type_hints, float):\n+#                 kwargs[name][\"type\"] = float\n+#             elif (contains_type(type_hints, dict)\n+#                 and (contains_type(type_hints, str)\n+#                     or any(is_not_builtin(th) for th in type_hints))):\n+#                 kwargs[name][\"type\"] = union_dict_and_str\n+#             elif contains_type(type_hints, dict):\n+#                 kwargs[name][\"type\"] = parse_type(json.loads)\n+#                 kwargs[name][\"help\"] += json_tip\n+#             elif (contains_type(type_hints, str)\n+#                 or any(is_not_builtin(th) for th in type_hints)):\n+#                 kwargs[name][\"type\"] = str\n+#             else:\n+#                 raise ValueError(\n+#                     f\"Unsupported type {type_hints} for argument {name}.\")\n+\n+#             # If the type hint was a sequence of literals, use the helper function\n+#             # to update the type and choices\n+#             if get_origin(kwargs[name].get(\"type\")) is Literal:\n+#                 kwargs[name].update(literal_to_kwargs({kwargs[name][\"type\"]}))\n+\n+#             # If None is in type_hints, make the argument optional.\n+#             # But not if it's a bool, argparse will handle this better.\n+#             if type(None) in type_hints and not contains_type(type_hints, bool):\n+#                 kwargs[name][\"type\"] = optional_type(kwargs[name][\"type\"])\n+#                 if kwargs[name].get(\"choices\"):\n+#                     kwargs[name][\"choices\"].append(\"None\")\n+#         return kwargs",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2173093200",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20206,
        "pr_file": "vllm/utils.py",
        "discussion_id": "2173093200",
        "commented_code": "@@ -2917,3 +2917,225 @@ def is_torch_equal_or_newer(target: str) -> bool:\n     except Exception:\n         # Fallback to PKG-INFO to load the package info, needed by the doc gen.\n         return Version(importlib.metadata.version('torch')) >= Version(target)\n+\n+\n+# class ArgParserUtils:\n+    \n+#     @staticmethod\n+#     def parse_type(return_type: Callable[[str], T]) -> Callable[[str], T]:\n+\n+#         def _parse_type(val: str) -> T:\n+#             try:\n+#                 if return_type is json.loads and not re.match(\"^{.*}$\", val):\n+#                     return cast(T, nullable_kvs(val))\n+#                 return return_type(val)\n+#             except ValueError as e:\n+#                 raise argparse.ArgumentTypeError(\n+#                     f\"Value {val} cannot be converted to {return_type}.\") from e\n+\n+#         return _parse_type\n+\n+\n+#     @staticmethod\n+#     def optional_type(\n+#             return_type: Callable[[str], T]) -> Callable[[str], Optional[T]]:\n+\n+#         def _optional_type(val: str) -> Optional[T]:\n+#             if val == \"\" or val == \"None\":\n+#                 return None\n+#             return parse_type(return_type)(val)\n+\n+#         return _optional_type\n+\n+\n+#     @staticmethod\n+#     def union_dict_and_str(val: str) -> Optional[Union[str, dict[str, str]]]:\n+#         if not re.match(\"^{.*}$\", val):\n+#             return str(val)\n+#         return optional_type(json.loads)(val)\n+\n+\n+#     @deprecated(\n+#         \"Passing a JSON argument as a string containing comma separated key=value \"\n+#         \"pairs is deprecated. This will be removed in v0.10.0. Please use a JSON \"\n+#         \"string instead.\")\n+#     def nullable_kvs(val: str) -> dict[str, int]:\n+#         \"\"\"Parses a string containing comma separate key [str] to value [int]\n+#         pairs into a dictionary.\n+\n+#         Args:\n+#             val: String value to be parsed.\n+\n+#         Returns:\n+#             Dictionary with parsed values.\n+#         \"\"\"\n+#         out_dict: dict[str, int] = {}\n+#         for item in val.split(\",\"):\n+#             kv_parts = [part.lower().strip() for part in item.split(\"=\")]\n+#             if len(kv_parts) != 2:\n+#                 raise argparse.ArgumentTypeError(\n+#                     \"Each item should be in the form KEY=VALUE\")\n+#             key, value = kv_parts\n+\n+#             try:\n+#                 parsed_value = int(value)\n+#             except ValueError as exc:\n+#                 msg = f\"Failed to parse value of item {key}={value}\"\n+#                 raise argparse.ArgumentTypeError(msg) from exc\n+\n+#             if key in out_dict and out_dict[key] != parsed_value:\n+#                 raise argparse.ArgumentTypeError(\n+#                     f\"Conflicting values specified for key: {key}\")\n+#             out_dict[key] = parsed_value\n+\n+#         return out_dict\n+\n+\n+#     def is_type(type_hint: TypeHint, type: TypeHintT) -> TypeIs[TypeHintT]:\n+#         \"\"\"Check if the type hint is a specific type.\"\"\"\n+#         return type_hint is type or get_origin(type_hint) is type\n+\n+\n+#     def contains_type(type_hints: set[TypeHint], type: TypeHintT) -> bool:\n+#         \"\"\"Check if the type hints contain a specific type.\"\"\"\n+#         return any(is_type(type_hint, type) for type_hint in type_hints)\n+\n+\n+#     def get_type(type_hints: set[TypeHint], type: TypeHintT) -> TypeHintT:\n+#         \"\"\"Get the specific type from the type hints.\"\"\"\n+#         return next((th for th in type_hints if is_type(th, type)), None)\n+\n+\n+#     def literal_to_kwargs(type_hints: set[TypeHint]) -> dict[str, Any]:\n+#         \"\"\"Convert Literal type hints to argparse kwargs.\"\"\"\n+#         type_hint = get_type(type_hints, Literal)\n+#         choices = get_args(type_hint)\n+#         choice_type = type(choices[0])\n+#         if not all(isinstance(choice, choice_type) for choice in choices):\n+#             raise ValueError(\n+#                 \"All choices must be of the same type. \"\n+#                 f\"Got {choices} with types {[type(c) for c in choices]}\")\n+#         return {\"type\": choice_type, \"choices\": sorted(choices)}\n+\n+\n+#     def is_not_builtin(type_hint: TypeHint) -> bool:\n+#         \"\"\"Check if the class is not a built-in type.\"\"\"\n+#         return type_hint.__module__ != \"builtins\"\n+\n+\n+#     def get_type_hints(type_hint: TypeHint) -> set[TypeHint]:\n+#         \"\"\"Extract type hints from Annotated or Union type hints.\"\"\"\n+#         type_hints: set[TypeHint] = set()\n+#         origin = get_origin(type_hint)\n+#         args = get_args(type_hint)\n+\n+#         if origin is Annotated:\n+#             type_hints.update(get_type_hints(args[0]))\n+#         elif origin is Union:\n+#             for arg in args:\n+#                 type_hints.update(get_type_hints(arg))\n+#         else:\n+#             type_hints.add(type_hint)\n+\n+#         return type_hints\n+\n+\n+#     def get_kwargs(cls: ConfigType) -> dict[str, Any]:\n+#         cls_docs = get_attr_docs(cls)\n+#         kwargs = {}\n+#         for field in fields(cls):\n+#             # Get the set of possible types for the field\n+#             type_hints: set[TypeHint] = get_type_hints(field.type)\n+\n+#             # If the field is a dataclass, we can use the model_validate_json\n+#             generator = (th for th in type_hints if is_dataclass(th))\n+#             dataclass_cls = next(generator, None)\n+\n+#             # Get the default value of the field\n+#             if field.default is not MISSING:\n+#                 default = field.default\n+#             elif field.default_factory is not MISSING:\n+#                 default = field.default_factory()\n+\n+#             # Get the help text for the field\n+#             name = field.name\n+#             help = cls_docs[name].strip()\n+#             # Escape % for argparse\n+#             help = help.replace(\"%\", \"%%\")\n+\n+#             # Initialise the kwargs dictionary for the field\n+#             kwargs[name] = {\"default\": default, \"help\": help}\n+\n+#             # Set other kwargs based on the type hints\n+#             json_tip = \"\"\"\\n\\nShould either be a valid JSON string or JSON keys\n+#             passed individually. For example, the following sets of arguments are\n+#             equivalent:\\n\\n\n+#             - `--json-arg '{\"key1\": \"value1\", \"key2\": {\"key3\": \"value2\"}}'`\\n\n+#             - `--json-arg.key1 value1 --json-arg.key2.key3 value2`\\n\\n\"\"\"\n+#             if dataclass_cls is not None:\n+\n+#                 def parse_dataclass(val: str, cls=dataclass_cls) -> Any:\n+#                     try:\n+#                         if hasattr(cls, \"from_cli\"):\n+#                             return cls.from_cli(val)\n+#                         return TypeAdapter(cls).validate_json(val)\n+#                     except ValidationError as e:\n+#                         raise argparse.ArgumentTypeError(repr(e)) from e\n+\n+#                 kwargs[name][\"type\"] = parse_dataclass\n+#                 kwargs[name][\"help\"] += json_tip\n+#             elif contains_type(type_hints, bool):\n+#                 # Creates --no-<name> and --<name> flags\n+#                 kwargs[name][\"action\"] = argparse.BooleanOptionalAction\n+#             elif contains_type(type_hints, Literal):\n+#                 kwargs[name].update(literal_to_kwargs(type_hints))\n+#             elif contains_type(type_hints, tuple):\n+#                 type_hint = get_type(type_hints, tuple)\n+#                 types = get_args(type_hint)\n+#                 tuple_type = types[0]\n+#                 assert all(t is tuple_type for t in types if t is not Ellipsis), (\n+#                     \"All non-Ellipsis tuple elements must be of the same \"\n+#                     f\"type. Got {types}.\")\n+#                 kwargs[name][\"type\"] = tuple_type\n+#                 kwargs[name][\"nargs\"] = \"+\" if Ellipsis in types else len(types)\n+#             elif contains_type(type_hints, list):\n+#                 type_hint = get_type(type_hints, list)\n+#                 types = get_args(type_hint)\n+#                 assert len(types) == 1, (\n+#                     \"List type must have exactly one type. Got \"\n+#                     f\"{type_hint} with types {types}\")\n+#                 kwargs[name][\"type\"] = types[0]\n+#                 kwargs[name][\"nargs\"] = \"+\"\n+#             elif contains_type(type_hints, int):\n+#                 kwargs[name][\"type\"] = int\n+#                 # Special case for large integers\n+#                 if name in {\"max_model_len\", \"max_num_batched_tokens\"}:\n+#                     kwargs[name][\"type\"] = human_readable_int\n+#             elif contains_type(type_hints, float):\n+#                 kwargs[name][\"type\"] = float\n+#             elif (contains_type(type_hints, dict)\n+#                 and (contains_type(type_hints, str)\n+#                     or any(is_not_builtin(th) for th in type_hints))):\n+#                 kwargs[name][\"type\"] = union_dict_and_str\n+#             elif contains_type(type_hints, dict):\n+#                 kwargs[name][\"type\"] = parse_type(json.loads)\n+#                 kwargs[name][\"help\"] += json_tip\n+#             elif (contains_type(type_hints, str)\n+#                 or any(is_not_builtin(th) for th in type_hints)):\n+#                 kwargs[name][\"type\"] = str\n+#             else:\n+#                 raise ValueError(\n+#                     f\"Unsupported type {type_hints} for argument {name}.\")\n+\n+#             # If the type hint was a sequence of literals, use the helper function\n+#             # to update the type and choices\n+#             if get_origin(kwargs[name].get(\"type\")) is Literal:\n+#                 kwargs[name].update(literal_to_kwargs({kwargs[name][\"type\"]}))\n+\n+#             # If None is in type_hints, make the argument optional.\n+#             # But not if it's a bool, argparse will handle this better.\n+#             if type(None) in type_hints and not contains_type(type_hints, bool):\n+#                 kwargs[name][\"type\"] = optional_type(kwargs[name][\"type\"])\n+#                 if kwargs[name].get(\"choices\"):\n+#                     kwargs[name][\"choices\"].append(\"None\")\n+#         return kwargs",
        "comment_created_at": "2025-06-28T02:40:39+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis large block of commented-out code seems to be leftover from the refactoring. It should be removed to improve code clarity and maintainability.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2109771080",
    "pr_number": 18218,
    "pr_file": "vllm/model_executor/layers/mamba/mamba_mixer2.py",
    "created_at": "2025-05-27T17:25:46+00:00",
    "commented_code": "self.norm = Mixer2RMSNormGated(intermediate_size,\n                                        n_groups,\n                                        eps=rms_norm_eps)\n+        self.conv_in_triton = self.is_conv_in_Triton()\n+\n+    def forward_cuda(self, *args, **kwargs):\n+        if self.conv_in_triton:\n+            return self.forward_triton(*args, **kwargs)\n+        else:\n+            return self.forward_cuda_split(*args, **kwargs)\n+\n+    def is_conv_in_Triton(self):\n+        import os\n+        path = os.environ.get(\"VLLM_USE_TRITON_CONV1D\", None)\n+        if path is not None:\n+            print(\"mamba_mixer2 - VLLM_USE_TRITON_CONV1D\")\n+            return True\n+        return False\n \n     def forward_native(self, hidden_states: torch.Tensor,\n                        conv_state: torch.Tensor, ssm_state: torch.Tensor):\n         pass\n \n-    def forward_cuda(\n+    def forward_triton(\n+        self,\n+        hidden_states: torch.Tensor,\n+        mamba_cache_params: MambaCacheParams,\n+        mamba2_metadata: Mamba2Metadata,\n+    ):",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2109771080",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 18218,
        "pr_file": "vllm/model_executor/layers/mamba/mamba_mixer2.py",
        "discussion_id": "2109771080",
        "commented_code": "@@ -374,12 +377,335 @@ def __init__(self,\n         self.norm = Mixer2RMSNormGated(intermediate_size,\n                                        n_groups,\n                                        eps=rms_norm_eps)\n+        self.conv_in_triton = self.is_conv_in_Triton()\n+\n+    def forward_cuda(self, *args, **kwargs):\n+        if self.conv_in_triton:\n+            return self.forward_triton(*args, **kwargs)\n+        else:\n+            return self.forward_cuda_split(*args, **kwargs)\n+\n+    def is_conv_in_Triton(self):\n+        import os\n+        path = os.environ.get(\"VLLM_USE_TRITON_CONV1D\", None)\n+        if path is not None:\n+            print(\"mamba_mixer2 - VLLM_USE_TRITON_CONV1D\")\n+            return True\n+        return False\n \n     def forward_native(self, hidden_states: torch.Tensor,\n                        conv_state: torch.Tensor, ssm_state: torch.Tensor):\n         pass\n \n-    def forward_cuda(\n+    def forward_triton(\n+        self,\n+        hidden_states: torch.Tensor,\n+        mamba_cache_params: MambaCacheParams,\n+        mamba2_metadata: Mamba2Metadata,\n+    ):",
        "comment_created_at": "2025-05-27T17:25:46+00:00",
        "comment_author": "tlrmchlsmth",
        "comment_body": "This is adding a lot of duplicated code and logic between `forward_cuda` and `forward_triton`. Could you please change it so that the dispatching between CUDA and triton implementations of `causal_conv1d` happens at a lower level?",
        "pr_file_module": null
      },
      {
        "comment_id": "2127486865",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 18218,
        "pr_file": "vllm/model_executor/layers/mamba/mamba_mixer2.py",
        "discussion_id": "2109771080",
        "commented_code": "@@ -374,12 +377,335 @@ def __init__(self,\n         self.norm = Mixer2RMSNormGated(intermediate_size,\n                                        n_groups,\n                                        eps=rms_norm_eps)\n+        self.conv_in_triton = self.is_conv_in_Triton()\n+\n+    def forward_cuda(self, *args, **kwargs):\n+        if self.conv_in_triton:\n+            return self.forward_triton(*args, **kwargs)\n+        else:\n+            return self.forward_cuda_split(*args, **kwargs)\n+\n+    def is_conv_in_Triton(self):\n+        import os\n+        path = os.environ.get(\"VLLM_USE_TRITON_CONV1D\", None)\n+        if path is not None:\n+            print(\"mamba_mixer2 - VLLM_USE_TRITON_CONV1D\")\n+            return True\n+        return False\n \n     def forward_native(self, hidden_states: torch.Tensor,\n                        conv_state: torch.Tensor, ssm_state: torch.Tensor):\n         pass\n \n-    def forward_cuda(\n+    def forward_triton(\n+        self,\n+        hidden_states: torch.Tensor,\n+        mamba_cache_params: MambaCacheParams,\n+        mamba2_metadata: Mamba2Metadata,\n+    ):",
        "comment_created_at": "2025-06-04T21:41:53+00:00",
        "comment_author": "tlrmchlsmth",
        "comment_body": "This comment still hasn't been addressed.",
        "pr_file_module": null
      },
      {
        "comment_id": "2127827849",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 18218,
        "pr_file": "vllm/model_executor/layers/mamba/mamba_mixer2.py",
        "discussion_id": "2109771080",
        "commented_code": "@@ -374,12 +377,335 @@ def __init__(self,\n         self.norm = Mixer2RMSNormGated(intermediate_size,\n                                        n_groups,\n                                        eps=rms_norm_eps)\n+        self.conv_in_triton = self.is_conv_in_Triton()\n+\n+    def forward_cuda(self, *args, **kwargs):\n+        if self.conv_in_triton:\n+            return self.forward_triton(*args, **kwargs)\n+        else:\n+            return self.forward_cuda_split(*args, **kwargs)\n+\n+    def is_conv_in_Triton(self):\n+        import os\n+        path = os.environ.get(\"VLLM_USE_TRITON_CONV1D\", None)\n+        if path is not None:\n+            print(\"mamba_mixer2 - VLLM_USE_TRITON_CONV1D\")\n+            return True\n+        return False\n \n     def forward_native(self, hidden_states: torch.Tensor,\n                        conv_state: torch.Tensor, ssm_state: torch.Tensor):\n         pass\n \n-    def forward_cuda(\n+    def forward_triton(\n+        self,\n+        hidden_states: torch.Tensor,\n+        mamba_cache_params: MambaCacheParams,\n+        mamba2_metadata: Mamba2Metadata,\n+    ):",
        "comment_created_at": "2025-06-05T02:36:55+00:00",
        "comment_author": "thoangtrvn",
        "comment_body": "ah sorry I missed this. I updated the code to merge into one and migrate some code to a lower level. @tlrmchlsmth ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2164654266",
    "pr_number": 20034,
    "pr_file": "vllm/v1/attention/backends/flashinfer.py",
    "created_at": "2025-06-24T18:30:48+00:00",
    "commented_code": "\"\"\"\n         assert output is not None, \"Output tensor must be provided.\"\n \n+        print(\"kv_cache.shape = {}\".format(kv_cache.shape))",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2164654266",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20034,
        "pr_file": "vllm/v1/attention/backends/flashinfer.py",
        "discussion_id": "2164654266",
        "commented_code": "@@ -564,6 +564,8 @@ def forward(\n         \"\"\"\n         assert output is not None, \"Output tensor must be provided.\"\n \n+        print(\"kv_cache.shape = {}\".format(kv_cache.shape))",
        "comment_created_at": "2025-06-24T18:30:48+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis print statement should be removed before merging. It's useful for debugging, but not for production code.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190412907",
    "pr_number": 20579,
    "pr_file": "vllm/v1/core/sched/scheduler.py",
    "created_at": "2025-07-07T15:30:50+00:00",
    "commented_code": "def _free_blocks(self, request: Request):\n         assert request.is_finished()\n+\n+        delay_free_blocks, kv_xfer_params = self._connector_finished(request)\n+        self.encoder_cache_manager.free(request)\n+        request_id = request.request_id\n+        self._cached_reqs_data.pop(request_id, None)\n+        self.finished_req_ids.add(request_id)\n+        if self.finished_req_ids_dict is not None:\n+            self.finished_req_ids_dict[request.client_index].add(request_id)\n+\n+        if not delay_free_blocks:\n+            self._free_blocks(request)\n+\n+        return kv_xfer_params\n+\n+    def _free_blocks(self, request: Request):\n+        assert request.is_finished()\n+        assert request.request_id not in self._cached_reqs_data\n         self.kv_cache_manager.free(request)\n         self.kv_cache_manager.free_block_hashes(request)\n         del self.requests[request.request_id]",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2190412907",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20579,
        "pr_file": "vllm/v1/core/sched/scheduler.py",
        "discussion_id": "2190412907",
        "commented_code": "@@ -991,6 +1004,23 @@ def _free_request(self, request: Request) -> Optional[dict[str, Any]]:\n \n     def _free_blocks(self, request: Request):\n         assert request.is_finished()\n+\n+        delay_free_blocks, kv_xfer_params = self._connector_finished(request)\n+        self.encoder_cache_manager.free(request)\n+        request_id = request.request_id\n+        self._cached_reqs_data.pop(request_id, None)\n+        self.finished_req_ids.add(request_id)\n+        if self.finished_req_ids_dict is not None:\n+            self.finished_req_ids_dict[request.client_index].add(request_id)\n+\n+        if not delay_free_blocks:\n+            self._free_blocks(request)\n+\n+        return kv_xfer_params\n+\n+    def _free_blocks(self, request: Request):\n+        assert request.is_finished()\n+        assert request.request_id not in self._cached_reqs_data\n         self.kv_cache_manager.free(request)\n         self.kv_cache_manager.free_block_hashes(request)\n         del self.requests[request.request_id]",
        "comment_created_at": "2025-07-07T15:30:50+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe method `_free_blocks` is defined twice. The first definition on line 992 seems to contain the new logic. This second definition is the old implementation and will cause a syntax error. Please remove it.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190412914",
    "pr_number": 20579,
    "pr_file": "vllm/v1/worker/gpu_model_runner.py",
    "created_at": "2025-07-07T15:30:50+00:00",
    "commented_code": "return cu_num_tokens, arange\n \n+    def _get_cumsum_and_arange(\n+        self,\n+        num_tokens: np.ndarray,\n+        cumsum_dtype: Optional[np.dtype] = None,\n+    ) -> tuple[np.ndarray, np.ndarray]:\n+        \"\"\"Get the cumulative sum and batched arange of the given array.\n+        # E.g., [2, 5, 3] -> ([2, 7, 10], [0, 1, 0, 1, 2, 3, 4, 0, 1, 2])\n+        # Equivalent to but faster than:\n+        # np.concatenate([np.arange(n) for n in num_tokens])\n+        \"\"\"\n+        # Step 1. [2, 5, 3] -> [2, 7, 10]\n+        cu_num_tokens = np.cumsum(num_tokens, dtype=cumsum_dtype)\n+        total_num_tokens = cu_num_tokens[-1]\n+        # Step 2. [2, 7, 10] -> [0, 0, 2, 2, 2, 2, 2, 7, 7, 7]\n+        cumsums_offsets = np.repeat(cu_num_tokens - num_tokens, num_tokens)\n+        # Step 3. [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n+        arange = self.arange_np[:total_num_tokens] - cumsums_offsets\n+\n+        return cu_num_tokens, arange",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2190412914",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20579,
        "pr_file": "vllm/v1/worker/gpu_model_runner.py",
        "discussion_id": "2190412914",
        "commented_code": "@@ -575,6 +576,26 @@ def _get_cumsum_and_arange(\n \n         return cu_num_tokens, arange\n \n+    def _get_cumsum_and_arange(\n+        self,\n+        num_tokens: np.ndarray,\n+        cumsum_dtype: Optional[np.dtype] = None,\n+    ) -> tuple[np.ndarray, np.ndarray]:\n+        \"\"\"Get the cumulative sum and batched arange of the given array.\n+        # E.g., [2, 5, 3] -> ([2, 7, 10], [0, 1, 0, 1, 2, 3, 4, 0, 1, 2])\n+        # Equivalent to but faster than:\n+        # np.concatenate([np.arange(n) for n in num_tokens])\n+        \"\"\"\n+        # Step 1. [2, 5, 3] -> [2, 7, 10]\n+        cu_num_tokens = np.cumsum(num_tokens, dtype=cumsum_dtype)\n+        total_num_tokens = cu_num_tokens[-1]\n+        # Step 2. [2, 7, 10] -> [0, 0, 2, 2, 2, 2, 2, 7, 7, 7]\n+        cumsums_offsets = np.repeat(cu_num_tokens - num_tokens, num_tokens)\n+        # Step 3. [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n+        arange = self.arange_np[:total_num_tokens] - cumsums_offsets\n+\n+        return cu_num_tokens, arange",
        "comment_created_at": "2025-07-07T15:30:50+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe method `_get_cumsum_and_arange` is defined twice. This will cause a syntax error. Please remove one of the definitions.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190412993",
    "pr_number": 20579,
    "pr_file": "vllm/v1/worker/gpu_model_runner.py",
    "created_at": "2025-07-07T15:30:52+00:00",
    "commented_code": "num_pad, num_tokens_across_dp = self.get_dp_padding(num_tokens)\n         num_tokens += num_pad\n \n+        # Padding for DP\n+        num_pad, num_tokens_across_dp = self.get_dp_padding(num_tokens)\n+        num_tokens += num_pad",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2190412993",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20579,
        "pr_file": "vllm/v1/worker/gpu_model_runner.py",
        "discussion_id": "2190412993",
        "commented_code": "@@ -1979,6 +2000,10 @@ def _dummy_run(\n         num_pad, num_tokens_across_dp = self.get_dp_padding(num_tokens)\n         num_tokens += num_pad\n \n+        # Padding for DP\n+        num_pad, num_tokens_across_dp = self.get_dp_padding(num_tokens)\n+        num_tokens += num_pad",
        "comment_created_at": "2025-07-07T15:30:52+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis block of code for DP padding is a duplicate of the one on lines 2000-2001. Please remove this redundant block.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2184795877",
    "pr_number": 20482,
    "pr_file": "vllm/model_executor/models/bailing_moe.py",
    "created_at": "2025-07-04T09:05:10+00:00",
    "commented_code": "+# coding=utf-8\n+\"\"\" PyTorch Bailing model. \"\"\"\n+\n+from typing import Iterable, Optional, Tuple, Union, Set\n+\n+import torch\n+from torch import nn\n+\n+from vllm.model_executor.layers.activation import get_act_fn, SiluAndMul\n+from vllm.attention import Attention, AttentionMetadata\n+from vllm.config import CacheConfig, VllmConfig\n+from vllm.model_executor.layers.fused_moe import fused_moe, FusedMoE\n+from vllm.model_executor.layers.layernorm import RMSNorm\n+from vllm.model_executor.layers.linear import (ColumnParallelLinear,\n+                                               MergedColumnParallelLinear,\n+                                               ReplicatedLinear,\n+                                               QKVParallelLinear,\n+                                               RowParallelLinear)\n+from vllm.model_executor.layers.quantization.base_config import (\n+    QuantizationConfig)\n+from vllm.model_executor.layers.rotary_embedding import get_rope\n+from vllm.model_executor.layers.sampler import Sampler\n+from vllm.model_executor.layers.vocab_parallel_embedding import (\n+    ParallelLMHead, VocabParallelEmbedding)\n+from vllm.distributed import (get_pp_group,\n+                              get_tensor_model_parallel_rank,\n+                              get_tensor_model_parallel_world_size,\n+                              tensor_model_parallel_all_reduce)\n+from vllm.model_executor.sampling_metadata import SamplingMetadata\n+from vllm.model_executor.model_loader.weight_utils import default_weight_loader\n+from vllm.model_executor.utils import set_weight_attrs\n+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler\n+from vllm.sequence import IntermediateTensors\n+from vllm.transformers_utils.configs.bailing_moe import BailingMoeConfig\n+from vllm.model_executor.layers.logits_processor import LogitsProcessor\n+from vllm.config import LoRAConfig",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2184795877",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20482,
        "pr_file": "vllm/model_executor/models/bailing_moe.py",
        "discussion_id": "2184795877",
        "commented_code": "@@ -0,0 +1,520 @@\n+# coding=utf-8\n+\"\"\" PyTorch Bailing model. \"\"\"\n+\n+from typing import Iterable, Optional, Tuple, Union, Set\n+\n+import torch\n+from torch import nn\n+\n+from vllm.model_executor.layers.activation import get_act_fn, SiluAndMul\n+from vllm.attention import Attention, AttentionMetadata\n+from vllm.config import CacheConfig, VllmConfig\n+from vllm.model_executor.layers.fused_moe import fused_moe, FusedMoE\n+from vllm.model_executor.layers.layernorm import RMSNorm\n+from vllm.model_executor.layers.linear import (ColumnParallelLinear,\n+                                               MergedColumnParallelLinear,\n+                                               ReplicatedLinear,\n+                                               QKVParallelLinear,\n+                                               RowParallelLinear)\n+from vllm.model_executor.layers.quantization.base_config import (\n+    QuantizationConfig)\n+from vllm.model_executor.layers.rotary_embedding import get_rope\n+from vllm.model_executor.layers.sampler import Sampler\n+from vllm.model_executor.layers.vocab_parallel_embedding import (\n+    ParallelLMHead, VocabParallelEmbedding)\n+from vllm.distributed import (get_pp_group,\n+                              get_tensor_model_parallel_rank,\n+                              get_tensor_model_parallel_world_size,\n+                              tensor_model_parallel_all_reduce)\n+from vllm.model_executor.sampling_metadata import SamplingMetadata\n+from vllm.model_executor.model_loader.weight_utils import default_weight_loader\n+from vllm.model_executor.utils import set_weight_attrs\n+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler\n+from vllm.sequence import IntermediateTensors\n+from vllm.transformers_utils.configs.bailing_moe import BailingMoeConfig\n+from vllm.model_executor.layers.logits_processor import LogitsProcessor\n+from vllm.config import LoRAConfig",
        "comment_created_at": "2025-07-04T09:05:10+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThere are several unused imports in this file. Please remove them to keep the code clean and reduce clutter.\n\nThe unused imports are:\n- `get_act_fn` from `vllm.model_executor.layers.activation`\n- `fused_moe` from `vllm.model_executor.layers.fused_moe`\n- `ColumnParallelLinear` from `vllm.model_executor.layers.linear`\n- `set_weight_attrs` from `vllm.model_executor.utils`\n- `LoRAConfig` from `vllm.config`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189374579",
    "pr_number": 20554,
    "pr_file": "vllm/v1/worker/cpu_worker.py",
    "created_at": "2025-07-07T08:38:19+00:00",
    "commented_code": "\"fallback to no thread-binding. To get better performance,\"\n                 \"please try to manually bind threads.\")\n         return rank_to_cpus\n+\n+\n+    def get_cpus_id_binding_based_on_numa_nodes_ppc64le(self) -> str:\n+        \"\"\"\n+        Power (ppc64le) specific: Selects a subset of threads per core for \n+        each NUMA node.This is robust to SMT mode (SMT-8, SMT-4, etc) \n+        because the OS only exposes available threads.This maximizes \n+        performance by avoiding oversubscription of logical CPUs on Power.\n+        \"\"\"\n+\n+        def select_threads_per_power_core(node_cpu_ids):\n+            return [cpu for cpu in node_cpu_ids if cpu % 8 < 4]\n+\n+        rank_to_cpus = self.local_omp_cpuid\n+        world_size = self.vllm_config.parallel_config.world_size\n+        libnuma_found = util.find_spec(\"numa\") is not None\n+        psutil_found = util.find_spec(\"psutil\") is not None\n+        if libnuma_found and psutil_found:\n+            import psutil\n+            from numa import info\n+            cpus_allow_list = psutil.Process().cpu_affinity()\n+            numa_size = info.get_num_configured_nodes()\n+\n+            node_to_cpus = []\n+            for i in range(numa_size):\n+                node_intersect = set(\n+                    info.node_to_cpus(i)).intersection(cpus_allow_list)\n+                if bool(node_intersect):\n+                    node_to_cpus.append(sorted(list(node_intersect)))\n+\n+            if world_size > len(node_to_cpus):\n+                logger.error(\n+                    \"Auto thread-binding failed due to \"\n+                    \"world size: %d is larger than \"\n+                    \"allowed NUMA nodes number: %d.\"\n+                    \"Please try to bind threads manually.\", world_size,\n+                    len(node_to_cpus))\n+            else:\n+                node_cpus_this_rank = node_to_cpus[self.rank]\n+                node_cpus_this_rank = select_threads_per_power_core(\n+                    node_cpus_this_rank)\n+                cpu_count_per_numa = len(node_cpus_this_rank)\n+                num_of_reserved_cpu = min(envs.VLLM_CPU_NUM_OF_RESERVED_CPU,\n+                                          cpu_count_per_numa // 2)\n+                end = cpu_count_per_numa - num_of_reserved_cpu\n+                rank_to_cpus_list = node_cpus_this_rank[:end]\n+                rank_to_cpus = ','.join(str(x) for x in rank_to_cpus_list)\n+                logger.info(\"ppc64le thread-binding list: %s\", rank_to_cpus)\n+        else:\n+            logger.warning(\n+                \"Auto thread-binding is not supported due to \"\n+                \"the lack of package numa and psutil,\"\n+                \"fallback to no thread-binding. To get better performance,\"\n+                \"please try to manually bind threads.\")\n+        return rank_to_cpus",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2189374579",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20554,
        "pr_file": "vllm/v1/worker/cpu_worker.py",
        "discussion_id": "2189374579",
        "commented_code": "@@ -153,3 +159,58 @@ def get_cpus_id_binding_based_on_numa_nodes(self) -> str:\n                 \"fallback to no thread-binding. To get better performance,\"\n                 \"please try to manually bind threads.\")\n         return rank_to_cpus\n+\n+\n+    def get_cpus_id_binding_based_on_numa_nodes_ppc64le(self) -> str:\n+        \"\"\"\n+        Power (ppc64le) specific: Selects a subset of threads per core for \n+        each NUMA node.This is robust to SMT mode (SMT-8, SMT-4, etc) \n+        because the OS only exposes available threads.This maximizes \n+        performance by avoiding oversubscription of logical CPUs on Power.\n+        \"\"\"\n+\n+        def select_threads_per_power_core(node_cpu_ids):\n+            return [cpu for cpu in node_cpu_ids if cpu % 8 < 4]\n+\n+        rank_to_cpus = self.local_omp_cpuid\n+        world_size = self.vllm_config.parallel_config.world_size\n+        libnuma_found = util.find_spec(\"numa\") is not None\n+        psutil_found = util.find_spec(\"psutil\") is not None\n+        if libnuma_found and psutil_found:\n+            import psutil\n+            from numa import info\n+            cpus_allow_list = psutil.Process().cpu_affinity()\n+            numa_size = info.get_num_configured_nodes()\n+\n+            node_to_cpus = []\n+            for i in range(numa_size):\n+                node_intersect = set(\n+                    info.node_to_cpus(i)).intersection(cpus_allow_list)\n+                if bool(node_intersect):\n+                    node_to_cpus.append(sorted(list(node_intersect)))\n+\n+            if world_size > len(node_to_cpus):\n+                logger.error(\n+                    \"Auto thread-binding failed due to \"\n+                    \"world size: %d is larger than \"\n+                    \"allowed NUMA nodes number: %d.\"\n+                    \"Please try to bind threads manually.\", world_size,\n+                    len(node_to_cpus))\n+            else:\n+                node_cpus_this_rank = node_to_cpus[self.rank]\n+                node_cpus_this_rank = select_threads_per_power_core(\n+                    node_cpus_this_rank)\n+                cpu_count_per_numa = len(node_cpus_this_rank)\n+                num_of_reserved_cpu = min(envs.VLLM_CPU_NUM_OF_RESERVED_CPU,\n+                                          cpu_count_per_numa // 2)\n+                end = cpu_count_per_numa - num_of_reserved_cpu\n+                rank_to_cpus_list = node_cpus_this_rank[:end]\n+                rank_to_cpus = ','.join(str(x) for x in rank_to_cpus_list)\n+                logger.info(\"ppc64le thread-binding list: %s\", rank_to_cpus)\n+        else:\n+            logger.warning(\n+                \"Auto thread-binding is not supported due to \"\n+                \"the lack of package numa and psutil,\"\n+                \"fallback to no thread-binding. To get better performance,\"\n+                \"please try to manually bind threads.\")\n+        return rank_to_cpus",
        "comment_created_at": "2025-07-07T08:38:19+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe function `get_cpus_id_binding_based_on_numa_nodes_ppc64le` largely duplicates the logic in `get_cpus_id_binding_based_on_numa_nodes`. Refactor the common logic into a single function and conditionally apply the architecture-specific logic to reduce code duplication and improve maintainability.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2188983448",
    "pr_number": 20543,
    "pr_file": "vllm/model_executor/model_loader/utils.py",
    "created_at": "2025-07-07T05:11:19+00:00",
    "commented_code": "\"%s has no vLLM implementation, falling back to Transformers \"\n                 \"implementation. Some features may not be supported and \"\n                 \"performance may not be optimal.\", arch)\n-            architectures[i] = \"TransformersForCausalLM\"\n+            # Check if text-config is `self`. If not most probably it is\n+            # a composite config, i.e. mutlimodal\n+            if model_config.hf_config.get_text_config(\n+            ) != model_config.hf_config:\n+                architectures[i] = \"TransformersForMultimodalLM\"\n+            else:\n+                architectures[i] = \"TransformersForCausalLM\"",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2188983448",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20543,
        "pr_file": "vllm/model_executor/model_loader/utils.py",
        "discussion_id": "2188983448",
        "commented_code": "@@ -217,7 +223,13 @@ def resolve_transformers_arch(model_config: ModelConfig,\n                 \"%s has no vLLM implementation, falling back to Transformers \"\n                 \"implementation. Some features may not be supported and \"\n                 \"performance may not be optimal.\", arch)\n-            architectures[i] = \"TransformersForCausalLM\"\n+            # Check if text-config is `self`. If not most probably it is\n+            # a composite config, i.e. mutlimodal\n+            if model_config.hf_config.get_text_config(\n+            ) != model_config.hf_config:\n+                architectures[i] = \"TransformersForMultimodalLM\"\n+            else:\n+                architectures[i] = \"TransformersForCausalLM\"",
        "comment_created_at": "2025-07-07T05:11:19+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis block of logic for determining the transformer architecture is duplicated from lines 209-215. To improve maintainability and prevent potential inconsistencies, I recommend extracting this logic into a local helper function within `resolve_transformers_arch`.\n\nFor example:\n```python\ndef _get_transformers_arch():\n    # Check if text-config is `self`. If not, it is a composite config, i.e. multimodal\n    if model_config.hf_config.get_text_config() != model_config.hf_config:\n        return \"TransformersForMultimodalLM\"\n    else:\n        return \"TransformersForCausalLM\"\n\n# ... then call _get_transformers_arch() in both places.\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2182031441",
    "pr_number": 20418,
    "pr_file": "benchmarks/benchmark_utils.py",
    "created_at": "2025-07-03T07:11:26+00:00",
    "commented_code": "cls=InfEncoder,\n             default=lambda o: f\"<{type(o).__name__} object is not JSON serializable>\",\n         )\n+\n+\n+def get_memory_usage() -> float:\n+    \"\"\"Get peak memory usage in GiB using resource.getrusage().\"\"\"\n+    # Note: ru_maxrss is in kilobytes on Linux, bytes on macOS\n+    import platform\n+\n+    if platform.system() == \"Darwin\":  # macOS - ru_maxrss in bytes\n+        max_self_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / (1 << 30)\n+        max_children_usage = resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss / (\n+            1 << 30\n+        )\n+    else:  # Linux (including most CI systems) - ru_maxrss in kilobytes\n+        max_self_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / (1 << 20)\n+        max_children_usage = resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss / (\n+            1 << 20\n+        )\n+    return max_self_usage + max_children_usage",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2182031441",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20418,
        "pr_file": "benchmarks/benchmark_utils.py",
        "discussion_id": "2182031441",
        "commented_code": "@@ -72,3 +73,21 @@ def write_to_json(filename: str, records: list) -> None:\n             cls=InfEncoder,\n             default=lambda o: f\"<{type(o).__name__} object is not JSON serializable>\",\n         )\n+\n+\n+def get_memory_usage() -> float:\n+    \"\"\"Get peak memory usage in GiB using resource.getrusage().\"\"\"\n+    # Note: ru_maxrss is in kilobytes on Linux, bytes on macOS\n+    import platform\n+\n+    if platform.system() == \"Darwin\":  # macOS - ru_maxrss in bytes\n+        max_self_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / (1 << 30)\n+        max_children_usage = resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss / (\n+            1 << 30\n+        )\n+    else:  # Linux (including most CI systems) - ru_maxrss in kilobytes\n+        max_self_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / (1 << 20)\n+        max_children_usage = resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss / (\n+            1 << 20\n+        )\n+    return max_self_usage + max_children_usage",
        "comment_created_at": "2025-07-03T07:11:26+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis function can be refactored to improve readability and reduce code duplication.\n\n1.  The `import platform` should be at the top of the file, as per PEP 8 guidelines for imports.\n2.  The logic for calculating usage is duplicated inside the `if/else` blocks. This can be simplified by determining the correct divisor first, which also makes it easier to see that the same operation is applied to both `RUSAGE_SELF` and `RUSAGE_CHILDREN`.\n\nHere is a suggested refactoring. Please also move `import platform` to the top of the file.\n\n```suggestion\ndef get_memory_usage() -> float:\n    \"\"\"Get peak memory usage in GiB using resource.getrusage().\"\"\"\n    # Note: ru_maxrss is in kilobytes on Linux, bytes on macOS\n    import platform\n\n    if platform.system() == \"Darwin\":  # macOS - ru_maxrss in bytes\n        # Convert bytes to GiB\n        divisor = 1 << 30\n    else:  # Linux (including most CI systems) - ru_maxrss in kilobytes\n        # Convert kilobytes to GiB\n        divisor = 1 << 20\n\n    max_self_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / divisor\n    max_children_usage = resource.getrusage(\n        resource.RUSAGE_CHILDREN).ru_maxrss / divisor\n    return max_self_usage + max_children_usage\n```",
        "pr_file_module": null
      }
    ]
  }
]