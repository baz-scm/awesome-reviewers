[
  {
    "discussion_id": "2191256256",
    "pr_number": 20603,
    "pr_file": "requirements/test.in",
    "created_at": "2025-07-08T00:45:53+00:00",
    "commented_code": "transformers_stream_generator # required for qwen-vl test\n mamba_ssm # required for plamo2 test\n matplotlib # required for qwen-vl test\n-mistral_common[opencv] >= 1.6.2 # required for pixtral test\n+mistral_common[opencv] >= 1.6.2; python_version<=\"3.12\" # required for pixtral test",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2191256256",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20603,
        "pr_file": "requirements/test.in",
        "discussion_id": "2191256256",
        "commented_code": "@@ -28,7 +28,7 @@ torchvision==0.22.0\n transformers_stream_generator # required for qwen-vl test\n mamba_ssm # required for plamo2 test\n matplotlib # required for qwen-vl test\n-mistral_common[opencv] >= 1.6.2 # required for pixtral test\n+mistral_common[opencv] >= 1.6.2; python_version<=\"3.12\" # required for pixtral test",
        "comment_created_at": "2025-07-08T00:45:53+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nSimilar to the change in `requirements/common.txt`, it's important to verify that `mistral_common[opencv]` is indeed the only blocker for Python 3.13 support in the testing environment. If other packages also have compatibility issues, they should be addressed similarly. If `mistral_common[opencv]` is not compatible with Python 3.13, tests relying on this package may fail or produce unexpected results when run with Python 3.13. This could lead to inaccurate test results and potentially mask other issues. Consider adding a comment explaining why this limitation is necessary and linking to any relevant issues or documentation. This will help other developers understand the reasoning behind this change and avoid potential conflicts in the future.\n\n```\nmistral_common[opencv] >= 1.6.2; python_version<=\"3.12\" # Not compatible with Python 3.13 (see issue #TODO)\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2178596037",
    "pr_number": 20330,
    "pr_file": "csrc/cuda_compat.h",
    "created_at": "2025-07-01T21:47:51+00:00",
    "commented_code": "#include <hip/hip_runtime.h>\n #endif\n \n-#ifndef USE_ROCM\n-  #define WARP_SIZE 32\n+#if defined(USE_ROCM) && defined(__GFX9__)\n+  #define WARP_SIZE 64\n #else\n-  #define WARP_SIZE warpSize\n+  #define WARP_SIZE 32\n #endif",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2178596037",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20330,
        "pr_file": "csrc/cuda_compat.h",
        "discussion_id": "2178596037",
        "commented_code": "@@ -4,10 +4,10 @@\n   #include <hip/hip_runtime.h>\n #endif\n \n-#ifndef USE_ROCM\n-  #define WARP_SIZE 32\n+#if defined(USE_ROCM) && defined(__GFX9__)\n+  #define WARP_SIZE 64\n #else\n-  #define WARP_SIZE warpSize\n+  #define WARP_SIZE 32\n #endif",
        "comment_created_at": "2025-07-01T21:47:51+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThis implementation for `WARP_SIZE` is not robust for all ROCm architectures.\n\nWhile it correctly handles GFX9 (Vega) by setting `WARP_SIZE` to 64, it incorrectly hardcodes `WARP_SIZE` to 32 for all other ROCm architectures (e.g., GFX10+ like MI200/MI300).\n\nOn these newer architectures, the wavefront size can be 32 or 64, depending on compiler flags (`-mwavefrontsize32` or `-mwavefrontsize64`). If the code is compiled for a 64-wide wavefront, this macro will be 32, leading to incorrect behavior in warp-level operations (e.g., reductions, shuffles) and potential deadlocks.\n\nThe AMD documentation you linked to recommends using the `__AMDGPU_WAVEFRONT_SIZE__` macro to get the compile-time wavefront size. This is the correct approach to handle this breaking change in ROCm 7.0.\n\nI suggest using `__AMDGPU_WAVEFRONT_SIZE__` when available, with fallbacks for older compilers or other architectures.\n\n```suggestion\n#if defined(USE_ROCM)\n  #if defined(__AMDGPU_WAVEFRONT_SIZE__)\n    // This macro is defined by the ROCm compiler to the wavefront size.\n    // It's the recommended way to get a constexpr warp size.\n    #define WARP_SIZE __AMDGPU_WAVEFRONT_SIZE__\n  #elif defined(__GFX9__)\n    // Fallback for GFX9 architecture which always has a wavefront size of 64.\n    #define WARP_SIZE 64\n  #else\n    // For other architectures (GFX10+), default to 32 if the macro is not available.\n    #define WARP_SIZE 32\n  #endif\n#else // CUDA\n  #define WARP_SIZE 32\n#endif\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2178627404",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20330,
        "pr_file": "csrc/cuda_compat.h",
        "discussion_id": "2178596037",
        "commented_code": "@@ -4,10 +4,10 @@\n   #include <hip/hip_runtime.h>\n #endif\n \n-#ifndef USE_ROCM\n-  #define WARP_SIZE 32\n+#if defined(USE_ROCM) && defined(__GFX9__)\n+  #define WARP_SIZE 64\n #else\n-  #define WARP_SIZE warpSize\n+  #define WARP_SIZE 32\n #endif",
        "comment_created_at": "2025-07-01T22:17:18+00:00",
        "comment_author": "gshtras",
        "comment_body": "compile time __AMDGCN_WAVEFRONT_SIZE__ being deprecated alongside warpSize is exactly the reason for this PR.",
        "pr_file_module": null
      }
    ]
  }
]