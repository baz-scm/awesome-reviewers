[
  {
    "discussion_id": "2191737556",
    "pr_number": 20614,
    "pr_file": "tests/models/test_initialization.py",
    "created_at": "2025-07-08T07:43:35+00:00",
    "commented_code": "kv_cache_specs[0],\n             10 * GiB_bytes,\n         )\n-\n+        logger.debug(\"Using the patch function.........\")",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2191737556",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20614,
        "pr_file": "tests/models/test_initialization.py",
        "discussion_id": "2191737556",
        "commented_code": "@@ -76,7 +76,7 @@ def _initialize_kv_caches_v1(self, vllm_config):\n             kv_cache_specs[0],\n             10 * GiB_bytes,\n         )\n-\n+        logger.debug(\"Using the patch function.........\")",
        "comment_created_at": "2025-07-08T07:43:35+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis debug log message seems to be for temporary debugging. If it's intended to stay, it could be more descriptive. For example, it could mention which function is being patched. Also, the trailing periods are generally discouraged in log messages.\n\nIf this is a temporary debug statement, please remember to remove it before merging.\n\n```suggestion\n        logger.debug(\"Using patched _initialize_kv_caches_v1 for testing.\")\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2145741574",
    "pr_number": 19619,
    "pr_file": "vllm/engine/arg_utils.py",
    "created_at": "2025-06-13T18:14:36+00:00",
    "commented_code": "override_attention_dtype=self.override_attention_dtype,\n         )\n \n+    def no_valid_tensorizer_args_in_model_loader_extra_config(self) -> bool:\n+\n+        if self.model_loader_extra_config:\n+            for allowed_to_pass in [\"tensorizer_uri\", \"tensorizer_dir\"]:\n+                try:\n+                    logger.info(\"Got %s\", self.model_loader_extra_config)",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2145741574",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19619,
        "pr_file": "vllm/engine/arg_utils.py",
        "discussion_id": "2145741574",
        "commented_code": "@@ -952,11 +953,33 @@ def create_model_config(self) -> ModelConfig:\n             override_attention_dtype=self.override_attention_dtype,\n         )\n \n+    def no_valid_tensorizer_args_in_model_loader_extra_config(self) -> bool:\n+\n+        if self.model_loader_extra_config:\n+            for allowed_to_pass in [\"tensorizer_uri\", \"tensorizer_dir\"]:\n+                try:\n+                    logger.info(\"Got %s\", self.model_loader_extra_config)",
        "comment_created_at": "2025-06-13T18:14:36+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe logging message `logger.info(\"Got %s\", self.model_loader_extra_config)` is inside a loop. If `self.model_loader_extra_config` is large and neither `tensorizer_uri` nor `tensorizer_dir` are present, this could lead to multiple verbose log entries. Consider moving this log to be conditional or outside the loop if it's intended to log only once about the `model_loader_extra_config` being processed.",
        "pr_file_module": null
      },
      {
        "comment_id": "2145856528",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19619,
        "pr_file": "vllm/engine/arg_utils.py",
        "discussion_id": "2145741574",
        "commented_code": "@@ -952,11 +953,33 @@ def create_model_config(self) -> ModelConfig:\n             override_attention_dtype=self.override_attention_dtype,\n         )\n \n+    def no_valid_tensorizer_args_in_model_loader_extra_config(self) -> bool:\n+\n+        if self.model_loader_extra_config:\n+            for allowed_to_pass in [\"tensorizer_uri\", \"tensorizer_dir\"]:\n+                try:\n+                    logger.info(\"Got %s\", self.model_loader_extra_config)",
        "comment_created_at": "2025-06-13T18:40:22+00:00",
        "comment_author": "sangstar",
        "comment_body": "Irrelevant here; this is looping over 2 values.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190455590",
    "pr_number": 20570,
    "pr_file": "vllm/transformers_utils/configs/mistral.py",
    "created_at": "2025-07-07T15:48:31+00:00",
    "commented_code": "+# SPDX-License-Identifier: Apache-2.0\n+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n+from typing import Any\n+\n+from transformers import PretrainedConfig\n+\n+from vllm.logger import init_logger\n+\n+logger = init_logger(__name__)\n+\n+\n+def adapt_config_dict(config_dict: dict[str, Any],\n+                      **kwargs) -> PretrainedConfig:\n+    config_dict.update(kwargs)\n+\n+    is_quant = bool(config_dict.get(\"quantization\"))\n+    is_vision = bool(\n+        (config_dict.get(\"multimodal\") or {}).get(\"vision_encoder_args\")\n+        or config_dict.get(\"vision_encoder\"))\n+    is_moe = bool(config_dict.get(\"moe\"))\n+    is_yarn = bool(config_dict.get(\"yarn\"))\n+\n+    config_dict = _remap_general_mistral_args(config_dict)\n+    if is_quant:\n+        config_dict = _remap_mistral_quantization_args(config_dict)\n+\n+    if is_moe:\n+        config_dict[\"architectures\"] = [\"MixtralForCausalLM\"]\n+    else:\n+        config_dict[\"architectures\"] = [\"MistralForCausalLM\"]\n+\n+    if is_yarn:\n+        config_dict = _remap_mistral_yarn_args(config_dict)\n+    if is_vision:\n+        config_dict = _remap_mistral_vision_args(config_dict)\n+\n+    config = PretrainedConfig.from_dict(config_dict)\n+\n+    logger.info(\"Initialized config\", config)",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2190455590",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20570,
        "pr_file": "vllm/transformers_utils/configs/mistral.py",
        "discussion_id": "2190455590",
        "commented_code": "@@ -0,0 +1,132 @@\n+# SPDX-License-Identifier: Apache-2.0\n+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n+from typing import Any\n+\n+from transformers import PretrainedConfig\n+\n+from vllm.logger import init_logger\n+\n+logger = init_logger(__name__)\n+\n+\n+def adapt_config_dict(config_dict: dict[str, Any],\n+                      **kwargs) -> PretrainedConfig:\n+    config_dict.update(kwargs)\n+\n+    is_quant = bool(config_dict.get(\"quantization\"))\n+    is_vision = bool(\n+        (config_dict.get(\"multimodal\") or {}).get(\"vision_encoder_args\")\n+        or config_dict.get(\"vision_encoder\"))\n+    is_moe = bool(config_dict.get(\"moe\"))\n+    is_yarn = bool(config_dict.get(\"yarn\"))\n+\n+    config_dict = _remap_general_mistral_args(config_dict)\n+    if is_quant:\n+        config_dict = _remap_mistral_quantization_args(config_dict)\n+\n+    if is_moe:\n+        config_dict[\"architectures\"] = [\"MixtralForCausalLM\"]\n+    else:\n+        config_dict[\"architectures\"] = [\"MistralForCausalLM\"]\n+\n+    if is_yarn:\n+        config_dict = _remap_mistral_yarn_args(config_dict)\n+    if is_vision:\n+        config_dict = _remap_mistral_vision_args(config_dict)\n+\n+    config = PretrainedConfig.from_dict(config_dict)\n+\n+    logger.info(\"Initialized config\", config)",
        "comment_created_at": "2025-07-07T15:48:31+00:00",
        "comment_author": "ywang96",
        "comment_body": "Can we change this it `debug`? We're trying to improve startup log noise and I'm a bit concerned that printing out this entire `PretrainedConfig` might be too much. WDYT?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2167724211",
    "pr_number": 18293,
    "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py",
    "created_at": "2025-06-25T22:09:03+00:00",
    "commented_code": "self.tp_rank)\n \n             # Register with NIXL.\n-            descs = self.nixl_wrapper.get_xfer_descs(blocks_data, \"VRAM\")\n+            descs = self.nixl_wrapper.get_xfer_descs(blocks_data,\n+                                                     self.nixl_memory_type)\n             self.dst_xfer_side_handles[\n                 engine_id] = self.nixl_wrapper.prep_xfer_dlist(\n                     remote_agent_name, descs)\n \n         return remote_agent_name\n \n+    def sync_recved_kv_to_device(self,\n+                                 req_id: str,\n+                                 meta: Optional[ReqMeta] = None):\n+        \"\"\"copy recved kv from host buffer to device.\"\"\"\n+        if not self.use_host_buffer:\n+            return\n+        assert self.copy_blocks is not None\n+\n+        if meta is None:\n+            meta = self._recving_metadata.get(req_id)\n+        if meta and req_id not in self._recving_transfers:\n+            # local decode only\n+            if not meta.do_remote_prefill:\n+                return\n+            local_block_ids = meta.local_block_ids\n+            self.copy_blocks(self.host_xfer_buffers, self.device_kv_caches,\n+                             local_block_ids, local_block_ids, \"h2d\")\n+            logger.debug(\n+                \"synced recved kv of request[%s] to device kv buffer,\"\n+                \"local_block_ids: %s. \", req_id,\n+                \",\".join(map(str, meta.local_block_ids)))",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2167724211",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 18293,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py",
        "discussion_id": "2167724211",
        "commented_code": "@@ -766,13 +952,56 @@ def add_remote_agent(self,\n                 self.tp_rank)\n \n             # Register with NIXL.\n-            descs = self.nixl_wrapper.get_xfer_descs(blocks_data, \"VRAM\")\n+            descs = self.nixl_wrapper.get_xfer_descs(blocks_data,\n+                                                     self.nixl_memory_type)\n             self.dst_xfer_side_handles[\n                 engine_id] = self.nixl_wrapper.prep_xfer_dlist(\n                     remote_agent_name, descs)\n \n         return remote_agent_name\n \n+    def sync_recved_kv_to_device(self,\n+                                 req_id: str,\n+                                 meta: Optional[ReqMeta] = None):\n+        \"\"\"copy recved kv from host buffer to device.\"\"\"\n+        if not self.use_host_buffer:\n+            return\n+        assert self.copy_blocks is not None\n+\n+        if meta is None:\n+            meta = self._recving_metadata.get(req_id)\n+        if meta and req_id not in self._recving_transfers:\n+            # local decode only\n+            if not meta.do_remote_prefill:\n+                return\n+            local_block_ids = meta.local_block_ids\n+            self.copy_blocks(self.host_xfer_buffers, self.device_kv_caches,\n+                             local_block_ids, local_block_ids, \"h2d\")\n+            logger.debug(\n+                \"synced recved kv of request[%s] to device kv buffer,\"\n+                \"local_block_ids: %s. \", req_id,\n+                \",\".join(map(str, meta.local_block_ids)))",
        "comment_created_at": "2025-06-25T22:09:03+00:00",
        "comment_author": "njhill",
        "comment_body": "Best to guard these with `if logger.isEnabledFor(logging.DEBUG):` to avoid nontrivial parameter computation",
        "pr_file_module": null
      },
      {
        "comment_id": "2169527773",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 18293,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py",
        "discussion_id": "2167724211",
        "commented_code": "@@ -766,13 +952,56 @@ def add_remote_agent(self,\n                 self.tp_rank)\n \n             # Register with NIXL.\n-            descs = self.nixl_wrapper.get_xfer_descs(blocks_data, \"VRAM\")\n+            descs = self.nixl_wrapper.get_xfer_descs(blocks_data,\n+                                                     self.nixl_memory_type)\n             self.dst_xfer_side_handles[\n                 engine_id] = self.nixl_wrapper.prep_xfer_dlist(\n                     remote_agent_name, descs)\n \n         return remote_agent_name\n \n+    def sync_recved_kv_to_device(self,\n+                                 req_id: str,\n+                                 meta: Optional[ReqMeta] = None):\n+        \"\"\"copy recved kv from host buffer to device.\"\"\"\n+        if not self.use_host_buffer:\n+            return\n+        assert self.copy_blocks is not None\n+\n+        if meta is None:\n+            meta = self._recving_metadata.get(req_id)\n+        if meta and req_id not in self._recving_transfers:\n+            # local decode only\n+            if not meta.do_remote_prefill:\n+                return\n+            local_block_ids = meta.local_block_ids\n+            self.copy_blocks(self.host_xfer_buffers, self.device_kv_caches,\n+                             local_block_ids, local_block_ids, \"h2d\")\n+            logger.debug(\n+                \"synced recved kv of request[%s] to device kv buffer,\"\n+                \"local_block_ids: %s. \", req_id,\n+                \",\".join(map(str, meta.local_block_ids)))",
        "comment_created_at": "2025-06-26T17:14:30+00:00",
        "comment_author": "juncgu",
        "comment_body": "updated.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2146402243",
    "pr_number": 19636,
    "pr_file": "vllm/model_executor/layers/fused_moe/config.py",
    "created_at": "2025-06-14T03:28:47+00:00",
    "commented_code": "+# SPDX-License-Identifier: Apache-2.0\n+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n+from dataclasses import dataclass\n+from typing import Optional, Union\n+\n+import torch\n+from compressed_tensors.quantization import (QuantizationArgs,\n+                                             QuantizationStrategy,\n+                                             QuantizationType)\n+\n+import vllm.envs as envs\n+from vllm.config import ParallelConfig\n+from vllm.distributed import get_dp_group, get_tensor_model_parallel_rank\n+from vllm.model_executor.layers.quantization.base_config import (\n+    QuantizationConfig)\n+\n+# Note: this limit is somewhat arbitrary and might be changed later.\n+# The size of the activations will be E x MOE_DP_CHUNK_SIZE x hidden_dim.\n+MOE_DP_CHUNK_SIZE = 128\n+\n+\n+def _get_quant_config_quantization_args(\n+    quant_config: Optional[QuantizationConfig],\n+    prop_name: str,\n+) -> Optional[QuantizationArgs]:\n+    if (quant_config is not None and hasattr(quant_config, 'target_scheme_map')\n+            and \"Linear\" in quant_config.target_scheme_map and\n+            \"input_activations\" in quant_config.target_scheme_map[\"Linear\"]):\n+        return quant_config.target_scheme_map[\"Linear\"].get(prop_name)\n+    else:\n+        return None\n+\n+\n+def get_quant_config_input_quant(\n+        quant_config: Optional[QuantizationConfig]\n+) -> Optional[QuantizationArgs]:\n+    return _get_quant_config_quantization_args(quant_config,\n+                                               \"input_activations\")\n+\n+\n+def get_quant_config_weight_quant(\n+        quant_config: Optional[QuantizationConfig]\n+) -> Optional[QuantizationArgs]:\n+    return _get_quant_config_quantization_args(quant_config, \"weights\")\n+\n+\n+# TODO (bnell): use scalar_type instead of bools?\n+def get_config_quant_dtype(\n+    use_fp8_w8a8: bool,\n+    use_int8_w8a8: bool,\n+    use_int8_w8a16: bool,\n+    use_int4_w4a16: bool,\n+) -> Optional[torch.dtype]:\n+    if use_fp8_w8a8:\n+        return torch.float8_e4m3fn\n+    elif use_int8_w8a8:\n+        return torch.int8\n+    return None\n+\n+\n+@dataclass\n+class FusedMoEQuantConfig:\n+    # The post quantization activation type.\n+    quant_dtype: Optional[torch.dtype] = None\n+    per_act_token_quant: bool = False\n+    per_out_ch_quant: bool = False\n+    block_shape: Optional[list[int]] = None\n+\n+    # TODO: add col major flag?\n+    # add detailed quant info for input, intermediates, weights, etc?\n+\n+    @staticmethod\n+    def make(\n+        use_fp8_w8a8: bool = False,\n+        use_int8_w8a8: bool = False,\n+        use_int8_w8a16: bool = False,\n+        use_int4_w4a16: bool = False,\n+        per_act_token_quant: bool = False,\n+        per_out_ch_quant: bool = False,\n+        block_shape: Optional[list[int]] = None,\n+    ) -> \"FusedMoEQuantConfig\":\n+        quant_dtype = get_config_quant_dtype(use_fp8_w8a8=use_fp8_w8a8,\n+                                             use_int8_w8a8=use_int8_w8a8,\n+                                             use_int8_w8a16=use_int8_w8a16,\n+                                             use_int4_w4a16=use_int4_w4a16)\n+        return FusedMoEQuantConfig(\n+            quant_dtype,\n+            per_act_token_quant,\n+            per_out_ch_quant,\n+            block_shape,\n+        )\n+\n+\n+@dataclass\n+class FusedMoEParallelConfig:\n+    tp_size: int\n+    dp_size: int\n+    ep_size: int\n+    tp_rank: int\n+    dp_rank: int\n+    ep_rank: int\n+\n+    use_ep: bool  # whether to use EP or not\n+\n+    @property\n+    def use_all2all_kernels(self):\n+        return self.dp_size > 1 and self.use_ep\n+\n+    @property\n+    def use_pplx_kernels(self):\n+        return (self.use_all2all_kernels\n+                and envs.VLLM_ALL2ALL_BACKEND == \"pplx\")\n+\n+    @property\n+    def use_deepep_ht_kernels(self):\n+        return (self.use_all2all_kernels\n+                and envs.VLLM_ALL2ALL_BACKEND == \"deepep_high_throughput\")\n+\n+    @property\n+    def use_deepep_ll_kernels(self):\n+        return (self.use_all2all_kernels\n+                and envs.VLLM_ALL2ALL_BACKEND == \"deepep_low_latency\")\n+\n+    @staticmethod\n+    def make(tp_size_: int, dp_size_: int,\n+             vllm_parallel_config: ParallelConfig) -> \"FusedMoEParallelConfig\":\n+        \"\"\"\n+        Determine MoE parallel configuration. Based on the input tp_size_,\n+        dp_size_, ep_size_ and vllm's parallel config, determine what\n+        level's of parallelism to use in the fused moe layer.\n+\n+        Args:\n+            tp_size_ (int): tp_size passed into the FusedMoE constructor.\n+            dp_size_ (int): dp_size passed into the FusedMoE constructor.\n+            ep_size_ (int): ep_size passed into the FusedMoE constructor.\n+            vllm_parallel_config (ParallelConfig): vllm's parallel config\n+            object.\n+\n+        Examples:\n+        When there is no parallelism requested, i.e. tp_size_ = dp_size_ = 1,\n+        we simply return the sizes unaltered and the ranks set to 0.\n+\n+        Expert Parallelism is considered only when either dp_size_ or tp_size_\n+        is non trivial.\n+\n+        When TP = 2, DP = 1 and EP = False, the configuration on different\n+        devices,\n+            - device 0 : TP = {2, 0} DP = {1, 0} EP = {1, 0} //\n+                         legend : {size, rank}\n+            - device 1 : TP = {2, 1} DP = {1, 0} EP = {1, 0}\n+            - Comment : Tensors are sharded across 2 devices.\n+\n+        When TP = 1, DP = 2 and EP = False, the configuration on different\n+        devices,\n+            - device 0 : TP = {2, 0} DP = {2, 0} EP = {1, 0}\n+            - device 1 : TP = {2, 1} DP = {2, 1} EP = {1, 0}\n+            - Comment: There are 2 engine instances and the tensors are sharded\n+              across 2 decvices.\n+\n+        When TP = 2, DP = 2 and EP = False, the configuration on different\n+        devices,\n+            - device 0: TP = {4, 0} DP = {2, 0} EP = {1, 0}\n+            - device 1: TP = {4, 1} DP = {2, 0} EP = {1, 0}\n+            - device 2: TP = {4, 2} DP = {2, 1} EP = {1, 0}\n+            - device 3: TP = {4, 3} DP = {2, 1} EP = {1, 0}\n+            - Comment: There are 2 engine instances and the tensors are sharded\n+              across 4 devices.\n+\n+        When, TP = 2, DP = 1 and EP = True, the configuration on different\n+        devices,\n+            - device 0: TP = {1, 0} DP = {1, 0} EP = {2, 0}\n+            - device 1: TP = {1, 0} DP = {1, 0} EP = {2, 1}\n+            - Comment: The experts are split between the 2 devices.\n+\n+        When, TP = 1, DP = 2 and EP = True, the configuration on different\n+        devices,\n+            - device 0: TP = {1, 0} DP = {2, 0} EP = {2, 0}\n+            - device 1: TP = {1, 0} DP = {2, 1} EP = {2, 1}\n+            - Comment: There are 2 engine instances and the experts are split\n+              between the 2 devices.\n+\n+        When TP = 2, DP = 2 and EP = True, the configuration on different\n+        devices,\n+            - device 0: TP = {1, 0} DP = {2, 0} EP = {4, 0}\n+            - device 1: TP = {1, 0} DP = {2, 0} EP = {4, 1}\n+            - device 2: TP = {1, 0} DP = {2, 1} EP = {4, 2}\n+            - device 3: TP = {1, 0} DP = {2, 1} EP = {4, 3}\n+            - Comment: There are 2 engine instances and the experts are split\n+              between the 4 devices.\n+        \"\"\"\n+\n+        def flatten_tp_across_dp(dp_rank: int):\n+            tp_rank = 0 if tp_size_ == 1 else get_tensor_model_parallel_rank()\n+            # There are actually dp_size_ * tp_size_ devices. Update tp_size\n+            # and tp_rank so we shard across all devices.\n+            tp_size = dp_size_ * tp_size_\n+            tp_rank = dp_rank * tp_size_ + tp_rank\n+            return tp_size, tp_rank\n+\n+        use_ep = (dp_size_ * tp_size_ > 1\n+                  and vllm_parallel_config.enable_expert_parallel)\n+\n+        dp_size = dp_size_\n+        dp_rank = get_dp_group().rank_in_group if dp_size > 1 else 0\n+        tp_size, tp_rank = flatten_tp_across_dp(dp_rank)\n+\n+        if not use_ep:\n+            return FusedMoEParallelConfig(tp_size=tp_size,\n+                                          tp_rank=tp_rank,\n+                                          dp_size=dp_size,\n+                                          dp_rank=dp_rank,\n+                                          ep_size=1,\n+                                          ep_rank=0,\n+                                          use_ep=False)\n+        # DP + EP / TP + EP / DP + TP + EP\n+        assert use_ep\n+        # In EP, each device owns a set of experts fully. There is no tensor\n+        # parallel update tp_size, tp_rank, ep_size and ep_rank to reflect that.\n+        ep_size = tp_size\n+        ep_rank = tp_rank\n+        return FusedMoEParallelConfig(tp_size=1,\n+                                      tp_rank=0,\n+                                      dp_size=dp_size,\n+                                      dp_rank=dp_rank,\n+                                      ep_size=ep_size,\n+                                      ep_rank=ep_rank,\n+                                      use_ep=True)\n+\n+\n+# Adapted from pplx-kernels tests/all_to_all_utils.py\n+@dataclass\n+class FusedMoEConfig:\n+    num_experts: int\n+    experts_per_token: int\n+    hidden_dim: int\n+\n+    num_local_experts: int\n+    moe_parallel_config: FusedMoEParallelConfig\n+\n+    # The activation type.\n+    in_dtype: torch.dtype\n+\n+    quant_config: Optional[FusedMoEQuantConfig] = None\n+\n+    max_num_tokens: int = envs.VLLM_MOE_DP_CHUNK_SIZE\n+\n+    def __post_init__(self):\n+        if self.dp_size > 1:\n+            logger.debug(\"Using FusedMoEConfig::max_num_tokens=%d\",",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2146402243",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19636,
        "pr_file": "vllm/model_executor/layers/fused_moe/config.py",
        "discussion_id": "2146402243",
        "commented_code": "@@ -0,0 +1,384 @@\n+# SPDX-License-Identifier: Apache-2.0\n+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n+from dataclasses import dataclass\n+from typing import Optional, Union\n+\n+import torch\n+from compressed_tensors.quantization import (QuantizationArgs,\n+                                             QuantizationStrategy,\n+                                             QuantizationType)\n+\n+import vllm.envs as envs\n+from vllm.config import ParallelConfig\n+from vllm.distributed import get_dp_group, get_tensor_model_parallel_rank\n+from vllm.model_executor.layers.quantization.base_config import (\n+    QuantizationConfig)\n+\n+# Note: this limit is somewhat arbitrary and might be changed later.\n+# The size of the activations will be E x MOE_DP_CHUNK_SIZE x hidden_dim.\n+MOE_DP_CHUNK_SIZE = 128\n+\n+\n+def _get_quant_config_quantization_args(\n+    quant_config: Optional[QuantizationConfig],\n+    prop_name: str,\n+) -> Optional[QuantizationArgs]:\n+    if (quant_config is not None and hasattr(quant_config, 'target_scheme_map')\n+            and \"Linear\" in quant_config.target_scheme_map and\n+            \"input_activations\" in quant_config.target_scheme_map[\"Linear\"]):\n+        return quant_config.target_scheme_map[\"Linear\"].get(prop_name)\n+    else:\n+        return None\n+\n+\n+def get_quant_config_input_quant(\n+        quant_config: Optional[QuantizationConfig]\n+) -> Optional[QuantizationArgs]:\n+    return _get_quant_config_quantization_args(quant_config,\n+                                               \"input_activations\")\n+\n+\n+def get_quant_config_weight_quant(\n+        quant_config: Optional[QuantizationConfig]\n+) -> Optional[QuantizationArgs]:\n+    return _get_quant_config_quantization_args(quant_config, \"weights\")\n+\n+\n+# TODO (bnell): use scalar_type instead of bools?\n+def get_config_quant_dtype(\n+    use_fp8_w8a8: bool,\n+    use_int8_w8a8: bool,\n+    use_int8_w8a16: bool,\n+    use_int4_w4a16: bool,\n+) -> Optional[torch.dtype]:\n+    if use_fp8_w8a8:\n+        return torch.float8_e4m3fn\n+    elif use_int8_w8a8:\n+        return torch.int8\n+    return None\n+\n+\n+@dataclass\n+class FusedMoEQuantConfig:\n+    # The post quantization activation type.\n+    quant_dtype: Optional[torch.dtype] = None\n+    per_act_token_quant: bool = False\n+    per_out_ch_quant: bool = False\n+    block_shape: Optional[list[int]] = None\n+\n+    # TODO: add col major flag?\n+    # add detailed quant info for input, intermediates, weights, etc?\n+\n+    @staticmethod\n+    def make(\n+        use_fp8_w8a8: bool = False,\n+        use_int8_w8a8: bool = False,\n+        use_int8_w8a16: bool = False,\n+        use_int4_w4a16: bool = False,\n+        per_act_token_quant: bool = False,\n+        per_out_ch_quant: bool = False,\n+        block_shape: Optional[list[int]] = None,\n+    ) -> \"FusedMoEQuantConfig\":\n+        quant_dtype = get_config_quant_dtype(use_fp8_w8a8=use_fp8_w8a8,\n+                                             use_int8_w8a8=use_int8_w8a8,\n+                                             use_int8_w8a16=use_int8_w8a16,\n+                                             use_int4_w4a16=use_int4_w4a16)\n+        return FusedMoEQuantConfig(\n+            quant_dtype,\n+            per_act_token_quant,\n+            per_out_ch_quant,\n+            block_shape,\n+        )\n+\n+\n+@dataclass\n+class FusedMoEParallelConfig:\n+    tp_size: int\n+    dp_size: int\n+    ep_size: int\n+    tp_rank: int\n+    dp_rank: int\n+    ep_rank: int\n+\n+    use_ep: bool  # whether to use EP or not\n+\n+    @property\n+    def use_all2all_kernels(self):\n+        return self.dp_size > 1 and self.use_ep\n+\n+    @property\n+    def use_pplx_kernels(self):\n+        return (self.use_all2all_kernels\n+                and envs.VLLM_ALL2ALL_BACKEND == \"pplx\")\n+\n+    @property\n+    def use_deepep_ht_kernels(self):\n+        return (self.use_all2all_kernels\n+                and envs.VLLM_ALL2ALL_BACKEND == \"deepep_high_throughput\")\n+\n+    @property\n+    def use_deepep_ll_kernels(self):\n+        return (self.use_all2all_kernels\n+                and envs.VLLM_ALL2ALL_BACKEND == \"deepep_low_latency\")\n+\n+    @staticmethod\n+    def make(tp_size_: int, dp_size_: int,\n+             vllm_parallel_config: ParallelConfig) -> \"FusedMoEParallelConfig\":\n+        \"\"\"\n+        Determine MoE parallel configuration. Based on the input tp_size_,\n+        dp_size_, ep_size_ and vllm's parallel config, determine what\n+        level's of parallelism to use in the fused moe layer.\n+\n+        Args:\n+            tp_size_ (int): tp_size passed into the FusedMoE constructor.\n+            dp_size_ (int): dp_size passed into the FusedMoE constructor.\n+            ep_size_ (int): ep_size passed into the FusedMoE constructor.\n+            vllm_parallel_config (ParallelConfig): vllm's parallel config\n+            object.\n+\n+        Examples:\n+        When there is no parallelism requested, i.e. tp_size_ = dp_size_ = 1,\n+        we simply return the sizes unaltered and the ranks set to 0.\n+\n+        Expert Parallelism is considered only when either dp_size_ or tp_size_\n+        is non trivial.\n+\n+        When TP = 2, DP = 1 and EP = False, the configuration on different\n+        devices,\n+            - device 0 : TP = {2, 0} DP = {1, 0} EP = {1, 0} //\n+                         legend : {size, rank}\n+            - device 1 : TP = {2, 1} DP = {1, 0} EP = {1, 0}\n+            - Comment : Tensors are sharded across 2 devices.\n+\n+        When TP = 1, DP = 2 and EP = False, the configuration on different\n+        devices,\n+            - device 0 : TP = {2, 0} DP = {2, 0} EP = {1, 0}\n+            - device 1 : TP = {2, 1} DP = {2, 1} EP = {1, 0}\n+            - Comment: There are 2 engine instances and the tensors are sharded\n+              across 2 decvices.\n+\n+        When TP = 2, DP = 2 and EP = False, the configuration on different\n+        devices,\n+            - device 0: TP = {4, 0} DP = {2, 0} EP = {1, 0}\n+            - device 1: TP = {4, 1} DP = {2, 0} EP = {1, 0}\n+            - device 2: TP = {4, 2} DP = {2, 1} EP = {1, 0}\n+            - device 3: TP = {4, 3} DP = {2, 1} EP = {1, 0}\n+            - Comment: There are 2 engine instances and the tensors are sharded\n+              across 4 devices.\n+\n+        When, TP = 2, DP = 1 and EP = True, the configuration on different\n+        devices,\n+            - device 0: TP = {1, 0} DP = {1, 0} EP = {2, 0}\n+            - device 1: TP = {1, 0} DP = {1, 0} EP = {2, 1}\n+            - Comment: The experts are split between the 2 devices.\n+\n+        When, TP = 1, DP = 2 and EP = True, the configuration on different\n+        devices,\n+            - device 0: TP = {1, 0} DP = {2, 0} EP = {2, 0}\n+            - device 1: TP = {1, 0} DP = {2, 1} EP = {2, 1}\n+            - Comment: There are 2 engine instances and the experts are split\n+              between the 2 devices.\n+\n+        When TP = 2, DP = 2 and EP = True, the configuration on different\n+        devices,\n+            - device 0: TP = {1, 0} DP = {2, 0} EP = {4, 0}\n+            - device 1: TP = {1, 0} DP = {2, 0} EP = {4, 1}\n+            - device 2: TP = {1, 0} DP = {2, 1} EP = {4, 2}\n+            - device 3: TP = {1, 0} DP = {2, 1} EP = {4, 3}\n+            - Comment: There are 2 engine instances and the experts are split\n+              between the 4 devices.\n+        \"\"\"\n+\n+        def flatten_tp_across_dp(dp_rank: int):\n+            tp_rank = 0 if tp_size_ == 1 else get_tensor_model_parallel_rank()\n+            # There are actually dp_size_ * tp_size_ devices. Update tp_size\n+            # and tp_rank so we shard across all devices.\n+            tp_size = dp_size_ * tp_size_\n+            tp_rank = dp_rank * tp_size_ + tp_rank\n+            return tp_size, tp_rank\n+\n+        use_ep = (dp_size_ * tp_size_ > 1\n+                  and vllm_parallel_config.enable_expert_parallel)\n+\n+        dp_size = dp_size_\n+        dp_rank = get_dp_group().rank_in_group if dp_size > 1 else 0\n+        tp_size, tp_rank = flatten_tp_across_dp(dp_rank)\n+\n+        if not use_ep:\n+            return FusedMoEParallelConfig(tp_size=tp_size,\n+                                          tp_rank=tp_rank,\n+                                          dp_size=dp_size,\n+                                          dp_rank=dp_rank,\n+                                          ep_size=1,\n+                                          ep_rank=0,\n+                                          use_ep=False)\n+        # DP + EP / TP + EP / DP + TP + EP\n+        assert use_ep\n+        # In EP, each device owns a set of experts fully. There is no tensor\n+        # parallel update tp_size, tp_rank, ep_size and ep_rank to reflect that.\n+        ep_size = tp_size\n+        ep_rank = tp_rank\n+        return FusedMoEParallelConfig(tp_size=1,\n+                                      tp_rank=0,\n+                                      dp_size=dp_size,\n+                                      dp_rank=dp_rank,\n+                                      ep_size=ep_size,\n+                                      ep_rank=ep_rank,\n+                                      use_ep=True)\n+\n+\n+# Adapted from pplx-kernels tests/all_to_all_utils.py\n+@dataclass\n+class FusedMoEConfig:\n+    num_experts: int\n+    experts_per_token: int\n+    hidden_dim: int\n+\n+    num_local_experts: int\n+    moe_parallel_config: FusedMoEParallelConfig\n+\n+    # The activation type.\n+    in_dtype: torch.dtype\n+\n+    quant_config: Optional[FusedMoEQuantConfig] = None\n+\n+    max_num_tokens: int = envs.VLLM_MOE_DP_CHUNK_SIZE\n+\n+    def __post_init__(self):\n+        if self.dp_size > 1:\n+            logger.debug(\"Using FusedMoEConfig::max_num_tokens=%d\",",
        "comment_created_at": "2025-06-14T03:28:47+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe `logger.debug` call is used here, but the `logger` object is not defined within this file. This will lead to a `NameError` at runtime.\n\nPlease ensure that the logger is properly initialized, for example, by adding:\n```python\nfrom vllm.logger import init_logger\nlogger = init_logger(__name__)\n```\nat the beginning of the file.\n\n```suggestion\n            # logger.debug(\"Using FusedMoEConfig::max_num_tokens=%d\",\n            #              self.max_num_tokens)\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2187221312",
    "pr_number": 20510,
    "pr_file": "vllm/usage/usage_lib.py",
    "created_at": "2025-07-05T12:49:22+00:00",
    "commented_code": "self.gpu_type = torch_xla.tpu.get_tpu_type()\n                 self.gpu_memory_per_device = (\n                     torch_xla.core.xla_model.get_memory_info()[\"bytes_limit\"])\n-            except Exception:\n-                pass\n+            except Exception as e:\n+                logging.debug(\"Failed to collect TPU information: %s\", e)",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2187221312",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20510,
        "pr_file": "vllm/usage/usage_lib.py",
        "discussion_id": "2187221312",
        "commented_code": "@@ -182,8 +182,8 @@ def _report_usage_once(self, model_architecture: str,\n                 self.gpu_type = torch_xla.tpu.get_tpu_type()\n                 self.gpu_memory_per_device = (\n                     torch_xla.core.xla_model.get_memory_info()[\"bytes_limit\"])\n-            except Exception:\n-                pass\n+            except Exception as e:\n+                logging.debug(\"Failed to collect TPU information: %s\", e)",
        "comment_created_at": "2025-07-05T12:49:22+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nConsider using `exc_info=True` in the logging call to automatically include the full exception traceback in the debug log, which provides more context for debugging.\n\n```suggestion\n            except Exception:\n                logging.debug(\"Failed to collect TPU information\", exc_info=True)\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2147342598",
    "pr_number": 19654,
    "pr_file": "vllm/worker/cpu_worker.py",
    "created_at": "2025-06-14T21:39:46+00:00",
    "commented_code": "cpu_count = psutil.cpu_count(logical=False)\n             cpus_allow_list = psutil.Process().cpu_affinity()\n             numa_size = info.get_num_configured_nodes()\n-            cpu_count_per_numa = cpu_count // numa_size\n-            num_of_reserved_cpu = min(envs.VLLM_CPU_NUM_OF_RESERVED_CPU,\n-                                      cpu_count_per_numa // 2)\n-\n-            # check allow node_to_cpus list\n-            node_to_cpus = []\n-            for i in range(numa_size):\n-                node_intersect = set(\n-                    info.node_to_cpus(i)).intersection(cpus_allow_list)\n-                if bool(node_intersect):\n-                    node_to_cpus.append(list(node_intersect))\n-\n-            if world_size > len(node_to_cpus):\n-                logger.error(\n-                    \"Auto thread-binding failed due to \"\n-                    \"world size: %d is larger than \"\n-                    \"allowed NUMA nodes number: %d.\"\n-                    \"Please try to bind threads manually.\", world_size,\n-                    len(node_to_cpus))\n+            if numa_size == 0:\n+                logger.warn(\n+                    \"libnuma found but numa size was 0\"\n+                    \"set VLLM_CPU_OMP_THREADS_BIND manually.\")",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2147342598",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19654,
        "pr_file": "vllm/worker/cpu_worker.py",
        "discussion_id": "2147342598",
        "commented_code": "@@ -417,30 +417,35 @@ def get_cpus_id_binding_based_on_numa_nodes(self) -> str:\n             cpu_count = psutil.cpu_count(logical=False)\n             cpus_allow_list = psutil.Process().cpu_affinity()\n             numa_size = info.get_num_configured_nodes()\n-            cpu_count_per_numa = cpu_count // numa_size\n-            num_of_reserved_cpu = min(envs.VLLM_CPU_NUM_OF_RESERVED_CPU,\n-                                      cpu_count_per_numa // 2)\n-\n-            # check allow node_to_cpus list\n-            node_to_cpus = []\n-            for i in range(numa_size):\n-                node_intersect = set(\n-                    info.node_to_cpus(i)).intersection(cpus_allow_list)\n-                if bool(node_intersect):\n-                    node_to_cpus.append(list(node_intersect))\n-\n-            if world_size > len(node_to_cpus):\n-                logger.error(\n-                    \"Auto thread-binding failed due to \"\n-                    \"world size: %d is larger than \"\n-                    \"allowed NUMA nodes number: %d.\"\n-                    \"Please try to bind threads manually.\", world_size,\n-                    len(node_to_cpus))\n+            if numa_size == 0:\n+                logger.warn(\n+                    \"libnuma found but numa size was 0\"\n+                    \"set VLLM_CPU_OMP_THREADS_BIND manually.\")",
        "comment_created_at": "2025-06-14T21:39:46+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe logging call `logger.warn` is deprecated and has been replaced by `logger.warning`. It's advisable to use the updated method.\n\nFurthermore, the log message spanning lines 422-423 results from concatenating string literals. This can make the message less clear. Using a single, formatted string would improve readability and ensure correct spacing.\n\n```suggestion\n                logger.warning(\n                    \"libnuma found but numa size was 0. \"\n                    \"Set VLLM_CPU_OMP_THREADS_BIND manually.\")\n```",
        "pr_file_module": null
      }
    ]
  }
]