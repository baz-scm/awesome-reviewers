[
  {
    "discussion_id": "2183505996",
    "pr_number": 20450,
    "pr_file": "tools/pd_disagg/Justfile",
    "created_at": "2025-07-03T18:40:05+00:00",
    "commented_code": "+# Needed for the proxy server\n+vllm-directory := \"/home/rshaw/vllm/\" \n+\n+PREFILL_GPU := \"0\"\n+DECODE_GPU := \"2\"\n+MODEL := \"meta-llama/Llama-3.1-8B-Instruct\"\n+PROXY_PORT := \"8192\"\n+PREFILL_PORT := \"8100\"\n+DECODE_PORT := \"8200\"\n+PREFILL_NIXL_SIDE_CHANNEL_PORT := \"5557\"\n+DECODE_NIXL_SIDE_CHANNEL_PORT := \"5558\"\n+\n+prefill:\n+    VLLM_NIXL_SIDE_CHANNEL_PORT={{PREFILL_NIXL_SIDE_CHANNEL_PORT}} \\\n+    CUDA_VISIBLE_DEVICES={{PREFILL_GPU}} \\\n+    vllm serve {{MODEL}} \\\n+      --port {{PREFILL_PORT}} \\\n+      --tensor-parallel-size 1 \\\n+      --enforce-eager \\\n+      --disable-log-requests \\\n+      --block-size 128 \\\n+      --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}'\n+\n+decode:\n+    VLLM_NIXL_SIDE_CHANNEL_PORT={{DECODE_NIXL_SIDE_CHANNEL_PORT}} \\\n+    CUDA_VISIBLE_DEVICES={{DECODE_GPU}} \\\n+    vllm serve {{MODEL}} \\\n+      --port {{DECODE_PORT}} \\\n+      --tensor-parallel-size 1 \\\n+      --enforce-eager \\\n+      --disable-log-requests \\\n+      --block-size 128 \\\n+      --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}'\n+\n+proxy:\n+    python \"{{vllm-directory}}tests/v1/kv_connector/nixl_integration/toy_proxy_server.py\" \\\n+      --port {{PROXY_PORT}} \\\n+      --prefiller-port {{PREFILL_PORT}} \\\n+      --decoder-port {{DECODE_PORT}}\n+\n+send_request:\n+  curl -X POST http://localhost:{{PROXY_PORT}}/v1/completions \\\n+    -H \"Content-Type: application/json\" \\\n+    -d '{ \\\n+      \"model\": \"{{MODEL}}\", \\\n+      \"prompt\": \"Red Hat is the best open source company by far across Linux, K8s, and AI, and vLLM has the greatest community in open source AI software infrastructure. I love vLLM because\", \\\n+      \"max_tokens\": 150, \\\n+      \"temperature\": 0.7 \\\n+    }'\n+\n+benchmark NUM_PROMPTS:\n+  python {{vllm-directory}}/benchmarks/benchmark_serving.py \\\n+    --port {{PROXY_PORT}} \\\n+    --model {{MODEL}} \\\n+    --dataset-name random \\\n+    --random-input-len 30000 \\\n+    --random-output-len 10 \\\n+    --num-prompts {{NUM_PROMPTS}} \\\n+    --seed $(date +%s) \\\n+\n+benchmark_one INPUT_LEN:\n+  python {{vllm-directory}}benchmarks/benchmark_one_concurrent_req.py \\",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2183505996",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20450,
        "pr_file": "tools/pd_disagg/Justfile",
        "discussion_id": "2183505996",
        "commented_code": "@@ -0,0 +1,82 @@\n+# Needed for the proxy server\n+vllm-directory := \"/home/rshaw/vllm/\" \n+\n+PREFILL_GPU := \"0\"\n+DECODE_GPU := \"2\"\n+MODEL := \"meta-llama/Llama-3.1-8B-Instruct\"\n+PROXY_PORT := \"8192\"\n+PREFILL_PORT := \"8100\"\n+DECODE_PORT := \"8200\"\n+PREFILL_NIXL_SIDE_CHANNEL_PORT := \"5557\"\n+DECODE_NIXL_SIDE_CHANNEL_PORT := \"5558\"\n+\n+prefill:\n+    VLLM_NIXL_SIDE_CHANNEL_PORT={{PREFILL_NIXL_SIDE_CHANNEL_PORT}} \\\n+    CUDA_VISIBLE_DEVICES={{PREFILL_GPU}} \\\n+    vllm serve {{MODEL}} \\\n+      --port {{PREFILL_PORT}} \\\n+      --tensor-parallel-size 1 \\\n+      --enforce-eager \\\n+      --disable-log-requests \\\n+      --block-size 128 \\\n+      --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}'\n+\n+decode:\n+    VLLM_NIXL_SIDE_CHANNEL_PORT={{DECODE_NIXL_SIDE_CHANNEL_PORT}} \\\n+    CUDA_VISIBLE_DEVICES={{DECODE_GPU}} \\\n+    vllm serve {{MODEL}} \\\n+      --port {{DECODE_PORT}} \\\n+      --tensor-parallel-size 1 \\\n+      --enforce-eager \\\n+      --disable-log-requests \\\n+      --block-size 128 \\\n+      --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}'\n+\n+proxy:\n+    python \"{{vllm-directory}}tests/v1/kv_connector/nixl_integration/toy_proxy_server.py\" \\\n+      --port {{PROXY_PORT}} \\\n+      --prefiller-port {{PREFILL_PORT}} \\\n+      --decoder-port {{DECODE_PORT}}\n+\n+send_request:\n+  curl -X POST http://localhost:{{PROXY_PORT}}/v1/completions \\\n+    -H \"Content-Type: application/json\" \\\n+    -d '{ \\\n+      \"model\": \"{{MODEL}}\", \\\n+      \"prompt\": \"Red Hat is the best open source company by far across Linux, K8s, and AI, and vLLM has the greatest community in open source AI software infrastructure. I love vLLM because\", \\\n+      \"max_tokens\": 150, \\\n+      \"temperature\": 0.7 \\\n+    }'\n+\n+benchmark NUM_PROMPTS:\n+  python {{vllm-directory}}/benchmarks/benchmark_serving.py \\\n+    --port {{PROXY_PORT}} \\\n+    --model {{MODEL}} \\\n+    --dataset-name random \\\n+    --random-input-len 30000 \\\n+    --random-output-len 10 \\\n+    --num-prompts {{NUM_PROMPTS}} \\\n+    --seed $(date +%s) \\\n+\n+benchmark_one INPUT_LEN:\n+  python {{vllm-directory}}benchmarks/benchmark_one_concurrent_req.py \\",
        "comment_created_at": "2025-07-03T18:40:05+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe script name `benchmark_one_concurrent_req.py` appears to be incorrect. The file added in this pull request is named `benchmark_one_concurrent.py`. This typo will cause the command to fail.\n\n```\n  python {{vllm-directory}}benchmarks/benchmark_one_concurrent.py \\\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2183505998",
    "pr_number": 20450,
    "pr_file": "tools/pd_disagg/Justfile",
    "created_at": "2025-07-03T18:40:06+00:00",
    "commented_code": "+# Needed for the proxy server\n+vllm-directory := \"/home/rshaw/vllm/\" \n+\n+PREFILL_GPU := \"0\"\n+DECODE_GPU := \"2\"\n+MODEL := \"meta-llama/Llama-3.1-8B-Instruct\"\n+PROXY_PORT := \"8192\"\n+PREFILL_PORT := \"8100\"\n+DECODE_PORT := \"8200\"\n+PREFILL_NIXL_SIDE_CHANNEL_PORT := \"5557\"\n+DECODE_NIXL_SIDE_CHANNEL_PORT := \"5558\"\n+\n+prefill:\n+    VLLM_NIXL_SIDE_CHANNEL_PORT={{PREFILL_NIXL_SIDE_CHANNEL_PORT}} \\\n+    CUDA_VISIBLE_DEVICES={{PREFILL_GPU}} \\\n+    vllm serve {{MODEL}} \\\n+      --port {{PREFILL_PORT}} \\\n+      --tensor-parallel-size 1 \\\n+      --enforce-eager \\\n+      --disable-log-requests \\\n+      --block-size 128 \\\n+      --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}'\n+\n+decode:\n+    VLLM_NIXL_SIDE_CHANNEL_PORT={{DECODE_NIXL_SIDE_CHANNEL_PORT}} \\\n+    CUDA_VISIBLE_DEVICES={{DECODE_GPU}} \\\n+    vllm serve {{MODEL}} \\\n+      --port {{DECODE_PORT}} \\\n+      --tensor-parallel-size 1 \\\n+      --enforce-eager \\\n+      --disable-log-requests \\\n+      --block-size 128 \\\n+      --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}'\n+\n+proxy:\n+    python \"{{vllm-directory}}tests/v1/kv_connector/nixl_integration/toy_proxy_server.py\" \\\n+      --port {{PROXY_PORT}} \\\n+      --prefiller-port {{PREFILL_PORT}} \\\n+      --decoder-port {{DECODE_PORT}}\n+\n+send_request:\n+  curl -X POST http://localhost:{{PROXY_PORT}}/v1/completions \\\n+    -H \"Content-Type: application/json\" \\\n+    -d '{ \\\n+      \"model\": \"{{MODEL}}\", \\\n+      \"prompt\": \"Red Hat is the best open source company by far across Linux, K8s, and AI, and vLLM has the greatest community in open source AI software infrastructure. I love vLLM because\", \\\n+      \"max_tokens\": 150, \\\n+      \"temperature\": 0.7 \\\n+    }'\n+\n+benchmark NUM_PROMPTS:\n+  python {{vllm-directory}}/benchmarks/benchmark_serving.py \\\n+    --port {{PROXY_PORT}} \\\n+    --model {{MODEL}} \\\n+    --dataset-name random \\\n+    --random-input-len 30000 \\\n+    --random-output-len 10 \\\n+    --num-prompts {{NUM_PROMPTS}} \\\n+    --seed $(date +%s) \\\n+\n+benchmark_one INPUT_LEN:\n+  python {{vllm-directory}}benchmarks/benchmark_one_concurrent_req.py \\\n+    --port {{PROXY_PORT}} \\\n+    --model {{MODEL}} \\\n+    --input-len {{INPUT_LEN}} \\\n+    --output-len 1 \\\n+    --num-requests 10 \\\n+    --seed $(date +%s)\n+\n+benchmark_one_no_pd INPUT_LEN:\n+  python {{vllm-directory}}benchmarks/benchmark_one_concurrent_req.py \\",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2183505998",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20450,
        "pr_file": "tools/pd_disagg/Justfile",
        "discussion_id": "2183505998",
        "commented_code": "@@ -0,0 +1,82 @@\n+# Needed for the proxy server\n+vllm-directory := \"/home/rshaw/vllm/\" \n+\n+PREFILL_GPU := \"0\"\n+DECODE_GPU := \"2\"\n+MODEL := \"meta-llama/Llama-3.1-8B-Instruct\"\n+PROXY_PORT := \"8192\"\n+PREFILL_PORT := \"8100\"\n+DECODE_PORT := \"8200\"\n+PREFILL_NIXL_SIDE_CHANNEL_PORT := \"5557\"\n+DECODE_NIXL_SIDE_CHANNEL_PORT := \"5558\"\n+\n+prefill:\n+    VLLM_NIXL_SIDE_CHANNEL_PORT={{PREFILL_NIXL_SIDE_CHANNEL_PORT}} \\\n+    CUDA_VISIBLE_DEVICES={{PREFILL_GPU}} \\\n+    vllm serve {{MODEL}} \\\n+      --port {{PREFILL_PORT}} \\\n+      --tensor-parallel-size 1 \\\n+      --enforce-eager \\\n+      --disable-log-requests \\\n+      --block-size 128 \\\n+      --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}'\n+\n+decode:\n+    VLLM_NIXL_SIDE_CHANNEL_PORT={{DECODE_NIXL_SIDE_CHANNEL_PORT}} \\\n+    CUDA_VISIBLE_DEVICES={{DECODE_GPU}} \\\n+    vllm serve {{MODEL}} \\\n+      --port {{DECODE_PORT}} \\\n+      --tensor-parallel-size 1 \\\n+      --enforce-eager \\\n+      --disable-log-requests \\\n+      --block-size 128 \\\n+      --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}'\n+\n+proxy:\n+    python \"{{vllm-directory}}tests/v1/kv_connector/nixl_integration/toy_proxy_server.py\" \\\n+      --port {{PROXY_PORT}} \\\n+      --prefiller-port {{PREFILL_PORT}} \\\n+      --decoder-port {{DECODE_PORT}}\n+\n+send_request:\n+  curl -X POST http://localhost:{{PROXY_PORT}}/v1/completions \\\n+    -H \"Content-Type: application/json\" \\\n+    -d '{ \\\n+      \"model\": \"{{MODEL}}\", \\\n+      \"prompt\": \"Red Hat is the best open source company by far across Linux, K8s, and AI, and vLLM has the greatest community in open source AI software infrastructure. I love vLLM because\", \\\n+      \"max_tokens\": 150, \\\n+      \"temperature\": 0.7 \\\n+    }'\n+\n+benchmark NUM_PROMPTS:\n+  python {{vllm-directory}}/benchmarks/benchmark_serving.py \\\n+    --port {{PROXY_PORT}} \\\n+    --model {{MODEL}} \\\n+    --dataset-name random \\\n+    --random-input-len 30000 \\\n+    --random-output-len 10 \\\n+    --num-prompts {{NUM_PROMPTS}} \\\n+    --seed $(date +%s) \\\n+\n+benchmark_one INPUT_LEN:\n+  python {{vllm-directory}}benchmarks/benchmark_one_concurrent_req.py \\\n+    --port {{PROXY_PORT}} \\\n+    --model {{MODEL}} \\\n+    --input-len {{INPUT_LEN}} \\\n+    --output-len 1 \\\n+    --num-requests 10 \\\n+    --seed $(date +%s)\n+\n+benchmark_one_no_pd INPUT_LEN:\n+  python {{vllm-directory}}benchmarks/benchmark_one_concurrent_req.py \\",
        "comment_created_at": "2025-07-03T18:40:06+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe script name `benchmark_one_concurrent_req.py` appears to be incorrect. The file added in this pull request is named `benchmark_one_concurrent.py`. This typo will cause the command to fail.\n\n```\n  python {{vllm-directory}}benchmarks/benchmark_one_concurrent.py \\\n```",
        "pr_file_module": null
      }
    ]
  }
]