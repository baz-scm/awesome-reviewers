[
  {
    "discussion_id": "2161539744",
    "pr_number": 19983,
    "pr_file": "csrc/quantization/cutlass_w8a8/moe/moe_data.cu",
    "created_at": "2025-06-23T12:46:47+00:00",
    "commented_code": "static_cast<const int32_t*>(expert_num_tokens.data_ptr()), padded_m, n,\n       k);\n }\n+\n+// TODO utilize more CUDA threads\n+// this will probably need some extra padding for warps",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2161539744",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19983,
        "pr_file": "csrc/quantization/cutlass_w8a8/moe/moe_data.cu",
        "discussion_id": "2161539744",
        "commented_code": "@@ -161,3 +161,46 @@ void get_cutlass_pplx_moe_mm_data_caller(torch::Tensor& expert_offsets,\n       static_cast<const int32_t*>(expert_num_tokens.data_ptr()), padded_m, n,\n       k);\n }\n+\n+// TODO utilize more CUDA threads\n+// this will probably need some extra padding for warps",
        "comment_created_at": "2025-06-23T12:46:47+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe `TODO` comments here correctly identify that this kernel's performance could be improved. Consider using a 2D grid to parallelize over both the token and `k_scaled` dimensions.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2183356312",
    "pr_number": 20447,
    "pr_file": "csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu",
    "created_at": "2025-07-03T17:34:35+00:00",
    "commented_code": "torch::Tensor const& b_strides, torch::Tensor const& c_strides,\n     bool per_act_token, bool per_out_ch) {\n   int32_t version_num = get_sm_version_num();\n+#if defined ENABLE_CUTLASS_MOE_SM100 && ENABLE_CUTLASS_MOE_SM100\n+  cutlass_moe_mm_sm100(out_tensors, a_tensors, b_tensors, a_scales, b_scales,\n+                       expert_offsets, problem_sizes, a_strides, b_strides,\n+                       c_strides, per_act_token, per_out_ch);\n+  return;\n+#endif",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2183356312",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20447,
        "pr_file": "csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu",
        "discussion_id": "2183356312",
        "commented_code": "@@ -234,6 +247,12 @@ void cutlass_moe_mm(\n     torch::Tensor const& b_strides, torch::Tensor const& c_strides,\n     bool per_act_token, bool per_out_ch) {\n   int32_t version_num = get_sm_version_num();\n+#if defined ENABLE_CUTLASS_MOE_SM100 && ENABLE_CUTLASS_MOE_SM100\n+  cutlass_moe_mm_sm100(out_tensors, a_tensors, b_tensors, a_scales, b_scales,\n+                       expert_offsets, problem_sizes, a_strides, b_strides,\n+                       c_strides, per_act_token, per_out_ch);\n+  return;\n+#endif",
        "comment_created_at": "2025-07-03T17:34:35+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe dispatch logic here is incorrect. It relies only on compile-time preprocessor directives (`#if defined...`) and doesn't use the runtime SM version (`version_num`) for dispatching. This will lead to runtime errors if the code is compiled for multiple architectures (e.g., both SM90 and SM100).\n\nFor instance, if both `ENABLE_CUTLASS_MOE_SM100` and `ENABLE_CUTLASS_MOE_SM90` are defined, this code will always try to call `cutlass_moe_mm_sm100`, even when running on an SM90 GPU, which will fail.\n\nThe dispatch logic should be based on the `version_num` queried at runtime, similar to how `cutlass_scaled_mm` is implemented in this file. Please update this block and the subsequent one for SM90 to use `version_num` for correct runtime dispatch.\n\n```\n#if defined ENABLE_CUTLASS_MOE_SM100 && ENABLE_CUTLASS_MOE_SM100\n  if (version_num >= 100) {\n    cutlass_moe_mm_sm100(out_tensors, a_tensors, b_tensors, a_scales, b_scales,\n                         expert_offsets, problem_sizes, a_strides, b_strides,\n                         c_strides, per_act_token, per_out_ch);\n    return;\n  }\n#endif\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2190723841",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20447,
        "pr_file": "csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu",
        "discussion_id": "2183356312",
        "commented_code": "@@ -234,6 +247,12 @@ void cutlass_moe_mm(\n     torch::Tensor const& b_strides, torch::Tensor const& c_strides,\n     bool per_act_token, bool per_out_ch) {\n   int32_t version_num = get_sm_version_num();\n+#if defined ENABLE_CUTLASS_MOE_SM100 && ENABLE_CUTLASS_MOE_SM100\n+  cutlass_moe_mm_sm100(out_tensors, a_tensors, b_tensors, a_scales, b_scales,\n+                       expert_offsets, problem_sizes, a_strides, b_strides,\n+                       c_strides, per_act_token, per_out_ch);\n+  return;\n+#endif",
        "comment_created_at": "2025-07-07T17:51:00+00:00",
        "comment_author": "LucasWilkinson",
        "comment_body": "@djmmoss I think Gemini is actually correct here; we should also add runtime switches ",
        "pr_file_module": null
      }
    ]
  }
]