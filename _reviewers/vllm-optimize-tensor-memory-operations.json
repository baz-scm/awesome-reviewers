[
  {
    "discussion_id": "2167300709",
    "pr_number": 20087,
    "pr_file": "benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py",
    "created_at": "2025-06-25T17:58:02+00:00",
    "commented_code": "from vllm.triton_utils import triton\n \n \n-# Copied from\n-# https://github.com/deepseek-ai/DeepGEMM/blob/78cacf70d41d15d688bd493ebc85845f7f2a3d5d/tests/test_core.py#L9\n-def per_token_cast_to_fp8(\n-        x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n-    \"\"\"Convert tensor to FP8 format with per-token scaling.\"\"\"\n-    assert x.dim() == 2 and x.size(1) % 128 == 0\n-    m, n = x.shape\n-    x_view = x.view(m, -1, 128)\n-    x_amax = x_view.abs().float().amax(dim=2).view(m, -1).clamp(1e-4)\n-    return (x_view * (448.0 / x_amax.unsqueeze(2))).to(\n-        torch.float8_e4m3fn).view(m, n), (x_amax / 448.0).view(m, -1)\n-\n-\n # Copied from\n # https://github.com/deepseek-ai/DeepGEMM/blob/78cacf70d41d15d688bd493ebc85845f7f2a3d5d/tests/test_core.py#L17\n-def per_block_cast_to_fp8(\n-        x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+def per_block_cast_to_fp8_vllm(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n     \"\"\"Convert tensor to FP8 format with per-block scaling.\"\"\"\n     assert x.dim() == 2\n     m, n = x.shape\n-    x_padded = torch.zeros((ceil_div(m, 128) * 128, ceil_div(n, 128) * 128),\n-                           dtype=x.dtype,\n-                           device=x.device)\n+    x_padded = torch.zeros(\n+        (ceil_div(m, 128) * 128, ceil_div(n, 128) * 128), dtype=x.dtype, device=x.device\n+    )\n     x_padded[:m, :n] = x\n     x_view = x_padded.view(-1, 128, x_padded.size(1) // 128, 128)\n     x_amax = x_view.abs().float().amax(dim=(1, 3), keepdim=True).clamp(1e-4)\n     x_scaled = (x_view * (448.0 / x_amax)).to(torch.float8_e4m3fn)\n-    return x_scaled.view_as(x_padded)[:m, :n].contiguous(), (\n-        x_amax / 448.0).view(x_view.size(0), x_view.size(2))\n-\n-\n-def benchmark_shape(m: int,\n-                    n: int,\n-                    k: int,\n-                    warmup: int = 100,\n-                    repeat: int = 10000,\n-                    verbose: bool = False) -> dict:\n+    return x_scaled.view_as(x_padded)[:m, :n].contiguous(), (x_amax / 448.0).view(\n+        x_view.size(0), x_view.size(2)\n+    )\n+\n+\n+def benchmark_shape(\n+    m: int,\n+    n: int,\n+    k: int,\n+    warmup: int = 100,\n+    repeat: int = 10000,\n+    verbose: bool = False,\n+) -> dict:\n     \"\"\"Benchmark all implementations for a specific (m, n, k) shape.\"\"\"\n     if verbose:\n         print(f\"\n=== Benchmarking shape: m={m}, n={n}, k={k} ===\")\n \n-    # Create test tensors\n-    A = torch.randn((m, k), device='cuda', dtype=torch.bfloat16)\n-    B = torch.randn((n, k), device='cuda', dtype=torch.bfloat16)\n-\n-    # Reference result in BF16\n+    A = torch.randn((m, k), device=\"cuda\", dtype=torch.bfloat16)\n+    B = torch.randn((n, k), device=\"cuda\", dtype=torch.bfloat16)\n     torch.cuda.synchronize()\n     C_ref = A @ B.t()\n \n     # Pre-quantize B for all implementations\n     # (weights can be pre-quantized offline)\n     B_deepgemm, B_scale_deepgemm = per_block_cast_to_fp8(B)\n-    B_vllm, B_scale_vllm = per_block_cast_to_fp8(B)\n+    B_vllm, B_scale_vllm = per_block_cast_to_fp8_vllm(B)\n \n     # Block size configuration\n     block_size = [128, 128]\n \n     # Pre-quantize A for all implementations\n     A_deepgemm, A_scale_deepgemm = per_token_cast_to_fp8(A)\n-    A_scale_deepgemm = get_col_major_tma_aligned_tensor(A_scale_deepgemm)\n-    C_deepgemm = torch.empty((m, n), device='cuda', dtype=torch.bfloat16)\n+    C_deepgemm = (\n+        torch.empty((n, m), device=\"cuda\", dtype=torch.bfloat16).t().contiguous()\n+    )",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2167300709",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20087,
        "pr_file": "benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py",
        "discussion_id": "2167300709",
        "commented_code": "@@ -18,96 +16,84 @@\n from vllm.triton_utils import triton\n \n \n-# Copied from\n-# https://github.com/deepseek-ai/DeepGEMM/blob/78cacf70d41d15d688bd493ebc85845f7f2a3d5d/tests/test_core.py#L9\n-def per_token_cast_to_fp8(\n-        x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n-    \"\"\"Convert tensor to FP8 format with per-token scaling.\"\"\"\n-    assert x.dim() == 2 and x.size(1) % 128 == 0\n-    m, n = x.shape\n-    x_view = x.view(m, -1, 128)\n-    x_amax = x_view.abs().float().amax(dim=2).view(m, -1).clamp(1e-4)\n-    return (x_view * (448.0 / x_amax.unsqueeze(2))).to(\n-        torch.float8_e4m3fn).view(m, n), (x_amax / 448.0).view(m, -1)\n-\n-\n # Copied from\n # https://github.com/deepseek-ai/DeepGEMM/blob/78cacf70d41d15d688bd493ebc85845f7f2a3d5d/tests/test_core.py#L17\n-def per_block_cast_to_fp8(\n-        x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+def per_block_cast_to_fp8_vllm(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n     \"\"\"Convert tensor to FP8 format with per-block scaling.\"\"\"\n     assert x.dim() == 2\n     m, n = x.shape\n-    x_padded = torch.zeros((ceil_div(m, 128) * 128, ceil_div(n, 128) * 128),\n-                           dtype=x.dtype,\n-                           device=x.device)\n+    x_padded = torch.zeros(\n+        (ceil_div(m, 128) * 128, ceil_div(n, 128) * 128), dtype=x.dtype, device=x.device\n+    )\n     x_padded[:m, :n] = x\n     x_view = x_padded.view(-1, 128, x_padded.size(1) // 128, 128)\n     x_amax = x_view.abs().float().amax(dim=(1, 3), keepdim=True).clamp(1e-4)\n     x_scaled = (x_view * (448.0 / x_amax)).to(torch.float8_e4m3fn)\n-    return x_scaled.view_as(x_padded)[:m, :n].contiguous(), (\n-        x_amax / 448.0).view(x_view.size(0), x_view.size(2))\n-\n-\n-def benchmark_shape(m: int,\n-                    n: int,\n-                    k: int,\n-                    warmup: int = 100,\n-                    repeat: int = 10000,\n-                    verbose: bool = False) -> dict:\n+    return x_scaled.view_as(x_padded)[:m, :n].contiguous(), (x_amax / 448.0).view(\n+        x_view.size(0), x_view.size(2)\n+    )\n+\n+\n+def benchmark_shape(\n+    m: int,\n+    n: int,\n+    k: int,\n+    warmup: int = 100,\n+    repeat: int = 10000,\n+    verbose: bool = False,\n+) -> dict:\n     \"\"\"Benchmark all implementations for a specific (m, n, k) shape.\"\"\"\n     if verbose:\n         print(f\"\\n=== Benchmarking shape: m={m}, n={n}, k={k} ===\")\n \n-    # Create test tensors\n-    A = torch.randn((m, k), device='cuda', dtype=torch.bfloat16)\n-    B = torch.randn((n, k), device='cuda', dtype=torch.bfloat16)\n-\n-    # Reference result in BF16\n+    A = torch.randn((m, k), device=\"cuda\", dtype=torch.bfloat16)\n+    B = torch.randn((n, k), device=\"cuda\", dtype=torch.bfloat16)\n     torch.cuda.synchronize()\n     C_ref = A @ B.t()\n \n     # Pre-quantize B for all implementations\n     # (weights can be pre-quantized offline)\n     B_deepgemm, B_scale_deepgemm = per_block_cast_to_fp8(B)\n-    B_vllm, B_scale_vllm = per_block_cast_to_fp8(B)\n+    B_vllm, B_scale_vllm = per_block_cast_to_fp8_vllm(B)\n \n     # Block size configuration\n     block_size = [128, 128]\n \n     # Pre-quantize A for all implementations\n     A_deepgemm, A_scale_deepgemm = per_token_cast_to_fp8(A)\n-    A_scale_deepgemm = get_col_major_tma_aligned_tensor(A_scale_deepgemm)\n-    C_deepgemm = torch.empty((m, n), device='cuda', dtype=torch.bfloat16)\n+    C_deepgemm = (\n+        torch.empty((n, m), device=\"cuda\", dtype=torch.bfloat16).t().contiguous()\n+    )",
        "comment_created_at": "2025-06-25T17:58:02+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe `.t().contiguous()` operation can be simplified by directly specifying the desired memory format during tensor creation. This can potentially improve performance by avoiding an unnecessary transpose and copy operation.\n\n```python\nC_deepgemm = torch.empty((n, m), device=\"cuda\", dtype=torch.bfloat16, memory_format=torch.contiguous_format).t()\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2164750244",
    "pr_number": 20037,
    "pr_file": "vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py",
    "created_at": "2025-06-24T19:32:54+00:00",
    "commented_code": "+# SPDX-License-Identifier: Apache-2.0\n+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n+import functools\n+import importlib.util\n+from typing import Optional\n+\n+import torch\n+\n+import vllm.model_executor.layers.fused_moe.modular_kernel as mk\n+from vllm.logger import init_logger\n+from vllm.model_executor.layers.fused_moe.moe_permute_unpermute import (\n+    _moe_permute)\n+from vllm.model_executor.layers.fused_moe.flashinfer_cutlass_prepare_finalize import (\n+    FlashInferCutlassMoEPrepareAndFinalizeNoEP)\n+from vllm.model_executor.layers.fused_moe.utils import (\n+    _resize_cache, per_token_group_quant_fp8)\n+from vllm.utils import round_up\n+\n+logger = init_logger(__name__)\n+\n+from typing import TYPE_CHECKING\n+try:\n+    from flashinfer.fused_moe import cutlass_fused_moe as cutlass_fused_moe\n+    from flashinfer import fp4_quantize as fp4_quantize\n+except ImportError:\n+    if not TYPE_CHECKING:\n+        cutlass_fused_moe = None\n+\n+\n+\n+has_flashinfer_cutlass_fused_moe = cutlass_fused_moe is not None\n+def _valid_flashinfer_fused_moe(hidden_states: torch.Tensor, w1: torch.Tensor,\n+                     w2: torch.Tensor) -> bool:\n+    \"\"\"\n+    Check if the given problem size is supported by the DeepGemm grouped\n+    gemm kernel.  All of M, N, K and the quantization block_shape must be\n+    aligned by `dg.get_m_alignment_for_contiguous_layout()`.\n+    \"\"\"\n+    if not has_flashinfer_cutlass_fused_moe:\n+        logger.debug(\"FlashInferExperts disabled: flashinfer_cutlass_fused_moe not available.\")\n+        return False    \n+    # Data type checks\n+    if (w1.dtype != torch.uint8 or w2.dtype != torch.uint8 or\n+        hidden_states.dtype not in [torch.float32, torch.float16, torch.bfloat16]):\n+        logger.debug(f\"FlashInferExperts disabled: w1/w2 must be torch.uint8 (got w1={w1.dtype}, w2={w2.dtype}), \"\n+                     f\"hidden_states must be float32, float16, or bfloat16 (got {hidden_states.dtype}).\")\n+        return False\n+    return True\n+\n+class FlashInferExperts(mk.FusedMoEPermuteExpertsUnpermute):\n+    def __init__(self):\n+        super().__init__()\n+\n+\n+    def supports_chunking(self) -> bool:\n+        return True\n+\n+    def workspace_shapes(\n+        self, a: torch.Tensor, aq: torch.Tensor, M: int, N: int, K: int,\n+        topk: int, global_num_experts: int, local_num_experts: int\n+    ) -> tuple[tuple[int, ...], tuple[int, ...], tuple[int, ...], torch.dtype]:\n+        # We use global_num_experts due to how moe_align_block_size handles\n+        # expert_maps.\n+        # TODO(shuw): Just 0 Here\n+        num_experts = global_num_experts\n+        block_m = self.block_shape[0]\n+        M_sum = (M * topk) + num_experts * (block_m - 1)\n+        M_sum = round_up(M_sum, block_m)\n+        workspace1 = (M_sum, max(N * 2, K))\n+        workspace2 = (M_sum, max(N, K))\n+        output = (M * topk, K)\n+        return (workspace1, workspace2, output, a.dtype)\n+\n+    def apply(\n+        self,\n+        # output: torch.Tensor,\n+        hidden_states: torch.Tensor,\n+        w1: torch.Tensor,\n+        w2: torch.Tensor,\n+        topk_ids: torch.Tensor,\n+        topk_weights: torch.Tensor,\n+        activation: str,\n+        global_num_experts: int,\n+        w1_scale: Optional[torch.Tensor],\n+        w2_scale: Optional[torch.Tensor],\n+        w1_zp: Optional[torch.Tensor],\n+        w2_zp: Optional[torch.Tensor],\n+        a1_scale: Optional[torch.Tensor],\n+        a2_scale: Optional[torch.Tensor],\n+        expert_num_tokens: Optional[torch.Tensor],\n+        g1_alphas: torch.Tensor,\n+        g2_alphas: torch.Tensor,\n+        input_sf: torch.Tensor,\n+        out_dtype: torch.dtype,\n+        ep_rank: Optional[int] = 0,\n+        ep_size: Optional[int] = 1,\n+        tp_rank: Optional[int] = 0,\n+        tp_size: Optional[int] = 1,\n+    ):\n+        # Flashinfer CUTLASS kernel takes scalar global scales,\n+        # min because inv_scale. \n+                            \n+        quant_scales=[\n+            torch.min(a1_scale),\n+            w1_scale.view(torch.int32),\n+            g1_alphas,\n+            torch.min(a2_scale),\n+            w2_scale.view(torch.int32),\n+            g2_alphas,\n+        ]\n+        # Flashinfer CUTLASS moe takes in activations in BF16/Half/nvfp4 precision\n+        # print(\"calling flashinfer cutlass fused moe\n\"*100)\n+        return cutlass_fused_moe(\n+            hidden_states,\n+            topk_ids.to(torch.int),                                               \n+            topk_weights,                                                                                              \n+            w1.view(torch.long),                                                                         \n+            w2.view(torch.long),                                                                          \n+            out_dtype,                                                                  \n+            quant_scales=quant_scales,\n+            input_sf=input_sf,\n+            ep_size=ep_size,\n+            ep_rank=ep_rank,\n+            tp_size=tp_size,\n+            tp_rank=tp_rank,\n+        )[0]",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2164750244",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20037,
        "pr_file": "vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py",
        "discussion_id": "2164750244",
        "commented_code": "@@ -0,0 +1,253 @@\n+# SPDX-License-Identifier: Apache-2.0\n+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n+import functools\n+import importlib.util\n+from typing import Optional\n+\n+import torch\n+\n+import vllm.model_executor.layers.fused_moe.modular_kernel as mk\n+from vllm.logger import init_logger\n+from vllm.model_executor.layers.fused_moe.moe_permute_unpermute import (\n+    _moe_permute)\n+from vllm.model_executor.layers.fused_moe.flashinfer_cutlass_prepare_finalize import (\n+    FlashInferCutlassMoEPrepareAndFinalizeNoEP)\n+from vllm.model_executor.layers.fused_moe.utils import (\n+    _resize_cache, per_token_group_quant_fp8)\n+from vllm.utils import round_up\n+\n+logger = init_logger(__name__)\n+\n+from typing import TYPE_CHECKING\n+try:\n+    from flashinfer.fused_moe import cutlass_fused_moe as cutlass_fused_moe\n+    from flashinfer import fp4_quantize as fp4_quantize\n+except ImportError:\n+    if not TYPE_CHECKING:\n+        cutlass_fused_moe = None\n+\n+\n+\n+has_flashinfer_cutlass_fused_moe = cutlass_fused_moe is not None\n+def _valid_flashinfer_fused_moe(hidden_states: torch.Tensor, w1: torch.Tensor,\n+                     w2: torch.Tensor) -> bool:\n+    \"\"\"\n+    Check if the given problem size is supported by the DeepGemm grouped\n+    gemm kernel.  All of M, N, K and the quantization block_shape must be\n+    aligned by `dg.get_m_alignment_for_contiguous_layout()`.\n+    \"\"\"\n+    if not has_flashinfer_cutlass_fused_moe:\n+        logger.debug(\"FlashInferExperts disabled: flashinfer_cutlass_fused_moe not available.\")\n+        return False    \n+    # Data type checks\n+    if (w1.dtype != torch.uint8 or w2.dtype != torch.uint8 or\n+        hidden_states.dtype not in [torch.float32, torch.float16, torch.bfloat16]):\n+        logger.debug(f\"FlashInferExperts disabled: w1/w2 must be torch.uint8 (got w1={w1.dtype}, w2={w2.dtype}), \"\n+                     f\"hidden_states must be float32, float16, or bfloat16 (got {hidden_states.dtype}).\")\n+        return False\n+    return True\n+\n+class FlashInferExperts(mk.FusedMoEPermuteExpertsUnpermute):\n+    def __init__(self):\n+        super().__init__()\n+\n+\n+    def supports_chunking(self) -> bool:\n+        return True\n+\n+    def workspace_shapes(\n+        self, a: torch.Tensor, aq: torch.Tensor, M: int, N: int, K: int,\n+        topk: int, global_num_experts: int, local_num_experts: int\n+    ) -> tuple[tuple[int, ...], tuple[int, ...], tuple[int, ...], torch.dtype]:\n+        # We use global_num_experts due to how moe_align_block_size handles\n+        # expert_maps.\n+        # TODO(shuw): Just 0 Here\n+        num_experts = global_num_experts\n+        block_m = self.block_shape[0]\n+        M_sum = (M * topk) + num_experts * (block_m - 1)\n+        M_sum = round_up(M_sum, block_m)\n+        workspace1 = (M_sum, max(N * 2, K))\n+        workspace2 = (M_sum, max(N, K))\n+        output = (M * topk, K)\n+        return (workspace1, workspace2, output, a.dtype)\n+\n+    def apply(\n+        self,\n+        # output: torch.Tensor,\n+        hidden_states: torch.Tensor,\n+        w1: torch.Tensor,\n+        w2: torch.Tensor,\n+        topk_ids: torch.Tensor,\n+        topk_weights: torch.Tensor,\n+        activation: str,\n+        global_num_experts: int,\n+        w1_scale: Optional[torch.Tensor],\n+        w2_scale: Optional[torch.Tensor],\n+        w1_zp: Optional[torch.Tensor],\n+        w2_zp: Optional[torch.Tensor],\n+        a1_scale: Optional[torch.Tensor],\n+        a2_scale: Optional[torch.Tensor],\n+        expert_num_tokens: Optional[torch.Tensor],\n+        g1_alphas: torch.Tensor,\n+        g2_alphas: torch.Tensor,\n+        input_sf: torch.Tensor,\n+        out_dtype: torch.dtype,\n+        ep_rank: Optional[int] = 0,\n+        ep_size: Optional[int] = 1,\n+        tp_rank: Optional[int] = 0,\n+        tp_size: Optional[int] = 1,\n+    ):\n+        # Flashinfer CUTLASS kernel takes scalar global scales,\n+        # min because inv_scale. \n+                            \n+        quant_scales=[\n+            torch.min(a1_scale),\n+            w1_scale.view(torch.int32),\n+            g1_alphas,\n+            torch.min(a2_scale),\n+            w2_scale.view(torch.int32),\n+            g2_alphas,\n+        ]\n+        # Flashinfer CUTLASS moe takes in activations in BF16/Half/nvfp4 precision\n+        # print(\"calling flashinfer cutlass fused moe\\n\"*100)\n+        return cutlass_fused_moe(\n+            hidden_states,\n+            topk_ids.to(torch.int),                                               \n+            topk_weights,                                                                                              \n+            w1.view(torch.long),                                                                         \n+            w2.view(torch.long),                                                                          \n+            out_dtype,                                                                  \n+            quant_scales=quant_scales,\n+            input_sf=input_sf,\n+            ep_size=ep_size,\n+            ep_rank=ep_rank,\n+            tp_size=tp_size,\n+            tp_rank=tp_rank,\n+        )[0]",
        "comment_created_at": "2025-06-24T19:32:54+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe use of `.view(torch.int32)` on `w1_scale`/`w2_scale` and `.view(torch.long)` on `w1`/`w2` appears to be an unsafe method for passing tensor pointers to the underlying C++ kernel. This approach relies on the internal memory layout of PyTorch tensors, which can be brittle and may break with future PyTorch updates. It also makes the code difficult to understand and maintain.\n\nA safer and more standard approach is to use `tensor.data_ptr()` to get the memory address and pass it as an integer if the kernel expects a pointer.\n\nIf the `flashinfer` kernel API requires this specific `view` usage, it would be beneficial to add a comment explaining this requirement and the associated risks. However, I strongly recommend refactoring this to use `data_ptr()` if possible for better long-term stability.",
        "pr_file_module": null
      }
    ]
  }
]