[
  {
    "discussion_id": "2190461673",
    "pr_number": 20260,
    "pr_file": "vllm/entrypoints/openai/protocol.py",
    "created_at": "2025-07-07T15:51:11+00:00",
    "commented_code": "class ScoreRequest(OpenAIBaseModel):\n     model: Optional[str] = None\n-    text_1: Union[list[str], str]\n-    text_2: Union[list[str], str]\n+    data_1: Union[list[str], str, ScoreMultiModalParam]\n+    data_2: Union[list[str], str, ScoreMultiModalParam]",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2190461673",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20260,
        "pr_file": "vllm/entrypoints/openai/protocol.py",
        "discussion_id": "2190461673",
        "commented_code": "@@ -1261,15 +1262,21 @@ def to_pooling_params(self):\n \n class ScoreRequest(OpenAIBaseModel):\n     model: Optional[str] = None\n-    text_1: Union[list[str], str]\n-    text_2: Union[list[str], str]\n+    data_1: Union[list[str], str, ScoreMultiModalParam]\n+    data_2: Union[list[str], str, ScoreMultiModalParam]",
        "comment_created_at": "2025-07-07T15:51:11+00:00",
        "comment_author": "DarkLight1337",
        "comment_body": "We should continue accepting `text_1` and `text_2` to avoid breaking existing users' code. If you want to change the key, it should be done in accordance with [our deprecation policy](https://docs.vllm.ai/en/latest/contributing/deprecation_policy.html).",
        "pr_file_module": null
      },
      {
        "comment_id": "2190551152",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20260,
        "pr_file": "vllm/entrypoints/openai/protocol.py",
        "discussion_id": "2190461673",
        "commented_code": "@@ -1261,15 +1262,21 @@ def to_pooling_params(self):\n \n class ScoreRequest(OpenAIBaseModel):\n     model: Optional[str] = None\n-    text_1: Union[list[str], str]\n-    text_2: Union[list[str], str]\n+    data_1: Union[list[str], str, ScoreMultiModalParam]\n+    data_2: Union[list[str], str, ScoreMultiModalParam]",
        "comment_created_at": "2025-07-07T16:24:07+00:00",
        "comment_author": "shineran96",
        "comment_body": "Yeah, agreed. The API  name changes are primarily to make the multimodal capabilities more discoverable to users, but we can certainly keep the current name to maintains consistency with existing user code while also matching the usage of other rerank models.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2173093172",
    "pr_number": 20206,
    "pr_file": "vllm/entrypoints/openai/cli_args.py",
    "created_at": "2025-06-28T02:40:38+00:00",
    "commented_code": "adapter_list.append(PromptAdapterPath(name, path))\n         setattr(namespace, self.dest, adapter_list)\n \n+        \n+@dataclass\n+class FrontendArgs:\n+    \"\"\"Arguments for the OpenAI-compatible frontend server.\"\"\"\n+    host: Optional[str] = None\n+    \"\"\"Host name.\"\"\"\n+    port: int = 8000\n+    \"\"\"Port number.\"\"\"\n+    uvicorn_log_level: Literal[\"debug\", \"info\", \"warning\", \"error\", \"critical\", \"trace\"] = \"info\"\n+    \"\"\"Log level for uvicorn.\"\"\"\n+    disable_uvicorn_access_log: bool = False\n+    \"\"\"Disable uvicorn access log.\"\"\"\n+    allow_credentials: bool = False\n+    \"\"\"Allow credentials.\"\"\"\n+    allowed_origins: Optional[list[str]] = None\n+    \"\"\"Allowed origins.\"\"\"\n+    allowed_methods: Optional[list[str]] = None\n+    \"\"\"Allowed methods.\"\"\"\n+    allowed_headers: Optional[list[str]] = None\n+    \"\"\"Allowed headers.\"\"\"\n+    api_key: Optional[str] = None\n+    \"\"\"If provided, the server will require this key to be presented in the header.\"\"\"\n+    lora_modules: Optional[list[LoRAModulePath]] = None\n+    \"\"\"LoRA modules configurations in either 'name=path' format or JSON format or JSON list format. Example (old format): ``'name=path'`` Example (new format): ``{\\\"name\\\": \\\"name\\\", \\\"path\\\": \\\"lora_path\\\", \\\"base_model_name\\\": \\\"id\\\"}``\"\"\"\n+    prompt_adapters: Optional[list[PromptAdapterPath]] = None\n+    \"\"\"Prompt adapter configurations in the format name=path. Multiple adapters can be specified.\"\"\"\n+    chat_template: Optional[str] = None\n+    \"\"\"The file path to the chat template, or the template in single-line form for the specified model.\"\"\"\n+    chat_template_content_format: ChatTemplateContentFormatOption = \"auto\"\n+    \"\"\"The format to render message content within a chat template.\n \n-def make_arg_parser(parser: FlexibleArgumentParser) -> FlexibleArgumentParser:\n-    parser.add_argument(\"--host\",\n-                        type=optional_type(str),\n-                        default=None,\n-                        help=\"Host name.\")\n-    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port number.\")\n-    parser.add_argument(\n-        \"--uvicorn-log-level\",\n-        type=str,\n-        default=\"info\",\n-        choices=['debug', 'info', 'warning', 'error', 'critical', 'trace'],\n-        help=\"Log level for uvicorn.\")\n-    parser.add_argument(\"--disable-uvicorn-access-log\",\n-                        action=\"store_true\",\n-                        help=\"Disable uvicorn access log.\")\n-    parser.add_argument(\"--allow-credentials\",\n-                        action=\"store_true\",\n-                        help=\"Allow credentials.\")\n-    parser.add_argument(\"--allowed-origins\",\n-                        type=json.loads,\n-                        default=[\"*\"],\n-                        help=\"Allowed origins.\")\n-    parser.add_argument(\"--allowed-methods\",\n-                        type=json.loads,\n-                        default=[\"*\"],\n-                        help=\"Allowed methods.\")\n-    parser.add_argument(\"--allowed-headers\",\n-                        type=json.loads,\n-                        default=[\"*\"],\n-                        help=\"Allowed headers.\")\n-    parser.add_argument(\"--api-key\",\n-                        type=optional_type(str),\n-                        default=None,\n-                        help=\"If provided, the server will require this key \"\n-                        \"to be presented in the header.\")\n-    parser.add_argument(\n-        \"--lora-modules\",\n-        type=optional_type(str),\n-        default=None,\n-        nargs='+',\n-        action=LoRAParserAction,\n-        help=\"LoRA module configurations in either 'name=path' format\"\n-        \"or JSON format. \"\n-        \"Example (old format): ``'name=path'`` \"\n-        \"Example (new format): \"\n-        \"``{\\\"name\\\": \\\"name\\\", \\\"path\\\": \\\"lora_path\\\", \"\n-        \"\\\"base_model_name\\\": \\\"id\\\"}``\")\n-    parser.add_argument(\n-        \"--prompt-adapters\",\n-        type=optional_type(str),\n-        default=None,\n-        nargs='+',\n-        action=PromptAdapterParserAction,\n-        help=\"Prompt adapter configurations in the format name=path. \"\n-        \"Multiple adapters can be specified.\")\n-    parser.add_argument(\"--chat-template\",\n-                        type=optional_type(str),\n-                        default=None,\n-                        help=\"The file path to the chat template, \"\n-                        \"or the template in single-line form \"\n-                        \"for the specified model.\")\n-    parser.add_argument(\n-        '--chat-template-content-format',\n-        type=str,\n-        default=\"auto\",\n-        choices=get_args(ChatTemplateContentFormatOption),\n-        help='The format to render message content within a chat template.'\n-        '\n\n'\n-        '* \"string\" will render the content as a string. '\n-        'Example: ``\"Hello World\"``\n'\n-        '* \"openai\" will render the content as a list of dictionaries, '\n-        'similar to OpenAI schema. '\n-        'Example: ``[{\"type\": \"text\", \"text\": \"Hello world!\"}]``')\n-    parser.add_argument(\"--response-role\",\n-                        type=optional_type(str),\n-                        default=\"assistant\",\n-                        help=\"The role name to return if \"\n-                        \"``request.add_generation_prompt=true``.\")\n-    parser.add_argument(\"--ssl-keyfile\",\n-                        type=optional_type(str),\n-                        default=None,\n-                        help=\"The file path to the SSL key file.\")\n-    parser.add_argument(\"--ssl-certfile\",\n-                        type=optional_type(str),\n-                        default=None,\n-                        help=\"The file path to the SSL cert file.\")\n-    parser.add_argument(\"--ssl-ca-certs\",\n-                        type=optional_type(str),\n-                        default=None,\n-                        help=\"The CA certificates file.\")\n-    parser.add_argument(\n-        \"--enable-ssl-refresh\",\n-        action=\"store_true\",\n-        default=False,\n-        help=\"Refresh SSL Context when SSL certificate files change\")\n-    parser.add_argument(\n-        \"--ssl-cert-reqs\",\n-        type=int,\n-        default=int(ssl.CERT_NONE),\n-        help=\"Whether client certificate is required (see stdlib ssl module's).\"\n-    )\n-    parser.add_argument(\n-        \"--root-path\",\n-        type=optional_type(str),\n-        default=None,\n-        help=\"FastAPI root_path when app is behind a path based routing proxy.\"\n-    )\n-    parser.add_argument(\n-        \"--middleware\",\n-        type=optional_type(str),\n-        action=\"append\",\n-        default=[],\n-        help=\"Additional ASGI middleware to apply to the app. \"\n-        \"We accept multiple --middleware arguments. \"\n-        \"The value should be an import path. \"\n-        \"If a function is provided, vLLM will add it to the server \"\n-        \"using ``@app.middleware('http')``. \"\n-        \"If a class is provided, vLLM will add it to the server \"\n-        \"using ``app.add_middleware()``. \")\n-    parser.add_argument(\n-        \"--return-tokens-as-token-ids\",\n-        action=\"store_true\",\n-        help=\"When ``--max-logprobs`` is specified, represents single tokens \"\n-        \" as strings of the form 'token_id:{token_id}' so that tokens \"\n-        \"that are not JSON-encodable can be identified.\")\n-    parser.add_argument(\n-        \"--disable-frontend-multiprocessing\",\n-        action=\"store_true\",\n-        help=\"If specified, will run the OpenAI frontend server in the same \"\n-        \"process as the model serving engine.\")\n-    parser.add_argument(\n-        \"--enable-request-id-headers\",\n-        action=\"store_true\",\n-        help=\"If specified, API server will add X-Request-Id header to \"\n-        \"responses. Caution: this hurts performance at high QPS.\")\n-    parser.add_argument(\n-        \"--enable-auto-tool-choice\",\n-        action=\"store_true\",\n-        default=False,\n-        help=\"Enable auto tool choice for supported models. Use \"\n-        \"``--tool-call-parser`` to specify which parser to use.\")\n+* \"string\" will render the content as a string. Example: ``\"Hello World\"``\n+* \"openai\" will render the content as a list of dictionaries, similar to OpenAI schema. Example: ``[{\"type\": \"text\", \"text\": \"Hello world!\"}]``\"\"\"\n+    response_role: str = \"assistant\"\n+    \"\"\"The role name to return if ``request.add_generation_prompt=true``.\"\"\"\n+    ssl_keyfile: Optional[str] = None\n+    \"\"\"The file path to the SSL key file.\"\"\"\n+    ssl_certfile: Optional[str] = None\n+    \"\"\"The file path to the SSL cert file.\"\"\"\n+    ssl_ca_certs: Optional[str] = None\n+    \"\"\"The CA certificates file.\"\"\"\n+    enable_ssl_refresh: bool = False\n+    \"\"\"Refresh SSL Context when SSL certificate files change\"\"\"\n+    ssl_cert_reqs: int = int(ssl.CERT_NONE)\n+    \"\"\"Whether client certificate is required (see stdlib ssl module's).\"\"\"\n+    root_path: Optional[str] = None\n+    \"\"\"FastAPI root_path when app is behind a path based routing proxy.\"\"\"\n+    middleware: Optional[list[str]] = None\n+    \"\"\"Additional ASGI middleware to apply to the app. We accept multiple --middleware arguments. The value should be an import path. If a function is provided, vLLM will add it to the server using ``@app.middleware('http')``. If a class is provided, vLLM will add it to the server using ``app.add_middleware()``.\"\"\"\n+    return_tokens_as_token_ids: bool = False\n+    \"\"\"When ``--max-logprobs`` is specified, represents single tokens as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\"\"\"\n+    disable_frontend_multiprocessing: bool = False\n+    \"\"\"If specified, will run the OpenAI frontend server in the same process as the model serving engine.\"\"\"\n+    enable_request_id_headers: bool = False\n+    \"\"\"If specified, API server will add X-Request-Id header to responses. Caution: this hurts performance at high QPS.\"\"\"\n+    enable_auto_tool_choice: bool = False\n+    \"\"\"Enable auto tool choice for supported models. Use ``--tool-call-parser`` to specify which parser to use.\"\"\"\n+    tool_call_parser: Optional[str] = None\n+    \"\"\"Select the tool call parser depending on the model that you're using. This is used to parse the model-generated tool call into OpenAI API format. Required for ``--enable-auto-tool-choice``.\"\"\"\n+    tool_parser_plugin: str = \"\"\n+    \"\"\"Special the tool parser plugin write to parse the model-generated tool into OpenAI API format, the name register in this plugin can be used in ``--tool-call-parser``.\"\"\"\n+    log_config_file: Optional[str] = envs.VLLM_LOGGING_CONFIG_PATH\n+    \"\"\"Path to logging config JSON file for both vllm and uvicorn\"\"\"\n+    max_log_len: Optional[int] = None\n+    \"\"\"Max number of prompt characters or prompt ID numbers being printed in log. The default of None means unlimited.\"\"\"\n+    disable_fastapi_docs: bool = False\n+    \"\"\"Disable FastAPI's OpenAPI schema, Swagger UI, and ReDoc endpoint.\"\"\"\n+    enable_prompt_tokens_details: bool = False\n+    \"\"\"If set to True, enable prompt_tokens_details in usage.\"\"\"\n+    enable_server_load_tracking: bool = False\n+    \"\"\"If set to True, enable tracking server_load_metrics in the app state.\"\"\"\n+    \n+    def __post_init__(self):\n+        # Set default values for list fields that should not be None\n+        if self.allowed_origins is None:\n+            self.allowed_origins = [\"*\"]\n+        if self.allowed_methods is None:\n+            self.allowed_methods = [\"*\"]\n+        if self.allowed_headers is None:\n+            self.allowed_headers = [\"*\"]\n+        if self.middleware is None:\n+            self.middleware = []\n+    \n+    @staticmethod\n+    def add_cli_args(parser: FlexibleArgumentParser) -> FlexibleArgumentParser:\n+        from vllm.engine.arg_utils import get_kwargs\n+        \n+        frontend_kwargs = get_kwargs(FrontendArgs)\n+        frontend_group = parser.add_argument_group(\n+            title=\"Frontend\",\n+            description=FrontendArgs.__doc__,\n+        )\n+        \n+        frontend_group.add_argument(\"--host\", **frontend_kwargs[\"host\"])\n+        frontend_group.add_argument(\"--port\", **frontend_kwargs[\"port\"])\n+        frontend_group.add_argument(\"--uvicorn-log-level\", **frontend_kwargs[\"uvicorn_log_level\"])\n+        frontend_group.add_argument(\"--disable-uvicorn-access-log\", **frontend_kwargs[\"disable_uvicorn_access_log\"])\n+        frontend_group.add_argument(\"--allow-credentials\", **frontend_kwargs[\"allow_credentials\"])\n+        frontend_group.add_argument(\"--allowed-origins\", **frontend_kwargs[\"allowed_origins\"])\n+        frontend_group.add_argument(\"--allowed-methods\", **frontend_kwargs[\"allowed_methods\"])\n+        frontend_group.add_argument(\"--allowed-headers\", **frontend_kwargs[\"allowed_headers\"])",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2173093172",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20206,
        "pr_file": "vllm/entrypoints/openai/cli_args.py",
        "discussion_id": "2173093172",
        "commented_code": "@@ -81,205 +82,177 @@ def __call__(\n             adapter_list.append(PromptAdapterPath(name, path))\n         setattr(namespace, self.dest, adapter_list)\n \n+        \n+@dataclass\n+class FrontendArgs:\n+    \"\"\"Arguments for the OpenAI-compatible frontend server.\"\"\"\n+    host: Optional[str] = None\n+    \"\"\"Host name.\"\"\"\n+    port: int = 8000\n+    \"\"\"Port number.\"\"\"\n+    uvicorn_log_level: Literal[\"debug\", \"info\", \"warning\", \"error\", \"critical\", \"trace\"] = \"info\"\n+    \"\"\"Log level for uvicorn.\"\"\"\n+    disable_uvicorn_access_log: bool = False\n+    \"\"\"Disable uvicorn access log.\"\"\"\n+    allow_credentials: bool = False\n+    \"\"\"Allow credentials.\"\"\"\n+    allowed_origins: Optional[list[str]] = None\n+    \"\"\"Allowed origins.\"\"\"\n+    allowed_methods: Optional[list[str]] = None\n+    \"\"\"Allowed methods.\"\"\"\n+    allowed_headers: Optional[list[str]] = None\n+    \"\"\"Allowed headers.\"\"\"\n+    api_key: Optional[str] = None\n+    \"\"\"If provided, the server will require this key to be presented in the header.\"\"\"\n+    lora_modules: Optional[list[LoRAModulePath]] = None\n+    \"\"\"LoRA modules configurations in either 'name=path' format or JSON format or JSON list format. Example (old format): ``'name=path'`` Example (new format): ``{\\\"name\\\": \\\"name\\\", \\\"path\\\": \\\"lora_path\\\", \\\"base_model_name\\\": \\\"id\\\"}``\"\"\"\n+    prompt_adapters: Optional[list[PromptAdapterPath]] = None\n+    \"\"\"Prompt adapter configurations in the format name=path. Multiple adapters can be specified.\"\"\"\n+    chat_template: Optional[str] = None\n+    \"\"\"The file path to the chat template, or the template in single-line form for the specified model.\"\"\"\n+    chat_template_content_format: ChatTemplateContentFormatOption = \"auto\"\n+    \"\"\"The format to render message content within a chat template.\n \n-def make_arg_parser(parser: FlexibleArgumentParser) -> FlexibleArgumentParser:\n-    parser.add_argument(\"--host\",\n-                        type=optional_type(str),\n-                        default=None,\n-                        help=\"Host name.\")\n-    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port number.\")\n-    parser.add_argument(\n-        \"--uvicorn-log-level\",\n-        type=str,\n-        default=\"info\",\n-        choices=['debug', 'info', 'warning', 'error', 'critical', 'trace'],\n-        help=\"Log level for uvicorn.\")\n-    parser.add_argument(\"--disable-uvicorn-access-log\",\n-                        action=\"store_true\",\n-                        help=\"Disable uvicorn access log.\")\n-    parser.add_argument(\"--allow-credentials\",\n-                        action=\"store_true\",\n-                        help=\"Allow credentials.\")\n-    parser.add_argument(\"--allowed-origins\",\n-                        type=json.loads,\n-                        default=[\"*\"],\n-                        help=\"Allowed origins.\")\n-    parser.add_argument(\"--allowed-methods\",\n-                        type=json.loads,\n-                        default=[\"*\"],\n-                        help=\"Allowed methods.\")\n-    parser.add_argument(\"--allowed-headers\",\n-                        type=json.loads,\n-                        default=[\"*\"],\n-                        help=\"Allowed headers.\")\n-    parser.add_argument(\"--api-key\",\n-                        type=optional_type(str),\n-                        default=None,\n-                        help=\"If provided, the server will require this key \"\n-                        \"to be presented in the header.\")\n-    parser.add_argument(\n-        \"--lora-modules\",\n-        type=optional_type(str),\n-        default=None,\n-        nargs='+',\n-        action=LoRAParserAction,\n-        help=\"LoRA module configurations in either 'name=path' format\"\n-        \"or JSON format. \"\n-        \"Example (old format): ``'name=path'`` \"\n-        \"Example (new format): \"\n-        \"``{\\\"name\\\": \\\"name\\\", \\\"path\\\": \\\"lora_path\\\", \"\n-        \"\\\"base_model_name\\\": \\\"id\\\"}``\")\n-    parser.add_argument(\n-        \"--prompt-adapters\",\n-        type=optional_type(str),\n-        default=None,\n-        nargs='+',\n-        action=PromptAdapterParserAction,\n-        help=\"Prompt adapter configurations in the format name=path. \"\n-        \"Multiple adapters can be specified.\")\n-    parser.add_argument(\"--chat-template\",\n-                        type=optional_type(str),\n-                        default=None,\n-                        help=\"The file path to the chat template, \"\n-                        \"or the template in single-line form \"\n-                        \"for the specified model.\")\n-    parser.add_argument(\n-        '--chat-template-content-format',\n-        type=str,\n-        default=\"auto\",\n-        choices=get_args(ChatTemplateContentFormatOption),\n-        help='The format to render message content within a chat template.'\n-        '\\n\\n'\n-        '* \"string\" will render the content as a string. '\n-        'Example: ``\"Hello World\"``\\n'\n-        '* \"openai\" will render the content as a list of dictionaries, '\n-        'similar to OpenAI schema. '\n-        'Example: ``[{\"type\": \"text\", \"text\": \"Hello world!\"}]``')\n-    parser.add_argument(\"--response-role\",\n-                        type=optional_type(str),\n-                        default=\"assistant\",\n-                        help=\"The role name to return if \"\n-                        \"``request.add_generation_prompt=true``.\")\n-    parser.add_argument(\"--ssl-keyfile\",\n-                        type=optional_type(str),\n-                        default=None,\n-                        help=\"The file path to the SSL key file.\")\n-    parser.add_argument(\"--ssl-certfile\",\n-                        type=optional_type(str),\n-                        default=None,\n-                        help=\"The file path to the SSL cert file.\")\n-    parser.add_argument(\"--ssl-ca-certs\",\n-                        type=optional_type(str),\n-                        default=None,\n-                        help=\"The CA certificates file.\")\n-    parser.add_argument(\n-        \"--enable-ssl-refresh\",\n-        action=\"store_true\",\n-        default=False,\n-        help=\"Refresh SSL Context when SSL certificate files change\")\n-    parser.add_argument(\n-        \"--ssl-cert-reqs\",\n-        type=int,\n-        default=int(ssl.CERT_NONE),\n-        help=\"Whether client certificate is required (see stdlib ssl module's).\"\n-    )\n-    parser.add_argument(\n-        \"--root-path\",\n-        type=optional_type(str),\n-        default=None,\n-        help=\"FastAPI root_path when app is behind a path based routing proxy.\"\n-    )\n-    parser.add_argument(\n-        \"--middleware\",\n-        type=optional_type(str),\n-        action=\"append\",\n-        default=[],\n-        help=\"Additional ASGI middleware to apply to the app. \"\n-        \"We accept multiple --middleware arguments. \"\n-        \"The value should be an import path. \"\n-        \"If a function is provided, vLLM will add it to the server \"\n-        \"using ``@app.middleware('http')``. \"\n-        \"If a class is provided, vLLM will add it to the server \"\n-        \"using ``app.add_middleware()``. \")\n-    parser.add_argument(\n-        \"--return-tokens-as-token-ids\",\n-        action=\"store_true\",\n-        help=\"When ``--max-logprobs`` is specified, represents single tokens \"\n-        \" as strings of the form 'token_id:{token_id}' so that tokens \"\n-        \"that are not JSON-encodable can be identified.\")\n-    parser.add_argument(\n-        \"--disable-frontend-multiprocessing\",\n-        action=\"store_true\",\n-        help=\"If specified, will run the OpenAI frontend server in the same \"\n-        \"process as the model serving engine.\")\n-    parser.add_argument(\n-        \"--enable-request-id-headers\",\n-        action=\"store_true\",\n-        help=\"If specified, API server will add X-Request-Id header to \"\n-        \"responses. Caution: this hurts performance at high QPS.\")\n-    parser.add_argument(\n-        \"--enable-auto-tool-choice\",\n-        action=\"store_true\",\n-        default=False,\n-        help=\"Enable auto tool choice for supported models. Use \"\n-        \"``--tool-call-parser`` to specify which parser to use.\")\n+* \"string\" will render the content as a string. Example: ``\"Hello World\"``\n+* \"openai\" will render the content as a list of dictionaries, similar to OpenAI schema. Example: ``[{\"type\": \"text\", \"text\": \"Hello world!\"}]``\"\"\"\n+    response_role: str = \"assistant\"\n+    \"\"\"The role name to return if ``request.add_generation_prompt=true``.\"\"\"\n+    ssl_keyfile: Optional[str] = None\n+    \"\"\"The file path to the SSL key file.\"\"\"\n+    ssl_certfile: Optional[str] = None\n+    \"\"\"The file path to the SSL cert file.\"\"\"\n+    ssl_ca_certs: Optional[str] = None\n+    \"\"\"The CA certificates file.\"\"\"\n+    enable_ssl_refresh: bool = False\n+    \"\"\"Refresh SSL Context when SSL certificate files change\"\"\"\n+    ssl_cert_reqs: int = int(ssl.CERT_NONE)\n+    \"\"\"Whether client certificate is required (see stdlib ssl module's).\"\"\"\n+    root_path: Optional[str] = None\n+    \"\"\"FastAPI root_path when app is behind a path based routing proxy.\"\"\"\n+    middleware: Optional[list[str]] = None\n+    \"\"\"Additional ASGI middleware to apply to the app. We accept multiple --middleware arguments. The value should be an import path. If a function is provided, vLLM will add it to the server using ``@app.middleware('http')``. If a class is provided, vLLM will add it to the server using ``app.add_middleware()``.\"\"\"\n+    return_tokens_as_token_ids: bool = False\n+    \"\"\"When ``--max-logprobs`` is specified, represents single tokens as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\"\"\"\n+    disable_frontend_multiprocessing: bool = False\n+    \"\"\"If specified, will run the OpenAI frontend server in the same process as the model serving engine.\"\"\"\n+    enable_request_id_headers: bool = False\n+    \"\"\"If specified, API server will add X-Request-Id header to responses. Caution: this hurts performance at high QPS.\"\"\"\n+    enable_auto_tool_choice: bool = False\n+    \"\"\"Enable auto tool choice for supported models. Use ``--tool-call-parser`` to specify which parser to use.\"\"\"\n+    tool_call_parser: Optional[str] = None\n+    \"\"\"Select the tool call parser depending on the model that you're using. This is used to parse the model-generated tool call into OpenAI API format. Required for ``--enable-auto-tool-choice``.\"\"\"\n+    tool_parser_plugin: str = \"\"\n+    \"\"\"Special the tool parser plugin write to parse the model-generated tool into OpenAI API format, the name register in this plugin can be used in ``--tool-call-parser``.\"\"\"\n+    log_config_file: Optional[str] = envs.VLLM_LOGGING_CONFIG_PATH\n+    \"\"\"Path to logging config JSON file for both vllm and uvicorn\"\"\"\n+    max_log_len: Optional[int] = None\n+    \"\"\"Max number of prompt characters or prompt ID numbers being printed in log. The default of None means unlimited.\"\"\"\n+    disable_fastapi_docs: bool = False\n+    \"\"\"Disable FastAPI's OpenAPI schema, Swagger UI, and ReDoc endpoint.\"\"\"\n+    enable_prompt_tokens_details: bool = False\n+    \"\"\"If set to True, enable prompt_tokens_details in usage.\"\"\"\n+    enable_server_load_tracking: bool = False\n+    \"\"\"If set to True, enable tracking server_load_metrics in the app state.\"\"\"\n+    \n+    def __post_init__(self):\n+        # Set default values for list fields that should not be None\n+        if self.allowed_origins is None:\n+            self.allowed_origins = [\"*\"]\n+        if self.allowed_methods is None:\n+            self.allowed_methods = [\"*\"]\n+        if self.allowed_headers is None:\n+            self.allowed_headers = [\"*\"]\n+        if self.middleware is None:\n+            self.middleware = []\n+    \n+    @staticmethod\n+    def add_cli_args(parser: FlexibleArgumentParser) -> FlexibleArgumentParser:\n+        from vllm.engine.arg_utils import get_kwargs\n+        \n+        frontend_kwargs = get_kwargs(FrontendArgs)\n+        frontend_group = parser.add_argument_group(\n+            title=\"Frontend\",\n+            description=FrontendArgs.__doc__,\n+        )\n+        \n+        frontend_group.add_argument(\"--host\", **frontend_kwargs[\"host\"])\n+        frontend_group.add_argument(\"--port\", **frontend_kwargs[\"port\"])\n+        frontend_group.add_argument(\"--uvicorn-log-level\", **frontend_kwargs[\"uvicorn_log_level\"])\n+        frontend_group.add_argument(\"--disable-uvicorn-access-log\", **frontend_kwargs[\"disable_uvicorn_access_log\"])\n+        frontend_group.add_argument(\"--allow-credentials\", **frontend_kwargs[\"allow_credentials\"])\n+        frontend_group.add_argument(\"--allowed-origins\", **frontend_kwargs[\"allowed_origins\"])\n+        frontend_group.add_argument(\"--allowed-methods\", **frontend_kwargs[\"allowed_methods\"])\n+        frontend_group.add_argument(\"--allowed-headers\", **frontend_kwargs[\"allowed_headers\"])",
        "comment_created_at": "2025-06-28T02:40:38+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThis refactoring changes how list arguments like `--allowed-origins`, `--allowed-methods`, and `--allowed-headers` are parsed. Previously, they accepted a single JSON-formatted string (e.g., `'[\"http://localhost\"]'`). The new implementation appears to expect space-separated values (e.g., `http://localhost`). This is a breaking change for users of the CLI and should be documented. If this change is unintentional, you might need to adjust the `get_kwargs` logic or provide a custom `type` for these arguments to restore the `json.loads` behavior.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190298837",
    "pr_number": 20575,
    "pr_file": "vllm/entrypoints/openai/api_server.py",
    "created_at": "2025-07-07T14:39:54+00:00",
    "commented_code": "assert_never(generator)\n \n \n+@router.get(\"/get_tokenizer_info\")",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2190298837",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20575,
        "pr_file": "vllm/entrypoints/openai/api_server.py",
        "discussion_id": "2190298837",
        "commented_code": "@@ -523,6 +524,15 @@ async def detokenize(request: DetokenizeRequest, raw_request: Request):\n     assert_never(generator)\n \n \n+@router.get(\"/get_tokenizer_info\")",
        "comment_created_at": "2025-07-07T14:39:54+00:00",
        "comment_author": "DarkLight1337",
        "comment_body": "Let's make this opt-in, in case some users don't want to expose their chat template",
        "pr_file_module": null
      },
      {
        "comment_id": "2190389680",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20575,
        "pr_file": "vllm/entrypoints/openai/api_server.py",
        "discussion_id": "2190298837",
        "commented_code": "@@ -523,6 +524,15 @@ async def detokenize(request: DetokenizeRequest, raw_request: Request):\n     assert_never(generator)\n \n \n+@router.get(\"/get_tokenizer_info\")",
        "comment_created_at": "2025-07-07T15:20:47+00:00",
        "comment_author": "m-misiura",
        "comment_body": "Many thanks for a great suggestion @DarkLight1337 \r\n\r\nI've implemented this via a query param, i.e in the above example\r\n```bash\r\n vllm serve Qwen/Qwen2.5-0.5B-Instruct\r\n ```\r\n \r\n we now would get\r\n ```bash\r\n curl -X GET \"http://localhost:8000/get_tokenizer_info\" | jq\r\n ```\r\n which would expose basic (presumably non confidential information, i.e.\r\n ```\r\n {\r\n  \"tokenizer_class\": \"Qwen2TokenizerFast\",\r\n  \"unk_token\": null,\r\n  \"bos_token\": null,\r\n  \"eos_token\": \"<|im_end|>\",\r\n  \"pad_token\": \"<|endoftext|>\",\r\n  \"add_bos_token\": false,\r\n  \"add_prefix_space\": false,\r\n  \"additional_special_tokens\": [\r\n    \"<|im_start|>\",\r\n    \"<|im_end|>\",\r\n    \"<|object_ref_start|>\",\r\n    \"<|object_ref_end|>\",\r\n    \"<|box_start|>\",\r\n    \"<|box_end|>\",\r\n    \"<|quad_start|>\",\r\n    \"<|quad_end|>\",\r\n    \"<|vision_start|>\",\r\n    \"<|vision_end|>\",\r\n    \"<|vision_pad|>\",\r\n    \"<|image_pad|>\",\r\n    \"<|video_pad|>\"\r\n  ],\r\n  \"clean_up_tokenization_spaces\": false,\r\n  \"errors\": \"replace\",\r\n  \"model_max_length\": 131072,\r\n  \"split_special_tokens\": false,\r\n  \"max_loras\": 0,\r\n  \"truncation_side\": \"left\",\r\n  \"name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\"\r\n}\r\n```\r\n\r\nTo get the chat_template, u user would\r\n```bash\r\ncurl -X GET \"http://localhost:8000/get_tokenizer_info?include_chat_template=true\" | jq\r\n```\r\n\r\nwhich would yield\r\n```json\r\n{\r\n  \"tokenizer_class\": \"Qwen2TokenizerFast\",\r\n  \"unk_token\": null,\r\n  \"bos_token\": null,\r\n  \"eos_token\": \"<|im_end|>\",\r\n  \"pad_token\": \"<|endoftext|>\",\r\n  \"add_bos_token\": false,\r\n  \"add_prefix_space\": false,\r\n  \"additional_special_tokens\": [\r\n    \"<|im_start|>\",\r\n    \"<|im_end|>\",\r\n    \"<|object_ref_start|>\",\r\n    \"<|object_ref_end|>\",\r\n    \"<|box_start|>\",\r\n    \"<|box_end|>\",\r\n    \"<|quad_start|>\",\r\n    \"<|quad_end|>\",\r\n    \"<|vision_start|>\",\r\n    \"<|vision_end|>\",\r\n    \"<|vision_pad|>\",\r\n    \"<|image_pad|>\",\r\n    \"<|video_pad|>\"\r\n  ],\r\n  \"clean_up_tokenization_spaces\": false,\r\n  \"errors\": \"replace\",\r\n  \"model_max_length\": 131072,\r\n  \"split_special_tokens\": false,\r\n  \"max_loras\": 0,\r\n  \"truncation_side\": \"left\",\r\n  \"name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\r\n  \"chat_template\": \"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\\n    {%- else %}\\n        {{- '<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content %}\\n            {{- '\\\\n' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\", \\\"arguments\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- '}\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\"\r\n}\r\n ```\r\n \r\n Let me know what you think :)",
        "pr_file_module": null
      },
      {
        "comment_id": "2190430504",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20575,
        "pr_file": "vllm/entrypoints/openai/api_server.py",
        "discussion_id": "2190298837",
        "commented_code": "@@ -523,6 +524,15 @@ async def detokenize(request: DetokenizeRequest, raw_request: Request):\n     assert_never(generator)\n \n \n+@router.get(\"/get_tokenizer_info\")",
        "comment_created_at": "2025-07-07T15:39:54+00:00",
        "comment_author": "DarkLight1337",
        "comment_body": "I mean that we should disable this endpoint entirely unless a flag is passed to `vllm serve` command. Otherwise, people who are serving (not using) vLLM may unintentionally leak their chat templates.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2154956342",
    "pr_number": 19555,
    "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
    "created_at": "2025-06-18T15:46:01+00:00",
    "commented_code": "returned by the engine.\n         \"\"\"\n         return False, None\n+\n+    def get_finished(\n+        self,\n+        model_runner_output: ModelRunnerOutput,",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2154956342",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2154956342",
        "commented_code": "@@ -281,3 +284,21 @@ def request_finished(\n             returned by the engine.\n         \"\"\"\n         return False, None\n+\n+    def get_finished(\n+        self,\n+        model_runner_output: ModelRunnerOutput,",
        "comment_created_at": "2025-06-18T15:46:01+00:00",
        "comment_author": "sdavidbd",
        "comment_body": "Why `ModelRunnerOutput` and not `KVConnectorMetadata`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2157147373",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2154956342",
        "commented_code": "@@ -281,3 +284,21 @@ def request_finished(\n             returned by the engine.\n         \"\"\"\n         return False, None\n+\n+    def get_finished(\n+        self,\n+        model_runner_output: ModelRunnerOutput,",
        "comment_created_at": "2025-06-19T14:21:00+00:00",
        "comment_author": "orozery",
        "comment_body": "To allow the connector full awareness of the model output (maybe someone will want `sampled_token_ids`).\r\nSame way the connector gets full access of the `SchedulerOutput`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2160309533",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2154956342",
        "commented_code": "@@ -281,3 +284,21 @@ def request_finished(\n             returned by the engine.\n         \"\"\"\n         return False, None\n+\n+    def get_finished(\n+        self,\n+        model_runner_output: ModelRunnerOutput,",
        "comment_created_at": "2025-06-22T11:36:13+00:00",
        "comment_author": "sdavidbd",
        "comment_body": "The scheduler connector only gets access to `SchedulerOutput` for the purpose of creating metadata for the worker connector. Similarly, the worker connector should only access `ModelRunnerOutput` to generate metadata for the scheduler connector.\r\n\r\nIn any case, I don\u2019t think the scheduler connector should have access to `ModelRunnerOutput`. That separation helps keep responsibilities clear and avoids unnecessary coupling.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2178549662",
    "pr_number": 19555,
    "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
    "created_at": "2025-07-01T21:20:49+00:00",
    "commented_code": "\"\"\"\n         pass\n \n-    def get_finished(\n-        self, finished_req_ids: set[str]\n-    ) -> tuple[Optional[set[str]], Optional[set[str]]]:\n+    def build_worker_events(\n+        self, model_runner_output: ModelRunnerOutput\n+    ) -> Optional[KVConnectorWorkerEvents]:\n         \"\"\"\n-        Notifies worker-side connector ids of requests that have\n-        finished generating tokens.\n+        Get worker events for this step, to be sent to the scheduler.\n \n-        Returns:\n-            ids of requests that have finished asynchronous transfer\n-            (requests that previously returned True from request_finished()),\n-            tuple of (sending/saving ids, recving/loading ids).\n-            The finished saves/sends req ids must belong to a set provided in a\n-            call to this method (this call or a prior one).\n+        This function should NOT modify fields of its arguments.\n+\n+        Args:\n+            model_runner_output (ModelRunnerOutput):\n+                the model runner (worker) output object.\n         \"\"\"\n-        return None, None",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2178549662",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2178549662",
        "commented_code": "@@ -185,21 +211,19 @@ def wait_for_save(self):\n         \"\"\"\n         pass\n \n-    def get_finished(\n-        self, finished_req_ids: set[str]\n-    ) -> tuple[Optional[set[str]], Optional[set[str]]]:\n+    def build_worker_events(\n+        self, model_runner_output: ModelRunnerOutput\n+    ) -> Optional[KVConnectorWorkerEvents]:\n         \"\"\"\n-        Notifies worker-side connector ids of requests that have\n-        finished generating tokens.\n+        Get worker events for this step, to be sent to the scheduler.\n \n-        Returns:\n-            ids of requests that have finished asynchronous transfer\n-            (requests that previously returned True from request_finished()),\n-            tuple of (sending/saving ids, recving/loading ids).\n-            The finished saves/sends req ids must belong to a set provided in a\n-            call to this method (this call or a prior one).\n+        This function should NOT modify fields of its arguments.\n+\n+        Args:\n+            model_runner_output (ModelRunnerOutput):\n+                the model runner (worker) output object.\n         \"\"\"\n-        return None, None",
        "comment_created_at": "2025-07-01T21:20:49+00:00",
        "comment_author": "ApostaC",
        "comment_body": "Currently, lmcache_connector needs this `get_finished` interface, and removing it will break the LMCache functionality.\r\n\r\nCan we keep this interface, or have a default implementation for `build_worker_events` based on the output of `get_finished` to maintain backward compatibility?",
        "pr_file_module": null
      },
      {
        "comment_id": "2178566575",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2178549662",
        "commented_code": "@@ -185,21 +211,19 @@ def wait_for_save(self):\n         \"\"\"\n         pass\n \n-    def get_finished(\n-        self, finished_req_ids: set[str]\n-    ) -> tuple[Optional[set[str]], Optional[set[str]]]:\n+    def build_worker_events(\n+        self, model_runner_output: ModelRunnerOutput\n+    ) -> Optional[KVConnectorWorkerEvents]:\n         \"\"\"\n-        Notifies worker-side connector ids of requests that have\n-        finished generating tokens.\n+        Get worker events for this step, to be sent to the scheduler.\n \n-        Returns:\n-            ids of requests that have finished asynchronous transfer\n-            (requests that previously returned True from request_finished()),\n-            tuple of (sending/saving ids, recving/loading ids).\n-            The finished saves/sends req ids must belong to a set provided in a\n-            call to this method (this call or a prior one).\n+        This function should NOT modify fields of its arguments.\n+\n+        Args:\n+            model_runner_output (ModelRunnerOutput):\n+                the model runner (worker) output object.\n         \"\"\"\n-        return None, None",
        "comment_created_at": "2025-07-01T21:28:37+00:00",
        "comment_author": "ApostaC",
        "comment_body": "Also, the incoming nixl cpu KV connector uses this API",
        "pr_file_module": null
      },
      {
        "comment_id": "2179023726",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2178549662",
        "commented_code": "@@ -185,21 +211,19 @@ def wait_for_save(self):\n         \"\"\"\n         pass\n \n-    def get_finished(\n-        self, finished_req_ids: set[str]\n-    ) -> tuple[Optional[set[str]], Optional[set[str]]]:\n+    def build_worker_events(\n+        self, model_runner_output: ModelRunnerOutput\n+    ) -> Optional[KVConnectorWorkerEvents]:\n         \"\"\"\n-        Notifies worker-side connector ids of requests that have\n-        finished generating tokens.\n+        Get worker events for this step, to be sent to the scheduler.\n \n-        Returns:\n-            ids of requests that have finished asynchronous transfer\n-            (requests that previously returned True from request_finished()),\n-            tuple of (sending/saving ids, recving/loading ids).\n-            The finished saves/sends req ids must belong to a set provided in a\n-            call to this method (this call or a prior one).\n+        This function should NOT modify fields of its arguments.\n+\n+        Args:\n+            model_runner_output (ModelRunnerOutput):\n+                the model runner (worker) output object.\n         \"\"\"\n-        return None, None",
        "comment_created_at": "2025-07-02T04:32:19+00:00",
        "comment_author": "orozery",
        "comment_body": "I modified the nixl and multi connectors to use the new API (`build_worker_events` instead of `get_finished`).\r\nTo fix the lmcache connector you need to make a PR to both vllm and lmcache.\r\nBTW, looking at the current lmcache code (`97590e`) it seems it does not really use `get_finished`, but simply returns `None, None`.\r\n\r\nRegarding backward compatibility: from what I recall discussing with @njhill this API is declared beta so I think the keeping it clean and compact is better than backward compatibility. Of course, once it is declared stable, we should start guaranteeing backward compatibility.  ",
        "pr_file_module": null
      },
      {
        "comment_id": "2180488663",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2178549662",
        "commented_code": "@@ -185,21 +211,19 @@ def wait_for_save(self):\n         \"\"\"\n         pass\n \n-    def get_finished(\n-        self, finished_req_ids: set[str]\n-    ) -> tuple[Optional[set[str]], Optional[set[str]]]:\n+    def build_worker_events(\n+        self, model_runner_output: ModelRunnerOutput\n+    ) -> Optional[KVConnectorWorkerEvents]:\n         \"\"\"\n-        Notifies worker-side connector ids of requests that have\n-        finished generating tokens.\n+        Get worker events for this step, to be sent to the scheduler.\n \n-        Returns:\n-            ids of requests that have finished asynchronous transfer\n-            (requests that previously returned True from request_finished()),\n-            tuple of (sending/saving ids, recving/loading ids).\n-            The finished saves/sends req ids must belong to a set provided in a\n-            call to this method (this call or a prior one).\n+        This function should NOT modify fields of its arguments.\n+\n+        Args:\n+            model_runner_output (ModelRunnerOutput):\n+                the model runner (worker) output object.\n         \"\"\"\n-        return None, None",
        "comment_created_at": "2025-07-02T16:24:25+00:00",
        "comment_author": "ApostaC",
        "comment_body": "Got it. I'm not sure what's a correct way to use the two new APIs (e.g., what should be put into the return value? How to construct those data structures? What are the \"physical meaning\" of those data structures?)\r\n\r\nCan you help this by adding detailed doctoring to the newly added function? I believe this would help a lot for not only us but also the contributors in the future",
        "pr_file_module": null
      },
      {
        "comment_id": "2180560249",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2178549662",
        "commented_code": "@@ -185,21 +211,19 @@ def wait_for_save(self):\n         \"\"\"\n         pass\n \n-    def get_finished(\n-        self, finished_req_ids: set[str]\n-    ) -> tuple[Optional[set[str]], Optional[set[str]]]:\n+    def build_worker_events(\n+        self, model_runner_output: ModelRunnerOutput\n+    ) -> Optional[KVConnectorWorkerEvents]:\n         \"\"\"\n-        Notifies worker-side connector ids of requests that have\n-        finished generating tokens.\n+        Get worker events for this step, to be sent to the scheduler.\n \n-        Returns:\n-            ids of requests that have finished asynchronous transfer\n-            (requests that previously returned True from request_finished()),\n-            tuple of (sending/saving ids, recving/loading ids).\n-            The finished saves/sends req ids must belong to a set provided in a\n-            call to this method (this call or a prior one).\n+        This function should NOT modify fields of its arguments.\n+\n+        Args:\n+            model_runner_output (ModelRunnerOutput):\n+                the model runner (worker) output object.\n         \"\"\"\n-        return None, None",
        "comment_created_at": "2025-07-02T16:58:41+00:00",
        "comment_author": "ApostaC",
        "comment_body": "Also, although the connector API is beta, there are multiple parties working on it right now. Therefore, I think there should be a more formal community discussion before we decide to change the core logic here.",
        "pr_file_module": null
      }
    ]
  }
]