[
  {
    "discussion_id": "2190461673",
    "pr_number": 20260,
    "pr_file": "vllm/entrypoints/openai/protocol.py",
    "created_at": "2025-07-07T15:51:11+00:00",
    "commented_code": "class ScoreRequest(OpenAIBaseModel):\n     model: Optional[str] = None\n-    text_1: Union[list[str], str]\n-    text_2: Union[list[str], str]\n+    data_1: Union[list[str], str, ScoreMultiModalParam]\n+    data_2: Union[list[str], str, ScoreMultiModalParam]",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2190461673",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20260,
        "pr_file": "vllm/entrypoints/openai/protocol.py",
        "discussion_id": "2190461673",
        "commented_code": "@@ -1261,15 +1262,21 @@ def to_pooling_params(self):\n \n class ScoreRequest(OpenAIBaseModel):\n     model: Optional[str] = None\n-    text_1: Union[list[str], str]\n-    text_2: Union[list[str], str]\n+    data_1: Union[list[str], str, ScoreMultiModalParam]\n+    data_2: Union[list[str], str, ScoreMultiModalParam]",
        "comment_created_at": "2025-07-07T15:51:11+00:00",
        "comment_author": "DarkLight1337",
        "comment_body": "We should continue accepting `text_1` and `text_2` to avoid breaking existing users' code. If you want to change the key, it should be done in accordance with [our deprecation policy](https://docs.vllm.ai/en/latest/contributing/deprecation_policy.html).",
        "pr_file_module": null
      },
      {
        "comment_id": "2190551152",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20260,
        "pr_file": "vllm/entrypoints/openai/protocol.py",
        "discussion_id": "2190461673",
        "commented_code": "@@ -1261,15 +1262,21 @@ def to_pooling_params(self):\n \n class ScoreRequest(OpenAIBaseModel):\n     model: Optional[str] = None\n-    text_1: Union[list[str], str]\n-    text_2: Union[list[str], str]\n+    data_1: Union[list[str], str, ScoreMultiModalParam]\n+    data_2: Union[list[str], str, ScoreMultiModalParam]",
        "comment_created_at": "2025-07-07T16:24:07+00:00",
        "comment_author": "shineran96",
        "comment_body": "Yeah, agreed. The API  name changes are primarily to make the multimodal capabilities more discoverable to users, but we can certainly keep the current name to maintains consistency with existing user code while also matching the usage of other rerank models.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190298837",
    "pr_number": 20575,
    "pr_file": "vllm/entrypoints/openai/api_server.py",
    "created_at": "2025-07-07T14:39:54+00:00",
    "commented_code": "assert_never(generator)\n \n \n+@router.get(\"/get_tokenizer_info\")",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2190298837",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20575,
        "pr_file": "vllm/entrypoints/openai/api_server.py",
        "discussion_id": "2190298837",
        "commented_code": "@@ -523,6 +524,15 @@ async def detokenize(request: DetokenizeRequest, raw_request: Request):\n     assert_never(generator)\n \n \n+@router.get(\"/get_tokenizer_info\")",
        "comment_created_at": "2025-07-07T14:39:54+00:00",
        "comment_author": "DarkLight1337",
        "comment_body": "Let's make this opt-in, in case some users don't want to expose their chat template",
        "pr_file_module": null
      },
      {
        "comment_id": "2190389680",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20575,
        "pr_file": "vllm/entrypoints/openai/api_server.py",
        "discussion_id": "2190298837",
        "commented_code": "@@ -523,6 +524,15 @@ async def detokenize(request: DetokenizeRequest, raw_request: Request):\n     assert_never(generator)\n \n \n+@router.get(\"/get_tokenizer_info\")",
        "comment_created_at": "2025-07-07T15:20:47+00:00",
        "comment_author": "m-misiura",
        "comment_body": "Many thanks for a great suggestion @DarkLight1337 \r\n\r\nI've implemented this via a query param, i.e in the above example\r\n```bash\r\n vllm serve Qwen/Qwen2.5-0.5B-Instruct\r\n ```\r\n \r\n we now would get\r\n ```bash\r\n curl -X GET \"http://localhost:8000/get_tokenizer_info\" | jq\r\n ```\r\n which would expose basic (presumably non confidential information, i.e.\r\n ```\r\n {\r\n  \"tokenizer_class\": \"Qwen2TokenizerFast\",\r\n  \"unk_token\": null,\r\n  \"bos_token\": null,\r\n  \"eos_token\": \"<|im_end|>\",\r\n  \"pad_token\": \"<|endoftext|>\",\r\n  \"add_bos_token\": false,\r\n  \"add_prefix_space\": false,\r\n  \"additional_special_tokens\": [\r\n    \"<|im_start|>\",\r\n    \"<|im_end|>\",\r\n    \"<|object_ref_start|>\",\r\n    \"<|object_ref_end|>\",\r\n    \"<|box_start|>\",\r\n    \"<|box_end|>\",\r\n    \"<|quad_start|>\",\r\n    \"<|quad_end|>\",\r\n    \"<|vision_start|>\",\r\n    \"<|vision_end|>\",\r\n    \"<|vision_pad|>\",\r\n    \"<|image_pad|>\",\r\n    \"<|video_pad|>\"\r\n  ],\r\n  \"clean_up_tokenization_spaces\": false,\r\n  \"errors\": \"replace\",\r\n  \"model_max_length\": 131072,\r\n  \"split_special_tokens\": false,\r\n  \"max_loras\": 0,\r\n  \"truncation_side\": \"left\",\r\n  \"name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\"\r\n}\r\n```\r\n\r\nTo get the chat_template, u user would\r\n```bash\r\ncurl -X GET \"http://localhost:8000/get_tokenizer_info?include_chat_template=true\" | jq\r\n```\r\n\r\nwhich would yield\r\n```json\r\n{\r\n  \"tokenizer_class\": \"Qwen2TokenizerFast\",\r\n  \"unk_token\": null,\r\n  \"bos_token\": null,\r\n  \"eos_token\": \"<|im_end|>\",\r\n  \"pad_token\": \"<|endoftext|>\",\r\n  \"add_bos_token\": false,\r\n  \"add_prefix_space\": false,\r\n  \"additional_special_tokens\": [\r\n    \"<|im_start|>\",\r\n    \"<|im_end|>\",\r\n    \"<|object_ref_start|>\",\r\n    \"<|object_ref_end|>\",\r\n    \"<|box_start|>\",\r\n    \"<|box_end|>\",\r\n    \"<|quad_start|>\",\r\n    \"<|quad_end|>\",\r\n    \"<|vision_start|>\",\r\n    \"<|vision_end|>\",\r\n    \"<|vision_pad|>\",\r\n    \"<|image_pad|>\",\r\n    \"<|video_pad|>\"\r\n  ],\r\n  \"clean_up_tokenization_spaces\": false,\r\n  \"errors\": \"replace\",\r\n  \"model_max_length\": 131072,\r\n  \"split_special_tokens\": false,\r\n  \"max_loras\": 0,\r\n  \"truncation_side\": \"left\",\r\n  \"name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\r\n  \"chat_template\": \"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\\n    {%- else %}\\n        {{- '<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content %}\\n            {{- '\\\\n' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\", \\\"arguments\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- '}\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\"\r\n}\r\n ```\r\n \r\n Let me know what you think :)",
        "pr_file_module": null
      },
      {
        "comment_id": "2190430504",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20575,
        "pr_file": "vllm/entrypoints/openai/api_server.py",
        "discussion_id": "2190298837",
        "commented_code": "@@ -523,6 +524,15 @@ async def detokenize(request: DetokenizeRequest, raw_request: Request):\n     assert_never(generator)\n \n \n+@router.get(\"/get_tokenizer_info\")",
        "comment_created_at": "2025-07-07T15:39:54+00:00",
        "comment_author": "DarkLight1337",
        "comment_body": "I mean that we should disable this endpoint entirely unless a flag is passed to `vllm serve` command. Otherwise, people who are serving (not using) vLLM may unintentionally leak their chat templates.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2154956342",
    "pr_number": 19555,
    "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
    "created_at": "2025-06-18T15:46:01+00:00",
    "commented_code": "returned by the engine.\n         \"\"\"\n         return False, None\n+\n+    def get_finished(\n+        self,\n+        model_runner_output: ModelRunnerOutput,",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2154956342",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2154956342",
        "commented_code": "@@ -281,3 +284,21 @@ def request_finished(\n             returned by the engine.\n         \"\"\"\n         return False, None\n+\n+    def get_finished(\n+        self,\n+        model_runner_output: ModelRunnerOutput,",
        "comment_created_at": "2025-06-18T15:46:01+00:00",
        "comment_author": "sdavidbd",
        "comment_body": "Why `ModelRunnerOutput` and not `KVConnectorMetadata`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2157147373",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2154956342",
        "commented_code": "@@ -281,3 +284,21 @@ def request_finished(\n             returned by the engine.\n         \"\"\"\n         return False, None\n+\n+    def get_finished(\n+        self,\n+        model_runner_output: ModelRunnerOutput,",
        "comment_created_at": "2025-06-19T14:21:00+00:00",
        "comment_author": "orozery",
        "comment_body": "To allow the connector full awareness of the model output (maybe someone will want `sampled_token_ids`).\r\nSame way the connector gets full access of the `SchedulerOutput`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2160309533",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2154956342",
        "commented_code": "@@ -281,3 +284,21 @@ def request_finished(\n             returned by the engine.\n         \"\"\"\n         return False, None\n+\n+    def get_finished(\n+        self,\n+        model_runner_output: ModelRunnerOutput,",
        "comment_created_at": "2025-06-22T11:36:13+00:00",
        "comment_author": "sdavidbd",
        "comment_body": "The scheduler connector only gets access to `SchedulerOutput` for the purpose of creating metadata for the worker connector. Similarly, the worker connector should only access `ModelRunnerOutput` to generate metadata for the scheduler connector.\r\n\r\nIn any case, I don’t think the scheduler connector should have access to `ModelRunnerOutput`. That separation helps keep responsibilities clear and avoids unnecessary coupling.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2178549662",
    "pr_number": 19555,
    "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
    "created_at": "2025-07-01T21:20:49+00:00",
    "commented_code": "\"\"\"\n         pass\n \n-    def get_finished(\n-        self, finished_req_ids: set[str]\n-    ) -> tuple[Optional[set[str]], Optional[set[str]]]:\n+    def build_worker_events(\n+        self, model_runner_output: ModelRunnerOutput\n+    ) -> Optional[KVConnectorWorkerEvents]:\n         \"\"\"\n-        Notifies worker-side connector ids of requests that have\n-        finished generating tokens.\n+        Get worker events for this step, to be sent to the scheduler.\n \n-        Returns:\n-            ids of requests that have finished asynchronous transfer\n-            (requests that previously returned True from request_finished()),\n-            tuple of (sending/saving ids, recving/loading ids).\n-            The finished saves/sends req ids must belong to a set provided in a\n-            call to this method (this call or a prior one).\n+        This function should NOT modify fields of its arguments.\n+\n+        Args:\n+            model_runner_output (ModelRunnerOutput):\n+                the model runner (worker) output object.\n         \"\"\"\n-        return None, None",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2178549662",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2178549662",
        "commented_code": "@@ -185,21 +211,19 @@ def wait_for_save(self):\n         \"\"\"\n         pass\n \n-    def get_finished(\n-        self, finished_req_ids: set[str]\n-    ) -> tuple[Optional[set[str]], Optional[set[str]]]:\n+    def build_worker_events(\n+        self, model_runner_output: ModelRunnerOutput\n+    ) -> Optional[KVConnectorWorkerEvents]:\n         \"\"\"\n-        Notifies worker-side connector ids of requests that have\n-        finished generating tokens.\n+        Get worker events for this step, to be sent to the scheduler.\n \n-        Returns:\n-            ids of requests that have finished asynchronous transfer\n-            (requests that previously returned True from request_finished()),\n-            tuple of (sending/saving ids, recving/loading ids).\n-            The finished saves/sends req ids must belong to a set provided in a\n-            call to this method (this call or a prior one).\n+        This function should NOT modify fields of its arguments.\n+\n+        Args:\n+            model_runner_output (ModelRunnerOutput):\n+                the model runner (worker) output object.\n         \"\"\"\n-        return None, None",
        "comment_created_at": "2025-07-01T21:20:49+00:00",
        "comment_author": "ApostaC",
        "comment_body": "Currently, lmcache_connector needs this `get_finished` interface, and removing it will break the LMCache functionality.\r\n\r\nCan we keep this interface, or have a default implementation for `build_worker_events` based on the output of `get_finished` to maintain backward compatibility?",
        "pr_file_module": null
      },
      {
        "comment_id": "2178566575",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2178549662",
        "commented_code": "@@ -185,21 +211,19 @@ def wait_for_save(self):\n         \"\"\"\n         pass\n \n-    def get_finished(\n-        self, finished_req_ids: set[str]\n-    ) -> tuple[Optional[set[str]], Optional[set[str]]]:\n+    def build_worker_events(\n+        self, model_runner_output: ModelRunnerOutput\n+    ) -> Optional[KVConnectorWorkerEvents]:\n         \"\"\"\n-        Notifies worker-side connector ids of requests that have\n-        finished generating tokens.\n+        Get worker events for this step, to be sent to the scheduler.\n \n-        Returns:\n-            ids of requests that have finished asynchronous transfer\n-            (requests that previously returned True from request_finished()),\n-            tuple of (sending/saving ids, recving/loading ids).\n-            The finished saves/sends req ids must belong to a set provided in a\n-            call to this method (this call or a prior one).\n+        This function should NOT modify fields of its arguments.\n+\n+        Args:\n+            model_runner_output (ModelRunnerOutput):\n+                the model runner (worker) output object.\n         \"\"\"\n-        return None, None",
        "comment_created_at": "2025-07-01T21:28:37+00:00",
        "comment_author": "ApostaC",
        "comment_body": "Also, the incoming nixl cpu KV connector uses this API",
        "pr_file_module": null
      },
      {
        "comment_id": "2179023726",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2178549662",
        "commented_code": "@@ -185,21 +211,19 @@ def wait_for_save(self):\n         \"\"\"\n         pass\n \n-    def get_finished(\n-        self, finished_req_ids: set[str]\n-    ) -> tuple[Optional[set[str]], Optional[set[str]]]:\n+    def build_worker_events(\n+        self, model_runner_output: ModelRunnerOutput\n+    ) -> Optional[KVConnectorWorkerEvents]:\n         \"\"\"\n-        Notifies worker-side connector ids of requests that have\n-        finished generating tokens.\n+        Get worker events for this step, to be sent to the scheduler.\n \n-        Returns:\n-            ids of requests that have finished asynchronous transfer\n-            (requests that previously returned True from request_finished()),\n-            tuple of (sending/saving ids, recving/loading ids).\n-            The finished saves/sends req ids must belong to a set provided in a\n-            call to this method (this call or a prior one).\n+        This function should NOT modify fields of its arguments.\n+\n+        Args:\n+            model_runner_output (ModelRunnerOutput):\n+                the model runner (worker) output object.\n         \"\"\"\n-        return None, None",
        "comment_created_at": "2025-07-02T04:32:19+00:00",
        "comment_author": "orozery",
        "comment_body": "I modified the nixl and multi connectors to use the new API (`build_worker_events` instead of `get_finished`).\r\nTo fix the lmcache connector you need to make a PR to both vllm and lmcache.\r\nBTW, looking at the current lmcache code (`97590e`) it seems it does not really use `get_finished`, but simply returns `None, None`.\r\n\r\nRegarding backward compatibility: from what I recall discussing with @njhill this API is declared beta so I think the keeping it clean and compact is better than backward compatibility. Of course, once it is declared stable, we should start guaranteeing backward compatibility.  ",
        "pr_file_module": null
      },
      {
        "comment_id": "2180488663",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2178549662",
        "commented_code": "@@ -185,21 +211,19 @@ def wait_for_save(self):\n         \"\"\"\n         pass\n \n-    def get_finished(\n-        self, finished_req_ids: set[str]\n-    ) -> tuple[Optional[set[str]], Optional[set[str]]]:\n+    def build_worker_events(\n+        self, model_runner_output: ModelRunnerOutput\n+    ) -> Optional[KVConnectorWorkerEvents]:\n         \"\"\"\n-        Notifies worker-side connector ids of requests that have\n-        finished generating tokens.\n+        Get worker events for this step, to be sent to the scheduler.\n \n-        Returns:\n-            ids of requests that have finished asynchronous transfer\n-            (requests that previously returned True from request_finished()),\n-            tuple of (sending/saving ids, recving/loading ids).\n-            The finished saves/sends req ids must belong to a set provided in a\n-            call to this method (this call or a prior one).\n+        This function should NOT modify fields of its arguments.\n+\n+        Args:\n+            model_runner_output (ModelRunnerOutput):\n+                the model runner (worker) output object.\n         \"\"\"\n-        return None, None",
        "comment_created_at": "2025-07-02T16:24:25+00:00",
        "comment_author": "ApostaC",
        "comment_body": "Got it. I'm not sure what's a correct way to use the two new APIs (e.g., what should be put into the return value? How to construct those data structures? What are the \"physical meaning\" of those data structures?)\r\n\r\nCan you help this by adding detailed doctoring to the newly added function? I believe this would help a lot for not only us but also the contributors in the future",
        "pr_file_module": null
      },
      {
        "comment_id": "2180560249",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19555,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/base.py",
        "discussion_id": "2178549662",
        "commented_code": "@@ -185,21 +211,19 @@ def wait_for_save(self):\n         \"\"\"\n         pass\n \n-    def get_finished(\n-        self, finished_req_ids: set[str]\n-    ) -> tuple[Optional[set[str]], Optional[set[str]]]:\n+    def build_worker_events(\n+        self, model_runner_output: ModelRunnerOutput\n+    ) -> Optional[KVConnectorWorkerEvents]:\n         \"\"\"\n-        Notifies worker-side connector ids of requests that have\n-        finished generating tokens.\n+        Get worker events for this step, to be sent to the scheduler.\n \n-        Returns:\n-            ids of requests that have finished asynchronous transfer\n-            (requests that previously returned True from request_finished()),\n-            tuple of (sending/saving ids, recving/loading ids).\n-            The finished saves/sends req ids must belong to a set provided in a\n-            call to this method (this call or a prior one).\n+        This function should NOT modify fields of its arguments.\n+\n+        Args:\n+            model_runner_output (ModelRunnerOutput):\n+                the model runner (worker) output object.\n         \"\"\"\n-        return None, None",
        "comment_created_at": "2025-07-02T16:58:41+00:00",
        "comment_author": "ApostaC",
        "comment_body": "Also, although the connector API is beta, there are multiple parties working on it right now. Therefore, I think there should be a more formal community discussion before we decide to change the core logic here.",
        "pr_file_module": null
      }
    ]
  }
]