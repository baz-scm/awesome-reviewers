[
  {
    "discussion_id": "2161539732",
    "pr_number": 19983,
    "pr_file": "csrc/quantization/cutlass_w8a8/moe/get_group_starts.cuh",
    "created_at": "2025-06-23T12:46:47+00:00",
    "commented_code": "#include \"cutlass/bfloat16.h\"\n #include \"cutlass/float8.h\"\n \n+#include <iostream>",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2161539732",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19983,
        "pr_file": "csrc/quantization/cutlass_w8a8/moe/get_group_starts.cuh",
        "discussion_id": "2161539732",
        "commented_code": "@@ -8,6 +8,8 @@\n #include \"cutlass/bfloat16.h\"\n #include \"cutlass/float8.h\"\n \n+#include <iostream>",
        "comment_created_at": "2025-06-23T12:46:47+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe `<iostream>` header is included. Please remove it as it's not needed in CUDA header files and increases compile times.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2161539735",
    "pr_number": 19983,
    "pr_file": "csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x_with_blockwise_scaling.cu",
    "created_at": "2025-06-23T12:46:47+00:00",
    "commented_code": "+#include <cudaTypedefs.h>\n+\n+#include <c10/cuda/CUDAGuard.h>\n+#include <torch/all.h>\n+\n+#include \"cutlass/cutlass.h\"\n+#include \"grouped_mm_c3x_with_blockwise_scaling.cuh\"\n+\n+using namespace cute;\n+\n+namespace vllm::cutlass_moe::blockwise_scaling {\n+\n+template <typename InType, typename OutType,\n+          template <typename, typename, typename> typename Epilogue>\n+struct sm90_fp8_config_default {\n+  // M in (16, inf)\n+  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+  using KernelSchedule = cutlass::gemm::\n+      KernelPtrArrayTmaWarpSpecializedCooperativeFP8BlockScaledAccum;\n+  using EpilogueSchedule =\n+      cutlass::epilogue::PtrArrayTmaWarpSpecializedCooperative;\n+  using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;\n+  using ClusterShape = cute::Shape<cute::_1, cute::_2, cute::_1>;\n+\n+  using Cutlass3xGemm =\n+      cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+                                      ClusterShape, KernelSchedule,\n+                                      EpilogueSchedule>;\n+};\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_default {\n+//   // M in (16, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_256, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_2, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_M16 {\n+//   // M in [1, 16]\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_64, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_4, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_K8192 {\n+//   // K in [8192, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_8, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_N8192 {\n+//   // N in [8192, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_128, cute::_256>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_8, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2161539735",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19983,
        "pr_file": "csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x_with_blockwise_scaling.cu",
        "discussion_id": "2161539735",
        "commented_code": "@@ -0,0 +1,190 @@\n+#include <cudaTypedefs.h>\n+\n+#include <c10/cuda/CUDAGuard.h>\n+#include <torch/all.h>\n+\n+#include \"cutlass/cutlass.h\"\n+#include \"grouped_mm_c3x_with_blockwise_scaling.cuh\"\n+\n+using namespace cute;\n+\n+namespace vllm::cutlass_moe::blockwise_scaling {\n+\n+template <typename InType, typename OutType,\n+          template <typename, typename, typename> typename Epilogue>\n+struct sm90_fp8_config_default {\n+  // M in (16, inf)\n+  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+  using KernelSchedule = cutlass::gemm::\n+      KernelPtrArrayTmaWarpSpecializedCooperativeFP8BlockScaledAccum;\n+  using EpilogueSchedule =\n+      cutlass::epilogue::PtrArrayTmaWarpSpecializedCooperative;\n+  using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;\n+  using ClusterShape = cute::Shape<cute::_1, cute::_2, cute::_1>;\n+\n+  using Cutlass3xGemm =\n+      cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+                                      ClusterShape, KernelSchedule,\n+                                      EpilogueSchedule>;\n+};\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_default {\n+//   // M in (16, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_256, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_2, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_M16 {\n+//   // M in [1, 16]\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_64, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_4, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_K8192 {\n+//   // K in [8192, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_8, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_N8192 {\n+//   // N in [8192, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_128, cute::_256>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_8, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };",
        "comment_created_at": "2025-06-23T12:46:47+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis file contains large blocks of commented-out code defining alternative kernel configurations. Remove this commented-out code to improve readability and maintainability.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2161539738",
    "pr_number": 19983,
    "pr_file": "csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x_with_blockwise_scaling.cu",
    "created_at": "2025-06-23T12:46:47+00:00",
    "commented_code": "+#include <cudaTypedefs.h>\n+\n+#include <c10/cuda/CUDAGuard.h>\n+#include <torch/all.h>\n+\n+#include \"cutlass/cutlass.h\"\n+#include \"grouped_mm_c3x_with_blockwise_scaling.cuh\"\n+\n+using namespace cute;\n+\n+namespace vllm::cutlass_moe::blockwise_scaling {\n+\n+template <typename InType, typename OutType,\n+          template <typename, typename, typename> typename Epilogue>\n+struct sm90_fp8_config_default {\n+  // M in (16, inf)\n+  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+  using KernelSchedule = cutlass::gemm::\n+      KernelPtrArrayTmaWarpSpecializedCooperativeFP8BlockScaledAccum;\n+  using EpilogueSchedule =\n+      cutlass::epilogue::PtrArrayTmaWarpSpecializedCooperative;\n+  using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;\n+  using ClusterShape = cute::Shape<cute::_1, cute::_2, cute::_1>;\n+\n+  using Cutlass3xGemm =\n+      cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+                                      ClusterShape, KernelSchedule,\n+                                      EpilogueSchedule>;\n+};\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_default {\n+//   // M in (16, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_256, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_2, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_M16 {\n+//   // M in [1, 16]\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_64, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_4, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_K8192 {\n+//   // K in [8192, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_8, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_N8192 {\n+//   // N in [8192, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_128, cute::_256>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_8, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+template <typename InType, typename OutType>\n+void run_cutlass_moe_blockwise_mm_sm90(\n+    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,\n+    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,\n+    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,\n+    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,\n+    torch::Tensor const& b_strides, torch::Tensor const& c_strides,\n+    bool per_act_block) {\n+  TORCH_CHECK(a_tensors.size(0) > 0, \"No input A tensors provided.\");\n+  TORCH_CHECK(b_tensors.size(0) > 0, \"No input B tensors provided.\");\n+  TORCH_CHECK(out_tensors.size(0) > 0, \"No output tensors provided.\");\n+\n+  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn,\n+              \"A tensors must be of type float8_e4m3fn.\");\n+  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn,\n+              \"B tensors must be of type float8_e4m3fn.\");\n+\n+  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn);\n+  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn);",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2161539738",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19983,
        "pr_file": "csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x_with_blockwise_scaling.cu",
        "discussion_id": "2161539738",
        "commented_code": "@@ -0,0 +1,190 @@\n+#include <cudaTypedefs.h>\n+\n+#include <c10/cuda/CUDAGuard.h>\n+#include <torch/all.h>\n+\n+#include \"cutlass/cutlass.h\"\n+#include \"grouped_mm_c3x_with_blockwise_scaling.cuh\"\n+\n+using namespace cute;\n+\n+namespace vllm::cutlass_moe::blockwise_scaling {\n+\n+template <typename InType, typename OutType,\n+          template <typename, typename, typename> typename Epilogue>\n+struct sm90_fp8_config_default {\n+  // M in (16, inf)\n+  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+  using KernelSchedule = cutlass::gemm::\n+      KernelPtrArrayTmaWarpSpecializedCooperativeFP8BlockScaledAccum;\n+  using EpilogueSchedule =\n+      cutlass::epilogue::PtrArrayTmaWarpSpecializedCooperative;\n+  using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;\n+  using ClusterShape = cute::Shape<cute::_1, cute::_2, cute::_1>;\n+\n+  using Cutlass3xGemm =\n+      cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+                                      ClusterShape, KernelSchedule,\n+                                      EpilogueSchedule>;\n+};\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_default {\n+//   // M in (16, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_256, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_2, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_M16 {\n+//   // M in [1, 16]\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_64, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_4, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_K8192 {\n+//   // K in [8192, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_8, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_N8192 {\n+//   // N in [8192, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_128, cute::_256>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_8, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+template <typename InType, typename OutType>\n+void run_cutlass_moe_blockwise_mm_sm90(\n+    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,\n+    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,\n+    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,\n+    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,\n+    torch::Tensor const& b_strides, torch::Tensor const& c_strides,\n+    bool per_act_block) {\n+  TORCH_CHECK(a_tensors.size(0) > 0, \"No input A tensors provided.\");\n+  TORCH_CHECK(b_tensors.size(0) > 0, \"No input B tensors provided.\");\n+  TORCH_CHECK(out_tensors.size(0) > 0, \"No output tensors provided.\");\n+\n+  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn,\n+              \"A tensors must be of type float8_e4m3fn.\");\n+  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn,\n+              \"B tensors must be of type float8_e4m3fn.\");\n+\n+  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn);\n+  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn);",
        "comment_created_at": "2025-06-23T12:46:47+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThese `TORCH_CHECK`s are redundant. The same checks are performed on lines 115-118 with more descriptive error messages. Remove these duplicate lines.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2161539742",
    "pr_number": 19983,
    "pr_file": "csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x_with_blockwise_scaling.cu",
    "created_at": "2025-06-23T12:46:47+00:00",
    "commented_code": "+#include <cudaTypedefs.h>\n+\n+#include <c10/cuda/CUDAGuard.h>\n+#include <torch/all.h>\n+\n+#include \"cutlass/cutlass.h\"\n+#include \"grouped_mm_c3x_with_blockwise_scaling.cuh\"\n+\n+using namespace cute;\n+\n+namespace vllm::cutlass_moe::blockwise_scaling {\n+\n+template <typename InType, typename OutType,\n+          template <typename, typename, typename> typename Epilogue>\n+struct sm90_fp8_config_default {\n+  // M in (16, inf)\n+  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+  using KernelSchedule = cutlass::gemm::\n+      KernelPtrArrayTmaWarpSpecializedCooperativeFP8BlockScaledAccum;\n+  using EpilogueSchedule =\n+      cutlass::epilogue::PtrArrayTmaWarpSpecializedCooperative;\n+  using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;\n+  using ClusterShape = cute::Shape<cute::_1, cute::_2, cute::_1>;\n+\n+  using Cutlass3xGemm =\n+      cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+                                      ClusterShape, KernelSchedule,\n+                                      EpilogueSchedule>;\n+};\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_default {\n+//   // M in (16, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_256, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_2, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_M16 {\n+//   // M in [1, 16]\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_64, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_4, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_K8192 {\n+//   // K in [8192, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_8, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_N8192 {\n+//   // N in [8192, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_128, cute::_256>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_8, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+template <typename InType, typename OutType>\n+void run_cutlass_moe_blockwise_mm_sm90(\n+    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,\n+    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,\n+    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,\n+    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,\n+    torch::Tensor const& b_strides, torch::Tensor const& c_strides,\n+    bool per_act_block) {\n+  TORCH_CHECK(a_tensors.size(0) > 0, \"No input A tensors provided.\");\n+  TORCH_CHECK(b_tensors.size(0) > 0, \"No input B tensors provided.\");\n+  TORCH_CHECK(out_tensors.size(0) > 0, \"No output tensors provided.\");\n+\n+  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn,\n+              \"A tensors must be of type float8_e4m3fn.\");\n+  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn,\n+              \"B tensors must be of type float8_e4m3fn.\");\n+\n+  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn);\n+  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn);\n+\n+  //   using Cutlass3xGemmN8192 = typename sm90_fp8_config_N8192<\n+  //       InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;\n+  //   using Cutlass3xGemmK8192 = typename sm90_fp8_config_K8192<\n+  //       InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;\n+  //   using Cutlass3xGemmM16 = typename sm90_fp8_config_M16<\n+  //       InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;\n+  using Cutlass3xGemmDefault = typename sm90_fp8_config_default<\n+      InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;\n+\n+  uint32_t const m = a_tensors.size(0);\n+  uint32_t const n = out_tensors.size(1);\n+  uint32_t const k = a_tensors.size(1);\n+\n+  //   if (n >= 8192) {\n+  //     cutlass_blockwise_group_gemm_caller<Cutlass3xGemmN8192>(\n+  //         out_tensors, a_tensors, b_tensors, a_scales, b_scales,\n+  //         expert_offsets, problem_sizes, a_strides, b_strides, c_strides,\n+  //         per_act_block);\n+  //   } else if (k >= 8192) {\n+  //     cutlass_blockwise_group_gemm_caller<Cutlass3xGemmK8192>(\n+  //         out_tensors, a_tensors, b_tensors, a_scales, b_scales,\n+  //         expert_offsets, problem_sizes, a_strides, b_strides, c_strides,\n+  //         per_act_block);\n+  //   } else if (m <= 16) {\n+  //     cutlass_blockwise_group_gemm_caller<Cutlass3xGemmM16>(\n+  //         out_tensors, a_tensors, b_tensors, a_scales, b_scales,\n+  //         expert_offsets, problem_sizes, a_strides, b_strides, c_strides,\n+  //         per_act_block,);\n+  //   } else {\n+  cutlass_blockwise_group_gemm_caller<Cutlass3xGemmDefault>(\n+      out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,\n+      problem_sizes, a_strides, b_strides, c_strides, per_act_block);\n+  //   }",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2161539742",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19983,
        "pr_file": "csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x_with_blockwise_scaling.cu",
        "discussion_id": "2161539742",
        "commented_code": "@@ -0,0 +1,190 @@\n+#include <cudaTypedefs.h>\n+\n+#include <c10/cuda/CUDAGuard.h>\n+#include <torch/all.h>\n+\n+#include \"cutlass/cutlass.h\"\n+#include \"grouped_mm_c3x_with_blockwise_scaling.cuh\"\n+\n+using namespace cute;\n+\n+namespace vllm::cutlass_moe::blockwise_scaling {\n+\n+template <typename InType, typename OutType,\n+          template <typename, typename, typename> typename Epilogue>\n+struct sm90_fp8_config_default {\n+  // M in (16, inf)\n+  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+  using KernelSchedule = cutlass::gemm::\n+      KernelPtrArrayTmaWarpSpecializedCooperativeFP8BlockScaledAccum;\n+  using EpilogueSchedule =\n+      cutlass::epilogue::PtrArrayTmaWarpSpecializedCooperative;\n+  using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;\n+  using ClusterShape = cute::Shape<cute::_1, cute::_2, cute::_1>;\n+\n+  using Cutlass3xGemm =\n+      cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+                                      ClusterShape, KernelSchedule,\n+                                      EpilogueSchedule>;\n+};\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_default {\n+//   // M in (16, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_256, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_2, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_M16 {\n+//   // M in [1, 16]\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_64, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_4, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_K8192 {\n+//   // K in [8192, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_8, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+// template <typename InType, typename OutType,\n+//           template <typename, typename, typename> typename Epilogue>\n+// struct sm90_fp8_config_N8192 {\n+//   // N in [8192, inf)\n+//   static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+//   using KernelSchedule =\n+//       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;\n+//   using EpilogueSchedule =\n+//       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n+//   using TileShape = cute::Shape<cute::_64, cute::_128, cute::_256>;\n+//   using ClusterShape = cute::Shape<cute::_1, cute::_8, cute::_1>;\n+\n+//   using Cutlass3xGemm =\n+//       cutlass_3x_blockwise_group_gemm<InType, OutType, Epilogue, TileShape,\n+//       ClusterShape,\n+//                             KernelSchedule, EpilogueSchedule>;\n+// };\n+\n+template <typename InType, typename OutType>\n+void run_cutlass_moe_blockwise_mm_sm90(\n+    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,\n+    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,\n+    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,\n+    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,\n+    torch::Tensor const& b_strides, torch::Tensor const& c_strides,\n+    bool per_act_block) {\n+  TORCH_CHECK(a_tensors.size(0) > 0, \"No input A tensors provided.\");\n+  TORCH_CHECK(b_tensors.size(0) > 0, \"No input B tensors provided.\");\n+  TORCH_CHECK(out_tensors.size(0) > 0, \"No output tensors provided.\");\n+\n+  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn,\n+              \"A tensors must be of type float8_e4m3fn.\");\n+  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn,\n+              \"B tensors must be of type float8_e4m3fn.\");\n+\n+  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn);\n+  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn);\n+\n+  //   using Cutlass3xGemmN8192 = typename sm90_fp8_config_N8192<\n+  //       InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;\n+  //   using Cutlass3xGemmK8192 = typename sm90_fp8_config_K8192<\n+  //       InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;\n+  //   using Cutlass3xGemmM16 = typename sm90_fp8_config_M16<\n+  //       InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;\n+  using Cutlass3xGemmDefault = typename sm90_fp8_config_default<\n+      InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;\n+\n+  uint32_t const m = a_tensors.size(0);\n+  uint32_t const n = out_tensors.size(1);\n+  uint32_t const k = a_tensors.size(1);\n+\n+  //   if (n >= 8192) {\n+  //     cutlass_blockwise_group_gemm_caller<Cutlass3xGemmN8192>(\n+  //         out_tensors, a_tensors, b_tensors, a_scales, b_scales,\n+  //         expert_offsets, problem_sizes, a_strides, b_strides, c_strides,\n+  //         per_act_block);\n+  //   } else if (k >= 8192) {\n+  //     cutlass_blockwise_group_gemm_caller<Cutlass3xGemmK8192>(\n+  //         out_tensors, a_tensors, b_tensors, a_scales, b_scales,\n+  //         expert_offsets, problem_sizes, a_strides, b_strides, c_strides,\n+  //         per_act_block);\n+  //   } else if (m <= 16) {\n+  //     cutlass_blockwise_group_gemm_caller<Cutlass3xGemmM16>(\n+  //         out_tensors, a_tensors, b_tensors, a_scales, b_scales,\n+  //         expert_offsets, problem_sizes, a_strides, b_strides, c_strides,\n+  //         per_act_block,);\n+  //   } else {\n+  cutlass_blockwise_group_gemm_caller<Cutlass3xGemmDefault>(\n+      out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,\n+      problem_sizes, a_strides, b_strides, c_strides, per_act_block);\n+  //   }",
        "comment_created_at": "2025-06-23T12:46:47+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis block of commented-out code appears to be an alternative dispatch logic based on problem dimensions (M, N, K). Since it's not currently used, it should be removed to keep the code clean.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2183356358",
    "pr_number": 20447,
    "pr_file": "csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x_sm100.cu",
    "created_at": "2025-07-03T17:34:35+00:00",
    "commented_code": "+#include <cudaTypedefs.h>\n+\n+#include <c10/cuda/CUDAGuard.h>\n+#include <torch/all.h>\n+\n+#include \"cutlass/cutlass.h\"\n+#include \"grouped_mm_c3x.cuh\"\n+\n+using namespace cute;\n+\n+namespace {\n+\n+template <typename InType, typename OutType,\n+          template <typename, typename, typename> typename Epilogue>\n+struct sm100_fp8_config_default {\n+  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+  using KernelSchedule =\n+      cutlass::gemm::KernelPtrArrayTmaWarpSpecialized1SmSm100;\n+  using EpilogueSchedule = cutlass::epilogue::PtrArrayTmaWarpSpecialized1Sm;\n+  using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;\n+  using ClusterShape = cute::Shape<cute::_1, cute::_1, cute::_1>;\n+  using ArchTag = cutlass::arch::Sm100;\n+\n+  using Cutlass3xGemm =\n+      cutlass_3x_group_gemm<InType, OutType, ArchTag, Epilogue, TileShape,\n+                            ClusterShape, KernelSchedule, EpilogueSchedule>;\n+};\n+\n+template <typename InType, typename OutType>\n+void run_cutlass_moe_mm_sm100(\n+    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,\n+    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,\n+    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,\n+    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,\n+    torch::Tensor const& b_strides, torch::Tensor const& c_strides,\n+    bool per_act_token, bool per_out_ch) {\n+  TORCH_CHECK(a_tensors.size(0) > 0, \"No input A tensors provided.\");\n+  TORCH_CHECK(b_tensors.size(0) > 0, \"No input B tensors provided.\");\n+  TORCH_CHECK(out_tensors.size(0) > 0, \"No output tensors provided.\");\n+\n+  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn,\n+              \"A tensors must be of type float8_e4m3fn.\");\n+  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn,\n+              \"B tensors must be of type float8_e4m3fn.\");\n+\n+  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn);\n+  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn);",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2183356358",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20447,
        "pr_file": "csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x_sm100.cu",
        "discussion_id": "2183356358",
        "commented_code": "@@ -0,0 +1,90 @@\n+#include <cudaTypedefs.h>\n+\n+#include <c10/cuda/CUDAGuard.h>\n+#include <torch/all.h>\n+\n+#include \"cutlass/cutlass.h\"\n+#include \"grouped_mm_c3x.cuh\"\n+\n+using namespace cute;\n+\n+namespace {\n+\n+template <typename InType, typename OutType,\n+          template <typename, typename, typename> typename Epilogue>\n+struct sm100_fp8_config_default {\n+  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());\n+  using KernelSchedule =\n+      cutlass::gemm::KernelPtrArrayTmaWarpSpecialized1SmSm100;\n+  using EpilogueSchedule = cutlass::epilogue::PtrArrayTmaWarpSpecialized1Sm;\n+  using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;\n+  using ClusterShape = cute::Shape<cute::_1, cute::_1, cute::_1>;\n+  using ArchTag = cutlass::arch::Sm100;\n+\n+  using Cutlass3xGemm =\n+      cutlass_3x_group_gemm<InType, OutType, ArchTag, Epilogue, TileShape,\n+                            ClusterShape, KernelSchedule, EpilogueSchedule>;\n+};\n+\n+template <typename InType, typename OutType>\n+void run_cutlass_moe_mm_sm100(\n+    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,\n+    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,\n+    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,\n+    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,\n+    torch::Tensor const& b_strides, torch::Tensor const& c_strides,\n+    bool per_act_token, bool per_out_ch) {\n+  TORCH_CHECK(a_tensors.size(0) > 0, \"No input A tensors provided.\");\n+  TORCH_CHECK(b_tensors.size(0) > 0, \"No input B tensors provided.\");\n+  TORCH_CHECK(out_tensors.size(0) > 0, \"No output tensors provided.\");\n+\n+  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn,\n+              \"A tensors must be of type float8_e4m3fn.\");\n+  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn,\n+              \"B tensors must be of type float8_e4m3fn.\");\n+\n+  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn);\n+  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn);",
        "comment_created_at": "2025-07-03T17:34:35+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThese data type checks are redundant, as the same checks are already performed on lines 41-44 with more descriptive error messages. Removing these duplicate checks will improve code clarity.",
        "pr_file_module": null
      }
    ]
  }
]