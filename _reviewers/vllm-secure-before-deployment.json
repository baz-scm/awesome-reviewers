[
  {
    "discussion_id": "2186204047",
    "pr_number": 20504,
    "pr_file": "vllm/entrypoints/openai/api_server.py",
    "created_at": "2025-07-04T23:32:44+00:00",
    "commented_code": "return JSONResponse(content=ver)\n \n \n+@router.post(\"/v1/responses\",\n+             dependencies=[Depends(validate_json_request)],\n+             responses={\n+                 HTTPStatus.OK.value: {\n+                     \"content\": {\n+                         \"text/event-stream\": {}\n+                     }\n+                 },\n+                 HTTPStatus.BAD_REQUEST.value: {\n+                     \"model\": ErrorResponse\n+                 },\n+                 HTTPStatus.NOT_FOUND.value: {\n+                     \"model\": ErrorResponse\n+                 },\n+                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n+                     \"model\": ErrorResponse\n+                 },\n+             })\n+@with_cancellation\n+async def create_responses(request: ResponsesRequest, raw_request: Request):\n+    handler = responses(raw_request)\n+    if handler is None:\n+        return base(raw_request).create_error_response(\n+            message=\"The model does not support Responses API\")\n+\n+    print(request, raw_request)",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2186204047",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20504,
        "pr_file": "vllm/entrypoints/openai/api_server.py",
        "discussion_id": "2186204047",
        "commented_code": "@@ -531,6 +537,44 @@ async def show_version():\n     return JSONResponse(content=ver)\n \n \n+@router.post(\"/v1/responses\",\n+             dependencies=[Depends(validate_json_request)],\n+             responses={\n+                 HTTPStatus.OK.value: {\n+                     \"content\": {\n+                         \"text/event-stream\": {}\n+                     }\n+                 },\n+                 HTTPStatus.BAD_REQUEST.value: {\n+                     \"model\": ErrorResponse\n+                 },\n+                 HTTPStatus.NOT_FOUND.value: {\n+                     \"model\": ErrorResponse\n+                 },\n+                 HTTPStatus.INTERNAL_SERVER_ERROR.value: {\n+                     \"model\": ErrorResponse\n+                 },\n+             })\n+@with_cancellation\n+async def create_responses(request: ResponsesRequest, raw_request: Request):\n+    handler = responses(raw_request)\n+    if handler is None:\n+        return base(raw_request).create_error_response(\n+            message=\"The model does not support Responses API\")\n+\n+    print(request, raw_request)",
        "comment_created_at": "2025-07-04T23:32:44+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis `print` statement appears to be for debugging. It should be removed to prevent leaking request data into production logs.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2184782077",
    "pr_number": 20481,
    "pr_file": "vllm/collect_env.py",
    "created_at": "2025-07-04T08:57:58+00:00",
    "commented_code": "\"pynvml\",\n }\n \n+COMMAND_BLACKLIST = [\n+    'rm',\n+    'wget',\n+    'curl',\n+    'nc',\n+    'bash',\n+    'sh',\n+    'mkfs',\n+    'dd',\n+    'scp',\n+    'ftp',\n+    'poweroff',\n+    'reboot',\n+    'init',\n+]\n+\n+\n+def is_dangerous_cmd(cmd):\n+    cmd_base = os.path.basename(cmd)\n+    return cmd_base in COMMAND_BLACKLIST",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2184782077",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20481,
        "pr_file": "vllm/collect_env.py",
        "discussion_id": "2184782077",
        "commented_code": "@@ -92,29 +94,58 @@\n     \"pynvml\",\n }\n \n+COMMAND_BLACKLIST = [\n+    'rm',\n+    'wget',\n+    'curl',\n+    'nc',\n+    'bash',\n+    'sh',\n+    'mkfs',\n+    'dd',\n+    'scp',\n+    'ftp',\n+    'poweroff',\n+    'reboot',\n+    'init',\n+]\n+\n+\n+def is_dangerous_cmd(cmd):\n+    cmd_base = os.path.basename(cmd)\n+    return cmd_base in COMMAND_BLACKLIST",
        "comment_created_at": "2025-07-04T08:57:58+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe current implementation of `is_dangerous_cmd` can be bypassed using symbolic links. It checks `os.path.basename(cmd)`, but an attacker could create a symlink (e.g., `ln -s /bin/rm my_command`) which would not be caught by the blacklist but would execute the dangerous command.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2186454211",
    "pr_number": 20508,
    "pr_file": "vllm/entrypoints/openai/api_server.py",
    "created_at": "2025-07-05T02:49:50+00:00",
    "commented_code": "}\n \n if envs.VLLM_SERVER_DEV_MODE:\n+    logger.warning(\"SECURITY WARNING: Development endpoints are enabled! \"\n+                   \"This should NOT be used in production!\")",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2186454211",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20508,
        "pr_file": "vllm/entrypoints/openai/api_server.py",
        "discussion_id": "2186454211",
        "commented_code": "@@ -910,6 +910,8 @@ async def do_rerank_v2(request: RerankRequest, raw_request: Request):\n }\n \n if envs.VLLM_SERVER_DEV_MODE:\n+    logger.warning(\"SECURITY WARNING: Development endpoints are enabled! \"\n+                   \"This should NOT be used in production!\")",
        "comment_created_at": "2025-07-05T02:49:50+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis is a great addition for security awareness. To prevent accidental removal or changes to this important warning in the future, it would be beneficial to add a test case. The test should enable `VLLM_SERVER_DEV_MODE` and assert that this warning is logged.",
        "pr_file_module": null
      }
    ]
  }
]