[
  {
    "discussion_id": "2165694717",
    "pr_number": 20040,
    "pr_file": "docs/deployment/integrations/kuberay.md",
    "created_at": "2025-06-25T04:24:25+00:00",
    "commented_code": "+---\n+title: KubeRay\n+---\n+[](){ #deployment-kuberay }\n+\n+[KubeRay](https://github.com/ray-project/kuberay) provides a Kubernetes-native way to run vLLM workloads on Ray clusters.\n+A Ray cluster can be declared in YAML, and the operator then handles pod scheduling, networking configuration, restarts, and rolling upgrades\u2014all while preserving the familiar Kubernetes experience.\n+\n+---\n+\n+## Why KubeRay instead of manual scripts?\n+\n+| Feature | Manual scripts | KubeRay |\n+|---------|-----------------------------------------------------------|---------|\n+| Cluster bootstrap | Manually SSH into every node and run a script | One command to create or update the whole cluster: `kubectl apply -f cluster.yaml` |\n+| Fault-tolerance | Nodes must be restarted by hand | Pods are automatically rescheduled; head-node fail-over supported |\n+| Autoscaling | Unsupported | Native horizontal **and** vertical autoscaling via Ray Autoscaler & Kubernetes HPA |\n+| Upgrades | Tear down & re-create manually | Rolling updates handled by the operator |\n+| Monitoring | ad-hoc | Distributed observability with Ray Dashboard |\n+| Declarative config | Bash flags & environment variables | Git-ops-friendly YAML CRDs (RayCluster/RayService) |\n+\n+Using KubeRay reduces the operational burden and simplifies integration of Ray + vLLM with existing Kubernetes workflows (CI/CD, secrets, storage classes, etc.).\n+\n+---\n+\n+## Quick start\n+\n+1. Install the KubeRay operator (via Helm or `kubectl apply`).\n+2. Create a `RayService` that runs vLLM.\n+\n+```bash\n+# FIXME create this yaml before merging PR\n+kubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/refs/heads/master/ray-operator/config/samples/vllm/ray-service.vllm.yaml",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2165694717",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20040,
        "pr_file": "docs/deployment/integrations/kuberay.md",
        "discussion_id": "2165694717",
        "commented_code": "@@ -0,0 +1,61 @@\n+---\n+title: KubeRay\n+---\n+[](){ #deployment-kuberay }\n+\n+[KubeRay](https://github.com/ray-project/kuberay) provides a Kubernetes-native way to run vLLM workloads on Ray clusters.\n+A Ray cluster can be declared in YAML, and the operator then handles pod scheduling, networking configuration, restarts, and rolling upgrades\u2014all while preserving the familiar Kubernetes experience.\n+\n+---\n+\n+## Why KubeRay instead of manual scripts?\n+\n+| Feature | Manual scripts | KubeRay |\n+|---------|-----------------------------------------------------------|---------|\n+| Cluster bootstrap | Manually SSH into every node and run a script | One command to create or update the whole cluster: `kubectl apply -f cluster.yaml` |\n+| Fault-tolerance | Nodes must be restarted by hand | Pods are automatically rescheduled; head-node fail-over supported |\n+| Autoscaling | Unsupported | Native horizontal **and** vertical autoscaling via Ray Autoscaler & Kubernetes HPA |\n+| Upgrades | Tear down & re-create manually | Rolling updates handled by the operator |\n+| Monitoring | ad-hoc | Distributed observability with Ray Dashboard |\n+| Declarative config | Bash flags & environment variables | Git-ops-friendly YAML CRDs (RayCluster/RayService) |\n+\n+Using KubeRay reduces the operational burden and simplifies integration of Ray + vLLM with existing Kubernetes workflows (CI/CD, secrets, storage classes, etc.).\n+\n+---\n+\n+## Quick start\n+\n+1. Install the KubeRay operator (via Helm or `kubectl apply`).\n+2. Create a `RayService` that runs vLLM.\n+\n+```bash\n+# FIXME create this yaml before merging PR\n+kubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/refs/heads/master/ray-operator/config/samples/vllm/ray-service.vllm.yaml",
        "comment_created_at": "2025-06-25T04:24:25+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe `FIXME` comment indicates that this YAML file needs to be created before merging. Additionally, using `refs/heads/master` in documentation is generally not recommended as it points to a volatile branch. It's better to link to a stable release tag or a specific commit hash to ensure the example remains functional and consistent over time.\n\n```suggestion\n# TODO: Create this YAML file and update the URL to a stable release or specific commit hash.\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/vX.Y.Z/ray-operator/config/samples/vllm/ray-service.vllm.yaml\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189600990",
    "pr_number": 20563,
    "pr_file": "docs/features/structured_outputs.md",
    "created_at": "2025-07-07T10:19:13+00:00",
    "commented_code": "print(\"content: \", completion.choices[0].message.content)\n     ```\n \n-See also: [full example](https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs.html)\n+See also: [full example](../examples/online_serving/structured_outputs.html)",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2189600990",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20563,
        "pr_file": "docs/features/structured_outputs.md",
        "discussion_id": "2189600990",
        "commented_code": "@@ -200,7 +200,7 @@ Note that you can use reasoning with any provided structured outputs feature. Th\n     print(\"content: \", completion.choices[0].message.content)\n     ```\n \n-See also: [full example](https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs.html)\n+See also: [full example](../examples/online_serving/structured_outputs.html)",
        "comment_created_at": "2025-07-07T10:19:13+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThere's an inconsistency in the file extensions for the updated links. This link and the one on line 328 point to a `.html` file, while the link on line 160 points to a `.md` file.\n\nTo ensure consistency and that the links work correctly within the markdown source, they should probably all point to the markdown file (`.md`).\n\n```suggestion\nSee also: [full example](../examples/online_serving/structured_outputs.md)\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189600996",
    "pr_number": 20563,
    "pr_file": "docs/features/structured_outputs.md",
    "created_at": "2025-07-07T10:19:13+00:00",
    "commented_code": "print(outputs[0].outputs[0].text)\n     ```\n \n-See also: [full example](https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs.html)\n+See also: [full example](../examples/online_serving/structured_outputs.html)",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2189600996",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20563,
        "pr_file": "docs/features/structured_outputs.md",
        "discussion_id": "2189600996",
        "commented_code": "@@ -325,4 +325,4 @@ shown below:\n     print(outputs[0].outputs[0].text)\n     ```\n \n-See also: [full example](https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs.html)\n+See also: [full example](../examples/online_serving/structured_outputs.html)",
        "comment_created_at": "2025-07-07T10:19:13+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nAs mentioned in the comment for line 203, this link should likely point to a `.md` file for consistency with the change on line 160.\n\n```suggestion\nSee also: [full example](../examples/online_serving/structured_outputs.md)\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189600999",
    "pr_number": 20563,
    "pr_file": "docs/features/structured_outputs.md",
    "created_at": "2025-07-07T10:19:13+00:00",
    "commented_code": "print(completion.choices[0].message.content)\n     ```\n \n-See also: [full example](https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs.html)\n+See also: [full example](../examples/online_serving/structured_outputs.md)",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2189600999",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20563,
        "pr_file": "docs/features/structured_outputs.md",
        "discussion_id": "2189600999",
        "commented_code": "@@ -157,7 +157,7 @@ As an example, we can use to define a specific format of simplified SQL queries:\n     print(completion.choices[0].message.content)\n     ```\n \n-See also: [full example](https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs.html)\n+See also: [full example](../examples/online_serving/structured_outputs.md)",
        "comment_created_at": "2025-07-07T10:19:13+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThanks for updating the link to be relative! This is a great improvement for versioned documentation.\n\nI have a question about the path correctness. Based on the file structure provided in the context, the markdown file for this example seems to be located at `examples/online_serving/structured_outputs/README.md`.\n\nCould you please double-check if this link to `structured_outputs.md` will resolve correctly in the documentation build? It might need to be something like `../examples/online_serving/structured_outputs/README.md` or `../examples/online_serving/structured_outputs/` depending on how the documentation generator is configured.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189607799",
    "pr_number": 20564,
    "pr_file": "docs/ci/update_pytorch_version.md",
    "created_at": "2025-07-07T10:22:23+00:00",
    "commented_code": "it doesn't populate the cache, so re-running it to warm up the cache\n is ineffective.\n \n-While ongoing efforts like [#17419](https://github.com/vllm-project/vllm/issues/17419)\n+While ongoing efforts like [#17419](gh-issue:17419)",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2189607799",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20564,
        "pr_file": "docs/ci/update_pytorch_version.md",
        "discussion_id": "2189607799",
        "commented_code": "@@ -68,7 +67,7 @@ and timeout. Additionally, since vLLM's fastcheck pipeline runs in read-only mod\n it doesn't populate the cache, so re-running it to warm up the cache\n is ineffective.\n \n-While ongoing efforts like [#17419](https://github.com/vllm-project/vllm/issues/17419)\n+While ongoing efforts like [#17419](gh-issue:17419)",
        "comment_created_at": "2025-07-07T10:22:23+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nTo maintain consistency with other changes in this file, use the autolink syntax. Given the use of `<gh-pr:16859>` for `[#16859]` elsewhere, apply `<gh-issue:17419>` here.\n\n```suggestion\nWhile ongoing efforts like <gh-issue:17419>\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2186262325",
    "pr_number": 20506,
    "pr_file": "docs/contributing/incremental_build.md",
    "created_at": "2025-07-05T00:44:15+00:00",
    "commented_code": "VLLM_USE_PRECOMPILED=1 uv pip install -U -e . --torch-backend=auto\n     ```\n \n-2. **CUDA Toolkit:** Verify that the NVIDIA CUDA Toolkit is correctly installed and `nvcc` is accessible in your `PATH`. CMake relies on `nvcc` to compile CUDA code. You can typically find `nvcc` in `$CUDA_HOME/bin/nvcc` or by running `which nvcc`. If you encounter issues, refer to the [official CUDA Toolkit installation guides](https://developer.nvidia.com/cuda-toolkit-archive) and vLLM's main [GPU installation documentation](../getting_started/installation/gpu/cuda.inc.md#troubleshooting) for troubleshooting. The `CMAKE_CUDA_COMPILER` variable in your `CMakeUserPresets.json` should also point to your `nvcc` binary.\n+2. **CUDA Toolkit:** Verify that the NVIDIA CUDA Toolkit is correctly installed and `nvcc` is accessible in your `PATH`. CMake relies on `nvcc` to compile CUDA code. You can typically find `nvcc` in `$CUDA_HOME/bin/nvcc` or by running `which nvcc`. If you encounter issues, refer to the [official CUDA Toolkit installation guides](https://developer.nvidia.com/cuda-toolkit-archive) and vLLM's main [GPU installation documentation](../getting_started/installation/gpu.md#troubleshooting) for troubleshooting. The `CMAKE_CUDA_COMPILER` variable in your `CMakeUserPresets.json` should also point to your `nvcc` binary.",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2186262325",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20506,
        "pr_file": "docs/contributing/incremental_build.md",
        "discussion_id": "2186262325",
        "commented_code": "@@ -14,7 +14,7 @@ Before setting up the incremental build:\n     VLLM_USE_PRECOMPILED=1 uv pip install -U -e . --torch-backend=auto\n     ```\n \n-2. **CUDA Toolkit:** Verify that the NVIDIA CUDA Toolkit is correctly installed and `nvcc` is accessible in your `PATH`. CMake relies on `nvcc` to compile CUDA code. You can typically find `nvcc` in `$CUDA_HOME/bin/nvcc` or by running `which nvcc`. If you encounter issues, refer to the [official CUDA Toolkit installation guides](https://developer.nvidia.com/cuda-toolkit-archive) and vLLM's main [GPU installation documentation](../getting_started/installation/gpu/cuda.inc.md#troubleshooting) for troubleshooting. The `CMAKE_CUDA_COMPILER` variable in your `CMakeUserPresets.json` should also point to your `nvcc` binary.\n+2. **CUDA Toolkit:** Verify that the NVIDIA CUDA Toolkit is correctly installed and `nvcc` is accessible in your `PATH`. CMake relies on `nvcc` to compile CUDA code. You can typically find `nvcc` in `$CUDA_HOME/bin/nvcc` or by running `which nvcc`. If you encounter issues, refer to the [official CUDA Toolkit installation guides](https://developer.nvidia.com/cuda-toolkit-archive) and vLLM's main [GPU installation documentation](../getting_started/installation/gpu.md#troubleshooting) for troubleshooting. The `CMAKE_CUDA_COMPILER` variable in your `CMakeUserPresets.json` should also point to your `nvcc` binary.",
        "comment_created_at": "2025-07-05T00:44:15+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe link to `gpu.md#troubleshooting` might not directly guide users to the NVIDIA CUDA troubleshooting section, as the page uses tabs. Consider linking directly to the NVIDIA CUDA tab for better navigation.\n\n```suggestion\n2. **CUDA Toolkit:** Verify that the NVIDIA CUDA Toolkit is correctly installed and `nvcc` is accessible in your `PATH`. CMake relies on `nvcc` to compile CUDA code. You can typically find `nvcc` in `$CUDA_HOME/bin/nvcc` or by running `which nvcc`. If you encounter issues, refer to the [official CUDA Toolkit installation guides](https://developer.nvidia.com/cuda-toolkit-archive) and vLLM's main [GPU installation documentation](../getting_started/installation/gpu.md#nvidia-cuda) for troubleshooting. The `CMAKE_CUDA_COMPILER` variable in your `CMakeUserPresets.json` should also point to your `nvcc` binary.\n```",
        "pr_file_module": null
      }
    ]
  }
]