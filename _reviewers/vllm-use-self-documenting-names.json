[
  {
    "discussion_id": "2183786658",
    "pr_number": 19719,
    "pr_file": "vllm/compilation/decorators.py",
    "created_at": "2025-07-03T21:37:20+00:00",
    "commented_code": "_T = TypeVar(\"_T\", bound=type[nn.Module])\n \n \n+def skip_torch_compile(cls: _T) -> _T:\n+    cls._skip_compile_vllm = True\n+    return cls",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2183786658",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19719,
        "pr_file": "vllm/compilation/decorators.py",
        "discussion_id": "2183786658",
        "commented_code": "@@ -23,6 +23,11 @@\n _T = TypeVar(\"_T\", bound=type[nn.Module])\n \n \n+def skip_torch_compile(cls: _T) -> _T:\n+    cls._skip_compile_vllm = True\n+    return cls",
        "comment_created_at": "2025-07-03T21:37:20+00:00",
        "comment_author": "zou3519",
        "comment_body": "nit: API naming a bit weird to me, this is like: \"if this module has a support_torch_compile decorator directly applied on it, we will ignore it\"",
        "pr_file_module": null
      },
      {
        "comment_id": "2190701719",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19719,
        "pr_file": "vllm/compilation/decorators.py",
        "discussion_id": "2183786658",
        "commented_code": "@@ -23,6 +23,11 @@\n _T = TypeVar(\"_T\", bound=type[nn.Module])\n \n \n+def skip_torch_compile(cls: _T) -> _T:\n+    cls._skip_compile_vllm = True\n+    return cls",
        "comment_created_at": "2025-07-07T17:36:56+00:00",
        "comment_author": "sarckk",
        "comment_body": "how about `ignore_torch_compile`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2191324263",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19719,
        "pr_file": "vllm/compilation/decorators.py",
        "discussion_id": "2183786658",
        "commented_code": "@@ -23,6 +23,11 @@\n _T = TypeVar(\"_T\", bound=type[nn.Module])\n \n \n+def skip_torch_compile(cls: _T) -> _T:\n+    cls._skip_compile_vllm = True\n+    return cls",
        "comment_created_at": "2025-07-08T02:01:12+00:00",
        "comment_author": "zou3519",
        "comment_body": "`ignore_torch_compile` seems better, I also want a comment here to the effect of \"if this module has a support_torch_compile decorator directly applied on it, we will ignore it, but we will not ignore subgraphs that have support_torch_compile\", something to point out that it isn't recursive.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2162282949",
    "pr_number": 18293,
    "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py",
    "created_at": "2025-06-23T18:30:44+00:00",
    "commented_code": "# New requests are added by update_state_after_alloc in\n         # the scheduler. Used to make metadata passed to Worker.\n         self._reqs_need_recv: dict[str, tuple[Request, list[int]]] = {}\n+        self._reqs_need_send: dict[str, tuple[Request, list[int]]] = {}",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2162282949",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 18293,
        "pr_file": "vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py",
        "discussion_id": "2162282949",
        "commented_code": "@@ -191,6 +227,7 @@ def __init__(self, vllm_config: VllmConfig, engine_id: str):\n         # New requests are added by update_state_after_alloc in\n         # the scheduler. Used to make metadata passed to Worker.\n         self._reqs_need_recv: dict[str, tuple[Request, list[int]]] = {}\n+        self._reqs_need_send: dict[str, tuple[Request, list[int]]] = {}",
        "comment_created_at": "2025-06-23T18:30:44+00:00",
        "comment_author": "njhill",
        "comment_body": "I would suggest renaming this to something like `_reqs_need_save`. Since that's what it controls and it doesn't apply in the direct GPU send case.",
        "pr_file_module": null
      }
    ]
  }
]