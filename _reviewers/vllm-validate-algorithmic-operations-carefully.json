[
  {
    "discussion_id": "2183356331",
    "pr_number": 20447,
    "pr_file": "vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py",
    "created_at": "2025-07-03T17:34:35+00:00",
    "commented_code": "input_quant.strategy == QuantizationStrategy.TENSOR)\n         return is_symmetric_activation and is_per_tensor_activation\n \n-    def _is_fp8_w8a8_sm90(self, weight_quant: BaseModel,\n-                          input_quant: BaseModel) -> bool:\n-        return (self._check_scheme_supported(90, error=False, match_exact=True)\n-                and self._is_fp8_w8a8(weight_quant, input_quant))\n+    def _is_fp8_w8a8_sm90_or_sm100(self, weight_quant: BaseModel,\n+                                   input_quant: BaseModel) -> bool:\n+        return (\n+            self._check_scheme_supported(90, error=False, match_exact=True)\n+            or self._check_scheme_supported(100, error=False, match_exact=True)\n+            and self._is_fp8_w8a8(weight_quant, input_quant))",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2183356331",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20447,
        "pr_file": "vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py",
        "discussion_id": "2183356331",
        "commented_code": "@@ -327,10 +327,12 @@ def _is_fp8_w8a8(self, weight_quant: BaseModel,\n             input_quant.strategy == QuantizationStrategy.TENSOR)\n         return is_symmetric_activation and is_per_tensor_activation\n \n-    def _is_fp8_w8a8_sm90(self, weight_quant: BaseModel,\n-                          input_quant: BaseModel) -> bool:\n-        return (self._check_scheme_supported(90, error=False, match_exact=True)\n-                and self._is_fp8_w8a8(weight_quant, input_quant))\n+    def _is_fp8_w8a8_sm90_or_sm100(self, weight_quant: BaseModel,\n+                                   input_quant: BaseModel) -> bool:\n+        return (\n+            self._check_scheme_supported(90, error=False, match_exact=True)\n+            or self._check_scheme_supported(100, error=False, match_exact=True)\n+            and self._is_fp8_w8a8(weight_quant, input_quant))",
        "comment_created_at": "2025-07-03T17:34:35+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThere is a logical error in this condition due to Python's operator precedence, where `and` is evaluated before `or`.\n\nThe current logic is equivalent to:\n`self._check_scheme_supported(...) or (self._check_scheme_supported(...) and self._is_fp8_w8a8(...))`\n\nThis means if SM90 is supported, the function will return `True` even if the quantization scheme is not `w8a8`, which is incorrect.\n\nYou should add parentheses to group the `or` condition to ensure the architecture check is performed before checking the quantization scheme.\n\n```suggestion\n        return (\n            (self._check_scheme_supported(90, error=False, match_exact=True) or\n             self._check_scheme_supported(100, error=False, match_exact=True)) and\n            self._is_fp8_w8a8(weight_quant, input_quant))\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190362573",
    "pr_number": 20578,
    "pr_file": "benchmarks/kernels/bench_nvfp4_gemm.py",
    "created_at": "2025-07-07T15:08:07+00:00",
    "commented_code": "+# SPDX-License-Identifier: Apache-2.0\n+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n+import argparse\n+import copy\n+import itertools\n+\n+import torch\n+from weight_shapes import WEIGHT_SHAPES\n+\n+from vllm import _custom_ops as ops\n+from vllm.scalar_type import scalar_types\n+from vllm.triton_utils import triton\n+\n+FLOAT4_E2M1_MAX = scalar_types.float4_e2m1f.max()\n+FLOAT8_E4M3_MAX = torch.finfo(torch.float8_e4m3fn).max\n+\n+PROVIDER_CFGS = {\n+    \"torch-bf16\": dict(enabled=True),\n+    \"nvfp4\": dict(no_a_quant=False, enabled=True),\n+    \"nvfp4-noquant\": dict(no_a_quant=True, enabled=True),\n+}\n+\n+_enabled = [k for k, v in PROVIDER_CFGS.items() if v[\"enabled\"]]\n+\n+\n+def _quant_weight_nvfp4(b: torch.Tensor, device: str):\n+    # Compute global scale for weight\n+    b_global_scale = (\n+        (FLOAT8_E4M3_MAX * FLOAT4_E2M1_MAX) / torch.amax(b.flatten(), dim=-1)\n+    ).to(torch.float32)",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2190362573",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20578,
        "pr_file": "benchmarks/kernels/bench_nvfp4_gemm.py",
        "discussion_id": "2190362573",
        "commented_code": "@@ -0,0 +1,137 @@\n+# SPDX-License-Identifier: Apache-2.0\n+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n+import argparse\n+import copy\n+import itertools\n+\n+import torch\n+from weight_shapes import WEIGHT_SHAPES\n+\n+from vllm import _custom_ops as ops\n+from vllm.scalar_type import scalar_types\n+from vllm.triton_utils import triton\n+\n+FLOAT4_E2M1_MAX = scalar_types.float4_e2m1f.max()\n+FLOAT8_E4M3_MAX = torch.finfo(torch.float8_e4m3fn).max\n+\n+PROVIDER_CFGS = {\n+    \"torch-bf16\": dict(enabled=True),\n+    \"nvfp4\": dict(no_a_quant=False, enabled=True),\n+    \"nvfp4-noquant\": dict(no_a_quant=True, enabled=True),\n+}\n+\n+_enabled = [k for k, v in PROVIDER_CFGS.items() if v[\"enabled\"]]\n+\n+\n+def _quant_weight_nvfp4(b: torch.Tensor, device: str):\n+    # Compute global scale for weight\n+    b_global_scale = (\n+        (FLOAT8_E4M3_MAX * FLOAT4_E2M1_MAX) / torch.amax(b.flatten(), dim=-1)\n+    ).to(torch.float32)",
        "comment_created_at": "2025-07-07T15:08:07+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe calculation of `b_global_scale` uses `torch.amax` on the tensor `b` directly. Since `b` is initialized with `torch.randn`, it will contain negative values. The scale should be calculated based on the maximum absolute value to ensure correct quantization for both positive and negative values. This is a correctness issue that will lead to incorrect benchmark results for inputs with negative values.\n\nYou should use `torch.amax(torch.abs(b))` instead. Additionally, `.flatten()` and `dim=-1` are redundant as `torch.amax` without a `dim` argument computes the maximum over the entire tensor.\n\n```suggestion\n    b_global_scale = (\n        (FLOAT8_E4M3_MAX * FLOAT4_E2M1_MAX) / torch.amax(torch.abs(b))\n    ).to(torch.float32)\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2184309699",
    "pr_number": 20466,
    "pr_file": "vllm/v1/spec_decode/eagle.py",
    "created_at": "2025-07-04T04:25:19+00:00",
    "commented_code": "draft_token_ids = torch.stack(draft_token_ids_list, dim=1)\n         return draft_token_ids\n \n-    @staticmethod\n     def prepare_inputs(\n-        # [batch_size + 1]\n-        cu_target_query_lens: torch.Tensor,\n-        # [batch_size]\n-        num_rejected_tokens: torch.Tensor,\n-        num_tokens: int,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n-        # cu_target_query_lens: [0, a, a + b, a + b + c]\n+            self,\n+            common_attn_metadata: CommonAttentionMetadata,\n+            # [batch_size]\n+            num_rejected_tokens: torch.Tensor,\n+            num_tokens: int) -> tuple[CommonAttentionMetadata, torch.Tensor]:\n+        # query_start_loc_cpu: [0, a, a + b, a + b + c]\n         # num_rejected_tokens: [n1, n2, n3]\n         # num_tokens_per_req: [a - n1, b - n2, c - n3]\n         # cu_num_tokens: [0, a - n1, a + b - n1 - n2, a + b + c - n1 - n2 - n3]\n         # token_indices: [0, 1, ..., a - n1 - 1,\n         #                 a, a + 1, ..., a + b - n2 - 1,\n         #                 a + b, a + b + 1, ..., a + b + c - n3 - 1]\n \n+        device = common_attn_metadata.query_start_loc.device\n+        query_start_loc_cpu = common_attn_metadata.query_start_loc_cpu\n+        spec_seq_lens_cpu =\\\n+            common_attn_metadata.seq_lens_cpu - num_rejected_tokens\n+\n         # [0, a, a + b, a + b + c] -> [a, b, c]\n-        query_len_per_req = (cu_target_query_lens[1:] -\n-                             cu_target_query_lens[:-1])\n+        query_len_per_req = (query_start_loc_cpu[1:] -\n+                             query_start_loc_cpu[:-1])\n         # [a, b, c] -> [a - n1, b - n2, c - n3]\n         num_tokens_per_req = query_len_per_req - num_rejected_tokens\n \n         # [a - n1, b - n2, c - n3] ->\n         # [0, a - n1, a + b - n1 - n2, a + b + c - n1 - n2 - n3]\n-        cu_num_tokens = torch.zeros_like(cu_target_query_lens)\n-        torch.cumsum(num_tokens_per_req, dim=0, out=cu_num_tokens[1:])\n-        token_indices = torch.empty(\n-            num_tokens,\n-            dtype=torch.int32,\n-            device=cu_target_query_lens.device,\n-        )\n-        batch_size = num_rejected_tokens.shape[0]\n-        BLOCK_SIZE = 1024\n-        prepare_eagle_input_kernel[(batch_size, )](\n-            token_indices,\n-            cu_target_query_lens,\n-            cu_num_tokens,\n-            BLOCK_SIZE=BLOCK_SIZE,\n+        spec_query_start_loc_cpu = torch.zeros_like(query_start_loc_cpu,\n+                                                    pin_memory=True)\n+        torch.cumsum(num_tokens_per_req,\n+                     dim=0,\n+                     out=spec_query_start_loc_cpu[1:])\n+        \"\"\"Get the cumulative sum and batched arange of the given array.\n+        # E.g., [2, 5, 3] -> ([2, 7, 10], [0, 1, 0, 1, 2, 3, 4, 0, 1, 2])\n+        # Equivalent to but faster than:\n+        # np.concatenate([np.arange(n) for n in num_tokens])\n+        \"\"\"\n+        # Step 1. [2, 5, 3] -> [2, 7, 10]\n+        total_num_tokens = spec_query_start_loc_cpu[-1]\n+        # Step 2. [2, 7, 10] -> [0, 0, 2, 2, 2, 2, 2, 7, 7, 7]\n+        cumsums_offsets = np.repeat(\n+            spec_query_start_loc_cpu[1:].numpy() - num_tokens_per_req.numpy(),\n+            num_tokens_per_req.numpy())\n+        # Step 3. [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n+        arange = self.arange_np[:total_num_tokens] - cumsums_offsets\n+\n+        tokens_indices = arange + query_start_loc_cpu[:-1]",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2184309699",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20466,
        "pr_file": "vllm/v1/spec_decode/eagle.py",
        "discussion_id": "2184309699",
        "commented_code": "@@ -275,46 +240,70 @@ def propose(\n         draft_token_ids = torch.stack(draft_token_ids_list, dim=1)\n         return draft_token_ids\n \n-    @staticmethod\n     def prepare_inputs(\n-        # [batch_size + 1]\n-        cu_target_query_lens: torch.Tensor,\n-        # [batch_size]\n-        num_rejected_tokens: torch.Tensor,\n-        num_tokens: int,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n-        # cu_target_query_lens: [0, a, a + b, a + b + c]\n+            self,\n+            common_attn_metadata: CommonAttentionMetadata,\n+            # [batch_size]\n+            num_rejected_tokens: torch.Tensor,\n+            num_tokens: int) -> tuple[CommonAttentionMetadata, torch.Tensor]:\n+        # query_start_loc_cpu: [0, a, a + b, a + b + c]\n         # num_rejected_tokens: [n1, n2, n3]\n         # num_tokens_per_req: [a - n1, b - n2, c - n3]\n         # cu_num_tokens: [0, a - n1, a + b - n1 - n2, a + b + c - n1 - n2 - n3]\n         # token_indices: [0, 1, ..., a - n1 - 1,\n         #                 a, a + 1, ..., a + b - n2 - 1,\n         #                 a + b, a + b + 1, ..., a + b + c - n3 - 1]\n \n+        device = common_attn_metadata.query_start_loc.device\n+        query_start_loc_cpu = common_attn_metadata.query_start_loc_cpu\n+        spec_seq_lens_cpu =\\\n+            common_attn_metadata.seq_lens_cpu - num_rejected_tokens\n+\n         # [0, a, a + b, a + b + c] -> [a, b, c]\n-        query_len_per_req = (cu_target_query_lens[1:] -\n-                             cu_target_query_lens[:-1])\n+        query_len_per_req = (query_start_loc_cpu[1:] -\n+                             query_start_loc_cpu[:-1])\n         # [a, b, c] -> [a - n1, b - n2, c - n3]\n         num_tokens_per_req = query_len_per_req - num_rejected_tokens\n \n         # [a - n1, b - n2, c - n3] ->\n         # [0, a - n1, a + b - n1 - n2, a + b + c - n1 - n2 - n3]\n-        cu_num_tokens = torch.zeros_like(cu_target_query_lens)\n-        torch.cumsum(num_tokens_per_req, dim=0, out=cu_num_tokens[1:])\n-        token_indices = torch.empty(\n-            num_tokens,\n-            dtype=torch.int32,\n-            device=cu_target_query_lens.device,\n-        )\n-        batch_size = num_rejected_tokens.shape[0]\n-        BLOCK_SIZE = 1024\n-        prepare_eagle_input_kernel[(batch_size, )](\n-            token_indices,\n-            cu_target_query_lens,\n-            cu_num_tokens,\n-            BLOCK_SIZE=BLOCK_SIZE,\n+        spec_query_start_loc_cpu = torch.zeros_like(query_start_loc_cpu,\n+                                                    pin_memory=True)\n+        torch.cumsum(num_tokens_per_req,\n+                     dim=0,\n+                     out=spec_query_start_loc_cpu[1:])\n+        \"\"\"Get the cumulative sum and batched arange of the given array.\n+        # E.g., [2, 5, 3] -> ([2, 7, 10], [0, 1, 0, 1, 2, 3, 4, 0, 1, 2])\n+        # Equivalent to but faster than:\n+        # np.concatenate([np.arange(n) for n in num_tokens])\n+        \"\"\"\n+        # Step 1. [2, 5, 3] -> [2, 7, 10]\n+        total_num_tokens = spec_query_start_loc_cpu[-1]\n+        # Step 2. [2, 7, 10] -> [0, 0, 2, 2, 2, 2, 2, 7, 7, 7]\n+        cumsums_offsets = np.repeat(\n+            spec_query_start_loc_cpu[1:].numpy() - num_tokens_per_req.numpy(),\n+            num_tokens_per_req.numpy())\n+        # Step 3. [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n+        arange = self.arange_np[:total_num_tokens] - cumsums_offsets\n+\n+        tokens_indices = arange + query_start_loc_cpu[:-1]",
        "comment_created_at": "2025-07-04T04:25:19+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![critical](https://www.gstatic.com/codereviewagent/critical.svg)\n\nThe calculation of `tokens_indices` is incorrect. `arange` is a numpy array of shape `(total_num_tokens,)` and `query_start_loc_cpu[:-1]` is a torch tensor of shape `(batch_size,)`. Adding them directly will raise a `TypeError` and not produce the intended result due to broadcasting rules.\n\nTo get the correct token indices, you need to repeat the start location of each request to match the length of `arange`.\n\n```suggestion\n        tokens_indices = arange + np.repeat(\n            query_start_loc_cpu[:-1].numpy(), num_tokens_per_req.numpy())\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2157218429",
    "pr_number": 19351,
    "pr_file": "vllm/v1/core/single_type_kv_cache_manager.py",
    "created_at": "2025-06-19T14:52:02+00:00",
    "commented_code": "\"\"\"\n         NOTE(Chen): The prefix blocks are null blocks for sliding window layers.\n         So it's not correct to count ref_cnt like FullAttentionManager. Return \n-        0 here for correctness. Need to support cascade attention + sliding \n+        0 here for correctness. Need to support cascade attention + sliding\n         window in the future.\n         \"\"\"\n         return 0\n \n \n+class ChunkedLocalAttentionManager(SingleTypeKVCacheManager):\n+\n+    def __init__(self, kv_cache_spec: ChunkedLocalAttentionSpec,\n+                 block_pool: BlockPool, **kwargs) -> None:\n+        super().__init__(kv_cache_spec, block_pool, **kwargs)\n+        self.attention_chunk_size = kv_cache_spec.attention_chunk_size\n+        self._null_block = block_pool.null_block\n+\n+    @classmethod\n+    def find_longest_cache_hit(\n+        cls,\n+        block_hashes: list[BlockHash],\n+        max_length: int,\n+        kv_cache_group_ids: list[int],\n+        block_pool: BlockPool,\n+        kv_cache_spec: KVCacheSpec,\n+        use_eagle: bool,\n+    ) -> tuple[list[KVCacheBlock], ...]:\n+        assert isinstance(kv_cache_spec, ChunkedLocalAttentionSpec), (\n+            \"ChunkedLocalAttentionManager can only be used for \" +\n+            \"chunked local attention groups\")\n+        max_num_blocks = max_length // kv_cache_spec.block_size\n+        if max_length > 0:\n+            local_attention_start_idx = (max_length //\n+                                         kv_cache_spec.attention_chunk_size *\n+                                         kv_cache_spec.attention_chunk_size)\n+        else:\n+            local_attention_start_idx = 0\n+        # [ block 0, ..., block x(x_start<=first_attention_token),\n+        # block x+1, ..,  block N (N_end <=max_len), ...]\n+        local_attention_start_block_idx = (local_attention_start_idx //\n+                                           kv_cache_spec.block_size)\n+        computed_blocks: tuple[list[KVCacheBlock], ...] = tuple(\n+            [block_pool.null_block] * local_attention_start_block_idx\n+            for _ in range(len(kv_cache_group_ids)))\n+        # we marked blocks out of window as computed\n+        # with null blocks, and blocks inside window\n+        # based on cache lookup result\n+        # [null] [null] ... [null] [hit block 1 (1st block contain last window)]\n+        # [hit block 2] ... [hit block x]\n+        for i in range(local_attention_start_block_idx, max_num_blocks):\n+            block_hash = block_hashes[i]\n+            if cached_block := block_pool.get_cached_block(\n+                    block_hash, kv_cache_group_ids):\n+                for computed, cached in zip(computed_blocks, cached_block):\n+                    computed.append(cached)\n+            else:\n+                break\n+        if use_eagle and computed_blocks[0]:\n+            for computed in computed_blocks:\n+                computed.pop()",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2157218429",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19351,
        "pr_file": "vllm/v1/core/single_type_kv_cache_manager.py",
        "discussion_id": "2157218429",
        "commented_code": "@@ -385,12 +386,102 @@ def get_num_common_prefix_blocks(self, request_id: str,\n         \"\"\"\n         NOTE(Chen): The prefix blocks are null blocks for sliding window layers.\n         So it's not correct to count ref_cnt like FullAttentionManager. Return \n-        0 here for correctness. Need to support cascade attention + sliding \n+        0 here for correctness. Need to support cascade attention + sliding\n         window in the future.\n         \"\"\"\n         return 0\n \n \n+class ChunkedLocalAttentionManager(SingleTypeKVCacheManager):\n+\n+    def __init__(self, kv_cache_spec: ChunkedLocalAttentionSpec,\n+                 block_pool: BlockPool, **kwargs) -> None:\n+        super().__init__(kv_cache_spec, block_pool, **kwargs)\n+        self.attention_chunk_size = kv_cache_spec.attention_chunk_size\n+        self._null_block = block_pool.null_block\n+\n+    @classmethod\n+    def find_longest_cache_hit(\n+        cls,\n+        block_hashes: list[BlockHash],\n+        max_length: int,\n+        kv_cache_group_ids: list[int],\n+        block_pool: BlockPool,\n+        kv_cache_spec: KVCacheSpec,\n+        use_eagle: bool,\n+    ) -> tuple[list[KVCacheBlock], ...]:\n+        assert isinstance(kv_cache_spec, ChunkedLocalAttentionSpec), (\n+            \"ChunkedLocalAttentionManager can only be used for \" +\n+            \"chunked local attention groups\")\n+        max_num_blocks = max_length // kv_cache_spec.block_size\n+        if max_length > 0:\n+            local_attention_start_idx = (max_length //\n+                                         kv_cache_spec.attention_chunk_size *\n+                                         kv_cache_spec.attention_chunk_size)\n+        else:\n+            local_attention_start_idx = 0\n+        # [ block 0, ..., block x(x_start<=first_attention_token),\n+        # block x+1, ..,  block N (N_end <=max_len), ...]\n+        local_attention_start_block_idx = (local_attention_start_idx //\n+                                           kv_cache_spec.block_size)\n+        computed_blocks: tuple[list[KVCacheBlock], ...] = tuple(\n+            [block_pool.null_block] * local_attention_start_block_idx\n+            for _ in range(len(kv_cache_group_ids)))\n+        # we marked blocks out of window as computed\n+        # with null blocks, and blocks inside window\n+        # based on cache lookup result\n+        # [null] [null] ... [null] [hit block 1 (1st block contain last window)]\n+        # [hit block 2] ... [hit block x]\n+        for i in range(local_attention_start_block_idx, max_num_blocks):\n+            block_hash = block_hashes[i]\n+            if cached_block := block_pool.get_cached_block(\n+                    block_hash, kv_cache_group_ids):\n+                for computed, cached in zip(computed_blocks, cached_block):\n+                    computed.append(cached)\n+            else:\n+                break\n+        if use_eagle and computed_blocks[0]:\n+            for computed in computed_blocks:\n+                computed.pop()",
        "comment_created_at": "2025-06-19T14:52:02+00:00",
        "comment_author": "heheda12345",
        "comment_body": "In eagle, we can't simply pop the last block.\r\nFor example, chunk size 2 and block size 1:\r\n[miss, miss] [miss miss] -> cache_hit_length 4\r\nif we remove the 3-th block (0-indexed), the cache_hit_length becomes 3, but [miss, miss] [miss] is not a valid cache hit prefix. I think we should return cache_hit_length 2 in this case.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2185279923",
    "pr_number": 20488,
    "pr_file": "vllm/_custom_ops.py",
    "created_at": "2025-07-04T12:44:23+00:00",
    "commented_code": "output_mask: A boolean tensor indicating which tokens appear in the output.\n         repetition_penalties: The repetition penalties of shape (num_seqs, ).\n     \"\"\"\n-    if current_platform.is_cuda() and logits.is_contiguous():\n+    if current_platform.is_cuda() and logits.is_contiguous() and logits.size(0) > 0:",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2185279923",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20488,
        "pr_file": "vllm/_custom_ops.py",
        "discussion_id": "2185279923",
        "commented_code": "@@ -311,7 +311,7 @@ def apply_repetition_penalties(logits: torch.Tensor, prompt_mask: torch.Tensor,\n         output_mask: A boolean tensor indicating which tokens appear in the output.\n         repetition_penalties: The repetition penalties of shape (num_seqs, ).\n     \"\"\"\n-    if current_platform.is_cuda() and logits.is_contiguous():\n+    if current_platform.is_cuda() and logits.is_contiguous() and logits.size(0) > 0:",
        "comment_created_at": "2025-07-04T12:44:23+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nUsing `logits.numel() > 0` would be a more robust check here. While `logits.size(0) > 0` correctly fixes the reported crash when the number of sequences is zero, the CUDA kernel might also face issues if `vocab_size` is zero (`logits.size(1) == 0`).\n\n`logits.numel() > 0` ensures that both dimensions of the `logits` tensor are non-zero, preventing potential crashes in either case and making the guard more comprehensive.\n\n```suggestion\n    if current_platform.is_cuda() and logits.is_contiguous() and logits.numel() > 0:\n```",
        "pr_file_module": null
      }
    ]
  }
]