[
  {
    "discussion_id": "2157218429",
    "pr_number": 19351,
    "pr_file": "vllm/v1/core/single_type_kv_cache_manager.py",
    "created_at": "2025-06-19T14:52:02+00:00",
    "commented_code": "\"\"\"\n         NOTE(Chen): The prefix blocks are null blocks for sliding window layers.\n         So it's not correct to count ref_cnt like FullAttentionManager. Return \n-        0 here for correctness. Need to support cascade attention + sliding \n+        0 here for correctness. Need to support cascade attention + sliding\n         window in the future.\n         \"\"\"\n         return 0\n \n \n+class ChunkedLocalAttentionManager(SingleTypeKVCacheManager):\n+\n+    def __init__(self, kv_cache_spec: ChunkedLocalAttentionSpec,\n+                 block_pool: BlockPool, **kwargs) -> None:\n+        super().__init__(kv_cache_spec, block_pool, **kwargs)\n+        self.attention_chunk_size = kv_cache_spec.attention_chunk_size\n+        self._null_block = block_pool.null_block\n+\n+    @classmethod\n+    def find_longest_cache_hit(\n+        cls,\n+        block_hashes: list[BlockHash],\n+        max_length: int,\n+        kv_cache_group_ids: list[int],\n+        block_pool: BlockPool,\n+        kv_cache_spec: KVCacheSpec,\n+        use_eagle: bool,\n+    ) -> tuple[list[KVCacheBlock], ...]:\n+        assert isinstance(kv_cache_spec, ChunkedLocalAttentionSpec), (\n+            \"ChunkedLocalAttentionManager can only be used for \" +\n+            \"chunked local attention groups\")\n+        max_num_blocks = max_length // kv_cache_spec.block_size\n+        if max_length > 0:\n+            local_attention_start_idx = (max_length //\n+                                         kv_cache_spec.attention_chunk_size *\n+                                         kv_cache_spec.attention_chunk_size)\n+        else:\n+            local_attention_start_idx = 0\n+        # [ block 0, ..., block x(x_start<=first_attention_token),\n+        # block x+1, ..,  block N (N_end <=max_len), ...]\n+        local_attention_start_block_idx = (local_attention_start_idx //\n+                                           kv_cache_spec.block_size)\n+        computed_blocks: tuple[list[KVCacheBlock], ...] = tuple(\n+            [block_pool.null_block] * local_attention_start_block_idx\n+            for _ in range(len(kv_cache_group_ids)))\n+        # we marked blocks out of window as computed\n+        # with null blocks, and blocks inside window\n+        # based on cache lookup result\n+        # [null] [null] ... [null] [hit block 1 (1st block contain last window)]\n+        # [hit block 2] ... [hit block x]\n+        for i in range(local_attention_start_block_idx, max_num_blocks):\n+            block_hash = block_hashes[i]\n+            if cached_block := block_pool.get_cached_block(\n+                    block_hash, kv_cache_group_ids):\n+                for computed, cached in zip(computed_blocks, cached_block):\n+                    computed.append(cached)\n+            else:\n+                break\n+        if use_eagle and computed_blocks[0]:\n+            for computed in computed_blocks:\n+                computed.pop()",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2157218429",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19351,
        "pr_file": "vllm/v1/core/single_type_kv_cache_manager.py",
        "discussion_id": "2157218429",
        "commented_code": "@@ -385,12 +386,102 @@ def get_num_common_prefix_blocks(self, request_id: str,\n         \"\"\"\n         NOTE(Chen): The prefix blocks are null blocks for sliding window layers.\n         So it's not correct to count ref_cnt like FullAttentionManager. Return \n-        0 here for correctness. Need to support cascade attention + sliding \n+        0 here for correctness. Need to support cascade attention + sliding\n         window in the future.\n         \"\"\"\n         return 0\n \n \n+class ChunkedLocalAttentionManager(SingleTypeKVCacheManager):\n+\n+    def __init__(self, kv_cache_spec: ChunkedLocalAttentionSpec,\n+                 block_pool: BlockPool, **kwargs) -> None:\n+        super().__init__(kv_cache_spec, block_pool, **kwargs)\n+        self.attention_chunk_size = kv_cache_spec.attention_chunk_size\n+        self._null_block = block_pool.null_block\n+\n+    @classmethod\n+    def find_longest_cache_hit(\n+        cls,\n+        block_hashes: list[BlockHash],\n+        max_length: int,\n+        kv_cache_group_ids: list[int],\n+        block_pool: BlockPool,\n+        kv_cache_spec: KVCacheSpec,\n+        use_eagle: bool,\n+    ) -> tuple[list[KVCacheBlock], ...]:\n+        assert isinstance(kv_cache_spec, ChunkedLocalAttentionSpec), (\n+            \"ChunkedLocalAttentionManager can only be used for \" +\n+            \"chunked local attention groups\")\n+        max_num_blocks = max_length // kv_cache_spec.block_size\n+        if max_length > 0:\n+            local_attention_start_idx = (max_length //\n+                                         kv_cache_spec.attention_chunk_size *\n+                                         kv_cache_spec.attention_chunk_size)\n+        else:\n+            local_attention_start_idx = 0\n+        # [ block 0, ..., block x(x_start<=first_attention_token),\n+        # block x+1, ..,  block N (N_end <=max_len), ...]\n+        local_attention_start_block_idx = (local_attention_start_idx //\n+                                           kv_cache_spec.block_size)\n+        computed_blocks: tuple[list[KVCacheBlock], ...] = tuple(\n+            [block_pool.null_block] * local_attention_start_block_idx\n+            for _ in range(len(kv_cache_group_ids)))\n+        # we marked blocks out of window as computed\n+        # with null blocks, and blocks inside window\n+        # based on cache lookup result\n+        # [null] [null] ... [null] [hit block 1 (1st block contain last window)]\n+        # [hit block 2] ... [hit block x]\n+        for i in range(local_attention_start_block_idx, max_num_blocks):\n+            block_hash = block_hashes[i]\n+            if cached_block := block_pool.get_cached_block(\n+                    block_hash, kv_cache_group_ids):\n+                for computed, cached in zip(computed_blocks, cached_block):\n+                    computed.append(cached)\n+            else:\n+                break\n+        if use_eagle and computed_blocks[0]:\n+            for computed in computed_blocks:\n+                computed.pop()",
        "comment_created_at": "2025-06-19T14:52:02+00:00",
        "comment_author": "heheda12345",
        "comment_body": "In eagle, we can't simply pop the last block.\r\nFor example, chunk size 2 and block size 1:\r\n[miss, miss] [miss miss] -> cache_hit_length 4\r\nif we remove the 3-th block (0-indexed), the cache_hit_length becomes 3, but [miss, miss] [miss] is not a valid cache hit prefix. I think we should return cache_hit_length 2 in this case.",
        "pr_file_module": null
      }
    ]
  }
]