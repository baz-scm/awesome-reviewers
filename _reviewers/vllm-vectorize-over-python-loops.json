[
  {
    "discussion_id": "2191557263",
    "pr_number": 20608,
    "pr_file": "vllm/v1/core/sched/request_queue.py",
    "created_at": "2025-07-08T06:02:44+00:00",
    "commented_code": "return super().__reversed__()\n \n \n+class PrioritizedItem:\n+\n+    def __init__(self, request: Request, aging_factor: float = 0.1):\n+        self.request = request\n+        self.aging_factor = aging_factor\n+        self.insert_time = request.arrival_time\n+\n+    def __lt__(self, other: PrioritizedItem) -> bool:\n+        now = time.time()\n+        eff_self = self.request.priority - self.aging_factor * (\n+            now - self.insert_time)\n+        eff_other = other.request.priority - other.aging_factor * (\n+            now - other.insert_time)\n+\n+        if eff_self != eff_other:\n+            return eff_self < eff_other\n+        return self.insert_time < other.insert_time",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2191557263",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20608,
        "pr_file": "vllm/v1/core/sched/request_queue.py",
        "discussion_id": "2191557263",
        "commented_code": "@@ -136,36 +137,57 @@ def __reversed__(self) -> Iterator[Request]:\n         return super().__reversed__()\n \n \n+class PrioritizedItem:\n+\n+    def __init__(self, request: Request, aging_factor: float = 0.1):\n+        self.request = request\n+        self.aging_factor = aging_factor\n+        self.insert_time = request.arrival_time\n+\n+    def __lt__(self, other: PrioritizedItem) -> bool:\n+        now = time.time()\n+        eff_self = self.request.priority - self.aging_factor * (\n+            now - self.insert_time)\n+        eff_other = other.request.priority - other.aging_factor * (\n+            now - other.insert_time)\n+\n+        if eff_self != eff_other:\n+            return eff_self < eff_other\n+        return self.insert_time < other.insert_time",
        "comment_created_at": "2025-07-08T06:02:44+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe comparison logic in `__lt__` uses `time.time()` to calculate the effective priority. Calling `time.time()` in the comparison function, which can be called frequently by the heap, could introduce performance overhead. It might be more efficient to calculate the effective priority once when the item is added to the queue and store it as an attribute of the `PrioritizedItem`. This would avoid repeated calls to `time.time()` during comparisons.\n\nAlso, consider adding a small constant to `now` to avoid the edge case where `now` and `self.insert_time` are equal, which could lead to `eff_self` and `eff_other` being equal and the comparison falling back to `self.insert_time < other.insert_time` unnecessarily.\n\n```python\nclass PrioritizedItem:\n\n    def __init__(self, request: Request, aging_factor: float = 0.1):\n        self.request = request\n        self.aging_factor = aging_factor\n        self.insert_time = request.arrival_time\n        # Calculate effective priority at insertion\n        self.effective_priority = self._calculate_effective_priority()\n\n    def _calculate_effective_priority(self) -> float:\n        now = time.time()\n        return self.request.priority - self.aging_factor * (now - self.insert_time)\n\n    def __lt__(self, other: PrioritizedItem) -> bool:\n        if self.effective_priority != other.effective_priority:\n            return self.effective_priority < other.effective_priority\n        return self.insert_time < other.insert_time\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190140625",
    "pr_number": 20495,
    "pr_file": "vllm/model_executor/models/gemma3n_mm.py",
    "created_at": "2025-07-07T13:38:34+00:00",
    "commented_code": "+# SPDX-License-Identifier: Apache-2.0\n+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n+from collections.abc import Iterable, Mapping, Sequence\n+from typing import Any, Optional, TypedDict, Union, cast\n+\n+import torch\n+from torch import nn\n+from transformers import AutoModel, BatchFeature\n+from transformers.models.gemma3n import (Gemma3nAudioConfig,\n+                                         Gemma3nAudioFeatureExtractor,\n+                                         Gemma3nConfig, Gemma3nProcessor,\n+                                         Gemma3nTextConfig,\n+                                         Gemma3nVisionConfig)\n+from transformers.models.siglip import SiglipImageProcessorFast\n+\n+from vllm.config import VllmConfig\n+from vllm.logger import init_logger\n+from vllm.model_executor.layers.layernorm import RMSNorm\n+from vllm.model_executor.layers.linear import RowParallelLinear\n+from vllm.model_executor.layers.vocab_parallel_embedding import (\n+    VocabParallelEmbedding)\n+from vllm.model_executor.models.gemma3n import Gemma3nForCausalLM\n+from vllm.model_executor.models.module_mapping import MultiModelKeys\n+from vllm.model_executor.sampling_metadata import SamplingMetadata\n+from vllm.multimodal import MULTIMODAL_REGISTRY\n+from vllm.multimodal.inputs import (MultiModalDataDict, MultiModalFieldConfig,\n+                                    MultiModalKwargs)\n+from vllm.multimodal.parse import ImageProcessorItems, MultiModalDataItems, MultiModalDataParser\n+# yapf: disable\n+from vllm.multimodal.processing import (BaseMultiModalProcessor,\n+                                        BaseProcessingInfo, BoundPromptUpdate,\n+                                        PlaceholderFeaturesInfo,\n+                                        PromptReplacement, PromptTargetMatch,\n+                                        PromptUpdate, find_mm_placeholders,\n+                                        replace_token_matches)\n+# yapf: enable\n+from vllm.multimodal.profiling import BaseDummyInputsBuilder\n+from vllm.sequence import IntermediateTensors\n+\n+from .interfaces import MultiModalEmbeddings, SupportsMultiModal\n+from .utils import (AutoWeightsLoader, WeightsMapper, flatten_bn,\n+                    init_vllm_registered_model, maybe_prefix,\n+                    merge_multimodal_embeddings)\n+\n+logger = init_logger(__name__)\n+\n+# This should be based on model config but we hardcode them for now.\n+TOKENS_PER_IMAGE = 256\n+TOKENS_PER_AUDIO = 188\n+\n+\n+class Gemma3nImagePixelInputs(TypedDict):\n+    pixel_values: torch.Tensor\n+    \"\"\"Shape: `(batch_size * num_images, num_channels, height, width)`\"\"\"\n+\n+\n+class Gemma3nAudioInputs(TypedDict):\n+    input_features: torch.Tensor\n+    \"\"\"Shape: `(batch_size * num_audio, seq_length, num_features)`\"\"\"\n+    input_features_mask: torch.Tensor\n+    \"\"\"Shape: `(batch_size * num_audio, seq_length)`\"\"\"\n+\n+\n+Gemma3nImageInputs = Gemma3nImagePixelInputs\n+\n+\n+class Gemma3nProcessingInfo(BaseProcessingInfo):\n+\n+    def get_hf_config(self):\n+        return self.ctx.get_hf_config(Gemma3nConfig)\n+\n+    def get_hf_processor(self, **kwargs: object):\n+        return self.ctx.get_hf_processor(Gemma3nProcessor, **kwargs)\n+\n+    def get_supported_mm_limits(self) -> Mapping[str, Optional[int]]:\n+        return {\"image\": None, \"audio\": None}\n+\n+    def get_max_tokens_per_item(\n+            self, seq_len: int,\n+            mm_counts: Mapping[str, int]) -> Optional[Mapping[str, int]]:\n+\n+        return {\"image\": TOKENS_PER_IMAGE, \"audio\": TOKENS_PER_AUDIO}\n+\n+    def get_image_repl(\n+        self,\n+        *,\n+        image_width: int,\n+        image_height: int,\n+        processor: Optional[Gemma3nProcessor],\n+    ) -> str:\n+        \"\"\"\n+        Get the replacement text for image tokens.\n+        \n+        For Gemma3n, this should return the full_image_sequence which includes\n+        BOI token, repeated image tokens, and EOI token.\n+        \"\"\"\n+        if processor is None:\n+            processor = self.get_hf_processor()\n+\n+        # Return the full image sequence as defined by the processor\n+        return processor.full_image_sequence\n+\n+    def get_audio_repl(\n+        self,\n+        *,\n+        processor: Optional[Gemma3nProcessor],\n+    ) -> str:\n+        \"\"\"\n+        Get the replacement text for audio tokens.\n+        \n+        For Gemma3n, this should return the full_audio_sequence which includes\n+        BOA token, repeated audio tokens, and EOA token.\n+        \"\"\"\n+        if processor is None:\n+            processor = self.get_hf_processor()\n+\n+        # Return the full audio sequence as defined by the processor\n+        return processor.full_audio_sequence\n+\n+\n+class Gemma3nDummyInputsBuilder(BaseDummyInputsBuilder[Gemma3nProcessingInfo]):\n+\n+    def get_dummy_text(self, mm_counts: Mapping[str, int]) -> str:\n+        num_images = mm_counts.get(\"image\", 0)\n+        num_audios = mm_counts.get(\"audio\", 0)\n+\n+        processor = self.info.get_hf_processor()\n+        image_token = processor.image_token\n+        audio_token = processor.audio_token\n+\n+        return image_token * num_images + audio_token * num_audios\n+\n+    def get_dummy_mm_data(\n+        self,\n+        seq_len: int,\n+        mm_counts: Mapping[str, int],\n+    ) -> MultiModalDataDict:\n+        num_images = mm_counts.get(\"image\", 0)\n+        num_audios = mm_counts.get(\"audio\", 0)\n+        processor = self.info.get_hf_processor()\n+        feature_extractor: Gemma3nAudioFeatureExtractor = processor.feature_extractor  # noqa: E501\n+        # audio_len = feature_extractor.max_length\n+        audio_len = feature_extractor.fft_length\n+        image_processor: SiglipImageProcessorFast = processor.image_processor\n+        img_width = image_processor.size.get(\"width\", 224)\n+        img_height = image_processor.size.get(\"height\", 224)\n+\n+        return {\n+            \"image\":\n+            self._get_dummy_images(width=img_width,\n+                                   height=img_height,\n+                                   num_images=num_images),\n+            \"audio\":\n+            self._get_dummy_audios(length=audio_len, num_audios=num_audios)\n+        }\n+\n+\n+class Gemma3MultiModalProcessor(BaseMultiModalProcessor[Gemma3nProcessingInfo]\n+                                ):\n+\n+    def _get_data_parser(self) -> MultiModalDataParser:\n+        feature_extractor = self.info.get_hf_processor().feature_extractor\n+        return MultiModalDataParser(target_sr=feature_extractor.sampling_rate)\n+\n+    def _call_hf_processor(\n+        self,\n+        prompt: str,\n+        mm_data: Mapping[str, object],\n+        mm_kwargs: Mapping[str, object],\n+    ) -> BatchFeature:\n+        processed_outputs = super()._call_hf_processor(\n+            prompt,\n+            mm_data,\n+            mm_kwargs,\n+        )\n+        return processed_outputs\n+\n+    def _get_mm_fields_config(\n+        self,\n+        hf_inputs: BatchFeature,\n+        hf_processor_mm_kwargs: Mapping[str, object],\n+    ) -> Mapping[str, MultiModalFieldConfig]:\n+\n+        return dict(\n+            pixel_values=MultiModalFieldConfig.batched(\"image\"),\n+            input_features=MultiModalFieldConfig.batched(\"audio\"),\n+            input_features_mask=MultiModalFieldConfig.batched(\"audio\"),\n+        )\n+\n+    def _get_prompt_updates(\n+        self,\n+        mm_items: MultiModalDataItems,\n+        hf_processor_mm_kwargs: Mapping[str, Any],\n+        out_mm_kwargs: MultiModalKwargs,\n+    ) -> Sequence[PromptUpdate]:\n+        hf_processor = self.info.get_hf_processor(**hf_processor_mm_kwargs)\n+        \n+        prompt_updates = []\n+        \n+        # Handle image tokens\n+        if \"image\" in mm_items:\n+            image_token = hf_processor.image_token\n+\n+            def get_replacement_image(item_idx: int):\n+                images = mm_items.get_items(\"image\", ImageProcessorItems)\n+                image_size = images.get_image_size(item_idx)\n+                return self.info.get_image_repl(\n+                    image_width=image_size.width,\n+                    image_height=image_size.height,\n+                    processor=hf_processor,\n+                )\n+\n+            prompt_updates.append(\n+                PromptReplacement(\n+                    modality=\"image\",\n+                    target=image_token,\n+                    replacement=get_replacement_image,\n+                )\n+            )\n+        \n+        # Handle audio tokens\n+        if \"audio\" in mm_items:\n+            audio_token = hf_processor.audio_token\n+\n+            def get_replacement_audio(item_idx: int):\n+                return self.info.get_audio_repl(\n+                    processor=hf_processor,\n+                )\n+\n+            prompt_updates.append(\n+                PromptReplacement(\n+                    modality=\"audio\",\n+                    target=audio_token,\n+                    replacement=get_replacement_audio,\n+                )\n+            )\n+\n+        return prompt_updates\n+\n+    def _apply_token_matches(\n+        self,\n+        prompt: list[int],\n+        mm_matches: Mapping[str, Sequence[PromptTargetMatch]],\n+        mm_item_counts: Mapping[str, int],\n+    ) -> list[int]:\n+        token_ids = super()._apply_token_matches(\n+            prompt,\n+            mm_matches,\n+            mm_item_counts,\n+        )\n+\n+        # \"\n\n\n\" and \"\n\n\n\n\" are single tokens\n+        # Since our replacement can insert \"\n\n\" next to \"\n\"\n+        # tokens, we have to combine them to be consistent with\n+        # the output of the tokenizer\n+        tokenizer = self.info.get_tokenizer()\n+        vocab = tokenizer.get_vocab()\n+        newline_1 = vocab[\"\n\"]\n+        newline_2 = vocab[\"\n\n\"]\n+        newline_3 = vocab[\"\n\n\n\"]\n+        newline_4 = vocab[\"\n\n\n\n\"]\n+\n+        token_ids = replace_token_matches(\n+            token_ids,\n+            [newline_1, newline_2],\n+            [newline_3],\n+        )\n+        token_ids = replace_token_matches(\n+            token_ids,\n+            [newline_2, newline_1],\n+            [newline_3],\n+        )\n+        token_ids = replace_token_matches(\n+            token_ids,\n+            [newline_2, newline_2],\n+            [newline_4],\n+        )\n+\n+        return token_ids\n+\n+    def _find_mm_placeholders(\n+        self,\n+        mm_prompt_updates: Mapping[str, Sequence[BoundPromptUpdate]],\n+        new_token_ids: list[int],\n+        mm_item_counts: Mapping[str, int],\n+    ) -> Mapping[str, list[PlaceholderFeaturesInfo]]:\n+        # We need to detect \"\n\n\" inside \"\n\n\n\" and \"\n\n\n\n\"\n+        tokenizer = self.info.get_tokenizer()\n+        vocab = tokenizer.get_vocab()\n+        newline_1 = vocab[\"\n\"]\n+        newline_2 = vocab[\"\n\n\"]\n+        newline_3 = vocab[\"\n\n\n\"]\n+        newline_4 = vocab[\"\n\n\n\n\"]\n+\n+        def get_repl_toks(tok: int) -> list[int]:\n+            if tok == newline_3:\n+                return [newline_1, newline_2]\n+            if tok == newline_4:\n+                return [newline_2, newline_2]\n+\n+            return [tok]\n+\n+        repl_token_ids = list[int]()\n+        repl_orig_idxs = list[int]()\n+        for orig_idx, orig_tok in enumerate(new_token_ids):\n+            repl_toks = get_repl_toks(orig_tok)\n+            repl_token_ids.extend(repl_toks)\n+            repl_orig_idxs.extend(orig_idx for _ in range(len(repl_toks)))\n+\n+        repls = find_mm_placeholders(mm_prompt_updates, repl_token_ids,\n+                                     mm_item_counts)\n+\n+        return {\n+            modality: [\n+                PlaceholderFeaturesInfo(\n+                    modality=p.modality,\n+                    item_idx=p.item_idx,\n+                    start_idx=repl_orig_idxs[p.start_idx],\n+                    tokens=p.tokens,\n+                    is_embed=p.is_embed,\n+                ) for p in placeholders\n+            ]\n+            for modality, placeholders in repls.items()\n+        }\n+\n+\n+class Gemma3nMultimodalEmbedder(nn.Module):\n+    \"\"\"Embeds token ids or soft tokens for multimodal content into language \n+    model space.\"\"\"\n+\n+    def __init__(\n+        self,\n+        multimodal_config: Union[Gemma3nAudioConfig, Gemma3nVisionConfig],\n+        text_config: Gemma3nTextConfig,\n+    ):\n+        super().__init__()\n+\n+        self.multimodal_hidden_size = multimodal_config.hidden_size\n+        self.eps = multimodal_config.rms_norm_eps\n+        self.vocab_offset = multimodal_config.vocab_offset\n+        self.vocab_size = multimodal_config.vocab_size\n+        self.text_hidden_size = text_config.hidden_size\n+\n+        self.embedding = VocabParallelEmbedding(\n+            self.vocab_size,\n+            self.multimodal_hidden_size,\n+        )\n+\n+        self.hard_embedding_norm = RMSNorm(\n+            self.multimodal_hidden_size,\n+            eps=self.eps,\n+        )\n+\n+        self.soft_embedding_norm = RMSNorm(\n+            self.multimodal_hidden_size,\n+            eps=self.eps,\n+        )\n+\n+        self.embedding_projection = RowParallelLinear(\n+            self.multimodal_hidden_size,\n+            self.text_hidden_size,\n+            bias=False,\n+        )\n+\n+        self.embedding_post_projection_norm = RMSNorm(\n+            self.text_hidden_size,\n+            eps=self.eps,\n+            has_weight=False,\n+        )\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        \"\"\"Embeds token ids or soft tokens for multimodal content into language model space.\n+\n+        Args:\n+            input_ids: A torch.LongTensor containing the token ids to embed. Values should be in the range\n+                `[vocab_offset, vocab_offset + vocab_size)`.\n+            inputs_embeds: A torch.Tensor containing the soft tokens to embed.\n+\n+        Returns:\n+            A torch.Tensor of embeddings with  shape `[batch_size, seq_len, self.config.text_config.hidden_size]`.\n+        \"\"\"  # noqa: E501\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is not None:\n+            emb_norm = self.soft_embedding_norm(inputs_embeds)\n+        else:\n+            hard_emb = self.embedding(input_ids - self.vocab_offset)\n+            emb_norm = self.hard_embedding_norm(hard_emb)\n+\n+        emb_norm_proj, _ = self.embedding_projection(emb_norm)\n+        return self.embedding_post_projection_norm(emb_norm_proj)\n+\n+\n+@MULTIMODAL_REGISTRY.register_processor(Gemma3MultiModalProcessor,\n+                                        info=Gemma3nProcessingInfo,\n+                                        dummy_inputs=Gemma3nDummyInputsBuilder)\n+class Gemma3nForConditionalGeneration(nn.Module, SupportsMultiModal):\n+    packed_modules_mapping = {\n+        \"qkv_proj\": [\n+            \"q_proj\",\n+            \"k_proj\",\n+            \"v_proj\",\n+        ],\n+        \"gate_up_proj\": [\n+            \"gate_proj\",\n+            \"up_proj\",\n+        ],\n+    }\n+\n+    hf_to_vllm_mapper = WeightsMapper(\n+        orig_to_new_prefix={\n+            # mapping for new names in checkpoint saved after transformers v4.52\n+            \"model.embed_audio.\": \"embed_audio.\",\n+            \"model.embed_vision.\": \"embed_vision.\",\n+            \"model.language_model.\": \"language_model.model.\",\n+            \"model.vision_tower.\": \"vision_tower.\",\n+            \"model.audio_tower.\": \"audio_tower.\",\n+            \"model.multi_modal_projector.\": \"multi_modal_projector.\",\n+            \"lm_head.\": \"language_model.lm_head.\",\n+            \"model\": \"language_model.model\",\n+\n+        })\n+\n+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = \"\"):\n+        super().__init__()\n+        config = vllm_config.model_config.hf_config\n+        quant_config = vllm_config.quant_config\n+        multimodal_config = vllm_config.model_config.multimodal_config\n+        self.config = config\n+        self.quant_config = quant_config\n+        self.multimodal_config = multimodal_config\n+        self.vocab_size = config.text_config.vocab_size\n+\n+        self.sliding_window = getattr(config.text_config,\n+                                      \"interleaved_sliding_window\", None)\n+\n+        self.vision_tower = AutoModel.from_config(config=config.vision_config)\n+        self.audio_tower = AutoModel.from_config(config=config.audio_config)\n+        self.embed_vision = Gemma3nMultimodalEmbedder(config.vision_config,\n+                                                      config.text_config)\n+        self.embed_audio = Gemma3nMultimodalEmbedder(config.audio_config,\n+                                                     config.text_config)\n+\n+        self.language_model: nn.Module = init_vllm_registered_model(\n+            vllm_config=vllm_config,\n+            hf_config=config.text_config,\n+            prefix=maybe_prefix(prefix, \"language_model\"),\n+            architectures=[\"Gemma3nForCausalLM\"],\n+        )\n+        self.language_model = cast(Gemma3nForCausalLM, self.language_model)\n+\n+    @property\n+    def dtype(self):\n+        return next(self.parameters()).dtype\n+\n+    def _validate_pixel_values(self, data: torch.Tensor) -> torch.Tensor:\n+        # TODO check if there are any \n+        return data\n+\n+    def _parse_and_validate_image_input(\n+            self, **kwargs: object) -> Optional[Gemma3nImageInputs]:\n+        pixel_values = kwargs.pop(\"pixel_values\", None)\n+        image_embeds = kwargs.pop(\"image_embeds\", None)\n+        # TODO is this the case?\n+        assert image_embeds is None, \"Gemma3n does not support image_embeds.\"\n+        if pixel_values is None:\n+            return None\n+\n+        if not isinstance(pixel_values, (torch.Tensor, list)):\n+            raise ValueError(\"Incorrect type of pixel values. \"\n+                             f\"Got type: {type(pixel_values)}\")\n+\n+        pixel_values = flatten_bn(pixel_values, concat=True)\n+        pixel_values = pixel_values.contiguous()\n+\n+        return Gemma3nImagePixelInputs(\n+            pixel_values=self._validate_pixel_values(pixel_values),\n+        )\n+    \n+    def _parse_and_validate_audio_input(\n+            self, **kwargs: object) -> Optional[Gemma3nAudioInputs]:\n+        input_features = kwargs.pop(\"input_features\", None)\n+        if input_features is None:\n+            return None\n+        \n+        input_features_mask = kwargs.pop(\"input_features_mask\", None)\n+        if input_features_mask is None:\n+            return None\n+        \n+        return Gemma3nAudioInputs(\n+            input_features=input_features,\n+            input_features_mask=input_features_mask,\n+        )\n+    \n+    def _parse_and_validate_multimodal_inputs(self, **kwargs: object) -> dict:\n+        mm_input_by_modality = {}\n+\n+        # Preserve the order of modalities if there are multiple of them\n+        # from the order of kwargs.\n+        for input_key in kwargs:\n+            if input_key in (\"pixel_values\", \"image_embeds\"\n+                            ) and \"image\" not in mm_input_by_modality:\n+                mm_input_by_modality[\n+                    \"image\"] = self._parse_and_validate_image_input(**kwargs)\n+            if input_key in (\"input_audio_features\"\n+                            ) and \"audio\" not in mm_input_by_modality:\n+                mm_input_by_modality[\n+                    \"audio\"] = self._parse_and_validate_audio_input(**kwargs)\n+        return mm_input_by_modality\n+\n+    def _process_image_input(\n+        self,\n+        image_input: Gemma3nImageInputs,\n+    ) -> list[torch.Tensor]:\n+        assert self.vision_tower is not None\n+\n+        pixel_values = image_input[\"pixel_values\"]\n+        vision_outputs = self.vision_tower(pixel_values=pixel_values,\n+                                           do_pooling=False,\n+                                           return_dict=True).last_hidden_state\n+        # TODO try to avoid copy here",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2190140625",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20495,
        "pr_file": "vllm/model_executor/models/gemma3n_mm.py",
        "discussion_id": "2190140625",
        "commented_code": "@@ -0,0 +1,725 @@\n+# SPDX-License-Identifier: Apache-2.0\n+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n+from collections.abc import Iterable, Mapping, Sequence\n+from typing import Any, Optional, TypedDict, Union, cast\n+\n+import torch\n+from torch import nn\n+from transformers import AutoModel, BatchFeature\n+from transformers.models.gemma3n import (Gemma3nAudioConfig,\n+                                         Gemma3nAudioFeatureExtractor,\n+                                         Gemma3nConfig, Gemma3nProcessor,\n+                                         Gemma3nTextConfig,\n+                                         Gemma3nVisionConfig)\n+from transformers.models.siglip import SiglipImageProcessorFast\n+\n+from vllm.config import VllmConfig\n+from vllm.logger import init_logger\n+from vllm.model_executor.layers.layernorm import RMSNorm\n+from vllm.model_executor.layers.linear import RowParallelLinear\n+from vllm.model_executor.layers.vocab_parallel_embedding import (\n+    VocabParallelEmbedding)\n+from vllm.model_executor.models.gemma3n import Gemma3nForCausalLM\n+from vllm.model_executor.models.module_mapping import MultiModelKeys\n+from vllm.model_executor.sampling_metadata import SamplingMetadata\n+from vllm.multimodal import MULTIMODAL_REGISTRY\n+from vllm.multimodal.inputs import (MultiModalDataDict, MultiModalFieldConfig,\n+                                    MultiModalKwargs)\n+from vllm.multimodal.parse import ImageProcessorItems, MultiModalDataItems, MultiModalDataParser\n+# yapf: disable\n+from vllm.multimodal.processing import (BaseMultiModalProcessor,\n+                                        BaseProcessingInfo, BoundPromptUpdate,\n+                                        PlaceholderFeaturesInfo,\n+                                        PromptReplacement, PromptTargetMatch,\n+                                        PromptUpdate, find_mm_placeholders,\n+                                        replace_token_matches)\n+# yapf: enable\n+from vllm.multimodal.profiling import BaseDummyInputsBuilder\n+from vllm.sequence import IntermediateTensors\n+\n+from .interfaces import MultiModalEmbeddings, SupportsMultiModal\n+from .utils import (AutoWeightsLoader, WeightsMapper, flatten_bn,\n+                    init_vllm_registered_model, maybe_prefix,\n+                    merge_multimodal_embeddings)\n+\n+logger = init_logger(__name__)\n+\n+# This should be based on model config but we hardcode them for now.\n+TOKENS_PER_IMAGE = 256\n+TOKENS_PER_AUDIO = 188\n+\n+\n+class Gemma3nImagePixelInputs(TypedDict):\n+    pixel_values: torch.Tensor\n+    \"\"\"Shape: `(batch_size * num_images, num_channels, height, width)`\"\"\"\n+\n+\n+class Gemma3nAudioInputs(TypedDict):\n+    input_features: torch.Tensor\n+    \"\"\"Shape: `(batch_size * num_audio, seq_length, num_features)`\"\"\"\n+    input_features_mask: torch.Tensor\n+    \"\"\"Shape: `(batch_size * num_audio, seq_length)`\"\"\"\n+\n+\n+Gemma3nImageInputs = Gemma3nImagePixelInputs\n+\n+\n+class Gemma3nProcessingInfo(BaseProcessingInfo):\n+\n+    def get_hf_config(self):\n+        return self.ctx.get_hf_config(Gemma3nConfig)\n+\n+    def get_hf_processor(self, **kwargs: object):\n+        return self.ctx.get_hf_processor(Gemma3nProcessor, **kwargs)\n+\n+    def get_supported_mm_limits(self) -> Mapping[str, Optional[int]]:\n+        return {\"image\": None, \"audio\": None}\n+\n+    def get_max_tokens_per_item(\n+            self, seq_len: int,\n+            mm_counts: Mapping[str, int]) -> Optional[Mapping[str, int]]:\n+\n+        return {\"image\": TOKENS_PER_IMAGE, \"audio\": TOKENS_PER_AUDIO}\n+\n+    def get_image_repl(\n+        self,\n+        *,\n+        image_width: int,\n+        image_height: int,\n+        processor: Optional[Gemma3nProcessor],\n+    ) -> str:\n+        \"\"\"\n+        Get the replacement text for image tokens.\n+        \n+        For Gemma3n, this should return the full_image_sequence which includes\n+        BOI token, repeated image tokens, and EOI token.\n+        \"\"\"\n+        if processor is None:\n+            processor = self.get_hf_processor()\n+\n+        # Return the full image sequence as defined by the processor\n+        return processor.full_image_sequence\n+\n+    def get_audio_repl(\n+        self,\n+        *,\n+        processor: Optional[Gemma3nProcessor],\n+    ) -> str:\n+        \"\"\"\n+        Get the replacement text for audio tokens.\n+        \n+        For Gemma3n, this should return the full_audio_sequence which includes\n+        BOA token, repeated audio tokens, and EOA token.\n+        \"\"\"\n+        if processor is None:\n+            processor = self.get_hf_processor()\n+\n+        # Return the full audio sequence as defined by the processor\n+        return processor.full_audio_sequence\n+\n+\n+class Gemma3nDummyInputsBuilder(BaseDummyInputsBuilder[Gemma3nProcessingInfo]):\n+\n+    def get_dummy_text(self, mm_counts: Mapping[str, int]) -> str:\n+        num_images = mm_counts.get(\"image\", 0)\n+        num_audios = mm_counts.get(\"audio\", 0)\n+\n+        processor = self.info.get_hf_processor()\n+        image_token = processor.image_token\n+        audio_token = processor.audio_token\n+\n+        return image_token * num_images + audio_token * num_audios\n+\n+    def get_dummy_mm_data(\n+        self,\n+        seq_len: int,\n+        mm_counts: Mapping[str, int],\n+    ) -> MultiModalDataDict:\n+        num_images = mm_counts.get(\"image\", 0)\n+        num_audios = mm_counts.get(\"audio\", 0)\n+        processor = self.info.get_hf_processor()\n+        feature_extractor: Gemma3nAudioFeatureExtractor = processor.feature_extractor  # noqa: E501\n+        # audio_len = feature_extractor.max_length\n+        audio_len = feature_extractor.fft_length\n+        image_processor: SiglipImageProcessorFast = processor.image_processor\n+        img_width = image_processor.size.get(\"width\", 224)\n+        img_height = image_processor.size.get(\"height\", 224)\n+\n+        return {\n+            \"image\":\n+            self._get_dummy_images(width=img_width,\n+                                   height=img_height,\n+                                   num_images=num_images),\n+            \"audio\":\n+            self._get_dummy_audios(length=audio_len, num_audios=num_audios)\n+        }\n+\n+\n+class Gemma3MultiModalProcessor(BaseMultiModalProcessor[Gemma3nProcessingInfo]\n+                                ):\n+\n+    def _get_data_parser(self) -> MultiModalDataParser:\n+        feature_extractor = self.info.get_hf_processor().feature_extractor\n+        return MultiModalDataParser(target_sr=feature_extractor.sampling_rate)\n+\n+    def _call_hf_processor(\n+        self,\n+        prompt: str,\n+        mm_data: Mapping[str, object],\n+        mm_kwargs: Mapping[str, object],\n+    ) -> BatchFeature:\n+        processed_outputs = super()._call_hf_processor(\n+            prompt,\n+            mm_data,\n+            mm_kwargs,\n+        )\n+        return processed_outputs\n+\n+    def _get_mm_fields_config(\n+        self,\n+        hf_inputs: BatchFeature,\n+        hf_processor_mm_kwargs: Mapping[str, object],\n+    ) -> Mapping[str, MultiModalFieldConfig]:\n+\n+        return dict(\n+            pixel_values=MultiModalFieldConfig.batched(\"image\"),\n+            input_features=MultiModalFieldConfig.batched(\"audio\"),\n+            input_features_mask=MultiModalFieldConfig.batched(\"audio\"),\n+        )\n+\n+    def _get_prompt_updates(\n+        self,\n+        mm_items: MultiModalDataItems,\n+        hf_processor_mm_kwargs: Mapping[str, Any],\n+        out_mm_kwargs: MultiModalKwargs,\n+    ) -> Sequence[PromptUpdate]:\n+        hf_processor = self.info.get_hf_processor(**hf_processor_mm_kwargs)\n+        \n+        prompt_updates = []\n+        \n+        # Handle image tokens\n+        if \"image\" in mm_items:\n+            image_token = hf_processor.image_token\n+\n+            def get_replacement_image(item_idx: int):\n+                images = mm_items.get_items(\"image\", ImageProcessorItems)\n+                image_size = images.get_image_size(item_idx)\n+                return self.info.get_image_repl(\n+                    image_width=image_size.width,\n+                    image_height=image_size.height,\n+                    processor=hf_processor,\n+                )\n+\n+            prompt_updates.append(\n+                PromptReplacement(\n+                    modality=\"image\",\n+                    target=image_token,\n+                    replacement=get_replacement_image,\n+                )\n+            )\n+        \n+        # Handle audio tokens\n+        if \"audio\" in mm_items:\n+            audio_token = hf_processor.audio_token\n+\n+            def get_replacement_audio(item_idx: int):\n+                return self.info.get_audio_repl(\n+                    processor=hf_processor,\n+                )\n+\n+            prompt_updates.append(\n+                PromptReplacement(\n+                    modality=\"audio\",\n+                    target=audio_token,\n+                    replacement=get_replacement_audio,\n+                )\n+            )\n+\n+        return prompt_updates\n+\n+    def _apply_token_matches(\n+        self,\n+        prompt: list[int],\n+        mm_matches: Mapping[str, Sequence[PromptTargetMatch]],\n+        mm_item_counts: Mapping[str, int],\n+    ) -> list[int]:\n+        token_ids = super()._apply_token_matches(\n+            prompt,\n+            mm_matches,\n+            mm_item_counts,\n+        )\n+\n+        # \"\\n\\n\\n\" and \"\\n\\n\\n\\n\" are single tokens\n+        # Since our replacement can insert \"\\n\\n\" next to \"\\n\"\n+        # tokens, we have to combine them to be consistent with\n+        # the output of the tokenizer\n+        tokenizer = self.info.get_tokenizer()\n+        vocab = tokenizer.get_vocab()\n+        newline_1 = vocab[\"\\n\"]\n+        newline_2 = vocab[\"\\n\\n\"]\n+        newline_3 = vocab[\"\\n\\n\\n\"]\n+        newline_4 = vocab[\"\\n\\n\\n\\n\"]\n+\n+        token_ids = replace_token_matches(\n+            token_ids,\n+            [newline_1, newline_2],\n+            [newline_3],\n+        )\n+        token_ids = replace_token_matches(\n+            token_ids,\n+            [newline_2, newline_1],\n+            [newline_3],\n+        )\n+        token_ids = replace_token_matches(\n+            token_ids,\n+            [newline_2, newline_2],\n+            [newline_4],\n+        )\n+\n+        return token_ids\n+\n+    def _find_mm_placeholders(\n+        self,\n+        mm_prompt_updates: Mapping[str, Sequence[BoundPromptUpdate]],\n+        new_token_ids: list[int],\n+        mm_item_counts: Mapping[str, int],\n+    ) -> Mapping[str, list[PlaceholderFeaturesInfo]]:\n+        # We need to detect \"\\n\\n\" inside \"\\n\\n\\n\" and \"\\n\\n\\n\\n\"\n+        tokenizer = self.info.get_tokenizer()\n+        vocab = tokenizer.get_vocab()\n+        newline_1 = vocab[\"\\n\"]\n+        newline_2 = vocab[\"\\n\\n\"]\n+        newline_3 = vocab[\"\\n\\n\\n\"]\n+        newline_4 = vocab[\"\\n\\n\\n\\n\"]\n+\n+        def get_repl_toks(tok: int) -> list[int]:\n+            if tok == newline_3:\n+                return [newline_1, newline_2]\n+            if tok == newline_4:\n+                return [newline_2, newline_2]\n+\n+            return [tok]\n+\n+        repl_token_ids = list[int]()\n+        repl_orig_idxs = list[int]()\n+        for orig_idx, orig_tok in enumerate(new_token_ids):\n+            repl_toks = get_repl_toks(orig_tok)\n+            repl_token_ids.extend(repl_toks)\n+            repl_orig_idxs.extend(orig_idx for _ in range(len(repl_toks)))\n+\n+        repls = find_mm_placeholders(mm_prompt_updates, repl_token_ids,\n+                                     mm_item_counts)\n+\n+        return {\n+            modality: [\n+                PlaceholderFeaturesInfo(\n+                    modality=p.modality,\n+                    item_idx=p.item_idx,\n+                    start_idx=repl_orig_idxs[p.start_idx],\n+                    tokens=p.tokens,\n+                    is_embed=p.is_embed,\n+                ) for p in placeholders\n+            ]\n+            for modality, placeholders in repls.items()\n+        }\n+\n+\n+class Gemma3nMultimodalEmbedder(nn.Module):\n+    \"\"\"Embeds token ids or soft tokens for multimodal content into language \n+    model space.\"\"\"\n+\n+    def __init__(\n+        self,\n+        multimodal_config: Union[Gemma3nAudioConfig, Gemma3nVisionConfig],\n+        text_config: Gemma3nTextConfig,\n+    ):\n+        super().__init__()\n+\n+        self.multimodal_hidden_size = multimodal_config.hidden_size\n+        self.eps = multimodal_config.rms_norm_eps\n+        self.vocab_offset = multimodal_config.vocab_offset\n+        self.vocab_size = multimodal_config.vocab_size\n+        self.text_hidden_size = text_config.hidden_size\n+\n+        self.embedding = VocabParallelEmbedding(\n+            self.vocab_size,\n+            self.multimodal_hidden_size,\n+        )\n+\n+        self.hard_embedding_norm = RMSNorm(\n+            self.multimodal_hidden_size,\n+            eps=self.eps,\n+        )\n+\n+        self.soft_embedding_norm = RMSNorm(\n+            self.multimodal_hidden_size,\n+            eps=self.eps,\n+        )\n+\n+        self.embedding_projection = RowParallelLinear(\n+            self.multimodal_hidden_size,\n+            self.text_hidden_size,\n+            bias=False,\n+        )\n+\n+        self.embedding_post_projection_norm = RMSNorm(\n+            self.text_hidden_size,\n+            eps=self.eps,\n+            has_weight=False,\n+        )\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        \"\"\"Embeds token ids or soft tokens for multimodal content into language model space.\n+\n+        Args:\n+            input_ids: A torch.LongTensor containing the token ids to embed. Values should be in the range\n+                `[vocab_offset, vocab_offset + vocab_size)`.\n+            inputs_embeds: A torch.Tensor containing the soft tokens to embed.\n+\n+        Returns:\n+            A torch.Tensor of embeddings with  shape `[batch_size, seq_len, self.config.text_config.hidden_size]`.\n+        \"\"\"  # noqa: E501\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is not None:\n+            emb_norm = self.soft_embedding_norm(inputs_embeds)\n+        else:\n+            hard_emb = self.embedding(input_ids - self.vocab_offset)\n+            emb_norm = self.hard_embedding_norm(hard_emb)\n+\n+        emb_norm_proj, _ = self.embedding_projection(emb_norm)\n+        return self.embedding_post_projection_norm(emb_norm_proj)\n+\n+\n+@MULTIMODAL_REGISTRY.register_processor(Gemma3MultiModalProcessor,\n+                                        info=Gemma3nProcessingInfo,\n+                                        dummy_inputs=Gemma3nDummyInputsBuilder)\n+class Gemma3nForConditionalGeneration(nn.Module, SupportsMultiModal):\n+    packed_modules_mapping = {\n+        \"qkv_proj\": [\n+            \"q_proj\",\n+            \"k_proj\",\n+            \"v_proj\",\n+        ],\n+        \"gate_up_proj\": [\n+            \"gate_proj\",\n+            \"up_proj\",\n+        ],\n+    }\n+\n+    hf_to_vllm_mapper = WeightsMapper(\n+        orig_to_new_prefix={\n+            # mapping for new names in checkpoint saved after transformers v4.52\n+            \"model.embed_audio.\": \"embed_audio.\",\n+            \"model.embed_vision.\": \"embed_vision.\",\n+            \"model.language_model.\": \"language_model.model.\",\n+            \"model.vision_tower.\": \"vision_tower.\",\n+            \"model.audio_tower.\": \"audio_tower.\",\n+            \"model.multi_modal_projector.\": \"multi_modal_projector.\",\n+            \"lm_head.\": \"language_model.lm_head.\",\n+            \"model\": \"language_model.model\",\n+\n+        })\n+\n+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = \"\"):\n+        super().__init__()\n+        config = vllm_config.model_config.hf_config\n+        quant_config = vllm_config.quant_config\n+        multimodal_config = vllm_config.model_config.multimodal_config\n+        self.config = config\n+        self.quant_config = quant_config\n+        self.multimodal_config = multimodal_config\n+        self.vocab_size = config.text_config.vocab_size\n+\n+        self.sliding_window = getattr(config.text_config,\n+                                      \"interleaved_sliding_window\", None)\n+\n+        self.vision_tower = AutoModel.from_config(config=config.vision_config)\n+        self.audio_tower = AutoModel.from_config(config=config.audio_config)\n+        self.embed_vision = Gemma3nMultimodalEmbedder(config.vision_config,\n+                                                      config.text_config)\n+        self.embed_audio = Gemma3nMultimodalEmbedder(config.audio_config,\n+                                                     config.text_config)\n+\n+        self.language_model: nn.Module = init_vllm_registered_model(\n+            vllm_config=vllm_config,\n+            hf_config=config.text_config,\n+            prefix=maybe_prefix(prefix, \"language_model\"),\n+            architectures=[\"Gemma3nForCausalLM\"],\n+        )\n+        self.language_model = cast(Gemma3nForCausalLM, self.language_model)\n+\n+    @property\n+    def dtype(self):\n+        return next(self.parameters()).dtype\n+\n+    def _validate_pixel_values(self, data: torch.Tensor) -> torch.Tensor:\n+        # TODO check if there are any \n+        return data\n+\n+    def _parse_and_validate_image_input(\n+            self, **kwargs: object) -> Optional[Gemma3nImageInputs]:\n+        pixel_values = kwargs.pop(\"pixel_values\", None)\n+        image_embeds = kwargs.pop(\"image_embeds\", None)\n+        # TODO is this the case?\n+        assert image_embeds is None, \"Gemma3n does not support image_embeds.\"\n+        if pixel_values is None:\n+            return None\n+\n+        if not isinstance(pixel_values, (torch.Tensor, list)):\n+            raise ValueError(\"Incorrect type of pixel values. \"\n+                             f\"Got type: {type(pixel_values)}\")\n+\n+        pixel_values = flatten_bn(pixel_values, concat=True)\n+        pixel_values = pixel_values.contiguous()\n+\n+        return Gemma3nImagePixelInputs(\n+            pixel_values=self._validate_pixel_values(pixel_values),\n+        )\n+    \n+    def _parse_and_validate_audio_input(\n+            self, **kwargs: object) -> Optional[Gemma3nAudioInputs]:\n+        input_features = kwargs.pop(\"input_features\", None)\n+        if input_features is None:\n+            return None\n+        \n+        input_features_mask = kwargs.pop(\"input_features_mask\", None)\n+        if input_features_mask is None:\n+            return None\n+        \n+        return Gemma3nAudioInputs(\n+            input_features=input_features,\n+            input_features_mask=input_features_mask,\n+        )\n+    \n+    def _parse_and_validate_multimodal_inputs(self, **kwargs: object) -> dict:\n+        mm_input_by_modality = {}\n+\n+        # Preserve the order of modalities if there are multiple of them\n+        # from the order of kwargs.\n+        for input_key in kwargs:\n+            if input_key in (\"pixel_values\", \"image_embeds\"\n+                            ) and \"image\" not in mm_input_by_modality:\n+                mm_input_by_modality[\n+                    \"image\"] = self._parse_and_validate_image_input(**kwargs)\n+            if input_key in (\"input_audio_features\"\n+                            ) and \"audio\" not in mm_input_by_modality:\n+                mm_input_by_modality[\n+                    \"audio\"] = self._parse_and_validate_audio_input(**kwargs)\n+        return mm_input_by_modality\n+\n+    def _process_image_input(\n+        self,\n+        image_input: Gemma3nImageInputs,\n+    ) -> list[torch.Tensor]:\n+        assert self.vision_tower is not None\n+\n+        pixel_values = image_input[\"pixel_values\"]\n+        vision_outputs = self.vision_tower(pixel_values=pixel_values,\n+                                           do_pooling=False,\n+                                           return_dict=True).last_hidden_state\n+        # TODO try to avoid copy here",
        "comment_created_at": "2025-07-07T13:38:34+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nI see the `TODO` here about avoiding a copy. The `.contiguous()` call after `permute()` indeed creates a copy of the tensor. While this might be necessary for performance of subsequent operations, it's worth investigating if the copy can be avoided, for example by changing how the tensor is manipulated upstream or if downstream operations can handle non-contiguous tensors. This is a good point for future optimization.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189517954",
    "pr_number": 20560,
    "pr_file": "vllm/v1/attention/backends/cpu_attn.py",
    "created_at": "2025-07-07T09:42:03+00:00",
    "commented_code": ")\n \n         return attn_metadata\n+\n+\n+class TorchSDPABackendImpl(AttentionImpl[TorchSDPAMetadata]):\n+\n+    def __init__(\n+        self,\n+        num_heads: int,\n+        head_size: int,\n+        scale: float,\n+        num_kv_heads: int,\n+        alibi_slopes: Optional[list[float]],\n+        sliding_window: Optional[int],\n+        kv_cache_dtype: str,\n+        blocksparse_params: Optional[dict[str, Any]] = None,\n+        logits_soft_cap: Optional[float] = None,\n+        attn_type: str = AttentionType.DECODER,\n+        kv_sharing_target_layer_name: Optional[str] = None,\n+        use_irope: bool = False,\n+    ) -> None:\n+        if kv_sharing_target_layer_name is not None:\n+            raise NotImplementedError(\"KV sharing is not supported in V0.\")\n+        if blocksparse_params is not None:\n+            raise ValueError(\n+                \"Torch SPDA does not support block-sparse attention.\")\n+        if logits_soft_cap is not None:\n+            logger.warning_once(\"Torch SPDA does not support logits soft cap. \"\n+                                \"Outputs may be slightly off.\")\n+        if use_irope:\n+            logger.warning_once(\n+                \"Using irope in Torch SPDA is not supported yet, it will fall\"\n+                \" back to global attention for long context.\")\n+        self.paged_attn_impl = _get_paged_attn_impl()\n+        self.num_heads = num_heads\n+        self.head_size = head_size\n+        self.scale = float(scale)\n+        self.num_kv_heads = num_kv_heads\n+        if alibi_slopes is not None:\n+            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)\n+        self.alibi_slopes = alibi_slopes\n+        self.sliding_window = sliding_window\n+        self.kv_cache_dtype = kv_cache_dtype\n+\n+        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n+        self.need_mask = (self.alibi_slopes is not None\n+                          or self.sliding_window is not None)\n+\n+        supported_head_sizes = self.paged_attn_impl \\\n+                                .get_supported_head_sizes()  # type: ignore\n+        if head_size not in supported_head_sizes:\n+            raise ValueError(\n+                f\"Head size {head_size} is not supported by PagedAttention. \"\n+                f\"Supported head sizes are: {supported_head_sizes}.\")\n+\n+        if is_quantized_kv_cache(kv_cache_dtype) and not _use_ipex:\n+            raise NotImplementedError(\n+                \"Torch SDPA backend FP8 KV cache requires \"\n+                \"intel_extension_for_pytorch support.\")\n+        self.attn_type = attn_type\n+\n+    def forward(\n+        self,\n+        layer: AttentionLayer,\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        kv_cache: torch.Tensor,\n+        attn_metadata: TorchSDPAMetadata,  # type: ignore\n+        output: Optional[torch.Tensor] = None,\n+        output_scale: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        \"\"\"Forward pass with torch SDPA and PagedAttention.\n+\n+        Args:\n+            query: shape = [num_tokens, num_heads * head_size]\n+            key: shape = [num_tokens, num_kv_heads * head_size]\n+            value: shape = [num_tokens, num_kv_heads * head_size]\n+            kv_cache = [2, num_blocks, block_size * num_kv_heads * head_size]\n+                NOTE: kv_cache will be an empty tensor with shape [0]\n+                for profiling run.\n+            attn_metadata: Metadata for attention.\n+        Returns:\n+            shape = [num_tokens, num_heads * head_size]\n+        \"\"\"\n+        if output_scale is not None:\n+            raise NotImplementedError(\n+                \"fused output quantization is not yet supported\"\n+                \" for TorchSDPABackendImpl\")\n+\n+        # For warming-up\n+        if attn_metadata is None:\n+            return query\n+\n+        attn_type = self.attn_type\n+        if (attn_type == AttentionType.ENCODER\n+                and (not attn_metadata.is_all_encoder_attn_metadata_set)):\n+            raise AttributeError(\"Encoder attention requires setting \"\n+                                 \"encoder metadata attributes.\")\n+        elif (attn_type == AttentionType.ENCODER_DECODER\n+              and (not attn_metadata.is_all_cross_attn_metadata_set)):\n+            raise AttributeError(\"Encoder/decoder cross-attention \"\n+                                 \"requires setting cross-attention \"\n+                                 \"metadata attributes.\")\n+\n+        # Reshape the query, key, and value tensors.\n+        query = query.view(-1, self.num_heads, self.head_size)\n+        if key is not None:\n+            assert value is not None\n+            key = key.view(-1, self.num_kv_heads, self.head_size)\n+            value = value.view(-1, self.num_kv_heads, self.head_size)\n+        else:\n+            assert value is None\n+\n+        if (attn_type != AttentionType.ENCODER and kv_cache.numel() > 0):\n+            # KV-cache during decoder-self- or\n+            # encoder-decoder-cross-attention, but not\n+            # during encoder attention.\n+            #\n+            # Even if there are no new key/value pairs to cache,\n+            # we still need to break out key_cache and value_cache\n+            # i.e. for later use by paged attention\n+            key_cache, value_cache = self.paged_attn_impl \\\n+            .split_kv_cache( # type: ignore\n+                kv_cache, self.num_kv_heads, self.head_size)\n+\n+            if (key is not None) and (value is not None):\n+                if attn_type == AttentionType.ENCODER_DECODER:\n+                    # Update cross-attention KV cache (prefill-only)\n+                    # During cross-attention decode, key & value will be None,\n+                    # preventing this IF-statement branch from running\n+                    updated_slot_mapping = attn_metadata.cross_slot_mapping\n+                else:\n+                    # Update self-attention KV cache (prefill/decode)\n+                    updated_slot_mapping = attn_metadata.slot_mapping\n+\n+                self.paged_attn_impl.write_to_paged_cache(  # type: ignore\n+                    key, value, key_cache, value_cache, updated_slot_mapping,\n+                    self.kv_cache_dtype, layer._k_scale, layer._v_scale)\n+\n+        if attn_type != AttentionType.ENCODER:\n+            # Decoder self-attention supports chunked prefill.\n+            # Encoder/decoder cross-attention requires no chunked\n+            # prefill (100% prefill or 100% decode tokens, no mix)\n+            num_prefill_tokens = attn_metadata.num_prefill_tokens\n+            num_decode_tokens = attn_metadata.num_decode_tokens\n+        else:\n+            # Encoder attention - chunked prefill is not applicable;\n+            # derive token-count from query shape & and treat them\n+            # as 100% prefill tokens\n+            assert attn_metadata.num_encoder_tokens is not None\n+            num_prefill_tokens = attn_metadata.num_encoder_tokens\n+            num_decode_tokens = 0\n+\n+        if attn_type == AttentionType.DECODER:\n+            # Only enforce this shape-constraint for decoder\n+            # self-attention\n+            assert key.shape[0] == num_prefill_tokens + num_decode_tokens\n+            assert value.shape[0] == num_prefill_tokens + num_decode_tokens\n+\n+        output = torch.empty_like(query)\n+        if prefill_meta := attn_metadata.prefill_metadata:\n+            if not prefill_meta.prefill_metadata.chunked_prefill:  # type: ignore\n+                assert attn_metadata.seq_lens is not None\n+                self._run_sdpa_forward(output,\n+                                       query,\n+                                       key,\n+                                       value,\n+                                       prefill_meta,\n+                                       attn_type=attn_type)\n+            else:\n+                # prefix-enabled attention\n+                assert not self.need_mask\n+                import intel_extension_for_pytorch.llm.modules as ipex_modules\n+                output = torch.empty_like(query)\n+                ipex_modules.PagedAttention.flash_attn_varlen_func(\n+                    output[:prefill_meta.num_prefill_tokens, :, :],\n+                    query[:prefill_meta.num_prefill_tokens, :, :],\n+                    key_cache,\n+                    value_cache,\n+                    prefill_meta.prefill_query_start_loc,\n+                    prefill_meta.kv_start_loc,\n+                    prefill_meta.max_query_len,\n+                    prefill_meta.max_kv_len,\n+                    self.scale,\n+                    True,\n+                    prefill_meta.prefill_block_tables,\n+                    self.alibi_slopes,\n+                )\n+\n+        if decode_meta := attn_metadata.decode_metadata:\n+            assert attn_type != AttentionType.ENCODER_ONLY, (\n+                \"Encoder-only models should not have decode metadata.\")\n+            # Decoding run.\n+            (\n+                seq_lens_arg,\n+                max_seq_len_arg,\n+                block_tables_arg,\n+            ) = decode_meta.get_seq_len_block_table_args(attn_type)\n+\n+            self.paged_attn_impl.forward_decode(  # type: ignore\n+                output[attn_metadata.num_prefill_tokens:, :, :],\n+                query[attn_metadata.num_prefill_tokens:, :, :],\n+                key_cache,\n+                value_cache,\n+                block_tables_arg,\n+                seq_lens_arg,\n+                max_seq_len_arg,\n+                self.kv_cache_dtype,\n+                self.num_kv_heads,\n+                self.scale,\n+                self.alibi_slopes,\n+                layer._k_scale,\n+                layer._v_scale,\n+            )\n+\n+        # Reshape the output tensor.\n+        return output.view(-1, self.num_heads * self.head_size)\n+\n+    def _run_sdpa_forward(\n+        self,\n+        output: torch.Tensor,\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        attn_metadata: TorchSDPAMetadata,\n+        attn_type: str = AttentionType.DECODER,\n+    ) -> None:\n+        if self.num_kv_heads != self.num_heads:\n+            key = key.repeat_interleave(self.num_queries_per_kv, dim=1)\n+            value = value.repeat_interleave(self.num_queries_per_kv, dim=1)\n+\n+        attn_masks = attn_metadata.get_attn_bias(attn_type)\n+        if attn_masks is None:\n+            if self.alibi_slopes is not None:\n+                attn_masks = _make_alibi_bias(\n+                    self.alibi_slopes, query.dtype,\n+                    attn_metadata.seq_lens)  # type: ignore\n+            elif self.sliding_window is not None:\n+                assert attn_metadata.seq_lens is not None\n+                attn_masks = _make_sliding_window_bias(\n+                    attn_metadata.seq_lens, self.sliding_window,\n+                    query.dtype)  # type: ignore\n+            else:\n+                seq_lens, _ = attn_metadata.get_seq_lens(attn_type)\n+                attn_masks = [None] * len(seq_lens)\n+            attn_metadata.set_attn_bias(attn_masks, attn_type)\n+\n+        query = query.movedim(0, query.dim() - 2)\n+        key = key.movedim(0, key.dim() - 2)\n+        value = value.movedim(0, value.dim() - 2)\n+\n+        causal_attn = (attn_type == AttentionType.DECODER)\n+\n+        seq_lens_q, seq_lens_kv = attn_metadata.get_seq_lens(attn_type)\n+        start_q, start_kv = 0, 0\n+        for seq_len_q, seq_len_kv, mask in zip(seq_lens_q, seq_lens_kv,\n+                                               attn_masks):\n+            end_q = start_q + seq_len_q\n+            end_kv = start_kv + seq_len_kv\n+            sub_out = scaled_dot_product_attention(\n+                query[None, :, start_q:end_q, :],\n+                key[None, :, start_kv:end_kv, :],\n+                value[None, :, start_kv:end_kv, :],\n+                attn_mask=mask,\n+                dropout_p=0.0,\n+                is_causal=causal_attn and mask is None,\n+                scale=self.scale).squeeze(0).movedim(query.dim() - 2, 0)\n+            output[start_q:end_q, :, :] = sub_out\n+            start_q, start_kv = end_q, end_kv",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2189517954",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20560,
        "pr_file": "vllm/v1/attention/backends/cpu_attn.py",
        "discussion_id": "2189517954",
        "commented_code": "@@ -182,3 +430,503 @@ def build(self, common_prefix_len: int,\n         )\n \n         return attn_metadata\n+\n+\n+class TorchSDPABackendImpl(AttentionImpl[TorchSDPAMetadata]):\n+\n+    def __init__(\n+        self,\n+        num_heads: int,\n+        head_size: int,\n+        scale: float,\n+        num_kv_heads: int,\n+        alibi_slopes: Optional[list[float]],\n+        sliding_window: Optional[int],\n+        kv_cache_dtype: str,\n+        blocksparse_params: Optional[dict[str, Any]] = None,\n+        logits_soft_cap: Optional[float] = None,\n+        attn_type: str = AttentionType.DECODER,\n+        kv_sharing_target_layer_name: Optional[str] = None,\n+        use_irope: bool = False,\n+    ) -> None:\n+        if kv_sharing_target_layer_name is not None:\n+            raise NotImplementedError(\"KV sharing is not supported in V0.\")\n+        if blocksparse_params is not None:\n+            raise ValueError(\n+                \"Torch SPDA does not support block-sparse attention.\")\n+        if logits_soft_cap is not None:\n+            logger.warning_once(\"Torch SPDA does not support logits soft cap. \"\n+                                \"Outputs may be slightly off.\")\n+        if use_irope:\n+            logger.warning_once(\n+                \"Using irope in Torch SPDA is not supported yet, it will fall\"\n+                \" back to global attention for long context.\")\n+        self.paged_attn_impl = _get_paged_attn_impl()\n+        self.num_heads = num_heads\n+        self.head_size = head_size\n+        self.scale = float(scale)\n+        self.num_kv_heads = num_kv_heads\n+        if alibi_slopes is not None:\n+            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)\n+        self.alibi_slopes = alibi_slopes\n+        self.sliding_window = sliding_window\n+        self.kv_cache_dtype = kv_cache_dtype\n+\n+        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n+        self.need_mask = (self.alibi_slopes is not None\n+                          or self.sliding_window is not None)\n+\n+        supported_head_sizes = self.paged_attn_impl \\\n+                                .get_supported_head_sizes()  # type: ignore\n+        if head_size not in supported_head_sizes:\n+            raise ValueError(\n+                f\"Head size {head_size} is not supported by PagedAttention. \"\n+                f\"Supported head sizes are: {supported_head_sizes}.\")\n+\n+        if is_quantized_kv_cache(kv_cache_dtype) and not _use_ipex:\n+            raise NotImplementedError(\n+                \"Torch SDPA backend FP8 KV cache requires \"\n+                \"intel_extension_for_pytorch support.\")\n+        self.attn_type = attn_type\n+\n+    def forward(\n+        self,\n+        layer: AttentionLayer,\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        kv_cache: torch.Tensor,\n+        attn_metadata: TorchSDPAMetadata,  # type: ignore\n+        output: Optional[torch.Tensor] = None,\n+        output_scale: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        \"\"\"Forward pass with torch SDPA and PagedAttention.\n+\n+        Args:\n+            query: shape = [num_tokens, num_heads * head_size]\n+            key: shape = [num_tokens, num_kv_heads * head_size]\n+            value: shape = [num_tokens, num_kv_heads * head_size]\n+            kv_cache = [2, num_blocks, block_size * num_kv_heads * head_size]\n+                NOTE: kv_cache will be an empty tensor with shape [0]\n+                for profiling run.\n+            attn_metadata: Metadata for attention.\n+        Returns:\n+            shape = [num_tokens, num_heads * head_size]\n+        \"\"\"\n+        if output_scale is not None:\n+            raise NotImplementedError(\n+                \"fused output quantization is not yet supported\"\n+                \" for TorchSDPABackendImpl\")\n+\n+        # For warming-up\n+        if attn_metadata is None:\n+            return query\n+\n+        attn_type = self.attn_type\n+        if (attn_type == AttentionType.ENCODER\n+                and (not attn_metadata.is_all_encoder_attn_metadata_set)):\n+            raise AttributeError(\"Encoder attention requires setting \"\n+                                 \"encoder metadata attributes.\")\n+        elif (attn_type == AttentionType.ENCODER_DECODER\n+              and (not attn_metadata.is_all_cross_attn_metadata_set)):\n+            raise AttributeError(\"Encoder/decoder cross-attention \"\n+                                 \"requires setting cross-attention \"\n+                                 \"metadata attributes.\")\n+\n+        # Reshape the query, key, and value tensors.\n+        query = query.view(-1, self.num_heads, self.head_size)\n+        if key is not None:\n+            assert value is not None\n+            key = key.view(-1, self.num_kv_heads, self.head_size)\n+            value = value.view(-1, self.num_kv_heads, self.head_size)\n+        else:\n+            assert value is None\n+\n+        if (attn_type != AttentionType.ENCODER and kv_cache.numel() > 0):\n+            # KV-cache during decoder-self- or\n+            # encoder-decoder-cross-attention, but not\n+            # during encoder attention.\n+            #\n+            # Even if there are no new key/value pairs to cache,\n+            # we still need to break out key_cache and value_cache\n+            # i.e. for later use by paged attention\n+            key_cache, value_cache = self.paged_attn_impl \\\n+            .split_kv_cache( # type: ignore\n+                kv_cache, self.num_kv_heads, self.head_size)\n+\n+            if (key is not None) and (value is not None):\n+                if attn_type == AttentionType.ENCODER_DECODER:\n+                    # Update cross-attention KV cache (prefill-only)\n+                    # During cross-attention decode, key & value will be None,\n+                    # preventing this IF-statement branch from running\n+                    updated_slot_mapping = attn_metadata.cross_slot_mapping\n+                else:\n+                    # Update self-attention KV cache (prefill/decode)\n+                    updated_slot_mapping = attn_metadata.slot_mapping\n+\n+                self.paged_attn_impl.write_to_paged_cache(  # type: ignore\n+                    key, value, key_cache, value_cache, updated_slot_mapping,\n+                    self.kv_cache_dtype, layer._k_scale, layer._v_scale)\n+\n+        if attn_type != AttentionType.ENCODER:\n+            # Decoder self-attention supports chunked prefill.\n+            # Encoder/decoder cross-attention requires no chunked\n+            # prefill (100% prefill or 100% decode tokens, no mix)\n+            num_prefill_tokens = attn_metadata.num_prefill_tokens\n+            num_decode_tokens = attn_metadata.num_decode_tokens\n+        else:\n+            # Encoder attention - chunked prefill is not applicable;\n+            # derive token-count from query shape & and treat them\n+            # as 100% prefill tokens\n+            assert attn_metadata.num_encoder_tokens is not None\n+            num_prefill_tokens = attn_metadata.num_encoder_tokens\n+            num_decode_tokens = 0\n+\n+        if attn_type == AttentionType.DECODER:\n+            # Only enforce this shape-constraint for decoder\n+            # self-attention\n+            assert key.shape[0] == num_prefill_tokens + num_decode_tokens\n+            assert value.shape[0] == num_prefill_tokens + num_decode_tokens\n+\n+        output = torch.empty_like(query)\n+        if prefill_meta := attn_metadata.prefill_metadata:\n+            if not prefill_meta.prefill_metadata.chunked_prefill:  # type: ignore\n+                assert attn_metadata.seq_lens is not None\n+                self._run_sdpa_forward(output,\n+                                       query,\n+                                       key,\n+                                       value,\n+                                       prefill_meta,\n+                                       attn_type=attn_type)\n+            else:\n+                # prefix-enabled attention\n+                assert not self.need_mask\n+                import intel_extension_for_pytorch.llm.modules as ipex_modules\n+                output = torch.empty_like(query)\n+                ipex_modules.PagedAttention.flash_attn_varlen_func(\n+                    output[:prefill_meta.num_prefill_tokens, :, :],\n+                    query[:prefill_meta.num_prefill_tokens, :, :],\n+                    key_cache,\n+                    value_cache,\n+                    prefill_meta.prefill_query_start_loc,\n+                    prefill_meta.kv_start_loc,\n+                    prefill_meta.max_query_len,\n+                    prefill_meta.max_kv_len,\n+                    self.scale,\n+                    True,\n+                    prefill_meta.prefill_block_tables,\n+                    self.alibi_slopes,\n+                )\n+\n+        if decode_meta := attn_metadata.decode_metadata:\n+            assert attn_type != AttentionType.ENCODER_ONLY, (\n+                \"Encoder-only models should not have decode metadata.\")\n+            # Decoding run.\n+            (\n+                seq_lens_arg,\n+                max_seq_len_arg,\n+                block_tables_arg,\n+            ) = decode_meta.get_seq_len_block_table_args(attn_type)\n+\n+            self.paged_attn_impl.forward_decode(  # type: ignore\n+                output[attn_metadata.num_prefill_tokens:, :, :],\n+                query[attn_metadata.num_prefill_tokens:, :, :],\n+                key_cache,\n+                value_cache,\n+                block_tables_arg,\n+                seq_lens_arg,\n+                max_seq_len_arg,\n+                self.kv_cache_dtype,\n+                self.num_kv_heads,\n+                self.scale,\n+                self.alibi_slopes,\n+                layer._k_scale,\n+                layer._v_scale,\n+            )\n+\n+        # Reshape the output tensor.\n+        return output.view(-1, self.num_heads * self.head_size)\n+\n+    def _run_sdpa_forward(\n+        self,\n+        output: torch.Tensor,\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        attn_metadata: TorchSDPAMetadata,\n+        attn_type: str = AttentionType.DECODER,\n+    ) -> None:\n+        if self.num_kv_heads != self.num_heads:\n+            key = key.repeat_interleave(self.num_queries_per_kv, dim=1)\n+            value = value.repeat_interleave(self.num_queries_per_kv, dim=1)\n+\n+        attn_masks = attn_metadata.get_attn_bias(attn_type)\n+        if attn_masks is None:\n+            if self.alibi_slopes is not None:\n+                attn_masks = _make_alibi_bias(\n+                    self.alibi_slopes, query.dtype,\n+                    attn_metadata.seq_lens)  # type: ignore\n+            elif self.sliding_window is not None:\n+                assert attn_metadata.seq_lens is not None\n+                attn_masks = _make_sliding_window_bias(\n+                    attn_metadata.seq_lens, self.sliding_window,\n+                    query.dtype)  # type: ignore\n+            else:\n+                seq_lens, _ = attn_metadata.get_seq_lens(attn_type)\n+                attn_masks = [None] * len(seq_lens)\n+            attn_metadata.set_attn_bias(attn_masks, attn_type)\n+\n+        query = query.movedim(0, query.dim() - 2)\n+        key = key.movedim(0, key.dim() - 2)\n+        value = value.movedim(0, value.dim() - 2)\n+\n+        causal_attn = (attn_type == AttentionType.DECODER)\n+\n+        seq_lens_q, seq_lens_kv = attn_metadata.get_seq_lens(attn_type)\n+        start_q, start_kv = 0, 0\n+        for seq_len_q, seq_len_kv, mask in zip(seq_lens_q, seq_lens_kv,\n+                                               attn_masks):\n+            end_q = start_q + seq_len_q\n+            end_kv = start_kv + seq_len_kv\n+            sub_out = scaled_dot_product_attention(\n+                query[None, :, start_q:end_q, :],\n+                key[None, :, start_kv:end_kv, :],\n+                value[None, :, start_kv:end_kv, :],\n+                attn_mask=mask,\n+                dropout_p=0.0,\n+                is_causal=causal_attn and mask is None,\n+                scale=self.scale).squeeze(0).movedim(query.dim() - 2, 0)\n+            output[start_q:end_q, :, :] = sub_out\n+            start_q, start_kv = end_q, end_kv",
        "comment_created_at": "2025-07-07T09:42:03+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![high](https://www.gstatic.com/codereviewagent/high-priority.svg)\n\nThe loop iterates through sequences one by one, which can be inefficient due to Python loop overhead and tensor slicing. `scaled_dot_product_attention` is optimized for batched operations. Vectorizing this operation by constructing a single batch-level attention mask and calling `scaled_dot_product_attention` once can improve performance.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2191056148",
    "pr_number": 20591,
    "pr_file": "vllm/model_executor/models/llama4_eagle.py",
    "created_at": "2025-07-07T21:45:22+00:00",
    "commented_code": "+# SPDX-License-Identifier: Apache-2.0\n+\n+from collections.abc import Iterable\n+\n+import torch\n+import torch.nn as nn\n+\n+from vllm.compilation.decorators import support_torch_compile\n+from vllm.config import VllmConfig\n+from vllm.distributed.parallel_state import get_pp_group\n+from vllm.logger import init_logger\n+from vllm.model_executor.layers.layernorm import RMSNorm\n+from vllm.model_executor.layers.logits_processor import LogitsProcessor\n+from vllm.model_executor.layers.quantization.base_config import (\n+    QuantizationConfig)\n+from vllm.model_executor.layers.quantization.torchao import TorchAOConfig\n+from vllm.model_executor.layers.vocab_parallel_embedding import (\n+    VocabParallelEmbedding)\n+from vllm.model_executor.model_loader.weight_utils import default_weight_loader\n+from vllm.model_executor.models.llama4 import (Llama4DecoderLayer,\n+                                               Llama4ForCausalLM)\n+from vllm.model_executor.models.utils import extract_layer_index\n+\n+from .utils import AutoWeightsLoader, maybe_prefix\n+from typing import Optional\n+\n+logger = init_logger(__name__)\n+\n+\n+@support_torch_compile\n+class LlamaModel(nn.Module):\n+\n+    def __init__(\n+        self,\n+        *,\n+        vllm_config: VllmConfig,\n+        prefix: str = \"\",\n+        start_layer_id: int = 0,\n+        quant_config: Optional[QuantizationConfig] = None,\n+    ) -> None:\n+        super().__init__()\n+        self.config = vllm_config. \\\n+            speculative_config.draft_model_config.hf_config\n+        self.validate_and_update_config(start_layer_id, quant_config)\n+        self.vocab_size = self.config.vocab_size\n+        self.embed_tokens = VocabParallelEmbedding(\n+            self.config.vocab_size,\n+            self.config.hidden_size,\n+            prefix=maybe_prefix(prefix, \"embed_tokens\"),\n+        )\n+\n+        self.layers = nn.ModuleList([\n+            Llama4DecoderLayer(\n+                self.config,\n+                quant_config=quant_config,\n+                prefix=maybe_prefix(prefix, f\"layers.{i + start_layer_id}\"),\n+            ) for i in range(self.config.num_hidden_layers)\n+        ])\n+        self.fc = torch.nn.Linear(self.config.hidden_size * 2,\n+                                  self.config.hidden_size,\n+                                  bias=False)\n+        self.norm = RMSNorm(self.config.hidden_size,\n+                            eps=self.config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor],\n+        positions: torch.Tensor,\n+        hidden_states: torch.Tensor,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_embeds = self.embed_tokens(input_ids)\n+        hidden_states = self.fc(\n+            torch.cat((input_embeds, hidden_states), dim=-1))\n+        residual = None\n+        for layer in self.layers:\n+            hidden_states, residual = layer(\n+                positions,\n+                hidden_states,\n+                residual,\n+            )\n+        hidden_states, _ = self.norm(hidden_states, residual)\n+        return hidden_states, hidden_states\n+\n+    def load_weights(self, weights: Iterable[tuple[str,\n+                                                   torch.Tensor]]) -> set[str]:\n+        stacked_params_mapping = [\n+            # (param_name, shard_name, shard_id)\n+            (\".qkv_proj\", \".q_proj\", \"q\"),\n+            (\".qkv_proj\", \".k_proj\", \"k\"),\n+            (\".qkv_proj\", \".v_proj\", \"v\"),\n+            (\".gate_up_proj\", \".gate_proj\", 0),\n+            (\".gate_up_proj\", \".up_proj\", 1),\n+        ]\n+        params_dict = dict(self.named_parameters())\n+        loaded_params: set[str] = set()\n+        for name, loaded_weight in weights:\n+            name = name.removeprefix(\"model.\")\n+            for param_name, weight_name, shard_id in stacked_params_mapping:\n+                if weight_name not in name:\n+                    continue\n+                name = name.replace(weight_name, param_name)\n+                param = params_dict[name]\n+                weight_loader = param.weight_loader\n+                weight_loader(param, loaded_weight, shard_id)\n+                break\n+            else:\n+                # if PP disabled then draft will share embed with target\n+                if get_pp_group().world_size == 1 and \\\n+                    \"embed_tokens.\" in name:\n+                    continue\n+                param = params_dict[name]\n+                weight_loader = getattr(param, \"weight_loader\",\n+                                        default_weight_loader)\n+                weight_loader(param, loaded_weight)\n+            loaded_params.add(name)\n+        for name in params_dict:\n+            # if PP disabled then draft will share embed with target\n+            if get_pp_group().world_size == 1 and \\\n+                \"embed_tokens.\" in name:\n+                continue\n+            assert name in loaded_params, f\"{name} is not loaded!\"\n+        return loaded_params\n+\n+    def validate_and_update_config(\n+            self,\n+            start_layer_id: int,\n+            quant_config: Optional[QuantizationConfig] = None) -> None:\n+        # yoco and moe is not supported by draft model yet\n+        assert self.config.yoco_global_kv_layer is None\n+        assert self.config.yoco_local_kv_layer is None\n+        assert len(self.config.moe_layers) == 0\n+        # draft model layer index is increased by start_layer_id,\n+        # so we need to pad relevant configs accordingly\n+        self.config.no_rope_layers = [\n+            0\n+        ] * start_layer_id + self.config.no_rope_layers\n+        # currently only TorchAO quantization is supported\n+        if isinstance(quant_config, TorchAOConfig):\n+\n+            def pad_layer_name(layer: str) -> str:\n+                layer_index = extract_layer_index(layer)\n+                return layer.replace(str(layer_index),\n+                                     str(layer_index + start_layer_id))\n+\n+            quant_config.torchao_config.module_fqn_to_config = {\n+                pad_layer_name(layer): quantization\n+                for layer, quantization in\n+                quant_config.torchao_config.module_fqn_to_config.items()\n+            }\n+\n+\n+class EagleLlama4ForCausalLM(Llama4ForCausalLM):\n+\n+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = \"\"):\n+        nn.Module.__init__(self)\n+        self.config = vllm_config. \\\n+            speculative_config.draft_model_config.hf_config\n+        target_layer_num = vllm_config.model_config.get_num_layers(\n+            vllm_config.parallel_config)\n+        # draft model quantization config may differ from target model\n+        quant_config = VllmConfig.get_quantization_config(\n+            vllm_config.speculative_config.draft_model_config,\n+            vllm_config.load_config)\n+        self.model = LlamaModel(vllm_config=vllm_config,\n+                                prefix=\"model\",\n+                                start_layer_id=target_layer_num,\n+                                quant_config=quant_config)\n+        logit_scale = getattr(self.config, \"logit_scale\", 1.0)\n+        self.logits_processor = LogitsProcessor(self.config.vocab_size,\n+                                                scale=logit_scale)\n+\n+    def forward(\n+        self,\n+        input_ids: torch.Tensor,\n+        positions: torch.Tensor,\n+        hidden_states: torch.Tensor,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        return self.model(input_ids, positions, hidden_states)\n+\n+    def load_weights(self, weights: Iterable[tuple[str, torch.Tensor]]):\n+        loader = AutoWeightsLoader(\n+            self,\n+            # lm_head is tied with target model (Llama4ForCausalLM)\n+            skip_prefixes=([\"lm_head.\"]),\n+        )\n+\n+        model_weights = {}\n+        weights = [\n+            self.permute_qk_weight_for_rotary(name, loaded_weight)\n+            for name, loaded_weight in weights\n+        ]\n+        for name, loaded_weight in weights:\n+            if \"lm_head\" not in name:\n+                name = \"model.\" + name\n+            model_weights[name] = loaded_weight\n+\n+        loader.load_weights(model_weights.items())",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2191056148",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20591,
        "pr_file": "vllm/model_executor/models/llama4_eagle.py",
        "discussion_id": "2191056148",
        "commented_code": "@@ -0,0 +1,197 @@\n+# SPDX-License-Identifier: Apache-2.0\n+\n+from collections.abc import Iterable\n+\n+import torch\n+import torch.nn as nn\n+\n+from vllm.compilation.decorators import support_torch_compile\n+from vllm.config import VllmConfig\n+from vllm.distributed.parallel_state import get_pp_group\n+from vllm.logger import init_logger\n+from vllm.model_executor.layers.layernorm import RMSNorm\n+from vllm.model_executor.layers.logits_processor import LogitsProcessor\n+from vllm.model_executor.layers.quantization.base_config import (\n+    QuantizationConfig)\n+from vllm.model_executor.layers.quantization.torchao import TorchAOConfig\n+from vllm.model_executor.layers.vocab_parallel_embedding import (\n+    VocabParallelEmbedding)\n+from vllm.model_executor.model_loader.weight_utils import default_weight_loader\n+from vllm.model_executor.models.llama4 import (Llama4DecoderLayer,\n+                                               Llama4ForCausalLM)\n+from vllm.model_executor.models.utils import extract_layer_index\n+\n+from .utils import AutoWeightsLoader, maybe_prefix\n+from typing import Optional\n+\n+logger = init_logger(__name__)\n+\n+\n+@support_torch_compile\n+class LlamaModel(nn.Module):\n+\n+    def __init__(\n+        self,\n+        *,\n+        vllm_config: VllmConfig,\n+        prefix: str = \"\",\n+        start_layer_id: int = 0,\n+        quant_config: Optional[QuantizationConfig] = None,\n+    ) -> None:\n+        super().__init__()\n+        self.config = vllm_config. \\\n+            speculative_config.draft_model_config.hf_config\n+        self.validate_and_update_config(start_layer_id, quant_config)\n+        self.vocab_size = self.config.vocab_size\n+        self.embed_tokens = VocabParallelEmbedding(\n+            self.config.vocab_size,\n+            self.config.hidden_size,\n+            prefix=maybe_prefix(prefix, \"embed_tokens\"),\n+        )\n+\n+        self.layers = nn.ModuleList([\n+            Llama4DecoderLayer(\n+                self.config,\n+                quant_config=quant_config,\n+                prefix=maybe_prefix(prefix, f\"layers.{i + start_layer_id}\"),\n+            ) for i in range(self.config.num_hidden_layers)\n+        ])\n+        self.fc = torch.nn.Linear(self.config.hidden_size * 2,\n+                                  self.config.hidden_size,\n+                                  bias=False)\n+        self.norm = RMSNorm(self.config.hidden_size,\n+                            eps=self.config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor],\n+        positions: torch.Tensor,\n+        hidden_states: torch.Tensor,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_embeds = self.embed_tokens(input_ids)\n+        hidden_states = self.fc(\n+            torch.cat((input_embeds, hidden_states), dim=-1))\n+        residual = None\n+        for layer in self.layers:\n+            hidden_states, residual = layer(\n+                positions,\n+                hidden_states,\n+                residual,\n+            )\n+        hidden_states, _ = self.norm(hidden_states, residual)\n+        return hidden_states, hidden_states\n+\n+    def load_weights(self, weights: Iterable[tuple[str,\n+                                                   torch.Tensor]]) -> set[str]:\n+        stacked_params_mapping = [\n+            # (param_name, shard_name, shard_id)\n+            (\".qkv_proj\", \".q_proj\", \"q\"),\n+            (\".qkv_proj\", \".k_proj\", \"k\"),\n+            (\".qkv_proj\", \".v_proj\", \"v\"),\n+            (\".gate_up_proj\", \".gate_proj\", 0),\n+            (\".gate_up_proj\", \".up_proj\", 1),\n+        ]\n+        params_dict = dict(self.named_parameters())\n+        loaded_params: set[str] = set()\n+        for name, loaded_weight in weights:\n+            name = name.removeprefix(\"model.\")\n+            for param_name, weight_name, shard_id in stacked_params_mapping:\n+                if weight_name not in name:\n+                    continue\n+                name = name.replace(weight_name, param_name)\n+                param = params_dict[name]\n+                weight_loader = param.weight_loader\n+                weight_loader(param, loaded_weight, shard_id)\n+                break\n+            else:\n+                # if PP disabled then draft will share embed with target\n+                if get_pp_group().world_size == 1 and \\\n+                    \"embed_tokens.\" in name:\n+                    continue\n+                param = params_dict[name]\n+                weight_loader = getattr(param, \"weight_loader\",\n+                                        default_weight_loader)\n+                weight_loader(param, loaded_weight)\n+            loaded_params.add(name)\n+        for name in params_dict:\n+            # if PP disabled then draft will share embed with target\n+            if get_pp_group().world_size == 1 and \\\n+                \"embed_tokens.\" in name:\n+                continue\n+            assert name in loaded_params, f\"{name} is not loaded!\"\n+        return loaded_params\n+\n+    def validate_and_update_config(\n+            self,\n+            start_layer_id: int,\n+            quant_config: Optional[QuantizationConfig] = None) -> None:\n+        # yoco and moe is not supported by draft model yet\n+        assert self.config.yoco_global_kv_layer is None\n+        assert self.config.yoco_local_kv_layer is None\n+        assert len(self.config.moe_layers) == 0\n+        # draft model layer index is increased by start_layer_id,\n+        # so we need to pad relevant configs accordingly\n+        self.config.no_rope_layers = [\n+            0\n+        ] * start_layer_id + self.config.no_rope_layers\n+        # currently only TorchAO quantization is supported\n+        if isinstance(quant_config, TorchAOConfig):\n+\n+            def pad_layer_name(layer: str) -> str:\n+                layer_index = extract_layer_index(layer)\n+                return layer.replace(str(layer_index),\n+                                     str(layer_index + start_layer_id))\n+\n+            quant_config.torchao_config.module_fqn_to_config = {\n+                pad_layer_name(layer): quantization\n+                for layer, quantization in\n+                quant_config.torchao_config.module_fqn_to_config.items()\n+            }\n+\n+\n+class EagleLlama4ForCausalLM(Llama4ForCausalLM):\n+\n+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = \"\"):\n+        nn.Module.__init__(self)\n+        self.config = vllm_config. \\\n+            speculative_config.draft_model_config.hf_config\n+        target_layer_num = vllm_config.model_config.get_num_layers(\n+            vllm_config.parallel_config)\n+        # draft model quantization config may differ from target model\n+        quant_config = VllmConfig.get_quantization_config(\n+            vllm_config.speculative_config.draft_model_config,\n+            vllm_config.load_config)\n+        self.model = LlamaModel(vllm_config=vllm_config,\n+                                prefix=\"model\",\n+                                start_layer_id=target_layer_num,\n+                                quant_config=quant_config)\n+        logit_scale = getattr(self.config, \"logit_scale\", 1.0)\n+        self.logits_processor = LogitsProcessor(self.config.vocab_size,\n+                                                scale=logit_scale)\n+\n+    def forward(\n+        self,\n+        input_ids: torch.Tensor,\n+        positions: torch.Tensor,\n+        hidden_states: torch.Tensor,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        return self.model(input_ids, positions, hidden_states)\n+\n+    def load_weights(self, weights: Iterable[tuple[str, torch.Tensor]]):\n+        loader = AutoWeightsLoader(\n+            self,\n+            # lm_head is tied with target model (Llama4ForCausalLM)\n+            skip_prefixes=([\"lm_head.\"]),\n+        )\n+\n+        model_weights = {}\n+        weights = [\n+            self.permute_qk_weight_for_rotary(name, loaded_weight)\n+            for name, loaded_weight in weights\n+        ]\n+        for name, loaded_weight in weights:\n+            if \"lm_head\" not in name:\n+                name = \"model.\" + name\n+            model_weights[name] = loaded_weight\n+\n+        loader.load_weights(model_weights.items())",
        "comment_created_at": "2025-07-07T21:45:22+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThis block of code for processing weights is not memory-efficient. It first creates a list of all permuted weights, and then a dictionary of these weights, both of which can consume a large amount of memory for large models.\n\nA more memory-efficient approach is to use a generator to process the weights one by one. This avoids loading all weights into memory at once.\n\n```python\n        def _processed_weights():\n            for name, loaded_weight in weights:\n                name, loaded_weight = self.permute_qk_weight_for_rotary(\n                    name, loaded_weight)\n                if \"lm_head\" not in name:\n                    name = \"model.\" + name\n                yield name, loaded_weight\n\n        loader.load_weights(_processed_weights())\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2186084945",
    "pr_number": 20503,
    "pr_file": "vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py",
    "created_at": "2025-07-04T20:33:32+00:00",
    "commented_code": "current_token_ids: Sequence[int],\n         delta_token_ids: Sequence[int],\n         request: ChatCompletionRequest,\n-    ) -> Union[DeltaMessage, None]:\n+    ) -> DeltaMessage | None:\n \n-        # if the tool call token is not in the tokens generated so far, append\n-        # output to contents since it's not a tool\n+        # Early return if no tool call token present\n         if self.bot_token not in current_text:\n             return DeltaMessage(content=delta_text)\n \n-        # if the tool call token ID IS in the tokens generated so far, that\n-        # means we're parsing as tool calls now\n-\n-        # handle if we detected the BOT token which means the start of tool\n-        # calling\n-        if (self.bot_token_id in delta_token_ids\n-                and len(delta_token_ids) == 1):\n-            # if it's the only token, return None, so we don't send a chat\n-            # completion any don't send a control token\n-            return None\n-\n-        # bit mask flags for partial JSON parsing. If the name hasn't been\n-        # sent yet, don't allow sending\n-        # an incomplete string since OpenAI only ever (as far as I have\n-        # seen) allows sending the entire tool/ function name at once.\n-        flags = Allow.ALL if self.current_tool_name_sent \\\n-            else Allow.ALL & ~Allow.STR\n-        try:\n-\n-            # replace BOT token with empty string, and convert single quotes\n-            # to double to allow parsing as JSON since mistral uses single\n-            # quotes instead of double for tool calls\n-            parsable_arr = current_text.split(self.bot_token)[-1]\n+        # Process delta text and extract additional content\n+        additional_content = self._process_delta_text(delta_text)\n \n-            # tool calls are generated in an array, so do partial JSON\n-            # parsing on the entire array\n-            try:\n-                tool_call_arr: list[dict] = partial_json_parser.loads(\n-                    parsable_arr, flags)\n-            except partial_json_parser.core.exceptions.MalformedJSON:\n-                logger.debug('not enough tokens to parse into JSON yet')\n-                return None\n-\n-            # select as the current tool call the one we're on the state at\n-\n-            current_tool_call: dict = tool_call_arr[self.current_tool_id] \\\n-                if len(tool_call_arr) > 0 else {}\n-\n-            # case -- if no tokens have been streamed for the tool, e.g.\n-            #   only the array brackets, stream nothing\n-            if len(tool_call_arr) == 0:\n-                return None\n-\n-            # case: we are starting a new tool in the array\n-            #   -> array has > 0 length AND length has moved past cursor\n-            elif (len(tool_call_arr) > 0\n-                  and len(tool_call_arr) > self.current_tool_id + 1):\n-\n-                # if we're moving on to a new call, first make sure we\n-                # haven't missed anything in the previous one that was\n-                # auto-generated due to JSON completions, but wasn't\n-                # streamed to the client yet.\n-                if self.current_tool_id >= 0:\n-                    diff: Union[str, None] = current_tool_call.get(\"arguments\")\n-\n-                    if diff:\n-                        diff = json.dumps(diff, ensure_ascii=False).replace(\n-                            self.streamed_args_for_tool[self.current_tool_id],\n-                            \"\")\n-                        delta = DeltaMessage(tool_calls=[\n-                            DeltaToolCall(index=self.current_tool_id,\n-                                          function=DeltaFunctionCall(\n-                                              arguments=diff).model_dump(\n-                                                  exclude_none=True))\n-                        ])\n-                        self.streamed_args_for_tool[\n-                            self.current_tool_id] += diff\n-                    else:\n-                        delta = None\n-                else:\n-                    delta = None\n-                # re-set stuff pertaining to progress in the current tool\n-                self.current_tool_id = len(tool_call_arr) - 1\n-                self.current_tool_name_sent = False\n-                self.streamed_args_for_tool.append(\"\")\n-                logger.debug(\"starting on new tool %d\", self.current_tool_id)\n-                return delta\n+        # Detect and handle V11 format\n+        if self._should_detect_v11_format():\n+            self._detect_v11_format()\n \n-            # case: update an existing tool - this is handled below\n-\n-            # if the current tool name hasn't been sent, send if available\n-            # - otherwise send nothing\n-            if not self.current_tool_name_sent:\n-                function_name = current_tool_call.get(\"name\")\n-                if function_name:\n-\n-                    delta = DeltaMessage(tool_calls=[\n-                        DeltaToolCall(index=self.current_tool_id,\n-                                      type=\"function\",\n-                                      id=MistralToolCall.generate_random_id(),\n-                                      function=DeltaFunctionCall(\n-                                          name=function_name).model_dump(\n-                                              exclude_none=True))\n-                    ])\n-                    self.current_tool_name_sent = True\n-                else:\n-                    delta = None\n+        if self.v11_tool_format:\n+            return self._extract_tool_calls_streaming_v11(\n+                additional_content, delta_text)\n \n-            # now we know we're on the same tool call and we're streaming\n-            # arguments\n+        # Check if tool calls have started\n+        if self.current_tool_start_index < 0:\n+            bracket_pos = self.raw_tool_calls.find(\"[\")\n+            if bracket_pos >= 0:\n+                self.current_tool_start_index = bracket_pos + 1\n+                self.current_tool_id += 1\n             else:\n-\n-                prev_arguments = self.prev_tool_call_arr[\n-                    self.current_tool_id].get(\"arguments\")\n-                cur_arguments = current_tool_call.get(\"arguments\")\n-\n-                new_text = delta_text.replace(\"\\'\", \"\\\"\")\n-                if ('\"}' in new_text):\n-                    new_text = new_text[:new_text.rindex('\"}')]\n-\n-                if not cur_arguments and not prev_arguments:\n-\n-                    delta = None\n-                elif not cur_arguments and prev_arguments:\n-                    logger.error(\n-                        \"INVARIANT - impossible to have arguments reset \"\n-                        \"mid-arguments\")\n-                    delta = None\n-                elif cur_arguments and not prev_arguments:\n-                    cur_arguments_json = json.dumps(cur_arguments,\n-                                                    ensure_ascii=False)[:-2]\n-                    logger.debug(\"finding %s in %s\", new_text,\n-                                 cur_arguments_json)\n-\n-                    if (new_text not in cur_arguments_json):\n-                        return None\n-                    arguments_delta = cur_arguments_json[:cur_arguments_json.\n-                                                         rindex(new_text) +\n-                                                         len(new_text)]\n-                    logger.debug(\"First tokens in arguments received: %s\",\n-                                 arguments_delta)\n-                    delta = DeltaMessage(tool_calls=[\n-                        DeltaToolCall(index=self.current_tool_id,\n-                                      function=DeltaFunctionCall(\n-                                          arguments=arguments_delta).\n-                                      model_dump(exclude_none=True))\n-                    ])\n-                    self.streamed_args_for_tool[\n-                        self.current_tool_id] += arguments_delta\n-\n-                elif cur_arguments and prev_arguments:\n-                    cur_args_json = json.dumps(cur_arguments,\n-                                               ensure_ascii=False)\n-                    prev_args_json = json.dumps(prev_arguments,\n-                                                ensure_ascii=False)\n-                    logger.debug(\"Searching for diff between \n%s\n%s\",\n-                                 cur_args_json, prev_args_json)\n-\n-                    argument_diff = extract_intermediate_diff(\n-                        cur_args_json, prev_args_json)\n-                    logger.debug(\"got arguments diff: %s\", argument_diff)\n-                    delta = DeltaMessage(tool_calls=[\n-                        DeltaToolCall(index=self.current_tool_id,\n-                                      function=DeltaFunctionCall(\n-                                          arguments=argument_diff).model_dump(\n-                                              exclude_none=True))\n-                    ])\n-                    self.streamed_args_for_tool[\n-                        self.current_tool_id] += argument_diff\n-                else:\n-                    # try parsing it with regular JSON - if it works we're\n-                    # at the end, and we need to send the difference between\n-                    # tokens streamed so far and the valid JSON\n-                    delta = None\n-\n-            # check to see if the name is defined and has been sent. if so,\n-            # stream the name - otherwise keep waiting\n-            # finish by setting old and returning None as base case\n-            self.prev_tool_call_arr = tool_call_arr\n-            return delta\n-\n-        except Exception:\n-            logger.exception(\"Error trying to handle streaming tool call.\")\n-            logger.debug(\n-                \"Skipping chunk as a result of tool streaming extraction \"\n-                \"error\")\n-            return None\n+                return self._none_or_additional_content(additional_content)\n+\n+        # Try to parse complete JSON with caching\n+        parse_success, end_index = self._try_parse_json_cached(\n+            self.raw_tool_calls)\n+        if parse_success:\n+            self.tools_parsing_finished = True\n+            if len(self.raw_tool_calls) > end_index:\n+                additional_content = self.raw_tool_calls[end_index:]\n+\n+        # Handle tool completion and transition to next tool\n+        if self._is_current_tool_complete():\n+            if self.tools_parsing_finished:\n+                return self._none_or_additional_content(additional_content)\n+\n+            if self._advance_to_next_tool():\n+                # Successfully moved to next tool, continue processing\n+                pass\n+            else:\n+                # No next tool ready yet\n+                return self._none_or_additional_content(additional_content)\n+\n+        if self.current_tool_start_index >= len(self.raw_tool_calls):\n+            # tool call has not started\n+            return self._none_or_additional_content(additional_content)\n+        raw_current_tool_call = self.raw_tool_calls[self.\n+                                                    current_tool_start_index:]",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2186084945",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20503,
        "pr_file": "vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py",
        "discussion_id": "2186084945",
        "commented_code": "@@ -183,187 +569,120 @@ def extract_tool_calls_streaming(\n         current_token_ids: Sequence[int],\n         delta_token_ids: Sequence[int],\n         request: ChatCompletionRequest,\n-    ) -> Union[DeltaMessage, None]:\n+    ) -> DeltaMessage | None:\n \n-        # if the tool call token is not in the tokens generated so far, append\n-        # output to contents since it's not a tool\n+        # Early return if no tool call token present\n         if self.bot_token not in current_text:\n             return DeltaMessage(content=delta_text)\n \n-        # if the tool call token ID IS in the tokens generated so far, that\n-        # means we're parsing as tool calls now\n-\n-        # handle if we detected the BOT token which means the start of tool\n-        # calling\n-        if (self.bot_token_id in delta_token_ids\n-                and len(delta_token_ids) == 1):\n-            # if it's the only token, return None, so we don't send a chat\n-            # completion any don't send a control token\n-            return None\n-\n-        # bit mask flags for partial JSON parsing. If the name hasn't been\n-        # sent yet, don't allow sending\n-        # an incomplete string since OpenAI only ever (as far as I have\n-        # seen) allows sending the entire tool/ function name at once.\n-        flags = Allow.ALL if self.current_tool_name_sent \\\n-            else Allow.ALL & ~Allow.STR\n-        try:\n-\n-            # replace BOT token with empty string, and convert single quotes\n-            # to double to allow parsing as JSON since mistral uses single\n-            # quotes instead of double for tool calls\n-            parsable_arr = current_text.split(self.bot_token)[-1]\n+        # Process delta text and extract additional content\n+        additional_content = self._process_delta_text(delta_text)\n \n-            # tool calls are generated in an array, so do partial JSON\n-            # parsing on the entire array\n-            try:\n-                tool_call_arr: list[dict] = partial_json_parser.loads(\n-                    parsable_arr, flags)\n-            except partial_json_parser.core.exceptions.MalformedJSON:\n-                logger.debug('not enough tokens to parse into JSON yet')\n-                return None\n-\n-            # select as the current tool call the one we're on the state at\n-\n-            current_tool_call: dict = tool_call_arr[self.current_tool_id] \\\n-                if len(tool_call_arr) > 0 else {}\n-\n-            # case -- if no tokens have been streamed for the tool, e.g.\n-            #   only the array brackets, stream nothing\n-            if len(tool_call_arr) == 0:\n-                return None\n-\n-            # case: we are starting a new tool in the array\n-            #   -> array has > 0 length AND length has moved past cursor\n-            elif (len(tool_call_arr) > 0\n-                  and len(tool_call_arr) > self.current_tool_id + 1):\n-\n-                # if we're moving on to a new call, first make sure we\n-                # haven't missed anything in the previous one that was\n-                # auto-generated due to JSON completions, but wasn't\n-                # streamed to the client yet.\n-                if self.current_tool_id >= 0:\n-                    diff: Union[str, None] = current_tool_call.get(\"arguments\")\n-\n-                    if diff:\n-                        diff = json.dumps(diff, ensure_ascii=False).replace(\n-                            self.streamed_args_for_tool[self.current_tool_id],\n-                            \"\")\n-                        delta = DeltaMessage(tool_calls=[\n-                            DeltaToolCall(index=self.current_tool_id,\n-                                          function=DeltaFunctionCall(\n-                                              arguments=diff).model_dump(\n-                                                  exclude_none=True))\n-                        ])\n-                        self.streamed_args_for_tool[\n-                            self.current_tool_id] += diff\n-                    else:\n-                        delta = None\n-                else:\n-                    delta = None\n-                # re-set stuff pertaining to progress in the current tool\n-                self.current_tool_id = len(tool_call_arr) - 1\n-                self.current_tool_name_sent = False\n-                self.streamed_args_for_tool.append(\"\")\n-                logger.debug(\"starting on new tool %d\", self.current_tool_id)\n-                return delta\n+        # Detect and handle V11 format\n+        if self._should_detect_v11_format():\n+            self._detect_v11_format()\n \n-            # case: update an existing tool - this is handled below\n-\n-            # if the current tool name hasn't been sent, send if available\n-            # - otherwise send nothing\n-            if not self.current_tool_name_sent:\n-                function_name = current_tool_call.get(\"name\")\n-                if function_name:\n-\n-                    delta = DeltaMessage(tool_calls=[\n-                        DeltaToolCall(index=self.current_tool_id,\n-                                      type=\"function\",\n-                                      id=MistralToolCall.generate_random_id(),\n-                                      function=DeltaFunctionCall(\n-                                          name=function_name).model_dump(\n-                                              exclude_none=True))\n-                    ])\n-                    self.current_tool_name_sent = True\n-                else:\n-                    delta = None\n+        if self.v11_tool_format:\n+            return self._extract_tool_calls_streaming_v11(\n+                additional_content, delta_text)\n \n-            # now we know we're on the same tool call and we're streaming\n-            # arguments\n+        # Check if tool calls have started\n+        if self.current_tool_start_index < 0:\n+            bracket_pos = self.raw_tool_calls.find(\"[\")\n+            if bracket_pos >= 0:\n+                self.current_tool_start_index = bracket_pos + 1\n+                self.current_tool_id += 1\n             else:\n-\n-                prev_arguments = self.prev_tool_call_arr[\n-                    self.current_tool_id].get(\"arguments\")\n-                cur_arguments = current_tool_call.get(\"arguments\")\n-\n-                new_text = delta_text.replace(\"\\'\", \"\\\"\")\n-                if ('\"}' in new_text):\n-                    new_text = new_text[:new_text.rindex('\"}')]\n-\n-                if not cur_arguments and not prev_arguments:\n-\n-                    delta = None\n-                elif not cur_arguments and prev_arguments:\n-                    logger.error(\n-                        \"INVARIANT - impossible to have arguments reset \"\n-                        \"mid-arguments\")\n-                    delta = None\n-                elif cur_arguments and not prev_arguments:\n-                    cur_arguments_json = json.dumps(cur_arguments,\n-                                                    ensure_ascii=False)[:-2]\n-                    logger.debug(\"finding %s in %s\", new_text,\n-                                 cur_arguments_json)\n-\n-                    if (new_text not in cur_arguments_json):\n-                        return None\n-                    arguments_delta = cur_arguments_json[:cur_arguments_json.\n-                                                         rindex(new_text) +\n-                                                         len(new_text)]\n-                    logger.debug(\"First tokens in arguments received: %s\",\n-                                 arguments_delta)\n-                    delta = DeltaMessage(tool_calls=[\n-                        DeltaToolCall(index=self.current_tool_id,\n-                                      function=DeltaFunctionCall(\n-                                          arguments=arguments_delta).\n-                                      model_dump(exclude_none=True))\n-                    ])\n-                    self.streamed_args_for_tool[\n-                        self.current_tool_id] += arguments_delta\n-\n-                elif cur_arguments and prev_arguments:\n-                    cur_args_json = json.dumps(cur_arguments,\n-                                               ensure_ascii=False)\n-                    prev_args_json = json.dumps(prev_arguments,\n-                                                ensure_ascii=False)\n-                    logger.debug(\"Searching for diff between \\n%s\\n%s\",\n-                                 cur_args_json, prev_args_json)\n-\n-                    argument_diff = extract_intermediate_diff(\n-                        cur_args_json, prev_args_json)\n-                    logger.debug(\"got arguments diff: %s\", argument_diff)\n-                    delta = DeltaMessage(tool_calls=[\n-                        DeltaToolCall(index=self.current_tool_id,\n-                                      function=DeltaFunctionCall(\n-                                          arguments=argument_diff).model_dump(\n-                                              exclude_none=True))\n-                    ])\n-                    self.streamed_args_for_tool[\n-                        self.current_tool_id] += argument_diff\n-                else:\n-                    # try parsing it with regular JSON - if it works we're\n-                    # at the end, and we need to send the difference between\n-                    # tokens streamed so far and the valid JSON\n-                    delta = None\n-\n-            # check to see if the name is defined and has been sent. if so,\n-            # stream the name - otherwise keep waiting\n-            # finish by setting old and returning None as base case\n-            self.prev_tool_call_arr = tool_call_arr\n-            return delta\n-\n-        except Exception:\n-            logger.exception(\"Error trying to handle streaming tool call.\")\n-            logger.debug(\n-                \"Skipping chunk as a result of tool streaming extraction \"\n-                \"error\")\n-            return None\n+                return self._none_or_additional_content(additional_content)\n+\n+        # Try to parse complete JSON with caching\n+        parse_success, end_index = self._try_parse_json_cached(\n+            self.raw_tool_calls)\n+        if parse_success:\n+            self.tools_parsing_finished = True\n+            if len(self.raw_tool_calls) > end_index:\n+                additional_content = self.raw_tool_calls[end_index:]\n+\n+        # Handle tool completion and transition to next tool\n+        if self._is_current_tool_complete():\n+            if self.tools_parsing_finished:\n+                return self._none_or_additional_content(additional_content)\n+\n+            if self._advance_to_next_tool():\n+                # Successfully moved to next tool, continue processing\n+                pass\n+            else:\n+                # No next tool ready yet\n+                return self._none_or_additional_content(additional_content)\n+\n+        if self.current_tool_start_index >= len(self.raw_tool_calls):\n+            # tool call has not started\n+            return self._none_or_additional_content(additional_content)\n+        raw_current_tool_call = self.raw_tool_calls[self.\n+                                                    current_tool_start_index:]",
        "comment_created_at": "2025-07-04T20:33:32+00:00",
        "comment_author": "gemini-code-assist[bot]",
        "comment_body": "![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)\n\nThe repeated creation of substrings from `self.raw_tool_calls` can be inefficient for long tool call strings. Refactor the parsing methods to directly use `self.raw_tool_calls` with an offset index to avoid unnecessary memory allocations and copies. Consider using the `pos` argument in the `re` module functions to start matching from a specific index.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2172940414",
    "pr_number": 19334,
    "pr_file": "vllm/utils.py",
    "created_at": "2025-06-27T22:27:32+00:00",
    "commented_code": "return str(uuid.uuid4().hex)\n \n \n+class AsyncMicrobatchTokenizer:\n+    \"\"\"Asynchronous tokenizer with micro-batching.\n+\n+    A single background task pulls pending encode / decode requests from\n+    an ``asyncio.Queue`` and groups them into micro-batches.  The actual\n+    blocking tokenizer call is executed inside a ``ThreadPoolExecutor`` \n+    with a single worker, so the event-loop never blocks.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        tokenizer,\n+        max_batch_size: int = 32,\n+        batch_wait_timeout_s: float = 0.002,\n+    ) -> None:\n+        self.tokenizer = tokenizer\n+        self.max_batch_size = max_batch_size\n+        self.batch_wait_timeout_s = batch_wait_timeout_s\n+\n+        self._loop = asyncio.get_running_loop()\n+        self._queues: dict[tuple, asyncio.Queue[tuple[str, object, dict,\n+                                                      asyncio.Future]]] = {}\n+        self._batcher_tasks: dict[tuple, asyncio.Task] = {}\n+\n+        # Single worker that owns the blocking tokenizer.\n+        self._executor = ThreadPoolExecutor(max_workers=1)\n+\n+    # ------------------------------------------------------------------\n+    # Public async API\n+    # ------------------------------------------------------------------\n+    async def __call__(self, prompt, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"encode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"encode\", prompt, kwargs, fut))\n+        return await fut\n+\n+    async def decode(self, token_ids, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"decode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"decode\", token_ids, kwargs, fut))\n+        return await fut\n+\n+    # ------------------------------------------------------------------\n+    # Internal helpers\n+    # ------------------------------------------------------------------\n+    def _ensure_queue_and_batcher(self, loop: asyncio.AbstractEventLoop,\n+                                  key: tuple):\n+        \"\"\"Return the queue for key, creating queue \n+        and batcher task if needed.\"\"\"\n+        if key not in self._queues:\n+            self._queues[key] = asyncio.Queue()\n+\n+        if key not in self._batcher_tasks or self._batcher_tasks[key].done():\n+            self._batcher_tasks[key] = loop.create_task(\n+                self._batch_loop(self._queues[key]))\n+\n+        return self._queues[key]\n+\n+    async def _batch_loop(self, queue: asyncio.Queue):\n+        \"\"\"Runs until the queue stays empty long enough to finish.\"\"\"\n+        while True:\n+            first_item = await queue.get()\n+            batch = [first_item]\n+            deadline = self._loop.time() + self.batch_wait_timeout_s\n+\n+            while len(batch) < self.max_batch_size:\n+                timeout = deadline - self._loop.time()\n+                if timeout <= 0:\n+                    break\n+                try:\n+                    item = await asyncio.wait_for(queue.get(), timeout)\n+                    batch.append(item)\n+                except asyncio.TimeoutError:\n+                    break\n+\n+            # Split by op type\n+            encode_items = [it for it in batch if it[0] == \"encode\"]\n+            decode_items = [it for it in batch if it[0] == \"decode\"]",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2172940414",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19334,
        "pr_file": "vllm/utils.py",
        "discussion_id": "2172940414",
        "commented_code": "@@ -510,6 +512,187 @@ def random_uuid() -> str:\n     return str(uuid.uuid4().hex)\n \n \n+class AsyncMicrobatchTokenizer:\n+    \"\"\"Asynchronous tokenizer with micro-batching.\n+\n+    A single background task pulls pending encode / decode requests from\n+    an ``asyncio.Queue`` and groups them into micro-batches.  The actual\n+    blocking tokenizer call is executed inside a ``ThreadPoolExecutor`` \n+    with a single worker, so the event-loop never blocks.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        tokenizer,\n+        max_batch_size: int = 32,\n+        batch_wait_timeout_s: float = 0.002,\n+    ) -> None:\n+        self.tokenizer = tokenizer\n+        self.max_batch_size = max_batch_size\n+        self.batch_wait_timeout_s = batch_wait_timeout_s\n+\n+        self._loop = asyncio.get_running_loop()\n+        self._queues: dict[tuple, asyncio.Queue[tuple[str, object, dict,\n+                                                      asyncio.Future]]] = {}\n+        self._batcher_tasks: dict[tuple, asyncio.Task] = {}\n+\n+        # Single worker that owns the blocking tokenizer.\n+        self._executor = ThreadPoolExecutor(max_workers=1)\n+\n+    # ------------------------------------------------------------------\n+    # Public async API\n+    # ------------------------------------------------------------------\n+    async def __call__(self, prompt, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"encode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"encode\", prompt, kwargs, fut))\n+        return await fut\n+\n+    async def decode(self, token_ids, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"decode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"decode\", token_ids, kwargs, fut))\n+        return await fut\n+\n+    # ------------------------------------------------------------------\n+    # Internal helpers\n+    # ------------------------------------------------------------------\n+    def _ensure_queue_and_batcher(self, loop: asyncio.AbstractEventLoop,\n+                                  key: tuple):\n+        \"\"\"Return the queue for key, creating queue \n+        and batcher task if needed.\"\"\"\n+        if key not in self._queues:\n+            self._queues[key] = asyncio.Queue()\n+\n+        if key not in self._batcher_tasks or self._batcher_tasks[key].done():\n+            self._batcher_tasks[key] = loop.create_task(\n+                self._batch_loop(self._queues[key]))\n+\n+        return self._queues[key]\n+\n+    async def _batch_loop(self, queue: asyncio.Queue):\n+        \"\"\"Runs until the queue stays empty long enough to finish.\"\"\"\n+        while True:\n+            first_item = await queue.get()\n+            batch = [first_item]\n+            deadline = self._loop.time() + self.batch_wait_timeout_s\n+\n+            while len(batch) < self.max_batch_size:\n+                timeout = deadline - self._loop.time()\n+                if timeout <= 0:\n+                    break\n+                try:\n+                    item = await asyncio.wait_for(queue.get(), timeout)\n+                    batch.append(item)\n+                except asyncio.TimeoutError:\n+                    break\n+\n+            # Split by op type\n+            encode_items = [it for it in batch if it[0] == \"encode\"]\n+            decode_items = [it for it in batch if it[0] == \"decode\"]",
        "comment_created_at": "2025-06-27T22:27:32+00:00",
        "comment_author": "njhill",
        "comment_body": "can we just have a bool arg to the method to indicate this. The whole batch will be the same type now right?\r\n\r\nActually maybe a separate method for encode / decode cases? Then the prompt and token_id lists can also be populated while reading from the queue, to avoid iterating again below.",
        "pr_file_module": null
      },
      {
        "comment_id": "2174124851",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19334,
        "pr_file": "vllm/utils.py",
        "discussion_id": "2172940414",
        "commented_code": "@@ -510,6 +512,187 @@ def random_uuid() -> str:\n     return str(uuid.uuid4().hex)\n \n \n+class AsyncMicrobatchTokenizer:\n+    \"\"\"Asynchronous tokenizer with micro-batching.\n+\n+    A single background task pulls pending encode / decode requests from\n+    an ``asyncio.Queue`` and groups them into micro-batches.  The actual\n+    blocking tokenizer call is executed inside a ``ThreadPoolExecutor`` \n+    with a single worker, so the event-loop never blocks.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        tokenizer,\n+        max_batch_size: int = 32,\n+        batch_wait_timeout_s: float = 0.002,\n+    ) -> None:\n+        self.tokenizer = tokenizer\n+        self.max_batch_size = max_batch_size\n+        self.batch_wait_timeout_s = batch_wait_timeout_s\n+\n+        self._loop = asyncio.get_running_loop()\n+        self._queues: dict[tuple, asyncio.Queue[tuple[str, object, dict,\n+                                                      asyncio.Future]]] = {}\n+        self._batcher_tasks: dict[tuple, asyncio.Task] = {}\n+\n+        # Single worker that owns the blocking tokenizer.\n+        self._executor = ThreadPoolExecutor(max_workers=1)\n+\n+    # ------------------------------------------------------------------\n+    # Public async API\n+    # ------------------------------------------------------------------\n+    async def __call__(self, prompt, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"encode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"encode\", prompt, kwargs, fut))\n+        return await fut\n+\n+    async def decode(self, token_ids, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"decode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"decode\", token_ids, kwargs, fut))\n+        return await fut\n+\n+    # ------------------------------------------------------------------\n+    # Internal helpers\n+    # ------------------------------------------------------------------\n+    def _ensure_queue_and_batcher(self, loop: asyncio.AbstractEventLoop,\n+                                  key: tuple):\n+        \"\"\"Return the queue for key, creating queue \n+        and batcher task if needed.\"\"\"\n+        if key not in self._queues:\n+            self._queues[key] = asyncio.Queue()\n+\n+        if key not in self._batcher_tasks or self._batcher_tasks[key].done():\n+            self._batcher_tasks[key] = loop.create_task(\n+                self._batch_loop(self._queues[key]))\n+\n+        return self._queues[key]\n+\n+    async def _batch_loop(self, queue: asyncio.Queue):\n+        \"\"\"Runs until the queue stays empty long enough to finish.\"\"\"\n+        while True:\n+            first_item = await queue.get()\n+            batch = [first_item]\n+            deadline = self._loop.time() + self.batch_wait_timeout_s\n+\n+            while len(batch) < self.max_batch_size:\n+                timeout = deadline - self._loop.time()\n+                if timeout <= 0:\n+                    break\n+                try:\n+                    item = await asyncio.wait_for(queue.get(), timeout)\n+                    batch.append(item)\n+                except asyncio.TimeoutError:\n+                    break\n+\n+            # Split by op type\n+            encode_items = [it for it in batch if it[0] == \"encode\"]\n+            decode_items = [it for it in batch if it[0] == \"decode\"]",
        "comment_created_at": "2025-06-30T03:05:14+00:00",
        "comment_author": "ztang2370",
        "comment_body": "updated",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2172953908",
    "pr_number": 19334,
    "pr_file": "vllm/utils.py",
    "created_at": "2025-06-27T22:47:50+00:00",
    "commented_code": "return str(uuid.uuid4().hex)\n \n \n+class AsyncMicrobatchTokenizer:\n+    \"\"\"Asynchronous tokenizer with micro-batching.\n+\n+    A single background task pulls pending encode / decode requests from\n+    an ``asyncio.Queue`` and groups them into micro-batches.  The actual\n+    blocking tokenizer call is executed inside a ``ThreadPoolExecutor`` \n+    with a single worker, so the event-loop never blocks.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        tokenizer,\n+        max_batch_size: int = 32,\n+        batch_wait_timeout_s: float = 0.002,\n+    ) -> None:\n+        self.tokenizer = tokenizer\n+        self.max_batch_size = max_batch_size\n+        self.batch_wait_timeout_s = batch_wait_timeout_s\n+\n+        self._loop = asyncio.get_running_loop()\n+        self._queues: dict[tuple, asyncio.Queue[tuple[str, object, dict,\n+                                                      asyncio.Future]]] = {}\n+        self._batcher_tasks: dict[tuple, asyncio.Task] = {}\n+\n+        # Single worker that owns the blocking tokenizer.\n+        self._executor = ThreadPoolExecutor(max_workers=1)\n+\n+    # ------------------------------------------------------------------\n+    # Public async API\n+    # ------------------------------------------------------------------\n+    async def __call__(self, prompt, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"encode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"encode\", prompt, kwargs, fut))\n+        return await fut\n+\n+    async def decode(self, token_ids, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"decode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"decode\", token_ids, kwargs, fut))\n+        return await fut\n+\n+    # ------------------------------------------------------------------\n+    # Internal helpers\n+    # ------------------------------------------------------------------\n+    def _ensure_queue_and_batcher(self, loop: asyncio.AbstractEventLoop,\n+                                  key: tuple):\n+        \"\"\"Return the queue for key, creating queue \n+        and batcher task if needed.\"\"\"\n+        if key not in self._queues:\n+            self._queues[key] = asyncio.Queue()\n+\n+        if key not in self._batcher_tasks or self._batcher_tasks[key].done():\n+            self._batcher_tasks[key] = loop.create_task(\n+                self._batch_loop(self._queues[key]))\n+\n+        return self._queues[key]\n+\n+    async def _batch_loop(self, queue: asyncio.Queue):\n+        \"\"\"Runs until the queue stays empty long enough to finish.\"\"\"\n+        while True:\n+            first_item = await queue.get()\n+            batch = [first_item]\n+            deadline = self._loop.time() + self.batch_wait_timeout_s\n+\n+            while len(batch) < self.max_batch_size:\n+                timeout = deadline - self._loop.time()\n+                if timeout <= 0:\n+                    break\n+                try:\n+                    item = await asyncio.wait_for(queue.get(), timeout)\n+                    batch.append(item)\n+                except asyncio.TimeoutError:\n+                    break\n+\n+            # Split by op type\n+            encode_items = [it for it in batch if it[0] == \"encode\"]\n+            decode_items = [it for it in batch if it[0] == \"decode\"]\n+\n+            # ---------------- Encode ----------------\n+            if encode_items:\n+                prompts = [it[1] for it in encode_items]\n+                kwargs_list = [it[2] for it in encode_items]\n+\n+                # If every request uses identical kwargs we can run a single\n+                # batched tokenizer call for a big speed-up.\n+                can_batch = all(kw == kwargs_list[0] for kw in kwargs_list)\n+\n+                if can_batch and len(prompts) > 1:\n+\n+                    def encode_fn(prompts=prompts, kwargs_list=kwargs_list):\n+                        \"\"\"Tokenize the micro-batch in one call \n+                        and split results.\"\"\"\n+                        grouped = self.tokenizer(prompts, **kwargs_list[0])\n+                        single_encodings = []\n+                        for i in range(len(prompts)):\n+                            data_i = {k: v[i] for k, v in grouped.items()}\n+                            single_encodings.append(BatchEncoding(data_i))\n+                        return single_encodings",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2172953908",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19334,
        "pr_file": "vllm/utils.py",
        "discussion_id": "2172953908",
        "commented_code": "@@ -510,6 +512,187 @@ def random_uuid() -> str:\n     return str(uuid.uuid4().hex)\n \n \n+class AsyncMicrobatchTokenizer:\n+    \"\"\"Asynchronous tokenizer with micro-batching.\n+\n+    A single background task pulls pending encode / decode requests from\n+    an ``asyncio.Queue`` and groups them into micro-batches.  The actual\n+    blocking tokenizer call is executed inside a ``ThreadPoolExecutor`` \n+    with a single worker, so the event-loop never blocks.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        tokenizer,\n+        max_batch_size: int = 32,\n+        batch_wait_timeout_s: float = 0.002,\n+    ) -> None:\n+        self.tokenizer = tokenizer\n+        self.max_batch_size = max_batch_size\n+        self.batch_wait_timeout_s = batch_wait_timeout_s\n+\n+        self._loop = asyncio.get_running_loop()\n+        self._queues: dict[tuple, asyncio.Queue[tuple[str, object, dict,\n+                                                      asyncio.Future]]] = {}\n+        self._batcher_tasks: dict[tuple, asyncio.Task] = {}\n+\n+        # Single worker that owns the blocking tokenizer.\n+        self._executor = ThreadPoolExecutor(max_workers=1)\n+\n+    # ------------------------------------------------------------------\n+    # Public async API\n+    # ------------------------------------------------------------------\n+    async def __call__(self, prompt, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"encode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"encode\", prompt, kwargs, fut))\n+        return await fut\n+\n+    async def decode(self, token_ids, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"decode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"decode\", token_ids, kwargs, fut))\n+        return await fut\n+\n+    # ------------------------------------------------------------------\n+    # Internal helpers\n+    # ------------------------------------------------------------------\n+    def _ensure_queue_and_batcher(self, loop: asyncio.AbstractEventLoop,\n+                                  key: tuple):\n+        \"\"\"Return the queue for key, creating queue \n+        and batcher task if needed.\"\"\"\n+        if key not in self._queues:\n+            self._queues[key] = asyncio.Queue()\n+\n+        if key not in self._batcher_tasks or self._batcher_tasks[key].done():\n+            self._batcher_tasks[key] = loop.create_task(\n+                self._batch_loop(self._queues[key]))\n+\n+        return self._queues[key]\n+\n+    async def _batch_loop(self, queue: asyncio.Queue):\n+        \"\"\"Runs until the queue stays empty long enough to finish.\"\"\"\n+        while True:\n+            first_item = await queue.get()\n+            batch = [first_item]\n+            deadline = self._loop.time() + self.batch_wait_timeout_s\n+\n+            while len(batch) < self.max_batch_size:\n+                timeout = deadline - self._loop.time()\n+                if timeout <= 0:\n+                    break\n+                try:\n+                    item = await asyncio.wait_for(queue.get(), timeout)\n+                    batch.append(item)\n+                except asyncio.TimeoutError:\n+                    break\n+\n+            # Split by op type\n+            encode_items = [it for it in batch if it[0] == \"encode\"]\n+            decode_items = [it for it in batch if it[0] == \"decode\"]\n+\n+            # ---------------- Encode ----------------\n+            if encode_items:\n+                prompts = [it[1] for it in encode_items]\n+                kwargs_list = [it[2] for it in encode_items]\n+\n+                # If every request uses identical kwargs we can run a single\n+                # batched tokenizer call for a big speed-up.\n+                can_batch = all(kw == kwargs_list[0] for kw in kwargs_list)\n+\n+                if can_batch and len(prompts) > 1:\n+\n+                    def encode_fn(prompts=prompts, kwargs_list=kwargs_list):\n+                        \"\"\"Tokenize the micro-batch in one call \n+                        and split results.\"\"\"\n+                        grouped = self.tokenizer(prompts, **kwargs_list[0])\n+                        single_encodings = []\n+                        for i in range(len(prompts)):\n+                            data_i = {k: v[i] for k, v in grouped.items()}\n+                            single_encodings.append(BatchEncoding(data_i))\n+                        return single_encodings",
        "comment_created_at": "2025-06-27T22:47:50+00:00",
        "comment_author": "njhill",
        "comment_body": "It would be better to just return results as-is and have a single loop outside rather than having to loop over the prompts again to set the future results.",
        "pr_file_module": null
      },
      {
        "comment_id": "2174125302",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19334,
        "pr_file": "vllm/utils.py",
        "discussion_id": "2172953908",
        "commented_code": "@@ -510,6 +512,187 @@ def random_uuid() -> str:\n     return str(uuid.uuid4().hex)\n \n \n+class AsyncMicrobatchTokenizer:\n+    \"\"\"Asynchronous tokenizer with micro-batching.\n+\n+    A single background task pulls pending encode / decode requests from\n+    an ``asyncio.Queue`` and groups them into micro-batches.  The actual\n+    blocking tokenizer call is executed inside a ``ThreadPoolExecutor`` \n+    with a single worker, so the event-loop never blocks.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        tokenizer,\n+        max_batch_size: int = 32,\n+        batch_wait_timeout_s: float = 0.002,\n+    ) -> None:\n+        self.tokenizer = tokenizer\n+        self.max_batch_size = max_batch_size\n+        self.batch_wait_timeout_s = batch_wait_timeout_s\n+\n+        self._loop = asyncio.get_running_loop()\n+        self._queues: dict[tuple, asyncio.Queue[tuple[str, object, dict,\n+                                                      asyncio.Future]]] = {}\n+        self._batcher_tasks: dict[tuple, asyncio.Task] = {}\n+\n+        # Single worker that owns the blocking tokenizer.\n+        self._executor = ThreadPoolExecutor(max_workers=1)\n+\n+    # ------------------------------------------------------------------\n+    # Public async API\n+    # ------------------------------------------------------------------\n+    async def __call__(self, prompt, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"encode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"encode\", prompt, kwargs, fut))\n+        return await fut\n+\n+    async def decode(self, token_ids, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"decode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"decode\", token_ids, kwargs, fut))\n+        return await fut\n+\n+    # ------------------------------------------------------------------\n+    # Internal helpers\n+    # ------------------------------------------------------------------\n+    def _ensure_queue_and_batcher(self, loop: asyncio.AbstractEventLoop,\n+                                  key: tuple):\n+        \"\"\"Return the queue for key, creating queue \n+        and batcher task if needed.\"\"\"\n+        if key not in self._queues:\n+            self._queues[key] = asyncio.Queue()\n+\n+        if key not in self._batcher_tasks or self._batcher_tasks[key].done():\n+            self._batcher_tasks[key] = loop.create_task(\n+                self._batch_loop(self._queues[key]))\n+\n+        return self._queues[key]\n+\n+    async def _batch_loop(self, queue: asyncio.Queue):\n+        \"\"\"Runs until the queue stays empty long enough to finish.\"\"\"\n+        while True:\n+            first_item = await queue.get()\n+            batch = [first_item]\n+            deadline = self._loop.time() + self.batch_wait_timeout_s\n+\n+            while len(batch) < self.max_batch_size:\n+                timeout = deadline - self._loop.time()\n+                if timeout <= 0:\n+                    break\n+                try:\n+                    item = await asyncio.wait_for(queue.get(), timeout)\n+                    batch.append(item)\n+                except asyncio.TimeoutError:\n+                    break\n+\n+            # Split by op type\n+            encode_items = [it for it in batch if it[0] == \"encode\"]\n+            decode_items = [it for it in batch if it[0] == \"decode\"]\n+\n+            # ---------------- Encode ----------------\n+            if encode_items:\n+                prompts = [it[1] for it in encode_items]\n+                kwargs_list = [it[2] for it in encode_items]\n+\n+                # If every request uses identical kwargs we can run a single\n+                # batched tokenizer call for a big speed-up.\n+                can_batch = all(kw == kwargs_list[0] for kw in kwargs_list)\n+\n+                if can_batch and len(prompts) > 1:\n+\n+                    def encode_fn(prompts=prompts, kwargs_list=kwargs_list):\n+                        \"\"\"Tokenize the micro-batch in one call \n+                        and split results.\"\"\"\n+                        grouped = self.tokenizer(prompts, **kwargs_list[0])\n+                        single_encodings = []\n+                        for i in range(len(prompts)):\n+                            data_i = {k: v[i] for k, v in grouped.items()}\n+                            single_encodings.append(BatchEncoding(data_i))\n+                        return single_encodings",
        "comment_created_at": "2025-06-30T03:05:54+00:00",
        "comment_author": "ztang2370",
        "comment_body": "updated",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189746416",
    "pr_number": 20413,
    "pr_file": "vllm/transformers_utils/detokenizer_utils.py",
    "created_at": "2025-07-07T11:12:43+00:00",
    "commented_code": "# NOTE(woosuk): The following code is slow because it runs a for loop over\n     # the output_tokens. In Python, running a for loop over a list can be slow\n     # even when the loop body is very simple.\n+    # Performance improvements: avoid repeated attribute and function lookups;\n+    # localize frequently used objects;\n+\n     sub_texts: list[str] = []\n     current_sub_text: list[str] = []\n-    all_special_tokens = set(tokenizer.all_special_tokens)\n+    convert_tokens_to_string = tokenizer.convert_tokens_to_string\n+    added_vocab_set = set(tokenizer.get_added_vocab())\n+    all_special_tokens = set(tokenizer.all_special_tokens) if skip_special_tokens else ()\n+\n     for token in output_tokens:\n+        # Use precomputed set for skip-special check\n         if skip_special_tokens and token in all_special_tokens:",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2189746416",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20413,
        "pr_file": "vllm/transformers_utils/detokenizer_utils.py",
        "discussion_id": "2189746416",
        "commented_code": "@@ -23,27 +23,31 @@ def _convert_tokens_to_string_with_added_encoders(\n     # NOTE(woosuk): The following code is slow because it runs a for loop over\n     # the output_tokens. In Python, running a for loop over a list can be slow\n     # even when the loop body is very simple.\n+    # Performance improvements: avoid repeated attribute and function lookups;\n+    # localize frequently used objects;\n+\n     sub_texts: list[str] = []\n     current_sub_text: list[str] = []\n-    all_special_tokens = set(tokenizer.all_special_tokens)\n+    convert_tokens_to_string = tokenizer.convert_tokens_to_string\n+    added_vocab_set = set(tokenizer.get_added_vocab())\n+    all_special_tokens = set(tokenizer.all_special_tokens) if skip_special_tokens else ()\n+\n     for token in output_tokens:\n+        # Use precomputed set for skip-special check\n         if skip_special_tokens and token in all_special_tokens:",
        "comment_created_at": "2025-07-07T11:12:43+00:00",
        "comment_author": "njhill",
        "comment_body": "could be simplified now\r\n```suggestion\r\n        if token in all_special_tokens:\r\n```",
        "pr_file_module": null
      }
    ]
  }
]