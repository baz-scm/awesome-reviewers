[
  {
    "discussion_id": "2172940414",
    "pr_number": 19334,
    "pr_file": "vllm/utils.py",
    "created_at": "2025-06-27T22:27:32+00:00",
    "commented_code": "return str(uuid.uuid4().hex)\n \n \n+class AsyncMicrobatchTokenizer:\n+    \"\"\"Asynchronous tokenizer with micro-batching.\n+\n+    A single background task pulls pending encode / decode requests from\n+    an ``asyncio.Queue`` and groups them into micro-batches.  The actual\n+    blocking tokenizer call is executed inside a ``ThreadPoolExecutor`` \n+    with a single worker, so the event-loop never blocks.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        tokenizer,\n+        max_batch_size: int = 32,\n+        batch_wait_timeout_s: float = 0.002,\n+    ) -> None:\n+        self.tokenizer = tokenizer\n+        self.max_batch_size = max_batch_size\n+        self.batch_wait_timeout_s = batch_wait_timeout_s\n+\n+        self._loop = asyncio.get_running_loop()\n+        self._queues: dict[tuple, asyncio.Queue[tuple[str, object, dict,\n+                                                      asyncio.Future]]] = {}\n+        self._batcher_tasks: dict[tuple, asyncio.Task] = {}\n+\n+        # Single worker that owns the blocking tokenizer.\n+        self._executor = ThreadPoolExecutor(max_workers=1)\n+\n+    # ------------------------------------------------------------------\n+    # Public async API\n+    # ------------------------------------------------------------------\n+    async def __call__(self, prompt, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"encode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"encode\", prompt, kwargs, fut))\n+        return await fut\n+\n+    async def decode(self, token_ids, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"decode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"decode\", token_ids, kwargs, fut))\n+        return await fut\n+\n+    # ------------------------------------------------------------------\n+    # Internal helpers\n+    # ------------------------------------------------------------------\n+    def _ensure_queue_and_batcher(self, loop: asyncio.AbstractEventLoop,\n+                                  key: tuple):\n+        \"\"\"Return the queue for key, creating queue \n+        and batcher task if needed.\"\"\"\n+        if key not in self._queues:\n+            self._queues[key] = asyncio.Queue()\n+\n+        if key not in self._batcher_tasks or self._batcher_tasks[key].done():\n+            self._batcher_tasks[key] = loop.create_task(\n+                self._batch_loop(self._queues[key]))\n+\n+        return self._queues[key]\n+\n+    async def _batch_loop(self, queue: asyncio.Queue):\n+        \"\"\"Runs until the queue stays empty long enough to finish.\"\"\"\n+        while True:\n+            first_item = await queue.get()\n+            batch = [first_item]\n+            deadline = self._loop.time() + self.batch_wait_timeout_s\n+\n+            while len(batch) < self.max_batch_size:\n+                timeout = deadline - self._loop.time()\n+                if timeout <= 0:\n+                    break\n+                try:\n+                    item = await asyncio.wait_for(queue.get(), timeout)\n+                    batch.append(item)\n+                except asyncio.TimeoutError:\n+                    break\n+\n+            # Split by op type\n+            encode_items = [it for it in batch if it[0] == \"encode\"]\n+            decode_items = [it for it in batch if it[0] == \"decode\"]",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2172940414",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19334,
        "pr_file": "vllm/utils.py",
        "discussion_id": "2172940414",
        "commented_code": "@@ -510,6 +512,187 @@ def random_uuid() -> str:\n     return str(uuid.uuid4().hex)\n \n \n+class AsyncMicrobatchTokenizer:\n+    \"\"\"Asynchronous tokenizer with micro-batching.\n+\n+    A single background task pulls pending encode / decode requests from\n+    an ``asyncio.Queue`` and groups them into micro-batches.  The actual\n+    blocking tokenizer call is executed inside a ``ThreadPoolExecutor`` \n+    with a single worker, so the event-loop never blocks.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        tokenizer,\n+        max_batch_size: int = 32,\n+        batch_wait_timeout_s: float = 0.002,\n+    ) -> None:\n+        self.tokenizer = tokenizer\n+        self.max_batch_size = max_batch_size\n+        self.batch_wait_timeout_s = batch_wait_timeout_s\n+\n+        self._loop = asyncio.get_running_loop()\n+        self._queues: dict[tuple, asyncio.Queue[tuple[str, object, dict,\n+                                                      asyncio.Future]]] = {}\n+        self._batcher_tasks: dict[tuple, asyncio.Task] = {}\n+\n+        # Single worker that owns the blocking tokenizer.\n+        self._executor = ThreadPoolExecutor(max_workers=1)\n+\n+    # ------------------------------------------------------------------\n+    # Public async API\n+    # ------------------------------------------------------------------\n+    async def __call__(self, prompt, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"encode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"encode\", prompt, kwargs, fut))\n+        return await fut\n+\n+    async def decode(self, token_ids, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"decode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"decode\", token_ids, kwargs, fut))\n+        return await fut\n+\n+    # ------------------------------------------------------------------\n+    # Internal helpers\n+    # ------------------------------------------------------------------\n+    def _ensure_queue_and_batcher(self, loop: asyncio.AbstractEventLoop,\n+                                  key: tuple):\n+        \"\"\"Return the queue for key, creating queue \n+        and batcher task if needed.\"\"\"\n+        if key not in self._queues:\n+            self._queues[key] = asyncio.Queue()\n+\n+        if key not in self._batcher_tasks or self._batcher_tasks[key].done():\n+            self._batcher_tasks[key] = loop.create_task(\n+                self._batch_loop(self._queues[key]))\n+\n+        return self._queues[key]\n+\n+    async def _batch_loop(self, queue: asyncio.Queue):\n+        \"\"\"Runs until the queue stays empty long enough to finish.\"\"\"\n+        while True:\n+            first_item = await queue.get()\n+            batch = [first_item]\n+            deadline = self._loop.time() + self.batch_wait_timeout_s\n+\n+            while len(batch) < self.max_batch_size:\n+                timeout = deadline - self._loop.time()\n+                if timeout <= 0:\n+                    break\n+                try:\n+                    item = await asyncio.wait_for(queue.get(), timeout)\n+                    batch.append(item)\n+                except asyncio.TimeoutError:\n+                    break\n+\n+            # Split by op type\n+            encode_items = [it for it in batch if it[0] == \"encode\"]\n+            decode_items = [it for it in batch if it[0] == \"decode\"]",
        "comment_created_at": "2025-06-27T22:27:32+00:00",
        "comment_author": "njhill",
        "comment_body": "can we just have a bool arg to the method to indicate this. The whole batch will be the same type now right?\r\n\r\nActually maybe a separate method for encode / decode cases? Then the prompt and token_id lists can also be populated while reading from the queue, to avoid iterating again below.",
        "pr_file_module": null
      },
      {
        "comment_id": "2174124851",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19334,
        "pr_file": "vllm/utils.py",
        "discussion_id": "2172940414",
        "commented_code": "@@ -510,6 +512,187 @@ def random_uuid() -> str:\n     return str(uuid.uuid4().hex)\n \n \n+class AsyncMicrobatchTokenizer:\n+    \"\"\"Asynchronous tokenizer with micro-batching.\n+\n+    A single background task pulls pending encode / decode requests from\n+    an ``asyncio.Queue`` and groups them into micro-batches.  The actual\n+    blocking tokenizer call is executed inside a ``ThreadPoolExecutor`` \n+    with a single worker, so the event-loop never blocks.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        tokenizer,\n+        max_batch_size: int = 32,\n+        batch_wait_timeout_s: float = 0.002,\n+    ) -> None:\n+        self.tokenizer = tokenizer\n+        self.max_batch_size = max_batch_size\n+        self.batch_wait_timeout_s = batch_wait_timeout_s\n+\n+        self._loop = asyncio.get_running_loop()\n+        self._queues: dict[tuple, asyncio.Queue[tuple[str, object, dict,\n+                                                      asyncio.Future]]] = {}\n+        self._batcher_tasks: dict[tuple, asyncio.Task] = {}\n+\n+        # Single worker that owns the blocking tokenizer.\n+        self._executor = ThreadPoolExecutor(max_workers=1)\n+\n+    # ------------------------------------------------------------------\n+    # Public async API\n+    # ------------------------------------------------------------------\n+    async def __call__(self, prompt, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"encode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"encode\", prompt, kwargs, fut))\n+        return await fut\n+\n+    async def decode(self, token_ids, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"decode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"decode\", token_ids, kwargs, fut))\n+        return await fut\n+\n+    # ------------------------------------------------------------------\n+    # Internal helpers\n+    # ------------------------------------------------------------------\n+    def _ensure_queue_and_batcher(self, loop: asyncio.AbstractEventLoop,\n+                                  key: tuple):\n+        \"\"\"Return the queue for key, creating queue \n+        and batcher task if needed.\"\"\"\n+        if key not in self._queues:\n+            self._queues[key] = asyncio.Queue()\n+\n+        if key not in self._batcher_tasks or self._batcher_tasks[key].done():\n+            self._batcher_tasks[key] = loop.create_task(\n+                self._batch_loop(self._queues[key]))\n+\n+        return self._queues[key]\n+\n+    async def _batch_loop(self, queue: asyncio.Queue):\n+        \"\"\"Runs until the queue stays empty long enough to finish.\"\"\"\n+        while True:\n+            first_item = await queue.get()\n+            batch = [first_item]\n+            deadline = self._loop.time() + self.batch_wait_timeout_s\n+\n+            while len(batch) < self.max_batch_size:\n+                timeout = deadline - self._loop.time()\n+                if timeout <= 0:\n+                    break\n+                try:\n+                    item = await asyncio.wait_for(queue.get(), timeout)\n+                    batch.append(item)\n+                except asyncio.TimeoutError:\n+                    break\n+\n+            # Split by op type\n+            encode_items = [it for it in batch if it[0] == \"encode\"]\n+            decode_items = [it for it in batch if it[0] == \"decode\"]",
        "comment_created_at": "2025-06-30T03:05:14+00:00",
        "comment_author": "ztang2370",
        "comment_body": "updated",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2172953908",
    "pr_number": 19334,
    "pr_file": "vllm/utils.py",
    "created_at": "2025-06-27T22:47:50+00:00",
    "commented_code": "return str(uuid.uuid4().hex)\n \n \n+class AsyncMicrobatchTokenizer:\n+    \"\"\"Asynchronous tokenizer with micro-batching.\n+\n+    A single background task pulls pending encode / decode requests from\n+    an ``asyncio.Queue`` and groups them into micro-batches.  The actual\n+    blocking tokenizer call is executed inside a ``ThreadPoolExecutor`` \n+    with a single worker, so the event-loop never blocks.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        tokenizer,\n+        max_batch_size: int = 32,\n+        batch_wait_timeout_s: float = 0.002,\n+    ) -> None:\n+        self.tokenizer = tokenizer\n+        self.max_batch_size = max_batch_size\n+        self.batch_wait_timeout_s = batch_wait_timeout_s\n+\n+        self._loop = asyncio.get_running_loop()\n+        self._queues: dict[tuple, asyncio.Queue[tuple[str, object, dict,\n+                                                      asyncio.Future]]] = {}\n+        self._batcher_tasks: dict[tuple, asyncio.Task] = {}\n+\n+        # Single worker that owns the blocking tokenizer.\n+        self._executor = ThreadPoolExecutor(max_workers=1)\n+\n+    # ------------------------------------------------------------------\n+    # Public async API\n+    # ------------------------------------------------------------------\n+    async def __call__(self, prompt, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"encode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"encode\", prompt, kwargs, fut))\n+        return await fut\n+\n+    async def decode(self, token_ids, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"decode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"decode\", token_ids, kwargs, fut))\n+        return await fut\n+\n+    # ------------------------------------------------------------------\n+    # Internal helpers\n+    # ------------------------------------------------------------------\n+    def _ensure_queue_and_batcher(self, loop: asyncio.AbstractEventLoop,\n+                                  key: tuple):\n+        \"\"\"Return the queue for key, creating queue \n+        and batcher task if needed.\"\"\"\n+        if key not in self._queues:\n+            self._queues[key] = asyncio.Queue()\n+\n+        if key not in self._batcher_tasks or self._batcher_tasks[key].done():\n+            self._batcher_tasks[key] = loop.create_task(\n+                self._batch_loop(self._queues[key]))\n+\n+        return self._queues[key]\n+\n+    async def _batch_loop(self, queue: asyncio.Queue):\n+        \"\"\"Runs until the queue stays empty long enough to finish.\"\"\"\n+        while True:\n+            first_item = await queue.get()\n+            batch = [first_item]\n+            deadline = self._loop.time() + self.batch_wait_timeout_s\n+\n+            while len(batch) < self.max_batch_size:\n+                timeout = deadline - self._loop.time()\n+                if timeout <= 0:\n+                    break\n+                try:\n+                    item = await asyncio.wait_for(queue.get(), timeout)\n+                    batch.append(item)\n+                except asyncio.TimeoutError:\n+                    break\n+\n+            # Split by op type\n+            encode_items = [it for it in batch if it[0] == \"encode\"]\n+            decode_items = [it for it in batch if it[0] == \"decode\"]\n+\n+            # ---------------- Encode ----------------\n+            if encode_items:\n+                prompts = [it[1] for it in encode_items]\n+                kwargs_list = [it[2] for it in encode_items]\n+\n+                # If every request uses identical kwargs we can run a single\n+                # batched tokenizer call for a big speed-up.\n+                can_batch = all(kw == kwargs_list[0] for kw in kwargs_list)\n+\n+                if can_batch and len(prompts) > 1:\n+\n+                    def encode_fn(prompts=prompts, kwargs_list=kwargs_list):\n+                        \"\"\"Tokenize the micro-batch in one call \n+                        and split results.\"\"\"\n+                        grouped = self.tokenizer(prompts, **kwargs_list[0])\n+                        single_encodings = []\n+                        for i in range(len(prompts)):\n+                            data_i = {k: v[i] for k, v in grouped.items()}\n+                            single_encodings.append(BatchEncoding(data_i))\n+                        return single_encodings",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2172953908",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19334,
        "pr_file": "vllm/utils.py",
        "discussion_id": "2172953908",
        "commented_code": "@@ -510,6 +512,187 @@ def random_uuid() -> str:\n     return str(uuid.uuid4().hex)\n \n \n+class AsyncMicrobatchTokenizer:\n+    \"\"\"Asynchronous tokenizer with micro-batching.\n+\n+    A single background task pulls pending encode / decode requests from\n+    an ``asyncio.Queue`` and groups them into micro-batches.  The actual\n+    blocking tokenizer call is executed inside a ``ThreadPoolExecutor`` \n+    with a single worker, so the event-loop never blocks.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        tokenizer,\n+        max_batch_size: int = 32,\n+        batch_wait_timeout_s: float = 0.002,\n+    ) -> None:\n+        self.tokenizer = tokenizer\n+        self.max_batch_size = max_batch_size\n+        self.batch_wait_timeout_s = batch_wait_timeout_s\n+\n+        self._loop = asyncio.get_running_loop()\n+        self._queues: dict[tuple, asyncio.Queue[tuple[str, object, dict,\n+                                                      asyncio.Future]]] = {}\n+        self._batcher_tasks: dict[tuple, asyncio.Task] = {}\n+\n+        # Single worker that owns the blocking tokenizer.\n+        self._executor = ThreadPoolExecutor(max_workers=1)\n+\n+    # ------------------------------------------------------------------\n+    # Public async API\n+    # ------------------------------------------------------------------\n+    async def __call__(self, prompt, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"encode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"encode\", prompt, kwargs, fut))\n+        return await fut\n+\n+    async def decode(self, token_ids, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"decode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"decode\", token_ids, kwargs, fut))\n+        return await fut\n+\n+    # ------------------------------------------------------------------\n+    # Internal helpers\n+    # ------------------------------------------------------------------\n+    def _ensure_queue_and_batcher(self, loop: asyncio.AbstractEventLoop,\n+                                  key: tuple):\n+        \"\"\"Return the queue for key, creating queue \n+        and batcher task if needed.\"\"\"\n+        if key not in self._queues:\n+            self._queues[key] = asyncio.Queue()\n+\n+        if key not in self._batcher_tasks or self._batcher_tasks[key].done():\n+            self._batcher_tasks[key] = loop.create_task(\n+                self._batch_loop(self._queues[key]))\n+\n+        return self._queues[key]\n+\n+    async def _batch_loop(self, queue: asyncio.Queue):\n+        \"\"\"Runs until the queue stays empty long enough to finish.\"\"\"\n+        while True:\n+            first_item = await queue.get()\n+            batch = [first_item]\n+            deadline = self._loop.time() + self.batch_wait_timeout_s\n+\n+            while len(batch) < self.max_batch_size:\n+                timeout = deadline - self._loop.time()\n+                if timeout <= 0:\n+                    break\n+                try:\n+                    item = await asyncio.wait_for(queue.get(), timeout)\n+                    batch.append(item)\n+                except asyncio.TimeoutError:\n+                    break\n+\n+            # Split by op type\n+            encode_items = [it for it in batch if it[0] == \"encode\"]\n+            decode_items = [it for it in batch if it[0] == \"decode\"]\n+\n+            # ---------------- Encode ----------------\n+            if encode_items:\n+                prompts = [it[1] for it in encode_items]\n+                kwargs_list = [it[2] for it in encode_items]\n+\n+                # If every request uses identical kwargs we can run a single\n+                # batched tokenizer call for a big speed-up.\n+                can_batch = all(kw == kwargs_list[0] for kw in kwargs_list)\n+\n+                if can_batch and len(prompts) > 1:\n+\n+                    def encode_fn(prompts=prompts, kwargs_list=kwargs_list):\n+                        \"\"\"Tokenize the micro-batch in one call \n+                        and split results.\"\"\"\n+                        grouped = self.tokenizer(prompts, **kwargs_list[0])\n+                        single_encodings = []\n+                        for i in range(len(prompts)):\n+                            data_i = {k: v[i] for k, v in grouped.items()}\n+                            single_encodings.append(BatchEncoding(data_i))\n+                        return single_encodings",
        "comment_created_at": "2025-06-27T22:47:50+00:00",
        "comment_author": "njhill",
        "comment_body": "It would be better to just return results as-is and have a single loop outside rather than having to loop over the prompts again to set the future results.",
        "pr_file_module": null
      },
      {
        "comment_id": "2174125302",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 19334,
        "pr_file": "vllm/utils.py",
        "discussion_id": "2172953908",
        "commented_code": "@@ -510,6 +512,187 @@ def random_uuid() -> str:\n     return str(uuid.uuid4().hex)\n \n \n+class AsyncMicrobatchTokenizer:\n+    \"\"\"Asynchronous tokenizer with micro-batching.\n+\n+    A single background task pulls pending encode / decode requests from\n+    an ``asyncio.Queue`` and groups them into micro-batches.  The actual\n+    blocking tokenizer call is executed inside a ``ThreadPoolExecutor`` \n+    with a single worker, so the event-loop never blocks.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        tokenizer,\n+        max_batch_size: int = 32,\n+        batch_wait_timeout_s: float = 0.002,\n+    ) -> None:\n+        self.tokenizer = tokenizer\n+        self.max_batch_size = max_batch_size\n+        self.batch_wait_timeout_s = batch_wait_timeout_s\n+\n+        self._loop = asyncio.get_running_loop()\n+        self._queues: dict[tuple, asyncio.Queue[tuple[str, object, dict,\n+                                                      asyncio.Future]]] = {}\n+        self._batcher_tasks: dict[tuple, asyncio.Task] = {}\n+\n+        # Single worker that owns the blocking tokenizer.\n+        self._executor = ThreadPoolExecutor(max_workers=1)\n+\n+    # ------------------------------------------------------------------\n+    # Public async API\n+    # ------------------------------------------------------------------\n+    async def __call__(self, prompt, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"encode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"encode\", prompt, kwargs, fut))\n+        return await fut\n+\n+    async def decode(self, token_ids, **kwargs):\n+        fut: asyncio.Future = self._loop.create_future()\n+        key = self._queue_key(\"decode\", kwargs)\n+        queue = self._ensure_queue_and_batcher(self._loop, key)\n+        await queue.put((\"decode\", token_ids, kwargs, fut))\n+        return await fut\n+\n+    # ------------------------------------------------------------------\n+    # Internal helpers\n+    # ------------------------------------------------------------------\n+    def _ensure_queue_and_batcher(self, loop: asyncio.AbstractEventLoop,\n+                                  key: tuple):\n+        \"\"\"Return the queue for key, creating queue \n+        and batcher task if needed.\"\"\"\n+        if key not in self._queues:\n+            self._queues[key] = asyncio.Queue()\n+\n+        if key not in self._batcher_tasks or self._batcher_tasks[key].done():\n+            self._batcher_tasks[key] = loop.create_task(\n+                self._batch_loop(self._queues[key]))\n+\n+        return self._queues[key]\n+\n+    async def _batch_loop(self, queue: asyncio.Queue):\n+        \"\"\"Runs until the queue stays empty long enough to finish.\"\"\"\n+        while True:\n+            first_item = await queue.get()\n+            batch = [first_item]\n+            deadline = self._loop.time() + self.batch_wait_timeout_s\n+\n+            while len(batch) < self.max_batch_size:\n+                timeout = deadline - self._loop.time()\n+                if timeout <= 0:\n+                    break\n+                try:\n+                    item = await asyncio.wait_for(queue.get(), timeout)\n+                    batch.append(item)\n+                except asyncio.TimeoutError:\n+                    break\n+\n+            # Split by op type\n+            encode_items = [it for it in batch if it[0] == \"encode\"]\n+            decode_items = [it for it in batch if it[0] == \"decode\"]\n+\n+            # ---------------- Encode ----------------\n+            if encode_items:\n+                prompts = [it[1] for it in encode_items]\n+                kwargs_list = [it[2] for it in encode_items]\n+\n+                # If every request uses identical kwargs we can run a single\n+                # batched tokenizer call for a big speed-up.\n+                can_batch = all(kw == kwargs_list[0] for kw in kwargs_list)\n+\n+                if can_batch and len(prompts) > 1:\n+\n+                    def encode_fn(prompts=prompts, kwargs_list=kwargs_list):\n+                        \"\"\"Tokenize the micro-batch in one call \n+                        and split results.\"\"\"\n+                        grouped = self.tokenizer(prompts, **kwargs_list[0])\n+                        single_encodings = []\n+                        for i in range(len(prompts)):\n+                            data_i = {k: v[i] for k, v in grouped.items()}\n+                            single_encodings.append(BatchEncoding(data_i))\n+                        return single_encodings",
        "comment_created_at": "2025-06-30T03:05:54+00:00",
        "comment_author": "ztang2370",
        "comment_body": "updated",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189746416",
    "pr_number": 20413,
    "pr_file": "vllm/transformers_utils/detokenizer_utils.py",
    "created_at": "2025-07-07T11:12:43+00:00",
    "commented_code": "# NOTE(woosuk): The following code is slow because it runs a for loop over\n     # the output_tokens. In Python, running a for loop over a list can be slow\n     # even when the loop body is very simple.\n+    # Performance improvements: avoid repeated attribute and function lookups;\n+    # localize frequently used objects;\n+\n     sub_texts: list[str] = []\n     current_sub_text: list[str] = []\n-    all_special_tokens = set(tokenizer.all_special_tokens)\n+    convert_tokens_to_string = tokenizer.convert_tokens_to_string\n+    added_vocab_set = set(tokenizer.get_added_vocab())\n+    all_special_tokens = set(tokenizer.all_special_tokens) if skip_special_tokens else ()\n+\n     for token in output_tokens:\n+        # Use precomputed set for skip-special check\n         if skip_special_tokens and token in all_special_tokens:",
    "repo_full_name": "vllm-project/vllm",
    "discussion_comments": [
      {
        "comment_id": "2189746416",
        "repo_full_name": "vllm-project/vllm",
        "pr_number": 20413,
        "pr_file": "vllm/transformers_utils/detokenizer_utils.py",
        "discussion_id": "2189746416",
        "commented_code": "@@ -23,27 +23,31 @@ def _convert_tokens_to_string_with_added_encoders(\n     # NOTE(woosuk): The following code is slow because it runs a for loop over\n     # the output_tokens. In Python, running a for loop over a list can be slow\n     # even when the loop body is very simple.\n+    # Performance improvements: avoid repeated attribute and function lookups;\n+    # localize frequently used objects;\n+\n     sub_texts: list[str] = []\n     current_sub_text: list[str] = []\n-    all_special_tokens = set(tokenizer.all_special_tokens)\n+    convert_tokens_to_string = tokenizer.convert_tokens_to_string\n+    added_vocab_set = set(tokenizer.get_added_vocab())\n+    all_special_tokens = set(tokenizer.all_special_tokens) if skip_special_tokens else ()\n+\n     for token in output_tokens:\n+        # Use precomputed set for skip-special check\n         if skip_special_tokens and token in all_special_tokens:",
        "comment_created_at": "2025-07-07T11:12:43+00:00",
        "comment_author": "njhill",
        "comment_body": "could be simplified now\r\n```suggestion\r\n        if token in all_special_tokens:\r\n```",
        "pr_file_module": null
      }
    ]
  }
]