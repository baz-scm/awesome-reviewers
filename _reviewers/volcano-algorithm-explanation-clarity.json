[
  {
    "discussion_id": "2287371227",
    "pr_number": 4552,
    "pr_file": "docs/design/session-persisted-state.md",
    "created_at": "2025-08-20T08:18:12+00:00",
    "commented_code": "+# Volcano Scheduler: Task State Persistence Across Sessions \n+\n+\n+## Background\n+\n+Under heavy preemption/reclaim, high-priority jobs could reclaim resources too aggressively, leading to excessive evictions (\u201coverkill\u201d) of low-priority jobs and instability. Pipelined tasks (pre-allocated but not bound) lost state across sessions, causing duplicate decisions, mis-accounting, and oscillations. Plugins could observe inconsistent resource accounting during rapid pipeline \u2192 allocate \u2192 unallocate transitions.",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2287371227",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4552,
        "pr_file": "docs/design/session-persisted-state.md",
        "discussion_id": "2287371227",
        "commented_code": "@@ -0,0 +1,138 @@\n+# Volcano Scheduler: Task State Persistence Across Sessions \n+\n+\n+## Background\n+\n+Under heavy preemption/reclaim, high-priority jobs could reclaim resources too aggressively, leading to excessive evictions (\u201coverkill\u201d) of low-priority jobs and instability. Pipelined tasks (pre-allocated but not bound) lost state across sessions, causing duplicate decisions, mis-accounting, and oscillations. Plugins could observe inconsistent resource accounting during rapid pipeline \u2192 allocate \u2192 unallocate transitions.",
        "comment_created_at": "2025-08-20T08:18:12+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "The explanation is too abstract.  Need to explain in detail why the overkill phenomenon occurs.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2188871171",
    "pr_number": 4391,
    "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
    "created_at": "2025-07-07T02:58:26+00:00",
    "commented_code": "+# ResourceStrategyFit Plugin\n+\n+## Summary\n+\n+The native k8s ResourceStrategyFit plug-in can only adopt one type of strategy for all resources, such as MostRequestedPriority and LeastRequestedPriority. However, in industrial practice, this design is not applicable in some scenarios. For example: in AI scenarios, we usually disperse CPU tasks in CPU machine groups to reduce hot spots. GPU tasks are gathered in GPU machine groups to reduce GPU fragmentation. Therefore, we need to expand a scheduling strategy to meet the needs of this scenario.\n+\n+## Motivation\n+\n+- Different resource types can be configured with different aggregation or dispersion strategies, and weights can be used to distinguish priorities\n+\n+## Design Considerationsui\n+\n+- The solution is more versatile, not limited to AI clusters or CPU clusters, and not limited to common CPU resources or extended GPU resources.\n+\n+- Different resource policies can be configured for different cluster types and prioritized in the form of weights.\n+\n+- Easy to expand\n+\n+### Goals\n+\n+- Different types of resources can be configured with different strategies to prioritize them in the form of weights\n+\n+### Non-Goals\n+\n+- None.\n+\n+## Proposal\n+\n+Extend one plug-ins to meet the above needs\n+\n+- ResourceStrategyFit\n+\n+## User Story\n+\n+### Story1\n+- Users hope that different resource strategies can be adopted for different resource types. For example, in AI scenarios, they hope that pods that apply for GPU resources will occupy as many machines as possible, while pods that only apply for CPU resources will be evenly distributed to different machines.",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2188871171",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4391,
        "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
        "discussion_id": "2188871171",
        "commented_code": "@@ -0,0 +1,70 @@\n+# ResourceStrategyFit Plugin\n+\n+## Summary\n+\n+The native k8s ResourceStrategyFit plug-in can only adopt one type of strategy for all resources, such as MostRequestedPriority and LeastRequestedPriority. However, in industrial practice, this design is not applicable in some scenarios. For example: in AI scenarios, we usually disperse CPU tasks in CPU machine groups to reduce hot spots. GPU tasks are gathered in GPU machine groups to reduce GPU fragmentation. Therefore, we need to expand a scheduling strategy to meet the needs of this scenario.\n+\n+## Motivation\n+\n+- Different resource types can be configured with different aggregation or dispersion strategies, and weights can be used to distinguish priorities\n+\n+## Design Considerationsui\n+\n+- The solution is more versatile, not limited to AI clusters or CPU clusters, and not limited to common CPU resources or extended GPU resources.\n+\n+- Different resource policies can be configured for different cluster types and prioritized in the form of weights.\n+\n+- Easy to expand\n+\n+### Goals\n+\n+- Different types of resources can be configured with different strategies to prioritize them in the form of weights\n+\n+### Non-Goals\n+\n+- None.\n+\n+## Proposal\n+\n+Extend one plug-ins to meet the above needs\n+\n+- ResourceStrategyFit\n+\n+## User Story\n+\n+### Story1\n+- Users hope that different resource strategies can be adopted for different resource types. For example, in AI scenarios, they hope that pods that apply for GPU resources will occupy as many machines as possible, while pods that only apply for CPU resources will be evenly distributed to different machines.",
        "comment_created_at": "2025-07-07T02:58:26+00:00",
        "comment_author": "Monokaix",
        "comment_body": "Here can give a more concrete scenario, like pytorch job? master pod using CPU should disperse to avoid hot node while worker pods using GPU should aggregate to reduce resource fragment?",
        "pr_file_module": null
      },
      {
        "comment_id": "2188901676",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4391,
        "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
        "discussion_id": "2188871171",
        "commented_code": "@@ -0,0 +1,70 @@\n+# ResourceStrategyFit Plugin\n+\n+## Summary\n+\n+The native k8s ResourceStrategyFit plug-in can only adopt one type of strategy for all resources, such as MostRequestedPriority and LeastRequestedPriority. However, in industrial practice, this design is not applicable in some scenarios. For example: in AI scenarios, we usually disperse CPU tasks in CPU machine groups to reduce hot spots. GPU tasks are gathered in GPU machine groups to reduce GPU fragmentation. Therefore, we need to expand a scheduling strategy to meet the needs of this scenario.\n+\n+## Motivation\n+\n+- Different resource types can be configured with different aggregation or dispersion strategies, and weights can be used to distinguish priorities\n+\n+## Design Considerationsui\n+\n+- The solution is more versatile, not limited to AI clusters or CPU clusters, and not limited to common CPU resources or extended GPU resources.\n+\n+- Different resource policies can be configured for different cluster types and prioritized in the form of weights.\n+\n+- Easy to expand\n+\n+### Goals\n+\n+- Different types of resources can be configured with different strategies to prioritize them in the form of weights\n+\n+### Non-Goals\n+\n+- None.\n+\n+## Proposal\n+\n+Extend one plug-ins to meet the above needs\n+\n+- ResourceStrategyFit\n+\n+## User Story\n+\n+### Story1\n+- Users hope that different resource strategies can be adopted for different resource types. For example, in AI scenarios, they hope that pods that apply for GPU resources will occupy as many machines as possible, while pods that only apply for CPU resources will be evenly distributed to different machines.",
        "comment_created_at": "2025-07-07T03:39:48+00:00",
        "comment_author": "LY-today",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189075545",
    "pr_number": 4391,
    "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
    "created_at": "2025-07-07T06:19:19+00:00",
    "commented_code": "+# ResourceStrategyFit Plugin\n+\n+## Summary\n+\n+The native k8s ResourceStrategyFit plug-in can only adopt one type of strategy for all resources, such as MostRequestedPriority and LeastRequestedPriority. However, in industrial practice, this design is not applicable in some scenarios. For example: in AI scenarios, we usually disperse CPU tasks in CPU machine groups to reduce hot spots. GPU tasks are gathered in GPU machine groups to reduce GPU fragmentation. Therefore, we need to expand a scheduling strategy to meet the needs of this scenario.\n+\n+## Motivation\n+\n+- Different resource types can be configured with different aggregation or dispersion strategies, and weights can be used to distinguish priorities\n+\n+## Design Considerationsui\n+\n+- The solution is more versatile, not limited to AI clusters or CPU clusters, and not limited to common CPU resources or extended GPU resources.\n+\n+- Different resource policies can be configured for different cluster types and prioritized in the form of weights.\n+\n+- Easy to expand\n+\n+### Goals\n+\n+- Different types of resources can be configured with different strategies to prioritize them in the form of weights\n+\n+### Non-Goals\n+\n+- None.\n+\n+## Proposal\n+\n+Extend one plug-ins to meet the above needs\n+\n+- ResourceStrategyFit\n+\n+## User Story\n+\n+### Story1\n+- Users hope that different resource strategies can be adopted for different resource types. For example, like pytorch job, master pod using CPU should disperse to avoid hot node while worker pods using GPU should aggregate to reduce resource fragment.",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2189075545",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4391,
        "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
        "discussion_id": "2189075545",
        "commented_code": "@@ -0,0 +1,86 @@\n+# ResourceStrategyFit Plugin\n+\n+## Summary\n+\n+The native k8s ResourceStrategyFit plug-in can only adopt one type of strategy for all resources, such as MostRequestedPriority and LeastRequestedPriority. However, in industrial practice, this design is not applicable in some scenarios. For example: in AI scenarios, we usually disperse CPU tasks in CPU machine groups to reduce hot spots. GPU tasks are gathered in GPU machine groups to reduce GPU fragmentation. Therefore, we need to expand a scheduling strategy to meet the needs of this scenario.\n+\n+## Motivation\n+\n+- Different resource types can be configured with different aggregation or dispersion strategies, and weights can be used to distinguish priorities\n+\n+## Design Considerationsui\n+\n+- The solution is more versatile, not limited to AI clusters or CPU clusters, and not limited to common CPU resources or extended GPU resources.\n+\n+- Different resource policies can be configured for different cluster types and prioritized in the form of weights.\n+\n+- Easy to expand\n+\n+### Goals\n+\n+- Different types of resources can be configured with different strategies to prioritize them in the form of weights\n+\n+### Non-Goals\n+\n+- None.\n+\n+## Proposal\n+\n+Extend one plug-ins to meet the above needs\n+\n+- ResourceStrategyFit\n+\n+## User Story\n+\n+### Story1\n+- Users hope that different resource strategies can be adopted for different resource types. For example, like pytorch job, master pod using CPU should disperse to avoid hot node while worker pods using GPU should aggregate to reduce resource fragment.",
        "comment_created_at": "2025-07-07T06:19:19+00:00",
        "comment_author": "Monokaix",
        "comment_body": "```suggestion\r\n- Users expect different resource allocation strategies to be applied based on resource types. For example, in PyTorch jobs, the master pod (which uses CPU) should be distributed to avoid node hotspots, while worker pods (which use GPU) should be aggregated to minimize resource fragmentation.\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2189079962",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4391,
        "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
        "discussion_id": "2189075545",
        "commented_code": "@@ -0,0 +1,86 @@\n+# ResourceStrategyFit Plugin\n+\n+## Summary\n+\n+The native k8s ResourceStrategyFit plug-in can only adopt one type of strategy for all resources, such as MostRequestedPriority and LeastRequestedPriority. However, in industrial practice, this design is not applicable in some scenarios. For example: in AI scenarios, we usually disperse CPU tasks in CPU machine groups to reduce hot spots. GPU tasks are gathered in GPU machine groups to reduce GPU fragmentation. Therefore, we need to expand a scheduling strategy to meet the needs of this scenario.\n+\n+## Motivation\n+\n+- Different resource types can be configured with different aggregation or dispersion strategies, and weights can be used to distinguish priorities\n+\n+## Design Considerationsui\n+\n+- The solution is more versatile, not limited to AI clusters or CPU clusters, and not limited to common CPU resources or extended GPU resources.\n+\n+- Different resource policies can be configured for different cluster types and prioritized in the form of weights.\n+\n+- Easy to expand\n+\n+### Goals\n+\n+- Different types of resources can be configured with different strategies to prioritize them in the form of weights\n+\n+### Non-Goals\n+\n+- None.\n+\n+## Proposal\n+\n+Extend one plug-ins to meet the above needs\n+\n+- ResourceStrategyFit\n+\n+## User Story\n+\n+### Story1\n+- Users hope that different resource strategies can be adopted for different resource types. For example, like pytorch job, master pod using CPU should disperse to avoid hot node while worker pods using GPU should aggregate to reduce resource fragment.",
        "comment_created_at": "2025-07-07T06:21:50+00:00",
        "comment_author": "LY-today",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1817992739",
    "pr_number": 3775,
    "pr_file": "docs/design/node-resource-reservation-design.md",
    "created_at": "2024-10-27T06:34:12+00:00",
    "commented_code": "+# Volcano node resource reservation\n+## background\n+* Consider such situation: there are thounsands of pods to scheduler a day, in the first hour 500 low priority pods are created and schedulered which used 99% of cluster resource, in the second hour 10 high priority pods are created, however, low priority pods are still running, high priority pods can not be scheduled due to lack of resource.\n+* The user want the high priority task in the second hour to have resource to schedule immediately every day and high priority task better not preempt low priority pods because some pod may have already run many days.\n+## design\n+![annotation](images/node-resource-reservation-annotation.png) \n+### recognize high priority pods\n+There are two ways to recognize high priority pods:\n+* set annotation volcano.sh/is-reserve: 1 in podgroup \n+which means all pods under podgroup are reserve tasks\n+* set annotation volcano.sh/is-reserve: 1 in pod\n+which means this pod is reserve task\n+### recognize pod max running time\n+* set annotation volcano.sh/runsec-max: 500 in podgroup\n+which means all pods under podgroup will run 500 seconds max\n+* set annotation volcano.sh/runsec-max: 500 in pod\n+which means this pod will run 500 seconds max\n+### reserve plugin\n+#### configuration\n+```\n+- plugins:\n+   - name: reserve\n+     arguments:\n+       reserve.nodeLabel: label1,label2\n+       reserve.define.label1: {\"business_type\": \"ebook\"}\n+       reserve.resources.label1: [{\"start_hour\": 3, \"end_hour\": 4, \"cpu\": 32, \"memory\": 64, \"start_reserve_ago\": \"2h\", \"pod_num\": 10, \"cron\": \"daily\"}]\n+       reserve.define.label2: {\"business_type\": \"computer\"}\n+       reserve.resources.label2: [{\"start_hour\": 7, \"end_hour\": 9, \"cpu\": 24, \"memory\": 96, \"start_reserve_ago\": \"30m\", \"pod_num\": 15, \"cron\": \"weekly 1,2,5\"}]\n+```\n+In the configuration, nodeLabel represent a node list which nodeselector satisfy the nodeLabel, resources represent a list of resource reservation configuration, for example, reserve.resources.label1 means in hour 3 to 4 everyday, 32 cpu, 64 memory need to be reserved for label1, and start to reserve 2h ago, if 10 reserve pods are scheduled or after hour 4, stop reserve.\n+#### PredicateFn\n+Predicate is used to restrict other pods to be scheduled on reserved nodes. Reserved nodes are filtered out from the list of nodes and will change dynamically. \n+* check if the task is a reserve task, if yes, permit the task to be scheduled on this node.",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1817992739",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3775,
        "pr_file": "docs/design/node-resource-reservation-design.md",
        "discussion_id": "1817992739",
        "commented_code": "@@ -0,0 +1,45 @@\n+# Volcano node resource reservation\n+## background\n+* Consider such situation: there are thounsands of pods to scheduler a day, in the first hour 500 low priority pods are created and schedulered which used 99% of cluster resource, in the second hour 10 high priority pods are created, however, low priority pods are still running, high priority pods can not be scheduled due to lack of resource.\n+* The user want the high priority task in the second hour to have resource to schedule immediately every day and high priority task better not preempt low priority pods because some pod may have already run many days.\n+## design\n+![annotation](images/node-resource-reservation-annotation.png) \n+### recognize high priority pods\n+There are two ways to recognize high priority pods:\n+* set annotation volcano.sh/is-reserve: 1 in podgroup \n+which means all pods under podgroup are reserve tasks\n+* set annotation volcano.sh/is-reserve: 1 in pod\n+which means this pod is reserve task\n+### recognize pod max running time\n+* set annotation volcano.sh/runsec-max: 500 in podgroup\n+which means all pods under podgroup will run 500 seconds max\n+* set annotation volcano.sh/runsec-max: 500 in pod\n+which means this pod will run 500 seconds max\n+### reserve plugin\n+#### configuration\n+```\n+- plugins:\n+   - name: reserve\n+     arguments:\n+       reserve.nodeLabel: label1,label2\n+       reserve.define.label1: {\"business_type\": \"ebook\"}\n+       reserve.resources.label1: [{\"start_hour\": 3, \"end_hour\": 4, \"cpu\": 32, \"memory\": 64, \"start_reserve_ago\": \"2h\", \"pod_num\": 10, \"cron\": \"daily\"}]\n+       reserve.define.label2: {\"business_type\": \"computer\"}\n+       reserve.resources.label2: [{\"start_hour\": 7, \"end_hour\": 9, \"cpu\": 24, \"memory\": 96, \"start_reserve_ago\": \"30m\", \"pod_num\": 15, \"cron\": \"weekly 1,2,5\"}]\n+```\n+In the configuration, nodeLabel represent a node list which nodeselector satisfy the nodeLabel, resources represent a list of resource reservation configuration, for example, reserve.resources.label1 means in hour 3 to 4 everyday, 32 cpu, 64 memory need to be reserved for label1, and start to reserve 2h ago, if 10 reserve pods are scheduled or after hour 4, stop reserve.\n+#### PredicateFn\n+Predicate is used to restrict other pods to be scheduled on reserved nodes. Reserved nodes are filtered out from the list of nodes and will change dynamically. \n+* check if the task is a reserve task, if yes, permit the task to be scheduled on this node.",
        "comment_created_at": "2024-10-27T06:34:12+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "Can you rearrange the explanation here according to the order of the flow chart? If it is a reserve task, it can be scheduled to this node directly? Doesn't it check whether the resources are sufficient?",
        "pr_file_module": null
      },
      {
        "comment_id": "1823812999",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3775,
        "pr_file": "docs/design/node-resource-reservation-design.md",
        "discussion_id": "1817992739",
        "commented_code": "@@ -0,0 +1,45 @@\n+# Volcano node resource reservation\n+## background\n+* Consider such situation: there are thounsands of pods to scheduler a day, in the first hour 500 low priority pods are created and schedulered which used 99% of cluster resource, in the second hour 10 high priority pods are created, however, low priority pods are still running, high priority pods can not be scheduled due to lack of resource.\n+* The user want the high priority task in the second hour to have resource to schedule immediately every day and high priority task better not preempt low priority pods because some pod may have already run many days.\n+## design\n+![annotation](images/node-resource-reservation-annotation.png) \n+### recognize high priority pods\n+There are two ways to recognize high priority pods:\n+* set annotation volcano.sh/is-reserve: 1 in podgroup \n+which means all pods under podgroup are reserve tasks\n+* set annotation volcano.sh/is-reserve: 1 in pod\n+which means this pod is reserve task\n+### recognize pod max running time\n+* set annotation volcano.sh/runsec-max: 500 in podgroup\n+which means all pods under podgroup will run 500 seconds max\n+* set annotation volcano.sh/runsec-max: 500 in pod\n+which means this pod will run 500 seconds max\n+### reserve plugin\n+#### configuration\n+```\n+- plugins:\n+   - name: reserve\n+     arguments:\n+       reserve.nodeLabel: label1,label2\n+       reserve.define.label1: {\"business_type\": \"ebook\"}\n+       reserve.resources.label1: [{\"start_hour\": 3, \"end_hour\": 4, \"cpu\": 32, \"memory\": 64, \"start_reserve_ago\": \"2h\", \"pod_num\": 10, \"cron\": \"daily\"}]\n+       reserve.define.label2: {\"business_type\": \"computer\"}\n+       reserve.resources.label2: [{\"start_hour\": 7, \"end_hour\": 9, \"cpu\": 24, \"memory\": 96, \"start_reserve_ago\": \"30m\", \"pod_num\": 15, \"cron\": \"weekly 1,2,5\"}]\n+```\n+In the configuration, nodeLabel represent a node list which nodeselector satisfy the nodeLabel, resources represent a list of resource reservation configuration, for example, reserve.resources.label1 means in hour 3 to 4 everyday, 32 cpu, 64 memory need to be reserved for label1, and start to reserve 2h ago, if 10 reserve pods are scheduled or after hour 4, stop reserve.\n+#### PredicateFn\n+Predicate is used to restrict other pods to be scheduled on reserved nodes. Reserved nodes are filtered out from the list of nodes and will change dynamically. \n+* check if the task is a reserve task, if yes, permit the task to be scheduled on this node.",
        "comment_created_at": "2024-10-31T06:10:55+00:00",
        "comment_author": "molei20021",
        "comment_body": "if it is reserve task, PredicateFn will not forbid the task to be scheduled to the node, if in allocate action, the node do not have enough resource, the task will not be scheduled to the node.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2057751628",
    "pr_number": 4213,
    "pr_file": "docs/design/Network Topology Aware Scheduling.md",
    "created_at": "2025-04-24T07:38:50+00:00",
    "commented_code": "- AddJobGroupReadyFn: check whether hyperJob minAvailable is met.(phase 2)  \n \n - AddHyperNodeOrderFn: score for hyperNodes.(take effect in hard limit, closest tiers have higher score)\n+1. If it is the first scheduling of a job, all HyperNodes that need to be scored will be given a score of 0 and returned. The HyperNode that is successfully scheduled in the end will be recorded as the `JobAllocatedHyperNode` attribute of the job.",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2057751628",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4213,
        "pr_file": "docs/design/Network Topology Aware Scheduling.md",
        "discussion_id": "2057751628",
        "commented_code": "@@ -570,8 +570,14 @@ Allocate resources for queue-\\> hyperJob \\-\\> Job \\-\\> Task.\n - AddJobGroupReadyFn: check whether hyperJob minAvailable is met.(phase 2)  \n \n - AddHyperNodeOrderFn: score for hyperNodes.(take effect in hard limit, closest tiers have higher score)\n+1. If it is the first scheduling of a job, all HyperNodes that need to be scored will be given a score of 0 and returned. The HyperNode that is successfully scheduled in the end will be recorded as the `JobAllocatedHyperNode` attribute of the job.",
        "comment_created_at": "2025-04-24T07:38:50+00:00",
        "comment_author": "Monokaix",
        "comment_body": "```suggestion\r\n1. If a Job is being scheduled for the very first time, all HyperNodes that need to be scored will get a score of 0 and then return right away. The name of the HyperNode where the Job eventually gets scheduled successfully will be recorded in the Job's annotations under the key JobAllocatedHyperNode\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2057752711",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4213,
        "pr_file": "docs/design/Network Topology Aware Scheduling.md",
        "discussion_id": "2057751628",
        "commented_code": "@@ -570,8 +570,14 @@ Allocate resources for queue-\\> hyperJob \\-\\> Job \\-\\> Task.\n - AddJobGroupReadyFn: check whether hyperJob minAvailable is met.(phase 2)  \n \n - AddHyperNodeOrderFn: score for hyperNodes.(take effect in hard limit, closest tiers have higher score)\n+1. If it is the first scheduling of a job, all HyperNodes that need to be scored will be given a score of 0 and returned. The HyperNode that is successfully scheduled in the end will be recorded as the `JobAllocatedHyperNode` attribute of the job.",
        "comment_created_at": "2025-04-24T07:39:30+00:00",
        "comment_author": "Monokaix",
        "comment_body": "Please also change to \"**plugin:** network-topology-aware\" in line 568.",
        "pr_file_module": null
      },
      {
        "comment_id": "2057829019",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4213,
        "pr_file": "docs/design/Network Topology Aware Scheduling.md",
        "discussion_id": "2057751628",
        "commented_code": "@@ -570,8 +570,14 @@ Allocate resources for queue-\\> hyperJob \\-\\> Job \\-\\> Task.\n - AddJobGroupReadyFn: check whether hyperJob minAvailable is met.(phase 2)  \n \n - AddHyperNodeOrderFn: score for hyperNodes.(take effect in hard limit, closest tiers have higher score)\n+1. If it is the first scheduling of a job, all HyperNodes that need to be scored will be given a score of 0 and returned. The HyperNode that is successfully scheduled in the end will be recorded as the `JobAllocatedHyperNode` attribute of the job.",
        "comment_created_at": "2025-04-24T08:19:43+00:00",
        "comment_author": "ecosysbin",
        "comment_body": "Ok, Done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1618057965",
    "pr_number": 3487,
    "pr_file": "docs/design/dynamic-resource-allocation.md",
    "created_at": "2024-05-29T01:31:26+00:00",
    "commented_code": "+# Dynamic Resources Allocation\n+\n+## Summary\n+\n+DRA(Dynamic resource allocation) is the new way of requesting access to resources available in Kubernetes 1.26+.\n+\n+## Motivation\n+\n+DRA allows one to move away from the limited  \"countable\" API  provided by device plugins today. Volcano's implementation of device plugins is present in `https://github.com/volcano-sh/devices`.\n+\n+The KEP for k8s DRA implementation: https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3063-dynamic-resource-allocation/README.md\n+\n+## Goals\n+\n+1. DRA should support resource allocation between multiple containers or pods.\n+2. Users can attach arbitrary constraints to a resource request to get the exact resource\n+   they are looking for.\n+\n+\n+## Out of Scope\n+\n+1. Writing drivers. Support for new hardware will be provided by hardware vendor add-ons. The resources(resource driver) being mentioned in this context will be owned by these add-ons.\n+2. Replace support for device plugin.\n+\n+## Implementation Details\n+![img_1.png](./images/img_dra1.png)\n+The DRA will be available as a separate plugin in Volcano. \n+The functions associated with the Implementation are listed above which are used in kube-schedulers DRA implementation.\n+\n+The two allocation modes supported by DRA are:\n+1. immediate\n+2. delayed(aka. WaitForFirstConsumer)\n+\n+In immediate allocation the allocation on nodes starts as soon as the `ResourceClaim` gets created whereas in delayed mode allocation is delayed until a Pod gets scheduled that needs the `ResourceClaim`.\n+\n+For the delayed allocation mode,PodSchedulingcontext is used. In this allocation mode a node is selected tentatively by the scheduler in an iterative process where the scheduler suggests some potential nodes that fit the other resource requirements of a Pod and resource drivers respond with PodSchedulingcontext information about whether they can allocate claims for those nodes.\n+\n+\n+In the `OnSessionOpen` function of the plugin the logic related to publish is written. The `publish` will be used in updating existing podSchedulingContext's state with any changes in the scheduling state. If there's no existing PodSchedulingContext, it creates a new one.",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1618057965",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3487,
        "pr_file": "docs/design/dynamic-resource-allocation.md",
        "discussion_id": "1618057965",
        "commented_code": "@@ -0,0 +1,64 @@\n+# Dynamic Resources Allocation\n+\n+## Summary\n+\n+DRA(Dynamic resource allocation) is the new way of requesting access to resources available in Kubernetes 1.26+.\n+\n+## Motivation\n+\n+DRA allows one to move away from the limited  \"countable\" API  provided by device plugins today. Volcano's implementation of device plugins is present in `https://github.com/volcano-sh/devices`.\n+\n+The KEP for k8s DRA implementation: https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3063-dynamic-resource-allocation/README.md\n+\n+## Goals\n+\n+1. DRA should support resource allocation between multiple containers or pods.\n+2. Users can attach arbitrary constraints to a resource request to get the exact resource\n+   they are looking for.\n+\n+\n+## Out of Scope\n+\n+1. Writing drivers. Support for new hardware will be provided by hardware vendor add-ons. The resources(resource driver) being mentioned in this context will be owned by these add-ons.\n+2. Replace support for device plugin.\n+\n+## Implementation Details\n+![img_1.png](./images/img_dra1.png)\n+The DRA will be available as a separate plugin in Volcano. \n+The functions associated with the Implementation are listed above which are used in kube-schedulers DRA implementation.\n+\n+The two allocation modes supported by DRA are:\n+1. immediate\n+2. delayed(aka. WaitForFirstConsumer)\n+\n+In immediate allocation the allocation on nodes starts as soon as the `ResourceClaim` gets created whereas in delayed mode allocation is delayed until a Pod gets scheduled that needs the `ResourceClaim`.\n+\n+For the delayed allocation mode,PodSchedulingcontext is used. In this allocation mode a node is selected tentatively by the scheduler in an iterative process where the scheduler suggests some potential nodes that fit the other resource requirements of a Pod and resource drivers respond with PodSchedulingcontext information about whether they can allocate claims for those nodes.\n+\n+\n+In the `OnSessionOpen` function of the plugin the logic related to publish is written. The `publish` will be used in updating existing podSchedulingContext's state with any changes in the scheduling state. If there's no existing PodSchedulingContext, it creates a new one.",
        "comment_created_at": "2024-05-29T01:31:26+00:00",
        "comment_author": "Monokaix",
        "comment_body": "Besides the `publish` func of dra, we should also add prefilter, filter, scroe, reserved callback here and add postbind logic in after task bind successfully, please also add these details: )",
        "pr_file_module": null
      },
      {
        "comment_id": "1618058240",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3487,
        "pr_file": "docs/design/dynamic-resource-allocation.md",
        "discussion_id": "1618057965",
        "commented_code": "@@ -0,0 +1,64 @@\n+# Dynamic Resources Allocation\n+\n+## Summary\n+\n+DRA(Dynamic resource allocation) is the new way of requesting access to resources available in Kubernetes 1.26+.\n+\n+## Motivation\n+\n+DRA allows one to move away from the limited  \"countable\" API  provided by device plugins today. Volcano's implementation of device plugins is present in `https://github.com/volcano-sh/devices`.\n+\n+The KEP for k8s DRA implementation: https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3063-dynamic-resource-allocation/README.md\n+\n+## Goals\n+\n+1. DRA should support resource allocation between multiple containers or pods.\n+2. Users can attach arbitrary constraints to a resource request to get the exact resource\n+   they are looking for.\n+\n+\n+## Out of Scope\n+\n+1. Writing drivers. Support for new hardware will be provided by hardware vendor add-ons. The resources(resource driver) being mentioned in this context will be owned by these add-ons.\n+2. Replace support for device plugin.\n+\n+## Implementation Details\n+![img_1.png](./images/img_dra1.png)\n+The DRA will be available as a separate plugin in Volcano. \n+The functions associated with the Implementation are listed above which are used in kube-schedulers DRA implementation.\n+\n+The two allocation modes supported by DRA are:\n+1. immediate\n+2. delayed(aka. WaitForFirstConsumer)\n+\n+In immediate allocation the allocation on nodes starts as soon as the `ResourceClaim` gets created whereas in delayed mode allocation is delayed until a Pod gets scheduled that needs the `ResourceClaim`.\n+\n+For the delayed allocation mode,PodSchedulingcontext is used. In this allocation mode a node is selected tentatively by the scheduler in an iterative process where the scheduler suggests some potential nodes that fit the other resource requirements of a Pod and resource drivers respond with PodSchedulingcontext information about whether they can allocate claims for those nodes.\n+\n+\n+In the `OnSessionOpen` function of the plugin the logic related to publish is written. The `publish` will be used in updating existing podSchedulingContext's state with any changes in the scheduling state. If there's no existing PodSchedulingContext, it creates a new one.",
        "comment_created_at": "2024-05-29T01:32:01+00:00",
        "comment_author": "Monokaix",
        "comment_body": "And draw a picture to show the whole process is better.",
        "pr_file_module": null
      },
      {
        "comment_id": "1628767404",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3487,
        "pr_file": "docs/design/dynamic-resource-allocation.md",
        "discussion_id": "1618057965",
        "commented_code": "@@ -0,0 +1,64 @@\n+# Dynamic Resources Allocation\n+\n+## Summary\n+\n+DRA(Dynamic resource allocation) is the new way of requesting access to resources available in Kubernetes 1.26+.\n+\n+## Motivation\n+\n+DRA allows one to move away from the limited  \"countable\" API  provided by device plugins today. Volcano's implementation of device plugins is present in `https://github.com/volcano-sh/devices`.\n+\n+The KEP for k8s DRA implementation: https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3063-dynamic-resource-allocation/README.md\n+\n+## Goals\n+\n+1. DRA should support resource allocation between multiple containers or pods.\n+2. Users can attach arbitrary constraints to a resource request to get the exact resource\n+   they are looking for.\n+\n+\n+## Out of Scope\n+\n+1. Writing drivers. Support for new hardware will be provided by hardware vendor add-ons. The resources(resource driver) being mentioned in this context will be owned by these add-ons.\n+2. Replace support for device plugin.\n+\n+## Implementation Details\n+![img_1.png](./images/img_dra1.png)\n+The DRA will be available as a separate plugin in Volcano. \n+The functions associated with the Implementation are listed above which are used in kube-schedulers DRA implementation.\n+\n+The two allocation modes supported by DRA are:\n+1. immediate\n+2. delayed(aka. WaitForFirstConsumer)\n+\n+In immediate allocation the allocation on nodes starts as soon as the `ResourceClaim` gets created whereas in delayed mode allocation is delayed until a Pod gets scheduled that needs the `ResourceClaim`.\n+\n+For the delayed allocation mode,PodSchedulingcontext is used. In this allocation mode a node is selected tentatively by the scheduler in an iterative process where the scheduler suggests some potential nodes that fit the other resource requirements of a Pod and resource drivers respond with PodSchedulingcontext information about whether they can allocate claims for those nodes.\n+\n+\n+In the `OnSessionOpen` function of the plugin the logic related to publish is written. The `publish` will be used in updating existing podSchedulingContext's state with any changes in the scheduling state. If there's no existing PodSchedulingContext, it creates a new one.",
        "comment_created_at": "2024-06-06T04:43:04+00:00",
        "comment_author": "Subhasish-Behera",
        "comment_body": "@Monokaix  sent a message regarding the callback part:)",
        "pr_file_module": null
      },
      {
        "comment_id": "1642717151",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3487,
        "pr_file": "docs/design/dynamic-resource-allocation.md",
        "discussion_id": "1618057965",
        "commented_code": "@@ -0,0 +1,64 @@\n+# Dynamic Resources Allocation\n+\n+## Summary\n+\n+DRA(Dynamic resource allocation) is the new way of requesting access to resources available in Kubernetes 1.26+.\n+\n+## Motivation\n+\n+DRA allows one to move away from the limited  \"countable\" API  provided by device plugins today. Volcano's implementation of device plugins is present in `https://github.com/volcano-sh/devices`.\n+\n+The KEP for k8s DRA implementation: https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3063-dynamic-resource-allocation/README.md\n+\n+## Goals\n+\n+1. DRA should support resource allocation between multiple containers or pods.\n+2. Users can attach arbitrary constraints to a resource request to get the exact resource\n+   they are looking for.\n+\n+\n+## Out of Scope\n+\n+1. Writing drivers. Support for new hardware will be provided by hardware vendor add-ons. The resources(resource driver) being mentioned in this context will be owned by these add-ons.\n+2. Replace support for device plugin.\n+\n+## Implementation Details\n+![img_1.png](./images/img_dra1.png)\n+The DRA will be available as a separate plugin in Volcano. \n+The functions associated with the Implementation are listed above which are used in kube-schedulers DRA implementation.\n+\n+The two allocation modes supported by DRA are:\n+1. immediate\n+2. delayed(aka. WaitForFirstConsumer)\n+\n+In immediate allocation the allocation on nodes starts as soon as the `ResourceClaim` gets created whereas in delayed mode allocation is delayed until a Pod gets scheduled that needs the `ResourceClaim`.\n+\n+For the delayed allocation mode,PodSchedulingcontext is used. In this allocation mode a node is selected tentatively by the scheduler in an iterative process where the scheduler suggests some potential nodes that fit the other resource requirements of a Pod and resource drivers respond with PodSchedulingcontext information about whether they can allocate claims for those nodes.\n+\n+\n+In the `OnSessionOpen` function of the plugin the logic related to publish is written. The `publish` will be used in updating existing podSchedulingContext's state with any changes in the scheduling state. If there's no existing PodSchedulingContext, it creates a new one.",
        "comment_created_at": "2024-06-17T12:10:49+00:00",
        "comment_author": "Subhasish-Behera",
        "comment_body": "@Monokaix I have some doubts regarding the `prebind` step which I have messaged on slack.",
        "pr_file_module": null
      }
    ]
  }
]