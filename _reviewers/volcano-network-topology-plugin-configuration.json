[
  {
    "discussion_id": "2096788810",
    "pr_number": 3983,
    "pr_file": "test/e2e/hypernode/network_topology_test.go",
    "created_at": "2025-05-20T03:13:35+00:00",
    "commented_code": "+package hypernode\n+\n+import (\n+\t\"context\"\n+\t\"time\"\n+\n+\t. \"github.com/onsi/ginkgo/v2\"\n+\t. \"github.com/onsi/gomega\"\n+\tv1 \"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+\t\"k8s.io/utils/ptr\"\n+\n+\tbatchv1alpha1 \"volcano.sh/apis/pkg/apis/batch/v1alpha1\"\n+\ttopologyv1alpha1 \"volcano.sh/apis/pkg/apis/topology/v1alpha1\"\n+\te2eutil \"volcano.sh/volcano/test/e2e/util\"\n+)\n+\n+var (\n+\terr error\n+\tctx *e2eutil.TestContext\n+\n+\ttolerations = []v1.Toleration{\n+\t\t{\n+\t\t\tKey:      \"kwok.x-k8s.io/node\",\n+\t\t\tOperator: v1.TolerationOpEqual,\n+\t\t\tValue:    \"fake\",\n+\t\t\tEffect:   v1.TaintEffectNoSchedule,\n+\t\t},\n+\t}\n+)\n+\n+var _ = Describe(\"Network Topology Tests\", func() {\n+\tBeforeEach(func() {\n+\t\tctx = e2eutil.InitTestContext(e2eutil.Options{\n+\t\t\tNodesNumLimit: 8,\n+\t\t})\n+\n+\t\t// Setup the 3-tier topology structure\n+\t\t//                          s6\n+\t\t//                   /              \\\n+\t\t//                 s4               s5\n+\t\t//              /     \\          /     \\\n+\t\t//            s0      s1       s2       s3\n+\t\t//          /  \\     /  \\     /  \\     /  \\\n+\t\t//         n0  n1   n2  n3   n4  n5   n6  n7\n+\t\t//\n+\t\tBy(\"Setup 3-tier hypernodes\")\n+\t\thyperNodes := []struct {\n+\t\t\tname  string\n+\t\t\tnodes []string\n+\t\t\ttier  int\n+\t\t}{\n+\t\t\t// Tier-1\n+\t\t\t{\"s0\", []string{\"kwok-node-0\", \"kwok-node-1\"}, 1},\n+\t\t\t{\"s1\", []string{\"kwok-node-2\", \"kwok-node-3\"}, 1},\n+\t\t\t{\"s2\", []string{\"kwok-node-4\", \"kwok-node-5\"}, 1},\n+\t\t\t{\"s3\", []string{\"kwok-node-6\", \"kwok-node-7\"}, 1},\n+\t\t\t// Tier-2\n+\t\t\t{\"s4\", []string{\"s0\", \"s1\"}, 2},\n+\t\t\t{\"s5\", []string{\"s2\", \"s3\"}, 2},\n+\t\t\t// Tier-3\n+\t\t\t{\"s6\", []string{\"s4\", \"s5\"}, 3},\n+\t\t}\n+\n+\t\tfor _, hn := range hyperNodes {\n+\t\t\thyperNodeType := topologyv1alpha1.MemberTypeHyperNode\n+\t\t\tif hn.tier == 1 {\n+\t\t\t\thyperNodeType = topologyv1alpha1.MemberTypeNode\n+\t\t\t}\n+\t\t\tspec := &topologyv1alpha1.HyperNode{\n+\t\t\t\tObjectMeta: metav1.ObjectMeta{\n+\t\t\t\t\tName: hn.name,\n+\t\t\t\t},\n+\t\t\t\tSpec: topologyv1alpha1.HyperNodeSpec{\n+\t\t\t\t\tTier: hn.tier,\n+\t\t\t\t\tMembers: []topologyv1alpha1.MemberSpec{\n+\t\t\t\t\t\t{\n+\t\t\t\t\t\t\tType: hyperNodeType,\n+\t\t\t\t\t\t\tSelector: topologyv1alpha1.MemberSelector{\n+\t\t\t\t\t\t\t\tExactMatch: &topologyv1alpha1.ExactMatch{\n+\t\t\t\t\t\t\t\t\tName: hn.nodes[0],\n+\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\t{\n+\t\t\t\t\t\t\tType: hyperNodeType,\n+\t\t\t\t\t\t\tSelector: topologyv1alpha1.MemberSelector{\n+\t\t\t\t\t\t\t\tExactMatch: &topologyv1alpha1.ExactMatch{\n+\t\t\t\t\t\t\t\t\tName: hn.nodes[1],\n+\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\n+\t\t\terr = e2eutil.SetupHyperNode(ctx, spec)\n+\t\t\tExpect(err).NotTo(HaveOccurred())\n+\t\t}\n+\n+\t\t// Wait for all hypernodes to be ready\n+\t\tBy(\"Wait for hypernodes to be ready\")\n+\t\tfor _, hn := range hyperNodes {\n+\t\t\tEventually(func() error {\n+\t\t\t\t_, err = ctx.Vcclient.TopologyV1alpha1().HyperNodes().Get(context.TODO(), hn.name, metav1.GetOptions{})\n+\t\t\t\treturn err\n+\t\t\t}, 30*time.Second, time.Second).Should(BeNil())\n+\t\t}\n+\t})\n+\n+\tAfterEach(func() {\n+\t\te2eutil.CleanupTestContext(ctx)\n+\t})\n+\n+\tContext(\"Hard Mode Tests\", func() {\n+\t\tIt(\"Case 1.1: Schedule to node-2 and node-3 when resources are enough\", func() {\n+\t\t\tBy(\"Create job that fits in s1's resources\")\n+\n+\t\t\t// schedule pod to s1 (node-2 and node-3) to make sure the s1's binpack score is higher\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-2\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-3\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to s1\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-2\", \"kwok-node-3\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 1.2: Schedule to s2 when s1's resources are insufficient and s2's score is higher\", func() {\n+\t\t\tBy(\"Create job that fits in s1's resources\")\n+\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// allocate to s1(kwok-node-2, kwok-node-3) to make s1 has insufficient resources\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-3\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t// allocate to s2(kwok-node-4, kwok-node-5) to make s2 has enough resources and higher score\n+\t\t\t\t{Name: \"pod-2\", Node: \"kwok-node-4\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to s2\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-4\", \"kwok-node-5\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 1.3: Pods remain pending when no hypernode has sufficient resources\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-2\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-3\", Node: \"kwok-node-3\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-4\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-5\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-6\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-7\", Node: \"kwok-node-7\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Verify pods are pending\")\n+\t\t\tExpect(e2eutil.WaitTaskPhase(ctx, topologyJob, []v1.PodPhase{v1.PodPending}, 2)).NotTo(HaveOccurred())\n+\t\t})\n+\t})\n+\n+\tContext(\"Tier Tests\", func() {\n+\t\tIt(\"Case 2.1: Schedule to tier 2 when tier 1 resources are insufficient\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// make sure tier 1 has insufficient resources\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-2\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-3\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t// make sure tier 2 s5 has enough resources and higher score\n+\t\t\t\t{Name: \"pod-4\", Node: \"kwok-node-5\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(2),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to tier 2 s5\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-5\", \"kwok-node-7\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 2.2: pods are pending when tier 2 resources are insufficient\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// make sure tier 1 and tier 2 has insufficient resources, tier 3 has enough resources\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-1\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-2\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-3\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-4\", Node: \"kwok-node-5\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-5\", Node: \"kwok-node-6\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(2),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Verify pods are pending\")\n+\t\t\tExpect(e2eutil.WaitTaskPhase(ctx, topologyJob, []v1.PodPhase{v1.PodPending}, 2)).NotTo(HaveOccurred())\n+\t\t})\n+\t})\n+\n+\tContext(\"Soft Mode Tests\", func() {\n+\t\tIt(\"Case 3: Schedule to single hypernode in soft mode\", func() {",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2096788810",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3983,
        "pr_file": "test/e2e/hypernode/network_topology_test.go",
        "discussion_id": "2096788810",
        "commented_code": "@@ -0,0 +1,463 @@\n+package hypernode\n+\n+import (\n+\t\"context\"\n+\t\"time\"\n+\n+\t. \"github.com/onsi/ginkgo/v2\"\n+\t. \"github.com/onsi/gomega\"\n+\tv1 \"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+\t\"k8s.io/utils/ptr\"\n+\n+\tbatchv1alpha1 \"volcano.sh/apis/pkg/apis/batch/v1alpha1\"\n+\ttopologyv1alpha1 \"volcano.sh/apis/pkg/apis/topology/v1alpha1\"\n+\te2eutil \"volcano.sh/volcano/test/e2e/util\"\n+)\n+\n+var (\n+\terr error\n+\tctx *e2eutil.TestContext\n+\n+\ttolerations = []v1.Toleration{\n+\t\t{\n+\t\t\tKey:      \"kwok.x-k8s.io/node\",\n+\t\t\tOperator: v1.TolerationOpEqual,\n+\t\t\tValue:    \"fake\",\n+\t\t\tEffect:   v1.TaintEffectNoSchedule,\n+\t\t},\n+\t}\n+)\n+\n+var _ = Describe(\"Network Topology Tests\", func() {\n+\tBeforeEach(func() {\n+\t\tctx = e2eutil.InitTestContext(e2eutil.Options{\n+\t\t\tNodesNumLimit: 8,\n+\t\t})\n+\n+\t\t// Setup the 3-tier topology structure\n+\t\t//                          s6\n+\t\t//                   /              \\\n+\t\t//                 s4               s5\n+\t\t//              /     \\          /     \\\n+\t\t//            s0      s1       s2       s3\n+\t\t//          /  \\     /  \\     /  \\     /  \\\n+\t\t//         n0  n1   n2  n3   n4  n5   n6  n7\n+\t\t//\n+\t\tBy(\"Setup 3-tier hypernodes\")\n+\t\thyperNodes := []struct {\n+\t\t\tname  string\n+\t\t\tnodes []string\n+\t\t\ttier  int\n+\t\t}{\n+\t\t\t// Tier-1\n+\t\t\t{\"s0\", []string{\"kwok-node-0\", \"kwok-node-1\"}, 1},\n+\t\t\t{\"s1\", []string{\"kwok-node-2\", \"kwok-node-3\"}, 1},\n+\t\t\t{\"s2\", []string{\"kwok-node-4\", \"kwok-node-5\"}, 1},\n+\t\t\t{\"s3\", []string{\"kwok-node-6\", \"kwok-node-7\"}, 1},\n+\t\t\t// Tier-2\n+\t\t\t{\"s4\", []string{\"s0\", \"s1\"}, 2},\n+\t\t\t{\"s5\", []string{\"s2\", \"s3\"}, 2},\n+\t\t\t// Tier-3\n+\t\t\t{\"s6\", []string{\"s4\", \"s5\"}, 3},\n+\t\t}\n+\n+\t\tfor _, hn := range hyperNodes {\n+\t\t\thyperNodeType := topologyv1alpha1.MemberTypeHyperNode\n+\t\t\tif hn.tier == 1 {\n+\t\t\t\thyperNodeType = topologyv1alpha1.MemberTypeNode\n+\t\t\t}\n+\t\t\tspec := &topologyv1alpha1.HyperNode{\n+\t\t\t\tObjectMeta: metav1.ObjectMeta{\n+\t\t\t\t\tName: hn.name,\n+\t\t\t\t},\n+\t\t\t\tSpec: topologyv1alpha1.HyperNodeSpec{\n+\t\t\t\t\tTier: hn.tier,\n+\t\t\t\t\tMembers: []topologyv1alpha1.MemberSpec{\n+\t\t\t\t\t\t{\n+\t\t\t\t\t\t\tType: hyperNodeType,\n+\t\t\t\t\t\t\tSelector: topologyv1alpha1.MemberSelector{\n+\t\t\t\t\t\t\t\tExactMatch: &topologyv1alpha1.ExactMatch{\n+\t\t\t\t\t\t\t\t\tName: hn.nodes[0],\n+\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\t{\n+\t\t\t\t\t\t\tType: hyperNodeType,\n+\t\t\t\t\t\t\tSelector: topologyv1alpha1.MemberSelector{\n+\t\t\t\t\t\t\t\tExactMatch: &topologyv1alpha1.ExactMatch{\n+\t\t\t\t\t\t\t\t\tName: hn.nodes[1],\n+\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\n+\t\t\terr = e2eutil.SetupHyperNode(ctx, spec)\n+\t\t\tExpect(err).NotTo(HaveOccurred())\n+\t\t}\n+\n+\t\t// Wait for all hypernodes to be ready\n+\t\tBy(\"Wait for hypernodes to be ready\")\n+\t\tfor _, hn := range hyperNodes {\n+\t\t\tEventually(func() error {\n+\t\t\t\t_, err = ctx.Vcclient.TopologyV1alpha1().HyperNodes().Get(context.TODO(), hn.name, metav1.GetOptions{})\n+\t\t\t\treturn err\n+\t\t\t}, 30*time.Second, time.Second).Should(BeNil())\n+\t\t}\n+\t})\n+\n+\tAfterEach(func() {\n+\t\te2eutil.CleanupTestContext(ctx)\n+\t})\n+\n+\tContext(\"Hard Mode Tests\", func() {\n+\t\tIt(\"Case 1.1: Schedule to node-2 and node-3 when resources are enough\", func() {\n+\t\t\tBy(\"Create job that fits in s1's resources\")\n+\n+\t\t\t// schedule pod to s1 (node-2 and node-3) to make sure the s1's binpack score is higher\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-2\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-3\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to s1\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-2\", \"kwok-node-3\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 1.2: Schedule to s2 when s1's resources are insufficient and s2's score is higher\", func() {\n+\t\t\tBy(\"Create job that fits in s1's resources\")\n+\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// allocate to s1(kwok-node-2, kwok-node-3) to make s1 has insufficient resources\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-3\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t// allocate to s2(kwok-node-4, kwok-node-5) to make s2 has enough resources and higher score\n+\t\t\t\t{Name: \"pod-2\", Node: \"kwok-node-4\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to s2\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-4\", \"kwok-node-5\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 1.3: Pods remain pending when no hypernode has sufficient resources\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-2\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-3\", Node: \"kwok-node-3\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-4\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-5\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-6\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-7\", Node: \"kwok-node-7\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Verify pods are pending\")\n+\t\t\tExpect(e2eutil.WaitTaskPhase(ctx, topologyJob, []v1.PodPhase{v1.PodPending}, 2)).NotTo(HaveOccurred())\n+\t\t})\n+\t})\n+\n+\tContext(\"Tier Tests\", func() {\n+\t\tIt(\"Case 2.1: Schedule to tier 2 when tier 1 resources are insufficient\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// make sure tier 1 has insufficient resources\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-2\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-3\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t// make sure tier 2 s5 has enough resources and higher score\n+\t\t\t\t{Name: \"pod-4\", Node: \"kwok-node-5\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(2),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to tier 2 s5\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-5\", \"kwok-node-7\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 2.2: pods are pending when tier 2 resources are insufficient\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// make sure tier 1 and tier 2 has insufficient resources, tier 3 has enough resources\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-1\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-2\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-3\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-4\", Node: \"kwok-node-5\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-5\", Node: \"kwok-node-6\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(2),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Verify pods are pending\")\n+\t\t\tExpect(e2eutil.WaitTaskPhase(ctx, topologyJob, []v1.PodPhase{v1.PodPending}, 2)).NotTo(HaveOccurred())\n+\t\t})\n+\t})\n+\n+\tContext(\"Soft Mode Tests\", func() {\n+\t\tIt(\"Case 3: Schedule to single hypernode in soft mode\", func() {",
        "comment_created_at": "2025-05-20T03:13:35+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "I think there is something wrong with this soft mode case. The reason for scheduling to kwok-node-5 and kwok-node-7 is only because of the influence of binpack plugin. There is already a `network-topology-aware` plugin: https://github.com/volcano-sh/volcano/blob/network-topology/pkg/scheduler/plugins/network-topology-aware/network_topology_aware.go. If the tier1 hypernode has enough resources to schedule a vcjob, then in soft mode, all pods in a vcjob are expected to be under the same hypernode.",
        "pr_file_module": null
      },
      {
        "comment_id": "2096799041",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3983,
        "pr_file": "test/e2e/hypernode/network_topology_test.go",
        "discussion_id": "2096788810",
        "commented_code": "@@ -0,0 +1,463 @@\n+package hypernode\n+\n+import (\n+\t\"context\"\n+\t\"time\"\n+\n+\t. \"github.com/onsi/ginkgo/v2\"\n+\t. \"github.com/onsi/gomega\"\n+\tv1 \"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+\t\"k8s.io/utils/ptr\"\n+\n+\tbatchv1alpha1 \"volcano.sh/apis/pkg/apis/batch/v1alpha1\"\n+\ttopologyv1alpha1 \"volcano.sh/apis/pkg/apis/topology/v1alpha1\"\n+\te2eutil \"volcano.sh/volcano/test/e2e/util\"\n+)\n+\n+var (\n+\terr error\n+\tctx *e2eutil.TestContext\n+\n+\ttolerations = []v1.Toleration{\n+\t\t{\n+\t\t\tKey:      \"kwok.x-k8s.io/node\",\n+\t\t\tOperator: v1.TolerationOpEqual,\n+\t\t\tValue:    \"fake\",\n+\t\t\tEffect:   v1.TaintEffectNoSchedule,\n+\t\t},\n+\t}\n+)\n+\n+var _ = Describe(\"Network Topology Tests\", func() {\n+\tBeforeEach(func() {\n+\t\tctx = e2eutil.InitTestContext(e2eutil.Options{\n+\t\t\tNodesNumLimit: 8,\n+\t\t})\n+\n+\t\t// Setup the 3-tier topology structure\n+\t\t//                          s6\n+\t\t//                   /              \\\n+\t\t//                 s4               s5\n+\t\t//              /     \\          /     \\\n+\t\t//            s0      s1       s2       s3\n+\t\t//          /  \\     /  \\     /  \\     /  \\\n+\t\t//         n0  n1   n2  n3   n4  n5   n6  n7\n+\t\t//\n+\t\tBy(\"Setup 3-tier hypernodes\")\n+\t\thyperNodes := []struct {\n+\t\t\tname  string\n+\t\t\tnodes []string\n+\t\t\ttier  int\n+\t\t}{\n+\t\t\t// Tier-1\n+\t\t\t{\"s0\", []string{\"kwok-node-0\", \"kwok-node-1\"}, 1},\n+\t\t\t{\"s1\", []string{\"kwok-node-2\", \"kwok-node-3\"}, 1},\n+\t\t\t{\"s2\", []string{\"kwok-node-4\", \"kwok-node-5\"}, 1},\n+\t\t\t{\"s3\", []string{\"kwok-node-6\", \"kwok-node-7\"}, 1},\n+\t\t\t// Tier-2\n+\t\t\t{\"s4\", []string{\"s0\", \"s1\"}, 2},\n+\t\t\t{\"s5\", []string{\"s2\", \"s3\"}, 2},\n+\t\t\t// Tier-3\n+\t\t\t{\"s6\", []string{\"s4\", \"s5\"}, 3},\n+\t\t}\n+\n+\t\tfor _, hn := range hyperNodes {\n+\t\t\thyperNodeType := topologyv1alpha1.MemberTypeHyperNode\n+\t\t\tif hn.tier == 1 {\n+\t\t\t\thyperNodeType = topologyv1alpha1.MemberTypeNode\n+\t\t\t}\n+\t\t\tspec := &topologyv1alpha1.HyperNode{\n+\t\t\t\tObjectMeta: metav1.ObjectMeta{\n+\t\t\t\t\tName: hn.name,\n+\t\t\t\t},\n+\t\t\t\tSpec: topologyv1alpha1.HyperNodeSpec{\n+\t\t\t\t\tTier: hn.tier,\n+\t\t\t\t\tMembers: []topologyv1alpha1.MemberSpec{\n+\t\t\t\t\t\t{\n+\t\t\t\t\t\t\tType: hyperNodeType,\n+\t\t\t\t\t\t\tSelector: topologyv1alpha1.MemberSelector{\n+\t\t\t\t\t\t\t\tExactMatch: &topologyv1alpha1.ExactMatch{\n+\t\t\t\t\t\t\t\t\tName: hn.nodes[0],\n+\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\t{\n+\t\t\t\t\t\t\tType: hyperNodeType,\n+\t\t\t\t\t\t\tSelector: topologyv1alpha1.MemberSelector{\n+\t\t\t\t\t\t\t\tExactMatch: &topologyv1alpha1.ExactMatch{\n+\t\t\t\t\t\t\t\t\tName: hn.nodes[1],\n+\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\n+\t\t\terr = e2eutil.SetupHyperNode(ctx, spec)\n+\t\t\tExpect(err).NotTo(HaveOccurred())\n+\t\t}\n+\n+\t\t// Wait for all hypernodes to be ready\n+\t\tBy(\"Wait for hypernodes to be ready\")\n+\t\tfor _, hn := range hyperNodes {\n+\t\t\tEventually(func() error {\n+\t\t\t\t_, err = ctx.Vcclient.TopologyV1alpha1().HyperNodes().Get(context.TODO(), hn.name, metav1.GetOptions{})\n+\t\t\t\treturn err\n+\t\t\t}, 30*time.Second, time.Second).Should(BeNil())\n+\t\t}\n+\t})\n+\n+\tAfterEach(func() {\n+\t\te2eutil.CleanupTestContext(ctx)\n+\t})\n+\n+\tContext(\"Hard Mode Tests\", func() {\n+\t\tIt(\"Case 1.1: Schedule to node-2 and node-3 when resources are enough\", func() {\n+\t\t\tBy(\"Create job that fits in s1's resources\")\n+\n+\t\t\t// schedule pod to s1 (node-2 and node-3) to make sure the s1's binpack score is higher\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-2\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-3\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to s1\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-2\", \"kwok-node-3\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 1.2: Schedule to s2 when s1's resources are insufficient and s2's score is higher\", func() {\n+\t\t\tBy(\"Create job that fits in s1's resources\")\n+\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// allocate to s1(kwok-node-2, kwok-node-3) to make s1 has insufficient resources\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-3\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t// allocate to s2(kwok-node-4, kwok-node-5) to make s2 has enough resources and higher score\n+\t\t\t\t{Name: \"pod-2\", Node: \"kwok-node-4\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to s2\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-4\", \"kwok-node-5\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 1.3: Pods remain pending when no hypernode has sufficient resources\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-2\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-3\", Node: \"kwok-node-3\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-4\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-5\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-6\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-7\", Node: \"kwok-node-7\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Verify pods are pending\")\n+\t\t\tExpect(e2eutil.WaitTaskPhase(ctx, topologyJob, []v1.PodPhase{v1.PodPending}, 2)).NotTo(HaveOccurred())\n+\t\t})\n+\t})\n+\n+\tContext(\"Tier Tests\", func() {\n+\t\tIt(\"Case 2.1: Schedule to tier 2 when tier 1 resources are insufficient\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// make sure tier 1 has insufficient resources\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-2\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-3\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t// make sure tier 2 s5 has enough resources and higher score\n+\t\t\t\t{Name: \"pod-4\", Node: \"kwok-node-5\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(2),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to tier 2 s5\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-5\", \"kwok-node-7\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 2.2: pods are pending when tier 2 resources are insufficient\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// make sure tier 1 and tier 2 has insufficient resources, tier 3 has enough resources\n+\t\t\t\t{Name: \"pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-1\", Node: \"kwok-node-1\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-2\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-3\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-4\", Node: \"kwok-node-5\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t\t{Name: \"pod-5\", Node: \"kwok-node-6\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\tExpect(ctx.Kubeclient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(2),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\tExpect(ctx.Vcclient.BatchV1alpha1().Jobs(topologyJob.Namespace).Delete(context.TODO(), topologyJob.Name, metav1.DeleteOptions{})).NotTo(HaveOccurred())\n+\t\t\t}()\n+\n+\t\t\tBy(\"Verify pods are pending\")\n+\t\t\tExpect(e2eutil.WaitTaskPhase(ctx, topologyJob, []v1.PodPhase{v1.PodPending}, 2)).NotTo(HaveOccurred())\n+\t\t})\n+\t})\n+\n+\tContext(\"Soft Mode Tests\", func() {\n+\t\tIt(\"Case 3: Schedule to single hypernode in soft mode\", func() {",
        "comment_created_at": "2025-05-20T03:27:59+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "If you need to test soft mode, you should also open the `network-topology-aware` plugin in https://github.com/volcano-sh/volcano/blob/network-topology/installer/helm/chart/volcano/config/volcano-scheduler-ci.conf",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2161600565",
    "pr_number": 3983,
    "pr_file": "test/e2e/hypernode/network_topology_test.go",
    "created_at": "2025-06-23T13:09:29+00:00",
    "commented_code": "+package hypernode\n+\n+import (\n+\t\"context\"\n+\t\"time\"\n+\n+\t. \"github.com/onsi/ginkgo/v2\"\n+\t. \"github.com/onsi/gomega\"\n+\tv1 \"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+\t\"k8s.io/utils/ptr\"\n+\n+\tbatchv1alpha1 \"volcano.sh/apis/pkg/apis/batch/v1alpha1\"\n+\ttopologyv1alpha1 \"volcano.sh/apis/pkg/apis/topology/v1alpha1\"\n+\te2eutil \"volcano.sh/volcano/test/e2e/util\"\n+)\n+\n+var (\n+\terr error\n+\tctx *e2eutil.TestContext\n+\n+\ttolerations = []v1.Toleration{\n+\t\t{\n+\t\t\tKey:      \"kwok.x-k8s.io/node\",\n+\t\t\tOperator: v1.TolerationOpEqual,\n+\t\t\tValue:    \"fake\",\n+\t\t\tEffect:   v1.TaintEffectNoSchedule,\n+\t\t},\n+\t}\n+)\n+\n+var _ = Describe(\"Network Topology Tests\", func() {\n+\tBeforeEach(func() {\n+\t\tctx = e2eutil.InitTestContext(e2eutil.Options{\n+\t\t\tNodesNumLimit: 8,\n+\t\t})\n+\n+\t\t// Setup the 3-tier topology structure\n+\t\t//                          s6\n+\t\t//                   /              \\\n+\t\t//                 s4               s5\n+\t\t//              /     \\          /     \\\n+\t\t//            s0      s1       s2       s3\n+\t\t//          /  \\     /  \\     /  \\     /  \\\n+\t\t//         n0  n1   n2  n3   n4  n5   n6  n7\n+\t\t//\n+\t\tBy(\"Setup 3-tier hypernodes\")\n+\t\thyperNodes := []struct {\n+\t\t\tname  string\n+\t\t\tnodes []string\n+\t\t\ttier  int\n+\t\t}{\n+\t\t\t// Tier-1\n+\t\t\t{\"s0\", []string{\"kwok-node-0\", \"kwok-node-1\"}, 1},\n+\t\t\t{\"s1\", []string{\"kwok-node-2\", \"kwok-node-3\"}, 1},\n+\t\t\t{\"s2\", []string{\"kwok-node-4\", \"kwok-node-5\"}, 1},\n+\t\t\t{\"s3\", []string{\"kwok-node-6\", \"kwok-node-7\"}, 1},\n+\t\t\t// Tier-2\n+\t\t\t{\"s4\", []string{\"s0\", \"s1\"}, 2},\n+\t\t\t{\"s5\", []string{\"s2\", \"s3\"}, 2},\n+\t\t\t// Tier-3\n+\t\t\t{\"s6\", []string{\"s4\", \"s5\"}, 3},\n+\t\t}\n+\n+\t\tfor _, hn := range hyperNodes {\n+\t\t\thyperNodeType := topologyv1alpha1.MemberTypeHyperNode\n+\t\t\tif hn.tier == 1 {\n+\t\t\t\thyperNodeType = topologyv1alpha1.MemberTypeNode\n+\t\t\t}\n+\t\t\tspec := &topologyv1alpha1.HyperNode{\n+\t\t\t\tObjectMeta: metav1.ObjectMeta{\n+\t\t\t\t\tName: hn.name,\n+\t\t\t\t},\n+\t\t\t\tSpec: topologyv1alpha1.HyperNodeSpec{\n+\t\t\t\t\tTier: hn.tier,\n+\t\t\t\t\tMembers: []topologyv1alpha1.MemberSpec{\n+\t\t\t\t\t\t{\n+\t\t\t\t\t\t\tType: hyperNodeType,\n+\t\t\t\t\t\t\tSelector: topologyv1alpha1.MemberSelector{\n+\t\t\t\t\t\t\t\tExactMatch: &topologyv1alpha1.ExactMatch{\n+\t\t\t\t\t\t\t\t\tName: hn.nodes[0],\n+\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\t{\n+\t\t\t\t\t\t\tType: hyperNodeType,\n+\t\t\t\t\t\t\tSelector: topologyv1alpha1.MemberSelector{\n+\t\t\t\t\t\t\t\tExactMatch: &topologyv1alpha1.ExactMatch{\n+\t\t\t\t\t\t\t\t\tName: hn.nodes[1],\n+\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\n+\t\t\terr = e2eutil.SetupHyperNode(ctx, spec)\n+\t\t\tExpect(err).NotTo(HaveOccurred())\n+\t\t}\n+\n+\t\t// Wait for all hypernodes to be ready\n+\t\tBy(\"Wait for hypernodes to be ready\")\n+\t\tfor _, hn := range hyperNodes {\n+\t\t\tEventually(func() error {\n+\t\t\t\t_, err = ctx.Vcclient.TopologyV1alpha1().HyperNodes().Get(context.TODO(), hn.name, metav1.GetOptions{})\n+\t\t\t\treturn err\n+\t\t\t}, 30*time.Second, time.Second).Should(BeNil())\n+\t\t}\n+\t})\n+\n+\tAfterEach(func() {\n+\t\te2eutil.CleanupTestContext(ctx)\n+\t})\n+\n+\tContext(\"Hard Mode Tests\", func() {\n+\t\tIt(\"Case 1.1: Schedule to node-2 and node-3 when resources are enough\", func() {\n+\t\t\tBy(\"Create job that fits in s1's resources\")\n+\n+\t\t\t// schedule pod to s1 (node-2 and node-3) to make sure the s1's binpack score is higher\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t{Name: \"case-1-1-pod-0\", Node: \"kwok-node-2\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-1-pod-1\", Node: \"kwok-node-3\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\te2eutil.DeletePod(ctx, pod)\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\te2eutil.DeleteJob(ctx, topologyJob)\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to s1\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-2\", \"kwok-node-3\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 1.2: Schedule to s2 when s1's resources are insufficient and s2's score is higher\", func() {\n+\t\t\tBy(\"Create job that fits in s1's resources\")\n+\t\t\t// s0 has enough resources, s1 has insufficient resources, s2 has enough resources and higher score\n+\t\t\t// pods should be scheduled to s2\n+\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// allocate to s1(kwok-node-2, kwok-node-3) to make s1 has insufficient resources\n+\t\t\t\t{Name: \"case-1-2-pod-0\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-2-pod-1\", Node: \"kwok-node-3\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t// allocate to s2(kwok-node-4, kwok-node-5) to make s2 has enough resources and higher score\n+\t\t\t\t{Name: \"case-1-2-pod-2\", Node: \"kwok-node-4\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\te2eutil.DeletePod(ctx, pod)\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1-2\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1-2\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\te2eutil.DeleteJob(ctx, topologyJob)\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to s2\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-4\", \"kwok-node-5\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 1.3: Pods remain pending when no hypernode has sufficient resources\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t{Name: \"case-1-3-pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-1\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-2\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-3\", Node: \"kwok-node-3\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-4\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-5\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-6\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-7\", Node: \"kwok-node-7\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\te2eutil.DeletePod(ctx, pod)\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1-3\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1-3\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\te2eutil.DeleteJob(ctx, topologyJob)\n+\t\t\t}()\n+\n+\t\t\tBy(\"Verify pods are pending\")\n+\t\t\tExpect(e2eutil.WaitTaskPhase(ctx, topologyJob, []v1.PodPhase{v1.PodPending}, 2)).NotTo(HaveOccurred())\n+\t\t})\n+\t})\n+\n+\tContext(\"Tier Tests\", func() {\n+\t\tIt(\"Case 2.1: Schedule to tier 2 when tier 1 resources are insufficient\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// make sure tier 1 has insufficient resources\n+\t\t\t\t{Name: \"case-2-1-pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-2-1-pod-1\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-2-1-pod-2\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-2-1-pod-3\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t// make sure tier 2 s5 has enough resources and higher score\n+\t\t\t\t{Name: \"case-2-1-pod-4\", Node: \"kwok-node-5\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\te2eutil.DeletePod(ctx, pod)\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-2-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(2),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-2-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\te2eutil.DeleteJob(ctx, topologyJob)\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to tier 2 s5\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-5\", \"kwok-node-7\"})).NotTo(HaveOccurred())",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2161600565",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3983,
        "pr_file": "test/e2e/hypernode/network_topology_test.go",
        "discussion_id": "2161600565",
        "commented_code": "@@ -0,0 +1,530 @@\n+package hypernode\n+\n+import (\n+\t\"context\"\n+\t\"time\"\n+\n+\t. \"github.com/onsi/ginkgo/v2\"\n+\t. \"github.com/onsi/gomega\"\n+\tv1 \"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+\t\"k8s.io/utils/ptr\"\n+\n+\tbatchv1alpha1 \"volcano.sh/apis/pkg/apis/batch/v1alpha1\"\n+\ttopologyv1alpha1 \"volcano.sh/apis/pkg/apis/topology/v1alpha1\"\n+\te2eutil \"volcano.sh/volcano/test/e2e/util\"\n+)\n+\n+var (\n+\terr error\n+\tctx *e2eutil.TestContext\n+\n+\ttolerations = []v1.Toleration{\n+\t\t{\n+\t\t\tKey:      \"kwok.x-k8s.io/node\",\n+\t\t\tOperator: v1.TolerationOpEqual,\n+\t\t\tValue:    \"fake\",\n+\t\t\tEffect:   v1.TaintEffectNoSchedule,\n+\t\t},\n+\t}\n+)\n+\n+var _ = Describe(\"Network Topology Tests\", func() {\n+\tBeforeEach(func() {\n+\t\tctx = e2eutil.InitTestContext(e2eutil.Options{\n+\t\t\tNodesNumLimit: 8,\n+\t\t})\n+\n+\t\t// Setup the 3-tier topology structure\n+\t\t//                          s6\n+\t\t//                   /              \\\n+\t\t//                 s4               s5\n+\t\t//              /     \\          /     \\\n+\t\t//            s0      s1       s2       s3\n+\t\t//          /  \\     /  \\     /  \\     /  \\\n+\t\t//         n0  n1   n2  n3   n4  n5   n6  n7\n+\t\t//\n+\t\tBy(\"Setup 3-tier hypernodes\")\n+\t\thyperNodes := []struct {\n+\t\t\tname  string\n+\t\t\tnodes []string\n+\t\t\ttier  int\n+\t\t}{\n+\t\t\t// Tier-1\n+\t\t\t{\"s0\", []string{\"kwok-node-0\", \"kwok-node-1\"}, 1},\n+\t\t\t{\"s1\", []string{\"kwok-node-2\", \"kwok-node-3\"}, 1},\n+\t\t\t{\"s2\", []string{\"kwok-node-4\", \"kwok-node-5\"}, 1},\n+\t\t\t{\"s3\", []string{\"kwok-node-6\", \"kwok-node-7\"}, 1},\n+\t\t\t// Tier-2\n+\t\t\t{\"s4\", []string{\"s0\", \"s1\"}, 2},\n+\t\t\t{\"s5\", []string{\"s2\", \"s3\"}, 2},\n+\t\t\t// Tier-3\n+\t\t\t{\"s6\", []string{\"s4\", \"s5\"}, 3},\n+\t\t}\n+\n+\t\tfor _, hn := range hyperNodes {\n+\t\t\thyperNodeType := topologyv1alpha1.MemberTypeHyperNode\n+\t\t\tif hn.tier == 1 {\n+\t\t\t\thyperNodeType = topologyv1alpha1.MemberTypeNode\n+\t\t\t}\n+\t\t\tspec := &topologyv1alpha1.HyperNode{\n+\t\t\t\tObjectMeta: metav1.ObjectMeta{\n+\t\t\t\t\tName: hn.name,\n+\t\t\t\t},\n+\t\t\t\tSpec: topologyv1alpha1.HyperNodeSpec{\n+\t\t\t\t\tTier: hn.tier,\n+\t\t\t\t\tMembers: []topologyv1alpha1.MemberSpec{\n+\t\t\t\t\t\t{\n+\t\t\t\t\t\t\tType: hyperNodeType,\n+\t\t\t\t\t\t\tSelector: topologyv1alpha1.MemberSelector{\n+\t\t\t\t\t\t\t\tExactMatch: &topologyv1alpha1.ExactMatch{\n+\t\t\t\t\t\t\t\t\tName: hn.nodes[0],\n+\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\t{\n+\t\t\t\t\t\t\tType: hyperNodeType,\n+\t\t\t\t\t\t\tSelector: topologyv1alpha1.MemberSelector{\n+\t\t\t\t\t\t\t\tExactMatch: &topologyv1alpha1.ExactMatch{\n+\t\t\t\t\t\t\t\t\tName: hn.nodes[1],\n+\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\n+\t\t\terr = e2eutil.SetupHyperNode(ctx, spec)\n+\t\t\tExpect(err).NotTo(HaveOccurred())\n+\t\t}\n+\n+\t\t// Wait for all hypernodes to be ready\n+\t\tBy(\"Wait for hypernodes to be ready\")\n+\t\tfor _, hn := range hyperNodes {\n+\t\t\tEventually(func() error {\n+\t\t\t\t_, err = ctx.Vcclient.TopologyV1alpha1().HyperNodes().Get(context.TODO(), hn.name, metav1.GetOptions{})\n+\t\t\t\treturn err\n+\t\t\t}, 30*time.Second, time.Second).Should(BeNil())\n+\t\t}\n+\t})\n+\n+\tAfterEach(func() {\n+\t\te2eutil.CleanupTestContext(ctx)\n+\t})\n+\n+\tContext(\"Hard Mode Tests\", func() {\n+\t\tIt(\"Case 1.1: Schedule to node-2 and node-3 when resources are enough\", func() {\n+\t\t\tBy(\"Create job that fits in s1's resources\")\n+\n+\t\t\t// schedule pod to s1 (node-2 and node-3) to make sure the s1's binpack score is higher\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t{Name: \"case-1-1-pod-0\", Node: \"kwok-node-2\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-1-pod-1\", Node: \"kwok-node-3\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\te2eutil.DeletePod(ctx, pod)\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\te2eutil.DeleteJob(ctx, topologyJob)\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to s1\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-2\", \"kwok-node-3\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 1.2: Schedule to s2 when s1's resources are insufficient and s2's score is higher\", func() {\n+\t\t\tBy(\"Create job that fits in s1's resources\")\n+\t\t\t// s0 has enough resources, s1 has insufficient resources, s2 has enough resources and higher score\n+\t\t\t// pods should be scheduled to s2\n+\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// allocate to s1(kwok-node-2, kwok-node-3) to make s1 has insufficient resources\n+\t\t\t\t{Name: \"case-1-2-pod-0\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-2-pod-1\", Node: \"kwok-node-3\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t// allocate to s2(kwok-node-4, kwok-node-5) to make s2 has enough resources and higher score\n+\t\t\t\t{Name: \"case-1-2-pod-2\", Node: \"kwok-node-4\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\te2eutil.DeletePod(ctx, pod)\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1-2\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1-2\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\te2eutil.DeleteJob(ctx, topologyJob)\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to s2\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-4\", \"kwok-node-5\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 1.3: Pods remain pending when no hypernode has sufficient resources\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t{Name: \"case-1-3-pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-1\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-2\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-3\", Node: \"kwok-node-3\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-4\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-5\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-6\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-7\", Node: \"kwok-node-7\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\te2eutil.DeletePod(ctx, pod)\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1-3\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1-3\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\te2eutil.DeleteJob(ctx, topologyJob)\n+\t\t\t}()\n+\n+\t\t\tBy(\"Verify pods are pending\")\n+\t\t\tExpect(e2eutil.WaitTaskPhase(ctx, topologyJob, []v1.PodPhase{v1.PodPending}, 2)).NotTo(HaveOccurred())\n+\t\t})\n+\t})\n+\n+\tContext(\"Tier Tests\", func() {\n+\t\tIt(\"Case 2.1: Schedule to tier 2 when tier 1 resources are insufficient\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// make sure tier 1 has insufficient resources\n+\t\t\t\t{Name: \"case-2-1-pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-2-1-pod-1\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-2-1-pod-2\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-2-1-pod-3\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t// make sure tier 2 s5 has enough resources and higher score\n+\t\t\t\t{Name: \"case-2-1-pod-4\", Node: \"kwok-node-5\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\te2eutil.DeletePod(ctx, pod)\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-2-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(2),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-2-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\te2eutil.DeleteJob(ctx, topologyJob)\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to tier 2 s5\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-5\", \"kwok-node-7\"})).NotTo(HaveOccurred())",
        "comment_created_at": "2025-06-23T13:09:29+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "How do we ensure that this job spans two layers? kwok-node-5 only consumes 1U1G, and this job requests 4U4G. Shouldn't it be scheduled to only kwok-node-5? This does not reflect that this job spans two layers. We need to specify more replicas, right?",
        "pr_file_module": null
      },
      {
        "comment_id": "2165217598",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3983,
        "pr_file": "test/e2e/hypernode/network_topology_test.go",
        "discussion_id": "2161600565",
        "commented_code": "@@ -0,0 +1,530 @@\n+package hypernode\n+\n+import (\n+\t\"context\"\n+\t\"time\"\n+\n+\t. \"github.com/onsi/ginkgo/v2\"\n+\t. \"github.com/onsi/gomega\"\n+\tv1 \"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+\t\"k8s.io/utils/ptr\"\n+\n+\tbatchv1alpha1 \"volcano.sh/apis/pkg/apis/batch/v1alpha1\"\n+\ttopologyv1alpha1 \"volcano.sh/apis/pkg/apis/topology/v1alpha1\"\n+\te2eutil \"volcano.sh/volcano/test/e2e/util\"\n+)\n+\n+var (\n+\terr error\n+\tctx *e2eutil.TestContext\n+\n+\ttolerations = []v1.Toleration{\n+\t\t{\n+\t\t\tKey:      \"kwok.x-k8s.io/node\",\n+\t\t\tOperator: v1.TolerationOpEqual,\n+\t\t\tValue:    \"fake\",\n+\t\t\tEffect:   v1.TaintEffectNoSchedule,\n+\t\t},\n+\t}\n+)\n+\n+var _ = Describe(\"Network Topology Tests\", func() {\n+\tBeforeEach(func() {\n+\t\tctx = e2eutil.InitTestContext(e2eutil.Options{\n+\t\t\tNodesNumLimit: 8,\n+\t\t})\n+\n+\t\t// Setup the 3-tier topology structure\n+\t\t//                          s6\n+\t\t//                   /              \\\n+\t\t//                 s4               s5\n+\t\t//              /     \\          /     \\\n+\t\t//            s0      s1       s2       s3\n+\t\t//          /  \\     /  \\     /  \\     /  \\\n+\t\t//         n0  n1   n2  n3   n4  n5   n6  n7\n+\t\t//\n+\t\tBy(\"Setup 3-tier hypernodes\")\n+\t\thyperNodes := []struct {\n+\t\t\tname  string\n+\t\t\tnodes []string\n+\t\t\ttier  int\n+\t\t}{\n+\t\t\t// Tier-1\n+\t\t\t{\"s0\", []string{\"kwok-node-0\", \"kwok-node-1\"}, 1},\n+\t\t\t{\"s1\", []string{\"kwok-node-2\", \"kwok-node-3\"}, 1},\n+\t\t\t{\"s2\", []string{\"kwok-node-4\", \"kwok-node-5\"}, 1},\n+\t\t\t{\"s3\", []string{\"kwok-node-6\", \"kwok-node-7\"}, 1},\n+\t\t\t// Tier-2\n+\t\t\t{\"s4\", []string{\"s0\", \"s1\"}, 2},\n+\t\t\t{\"s5\", []string{\"s2\", \"s3\"}, 2},\n+\t\t\t// Tier-3\n+\t\t\t{\"s6\", []string{\"s4\", \"s5\"}, 3},\n+\t\t}\n+\n+\t\tfor _, hn := range hyperNodes {\n+\t\t\thyperNodeType := topologyv1alpha1.MemberTypeHyperNode\n+\t\t\tif hn.tier == 1 {\n+\t\t\t\thyperNodeType = topologyv1alpha1.MemberTypeNode\n+\t\t\t}\n+\t\t\tspec := &topologyv1alpha1.HyperNode{\n+\t\t\t\tObjectMeta: metav1.ObjectMeta{\n+\t\t\t\t\tName: hn.name,\n+\t\t\t\t},\n+\t\t\t\tSpec: topologyv1alpha1.HyperNodeSpec{\n+\t\t\t\t\tTier: hn.tier,\n+\t\t\t\t\tMembers: []topologyv1alpha1.MemberSpec{\n+\t\t\t\t\t\t{\n+\t\t\t\t\t\t\tType: hyperNodeType,\n+\t\t\t\t\t\t\tSelector: topologyv1alpha1.MemberSelector{\n+\t\t\t\t\t\t\t\tExactMatch: &topologyv1alpha1.ExactMatch{\n+\t\t\t\t\t\t\t\t\tName: hn.nodes[0],\n+\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\t{\n+\t\t\t\t\t\t\tType: hyperNodeType,\n+\t\t\t\t\t\t\tSelector: topologyv1alpha1.MemberSelector{\n+\t\t\t\t\t\t\t\tExactMatch: &topologyv1alpha1.ExactMatch{\n+\t\t\t\t\t\t\t\t\tName: hn.nodes[1],\n+\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\n+\t\t\terr = e2eutil.SetupHyperNode(ctx, spec)\n+\t\t\tExpect(err).NotTo(HaveOccurred())\n+\t\t}\n+\n+\t\t// Wait for all hypernodes to be ready\n+\t\tBy(\"Wait for hypernodes to be ready\")\n+\t\tfor _, hn := range hyperNodes {\n+\t\t\tEventually(func() error {\n+\t\t\t\t_, err = ctx.Vcclient.TopologyV1alpha1().HyperNodes().Get(context.TODO(), hn.name, metav1.GetOptions{})\n+\t\t\t\treturn err\n+\t\t\t}, 30*time.Second, time.Second).Should(BeNil())\n+\t\t}\n+\t})\n+\n+\tAfterEach(func() {\n+\t\te2eutil.CleanupTestContext(ctx)\n+\t})\n+\n+\tContext(\"Hard Mode Tests\", func() {\n+\t\tIt(\"Case 1.1: Schedule to node-2 and node-3 when resources are enough\", func() {\n+\t\t\tBy(\"Create job that fits in s1's resources\")\n+\n+\t\t\t// schedule pod to s1 (node-2 and node-3) to make sure the s1's binpack score is higher\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t{Name: \"case-1-1-pod-0\", Node: \"kwok-node-2\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-1-pod-1\", Node: \"kwok-node-3\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\te2eutil.DeletePod(ctx, pod)\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\te2eutil.DeleteJob(ctx, topologyJob)\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to s1\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-2\", \"kwok-node-3\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 1.2: Schedule to s2 when s1's resources are insufficient and s2's score is higher\", func() {\n+\t\t\tBy(\"Create job that fits in s1's resources\")\n+\t\t\t// s0 has enough resources, s1 has insufficient resources, s2 has enough resources and higher score\n+\t\t\t// pods should be scheduled to s2\n+\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// allocate to s1(kwok-node-2, kwok-node-3) to make s1 has insufficient resources\n+\t\t\t\t{Name: \"case-1-2-pod-0\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-2-pod-1\", Node: \"kwok-node-3\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t// allocate to s2(kwok-node-4, kwok-node-5) to make s2 has enough resources and higher score\n+\t\t\t\t{Name: \"case-1-2-pod-2\", Node: \"kwok-node-4\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\te2eutil.DeletePod(ctx, pod)\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1-2\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1-2\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\te2eutil.DeleteJob(ctx, topologyJob)\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to s2\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-4\", \"kwok-node-5\"})).NotTo(HaveOccurred())\n+\t\t})\n+\n+\t\tIt(\"Case 1.3: Pods remain pending when no hypernode has sufficient resources\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t{Name: \"case-1-3-pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-1\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-2\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-3\", Node: \"kwok-node-3\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-4\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-5\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-6\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-1-3-pod-7\", Node: \"kwok-node-7\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\te2eutil.DeletePod(ctx, pod)\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-1-3\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(1),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-1-3\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\te2eutil.DeleteJob(ctx, topologyJob)\n+\t\t\t}()\n+\n+\t\t\tBy(\"Verify pods are pending\")\n+\t\t\tExpect(e2eutil.WaitTaskPhase(ctx, topologyJob, []v1.PodPhase{v1.PodPending}, 2)).NotTo(HaveOccurred())\n+\t\t})\n+\t})\n+\n+\tContext(\"Tier Tests\", func() {\n+\t\tIt(\"Case 2.1: Schedule to tier 2 when tier 1 resources are insufficient\", func() {\n+\t\t\tpodSpecs := []e2eutil.PodSpec{\n+\t\t\t\t// make sure tier 1 has insufficient resources\n+\t\t\t\t{Name: \"case-2-1-pod-0\", Node: \"kwok-node-0\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-2-1-pod-1\", Node: \"kwok-node-2\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-2-1-pod-2\", Node: \"kwok-node-4\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t{Name: \"case-2-1-pod-3\", Node: \"kwok-node-6\", Req: e2eutil.CPU4Mem4, Tolerations: tolerations},\n+\t\t\t\t// make sure tier 2 s5 has enough resources and higher score\n+\t\t\t\t{Name: \"case-2-1-pod-4\", Node: \"kwok-node-5\", Req: e2eutil.CPU1Mem1, Tolerations: tolerations},\n+\t\t\t}\n+\n+\t\t\tpods := make([]*v1.Pod, len(podSpecs))\n+\t\t\tfor i, podSpec := range podSpecs {\n+\t\t\t\tpods[i] = e2eutil.CreatePod(ctx, podSpec)\n+\t\t\t}\n+\n+\t\t\tdefer func() {\n+\t\t\t\tfor _, pod := range pods {\n+\t\t\t\t\te2eutil.DeletePod(ctx, pod)\n+\t\t\t\t}\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for all pods to be ready\")\n+\t\t\tfor _, pod := range pods {\n+\t\t\t\tExpect(e2eutil.WaitPodReady(ctx, pod)).NotTo(HaveOccurred())\n+\t\t\t}\n+\n+\t\t\tjob := &e2eutil.JobSpec{\n+\t\t\t\tName: \"job-2-1\",\n+\t\t\t\tNetworkTopology: &batchv1alpha1.NetworkTopologySpec{\n+\t\t\t\t\tMode:               batchv1alpha1.HardNetworkTopologyMode,\n+\t\t\t\t\tHighestTierAllowed: ptr.To(2),\n+\t\t\t\t},\n+\t\t\t\tTasks: []e2eutil.TaskSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        \"task-2-1\",\n+\t\t\t\t\t\tImg:         e2eutil.DefaultNginxImage,\n+\t\t\t\t\t\tReq:         e2eutil.CPU2Mem2,\n+\t\t\t\t\t\tMin:         2,\n+\t\t\t\t\t\tRep:         2,\n+\t\t\t\t\t\tTolerations: tolerations,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}\n+\t\t\ttopologyJob := e2eutil.CreateJob(ctx, job)\n+\n+\t\t\tdefer func() {\n+\t\t\t\tBy(\"Delete job\")\n+\t\t\t\te2eutil.DeleteJob(ctx, topologyJob)\n+\t\t\t}()\n+\n+\t\t\tBy(\"Wait for job running\")\n+\t\t\tExpect(e2eutil.WaitJobReady(ctx, topologyJob)).NotTo(HaveOccurred())\n+\n+\t\t\tBy(\"Verify pods are scheduled to tier 2 s5\")\n+\t\t\tExpect(e2eutil.VerifyPodScheduling(ctx, topologyJob, []string{\"kwok-node-5\", \"kwok-node-7\"})).NotTo(HaveOccurred())",
        "comment_created_at": "2025-06-25T00:49:31+00:00",
        "comment_author": "Xu-Wentao",
        "comment_body": "> How do we ensure that this job spans two layers? kwok-node-5 only consumes 1U1G, and this job requests 4U4G. Shouldn't it be scheduled to only kwok-node-5? This does not reflect that this job spans two layers. We need to specify more replicas, right?\r\n\r\nFixed, i changed the job resource requests to 5C5G to make sure two pods of this job will be scheduled to different node within same tier2 hypernode.",
        "pr_file_module": null
      }
    ]
  }
]