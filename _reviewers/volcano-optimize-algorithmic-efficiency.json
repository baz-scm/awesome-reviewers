[
  {
    "discussion_id": "2250635576",
    "pr_number": 4512,
    "pr_file": "pkg/scheduler/plugins/resource-strategy-fit/resource_strategy_fit.go",
    "created_at": "2025-08-04T11:49:00+00:00",
    "commented_code": "}\n }\n \n+func isValidWildcardPattern(pattern string) bool {\n+\tif pattern == \"*\" {\n+\t\treturn false\n+\t}\n+\n+\tif !strings.HasSuffix(pattern, \"*\") {\n+\t\treturn false\n+\t}\n+\n+\tasteriskCount := strings.Count(pattern, \"*\")\n+\tif asteriskCount != 1 {\n+\t\treturn false\n+\t}\n+\n+\tlastIndex := strings.LastIndex(pattern, \"*\")\n+\treturn lastIndex == len(pattern)-1",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2251243350",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4512,
        "pr_file": "pkg/scheduler/plugins/resource-strategy-fit/resource_strategy_fit.go",
        "discussion_id": "2250635576",
        "commented_code": "@@ -137,6 +151,48 @@ func (rsf *resourceStrategyFitPlugin) OnSessionOpen(ssn *framework.Session) {\n \t}\n }\n \n+func isValidWildcardPattern(pattern string) bool {\n+\tif pattern == \"*\" {\n+\t\treturn false\n+\t}\n+\n+\tif !strings.HasSuffix(pattern, \"*\") {\n+\t\treturn false\n+\t}\n+\n+\tasteriskCount := strings.Count(pattern, \"*\")\n+\tif asteriskCount != 1 {\n+\t\treturn false\n+\t}\n+\n+\tlastIndex := strings.LastIndex(pattern, \"*\")\n+\treturn lastIndex == len(pattern)-1",
        "comment_created_at": "2025-08-04T11:49:00+00:00",
        "comment_author": "ditingdapeng",
        "comment_body": "Thank you for the review and suggestion.\r\n\r\nWhile the logic `strings.Count(pattern, \"*\") == 1` would indeed ensure there's only one asterisk, it doesn't guarantee that the asterisk is at the **end** of the string, which is crucial for our prefix matching implementation.\r\n\r\nConsider these examples:\r\n- `\"*gpu\"` - has exactly 1 asterisk but it's at the beginning\r\n- `\"gpu*memory\"` - has exactly 1 asterisk but it's in the middle\r\n- `\"gpu*\"` - has exactly 1 asterisk and it's at the end \u2713\r\n\r\nOur current implementation specifically supports **suffix wildcard matching** (prefix matching with trailing asterisk) as mentioned in the issue discussion. The position check `lastIndex == len(pattern)-1` is essential to ensure we only accept patterns like `\"cloudml.gpu*\"` and reject patterns like `\"*gpu\"` or `\"gpu*memory\"`.\r\n\r\nThe current validation logic is intentionally designed this way to:\r\n1. Ensure exactly one asterisk exists\r\n2. Ensure that asterisk is at the end (enabling prefix matching)\r\n3. Maintain clear semantics for the wildcard feature\r\n\r\nIf we want to optimize, we could use `strings.HasSuffix(pattern, \"*\")` instead of `strings.LastIndex`, but the position validation itself should remain.",
        "pr_file_module": null
      },
      {
        "comment_id": "2275594867",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4512,
        "pr_file": "pkg/scheduler/plugins/resource-strategy-fit/resource_strategy_fit.go",
        "discussion_id": "2250635576",
        "commented_code": "@@ -137,6 +151,48 @@ func (rsf *resourceStrategyFitPlugin) OnSessionOpen(ssn *framework.Session) {\n \t}\n }\n \n+func isValidWildcardPattern(pattern string) bool {\n+\tif pattern == \"*\" {\n+\t\treturn false\n+\t}\n+\n+\tif !strings.HasSuffix(pattern, \"*\") {\n+\t\treturn false\n+\t}\n+\n+\tasteriskCount := strings.Count(pattern, \"*\")\n+\tif asteriskCount != 1 {\n+\t\treturn false\n+\t}\n+\n+\tlastIndex := strings.LastIndex(pattern, \"*\")\n+\treturn lastIndex == len(pattern)-1",
        "comment_created_at": "2025-08-14T06:18:59+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "The meaning of gemini is like this:\r\n```\r\n    if !strings.HasSuffix(pattern, \"*\") {\r\n\t\treturn false\r\n\t}\r\n\r\n\treturn strings.Count(pattern, \"*\") == 1\r\n```\r\nThis already ensures that * needs to be at the end and can only have one",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2300240436",
    "pr_number": 4562,
    "pr_file": "pkg/scheduler/plugins/capacity/capacity.go",
    "created_at": "2025-08-26T08:35:38+00:00",
    "commented_code": "ancestors []api.QueueID\n \tchildren  map[api.QueueID]*queueAttr\n \n-\tdeserved  *api.Resource\n-\tallocated *api.Resource\n-\trequest   *api.Resource\n+\tdeserved                               *api.Resource\n+\tallocated                              *api.Resource\n+\tallocatedWithoutSchedulingDisabledNode *api.Resource",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2300240436",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4562,
        "pr_file": "pkg/scheduler/plugins/capacity/capacity.go",
        "discussion_id": "2300240436",
        "commented_code": "@@ -61,9 +62,10 @@ type queueAttr struct {\n \tancestors []api.QueueID\n \tchildren  map[api.QueueID]*queueAttr\n \n-\tdeserved  *api.Resource\n-\tallocated *api.Resource\n-\trequest   *api.Resource\n+\tdeserved                               *api.Resource\n+\tallocated                              *api.Resource\n+\tallocatedWithoutSchedulingDisabledNode *api.Resource",
        "comment_created_at": "2025-08-26T08:35:38+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "Why should we add a new field rather than just directly modify `allocated`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2302780200",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4562,
        "pr_file": "pkg/scheduler/plugins/capacity/capacity.go",
        "discussion_id": "2300240436",
        "commented_code": "@@ -61,9 +62,10 @@ type queueAttr struct {\n \tancestors []api.QueueID\n \tchildren  map[api.QueueID]*queueAttr\n \n-\tdeserved  *api.Resource\n-\tallocated *api.Resource\n-\trequest   *api.Resource\n+\tdeserved                               *api.Resource\n+\tallocated                              *api.Resource\n+\tallocatedWithoutSchedulingDisabledNode *api.Resource",
        "comment_created_at": "2025-08-27T04:04:07+00:00",
        "comment_author": "dafu-wu",
        "comment_body": "Updated, reused allocated",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2144124910",
    "pr_number": 4373,
    "pr_file": "pkg/scheduler/framework/session.go",
    "created_at": "2025-06-13T03:09:50+00:00",
    "commented_code": "ssn.NamespaceInfo = snapshot.NamespaceInfo\n \t// calculate all nodes' resource only once in each schedule cycle, other plugins can clone it when need\n \tfor _, n := range ssn.Nodes {\n+\t\tstatus := getNodeStatus(n.Node)\n+\n+\t\tif len(status) != 0 || (len(status) == 1 && !slices.Contains(status, string(v1.NodeReady))) {\n+\t\t\tklog.V(3).Infof(\"node %s is not ready,need continue\", n.Name)\n+\t\t\tcontinue",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2144124910",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4373,
        "pr_file": "pkg/scheduler/framework/session.go",
        "discussion_id": "2144124910",
        "commented_code": "@@ -211,6 +212,13 @@ func openSession(cache cache.Cache) *Session {\n \tssn.NamespaceInfo = snapshot.NamespaceInfo\n \t// calculate all nodes' resource only once in each schedule cycle, other plugins can clone it when need\n \tfor _, n := range ssn.Nodes {\n+\t\tstatus := getNodeStatus(n.Node)\n+\n+\t\tif len(status) != 0 || (len(status) == 1 && !slices.Contains(status, string(v1.NodeReady))) {\n+\t\t\tklog.V(3).Infof(\"node %s is not ready,need continue\", n.Name)\n+\t\t\tcontinue",
        "comment_created_at": "2025-06-13T03:09:50+00:00",
        "comment_author": "googs1025",
        "comment_body": "Why not directly determine the state, like: \ud83e\udd14 \r\n\r\n\r\n```go\r\n   if slices.Contains(status, \"NotReady\") || slices.Contains(status, \"Unknown\") {\r\n        klog.V(3).Infof(\"node %s is not ready, need continue\", n.Name)\r\n        continue\r\n    }\r\n```\r\n\r\nIs it possible that there is a Ready when `len(status) != 0` ? Will this cause all nodes to be skipped?",
        "pr_file_module": null
      },
      {
        "comment_id": "2144128083",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4373,
        "pr_file": "pkg/scheduler/framework/session.go",
        "discussion_id": "2144124910",
        "commented_code": "@@ -211,6 +212,13 @@ func openSession(cache cache.Cache) *Session {\n \tssn.NamespaceInfo = snapshot.NamespaceInfo\n \t// calculate all nodes' resource only once in each schedule cycle, other plugins can clone it when need\n \tfor _, n := range ssn.Nodes {\n+\t\tstatus := getNodeStatus(n.Node)\n+\n+\t\tif len(status) != 0 || (len(status) == 1 && !slices.Contains(status, string(v1.NodeReady))) {\n+\t\t\tklog.V(3).Infof(\"node %s is not ready,need continue\", n.Name)\n+\t\t\tcontinue",
        "comment_created_at": "2025-06-13T03:14:08+00:00",
        "comment_author": "LY-today",
        "comment_body": "I refer to the source code implementation of k8s for node status. I think that as long as the status returned is not the only Ready state, it will affect resource scheduling, so I implement it according to this logic. The online situation seems to be the same. The elimination method requires enumeration, so it is better to make a positive judgment.\r\n\r\n\r\n<img width=\"1590\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b5dddd5c-81c3-4394-8ae1-67e1af133abd\" />\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2144129502",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4373,
        "pr_file": "pkg/scheduler/framework/session.go",
        "discussion_id": "2144124910",
        "commented_code": "@@ -211,6 +212,13 @@ func openSession(cache cache.Cache) *Session {\n \tssn.NamespaceInfo = snapshot.NamespaceInfo\n \t// calculate all nodes' resource only once in each schedule cycle, other plugins can clone it when need\n \tfor _, n := range ssn.Nodes {\n+\t\tstatus := getNodeStatus(n.Node)\n+\n+\t\tif len(status) != 0 || (len(status) == 1 && !slices.Contains(status, string(v1.NodeReady))) {\n+\t\t\tklog.V(3).Infof(\"node %s is not ready,need continue\", n.Name)\n+\t\t\tcontinue",
        "comment_created_at": "2025-06-13T03:16:13+00:00",
        "comment_author": "LY-today",
        "comment_body": "> Why not directly determine the state, like: \ud83e\udd14\r\n> \r\n> ```go\r\n>    if slices.Contains(status, \"NotReady\") || slices.Contains(status, \"Unknown\") {\r\n>         klog.V(3).Infof(\"node %s is not ready, need continue\", n.Name)\r\n>         continue\r\n>     }\r\n> ```\r\n> \r\n> Is it possible that there is a Ready when `len(status) != 0` ? Will this cause all nodes to be skipped?\r\n\r\nFor example, the SchedulingDisabled state is not taken into account here, and various exceptions need to be enumerated. It is better to solve them positively.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2144136570",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4373,
        "pr_file": "pkg/scheduler/framework/session.go",
        "discussion_id": "2144124910",
        "commented_code": "@@ -211,6 +212,13 @@ func openSession(cache cache.Cache) *Session {\n \tssn.NamespaceInfo = snapshot.NamespaceInfo\n \t// calculate all nodes' resource only once in each schedule cycle, other plugins can clone it when need\n \tfor _, n := range ssn.Nodes {\n+\t\tstatus := getNodeStatus(n.Node)\n+\n+\t\tif len(status) != 0 || (len(status) == 1 && !slices.Contains(status, string(v1.NodeReady))) {\n+\t\t\tklog.V(3).Infof(\"node %s is not ready,need continue\", n.Name)\n+\t\t\tcontinue",
        "comment_created_at": "2025-06-13T03:27:19+00:00",
        "comment_author": "googs1025",
        "comment_body": "sgtm",
        "pr_file_module": null
      },
      {
        "comment_id": "2150000459",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4373,
        "pr_file": "pkg/scheduler/framework/session.go",
        "discussion_id": "2144124910",
        "commented_code": "@@ -211,6 +212,13 @@ func openSession(cache cache.Cache) *Session {\n \tssn.NamespaceInfo = snapshot.NamespaceInfo\n \t// calculate all nodes' resource only once in each schedule cycle, other plugins can clone it when need\n \tfor _, n := range ssn.Nodes {\n+\t\tstatus := getNodeStatus(n.Node)\n+\n+\t\tif len(status) != 0 || (len(status) == 1 && !slices.Contains(status, string(v1.NodeReady))) {\n+\t\t\tklog.V(3).Infof(\"node %s is not ready,need continue\", n.Name)\n+\t\t\tcontinue",
        "comment_created_at": "2025-06-16T13:22:14+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "I think we should learn from Kube-Scheduler's logic rather than from printer",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2020501779",
    "pr_number": 4162,
    "pr_file": "pkg/scheduler/actions/backfill/backfill.go",
    "created_at": "2025-03-31T07:06:47+00:00",
    "commented_code": null,
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2020501779",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4162,
        "pr_file": "pkg/scheduler/actions/backfill/backfill.go",
        "discussion_id": "2020501779",
        "commented_code": null,
        "comment_created_at": "2025-03-31T07:06:47+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "https://github.com/volcano-sh/volcano/blob/91c92e9d619538c4470a1fa2a46a734ad39c961a/pkg/scheduler/api/job_info.go#L770-L775\r\nThe pod is only refreshed with nominatedNodeName when marked as Pipeline in allocate action, but it is not marked as Pipeline state during backfill. Is it necessary to write these for Besteffort pods as well?",
        "pr_file_module": null
      },
      {
        "comment_id": "2020542861",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4162,
        "pr_file": "pkg/scheduler/actions/backfill/backfill.go",
        "discussion_id": "2020501779",
        "commented_code": null,
        "comment_created_at": "2025-03-31T07:43:20+00:00",
        "comment_author": "SataQiu",
        "comment_body": "The `nominatedNodeName` can be marked in `preempt` action. Besteffort Pods can also preempt the other Besteffort Pods with low priority.\r\nhttps://github.com/volcano-sh/volcano/blob/91c92e9d619538c4470a1fa2a46a734ad39c961a/pkg/scheduler/actions/preempt/preempt.go#L318-L319\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2020550239",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4162,
        "pr_file": "pkg/scheduler/actions/backfill/backfill.go",
        "discussion_id": "2020501779",
        "commented_code": null,
        "comment_created_at": "2025-03-31T07:47:57+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "OK, can we extract a unified func for these codes? Because it's duplicate with https://github.com/volcano-sh/volcano/pull/4079",
        "pr_file_module": null
      },
      {
        "comment_id": "2020647953",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4162,
        "pr_file": "pkg/scheduler/actions/backfill/backfill.go",
        "discussion_id": "2020501779",
        "commented_code": null,
        "comment_created_at": "2025-03-31T08:58:42+00:00",
        "comment_author": "SataQiu",
        "comment_body": "There seems to be some difference between the two implementations:\r\nallocate action should consider whether the node resources are satisfied `task.InitResreq.LessEqual(nominatedNodeInfo.Idle, api.Zero)`, whereas backfill does not need this check.\r\n\r\nhttps://github.com/volcano-sh/volcano/blob/91c92e9d619538c4470a1fa2a46a734ad39c961a/pkg/scheduler/actions/allocate/allocate.go#L224\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2024179477",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4162,
        "pr_file": "pkg/scheduler/actions/backfill/backfill.go",
        "discussion_id": "2020501779",
        "commented_code": null,
        "comment_created_at": "2025-04-02T06:53:17+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "Maybe this new public func can accept an additional callback function as an argument? Where backfill passes nil, but allocate carries these:\r\n```\r\ntask.InitResreq.LessEqual(nominatedNodeInfo.Idle, api.Zero) \r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2083107481",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4162,
        "pr_file": "pkg/scheduler/actions/backfill/backfill.go",
        "discussion_id": "2020501779",
        "commented_code": null,
        "comment_created_at": "2025-05-10T10:53:05+00:00",
        "comment_author": "SataQiu",
        "comment_body": "ready to be reviewed again @JesseStutler ",
        "pr_file_module": null
      },
      {
        "comment_id": "2091118594",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4162,
        "pr_file": "pkg/scheduler/actions/backfill/backfill.go",
        "discussion_id": "2020501779",
        "commented_code": null,
        "comment_created_at": "2025-05-15T12:59:41+00:00",
        "comment_author": "SataQiu",
        "comment_body": "kindly ping @JesseStutler ",
        "pr_file_module": null
      },
      {
        "comment_id": "2091137058",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4162,
        "pr_file": "pkg/scheduler/actions/backfill/backfill.go",
        "discussion_id": "2020501779",
        "commented_code": null,
        "comment_created_at": "2025-05-15T13:09:26+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "OK, I will take a look later, a little bit busy in recent days",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2086717589",
    "pr_number": 4279,
    "pr_file": "pkg/scheduler/actions/preempt/preempt.go",
    "created_at": "2025-05-13T12:36:21+00:00",
    "commented_code": "return nil\n }\n+\n+func (pmpt *Action) topologyAwarePreempt(\n+\tssn *framework.Session,\n+\tstmt *framework.Statement,\n+\tpreemptor *api.TaskInfo,\n+\tfilter func(*api.TaskInfo) bool,\n+\tpredicateHelper util.PredicateHelper,\n+) (bool, error) {\n+\tif err := ssn.PrePredicateFn(preemptor); err != nil {\n+\t\treturn false, fmt.Errorf(\"PrePredicate for task %s/%s failed for: %v\", preemptor.Namespace, preemptor.Name, err)\n+\t}\n+\n+\t_, found := ssn.Jobs[preemptor.Job]\n+\tif !found {\n+\t\treturn false, fmt.Errorf(\"not found Job %s in Session\", preemptor.Job)\n+\t}\n+\n+\t// 1) Check whether the task is eligible to preempt others, e.g., check preemptionPolicy is `Never` or not\n+\teligible, reason := pmpt.taskEligibleToPreemptOthers(preemptor)\n+\tif !eligible {\n+\t\treturn false, fmt.Errorf(\"task %s/%s is not eligible to preempt others: %s\", preemptor.Namespace, preemptor.Name, reason)\n+\t}\n+\n+\t// 2) Find all preemption candidates.\n+\tcandidates, nodeToStatusMap, err := pmpt.findCandidates(preemptor, filter, predicateHelper, stmt)\n+\tif err != nil && len(candidates) == 0 {\n+\t\treturn false, err\n+\t}\n+\n+\t// Return error when there are no candidates that fit the pod.\n+\tif len(candidates) == 0 {\n+\t\t// Specify nominatedNodeName to clear the pod's nominatedNodeName status, if applicable.\n+\t\treturn false, fmt.Errorf(\"no candidates that fit the pod, the status of the nodes are %v\", nodeToStatusMap)\n+\t}\n+\n+\t// 3) Find the best candidate.\n+\tbestCandidate := SelectCandidate(candidates)\n+\tif bestCandidate == nil || len(bestCandidate.Name()) == 0 {\n+\t\treturn false, fmt.Errorf(\"no candidate node for preemption\")\n+\t}\n+\n+\tif status := prepareCandidate(bestCandidate, preemptor.Pod, stmt, ssn); !status.IsSuccess() {\n+\t\treturn false, fmt.Errorf(\"failed to prepare candidate: %v\", status)\n+\t}\n+\n+\tif err := stmt.Pipeline(preemptor, bestCandidate.Name(), true); err != nil {\n+\t\tklog.Errorf(\"Failed to pipeline Task <%s/%s> on Node <%s>\",\n+\t\t\tpreemptor.Namespace, preemptor.Name, bestCandidate.Name())\n+\t\tif rollbackErr := stmt.UnPipeline(preemptor); rollbackErr != nil {\n+\t\t\tklog.Errorf(\"Failed to unpipeline Task %v on %v in Session %v for %v.\",\n+\t\t\t\tpreemptor.UID, bestCandidate.Name(), ssn.UID, rollbackErr)\n+\t\t}\n+\t}\n+\n+\treturn true, nil\n+}\n+\n+func (pmpt *Action) taskEligibleToPreemptOthers(preemptor *api.TaskInfo) (bool, string) {",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2086717589",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4279,
        "pr_file": "pkg/scheduler/actions/preempt/preempt.go",
        "discussion_id": "2086717589",
        "commented_code": "@@ -342,3 +386,612 @@ func (pmpt *Action) taskEligibleToPreempt(preemptor *api.TaskInfo) error {\n \n \treturn nil\n }\n+\n+func (pmpt *Action) topologyAwarePreempt(\n+\tssn *framework.Session,\n+\tstmt *framework.Statement,\n+\tpreemptor *api.TaskInfo,\n+\tfilter func(*api.TaskInfo) bool,\n+\tpredicateHelper util.PredicateHelper,\n+) (bool, error) {\n+\tif err := ssn.PrePredicateFn(preemptor); err != nil {\n+\t\treturn false, fmt.Errorf(\"PrePredicate for task %s/%s failed for: %v\", preemptor.Namespace, preemptor.Name, err)\n+\t}\n+\n+\t_, found := ssn.Jobs[preemptor.Job]\n+\tif !found {\n+\t\treturn false, fmt.Errorf(\"not found Job %s in Session\", preemptor.Job)\n+\t}\n+\n+\t// 1) Check whether the task is eligible to preempt others, e.g., check preemptionPolicy is `Never` or not\n+\teligible, reason := pmpt.taskEligibleToPreemptOthers(preemptor)\n+\tif !eligible {\n+\t\treturn false, fmt.Errorf(\"task %s/%s is not eligible to preempt others: %s\", preemptor.Namespace, preemptor.Name, reason)\n+\t}\n+\n+\t// 2) Find all preemption candidates.\n+\tcandidates, nodeToStatusMap, err := pmpt.findCandidates(preemptor, filter, predicateHelper, stmt)\n+\tif err != nil && len(candidates) == 0 {\n+\t\treturn false, err\n+\t}\n+\n+\t// Return error when there are no candidates that fit the pod.\n+\tif len(candidates) == 0 {\n+\t\t// Specify nominatedNodeName to clear the pod's nominatedNodeName status, if applicable.\n+\t\treturn false, fmt.Errorf(\"no candidates that fit the pod, the status of the nodes are %v\", nodeToStatusMap)\n+\t}\n+\n+\t// 3) Find the best candidate.\n+\tbestCandidate := SelectCandidate(candidates)\n+\tif bestCandidate == nil || len(bestCandidate.Name()) == 0 {\n+\t\treturn false, fmt.Errorf(\"no candidate node for preemption\")\n+\t}\n+\n+\tif status := prepareCandidate(bestCandidate, preemptor.Pod, stmt, ssn); !status.IsSuccess() {\n+\t\treturn false, fmt.Errorf(\"failed to prepare candidate: %v\", status)\n+\t}\n+\n+\tif err := stmt.Pipeline(preemptor, bestCandidate.Name(), true); err != nil {\n+\t\tklog.Errorf(\"Failed to pipeline Task <%s/%s> on Node <%s>\",\n+\t\t\tpreemptor.Namespace, preemptor.Name, bestCandidate.Name())\n+\t\tif rollbackErr := stmt.UnPipeline(preemptor); rollbackErr != nil {\n+\t\t\tklog.Errorf(\"Failed to unpipeline Task %v on %v in Session %v for %v.\",\n+\t\t\t\tpreemptor.UID, bestCandidate.Name(), ssn.UID, rollbackErr)\n+\t\t}\n+\t}\n+\n+\treturn true, nil\n+}\n+\n+func (pmpt *Action) taskEligibleToPreemptOthers(preemptor *api.TaskInfo) (bool, string) {",
        "comment_created_at": "2025-05-13T12:36:21+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "There is already a taskEligibleToPreempt function, which can be unified together",
        "pr_file_module": null
      },
      {
        "comment_id": "2108569278",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4279,
        "pr_file": "pkg/scheduler/actions/preempt/preempt.go",
        "discussion_id": "2086717589",
        "commented_code": "@@ -342,3 +386,612 @@ func (pmpt *Action) taskEligibleToPreempt(preemptor *api.TaskInfo) error {\n \n \treturn nil\n }\n+\n+func (pmpt *Action) topologyAwarePreempt(\n+\tssn *framework.Session,\n+\tstmt *framework.Statement,\n+\tpreemptor *api.TaskInfo,\n+\tfilter func(*api.TaskInfo) bool,\n+\tpredicateHelper util.PredicateHelper,\n+) (bool, error) {\n+\tif err := ssn.PrePredicateFn(preemptor); err != nil {\n+\t\treturn false, fmt.Errorf(\"PrePredicate for task %s/%s failed for: %v\", preemptor.Namespace, preemptor.Name, err)\n+\t}\n+\n+\t_, found := ssn.Jobs[preemptor.Job]\n+\tif !found {\n+\t\treturn false, fmt.Errorf(\"not found Job %s in Session\", preemptor.Job)\n+\t}\n+\n+\t// 1) Check whether the task is eligible to preempt others, e.g., check preemptionPolicy is `Never` or not\n+\teligible, reason := pmpt.taskEligibleToPreemptOthers(preemptor)\n+\tif !eligible {\n+\t\treturn false, fmt.Errorf(\"task %s/%s is not eligible to preempt others: %s\", preemptor.Namespace, preemptor.Name, reason)\n+\t}\n+\n+\t// 2) Find all preemption candidates.\n+\tcandidates, nodeToStatusMap, err := pmpt.findCandidates(preemptor, filter, predicateHelper, stmt)\n+\tif err != nil && len(candidates) == 0 {\n+\t\treturn false, err\n+\t}\n+\n+\t// Return error when there are no candidates that fit the pod.\n+\tif len(candidates) == 0 {\n+\t\t// Specify nominatedNodeName to clear the pod's nominatedNodeName status, if applicable.\n+\t\treturn false, fmt.Errorf(\"no candidates that fit the pod, the status of the nodes are %v\", nodeToStatusMap)\n+\t}\n+\n+\t// 3) Find the best candidate.\n+\tbestCandidate := SelectCandidate(candidates)\n+\tif bestCandidate == nil || len(bestCandidate.Name()) == 0 {\n+\t\treturn false, fmt.Errorf(\"no candidate node for preemption\")\n+\t}\n+\n+\tif status := prepareCandidate(bestCandidate, preemptor.Pod, stmt, ssn); !status.IsSuccess() {\n+\t\treturn false, fmt.Errorf(\"failed to prepare candidate: %v\", status)\n+\t}\n+\n+\tif err := stmt.Pipeline(preemptor, bestCandidate.Name(), true); err != nil {\n+\t\tklog.Errorf(\"Failed to pipeline Task <%s/%s> on Node <%s>\",\n+\t\t\tpreemptor.Namespace, preemptor.Name, bestCandidate.Name())\n+\t\tif rollbackErr := stmt.UnPipeline(preemptor); rollbackErr != nil {\n+\t\t\tklog.Errorf(\"Failed to unpipeline Task %v on %v in Session %v for %v.\",\n+\t\t\t\tpreemptor.UID, bestCandidate.Name(), ssn.UID, rollbackErr)\n+\t\t}\n+\t}\n+\n+\treturn true, nil\n+}\n+\n+func (pmpt *Action) taskEligibleToPreemptOthers(preemptor *api.TaskInfo) (bool, string) {",
        "comment_created_at": "2025-05-27T08:27:26+00:00",
        "comment_author": "bibibox",
        "comment_body": "update it",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2048080985",
    "pr_number": 4068,
    "pr_file": "pkg/scheduler/cache/event_handlers.go",
    "created_at": "2025-04-17T02:18:14+00:00",
    "commented_code": "return\n \t}\n \tsc.nodeQueue.Add(newNode.Name)\n+\tif len(oldNode.GetLabels()) != len(newNode.GetLabels()) {",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2048080985",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4068,
        "pr_file": "pkg/scheduler/cache/event_handlers.go",
        "discussion_id": "2048080985",
        "commented_code": "@@ -568,6 +568,16 @@ func (sc *SchedulerCache) UpdateNode(oldObj, newObj interface{}) {\n \t\treturn\n \t}\n \tsc.nodeQueue.Add(newNode.Name)\n+\tif len(oldNode.GetLabels()) != len(newNode.GetLabels()) {",
        "comment_created_at": "2025-04-17T02:18:14+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "I think use reflect.DeepEqual is better?\r\n```go\r\nif !reflect.DeepEqual(oldNode.GetLabels(), newNode.GetLabels()) {\r\n\t\tsc.hyperNodesQueue.Add(string(hyperNodeEventSourceNode) + \"/\" + newNode.Name)\r\n\t}\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2048111784",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4068,
        "pr_file": "pkg/scheduler/cache/event_handlers.go",
        "discussion_id": "2048080985",
        "commented_code": "@@ -568,6 +568,16 @@ func (sc *SchedulerCache) UpdateNode(oldObj, newObj interface{}) {\n \t\treturn\n \t}\n \tsc.nodeQueue.Add(newNode.Name)\n+\tif len(oldNode.GetLabels()) != len(newNode.GetLabels()) {",
        "comment_created_at": "2025-04-17T03:05:07+00:00",
        "comment_author": "ecosysbin",
        "comment_body": "Good idea, Done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2088535277",
    "pr_number": 4272,
    "pr_file": "pkg/controllers/job/plugins/distributed-framework/mpi/mpi.go",
    "created_at": "2025-05-14T09:46:40+00:00",
    "commented_code": "}\n \n func (mp *Plugin) generateTaskHosts(task batch.TaskSpec, jobName string) string {\n-\thosts := \"\"\n+\tif task.Replicas == 0 {\n+\t\treturn \"\"\n+\t}\n+\n+\tvar builder strings.Builder\n \tfor i := 0; i < int(task.Replicas); i++ {\n \t\thostName := task.Template.Spec.Hostname\n \t\tsubdomain := task.Template.Spec.Subdomain\n-\t\tif len(hostName) == 0 {\n+\n+\t\tif hostName == \"\" {\n \t\t\thostName = helpers.MakePodName(jobName, task.Name, i)\n \t\t}\n-\t\tif len(subdomain) == 0 {\n+\t\tif subdomain == \"\" {\n \t\t\tsubdomain = jobName\n \t\t}\n-\t\thosts = hosts + hostName + \".\" + subdomain + \",\"\n-\t\tif len(task.Template.Spec.Hostname) != 0 {\n+\n+\t\tbuilder.WriteString(hostName)\n+\t\tbuilder.WriteString(\".\")\n+\t\tbuilder.WriteString(subdomain)\n+\n+\t\tif i < int(task.Replicas)-1 {\n+\t\t\tbuilder.WriteString(\",\")\n+\t\t}\n+\n+\t\t// If a hostname is explicitly specified, assume only one host is needed.\n+\t\t// Break the loop early to avoid generating additional hosts.\n+\t\tif task.Template.Spec.Hostname != \"\" {",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2088535277",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4272,
        "pr_file": "pkg/controllers/job/plugins/distributed-framework/mpi/mpi.go",
        "discussion_id": "2088535277",
        "commented_code": "@@ -108,22 +110,38 @@ func (mp *Plugin) OnPodCreate(pod *v1.Pod, job *batch.Job) error {\n }\n \n func (mp *Plugin) generateTaskHosts(task batch.TaskSpec, jobName string) string {\n-\thosts := \"\"\n+\tif task.Replicas == 0 {\n+\t\treturn \"\"\n+\t}\n+\n+\tvar builder strings.Builder\n \tfor i := 0; i < int(task.Replicas); i++ {\n \t\thostName := task.Template.Spec.Hostname\n \t\tsubdomain := task.Template.Spec.Subdomain\n-\t\tif len(hostName) == 0 {\n+\n+\t\tif hostName == \"\" {\n \t\t\thostName = helpers.MakePodName(jobName, task.Name, i)\n \t\t}\n-\t\tif len(subdomain) == 0 {\n+\t\tif subdomain == \"\" {\n \t\t\tsubdomain = jobName\n \t\t}\n-\t\thosts = hosts + hostName + \".\" + subdomain + \",\"\n-\t\tif len(task.Template.Spec.Hostname) != 0 {\n+\n+\t\tbuilder.WriteString(hostName)\n+\t\tbuilder.WriteString(\".\")\n+\t\tbuilder.WriteString(subdomain)\n+\n+\t\tif i < int(task.Replicas)-1 {\n+\t\t\tbuilder.WriteString(\",\")\n+\t\t}\n+\n+\t\t// If a hostname is explicitly specified, assume only one host is needed.\n+\t\t// Break the loop early to avoid generating additional hosts.\n+\t\tif task.Template.Spec.Hostname != \"\" {",
        "comment_created_at": "2025-05-14T09:46:40+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "The logic here is not right. If Hostname is not empty, there will be an extra comma at the end. The extra comma at the end will be removed in the old logic, but the current writing will add an extra comma at the end and didn't remove it",
        "pr_file_module": null
      },
      {
        "comment_id": "2088844741",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4272,
        "pr_file": "pkg/controllers/job/plugins/distributed-framework/mpi/mpi.go",
        "discussion_id": "2088535277",
        "commented_code": "@@ -108,22 +110,38 @@ func (mp *Plugin) OnPodCreate(pod *v1.Pod, job *batch.Job) error {\n }\n \n func (mp *Plugin) generateTaskHosts(task batch.TaskSpec, jobName string) string {\n-\thosts := \"\"\n+\tif task.Replicas == 0 {\n+\t\treturn \"\"\n+\t}\n+\n+\tvar builder strings.Builder\n \tfor i := 0; i < int(task.Replicas); i++ {\n \t\thostName := task.Template.Spec.Hostname\n \t\tsubdomain := task.Template.Spec.Subdomain\n-\t\tif len(hostName) == 0 {\n+\n+\t\tif hostName == \"\" {\n \t\t\thostName = helpers.MakePodName(jobName, task.Name, i)\n \t\t}\n-\t\tif len(subdomain) == 0 {\n+\t\tif subdomain == \"\" {\n \t\t\tsubdomain = jobName\n \t\t}\n-\t\thosts = hosts + hostName + \".\" + subdomain + \",\"\n-\t\tif len(task.Template.Spec.Hostname) != 0 {\n+\n+\t\tbuilder.WriteString(hostName)\n+\t\tbuilder.WriteString(\".\")\n+\t\tbuilder.WriteString(subdomain)\n+\n+\t\tif i < int(task.Replicas)-1 {\n+\t\t\tbuilder.WriteString(\",\")\n+\t\t}\n+\n+\t\t// If a hostname is explicitly specified, assume only one host is needed.\n+\t\t// Break the loop early to avoid generating additional hosts.\n+\t\tif task.Template.Spec.Hostname != \"\" {",
        "comment_created_at": "2025-05-14T12:40:07+00:00",
        "comment_author": "guoqinwill",
        "comment_body": "The new logic calculates \" int(task.Replicas) - 1\" , which adds commas after each element except the last one to prevent slicing errors. This ensures no extra commas are generated.",
        "pr_file_module": null
      },
      {
        "comment_id": "2088923749",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4272,
        "pr_file": "pkg/controllers/job/plugins/distributed-framework/mpi/mpi.go",
        "discussion_id": "2088535277",
        "commented_code": "@@ -108,22 +110,38 @@ func (mp *Plugin) OnPodCreate(pod *v1.Pod, job *batch.Job) error {\n }\n \n func (mp *Plugin) generateTaskHosts(task batch.TaskSpec, jobName string) string {\n-\thosts := \"\"\n+\tif task.Replicas == 0 {\n+\t\treturn \"\"\n+\t}\n+\n+\tvar builder strings.Builder\n \tfor i := 0; i < int(task.Replicas); i++ {\n \t\thostName := task.Template.Spec.Hostname\n \t\tsubdomain := task.Template.Spec.Subdomain\n-\t\tif len(hostName) == 0 {\n+\n+\t\tif hostName == \"\" {\n \t\t\thostName = helpers.MakePodName(jobName, task.Name, i)\n \t\t}\n-\t\tif len(subdomain) == 0 {\n+\t\tif subdomain == \"\" {\n \t\t\tsubdomain = jobName\n \t\t}\n-\t\thosts = hosts + hostName + \".\" + subdomain + \",\"\n-\t\tif len(task.Template.Spec.Hostname) != 0 {\n+\n+\t\tbuilder.WriteString(hostName)\n+\t\tbuilder.WriteString(\".\")\n+\t\tbuilder.WriteString(subdomain)\n+\n+\t\tif i < int(task.Replicas)-1 {\n+\t\t\tbuilder.WriteString(\",\")\n+\t\t}\n+\n+\t\t// If a hostname is explicitly specified, assume only one host is needed.\n+\t\t// Break the loop early to avoid generating additional hosts.\n+\t\tif task.Template.Spec.Hostname != \"\" {",
        "comment_created_at": "2025-05-14T13:18:14+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "No, I mean if your replicas is greater than 1, and both hostname and subdomain exist, then it will be \"hostname.subdomain,\" with an extra comma. If the hostname is not empty, then it break here, and then return builder directly, so there will be an extra comma.",
        "pr_file_module": null
      },
      {
        "comment_id": "2088962819",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4272,
        "pr_file": "pkg/controllers/job/plugins/distributed-framework/mpi/mpi.go",
        "discussion_id": "2088535277",
        "commented_code": "@@ -108,22 +110,38 @@ func (mp *Plugin) OnPodCreate(pod *v1.Pod, job *batch.Job) error {\n }\n \n func (mp *Plugin) generateTaskHosts(task batch.TaskSpec, jobName string) string {\n-\thosts := \"\"\n+\tif task.Replicas == 0 {\n+\t\treturn \"\"\n+\t}\n+\n+\tvar builder strings.Builder\n \tfor i := 0; i < int(task.Replicas); i++ {\n \t\thostName := task.Template.Spec.Hostname\n \t\tsubdomain := task.Template.Spec.Subdomain\n-\t\tif len(hostName) == 0 {\n+\n+\t\tif hostName == \"\" {\n \t\t\thostName = helpers.MakePodName(jobName, task.Name, i)\n \t\t}\n-\t\tif len(subdomain) == 0 {\n+\t\tif subdomain == \"\" {\n \t\t\tsubdomain = jobName\n \t\t}\n-\t\thosts = hosts + hostName + \".\" + subdomain + \",\"\n-\t\tif len(task.Template.Spec.Hostname) != 0 {\n+\n+\t\tbuilder.WriteString(hostName)\n+\t\tbuilder.WriteString(\".\")\n+\t\tbuilder.WriteString(subdomain)\n+\n+\t\tif i < int(task.Replicas)-1 {\n+\t\t\tbuilder.WriteString(\",\")\n+\t\t}\n+\n+\t\t// If a hostname is explicitly specified, assume only one host is needed.\n+\t\t// Break the loop early to avoid generating additional hosts.\n+\t\tif task.Template.Spec.Hostname != \"\" {",
        "comment_created_at": "2025-05-14T13:33:08+00:00",
        "comment_author": "guoqinwill",
        "comment_body": "Yes, you\u2019re right. I\u2019ve made the necessary changes.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1988544260",
    "pr_number": 3894,
    "pr_file": "pkg/scheduler/actions/allocate/allocate.go",
    "created_at": "2025-03-11T07:10:38+00:00",
    "commented_code": "}\n \t\t}\n \n+\t\ttask.JobAllocatedHyperNode = jobAllocatedNewHyperNode",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1988544260",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3894,
        "pr_file": "pkg/scheduler/actions/allocate/allocate.go",
        "discussion_id": "1988544260",
        "commented_code": "@@ -414,11 +398,21 @@ func (alloc *Action) allocateResourcesForTasks(tasks *util.PriorityQueue, job *a\n \t\t\t}\n \t\t}\n \n+\t\ttask.JobAllocatedHyperNode = jobAllocatedNewHyperNode",
        "comment_created_at": "2025-03-11T07:10:38+00:00",
        "comment_author": "Monokaix",
        "comment_body": "Does the modification in `allocateResourcesForTasks` func just want to get the hyperNode name of current task, if that's true, I think we should implement this in somewhere else because there is already the same logic in `allocateResourceForTasksWithTopology`, for instance we can abstract this in `allocateResources`.",
        "pr_file_module": null
      },
      {
        "comment_id": "1992523450",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3894,
        "pr_file": "pkg/scheduler/actions/allocate/allocate.go",
        "discussion_id": "1988544260",
        "commented_code": "@@ -414,11 +398,21 @@ func (alloc *Action) allocateResourcesForTasks(tasks *util.PriorityQueue, job *a\n \t\t\t}\n \t\t}\n \n+\t\ttask.JobAllocatedHyperNode = jobAllocatedNewHyperNode",
        "comment_created_at": "2025-03-13T01:43:47+00:00",
        "comment_author": "ecosysbin",
        "comment_body": "Mainly do this: for each task in the loop of tasks under the `job`, when a task is scheduled, a new `LCAHyperNode` will be generated by combining the task and the `jobHyperNode` in the cache, and this new `LCAHyperNode` will continue to be recorded in the cache. It will only be recorded in the PG when the `allocate` operation exits.   I have already made the modifications. The scheduling process in the soft mode should be distinguished as much as possible from the scenarios where no hyperNode is set. You can check it again.  ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1436712821",
    "pr_number": 3210,
    "pr_file": "pkg/scheduler/plugins/nodeorder/nodeorder.go",
    "created_at": "2023-12-27T03:08:00+00:00",
    "commented_code": "return nil, err\n \t\t}\n \n+\t\tdeviceScores, err := deviceScore(task.Pod, nodeInfo, weight.deviceScoreWeight)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\n \t\tfor _, node := range nodes {\n-\t\t\tnodeScores[node.Name] = podAffinityScores[node.Name] + nodeTolerationScores[node.Name] + podTopologySpreadScores[node.Name] + selectorSpreadScores[node.Name]\n+\t\t\tnodeScores[node.Name] = podAffinityScores[node.Name] + nodeTolerationScores[node.Name] + podTopologySpreadScores[node.Name] + selectorSpreadScores[node.Name] + deviceScores[node.Name]",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1436712821",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3210,
        "pr_file": "pkg/scheduler/plugins/nodeorder/nodeorder.go",
        "discussion_id": "1436712821",
        "commented_code": "@@ -340,8 +347,13 @@ func (pp *nodeOrderPlugin) OnSessionOpen(ssn *framework.Session) {\n \t\t\treturn nil, err\n \t\t}\n \n+\t\tdeviceScores, err := deviceScore(task.Pod, nodeInfo, weight.deviceScoreWeight)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\n \t\tfor _, node := range nodes {\n-\t\t\tnodeScores[node.Name] = podAffinityScores[node.Name] + nodeTolerationScores[node.Name] + podTopologySpreadScores[node.Name] + selectorSpreadScores[node.Name]\n+\t\t\tnodeScores[node.Name] = podAffinityScores[node.Name] + nodeTolerationScores[node.Name] + podTopologySpreadScores[node.Name] + selectorSpreadScores[node.Name] + deviceScores[node.Name]",
        "comment_created_at": "2023-12-27T03:08:00+00:00",
        "comment_author": "Monokaix",
        "comment_body": "Why put device score in batchNodeOrder?",
        "pr_file_module": null
      },
      {
        "comment_id": "1436811999",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3210,
        "pr_file": "pkg/scheduler/plugins/nodeorder/nodeorder.go",
        "discussion_id": "1436712821",
        "commented_code": "@@ -340,8 +347,13 @@ func (pp *nodeOrderPlugin) OnSessionOpen(ssn *framework.Session) {\n \t\t\treturn nil, err\n \t\t}\n \n+\t\tdeviceScores, err := deviceScore(task.Pod, nodeInfo, weight.deviceScoreWeight)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\n \t\tfor _, node := range nodes {\n-\t\t\tnodeScores[node.Name] = podAffinityScores[node.Name] + nodeTolerationScores[node.Name] + podTopologySpreadScores[node.Name] + selectorSpreadScores[node.Name]\n+\t\t\tnodeScores[node.Name] = podAffinityScores[node.Name] + nodeTolerationScores[node.Name] + podTopologySpreadScores[node.Name] + selectorSpreadScores[node.Name] + deviceScores[node.Name]",
        "comment_created_at": "2023-12-27T07:36:12+00:00",
        "comment_author": "archlitchi",
        "comment_body": "because a node may need the information of all available nodes in order to score accurately",
        "pr_file_module": null
      },
      {
        "comment_id": "1436821862",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3210,
        "pr_file": "pkg/scheduler/plugins/nodeorder/nodeorder.go",
        "discussion_id": "1436712821",
        "commented_code": "@@ -340,8 +347,13 @@ func (pp *nodeOrderPlugin) OnSessionOpen(ssn *framework.Session) {\n \t\t\treturn nil, err\n \t\t}\n \n+\t\tdeviceScores, err := deviceScore(task.Pod, nodeInfo, weight.deviceScoreWeight)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\n \t\tfor _, node := range nodes {\n-\t\t\tnodeScores[node.Name] = podAffinityScores[node.Name] + nodeTolerationScores[node.Name] + podTopologySpreadScores[node.Name] + selectorSpreadScores[node.Name]\n+\t\t\tnodeScores[node.Name] = podAffinityScores[node.Name] + nodeTolerationScores[node.Name] + podTopologySpreadScores[node.Name] + selectorSpreadScores[node.Name] + deviceScores[node.Name]",
        "comment_created_at": "2023-12-27T07:55:08+00:00",
        "comment_author": "Monokaix",
        "comment_body": "Please make it score in parallel like other batchNodeOrder plugin: )",
        "pr_file_module": null
      },
      {
        "comment_id": "1436879069",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3210,
        "pr_file": "pkg/scheduler/plugins/nodeorder/nodeorder.go",
        "discussion_id": "1436712821",
        "commented_code": "@@ -340,8 +347,13 @@ func (pp *nodeOrderPlugin) OnSessionOpen(ssn *framework.Session) {\n \t\t\treturn nil, err\n \t\t}\n \n+\t\tdeviceScores, err := deviceScore(task.Pod, nodeInfo, weight.deviceScoreWeight)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\n \t\tfor _, node := range nodes {\n-\t\t\tnodeScores[node.Name] = podAffinityScores[node.Name] + nodeTolerationScores[node.Name] + podTopologySpreadScores[node.Name] + selectorSpreadScores[node.Name]\n+\t\t\tnodeScores[node.Name] = podAffinityScores[node.Name] + nodeTolerationScores[node.Name] + podTopologySpreadScores[node.Name] + selectorSpreadScores[node.Name] + deviceScores[node.Name]",
        "comment_created_at": "2023-12-27T09:30:17+00:00",
        "comment_author": "archlitchi",
        "comment_body": "ok",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2051913646",
    "pr_number": 4000,
    "pr_file": "pkg/controllers/podgroup/pg_controller_handler.go",
    "created_at": "2025-04-21T03:33:57+00:00",
    "commented_code": "return nil\n }\n \n-func (pg *pgcontroller) getAnnotationsFromUpperRes(kind string, name string, namespace string) map[string]string {\n-\tswitch kind {\n-\tcase \"ReplicaSet\":\n-\t\trs, err := pg.kubeClient.AppsV1().ReplicaSets(namespace).Get(context.TODO(), name, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", kind, namespace, name, err)\n-\t\t\treturn map[string]string{}\n-\t\t}\n-\t\treturn rs.Annotations\n-\tcase \"DaemonSet\":\n-\t\tds, err := pg.kubeClient.AppsV1().DaemonSets(namespace).Get(context.TODO(), name, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", kind, namespace, name, err)\n-\t\t\treturn map[string]string{}\n+func (pg *pgcontroller) getAnnotationsFromUpperRes(pod *v1.Pod) map[string]string {\n+\tvar annotations = make(map[string]string)\n+\n+\tfor _, reference := range pod.OwnerReferences {\n+\t\tif reference.Kind != \"\" && reference.Name != \"\" {\n+\t\t\ttmp := make(map[string]string)\n+\t\t\tswitch reference.Kind {\n+\t\t\tcase \"ReplicaSet\":\n+\t\t\t\trs, err := pg.kubeClient.AppsV1().ReplicaSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = rs.Annotations\n+\t\t\tcase \"DaemonSet\":\n+\t\t\t\tds, err := pg.kubeClient.AppsV1().DaemonSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = ds.Annotations\n+\t\t\tcase \"StatefulSet\":\n+\t\t\t\tss, err := pg.kubeClient.AppsV1().StatefulSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = ss.Annotations\n+\t\t\tcase \"Job\":\n+\t\t\t\tjob, err := pg.kubeClient.BatchV1().Jobs(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = job.Annotations\n+\t\t\t}\n+\n+\t\t\tif err := mergo.Merge(&annotations, &tmp); err != nil {",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2051913646",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4000,
        "pr_file": "pkg/controllers/podgroup/pg_controller_handler.go",
        "discussion_id": "2051913646",
        "commented_code": "@@ -191,52 +193,78 @@ func (pg *pgcontroller) updatePodAnnotations(pod *v1.Pod, pgName string) error {\n \treturn nil\n }\n \n-func (pg *pgcontroller) getAnnotationsFromUpperRes(kind string, name string, namespace string) map[string]string {\n-\tswitch kind {\n-\tcase \"ReplicaSet\":\n-\t\trs, err := pg.kubeClient.AppsV1().ReplicaSets(namespace).Get(context.TODO(), name, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", kind, namespace, name, err)\n-\t\t\treturn map[string]string{}\n-\t\t}\n-\t\treturn rs.Annotations\n-\tcase \"DaemonSet\":\n-\t\tds, err := pg.kubeClient.AppsV1().DaemonSets(namespace).Get(context.TODO(), name, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", kind, namespace, name, err)\n-\t\t\treturn map[string]string{}\n+func (pg *pgcontroller) getAnnotationsFromUpperRes(pod *v1.Pod) map[string]string {\n+\tvar annotations = make(map[string]string)\n+\n+\tfor _, reference := range pod.OwnerReferences {\n+\t\tif reference.Kind != \"\" && reference.Name != \"\" {\n+\t\t\ttmp := make(map[string]string)\n+\t\t\tswitch reference.Kind {\n+\t\t\tcase \"ReplicaSet\":\n+\t\t\t\trs, err := pg.kubeClient.AppsV1().ReplicaSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = rs.Annotations\n+\t\t\tcase \"DaemonSet\":\n+\t\t\t\tds, err := pg.kubeClient.AppsV1().DaemonSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = ds.Annotations\n+\t\t\tcase \"StatefulSet\":\n+\t\t\t\tss, err := pg.kubeClient.AppsV1().StatefulSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = ss.Annotations\n+\t\t\tcase \"Job\":\n+\t\t\t\tjob, err := pg.kubeClient.BatchV1().Jobs(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = job.Annotations\n+\t\t\t}\n+\n+\t\t\tif err := mergo.Merge(&annotations, &tmp); err != nil {",
        "comment_created_at": "2025-04-21T03:33:57+00:00",
        "comment_author": "hwdef",
        "comment_body": "Can we not import the Mergo package? Feeling little necessary",
        "pr_file_module": null
      },
      {
        "comment_id": "2051987548",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4000,
        "pr_file": "pkg/controllers/podgroup/pg_controller_handler.go",
        "discussion_id": "2051913646",
        "commented_code": "@@ -191,52 +193,78 @@ func (pg *pgcontroller) updatePodAnnotations(pod *v1.Pod, pgName string) error {\n \treturn nil\n }\n \n-func (pg *pgcontroller) getAnnotationsFromUpperRes(kind string, name string, namespace string) map[string]string {\n-\tswitch kind {\n-\tcase \"ReplicaSet\":\n-\t\trs, err := pg.kubeClient.AppsV1().ReplicaSets(namespace).Get(context.TODO(), name, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", kind, namespace, name, err)\n-\t\t\treturn map[string]string{}\n-\t\t}\n-\t\treturn rs.Annotations\n-\tcase \"DaemonSet\":\n-\t\tds, err := pg.kubeClient.AppsV1().DaemonSets(namespace).Get(context.TODO(), name, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", kind, namespace, name, err)\n-\t\t\treturn map[string]string{}\n+func (pg *pgcontroller) getAnnotationsFromUpperRes(pod *v1.Pod) map[string]string {\n+\tvar annotations = make(map[string]string)\n+\n+\tfor _, reference := range pod.OwnerReferences {\n+\t\tif reference.Kind != \"\" && reference.Name != \"\" {\n+\t\t\ttmp := make(map[string]string)\n+\t\t\tswitch reference.Kind {\n+\t\t\tcase \"ReplicaSet\":\n+\t\t\t\trs, err := pg.kubeClient.AppsV1().ReplicaSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = rs.Annotations\n+\t\t\tcase \"DaemonSet\":\n+\t\t\t\tds, err := pg.kubeClient.AppsV1().DaemonSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = ds.Annotations\n+\t\t\tcase \"StatefulSet\":\n+\t\t\t\tss, err := pg.kubeClient.AppsV1().StatefulSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = ss.Annotations\n+\t\t\tcase \"Job\":\n+\t\t\t\tjob, err := pg.kubeClient.BatchV1().Jobs(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = job.Annotations\n+\t\t\t}\n+\n+\t\t\tif err := mergo.Merge(&annotations, &tmp); err != nil {",
        "comment_created_at": "2025-04-21T05:47:13+00:00",
        "comment_author": "sceneryback",
        "comment_body": "or simply write our own map merging func, e.g: \r\n```\r\nfunc mergeMap(src, dest map[string]string) {\r\n    for k, v := range dest {\r\n        if _, ok := src[k]; !ok {\r\n            src[k] = v\r\n        }\r\n    }\r\n}\r\n```\r\nor just use an inline one:   \r\n```\r\n    for k, v := range tmp {\r\n        if _, ok := annotations[k]; !ok {\r\n            annotations[k] = v\r\n        }\r\n    }\r\n```\r\nis this OK? @hwdef ",
        "pr_file_module": null
      },
      {
        "comment_id": "2052014303",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4000,
        "pr_file": "pkg/controllers/podgroup/pg_controller_handler.go",
        "discussion_id": "2051913646",
        "commented_code": "@@ -191,52 +193,78 @@ func (pg *pgcontroller) updatePodAnnotations(pod *v1.Pod, pgName string) error {\n \treturn nil\n }\n \n-func (pg *pgcontroller) getAnnotationsFromUpperRes(kind string, name string, namespace string) map[string]string {\n-\tswitch kind {\n-\tcase \"ReplicaSet\":\n-\t\trs, err := pg.kubeClient.AppsV1().ReplicaSets(namespace).Get(context.TODO(), name, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", kind, namespace, name, err)\n-\t\t\treturn map[string]string{}\n-\t\t}\n-\t\treturn rs.Annotations\n-\tcase \"DaemonSet\":\n-\t\tds, err := pg.kubeClient.AppsV1().DaemonSets(namespace).Get(context.TODO(), name, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", kind, namespace, name, err)\n-\t\t\treturn map[string]string{}\n+func (pg *pgcontroller) getAnnotationsFromUpperRes(pod *v1.Pod) map[string]string {\n+\tvar annotations = make(map[string]string)\n+\n+\tfor _, reference := range pod.OwnerReferences {\n+\t\tif reference.Kind != \"\" && reference.Name != \"\" {\n+\t\t\ttmp := make(map[string]string)\n+\t\t\tswitch reference.Kind {\n+\t\t\tcase \"ReplicaSet\":\n+\t\t\t\trs, err := pg.kubeClient.AppsV1().ReplicaSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = rs.Annotations\n+\t\t\tcase \"DaemonSet\":\n+\t\t\t\tds, err := pg.kubeClient.AppsV1().DaemonSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = ds.Annotations\n+\t\t\tcase \"StatefulSet\":\n+\t\t\t\tss, err := pg.kubeClient.AppsV1().StatefulSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = ss.Annotations\n+\t\t\tcase \"Job\":\n+\t\t\t\tjob, err := pg.kubeClient.BatchV1().Jobs(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = job.Annotations\n+\t\t\t}\n+\n+\t\t\tif err := mergo.Merge(&annotations, &tmp); err != nil {",
        "comment_created_at": "2025-04-21T06:23:43+00:00",
        "comment_author": "hwdef",
        "comment_body": "It's good:)",
        "pr_file_module": null
      },
      {
        "comment_id": "2052079190",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4000,
        "pr_file": "pkg/controllers/podgroup/pg_controller_handler.go",
        "discussion_id": "2051913646",
        "commented_code": "@@ -191,52 +193,78 @@ func (pg *pgcontroller) updatePodAnnotations(pod *v1.Pod, pgName string) error {\n \treturn nil\n }\n \n-func (pg *pgcontroller) getAnnotationsFromUpperRes(kind string, name string, namespace string) map[string]string {\n-\tswitch kind {\n-\tcase \"ReplicaSet\":\n-\t\trs, err := pg.kubeClient.AppsV1().ReplicaSets(namespace).Get(context.TODO(), name, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", kind, namespace, name, err)\n-\t\t\treturn map[string]string{}\n-\t\t}\n-\t\treturn rs.Annotations\n-\tcase \"DaemonSet\":\n-\t\tds, err := pg.kubeClient.AppsV1().DaemonSets(namespace).Get(context.TODO(), name, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", kind, namespace, name, err)\n-\t\t\treturn map[string]string{}\n+func (pg *pgcontroller) getAnnotationsFromUpperRes(pod *v1.Pod) map[string]string {\n+\tvar annotations = make(map[string]string)\n+\n+\tfor _, reference := range pod.OwnerReferences {\n+\t\tif reference.Kind != \"\" && reference.Name != \"\" {\n+\t\t\ttmp := make(map[string]string)\n+\t\t\tswitch reference.Kind {\n+\t\t\tcase \"ReplicaSet\":\n+\t\t\t\trs, err := pg.kubeClient.AppsV1().ReplicaSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = rs.Annotations\n+\t\t\tcase \"DaemonSet\":\n+\t\t\t\tds, err := pg.kubeClient.AppsV1().DaemonSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = ds.Annotations\n+\t\t\tcase \"StatefulSet\":\n+\t\t\t\tss, err := pg.kubeClient.AppsV1().StatefulSets(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = ss.Annotations\n+\t\t\tcase \"Job\":\n+\t\t\t\tjob, err := pg.kubeClient.BatchV1().Jobs(pod.Namespace).Get(context.TODO(), reference.Name, metav1.GetOptions{})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tklog.Errorf(\"Failed to get upper %s for Pod <%s/%s>: %v\", reference.Kind, pod.Namespace, reference.Name, err)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\ttmp = job.Annotations\n+\t\t\t}\n+\n+\t\t\tif err := mergo.Merge(&annotations, &tmp); err != nil {",
        "comment_created_at": "2025-04-21T07:34:41+00:00",
        "comment_author": "sceneryback",
        "comment_body": "fixed, please have a look @hwdef ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1894540966",
    "pr_number": 3825,
    "pr_file": "pkg/scheduler/actions/preempt/preempt.go",
    "created_at": "2024-12-21T02:59:56+00:00",
    "commented_code": "return false, fmt.Errorf(\"PrePredicate for task %s/%s failed for: %v\", preemptor.Namespace, preemptor.Name, err)\n \t}\n \n-\tpredicateFn := ssn.PredicateForPreemptAction\n+\tpredicateFn := pmpt.predicate\n+\tvar allNodeIntersections []*api.NodeInfo\n \t// we should filter out those nodes that are UnschedulableAndUnresolvable status got in allocate action\n \tallNodes := ssn.GetUnschedulableAndUnresolvableNodesForTask(preemptor)",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1894540966",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3825,
        "pr_file": "pkg/scheduler/actions/preempt/preempt.go",
        "discussion_id": "1894540966",
        "commented_code": "@@ -243,10 +264,19 @@ func (pmpt *Action) preempt(\n \t\treturn false, fmt.Errorf(\"PrePredicate for task %s/%s failed for: %v\", preemptor.Namespace, preemptor.Name, err)\n \t}\n \n-\tpredicateFn := ssn.PredicateForPreemptAction\n+\tpredicateFn := pmpt.predicate\n+\tvar allNodeIntersections []*api.NodeInfo\n \t// we should filter out those nodes that are UnschedulableAndUnresolvable status got in allocate action\n \tallNodes := ssn.GetUnschedulableAndUnresolvableNodesForTask(preemptor)",
        "comment_created_at": "2024-12-21T02:59:56+00:00",
        "comment_author": "lowang-bh",
        "comment_body": "We can filter out those nodes which have no evicable tasks in a function and just return a node list, and each node in the list has at least one preemptable task. \r\nI think it is enough to reduce nodes scale to avoid those invalid predication.\r\n\r\nrecord the preemptable task number in node infor is more effective than cal and record a node hash value. \r\nand preemptableNodeMap can also be removed",
        "pr_file_module": null
      },
      {
        "comment_id": "1896523084",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3825,
        "pr_file": "pkg/scheduler/actions/preempt/preempt.go",
        "discussion_id": "1894540966",
        "commented_code": "@@ -243,10 +264,19 @@ func (pmpt *Action) preempt(\n \t\treturn false, fmt.Errorf(\"PrePredicate for task %s/%s failed for: %v\", preemptor.Namespace, preemptor.Name, err)\n \t}\n \n-\tpredicateFn := ssn.PredicateForPreemptAction\n+\tpredicateFn := pmpt.predicate\n+\tvar allNodeIntersections []*api.NodeInfo\n \t// we should filter out those nodes that are UnschedulableAndUnresolvable status got in allocate action\n \tallNodes := ssn.GetUnschedulableAndUnresolvableNodesForTask(preemptor)",
        "comment_created_at": "2024-12-24T08:11:50+00:00",
        "comment_author": "molei20021",
        "comment_body": "but if there are many task in cluster and use leastrequested nodeorder strategy, almost all the nodes will have preemptees",
        "pr_file_module": null
      }
    ]
  }
]