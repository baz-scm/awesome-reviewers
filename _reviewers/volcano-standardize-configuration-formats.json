[
  {
    "discussion_id": "2206213702",
    "pr_number": 4454,
    "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
    "created_at": "2025-07-15T03:39:58+00:00",
    "commented_code": "## Alternatives\n \n ### Binpack VS ResourceStrategyFit\n-If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n\\ No newline at end of file\n+If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n+\n+## Best Practices\n+### AI scenario\n+In some AI scenarios, CPU tasks are usually dispersed into CPU machine groups to reduce hot spots. GPU tasks are gathered in the GPU machine group to reduce GPU fragmentation. At the same time, it is also necessary to avoid the situation that CPU tasks are assigned to GPU nodes, resulting in long-term waiting of GPU tasks due to insufficient CPU or MEM resources of nodes. In this scenario, we can combine **resourceStrategyFit** and **sra policy** to deal with this scenario. The corresponding example configuration is as follows:\n+\n+```yaml\n+  actions: \"enqueue, allocate, backfill, reclaim, preempt\"\n+  tiers:\n+    - plugins:\n+      - name: resource-strategy-fit\n+        arguments:\n+          resourceStrategyFitWeight: 10\n+          resources:\n+            nvidia.com/gpu:\n+              type: MostAllocated\n+              weight: 2\n+            cpu:\n+              type: LeastAllocated\n+              weight: 1\n+          sra.policy: retention",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2206213702",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4454,
        "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
        "discussion_id": "2206213702",
        "commented_code": "@@ -75,4 +75,28 @@ finalScoreNode = [(weight1 * resource1) + (weight2 * resource2) + \u2026 + (weightN\n ## Alternatives\n \n ### Binpack VS ResourceStrategyFit\n-If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n\\ No newline at end of file\n+If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n+\n+## Best Practices\n+### AI scenario\n+In some AI scenarios, CPU tasks are usually dispersed into CPU machine groups to reduce hot spots. GPU tasks are gathered in the GPU machine group to reduce GPU fragmentation. At the same time, it is also necessary to avoid the situation that CPU tasks are assigned to GPU nodes, resulting in long-term waiting of GPU tasks due to insufficient CPU or MEM resources of nodes. In this scenario, we can combine **resourceStrategyFit** and **sra policy** to deal with this scenario. The corresponding example configuration is as follows:\n+\n+```yaml\n+  actions: \"enqueue, allocate, backfill, reclaim, preempt\"\n+  tiers:\n+    - plugins:\n+      - name: resource-strategy-fit\n+        arguments:\n+          resourceStrategyFitWeight: 10\n+          resources:\n+            nvidia.com/gpu:\n+              type: MostAllocated\n+              weight: 2\n+            cpu:\n+              type: LeastAllocated\n+              weight: 1\n+          sra.policy: retention",
        "comment_created_at": "2025-07-15T03:39:58+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "This configuration seems a bit fragmented. SRA defines weights and resource types. The original definition by @LY-today also has a set of weights and resource types. Can this be reused? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2224073463",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4454,
        "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
        "discussion_id": "2206213702",
        "commented_code": "@@ -75,4 +75,28 @@ finalScoreNode = [(weight1 * resource1) + (weight2 * resource2) + \u2026 + (weightN\n ## Alternatives\n \n ### Binpack VS ResourceStrategyFit\n-If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n\\ No newline at end of file\n+If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n+\n+## Best Practices\n+### AI scenario\n+In some AI scenarios, CPU tasks are usually dispersed into CPU machine groups to reduce hot spots. GPU tasks are gathered in the GPU machine group to reduce GPU fragmentation. At the same time, it is also necessary to avoid the situation that CPU tasks are assigned to GPU nodes, resulting in long-term waiting of GPU tasks due to insufficient CPU or MEM resources of nodes. In this scenario, we can combine **resourceStrategyFit** and **sra policy** to deal with this scenario. The corresponding example configuration is as follows:\n+\n+```yaml\n+  actions: \"enqueue, allocate, backfill, reclaim, preempt\"\n+  tiers:\n+    - plugins:\n+      - name: resource-strategy-fit\n+        arguments:\n+          resourceStrategyFitWeight: 10\n+          resources:\n+            nvidia.com/gpu:\n+              type: MostAllocated\n+              weight: 2\n+            cpu:\n+              type: LeastAllocated\n+              weight: 1\n+          sra.policy: retention",
        "comment_created_at": "2025-07-23T00:33:29+00:00",
        "comment_author": "XbaoWu",
        "comment_body": "I tend to unify the style of configuration items into the style of `binpack`, as if other plugins are the same. What do you think\uff1f @LY-today ",
        "pr_file_module": null
      },
      {
        "comment_id": "2224112942",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4454,
        "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
        "discussion_id": "2206213702",
        "commented_code": "@@ -75,4 +75,28 @@ finalScoreNode = [(weight1 * resource1) + (weight2 * resource2) + \u2026 + (weightN\n ## Alternatives\n \n ### Binpack VS ResourceStrategyFit\n-If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n\\ No newline at end of file\n+If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n+\n+## Best Practices\n+### AI scenario\n+In some AI scenarios, CPU tasks are usually dispersed into CPU machine groups to reduce hot spots. GPU tasks are gathered in the GPU machine group to reduce GPU fragmentation. At the same time, it is also necessary to avoid the situation that CPU tasks are assigned to GPU nodes, resulting in long-term waiting of GPU tasks due to insufficient CPU or MEM resources of nodes. In this scenario, we can combine **resourceStrategyFit** and **sra policy** to deal with this scenario. The corresponding example configuration is as follows:\n+\n+```yaml\n+  actions: \"enqueue, allocate, backfill, reclaim, preempt\"\n+  tiers:\n+    - plugins:\n+      - name: resource-strategy-fit\n+        arguments:\n+          resourceStrategyFitWeight: 10\n+          resources:\n+            nvidia.com/gpu:\n+              type: MostAllocated\n+              weight: 2\n+            cpu:\n+              type: LeastAllocated\n+              weight: 1\n+          sra.policy: retention",
        "comment_created_at": "2025-07-23T01:17:31+00:00",
        "comment_author": "XbaoWu",
        "comment_body": "The definition of resources I suggest separate better, the reasons are as follows : \r\n1. Different semantic uses. The resource defined by `sra` is used to avoid, and `resourceStrategyFit` is used to determine whether to aggregate or disperse. \r\n2. Plugin structure. Because there are two strategies in a plugin at present, different strategies adjust the scheduling according to the resources, there are independent use scenarios, and the resource items may not be completely consistent. It is not like a scheduling strategy that applies a superposition effect around the same resource. \r\n3. Usage. When the user decides to turn off the `sra` policy, it is also necessary to pay attention to whether the `resourceStrategyFit` resource group refers to this part of the resource, which may be somewhat inconvenient.",
        "pr_file_module": null
      },
      {
        "comment_id": "2224769725",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4454,
        "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
        "discussion_id": "2206213702",
        "commented_code": "@@ -75,4 +75,28 @@ finalScoreNode = [(weight1 * resource1) + (weight2 * resource2) + \u2026 + (weightN\n ## Alternatives\n \n ### Binpack VS ResourceStrategyFit\n-If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n\\ No newline at end of file\n+If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n+\n+## Best Practices\n+### AI scenario\n+In some AI scenarios, CPU tasks are usually dispersed into CPU machine groups to reduce hot spots. GPU tasks are gathered in the GPU machine group to reduce GPU fragmentation. At the same time, it is also necessary to avoid the situation that CPU tasks are assigned to GPU nodes, resulting in long-term waiting of GPU tasks due to insufficient CPU or MEM resources of nodes. In this scenario, we can combine **resourceStrategyFit** and **sra policy** to deal with this scenario. The corresponding example configuration is as follows:\n+\n+```yaml\n+  actions: \"enqueue, allocate, backfill, reclaim, preempt\"\n+  tiers:\n+    - plugins:\n+      - name: resource-strategy-fit\n+        arguments:\n+          resourceStrategyFitWeight: 10\n+          resources:\n+            nvidia.com/gpu:\n+              type: MostAllocated\n+              weight: 2\n+            cpu:\n+              type: LeastAllocated\n+              weight: 1\n+          sra.policy: retention",
        "comment_created_at": "2025-07-23T08:14:13+00:00",
        "comment_author": "LY-today",
        "comment_body": "Between the architecture design of the scheduler framework and the best practices of koordinator, I personally recommend using struct form to parse parameters, because we have incubated simulated scheduler components in our internal scenarios in the past, which is more friendly to struct.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2224776008",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4454,
        "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
        "discussion_id": "2206213702",
        "commented_code": "@@ -75,4 +75,28 @@ finalScoreNode = [(weight1 * resource1) + (weight2 * resource2) + \u2026 + (weightN\n ## Alternatives\n \n ### Binpack VS ResourceStrategyFit\n-If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n\\ No newline at end of file\n+If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n+\n+## Best Practices\n+### AI scenario\n+In some AI scenarios, CPU tasks are usually dispersed into CPU machine groups to reduce hot spots. GPU tasks are gathered in the GPU machine group to reduce GPU fragmentation. At the same time, it is also necessary to avoid the situation that CPU tasks are assigned to GPU nodes, resulting in long-term waiting of GPU tasks due to insufficient CPU or MEM resources of nodes. In this scenario, we can combine **resourceStrategyFit** and **sra policy** to deal with this scenario. The corresponding example configuration is as follows:\n+\n+```yaml\n+  actions: \"enqueue, allocate, backfill, reclaim, preempt\"\n+  tiers:\n+    - plugins:\n+      - name: resource-strategy-fit\n+        arguments:\n+          resourceStrategyFitWeight: 10\n+          resources:\n+            nvidia.com/gpu:\n+              type: MostAllocated\n+              weight: 2\n+            cpu:\n+              type: LeastAllocated\n+              weight: 1\n+          sra.policy: retention",
        "comment_created_at": "2025-07-23T08:16:49+00:00",
        "comment_author": "LY-today",
        "comment_body": "Of course, I am willing to accept the official standardized process of Volcano, so there is no mandatory requirement for struct here. Do you have any suggestions?\r\n @JesseStutler ",
        "pr_file_module": null
      },
      {
        "comment_id": "2230042789",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4454,
        "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
        "discussion_id": "2206213702",
        "commented_code": "@@ -75,4 +75,28 @@ finalScoreNode = [(weight1 * resource1) + (weight2 * resource2) + \u2026 + (weightN\n ## Alternatives\n \n ### Binpack VS ResourceStrategyFit\n-If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n\\ No newline at end of file\n+If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n+\n+## Best Practices\n+### AI scenario\n+In some AI scenarios, CPU tasks are usually dispersed into CPU machine groups to reduce hot spots. GPU tasks are gathered in the GPU machine group to reduce GPU fragmentation. At the same time, it is also necessary to avoid the situation that CPU tasks are assigned to GPU nodes, resulting in long-term waiting of GPU tasks due to insufficient CPU or MEM resources of nodes. In this scenario, we can combine **resourceStrategyFit** and **sra policy** to deal with this scenario. The corresponding example configuration is as follows:\n+\n+```yaml\n+  actions: \"enqueue, allocate, backfill, reclaim, preempt\"\n+  tiers:\n+    - plugins:\n+      - name: resource-strategy-fit\n+        arguments:\n+          resourceStrategyFitWeight: 10\n+          resources:\n+            nvidia.com/gpu:\n+              type: MostAllocated\n+              weight: 2\n+            cpu:\n+              type: LeastAllocated\n+              weight: 1\n+          sra.policy: retention",
        "comment_created_at": "2025-07-25T03:09:33+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "Volcano does not have a unified format, but since you have already deployed it in an internal cluster, we can unify it into struct @LY-today ",
        "pr_file_module": null
      },
      {
        "comment_id": "2230468887",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4454,
        "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
        "discussion_id": "2206213702",
        "commented_code": "@@ -75,4 +75,28 @@ finalScoreNode = [(weight1 * resource1) + (weight2 * resource2) + \u2026 + (weightN\n ## Alternatives\n \n ### Binpack VS ResourceStrategyFit\n-If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n\\ No newline at end of file\n+If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n+\n+## Best Practices\n+### AI scenario\n+In some AI scenarios, CPU tasks are usually dispersed into CPU machine groups to reduce hot spots. GPU tasks are gathered in the GPU machine group to reduce GPU fragmentation. At the same time, it is also necessary to avoid the situation that CPU tasks are assigned to GPU nodes, resulting in long-term waiting of GPU tasks due to insufficient CPU or MEM resources of nodes. In this scenario, we can combine **resourceStrategyFit** and **sra policy** to deal with this scenario. The corresponding example configuration is as follows:\n+\n+```yaml\n+  actions: \"enqueue, allocate, backfill, reclaim, preempt\"\n+  tiers:\n+    - plugins:\n+      - name: resource-strategy-fit\n+        arguments:\n+          resourceStrategyFitWeight: 10\n+          resources:\n+            nvidia.com/gpu:\n+              type: MostAllocated\n+              weight: 2\n+            cpu:\n+              type: LeastAllocated\n+              weight: 1\n+          sra.policy: retention",
        "comment_created_at": "2025-07-25T08:13:40+00:00",
        "comment_author": "LY-today",
        "comment_body": "get",
        "pr_file_module": null
      },
      {
        "comment_id": "2232403737",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4454,
        "pr_file": "docs/design/resource-strategy-fit-scheduling.md",
        "discussion_id": "2206213702",
        "commented_code": "@@ -75,4 +75,28 @@ finalScoreNode = [(weight1 * resource1) + (weight2 * resource2) + \u2026 + (weightN\n ## Alternatives\n \n ### Binpack VS ResourceStrategyFit\n-If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n\\ No newline at end of file\n+If you want to use the clustering strategy for all resource types, you can choose the Binpack plugin. If you need to configure different clustering or scattering strategies for different resource types, you can choose the ResourceStrategyFit plugin. ResourceStrategyFit can also achieve the same results as Binpack by adjusting configuration parameters.\n+\n+## Best Practices\n+### AI scenario\n+In some AI scenarios, CPU tasks are usually dispersed into CPU machine groups to reduce hot spots. GPU tasks are gathered in the GPU machine group to reduce GPU fragmentation. At the same time, it is also necessary to avoid the situation that CPU tasks are assigned to GPU nodes, resulting in long-term waiting of GPU tasks due to insufficient CPU or MEM resources of nodes. In this scenario, we can combine **resourceStrategyFit** and **sra policy** to deal with this scenario. The corresponding example configuration is as follows:\n+\n+```yaml\n+  actions: \"enqueue, allocate, backfill, reclaim, preempt\"\n+  tiers:\n+    - plugins:\n+      - name: resource-strategy-fit\n+        arguments:\n+          resourceStrategyFitWeight: 10\n+          resources:\n+            nvidia.com/gpu:\n+              type: MostAllocated\n+              weight: 2\n+            cpu:\n+              type: LeastAllocated\n+              weight: 1\n+          sra.policy: retention",
        "comment_created_at": "2025-07-26T03:07:02+00:00",
        "comment_author": "XbaoWu",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2043274971",
    "pr_number": 4163,
    "pr_file": "docs/design/queue-level-scheduling-policy.md",
    "created_at": "2025-04-15T01:12:09+00:00",
    "commented_code": "+In the current design, Volcano adopts a global scheduling policy defined in a centralized configuration file. \n+As a result, all queues and jobs within the cluster share the same set of scheduling actions and plugins, \n+regardless of their specific characteristics. \n+However, with the increasing adoption of diverse workloads such as \n+training and inference tasks even-within the same tenant\u2014there is a growing demand for differentiated scheduling strategies. \n+To address this, it is essential to introduce support for scheduling policies at both the queue and job levels.\n+\n+# 1\u3001Changes to the CRD\n+\n+## 1.1\u3001Extend the queue CRD\n+\n+Add the `SchedulerPolicy` in the `QueueSpec`, allowing the queue to define its own scheduling policy configuration.\n+\n+```go\n+type QueueSpec struct {\n+    Weight     int32\n+    Capability v1.ResourceList\n+    State QueueState\n+    Reclaimable *bool\n+    ExtendClusters []Cluster\n+    Guarantee Guarantee `json:\"guarantee,omitempty\" protobuf:\"bytes,4,opt,name=guarantee\"`\n+    Affinity *Affinity `json:\"affinity,omitempty\" protobuf:\"bytes,6,opt,name=affinity\"`\n+    Type string `json:\"type,omitempty\" protobuf:\"bytes,7,opt,name=type\"`\n+    Parent string `json:\"parent,omitempty\" protobuf:\"bytes,8,opt,name=parent\"`\n+    Deserved v1.ResourceList `json:\"deserved,omitempty\" protobuf:\"bytes,9,opt,name=deserved\"`\n+    Priority int32 `json:\"priority,omitempty\" protobuf:\"bytes,10,opt,name=priority\"`\n+\t\n+    // Add SchedulerPolicy to point to the scheduling policy defined in the configuration file\n+    SchedulerPolicy string\n+}\n+\n+```\n+\n+## 1.2\u3001Extend the Job annotation\n+\n+To avoid potential issues caused by modifying the `JobSpec` directly, \n+job-level scheduling policies are specified via `annotations` in the Job definition.\n+\n+## 1.3\u3001Usage examples of scheduling policies at the queue and job levels\n+\n+An example for users is as follows:\n+\n+By configuring a globally unique ConfigMap to store actions and plugins.\n+\n+```yaml\n+apiVersion: v1\n+kind: ConfigMap\n+metadata:\n+  name: volcano-scheduler-policies\n+  namespace: volcano-system\n+data:\n+  # Queue-level policy (Key is the SchedulerPolicy name).\n+  SchedulerPolicy-example-1: |\n+    actions:\n+      - enqueue    \n+      - allocate   \n+      - preempt    \n+    plugins:\n+      - name: priority\n+        args:",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2043274971",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4163,
        "pr_file": "docs/design/queue-level-scheduling-policy.md",
        "discussion_id": "2043274971",
        "commented_code": "@@ -0,0 +1,612 @@\n+In the current design, Volcano adopts a global scheduling policy defined in a centralized configuration file. \n+As a result, all queues and jobs within the cluster share the same set of scheduling actions and plugins, \n+regardless of their specific characteristics. \n+However, with the increasing adoption of diverse workloads such as \n+training and inference tasks even-within the same tenant\u2014there is a growing demand for differentiated scheduling strategies. \n+To address this, it is essential to introduce support for scheduling policies at both the queue and job levels.\n+\n+# 1\u3001Changes to the CRD\n+\n+## 1.1\u3001Extend the queue CRD\n+\n+Add the `SchedulerPolicy` in the `QueueSpec`, allowing the queue to define its own scheduling policy configuration.\n+\n+```go\n+type QueueSpec struct {\n+    Weight     int32\n+    Capability v1.ResourceList\n+    State QueueState\n+    Reclaimable *bool\n+    ExtendClusters []Cluster\n+    Guarantee Guarantee `json:\"guarantee,omitempty\" protobuf:\"bytes,4,opt,name=guarantee\"`\n+    Affinity *Affinity `json:\"affinity,omitempty\" protobuf:\"bytes,6,opt,name=affinity\"`\n+    Type string `json:\"type,omitempty\" protobuf:\"bytes,7,opt,name=type\"`\n+    Parent string `json:\"parent,omitempty\" protobuf:\"bytes,8,opt,name=parent\"`\n+    Deserved v1.ResourceList `json:\"deserved,omitempty\" protobuf:\"bytes,9,opt,name=deserved\"`\n+    Priority int32 `json:\"priority,omitempty\" protobuf:\"bytes,10,opt,name=priority\"`\n+\t\n+    // Add SchedulerPolicy to point to the scheduling policy defined in the configuration file\n+    SchedulerPolicy string\n+}\n+\n+```\n+\n+## 1.2\u3001Extend the Job annotation\n+\n+To avoid potential issues caused by modifying the `JobSpec` directly, \n+job-level scheduling policies are specified via `annotations` in the Job definition.\n+\n+## 1.3\u3001Usage examples of scheduling policies at the queue and job levels\n+\n+An example for users is as follows:\n+\n+By configuring a globally unique ConfigMap to store actions and plugins.\n+\n+```yaml\n+apiVersion: v1\n+kind: ConfigMap\n+metadata:\n+  name: volcano-scheduler-policies\n+  namespace: volcano-system\n+data:\n+  # Queue-level policy (Key is the SchedulerPolicy name).\n+  SchedulerPolicy-example-1: |\n+    actions:\n+      - enqueue    \n+      - allocate   \n+      - preempt    \n+    plugins:\n+      - name: priority\n+        args:",
        "comment_created_at": "2025-04-15T01:12:09+00:00",
        "comment_author": "Monokaix",
        "comment_body": "Please keep the format and fields consistent with default configMap.",
        "pr_file_module": null
      },
      {
        "comment_id": "2043352797",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4163,
        "pr_file": "docs/design/queue-level-scheduling-policy.md",
        "discussion_id": "2043274971",
        "commented_code": "@@ -0,0 +1,612 @@\n+In the current design, Volcano adopts a global scheduling policy defined in a centralized configuration file. \n+As a result, all queues and jobs within the cluster share the same set of scheduling actions and plugins, \n+regardless of their specific characteristics. \n+However, with the increasing adoption of diverse workloads such as \n+training and inference tasks even-within the same tenant\u2014there is a growing demand for differentiated scheduling strategies. \n+To address this, it is essential to introduce support for scheduling policies at both the queue and job levels.\n+\n+# 1\u3001Changes to the CRD\n+\n+## 1.1\u3001Extend the queue CRD\n+\n+Add the `SchedulerPolicy` in the `QueueSpec`, allowing the queue to define its own scheduling policy configuration.\n+\n+```go\n+type QueueSpec struct {\n+    Weight     int32\n+    Capability v1.ResourceList\n+    State QueueState\n+    Reclaimable *bool\n+    ExtendClusters []Cluster\n+    Guarantee Guarantee `json:\"guarantee,omitempty\" protobuf:\"bytes,4,opt,name=guarantee\"`\n+    Affinity *Affinity `json:\"affinity,omitempty\" protobuf:\"bytes,6,opt,name=affinity\"`\n+    Type string `json:\"type,omitempty\" protobuf:\"bytes,7,opt,name=type\"`\n+    Parent string `json:\"parent,omitempty\" protobuf:\"bytes,8,opt,name=parent\"`\n+    Deserved v1.ResourceList `json:\"deserved,omitempty\" protobuf:\"bytes,9,opt,name=deserved\"`\n+    Priority int32 `json:\"priority,omitempty\" protobuf:\"bytes,10,opt,name=priority\"`\n+\t\n+    // Add SchedulerPolicy to point to the scheduling policy defined in the configuration file\n+    SchedulerPolicy string\n+}\n+\n+```\n+\n+## 1.2\u3001Extend the Job annotation\n+\n+To avoid potential issues caused by modifying the `JobSpec` directly, \n+job-level scheduling policies are specified via `annotations` in the Job definition.\n+\n+## 1.3\u3001Usage examples of scheduling policies at the queue and job levels\n+\n+An example for users is as follows:\n+\n+By configuring a globally unique ConfigMap to store actions and plugins.\n+\n+```yaml\n+apiVersion: v1\n+kind: ConfigMap\n+metadata:\n+  name: volcano-scheduler-policies\n+  namespace: volcano-system\n+data:\n+  # Queue-level policy (Key is the SchedulerPolicy name).\n+  SchedulerPolicy-example-1: |\n+    actions:\n+      - enqueue    \n+      - allocate   \n+      - preempt    \n+    plugins:\n+      - name: priority\n+        args:",
        "comment_created_at": "2025-04-15T02:06:52+00:00",
        "comment_author": "ElectricFish7",
        "comment_body": "done!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1817992083",
    "pr_number": 3775,
    "pr_file": "docs/design/node-resource-reservation-design.md",
    "created_at": "2024-10-27T06:28:41+00:00",
    "commented_code": "+# Volcano node resource reservation\n+## background\n+* Consider such situation: there are thounsands of pods to scheduler a day, in the first hour 500 low priority pods are created and schedulered which used 99% of cluster resource, in the second hour 10 high priority pods are created, however, low priority pods are still running, high priority pods can not be scheduled due to lack of resource.\n+* The user want the high priority task in the second hour to have resource to schedule immediately every day and high priority task better not preempt low priority pods because some pod may have already run many days.\n+## design\n+![annotation](images/node-resource-reservation-annotation.png) \n+### recognize high priority pods\n+There are two ways to recognize high priority pods:\n+* set annotation volcano.sh/is-reserve: 1 in podgroup \n+which means all pods under podgroup are reserve tasks\n+* set annotation volcano.sh/is-reserve: 1 in pod\n+which means this pod is reserve task\n+### recognize pod max running time\n+* set annotation volcano.sh/runsec-max: 500 in podgroup\n+which means all pods under podgroup will run 500 seconds max\n+* set annotation volcano.sh/runsec-max: 500 in pod\n+which means this pod will run 500 seconds max\n+### reserve plugin\n+#### configuration\n+```\n+- plugins:\n+   - name: reserve\n+     arguments:\n+       reserve.nodeLabel: label1,label2\n+       reserve.define.label1: {\"business_type\": \"ebook\"}\n+       reserve.resources.label1: [{\"start_hour\": 3, \"end_hour\": 4, \"cpu\": 32, \"memory\": 64, \"start_reserve_ago\": \"2h\", \"pod_num\": 10, \"cron\": \"daily\"}]",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1817992083",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3775,
        "pr_file": "docs/design/node-resource-reservation-design.md",
        "discussion_id": "1817992083",
        "commented_code": "@@ -0,0 +1,45 @@\n+# Volcano node resource reservation\n+## background\n+* Consider such situation: there are thounsands of pods to scheduler a day, in the first hour 500 low priority pods are created and schedulered which used 99% of cluster resource, in the second hour 10 high priority pods are created, however, low priority pods are still running, high priority pods can not be scheduled due to lack of resource.\n+* The user want the high priority task in the second hour to have resource to schedule immediately every day and high priority task better not preempt low priority pods because some pod may have already run many days.\n+## design\n+![annotation](images/node-resource-reservation-annotation.png) \n+### recognize high priority pods\n+There are two ways to recognize high priority pods:\n+* set annotation volcano.sh/is-reserve: 1 in podgroup \n+which means all pods under podgroup are reserve tasks\n+* set annotation volcano.sh/is-reserve: 1 in pod\n+which means this pod is reserve task\n+### recognize pod max running time\n+* set annotation volcano.sh/runsec-max: 500 in podgroup\n+which means all pods under podgroup will run 500 seconds max\n+* set annotation volcano.sh/runsec-max: 500 in pod\n+which means this pod will run 500 seconds max\n+### reserve plugin\n+#### configuration\n+```\n+- plugins:\n+   - name: reserve\n+     arguments:\n+       reserve.nodeLabel: label1,label2\n+       reserve.define.label1: {\"business_type\": \"ebook\"}\n+       reserve.resources.label1: [{\"start_hour\": 3, \"end_hour\": 4, \"cpu\": 32, \"memory\": 64, \"start_reserve_ago\": \"2h\", \"pod_num\": 10, \"cron\": \"daily\"}]",
        "comment_created_at": "2024-10-27T06:28:41+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "The format of the arguments here doesn't seem very formal. Can they be defined as APIs, or the arguments can be defined more formally? Like:\r\n```yaml\r\narguments:\r\n  reserveLabel:\r\n  - nodeSelector: label1\r\n     startHour:\r\n     endHour:\r\n     resources:\r\n        cpu:32\r\n        memory: 64\r\n        xxx\r\n     ...\r\n  - nodeSelector: label2\r\n     ...\r\n```\r\nJust give an example, we can discuss more later.",
        "pr_file_module": null
      },
      {
        "comment_id": "1823813583",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3775,
        "pr_file": "docs/design/node-resource-reservation-design.md",
        "discussion_id": "1817992083",
        "commented_code": "@@ -0,0 +1,45 @@\n+# Volcano node resource reservation\n+## background\n+* Consider such situation: there are thounsands of pods to scheduler a day, in the first hour 500 low priority pods are created and schedulered which used 99% of cluster resource, in the second hour 10 high priority pods are created, however, low priority pods are still running, high priority pods can not be scheduled due to lack of resource.\n+* The user want the high priority task in the second hour to have resource to schedule immediately every day and high priority task better not preempt low priority pods because some pod may have already run many days.\n+## design\n+![annotation](images/node-resource-reservation-annotation.png) \n+### recognize high priority pods\n+There are two ways to recognize high priority pods:\n+* set annotation volcano.sh/is-reserve: 1 in podgroup \n+which means all pods under podgroup are reserve tasks\n+* set annotation volcano.sh/is-reserve: 1 in pod\n+which means this pod is reserve task\n+### recognize pod max running time\n+* set annotation volcano.sh/runsec-max: 500 in podgroup\n+which means all pods under podgroup will run 500 seconds max\n+* set annotation volcano.sh/runsec-max: 500 in pod\n+which means this pod will run 500 seconds max\n+### reserve plugin\n+#### configuration\n+```\n+- plugins:\n+   - name: reserve\n+     arguments:\n+       reserve.nodeLabel: label1,label2\n+       reserve.define.label1: {\"business_type\": \"ebook\"}\n+       reserve.resources.label1: [{\"start_hour\": 3, \"end_hour\": 4, \"cpu\": 32, \"memory\": 64, \"start_reserve_ago\": \"2h\", \"pod_num\": 10, \"cron\": \"daily\"}]",
        "comment_created_at": "2024-10-31T06:11:43+00:00",
        "comment_author": "molei20021",
        "comment_body": "i have updated the document",
        "pr_file_module": null
      }
    ]
  }
]