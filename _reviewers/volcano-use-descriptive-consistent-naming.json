[
  {
    "discussion_id": "1817990749",
    "pr_number": 3775,
    "pr_file": "docs/design/node-resource-reservation-design.md",
    "created_at": "2024-10-27T06:17:50+00:00",
    "commented_code": "+# Volcano node resource reservation\n+## background\n+* Consider such situation: there are thounsands of pods to scheduler a day, in the first hour 500 low priority pods are created and schedulered which used 99% of cluster resource, in the second hour 10 high priority pods are created, however, low priority pods are still running, high priority pods can not be scheduled due to lack of resource.\n+* The user want the high priority task in the second hour to have resource to schedule immediately every day and high priority task better not preempt low priority pods because some pod may have already run many days.\n+## design\n+![annotation](images/node-resource-reservation-annotation.png) \n+### recognize high priority pods\n+There are two ways to recognize high priority pods:\n+* set annotation volcano.sh/is-reserve: 1 in podgroup",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1817990749",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3775,
        "pr_file": "docs/design/node-resource-reservation-design.md",
        "discussion_id": "1817990749",
        "commented_code": "@@ -0,0 +1,45 @@\n+# Volcano node resource reservation\n+## background\n+* Consider such situation: there are thounsands of pods to scheduler a day, in the first hour 500 low priority pods are created and schedulered which used 99% of cluster resource, in the second hour 10 high priority pods are created, however, low priority pods are still running, high priority pods can not be scheduled due to lack of resource.\n+* The user want the high priority task in the second hour to have resource to schedule immediately every day and high priority task better not preempt low priority pods because some pod may have already run many days.\n+## design\n+![annotation](images/node-resource-reservation-annotation.png) \n+### recognize high priority pods\n+There are two ways to recognize high priority pods:\n+* set annotation volcano.sh/is-reserve: 1 in podgroup ",
        "comment_created_at": "2024-10-27T06:17:50+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "`volcano.sh/is-reserve:1` is not such formal, maybe use `volcano.sh/reserveable: true` is better.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1817991111",
    "pr_number": 3775,
    "pr_file": "docs/design/node-resource-reservation-design.md",
    "created_at": "2024-10-27T06:20:16+00:00",
    "commented_code": "+# Volcano node resource reservation\n+## background\n+* Consider such situation: there are thounsands of pods to scheduler a day, in the first hour 500 low priority pods are created and schedulered which used 99% of cluster resource, in the second hour 10 high priority pods are created, however, low priority pods are still running, high priority pods can not be scheduled due to lack of resource.\n+* The user want the high priority task in the second hour to have resource to schedule immediately every day and high priority task better not preempt low priority pods because some pod may have already run many days.\n+## design\n+![annotation](images/node-resource-reservation-annotation.png) \n+### recognize high priority pods\n+There are two ways to recognize high priority pods:\n+* set annotation volcano.sh/is-reserve: 1 in podgroup \n+which means all pods under podgroup are reserve tasks\n+* set annotation volcano.sh/is-reserve: 1 in pod\n+which means this pod is reserve task\n+### recognize pod max running time\n+* set annotation volcano.sh/runsec-max: 500 in podgroup",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1817991111",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3775,
        "pr_file": "docs/design/node-resource-reservation-design.md",
        "discussion_id": "1817991111",
        "commented_code": "@@ -0,0 +1,45 @@\n+# Volcano node resource reservation\n+## background\n+* Consider such situation: there are thounsands of pods to scheduler a day, in the first hour 500 low priority pods are created and schedulered which used 99% of cluster resource, in the second hour 10 high priority pods are created, however, low priority pods are still running, high priority pods can not be scheduled due to lack of resource.\n+* The user want the high priority task in the second hour to have resource to schedule immediately every day and high priority task better not preempt low priority pods because some pod may have already run many days.\n+## design\n+![annotation](images/node-resource-reservation-annotation.png) \n+### recognize high priority pods\n+There are two ways to recognize high priority pods:\n+* set annotation volcano.sh/is-reserve: 1 in podgroup \n+which means all pods under podgroup are reserve tasks\n+* set annotation volcano.sh/is-reserve: 1 in pod\n+which means this pod is reserve task\n+### recognize pod max running time\n+* set annotation volcano.sh/runsec-max: 500 in podgroup",
        "comment_created_at": "2024-10-27T06:20:16+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "same as `volcano.sh/runsec-max: 500`, such name is not so formal, use `volcano.sh/maximum-runtime: 500s` is better",
        "pr_file_module": null
      },
      {
        "comment_id": "1823814731",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3775,
        "pr_file": "docs/design/node-resource-reservation-design.md",
        "discussion_id": "1817991111",
        "commented_code": "@@ -0,0 +1,45 @@\n+# Volcano node resource reservation\n+## background\n+* Consider such situation: there are thounsands of pods to scheduler a day, in the first hour 500 low priority pods are created and schedulered which used 99% of cluster resource, in the second hour 10 high priority pods are created, however, low priority pods are still running, high priority pods can not be scheduled due to lack of resource.\n+* The user want the high priority task in the second hour to have resource to schedule immediately every day and high priority task better not preempt low priority pods because some pod may have already run many days.\n+## design\n+![annotation](images/node-resource-reservation-annotation.png) \n+### recognize high priority pods\n+There are two ways to recognize high priority pods:\n+* set annotation volcano.sh/is-reserve: 1 in podgroup \n+which means all pods under podgroup are reserve tasks\n+* set annotation volcano.sh/is-reserve: 1 in pod\n+which means this pod is reserve task\n+### recognize pod max running time\n+* set annotation volcano.sh/runsec-max: 500 in podgroup",
        "comment_created_at": "2024-10-31T06:13:43+00:00",
        "comment_author": "molei20021",
        "comment_body": "thanks for your opinion, i have updated",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1846437987",
    "pr_number": 3775,
    "pr_file": "docs/design/node-resource-reservation-design.md",
    "created_at": "2024-11-18T11:37:11+00:00",
    "commented_code": "+# Volcano node resource reservation\n+## background\n+* Consider such situation: there are thounsands of pods to be scheduled evert day, in 1 to 2 o'clock 500 low priority pods are created and schedulered which used 99% of cluster resource, in 2 to 3 o'clock 10 high priority pods are created, however, low priority pods are still running, high priority pods can not be scheduled due to lack of resource.\n+* Users want high priority tasks in 2 to 3 o'clock have resource to schedule immediately every day and high priority tasks not preempt low priority pods because some low priority pods have already run many days.\n+## design\n+![annotation](images/node-resource-reservation-annotation.png) \n+### recognize high priority pods\n+There are two ways to recognize high priority pods:\n+* set annotation volcano.sh/reserveable: true in podgroup \n+which means all pods under podgroup are reserve tasks\n+* set annotation volcano.sh/reserveable: true in pod\n+which means this pod is reserve task\n+### recognize pod max running time\n+* set annotation volcano.sh/maximum-runtime: 500s in podgroup\n+which means the podgroup will run for a maximum of 500 seconds\n+* set annotation volcano.sh/maximum-runtime: 500s in pod\n+which means this pod will run for a maximum of 500 seconds\n+### reserve plugin\n+#### configuration\n+```\n+- plugins:\n+   - name: reserve\n+     arguments:\n+       reservelabels:\n+       - nodeSelector:\n+           business_type: ebook\n+         startHour: 3\n+         endHour: 4\n+         resources:\n+           cpu: \"32\"\n+           memory: 64Gi\n+         startReserveAgo: 2h\n+         podNumToReserve: 10\n+         cron: daily\n+```\n+In the configuration, reservelabels is consisted by nodeSelector which represent a node list, resources represent a list of resource reservation configuration.The overall meaning is that from 3 to 4 o'clock every day, 32 cpu, 64Gi memory need to be reserved and should start reserve 2h ago, if 10 reserve tasks are scheduled during reserve time range, stop reserve. which can save more resources for non-reserved tasks after 10 reserved tasks are scheduled during reserve period.\n+#### OpenSession\n+* make cache of nodeForbidMap which is used to cache forbidden nodes to forbid non-reserved tasks to be scheduled on reserved nodes, the calculation algorithm is as follows: firstly, order the nodes desc by node idle. Node idle is consisted of node resource unused and the resource will be released in the future before reserve start time which is taken by the annotation of pod max running time. secondly, traverse the ordered nodes, accumulate the node allocatable resource, if the accumulated resource is less than the resource to be reserved, add the node to nodeForbidMap which means the system will have the trend to reserve big resource other than many small resources.\n+* make cache of reservedTaskPendingResource which is used to cache the accumulated resource of pending tasks\n+* make cache of reservedTaskAllocatedResource which is used to cache the accumulated resource of allocated tasks\n+* make cache of resourceIdle which is used to accumulate the node futureidle resource.\n+* register plugin function PredicateFn\n+* register event handler AllocateFunc and DeallocateFunc to dynamically update the cache of reservedTaskPendingResource, reservedTaskAllocatedResource and resourceIdle\n+#### PredicateFn\n+Predicate is used to restrict other pods to be scheduled on reserved nodes. Reserved nodes are filtered out from the list of nodes and will change dynamically. \n+* check if the task is a reserve task, if yes, permit the task to be scheduled on this node.\n+* check if the current time is within the reserved time range, if no, permit the non-reserved task to be scheduled on this node.\n+* check if the number of reserve pods which have been scheduled is larger than the max pod number configured, if yes, permit the non-reserved task to be scheduled on this node.\n+* check if the node is in reserve node list(from nodeForbidMap cache), if yes, deny the non-reserved task to be scheduled on this node.",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1846437987",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3775,
        "pr_file": "docs/design/node-resource-reservation-design.md",
        "discussion_id": "1846437987",
        "commented_code": "@@ -0,0 +1,56 @@\n+# Volcano node resource reservation\n+## background\n+* Consider such situation: there are thounsands of pods to be scheduled evert day, in 1 to 2 o'clock 500 low priority pods are created and schedulered which used 99% of cluster resource, in 2 to 3 o'clock 10 high priority pods are created, however, low priority pods are still running, high priority pods can not be scheduled due to lack of resource.\n+* Users want high priority tasks in 2 to 3 o'clock have resource to schedule immediately every day and high priority tasks not preempt low priority pods because some low priority pods have already run many days.\n+## design\n+![annotation](images/node-resource-reservation-annotation.png) \n+### recognize high priority pods\n+There are two ways to recognize high priority pods:\n+* set annotation volcano.sh/reserveable: true in podgroup \n+which means all pods under podgroup are reserve tasks\n+* set annotation volcano.sh/reserveable: true in pod\n+which means this pod is reserve task\n+### recognize pod max running time\n+* set annotation volcano.sh/maximum-runtime: 500s in podgroup\n+which means the podgroup will run for a maximum of 500 seconds\n+* set annotation volcano.sh/maximum-runtime: 500s in pod\n+which means this pod will run for a maximum of 500 seconds\n+### reserve plugin\n+#### configuration\n+```\n+- plugins:\n+   - name: reserve\n+     arguments:\n+       reservelabels:\n+       - nodeSelector:\n+           business_type: ebook\n+         startHour: 3\n+         endHour: 4\n+         resources:\n+           cpu: \"32\"\n+           memory: 64Gi\n+         startReserveAgo: 2h\n+         podNumToReserve: 10\n+         cron: daily\n+```\n+In the configuration, reservelabels is consisted by nodeSelector which represent a node list, resources represent a list of resource reservation configuration.The overall meaning is that from 3 to 4 o'clock every day, 32 cpu, 64Gi memory need to be reserved and should start reserve 2h ago, if 10 reserve tasks are scheduled during reserve time range, stop reserve. which can save more resources for non-reserved tasks after 10 reserved tasks are scheduled during reserve period.\n+#### OpenSession\n+* make cache of nodeForbidMap which is used to cache forbidden nodes to forbid non-reserved tasks to be scheduled on reserved nodes, the calculation algorithm is as follows: firstly, order the nodes desc by node idle. Node idle is consisted of node resource unused and the resource will be released in the future before reserve start time which is taken by the annotation of pod max running time. secondly, traverse the ordered nodes, accumulate the node allocatable resource, if the accumulated resource is less than the resource to be reserved, add the node to nodeForbidMap which means the system will have the trend to reserve big resource other than many small resources.\n+* make cache of reservedTaskPendingResource which is used to cache the accumulated resource of pending tasks\n+* make cache of reservedTaskAllocatedResource which is used to cache the accumulated resource of allocated tasks\n+* make cache of resourceIdle which is used to accumulate the node futureidle resource.\n+* register plugin function PredicateFn\n+* register event handler AllocateFunc and DeallocateFunc to dynamically update the cache of reservedTaskPendingResource, reservedTaskAllocatedResource and resourceIdle\n+#### PredicateFn\n+Predicate is used to restrict other pods to be scheduled on reserved nodes. Reserved nodes are filtered out from the list of nodes and will change dynamically. \n+* check if the task is a reserve task, if yes, permit the task to be scheduled on this node.\n+* check if the current time is within the reserved time range, if no, permit the non-reserved task to be scheduled on this node.\n+* check if the number of reserve pods which have been scheduled is larger than the max pod number configured, if yes, permit the non-reserved task to be scheduled on this node.\n+* check if the node is in reserve node list(from nodeForbidMap cache), if yes, deny the non-reserved task to be scheduled on this node.",
        "comment_created_at": "2024-11-18T11:37:11+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "Why don't we just call `ReserveNodesMap`...? `nodeForbidMap` seems to have to be explained from the perspective of non-reserve tasks.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1860163629",
    "pr_number": 3846,
    "pr_file": "docs/design/jobflow/README.md",
    "created_at": "2024-11-27T08:06:37+00:00",
    "commented_code": "| ----------------- | ------------------------------------ | -------- | ------------- | ------------------------------------------------------------ |\n | `flows`           | [`Flow array`](#Flow) | Y        |               | Describes the dependencies between vcjobs. |\n | `jobRetainPolicy` | `string`                             | Y        | retain | After JobFlow succeed, keep the generated job. Otherwise, delete it. |\n+| `jobRetry` | `int`                          | N        | 1 | JobFlow global retry count.|",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1860163629",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3846,
        "pr_file": "docs/design/jobflow/README.md",
        "discussion_id": "1860163629",
        "commented_code": "@@ -125,6 +125,9 @@ The specification of cloud-native services defines service metadata, version lis\n | ----------------- | ------------------------------------ | -------- | ------------- | ------------------------------------------------------------ |\n | `flows`           | [`Flow array`](#Flow) | Y        |               | Describes the dependencies between vcjobs. |\n | `jobRetainPolicy` | `string`                             | Y        | retain | After JobFlow succeed, keep the generated job. Otherwise, delete it. |\n+| `jobRetry` | `int`                          | N        | 1 | JobFlow global retry count.|",
        "comment_created_at": "2024-11-27T08:06:37+00:00",
        "comment_author": "hwdef",
        "comment_body": "This name is not good.\r\nFor consistency. Let's use maxRetry.",
        "pr_file_module": null
      },
      {
        "comment_id": "1861415082",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3846,
        "pr_file": "docs/design/jobflow/README.md",
        "discussion_id": "1860163629",
        "commented_code": "@@ -125,6 +125,9 @@ The specification of cloud-native services defines service metadata, version lis\n | ----------------- | ------------------------------------ | -------- | ------------- | ------------------------------------------------------------ |\n | `flows`           | [`Flow array`](#Flow) | Y        |               | Describes the dependencies between vcjobs. |\n | `jobRetainPolicy` | `string`                             | Y        | retain | After JobFlow succeed, keep the generated job. Otherwise, delete it. |\n+| `jobRetry` | `int`                          | N        | 1 | JobFlow global retry count.|",
        "comment_created_at": "2024-11-28T01:56:59+00:00",
        "comment_author": "dongjiang1989",
        "comment_body": "Thanks @hwdef \r\nChange to global `maxRetry`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1866936615",
    "pr_number": 3850,
    "pr_file": "docs/design/Network Topology Aware Scheduling.md",
    "created_at": "2024-12-03T02:36:40+00:00",
    "commented_code": "+# Network Topology Aware Scheduling\n+\n+Author: William Wang, Peng Gu, Kevin Wang, Klaus Ma, Xuzheng Chang\n+\n+# Motivation\n+\n+In the LLM training scenario, the model parallels have extremely high requirements for the network throughput to exchange data, making the networking to be a bottleneck. There are diverse network in the datacenter, e.g. IB, RoCE, Nvswitch and in each type of network, there might be multiple levels of switch having different throughput and latency. Use requires that the workload can be scheduled to the best performance domain with highest throughput and lowest latency to accelerate data exchanging for training.\n+\n+## Use Case 1\n+\n+1. A training Job: 8 GPU per Pod \\* \\~3k (5k) Pods  \n+   1. MPI, PyTorch \u2026  \n+2. 1 BF3, 8 CX7, 8 H100\n+\n+![network topology usecase1](images/network-topology-aware/network-topology-usecase1.png)\n+\n+## \n+\n+3. Schedule the job, prefer scheduling all pods to one tier1 topology zone, if not enough nodes, try to schedule all the pods to one tier2 topology zone.  \n+4. Gang scheduling is also needed in this case, to make sure all pods are able to proceed with their work. \n+\n+## Use Case 2\n+\n+1. LLM training job:  16 NPU per pod, 3000 pod per job\n+\n+![network topology usecase2](images/network-topology-aware/network-topology-usecase2.png)\n+\n+2. There are 3 network control planes, which are VPC, Roce and HCCS.  \n+3. The pods in Job are expected to be grouped and the pods belonging to the same group have higher demand for the network bandwidth. The tensor parallels are performed on pods in the same group.  \n+![network topology usecase3](images/network-topology-aware/network-topology-usecase3.png)\n+\n+4. The pods belonging to the same group are required to be scheduled to HCCS topology zone.  \n+5. Prefer to schedule the pod group to one HCCS topology zone, if not enough nodes, try to schedule to the RoCE topology zone and then the VPC topology zone.  \n+6. Gang scheduling is required in this case, to make sure all pods are able to proceed with their work.\n+\n+# Scope:\n+\n+In Scope:\n+\n+* support clos network topology define and management  \n+* support network topology aware scheduling for Volcano job  \n+* support spine-leaf network\n+\n+# Function Detail\n+\n+### network topology management\n+\n+Option 1: Describe network topology by labels, there's a proposal in upstream  \n+Cons: \n+* complicated to construct tree for the topology information  \n+\n+Option 2: Describe network topology by CRD, NetworkTopology in Volcano \u2192 plugin/scheduling\n+\n+- label \\-\\> NetworkTopology   \n+- Rest API (NV, HW) \\-\\> NetworkTopology\n+\n+Pros: \n+  * easier for debugging:  \n+  * engineers don\u2019t need to construct the tree of topology info by themselves.   \n+  * the scheduler uses the same as engineers can see.\n+\n+* Components besides scheduler are able to use the CR as well\n+\n+#### network topology definition\n+\n+`HyperNode` is a performance domain which consists of a group of nodes or sub-performance domains.The network bandwidth and latency is the same in one HyperNode. This CRD is used to describe the network topology in Kubernetes cluster.\n+\n+`Tier` is a way to distinguish different performance domains. The bandwidth and latency are the same in one tier. The smaller the value of the tier, the higher the bandwidth. For example, compute-network and storage network can be in different tiers, or in the compute-network, there are several levels of spine, leaf switches, each level can be identified as a tier.\n+\n+```go\n+type HyperNode struct {\n+\tmetav1.TypeMeta `json:\u201d,inline\u201d`\n+\tmetav1.ObjectMeta `json:\u201dmetadata, omitempty\u201d`\n+\n+\tSpec HyperNodeSpec `json:\u201dspec\u201d`\n+\tStatus HyperNodeStatus `json:\u201dstatus\u201d`\n+}\n+\n+type HyperNodeSpec struct {\n+\ttier string\t`json:\"tier,omitempty\"`\n+\tmembers []MemberSpec\t`json:\"members,omitempty\"`\n+}\n+\n+type MemberSpec struct {\n+\tname string\t`json:\"name,omitempty\"`\n+\ttype string\t`json:\"type,omitempty\"`",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1866936615",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3850,
        "pr_file": "docs/design/Network Topology Aware Scheduling.md",
        "discussion_id": "1866936615",
        "commented_code": "@@ -0,0 +1,575 @@\n+# Network Topology Aware Scheduling\n+\n+Author: William Wang, Peng Gu, Kevin Wang, Klaus Ma, Xuzheng Chang\n+\n+# Motivation\n+\n+In the LLM training scenario, the model parallels have extremely high requirements for the network throughput to exchange data, making the networking to be a bottleneck. There are diverse network in the datacenter, e.g. IB, RoCE, Nvswitch and in each type of network, there might be multiple levels of switch having different throughput and latency. Use requires that the workload can be scheduled to the best performance domain with highest throughput and lowest latency to accelerate data exchanging for training.\n+\n+## Use Case 1\n+\n+1. A training Job: 8 GPU per Pod \\* \\~3k (5k) Pods  \n+   1. MPI, PyTorch \u2026  \n+2. 1 BF3, 8 CX7, 8 H100\n+\n+![network topology usecase1](images/network-topology-aware/network-topology-usecase1.png)\n+\n+## \n+\n+3. Schedule the job, prefer scheduling all pods to one tier1 topology zone, if not enough nodes, try to schedule all the pods to one tier2 topology zone.  \n+4. Gang scheduling is also needed in this case, to make sure all pods are able to proceed with their work. \n+\n+## Use Case 2\n+\n+1. LLM training job:  16 NPU per pod, 3000 pod per job\n+\n+![network topology usecase2](images/network-topology-aware/network-topology-usecase2.png)\n+\n+2. There are 3 network control planes, which are VPC, Roce and HCCS.  \n+3. The pods in Job are expected to be grouped and the pods belonging to the same group have higher demand for the network bandwidth. The tensor parallels are performed on pods in the same group.  \n+![network topology usecase3](images/network-topology-aware/network-topology-usecase3.png)\n+\n+4. The pods belonging to the same group are required to be scheduled to HCCS topology zone.  \n+5. Prefer to schedule the pod group to one HCCS topology zone, if not enough nodes, try to schedule to the RoCE topology zone and then the VPC topology zone.  \n+6. Gang scheduling is required in this case, to make sure all pods are able to proceed with their work.\n+\n+# Scope:\n+\n+In Scope:\n+\n+* support clos network topology define and management  \n+* support network topology aware scheduling for Volcano job  \n+* support spine-leaf network\n+\n+# Function Detail\n+\n+### network topology management\n+\n+Option 1: Describe network topology by labels, there's a proposal in upstream  \n+Cons: \n+* complicated to construct tree for the topology information  \n+\n+Option 2: Describe network topology by CRD, NetworkTopology in Volcano \u2192 plugin/scheduling\n+\n+- label \\-\\> NetworkTopology   \n+- Rest API (NV, HW) \\-\\> NetworkTopology\n+\n+Pros: \n+  * easier for debugging:  \n+  * engineers don\u2019t need to construct the tree of topology info by themselves.   \n+  * the scheduler uses the same as engineers can see.\n+\n+* Components besides scheduler are able to use the CR as well\n+\n+#### network topology definition\n+\n+`HyperNode` is a performance domain which consists of a group of nodes or sub-performance domains.The network bandwidth and latency is the same in one HyperNode. This CRD is used to describe the network topology in Kubernetes cluster.\n+\n+`Tier` is a way to distinguish different performance domains. The bandwidth and latency are the same in one tier. The smaller the value of the tier, the higher the bandwidth. For example, compute-network and storage network can be in different tiers, or in the compute-network, there are several levels of spine, leaf switches, each level can be identified as a tier.\n+\n+```go\n+type HyperNode struct {\n+\tmetav1.TypeMeta `json:\u201d,inline\u201d`\n+\tmetav1.ObjectMeta `json:\u201dmetadata, omitempty\u201d`\n+\n+\tSpec HyperNodeSpec `json:\u201dspec\u201d`\n+\tStatus HyperNodeStatus `json:\u201dstatus\u201d`\n+}\n+\n+type HyperNodeSpec struct {\n+\ttier string\t`json:\"tier,omitempty\"`\n+\tmembers []MemberSpec\t`json:\"members,omitempty\"`\n+}\n+\n+type MemberSpec struct {\n+\tname string\t`json:\"name,omitempty\"`\n+\ttype string\t`json:\"type,omitempty\"`",
        "comment_created_at": "2024-12-03T02:36:40+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "We should capitalize these fields? `type` conflicts with keywords now. Can the `type` field here be enumerated with constants? There are only two types: `Node` and `HyperNode`, right?",
        "pr_file_module": null
      },
      {
        "comment_id": "1897729386",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3850,
        "pr_file": "docs/design/Network Topology Aware Scheduling.md",
        "discussion_id": "1866936615",
        "commented_code": "@@ -0,0 +1,575 @@\n+# Network Topology Aware Scheduling\n+\n+Author: William Wang, Peng Gu, Kevin Wang, Klaus Ma, Xuzheng Chang\n+\n+# Motivation\n+\n+In the LLM training scenario, the model parallels have extremely high requirements for the network throughput to exchange data, making the networking to be a bottleneck. There are diverse network in the datacenter, e.g. IB, RoCE, Nvswitch and in each type of network, there might be multiple levels of switch having different throughput and latency. Use requires that the workload can be scheduled to the best performance domain with highest throughput and lowest latency to accelerate data exchanging for training.\n+\n+## Use Case 1\n+\n+1. A training Job: 8 GPU per Pod \\* \\~3k (5k) Pods  \n+   1. MPI, PyTorch \u2026  \n+2. 1 BF3, 8 CX7, 8 H100\n+\n+![network topology usecase1](images/network-topology-aware/network-topology-usecase1.png)\n+\n+## \n+\n+3. Schedule the job, prefer scheduling all pods to one tier1 topology zone, if not enough nodes, try to schedule all the pods to one tier2 topology zone.  \n+4. Gang scheduling is also needed in this case, to make sure all pods are able to proceed with their work. \n+\n+## Use Case 2\n+\n+1. LLM training job:  16 NPU per pod, 3000 pod per job\n+\n+![network topology usecase2](images/network-topology-aware/network-topology-usecase2.png)\n+\n+2. There are 3 network control planes, which are VPC, Roce and HCCS.  \n+3. The pods in Job are expected to be grouped and the pods belonging to the same group have higher demand for the network bandwidth. The tensor parallels are performed on pods in the same group.  \n+![network topology usecase3](images/network-topology-aware/network-topology-usecase3.png)\n+\n+4. The pods belonging to the same group are required to be scheduled to HCCS topology zone.  \n+5. Prefer to schedule the pod group to one HCCS topology zone, if not enough nodes, try to schedule to the RoCE topology zone and then the VPC topology zone.  \n+6. Gang scheduling is required in this case, to make sure all pods are able to proceed with their work.\n+\n+# Scope:\n+\n+In Scope:\n+\n+* support clos network topology define and management  \n+* support network topology aware scheduling for Volcano job  \n+* support spine-leaf network\n+\n+# Function Detail\n+\n+### network topology management\n+\n+Option 1: Describe network topology by labels, there's a proposal in upstream  \n+Cons: \n+* complicated to construct tree for the topology information  \n+\n+Option 2: Describe network topology by CRD, NetworkTopology in Volcano \u2192 plugin/scheduling\n+\n+- label \\-\\> NetworkTopology   \n+- Rest API (NV, HW) \\-\\> NetworkTopology\n+\n+Pros: \n+  * easier for debugging:  \n+  * engineers don\u2019t need to construct the tree of topology info by themselves.   \n+  * the scheduler uses the same as engineers can see.\n+\n+* Components besides scheduler are able to use the CR as well\n+\n+#### network topology definition\n+\n+`HyperNode` is a performance domain which consists of a group of nodes or sub-performance domains.The network bandwidth and latency is the same in one HyperNode. This CRD is used to describe the network topology in Kubernetes cluster.\n+\n+`Tier` is a way to distinguish different performance domains. The bandwidth and latency are the same in one tier. The smaller the value of the tier, the higher the bandwidth. For example, compute-network and storage network can be in different tiers, or in the compute-network, there are several levels of spine, leaf switches, each level can be identified as a tier.\n+\n+```go\n+type HyperNode struct {\n+\tmetav1.TypeMeta `json:\u201d,inline\u201d`\n+\tmetav1.ObjectMeta `json:\u201dmetadata, omitempty\u201d`\n+\n+\tSpec HyperNodeSpec `json:\u201dspec\u201d`\n+\tStatus HyperNodeStatus `json:\u201dstatus\u201d`\n+}\n+\n+type HyperNodeSpec struct {\n+\ttier string\t`json:\"tier,omitempty\"`\n+\tmembers []MemberSpec\t`json:\"members,omitempty\"`\n+}\n+\n+type MemberSpec struct {\n+\tname string\t`json:\"name,omitempty\"`\n+\ttype string\t`json:\"type,omitempty\"`",
        "comment_created_at": "2024-12-26T08:00:18+00:00",
        "comment_author": "Monokaix",
        "comment_body": "Here is just an example, and the API definition is below.",
        "pr_file_module": null
      }
    ]
  }
]