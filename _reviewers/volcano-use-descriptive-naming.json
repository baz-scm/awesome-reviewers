[
  {
    "discussion_id": "2265591221",
    "pr_number": 4520,
    "pr_file": "pkg/scheduler/framework/session.go",
    "created_at": "2025-08-11T03:08:16+00:00",
    "commented_code": "for status, tasks := range job.TaskStatusIndex {\n \t\t\tif api.AllocatedStatus(status) {\n \t\t\t\tfor _, task := range tasks {\n+\t\t\t\t\tnode, ok1 := ssn.Nodes[task.NodeName]",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2265591221",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4520,
        "pr_file": "pkg/scheduler/framework/session.go",
        "discussion_id": "2265591221",
        "commented_code": "@@ -259,6 +259,18 @@ func updateQueueStatus(ssn *Session) {\n \t\tfor status, tasks := range job.TaskStatusIndex {\n \t\t\tif api.AllocatedStatus(status) {\n \t\t\t\tfor _, task := range tasks {\n+\t\t\t\t\tnode, ok1 := ssn.Nodes[task.NodeName]",
        "comment_created_at": "2025-08-11T03:08:16+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "Should not name variable as `ok1`. `ok1` --> `ok`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1658867530",
    "pr_number": 3551,
    "pr_file": "pkg/scheduler/framework/session_plugins.go",
    "created_at": "2024-06-28T14:45:33+00:00",
    "commented_code": "// Reclaimable invoke reclaimable function of the plugins\n func (ssn *Session) Reclaimable(reclaimer *api.TaskInfo, reclaimees []*api.TaskInfo) []*api.TaskInfo {\n \tvar victims []*api.TaskInfo\n-\tvar init bool\n+\tvar init = true",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1658867530",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3551,
        "pr_file": "pkg/scheduler/framework/session_plugins.go",
        "discussion_id": "1658867530",
        "commented_code": "@@ -153,7 +153,7 @@ func (ssn *Session) AddJobStarvingFns(name string, fn api.ValidateFn) {\n // Reclaimable invoke reclaimable function of the plugins\n func (ssn *Session) Reclaimable(reclaimer *api.TaskInfo, reclaimees []*api.TaskInfo) []*api.TaskInfo {\n \tvar victims []*api.TaskInfo\n-\tvar init bool\n+\tvar init = true",
        "comment_created_at": "2024-06-28T14:45:33+00:00",
        "comment_author": "googs1025",
        "comment_body": "My question is: what does the init variable represent? Why is it better to change it to true?",
        "pr_file_module": null
      },
      {
        "comment_id": "1658869910",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3551,
        "pr_file": "pkg/scheduler/framework/session_plugins.go",
        "discussion_id": "1658867530",
        "commented_code": "@@ -153,7 +153,7 @@ func (ssn *Session) AddJobStarvingFns(name string, fn api.ValidateFn) {\n // Reclaimable invoke reclaimable function of the plugins\n func (ssn *Session) Reclaimable(reclaimer *api.TaskInfo, reclaimees []*api.TaskInfo) []*api.TaskInfo {\n \tvar victims []*api.TaskInfo\n-\tvar init bool\n+\tvar init = true",
        "comment_created_at": "2024-06-28T14:47:02+00:00",
        "comment_author": "googs1025",
        "comment_body": "This variable should represent whether it is initialized or not. I think the default value should be false. However, can the variable name be changed? It may cause misunderstanding.",
        "pr_file_module": null
      },
      {
        "comment_id": "1659607306",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3551,
        "pr_file": "pkg/scheduler/framework/session_plugins.go",
        "discussion_id": "1658867530",
        "commented_code": "@@ -153,7 +153,7 @@ func (ssn *Session) AddJobStarvingFns(name string, fn api.ValidateFn) {\n // Reclaimable invoke reclaimable function of the plugins\n func (ssn *Session) Reclaimable(reclaimer *api.TaskInfo, reclaimees []*api.TaskInfo) []*api.TaskInfo {\n \tvar victims []*api.TaskInfo\n-\tvar init bool\n+\tvar init = true",
        "comment_created_at": "2024-06-29T05:32:30+00:00",
        "comment_author": "PigNatovsky",
        "comment_body": "I was thinking in that direction. But for me init asks if we need to initialize the list of victims/if it's a first loop. So we can also name it first_iteration, for example. \nIt's like thinking -  we want to check if it's not initialized, or we want to check if we need to do init assigment.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2149954097",
    "pr_number": 4373,
    "pr_file": "pkg/scheduler/framework/session.go",
    "created_at": "2025-06-16T12:59:56+00:00",
    "commented_code": "ssn.NamespaceInfo = snapshot.NamespaceInfo\n \t// calculate all nodes' resource only once in each schedule cycle, other plugins can clone it when need\n \tfor _, n := range ssn.Nodes {\n+\t\tif !nodeIsNotReady(n.Node) {",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2149954097",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4373,
        "pr_file": "pkg/scheduler/framework/session.go",
        "discussion_id": "2149954097",
        "commented_code": "@@ -211,6 +211,11 @@ func openSession(cache cache.Cache) *Session {\n \tssn.NamespaceInfo = snapshot.NamespaceInfo\n \t// calculate all nodes' resource only once in each schedule cycle, other plugins can clone it when need\n \tfor _, n := range ssn.Nodes {\n+\t\tif !nodeIsNotReady(n.Node) {",
        "comment_created_at": "2025-06-16T12:59:56+00:00",
        "comment_author": "Monokaix",
        "comment_body": "Here `!nodeIsNotReady` means node ready, please correct it.",
        "pr_file_module": null
      },
      {
        "comment_id": "2149966053",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4373,
        "pr_file": "pkg/scheduler/framework/session.go",
        "discussion_id": "2149954097",
        "commented_code": "@@ -211,6 +211,11 @@ func openSession(cache cache.Cache) *Session {\n \tssn.NamespaceInfo = snapshot.NamespaceInfo\n \t// calculate all nodes' resource only once in each schedule cycle, other plugins can clone it when need\n \tfor _, n := range ssn.Nodes {\n+\t\tif !nodeIsNotReady(n.Node) {",
        "comment_created_at": "2025-06-16T13:06:16+00:00",
        "comment_author": "LY-today",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2086735594",
    "pr_number": 4279,
    "pr_file": "pkg/scheduler/actions/preempt/preempt.go",
    "created_at": "2025-05-13T12:45:49+00:00",
    "commented_code": "return nil\n }\n+\n+func (pmpt *Action) topologyAwarePreempt(\n+\tssn *framework.Session,\n+\tstmt *framework.Statement,\n+\tpreemptor *api.TaskInfo,\n+\tfilter func(*api.TaskInfo) bool,\n+\tpredicateHelper util.PredicateHelper,\n+) (bool, error) {\n+\tif err := ssn.PrePredicateFn(preemptor); err != nil {\n+\t\treturn false, fmt.Errorf(\"PrePredicate for task %s/%s failed for: %v\", preemptor.Namespace, preemptor.Name, err)\n+\t}\n+\n+\t_, found := ssn.Jobs[preemptor.Job]\n+\tif !found {\n+\t\treturn false, fmt.Errorf(\"not found Job %s in Session\", preemptor.Job)\n+\t}\n+\n+\t// 1) Check whether the task is eligible to preempt others, e.g., check preemptionPolicy is `Never` or not\n+\teligible, reason := pmpt.taskEligibleToPreemptOthers(preemptor)\n+\tif !eligible {\n+\t\treturn false, fmt.Errorf(\"task %s/%s is not eligible to preempt others: %s\", preemptor.Namespace, preemptor.Name, reason)\n+\t}\n+\n+\t// 2) Find all preemption candidates.\n+\tcandidates, nodeToStatusMap, err := pmpt.findCandidates(preemptor, filter, predicateHelper, stmt)\n+\tif err != nil && len(candidates) == 0 {\n+\t\treturn false, err\n+\t}\n+\n+\t// Return error when there are no candidates that fit the pod.\n+\tif len(candidates) == 0 {\n+\t\t// Specify nominatedNodeName to clear the pod's nominatedNodeName status, if applicable.\n+\t\treturn false, fmt.Errorf(\"no candidates that fit the pod, the status of the nodes are %v\", nodeToStatusMap)\n+\t}\n+\n+\t// 3) Find the best candidate.\n+\tbestCandidate := SelectCandidate(candidates)\n+\tif bestCandidate == nil || len(bestCandidate.Name()) == 0 {\n+\t\treturn false, fmt.Errorf(\"no candidate node for preemption\")\n+\t}\n+\n+\tif status := prepareCandidate(bestCandidate, preemptor.Pod, stmt, ssn); !status.IsSuccess() {\n+\t\treturn false, fmt.Errorf(\"failed to prepare candidate: %v\", status)\n+\t}\n+\n+\tif err := stmt.Pipeline(preemptor, bestCandidate.Name(), true); err != nil {\n+\t\tklog.Errorf(\"Failed to pipeline Task <%s/%s> on Node <%s>\",\n+\t\t\tpreemptor.Namespace, preemptor.Name, bestCandidate.Name())\n+\t\tif rollbackErr := stmt.UnPipeline(preemptor); rollbackErr != nil {\n+\t\t\tklog.Errorf(\"Failed to unpipeline Task %v on %v in Session %v for %v.\",\n+\t\t\t\tpreemptor.UID, bestCandidate.Name(), ssn.UID, rollbackErr)\n+\t\t}\n+\t}\n+\n+\treturn true, nil\n+}\n+\n+func (pmpt *Action) taskEligibleToPreemptOthers(preemptor *api.TaskInfo) (bool, string) {\n+\tif preemptor.Pod.Spec.PreemptionPolicy != nil && *preemptor.Pod.Spec.PreemptionPolicy == v1.PreemptNever {\n+\t\treturn false, \"not eligible to preempt other tasks due to preemptionPolicy is Never\"\n+\t}\n+\n+\tnomNodeName := preemptor.Pod.Status.NominatedNodeName\n+\tif len(nomNodeName) > 0 {\n+\t\tnodeInfo, ok := pmpt.ssn.Nodes[nomNodeName]\n+\t\tif !ok {\n+\t\t\treturn false, \"not eligible due to the pod's nominated node is not found in the session\"\n+\t\t}\n+\n+\t\terr := pmpt.ssn.PredicateFn(preemptor, nodeInfo)\n+\t\tif err == nil {\n+\t\t\treturn false, \"not eligible due to the pod's nominated node is already schedulable, which should not happen as preemption means no node is schedulable\"\n+\t\t}\n+\n+\t\tfitError, ok := err.(*api.FitError)\n+\t\tif !ok {\n+\t\t\treturn false, fmt.Sprintf(\"not eligible due to the predicate returned a non-FitError error, the error is: %v\", err)\n+\t\t}\n+\n+\t\t// If the pod's nominated node is considered as UnschedulableAndUnresolvable by the predicate,\n+\t\t// then the pod should be considered for preempting again.\n+\t\tif fitError.Status.ContainsUnschedulableAndUnresolvable() {\n+\t\t\treturn true, \"\"\n+\t\t}\n+\n+\t\tpodPriority := PodPriority(preemptor.Pod)",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2086735594",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4279,
        "pr_file": "pkg/scheduler/actions/preempt/preempt.go",
        "discussion_id": "2086735594",
        "commented_code": "@@ -342,3 +386,612 @@ func (pmpt *Action) taskEligibleToPreempt(preemptor *api.TaskInfo) error {\n \n \treturn nil\n }\n+\n+func (pmpt *Action) topologyAwarePreempt(\n+\tssn *framework.Session,\n+\tstmt *framework.Statement,\n+\tpreemptor *api.TaskInfo,\n+\tfilter func(*api.TaskInfo) bool,\n+\tpredicateHelper util.PredicateHelper,\n+) (bool, error) {\n+\tif err := ssn.PrePredicateFn(preemptor); err != nil {\n+\t\treturn false, fmt.Errorf(\"PrePredicate for task %s/%s failed for: %v\", preemptor.Namespace, preemptor.Name, err)\n+\t}\n+\n+\t_, found := ssn.Jobs[preemptor.Job]\n+\tif !found {\n+\t\treturn false, fmt.Errorf(\"not found Job %s in Session\", preemptor.Job)\n+\t}\n+\n+\t// 1) Check whether the task is eligible to preempt others, e.g., check preemptionPolicy is `Never` or not\n+\teligible, reason := pmpt.taskEligibleToPreemptOthers(preemptor)\n+\tif !eligible {\n+\t\treturn false, fmt.Errorf(\"task %s/%s is not eligible to preempt others: %s\", preemptor.Namespace, preemptor.Name, reason)\n+\t}\n+\n+\t// 2) Find all preemption candidates.\n+\tcandidates, nodeToStatusMap, err := pmpt.findCandidates(preemptor, filter, predicateHelper, stmt)\n+\tif err != nil && len(candidates) == 0 {\n+\t\treturn false, err\n+\t}\n+\n+\t// Return error when there are no candidates that fit the pod.\n+\tif len(candidates) == 0 {\n+\t\t// Specify nominatedNodeName to clear the pod's nominatedNodeName status, if applicable.\n+\t\treturn false, fmt.Errorf(\"no candidates that fit the pod, the status of the nodes are %v\", nodeToStatusMap)\n+\t}\n+\n+\t// 3) Find the best candidate.\n+\tbestCandidate := SelectCandidate(candidates)\n+\tif bestCandidate == nil || len(bestCandidate.Name()) == 0 {\n+\t\treturn false, fmt.Errorf(\"no candidate node for preemption\")\n+\t}\n+\n+\tif status := prepareCandidate(bestCandidate, preemptor.Pod, stmt, ssn); !status.IsSuccess() {\n+\t\treturn false, fmt.Errorf(\"failed to prepare candidate: %v\", status)\n+\t}\n+\n+\tif err := stmt.Pipeline(preemptor, bestCandidate.Name(), true); err != nil {\n+\t\tklog.Errorf(\"Failed to pipeline Task <%s/%s> on Node <%s>\",\n+\t\t\tpreemptor.Namespace, preemptor.Name, bestCandidate.Name())\n+\t\tif rollbackErr := stmt.UnPipeline(preemptor); rollbackErr != nil {\n+\t\t\tklog.Errorf(\"Failed to unpipeline Task %v on %v in Session %v for %v.\",\n+\t\t\t\tpreemptor.UID, bestCandidate.Name(), ssn.UID, rollbackErr)\n+\t\t}\n+\t}\n+\n+\treturn true, nil\n+}\n+\n+func (pmpt *Action) taskEligibleToPreemptOthers(preemptor *api.TaskInfo) (bool, string) {\n+\tif preemptor.Pod.Spec.PreemptionPolicy != nil && *preemptor.Pod.Spec.PreemptionPolicy == v1.PreemptNever {\n+\t\treturn false, \"not eligible to preempt other tasks due to preemptionPolicy is Never\"\n+\t}\n+\n+\tnomNodeName := preemptor.Pod.Status.NominatedNodeName\n+\tif len(nomNodeName) > 0 {\n+\t\tnodeInfo, ok := pmpt.ssn.Nodes[nomNodeName]\n+\t\tif !ok {\n+\t\t\treturn false, \"not eligible due to the pod's nominated node is not found in the session\"\n+\t\t}\n+\n+\t\terr := pmpt.ssn.PredicateFn(preemptor, nodeInfo)\n+\t\tif err == nil {\n+\t\t\treturn false, \"not eligible due to the pod's nominated node is already schedulable, which should not happen as preemption means no node is schedulable\"\n+\t\t}\n+\n+\t\tfitError, ok := err.(*api.FitError)\n+\t\tif !ok {\n+\t\t\treturn false, fmt.Sprintf(\"not eligible due to the predicate returned a non-FitError error, the error is: %v\", err)\n+\t\t}\n+\n+\t\t// If the pod's nominated node is considered as UnschedulableAndUnresolvable by the predicate,\n+\t\t// then the pod should be considered for preempting again.\n+\t\tif fitError.Status.ContainsUnschedulableAndUnresolvable() {\n+\t\t\treturn true, \"\"\n+\t\t}\n+\n+\t\tpodPriority := PodPriority(preemptor.Pod)",
        "comment_created_at": "2025-05-13T12:45:49+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "Better to rename as `preemptorPodPriority` for better clarification",
        "pr_file_module": null
      },
      {
        "comment_id": "2108569560",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4279,
        "pr_file": "pkg/scheduler/actions/preempt/preempt.go",
        "discussion_id": "2086735594",
        "commented_code": "@@ -342,3 +386,612 @@ func (pmpt *Action) taskEligibleToPreempt(preemptor *api.TaskInfo) error {\n \n \treturn nil\n }\n+\n+func (pmpt *Action) topologyAwarePreempt(\n+\tssn *framework.Session,\n+\tstmt *framework.Statement,\n+\tpreemptor *api.TaskInfo,\n+\tfilter func(*api.TaskInfo) bool,\n+\tpredicateHelper util.PredicateHelper,\n+) (bool, error) {\n+\tif err := ssn.PrePredicateFn(preemptor); err != nil {\n+\t\treturn false, fmt.Errorf(\"PrePredicate for task %s/%s failed for: %v\", preemptor.Namespace, preemptor.Name, err)\n+\t}\n+\n+\t_, found := ssn.Jobs[preemptor.Job]\n+\tif !found {\n+\t\treturn false, fmt.Errorf(\"not found Job %s in Session\", preemptor.Job)\n+\t}\n+\n+\t// 1) Check whether the task is eligible to preempt others, e.g., check preemptionPolicy is `Never` or not\n+\teligible, reason := pmpt.taskEligibleToPreemptOthers(preemptor)\n+\tif !eligible {\n+\t\treturn false, fmt.Errorf(\"task %s/%s is not eligible to preempt others: %s\", preemptor.Namespace, preemptor.Name, reason)\n+\t}\n+\n+\t// 2) Find all preemption candidates.\n+\tcandidates, nodeToStatusMap, err := pmpt.findCandidates(preemptor, filter, predicateHelper, stmt)\n+\tif err != nil && len(candidates) == 0 {\n+\t\treturn false, err\n+\t}\n+\n+\t// Return error when there are no candidates that fit the pod.\n+\tif len(candidates) == 0 {\n+\t\t// Specify nominatedNodeName to clear the pod's nominatedNodeName status, if applicable.\n+\t\treturn false, fmt.Errorf(\"no candidates that fit the pod, the status of the nodes are %v\", nodeToStatusMap)\n+\t}\n+\n+\t// 3) Find the best candidate.\n+\tbestCandidate := SelectCandidate(candidates)\n+\tif bestCandidate == nil || len(bestCandidate.Name()) == 0 {\n+\t\treturn false, fmt.Errorf(\"no candidate node for preemption\")\n+\t}\n+\n+\tif status := prepareCandidate(bestCandidate, preemptor.Pod, stmt, ssn); !status.IsSuccess() {\n+\t\treturn false, fmt.Errorf(\"failed to prepare candidate: %v\", status)\n+\t}\n+\n+\tif err := stmt.Pipeline(preemptor, bestCandidate.Name(), true); err != nil {\n+\t\tklog.Errorf(\"Failed to pipeline Task <%s/%s> on Node <%s>\",\n+\t\t\tpreemptor.Namespace, preemptor.Name, bestCandidate.Name())\n+\t\tif rollbackErr := stmt.UnPipeline(preemptor); rollbackErr != nil {\n+\t\t\tklog.Errorf(\"Failed to unpipeline Task %v on %v in Session %v for %v.\",\n+\t\t\t\tpreemptor.UID, bestCandidate.Name(), ssn.UID, rollbackErr)\n+\t\t}\n+\t}\n+\n+\treturn true, nil\n+}\n+\n+func (pmpt *Action) taskEligibleToPreemptOthers(preemptor *api.TaskInfo) (bool, string) {\n+\tif preemptor.Pod.Spec.PreemptionPolicy != nil && *preemptor.Pod.Spec.PreemptionPolicy == v1.PreemptNever {\n+\t\treturn false, \"not eligible to preempt other tasks due to preemptionPolicy is Never\"\n+\t}\n+\n+\tnomNodeName := preemptor.Pod.Status.NominatedNodeName\n+\tif len(nomNodeName) > 0 {\n+\t\tnodeInfo, ok := pmpt.ssn.Nodes[nomNodeName]\n+\t\tif !ok {\n+\t\t\treturn false, \"not eligible due to the pod's nominated node is not found in the session\"\n+\t\t}\n+\n+\t\terr := pmpt.ssn.PredicateFn(preemptor, nodeInfo)\n+\t\tif err == nil {\n+\t\t\treturn false, \"not eligible due to the pod's nominated node is already schedulable, which should not happen as preemption means no node is schedulable\"\n+\t\t}\n+\n+\t\tfitError, ok := err.(*api.FitError)\n+\t\tif !ok {\n+\t\t\treturn false, fmt.Sprintf(\"not eligible due to the predicate returned a non-FitError error, the error is: %v\", err)\n+\t\t}\n+\n+\t\t// If the pod's nominated node is considered as UnschedulableAndUnresolvable by the predicate,\n+\t\t// then the pod should be considered for preempting again.\n+\t\tif fitError.Status.ContainsUnschedulableAndUnresolvable() {\n+\t\t\treturn true, \"\"\n+\t\t}\n+\n+\t\tpodPriority := PodPriority(preemptor.Pod)",
        "comment_created_at": "2025-05-27T08:27:33+00:00",
        "comment_author": "bibibox",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2006714014",
    "pr_number": 4123,
    "pr_file": "pkg/scheduler/plugins/cdp/cdp.go",
    "created_at": "2025-03-21T01:46:05+00:00",
    "commented_code": "// OnSessionOpen implements framework.Plugin\n func (sp *CooldownProtectionPlugin) OnSessionOpen(ssn *framework.Session) {\n-\tpreemptableFn := func(preemptor *api.TaskInfo, preemptees []*api.TaskInfo) ([]*api.TaskInfo, int) {\n+\tpreemptableReclaimableFn := func(preemptor_reclaimer *api.TaskInfo, preemptees_reclaimees []*api.TaskInfo) ([]*api.TaskInfo, int) {",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "2006714014",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4123,
        "pr_file": "pkg/scheduler/plugins/cdp/cdp.go",
        "discussion_id": "2006714014",
        "commented_code": "@@ -72,15 +72,15 @@ func (sp *CooldownProtectionPlugin) podCooldownTime(pod *v1.Pod) (value time.Dur\n \n // OnSessionOpen implements framework.Plugin\n func (sp *CooldownProtectionPlugin) OnSessionOpen(ssn *framework.Session) {\n-\tpreemptableFn := func(preemptor *api.TaskInfo, preemptees []*api.TaskInfo) ([]*api.TaskInfo, int) {\n+\tpreemptableReclaimableFn := func(preemptor_reclaimer *api.TaskInfo, preemptees_reclaimees []*api.TaskInfo) ([]*api.TaskInfo, int) {",
        "comment_created_at": "2025-03-21T01:46:05+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "Go does not use underscores in variable naming but uses camel case instead. However, to avoid ambiguity, I recommend changing it like this:\r\n```\r\nfilterVictimsFn := fun(evictingTask *api.TaskInfo, candidateVictims []*api.TaskInfo) ([]*api.TaskInfo, int) \r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2007057308",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4123,
        "pr_file": "pkg/scheduler/plugins/cdp/cdp.go",
        "discussion_id": "2006714014",
        "commented_code": "@@ -72,15 +72,15 @@ func (sp *CooldownProtectionPlugin) podCooldownTime(pod *v1.Pod) (value time.Dur\n \n // OnSessionOpen implements framework.Plugin\n func (sp *CooldownProtectionPlugin) OnSessionOpen(ssn *framework.Session) {\n-\tpreemptableFn := func(preemptor *api.TaskInfo, preemptees []*api.TaskInfo) ([]*api.TaskInfo, int) {\n+\tpreemptableReclaimableFn := func(preemptor_reclaimer *api.TaskInfo, preemptees_reclaimees []*api.TaskInfo) ([]*api.TaskInfo, int) {",
        "comment_created_at": "2025-03-21T08:12:16+00:00",
        "comment_author": "qGentry",
        "comment_body": "Thanks. Fixed",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1948666049",
    "pr_number": 3894,
    "pr_file": "pkg/scheduler/plugins/networktopologyaware/networktopologyaware.go",
    "created_at": "2025-02-10T09:03:54+00:00",
    "commented_code": "+/*\n+Copyright 2019 The Volcano Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+*/\n+\n+package networktopologyaware\n+\n+import (\n+\t\"k8s.io/klog/v2\"\n+\n+\t\"volcano.sh/volcano/pkg/scheduler/api\"\n+\t\"volcano.sh/volcano/pkg/scheduler/framework\"\n+\t\"volcano.sh/volcano/pkg/scheduler/util\"\n+)\n+\n+const (\n+\t// PluginName indicates name of volcano scheduler plugin.\n+\tPluginName            = \"networktopologyaware\"\n+\tBaseScore             = 100.0\n+\tTaskSumBaseScore      = 10.0\n+\tZeroScore             = 0.0\n+\tNetworkTopologyWeight = \"weight\"\n+)\n+\n+type networkTopologyAwarePlugin struct {\n+\t// Arguments given for the plugin\n+\tpluginArguments framework.Arguments\n+\t*hyperNodesTier\n+}\n+\n+type hyperNodesTier struct {\n+\tmaxTier int\n+\tminTier int\n+}\n+\n+func (h *hyperNodesTier) init(hyperNodesSetByTier []int) {\n+\tif len(hyperNodesSetByTier) == 0 {\n+\t\treturn\n+\t}\n+\th.minTier = hyperNodesSetByTier[0]\n+\th.maxTier = hyperNodesSetByTier[len(hyperNodesSetByTier)-1]\n+}\n+\n+// New function returns prioritizePlugin object\n+func New(arguments framework.Arguments) framework.Plugin {\n+\treturn &networkTopologyAwarePlugin{\n+\t\tpluginArguments: arguments,\n+\t\thyperNodesTier:  &hyperNodesTier{},\n+\t}\n+}\n+\n+func (nta *networkTopologyAwarePlugin) Name() string {\n+\treturn PluginName\n+}\n+\n+func calculateWeight(args framework.Arguments) int {\n+\tweight := 1\n+\targs.GetInt(&weight, NetworkTopologyWeight)\n+\treturn weight\n+}\n+\n+func (nta *networkTopologyAwarePlugin) OnSessionOpen(ssn *framework.Session) {\n+\tklog.V(5).Infof(\"Enter networkTopologyAwarePlugin plugin ...\")\n+\tdefer func() {\n+\t\tklog.V(5).Infof(\"Leaving networkTopologyAware plugin ...\")\n+\t}()\n+\n+\tweight := calculateWeight(nta.pluginArguments)\n+\tnta.hyperNodesTier.init(ssn.HyperNodesTiers)\n+\n+\thyperNodeFn := func(job *api.JobInfo, hyperNodes map[string][]*api.NodeInfo) (map[string]float64, error) {\n+\t\thyperNodeScores := make(map[string]float64)\n+\n+\t\tjobAllocatedHyperNode := job.PodGroup.GetAnnotations()[api.JobAllocatedHyperNode]\n+\t\tif jobAllocatedHyperNode == \"\" {\n+\t\t\tfor hyperNode := range hyperNodes {\n+\t\t\t\thyperNodeScores[hyperNode] = ZeroScore\n+\t\t\t}\n+\t\t\treturn hyperNodeScores, nil\n+\t\t}\n+\t\t// The job still has remaining tasks to be scheduled, calculate score based on LCAHyperNode tier of the hyperNode and jobAllocatedHyperNode.\n+\t\tmaxScore := ZeroScore\n+\t\tscoreHyperNode := map[float64][]string{}\n+\t\tfor hyperNode := range hyperNodes {\n+\t\t\tscore := nta.networkTopologyAwareScore(hyperNode, jobAllocatedHyperNode, ssn.HyperNodes)\n+\t\t\tscore *= float64(weight)\n+\t\t\thyperNodeScores[hyperNode] = score\n+\t\t\tif score >= maxScore {\n+\t\t\t\tmaxScore = score\n+\t\t\t\tscoreHyperNode[maxScore] = append(scoreHyperNode[maxScore], hyperNode)\n+\t\t\t}\n+\t\t}\n+\t\t// Calculate score based on the number of tasks scheduled for the job when max score of hyperNode has more than one.\n+\t\tif maxScore != ZeroScore && len(scoreHyperNode[maxScore]) > 1 {\n+\t\t\treScoreHyperNodes := scoreHyperNode[maxScore]",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1948666049",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3894,
        "pr_file": "pkg/scheduler/plugins/networktopologyaware/networktopologyaware.go",
        "discussion_id": "1948666049",
        "commented_code": "@@ -0,0 +1,208 @@\n+/*\n+Copyright 2019 The Volcano Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+*/\n+\n+package networktopologyaware\n+\n+import (\n+\t\"k8s.io/klog/v2\"\n+\n+\t\"volcano.sh/volcano/pkg/scheduler/api\"\n+\t\"volcano.sh/volcano/pkg/scheduler/framework\"\n+\t\"volcano.sh/volcano/pkg/scheduler/util\"\n+)\n+\n+const (\n+\t// PluginName indicates name of volcano scheduler plugin.\n+\tPluginName            = \"networktopologyaware\"\n+\tBaseScore             = 100.0\n+\tTaskSumBaseScore      = 10.0\n+\tZeroScore             = 0.0\n+\tNetworkTopologyWeight = \"weight\"\n+)\n+\n+type networkTopologyAwarePlugin struct {\n+\t// Arguments given for the plugin\n+\tpluginArguments framework.Arguments\n+\t*hyperNodesTier\n+}\n+\n+type hyperNodesTier struct {\n+\tmaxTier int\n+\tminTier int\n+}\n+\n+func (h *hyperNodesTier) init(hyperNodesSetByTier []int) {\n+\tif len(hyperNodesSetByTier) == 0 {\n+\t\treturn\n+\t}\n+\th.minTier = hyperNodesSetByTier[0]\n+\th.maxTier = hyperNodesSetByTier[len(hyperNodesSetByTier)-1]\n+}\n+\n+// New function returns prioritizePlugin object\n+func New(arguments framework.Arguments) framework.Plugin {\n+\treturn &networkTopologyAwarePlugin{\n+\t\tpluginArguments: arguments,\n+\t\thyperNodesTier:  &hyperNodesTier{},\n+\t}\n+}\n+\n+func (nta *networkTopologyAwarePlugin) Name() string {\n+\treturn PluginName\n+}\n+\n+func calculateWeight(args framework.Arguments) int {\n+\tweight := 1\n+\targs.GetInt(&weight, NetworkTopologyWeight)\n+\treturn weight\n+}\n+\n+func (nta *networkTopologyAwarePlugin) OnSessionOpen(ssn *framework.Session) {\n+\tklog.V(5).Infof(\"Enter networkTopologyAwarePlugin plugin ...\")\n+\tdefer func() {\n+\t\tklog.V(5).Infof(\"Leaving networkTopologyAware plugin ...\")\n+\t}()\n+\n+\tweight := calculateWeight(nta.pluginArguments)\n+\tnta.hyperNodesTier.init(ssn.HyperNodesTiers)\n+\n+\thyperNodeFn := func(job *api.JobInfo, hyperNodes map[string][]*api.NodeInfo) (map[string]float64, error) {\n+\t\thyperNodeScores := make(map[string]float64)\n+\n+\t\tjobAllocatedHyperNode := job.PodGroup.GetAnnotations()[api.JobAllocatedHyperNode]\n+\t\tif jobAllocatedHyperNode == \"\" {\n+\t\t\tfor hyperNode := range hyperNodes {\n+\t\t\t\thyperNodeScores[hyperNode] = ZeroScore\n+\t\t\t}\n+\t\t\treturn hyperNodeScores, nil\n+\t\t}\n+\t\t// The job still has remaining tasks to be scheduled, calculate score based on LCAHyperNode tier of the hyperNode and jobAllocatedHyperNode.\n+\t\tmaxScore := ZeroScore\n+\t\tscoreHyperNode := map[float64][]string{}\n+\t\tfor hyperNode := range hyperNodes {\n+\t\t\tscore := nta.networkTopologyAwareScore(hyperNode, jobAllocatedHyperNode, ssn.HyperNodes)\n+\t\t\tscore *= float64(weight)\n+\t\t\thyperNodeScores[hyperNode] = score\n+\t\t\tif score >= maxScore {\n+\t\t\t\tmaxScore = score\n+\t\t\t\tscoreHyperNode[maxScore] = append(scoreHyperNode[maxScore], hyperNode)\n+\t\t\t}\n+\t\t}\n+\t\t// Calculate score based on the number of tasks scheduled for the job when max score of hyperNode has more than one.\n+\t\tif maxScore != ZeroScore && len(scoreHyperNode[maxScore]) > 1 {\n+\t\t\treScoreHyperNodes := scoreHyperNode[maxScore]",
        "comment_created_at": "2025-02-10T09:03:54+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "The name `reScoreHyperNodes` is strange, better to rename as `candicates` or `candidateHyperNodes`",
        "pr_file_module": null
      },
      {
        "comment_id": "1950099146",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3894,
        "pr_file": "pkg/scheduler/plugins/networktopologyaware/networktopologyaware.go",
        "discussion_id": "1948666049",
        "commented_code": "@@ -0,0 +1,208 @@\n+/*\n+Copyright 2019 The Volcano Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+*/\n+\n+package networktopologyaware\n+\n+import (\n+\t\"k8s.io/klog/v2\"\n+\n+\t\"volcano.sh/volcano/pkg/scheduler/api\"\n+\t\"volcano.sh/volcano/pkg/scheduler/framework\"\n+\t\"volcano.sh/volcano/pkg/scheduler/util\"\n+)\n+\n+const (\n+\t// PluginName indicates name of volcano scheduler plugin.\n+\tPluginName            = \"networktopologyaware\"\n+\tBaseScore             = 100.0\n+\tTaskSumBaseScore      = 10.0\n+\tZeroScore             = 0.0\n+\tNetworkTopologyWeight = \"weight\"\n+)\n+\n+type networkTopologyAwarePlugin struct {\n+\t// Arguments given for the plugin\n+\tpluginArguments framework.Arguments\n+\t*hyperNodesTier\n+}\n+\n+type hyperNodesTier struct {\n+\tmaxTier int\n+\tminTier int\n+}\n+\n+func (h *hyperNodesTier) init(hyperNodesSetByTier []int) {\n+\tif len(hyperNodesSetByTier) == 0 {\n+\t\treturn\n+\t}\n+\th.minTier = hyperNodesSetByTier[0]\n+\th.maxTier = hyperNodesSetByTier[len(hyperNodesSetByTier)-1]\n+}\n+\n+// New function returns prioritizePlugin object\n+func New(arguments framework.Arguments) framework.Plugin {\n+\treturn &networkTopologyAwarePlugin{\n+\t\tpluginArguments: arguments,\n+\t\thyperNodesTier:  &hyperNodesTier{},\n+\t}\n+}\n+\n+func (nta *networkTopologyAwarePlugin) Name() string {\n+\treturn PluginName\n+}\n+\n+func calculateWeight(args framework.Arguments) int {\n+\tweight := 1\n+\targs.GetInt(&weight, NetworkTopologyWeight)\n+\treturn weight\n+}\n+\n+func (nta *networkTopologyAwarePlugin) OnSessionOpen(ssn *framework.Session) {\n+\tklog.V(5).Infof(\"Enter networkTopologyAwarePlugin plugin ...\")\n+\tdefer func() {\n+\t\tklog.V(5).Infof(\"Leaving networkTopologyAware plugin ...\")\n+\t}()\n+\n+\tweight := calculateWeight(nta.pluginArguments)\n+\tnta.hyperNodesTier.init(ssn.HyperNodesTiers)\n+\n+\thyperNodeFn := func(job *api.JobInfo, hyperNodes map[string][]*api.NodeInfo) (map[string]float64, error) {\n+\t\thyperNodeScores := make(map[string]float64)\n+\n+\t\tjobAllocatedHyperNode := job.PodGroup.GetAnnotations()[api.JobAllocatedHyperNode]\n+\t\tif jobAllocatedHyperNode == \"\" {\n+\t\t\tfor hyperNode := range hyperNodes {\n+\t\t\t\thyperNodeScores[hyperNode] = ZeroScore\n+\t\t\t}\n+\t\t\treturn hyperNodeScores, nil\n+\t\t}\n+\t\t// The job still has remaining tasks to be scheduled, calculate score based on LCAHyperNode tier of the hyperNode and jobAllocatedHyperNode.\n+\t\tmaxScore := ZeroScore\n+\t\tscoreHyperNode := map[float64][]string{}\n+\t\tfor hyperNode := range hyperNodes {\n+\t\t\tscore := nta.networkTopologyAwareScore(hyperNode, jobAllocatedHyperNode, ssn.HyperNodes)\n+\t\t\tscore *= float64(weight)\n+\t\t\thyperNodeScores[hyperNode] = score\n+\t\t\tif score >= maxScore {\n+\t\t\t\tmaxScore = score\n+\t\t\t\tscoreHyperNode[maxScore] = append(scoreHyperNode[maxScore], hyperNode)\n+\t\t\t}\n+\t\t}\n+\t\t// Calculate score based on the number of tasks scheduled for the job when max score of hyperNode has more than one.\n+\t\tif maxScore != ZeroScore && len(scoreHyperNode[maxScore]) > 1 {\n+\t\t\treScoreHyperNodes := scoreHyperNode[maxScore]",
        "comment_created_at": "2025-02-11T01:07:18+00:00",
        "comment_author": "ecosysbin",
        "comment_body": "ok\uff0cdone",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1213978902",
    "pr_number": 2797,
    "pr_file": "pkg/scheduler/api/job_info.go",
    "created_at": "2023-06-02T06:33:47+00:00",
    "commented_code": "return \"\"\n }\n \n-func getTaskID(pod *v1.Pod) TaskID {",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1213978902",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 2797,
        "pr_file": "pkg/scheduler/api/job_info.go",
        "discussion_id": "1213978902",
        "commented_code": "@@ -142,9 +142,10 @@ func getJobID(pod *v1.Pod) JobID {\n \treturn \"\"\n }\n \n-func getTaskID(pod *v1.Pod) TaskID {",
        "comment_created_at": "2023-06-02T06:33:47+00:00",
        "comment_author": "william-wang",
        "comment_body": "TaskID is easy to read and it stands for UID.  Do not suggest to change it.",
        "pr_file_module": null
      },
      {
        "comment_id": "1232952632",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 2797,
        "pr_file": "pkg/scheduler/api/job_info.go",
        "discussion_id": "1213978902",
        "commented_code": "@@ -142,9 +142,10 @@ func getJobID(pod *v1.Pod) JobID {\n \treturn \"\"\n }\n \n-func getTaskID(pod *v1.Pod) TaskID {",
        "comment_created_at": "2023-06-17T04:24:33+00:00",
        "comment_author": "lowang-bh",
        "comment_body": "Yeah, TaskID stands for UID, but the content of this func does not return UID at all.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1213983666",
    "pr_number": 2797,
    "pr_file": "pkg/controllers/apis/job_info.go",
    "created_at": "2023-06-02T06:40:50+00:00",
    "commented_code": "// AddPod adds the k8s pod object values to the Pods field\n // of JobStruct if it doesn't exist. Otherwise it throws error.\n func (ji *JobInfo) AddPod(pod *v1.Pod) error {\n-\ttaskName, found := pod.Annotations[batch.TaskSpecKey]",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1213983666",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 2797,
        "pr_file": "pkg/controllers/apis/job_info.go",
        "discussion_id": "1213983666",
        "commented_code": "@@ -63,50 +63,45 @@ func (ji *JobInfo) SetJob(job *batch.Job) {\n // AddPod adds the k8s pod object values to the Pods field\n // of JobStruct if it doesn't exist. Otherwise it throws error.\n func (ji *JobInfo) AddPod(pod *v1.Pod) error {\n-\ttaskName, found := pod.Annotations[batch.TaskSpecKey]",
        "comment_created_at": "2023-06-02T06:40:50+00:00",
        "comment_author": "william-wang",
        "comment_body": "Seems taskName is more easier to understand. It's better to keep as it is.",
        "pr_file_module": null
      },
      {
        "comment_id": "1232951828",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 2797,
        "pr_file": "pkg/controllers/apis/job_info.go",
        "discussion_id": "1213983666",
        "commented_code": "@@ -63,50 +63,45 @@ func (ji *JobInfo) SetJob(job *batch.Job) {\n // AddPod adds the k8s pod object values to the Pods field\n // of JobStruct if it doesn't exist. Otherwise it throws error.\n func (ji *JobInfo) AddPod(pod *v1.Pod) error {\n-\ttaskName, found := pod.Annotations[batch.TaskSpecKey]",
        "comment_created_at": "2023-06-17T04:17:37+00:00",
        "comment_author": "lowang-bh",
        "comment_body": "In my opinion, `taskName` equals to name of pod which treats as task.  `specName` is a kind to distinguish from eath other's role in job, suce as master, worker, ps, chief.  I think we can be more strict on the variable name and make code have high readability.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "946430722",
    "pr_number": 2358,
    "pr_file": "pkg/webhooks/admission/pods/validate/admit_pod.go",
    "created_at": "2022-08-16T07:33:50+00:00",
    "commented_code": "return msg\n }\n \n+func isVcJob(pod *v1.Pod) bool {",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "946430722",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 2358,
        "pr_file": "pkg/webhooks/admission/pods/validate/admit_pod.go",
        "discussion_id": "946430722",
        "commented_code": "@@ -145,6 +146,15 @@ func validatePod(pod *v1.Pod, reviewResponse *admissionv1.AdmissionResponse) str\n \treturn msg\n }\n \n+func isVcJob(pod *v1.Pod) bool {",
        "comment_created_at": "2022-08-16T07:33:50+00:00",
        "comment_author": "Thor-wl",
        "comment_body": "Suggest to rename to `BelongToVcJob`",
        "pr_file_module": null
      },
      {
        "comment_id": "949742908",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 2358,
        "pr_file": "pkg/webhooks/admission/pods/validate/admit_pod.go",
        "discussion_id": "946430722",
        "commented_code": "@@ -145,6 +146,15 @@ func validatePod(pod *v1.Pod, reviewResponse *admissionv1.AdmissionResponse) str\n \treturn msg\n }\n \n+func isVcJob(pod *v1.Pod) bool {",
        "comment_created_at": "2022-08-19T02:23:00+00:00",
        "comment_author": "zbbkeepgoing",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1436826014",
    "pr_number": 3210,
    "pr_file": "pkg/scheduler/api/devices/nvidia/vgpu/utils.go",
    "created_at": "2023-12-27T08:02:30+00:00",
    "commented_code": "}\n \n // checkNodeGPUSharingPredicate checks if a pod with gpu requirement can be scheduled on a node.\n-func checkNodeGPUSharingPredicate(pod *v1.Pod, gssnap *GPUDevices, replicate bool) (bool, []ContainerDevices, error) {\n+func checkNodeGPUSharingPredicate(pod *v1.Pod, gssnap *GPUDevices, replicate bool) (bool, []ContainerDevices, float64, error) {",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1436826014",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3210,
        "pr_file": "pkg/scheduler/api/devices/nvidia/vgpu/utils.go",
        "discussion_id": "1436826014",
        "commented_code": "@@ -326,14 +327,15 @@ func getGPUDeviceSnapShot(snap *GPUDevices) *GPUDevices {\n }\n \n // checkNodeGPUSharingPredicate checks if a pod with gpu requirement can be scheduled on a node.\n-func checkNodeGPUSharingPredicate(pod *v1.Pod, gssnap *GPUDevices, replicate bool) (bool, []ContainerDevices, error) {\n+func checkNodeGPUSharingPredicate(pod *v1.Pod, gssnap *GPUDevices, replicate bool) (bool, []ContainerDevices, float64, error) {",
        "comment_created_at": "2023-12-27T08:02:30+00:00",
        "comment_author": "Monokaix",
        "comment_body": "predict and score are both in one function, but funcname is predict, a little wired here. Can we seperate them?",
        "pr_file_module": null
      },
      {
        "comment_id": "1436884733",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3210,
        "pr_file": "pkg/scheduler/api/devices/nvidia/vgpu/utils.go",
        "discussion_id": "1436826014",
        "commented_code": "@@ -326,14 +327,15 @@ func getGPUDeviceSnapShot(snap *GPUDevices) *GPUDevices {\n }\n \n // checkNodeGPUSharingPredicate checks if a pod with gpu requirement can be scheduled on a node.\n-func checkNodeGPUSharingPredicate(pod *v1.Pod, gssnap *GPUDevices, replicate bool) (bool, []ContainerDevices, error) {\n+func checkNodeGPUSharingPredicate(pod *v1.Pod, gssnap *GPUDevices, replicate bool) (bool, []ContainerDevices, float64, error) {",
        "comment_created_at": "2023-12-27T09:39:40+00:00",
        "comment_author": "archlitchi",
        "comment_body": "i thought about that, but either \"predicate\" or \"score\" needs to iterate all available devices on this node, which is a rather slow move. so i calculate and cache the score in \"predicate\", and return the corresponding score directly in \"score\" in order to improve performance",
        "pr_file_module": null
      },
      {
        "comment_id": "1436888093",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3210,
        "pr_file": "pkg/scheduler/api/devices/nvidia/vgpu/utils.go",
        "discussion_id": "1436826014",
        "commented_code": "@@ -326,14 +327,15 @@ func getGPUDeviceSnapShot(snap *GPUDevices) *GPUDevices {\n }\n \n // checkNodeGPUSharingPredicate checks if a pod with gpu requirement can be scheduled on a node.\n-func checkNodeGPUSharingPredicate(pod *v1.Pod, gssnap *GPUDevices, replicate bool) (bool, []ContainerDevices, error) {\n+func checkNodeGPUSharingPredicate(pod *v1.Pod, gssnap *GPUDevices, replicate bool) (bool, []ContainerDevices, float64, error) {",
        "comment_created_at": "2023-12-27T09:45:26+00:00",
        "comment_author": "Monokaix",
        "comment_body": "If so, we'd better rename this function name.",
        "pr_file_module": null
      },
      {
        "comment_id": "1436889915",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 3210,
        "pr_file": "pkg/scheduler/api/devices/nvidia/vgpu/utils.go",
        "discussion_id": "1436826014",
        "commented_code": "@@ -326,14 +327,15 @@ func getGPUDeviceSnapShot(snap *GPUDevices) *GPUDevices {\n }\n \n // checkNodeGPUSharingPredicate checks if a pod with gpu requirement can be scheduled on a node.\n-func checkNodeGPUSharingPredicate(pod *v1.Pod, gssnap *GPUDevices, replicate bool) (bool, []ContainerDevices, error) {\n+func checkNodeGPUSharingPredicate(pod *v1.Pod, gssnap *GPUDevices, replicate bool) (bool, []ContainerDevices, float64, error) {",
        "comment_created_at": "2023-12-27T09:48:26+00:00",
        "comment_author": "archlitchi",
        "comment_body": "ok, i 'll rename it to checkNodeGPUSharingPredicateWithScore(...)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1948974926",
    "pr_number": 4000,
    "pr_file": "pkg/controllers/podgroup/pg_controller_handler.go",
    "created_at": "2025-02-10T12:31:58+00:00",
    "commented_code": "}\n }\n \n+func (pg *pgcontroller) getMinMemberFromUpperRes(pod *v1.Pod) *int32 {\n+\tdefaultMinMember := int32(1)",
    "repo_full_name": "volcano-sh/volcano",
    "discussion_comments": [
      {
        "comment_id": "1948974926",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4000,
        "pr_file": "pkg/controllers/podgroup/pg_controller_handler.go",
        "discussion_id": "1948974926",
        "commented_code": "@@ -177,6 +181,37 @@ func (pg *pgcontroller) getAnnotationsFromUpperRes(kind string, name string, nam\n \t}\n }\n \n+func (pg *pgcontroller) getMinMemberFromUpperRes(pod *v1.Pod) *int32 {\n+\tdefaultMinMember := int32(1)",
        "comment_created_at": "2025-02-10T12:31:58+00:00",
        "comment_author": "JesseStutler",
        "comment_body": "This variable name is very strange. You can set 1 as a constant (as defaultMinMember), and then the variable here is initialized to defaultMinMember. `minMember` here is ok",
        "pr_file_module": null
      },
      {
        "comment_id": "1950174944",
        "repo_full_name": "volcano-sh/volcano",
        "pr_number": 4000,
        "pr_file": "pkg/controllers/podgroup/pg_controller_handler.go",
        "discussion_id": "1948974926",
        "commented_code": "@@ -177,6 +181,37 @@ func (pg *pgcontroller) getAnnotationsFromUpperRes(kind string, name string, nam\n \t}\n }\n \n+func (pg *pgcontroller) getMinMemberFromUpperRes(pod *v1.Pod) *int32 {\n+\tdefaultMinMember := int32(1)",
        "comment_created_at": "2025-02-11T03:00:54+00:00",
        "comment_author": "sceneryback",
        "comment_body": "Updated",
        "pr_file_module": null
      }
    ]
  }
]