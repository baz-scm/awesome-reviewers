[
  {
    "discussion_id": "2294153724",
    "pr_number": 4861,
    "pr_file": "src/workerd/api/kv.c++",
    "created_at": "2025-08-22T16:18:34+00:00",
    "commented_code": "});\n     });\n \n-    return context.awaitIo(js, kj::mv(promise), [&](jsg::Lock& js, kj::String text) mutable {\n+    return context\n+        .awaitIo(js, kj::mv(promise), [&](jsg::Lock& js, kj::String text) mutable {",
    "repo_full_name": "cloudflare/workerd",
    "discussion_comments": [
      {
        "comment_id": "2294153724",
        "repo_full_name": "cloudflare/workerd",
        "pr_number": 4861,
        "pr_file": "src/workerd/api/kv.c++",
        "discussion_id": "2294153724",
        "commented_code": "@@ -165,7 +165,8 @@ jsg::Promise<jsg::JsRef<jsg::JsMap>> KvNamespace::getBulk(jsg::Lock& js,\n       });\n     });\n \n-    return context.awaitIo(js, kj::mv(promise), [&](jsg::Lock& js, kj::String text) mutable {\n+    return context\n+        .awaitIo(js, kj::mv(promise), [&](jsg::Lock& js, kj::String text) mutable {",
        "comment_created_at": "2025-08-22T16:18:34+00:00",
        "comment_author": "kentonv",
        "comment_body": "Not new in this PR, but this `[&]` capture needs to be replaced with an explicit capture list. Capturing `[&]` is only safe when the lambda is executed synchronously, which it isn't here.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2288262209",
    "pr_number": 4821,
    "pr_file": "src/workerd/server/server.c++",
    "created_at": "2025-08-20T13:55:35+00:00",
    "commented_code": "jsg::Lock& js, const Worker::Api& api, v8::Local<v8::Object> target) {\n           env.populateJsObject(js, jsg::JsObject(target));\n         },\n+\n+        .maybeOwnedSourceCode = !source.ownContentIsRpcResponse\n+            ? kj::Maybe<kj::Own<void>>(kj::mv(source.ownContent))\n+            : kj::Maybe<kj::Own<void>>(kj::none),",
    "repo_full_name": "cloudflare/workerd",
    "discussion_comments": [
      {
        "comment_id": "2288262209",
        "repo_full_name": "cloudflare/workerd",
        "pr_number": 4821,
        "pr_file": "src/workerd/server/server.c++",
        "discussion_id": "2288262209",
        "commented_code": "@@ -3757,6 +3761,10 @@ class Server::WorkerLoaderNamespace: public kj::Refcounted {\n             jsg::Lock& js, const Worker::Api& api, v8::Local<v8::Object> target) {\n           env.populateJsObject(js, jsg::JsObject(target));\n         },\n+\n+        .maybeOwnedSourceCode = !source.ownContentIsRpcResponse\n+            ? kj::Maybe<kj::Own<void>>(kj::mv(source.ownContent))\n+            : kj::Maybe<kj::Own<void>>(kj::none),",
        "comment_created_at": "2025-08-20T13:55:35+00:00",
        "comment_author": "kentonv",
        "comment_body": "@jasnell This case, if it ever happened, would lead to a UAF -- the RPC response would presumably be destroyed after the worker is loaded.\r\n\r\nHowever, in workerd, `ownContentIsRpcResponse` is always false, so this won't happen.\r\n\r\nBut we should change this to either:\r\n* Assert that it's false.\r\n* Ignore the flag and always keep the owned content. This would be fine in workerd given it is single-threaded, so the response wouldn't be carried across event loops.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2254584893",
    "pr_number": 4684,
    "pr_file": "src/workerd/api/worker-rpc.h",
    "created_at": "2025-08-05T14:46:01+00:00",
    "commented_code": "JSG_NESTED_TYPE_NAMED(JsRpcTarget, RpcTarget);\n \n     JSG_METHOD(waitUntil);\n+    JSG_METHOD(registerRpcTargetClass);\n   }\n+\n+ private:\n+  // Global registry of RPC-capable class constructor names\n+  static kj::Vector<kj::String> registeredRpcClasses;\n+  static std::mutex registryMutex;",
    "repo_full_name": "cloudflare/workerd",
    "discussion_comments": [
      {
        "comment_id": "2254584893",
        "repo_full_name": "cloudflare/workerd",
        "pr_number": 4684,
        "pr_file": "src/workerd/api/worker-rpc.h",
        "discussion_id": "2254584893",
        "commented_code": "@@ -566,7 +574,13 @@ class EntrypointsModule: public jsg::Object {\n     JSG_NESTED_TYPE_NAMED(JsRpcTarget, RpcTarget);\n \n     JSG_METHOD(waitUntil);\n+    JSG_METHOD(registerRpcTargetClass);\n   }\n+\n+ private:\n+  // Global registry of RPC-capable class constructor names\n+  static kj::Vector<kj::String> registeredRpcClasses;\n+  static std::mutex registryMutex;",
        "comment_created_at": "2025-08-05T14:46:01+00:00",
        "comment_author": "jasnell",
        "comment_body": "Should this use a `kj::MutexGuarded` instead?",
        "pr_file_module": null
      },
      {
        "comment_id": "2254587724",
        "repo_full_name": "cloudflare/workerd",
        "pr_number": 4684,
        "pr_file": "src/workerd/api/worker-rpc.h",
        "discussion_id": "2254584893",
        "commented_code": "@@ -566,7 +574,13 @@ class EntrypointsModule: public jsg::Object {\n     JSG_NESTED_TYPE_NAMED(JsRpcTarget, RpcTarget);\n \n     JSG_METHOD(waitUntil);\n+    JSG_METHOD(registerRpcTargetClass);\n   }\n+\n+ private:\n+  // Global registry of RPC-capable class constructor names\n+  static kj::Vector<kj::String> registeredRpcClasses;\n+  static std::mutex registryMutex;",
        "comment_created_at": "2025-08-05T14:47:06+00:00",
        "comment_author": "jasnell",
        "comment_body": "Also, is a mutex actually needed in this? Operations on this should be guarded by the isolate lock already?",
        "pr_file_module": null
      },
      {
        "comment_id": "2254594057",
        "repo_full_name": "cloudflare/workerd",
        "pr_number": 4684,
        "pr_file": "src/workerd/api/worker-rpc.h",
        "discussion_id": "2254584893",
        "commented_code": "@@ -566,7 +574,13 @@ class EntrypointsModule: public jsg::Object {\n     JSG_NESTED_TYPE_NAMED(JsRpcTarget, RpcTarget);\n \n     JSG_METHOD(waitUntil);\n+    JSG_METHOD(registerRpcTargetClass);\n   }\n+\n+ private:\n+  // Global registry of RPC-capable class constructor names\n+  static kj::Vector<kj::String> registeredRpcClasses;\n+  static std::mutex registryMutex;",
        "comment_created_at": "2025-08-05T14:49:23+00:00",
        "comment_author": "jasnell",
        "comment_body": "ah, nevermind, just noticed this is a static... I'm not sure that's a great approach here. the registry would end up being shared across *all* workers in the process but I don't think that's actually what we want, is it? Could easily end up having conflicts across workers from different customers, etc.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2053294284",
    "pr_number": 3813,
    "pr_file": "src/workerd/api/messagechannel.c++",
    "created_at": "2025-04-22T04:06:48+00:00",
    "commented_code": "+#include \"messagechannel.h\"\n+\n+#include \"global-scope.h\"\n+#include \"worker-rpc.h\"\n+\n+#include <workerd/io/worker.h>\n+#include <workerd/jsg/ser.h>\n+\n+#include <capnp/message.h>\n+\n+namespace workerd::api {\n+jsg::Ref<MessagePort::MessageEvent> MessagePort::MessageEvent::New(jsg::Lock& js,\n+    jsg::JsRef<jsg::JsValue> data,\n+    kj::String lastEventId,\n+    jsg::Ref<MessagePort> source,\n+    kj::Array<jsg::Ref<MessagePort>> ports) {\n+  return jsg::alloc<MessagePort::MessageEvent>(\n+      kj::str(\"message\"), kj::mv(data), kj::mv(lastEventId), kj::mv(source), kj::mv(ports));\n+}\n+\n+jsg::Ref<MessagePort::MessageEvent> MessagePort::MessageEvent::NewError(jsg::Lock& js,\n+    jsg::JsRef<jsg::JsValue> data,\n+    kj::String lastEventId,\n+    jsg::Ref<MessagePort> source,\n+    kj::Array<jsg::Ref<MessagePort>> ports) {\n+  return jsg::alloc<MessagePort::MessageEvent>(\n+      kj::str(\"messageerror\"), kj::mv(data), kj::mv(lastEventId), kj::mv(source), kj::mv(ports));\n+}\n+\n+MessagePort::MessagePort(IoContext& ioContext): ioContext(ioContext), state(Pending()) {\n+  // We set a callback on the underlying EventTarget to be notified when\n+  // a listener for the message event is added or removed. When there\n+  // are no listeners, we move back to the Pending state, otherwise we\n+  // will switch to the Started state if necessary.\n+  setEventListenerCallback([&](jsg::Lock& js, kj::StringPtr name, size_t count) {\n+    if (name == \"message\"_kj) {\n+      KJ_SWITCH_ONEOF(state) {\n+        KJ_CASE_ONEOF(pending, Pending) {\n+          // If we are in the pending state, start the port if we have listeners.\n+          // This is technically not spec compliant, but it is what Node.js\n+          // supports. Specifically, adding a new message listener using the\n+          // addEventListener method is *technically* not supposed to start\n+          // the port but we're going to do what Node.js does.\n+          if (count > 0 || onmessageValue != kj::none) {\n+            start(js);\n+          }\n+        }\n+        KJ_CASE_ONEOF(started, Started) {\n+          // If we are in the started state, stop the port if there are no listeners.\n+          if (count == 0 && onmessageValue == kj::none) {\n+            state = Pending();\n+          }\n+        }\n+        KJ_CASE_ONEOF(_, Closed) {\n+          // Nothing to do. We're already closed so we don't care.\n+        }\n+      }\n+    }\n+  });\n+}\n+\n+// Deliver the message to the \"message\" or \"messageerror\" event on this port.\n+void MessagePort::deliver(\n+    jsg::Lock& js, kj::Own<rpc::JsValue::Reader> message, jsg::Ref<MessagePort> port) {\n+  auto event = js.tryCatch([&] {\n+    jsg::Deserializer deserializer(js, message->getV8Serialized());\n+    return MessageEvent::New(\n+        js, jsg::JsRef(js, deserializer.readValue(js)), kj::str(), port->addRef(), {});\n+  }, [&](jsg::Value exception) {\n+    return MessageEvent::NewError(\n+        js, jsg::JsRef(js, jsg::JsValue(exception.getHandle(js))), kj::str(), port->addRef(), {});\n+  });\n+\n+  // If any of the message/messageerror event handlers throw,\n+  // capture the error and pass it to reportError instead of\n+  // propagating up.\n+  js.tryCatch([&] { port->dispatchEventImpl(js, kj::mv(event)); }, [&](jsg::Value exception) {\n+    auto context = js.v8Context();\n+    auto& global =\n+        jsg::extractInternalPointer<ServiceWorkerGlobalScope, true>(context, context->Global());\n+    global.reportError(js, jsg::JsValue(exception.getHandle(js)));\n+  });\n+}\n+\n+// Deliver the message to all the jsrpc remotes we have\n+kj::Promise<void> MessagePort::sendToRpc(kj::Own<rpc::JsValue::Reader> message) {\n+  KJ_IF_SOME(outputLocks, ioContext.waitForOutputLocksIfNecessary()) {\n+    co_await outputLocks;\n+  }\n+  kj::Vector<kj::Promise<void>> promises;\n+  for (rpc::JsMessagePort::Client cap: rpcClients) {\n+    auto req = cap.callRequest();\n+    req.setData(*message);\n+    // TODO(message-port): Removing the port when dropped?\n+    promises.add(req.send().ignoreResult());\n+  }\n+  co_await kj::joinPromises(promises.releaseAsArray());\n+}\n+\n+// Deliver the message to this port, buffering if necessary if the port\n+// has not been started. Buffered messages will be delivered when the\n+// port is started later.\n+void MessagePort::deliverMessage(jsg::Lock& js, rpc::JsValue::Reader value) {\n+  KJ_SWITCH_ONEOF(state) {\n+    KJ_CASE_ONEOF(pending, Pending) {\n+      // We have not yet started the port so buffer to message.\n+      pending.add(capnp::clone(value));\n+    }\n+    KJ_CASE_ONEOF(started, Started) {\n+      ioContext.addTask(sendToRpc(capnp::clone(value)));\n+      deliver(js, capnp::clone(value), addRef());\n+    }\n+    KJ_CASE_ONEOF(_, Closed) {\n+      // Nothing to do in this case. Drop the message on the floor.\n+    }\n+  }\n+}\n+\n+// Binds two ports to each other such that messages posted to one\n+// are delivered on the other.\n+void MessagePort::entangle(MessagePort& port1, MessagePort& port2) {\n+  port1.other = port2.addRef();\n+  port2.other = port1.addRef();\n+}\n+\n+// Post a message to the entangled port.\n+void MessagePort::postMessage(jsg::Lock& js,\n+    jsg::Optional<jsg::JsRef<jsg::JsValue>> data,\n+    jsg::Optional<TransferListOrOptions> options) {\n+\n+  KJ_IF_SOME(opt, options) {\n+    // We don't currently support transfer lists, even for local\n+    // same-isolate delivery.\n+    // TODO(conform): Implement transfer later?\n+    KJ_SWITCH_ONEOF(opt) {\n+      KJ_CASE_ONEOF(list, TransferList) {\n+        JSG_REQUIRE(list.size() == 0, Error, \"Transfer list is not supported\");\n+      }\n+      KJ_CASE_ONEOF(opts, PostMessageOptions) {\n+        KJ_IF_SOME(list, opts.transfer) {\n+          JSG_REQUIRE(list.size() == 0, Error, \"Transfer list is not supported\");\n+        }\n+      }\n+    }\n+  }\n+\n+  KJ_IF_SOME(o, other) {\n+    // TODO(message-port): Set up the external handler to support more types.\n+    jsg::Serializer ser(js);\n+    KJ_IF_SOME(d, data) {\n+      ser.write(js, d.getHandle(js));\n+    }\n+    auto released = ser.release();\n+    JSG_REQUIRE(released.sharedArrayBuffers.size() == 0, Error, \"SharedArrayBuffer is unsupported\");\n+\n+    capnp::MallocMessageBuilder builder;\n+    rpc::JsValue::Builder val = builder.initRoot<rpc::JsValue>();\n+    val.setV8Serialized(kj::mv(released.data));\n+    o->deliverMessage(js, val.asReader());\n+  }\n+}\n+\n+// Should close this port, the entangle port, and any known rpc clients.\n+void MessagePort::close() {\n+  state = Closed{};\n+  rpcClients.clear();\n+  KJ_IF_SOME(o, other) {\n+    auto closing = kj::mv(o);\n+    other = kj::none;\n+    closing->close();\n+  }\n+}\n+\n+// Start delivering messages on this port. Any messages that are\n+// buffered will be drained immediately.\n+void MessagePort::start(jsg::Lock& js) {\n+  KJ_SWITCH_ONEOF(state) {\n+    KJ_CASE_ONEOF(pending, Pending) {\n+      auto list = kj::mv(pending);\n+      state = Started{};\n+      for (auto& item: list) {\n+        ioContext.addTask(sendToRpc(capnp::clone(*item)));\n+        // Local delivery\n+        deliver(js, kj::mv(item), addRef());\n+      }\n+    }\n+    KJ_CASE_ONEOF(_, Started) {\n+      // Nothing to do in this case. We are already started!\n+    }\n+    KJ_CASE_ONEOF(_, Closed) {\n+      // Nothing to do in this case. Can't start after closing.\n+    }\n+  }\n+}\n+\n+kj::Maybe<jsg::JsValue> MessagePort::getOnMessage(jsg::Lock& js) {\n+  return onmessageValue.map(\n+      [&](jsg::JsRef<jsg::JsValue>& ref) -> jsg::JsValue { return ref.getHandle(js); });\n+}\n+\n+void MessagePort::setOnMessage(jsg::Lock& js, jsg::JsValue value) {\n+  if (!value.isObject() && !value.isFunction()) {\n+    onmessageValue = kj::none;\n+    // If we have no handlers and no onmessage ...\n+    if (getHandlerCount(\"message\"_kj) == 0 && onmessageValue == kj::none) {\n+      // ...Put the port back into a pending state where messages\n+      // will be enqueued until another listener is attached.\n+      state = Pending();\n+    }\n+  } else {\n+    onmessageValue = jsg::JsRef<jsg::JsValue>(js, value);\n+    start(js);\n+  }\n+}\n+\n+namespace {\n+// The jsrpc handler that receives messages posted from the remote and\n+// delivers them to the local port.\n+class JsMessagePortImpl final: public rpc::JsMessagePort::Server {\n+ public:\n+  JsMessagePortImpl(IoContext& ctx, jsg::Ref<MessagePort> port)\n+      : port(kj::mv(port)),\n+        weakIoContext(ctx.getWeakRef()) {}\n+  ~JsMessagePortImpl() {\n+    port->close();\n+  }\n+\n+  kj::Promise<void> call(CallContext context) override {\n+    IoContext& ctx = JSG_REQUIRE_NONNULL(weakIoContext->tryGet(), Error,\n+        \"The destination object for this message port no longer exists.\");\n+\n+    KJ_IF_SOME(other, port->getOther()) {\n+      // We can only dispatch messages under the isolate lock, so acquire\n+      // that here then deliver...\n+      auto lock = co_await ctx.getWorker()->takeAsyncLockWithoutRequest(nullptr);",
    "repo_full_name": "cloudflare/workerd",
    "discussion_comments": [
      {
        "comment_id": "2053294284",
        "repo_full_name": "cloudflare/workerd",
        "pr_number": 3813,
        "pr_file": "src/workerd/api/messagechannel.c++",
        "discussion_id": "2053294284",
        "commented_code": "@@ -0,0 +1,317 @@\n+#include \"messagechannel.h\"\n+\n+#include \"global-scope.h\"\n+#include \"worker-rpc.h\"\n+\n+#include <workerd/io/worker.h>\n+#include <workerd/jsg/ser.h>\n+\n+#include <capnp/message.h>\n+\n+namespace workerd::api {\n+jsg::Ref<MessagePort::MessageEvent> MessagePort::MessageEvent::New(jsg::Lock& js,\n+    jsg::JsRef<jsg::JsValue> data,\n+    kj::String lastEventId,\n+    jsg::Ref<MessagePort> source,\n+    kj::Array<jsg::Ref<MessagePort>> ports) {\n+  return jsg::alloc<MessagePort::MessageEvent>(\n+      kj::str(\"message\"), kj::mv(data), kj::mv(lastEventId), kj::mv(source), kj::mv(ports));\n+}\n+\n+jsg::Ref<MessagePort::MessageEvent> MessagePort::MessageEvent::NewError(jsg::Lock& js,\n+    jsg::JsRef<jsg::JsValue> data,\n+    kj::String lastEventId,\n+    jsg::Ref<MessagePort> source,\n+    kj::Array<jsg::Ref<MessagePort>> ports) {\n+  return jsg::alloc<MessagePort::MessageEvent>(\n+      kj::str(\"messageerror\"), kj::mv(data), kj::mv(lastEventId), kj::mv(source), kj::mv(ports));\n+}\n+\n+MessagePort::MessagePort(IoContext& ioContext): ioContext(ioContext), state(Pending()) {\n+  // We set a callback on the underlying EventTarget to be notified when\n+  // a listener for the message event is added or removed. When there\n+  // are no listeners, we move back to the Pending state, otherwise we\n+  // will switch to the Started state if necessary.\n+  setEventListenerCallback([&](jsg::Lock& js, kj::StringPtr name, size_t count) {\n+    if (name == \"message\"_kj) {\n+      KJ_SWITCH_ONEOF(state) {\n+        KJ_CASE_ONEOF(pending, Pending) {\n+          // If we are in the pending state, start the port if we have listeners.\n+          // This is technically not spec compliant, but it is what Node.js\n+          // supports. Specifically, adding a new message listener using the\n+          // addEventListener method is *technically* not supposed to start\n+          // the port but we're going to do what Node.js does.\n+          if (count > 0 || onmessageValue != kj::none) {\n+            start(js);\n+          }\n+        }\n+        KJ_CASE_ONEOF(started, Started) {\n+          // If we are in the started state, stop the port if there are no listeners.\n+          if (count == 0 && onmessageValue == kj::none) {\n+            state = Pending();\n+          }\n+        }\n+        KJ_CASE_ONEOF(_, Closed) {\n+          // Nothing to do. We're already closed so we don't care.\n+        }\n+      }\n+    }\n+  });\n+}\n+\n+// Deliver the message to the \"message\" or \"messageerror\" event on this port.\n+void MessagePort::deliver(\n+    jsg::Lock& js, kj::Own<rpc::JsValue::Reader> message, jsg::Ref<MessagePort> port) {\n+  auto event = js.tryCatch([&] {\n+    jsg::Deserializer deserializer(js, message->getV8Serialized());\n+    return MessageEvent::New(\n+        js, jsg::JsRef(js, deserializer.readValue(js)), kj::str(), port->addRef(), {});\n+  }, [&](jsg::Value exception) {\n+    return MessageEvent::NewError(\n+        js, jsg::JsRef(js, jsg::JsValue(exception.getHandle(js))), kj::str(), port->addRef(), {});\n+  });\n+\n+  // If any of the message/messageerror event handlers throw,\n+  // capture the error and pass it to reportError instead of\n+  // propagating up.\n+  js.tryCatch([&] { port->dispatchEventImpl(js, kj::mv(event)); }, [&](jsg::Value exception) {\n+    auto context = js.v8Context();\n+    auto& global =\n+        jsg::extractInternalPointer<ServiceWorkerGlobalScope, true>(context, context->Global());\n+    global.reportError(js, jsg::JsValue(exception.getHandle(js)));\n+  });\n+}\n+\n+// Deliver the message to all the jsrpc remotes we have\n+kj::Promise<void> MessagePort::sendToRpc(kj::Own<rpc::JsValue::Reader> message) {\n+  KJ_IF_SOME(outputLocks, ioContext.waitForOutputLocksIfNecessary()) {\n+    co_await outputLocks;\n+  }\n+  kj::Vector<kj::Promise<void>> promises;\n+  for (rpc::JsMessagePort::Client cap: rpcClients) {\n+    auto req = cap.callRequest();\n+    req.setData(*message);\n+    // TODO(message-port): Removing the port when dropped?\n+    promises.add(req.send().ignoreResult());\n+  }\n+  co_await kj::joinPromises(promises.releaseAsArray());\n+}\n+\n+// Deliver the message to this port, buffering if necessary if the port\n+// has not been started. Buffered messages will be delivered when the\n+// port is started later.\n+void MessagePort::deliverMessage(jsg::Lock& js, rpc::JsValue::Reader value) {\n+  KJ_SWITCH_ONEOF(state) {\n+    KJ_CASE_ONEOF(pending, Pending) {\n+      // We have not yet started the port so buffer to message.\n+      pending.add(capnp::clone(value));\n+    }\n+    KJ_CASE_ONEOF(started, Started) {\n+      ioContext.addTask(sendToRpc(capnp::clone(value)));\n+      deliver(js, capnp::clone(value), addRef());\n+    }\n+    KJ_CASE_ONEOF(_, Closed) {\n+      // Nothing to do in this case. Drop the message on the floor.\n+    }\n+  }\n+}\n+\n+// Binds two ports to each other such that messages posted to one\n+// are delivered on the other.\n+void MessagePort::entangle(MessagePort& port1, MessagePort& port2) {\n+  port1.other = port2.addRef();\n+  port2.other = port1.addRef();\n+}\n+\n+// Post a message to the entangled port.\n+void MessagePort::postMessage(jsg::Lock& js,\n+    jsg::Optional<jsg::JsRef<jsg::JsValue>> data,\n+    jsg::Optional<TransferListOrOptions> options) {\n+\n+  KJ_IF_SOME(opt, options) {\n+    // We don't currently support transfer lists, even for local\n+    // same-isolate delivery.\n+    // TODO(conform): Implement transfer later?\n+    KJ_SWITCH_ONEOF(opt) {\n+      KJ_CASE_ONEOF(list, TransferList) {\n+        JSG_REQUIRE(list.size() == 0, Error, \"Transfer list is not supported\");\n+      }\n+      KJ_CASE_ONEOF(opts, PostMessageOptions) {\n+        KJ_IF_SOME(list, opts.transfer) {\n+          JSG_REQUIRE(list.size() == 0, Error, \"Transfer list is not supported\");\n+        }\n+      }\n+    }\n+  }\n+\n+  KJ_IF_SOME(o, other) {\n+    // TODO(message-port): Set up the external handler to support more types.\n+    jsg::Serializer ser(js);\n+    KJ_IF_SOME(d, data) {\n+      ser.write(js, d.getHandle(js));\n+    }\n+    auto released = ser.release();\n+    JSG_REQUIRE(released.sharedArrayBuffers.size() == 0, Error, \"SharedArrayBuffer is unsupported\");\n+\n+    capnp::MallocMessageBuilder builder;\n+    rpc::JsValue::Builder val = builder.initRoot<rpc::JsValue>();\n+    val.setV8Serialized(kj::mv(released.data));\n+    o->deliverMessage(js, val.asReader());\n+  }\n+}\n+\n+// Should close this port, the entangle port, and any known rpc clients.\n+void MessagePort::close() {\n+  state = Closed{};\n+  rpcClients.clear();\n+  KJ_IF_SOME(o, other) {\n+    auto closing = kj::mv(o);\n+    other = kj::none;\n+    closing->close();\n+  }\n+}\n+\n+// Start delivering messages on this port. Any messages that are\n+// buffered will be drained immediately.\n+void MessagePort::start(jsg::Lock& js) {\n+  KJ_SWITCH_ONEOF(state) {\n+    KJ_CASE_ONEOF(pending, Pending) {\n+      auto list = kj::mv(pending);\n+      state = Started{};\n+      for (auto& item: list) {\n+        ioContext.addTask(sendToRpc(capnp::clone(*item)));\n+        // Local delivery\n+        deliver(js, kj::mv(item), addRef());\n+      }\n+    }\n+    KJ_CASE_ONEOF(_, Started) {\n+      // Nothing to do in this case. We are already started!\n+    }\n+    KJ_CASE_ONEOF(_, Closed) {\n+      // Nothing to do in this case. Can't start after closing.\n+    }\n+  }\n+}\n+\n+kj::Maybe<jsg::JsValue> MessagePort::getOnMessage(jsg::Lock& js) {\n+  return onmessageValue.map(\n+      [&](jsg::JsRef<jsg::JsValue>& ref) -> jsg::JsValue { return ref.getHandle(js); });\n+}\n+\n+void MessagePort::setOnMessage(jsg::Lock& js, jsg::JsValue value) {\n+  if (!value.isObject() && !value.isFunction()) {\n+    onmessageValue = kj::none;\n+    // If we have no handlers and no onmessage ...\n+    if (getHandlerCount(\"message\"_kj) == 0 && onmessageValue == kj::none) {\n+      // ...Put the port back into a pending state where messages\n+      // will be enqueued until another listener is attached.\n+      state = Pending();\n+    }\n+  } else {\n+    onmessageValue = jsg::JsRef<jsg::JsValue>(js, value);\n+    start(js);\n+  }\n+}\n+\n+namespace {\n+// The jsrpc handler that receives messages posted from the remote and\n+// delivers them to the local port.\n+class JsMessagePortImpl final: public rpc::JsMessagePort::Server {\n+ public:\n+  JsMessagePortImpl(IoContext& ctx, jsg::Ref<MessagePort> port)\n+      : port(kj::mv(port)),\n+        weakIoContext(ctx.getWeakRef()) {}\n+  ~JsMessagePortImpl() {\n+    port->close();\n+  }\n+\n+  kj::Promise<void> call(CallContext context) override {\n+    IoContext& ctx = JSG_REQUIRE_NONNULL(weakIoContext->tryGet(), Error,\n+        \"The destination object for this message port no longer exists.\");\n+\n+    KJ_IF_SOME(other, port->getOther()) {\n+      // We can only dispatch messages under the isolate lock, so acquire\n+      // that here then deliver...\n+      auto lock = co_await ctx.getWorker()->takeAsyncLockWithoutRequest(nullptr);",
        "comment_created_at": "2025-04-22T04:06:48+00:00",
        "comment_author": "kentonv",
        "comment_body": "Use `ctx.run` to enter the isolate. The code you have here is manually doing a lot of what `ctx.run` does, and is missing important things like setting the current `IoContext`, honoring input locks, etc.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2053299338",
    "pr_number": 3813,
    "pr_file": "src/workerd/api/messagechannel.c++",
    "created_at": "2025-04-22T04:15:18+00:00",
    "commented_code": "+#include \"messagechannel.h\"\n+\n+#include \"global-scope.h\"\n+#include \"worker-rpc.h\"\n+\n+#include <workerd/io/worker.h>\n+#include <workerd/jsg/ser.h>\n+\n+#include <capnp/message.h>\n+\n+namespace workerd::api {\n+jsg::Ref<MessagePort::MessageEvent> MessagePort::MessageEvent::New(jsg::Lock& js,\n+    jsg::JsRef<jsg::JsValue> data,\n+    kj::String lastEventId,\n+    jsg::Ref<MessagePort> source,\n+    kj::Array<jsg::Ref<MessagePort>> ports) {\n+  return jsg::alloc<MessagePort::MessageEvent>(\n+      kj::str(\"message\"), kj::mv(data), kj::mv(lastEventId), kj::mv(source), kj::mv(ports));\n+}\n+\n+jsg::Ref<MessagePort::MessageEvent> MessagePort::MessageEvent::NewError(jsg::Lock& js,\n+    jsg::JsRef<jsg::JsValue> data,\n+    kj::String lastEventId,\n+    jsg::Ref<MessagePort> source,\n+    kj::Array<jsg::Ref<MessagePort>> ports) {\n+  return jsg::alloc<MessagePort::MessageEvent>(\n+      kj::str(\"messageerror\"), kj::mv(data), kj::mv(lastEventId), kj::mv(source), kj::mv(ports));\n+}\n+\n+MessagePort::MessagePort(IoContext& ioContext): ioContext(ioContext), state(Pending()) {\n+  // We set a callback on the underlying EventTarget to be notified when\n+  // a listener for the message event is added or removed. When there\n+  // are no listeners, we move back to the Pending state, otherwise we\n+  // will switch to the Started state if necessary.\n+  setEventListenerCallback([&](jsg::Lock& js, kj::StringPtr name, size_t count) {\n+    if (name == \"message\"_kj) {\n+      KJ_SWITCH_ONEOF(state) {\n+        KJ_CASE_ONEOF(pending, Pending) {\n+          // If we are in the pending state, start the port if we have listeners.\n+          // This is technically not spec compliant, but it is what Node.js\n+          // supports. Specifically, adding a new message listener using the\n+          // addEventListener method is *technically* not supposed to start\n+          // the port but we're going to do what Node.js does.\n+          if (count > 0 || onmessageValue != kj::none) {\n+            start(js);\n+          }\n+        }\n+        KJ_CASE_ONEOF(started, Started) {\n+          // If we are in the started state, stop the port if there are no listeners.\n+          if (count == 0 && onmessageValue == kj::none) {\n+            state = Pending();\n+          }\n+        }\n+        KJ_CASE_ONEOF(_, Closed) {\n+          // Nothing to do. We're already closed so we don't care.\n+        }\n+      }\n+    }\n+  });\n+}\n+\n+// Deliver the message to the \"message\" or \"messageerror\" event on this port.\n+void MessagePort::deliver(\n+    jsg::Lock& js, kj::Own<rpc::JsValue::Reader> message, jsg::Ref<MessagePort> port) {\n+  auto event = js.tryCatch([&] {\n+    jsg::Deserializer deserializer(js, message->getV8Serialized());\n+    return MessageEvent::New(\n+        js, jsg::JsRef(js, deserializer.readValue(js)), kj::str(), port->addRef(), {});\n+  }, [&](jsg::Value exception) {\n+    return MessageEvent::NewError(\n+        js, jsg::JsRef(js, jsg::JsValue(exception.getHandle(js))), kj::str(), port->addRef(), {});\n+  });\n+\n+  // If any of the message/messageerror event handlers throw,\n+  // capture the error and pass it to reportError instead of\n+  // propagating up.\n+  js.tryCatch([&] { port->dispatchEventImpl(js, kj::mv(event)); }, [&](jsg::Value exception) {\n+    auto context = js.v8Context();\n+    auto& global =\n+        jsg::extractInternalPointer<ServiceWorkerGlobalScope, true>(context, context->Global());\n+    global.reportError(js, jsg::JsValue(exception.getHandle(js)));\n+  });\n+}\n+\n+// Deliver the message to all the jsrpc remotes we have\n+kj::Promise<void> MessagePort::sendToRpc(kj::Own<rpc::JsValue::Reader> message) {\n+  KJ_IF_SOME(outputLocks, ioContext.waitForOutputLocksIfNecessary()) {\n+    co_await outputLocks;\n+  }\n+  kj::Vector<kj::Promise<void>> promises;\n+  for (rpc::JsMessagePort::Client cap: rpcClients) {\n+    auto req = cap.callRequest();\n+    req.setData(*message);\n+    // TODO(message-port): Removing the port when dropped?\n+    promises.add(req.send().ignoreResult());\n+  }\n+  co_await kj::joinPromises(promises.releaseAsArray());\n+}\n+\n+// Deliver the message to this port, buffering if necessary if the port\n+// has not been started. Buffered messages will be delivered when the\n+// port is started later.\n+void MessagePort::deliverMessage(jsg::Lock& js, rpc::JsValue::Reader value) {\n+  KJ_SWITCH_ONEOF(state) {\n+    KJ_CASE_ONEOF(pending, Pending) {\n+      // We have not yet started the port so buffer to message.\n+      pending.add(capnp::clone(value));\n+    }\n+    KJ_CASE_ONEOF(started, Started) {\n+      ioContext.addTask(sendToRpc(capnp::clone(value)));\n+      deliver(js, capnp::clone(value), addRef());\n+    }\n+    KJ_CASE_ONEOF(_, Closed) {\n+      // Nothing to do in this case. Drop the message on the floor.\n+    }\n+  }\n+}\n+\n+// Binds two ports to each other such that messages posted to one\n+// are delivered on the other.\n+void MessagePort::entangle(MessagePort& port1, MessagePort& port2) {\n+  port1.other = port2.addRef();\n+  port2.other = port1.addRef();\n+}\n+\n+// Post a message to the entangled port.\n+void MessagePort::postMessage(jsg::Lock& js,\n+    jsg::Optional<jsg::JsRef<jsg::JsValue>> data,\n+    jsg::Optional<TransferListOrOptions> options) {\n+\n+  KJ_IF_SOME(opt, options) {\n+    // We don't currently support transfer lists, even for local\n+    // same-isolate delivery.\n+    // TODO(conform): Implement transfer later?\n+    KJ_SWITCH_ONEOF(opt) {\n+      KJ_CASE_ONEOF(list, TransferList) {\n+        JSG_REQUIRE(list.size() == 0, Error, \"Transfer list is not supported\");\n+      }\n+      KJ_CASE_ONEOF(opts, PostMessageOptions) {\n+        KJ_IF_SOME(list, opts.transfer) {\n+          JSG_REQUIRE(list.size() == 0, Error, \"Transfer list is not supported\");\n+        }\n+      }\n+    }\n+  }\n+\n+  KJ_IF_SOME(o, other) {\n+    // TODO(message-port): Set up the external handler to support more types.\n+    jsg::Serializer ser(js);\n+    KJ_IF_SOME(d, data) {\n+      ser.write(js, d.getHandle(js));\n+    }\n+    auto released = ser.release();\n+    JSG_REQUIRE(released.sharedArrayBuffers.size() == 0, Error, \"SharedArrayBuffer is unsupported\");\n+\n+    capnp::MallocMessageBuilder builder;\n+    rpc::JsValue::Builder val = builder.initRoot<rpc::JsValue>();\n+    val.setV8Serialized(kj::mv(released.data));\n+    o->deliverMessage(js, val.asReader());\n+  }\n+}\n+\n+// Should close this port, the entangle port, and any known rpc clients.\n+void MessagePort::close() {\n+  state = Closed{};\n+  rpcClients.clear();\n+  KJ_IF_SOME(o, other) {\n+    auto closing = kj::mv(o);\n+    other = kj::none;\n+    closing->close();\n+  }\n+}\n+\n+// Start delivering messages on this port. Any messages that are\n+// buffered will be drained immediately.\n+void MessagePort::start(jsg::Lock& js) {\n+  KJ_SWITCH_ONEOF(state) {\n+    KJ_CASE_ONEOF(pending, Pending) {\n+      auto list = kj::mv(pending);\n+      state = Started{};\n+      for (auto& item: list) {\n+        ioContext.addTask(sendToRpc(capnp::clone(*item)));\n+        // Local delivery\n+        deliver(js, kj::mv(item), addRef());\n+      }\n+    }\n+    KJ_CASE_ONEOF(_, Started) {\n+      // Nothing to do in this case. We are already started!\n+    }\n+    KJ_CASE_ONEOF(_, Closed) {\n+      // Nothing to do in this case. Can't start after closing.\n+    }\n+  }\n+}\n+\n+kj::Maybe<jsg::JsValue> MessagePort::getOnMessage(jsg::Lock& js) {\n+  return onmessageValue.map(\n+      [&](jsg::JsRef<jsg::JsValue>& ref) -> jsg::JsValue { return ref.getHandle(js); });\n+}\n+\n+void MessagePort::setOnMessage(jsg::Lock& js, jsg::JsValue value) {\n+  if (!value.isObject() && !value.isFunction()) {\n+    onmessageValue = kj::none;\n+    // If we have no handlers and no onmessage ...\n+    if (getHandlerCount(\"message\"_kj) == 0 && onmessageValue == kj::none) {\n+      // ...Put the port back into a pending state where messages\n+      // will be enqueued until another listener is attached.\n+      state = Pending();\n+    }\n+  } else {\n+    onmessageValue = jsg::JsRef<jsg::JsValue>(js, value);\n+    start(js);\n+  }\n+}\n+\n+namespace {\n+// The jsrpc handler that receives messages posted from the remote and\n+// delivers them to the local port.\n+class JsMessagePortImpl final: public rpc::JsMessagePort::Server {\n+ public:\n+  JsMessagePortImpl(IoContext& ctx, jsg::Ref<MessagePort> port)\n+      : port(kj::mv(port)),\n+        weakIoContext(ctx.getWeakRef()) {}",
    "repo_full_name": "cloudflare/workerd",
    "discussion_comments": [
      {
        "comment_id": "2053299338",
        "repo_full_name": "cloudflare/workerd",
        "pr_number": 3813,
        "pr_file": "src/workerd/api/messagechannel.c++",
        "discussion_id": "2053299338",
        "commented_code": "@@ -0,0 +1,317 @@\n+#include \"messagechannel.h\"\n+\n+#include \"global-scope.h\"\n+#include \"worker-rpc.h\"\n+\n+#include <workerd/io/worker.h>\n+#include <workerd/jsg/ser.h>\n+\n+#include <capnp/message.h>\n+\n+namespace workerd::api {\n+jsg::Ref<MessagePort::MessageEvent> MessagePort::MessageEvent::New(jsg::Lock& js,\n+    jsg::JsRef<jsg::JsValue> data,\n+    kj::String lastEventId,\n+    jsg::Ref<MessagePort> source,\n+    kj::Array<jsg::Ref<MessagePort>> ports) {\n+  return jsg::alloc<MessagePort::MessageEvent>(\n+      kj::str(\"message\"), kj::mv(data), kj::mv(lastEventId), kj::mv(source), kj::mv(ports));\n+}\n+\n+jsg::Ref<MessagePort::MessageEvent> MessagePort::MessageEvent::NewError(jsg::Lock& js,\n+    jsg::JsRef<jsg::JsValue> data,\n+    kj::String lastEventId,\n+    jsg::Ref<MessagePort> source,\n+    kj::Array<jsg::Ref<MessagePort>> ports) {\n+  return jsg::alloc<MessagePort::MessageEvent>(\n+      kj::str(\"messageerror\"), kj::mv(data), kj::mv(lastEventId), kj::mv(source), kj::mv(ports));\n+}\n+\n+MessagePort::MessagePort(IoContext& ioContext): ioContext(ioContext), state(Pending()) {\n+  // We set a callback on the underlying EventTarget to be notified when\n+  // a listener for the message event is added or removed. When there\n+  // are no listeners, we move back to the Pending state, otherwise we\n+  // will switch to the Started state if necessary.\n+  setEventListenerCallback([&](jsg::Lock& js, kj::StringPtr name, size_t count) {\n+    if (name == \"message\"_kj) {\n+      KJ_SWITCH_ONEOF(state) {\n+        KJ_CASE_ONEOF(pending, Pending) {\n+          // If we are in the pending state, start the port if we have listeners.\n+          // This is technically not spec compliant, but it is what Node.js\n+          // supports. Specifically, adding a new message listener using the\n+          // addEventListener method is *technically* not supposed to start\n+          // the port but we're going to do what Node.js does.\n+          if (count > 0 || onmessageValue != kj::none) {\n+            start(js);\n+          }\n+        }\n+        KJ_CASE_ONEOF(started, Started) {\n+          // If we are in the started state, stop the port if there are no listeners.\n+          if (count == 0 && onmessageValue == kj::none) {\n+            state = Pending();\n+          }\n+        }\n+        KJ_CASE_ONEOF(_, Closed) {\n+          // Nothing to do. We're already closed so we don't care.\n+        }\n+      }\n+    }\n+  });\n+}\n+\n+// Deliver the message to the \"message\" or \"messageerror\" event on this port.\n+void MessagePort::deliver(\n+    jsg::Lock& js, kj::Own<rpc::JsValue::Reader> message, jsg::Ref<MessagePort> port) {\n+  auto event = js.tryCatch([&] {\n+    jsg::Deserializer deserializer(js, message->getV8Serialized());\n+    return MessageEvent::New(\n+        js, jsg::JsRef(js, deserializer.readValue(js)), kj::str(), port->addRef(), {});\n+  }, [&](jsg::Value exception) {\n+    return MessageEvent::NewError(\n+        js, jsg::JsRef(js, jsg::JsValue(exception.getHandle(js))), kj::str(), port->addRef(), {});\n+  });\n+\n+  // If any of the message/messageerror event handlers throw,\n+  // capture the error and pass it to reportError instead of\n+  // propagating up.\n+  js.tryCatch([&] { port->dispatchEventImpl(js, kj::mv(event)); }, [&](jsg::Value exception) {\n+    auto context = js.v8Context();\n+    auto& global =\n+        jsg::extractInternalPointer<ServiceWorkerGlobalScope, true>(context, context->Global());\n+    global.reportError(js, jsg::JsValue(exception.getHandle(js)));\n+  });\n+}\n+\n+// Deliver the message to all the jsrpc remotes we have\n+kj::Promise<void> MessagePort::sendToRpc(kj::Own<rpc::JsValue::Reader> message) {\n+  KJ_IF_SOME(outputLocks, ioContext.waitForOutputLocksIfNecessary()) {\n+    co_await outputLocks;\n+  }\n+  kj::Vector<kj::Promise<void>> promises;\n+  for (rpc::JsMessagePort::Client cap: rpcClients) {\n+    auto req = cap.callRequest();\n+    req.setData(*message);\n+    // TODO(message-port): Removing the port when dropped?\n+    promises.add(req.send().ignoreResult());\n+  }\n+  co_await kj::joinPromises(promises.releaseAsArray());\n+}\n+\n+// Deliver the message to this port, buffering if necessary if the port\n+// has not been started. Buffered messages will be delivered when the\n+// port is started later.\n+void MessagePort::deliverMessage(jsg::Lock& js, rpc::JsValue::Reader value) {\n+  KJ_SWITCH_ONEOF(state) {\n+    KJ_CASE_ONEOF(pending, Pending) {\n+      // We have not yet started the port so buffer to message.\n+      pending.add(capnp::clone(value));\n+    }\n+    KJ_CASE_ONEOF(started, Started) {\n+      ioContext.addTask(sendToRpc(capnp::clone(value)));\n+      deliver(js, capnp::clone(value), addRef());\n+    }\n+    KJ_CASE_ONEOF(_, Closed) {\n+      // Nothing to do in this case. Drop the message on the floor.\n+    }\n+  }\n+}\n+\n+// Binds two ports to each other such that messages posted to one\n+// are delivered on the other.\n+void MessagePort::entangle(MessagePort& port1, MessagePort& port2) {\n+  port1.other = port2.addRef();\n+  port2.other = port1.addRef();\n+}\n+\n+// Post a message to the entangled port.\n+void MessagePort::postMessage(jsg::Lock& js,\n+    jsg::Optional<jsg::JsRef<jsg::JsValue>> data,\n+    jsg::Optional<TransferListOrOptions> options) {\n+\n+  KJ_IF_SOME(opt, options) {\n+    // We don't currently support transfer lists, even for local\n+    // same-isolate delivery.\n+    // TODO(conform): Implement transfer later?\n+    KJ_SWITCH_ONEOF(opt) {\n+      KJ_CASE_ONEOF(list, TransferList) {\n+        JSG_REQUIRE(list.size() == 0, Error, \"Transfer list is not supported\");\n+      }\n+      KJ_CASE_ONEOF(opts, PostMessageOptions) {\n+        KJ_IF_SOME(list, opts.transfer) {\n+          JSG_REQUIRE(list.size() == 0, Error, \"Transfer list is not supported\");\n+        }\n+      }\n+    }\n+  }\n+\n+  KJ_IF_SOME(o, other) {\n+    // TODO(message-port): Set up the external handler to support more types.\n+    jsg::Serializer ser(js);\n+    KJ_IF_SOME(d, data) {\n+      ser.write(js, d.getHandle(js));\n+    }\n+    auto released = ser.release();\n+    JSG_REQUIRE(released.sharedArrayBuffers.size() == 0, Error, \"SharedArrayBuffer is unsupported\");\n+\n+    capnp::MallocMessageBuilder builder;\n+    rpc::JsValue::Builder val = builder.initRoot<rpc::JsValue>();\n+    val.setV8Serialized(kj::mv(released.data));\n+    o->deliverMessage(js, val.asReader());\n+  }\n+}\n+\n+// Should close this port, the entangle port, and any known rpc clients.\n+void MessagePort::close() {\n+  state = Closed{};\n+  rpcClients.clear();\n+  KJ_IF_SOME(o, other) {\n+    auto closing = kj::mv(o);\n+    other = kj::none;\n+    closing->close();\n+  }\n+}\n+\n+// Start delivering messages on this port. Any messages that are\n+// buffered will be drained immediately.\n+void MessagePort::start(jsg::Lock& js) {\n+  KJ_SWITCH_ONEOF(state) {\n+    KJ_CASE_ONEOF(pending, Pending) {\n+      auto list = kj::mv(pending);\n+      state = Started{};\n+      for (auto& item: list) {\n+        ioContext.addTask(sendToRpc(capnp::clone(*item)));\n+        // Local delivery\n+        deliver(js, kj::mv(item), addRef());\n+      }\n+    }\n+    KJ_CASE_ONEOF(_, Started) {\n+      // Nothing to do in this case. We are already started!\n+    }\n+    KJ_CASE_ONEOF(_, Closed) {\n+      // Nothing to do in this case. Can't start after closing.\n+    }\n+  }\n+}\n+\n+kj::Maybe<jsg::JsValue> MessagePort::getOnMessage(jsg::Lock& js) {\n+  return onmessageValue.map(\n+      [&](jsg::JsRef<jsg::JsValue>& ref) -> jsg::JsValue { return ref.getHandle(js); });\n+}\n+\n+void MessagePort::setOnMessage(jsg::Lock& js, jsg::JsValue value) {\n+  if (!value.isObject() && !value.isFunction()) {\n+    onmessageValue = kj::none;\n+    // If we have no handlers and no onmessage ...\n+    if (getHandlerCount(\"message\"_kj) == 0 && onmessageValue == kj::none) {\n+      // ...Put the port back into a pending state where messages\n+      // will be enqueued until another listener is attached.\n+      state = Pending();\n+    }\n+  } else {\n+    onmessageValue = jsg::JsRef<jsg::JsValue>(js, value);\n+    start(js);\n+  }\n+}\n+\n+namespace {\n+// The jsrpc handler that receives messages posted from the remote and\n+// delivers them to the local port.\n+class JsMessagePortImpl final: public rpc::JsMessagePort::Server {\n+ public:\n+  JsMessagePortImpl(IoContext& ctx, jsg::Ref<MessagePort> port)\n+      : port(kj::mv(port)),\n+        weakIoContext(ctx.getWeakRef()) {}",
        "comment_created_at": "2025-04-22T04:15:18+00:00",
        "comment_author": "kentonv",
        "comment_body": "You also need to call `ctx.registerPendingEvent()` and hold onto it as long as this object is alive, since this is a source of events that could cause JavaScript to run in the IoContext.\r\n\r\nALTERNATIVELY, you might consider a \"pull\" approach instead of a \"push\" approach: Have JsMessagePortImpl actually implement a producer-consumer queue. When messages arrive, just put them in the queue. Have a method `Promise<Message> next()` that returns the next message. Don't hold a reference to the IoContext at all.\r\n\r\nThen, inside `MessagePort::start()`, you set up a loop that repeatedly calls `next()` and then delivers the event. This loop would use `ctx.awaitIo()` for each `next()` call.\r\n\r\nThe nice part about doing it this way is that you don't have to mess around with weak references and pending events. `awaitIo()` just does the right thing. You also don't need any complexity to wait for `start()` to be called before delivering events. The code ends up looking more like the WebSocket implementation, which I think fits pretty well here.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2150619805",
    "pr_number": 4347,
    "pr_file": "src/workerd/server/workerd-api.c++",
    "created_at": "2025-06-16T18:36:11+00:00",
    "commented_code": "return kj::none;\n   }\n \n-  {\n-    kj::Thread([&]() {\n-      kj::String url =\n-          kj::str(\"https://pyodide-capnp-bin.edgeworker.net/pyodide_\", version, \".capnp.bin\");\n-      KJ_LOG(INFO, \"Loading Pyodide bundle from internet\", url);\n-      kj::AsyncIoContext io = kj::setupAsyncIo();\n-      kj::HttpHeaderTable table;\n+  kj::String url =\n+      kj::str(\"https://pyodide-capnp-bin.edgeworker.net/pyodide_\", version, \".capnp.bin\");\n+  KJ_LOG(INFO, \"Loading Pyodide bundle from internet\", url);\n+  kj::HttpHeaderTable table;\n \n-      kj::TlsContext::Options options;\n-      options.useSystemTrustStore = true;\n+  kj::TlsContext::Options options;\n+  options.useSystemTrustStore = true;\n \n-      kj::Own<kj::TlsContext> tls = kj::heap<kj::TlsContext>(kj::mv(options));\n-      auto& network = io.provider->getNetwork();\n-      auto tlsNetwork = tls->wrapNetwork(network);\n-      auto& timer = io.provider->getTimer();\n+  kj::Own<kj::TlsContext> tls = kj::heap<kj::TlsContext>(kj::mv(options));\n+  auto& network = io.provider->getNetwork();\n+  auto tlsNetwork = tls->wrapNetwork(network);\n+  auto& timer = io.provider->getTimer();\n \n-      auto client = kj::newHttpClient(timer, table, network, *tlsNetwork);\n+  auto client = kj::newHttpClient(timer, table, network, *tlsNetwork);\n \n-      kj::HttpHeaders headers(table);\n+  kj::HttpHeaders headers(table);\n \n-      auto req = client->request(kj::HttpMethod::GET, url.asPtr(), headers);\n+  auto req = client->request(kj::HttpMethod::GET, url.asPtr(), headers);\n \n-      auto res = req.response.wait(io.waitScope);\n-      KJ_ASSERT(res.statusCode == 200,\n-          kj::str(\n-              \"Request for Pyodide bundle at \", url, \" failed with HTTP status \", res.statusCode));\n-      auto body = res.body->readAllBytes().wait(io.waitScope);\n+  auto res = req.response.wait(io.waitScope);",
    "repo_full_name": "cloudflare/workerd",
    "discussion_comments": [
      {
        "comment_id": "2150619805",
        "repo_full_name": "cloudflare/workerd",
        "pr_number": 4347,
        "pr_file": "src/workerd/server/workerd-api.c++",
        "discussion_id": "2150619805",
        "commented_code": "@@ -295,39 +295,33 @@ kj::Maybe<jsg::Bundle::Reader> fetchPyodideBundle(\n     return kj::none;\n   }\n \n-  {\n-    kj::Thread([&]() {\n-      kj::String url =\n-          kj::str(\"https://pyodide-capnp-bin.edgeworker.net/pyodide_\", version, \".capnp.bin\");\n-      KJ_LOG(INFO, \"Loading Pyodide bundle from internet\", url);\n-      kj::AsyncIoContext io = kj::setupAsyncIo();\n-      kj::HttpHeaderTable table;\n+  kj::String url =\n+      kj::str(\"https://pyodide-capnp-bin.edgeworker.net/pyodide_\", version, \".capnp.bin\");\n+  KJ_LOG(INFO, \"Loading Pyodide bundle from internet\", url);\n+  kj::HttpHeaderTable table;\n \n-      kj::TlsContext::Options options;\n-      options.useSystemTrustStore = true;\n+  kj::TlsContext::Options options;\n+  options.useSystemTrustStore = true;\n \n-      kj::Own<kj::TlsContext> tls = kj::heap<kj::TlsContext>(kj::mv(options));\n-      auto& network = io.provider->getNetwork();\n-      auto tlsNetwork = tls->wrapNetwork(network);\n-      auto& timer = io.provider->getTimer();\n+  kj::Own<kj::TlsContext> tls = kj::heap<kj::TlsContext>(kj::mv(options));\n+  auto& network = io.provider->getNetwork();\n+  auto tlsNetwork = tls->wrapNetwork(network);\n+  auto& timer = io.provider->getTimer();\n \n-      auto client = kj::newHttpClient(timer, table, network, *tlsNetwork);\n+  auto client = kj::newHttpClient(timer, table, network, *tlsNetwork);\n \n-      kj::HttpHeaders headers(table);\n+  kj::HttpHeaders headers(table);\n \n-      auto req = client->request(kj::HttpMethod::GET, url.asPtr(), headers);\n+  auto req = client->request(kj::HttpMethod::GET, url.asPtr(), headers);\n \n-      auto res = req.response.wait(io.waitScope);\n-      KJ_ASSERT(res.statusCode == 200,\n-          kj::str(\n-              \"Request for Pyodide bundle at \", url, \" failed with HTTP status \", res.statusCode));\n-      auto body = res.body->readAllBytes().wait(io.waitScope);\n+  auto res = req.response.wait(io.waitScope);",
        "comment_created_at": "2025-06-16T18:36:11+00:00",
        "comment_author": "danlapid",
        "comment_body": "Once you convert this function to a promise you won't need the waitScope, you could just await.",
        "pr_file_module": null
      },
      {
        "comment_id": "2150641422",
        "repo_full_name": "cloudflare/workerd",
        "pr_number": 4347,
        "pr_file": "src/workerd/server/workerd-api.c++",
        "discussion_id": "2150619805",
        "commented_code": "@@ -295,39 +295,33 @@ kj::Maybe<jsg::Bundle::Reader> fetchPyodideBundle(\n     return kj::none;\n   }\n \n-  {\n-    kj::Thread([&]() {\n-      kj::String url =\n-          kj::str(\"https://pyodide-capnp-bin.edgeworker.net/pyodide_\", version, \".capnp.bin\");\n-      KJ_LOG(INFO, \"Loading Pyodide bundle from internet\", url);\n-      kj::AsyncIoContext io = kj::setupAsyncIo();\n-      kj::HttpHeaderTable table;\n+  kj::String url =\n+      kj::str(\"https://pyodide-capnp-bin.edgeworker.net/pyodide_\", version, \".capnp.bin\");\n+  KJ_LOG(INFO, \"Loading Pyodide bundle from internet\", url);\n+  kj::HttpHeaderTable table;\n \n-      kj::TlsContext::Options options;\n-      options.useSystemTrustStore = true;\n+  kj::TlsContext::Options options;\n+  options.useSystemTrustStore = true;\n \n-      kj::Own<kj::TlsContext> tls = kj::heap<kj::TlsContext>(kj::mv(options));\n-      auto& network = io.provider->getNetwork();\n-      auto tlsNetwork = tls->wrapNetwork(network);\n-      auto& timer = io.provider->getTimer();\n+  kj::Own<kj::TlsContext> tls = kj::heap<kj::TlsContext>(kj::mv(options));\n+  auto& network = io.provider->getNetwork();\n+  auto tlsNetwork = tls->wrapNetwork(network);\n+  auto& timer = io.provider->getTimer();\n \n-      auto client = kj::newHttpClient(timer, table, network, *tlsNetwork);\n+  auto client = kj::newHttpClient(timer, table, network, *tlsNetwork);\n \n-      kj::HttpHeaders headers(table);\n+  kj::HttpHeaders headers(table);\n \n-      auto req = client->request(kj::HttpMethod::GET, url.asPtr(), headers);\n+  auto req = client->request(kj::HttpMethod::GET, url.asPtr(), headers);\n \n-      auto res = req.response.wait(io.waitScope);\n-      KJ_ASSERT(res.statusCode == 200,\n-          kj::str(\n-              \"Request for Pyodide bundle at \", url, \" failed with HTTP status \", res.statusCode));\n-      auto body = res.body->readAllBytes().wait(io.waitScope);\n+  auto res = req.response.wait(io.waitScope);",
        "comment_created_at": "2025-06-16T18:48:12+00:00",
        "comment_author": "kentonv",
        "comment_body": "Oh wow, just looked closer at this. This whole hack needs to be refactored -- we shouldn't be blocking on I/O in a synchronous function, either using a thread or using `.wait()`.\r\n\r\nI guess it happens to work today only because workerd constructs all `WorkerdApi` objects at startup anyway, so this only blocks startup. But that won't be true forever. I'm working on an API right now for dynamically loading isolates, which will need to construct new `WorkerdApi` instances at runtime.\r\n\r\nI think that Python dependencies need to be fetched at a higher level, and then passed down into `WorkerdApi`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2211850018",
    "pr_number": 4371,
    "pr_file": "src/workerd/api/container.c++",
    "created_at": "2025-07-16T23:54:47+00:00",
    "commented_code": "}\n \n   IoContext::current().addTask(req.sendIgnoringResult());\n-\n   running = true;\n+  monitorOnBackgroundIfNeeded();\n+}\n+\n+void Container::monitorOnBackgroundIfNeeded() {\n+  if (monitoring || !running) {\n+    return;\n+  }\n+  monitoring = true;\n+\n+  if (monitorKjPromise == kj::none) {\n+    monitorKjPromise = rpcClient->monitorRequest(capnp::MessageSize{4, 0})\n+                           .send()\n+                           .then([](auto&& results) -> uint8_t {\n+      return results.getExitCode();\n+    }).fork();\n+  }\n+\n+  auto req = KJ_ASSERT_NONNULL(monitorKjPromise)\n+                 .addBranch()\n+                 .then([](uint8_t exitCode) {}, [this, self = JSG_THIS](kj::Exception&& error) {\n+    // Check if there is an existing monitor request made by the user.\n+    // If not, we can abort the current context and error.\n+    if (!monitoringExplicitly) {\n+      IoContext::current().abort(kj::mv(error));",
    "repo_full_name": "cloudflare/workerd",
    "discussion_comments": [
      {
        "comment_id": "2211850018",
        "repo_full_name": "cloudflare/workerd",
        "pr_number": 4371,
        "pr_file": "src/workerd/api/container.c++",
        "discussion_id": "2211850018",
        "commented_code": "@@ -48,18 +48,64 @@ void Container::start(jsg::Lock& js, jsg::Optional<StartupOptions> maybeOptions)\n   }\n \n   IoContext::current().addTask(req.sendIgnoringResult());\n-\n   running = true;\n+  monitorOnBackgroundIfNeeded();\n+}\n+\n+void Container::monitorOnBackgroundIfNeeded() {\n+  if (monitoring || !running) {\n+    return;\n+  }\n+  monitoring = true;\n+\n+  if (monitorKjPromise == kj::none) {\n+    monitorKjPromise = rpcClient->monitorRequest(capnp::MessageSize{4, 0})\n+                           .send()\n+                           .then([](auto&& results) -> uint8_t {\n+      return results.getExitCode();\n+    }).fork();\n+  }\n+\n+  auto req = KJ_ASSERT_NONNULL(monitorKjPromise)\n+                 .addBranch()\n+                 .then([](uint8_t exitCode) {}, [this, self = JSG_THIS](kj::Exception&& error) {\n+    // Check if there is an existing monitor request made by the user.\n+    // If not, we can abort the current context and error.\n+    if (!monitoringExplicitly) {\n+      IoContext::current().abort(kj::mv(error));",
        "comment_created_at": "2025-07-16T23:54:47+00:00",
        "comment_author": "kentonv",
        "comment_body": "Since this is a KJ promise continuation, when it runs, the isolate lock will not be held, and there will be no current `IoContext`, so this throws an exception.\r\n\r\nYou need to capture `IoContext::current().getWeakRef()` instead, and use that to call `abort()` (which can be called without holding the lock). I would also recommend putting the `monitoringExplicitly` flag into a separate refcounted object so that you aren't reaching into a JSG object here -- that way you don't need to capture the `self` reference, which isn't safe to access without the lock held. (Note that the separate refcounted flag object would need to be held by the `Container` using an `IoOwn`.)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2211866813",
    "pr_number": 4371,
    "pr_file": "src/workerd/api/container.c++",
    "created_at": "2025-07-17T00:10:58+00:00",
    "commented_code": "}\n \n   IoContext::current().addTask(req.sendIgnoringResult());\n-\n   running = true;\n+  monitorOnBackgroundIfNeeded();\n+}\n+\n+void Container::monitorOnBackgroundIfNeeded() {\n+  if (monitoring || !running) {\n+    return;\n+  }\n+  monitoring = true;\n+\n+  if (monitorKjPromise == kj::none) {\n+    monitorKjPromise = rpcClient->monitorRequest(capnp::MessageSize{4, 0})\n+                           .send()\n+                           .then([](auto&& results) -> uint8_t {\n+      return results.getExitCode();\n+    }).fork();\n+  }\n+\n+  auto req = KJ_ASSERT_NONNULL(monitorKjPromise)\n+                 .addBranch()\n+                 .then([](uint8_t exitCode) {}, [this, self = JSG_THIS](kj::Exception&& error) {\n+    // Check if there is an existing monitor request made by the user.\n+    // If not, we can abort the current context and error.\n+    if (!monitoringExplicitly) {\n+      IoContext::current().abort(kj::mv(error));\n+    }\n+  }).attach(kj::defer([this, self = JSG_THIS]() {\n+    running = false;",
    "repo_full_name": "cloudflare/workerd",
    "discussion_comments": [
      {
        "comment_id": "2211866813",
        "repo_full_name": "cloudflare/workerd",
        "pr_number": 4371,
        "pr_file": "src/workerd/api/container.c++",
        "discussion_id": "2211866813",
        "commented_code": "@@ -48,18 +48,64 @@ void Container::start(jsg::Lock& js, jsg::Optional<StartupOptions> maybeOptions)\n   }\n \n   IoContext::current().addTask(req.sendIgnoringResult());\n-\n   running = true;\n+  monitorOnBackgroundIfNeeded();\n+}\n+\n+void Container::monitorOnBackgroundIfNeeded() {\n+  if (monitoring || !running) {\n+    return;\n+  }\n+  monitoring = true;\n+\n+  if (monitorKjPromise == kj::none) {\n+    monitorKjPromise = rpcClient->monitorRequest(capnp::MessageSize{4, 0})\n+                           .send()\n+                           .then([](auto&& results) -> uint8_t {\n+      return results.getExitCode();\n+    }).fork();\n+  }\n+\n+  auto req = KJ_ASSERT_NONNULL(monitorKjPromise)\n+                 .addBranch()\n+                 .then([](uint8_t exitCode) {}, [this, self = JSG_THIS](kj::Exception&& error) {\n+    // Check if there is an existing monitor request made by the user.\n+    // If not, we can abort the current context and error.\n+    if (!monitoringExplicitly) {\n+      IoContext::current().abort(kj::mv(error));\n+    }\n+  }).attach(kj::defer([this, self = JSG_THIS]() {\n+    running = false;",
        "comment_created_at": "2025-07-17T00:10:58+00:00",
        "comment_author": "kentonv",
        "comment_body": "Here, too, you don't have the isolate lock, so accessing the JSG object is unsafe. Perhaps these flags can also be moved to the separate refcounted object.\r\n\r\nOr, alternatively, perhaps updating these flags can be done in a continuation of the JSG promise returned by `monitor()`, instead of here. Note that if `monitor()` wasn't called, then we will have aborted the `IoContext` anyway, in which case updating these flags is not important.",
        "pr_file_module": null
      }
    ]
  }
]