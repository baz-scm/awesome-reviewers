[
  {
    "discussion_id": "2273224131",
    "pr_number": 10245,
    "pr_file": "packages/miniflare/src/shared/external-service.ts",
    "created_at": "2025-08-13T12:10:09+00:00",
    "commented_code": "const klass = createProxyPrototypeClass(DurableObject, (key) => {\n \t\t\t\t\t\t\t\t\tconst message = ${\n \t\t\t\t\t\t\t\t\t\tisProxyEnabled\n-\t\t\t\t\t\t\t\t\t\t\t? `\\`Cannot access \"\\${key}\" as Durable Object RPC is not yet supported between multiple dev sessions.\\``\n+\t\t\t\t\t\t\t\t\t\t\t? `\\`Cannot access \"\\${className}#\\${key}\" as Durable Object RPC is not yet supported between multiple dev sessions.\\``",
    "repo_full_name": "cloudflare/workers-sdk",
    "discussion_comments": [
      {
        "comment_id": "2273224131",
        "repo_full_name": "cloudflare/workers-sdk",
        "pr_number": 10245,
        "pr_file": "packages/miniflare/src/shared/external-service.ts",
        "discussion_id": "2273224131",
        "commented_code": "@@ -212,7 +212,7 @@ export function createOutboundDoProxyService(\n                                 const klass = createProxyPrototypeClass(DurableObject, (key) => {\n \t\t\t\t\t\t\t\t\tconst message = ${\n \t\t\t\t\t\t\t\t\t\tisProxyEnabled\n-\t\t\t\t\t\t\t\t\t\t\t? `\\`Cannot access \"\\${key}\" as Durable Object RPC is not yet supported between multiple dev sessions.\\``\n+\t\t\t\t\t\t\t\t\t\t\t? `\\`Cannot access \"\\${className}#\\${key}\" as Durable Object RPC is not yet supported between multiple dev sessions.\\``",
        "comment_created_at": "2025-08-13T12:10:09+00:00",
        "comment_author": "edmundhung",
        "comment_body": "Minor improvement to include the class name in the error message.\r\ne.g. `Cannot access \"MyDurableObject#ping\" as Durable Object RPC is not yet supported between multiple dev sessions.`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2139763604",
    "pr_number": 9509,
    "pr_file": "packages/workers-shared/utils/configuration/parseStaticRouting.ts",
    "created_at": "2025-06-11T10:18:09+00:00",
    "commented_code": "+import { MAX_ROUTES_RULE_LENGTH, MAX_ROUTES_RULES } from \"./constants\";\n+import type { StaticRouting } from \"../types\";\n+\n+export function parseStaticRouting(input: string[]): {\n+\tparsed: StaticRouting;\n+\terrorMessage: string | undefined;\n+} {\n+\tif (input.length === 0) {\n+\t\tthrow new Error(\n+\t\t\t\"No `run_worker_first` rules were provided; must provide at least 1 rule.\"\n+\t\t);\n+\t}\n+\tif (input.length > MAX_ROUTES_RULES) {\n+\t\tthrow new Error(\n+\t\t\t`Too many rules were provided; ${input.length} rules provided exceeds max of ${MAX_ROUTES_RULES}`\n+\t\t);\n+\t}\n+\n+\tconst rawAssetWorkerRules = [];\n+\tconst assetWorkerRules = [];\n+\tconst userWorkerRules = [];\n+\tconst invalidRules = [];\n+\n+\tfor (const rule of input) {\n+\t\tif (rule.startsWith(\"!/\")) {\n+\t\t\tassetWorkerRules.push(rule.slice(1)); // Remove leading !\n+\t\t\trawAssetWorkerRules.push(rule);\n+\t\t} else if (rule.startsWith(\"/\")) {\n+\t\t\tuserWorkerRules.push(rule);\n+\t\t} else {\n+\t\t\tif (rule.startsWith(\"!\")) {\n+\t\t\t\tinvalidRules.push(`'${rule}': negative rules must start with '!/'`);\n+\t\t\t} else {\n+\t\t\t\tinvalidRules.push(`'${rule}': rules must start with '/' or '!/'`);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif (assetWorkerRules.length > 0 && userWorkerRules.length === 0) {\n+\t\tthrow new Error(\n+\t\t\t`Only negative rules were provided; must provide at least 1 non-negative rule`\n+\t\t);\n+\t}\n+\n+\tconst invalidAssetWorkerRules =\n+\t\tvalidateStaticRoutingRules(rawAssetWorkerRules);\n+\tconst invalidUserWorkerRules = validateStaticRoutingRules(userWorkerRules);\n+\n+\tconst errorMessage = formatInvalidRoutes([\n+\t\t...invalidRules,\n+\t\t...invalidUserWorkerRules,\n+\t\t...invalidAssetWorkerRules,\n+\t]);\n+\treturn {\n+\t\tparsed: { asset_worker: assetWorkerRules, user_worker: userWorkerRules },\n+\t\terrorMessage,\n+\t};\n+}\n+\n+function validateStaticRoutingRules(rules: string[]): string[] {\n+\tconst invalid: string[] = [];\n+\tconst seen = new Set<string>();\n+\tfor (const rule of rules) {\n+\t\tif (rule.length > MAX_ROUTES_RULE_LENGTH) {\n+\t\t\tinvalid.push(\n+\t\t\t\t`'${rule}': all rules must be less than ${MAX_ROUTES_RULE_LENGTH} characters in length`\n+\t\t\t);\n+\t\t}\n+\t\tif (rule.endsWith(\"/*\")) {\n+\t\t\t// Check for redundant rules due to a glob\n+\t\t\tfor (const otherRule of rules) {\n+\t\t\t\tif (otherRule !== rule && otherRule.startsWith(rule.slice(0, -1))) {\n+\t\t\t\t\tinvalid.push(`'${rule}': rule '${otherRule}' makes it redundant`);",
    "repo_full_name": "cloudflare/workers-sdk",
    "discussion_comments": [
      {
        "comment_id": "2139763604",
        "repo_full_name": "cloudflare/workers-sdk",
        "pr_number": 9509,
        "pr_file": "packages/workers-shared/utils/configuration/parseStaticRouting.ts",
        "discussion_id": "2139763604",
        "commented_code": "@@ -0,0 +1,90 @@\n+import { MAX_ROUTES_RULE_LENGTH, MAX_ROUTES_RULES } from \"./constants\";\n+import type { StaticRouting } from \"../types\";\n+\n+export function parseStaticRouting(input: string[]): {\n+\tparsed: StaticRouting;\n+\terrorMessage: string | undefined;\n+} {\n+\tif (input.length === 0) {\n+\t\tthrow new Error(\n+\t\t\t\"No `run_worker_first` rules were provided; must provide at least 1 rule.\"\n+\t\t);\n+\t}\n+\tif (input.length > MAX_ROUTES_RULES) {\n+\t\tthrow new Error(\n+\t\t\t`Too many rules were provided; ${input.length} rules provided exceeds max of ${MAX_ROUTES_RULES}`\n+\t\t);\n+\t}\n+\n+\tconst rawAssetWorkerRules = [];\n+\tconst assetWorkerRules = [];\n+\tconst userWorkerRules = [];\n+\tconst invalidRules = [];\n+\n+\tfor (const rule of input) {\n+\t\tif (rule.startsWith(\"!/\")) {\n+\t\t\tassetWorkerRules.push(rule.slice(1)); // Remove leading !\n+\t\t\trawAssetWorkerRules.push(rule);\n+\t\t} else if (rule.startsWith(\"/\")) {\n+\t\t\tuserWorkerRules.push(rule);\n+\t\t} else {\n+\t\t\tif (rule.startsWith(\"!\")) {\n+\t\t\t\tinvalidRules.push(`'${rule}': negative rules must start with '!/'`);\n+\t\t\t} else {\n+\t\t\t\tinvalidRules.push(`'${rule}': rules must start with '/' or '!/'`);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif (assetWorkerRules.length > 0 && userWorkerRules.length === 0) {\n+\t\tthrow new Error(\n+\t\t\t`Only negative rules were provided; must provide at least 1 non-negative rule`\n+\t\t);\n+\t}\n+\n+\tconst invalidAssetWorkerRules =\n+\t\tvalidateStaticRoutingRules(rawAssetWorkerRules);\n+\tconst invalidUserWorkerRules = validateStaticRoutingRules(userWorkerRules);\n+\n+\tconst errorMessage = formatInvalidRoutes([\n+\t\t...invalidRules,\n+\t\t...invalidUserWorkerRules,\n+\t\t...invalidAssetWorkerRules,\n+\t]);\n+\treturn {\n+\t\tparsed: { asset_worker: assetWorkerRules, user_worker: userWorkerRules },\n+\t\terrorMessage,\n+\t};\n+}\n+\n+function validateStaticRoutingRules(rules: string[]): string[] {\n+\tconst invalid: string[] = [];\n+\tconst seen = new Set<string>();\n+\tfor (const rule of rules) {\n+\t\tif (rule.length > MAX_ROUTES_RULE_LENGTH) {\n+\t\t\tinvalid.push(\n+\t\t\t\t`'${rule}': all rules must be less than ${MAX_ROUTES_RULE_LENGTH} characters in length`\n+\t\t\t);\n+\t\t}\n+\t\tif (rule.endsWith(\"/*\")) {\n+\t\t\t// Check for redundant rules due to a glob\n+\t\t\tfor (const otherRule of rules) {\n+\t\t\t\tif (otherRule !== rule && otherRule.startsWith(rule.slice(0, -1))) {\n+\t\t\t\t\tinvalid.push(`'${rule}': rule '${otherRule}' makes it redundant`);",
        "comment_created_at": "2025-06-11T10:18:09+00:00",
        "comment_author": "petebacondarwin",
        "comment_body": "I think this error is backwards?\r\n\r\n```suggestion\r\n\t\t\t\t\tinvalid.push(`'${otherRule}': rule '${rule}' makes it redundant`);\r\n```\r\n\r\nBut doing it this way might result in duplicates. So perhaps just add the redundant rules to a set and then generate the error messages from the set at the end?",
        "pr_file_module": null
      },
      {
        "comment_id": "2140143792",
        "repo_full_name": "cloudflare/workers-sdk",
        "pr_number": 9509,
        "pr_file": "packages/workers-shared/utils/configuration/parseStaticRouting.ts",
        "discussion_id": "2139763604",
        "commented_code": "@@ -0,0 +1,90 @@\n+import { MAX_ROUTES_RULE_LENGTH, MAX_ROUTES_RULES } from \"./constants\";\n+import type { StaticRouting } from \"../types\";\n+\n+export function parseStaticRouting(input: string[]): {\n+\tparsed: StaticRouting;\n+\terrorMessage: string | undefined;\n+} {\n+\tif (input.length === 0) {\n+\t\tthrow new Error(\n+\t\t\t\"No `run_worker_first` rules were provided; must provide at least 1 rule.\"\n+\t\t);\n+\t}\n+\tif (input.length > MAX_ROUTES_RULES) {\n+\t\tthrow new Error(\n+\t\t\t`Too many rules were provided; ${input.length} rules provided exceeds max of ${MAX_ROUTES_RULES}`\n+\t\t);\n+\t}\n+\n+\tconst rawAssetWorkerRules = [];\n+\tconst assetWorkerRules = [];\n+\tconst userWorkerRules = [];\n+\tconst invalidRules = [];\n+\n+\tfor (const rule of input) {\n+\t\tif (rule.startsWith(\"!/\")) {\n+\t\t\tassetWorkerRules.push(rule.slice(1)); // Remove leading !\n+\t\t\trawAssetWorkerRules.push(rule);\n+\t\t} else if (rule.startsWith(\"/\")) {\n+\t\t\tuserWorkerRules.push(rule);\n+\t\t} else {\n+\t\t\tif (rule.startsWith(\"!\")) {\n+\t\t\t\tinvalidRules.push(`'${rule}': negative rules must start with '!/'`);\n+\t\t\t} else {\n+\t\t\t\tinvalidRules.push(`'${rule}': rules must start with '/' or '!/'`);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif (assetWorkerRules.length > 0 && userWorkerRules.length === 0) {\n+\t\tthrow new Error(\n+\t\t\t`Only negative rules were provided; must provide at least 1 non-negative rule`\n+\t\t);\n+\t}\n+\n+\tconst invalidAssetWorkerRules =\n+\t\tvalidateStaticRoutingRules(rawAssetWorkerRules);\n+\tconst invalidUserWorkerRules = validateStaticRoutingRules(userWorkerRules);\n+\n+\tconst errorMessage = formatInvalidRoutes([\n+\t\t...invalidRules,\n+\t\t...invalidUserWorkerRules,\n+\t\t...invalidAssetWorkerRules,\n+\t]);\n+\treturn {\n+\t\tparsed: { asset_worker: assetWorkerRules, user_worker: userWorkerRules },\n+\t\terrorMessage,\n+\t};\n+}\n+\n+function validateStaticRoutingRules(rules: string[]): string[] {\n+\tconst invalid: string[] = [];\n+\tconst seen = new Set<string>();\n+\tfor (const rule of rules) {\n+\t\tif (rule.length > MAX_ROUTES_RULE_LENGTH) {\n+\t\t\tinvalid.push(\n+\t\t\t\t`'${rule}': all rules must be less than ${MAX_ROUTES_RULE_LENGTH} characters in length`\n+\t\t\t);\n+\t\t}\n+\t\tif (rule.endsWith(\"/*\")) {\n+\t\t\t// Check for redundant rules due to a glob\n+\t\t\tfor (const otherRule of rules) {\n+\t\t\t\tif (otherRule !== rule && otherRule.startsWith(rule.slice(0, -1))) {\n+\t\t\t\t\tinvalid.push(`'${rule}': rule '${otherRule}' makes it redundant`);",
        "comment_created_at": "2025-06-11T13:31:18+00:00",
        "comment_author": "emily-shen",
        "comment_body": "\ud83e\udd26 ive done this in both PRs.\r\n\r\ni don't think the duplicates are a huge deal, if multiple rules make another rule redundant, or vice versa, that's useful to surface right? \r\nalso would prefer to keep the code as close to EWC as possible.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1705959062",
    "pr_number": 6374,
    "pr_file": "packages/wrangler/src/experimental-assets.ts",
    "created_at": "2024-08-06T18:34:20+00:00",
    "commented_code": "+import assert from \"node:assert\";\n import { existsSync } from \"node:fs\";\n-import path from \"node:path\";\n-import { UserError } from \"./errors\";\n+import { readdir, readFile, stat } from \"node:fs/promises\";\n+import * as path from \"node:path\";\n+import chalk from \"chalk\";\n+import { getType } from \"mime\";\n+import PQueue from \"p-queue\";\n+import prettyBytes from \"pretty-bytes\";\n+import { fetchResult } from \"./cfetch\";\n+import { formatTime } from \"./deploy/deploy\";\n+import { FatalError, UserError } from \"./errors\";\n+import { logger, LOGGER_LEVELS } from \"./logger\";\n+import { hashFile } from \"./pages/hash\";\n+import { isJwtExpired } from \"./pages/upload\";\n+import { APIError } from \"./parse\";\n+import { urlSafe } from \"./sites\";\n import type { Config } from \"./config\";\n \n+export type AssetManifest = { [path: string]: { hash: string; size: number } };\n+\n+type InitializeAssetsResponse = {\n+\t// string of file hashes per bucket\n+\tbuckets: string[][];\n+\tjwt: string;\n+};\n+\n+export type UploadPayloadFile = {\n+\tname: string;\n+\thash: string;\n+\tcontents: string;\n+\tmetadata: {\n+\t\tcontentType: string;\n+\t};\n+};\n+\n+type UploadResponse = {\n+\tjwt?: string;\n+};\n+\n+// constants same as Pages for now\n+const BULK_UPLOAD_CONCURRENCY = 3;\n+const MAX_ASSET_COUNT = 20_000;\n+const MAX_ASSET_SIZE = 25 * 1024 * 1024;\n+const MAX_UPLOAD_ATTEMPTS = 5;\n+const MAX_UPLOAD_GATEWAY_ERRORS = 5;\n+\n+export const syncExperimentalAssets = async (\n+\taccountId: string | undefined,\n+\tscriptName: string,\n+\tassetDirectory: string | undefined,\n+\tdryRun: boolean | undefined\n+): Promise<string | undefined> => {\n+\tif (assetDirectory === undefined) {\n+\t\treturn;\n+\t}\n+\tif (dryRun) {\n+\t\tlogger.log(\"(Note: doing a dry run, not uploading or deleting anything.)\");\n+\t\treturn;\n+\t}\n+\tassert(accountId, \"Missing accountId\");\n+\n+\t// 1. generate asset manifest\n+\tlogger.info(\"\ud83c\udf00 Building list of assets...\");\n+\tconst manifest = await walk(assetDirectory, {});\n+\n+\t// 2. fetch buckets w/ hashes\n+\tlogger.info(\"\ud83c\udf00 Uploading asset manifest...\");\n+\tconst initializeAssetsResponse = await fetchResult<InitializeAssetsResponse>(\n+\t\t`/accounts/${accountId}/workers/scripts/${scriptName}/assets-upload-session`,\n+\t\t{\n+\t\t\theaders: { \"Content-Type\": \"application/json\" },\n+\t\t\tmethod: \"POST\",\n+\t\t\tbody: JSON.stringify({ manifest: manifest }),\n+\t\t}\n+\t);\n+\n+\t// if nothing to upload, return\n+\tif (initializeAssetsResponse.buckets.flat().length === 0) {\n+\t\tlogger.info(`\u2728 Success! (No files to upload)`);\n+\t\treturn initializeAssetsResponse.jwt;",
    "repo_full_name": "cloudflare/workers-sdk",
    "discussion_comments": [
      {
        "comment_id": "1705959062",
        "repo_full_name": "cloudflare/workers-sdk",
        "pr_number": 6374,
        "pr_file": "packages/wrangler/src/experimental-assets.ts",
        "discussion_id": "1705959062",
        "commented_code": "@@ -1,8 +1,301 @@\n+import assert from \"node:assert\";\n import { existsSync } from \"node:fs\";\n-import path from \"node:path\";\n-import { UserError } from \"./errors\";\n+import { readdir, readFile, stat } from \"node:fs/promises\";\n+import * as path from \"node:path\";\n+import chalk from \"chalk\";\n+import { getType } from \"mime\";\n+import PQueue from \"p-queue\";\n+import prettyBytes from \"pretty-bytes\";\n+import { fetchResult } from \"./cfetch\";\n+import { formatTime } from \"./deploy/deploy\";\n+import { FatalError, UserError } from \"./errors\";\n+import { logger, LOGGER_LEVELS } from \"./logger\";\n+import { hashFile } from \"./pages/hash\";\n+import { isJwtExpired } from \"./pages/upload\";\n+import { APIError } from \"./parse\";\n+import { urlSafe } from \"./sites\";\n import type { Config } from \"./config\";\n \n+export type AssetManifest = { [path: string]: { hash: string; size: number } };\n+\n+type InitializeAssetsResponse = {\n+\t// string of file hashes per bucket\n+\tbuckets: string[][];\n+\tjwt: string;\n+};\n+\n+export type UploadPayloadFile = {\n+\tname: string;\n+\thash: string;\n+\tcontents: string;\n+\tmetadata: {\n+\t\tcontentType: string;\n+\t};\n+};\n+\n+type UploadResponse = {\n+\tjwt?: string;\n+};\n+\n+// constants same as Pages for now\n+const BULK_UPLOAD_CONCURRENCY = 3;\n+const MAX_ASSET_COUNT = 20_000;\n+const MAX_ASSET_SIZE = 25 * 1024 * 1024;\n+const MAX_UPLOAD_ATTEMPTS = 5;\n+const MAX_UPLOAD_GATEWAY_ERRORS = 5;\n+\n+export const syncExperimentalAssets = async (\n+\taccountId: string | undefined,\n+\tscriptName: string,\n+\tassetDirectory: string | undefined,\n+\tdryRun: boolean | undefined\n+): Promise<string | undefined> => {\n+\tif (assetDirectory === undefined) {\n+\t\treturn;\n+\t}\n+\tif (dryRun) {\n+\t\tlogger.log(\"(Note: doing a dry run, not uploading or deleting anything.)\");\n+\t\treturn;\n+\t}\n+\tassert(accountId, \"Missing accountId\");\n+\n+\t// 1. generate asset manifest\n+\tlogger.info(\"\ud83c\udf00 Building list of assets...\");\n+\tconst manifest = await walk(assetDirectory, {});\n+\n+\t// 2. fetch buckets w/ hashes\n+\tlogger.info(\"\ud83c\udf00 Uploading asset manifest...\");\n+\tconst initializeAssetsResponse = await fetchResult<InitializeAssetsResponse>(\n+\t\t`/accounts/${accountId}/workers/scripts/${scriptName}/assets-upload-session`,\n+\t\t{\n+\t\t\theaders: { \"Content-Type\": \"application/json\" },\n+\t\t\tmethod: \"POST\",\n+\t\t\tbody: JSON.stringify({ manifest: manifest }),\n+\t\t}\n+\t);\n+\n+\t// if nothing to upload, return\n+\tif (initializeAssetsResponse.buckets.flat().length === 0) {\n+\t\tlogger.info(`\u2728 Success! (No files to upload)`);\n+\t\treturn initializeAssetsResponse.jwt;",
        "comment_created_at": "2024-08-06T18:34:20+00:00",
        "comment_author": "GregBrimble",
        "comment_body": "Should we verify that there is indeed a `jwt` here first though? Throw an error if not, rather than blindly proceeding without the assets bit.\r\n\r\n```suggestion\r\n    if (!!initializeAssetsResponse.jwt) {\r\n      throw new FatalError(\"Could not find assets information to attach to deployment. Please try again.\", 1);\r\n    }\r\n\t\treturn initializeAssetsResponse.jwt;\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1707336111",
        "repo_full_name": "cloudflare/workers-sdk",
        "pr_number": 6374,
        "pr_file": "packages/wrangler/src/experimental-assets.ts",
        "discussion_id": "1705959062",
        "commented_code": "@@ -1,8 +1,301 @@\n+import assert from \"node:assert\";\n import { existsSync } from \"node:fs\";\n-import path from \"node:path\";\n-import { UserError } from \"./errors\";\n+import { readdir, readFile, stat } from \"node:fs/promises\";\n+import * as path from \"node:path\";\n+import chalk from \"chalk\";\n+import { getType } from \"mime\";\n+import PQueue from \"p-queue\";\n+import prettyBytes from \"pretty-bytes\";\n+import { fetchResult } from \"./cfetch\";\n+import { formatTime } from \"./deploy/deploy\";\n+import { FatalError, UserError } from \"./errors\";\n+import { logger, LOGGER_LEVELS } from \"./logger\";\n+import { hashFile } from \"./pages/hash\";\n+import { isJwtExpired } from \"./pages/upload\";\n+import { APIError } from \"./parse\";\n+import { urlSafe } from \"./sites\";\n import type { Config } from \"./config\";\n \n+export type AssetManifest = { [path: string]: { hash: string; size: number } };\n+\n+type InitializeAssetsResponse = {\n+\t// string of file hashes per bucket\n+\tbuckets: string[][];\n+\tjwt: string;\n+};\n+\n+export type UploadPayloadFile = {\n+\tname: string;\n+\thash: string;\n+\tcontents: string;\n+\tmetadata: {\n+\t\tcontentType: string;\n+\t};\n+};\n+\n+type UploadResponse = {\n+\tjwt?: string;\n+};\n+\n+// constants same as Pages for now\n+const BULK_UPLOAD_CONCURRENCY = 3;\n+const MAX_ASSET_COUNT = 20_000;\n+const MAX_ASSET_SIZE = 25 * 1024 * 1024;\n+const MAX_UPLOAD_ATTEMPTS = 5;\n+const MAX_UPLOAD_GATEWAY_ERRORS = 5;\n+\n+export const syncExperimentalAssets = async (\n+\taccountId: string | undefined,\n+\tscriptName: string,\n+\tassetDirectory: string | undefined,\n+\tdryRun: boolean | undefined\n+): Promise<string | undefined> => {\n+\tif (assetDirectory === undefined) {\n+\t\treturn;\n+\t}\n+\tif (dryRun) {\n+\t\tlogger.log(\"(Note: doing a dry run, not uploading or deleting anything.)\");\n+\t\treturn;\n+\t}\n+\tassert(accountId, \"Missing accountId\");\n+\n+\t// 1. generate asset manifest\n+\tlogger.info(\"\ud83c\udf00 Building list of assets...\");\n+\tconst manifest = await walk(assetDirectory, {});\n+\n+\t// 2. fetch buckets w/ hashes\n+\tlogger.info(\"\ud83c\udf00 Uploading asset manifest...\");\n+\tconst initializeAssetsResponse = await fetchResult<InitializeAssetsResponse>(\n+\t\t`/accounts/${accountId}/workers/scripts/${scriptName}/assets-upload-session`,\n+\t\t{\n+\t\t\theaders: { \"Content-Type\": \"application/json\" },\n+\t\t\tmethod: \"POST\",\n+\t\t\tbody: JSON.stringify({ manifest: manifest }),\n+\t\t}\n+\t);\n+\n+\t// if nothing to upload, return\n+\tif (initializeAssetsResponse.buckets.flat().length === 0) {\n+\t\tlogger.info(`\u2728 Success! (No files to upload)`);\n+\t\treturn initializeAssetsResponse.jwt;",
        "comment_created_at": "2024-08-07T15:59:55+00:00",
        "comment_author": "CarmenPopoviciu",
        "comment_body": "I see this was addressed here: https://github.com/cloudflare/workers-sdk/pull/6374/files#diff-99329bc31323333c32855181ed3b738e17d682a5f16c825ec49be45d30e54082R79 but shouldn't that check be performed independent of whether `initializeAssetsResponse.buckets.flat().length === 0`?\r\n\r\nso basically\r\n```\r\nif (!initializeAssetsResponse.jwt) {\r\n\tthrow new FatalError(\r\n\t\t\"Could not find assets information to attach to deployment. Please try again.\",\r\n\t\t1\r\n\t);\r\n}\r\n\r\nif (initializeAssetsResponse.buckets.flat().length === 0) {\r\n\tlogger.info(`\u2728 Success! (No files to upload)`);\r\n\treturn initializeAssetsResponse.jwt;\r\n}\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1707665144",
        "repo_full_name": "cloudflare/workers-sdk",
        "pr_number": 6374,
        "pr_file": "packages/wrangler/src/experimental-assets.ts",
        "discussion_id": "1705959062",
        "commented_code": "@@ -1,8 +1,301 @@\n+import assert from \"node:assert\";\n import { existsSync } from \"node:fs\";\n-import path from \"node:path\";\n-import { UserError } from \"./errors\";\n+import { readdir, readFile, stat } from \"node:fs/promises\";\n+import * as path from \"node:path\";\n+import chalk from \"chalk\";\n+import { getType } from \"mime\";\n+import PQueue from \"p-queue\";\n+import prettyBytes from \"pretty-bytes\";\n+import { fetchResult } from \"./cfetch\";\n+import { formatTime } from \"./deploy/deploy\";\n+import { FatalError, UserError } from \"./errors\";\n+import { logger, LOGGER_LEVELS } from \"./logger\";\n+import { hashFile } from \"./pages/hash\";\n+import { isJwtExpired } from \"./pages/upload\";\n+import { APIError } from \"./parse\";\n+import { urlSafe } from \"./sites\";\n import type { Config } from \"./config\";\n \n+export type AssetManifest = { [path: string]: { hash: string; size: number } };\n+\n+type InitializeAssetsResponse = {\n+\t// string of file hashes per bucket\n+\tbuckets: string[][];\n+\tjwt: string;\n+};\n+\n+export type UploadPayloadFile = {\n+\tname: string;\n+\thash: string;\n+\tcontents: string;\n+\tmetadata: {\n+\t\tcontentType: string;\n+\t};\n+};\n+\n+type UploadResponse = {\n+\tjwt?: string;\n+};\n+\n+// constants same as Pages for now\n+const BULK_UPLOAD_CONCURRENCY = 3;\n+const MAX_ASSET_COUNT = 20_000;\n+const MAX_ASSET_SIZE = 25 * 1024 * 1024;\n+const MAX_UPLOAD_ATTEMPTS = 5;\n+const MAX_UPLOAD_GATEWAY_ERRORS = 5;\n+\n+export const syncExperimentalAssets = async (\n+\taccountId: string | undefined,\n+\tscriptName: string,\n+\tassetDirectory: string | undefined,\n+\tdryRun: boolean | undefined\n+): Promise<string | undefined> => {\n+\tif (assetDirectory === undefined) {\n+\t\treturn;\n+\t}\n+\tif (dryRun) {\n+\t\tlogger.log(\"(Note: doing a dry run, not uploading or deleting anything.)\");\n+\t\treturn;\n+\t}\n+\tassert(accountId, \"Missing accountId\");\n+\n+\t// 1. generate asset manifest\n+\tlogger.info(\"\ud83c\udf00 Building list of assets...\");\n+\tconst manifest = await walk(assetDirectory, {});\n+\n+\t// 2. fetch buckets w/ hashes\n+\tlogger.info(\"\ud83c\udf00 Uploading asset manifest...\");\n+\tconst initializeAssetsResponse = await fetchResult<InitializeAssetsResponse>(\n+\t\t`/accounts/${accountId}/workers/scripts/${scriptName}/assets-upload-session`,\n+\t\t{\n+\t\t\theaders: { \"Content-Type\": \"application/json\" },\n+\t\t\tmethod: \"POST\",\n+\t\t\tbody: JSON.stringify({ manifest: manifest }),\n+\t\t}\n+\t);\n+\n+\t// if nothing to upload, return\n+\tif (initializeAssetsResponse.buckets.flat().length === 0) {\n+\t\tlogger.info(`\u2728 Success! (No files to upload)`);\n+\t\treturn initializeAssetsResponse.jwt;",
        "comment_created_at": "2024-08-07T18:45:41+00:00",
        "comment_author": "emily-shen",
        "comment_body": "It doesn't make it that far down - it returns immediately.",
        "pr_file_module": null
      },
      {
        "comment_id": "1707776751",
        "repo_full_name": "cloudflare/workers-sdk",
        "pr_number": 6374,
        "pr_file": "packages/wrangler/src/experimental-assets.ts",
        "discussion_id": "1705959062",
        "commented_code": "@@ -1,8 +1,301 @@\n+import assert from \"node:assert\";\n import { existsSync } from \"node:fs\";\n-import path from \"node:path\";\n-import { UserError } from \"./errors\";\n+import { readdir, readFile, stat } from \"node:fs/promises\";\n+import * as path from \"node:path\";\n+import chalk from \"chalk\";\n+import { getType } from \"mime\";\n+import PQueue from \"p-queue\";\n+import prettyBytes from \"pretty-bytes\";\n+import { fetchResult } from \"./cfetch\";\n+import { formatTime } from \"./deploy/deploy\";\n+import { FatalError, UserError } from \"./errors\";\n+import { logger, LOGGER_LEVELS } from \"./logger\";\n+import { hashFile } from \"./pages/hash\";\n+import { isJwtExpired } from \"./pages/upload\";\n+import { APIError } from \"./parse\";\n+import { urlSafe } from \"./sites\";\n import type { Config } from \"./config\";\n \n+export type AssetManifest = { [path: string]: { hash: string; size: number } };\n+\n+type InitializeAssetsResponse = {\n+\t// string of file hashes per bucket\n+\tbuckets: string[][];\n+\tjwt: string;\n+};\n+\n+export type UploadPayloadFile = {\n+\tname: string;\n+\thash: string;\n+\tcontents: string;\n+\tmetadata: {\n+\t\tcontentType: string;\n+\t};\n+};\n+\n+type UploadResponse = {\n+\tjwt?: string;\n+};\n+\n+// constants same as Pages for now\n+const BULK_UPLOAD_CONCURRENCY = 3;\n+const MAX_ASSET_COUNT = 20_000;\n+const MAX_ASSET_SIZE = 25 * 1024 * 1024;\n+const MAX_UPLOAD_ATTEMPTS = 5;\n+const MAX_UPLOAD_GATEWAY_ERRORS = 5;\n+\n+export const syncExperimentalAssets = async (\n+\taccountId: string | undefined,\n+\tscriptName: string,\n+\tassetDirectory: string | undefined,\n+\tdryRun: boolean | undefined\n+): Promise<string | undefined> => {\n+\tif (assetDirectory === undefined) {\n+\t\treturn;\n+\t}\n+\tif (dryRun) {\n+\t\tlogger.log(\"(Note: doing a dry run, not uploading or deleting anything.)\");\n+\t\treturn;\n+\t}\n+\tassert(accountId, \"Missing accountId\");\n+\n+\t// 1. generate asset manifest\n+\tlogger.info(\"\ud83c\udf00 Building list of assets...\");\n+\tconst manifest = await walk(assetDirectory, {});\n+\n+\t// 2. fetch buckets w/ hashes\n+\tlogger.info(\"\ud83c\udf00 Uploading asset manifest...\");\n+\tconst initializeAssetsResponse = await fetchResult<InitializeAssetsResponse>(\n+\t\t`/accounts/${accountId}/workers/scripts/${scriptName}/assets-upload-session`,\n+\t\t{\n+\t\t\theaders: { \"Content-Type\": \"application/json\" },\n+\t\t\tmethod: \"POST\",\n+\t\t\tbody: JSON.stringify({ manifest: manifest }),\n+\t\t}\n+\t);\n+\n+\t// if nothing to upload, return\n+\tif (initializeAssetsResponse.buckets.flat().length === 0) {\n+\t\tlogger.info(`\u2728 Success! (No files to upload)`);\n+\t\treturn initializeAssetsResponse.jwt;",
        "comment_created_at": "2024-08-07T19:48:00+00:00",
        "comment_author": "GregBrimble",
        "comment_body": "yeah, I think you're good with this in it's current state. You do a similar check later on too :)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1705987597",
    "pr_number": 6374,
    "pr_file": "packages/wrangler/src/experimental-assets.ts",
    "created_at": "2024-08-06T19:03:31+00:00",
    "commented_code": "+import assert from \"node:assert\";\n import { existsSync } from \"node:fs\";\n-import path from \"node:path\";\n-import { UserError } from \"./errors\";\n+import { readdir, readFile, stat } from \"node:fs/promises\";\n+import * as path from \"node:path\";\n+import chalk from \"chalk\";\n+import { getType } from \"mime\";\n+import PQueue from \"p-queue\";\n+import prettyBytes from \"pretty-bytes\";\n+import { fetchResult } from \"./cfetch\";\n+import { formatTime } from \"./deploy/deploy\";\n+import { FatalError, UserError } from \"./errors\";\n+import { logger, LOGGER_LEVELS } from \"./logger\";\n+import { hashFile } from \"./pages/hash\";\n+import { isJwtExpired } from \"./pages/upload\";\n+import { APIError } from \"./parse\";\n+import { urlSafe } from \"./sites\";\n import type { Config } from \"./config\";\n \n+export type AssetManifest = { [path: string]: { hash: string; size: number } };\n+\n+type InitializeAssetsResponse = {\n+\t// string of file hashes per bucket\n+\tbuckets: string[][];\n+\tjwt: string;\n+};\n+\n+export type UploadPayloadFile = {\n+\tname: string;\n+\thash: string;\n+\tcontents: string;\n+\tmetadata: {\n+\t\tcontentType: string;\n+\t};\n+};\n+\n+type UploadResponse = {\n+\tjwt?: string;\n+};\n+\n+// constants same as Pages for now\n+const BULK_UPLOAD_CONCURRENCY = 3;\n+const MAX_ASSET_COUNT = 20_000;\n+const MAX_ASSET_SIZE = 25 * 1024 * 1024;\n+const MAX_UPLOAD_ATTEMPTS = 5;\n+const MAX_UPLOAD_GATEWAY_ERRORS = 5;\n+\n+export const syncExperimentalAssets = async (\n+\taccountId: string | undefined,\n+\tscriptName: string,\n+\tassetDirectory: string | undefined,\n+\tdryRun: boolean | undefined\n+): Promise<string | undefined> => {\n+\tif (assetDirectory === undefined) {\n+\t\treturn;\n+\t}\n+\tif (dryRun) {\n+\t\tlogger.log(\"(Note: doing a dry run, not uploading or deleting anything.)\");\n+\t\treturn;\n+\t}\n+\tassert(accountId, \"Missing accountId\");\n+\n+\t// 1. generate asset manifest\n+\tlogger.info(\"\ud83c\udf00 Building list of assets...\");\n+\tconst manifest = await walk(assetDirectory, {});\n+\n+\t// 2. fetch buckets w/ hashes\n+\tlogger.info(\"\ud83c\udf00 Uploading asset manifest...\");\n+\tconst initializeAssetsResponse = await fetchResult<InitializeAssetsResponse>(\n+\t\t`/accounts/${accountId}/workers/scripts/${scriptName}/assets-upload-session`,\n+\t\t{\n+\t\t\theaders: { \"Content-Type\": \"application/json\" },\n+\t\t\tmethod: \"POST\",\n+\t\t\tbody: JSON.stringify({ manifest: manifest }),\n+\t\t}\n+\t);\n+\n+\t// if nothing to upload, return\n+\tif (initializeAssetsResponse.buckets.flat().length === 0) {\n+\t\tlogger.info(`\u2728 Success! (No files to upload)`);\n+\t\treturn initializeAssetsResponse.jwt;\n+\t}\n+\n+\t// 3. fill buckets and upload assets\n+\tconst numberFilesToUpload = initializeAssetsResponse.buckets.flat().length;\n+\tlogger.info(`\ud83c\udf00 Uploading ${numberFilesToUpload} file(s)...`);\n+\n+\t// Create the buckets outside of doUpload so we can retry without losing track of potential duplicate files\n+\t// But don't add the actual content until uploading so we don't run out of memory\n+\tconst manifestLookup = Object.entries(manifest);\n+\tlet assetLogCount = 0;\n+\tconst bucketSkeletons = initializeAssetsResponse.buckets.map((bucket) => {\n+\t\treturn bucket.map((fileHash) => {\n+\t\t\tconst manifestEntryIndex = manifestLookup.findIndex(\n+\t\t\t\t(file) => file[1].hash === fileHash\n+\t\t\t);\n+\t\t\tif (manifestEntryIndex === -1) {\n+\t\t\t\tthrow new FatalError(\n+\t\t\t\t\t`A file was requested that does not appear to exist.`,\n+\t\t\t\t\t1\n+\t\t\t\t);\n+\t\t\t}\n+\t\t\tconst manifestEntry = manifestLookup.splice(manifestEntryIndex, 1)[0];\n+\t\t\t// just logging file uploads at the moment...\n+\t\t\t// unsure how to log deletion vs unchanged file ignored/if we want to log this\n+\t\t\tassetLogCount = logAssetUpload(`+ ${manifestEntry[0]}`, assetLogCount);\n+\t\t\treturn manifestEntry;\n+\t\t});\n+\t});\n+\n+\tconst queue = new PQueue({ concurrency: BULK_UPLOAD_CONCURRENCY });\n+\tlet attempts = 0;\n+\tconst start = Date.now();\n+\tlet bucketUploadCount = 0;\n+\tlet completionJwt = \"\";\n+\n+\tfor (const [bucketIndex, bucketSkeleton] of bucketSkeletons.entries()) {\n+\t\tattempts = 0;\n+\t\tlet gatewayErrors = 0;\n+\t\tconst doUpload = async (): Promise<UploadResponse> => {\n+\t\t\t// Populate the payload only when actually uploading (this is limited to 3 concurrent uploads at 50 MiB per bucket meaning we'd only load in a max of ~150 MiB)\n+\t\t\t// This is so we don't run out of memory trying to upload the files.\n+\t\t\tconst payload: UploadPayloadFile[] = await Promise.all(\n+\t\t\t\tbucketSkeleton.map(async (manifestEntry) => {\n+\t\t\t\t\tconst absFilePath = path.join(assetDirectory, manifestEntry[0]);\n+\t\t\t\t\treturn {\n+\t\t\t\t\t\tname: manifestEntry[0],\n+\t\t\t\t\t\thash: manifestEntry[1].hash,\n+\t\t\t\t\t\tcontents: (await readFile(absFilePath)).toString(\"base64\"),\n+\t\t\t\t\t\tmetadata: {\n+\t\t\t\t\t\t\tcontentType: getType(absFilePath) || \"application/octet-stream\",\n+\t\t\t\t\t\t},\n+\t\t\t\t\t};\n+\t\t\t\t})\n+\t\t\t);\n+\n+\t\t\ttry {\n+\t\t\t\tconst res = await fetchResult<UploadResponse>(\n+\t\t\t\t\t`/accounts/${accountId}/workers/assets/upload`,\n+\t\t\t\t\t{\n+\t\t\t\t\t\tmethod: \"POST\",\n+\t\t\t\t\t\theaders: {\n+\t\t\t\t\t\t\t\"Content-Type\": \"application/x-ndjson\",\n+\t\t\t\t\t\t\tAuthorization: `Bearer ${initializeAssetsResponse.jwt}`,\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tbody: payload.map((x) => JSON.stringify(x)).join(\"\n\"),\n+\t\t\t\t\t}\n+\t\t\t\t);\n+\t\t\t\tlogger.info(\n+\t\t\t\t\t`Uploaded bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}`\n+\t\t\t\t);\n+\t\t\t\treturn res;\n+\t\t\t} catch (e) {\n+\t\t\t\tif (attempts < MAX_UPLOAD_ATTEMPTS) {\n+\t\t\t\t\tlogger.info(\n+\t\t\t\t\t\tchalk.dim(\n+\t\t\t\t\t\t\t`Bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length} upload failed. Retrying...\n`,\n+\t\t\t\t\t\t\te\n+\t\t\t\t\t\t)\n+\t\t\t\t\t);\n+\t\t\t\t\t// Exponential backoff, 1 second first time, then 2 second, then 4 second etc.\n+\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, attempts) * 1000)\n+\t\t\t\t\t);\n+\t\t\t\t\tif (e instanceof APIError && e.isGatewayError()) {\n+\t\t\t\t\t\t// Gateway problem, wait for some additional time and set concurrency to 1\n+\t\t\t\t\t\tqueue.concurrency = 1;\n+\t\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, gatewayErrors) * 5000)\n+\t\t\t\t\t\t);\n+\t\t\t\t\t\tgatewayErrors++;\n+\t\t\t\t\t\t// only count as a failed attempt after a few initial gateway errors\n+\t\t\t\t\t\tif (gatewayErrors >= MAX_UPLOAD_GATEWAY_ERRORS) {\n+\t\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn doUpload();\n+\t\t\t\t} else if (isJwtExpired(initializeAssetsResponse.jwt)) {\n+\t\t\t\t\tthrow new FatalError(\n+\t\t\t\t\t\t`Asset upload took too long on bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}. Please try again.`",
    "repo_full_name": "cloudflare/workers-sdk",
    "discussion_comments": [
      {
        "comment_id": "1705987597",
        "repo_full_name": "cloudflare/workers-sdk",
        "pr_number": 6374,
        "pr_file": "packages/wrangler/src/experimental-assets.ts",
        "discussion_id": "1705987597",
        "commented_code": "@@ -1,8 +1,301 @@\n+import assert from \"node:assert\";\n import { existsSync } from \"node:fs\";\n-import path from \"node:path\";\n-import { UserError } from \"./errors\";\n+import { readdir, readFile, stat } from \"node:fs/promises\";\n+import * as path from \"node:path\";\n+import chalk from \"chalk\";\n+import { getType } from \"mime\";\n+import PQueue from \"p-queue\";\n+import prettyBytes from \"pretty-bytes\";\n+import { fetchResult } from \"./cfetch\";\n+import { formatTime } from \"./deploy/deploy\";\n+import { FatalError, UserError } from \"./errors\";\n+import { logger, LOGGER_LEVELS } from \"./logger\";\n+import { hashFile } from \"./pages/hash\";\n+import { isJwtExpired } from \"./pages/upload\";\n+import { APIError } from \"./parse\";\n+import { urlSafe } from \"./sites\";\n import type { Config } from \"./config\";\n \n+export type AssetManifest = { [path: string]: { hash: string; size: number } };\n+\n+type InitializeAssetsResponse = {\n+\t// string of file hashes per bucket\n+\tbuckets: string[][];\n+\tjwt: string;\n+};\n+\n+export type UploadPayloadFile = {\n+\tname: string;\n+\thash: string;\n+\tcontents: string;\n+\tmetadata: {\n+\t\tcontentType: string;\n+\t};\n+};\n+\n+type UploadResponse = {\n+\tjwt?: string;\n+};\n+\n+// constants same as Pages for now\n+const BULK_UPLOAD_CONCURRENCY = 3;\n+const MAX_ASSET_COUNT = 20_000;\n+const MAX_ASSET_SIZE = 25 * 1024 * 1024;\n+const MAX_UPLOAD_ATTEMPTS = 5;\n+const MAX_UPLOAD_GATEWAY_ERRORS = 5;\n+\n+export const syncExperimentalAssets = async (\n+\taccountId: string | undefined,\n+\tscriptName: string,\n+\tassetDirectory: string | undefined,\n+\tdryRun: boolean | undefined\n+): Promise<string | undefined> => {\n+\tif (assetDirectory === undefined) {\n+\t\treturn;\n+\t}\n+\tif (dryRun) {\n+\t\tlogger.log(\"(Note: doing a dry run, not uploading or deleting anything.)\");\n+\t\treturn;\n+\t}\n+\tassert(accountId, \"Missing accountId\");\n+\n+\t// 1. generate asset manifest\n+\tlogger.info(\"\ud83c\udf00 Building list of assets...\");\n+\tconst manifest = await walk(assetDirectory, {});\n+\n+\t// 2. fetch buckets w/ hashes\n+\tlogger.info(\"\ud83c\udf00 Uploading asset manifest...\");\n+\tconst initializeAssetsResponse = await fetchResult<InitializeAssetsResponse>(\n+\t\t`/accounts/${accountId}/workers/scripts/${scriptName}/assets-upload-session`,\n+\t\t{\n+\t\t\theaders: { \"Content-Type\": \"application/json\" },\n+\t\t\tmethod: \"POST\",\n+\t\t\tbody: JSON.stringify({ manifest: manifest }),\n+\t\t}\n+\t);\n+\n+\t// if nothing to upload, return\n+\tif (initializeAssetsResponse.buckets.flat().length === 0) {\n+\t\tlogger.info(`\u2728 Success! (No files to upload)`);\n+\t\treturn initializeAssetsResponse.jwt;\n+\t}\n+\n+\t// 3. fill buckets and upload assets\n+\tconst numberFilesToUpload = initializeAssetsResponse.buckets.flat().length;\n+\tlogger.info(`\ud83c\udf00 Uploading ${numberFilesToUpload} file(s)...`);\n+\n+\t// Create the buckets outside of doUpload so we can retry without losing track of potential duplicate files\n+\t// But don't add the actual content until uploading so we don't run out of memory\n+\tconst manifestLookup = Object.entries(manifest);\n+\tlet assetLogCount = 0;\n+\tconst bucketSkeletons = initializeAssetsResponse.buckets.map((bucket) => {\n+\t\treturn bucket.map((fileHash) => {\n+\t\t\tconst manifestEntryIndex = manifestLookup.findIndex(\n+\t\t\t\t(file) => file[1].hash === fileHash\n+\t\t\t);\n+\t\t\tif (manifestEntryIndex === -1) {\n+\t\t\t\tthrow new FatalError(\n+\t\t\t\t\t`A file was requested that does not appear to exist.`,\n+\t\t\t\t\t1\n+\t\t\t\t);\n+\t\t\t}\n+\t\t\tconst manifestEntry = manifestLookup.splice(manifestEntryIndex, 1)[0];\n+\t\t\t// just logging file uploads at the moment...\n+\t\t\t// unsure how to log deletion vs unchanged file ignored/if we want to log this\n+\t\t\tassetLogCount = logAssetUpload(`+ ${manifestEntry[0]}`, assetLogCount);\n+\t\t\treturn manifestEntry;\n+\t\t});\n+\t});\n+\n+\tconst queue = new PQueue({ concurrency: BULK_UPLOAD_CONCURRENCY });\n+\tlet attempts = 0;\n+\tconst start = Date.now();\n+\tlet bucketUploadCount = 0;\n+\tlet completionJwt = \"\";\n+\n+\tfor (const [bucketIndex, bucketSkeleton] of bucketSkeletons.entries()) {\n+\t\tattempts = 0;\n+\t\tlet gatewayErrors = 0;\n+\t\tconst doUpload = async (): Promise<UploadResponse> => {\n+\t\t\t// Populate the payload only when actually uploading (this is limited to 3 concurrent uploads at 50 MiB per bucket meaning we'd only load in a max of ~150 MiB)\n+\t\t\t// This is so we don't run out of memory trying to upload the files.\n+\t\t\tconst payload: UploadPayloadFile[] = await Promise.all(\n+\t\t\t\tbucketSkeleton.map(async (manifestEntry) => {\n+\t\t\t\t\tconst absFilePath = path.join(assetDirectory, manifestEntry[0]);\n+\t\t\t\t\treturn {\n+\t\t\t\t\t\tname: manifestEntry[0],\n+\t\t\t\t\t\thash: manifestEntry[1].hash,\n+\t\t\t\t\t\tcontents: (await readFile(absFilePath)).toString(\"base64\"),\n+\t\t\t\t\t\tmetadata: {\n+\t\t\t\t\t\t\tcontentType: getType(absFilePath) || \"application/octet-stream\",\n+\t\t\t\t\t\t},\n+\t\t\t\t\t};\n+\t\t\t\t})\n+\t\t\t);\n+\n+\t\t\ttry {\n+\t\t\t\tconst res = await fetchResult<UploadResponse>(\n+\t\t\t\t\t`/accounts/${accountId}/workers/assets/upload`,\n+\t\t\t\t\t{\n+\t\t\t\t\t\tmethod: \"POST\",\n+\t\t\t\t\t\theaders: {\n+\t\t\t\t\t\t\t\"Content-Type\": \"application/x-ndjson\",\n+\t\t\t\t\t\t\tAuthorization: `Bearer ${initializeAssetsResponse.jwt}`,\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tbody: payload.map((x) => JSON.stringify(x)).join(\"\\n\"),\n+\t\t\t\t\t}\n+\t\t\t\t);\n+\t\t\t\tlogger.info(\n+\t\t\t\t\t`Uploaded bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}`\n+\t\t\t\t);\n+\t\t\t\treturn res;\n+\t\t\t} catch (e) {\n+\t\t\t\tif (attempts < MAX_UPLOAD_ATTEMPTS) {\n+\t\t\t\t\tlogger.info(\n+\t\t\t\t\t\tchalk.dim(\n+\t\t\t\t\t\t\t`Bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length} upload failed. Retrying...\\n`,\n+\t\t\t\t\t\t\te\n+\t\t\t\t\t\t)\n+\t\t\t\t\t);\n+\t\t\t\t\t// Exponential backoff, 1 second first time, then 2 second, then 4 second etc.\n+\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, attempts) * 1000)\n+\t\t\t\t\t);\n+\t\t\t\t\tif (e instanceof APIError && e.isGatewayError()) {\n+\t\t\t\t\t\t// Gateway problem, wait for some additional time and set concurrency to 1\n+\t\t\t\t\t\tqueue.concurrency = 1;\n+\t\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, gatewayErrors) * 5000)\n+\t\t\t\t\t\t);\n+\t\t\t\t\t\tgatewayErrors++;\n+\t\t\t\t\t\t// only count as a failed attempt after a few initial gateway errors\n+\t\t\t\t\t\tif (gatewayErrors >= MAX_UPLOAD_GATEWAY_ERRORS) {\n+\t\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn doUpload();\n+\t\t\t\t} else if (isJwtExpired(initializeAssetsResponse.jwt)) {\n+\t\t\t\t\tthrow new FatalError(\n+\t\t\t\t\t\t`Asset upload took too long on bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}. Please try again.`",
        "comment_created_at": "2024-08-06T19:03:31+00:00",
        "comment_author": "GregBrimble",
        "comment_body": "Idk, maybe worth trying to say something like this? Take it, edit it, or leave it. I'm just trying to say that they don't need to worry about having to start from scratch again to indeed encourage trying again. If that just confuses the message, feel free to not bother.\r\n\r\n```suggestion\r\n\t\t\t\t\t\t`Asset upload took too long on bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}. Please try again \u2014 assets already uploaded have been saved, so the next attempt will automatically resume from this point.`\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1707411680",
    "pr_number": 6374,
    "pr_file": "packages/wrangler/src/experimental-assets.ts",
    "created_at": "2024-08-07T16:36:36+00:00",
    "commented_code": "+import assert from \"node:assert\";\n import { existsSync } from \"node:fs\";\n-import path from \"node:path\";\n-import { UserError } from \"./errors\";\n+import { readdir, readFile, stat } from \"node:fs/promises\";\n+import * as path from \"node:path\";\n+import chalk from \"chalk\";\n+import { getType } from \"mime\";\n+import PQueue from \"p-queue\";\n+import prettyBytes from \"pretty-bytes\";\n+import { fetchResult } from \"./cfetch\";\n+import { formatTime } from \"./deploy/deploy\";\n+import { FatalError, UserError } from \"./errors\";\n+import { logger, LOGGER_LEVELS } from \"./logger\";\n+import { hashFile } from \"./pages/hash\";\n+import { isJwtExpired } from \"./pages/upload\";\n+import { APIError } from \"./parse\";\n+import { urlSafe } from \"./sites\";\n import type { Config } from \"./config\";\n \n+export type AssetManifest = { [path: string]: { hash: string; size: number } };\n+\n+type InitializeAssetsResponse = {\n+\t// string of file hashes per bucket\n+\tbuckets: string[][];\n+\tjwt: string;\n+};\n+\n+export type UploadPayloadFile = {\n+\tbase64: boolean;\n+\tkey: string;\n+\tvalue: string;\n+\tmetadata: {\n+\t\tcontentType: string;\n+\t};\n+};\n+\n+type UploadResponse = {\n+\tjwt?: string;\n+};\n+\n+// constants same as Pages for now\n+const BULK_UPLOAD_CONCURRENCY = 3;\n+const MAX_ASSET_COUNT = 20_000;\n+const MAX_ASSET_SIZE = 25 * 1024 * 1024;\n+const MAX_UPLOAD_ATTEMPTS = 5;\n+const MAX_UPLOAD_GATEWAY_ERRORS = 5;\n+\n+export const syncExperimentalAssets = async (\n+\taccountId: string | undefined,\n+\tscriptName: string,\n+\tassetDirectory: string | undefined,\n+\tdryRun: boolean | undefined\n+): Promise<string | undefined> => {\n+\tif (assetDirectory === undefined) {\n+\t\treturn;\n+\t}\n+\tif (dryRun) {\n+\t\tlogger.log(\"(Note: doing a dry run, not uploading or deleting anything.)\");\n+\t\treturn;\n+\t}\n+\tassert(accountId, \"Missing accountId\");\n+\n+\t// 1. generate asset manifest\n+\tlogger.info(\"\ud83c\udf00 Building list of assets...\");\n+\tconst manifest = await walk(assetDirectory, {});\n+\n+\t// 2. fetch buckets w/ hashes\n+\tlogger.info(\"\ud83c\udf00 Starting asset upload...\");\n+\tconst initializeAssetsResponse = await fetchResult<InitializeAssetsResponse>(\n+\t\t`/accounts/${accountId}/workers/scripts/${scriptName}/assets-upload-session`,\n+\t\t{\n+\t\t\theaders: { \"Content-Type\": \"application/json\" },\n+\t\t\tmethod: \"POST\",\n+\t\t\tbody: JSON.stringify({ manifest: manifest }),\n+\t\t}\n+\t);\n+\n+\t// if nothing to upload, return\n+\tif (initializeAssetsResponse.buckets.flat().length === 0) {\n+\t\tif (!initializeAssetsResponse.jwt) {\n+\t\t\tthrow new FatalError(\n+\t\t\t\t\"Could not find assets information to attach to deployment. Please try again.\",\n+\t\t\t\t1\n+\t\t\t);\n+\t\t}\n+\t\tlogger.info(`\u2728 Success! (No files to upload)`);\n+\t\treturn initializeAssetsResponse.jwt;\n+\t}\n+\n+\t// 3. fill buckets and upload assets\n+\tconst numberFilesToUpload = initializeAssetsResponse.buckets.flat().length;\n+\tlogger.info(`\ud83c\udf00 Uploading ${numberFilesToUpload} file(s)...`);\n+\n+\t// Create the buckets outside of doUpload so we can retry without losing track of potential duplicate files\n+\t// But don't add the actual content until uploading so we don't run out of memory\n+\tconst manifestLookup = Object.entries(manifest);\n+\tlet assetLogCount = 0;\n+\tconst bucketSkeletons = initializeAssetsResponse.buckets.map((bucket) => {\n+\t\treturn bucket.map((fileHash) => {\n+\t\t\tconst manifestEntry = manifestLookup.find(\n+\t\t\t\t(file) => file[1].hash === fileHash\n+\t\t\t);\n+\t\t\tif (manifestEntry === undefined) {\n+\t\t\t\tthrow new FatalError(\n+\t\t\t\t\t`A file was requested that does not appear to exist.`,\n+\t\t\t\t\t1\n+\t\t\t\t);\n+\t\t\t}\n+\t\t\t// just logging file uploads at the moment...\n+\t\t\t// unsure how to log deletion vs unchanged file ignored/if we want to log this\n+\t\t\tassetLogCount = logAssetUpload(`+ ${manifestEntry[0]}`, assetLogCount);\n+\t\t\treturn manifestEntry;\n+\t\t});\n+\t});\n+\n+\tconst queue = new PQueue({ concurrency: BULK_UPLOAD_CONCURRENCY });\n+\tlet attempts = 0;\n+\tconst start = Date.now();\n+\tlet completionJwt = \"\";\n+\n+\tfor (const [bucketIndex, bucketSkeleton] of bucketSkeletons.entries()) {\n+\t\tattempts = 0;\n+\t\tlet gatewayErrors = 0;\n+\t\tconst doUpload = async (): Promise<UploadResponse> => {\n+\t\t\t// Populate the payload only when actually uploading (this is limited to 3 concurrent uploads at 50 MiB per bucket meaning we'd only load in a max of ~150 MiB)\n+\t\t\t// This is so we don't run out of memory trying to upload the files.\n+\t\t\tconst payload: UploadPayloadFile[] = await Promise.all(\n+\t\t\t\tbucketSkeleton.map(async (manifestEntry) => {\n+\t\t\t\t\tconst absFilePath = path.join(assetDirectory, manifestEntry[0]);\n+\t\t\t\t\treturn {\n+\t\t\t\t\t\tbase64: true,\n+\t\t\t\t\t\tkey: manifestEntry[1].hash,\n+\t\t\t\t\t\tmetadata: {\n+\t\t\t\t\t\t\tcontentType: getType(absFilePath) || \"application/octet-stream\",\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tvalue: (await readFile(absFilePath)).toString(\"base64\"),\n+\t\t\t\t\t};\n+\t\t\t\t})\n+\t\t\t);\n+\n+\t\t\ttry {\n+\t\t\t\tconst res = await fetchResult<UploadResponse>(\n+\t\t\t\t\t`/accounts/${accountId}/workers/assets/upload`,\n+\t\t\t\t\t{\n+\t\t\t\t\t\tmethod: \"POST\",\n+\t\t\t\t\t\theaders: {\n+\t\t\t\t\t\t\t\"Content-Type\": \"application/x-ndjson\",\n+\t\t\t\t\t\t\tAuthorization: `Bearer ${initializeAssetsResponse.jwt}`,\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tbody: payload.map((x) => JSON.stringify(x)).join(\"\n\"),\n+\t\t\t\t\t}\n+\t\t\t\t);\n+\t\t\t\tlogger.info(\n+\t\t\t\t\t`Uploaded bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}`\n+\t\t\t\t);\n+\t\t\t\treturn res;\n+\t\t\t} catch (e) {\n+\t\t\t\tif (attempts < MAX_UPLOAD_ATTEMPTS) {\n+\t\t\t\t\tlogger.info(\n+\t\t\t\t\t\tchalk.dim(\n+\t\t\t\t\t\t\t`Bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length} upload failed. Retrying...\n`,\n+\t\t\t\t\t\t\te\n+\t\t\t\t\t\t)\n+\t\t\t\t\t);\n+\t\t\t\t\t// Exponential backoff, 1 second first time, then 2 second, then 4 second etc.\n+\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, attempts) * 1000)\n+\t\t\t\t\t);\n+\t\t\t\t\tif (e instanceof APIError && e.isGatewayError()) {\n+\t\t\t\t\t\t// Gateway problem, wait for some additional time and set concurrency to 1\n+\t\t\t\t\t\tqueue.concurrency = 1;\n+\t\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, gatewayErrors) * 5000)\n+\t\t\t\t\t\t);\n+\t\t\t\t\t\tgatewayErrors++;\n+\t\t\t\t\t\t// only count as a failed attempt after a few initial gateway errors\n+\t\t\t\t\t\tif (gatewayErrors >= MAX_UPLOAD_GATEWAY_ERRORS) {\n+\t\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn doUpload();\n+\t\t\t\t} else if (isJwtExpired(initializeAssetsResponse.jwt)) {\n+\t\t\t\t\tthrow new FatalError(",
    "repo_full_name": "cloudflare/workers-sdk",
    "discussion_comments": [
      {
        "comment_id": "1707411680",
        "repo_full_name": "cloudflare/workers-sdk",
        "pr_number": 6374,
        "pr_file": "packages/wrangler/src/experimental-assets.ts",
        "discussion_id": "1707411680",
        "commented_code": "@@ -1,8 +1,299 @@\n+import assert from \"node:assert\";\n import { existsSync } from \"node:fs\";\n-import path from \"node:path\";\n-import { UserError } from \"./errors\";\n+import { readdir, readFile, stat } from \"node:fs/promises\";\n+import * as path from \"node:path\";\n+import chalk from \"chalk\";\n+import { getType } from \"mime\";\n+import PQueue from \"p-queue\";\n+import prettyBytes from \"pretty-bytes\";\n+import { fetchResult } from \"./cfetch\";\n+import { formatTime } from \"./deploy/deploy\";\n+import { FatalError, UserError } from \"./errors\";\n+import { logger, LOGGER_LEVELS } from \"./logger\";\n+import { hashFile } from \"./pages/hash\";\n+import { isJwtExpired } from \"./pages/upload\";\n+import { APIError } from \"./parse\";\n+import { urlSafe } from \"./sites\";\n import type { Config } from \"./config\";\n \n+export type AssetManifest = { [path: string]: { hash: string; size: number } };\n+\n+type InitializeAssetsResponse = {\n+\t// string of file hashes per bucket\n+\tbuckets: string[][];\n+\tjwt: string;\n+};\n+\n+export type UploadPayloadFile = {\n+\tbase64: boolean;\n+\tkey: string;\n+\tvalue: string;\n+\tmetadata: {\n+\t\tcontentType: string;\n+\t};\n+};\n+\n+type UploadResponse = {\n+\tjwt?: string;\n+};\n+\n+// constants same as Pages for now\n+const BULK_UPLOAD_CONCURRENCY = 3;\n+const MAX_ASSET_COUNT = 20_000;\n+const MAX_ASSET_SIZE = 25 * 1024 * 1024;\n+const MAX_UPLOAD_ATTEMPTS = 5;\n+const MAX_UPLOAD_GATEWAY_ERRORS = 5;\n+\n+export const syncExperimentalAssets = async (\n+\taccountId: string | undefined,\n+\tscriptName: string,\n+\tassetDirectory: string | undefined,\n+\tdryRun: boolean | undefined\n+): Promise<string | undefined> => {\n+\tif (assetDirectory === undefined) {\n+\t\treturn;\n+\t}\n+\tif (dryRun) {\n+\t\tlogger.log(\"(Note: doing a dry run, not uploading or deleting anything.)\");\n+\t\treturn;\n+\t}\n+\tassert(accountId, \"Missing accountId\");\n+\n+\t// 1. generate asset manifest\n+\tlogger.info(\"\ud83c\udf00 Building list of assets...\");\n+\tconst manifest = await walk(assetDirectory, {});\n+\n+\t// 2. fetch buckets w/ hashes\n+\tlogger.info(\"\ud83c\udf00 Starting asset upload...\");\n+\tconst initializeAssetsResponse = await fetchResult<InitializeAssetsResponse>(\n+\t\t`/accounts/${accountId}/workers/scripts/${scriptName}/assets-upload-session`,\n+\t\t{\n+\t\t\theaders: { \"Content-Type\": \"application/json\" },\n+\t\t\tmethod: \"POST\",\n+\t\t\tbody: JSON.stringify({ manifest: manifest }),\n+\t\t}\n+\t);\n+\n+\t// if nothing to upload, return\n+\tif (initializeAssetsResponse.buckets.flat().length === 0) {\n+\t\tif (!initializeAssetsResponse.jwt) {\n+\t\t\tthrow new FatalError(\n+\t\t\t\t\"Could not find assets information to attach to deployment. Please try again.\",\n+\t\t\t\t1\n+\t\t\t);\n+\t\t}\n+\t\tlogger.info(`\u2728 Success! (No files to upload)`);\n+\t\treturn initializeAssetsResponse.jwt;\n+\t}\n+\n+\t// 3. fill buckets and upload assets\n+\tconst numberFilesToUpload = initializeAssetsResponse.buckets.flat().length;\n+\tlogger.info(`\ud83c\udf00 Uploading ${numberFilesToUpload} file(s)...`);\n+\n+\t// Create the buckets outside of doUpload so we can retry without losing track of potential duplicate files\n+\t// But don't add the actual content until uploading so we don't run out of memory\n+\tconst manifestLookup = Object.entries(manifest);\n+\tlet assetLogCount = 0;\n+\tconst bucketSkeletons = initializeAssetsResponse.buckets.map((bucket) => {\n+\t\treturn bucket.map((fileHash) => {\n+\t\t\tconst manifestEntry = manifestLookup.find(\n+\t\t\t\t(file) => file[1].hash === fileHash\n+\t\t\t);\n+\t\t\tif (manifestEntry === undefined) {\n+\t\t\t\tthrow new FatalError(\n+\t\t\t\t\t`A file was requested that does not appear to exist.`,\n+\t\t\t\t\t1\n+\t\t\t\t);\n+\t\t\t}\n+\t\t\t// just logging file uploads at the moment...\n+\t\t\t// unsure how to log deletion vs unchanged file ignored/if we want to log this\n+\t\t\tassetLogCount = logAssetUpload(`+ ${manifestEntry[0]}`, assetLogCount);\n+\t\t\treturn manifestEntry;\n+\t\t});\n+\t});\n+\n+\tconst queue = new PQueue({ concurrency: BULK_UPLOAD_CONCURRENCY });\n+\tlet attempts = 0;\n+\tconst start = Date.now();\n+\tlet completionJwt = \"\";\n+\n+\tfor (const [bucketIndex, bucketSkeleton] of bucketSkeletons.entries()) {\n+\t\tattempts = 0;\n+\t\tlet gatewayErrors = 0;\n+\t\tconst doUpload = async (): Promise<UploadResponse> => {\n+\t\t\t// Populate the payload only when actually uploading (this is limited to 3 concurrent uploads at 50 MiB per bucket meaning we'd only load in a max of ~150 MiB)\n+\t\t\t// This is so we don't run out of memory trying to upload the files.\n+\t\t\tconst payload: UploadPayloadFile[] = await Promise.all(\n+\t\t\t\tbucketSkeleton.map(async (manifestEntry) => {\n+\t\t\t\t\tconst absFilePath = path.join(assetDirectory, manifestEntry[0]);\n+\t\t\t\t\treturn {\n+\t\t\t\t\t\tbase64: true,\n+\t\t\t\t\t\tkey: manifestEntry[1].hash,\n+\t\t\t\t\t\tmetadata: {\n+\t\t\t\t\t\t\tcontentType: getType(absFilePath) || \"application/octet-stream\",\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tvalue: (await readFile(absFilePath)).toString(\"base64\"),\n+\t\t\t\t\t};\n+\t\t\t\t})\n+\t\t\t);\n+\n+\t\t\ttry {\n+\t\t\t\tconst res = await fetchResult<UploadResponse>(\n+\t\t\t\t\t`/accounts/${accountId}/workers/assets/upload`,\n+\t\t\t\t\t{\n+\t\t\t\t\t\tmethod: \"POST\",\n+\t\t\t\t\t\theaders: {\n+\t\t\t\t\t\t\t\"Content-Type\": \"application/x-ndjson\",\n+\t\t\t\t\t\t\tAuthorization: `Bearer ${initializeAssetsResponse.jwt}`,\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tbody: payload.map((x) => JSON.stringify(x)).join(\"\\n\"),\n+\t\t\t\t\t}\n+\t\t\t\t);\n+\t\t\t\tlogger.info(\n+\t\t\t\t\t`Uploaded bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}`\n+\t\t\t\t);\n+\t\t\t\treturn res;\n+\t\t\t} catch (e) {\n+\t\t\t\tif (attempts < MAX_UPLOAD_ATTEMPTS) {\n+\t\t\t\t\tlogger.info(\n+\t\t\t\t\t\tchalk.dim(\n+\t\t\t\t\t\t\t`Bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length} upload failed. Retrying...\\n`,\n+\t\t\t\t\t\t\te\n+\t\t\t\t\t\t)\n+\t\t\t\t\t);\n+\t\t\t\t\t// Exponential backoff, 1 second first time, then 2 second, then 4 second etc.\n+\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, attempts) * 1000)\n+\t\t\t\t\t);\n+\t\t\t\t\tif (e instanceof APIError && e.isGatewayError()) {\n+\t\t\t\t\t\t// Gateway problem, wait for some additional time and set concurrency to 1\n+\t\t\t\t\t\tqueue.concurrency = 1;\n+\t\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, gatewayErrors) * 5000)\n+\t\t\t\t\t\t);\n+\t\t\t\t\t\tgatewayErrors++;\n+\t\t\t\t\t\t// only count as a failed attempt after a few initial gateway errors\n+\t\t\t\t\t\tif (gatewayErrors >= MAX_UPLOAD_GATEWAY_ERRORS) {\n+\t\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn doUpload();\n+\t\t\t\t} else if (isJwtExpired(initializeAssetsResponse.jwt)) {\n+\t\t\t\t\tthrow new FatalError(",
        "comment_created_at": "2024-08-07T16:36:36+00:00",
        "comment_author": "CarmenPopoviciu",
        "comment_body": "\n```suggestion\n\t\t\t\t\tthrow new FatalError(\n\t\t\t\t\t\t`Upload took too long.\\n` +\n            `Asset upload took too long on bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}. Please try again.\n\t\t\t\t\t\tAssets already uploaded have been saved, so the next attempt will automatically resume from this point.`\n          );\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1707443153",
    "pr_number": 6374,
    "pr_file": "packages/wrangler/src/experimental-assets.ts",
    "created_at": "2024-08-07T16:51:55+00:00",
    "commented_code": "+import assert from \"node:assert\";\n import { existsSync } from \"node:fs\";\n-import path from \"node:path\";\n-import { UserError } from \"./errors\";\n+import { readdir, readFile, stat } from \"node:fs/promises\";\n+import * as path from \"node:path\";\n+import chalk from \"chalk\";\n+import { getType } from \"mime\";\n+import PQueue from \"p-queue\";\n+import prettyBytes from \"pretty-bytes\";\n+import { fetchResult } from \"./cfetch\";\n+import { formatTime } from \"./deploy/deploy\";\n+import { FatalError, UserError } from \"./errors\";\n+import { logger, LOGGER_LEVELS } from \"./logger\";\n+import { hashFile } from \"./pages/hash\";\n+import { isJwtExpired } from \"./pages/upload\";\n+import { APIError } from \"./parse\";\n+import { urlSafe } from \"./sites\";\n import type { Config } from \"./config\";\n \n+export type AssetManifest = { [path: string]: { hash: string; size: number } };\n+\n+type InitializeAssetsResponse = {\n+\t// string of file hashes per bucket\n+\tbuckets: string[][];\n+\tjwt: string;\n+};\n+\n+export type UploadPayloadFile = {\n+\tbase64: boolean;\n+\tkey: string;\n+\tvalue: string;\n+\tmetadata: {\n+\t\tcontentType: string;\n+\t};\n+};\n+\n+type UploadResponse = {\n+\tjwt?: string;\n+};\n+\n+// constants same as Pages for now\n+const BULK_UPLOAD_CONCURRENCY = 3;\n+const MAX_ASSET_COUNT = 20_000;\n+const MAX_ASSET_SIZE = 25 * 1024 * 1024;\n+const MAX_UPLOAD_ATTEMPTS = 5;\n+const MAX_UPLOAD_GATEWAY_ERRORS = 5;\n+\n+export const syncExperimentalAssets = async (\n+\taccountId: string | undefined,\n+\tscriptName: string,\n+\tassetDirectory: string | undefined,\n+\tdryRun: boolean | undefined\n+): Promise<string | undefined> => {\n+\tif (assetDirectory === undefined) {\n+\t\treturn;\n+\t}\n+\tif (dryRun) {\n+\t\tlogger.log(\"(Note: doing a dry run, not uploading or deleting anything.)\");\n+\t\treturn;\n+\t}\n+\tassert(accountId, \"Missing accountId\");\n+\n+\t// 1. generate asset manifest\n+\tlogger.info(\"\ud83c\udf00 Building list of assets...\");\n+\tconst manifest = await walk(assetDirectory, {});\n+\n+\t// 2. fetch buckets w/ hashes\n+\tlogger.info(\"\ud83c\udf00 Starting asset upload...\");\n+\tconst initializeAssetsResponse = await fetchResult<InitializeAssetsResponse>(\n+\t\t`/accounts/${accountId}/workers/scripts/${scriptName}/assets-upload-session`,\n+\t\t{\n+\t\t\theaders: { \"Content-Type\": \"application/json\" },\n+\t\t\tmethod: \"POST\",\n+\t\t\tbody: JSON.stringify({ manifest: manifest }),\n+\t\t}\n+\t);\n+\n+\t// if nothing to upload, return\n+\tif (initializeAssetsResponse.buckets.flat().length === 0) {\n+\t\tif (!initializeAssetsResponse.jwt) {\n+\t\t\tthrow new FatalError(\n+\t\t\t\t\"Could not find assets information to attach to deployment. Please try again.\",\n+\t\t\t\t1\n+\t\t\t);\n+\t\t}\n+\t\tlogger.info(`\u2728 Success! (No files to upload)`);\n+\t\treturn initializeAssetsResponse.jwt;\n+\t}\n+\n+\t// 3. fill buckets and upload assets\n+\tconst numberFilesToUpload = initializeAssetsResponse.buckets.flat().length;\n+\tlogger.info(`\ud83c\udf00 Uploading ${numberFilesToUpload} file(s)...`);\n+\n+\t// Create the buckets outside of doUpload so we can retry without losing track of potential duplicate files\n+\t// But don't add the actual content until uploading so we don't run out of memory\n+\tconst manifestLookup = Object.entries(manifest);\n+\tlet assetLogCount = 0;\n+\tconst bucketSkeletons = initializeAssetsResponse.buckets.map((bucket) => {\n+\t\treturn bucket.map((fileHash) => {\n+\t\t\tconst manifestEntry = manifestLookup.find(\n+\t\t\t\t(file) => file[1].hash === fileHash\n+\t\t\t);\n+\t\t\tif (manifestEntry === undefined) {\n+\t\t\t\tthrow new FatalError(\n+\t\t\t\t\t`A file was requested that does not appear to exist.`,\n+\t\t\t\t\t1\n+\t\t\t\t);\n+\t\t\t}\n+\t\t\t// just logging file uploads at the moment...\n+\t\t\t// unsure how to log deletion vs unchanged file ignored/if we want to log this\n+\t\t\tassetLogCount = logAssetUpload(`+ ${manifestEntry[0]}`, assetLogCount);\n+\t\t\treturn manifestEntry;\n+\t\t});\n+\t});\n+\n+\tconst queue = new PQueue({ concurrency: BULK_UPLOAD_CONCURRENCY });\n+\tlet attempts = 0;\n+\tconst start = Date.now();\n+\tlet completionJwt = \"\";\n+\n+\tfor (const [bucketIndex, bucketSkeleton] of bucketSkeletons.entries()) {\n+\t\tattempts = 0;\n+\t\tlet gatewayErrors = 0;\n+\t\tconst doUpload = async (): Promise<UploadResponse> => {\n+\t\t\t// Populate the payload only when actually uploading (this is limited to 3 concurrent uploads at 50 MiB per bucket meaning we'd only load in a max of ~150 MiB)\n+\t\t\t// This is so we don't run out of memory trying to upload the files.\n+\t\t\tconst payload: UploadPayloadFile[] = await Promise.all(\n+\t\t\t\tbucketSkeleton.map(async (manifestEntry) => {\n+\t\t\t\t\tconst absFilePath = path.join(assetDirectory, manifestEntry[0]);\n+\t\t\t\t\treturn {\n+\t\t\t\t\t\tbase64: true,\n+\t\t\t\t\t\tkey: manifestEntry[1].hash,\n+\t\t\t\t\t\tmetadata: {\n+\t\t\t\t\t\t\tcontentType: getType(absFilePath) || \"application/octet-stream\",\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tvalue: (await readFile(absFilePath)).toString(\"base64\"),\n+\t\t\t\t\t};\n+\t\t\t\t})\n+\t\t\t);\n+\n+\t\t\ttry {\n+\t\t\t\tconst res = await fetchResult<UploadResponse>(\n+\t\t\t\t\t`/accounts/${accountId}/workers/assets/upload`,\n+\t\t\t\t\t{\n+\t\t\t\t\t\tmethod: \"POST\",\n+\t\t\t\t\t\theaders: {\n+\t\t\t\t\t\t\t\"Content-Type\": \"application/x-ndjson\",\n+\t\t\t\t\t\t\tAuthorization: `Bearer ${initializeAssetsResponse.jwt}`,\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tbody: payload.map((x) => JSON.stringify(x)).join(\"\n\"),\n+\t\t\t\t\t}\n+\t\t\t\t);\n+\t\t\t\tlogger.info(\n+\t\t\t\t\t`Uploaded bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}`\n+\t\t\t\t);\n+\t\t\t\treturn res;\n+\t\t\t} catch (e) {\n+\t\t\t\tif (attempts < MAX_UPLOAD_ATTEMPTS) {\n+\t\t\t\t\tlogger.info(\n+\t\t\t\t\t\tchalk.dim(\n+\t\t\t\t\t\t\t`Bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length} upload failed. Retrying...\n`,\n+\t\t\t\t\t\t\te\n+\t\t\t\t\t\t)\n+\t\t\t\t\t);\n+\t\t\t\t\t// Exponential backoff, 1 second first time, then 2 second, then 4 second etc.\n+\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, attempts) * 1000)\n+\t\t\t\t\t);\n+\t\t\t\t\tif (e instanceof APIError && e.isGatewayError()) {\n+\t\t\t\t\t\t// Gateway problem, wait for some additional time and set concurrency to 1\n+\t\t\t\t\t\tqueue.concurrency = 1;\n+\t\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, gatewayErrors) * 5000)\n+\t\t\t\t\t\t);\n+\t\t\t\t\t\tgatewayErrors++;\n+\t\t\t\t\t\t// only count as a failed attempt after a few initial gateway errors\n+\t\t\t\t\t\tif (gatewayErrors >= MAX_UPLOAD_GATEWAY_ERRORS) {\n+\t\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn doUpload();\n+\t\t\t\t} else if (isJwtExpired(initializeAssetsResponse.jwt)) {\n+\t\t\t\t\tthrow new FatalError(\n+\t\t\t\t\t\t`Asset upload took too long on bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}. Please try again.\n+\t\t\t\t\t\tAssets already uploaded have been saved, so the next attempt will automatically resume from this point.`\n+\t\t\t\t\t);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow e;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t};\n+\t\t// add to queue and run it if we haven't reached concurrency limit\n+\t\tvoid queue.add(() =>\n+\t\t\tdoUpload().then((res) => {\n+\t\t\t\tcompletionJwt = res.jwt || completionJwt;\n+\t\t\t})\n+\t\t);\n+\t}\n+\tqueue.on(\"error\", (error) => {\n+\t\tlogger.error(error.message);\n+\t\tthrow error;\n+\t});\n+\tawait queue.onIdle();\n+\n+\t// if queue finishes without receiving JWT from asset upload service (AUS)\n+\t// AUS only returns this in the final bucket upload response\n+\tif (!completionJwt) {\n+\t\tthrow new FatalError(\n+\t\t\t\"Failed to complete asset upload. Please try again.\",\n+\t\t\t1\n+\t\t);\n+\t}\n+\n+\tconst uploadMs = Date.now() - start;\n+\tconst skipped = Object.keys(manifest).length - numberFilesToUpload;\n+\tconst skippedMessage = skipped > 0 ? `(${skipped} already uploaded) ` : \"\";\n+\n+\tlogger.log(\n+\t\t`\u2728 Success! Uploaded ${numberFilesToUpload} file(s) ${skippedMessage}${formatTime(uploadMs)}\n`\n+\t);\n+\n+\treturn completionJwt;\n+};\n+\n+// modified from /pages/validate.tsx\n+const walk = async (\n+\tdir: string,\n+\tmanifest: AssetManifest,\n+\tstartingDir: string = dir\n+) => {\n+\tconst files = await readdir(dir);\n+\n+\tlet counter = 0;\n+\tawait Promise.all(\n+\t\tfiles.map(async (file) => {\n+\t\t\tconst filepath = path.join(dir, file);\n+\t\t\tconst relativeFilepath = path.relative(startingDir, filepath);\n+\t\t\tconst filestat = await stat(filepath);\n+\n+\t\t\tif (filestat.isSymbolicLink()) {\n+\t\t\t\treturn;\n+\t\t\t}\n+\n+\t\t\tif (filestat.isDirectory()) {\n+\t\t\t\tmanifest = await walk(filepath, manifest, startingDir);\n+\t\t\t} else {\n+\t\t\t\tif (counter >= MAX_ASSET_COUNT) {\n+\t\t\t\t\tthrow new UserError(\n+\t\t\t\t\t\t`Max asset count exceeded: ${counter.toLocaleString()} files found.\n+\t\t\t\t\t\tYou cannot have more than ${MAX_ASSET_COUNT.toLocaleString()} files in a deployment.\n+\t\t\t\t\t\tEnsure you have specified your build output directory correctly (currently set as ${startingDir}).`",
    "repo_full_name": "cloudflare/workers-sdk",
    "discussion_comments": [
      {
        "comment_id": "1707443153",
        "repo_full_name": "cloudflare/workers-sdk",
        "pr_number": 6374,
        "pr_file": "packages/wrangler/src/experimental-assets.ts",
        "discussion_id": "1707443153",
        "commented_code": "@@ -1,8 +1,299 @@\n+import assert from \"node:assert\";\n import { existsSync } from \"node:fs\";\n-import path from \"node:path\";\n-import { UserError } from \"./errors\";\n+import { readdir, readFile, stat } from \"node:fs/promises\";\n+import * as path from \"node:path\";\n+import chalk from \"chalk\";\n+import { getType } from \"mime\";\n+import PQueue from \"p-queue\";\n+import prettyBytes from \"pretty-bytes\";\n+import { fetchResult } from \"./cfetch\";\n+import { formatTime } from \"./deploy/deploy\";\n+import { FatalError, UserError } from \"./errors\";\n+import { logger, LOGGER_LEVELS } from \"./logger\";\n+import { hashFile } from \"./pages/hash\";\n+import { isJwtExpired } from \"./pages/upload\";\n+import { APIError } from \"./parse\";\n+import { urlSafe } from \"./sites\";\n import type { Config } from \"./config\";\n \n+export type AssetManifest = { [path: string]: { hash: string; size: number } };\n+\n+type InitializeAssetsResponse = {\n+\t// string of file hashes per bucket\n+\tbuckets: string[][];\n+\tjwt: string;\n+};\n+\n+export type UploadPayloadFile = {\n+\tbase64: boolean;\n+\tkey: string;\n+\tvalue: string;\n+\tmetadata: {\n+\t\tcontentType: string;\n+\t};\n+};\n+\n+type UploadResponse = {\n+\tjwt?: string;\n+};\n+\n+// constants same as Pages for now\n+const BULK_UPLOAD_CONCURRENCY = 3;\n+const MAX_ASSET_COUNT = 20_000;\n+const MAX_ASSET_SIZE = 25 * 1024 * 1024;\n+const MAX_UPLOAD_ATTEMPTS = 5;\n+const MAX_UPLOAD_GATEWAY_ERRORS = 5;\n+\n+export const syncExperimentalAssets = async (\n+\taccountId: string | undefined,\n+\tscriptName: string,\n+\tassetDirectory: string | undefined,\n+\tdryRun: boolean | undefined\n+): Promise<string | undefined> => {\n+\tif (assetDirectory === undefined) {\n+\t\treturn;\n+\t}\n+\tif (dryRun) {\n+\t\tlogger.log(\"(Note: doing a dry run, not uploading or deleting anything.)\");\n+\t\treturn;\n+\t}\n+\tassert(accountId, \"Missing accountId\");\n+\n+\t// 1. generate asset manifest\n+\tlogger.info(\"\ud83c\udf00 Building list of assets...\");\n+\tconst manifest = await walk(assetDirectory, {});\n+\n+\t// 2. fetch buckets w/ hashes\n+\tlogger.info(\"\ud83c\udf00 Starting asset upload...\");\n+\tconst initializeAssetsResponse = await fetchResult<InitializeAssetsResponse>(\n+\t\t`/accounts/${accountId}/workers/scripts/${scriptName}/assets-upload-session`,\n+\t\t{\n+\t\t\theaders: { \"Content-Type\": \"application/json\" },\n+\t\t\tmethod: \"POST\",\n+\t\t\tbody: JSON.stringify({ manifest: manifest }),\n+\t\t}\n+\t);\n+\n+\t// if nothing to upload, return\n+\tif (initializeAssetsResponse.buckets.flat().length === 0) {\n+\t\tif (!initializeAssetsResponse.jwt) {\n+\t\t\tthrow new FatalError(\n+\t\t\t\t\"Could not find assets information to attach to deployment. Please try again.\",\n+\t\t\t\t1\n+\t\t\t);\n+\t\t}\n+\t\tlogger.info(`\u2728 Success! (No files to upload)`);\n+\t\treturn initializeAssetsResponse.jwt;\n+\t}\n+\n+\t// 3. fill buckets and upload assets\n+\tconst numberFilesToUpload = initializeAssetsResponse.buckets.flat().length;\n+\tlogger.info(`\ud83c\udf00 Uploading ${numberFilesToUpload} file(s)...`);\n+\n+\t// Create the buckets outside of doUpload so we can retry without losing track of potential duplicate files\n+\t// But don't add the actual content until uploading so we don't run out of memory\n+\tconst manifestLookup = Object.entries(manifest);\n+\tlet assetLogCount = 0;\n+\tconst bucketSkeletons = initializeAssetsResponse.buckets.map((bucket) => {\n+\t\treturn bucket.map((fileHash) => {\n+\t\t\tconst manifestEntry = manifestLookup.find(\n+\t\t\t\t(file) => file[1].hash === fileHash\n+\t\t\t);\n+\t\t\tif (manifestEntry === undefined) {\n+\t\t\t\tthrow new FatalError(\n+\t\t\t\t\t`A file was requested that does not appear to exist.`,\n+\t\t\t\t\t1\n+\t\t\t\t);\n+\t\t\t}\n+\t\t\t// just logging file uploads at the moment...\n+\t\t\t// unsure how to log deletion vs unchanged file ignored/if we want to log this\n+\t\t\tassetLogCount = logAssetUpload(`+ ${manifestEntry[0]}`, assetLogCount);\n+\t\t\treturn manifestEntry;\n+\t\t});\n+\t});\n+\n+\tconst queue = new PQueue({ concurrency: BULK_UPLOAD_CONCURRENCY });\n+\tlet attempts = 0;\n+\tconst start = Date.now();\n+\tlet completionJwt = \"\";\n+\n+\tfor (const [bucketIndex, bucketSkeleton] of bucketSkeletons.entries()) {\n+\t\tattempts = 0;\n+\t\tlet gatewayErrors = 0;\n+\t\tconst doUpload = async (): Promise<UploadResponse> => {\n+\t\t\t// Populate the payload only when actually uploading (this is limited to 3 concurrent uploads at 50 MiB per bucket meaning we'd only load in a max of ~150 MiB)\n+\t\t\t// This is so we don't run out of memory trying to upload the files.\n+\t\t\tconst payload: UploadPayloadFile[] = await Promise.all(\n+\t\t\t\tbucketSkeleton.map(async (manifestEntry) => {\n+\t\t\t\t\tconst absFilePath = path.join(assetDirectory, manifestEntry[0]);\n+\t\t\t\t\treturn {\n+\t\t\t\t\t\tbase64: true,\n+\t\t\t\t\t\tkey: manifestEntry[1].hash,\n+\t\t\t\t\t\tmetadata: {\n+\t\t\t\t\t\t\tcontentType: getType(absFilePath) || \"application/octet-stream\",\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tvalue: (await readFile(absFilePath)).toString(\"base64\"),\n+\t\t\t\t\t};\n+\t\t\t\t})\n+\t\t\t);\n+\n+\t\t\ttry {\n+\t\t\t\tconst res = await fetchResult<UploadResponse>(\n+\t\t\t\t\t`/accounts/${accountId}/workers/assets/upload`,\n+\t\t\t\t\t{\n+\t\t\t\t\t\tmethod: \"POST\",\n+\t\t\t\t\t\theaders: {\n+\t\t\t\t\t\t\t\"Content-Type\": \"application/x-ndjson\",\n+\t\t\t\t\t\t\tAuthorization: `Bearer ${initializeAssetsResponse.jwt}`,\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tbody: payload.map((x) => JSON.stringify(x)).join(\"\\n\"),\n+\t\t\t\t\t}\n+\t\t\t\t);\n+\t\t\t\tlogger.info(\n+\t\t\t\t\t`Uploaded bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}`\n+\t\t\t\t);\n+\t\t\t\treturn res;\n+\t\t\t} catch (e) {\n+\t\t\t\tif (attempts < MAX_UPLOAD_ATTEMPTS) {\n+\t\t\t\t\tlogger.info(\n+\t\t\t\t\t\tchalk.dim(\n+\t\t\t\t\t\t\t`Bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length} upload failed. Retrying...\\n`,\n+\t\t\t\t\t\t\te\n+\t\t\t\t\t\t)\n+\t\t\t\t\t);\n+\t\t\t\t\t// Exponential backoff, 1 second first time, then 2 second, then 4 second etc.\n+\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, attempts) * 1000)\n+\t\t\t\t\t);\n+\t\t\t\t\tif (e instanceof APIError && e.isGatewayError()) {\n+\t\t\t\t\t\t// Gateway problem, wait for some additional time and set concurrency to 1\n+\t\t\t\t\t\tqueue.concurrency = 1;\n+\t\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, gatewayErrors) * 5000)\n+\t\t\t\t\t\t);\n+\t\t\t\t\t\tgatewayErrors++;\n+\t\t\t\t\t\t// only count as a failed attempt after a few initial gateway errors\n+\t\t\t\t\t\tif (gatewayErrors >= MAX_UPLOAD_GATEWAY_ERRORS) {\n+\t\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn doUpload();\n+\t\t\t\t} else if (isJwtExpired(initializeAssetsResponse.jwt)) {\n+\t\t\t\t\tthrow new FatalError(\n+\t\t\t\t\t\t`Asset upload took too long on bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}. Please try again.\n+\t\t\t\t\t\tAssets already uploaded have been saved, so the next attempt will automatically resume from this point.`\n+\t\t\t\t\t);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow e;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t};\n+\t\t// add to queue and run it if we haven't reached concurrency limit\n+\t\tvoid queue.add(() =>\n+\t\t\tdoUpload().then((res) => {\n+\t\t\t\tcompletionJwt = res.jwt || completionJwt;\n+\t\t\t})\n+\t\t);\n+\t}\n+\tqueue.on(\"error\", (error) => {\n+\t\tlogger.error(error.message);\n+\t\tthrow error;\n+\t});\n+\tawait queue.onIdle();\n+\n+\t// if queue finishes without receiving JWT from asset upload service (AUS)\n+\t// AUS only returns this in the final bucket upload response\n+\tif (!completionJwt) {\n+\t\tthrow new FatalError(\n+\t\t\t\"Failed to complete asset upload. Please try again.\",\n+\t\t\t1\n+\t\t);\n+\t}\n+\n+\tconst uploadMs = Date.now() - start;\n+\tconst skipped = Object.keys(manifest).length - numberFilesToUpload;\n+\tconst skippedMessage = skipped > 0 ? `(${skipped} already uploaded) ` : \"\";\n+\n+\tlogger.log(\n+\t\t`\u2728 Success! Uploaded ${numberFilesToUpload} file(s) ${skippedMessage}${formatTime(uploadMs)}\\n`\n+\t);\n+\n+\treturn completionJwt;\n+};\n+\n+// modified from /pages/validate.tsx\n+const walk = async (\n+\tdir: string,\n+\tmanifest: AssetManifest,\n+\tstartingDir: string = dir\n+) => {\n+\tconst files = await readdir(dir);\n+\n+\tlet counter = 0;\n+\tawait Promise.all(\n+\t\tfiles.map(async (file) => {\n+\t\t\tconst filepath = path.join(dir, file);\n+\t\t\tconst relativeFilepath = path.relative(startingDir, filepath);\n+\t\t\tconst filestat = await stat(filepath);\n+\n+\t\t\tif (filestat.isSymbolicLink()) {\n+\t\t\t\treturn;\n+\t\t\t}\n+\n+\t\t\tif (filestat.isDirectory()) {\n+\t\t\t\tmanifest = await walk(filepath, manifest, startingDir);\n+\t\t\t} else {\n+\t\t\t\tif (counter >= MAX_ASSET_COUNT) {\n+\t\t\t\t\tthrow new UserError(\n+\t\t\t\t\t\t`Max asset count exceeded: ${counter.toLocaleString()} files found.\n+\t\t\t\t\t\tYou cannot have more than ${MAX_ASSET_COUNT.toLocaleString()} files in a deployment.\n+\t\t\t\t\t\tEnsure you have specified your build output directory correctly (currently set as ${startingDir}).`",
        "comment_created_at": "2024-08-07T16:51:55+00:00",
        "comment_author": "CarmenPopoviciu",
        "comment_body": "> `Ensure you have specified your build output directory correctly (currently set as ${startingDir}).`\r\n\r\nthe action we are recommending here feels a bit misleading to me. We're pointing the problem at the assets directory, when in fact it's the number of files that's the problem. I expect in most cases the assets directory specififed by users will be the correct one, and the action they need to take is to restrict the number of assets in said directory. To that effect, I wonder if a better action here would be\r\n\r\n\r\n```suggestion\r\n\t\t\t\t\tthrow new UserError(\r\n\t\t\t\t\t\t`Maximum number of assets exceeded.\r\n\t\t\t\t\t\tCloudflare Workers supports up to ${MAX_ASSET_COUNT.toLocaleString()} assets in a version. We found ${counter.toLocaleString()} files in the specified assets directory \"${startingDir}\".\r\n\t\t\t\t\t\tEnsure your assets directory contains a maximum of ${MAX_ASSET_COUNT.toLocaleString()} files, or that you have specified your assets directory correctly.`\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1707458650",
    "pr_number": 6374,
    "pr_file": "packages/wrangler/src/experimental-assets.ts",
    "created_at": "2024-08-07T17:00:06+00:00",
    "commented_code": "+import assert from \"node:assert\";\n import { existsSync } from \"node:fs\";\n-import path from \"node:path\";\n-import { UserError } from \"./errors\";\n+import { readdir, readFile, stat } from \"node:fs/promises\";\n+import * as path from \"node:path\";\n+import chalk from \"chalk\";\n+import { getType } from \"mime\";\n+import PQueue from \"p-queue\";\n+import prettyBytes from \"pretty-bytes\";\n+import { fetchResult } from \"./cfetch\";\n+import { formatTime } from \"./deploy/deploy\";\n+import { FatalError, UserError } from \"./errors\";\n+import { logger, LOGGER_LEVELS } from \"./logger\";\n+import { hashFile } from \"./pages/hash\";\n+import { isJwtExpired } from \"./pages/upload\";\n+import { APIError } from \"./parse\";\n+import { urlSafe } from \"./sites\";\n import type { Config } from \"./config\";\n \n+export type AssetManifest = { [path: string]: { hash: string; size: number } };\n+\n+type InitializeAssetsResponse = {\n+\t// string of file hashes per bucket\n+\tbuckets: string[][];\n+\tjwt: string;\n+};\n+\n+export type UploadPayloadFile = {\n+\tbase64: boolean;\n+\tkey: string;\n+\tvalue: string;\n+\tmetadata: {\n+\t\tcontentType: string;\n+\t};\n+};\n+\n+type UploadResponse = {\n+\tjwt?: string;\n+};\n+\n+// constants same as Pages for now\n+const BULK_UPLOAD_CONCURRENCY = 3;\n+const MAX_ASSET_COUNT = 20_000;\n+const MAX_ASSET_SIZE = 25 * 1024 * 1024;\n+const MAX_UPLOAD_ATTEMPTS = 5;\n+const MAX_UPLOAD_GATEWAY_ERRORS = 5;\n+\n+export const syncExperimentalAssets = async (\n+\taccountId: string | undefined,\n+\tscriptName: string,\n+\tassetDirectory: string | undefined,\n+\tdryRun: boolean | undefined\n+): Promise<string | undefined> => {\n+\tif (assetDirectory === undefined) {\n+\t\treturn;\n+\t}\n+\tif (dryRun) {\n+\t\tlogger.log(\"(Note: doing a dry run, not uploading or deleting anything.)\");\n+\t\treturn;\n+\t}\n+\tassert(accountId, \"Missing accountId\");\n+\n+\t// 1. generate asset manifest\n+\tlogger.info(\"\ud83c\udf00 Building list of assets...\");\n+\tconst manifest = await walk(assetDirectory, {});\n+\n+\t// 2. fetch buckets w/ hashes\n+\tlogger.info(\"\ud83c\udf00 Starting asset upload...\");\n+\tconst initializeAssetsResponse = await fetchResult<InitializeAssetsResponse>(\n+\t\t`/accounts/${accountId}/workers/scripts/${scriptName}/assets-upload-session`,\n+\t\t{\n+\t\t\theaders: { \"Content-Type\": \"application/json\" },\n+\t\t\tmethod: \"POST\",\n+\t\t\tbody: JSON.stringify({ manifest: manifest }),\n+\t\t}\n+\t);\n+\n+\t// if nothing to upload, return\n+\tif (initializeAssetsResponse.buckets.flat().length === 0) {\n+\t\tif (!initializeAssetsResponse.jwt) {\n+\t\t\tthrow new FatalError(\n+\t\t\t\t\"Could not find assets information to attach to deployment. Please try again.\",\n+\t\t\t\t1\n+\t\t\t);\n+\t\t}\n+\t\tlogger.info(`\u2728 Success! (No files to upload)`);\n+\t\treturn initializeAssetsResponse.jwt;\n+\t}\n+\n+\t// 3. fill buckets and upload assets\n+\tconst numberFilesToUpload = initializeAssetsResponse.buckets.flat().length;\n+\tlogger.info(`\ud83c\udf00 Uploading ${numberFilesToUpload} file(s)...`);\n+\n+\t// Create the buckets outside of doUpload so we can retry without losing track of potential duplicate files\n+\t// But don't add the actual content until uploading so we don't run out of memory\n+\tconst manifestLookup = Object.entries(manifest);\n+\tlet assetLogCount = 0;\n+\tconst bucketSkeletons = initializeAssetsResponse.buckets.map((bucket) => {\n+\t\treturn bucket.map((fileHash) => {\n+\t\t\tconst manifestEntry = manifestLookup.find(\n+\t\t\t\t(file) => file[1].hash === fileHash\n+\t\t\t);\n+\t\t\tif (manifestEntry === undefined) {\n+\t\t\t\tthrow new FatalError(\n+\t\t\t\t\t`A file was requested that does not appear to exist.`,\n+\t\t\t\t\t1\n+\t\t\t\t);\n+\t\t\t}\n+\t\t\t// just logging file uploads at the moment...\n+\t\t\t// unsure how to log deletion vs unchanged file ignored/if we want to log this\n+\t\t\tassetLogCount = logAssetUpload(`+ ${manifestEntry[0]}`, assetLogCount);\n+\t\t\treturn manifestEntry;\n+\t\t});\n+\t});\n+\n+\tconst queue = new PQueue({ concurrency: BULK_UPLOAD_CONCURRENCY });\n+\tlet attempts = 0;\n+\tconst start = Date.now();\n+\tlet completionJwt = \"\";\n+\n+\tfor (const [bucketIndex, bucketSkeleton] of bucketSkeletons.entries()) {\n+\t\tattempts = 0;\n+\t\tlet gatewayErrors = 0;\n+\t\tconst doUpload = async (): Promise<UploadResponse> => {\n+\t\t\t// Populate the payload only when actually uploading (this is limited to 3 concurrent uploads at 50 MiB per bucket meaning we'd only load in a max of ~150 MiB)\n+\t\t\t// This is so we don't run out of memory trying to upload the files.\n+\t\t\tconst payload: UploadPayloadFile[] = await Promise.all(\n+\t\t\t\tbucketSkeleton.map(async (manifestEntry) => {\n+\t\t\t\t\tconst absFilePath = path.join(assetDirectory, manifestEntry[0]);\n+\t\t\t\t\treturn {\n+\t\t\t\t\t\tbase64: true,\n+\t\t\t\t\t\tkey: manifestEntry[1].hash,\n+\t\t\t\t\t\tmetadata: {\n+\t\t\t\t\t\t\tcontentType: getType(absFilePath) || \"application/octet-stream\",\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tvalue: (await readFile(absFilePath)).toString(\"base64\"),\n+\t\t\t\t\t};\n+\t\t\t\t})\n+\t\t\t);\n+\n+\t\t\ttry {\n+\t\t\t\tconst res = await fetchResult<UploadResponse>(\n+\t\t\t\t\t`/accounts/${accountId}/workers/assets/upload`,\n+\t\t\t\t\t{\n+\t\t\t\t\t\tmethod: \"POST\",\n+\t\t\t\t\t\theaders: {\n+\t\t\t\t\t\t\t\"Content-Type\": \"application/x-ndjson\",\n+\t\t\t\t\t\t\tAuthorization: `Bearer ${initializeAssetsResponse.jwt}`,\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tbody: payload.map((x) => JSON.stringify(x)).join(\"\n\"),\n+\t\t\t\t\t}\n+\t\t\t\t);\n+\t\t\t\tlogger.info(\n+\t\t\t\t\t`Uploaded bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}`\n+\t\t\t\t);\n+\t\t\t\treturn res;\n+\t\t\t} catch (e) {\n+\t\t\t\tif (attempts < MAX_UPLOAD_ATTEMPTS) {\n+\t\t\t\t\tlogger.info(\n+\t\t\t\t\t\tchalk.dim(\n+\t\t\t\t\t\t\t`Bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length} upload failed. Retrying...\n`,\n+\t\t\t\t\t\t\te\n+\t\t\t\t\t\t)\n+\t\t\t\t\t);\n+\t\t\t\t\t// Exponential backoff, 1 second first time, then 2 second, then 4 second etc.\n+\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, attempts) * 1000)\n+\t\t\t\t\t);\n+\t\t\t\t\tif (e instanceof APIError && e.isGatewayError()) {\n+\t\t\t\t\t\t// Gateway problem, wait for some additional time and set concurrency to 1\n+\t\t\t\t\t\tqueue.concurrency = 1;\n+\t\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, gatewayErrors) * 5000)\n+\t\t\t\t\t\t);\n+\t\t\t\t\t\tgatewayErrors++;\n+\t\t\t\t\t\t// only count as a failed attempt after a few initial gateway errors\n+\t\t\t\t\t\tif (gatewayErrors >= MAX_UPLOAD_GATEWAY_ERRORS) {\n+\t\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn doUpload();\n+\t\t\t\t} else if (isJwtExpired(initializeAssetsResponse.jwt)) {\n+\t\t\t\t\tthrow new FatalError(\n+\t\t\t\t\t\t`Asset upload took too long on bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}. Please try again.\n+\t\t\t\t\t\tAssets already uploaded have been saved, so the next attempt will automatically resume from this point.`\n+\t\t\t\t\t);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow e;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t};\n+\t\t// add to queue and run it if we haven't reached concurrency limit\n+\t\tvoid queue.add(() =>\n+\t\t\tdoUpload().then((res) => {\n+\t\t\t\tcompletionJwt = res.jwt || completionJwt;\n+\t\t\t})\n+\t\t);\n+\t}\n+\tqueue.on(\"error\", (error) => {\n+\t\tlogger.error(error.message);\n+\t\tthrow error;\n+\t});\n+\tawait queue.onIdle();\n+\n+\t// if queue finishes without receiving JWT from asset upload service (AUS)\n+\t// AUS only returns this in the final bucket upload response\n+\tif (!completionJwt) {\n+\t\tthrow new FatalError(\n+\t\t\t\"Failed to complete asset upload. Please try again.\",\n+\t\t\t1\n+\t\t);\n+\t}\n+\n+\tconst uploadMs = Date.now() - start;\n+\tconst skipped = Object.keys(manifest).length - numberFilesToUpload;\n+\tconst skippedMessage = skipped > 0 ? `(${skipped} already uploaded) ` : \"\";\n+\n+\tlogger.log(\n+\t\t`\u2728 Success! Uploaded ${numberFilesToUpload} file(s) ${skippedMessage}${formatTime(uploadMs)}\n`\n+\t);\n+\n+\treturn completionJwt;\n+};\n+\n+// modified from /pages/validate.tsx\n+const walk = async (\n+\tdir: string,\n+\tmanifest: AssetManifest,\n+\tstartingDir: string = dir\n+) => {\n+\tconst files = await readdir(dir);\n+\n+\tlet counter = 0;\n+\tawait Promise.all(\n+\t\tfiles.map(async (file) => {\n+\t\t\tconst filepath = path.join(dir, file);\n+\t\t\tconst relativeFilepath = path.relative(startingDir, filepath);\n+\t\t\tconst filestat = await stat(filepath);\n+\n+\t\t\tif (filestat.isSymbolicLink()) {\n+\t\t\t\treturn;\n+\t\t\t}\n+\n+\t\t\tif (filestat.isDirectory()) {\n+\t\t\t\tmanifest = await walk(filepath, manifest, startingDir);\n+\t\t\t} else {\n+\t\t\t\tif (counter >= MAX_ASSET_COUNT) {\n+\t\t\t\t\tthrow new UserError(\n+\t\t\t\t\t\t`Max asset count exceeded: ${counter.toLocaleString()} files found.\n+\t\t\t\t\t\tYou cannot have more than ${MAX_ASSET_COUNT.toLocaleString()} files in a deployment.\n+\t\t\t\t\t\tEnsure you have specified your build output directory correctly (currently set as ${startingDir}).`\n+\t\t\t\t\t);\n+\t\t\t\t}\n+\n+\t\t\t\tconst name = urlSafe(relativeFilepath);\n+\t\t\t\tif (filestat.size > MAX_ASSET_SIZE) {\n+\t\t\t\t\tthrow new UserError(",
    "repo_full_name": "cloudflare/workers-sdk",
    "discussion_comments": [
      {
        "comment_id": "1707458650",
        "repo_full_name": "cloudflare/workers-sdk",
        "pr_number": 6374,
        "pr_file": "packages/wrangler/src/experimental-assets.ts",
        "discussion_id": "1707458650",
        "commented_code": "@@ -1,8 +1,299 @@\n+import assert from \"node:assert\";\n import { existsSync } from \"node:fs\";\n-import path from \"node:path\";\n-import { UserError } from \"./errors\";\n+import { readdir, readFile, stat } from \"node:fs/promises\";\n+import * as path from \"node:path\";\n+import chalk from \"chalk\";\n+import { getType } from \"mime\";\n+import PQueue from \"p-queue\";\n+import prettyBytes from \"pretty-bytes\";\n+import { fetchResult } from \"./cfetch\";\n+import { formatTime } from \"./deploy/deploy\";\n+import { FatalError, UserError } from \"./errors\";\n+import { logger, LOGGER_LEVELS } from \"./logger\";\n+import { hashFile } from \"./pages/hash\";\n+import { isJwtExpired } from \"./pages/upload\";\n+import { APIError } from \"./parse\";\n+import { urlSafe } from \"./sites\";\n import type { Config } from \"./config\";\n \n+export type AssetManifest = { [path: string]: { hash: string; size: number } };\n+\n+type InitializeAssetsResponse = {\n+\t// string of file hashes per bucket\n+\tbuckets: string[][];\n+\tjwt: string;\n+};\n+\n+export type UploadPayloadFile = {\n+\tbase64: boolean;\n+\tkey: string;\n+\tvalue: string;\n+\tmetadata: {\n+\t\tcontentType: string;\n+\t};\n+};\n+\n+type UploadResponse = {\n+\tjwt?: string;\n+};\n+\n+// constants same as Pages for now\n+const BULK_UPLOAD_CONCURRENCY = 3;\n+const MAX_ASSET_COUNT = 20_000;\n+const MAX_ASSET_SIZE = 25 * 1024 * 1024;\n+const MAX_UPLOAD_ATTEMPTS = 5;\n+const MAX_UPLOAD_GATEWAY_ERRORS = 5;\n+\n+export const syncExperimentalAssets = async (\n+\taccountId: string | undefined,\n+\tscriptName: string,\n+\tassetDirectory: string | undefined,\n+\tdryRun: boolean | undefined\n+): Promise<string | undefined> => {\n+\tif (assetDirectory === undefined) {\n+\t\treturn;\n+\t}\n+\tif (dryRun) {\n+\t\tlogger.log(\"(Note: doing a dry run, not uploading or deleting anything.)\");\n+\t\treturn;\n+\t}\n+\tassert(accountId, \"Missing accountId\");\n+\n+\t// 1. generate asset manifest\n+\tlogger.info(\"\ud83c\udf00 Building list of assets...\");\n+\tconst manifest = await walk(assetDirectory, {});\n+\n+\t// 2. fetch buckets w/ hashes\n+\tlogger.info(\"\ud83c\udf00 Starting asset upload...\");\n+\tconst initializeAssetsResponse = await fetchResult<InitializeAssetsResponse>(\n+\t\t`/accounts/${accountId}/workers/scripts/${scriptName}/assets-upload-session`,\n+\t\t{\n+\t\t\theaders: { \"Content-Type\": \"application/json\" },\n+\t\t\tmethod: \"POST\",\n+\t\t\tbody: JSON.stringify({ manifest: manifest }),\n+\t\t}\n+\t);\n+\n+\t// if nothing to upload, return\n+\tif (initializeAssetsResponse.buckets.flat().length === 0) {\n+\t\tif (!initializeAssetsResponse.jwt) {\n+\t\t\tthrow new FatalError(\n+\t\t\t\t\"Could not find assets information to attach to deployment. Please try again.\",\n+\t\t\t\t1\n+\t\t\t);\n+\t\t}\n+\t\tlogger.info(`\u2728 Success! (No files to upload)`);\n+\t\treturn initializeAssetsResponse.jwt;\n+\t}\n+\n+\t// 3. fill buckets and upload assets\n+\tconst numberFilesToUpload = initializeAssetsResponse.buckets.flat().length;\n+\tlogger.info(`\ud83c\udf00 Uploading ${numberFilesToUpload} file(s)...`);\n+\n+\t// Create the buckets outside of doUpload so we can retry without losing track of potential duplicate files\n+\t// But don't add the actual content until uploading so we don't run out of memory\n+\tconst manifestLookup = Object.entries(manifest);\n+\tlet assetLogCount = 0;\n+\tconst bucketSkeletons = initializeAssetsResponse.buckets.map((bucket) => {\n+\t\treturn bucket.map((fileHash) => {\n+\t\t\tconst manifestEntry = manifestLookup.find(\n+\t\t\t\t(file) => file[1].hash === fileHash\n+\t\t\t);\n+\t\t\tif (manifestEntry === undefined) {\n+\t\t\t\tthrow new FatalError(\n+\t\t\t\t\t`A file was requested that does not appear to exist.`,\n+\t\t\t\t\t1\n+\t\t\t\t);\n+\t\t\t}\n+\t\t\t// just logging file uploads at the moment...\n+\t\t\t// unsure how to log deletion vs unchanged file ignored/if we want to log this\n+\t\t\tassetLogCount = logAssetUpload(`+ ${manifestEntry[0]}`, assetLogCount);\n+\t\t\treturn manifestEntry;\n+\t\t});\n+\t});\n+\n+\tconst queue = new PQueue({ concurrency: BULK_UPLOAD_CONCURRENCY });\n+\tlet attempts = 0;\n+\tconst start = Date.now();\n+\tlet completionJwt = \"\";\n+\n+\tfor (const [bucketIndex, bucketSkeleton] of bucketSkeletons.entries()) {\n+\t\tattempts = 0;\n+\t\tlet gatewayErrors = 0;\n+\t\tconst doUpload = async (): Promise<UploadResponse> => {\n+\t\t\t// Populate the payload only when actually uploading (this is limited to 3 concurrent uploads at 50 MiB per bucket meaning we'd only load in a max of ~150 MiB)\n+\t\t\t// This is so we don't run out of memory trying to upload the files.\n+\t\t\tconst payload: UploadPayloadFile[] = await Promise.all(\n+\t\t\t\tbucketSkeleton.map(async (manifestEntry) => {\n+\t\t\t\t\tconst absFilePath = path.join(assetDirectory, manifestEntry[0]);\n+\t\t\t\t\treturn {\n+\t\t\t\t\t\tbase64: true,\n+\t\t\t\t\t\tkey: manifestEntry[1].hash,\n+\t\t\t\t\t\tmetadata: {\n+\t\t\t\t\t\t\tcontentType: getType(absFilePath) || \"application/octet-stream\",\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tvalue: (await readFile(absFilePath)).toString(\"base64\"),\n+\t\t\t\t\t};\n+\t\t\t\t})\n+\t\t\t);\n+\n+\t\t\ttry {\n+\t\t\t\tconst res = await fetchResult<UploadResponse>(\n+\t\t\t\t\t`/accounts/${accountId}/workers/assets/upload`,\n+\t\t\t\t\t{\n+\t\t\t\t\t\tmethod: \"POST\",\n+\t\t\t\t\t\theaders: {\n+\t\t\t\t\t\t\t\"Content-Type\": \"application/x-ndjson\",\n+\t\t\t\t\t\t\tAuthorization: `Bearer ${initializeAssetsResponse.jwt}`,\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tbody: payload.map((x) => JSON.stringify(x)).join(\"\\n\"),\n+\t\t\t\t\t}\n+\t\t\t\t);\n+\t\t\t\tlogger.info(\n+\t\t\t\t\t`Uploaded bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}`\n+\t\t\t\t);\n+\t\t\t\treturn res;\n+\t\t\t} catch (e) {\n+\t\t\t\tif (attempts < MAX_UPLOAD_ATTEMPTS) {\n+\t\t\t\t\tlogger.info(\n+\t\t\t\t\t\tchalk.dim(\n+\t\t\t\t\t\t\t`Bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length} upload failed. Retrying...\\n`,\n+\t\t\t\t\t\t\te\n+\t\t\t\t\t\t)\n+\t\t\t\t\t);\n+\t\t\t\t\t// Exponential backoff, 1 second first time, then 2 second, then 4 second etc.\n+\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, attempts) * 1000)\n+\t\t\t\t\t);\n+\t\t\t\t\tif (e instanceof APIError && e.isGatewayError()) {\n+\t\t\t\t\t\t// Gateway problem, wait for some additional time and set concurrency to 1\n+\t\t\t\t\t\tqueue.concurrency = 1;\n+\t\t\t\t\t\tawait new Promise((resolvePromise) =>\n+\t\t\t\t\t\t\tsetTimeout(resolvePromise, Math.pow(2, gatewayErrors) * 5000)\n+\t\t\t\t\t\t);\n+\t\t\t\t\t\tgatewayErrors++;\n+\t\t\t\t\t\t// only count as a failed attempt after a few initial gateway errors\n+\t\t\t\t\t\tif (gatewayErrors >= MAX_UPLOAD_GATEWAY_ERRORS) {\n+\t\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tattempts++;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn doUpload();\n+\t\t\t\t} else if (isJwtExpired(initializeAssetsResponse.jwt)) {\n+\t\t\t\t\tthrow new FatalError(\n+\t\t\t\t\t\t`Asset upload took too long on bucket ${bucketIndex + 1}/${initializeAssetsResponse.buckets.length}. Please try again.\n+\t\t\t\t\t\tAssets already uploaded have been saved, so the next attempt will automatically resume from this point.`\n+\t\t\t\t\t);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow e;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t};\n+\t\t// add to queue and run it if we haven't reached concurrency limit\n+\t\tvoid queue.add(() =>\n+\t\t\tdoUpload().then((res) => {\n+\t\t\t\tcompletionJwt = res.jwt || completionJwt;\n+\t\t\t})\n+\t\t);\n+\t}\n+\tqueue.on(\"error\", (error) => {\n+\t\tlogger.error(error.message);\n+\t\tthrow error;\n+\t});\n+\tawait queue.onIdle();\n+\n+\t// if queue finishes without receiving JWT from asset upload service (AUS)\n+\t// AUS only returns this in the final bucket upload response\n+\tif (!completionJwt) {\n+\t\tthrow new FatalError(\n+\t\t\t\"Failed to complete asset upload. Please try again.\",\n+\t\t\t1\n+\t\t);\n+\t}\n+\n+\tconst uploadMs = Date.now() - start;\n+\tconst skipped = Object.keys(manifest).length - numberFilesToUpload;\n+\tconst skippedMessage = skipped > 0 ? `(${skipped} already uploaded) ` : \"\";\n+\n+\tlogger.log(\n+\t\t`\u2728 Success! Uploaded ${numberFilesToUpload} file(s) ${skippedMessage}${formatTime(uploadMs)}\\n`\n+\t);\n+\n+\treturn completionJwt;\n+};\n+\n+// modified from /pages/validate.tsx\n+const walk = async (\n+\tdir: string,\n+\tmanifest: AssetManifest,\n+\tstartingDir: string = dir\n+) => {\n+\tconst files = await readdir(dir);\n+\n+\tlet counter = 0;\n+\tawait Promise.all(\n+\t\tfiles.map(async (file) => {\n+\t\t\tconst filepath = path.join(dir, file);\n+\t\t\tconst relativeFilepath = path.relative(startingDir, filepath);\n+\t\t\tconst filestat = await stat(filepath);\n+\n+\t\t\tif (filestat.isSymbolicLink()) {\n+\t\t\t\treturn;\n+\t\t\t}\n+\n+\t\t\tif (filestat.isDirectory()) {\n+\t\t\t\tmanifest = await walk(filepath, manifest, startingDir);\n+\t\t\t} else {\n+\t\t\t\tif (counter >= MAX_ASSET_COUNT) {\n+\t\t\t\t\tthrow new UserError(\n+\t\t\t\t\t\t`Max asset count exceeded: ${counter.toLocaleString()} files found.\n+\t\t\t\t\t\tYou cannot have more than ${MAX_ASSET_COUNT.toLocaleString()} files in a deployment.\n+\t\t\t\t\t\tEnsure you have specified your build output directory correctly (currently set as ${startingDir}).`\n+\t\t\t\t\t);\n+\t\t\t\t}\n+\n+\t\t\t\tconst name = urlSafe(relativeFilepath);\n+\t\t\t\tif (filestat.size > MAX_ASSET_SIZE) {\n+\t\t\t\t\tthrow new UserError(",
        "comment_created_at": "2024-08-07T17:00:06+00:00",
        "comment_author": "CarmenPopoviciu",
        "comment_body": "\n```suggestion\n\t\t\t\t\tthrow new UserError(\n            `Asset too large.\\n` + \n            `Cloudflare Workers supports assets with sizes of up to ${prettyBytes(MAX_ASSET_SIZE, {\n\t\t\t\t\t\t\tbinary: true,\n\t\t\t\t\t\t})}. We found file ${filepath} with a size of ${prettyBytes(filestat.size, {\n\t\t\t\t\t\t\tbinary: true,\n\t\t\t\t\t\t})}.\\n`\n            `Ensure all assets in your assets directory \"${startDir}\" conform with the Workers maximum size requirement.`\n\t\t\t\t\t);\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1541867486",
    "pr_number": 5215,
    "pr_file": "packages/wrangler/src/dev/miniflare.ts",
    "created_at": "2024-03-27T19:04:59+00:00",
    "commented_code": "// where they're defined, and receives all requests from other Wrangler sessions\n // for this session's Durable Objects. Note the original request URL may contain\n // non-standard protocols, so we store it in a header to restore later.\n-const EXTERNAL_DURABLE_OBJECTS_WORKER_NAME =\n+// It also provides stub classes for services that couldn't be found, for\n+// improved error messages when trying to call RPC methods.\n+const EXTERNAL_SERVICE_WORKER_NAME =\n \t\"__WRANGLER_EXTERNAL_DURABLE_OBJECTS_WORKER\";\n-// TODO(someday): could we do some sort of `Proxy`-prototype thing here\n-//  for a nice error message if a user tried to call an RPC method?\n-const EXTERNAL_DURABLE_OBJECTS_WORKER_SCRIPT = `\n+const EXTERNAL_SERVICE_WORKER_SCRIPT = `\n+import { DurableObject, WorkerEntrypoint } from \"cloudflare:workers\";\n+\n const HEADER_URL = \"X-Miniflare-Durable-Object-URL\";\n const HEADER_NAME = \"X-Miniflare-Durable-Object-Name\";\n const HEADER_ID = \"X-Miniflare-Durable-Object-Id\";\n const HEADER_CF_BLOB = \"X-Miniflare-Durable-Object-Cf-Blob\";\n \n-function createClass({ className, proxyUrl }) {\n-\treturn class {\n-\t\tconstructor(state) {\n-\t\t\tthis.id = state.id.toString();\n-\t\t}\n-\t\tfetch(request) {\n-\t\t\tif (proxyUrl === undefined) {\n-\t\t\t\treturn new Response(\\`[wrangler] Couldn't find \\\\\\`wrangler dev\\\\\\` session for class \"\\${className}\" to proxy to\\`, { status: 503 });\n+const HANDLER_RESERVED_KEYS = new Set([\n+\t\"tail\",\n+\t\"trace\",\n+\t\"scheduled\",\n+\t\"alarm\",\n+\t\"test\",\n+\t\"webSocketMessage\",\n+\t\"webSocketClose\",\n+\t\"webSocketError\",\n+\t\"self\",\n+]);\n+\n+function createProxyPrototypeClass(handlerSuperKlass, getUnknownPrototypeKey) {\n+\t// Build a class with a \"Proxy\"-prototype, so we can intercept RPC calls and\n+\t// throw unsupported exceptions :see_no_evil:\n+\tfunction klass(ctx, env) {\n+\t\t// Delay proxying prototype until construction, so workerd sees this as a\n+\t\t// regular class when introspecting it. This check fails if we don't do this:\n+\t\t// https://github.com/cloudflare/workerd/blob/9e915ed637d65adb3c57522607d2cd8b8d692b6b/src/workerd/io/worker.c%2B%2B#L1920-L1921\n+\t\tklass.prototype = new Proxy(klass.prototype, {\n+\t\t\tget(target, key, receiver) {\n+\t\t\t\tconst value = Reflect.get(target, key, receiver);\n+\t\t\t\tif (value !== undefined) return value;\n+\t\t\t\tif (HANDLER_RESERVED_KEYS.has(key)) return;\n+\t\t\t\treturn getUnknownPrototypeKey(key);\n \t\t\t}\n-\t\t\tconst proxyRequest = new Request(proxyUrl, request);\n-\t\t\tproxyRequest.headers.set(HEADER_URL, request.url);\n-\t\t\tproxyRequest.headers.set(HEADER_NAME, className);\n-\t\t\tproxyRequest.headers.set(HEADER_ID, this.id);\n-\t\t\tproxyRequest.headers.set(HEADER_CF_BLOB, JSON.stringify(request.cf));\n-\t\t\treturn fetch(proxyRequest);\n-\t\t}\n+\t\t});\n+\t\n+\t\treturn Reflect.construct(handlerSuperKlass, [ctx, env], klass);\n \t}\n+\t\t\n+\tReflect.setPrototypeOf(klass.prototype, handlerSuperKlass.prototype);\n+\tReflect.setPrototypeOf(klass, handlerSuperKlass);\n+\n+\treturn klass;\n+}\n+\n+function createDurableObjectClass({ className, proxyUrl }) {\n+\tconst klass = createProxyPrototypeClass(DurableObject, (key) => {\n+\t\tthrow new Error(\\`Cannot access \\\\\\`\\${className}#\\${key}\\\\\\` as Durable Object RPC is not yet supported between multiple \\\\\\`wrangler dev\\\\\\` sessions. We recommend you only bind to Durable Objects defined in the same Worker. You can define an entrypoint sub-classing \\\\\\`WorkerEntrypoint\\\\\\` to expose an RPC interface between Workers.\\`);",
    "repo_full_name": "cloudflare/workers-sdk",
    "discussion_comments": [
      {
        "comment_id": "1541867486",
        "repo_full_name": "cloudflare/workers-sdk",
        "pr_number": 5215,
        "pr_file": "packages/wrangler/src/dev/miniflare.ts",
        "discussion_id": "1541867486",
        "commented_code": "@@ -53,33 +53,88 @@ import type { Readable } from \"node:stream\";\n // where they're defined, and receives all requests from other Wrangler sessions\n // for this session's Durable Objects. Note the original request URL may contain\n // non-standard protocols, so we store it in a header to restore later.\n-const EXTERNAL_DURABLE_OBJECTS_WORKER_NAME =\n+// It also provides stub classes for services that couldn't be found, for\n+// improved error messages when trying to call RPC methods.\n+const EXTERNAL_SERVICE_WORKER_NAME =\n \t\"__WRANGLER_EXTERNAL_DURABLE_OBJECTS_WORKER\";\n-// TODO(someday): could we do some sort of `Proxy`-prototype thing here\n-//  for a nice error message if a user tried to call an RPC method?\n-const EXTERNAL_DURABLE_OBJECTS_WORKER_SCRIPT = `\n+const EXTERNAL_SERVICE_WORKER_SCRIPT = `\n+import { DurableObject, WorkerEntrypoint } from \"cloudflare:workers\";\n+\n const HEADER_URL = \"X-Miniflare-Durable-Object-URL\";\n const HEADER_NAME = \"X-Miniflare-Durable-Object-Name\";\n const HEADER_ID = \"X-Miniflare-Durable-Object-Id\";\n const HEADER_CF_BLOB = \"X-Miniflare-Durable-Object-Cf-Blob\";\n \n-function createClass({ className, proxyUrl }) {\n-\treturn class {\n-\t\tconstructor(state) {\n-\t\t\tthis.id = state.id.toString();\n-\t\t}\n-\t\tfetch(request) {\n-\t\t\tif (proxyUrl === undefined) {\n-\t\t\t\treturn new Response(\\`[wrangler] Couldn't find \\\\\\`wrangler dev\\\\\\` session for class \"\\${className}\" to proxy to\\`, { status: 503 });\n+const HANDLER_RESERVED_KEYS = new Set([\n+\t\"tail\",\n+\t\"trace\",\n+\t\"scheduled\",\n+\t\"alarm\",\n+\t\"test\",\n+\t\"webSocketMessage\",\n+\t\"webSocketClose\",\n+\t\"webSocketError\",\n+\t\"self\",\n+]);\n+\n+function createProxyPrototypeClass(handlerSuperKlass, getUnknownPrototypeKey) {\n+\t// Build a class with a \"Proxy\"-prototype, so we can intercept RPC calls and\n+\t// throw unsupported exceptions :see_no_evil:\n+\tfunction klass(ctx, env) {\n+\t\t// Delay proxying prototype until construction, so workerd sees this as a\n+\t\t// regular class when introspecting it. This check fails if we don't do this:\n+\t\t// https://github.com/cloudflare/workerd/blob/9e915ed637d65adb3c57522607d2cd8b8d692b6b/src/workerd/io/worker.c%2B%2B#L1920-L1921\n+\t\tklass.prototype = new Proxy(klass.prototype, {\n+\t\t\tget(target, key, receiver) {\n+\t\t\t\tconst value = Reflect.get(target, key, receiver);\n+\t\t\t\tif (value !== undefined) return value;\n+\t\t\t\tif (HANDLER_RESERVED_KEYS.has(key)) return;\n+\t\t\t\treturn getUnknownPrototypeKey(key);\n \t\t\t}\n-\t\t\tconst proxyRequest = new Request(proxyUrl, request);\n-\t\t\tproxyRequest.headers.set(HEADER_URL, request.url);\n-\t\t\tproxyRequest.headers.set(HEADER_NAME, className);\n-\t\t\tproxyRequest.headers.set(HEADER_ID, this.id);\n-\t\t\tproxyRequest.headers.set(HEADER_CF_BLOB, JSON.stringify(request.cf));\n-\t\t\treturn fetch(proxyRequest);\n-\t\t}\n+\t\t});\n+\t\n+\t\treturn Reflect.construct(handlerSuperKlass, [ctx, env], klass);\n \t}\n+\t\t\n+\tReflect.setPrototypeOf(klass.prototype, handlerSuperKlass.prototype);\n+\tReflect.setPrototypeOf(klass, handlerSuperKlass);\n+\n+\treturn klass;\n+}\n+\n+function createDurableObjectClass({ className, proxyUrl }) {\n+\tconst klass = createProxyPrototypeClass(DurableObject, (key) => {\n+\t\tthrow new Error(\\`Cannot access \\\\\\`\\${className}#\\${key}\\\\\\` as Durable Object RPC is not yet supported between multiple \\\\\\`wrangler dev\\\\\\` sessions. We recommend you only bind to Durable Objects defined in the same Worker. You can define an entrypoint sub-classing \\\\\\`WorkerEntrypoint\\\\\\` to expose an RPC interface between Workers.\\`);",
        "comment_created_at": "2024-03-27T19:04:59+00:00",
        "comment_author": "GregBrimble",
        "comment_body": "```suggestion\r\n\t\tthrow new Error(\\`Cannot access \\\\\\`\\${className}#\\${key}\\\\\\` as Durable Object RPC is not yet supported between multiple \\\\\\`wrangler dev\\\\\\` sessions.\\`);\r\n```",
        "pr_file_module": null
      }
    ]
  }
]