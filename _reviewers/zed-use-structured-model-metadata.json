[
  {
    "discussion_id": "2159569712",
    "pr_number": 33068,
    "pr_file": "crates/bedrock/src/models.rs",
    "created_at": "2025-06-20T19:38:52+00:00",
    "commented_code": "Model::Claude3_5Sonnet\n                 | Model::Claude3_5SonnetV2\n                 | Model::Claude3Haiku\n-                | Model::Claude3Sonnet,\n+                | Model::Claude3Sonnet\n+                | Model::Claude3_7Sonnet",
    "repo_full_name": "zed-industries/zed",
    "discussion_comments": [
      {
        "comment_id": "2159569712",
        "repo_full_name": "zed-industries/zed",
        "pr_number": 33068,
        "pr_file": "crates/bedrock/src/models.rs",
        "discussion_id": "2159569712",
        "commented_code": "@@ -496,7 +496,9 @@ impl Model {\n                 Model::Claude3_5Sonnet\n                 | Model::Claude3_5SonnetV2\n                 | Model::Claude3Haiku\n-                | Model::Claude3Sonnet,\n+                | Model::Claude3Sonnet\n+                | Model::Claude3_7Sonnet",
        "comment_created_at": "2025-06-20T19:38:52+00:00",
        "comment_author": "5herlocked",
        "comment_body": "Probably will make sense -- to also include `Claude3_7SonnetThinking` and `ClaudeSonnet4Thinking` since it's the same model.",
        "pr_file_module": null
      },
      {
        "comment_id": "2159806062",
        "repo_full_name": "zed-industries/zed",
        "pr_number": 33068,
        "pr_file": "crates/bedrock/src/models.rs",
        "discussion_id": "2159569712",
        "commented_code": "@@ -496,7 +496,9 @@ impl Model {\n                 Model::Claude3_5Sonnet\n                 | Model::Claude3_5SonnetV2\n                 | Model::Claude3Haiku\n-                | Model::Claude3Sonnet,\n+                | Model::Claude3Sonnet\n+                | Model::Claude3_7Sonnet",
        "comment_created_at": "2025-06-21T01:15:23+00:00",
        "comment_author": "willemt",
        "comment_body": "thanks I added them in a commit I just pushed",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2156290967",
    "pr_number": 33007,
    "pr_file": "crates/copilot/src/copilot_chat.rs",
    "created_at": "2025-06-19T07:03:56+00:00",
    "commented_code": ".into_iter()\n         .filter(|model| {\n             model.model_picker_enabled\n+                && model.capabilities.model_type.as_str() == \"chat\"",
    "repo_full_name": "zed-industries/zed",
    "discussion_comments": [
      {
        "comment_id": "2156290967",
        "repo_full_name": "zed-industries/zed",
        "pr_number": 33007,
        "pr_file": "crates/copilot/src/copilot_chat.rs",
        "discussion_id": "2156290967",
        "commented_code": "@@ -597,6 +608,7 @@ async fn get_models(\n         .into_iter()\n         .filter(|model| {\n             model.model_picker_enabled\n+                && model.capabilities.model_type.as_str() == \"chat\"",
        "comment_created_at": "2025-06-19T07:03:56+00:00",
        "comment_author": "imumesh18",
        "comment_body": "What's the value for model type are? Is it like chat,edit, agent?",
        "pr_file_module": null
      },
      {
        "comment_id": "2156305756",
        "repo_full_name": "zed-industries/zed",
        "pr_number": 33007,
        "pr_file": "crates/copilot/src/copilot_chat.rs",
        "discussion_id": "2156290967",
        "commented_code": "@@ -597,6 +608,7 @@ async fn get_models(\n         .into_iter()\n         .filter(|model| {\n             model.model_picker_enabled\n+                && model.capabilities.model_type.as_str() == \"chat\"",
        "comment_created_at": "2025-06-19T07:13:05+00:00",
        "comment_author": "lj3954",
        "comment_body": "I've seen \"chat\" and \"completion\". Without adding this check, \"GPT-4o Copilot\" would show up in the model picker, which is the model used for tab completions.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2050415117",
    "pr_number": 29027,
    "pr_file": "crates/copilot/src/copilot_chat.rs",
    "created_at": "2025-04-18T09:32:22+00:00",
    "commented_code": "System,\n }\n \n-#[cfg_attr(feature = \"schemars\", derive(schemars::JsonSchema))]\n-#[derive(Clone, Debug, Default, Serialize, Deserialize, PartialEq, EnumIter)]\n-pub enum Model {\n-    #[default]\n-    #[serde(alias = \"gpt-4o\", rename = \"gpt-4o-2024-05-13\")]\n-    Gpt4o,\n-    #[serde(alias = \"gpt-4\", rename = \"gpt-4\")]\n-    Gpt4,\n-    #[serde(alias = \"gpt-4.1\", rename = \"gpt-4.1\")]\n-    Gpt4_1,\n-    #[serde(alias = \"gpt-3.5-turbo\", rename = \"gpt-3.5-turbo\")]\n-    Gpt3_5Turbo,\n-    #[serde(alias = \"o1\", rename = \"o1\")]\n-    O1,\n-    #[serde(alias = \"o1-mini\", rename = \"o3-mini\")]\n-    O3Mini,\n-    #[serde(alias = \"o3\", rename = \"o3\")]\n-    O3,\n-    #[serde(alias = \"o4-mini\", rename = \"o4-mini\")]\n-    O4Mini,\n-    #[serde(alias = \"claude-3-5-sonnet\", rename = \"claude-3.5-sonnet\")]\n-    Claude3_5Sonnet,\n-    #[serde(alias = \"claude-3-7-sonnet\", rename = \"claude-3.7-sonnet\")]\n-    Claude3_7Sonnet,\n-    #[serde(\n-        alias = \"claude-3.7-sonnet-thought\",\n-        rename = \"claude-3.7-sonnet-thought\"\n-    )]\n-    Claude3_7SonnetThinking,\n-    #[serde(alias = \"gemini-2.0-flash\", rename = \"gemini-2.0-flash-001\")]\n-    Gemini20Flash,\n-    #[serde(alias = \"gemini-2.5-pro\", rename = \"gemini-2.5-pro\")]\n-    Gemini25Pro,\n+#[serde_with::serde_as]\n+#[derive(Deserialize)]\n+struct ModelSchema {\n+    #[serde_as(as = \"serde_with::VecSkipError<_>\")]\n+    data: Vec<Model>,\n+}\n+\n+#[derive(Clone, Serialize, Deserialize, Debug, Eq, PartialEq)]\n+pub struct Model {\n+    capabilities: ModelCapabilities,\n+    id: String,\n+    name: String,\n+    policy: Option<ModelPolicy>,\n+    vendor: ModelVendor,\n+    model_picker_enabled: bool,\n+}\n+\n+#[derive(Clone, Serialize, Deserialize, Debug, Eq, PartialEq)]\n+struct ModelCapabilities {\n+    family: String,\n+    #[serde(default)]\n+    limits: ModelLimits,\n+    supports: ModelSupportedFeatures,\n+}\n+\n+#[derive(Default, Clone, Serialize, Deserialize, Debug, Eq, PartialEq)]\n+struct ModelLimits {\n+    #[serde(default)]\n+    max_context_window_tokens: usize,\n+    #[serde(default)]\n+    max_output_tokens: usize,\n+    #[serde(default)]\n+    max_prompt_tokens: usize,\n+}\n+\n+#[derive(Clone, Serialize, Deserialize, Debug, Eq, PartialEq)]\n+struct ModelPolicy {\n+    state: String,\n+}\n+\n+#[derive(Clone, Serialize, Deserialize, Debug, Eq, PartialEq)]\n+struct ModelSupportedFeatures {\n+    #[serde(default)]\n+    streaming: bool,\n+    #[serde(default)]\n+    tool_calls: bool,\n+}\n+\n+#[derive(Clone, Copy, Serialize, Deserialize, Debug, Eq, PartialEq)]\n+pub enum ModelVendor {\n+    // Azure OpenAI should have no functional difference from OpenAI in Copilot Chat\n+    #[serde(alias = \"Azure OpenAI\")]\n+    OpenAI,\n+    Google,\n+    Anthropic,\n }\n \n impl Model {\n     pub fn uses_streaming(&self) -> bool {\n-        match self {\n-            Self::Gpt4o\n-            | Self::Gpt4\n-            | Self::Gpt4_1\n-            | Self::Gpt3_5Turbo\n-            | Self::O3\n-            | Self::O4Mini\n-            | Self::Claude3_5Sonnet\n-            | Self::Claude3_7Sonnet\n-            | Self::Claude3_7SonnetThinking => true,\n-            Self::O3Mini | Self::O1 | Self::Gemini20Flash | Self::Gemini25Pro => false,\n-        }\n+        self.capabilities.supports.streaming\n     }\n \n-    pub fn from_id(id: &str) -> Result<Self> {\n-        match id {\n-            \"gpt-4o\" => Ok(Self::Gpt4o),\n-            \"gpt-4\" => Ok(Self::Gpt4),\n-            \"gpt-4.1\" => Ok(Self::Gpt4_1),\n-            \"gpt-3.5-turbo\" => Ok(Self::Gpt3_5Turbo),\n-            \"o1\" => Ok(Self::O1),\n-            \"o3-mini\" => Ok(Self::O3Mini),\n-            \"o3\" => Ok(Self::O3),\n-            \"o4-mini\" => Ok(Self::O4Mini),\n-            \"claude-3-5-sonnet\" => Ok(Self::Claude3_5Sonnet),\n-            \"claude-3-7-sonnet\" => Ok(Self::Claude3_7Sonnet),\n-            \"claude-3.7-sonnet-thought\" => Ok(Self::Claude3_7SonnetThinking),\n-            \"gemini-2.0-flash-001\" => Ok(Self::Gemini20Flash),\n-            \"gemini-2.5-pro\" => Ok(Self::Gemini25Pro),\n-            _ => Err(anyhow!(\"Invalid model id: {}\", id)),\n-        }\n+    pub fn id(&self) -> &str {\n+        self.id.as_str()\n     }\n \n-    pub fn id(&self) -> &'static str {\n-        match self {\n-            Self::Gpt3_5Turbo => \"gpt-3.5-turbo\",\n-            Self::Gpt4 => \"gpt-4\",\n-            Self::Gpt4_1 => \"gpt-4.1\",\n-            Self::Gpt4o => \"gpt-4o\",\n-            Self::O3Mini => \"o3-mini\",\n-            Self::O1 => \"o1\",\n-            Self::O3 => \"o3\",\n-            Self::O4Mini => \"o4-mini\",\n-            Self::Claude3_5Sonnet => \"claude-3-5-sonnet\",\n-            Self::Claude3_7Sonnet => \"claude-3-7-sonnet\",\n-            Self::Claude3_7SonnetThinking => \"claude-3.7-sonnet-thought\",\n-            Self::Gemini20Flash => \"gemini-2.0-flash-001\",\n-            Self::Gemini25Pro => \"gemini-2.5-pro\",\n-        }\n+    pub fn display_name(&self) -> &str {\n+        self.name.as_str()\n     }\n \n-    pub fn display_name(&self) -> &'static str {\n-        match self {\n-            Self::Gpt3_5Turbo => \"GPT-3.5\",\n-            Self::Gpt4 => \"GPT-4\",\n-            Self::Gpt4_1 => \"GPT-4.1\",\n-            Self::Gpt4o => \"GPT-4o\",\n-            Self::O3Mini => \"o3-mini\",\n-            Self::O1 => \"o1\",\n-            Self::O3 => \"o3\",\n-            Self::O4Mini => \"o4-mini\",\n-            Self::Claude3_5Sonnet => \"Claude 3.5 Sonnet\",\n-            Self::Claude3_7Sonnet => \"Claude 3.7 Sonnet\",\n-            Self::Claude3_7SonnetThinking => \"Claude 3.7 Sonnet Thinking\",\n-            Self::Gemini20Flash => \"Gemini 2.0 Flash\",\n-            Self::Gemini25Pro => \"Gemini 2.5 Pro\",\n-        }\n+    pub fn max_token_count(&self) -> usize {\n+        self.capabilities.limits.max_output_tokens",
    "repo_full_name": "zed-industries/zed",
    "discussion_comments": [
      {
        "comment_id": "2050415117",
        "repo_full_name": "zed-industries/zed",
        "pr_number": 29027,
        "pr_file": "crates/copilot/src/copilot_chat.rs",
        "discussion_id": "2050415117",
        "commented_code": "@@ -25,128 +26,86 @@ pub enum Role {\n     System,\n }\n \n-#[cfg_attr(feature = \"schemars\", derive(schemars::JsonSchema))]\n-#[derive(Clone, Debug, Default, Serialize, Deserialize, PartialEq, EnumIter)]\n-pub enum Model {\n-    #[default]\n-    #[serde(alias = \"gpt-4o\", rename = \"gpt-4o-2024-05-13\")]\n-    Gpt4o,\n-    #[serde(alias = \"gpt-4\", rename = \"gpt-4\")]\n-    Gpt4,\n-    #[serde(alias = \"gpt-4.1\", rename = \"gpt-4.1\")]\n-    Gpt4_1,\n-    #[serde(alias = \"gpt-3.5-turbo\", rename = \"gpt-3.5-turbo\")]\n-    Gpt3_5Turbo,\n-    #[serde(alias = \"o1\", rename = \"o1\")]\n-    O1,\n-    #[serde(alias = \"o1-mini\", rename = \"o3-mini\")]\n-    O3Mini,\n-    #[serde(alias = \"o3\", rename = \"o3\")]\n-    O3,\n-    #[serde(alias = \"o4-mini\", rename = \"o4-mini\")]\n-    O4Mini,\n-    #[serde(alias = \"claude-3-5-sonnet\", rename = \"claude-3.5-sonnet\")]\n-    Claude3_5Sonnet,\n-    #[serde(alias = \"claude-3-7-sonnet\", rename = \"claude-3.7-sonnet\")]\n-    Claude3_7Sonnet,\n-    #[serde(\n-        alias = \"claude-3.7-sonnet-thought\",\n-        rename = \"claude-3.7-sonnet-thought\"\n-    )]\n-    Claude3_7SonnetThinking,\n-    #[serde(alias = \"gemini-2.0-flash\", rename = \"gemini-2.0-flash-001\")]\n-    Gemini20Flash,\n-    #[serde(alias = \"gemini-2.5-pro\", rename = \"gemini-2.5-pro\")]\n-    Gemini25Pro,\n+#[serde_with::serde_as]\n+#[derive(Deserialize)]\n+struct ModelSchema {\n+    #[serde_as(as = \"serde_with::VecSkipError<_>\")]\n+    data: Vec<Model>,\n+}\n+\n+#[derive(Clone, Serialize, Deserialize, Debug, Eq, PartialEq)]\n+pub struct Model {\n+    capabilities: ModelCapabilities,\n+    id: String,\n+    name: String,\n+    policy: Option<ModelPolicy>,\n+    vendor: ModelVendor,\n+    model_picker_enabled: bool,\n+}\n+\n+#[derive(Clone, Serialize, Deserialize, Debug, Eq, PartialEq)]\n+struct ModelCapabilities {\n+    family: String,\n+    #[serde(default)]\n+    limits: ModelLimits,\n+    supports: ModelSupportedFeatures,\n+}\n+\n+#[derive(Default, Clone, Serialize, Deserialize, Debug, Eq, PartialEq)]\n+struct ModelLimits {\n+    #[serde(default)]\n+    max_context_window_tokens: usize,\n+    #[serde(default)]\n+    max_output_tokens: usize,\n+    #[serde(default)]\n+    max_prompt_tokens: usize,\n+}\n+\n+#[derive(Clone, Serialize, Deserialize, Debug, Eq, PartialEq)]\n+struct ModelPolicy {\n+    state: String,\n+}\n+\n+#[derive(Clone, Serialize, Deserialize, Debug, Eq, PartialEq)]\n+struct ModelSupportedFeatures {\n+    #[serde(default)]\n+    streaming: bool,\n+    #[serde(default)]\n+    tool_calls: bool,\n+}\n+\n+#[derive(Clone, Copy, Serialize, Deserialize, Debug, Eq, PartialEq)]\n+pub enum ModelVendor {\n+    // Azure OpenAI should have no functional difference from OpenAI in Copilot Chat\n+    #[serde(alias = \"Azure OpenAI\")]\n+    OpenAI,\n+    Google,\n+    Anthropic,\n }\n \n impl Model {\n     pub fn uses_streaming(&self) -> bool {\n-        match self {\n-            Self::Gpt4o\n-            | Self::Gpt4\n-            | Self::Gpt4_1\n-            | Self::Gpt3_5Turbo\n-            | Self::O3\n-            | Self::O4Mini\n-            | Self::Claude3_5Sonnet\n-            | Self::Claude3_7Sonnet\n-            | Self::Claude3_7SonnetThinking => true,\n-            Self::O3Mini | Self::O1 | Self::Gemini20Flash | Self::Gemini25Pro => false,\n-        }\n+        self.capabilities.supports.streaming\n     }\n \n-    pub fn from_id(id: &str) -> Result<Self> {\n-        match id {\n-            \"gpt-4o\" => Ok(Self::Gpt4o),\n-            \"gpt-4\" => Ok(Self::Gpt4),\n-            \"gpt-4.1\" => Ok(Self::Gpt4_1),\n-            \"gpt-3.5-turbo\" => Ok(Self::Gpt3_5Turbo),\n-            \"o1\" => Ok(Self::O1),\n-            \"o3-mini\" => Ok(Self::O3Mini),\n-            \"o3\" => Ok(Self::O3),\n-            \"o4-mini\" => Ok(Self::O4Mini),\n-            \"claude-3-5-sonnet\" => Ok(Self::Claude3_5Sonnet),\n-            \"claude-3-7-sonnet\" => Ok(Self::Claude3_7Sonnet),\n-            \"claude-3.7-sonnet-thought\" => Ok(Self::Claude3_7SonnetThinking),\n-            \"gemini-2.0-flash-001\" => Ok(Self::Gemini20Flash),\n-            \"gemini-2.5-pro\" => Ok(Self::Gemini25Pro),\n-            _ => Err(anyhow!(\"Invalid model id: {}\", id)),\n-        }\n+    pub fn id(&self) -> &str {\n+        self.id.as_str()\n     }\n \n-    pub fn id(&self) -> &'static str {\n-        match self {\n-            Self::Gpt3_5Turbo => \"gpt-3.5-turbo\",\n-            Self::Gpt4 => \"gpt-4\",\n-            Self::Gpt4_1 => \"gpt-4.1\",\n-            Self::Gpt4o => \"gpt-4o\",\n-            Self::O3Mini => \"o3-mini\",\n-            Self::O1 => \"o1\",\n-            Self::O3 => \"o3\",\n-            Self::O4Mini => \"o4-mini\",\n-            Self::Claude3_5Sonnet => \"claude-3-5-sonnet\",\n-            Self::Claude3_7Sonnet => \"claude-3-7-sonnet\",\n-            Self::Claude3_7SonnetThinking => \"claude-3.7-sonnet-thought\",\n-            Self::Gemini20Flash => \"gemini-2.0-flash-001\",\n-            Self::Gemini25Pro => \"gemini-2.5-pro\",\n-        }\n+    pub fn display_name(&self) -> &str {\n+        self.name.as_str()\n     }\n \n-    pub fn display_name(&self) -> &'static str {\n-        match self {\n-            Self::Gpt3_5Turbo => \"GPT-3.5\",\n-            Self::Gpt4 => \"GPT-4\",\n-            Self::Gpt4_1 => \"GPT-4.1\",\n-            Self::Gpt4o => \"GPT-4o\",\n-            Self::O3Mini => \"o3-mini\",\n-            Self::O1 => \"o1\",\n-            Self::O3 => \"o3\",\n-            Self::O4Mini => \"o4-mini\",\n-            Self::Claude3_5Sonnet => \"Claude 3.5 Sonnet\",\n-            Self::Claude3_7Sonnet => \"Claude 3.7 Sonnet\",\n-            Self::Claude3_7SonnetThinking => \"Claude 3.7 Sonnet Thinking\",\n-            Self::Gemini20Flash => \"Gemini 2.0 Flash\",\n-            Self::Gemini25Pro => \"Gemini 2.5 Pro\",\n-        }\n+    pub fn max_token_count(&self) -> usize {\n+        self.capabilities.limits.max_output_tokens",
        "comment_created_at": "2025-04-18T09:32:22+00:00",
        "comment_author": "imumesh18",
        "comment_body": "This should be max_prompt_tokens as per copilot implementation.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2080646647",
    "pr_number": 30155,
    "pr_file": "crates/copilot/src/copilot_chat.rs",
    "created_at": "2025-05-08T23:50:45+00:00",
    "commented_code": "Self::Gemini25Pro => 128_000,\n         }\n     }\n+\n+    pub fn supports_vision(&self) -> bool {\n+        match self {\n+            Self::Gpt4o\n+            | Self::Gpt4_1\n+            | Self::O3\n+            | Self::O4Mini\n+            | Self::Gemini20Flash\n+            | Self::Gemini25Pro\n+            | Self::Claude3_5Sonnet\n+            | Self::Claude3_7Sonnet\n+            | Self::Claude3_7SonnetThinking => true,\n+            _ => false,\n+        }\n+    }",
    "repo_full_name": "zed-industries/zed",
    "discussion_comments": [
      {
        "comment_id": "2080646647",
        "repo_full_name": "zed-industries/zed",
        "pr_number": 30155,
        "pr_file": "crates/copilot/src/copilot_chat.rs",
        "discussion_id": "2080646647",
        "commented_code": "@@ -152,6 +166,21 @@ impl Model {\n             Self::Gemini25Pro => 128_000,\n         }\n     }\n+\n+    pub fn supports_vision(&self) -> bool {\n+        match self {\n+            Self::Gpt4o\n+            | Self::Gpt4_1\n+            | Self::O3\n+            | Self::O4Mini\n+            | Self::Gemini20Flash\n+            | Self::Gemini25Pro\n+            | Self::Claude3_5Sonnet\n+            | Self::Claude3_7Sonnet\n+            | Self::Claude3_7SonnetThinking => true,\n+            _ => false,\n+        }\n+    }",
        "comment_created_at": "2025-05-08T23:50:45+00:00",
        "comment_author": "lj3954",
        "comment_body": "If #29027 is merged, you'll need to instead use (and add) the vision field in model supported features.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2069404208",
    "pr_number": 29563,
    "pr_file": "crates/language_models/src/provider/ollama.rs",
    "created_at": "2025-04-30T20:36:30+00:00",
    "commented_code": "cx.spawn(async move |this, cx| {\n             let models = get_models(http_client.as_ref(), &api_url, None).await?;\n \n-            let mut models: Vec<ollama::Model> = models\n-                .into_iter()\n-                // Since there is no metadata from the Ollama API\n-                // indicating which models are embedding models,\n-                // simply filter out models with \"-embed\" in their name\n+            // Since there is no metadata from the Ollama API\n+            // indicating which models are embedding models,\n+            // simply filter out models with \"-embed\" in their name\n+            let models: Vec<_> = models\n+                .iter()\n                 .filter(|model| !model.name.contains(\"-embed\"))\n-                .map(|model| ollama::Model::new(&model.name, None, None))\n                 .collect();\n \n-            models.sort_by(|a, b| a.name.cmp(&b.name));\n+            let mut ollama_models: Vec<ollama::Model> = Vec::with_capacity(models.len());\n+\n+            for model in models {",
    "repo_full_name": "zed-industries/zed",
    "discussion_comments": [
      {
        "comment_id": "2069404208",
        "repo_full_name": "zed-industries/zed",
        "pr_number": 29563,
        "pr_file": "crates/language_models/src/provider/ollama.rs",
        "discussion_id": "2069404208",
        "commented_code": "@@ -75,19 +80,29 @@ impl State {\n         cx.spawn(async move |this, cx| {\n             let models = get_models(http_client.as_ref(), &api_url, None).await?;\n \n-            let mut models: Vec<ollama::Model> = models\n-                .into_iter()\n-                // Since there is no metadata from the Ollama API\n-                // indicating which models are embedding models,\n-                // simply filter out models with \"-embed\" in their name\n+            // Since there is no metadata from the Ollama API\n+            // indicating which models are embedding models,\n+            // simply filter out models with \"-embed\" in their name\n+            let models: Vec<_> = models\n+                .iter()\n                 .filter(|model| !model.name.contains(\"-embed\"))\n-                .map(|model| ollama::Model::new(&model.name, None, None))\n                 .collect();\n \n-            models.sort_by(|a, b| a.name.cmp(&b.name));\n+            let mut ollama_models: Vec<ollama::Model> = Vec::with_capacity(models.len());\n+\n+            for model in models {",
        "comment_created_at": "2025-04-30T20:36:30+00:00",
        "comment_author": "lj3954",
        "comment_body": "```suggestion\r\n            for model in models {\r\n                // Filter out embedding models\r\n                if model.name.contains(\"-embed\") {\r\n                    continue;\r\n                }\r\n```",
        "pr_file_module": null
      }
    ]
  }
]