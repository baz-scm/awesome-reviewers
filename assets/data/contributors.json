{
  "bartlomieju": {
    "repos": [
      "denoland/deno"
    ],
    "entries": [
      {
        "slug": "deno-add-comprehensive-test-coverage",
        "title": "Add comprehensive test coverage"
      },
      {
        "slug": "deno-avoid-ambiguous-naming",
        "title": "Avoid ambiguous naming"
      },
      {
        "slug": "deno-avoid-implementation-detail-leakage",
        "title": "avoid implementation detail leakage"
      },
      {
        "slug": "deno-avoid-panics-gracefully",
        "title": "avoid panics gracefully"
      },
      {
        "slug": "deno-avoid-redundant-observability-data",
        "title": "avoid redundant observability data"
      },
      {
        "slug": "deno-benchmark-performance-assumptions",
        "title": "benchmark performance assumptions"
      },
      {
        "slug": "deno-comprehensive-test-coverage",
        "title": "comprehensive test coverage"
      },
      {
        "slug": "deno-control-cache-lifecycle",
        "title": "Control cache lifecycle"
      },
      {
        "slug": "deno-explain-non-obvious-decisions",
        "title": "Explain non-obvious decisions"
      },
      {
        "slug": "deno-explicit-dependency-configuration",
        "title": "explicit dependency configuration"
      },
      {
        "slug": "deno-extract-complex-inline-logic",
        "title": "Extract complex inline logic"
      },
      {
        "slug": "deno-manage-async-operation-lifecycle",
        "title": "Manage async operation lifecycle"
      },
      {
        "slug": "deno-minimize-memory-allocations",
        "title": "minimize memory allocations"
      },
      {
        "slug": "deno-organize-code-structure",
        "title": "organize code structure"
      },
      {
        "slug": "deno-prefer-safe-optional-returns",
        "title": "prefer safe optional returns"
      },
      {
        "slug": "deno-prevent-prototype-pollution",
        "title": "prevent prototype pollution"
      },
      {
        "slug": "deno-use-appropriate-error-types",
        "title": "Use appropriate error types"
      },
      {
        "slug": "deno-use-appropriate-synchronization-mechanisms",
        "title": "Use appropriate synchronization mechanisms"
      },
      {
        "slug": "deno-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      },
      {
        "slug": "deno-use-standard-api-interfaces",
        "title": "Use standard API interfaces"
      },
      {
        "slug": "deno-validate-configuration-schemas",
        "title": "validate configuration schemas"
      },
      {
        "slug": "deno-verify-algorithm-correctness",
        "title": "Verify algorithm correctness"
      }
    ],
    "comments": {
      "deno-prefer-safe-optional-returns": [
        "Yeah, I really think we should add specialized API to avoid all these `Default::default()` and rogue `None`s throughout the codebase",
        "Good point! It will always be a path, so probably error is unreachable, but I'll handle that"
      ],
      "deno-avoid-implementation-detail-leakage": [
        "I'm not sure I understand what's the problem here - is this touching some internal implementation details?",
        "We could expose it as a public API down the road?"
      ],
      "deno-use-appropriate-synchronization-mechanisms": [
        "@manzt @rgbkrk if we go with `EventListener` approach for a comm then we can expect more than one consumer of the \"comm\". That means we need to use `tokio::sync::broadcast` channel which is buffered. We can go with a rather big number for the buffer like 1024 or 65536. If we expect only a single consumer then we could use an `mpsc` channel that is unbounded. Which one should we go with?"
      ],
      "deno-add-comprehensive-test-coverage": [
        "Agreed, especially that the parsing is a bit quirky",
        "Great, could you add a couple tests for other directives to ensure that they can accept the \"reason for ignoring\"?"
      ],
      "deno-organize-code-structure": [
        "Since this is an implementation detail, maybe you could move it to `bundle/esbuild.rs` module?",
        "Maybe move it to a helper function and add some comments on the purpose of this hack?\r\n\r\nAre there tests that cover it?",
        "I agree with David",
        "I'm wondering if we should bury `Arc` inside `FeatureChecker`. Seems like an implementation detail. Feel free to do it in a follow up",
        "We should just export this from `deno_core` - this is the third place we're redefining this macro"
      ],
      "deno-use-descriptive-identifiers": [
        "Ditto, suggest to rename it to `readWithCancelHandle`"
      ],
      "deno-validate-configuration-schemas": [
        "Shouldn't this be `oneOf` with a string or an enum? Seems like adding a custom library will cause a diagnostic.",
        "Side note: we should add an automated test to make sure this is up to date, like in https://github.com/denoland/deno/blob/a9f404e479262af62179c38622785968888293d3/cli/tools/lint/mod.rs#L705",
        "```suggestion\r\n          \"description\": \"UNSTABLE: List of plugins to load. These can be paths, npm or jsr specifiers\",\r\n```"
      ],
      "deno-avoid-ambiguous-naming": [
        "Nit: since this is just `import.meta.main` transform maybe rename to `ImportMetaMainTransform` so we can chain more transforms in the future as needed?",
        "Please use an enum here as these booleans are easy to mistake:\r\n```\r\n#[derive(Default, Debug)]\r\nenum SubdomainWildcardSupport {\r\n  Enabled,\r\n  #[default]\r\n  Disabled\r\n}\r\n```",
        "+1 here - ideally we'd find a way to ban `bool` in permission code :D everything should be an enum here with descriptive name",
        "Nitpick: at first glance in JS I thought this means \"to cancel a read\" while in fact it's a \"read with maybe cancel\". Can you rename this op to `op_read_with_cancel_handle`?",
        "Nitpick: consider renaming to `UnconfiguredRuntime` or `PrewarmedRuntime` - I found `Unconfigured` not very intelligible when reviewing `cli/` part. Fine to do it in a follow up PR to not block this one"
      ],
      "deno-avoid-panics-gracefully": [
        "This unwrap is sus here 😬 maybe consider failing gracefully with an error message? Also you don't need to do the conversion to string first - `Url::from_file_path` will accept a `PathBuf`",
        "We shouldn't use `expect` here - we're gonna find all sorts of strange or broken files that are included by mistake - we should error out or print a warning in this case instead of panicking."
      ],
      "deno-explain-non-obvious-decisions": [
        "Maybe add a comment why are we forcing this to `true`?",
        "I think it might be useful to add a comment here when we want to skip this validation - it's not so obvious to me so far",
        "Thanks!"
      ],
      "deno-use-standard-api-interfaces": [
        "Shouldn't this be a direct import of `assert` module - since we use ESM instead of CJS I don't think there's a need for going through `process.getBuiltinModule`",
        "We don't support this syntax (yet!), so you'll need something like this:\r\n\r\n```ts\r\nexport default {\r\n  fetch(req: Request) {\r\n    return new Response(\"Hello from declarative server\");\r\n  },\r\n  onListen(info) {\r\n    console.log(info);\r\n  }\r\n} satisfies Deno.ServeDefaultExport;\r\n```"
      ],
      "deno-extract-complex-inline-logic": [
        "Consider creating another function that is called inside `try {} finally {}` instead of inlining all of this logic here."
      ],
      "deno-minimize-memory-allocations": [
        "Pre-allocate based on `args.extra_stdio.len()`",
        "Maybe preallocate this vector to some sensible value? Like 512?"
      ],
      "deno-avoid-redundant-observability-data": [
        "I'm fine exposing `fd` in the Node compat API, but we shouldn't do it in the `Deno` APIs. IMO this should be handled the same way we're passing resource IDs between Deno APIs and Node APIs (via a private symbol)"
      ],
      "deno-comprehensive-test-coverage": [
        "Won't it crash/error if you try to iterate again? Can you add a test for that?",
        "Could you wrap this whole thing in a `Deno.test()` so that we get the benefit of sanitizers?"
      ],
      "deno-manage-async-operation-lifecycle": [
        "Is this correct that we're unrefing and refing immediately after?"
      ],
      "deno-benchmark-performance-assumptions": [
        "Ugh, do we really want to create an array here? Maybe use 2 separate arguments here to make it so slightly faster?",
        "I think it's 3th, but yes",
        "Okay, let's not do that then."
      ],
      "deno-prevent-prototype-pollution": [
        "I believe it should stay disabled in the TSC implementation. There won't be any user code executed here that might require it.",
        "Do you know of any package that only uses getter?",
        "These should use primordials like `ObjectDefineProperty`, `ObjectPrototype` and `ObjectGetPrototypeOf`. You will need to move this after line 53"
      ],
      "deno-explicit-dependency-configuration": [
        "Let's remove this alias before landing",
        "Need to use an actual released version"
      ],
      "deno-verify-algorithm-correctness": [
        "This appears to be wrong, I think you should use a logic similar to this:\r\n```\r\n((typeof listener.options === \"boolean\" &&\r\n          listener.options === options.capture) ||\r\n          (typeof listener.options === \"object\" &&\r\n            listener.options.capture === options.capture)) &&\r\n        listener.callback === callback\r\n```"
      ],
      "deno-use-appropriate-error-types": [
        "Should this one be covered too? ",
        "Okay, but shouldn't it be a specific error instead of `notImplemented` error?",
        "😢 can we use `Deno.errors.<something>` here? This seems super brittle",
        "Add a limit here so the test won't hang if something goes wrong"
      ],
      "deno-control-cache-lifecycle": [
        "Could we clear it after a request is completed instead? I did a similar thing in https://github.com/denoland/deno/pull/27831"
      ]
    },
    "profile": {
      "location": "Warsaw, PL",
      "company": "@denoland",
      "blog": "",
      "twitter_username": "biwanczuk",
      "site_admin": false,
      "followers": 1035,
      "following": 8
    }
  },
  "DiegoAndai": {
    "repos": [
      "mui/material-ui"
    ],
    "entries": [
      {
        "slug": "material-ui-avoid-render-cycle-allocations",
        "title": "Avoid render cycle allocations"
      },
      {
        "slug": "material-ui-consistent-component-api-patterns",
        "title": "Consistent component API patterns"
      },
      {
        "slug": "material-ui-consistent-package-naming",
        "title": "Consistent package naming"
      },
      {
        "slug": "material-ui-defensively-handle-nullables",
        "title": "Defensively handle nullables"
      },
      {
        "slug": "material-ui-explicit-configuration-specifications",
        "title": "Explicit configuration specifications"
      },
      {
        "slug": "material-ui-follow-library-recommendations",
        "title": "Follow library recommendations"
      },
      {
        "slug": "material-ui-graceful-component-errors",
        "title": "Graceful component errors"
      },
      {
        "slug": "material-ui-isolate-dom-security-boundaries",
        "title": "Isolate DOM security boundaries"
      },
      {
        "slug": "material-ui-meaningful-and-consistent-names",
        "title": "Meaningful and consistent names"
      },
      {
        "slug": "material-ui-parameter-interaction-design",
        "title": "Parameter interaction design"
      },
      {
        "slug": "material-ui-parameterize-build-processes",
        "title": "Parameterize build processes"
      },
      {
        "slug": "material-ui-standardize-build-configurations",
        "title": "Standardize build configurations"
      },
      {
        "slug": "material-ui-strict-mode-proof-hooks",
        "title": "Strict mode-proof hooks"
      },
      {
        "slug": "material-ui-test-behavior-not-implementation",
        "title": "Test behavior not implementation"
      },
      {
        "slug": "material-ui-use-theme-utilities-consistently",
        "title": "Use theme utilities consistently"
      }
    ],
    "comments": {
      "material-ui-avoid-render-cycle-allocations": [
        "Should we use a ref here instead?"
      ],
      "material-ui-defensively-handle-nullables": [
        "I don't mind the `!!` to cast into boolean",
        "I would argue it's a fix as it was always supposed to be `() => boolean` (it's only `boolean` in the `CSSObjectWithVariants` type), but it wasn't actually because the type wasn't being correctly picked up.\r\n\r\nIt's one of those fixes that could be perceived as a breaking change.\r\n\r\nMaybe we could compromise with a minor release and an explanation on the changelog?",
        "This would be the last thing that we need to decide on. Similar to the `styles` now being required, this is a fix that might be perceived as breaking.\r\n\r\nMy proposal is that we release it with in a minor version and a note."
      ],
      "material-ui-parameterize-build-processes": [
        "What creates the `lib/esm/package.json` file that we need to remove? Can we stop creating it?",
        "> I didn't necessarily want to add a special case to the build for a single package, but we could.\r\n\r\nThat's ok, I was just checking 👍🏼 let's go with whatever option you think it's better."
      ],
      "material-ui-meaningful-and-consistent-names": [
        "Why call the handle `event`? Something like `textareaHandle` would be easier to understand IMO.",
        "I agree with replacing the term `tag` in our code. I think we can use `item` and explain in the docs what props are provided. We can explain in the docs that these include some props specific to Chip (`onDelete`) but other's that aren't. For example, the `data-tag-index` should be transformed to `data-item-index`, and this will be important to spread regardless of whether the component is a Chip or not."
      ],
      "material-ui-consistent-package-naming": [
        "Not strictly necessary, but if we could rename `@app/next-app` to `@app/pigment-css-next-app`, that would help clarify the ignore."
      ],
      "material-ui-test-behavior-not-implementation": [
        "Is this required?",
        "Right, but I think the `textarea` element that the `TextareaAutosize` renders should have the `textbox` role by default, doesn't it?",
        "Ah, I see. In that case I think we should do\r\n\r\n```\r\n      slotProps={{\r\n        input: {\r\n          'data-testid': 'input',\r\n        },\r\n      }}\r\n```\r\n\r\nAnd use `page.getByTestId('input')`\r\n\r\nWould that work?"
      ],
      "material-ui-standardize-build-configurations": [
        "I don't understand what's the function of this 😅 "
      ],
      "material-ui-parameter-interaction-design": [
        "If providing the `container` prop was required when using `disablePortal`, this could be simplified to \r\n\r\n```suggestion\r\nconst resolvedContainer = getContainer(container) || getDoc().body;\r\n```\r\n\r\nRight?"
      ],
      "material-ui-explicit-configuration-specifications": [
        "This makes sense, let's use `^6.0.0`"
      ],
      "material-ui-consistent-component-api-patterns": [
        "The `useSlot` hook already has merging strategies implemented, but I think the problem here is that the Autocomplete should handle the slot props better:\r\n\r\n- Refactor `Autocomplete` to use `slotProps` instead of `InputProps` and `inputProps`\r\n- Inside `Autocomplete`, merge `slotProps.input` and `slotProps.htmlInput`\r\n\r\n```js\r\nconst [HtmlInputSlot, htmlInputProps] = useSlot('htmlInput', {\r\n    elementType: 'input',\r\n    externalForwardedProps, // already defined above\r\n    additionalProps: {\r\n        disabled,\r\n        readOnly,\r\n        ...getInputProps(),\r\n    },\r\n    className: classes.input,\r\n    ownerState,\r\n  });\r\n\r\n// ...\r\n\r\n// line 730\r\n    {renderInput(\r\n     // ...\r\n     // line 778\r\n    slotProps: {\r\n        htmlInput: htmlInputProps,\r\n    },\r\n},\r\n```\r\n\r\nI haven't tested this, but it should be something similar to the code above.\r\n\r\nThis will be a complex PR, but in the end, the result should be the slots pattern properly implemented in the `Autocomplete` component.\r\n\r\nWe're still discussing if the slots pattern needs changes, cc: @aarongarciah, so this PR might require changes in the future given that decision.\r\n\r\nDoes that make sense @sai6855?",
        "> are you imagining renderInput would look something like this after refactoring?\r\n\r\nYes, exactly that\r\n\r\n> As Autocomplete component don't have any control over slotProps in highlighted line, is it possible to merge without changes to be done in userland?\r\n\r\nI don't understand the issue. Is it that `params` has a `slotProps` key as well, so it's overridden?",
        "Oh ok, I think I see the issue. The Autocomplete structure confused me. Yes, my suggestion is not correct, I was confusing `Autocomplete` slots with `TextField` slots.",
        "@sai6855 I think these should be merged in userland, this was always required, see: https://github.com/mui/material-ui/issues/43573#issuecomment-2402630864.\r\n\r\nLet's wait and see if we can move forward with migrating the usage of `InputProps`, `InputLabelProps` to slotProps in Autocomplete",
        "@sai6855 sorry for the late reply.\r\n\r\nThe path forward for this PR is what I commented here: https://github.com/mui/material-ui/issues/43573#issuecomment-2402630864. We should update Autocomplete to use `slotProps` internally to make it consistent but not merge anything on our side. We should also add documentation explaining what I explained in that same comment: \"When overriding `slotProps.input` for the Autocomplete's TextField, `params.slotProps.input` must be spread\"\r\n\r\nDoes that make sense?",
        "> If we go with above approach we would be breaking AutocompleteRenderInputParams type\r\n\r\nCould we:\r\n\r\n- Add `slotProps`\r\n- Keep and deprecate `InputProps` and `inputProps`\r\n\r\nInternally, the Autocomplete would have to forward `InputProps` and `inputProps` to `slotProps` accordingly.\r\n\r\nIf that would work, it wouldn't require a breaking change, right?"
      ],
      "material-ui-use-theme-utilities-consistently": [
        "Let's make this change\r\n\r\n```suggestion\r\n    overflowY: 'auto',\r\n```\r\n\r\nNo need for a comment 😊 "
      ],
      "material-ui-follow-library-recommendations": [
        "Is there a particular reason a double click is needed?"
      ],
      "material-ui-isolate-dom-security-boundaries": [
        "We shouldn't do this:\r\n\r\nIf multiple modals are open, or the developer applied `aria-hidden` to one of the modal's ancestors, it's the developer responsibility to fix it.\r\n\r\nThis is not an acceptable side-effect:\r\n\r\n> [...] if a developer manually applied aria-hidden to hide certain content, removing it could lead to unintended accessibility issues.\r\n\r\n"
      ],
      "material-ui-strict-mode-proof-hooks": [
        "Using the register pattern without an unregister function sounds to me like it can introduce weird edge cases. Did you consider registering/unregistering on an effect instead?",
        "Deriving the `value` from the position sounds good to me 👍🏼 \r\n\r\n> While registerTab is already idempotent when used with explicit values (via valueToIndex.has(finalValue)), we can't enforce value on Tab without introducing a breaking change.\r\n\r\nBut if we create a `value` for the position on our side, then we would have a `finalValue`, no? ",
        "> Yes, we do generate a finalValue based on position internally when value isn't provided. The issue is that this implicit value depends on render order and a shared index (childIndexRef), which is incremented during registration.\r\n\r\nBut what if we \"store\" that value in the Tab, to be used in subsequent registrations? Wouldn't that remove the issue?\r\n\r\nSomething like:\r\n1. Tab runs register for the first time. There's no value, so we store `finalValue`\r\n2. In any subsequent register call, we use `finalValue` to identify the Tab, and thus we don't have to calculate a new `finalValue`.\r\n\r\nWould this work?",
        "> Why do we want to pass the same finalValue to the next registration call? If we want to pass this stored finalValue, we shouldn't call registerTab again in the first place (supposedly in the effect).\r\n\r\nBecause we want to return the unregistering callback from the effect, so it's run on unmount.\r\n\r\n> Even in Base UI, they are making the value prop required on Tab\r\n\r\nThis is not an option for us unless we want to wait for a new major.\r\n\r\n> Supporting implicit value in Tab cause them issues like\r\n\r\nWe're already supporting implicit value, aren't we? With or without my suggestion.",
        "> But wouldn't the useEffect's setup function do nothing?\r\n\r\nOnly on the first run of the effect. Subsequent runs would unregister (cleanup) and register back. This is the same as having register/unregister in an effect, except that the very first time we already registered inside `useState`. I don't see an issue with it if register is idempotent 🤔 ",
        "Which test failed? What was the message?"
      ],
      "material-ui-graceful-component-errors": [
        "```suggestion\r\n    throw new Error('Material UI: The Tab component must be used inside a Tabs component');\r\n```",
        "I wonder if we should throw here. If it were a new component, yes, for sure, but maybe we can take a softer approach? Just to cover for users who are using the `Tab` component on its own. Or is that too cautious?",
        "We don't know how users are using it in the wild, so it's better to be conservative. Let's have a warning in development instead of throwing."
      ]
    },
    "profile": {
      "location": "Santiago, Chile",
      "company": "MUI",
      "blog": "",
      "twitter_username": "DiegoAndaiC",
      "site_admin": false,
      "followers": 66,
      "following": 2
    }
  },
  "ollevche": {
    "repos": [
      "opentofu/opentofu"
    ],
    "entries": [
      {
        "slug": "opentofu-clear-concise-documentation",
        "title": "Clear concise documentation"
      },
      {
        "slug": "opentofu-craft-actionable-errors",
        "title": "Craft actionable errors"
      },
      {
        "slug": "opentofu-defensive-null-handling",
        "title": "Defensive null handling"
      },
      {
        "slug": "opentofu-document-intent-and-limitations",
        "title": "Document intent and limitations"
      },
      {
        "slug": "opentofu-document-phased-migration-paths",
        "title": "Document phased migration paths"
      },
      {
        "slug": "opentofu-document-with-examples",
        "title": "Document with examples"
      },
      {
        "slug": "opentofu-names-preserve-cognitive-context",
        "title": "Names preserve cognitive context"
      },
      {
        "slug": "opentofu-preserve-sensitive-data-marks",
        "title": "Preserve sensitive data marks"
      },
      {
        "slug": "opentofu-prevent-backing-array-surprises",
        "title": "Prevent backing array surprises"
      },
      {
        "slug": "opentofu-reduce-code-nesting",
        "title": "Reduce code nesting"
      },
      {
        "slug": "opentofu-review-consistency-assumptions",
        "title": "Review consistency assumptions"
      },
      {
        "slug": "opentofu-separate-configuration-lifecycles",
        "title": "Separate configuration lifecycles"
      },
      {
        "slug": "opentofu-specify-configuration-behaviors",
        "title": "Specify configuration behaviors"
      },
      {
        "slug": "opentofu-structure-tests-thoroughly",
        "title": "Structure tests thoroughly"
      },
      {
        "slug": "opentofu-use-relative-documentation-links",
        "title": "Use relative documentation links"
      }
    ],
    "comments": {
      "opentofu-use-relative-documentation-links": [
        "```suggestion\r\nUnlike other [custom conditions](../expressions/custom-conditions.mdx), assertions do not affect OpenTofu's execution of an operation. A failed assertion reports a warning without halting the ongoing operation. This contrasts with other custom conditions, such as a postcondition, where OpenTofu produces an error immediately, halting the operation and blocking the application or planning of future resources.\r\n```",
        "```suggestion\r\nPreconditions are unique amongst the custom conditions in that they execute _before_ a resource change is applied or planned. [Choosing Between Preconditions and Postconditions](../expressions/custom-conditions.mdx#choosing-between-preconditions-and-postconditions) offers guidance on choosing between a precondition and a postcondition, and the same topics also apply to choosing between a precondition and a check block.\r\n```"
      ],
      "opentofu-document-with-examples": [
        "We are not consistent with those, so it is more of nit. To follow other updates tone of voice:\r\n\r\n```suggestion\r\n* \"force-unlock\" option is now supported by the HTTP backend. ([#2381](https://github.com/opentofu/opentofu/pull/2381))\r\n```"
      ],
      "opentofu-review-consistency-assumptions": [
        "One more thing to keep in mind is that `s3` backend is being used with other non-AWS s3 compatible storage implementations. Those services may or may not support the conditional locking feature. Ideally, we would check somehow if that feature is supported and error otherwise. Not sure if there's an easy way to check that, but we need to at least document this pitfall.",
        "We need not to forget to update the user-facing docs as well.",
        "I may be missing something, but do we need to update the digest? If I understand correctly, we could go with a `IfNoneMatch` lock creation so it would fail for any other tofu run, while the file exists. On Unlock, we could just delete it altogether. As far as I can tell, this matches the dynamodb implementation and eliminates data races via `IfNoneMatch` option. \r\n\r\nWhat do you think?",
        "Got you, thanks! I missed that part, I will re-read it and leave another comment.",
        "After some digging, I think this option is unnecessary even for dynamodb locking. This feature was introduced to workaround the eventual consistency nature of s3 (before 2020). It is checking if the read state file is the latest written one and we no longer need to check this since s3 become strongly consistent ([blogpost](https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/)) in 2020.",
        "This is something only slightly relevant for this RFC, so I think we can tackle it separately. Alternatively, we can leave it as is if we plan to deprecate dynamodb locking completely. Otherwise, removing md5 write / read from dynamodb locking will positively affect the performance."
      ],
      "opentofu-structure-tests-thoroughly": [
        "Makes sense, I added names for test cases in [9d345f3](https://github.com/opentofu/opentofu/pull/2673/commits/9d345f3385a809b204485d667b1c7b5b0d77a370)",
        "Thanks, added in [a4f25c2](https://github.com/opentofu/opentofu/pull/1728/commits/a4f25c288cf40005fb55b8f6be479d540716acc2)",
        "I would prefer a more clear separation between test logic and test data (inputs and outputs). I think the following structure covers test needs in a slightly simpler way (you would also need to change test run logic inside a loop):\r\n\r\n```suggestion\r\n\tinputLockInfo := statemgr.LockInfo{\r\n\t\tID:        \"ada-lovelace-state-lock-id\",\r\n\t\tWho:       \"AdaLovelace\",\r\n\t\tOperation: \"TestTypePlan\",\r\n\t}\r\n\r\n\ttestCases := []struct {\r\n\t\tname\t\t\t\tstring\r\n\t\tlockResponseStatus\tint\r\n\t\tlockResponseBody\t\t[]byte\r\n\t\texpectedLockInfo\t\t*statemgr.LockInfo // or just an ID\r\n\t\texpectedErrorMsg\t\tstring\r\n\t}{\r\n\t...\r\n```"
      ],
      "opentofu-reduce-code-nesting": [
        "we can return early here since we will return anyway right after the switch-case",
        "let's reduce nesting here:\r\n\r\n```suggestion\r\n\t\tif pv, ok := v.(GraphNodeProviderConsumer); !ok {\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\t...\r\n```",
        "Thanks! Made it more idiomatic in [8075c3f](https://github.com/opentofu/opentofu/pull/1959/commits/8075c3f26301c29062c5e2568f6bba9a762b71c5)",
        "[354964b](https://github.com/opentofu/opentofu/pull/1750/commits/354964b836ed72308b6fc8e48f5b7d42cda7c460)",
        "nit: just a bit clearer with reduced nesting\r\n\r\n```suggestion\r\n                ext := tfFileExt(p)\r\n\t\tif ext == \"\" {\r\n\t\t\trelevantPaths = append(relevantPaths, p)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t        parallelTofuExt := strings.ReplaceAll(ext, \".tf\", \".tofu\")\r\n\t        pathWithoutExt, _ := strings.CutSuffix(p, ext)\r\n\t        parallelTofuPath := pathWithoutExt + parallelTofuExt\r\n        \r\n\t        // If the .tf file has a parallel .tofu file in the directory,\r\n\t        // we'll ignore the .tf file and only use the .tofu file\r\n\t        if slices.Contains(paths, parallelTofuPath) {\r\n\t\t        ignoredPaths = append(ignoredPaths, p)\r\n\t        } else {\r\n\t\t        relevantPaths = append(relevantPaths, p)\r\n\t        }\r\n```"
      ],
      "opentofu-preserve-sensitive-data-marks": [
        "This is actually irrelevant for the deprecation marks. However, we don't need to remove sensitive marks if there are none.",
        "I removed that change. The code below is not only removing sensitive marks, but also reordering existing ones (from nested to top level). I believe this was not an issue for most cases, but now that we actually care about other marks, it could be a problem. I will review usages.",
        "Found a few more cases, where sensitive marks handling needs to be refactored and put the changes under a separate PR - https://github.com/opentofu/opentofu/pull/2673"
      ],
      "opentofu-craft-actionable-errors": [
        "Makes sense, changed tone of voice in d44bf936584acd2e90273774b29711082cbb7377"
      ],
      "opentofu-defensive-null-handling": [
        "`m.Aliases` must be allocated before the loop.",
        "`AsString()` panics if the value is marked so we need to call it only after the sensitivity check.",
        "`AliasExpr` could be nil here and lead to a panic.\r\n\r\nnit: it may be also useful to split the condition into multiple IFs / switch to make it easier to read and provide a more personalized error message.",
        "It is nit, you can just ignore that part of the comment!",
        "Yep, I completely agree with you. I moved it higher and changed `buildAllVariables` to write into an existing map so we use the same map everywhere.\r\n\r\nbadc0309785d7139b9e9b66cdd5d41714814d142"
      ],
      "opentofu-document-intent-and-limitations": [
        "nit: it is useful to have a comment here describing when/how `Provider` struct is built",
        "I extended the comment to describe a little more and also named a few examples: [920b9ab](https://github.com/opentofu/opentofu/pull/1728/commits/920b9ab7ad00e3ec97644ed62bcb724cc900d2fa). ",
        "Do you mind leaving a note on why do we multiply by 2?",
        "Yes, sure. I mean let's add a comment to the code to make it totally clear."
      ],
      "opentofu-names-preserve-cognitive-context": [
        "`psuedo` should be renamed like `decodedExpectedVar` to be more readable."
      ],
      "opentofu-prevent-backing-array-surprises": [
        "I extended the comment to `ComposeBySchema` function to elaborate more on seed and randomization sequence: [6447b38](https://github.com/opentofu/opentofu/pull/1772/commits/6447b38ff4b9824809a67c80b098174f96fee3df).\r\n\r\nAlso, I put more comments where iteration order is important: [ea83b47](https://github.com/opentofu/opentofu/pull/1772/commits/ea83b47d87067a31601c1c4f389f0d7598318363).",
        "`reflect.DeepEqual` feels a bit of an overkill here. Also, it could fail with wrapped diags.\r\n\r\nWould you mind adapting the logic in [ConsolidateWarnings](https://github.com/opentofu/opentofu/blob/main/internal/tfdiags/consolidate_warnings.go#L25) here?\r\n\r\nI think we can treat two diags as duplicates if:\r\n* their `Severity`s are equal\r\n* their `Source.Subject`s are present and the whole `Source`s are equal\r\n* their `Description`s are equal\r\n\r\nI would also check if `DoNotConsolidateDiagnostic` function allows merging the diag.",
        "Yep, I agree `DoNotConsolidateDiagnostic` check is optional here.\r\n\r\n> perhaps with the Ranges added as well.\r\n\r\n`Source`s should be equal and `Source.Subject`s (ranges) should be non-zero. "
      ],
      "opentofu-clear-concise-documentation": [
        "nit:\r\n```suggestion\r\n`terraform_remote_state` does not respect the sensitivity of source state snapshots. Use with caution when referencing state with sensitive root module output values.\r\n```",
        "nit: not sure if we need an example here, IMO it increases page complexity more than adds relevant context (for this particular page).",
        "Yep, makes sense. Moved note to usage block for most of the commands: [3980be3](https://github.com/opentofu/opentofu/pull/1843/commits/3980be38e72560ca2a97aea35f51c0f0ea66b9cf)",
        "that's a miss, thank you!\r\n\r\n[e27f67c](https://github.com/opentofu/opentofu/pull/1843/commits/e27f67c2a6ed5485312283c102c84e11accd337b)"
      ],
      "opentofu-specify-configuration-behaviors": [
        "```suggestion\r\n> When OpenTofu S3 backend is used with an S3 compatible provider, it needs to be checked that the provider supports conditional writes in the same way AWS S3 is offering.\r\n```"
      ],
      "opentofu-document-phased-migration-paths": [
        "Another comment about the migration path - what users actually need to do? I assume this is the following flow:\r\n1. Change the configuration to enable native locking\r\n2. `tofu apply`\r\n3. Change the configuration to remove dynamodb usage\r\n\r\nHowever, I am not sure if changes to the backend configuration would trigger state write. We need to test it additionally and document properly. "
      ],
      "opentofu-separate-configuration-lifecycles": [
        "We already track some of the modernization/refactoring issues and I would suggest we document that down to address later. Both @Evi1Pumpkin and @cam72cam arguments sound pretty reasonable.\r\n\r\nBreaking down config package structures into different stages such as loading (raw) and evaluation (ready for graph processing) makes perfect sense to me.",
        "IMO we have to ensure other tools **fail** correctly if this field is something they rely on. I think it should be omit completely.",
        "We treat it as an error because the provider could be set in both resource and import blocks. If that is the case, providers (and aliases) should match. If the provider alias contains `each` / `count`, they will be different for sure so we error. Does it answer your question?",
        "Extended the comments in [82308c0](https://github.com/opentofu/opentofu/pull/1960/commits/82308c030a18c975a39138db95a8bd4adaf81945)",
        "```suggestion\r\n\t\t\tDetail:   \"count was declared as a provider attribute in a test file\",\r\n```",
        "I checked the usage of `Sensitive` field, and I believe we have a few more places where we could use `Deprecated` field.\r\n\r\nThe first one is the interactive input. There are few options: let the user know the field is deprecated beforehand or even skip the interactive input if the field is deprecated. Not sure about the last one, however deprecation would mean we have another field, which should be used instead, and thus it makes sense to just skip the deprecated one. The functions I am referring to are [Local.interactivelyCollectVariables](https://github.com/opentofu/opentofu/blob/main/internal/backend/local/backend_local.go#L417) and [Meta.getInput](https://github.com/opentofu/opentofu/blob/main/internal/command/meta_config.go#L162).\r\n\r\nThe second one is JSON output mode. It includes the configuration part, which IMO should reflect the deprecation as well as other configured options. For example, here is a part of configuration planned into a file:\r\n\r\n```terraform\r\nmodule \"mod\" {\r\n  source = \"./mod\"\r\n  a = \"b\"\r\n}\r\n```\r\n\r\nAnd this is a part of `tofu show --json planfile` command:\r\n\r\n```json\r\n    \"root_module\": {\r\n      \"module_calls\": {\r\n        \"mod\": {\r\n          \"source\": \"./mod\",\r\n          \"expressions\": {\r\n            \"a\": {\r\n              \"constant_value\": \"b\"\r\n            }\r\n          },\r\n          \"module\": {\r\n            \"variables\": {\r\n              \"a\": {\r\n                \"default\": \"b\"\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  ```\r\n  \r\n  What do you think?",
        "Thanks! JSON outputs look good to me. However, I am not sure if I understand how the deprecated message is shown on interactive input. I would use similar approach as in the warning, where we say \"Variable is marked as deprecated with the following message: <MESSAGE>\". ",
        "Nice catch, thanks!\r\n\r\n[5fa601a](https://github.com/opentofu/opentofu/pull/1772/commits/5fa601a8bae2f435f88d08e2b390d95febabbb6d)"
      ]
    },
    "profile": {
      "location": "Kyiv",
      "blog": "",
      "site_admin": false,
      "followers": 34,
      "following": 20
    }
  },
  "cipolleschi": {
    "repos": [
      "facebook/react-native"
    ],
    "entries": [
      {
        "slug": "react-native-avoid-synchronous-main-dispatch",
        "title": "Avoid synchronous main dispatch"
      },
      {
        "slug": "react-native-avoid-unnecessary-allocations",
        "title": "avoid unnecessary allocations"
      },
      {
        "slug": "react-native-component-initialization-state",
        "title": "Component initialization state"
      },
      {
        "slug": "react-native-configuration-validation-and-defaults",
        "title": "Configuration validation and defaults"
      },
      {
        "slug": "react-native-descriptive-specific-naming",
        "title": "descriptive specific naming"
      },
      {
        "slug": "react-native-document-configuration-logic",
        "title": "Document configuration logic"
      },
      {
        "slug": "react-native-eliminate-unnecessary-computations",
        "title": "Eliminate unnecessary computations"
      },
      {
        "slug": "react-native-environment-variable-validation",
        "title": "Environment variable validation"
      },
      {
        "slug": "react-native-extract-complex-logic",
        "title": "extract complex logic"
      },
      {
        "slug": "react-native-follow-established-naming-conventions",
        "title": "Follow established naming conventions"
      },
      {
        "slug": "react-native-minimize-public-api-surface",
        "title": "minimize public API surface"
      },
      {
        "slug": "react-native-optimize-ci-platform-builds",
        "title": "Optimize CI platform builds"
      },
      {
        "slug": "react-native-platform-aware-configuration-messages",
        "title": "platform-aware configuration messages"
      },
      {
        "slug": "react-native-preserve-component-patterns",
        "title": "preserve component patterns"
      },
      {
        "slug": "react-native-simplify-redundant-logic",
        "title": "simplify redundant logic"
      },
      {
        "slug": "react-native-use-appropriate-log-levels",
        "title": "Use appropriate log levels"
      },
      {
        "slug": "react-native-validate-configuration-formats",
        "title": "Validate configuration formats"
      }
    ],
    "comments": {
      "react-native-avoid-synchronous-main-dispatch": [
        "this might deadlock... Do we have a list of devices that supports 120 vs 60?\r\nI kind of think that we don't have to support 60 FPS devices anymore...",
        "We had deadlocks in internal app at startup because multiple things can request access to the main queue.\r\n\r\nSo imagine that a process starts on the main queue, than dispatch sync to a BG queue, then the other queue calls `RCTSingleFrameInterval` which dispatch sync on the main queue. In this scenario the main queue is locked, waiting for the BG queue and we deadlock.\r\n\r\nI saw this happening in other parts of React Native that looked safe, unfortunately.\r\n\r\nPerhaps we can have `NativeAnimatedTurboModule` require the main queue to be initialized (if it does not need it already) and have the turbomodule compute this in the init. That would be safe.",
        "This could potentially deadlock. We should not run the unsafe variant of this method. Can you change it with `RCTExecuteOnMainQueue`?",
        "One thing that confuses me is that, in your stacktrace, the crash is happening in Thread 26... but this assert should force the app to be on the main thread, which is not the Thread 26... how's this possible?",
        "That's a good explanation, but then we should see crashes in development happening because of the assertion. And IIUC, the app does not crash in development, right?",
        "By looking at the crash log, the JS thread is triggering the invalidation. I think that this is the root of the problem: after the JS thread detect the invalidation, we should jump on the UI thread to invalidate everything...",
        "Can you use [RCTExecuteOnMainQueue](https://github.com/facebook/react-native/blob/main/packages/react-native/React/Base/RCTUtils.h#L41) instead? It avoids the jump if we are already on the main queue."
      ],
      "react-native-use-appropriate-log-levels": [
        "This should go in a different pr. Also:\r\n```suggestion\r\nfunction prebuildLog(\r\n  message /*: string */,\r\n  level /*: 'info' | 'warning' | 'error' */ = 'warning',\r\n) {\r\n  // Simple log coloring for terminal output\r\n  const prefix = '[Prebuild] ';\r\n  let colorFn = (x /*:string*/) => x;\r\n  if (process.stdout.isTTY) {\r\n    if (level === 'info') colorFn = x => `\\x1b[32m${x}\\x1b[0m`;\r\n    else if (level === 'error') colorFn = x => `\\x1b[31m${x}\\x1b[0m`;\r\n    else colorFn = x => `\\x1b[33m${x}\\x1b[0m`;\r\n  }\r\n\r\n  console.log(colorFn(prefix + message));\r\n}\r\n```"
      ],
      "react-native-component-initialization-state": [
        "Hi @zhongwuzw I made some investigation and I don't think that this is the right fix.\r\n\r\nThe problem with this is that, after recycle, the default value for the UISwitch is `true`. While the oldProp is holding a value of `false`. The new prop is `false`, so the value is not set.\r\n\r\nI think that the right fix should be applied in the `updateProps:` function, like this:\r\n\r\n```diff\r\n-  if (oldSwitchProps.value != newSwitchProps.value) {\r\n+  if (!_isInitialValueSet || oldSwitchProps.value != newSwitchProps.value) {\r\n    BOOL shouldAnimate = _isInitialValueSet == YES;\r\n    [_switchView setOn:newSwitchProps.value animated:shouldAnimate];\r\n  }\r\n```\r\nThis will force ReactNative to set the initial value on the switch in the first rendering, which is the correct behavior, because that will be the first rendering for the recycled component.\r\n",
        "Can you explain why removing this was relevant? Is it necessary to remove it?"
      ],
      "react-native-descriptive-specific-naming": [
        "```suggestion\r\nfunction _downloadPrebuildReleaseTarball(\r\n```\r\nThe leading `_` is ok, that's the convention for \"private\" methods"
      ],
      "react-native-follow-established-naming-conventions": [
        "Method says `download_prebuild_release_tarball`... should we call it `stable`?\r\n```suggestion\r\n        url = release_tarball_url(@@react_native_version, :release)\r\n```",
        "The convention we are following is:\r\n- REACT_NATIVE is shortened to RCT\r\n- The default case, running just `bundle exec pod install` should not change. We want the dependecies to be an opt-in for now, not an opt out.\r\n\r\nI think we should use: `RCT_USE_DEP_PREBUILD` instead of `REACT_NATIVE_DEPS_BUILD_FROM_SOURCE`."
      ],
      "react-native-preserve-component-patterns": [
        "shouldn't these two be inverted? or, putting it in another way, can `SafeAreView` work as `View` (and we can remove one indentation level)?"
      ],
      "react-native-eliminate-unnecessary-computations": [
        "@javache No value in calling hash_combine again with seed 0.\r\n```suggestion\r\nreturn color.getUIColorHash();\r\n```"
      ],
      "react-native-optimize-ci-platform-builds": [
        "we only need to add maccatalyst. We don't care about the others because they are OOT platform.\r\n",
        "yep. But for building in CI, we still have to pass them as a separate param, so we can parallelize the build",
        "we only need to build for ios, ios-simulator, mac catalyst. All the others are OOT platforms that have their own fork, so we can't really build for them."
      ],
      "react-native-avoid-unnecessary-allocations": [
        "I'm not sure we need to create a new pointer with `make_shared` here.\r\nThe type of the scrollEvent is already the right one. If previously we were passing the scrollEvent, we should be able to pass the scrollEvent even now, types should match.",
        "`ScrollEndDragEvent` event is a subclass of `ScrollEvent`, we probably don't need to change this method at all, as you are allowed to pass a subclass to a method that accepts a superclass. The language should take care of it. ([LSP - Liskov substitution principle](https://en.wikipedia.org/wiki/Liskov_substitution_principle))\r\n\r\nCan you remove these changes and test if this works properly without these?\r\n"
      ],
      "react-native-environment-variable-validation": [
        "```suggestion\r\n        if ENV[\"RCT_USE_PREBUILT_RNCORE\"] == \"1\"\r\n```\r\nIf `RCT_USE_PREBUILT_RNCORE` is not set should not enter in the `if`."
      ],
      "react-native-validate-configuration-formats": [
        "@shubhamguptadream11 assuming the `env.oncall1` and `env.oncall2` variables are correct, is this the right format to pass a list of ids to the `repo-monitor` action?",
        "Added this to avoid reading cocoapods. That's in general better, we can find a way to read those from react-native rather than have a duplication.\r\nHowever, these are fixed for the Release, so they should never change for 0.75"
      ],
      "react-native-document-configuration-logic": [
        "@NickGerleman \r\n> It would be best to keep the changes to the Fabric component instead of this one.",
        "@ArekChr I believe that the suggestion here was to not touch the files in the old architecture. The PR already updates the New Architecture, so we are good.\r\n---\r\n@SimpleCreations Thanks for the feedback, I believe that in this case we can keep the fix here. As a general thought, though.. Yes, in the future the Old Architecture will not be maintained. \r\n\r\nRight now, we already released 0.76 and 0.77 where the New Architecture is the default. We are already working on 0.78 and once that's out, 0.75 will go out of support. That is the last version where the Old Architecture is used by default.\r\n\r\nConsider that new features are only developed for the New Architecture and many libraries will be only compatible with the New Architecture, moving forward (reanimated and react-native-vision-camera to mention two of them). So we strongly advise to start migrating to the New Architecture.\r\n\r\nI'd be curious to know what is holding you back in more details. If you can replicate Performance and bundle issues in separate reproducers, we are more than happy to look into them as soon as possible.",
        "shouldn't this be in the external pod? Or are we keeping it here while we support the old JSC and the plan is to move it away or delete it when we only have hermes or the external jsc pod from community?"
      ],
      "react-native-extract-complex-logic": [
        "I'd rather extract all the validation in a separate function. "
      ],
      "react-native-configuration-validation-and-defaults": [
        "Thanks for raising this other case. I believe that if there is just the path, it links for all the platforms, right? ",
        "what's the default? The question holds for all the items below\r\n\r\nWhat I don't like is that to do everything, we need to call\r\n```\r\nnode <script> -s -w -b -c\r\n```\r\n\r\nI would rather have the default case that does everything."
      ],
      "react-native-simplify-redundant-logic": [
        "@NickGerleman \r\n> nit: these checks are redundant, since one inverts both the subtraction order, and then inverts again for the difference.\r\n\r\n```suggestion\r\n  CGFloat difference = textHeight - lineHeight;\r\n  CGFloat verticalOffset = difference / 2.0;\r\n```",
        "instead of this prop... can't we just use the `_props` property and check whether it is initialized or not?",
        "Same as before. I'd extract the lambda to avoid duplicating code."
      ],
      "react-native-platform-aware-configuration-messages": [
        "No, android can't disable the new arch at runtime. You can disable some pieces, but it is not recommended."
      ],
      "react-native-minimize-public-api-surface": [
        "do we really need this extra function in the header? This will be a new public API that we need to mmaintain. Given that it is used only in RCTUtils.mm, can we keep it private in the .mm file instead?"
      ]
    },
    "profile": {
      "location": "London, UK",
      "company": "Meta",
      "blog": "https://medium.com/@riccardocipolleschi",
      "site_admin": false,
      "followers": 479,
      "following": 0
    }
  },
  "anthonyshew": {
    "repos": [
      "vercel/turborepo"
    ],
    "entries": [
      {
        "slug": "turborepo-boundary-case-handling",
        "title": "Boundary case handling"
      },
      {
        "slug": "turborepo-configuration-precision-matters",
        "title": "Configuration precision matters"
      },
      {
        "slug": "turborepo-define-api-boundaries",
        "title": "Define API boundaries"
      },
      {
        "slug": "turborepo-document-cache-strategies",
        "title": "Document cache strategies"
      },
      {
        "slug": "turborepo-document-configuration-alternatives",
        "title": "Document configuration alternatives"
      },
      {
        "slug": "turborepo-eliminate-code-duplication",
        "title": "Eliminate code duplication"
      },
      {
        "slug": "turborepo-framework-specific-entrypoints-organization",
        "title": "Framework-specific entrypoints organization"
      },
      {
        "slug": "turborepo-hybrid-monorepo-testing",
        "title": "Hybrid monorepo testing"
      },
      {
        "slug": "turborepo-keep-build-tooling-updated",
        "title": "Keep build tooling updated"
      },
      {
        "slug": "turborepo-know-your-implicit-configurations",
        "title": "Know your implicit configurations"
      },
      {
        "slug": "turborepo-link-terms-provide-examples",
        "title": "Link terms, provide examples"
      },
      {
        "slug": "turborepo-propagate-errors-with-context",
        "title": "Propagate errors with context"
      },
      {
        "slug": "turborepo-standardize-package-manager-commands",
        "title": "Standardize package manager commands"
      },
      {
        "slug": "turborepo-validate-configuration-structures",
        "title": "Validate configuration structures"
      },
      {
        "slug": "turborepo-validate-configurations-comprehensively",
        "title": "Validate configurations comprehensively"
      },
      {
        "slug": "turborepo-validate-performance-impact-first",
        "title": "Validate performance impact first"
      },
      {
        "slug": "turborepo-verify-test-commands",
        "title": "Verify test commands"
      }
    ],
    "comments": {
      "turborepo-boundary-case-handling": [
        "Classic me not knowing methods I have available."
      ],
      "turborepo-keep-build-tooling-updated": [
        "Run `npx @turbo/codemod upgrade` to upgrade to `turbo@2`. 🥳 ",
        "Run `npx @turbo/codemod upgrade` to upgrade to `turbo@2`. 🥳 "
      ],
      "turborepo-verify-test-commands": [
        "```suggestion\r\ncargo coverage -- --open\r\n```"
      ],
      "turborepo-document-configuration-alternatives": [
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```",
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```",
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```"
      ],
      "turborepo-standardize-package-manager-commands": [
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```",
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```"
      ],
      "turborepo-know-your-implicit-configurations": [
        "@chris-olszewski, I actually don't know this one. Can I ignore a lockfile? I'm figuring I can't since we go parse it separately from collecting file inputs.\r\n\r\n I shouldn't be able to, but can I? I'm trying to see if I can trick it into doing so and it doesn't look like it.",
        "```suggestion\n- `turbo.json`\n- Package manager lockfiles\n```",
        "```suggestion\r\n  may want to specify a range (for example, `\">=15\"`) according to your needs.\r\n  \r\n  Additionally, for older package managers, you may need to instruct your package manager to install peer dependencies with configuration, or add the dependency to `devDependencies` as a workaround.\r\n```",
        "```suggestion\r\nTo help with incremental migration or in situations where you can't use the `packageManager` field, you may use `--dangerously-disable-package-manager-check` to opt out of this check and assume the risks of unstable lockfiles producing unpredictable behavior. When disabled, Turborepo will attempt a best-effort discovery of the intended package manager meant for the repository.\r\n```",
        "```suggestion\r\nTo help with incremental migration or in situations where you cannot use the `packageManager` field, you may use `--dangerously-disable-package-manager-check` to opt out of this check and assume the risks of unstable lockfiles producing unpredictable behavior. When disabled, Turborepo will attempt a best-effort discovery of the intended package manager meant for the repository.\r\n```"
      ],
      "turborepo-define-api-boundaries": [
        "```suggestion\r\n/* Add x-artifact-tag header to artifact download endpoint response */\r\n```",
        "My gut reaction is to say no, but I could be talked into saying yes. This has come up _once_ in my memory, or has it been more? I'm hesitant to get into specifics in these sections beyond the feature lists in the pre-stable sections. Imagine stepping onto this page with no prior context...Seeing a specific mention for unstructured terminal output would feel...odd.\r\n\r\nMaybe a broader question: Do other CLI tools consider their stdout/stderr as semver-protected if its meant for human readability? I've never looked into or thought about this...",
        "Aligned, let's omit.",
        "```suggestion\r\n        needed for self-hosted Remote Caches that implement an endpoint that dynamically creates tokens.\r\n```",
        "```suggestion\r\nNote that this example uses the [Just-in-Time Package](/repo/docs/core-concepts/internal-packages#just-in-time-packages) pattern for simplicity. It exports TypeScript directly, but you might choose to use the [Compiled Package](/repo/docs/core-concepts/internal-packages#compiled-packages) pattern instead.\r\n```"
      ],
      "turborepo-framework-specific-entrypoints-organization": [
        "```suggestion\r\n- `./next-js/link`: A customized version of [the Next.js `Link` component](https://nextjs.org/docs/app/building-your-application/routing/linking-and-navigating#link-component) with props that are preset to your organization's preferences\r\n- `./svelte/link`: A customized version of an [`a` tag for Svelte](https://svelte.dev/docs/kit/link-options) with presets.\r\n```"
      ],
      "turborepo-document-cache-strategies": [
        "I wasn't sure if `no Remote Cache activity` is correct. Can someone confirm?",
        "We talked about this to make sure and it is correct!",
        "We need to keep this information. It's important that folks know that no artifacts will be cached."
      ],
      "turborepo-configuration-precision-matters": [
        "I'm upgrading the `engines` in package.json. I don't know if this is contentious or not. I'm seeing multiple versions of CI around the repository so I'm not sure if we consider this value important or not.",
        "As it turns out, we should definitely be doing this. This is now our source of truth."
      ],
      "turborepo-propagate-errors-with-context": [
        "Would it make sense to do:\r\n\r\n```suggestion\r\n    #[error(\"Unable to persist preferences. Please file a bug report.\")]\r\n```"
      ],
      "turborepo-hybrid-monorepo-testing": [
        "```suggestion\r\nYou can combine the benefits of both approaches by implementing a hybrid solution.This approach unifies local development using Vitest's Workspace approach while preserving Turborepo's caching in CI.  This comes at the tradeoff of slightly more configuration and a mixed task running model in the repository.\r\n```",
        "```suggestion\r\n      \"outputs\": [\"coverage/**\"]\r\n```"
      ],
      "turborepo-validate-performance-impact-first": [
        "Making sure to call extra attention here: Users mentioned they felt 1024 was too small, so I doubled it here. Not sure about if there's a reason I shouldn't do this or if a different value should be used. Just did this for the sake of discussion.",
        "Interesting, muchas gracias."
      ],
      "turborepo-link-terms-provide-examples": [
        "In general, we like to provide the link the first time we mention a term. The game I like to play is: Imagine your coworker just linked you this page and you're only glancingly aware that Turborepo is in your company's codebase. You're not terribly familiar with Turborepo but you're here to learn more about it.\r\n\r\nPutting yourself in those shoes, you're likely to read this \"Environment Modes\" term and think \"Uh, what's that?\", and want to be linked to more information in that moment. You could, of course, keep reading and make it to the end of this section, but you'd probably appreciated the link sooner.\r\n\r\nSo, that said, I'd be lifting the link that's a few lines below to here, and removing the sentence on 153. Both a context and conciseness win!",
        "Ah, heard! Ignore then. 😄 ",
        "Nice, good call."
      ],
      "turborepo-validate-configurations-comprehensively": [
        "```suggestion\r\n    /// Validates field placement to ensure root-only and package-only fields\r\n    /// are used in the correct configuration types.\r\n    ///\r\n    /// This uses an allowlist approach - ALL fields must be explicitly\r\n    /// categorized.\r\n```",
        "```suggestion\r\n#[error(\"$$ROOT$$ syntax is not allowed in globalDependencies, since globalDependencies is already relative to the root of the Workspace.\")]\r\n```",
        "```suggestion\r\n    #[error(\"\\\"$TURBO_ROOT$\\\" must be used at the start of glob.\")]\r\n```"
      ],
      "turborepo-validate-configuration-structures": [
        "```suggestion\r\n          // TODO: Our code was allowing both config files to exist. This is a bug, needs to be fixed.\r\n```",
        "No worries. Thanks for the tag. Will fix.",
        "Here's the fix: https://github.com/vercel/turborepo/pull/10105\r\n\r\nSorry about that!"
      ],
      "turborepo-eliminate-code-duplication": [
        "Some of my best \"past midnight with a screaming baby\" code, truly."
      ]
    },
    "profile": {
      "company": "Vercel",
      "blog": "https://shew.dev",
      "twitter_username": "anthonysheww",
      "site_admin": false,
      "followers": 310,
      "following": 4
    }
  },
  "dnr": {
    "repos": [
      "temporalio/temporal"
    ],
    "entries": [
      {
        "slug": "temporal-consistent-naming-patterns",
        "title": "Consistent naming patterns"
      },
      {
        "slug": "temporal-context-aware-network-calls",
        "title": "Context-aware network calls"
      },
      {
        "slug": "temporal-design-stable-apis",
        "title": "Design stable APIs"
      },
      {
        "slug": "temporal-dynamic-config-usage-principles",
        "title": "Dynamic config usage principles"
      },
      {
        "slug": "temporal-ensure-deterministic-execution",
        "title": "Ensure deterministic execution"
      },
      {
        "slug": "temporal-minimize-code-nesting-depth",
        "title": "Minimize code nesting depth"
      },
      {
        "slug": "temporal-minimize-credential-data-exposure",
        "title": "Minimize credential data exposure"
      },
      {
        "slug": "temporal-names-reflect-precise-behavior",
        "title": "Names reflect precise behavior"
      },
      {
        "slug": "temporal-optimize-api-consumption",
        "title": "Optimize API consumption"
      },
      {
        "slug": "temporal-precompute-and-cache",
        "title": "Precompute and cache"
      },
      {
        "slug": "temporal-protocol-buffer-organization",
        "title": "Protocol buffer organization"
      },
      {
        "slug": "temporal-safe-lock-usage-patterns",
        "title": "Safe lock usage patterns"
      },
      {
        "slug": "temporal-specific-assertion-methods",
        "title": "Specific assertion methods"
      },
      {
        "slug": "temporal-structured-contextual-logging",
        "title": "Structured contextual logging"
      },
      {
        "slug": "temporal-trust-getx-accessors",
        "title": "Trust GetX accessors"
      },
      {
        "slug": "temporal-use-dedicated-configuration-files",
        "title": "Use dedicated configuration files"
      }
    ],
    "comments": {
      "temporal-design-stable-apis": [
        "actually nevermind.. after reading more I think this should return a slice of counts, one per subqueue. taskQueueDB should work in terms of subqueues only, it shouldn't know anything about their semantics, like priority"
      ],
      "temporal-minimize-credential-data-exposure": [
        "The TLS fields in here seem like they could get pretty big. Does it really make sense to pass full client cert chains (keeping in mind that they've already been validated here)? If not, should we maybe copy this struct and clear some of the larger and less useful fields?\r\n\r\n`TLSSubject.Names` is probably not useful since the common ones are parsed into the fields above it (and of course everyone only uses CommonName)\r\n\r\n`TLSConnection.State.PeerCertificates`, `VerifiedChain`, etc. could be quite large and probably were not intended to be encoded as JSON\r\n\r\nDo you have an example of what a typical value looks like in JSON for TLS and non-TLS?\r\n"
      ],
      "temporal-minimize-code-nesting-depth": [
        "sure"
      ],
      "temporal-consistent-naming-patterns": [
        "oh, I meant that rename comment as a joke, I don't think we should rename it, it's better (more readable, less confusing) to be consistent with the name in workflowservice.",
        "Yeah.. I did it this way since several of the \"id\" fields are already there and I wanted to reuse them instead of having a duplicate id field. I think with a few wrapper functions we can minimize the risk of mistakes. Also it's more efficient to inline it instead of another message with just two fields.\r\n\r\nI'm like 80% sure I want to do it this way. Maybe let's reconsider after the whole first batch of PRs?",
        "I didn't want to rename the existing fields because of compatibility, but actually that only matters for proto-json which we're not using internally, so I'll rename them.",
        "On the idea of a submessage: messages with the new fields will be persisted in a different table, so it's not about compatibility per se. Though I would like to be able to use the same messages for both tables.\r\n\r\nFor AllocatedTaskInfo: we have a lot of those in memory and in db so I want to pay attention to efficiency there. I think it makes sense to just inline the two fields.\r\n\r\nFor SubqueueInfo: this is persisted, but not that many of them, so we could use a new field.\r\n\r\nFor InternalTaskQueueStatus: yes, could use a new field.\r\n\r\nI really hate having to do an extra allocation and chase another pointer just for these values. If we were using gogo it could be non-nullable and embedded, or we could just write custom getters and setters to avoid misuse :(\r\n\r\nLet me try it and see how it looks.",
        "I tried it. Mostly it's about the same readability since we still need wrappers to convert to/from structs. Some parts are slightly clearer. Overall I'd say slight improvement for readability. Adapting the existing code took `6 files changed, 74 insertions(+), 73 deletions(-)`  But it reduces the diff from main slightly.\r\n\r\nOverall I'm unhappy about the tradeoff but I'll go with it.",
        "```suggestion\r\n    temporal.api.workflowservice.v1.RecordWorkerHeartbeatRequest api_request = 2;\r\n```\r\n?\r\ndoesn't matter, just wondering",
        "also usually it's just called `request`"
      ],
      "temporal-use-dedicated-configuration-files": [
        "Can we put a comment here that says to not remove this, and add ignores to `proto/internal/buf.yaml` instead?",
        "don't do this, add lines here instead:\r\nhttps://github.com/temporalio/temporal/blob/main/proto/internal/buf.yaml#L14\r\n",
        "I know, but using that file is a better way to disable the check temporarily:\r\n- it's scoped to specific files instead of everything\r\n- if you forget to re-enable it, someone will notice eventually\r\n- keeps git history of the makefile cleaner"
      ],
      "temporal-precompute-and-cache": [
        "this is parsing the query on every evaluation? shouldn't that happen upfront in newWorkerQueryEngine?",
        "Yes, `&cv` would allocate a new ConstrainedValue, which would defeat the caching. This returns a pointer into the given slice, which comes directly from the Client. I'll add a comment",
        "Ideally we would do the exclusion before the call to get the dynamic config, to avoid that call if we're excluding anyway",
        "It'll be less expensive after https://github.com/temporalio/temporal/pull/7052 that caches conversions"
      ],
      "temporal-context-aware-network-calls": [
        "Would it be possible to do all the headers in one call to `metadata.AppendToOutgoingContext`?",
        "Can you use NewRequestWithContext or WithContext to make the http call context-aware? "
      ],
      "temporal-dynamic-config-usage-principles": [
        "we don't actually need a dynamic config here since we're not going to modify it in a real environment, just unit tests, so we can do it like:\r\n```suggestion\r\n    RateLimiterRefreshInterval: time.Minute,\r\n```\r\nand get rid of the dynamic config setting. then just set it to another value in the test.",
        "Maybe say a little more here about what happens when the percentage is above or below this threshold?",
        "But the setting has a single meaning and a specific effect.. the docstring should describe those. And the name should too, ideally. There shouldn't be multiple consumers with different semantics for the same setting.\r\n\r\nAnyway, 0.05 isn't \"the percentage of hosts that are not yet ready to serve traffic\", the percentage of hosts not ready to serve traffic is usually 0%, sometimes 10%, sometimes 50%, etc. The setting is a threshold for some behavior to change based on that value.",
        "should be dynamic config? (also to enable/disable)",
        "don't take the whole Collection here, take a dynamicconfig.DurationPropertyFn (just `func () time.Duration`). that makes it easier to test and also more efficient (by a tiny bit).\r\n\r\nyou can use dynamicconfig.GetDurationPropertyFn to create a static one for tests"
      ],
      "temporal-optimize-api-consumption": [
        "let's factor out:\r\n```\r\ncall() {\r\n  curl -fL -X POST -H \"Accept: application/vnd.github.v3+json\" -H \"Authorization: token $PAT\" \"$@\"\r\n}\r\n```\r\nthen use it like\r\n```\r\ncall \"https://api.github.com/repos/$PARENT_REPO/actions/workflows/$WORKFLOW_ID/dispatches\" -d '{\"ref\":'\"$PARENT_BRANCH\"', \"inputs\": { \"repo\":'\"$REPO\"', \"branch\":'\"$BRANCH\"' }}'\r\n```",
        "get both of these with one call, like\r\n```suggestion\r\n  result=$(call \"https://api.github.com/repos/$PARENT_REPO/actions/runs/$run_id\")\r\n  status=$(echo \"$result\" | jq -r .status)\r\n  conclusion=$(echo \"$result\" | jq -r .conclusion)\r\n```"
      ],
      "temporal-structured-contextual-logging": [
        "use log tags, not string manipulation. or just remove this before merging",
        "this one seems like it needs namespace [and/or id] tags to be helpful"
      ],
      "temporal-trust-getx-accessors": [
        "The GetX() wrappers check for nil receivers, so you can just write `if userData.GetData() == nil`",
        "this may be nil so you have to be careful:\r\n```suggestion\r\n\t\ttaskQueueType := req.GetTaskQueueType()\r\n\t\ttaskQueueUserData := userData.GetData().GetPerType()[int32(taskQueueType)]\r\n\t\tdescrResp.DescResponse.Configs = taskQueueUserData.GetTaskQueueConfig()\r\n```",
        "I don't think we need to deepcopy the key, you can't use reference types as keys, and for things like structs or arrays, you can't mutate a field in the key.\r\n\r\nI'll add a test.",
        "You don't actually need these two if statements, nils are handled correctly everywhere, you can just do\r\n```suggestion\r\n\t\t\ttypedUserData := userData.GetData().GetPerType()[int32(pm.Partition().TaskType())]\r\n\t\t\tfor _, v := range typedUserData.GetDeploymentData().GetVersions() {\r\n```",
        "The map lookup would return nil.. `nil.GetDeploymentData().GetVersions()` is fine, it just returns a nil map and the range over it does nothing. If you did something like assign to a field that would npe though."
      ],
      "temporal-ensure-deterministic-execution": [
        "Changes to this condition can cause nondeterminism errors so if we do need to do this (and I don't think we do) then it should be versioned.\r\n\r\nAs far as I know proto.Equal is totally fine and correct here. If you disagree could you point to some evidence otherwise?\r\n\r\n(Yes there are replay tests but they can't catch all edge cases)",
        "Just because it's flagged by workflowcheck doesn't mean that it's actually nondeterministic. And just because it's not flagged doesn't mean that it is determistic (how did proto.Marshal get by? that one actually is documented to be nondetermistic)",
        "I was only suggesting reverting the changes to the schedule wf (at least the proto.Equal) and adding ignore comments, not the whole PR"
      ],
      "temporal-safe-lock-usage-patterns": [
        "we usually use \"lock\"\n```suggestion\n\t\ttaskTrackerLock            sync.RWMutex\n```"
      ],
      "temporal-protocol-buffer-organization": [
        "I'm having second thoughts here... duplicating all of this structure seems like a waste of effort and opportunity for bugs. as you said, we have an HLC value available at the top level, and that's the one we're using to merge anyway... what do you think about basically reusing the public api message types here?",
        "put this field on line 380 above min_task_id, so we always have <pass, id> next to each other and in that order wherever they appear"
      ],
      "temporal-names-reflect-precise-behavior": [
        "elsewhere in this file I see a lot of checks against `wh.config.MaxIDLengthLimit()`, so might as well use that one for identity to be consistent",
        "Oh that's much better!",
        "\"attachRateLimiter\" doesn't \"attach\" anything, it just returns a new RateLimiter. also it doesn't use anything from the taskQueueConfig or the MatcherTestSuite. so this should just be `func newDefaultRateLimiter() quotasRateLimiter`",
        "I'd agree with that rename.. \"drainBuffer\" sounds like it'll be empty when it returns"
      ],
      "temporal-specific-assertion-methods": [
        "```suggestion\r\n\t\trequire.Empty(t, description.GetPendingActivities())\r\n```",
        "```suggestion\r\n\t\trequire.Empty(t, description.GetPendingActivities())\r\n```",
        "```suggestion\r\n\t\trequire.NotZero(t, startedActivityCount.Load())\r\n```"
      ]
    },
    "profile": {
      "blog": "https://dnr.im/tech/",
      "site_admin": false,
      "followers": 36,
      "following": 0
    }
  },
  "sosukesuzuki": {
    "repos": [
      "prettier/prettier"
    ],
    "entries": [
      {
        "slug": "prettier-add-explanatory-comments",
        "title": "Add explanatory comments"
      },
      {
        "slug": "prettier-angular-syntax-parsing",
        "title": "Angular syntax parsing"
      },
      {
        "slug": "prettier-api-documentation-clarity",
        "title": "API documentation clarity"
      },
      {
        "slug": "prettier-benchmark-performance-optimizations",
        "title": "Benchmark performance optimizations"
      },
      {
        "slug": "prettier-cache-correctness-validation",
        "title": "Cache correctness validation"
      },
      {
        "slug": "prettier-documentation-clarity-standards",
        "title": "Documentation clarity standards"
      },
      {
        "slug": "prettier-documentation-example-consistency",
        "title": "Documentation example consistency"
      },
      {
        "slug": "prettier-ensure-semantic-naming-accuracy",
        "title": "Ensure semantic naming accuracy"
      },
      {
        "slug": "prettier-environment-specific-error-handling",
        "title": "Environment-specific error handling"
      },
      {
        "slug": "prettier-maintain-api-backward-compatibility",
        "title": "maintain API backward compatibility"
      },
      {
        "slug": "prettier-organize-tests-properly",
        "title": "Organize tests properly"
      },
      {
        "slug": "prettier-prefer-efficient-algorithms",
        "title": "prefer efficient algorithms"
      },
      {
        "slug": "prettier-refactor-complex-conditions",
        "title": "refactor complex conditions"
      },
      {
        "slug": "prettier-test-all-variations",
        "title": "Test all variations"
      },
      {
        "slug": "prettier-use-descriptive-variable-names",
        "title": "Use descriptive variable names"
      },
      {
        "slug": "prettier-validate-configuration-changes",
        "title": "validate configuration changes"
      },
      {
        "slug": "prettier-validate-configuration-values",
        "title": "validate configuration values"
      },
      {
        "slug": "prettier-verify-optional-chaining-necessity",
        "title": "Verify optional chaining necessity"
      },
      {
        "slug": "prettier-vue-syntax-parsing-robustness",
        "title": "Vue syntax parsing robustness"
      }
    ],
    "comments": {
      "prettier-api-documentation-clarity": [
        "Nits for Node.js beginners\r\n\r\n```suggestion\r\nStrings provided to `plugins` are ultimately passed to [`import()` expression](https://nodejs.org/api/esm.html#import-expressions), so you can provide a module/package name, a path, or anything else `import()` takes.\r\n```"
      ],
      "prettier-environment-specific-error-handling": [
        "I think assertions for production environments should be considered separately from assertions for development environments.\r\nTherefore, I propose to add a simple validation like the one below and remove the relevant part from `assertComment` and wrap `assertComment` call in `if (process.env.NODE_ENV !== \"production\")`:\r\n\r\n```js\r\nif (!isLineComment(comment) && !isBlockComment(comment)) {\r\n    throw new TypeError(`Unknown comment type: \"${comment.type}\".`);\r\n}\r\n```"
      ],
      "prettier-cache-correctness-validation": [
        "For minimum speed and security, why not check the length of the file content rather than the content of the file?",
        "Thanks, I've added `--cache-strategy` option!",
        "[9cf4a31](https://github.com/prettier/prettier/pull/12800/commits/9cf4a312aafa1037d48d37f034c857018aa3aeb4)"
      ],
      "prettier-validate-configuration-changes": [
        "I prefer using `\"error\"` in even non CI environment.",
        "FYI Some other packages are duduped by `npx yarn-dedupulicate`. [90d6098](https://github.com/prettier/prettier/pull/13872/commits/90d6098cf250ec25714e10b15ba39594c24d04fe)"
      ],
      "prettier-benchmark-performance-optimizations": [
        "We originally did not intend to add the `--cache-strategy` option and always intended to use `metadata`. However, following a comment from @7rulnik ( https://github.com/prettier/prettier/pull/12800#discussion_r878044023 ), we added the `--cache-strategy` option for CI use cases so that `content` can be used.\r\n\r\nI had thought there was a tradeoff between `metadata` and `content` as follows:\r\n\r\n|        | `metadata`  | `content`   |\r\n| ------ | ----------- | ----------- |\r\n| for CI | :no_good:   | :ok_person: |\r\n| perf   | :ok_person: | :no_good:   |\r\n\r\n\r\n\r\nHowever, following your comment, I investigated and found that there is actually not much difference in performance between the two.\r\n\r\n***\r\n\r\n`--cache-strategy=metadata`:\r\n\r\n```\r\n$ ./bin/prettier.js . \"!test*\" --check --write --cache\r\nChecking formatting...\r\nAll matched files use Prettier code style!\r\n✨  Done in 16.51s.\r\n\r\n$ ./bin/prettier.js . \"!test*\" --check --write --cache\r\nChecking formatting...\r\nAll matched files use Prettier code style!\r\n✨  Done in 6.56s.\r\n```\r\n\r\n`--cache-strategy=content`:\r\n\r\n```\r\n$ ./bin/prettier.js . \"!test*\" --check --write --cache --cache-strategy=content\r\nChecking formatting...\r\nAll matched files use Prettier code style!\r\n✨  Done in 16.46s.\r\n\r\n$ ./bin/prettier.js . \"!test*\" --check --write --cache --cache-strategy=content\r\nChecking formatting...\r\nAll matched files use Prettier code style!\r\n✨  Done in 6.81s.\r\n```\r\n\r\n***\r\n\r\nFrom this result I think we can remove the `--cache-strategy` option and always use `content`.\r\n\r\nWhat do you think? @7rulnik, @fisker",
        "I created following script:\r\n\r\n<details>\r\n  <summary>test.benchmark.js</summary>\r\n  \r\n```js\r\nconst Benchmark = require(\"benchmark\");\r\nconst { execaSync } = require(\"./vendors/execa\");\r\n\r\nconst runWithMetadata = () =>\r\n  execaSync(\"./bin/prettier.js\", [\".\", \"!test*\", \"--check\", \"--cache\"]);\r\n\r\nconst runWithContent = () =>\r\n  execaSync(\"./bin/prettier.js\", [\r\n    \".\",\r\n    \"!test*\",\r\n    \"--check\",\r\n    \"--cache\",\r\n    \"--cache-strategy\",\r\n    \"content\",\r\n  ]);\r\n\r\nconst clearCache = () =>\r\n  execaSync(\"rm\", [\"-f\", \"./node_modules/.cache/prettier/.prettier-cache\"]);\r\n\r\nfunction runSuite01() {\r\n  const suite01 = new Benchmark.Suite();\r\n\r\n  clearCache();\r\n  runWithMetadata();\r\n\r\n  suite01\r\n    .add(\"--cache-strategy=metadata\", function () {\r\n      runWithMetadata();\r\n    })\r\n    .on(\"cycle\", function (event) {\r\n      console.log(String(event.target));\r\n    })\r\n    .run();\r\n}\r\n\r\nfunction runSuite02() {\r\n  const suit02 = new Benchmark.Suite();\r\n\r\n  clearCache();\r\n  runWithContent();\r\n\r\n  suit02\r\n    .add(\"--cache-strategy=content\", function () {\r\n      runWithContent();\r\n    })\r\n    .on(\"cycle\", function (event) {\r\n      console.log(String(event.target));\r\n    })\r\n    .run();\r\n}\r\n\r\nrunSuite01();\r\nrunSuite02();\r\n\r\n```\r\n\r\n</details>\r\n\r\nThe results were as follows:\r\n\r\n```\r\n--cache-strategy=metadata x 0.30 ops/sec ±6.05% (5 runs sampled)\r\n--cache-strategy=content x 0.30 ops/sec ±0.94% (5 runs sampled)\r\n```\r\n\r\nI ran several other benchmarks and found no significant differences. This is strange.\r\n\r\nBut, ESLint cache (Almost the same mechanism as our cache) with similar results.\r\n\r\n***\r\n\r\n**ESLint with `--cache-strategy=metadata`:**\r\n\r\n```\r\n$ cross-env EFF_NO_LINK_RULES=true eslint . --format friendly --cache --cache-strategy=metadata\r\n✨  Done in 16.27s.\r\n\r\n$ cross-env EFF_NO_LINK_RULES=true eslint . --format friendly --cache --cache-strategy=metadata\r\n✨  Done in 3.05s.\r\n```\r\n\r\n**ESLint with `--cache-strategy=content`:**\r\n\r\n```\r\n$ cross-env EFF_NO_LINK_RULES=true eslint . --format friendly --cache --cache-strategy=content\r\n✨  Done in 13.69s.\r\n\r\n$ cross-env EFF_NO_LINK_RULES=true eslint . --format friendly --cache --cache-strategy=content\r\n✨  Done in 2.96s.\r\n```\r\n\r\n***\r\n\r\nI ran the benchmark using Prettier's source code this time, but the results may be different for projects where each file is more huge.",
        "(Even in the Babel source code, there was little difference in speed between `metadata` and `content`.)",
        "To maintain both `--cache-strategy=content` and `--cache-strategy=metadata` isn't hard. So I think we can keep both.",
        "I created new benchmark script https://gist.github.com/sosukesuzuki/ded586261381a6261e02a69b188a8e44\r\n\r\nResults ware follows:\r\n\r\n```\r\n======= 100 lines, --cache-strategy=metadata, 0ms ============\r\n======= 1000 lines, --cache-strategy=metadata, 1ms ============\r\n======= 10000 lines, --cache-strategy=metadata, 1ms ============\r\n======= 100000 lines, --cache-strategy=metadata, 0ms ============\r\n======= 100 lines, --cache-strategy=content, 17ms ============\r\n======= 1000 lines, --cache-strategy=content, 12ms ============\r\n======= 10000 lines, --cache-strategy=content, 13ms ============\r\n======= 100000 lines, --cache-strategy=content, 12ms ============\r\n```\r\n\r\nThis benchmark script measures execution time when cache is enabled. There is only one file of interest, and each test combines the number of lines in the file with the cache strategy.\r\n\r\nAccording to this, performance still seems to be better with `--cache-strategy=metadata`(However, the number of lines does not seem to matter much). I think the reason why the previously posted benchmarks did not show a valid difference is probably due to overhead in areas other than formatting.",
        "> But I definitely think we should default to content.\r\n\r\n@lydell  Why you prefer `content`? Because it works on CI?",
        "I've updated the documentation to use `content` by default. For normal projects, there is no performance difference, and `content` is more convenient."
      ],
      "prettier-documentation-example-consistency": [
        "Sounds good!"
      ],
      "prettier-maintain-api-backward-compatibility": [
        "You are right. However, Import Assertions was Stage 3 and was dropped from TC39. Therefore, it would make sense for us to drop its syntax. And I think v3 is a good opportunity to do so.",
        "Thank you @fisker and @nicolo-ribaudo. I got it.",
        "[f1d6a38](https://github.com/prettier/prettier/pull/14863/commits/f1d6a3865d9f523fe311025a2522c145b50d0507)"
      ],
      "prettier-documentation-clarity-standards": [
        "Thank you for pointing it out. I would like to know if there is a better wording.",
        "looks good to me",
        "Since `standalone.mjs` does not exist in version 2.x, it would be good to clarify this.\r\n\r\n```suggestion\r\n- ES modules: `standalone.mjs`, starting in version 3.0 (In version 2, `esm/standalone.mjs`.)\r\n```"
      ],
      "prettier-verify-optional-chaining-necessity": [
        "I am wondering if we should include brand checking for safety..."
      ],
      "prettier-validate-configuration-values": [
        "[fefe40a](https://github.com/prettier/prettier/pull/12800/commits/fefe40a7b51edbee5a09aabe2b9802d63ade40af)",
        "?\r\n```suggestion\r\n// To prevent `chalk` module from being included in the `standalone.js` bundle,\r\n// it will take that as an argument if needed.\r\nconst getFlagSchema = (colorsModule) =>\r\n```"
      ],
      "prettier-add-explanatory-comments": [
        "Can we add comment  like `// Block names that can have parameters` ?",
        "It seems unreadable to me... Can you add comment or extract these conditions to variables?",
        "I expect comments that describes what concrete syntax each matcher represents.",
        "Can you add a comment that explains why `\"stylus\"` is here? e.g.:\r\n\r\n```suggestion\r\n  // Prettier does not officially support stylus.\r\n  // But, we need to handle `\"stylus\"` here for printing a style block in Vue SFC as stylus code by external plugin.\r\n  // https://github.com/prettier/prettier/pull/12707\r\n  if (lang === \"stylus\") {\r\n    return inferParserByLanguage(\"stylus\", options);\r\n  }\r\n```",
        "What do you think the following comment?\r\n\r\n```js\r\n// For example, there is the following key-value pair:\r\n//\r\n//   \"xs-only\": \"only screen and (max-width: #{map-get($grid-breakpoints, \"sm\")-1})\"\r\n//\r\n// \"only screen and (max-width: #{map-get($grid-breakpoints, \" is a \"value-string\"\r\n// and \"sm\" is a \"value-word\"\r\n// We should not insert any spaces and lines here.\r\n```"
      ],
      "prettier-vue-syntax-parsing-robustness": [
        "Thank you.\r\n\r\nIt is unsafe to use TS parser always. For example, The below code is valid as JS and TS.\r\n\r\n```ts\r\nconst foo = doSomething<T1 | T2>(param)\r\n```\r\n\r\n```js\r\n// as JS\r\nconst foo = (doSomething < T1) | (T2 > param);\r\n```\r\n\r\nTS and JS parse differently for such codes. Therefore, it is safe to parse as TypeScript only when the script tag explicitly specifies the use of TS.",
        "I've fixed to avoid using `ast.walk`. [35b2600](https://github.com/prettier/prettier/pull/12584/commits/35b2600c0d3b7d295dfc31ce0877c030bf8af51b)"
      ],
      "prettier-ensure-semantic-naming-accuracy": [
        "`PropertyKey` is defined in TypeScript (`/node_modules/typescript`)"
      ],
      "prettier-angular-syntax-parsing": [
        "We need to normalize node name `else if`. Angular allows `else         if` current our implementation does not:\r\n\r\n**Prettier pr-15606**\r\n[Playground link](https://deploy-preview-15606--prettier.netlify.app/playground/#N4Igxg9gdgLgprEAuEABAlgMwAQAoCU2wAOlAL7apwA2AznNo1nocGSADQgQAOM60WslABDAE5iIAdwAK4hEJQjqUkQE8hXAEZiRYANZwYAZREBbOABl0UOMkzL623QaPGeemwHNkMMQFc4LnozdF8AoJBab2o4AEV-CHh7R0iAK1oAD2MY+MTkpAc6SIBHfLgZSR5FEBFaAFpbOAATFs4QPxF0am8AYQgzMxFkWupqduioL1iAQRg-dC1-eBk4MWtbFOKuAAsYM2oAdR30eFoPMDhjBVP0ADdTtRGwWk0QO8CASShW2GMwMToPgzH7GGBqWJbJwgHiSeiHXQ8EawuD0MR3OxcGxomCVEReIZQyIeMRokYiKb+ajidqwmwwQ7oZowHbIAAcAAYuGI4GV0Dy8QThoVUlwYCItIzmaykAAmLj+egAFQliiK0LgZi0LVazUsFK8-nxcAAYhAxEN5t5ycsICAyGQgA)\r\n<!-- prettier-ignore -->\r\n```sh\r\n--parser angular\r\n```\r\n\r\n**Input:**\r\n<!-- prettier-ignore -->\r\n```html\r\n@if () {\r\n} @else   if () {}\r\n```\r\n\r\n**Output:**\r\n<!-- prettier-ignore -->\r\n```html\r\nError: Unknown block name: else   if\r\n    at is (https://deploy-preview-15606--prettier.netlify.app/lib/plugins/html.js:14:11151)\r\n    at Object.ba [as print] (https://deploy-preview-15606--prettier.netlify.app/lib/plugins/html.js:14:12544)\r\n    at Yn (https://deploy-preview-15606--prettier.netlify.app/lib/standalone.js:30:7727)\r\n    at D (https://deploy-preview-15606--prettier.netlify.app/lib/standalone.js:30:7547)\r\n    at s (https://deploy-preview-15606--prettier.netlify.app/lib/standalone.js:30:7355)\r\n    at rt (https://deploy-preview-15606--prettier.netlify.app/lib/plugins/html.js:14:7858)\r\n    at https://deploy-preview-15606--prettier.netlify.app/lib/plugins/html.js:14:8860\r\n    at https://deploy-preview-15606--prettier.netlify.app/lib/standalone.js:26:1596\r\n    at Vt.each (https://deploy-preview-15606--prettier.netlify.app/lib/standalone.js:26:1499)\r\n    at Vt.map (https://deploy-preview-15606--prettier.netlify.app/lib/standalone.js:26:1576)\r\n```"
      ],
      "prettier-use-descriptive-variable-names": [
        "Is this a just boolean value? If true I prefer naming as `shouldCache`. To clarify whether this is an object that has cached values or a just boolean:\r\n\r\n```suggestion\r\n  #shouldCache;\r\n```",
        "This function is only used in `config-searcher.js`, right? I prefer to move from `common.js` to `config-searcher.js` and name the function something like `createCachedSearchFunction`. The name createCachedFunction is too generic.",
        "The variable name clone sounds like a function, so cloned or clonedNode is preferable.",
        "I prefer to define the following function in `language-markdown/utils.js`:\r\n\r\n```js\r\nfunction isMarkdownlintComment(node) {\r\n  return (\r\n    node.type === \"html\" &&\r\n    node.value.startsWith(\"<!--\") &&\r\n    node.value(\"-->\") &&\r\n    node.value === \"markdownlint-disable-next-line\"\r\n  );\r\n}\r\n```\r\n\r\nI also prefer variable name `isPrevNodeMarkdownlintComment`.",
        "I prefer `whitespaceCharacters` or `whitespaceChars`"
      ],
      "prettier-organize-tests-properly": [
        "This is actually for both invalid and valid. We should separate tests like:\r\n\r\n```js\r\ndescribe(\"doc builders\", () => {\r\n  test(\"Invalid usage\", () => {\r\n    // tests\r\n  });\r\n  test(\"Valid usage\", () => {\r\n    // tests\r\n  });\r\n});\r\n```",
        "This will not be called if `runPrettier` is rejected. So I think we should call `runPrettier` in `try` block and call this in `finally` block.",
        "Because changing `process.version` affects out of this test case.\r\n\r\nIf we add the test like below under the `test(\"node version error\", async () => ...);`.\r\n\r\n```js\r\ntest(\"checks version\", () => {\r\n  expect(process.version).toBe(\"v14.18.2\");\r\n});\r\n```\r\n\r\nif current node version is `14.18.2` and `runPrettier` is rejected, this test is failed via:\r\n\r\n```\r\nchecks version\r\n\r\n    expect(received).toBe(expected) // Object.is equality\r\n\r\n    Expected: \"v14.18.2\"\r\n    Received: \"v8.0.0\"\r\n\r\n      116 |\r\n      117 | test(\"checks version\", () => {\r\n    > 118 |   expect(process.version).toBe(\"v14.18.2\");\r\n          |                           ^\r\n      119 | });\r\n      120 |\r\n```\r\n\r\nSuch tests are rarely written, but it's better to use `finally` just in case."
      ],
      "prettier-refactor-complex-conditions": [
        "This isn't related directly to your change, but can you refactor this complex inline condition?\r\n\r\ne.g.\r\n\r\n```js\r\nif (value !== \"\" && canLineBreakBeConvertedToSpace(path, value, adjacentNodes)) {\r\n  if (isBreakable) {\r\n    return line;\r\n  }\r\n  return \" \";\r\n}\r\nif (isBreakable) {\r\n  return softline;\r\n}\r\nreturn \"\";\r\n```",
        "Also can you extract `canLineBreakBeConvertedToSpace` as an another function?",
        "> Do you think this is still too complex? Nested if looks ugly and verbose for me.\r\n\r\nThank you, the below code that you showed first looks enough readable to me (I prefer the first one than second one.)\r\n\r\n```js\r\nif (value !== \"\" && canLineBreakBeConvertedToSpace(path, value, adjacentNodes)) {\r\n  return isBreakable ? line : \" \";\r\n}\r\n// Space is not allowed, just \"\" or \"\\n\"\r\nreturn isBreakable ? softline : \"\";\r\n```"
      ],
      "prettier-test-all-variations": [
        "If we don't have a test for non-promise besides this one, I think we should add a new test instead of modifying this one.",
        "[b78e90f](https://github.com/prettier/prettier/pull/15888/commits/b78e90f25226056de6c869411e7aabaac402f940)"
      ],
      "prettier-prefer-efficient-algorithms": [
        "@fisker You’re right—thanks! (Actually, we should be using `===` instead of `!==`.)\r\n\r\nI’ve also noticed a more fundamental problem: the location info for value-words that start with a hyphen is brokwn. For the `--foo` node, both `locStart` and `locEnd` are reported two characters too far forward.\r\n\r\nThis looks like a parser bug. I worked around it with a helper function called `fixValueWordLoc` ( https://github.com/prettier/prettier/pull/17398/commits/41838a2430ee3d000b1db9ec01b5dbedd7d818c5 )\r\n\r\nDo you have a better idea?"
      ]
    },
    "profile": {
      "location": "Tokyo, Japan",
      "company": "@ubie-oss",
      "blog": "https://sosukesuzuki.dev",
      "twitter_username": "__sosukesuzuki",
      "site_admin": false,
      "followers": 581,
      "following": 15
    }
  },
  "Gargron": {
    "repos": [
      "mastodon/mastodon"
    ],
    "entries": [
      {
        "slug": "mastodon-avoid-deprecated-sass-syntax",
        "title": "avoid deprecated SASS syntax"
      },
      {
        "slug": "mastodon-choose-appropriate-exception-types",
        "title": "Choose appropriate exception types"
      },
      {
        "slug": "mastodon-complete-translatable-sentences",
        "title": "Complete translatable sentences"
      },
      {
        "slug": "mastodon-comprehensive-authorization-checks",
        "title": "comprehensive authorization checks"
      },
      {
        "slug": "mastodon-extract-view-complexity",
        "title": "Extract view complexity"
      },
      {
        "slug": "mastodon-framework-aware-text-composition",
        "title": "Framework-aware text composition"
      },
      {
        "slug": "mastodon-hook-responsibility-separation",
        "title": "Hook responsibility separation"
      },
      {
        "slug": "mastodon-leverage-existing-configuration-sources",
        "title": "leverage existing configuration sources"
      },
      {
        "slug": "mastodon-migration-data-dependencies",
        "title": "migration data dependencies"
      },
      {
        "slug": "mastodon-minimize-html-attack-surface",
        "title": "Minimize HTML attack surface"
      },
      {
        "slug": "mastodon-network-resource-limits",
        "title": "Network resource limits"
      },
      {
        "slug": "mastodon-optimize-database-queries",
        "title": "Optimize database queries"
      },
      {
        "slug": "mastodon-optimize-react-hooks",
        "title": "Optimize React hooks"
      },
      {
        "slug": "mastodon-prefer-early-returns",
        "title": "prefer early returns"
      },
      {
        "slug": "mastodon-prefer-established-configuration-patterns",
        "title": "prefer established configuration patterns"
      },
      {
        "slug": "mastodon-use-accessible-terminology",
        "title": "Use accessible terminology"
      },
      {
        "slug": "mastodon-use-contextually-descriptive-names",
        "title": "Use contextually descriptive names"
      },
      {
        "slug": "mastodon-use-descriptive-specific-names",
        "title": "Use descriptive specific names"
      },
      {
        "slug": "mastodon-use-semantic-null-handling",
        "title": "Use semantic null handling"
      }
    ],
    "comments": {
      "mastodon-use-semantic-null-handling": [
        "Thanks, I think I made it better!"
      ],
      "mastodon-optimize-react-hooks": [
        "You can define `handleDocumentClick` locally within the `useEffect` and avoid having to use `useCallback` for it.",
        "Refs do not need to be passed as dependencies. This `useEffect` can have a dependency of `[]`."
      ],
      "mastodon-choose-appropriate-exception-types": [
        "Thank you!"
      ],
      "mastodon-framework-aware-text-composition": [
        "I think this may be wrong. When I sign in and get this message, the terms will long have been in effect already. I think it should be in past tense.",
        "This is a common footgun, Rails will combine the message string here with the localized attribute name (`:fields`) so the printed out message will be \"Fields Names of extra profile fields with values cannot be empty\""
      ],
      "mastodon-leverage-existing-configuration-sources": [
        "I think it might be simpler to fetch a full page of tags from the API and limit to displaying the first 4 in the sidebar, if it means avoiding keeping track of \"where will this be displayed\" in actions/reducers. That's what we do with lists."
      ],
      "mastodon-migration-data-dependencies": [
        "I'm not sure how much it makes sense to modify people's blocks after the initial import. We can't really ever remove anything, right? There's something similar for user roles."
      ],
      "mastodon-avoid-deprecated-sass-syntax": [
        "I noticed it just now, I should've just done the math in my head right away."
      ],
      "mastodon-extract-view-complexity": [
        "This would make sense to me if multiple views reused the form, but this is the only occurence, so I do not really see the benefit."
      ],
      "mastodon-use-contextually-descriptive-names": [
        "I am not so sure about long variable names but I added a comment to clarify it in the code at least.",
        "It was somewhat intentional, since this is a global controller, I thought the name params would be fitting. But I can think of something else."
      ],
      "mastodon-minimize-html-attack-surface": [
        "I think allowing `style` might actually introduce potential security issues. Not sure this is necessary."
      ],
      "mastodon-prefer-early-returns": [
        "Refactored."
      ],
      "mastodon-use-descriptive-specific-names": [
        "I would suggest `silo` as well..."
      ],
      "mastodon-use-accessible-terminology": [
        "https://en.wikipedia.org/wiki/Scunthorpe_problem"
      ],
      "mastodon-comprehensive-authorization-checks": [
        "It seems like currently there is no safeguard for streaming from private groups here?"
      ],
      "mastodon-complete-translatable-sentences": [
        "I really doubt that a number you can check once per year will encourage any kind of day-to-day behaviour change. This feature is meant to be fun rather than simply a collection of metrics, so presentation matters a lot. But \"fun\" is difficult to localize equally to all languages and cultures. ",
        "Okay, it's a bit clunky but it works."
      ],
      "mastodon-optimize-database-queries": [
        "Another alternative is fetching all partial words from the table and building a regex from them, this is what we do for IP blocks. Let me know what you'd prefer. Neither option seems super optimized for very large numbers of blocks.",
        "I believe this index is not needed, as the index above will be used since it begins with `account_id`",
        "This could be rewritten with `exists?` without having to perform a `COUNT()`"
      ],
      "mastodon-prefer-established-configuration-patterns": [
        "We have `reduceMotion` exported from `mastodon/initial_state` which is manually set in preferences. Probably we should use that since that's used everywhere else?"
      ],
      "mastodon-hook-responsibility-separation": [
        "Maybe this could've been part of `useLinks`?",
        "I see that you have to \"fake\" typing into the input here because most of the logic happens in the `handleChange` handler. Perhaps it is a sign that some of that logic should be moved out to a `useEffect`.",
        "I still get a bit confused about `useCallback` depending on another `useCallback` so wanted to avoid that by extracting the helper function outside the component. The most important bit here (and I think I should've left a comment maybe) is that it's important the `event` literal is instantiated *outside* the `setHotkeyEvents` callback because it runs at a later time and so the `videoRef.current.paused` value will already be different. By passing the `event` literal to this function instead, I keep it all on one line where it's called, and the `event` value is static."
      ],
      "mastodon-network-resource-limits": [
        "The alternative is to have one channel for every public or unlisted post by every account, do I understand that correctly?"
      ]
    },
    "profile": {
      "location": "Germany",
      "company": "@mastodon",
      "blog": "https://zeonfederated.com",
      "site_admin": false,
      "followers": 2275,
      "following": 17
    }
  },
  "sapphi-red": {
    "repos": [
      "vitejs/vite"
    ],
    "entries": [
      {
        "slug": "vite-break-down-complex-functions",
        "title": "Break down complex functions"
      },
      {
        "slug": "vite-clean-configuration-organization",
        "title": "Clean configuration organization"
      },
      {
        "slug": "vite-clean-network-resources",
        "title": "Clean network resources"
      },
      {
        "slug": "vite-complete-deployment-commands",
        "title": "Complete deployment commands"
      },
      {
        "slug": "vite-descriptive-consistent-naming",
        "title": "Descriptive consistent naming"
      },
      {
        "slug": "vite-document-code-purposefully",
        "title": "Document code purposefully"
      },
      {
        "slug": "vite-document-protocol-configurations-clearly",
        "title": "Document protocol configurations clearly"
      },
      {
        "slug": "vite-ensure-documentation-accuracy",
        "title": "Ensure documentation accuracy"
      },
      {
        "slug": "vite-environment-variable-management",
        "title": "Environment variable management"
      },
      {
        "slug": "vite-escape-html-content-properly",
        "title": "Escape HTML content properly"
      },
      {
        "slug": "vite-evolve-apis-with-compatibility",
        "title": "Evolve APIs with compatibility"
      },
      {
        "slug": "vite-explicit-version-requirements",
        "title": "Explicit version requirements"
      },
      {
        "slug": "vite-manage-configuration-inheritance-carefully",
        "title": "Manage configuration inheritance carefully"
      },
      {
        "slug": "vite-minimize-memory-allocations",
        "title": "Minimize memory allocations"
      },
      {
        "slug": "vite-optimize-glob-operations",
        "title": "Optimize glob operations"
      },
      {
        "slug": "vite-permission-hierarchy-awareness",
        "title": "Permission hierarchy awareness"
      },
      {
        "slug": "vite-precise-documentation-language",
        "title": "Precise documentation language"
      },
      {
        "slug": "vite-propagate-errors-with-context",
        "title": "Propagate errors with context"
      },
      {
        "slug": "vite-react-transformation-tool-clarity",
        "title": "React transformation tool clarity"
      },
      {
        "slug": "vite-restrict-server-access",
        "title": "Restrict server access"
      },
      {
        "slug": "vite-runtime-agnostic-api-design",
        "title": "Runtime-agnostic API design"
      },
      {
        "slug": "vite-secure-workflow-permissions",
        "title": "Secure workflow permissions"
      },
      {
        "slug": "vite-separate-configuration-responsibilities",
        "title": "Separate configuration responsibilities"
      },
      {
        "slug": "vite-vue-component-import-handling",
        "title": "Vue component import handling"
      }
    ],
    "comments": {
      "vite-precise-documentation-language": [
        "I replaced \"files generated by create-vite\" with \"files generated from those files\" so that we don't say anything about files generated by the redirected CLIs (e.g. create-vue, create svelte)."
      ],
      "vite-separate-configuration-responsibilities": [
        "```suggestion\r\n      \"fileMatch\": [\"packages\\/create-vite\\/src\\/index\\\\.ts$\"],\r\n```",
        "Updated 👍 "
      ],
      "vite-minimize-memory-allocations": [
        "```suggestion\r\n                      size: Buffer.byteLength(chunk.code),\r\n```\r\n`Buffer.byteLength` should be performant than `Buffer.from().length` as it doesn't require the whole converted value to be held in the memory.\r\n",
        "Good catch! Instead of doing that, I made the `deepClone` to be call only once before merging with defaults (ffb92fdf743026c98edd8830e0868878db36f060, 2d6bc9cc73f15e4b7e551af649bc69c3265e4e0d). This way, the properties of defaults that would be overridden with the values will be cloned unnecessarily compared to your suggestion, but I think that's negligible.\r\n\r\n\r\n\r\n",
        "It was mainly for `RegExp`. I put the `structuredClone` in a condition 👍 (38162a0a1603da5de06d683d97a1b175289caeeb, e4364e59fbd760c2f7f6e63a2cc137720cd2690d)."
      ],
      "vite-react-transformation-tool-clarity": [
        "```suggestion\r\n### `@vitejs/plugin-react-oxc`\r\n\r\nWhen using `@vitejs/plugin-react` or `@vitejs/plugin-react-swc`, you can switch to the `@vitejs/plugin-react-oxc` plugin, which uses Oxc for JSX/TSX transformation instead of esbuild. It is designed to be a drop-in replacement, providing better build performance and aligning with the underlying architecture of `rolldown-vite`.\r\n\r\nBe aware that you can only switch to `@vitejs/plugin-react-oxc` if you are not using any Babel or SWC plugins (including the React compiler), or mutate the SWC options.\r\n```",
        "> which uses Oxc for JSX/TSX transformation instead of esbuild.\r\n\r\nThis isn't correct. These two plugins use transformers as shown in the table below. The difference is what handles the react-refresh transformation.\r\n\r\n- plugin-react (without plugins)\r\n  - dev (rollup-vite): uses babel for react-refresh transform + uses esbuild for JSX/TSX transform\r\n  - dev (rolldown-vite): uses babel for react-refresh transform + uses Oxc for JSX/TSX transform\r\n  - build (rollup-vite): uses esbuild for JSX/TSX transform\r\n  - build (rolldown-vite): uses Oxc for JSX/TSX transform\r\n- plugin-react-swc (without plugins)\r\n  - dev (rollup-vite): uses SWC for react-refresh and JSX/TSX transform\r\n  - dev (rolldown-vite): uses SWC for react-refresh and JSX/TSX transform\r\n  - build (rollup-vite): uses esbuild for JSX/TSX transform\r\n  - build (rolldown-vite): uses Oxc for JSX/TSX transform\r\n- plugin-react-oxc\r\n  - dev (rolldown-vite): uses Oxc for react-refresh and JSX/TSX transform\r\n  - build (rolldown-vite): uses Oxc for JSX/TSX transform"
      ],
      "vite-document-code-purposefully": [
        "```suggestion\r\n  /** @internal */\r\n  hostname: Hostname\r\n```\r\nI'd like to mark this as a internal type.",
        "```suggestion\r\n              // only limit to these extensions because:\r\n              // - for the `@import`/`@use`s written in file loaded by `load` function,\r\n              //   the `canonicalize` function of that `importer` is called first\r\n              // - the `load` function of an importer is only called for the importer\r\n              //   that returned a non-null result from its `canonicalize` function\r\n              (resolved.endsWith('.css') ||\r\n                resolved.endsWith('.scss') ||\r\n                resolved.endsWith('.sass'))\r\n```\r\nLet's add a comment that explains why we need to limit the extension here."
      ],
      "vite-optimize-glob-operations": [
        "We need the returned `files` to be absolute. I thought we can utilize the `cwd` option like:\r\n```ts\r\nconst files = globSync(globPattern, {\r\n  cwd: path.resolve(path.dirname(id), dir),\r\n  absolute: true,\r\n  expandDirectories: false,\r\n  ignore: ['**/node_modules/**'],\r\n})\r\n```\r\nI think this is what @SuperchupuDev suggested.\r\n",
        "Given that the problem is that `toAbsoluteGlob` escaping the characters unnecessary, how about not using that function?\r\nChecking the `toAbsoluteGlob` function, most of the code is handling about globs and escaping the paths. I think it would be trimmed down to:\r\n```js\r\n// replace `normalizeGlobPattern` + `toAbsoluteGlob` with\r\nconst dir = importer ? dirname(importer) : root\r\nconst normalized = rawPattern[0] === '/' ? posix.join(root, rawPattern.slice(1)) : posix.join(dir, rawPattern)\r\n\r\n// pass it to `newRawPattern`\r\nlet newRawPattern = posix.relative(posix.dirname(importer), normalized)\r\n```\r\n"
      ],
      "vite-environment-variable-management": [
        "```suggestion\r\nThe directory from which `.env` files are loaded. Can be an absolute path, or a path relative to the project root. `false` will disable the `.env` file loading.\r\n\r\nSee [here](/guide/env-and-mode#env-files) for more about environment files.\r\n```\r\nI think it doesn't have to be a warning.\r\n",
        "The current change would make this example broken (`env.APP_ENV` will be `undefined`).\r\n\r\n```suggestion\r\n  // Set the third parameter to 'APP_' to load envs with the `APP_` prefix.\r\n  // If necessary, you can set the optional third parameter to '' to load all env regardless of the `VITE_` prefix.\r\n  const env = loadEnv(mode, process.cwd(), 'APP_')\r\n```\r\n",
        "```suggestion\r\nThe variables declared in an env file for a specific mode (e.g. `.env.production`) will take higher priority than the ones in a generic one (e.g. `.env`).\r\n```\r\nThe fact that `.env` is always loaded is written above in the code block. Does changing the sentence to clarify that the priority is talking about the variables rather than the files make things clear?\r\n",
        "> When running with a specific mode, Vite will always load `.env` and `.env.local` in addition to the mode-specific `.env.[mode]` file. Variables declared in mode-specific files will take precedence over those in generic files, but variables defined only in .env or .env.local will still be available in the environment.\r\n\r\nLooks good to me. I think we can remove the `When running with a specific mode, ` as Vite always have a mode set (it's `development` in dev and `production` in build by default).\r\n"
      ],
      "vite-vue-component-import-handling": [
        "yes\r\nhttps://vite.dev/changes/hotupdate-hook.html#:~:text=handle%20additional%20watch%20events%20with%20create%20and%20delete."
      ],
      "vite-permission-hierarchy-awareness": [
        "I guess this diff is not needed. `permissions.triage` is always true for the ones that has `permissions.write`/`permissions.admin`."
      ],
      "vite-break-down-complex-functions": [
        "I think splitting the function into 3 like this will make the function more easier to understand\r\n```ts\r\nconst toStyleSheetLinkTag = (\r\n  file: string,\r\n  toOutputPath: (filename: string) => string,\r\n): HtmlTagDescriptor => ({\r\n  tag: 'link',\r\n  attrs: {\r\n    rel: 'stylesheet',\r\n    crossorigin: true,\r\n    href: toOutputPath(file),\r\n  },\r\n})\r\n\r\nconst getCssFilesForChunk = (\r\n  chunk: OutputChunk,\r\n  seenChunks: Set<string> = new Set(),\r\n  seenCss: Set<string> = new Set(),\r\n): string[] => {\r\n  if (seenChunks.has(chunk.fileName)) {\r\n    return []\r\n  }\r\n  seenChunks.add(chunk.fileName)\r\n\r\n  if (analyzedChunk.has(chunk)) {\r\n    return analyzedChunk.get(chunk)!\r\n  }\r\n\r\n  const files: string[] = []\r\n  chunk.imports.forEach((file) => {\r\n    const importee = bundle[file]\r\n    if (importee?.type === 'chunk') {\r\n      files.push(...getCssFilesForChunk(importee, seenChunks, seenCss))\r\n    }\r\n  })\r\n  analyzedChunk.set(chunk, files)\r\n\r\n  chunk.viteMetadata!.importedCss.forEach((file) => {\r\n    if (!seenCss.has(file)) {\r\n      seenCss.add(file)\r\n      files.push(file)\r\n    }\r\n  })\r\n\r\n  return files\r\n}\r\n\r\nconst getCssTagsForChunk = (\r\n  chunk: OutputChunk,\r\n  toOutputPath: (filename: string) => string,\r\n) =>\r\n  getCssFilesForChunk(chunk).map((file) =>\r\n    toStyleSheetLinkTag(file, toOutputPath),\r\n  )\r\n```\r\nI think `analyzedChunk` should be renamed to `analyzedImportedCssFiles` in this case.",
        "Ah, yeah, `getCssFilesForChunk` should be\r\n```diff\r\nconst getCssFilesForChunk = (\r\n  chunk: OutputChunk,\r\n  seenChunks: Set<string> = new Set(),\r\n  seenCss: Set<string> = new Set(),\r\n): string[] => {\r\n  if (seenChunks.has(chunk.fileName)) {\r\n    return []\r\n  }\r\n  seenChunks.add(chunk.fileName)\r\n\r\n  if (analyzedChunk.has(chunk)) {\r\n+    const files = analyzedChunk.get(chunk)!\r\n+    const additionals = files.filter(file => !seenCss.has(file))\r\n+    additionals.forEach(file => seenCss.add(file))\r\n+    return additionals\r\n-    return analyzedChunk.get(chunk)!\r\n  }\r\n\r\n  const files: string[] = []\r\n  chunk.imports.forEach((file) => {\r\n    const importee = bundle[file]\r\n    if (importee?.type === 'chunk') {\r\n      files.push(...getCssFilesForChunk(importee, seenChunks, seenCss))\r\n    }\r\n  })\r\n  analyzedChunk.set(chunk, files)\r\n\r\n  chunk.viteMetadata!.importedCss.forEach((file) => {\r\n    if (!seenCss.has(file)) {\r\n      seenCss.add(file)\r\n      files.push(file)\r\n    }\r\n  })\r\n\r\n  return files\r\n}\r\n```"
      ],
      "vite-complete-deployment-commands": [
        "```suggestion\r\n   - **Build Command**: `npm install && npm run build`\r\n```",
        "Note for others: `npm install` is required\r\nhttps://docs.render.com/deploy-sveltekit#manual-deploy\r\n"
      ],
      "vite-secure-workflow-permissions": [
        "Good catch 🙏 ",
        "Let's move this step and the \"React Based on Permissions\" step to the top so that nothing other than the reaction would happen for users without permission."
      ],
      "vite-escape-html-content-properly": [
        "Checking [the example](https://github.com/component/escape-html#:~:text=console.dir(%27%3Cinput%20name%3D%22full_name%22%20value%3D%22%27%20%2B%20escapeHtml(fullName)%20%2B%20%27%22%3E%27)), I think it should be\r\n```suggestion\r\n      res += ` ${key}=\"${escapeHtml(attrs[key])}\"`\r\n```\r\notherwise, `JSON.stringify` will do unnecessary escapes."
      ],
      "vite-runtime-agnostic-api-design": [
        "I've pushed a commit that replaces them with `CustomDevEnvironment` and `.runEntrypoint` so that it doesn't look like it can satisfy the `FetchableDevEnvironment` interface."
      ],
      "vite-evolve-apis-with-compatibility": [
        "I think we can deprecate these two methods as well. When migrated from `ssrLoadModule` to `ModuleRunner`, these APIs won't need to be called anymore as the `ModuleRunner` will interpret sourcemaps and rewrite the error stacks."
      ],
      "vite-propagate-errors-with-context": [
        "I think this should be\r\n```suggestion\r\n    if (!fs.existsSync(srcFile)) {\r\n```\r\nAlso I think it'd nice to avoid this `existsSync` call and add an `try-catch` to `fs.statSync` instead. `fs.statSync` throwed `ENOENT` on my machine (both Windows and WSL), did `fs.statSync` return a result on your machine?\r\n",
        "We need to wrap the whole handler with `try-catch` and call `next(error)` in the `catch`, so that `next` is called  when an error happens asynchronously.",
        "I think that is the expected behavior.",
        "For `send`s that were called while connecting the error would be voided and won't be able to catch by the user. In Vite 5, the error happened for the `send` call later that was called (which is confusing I guess).\r\nhttps://github.com/vitejs/vite/blob/c54c860f9d90e4074e5321648f9c5ee9fbda7038/packages/vite/src/shared/hmr.ts#L180-L190\r\n\r\n",
        "I think the order itself is natural as this is how the cause property works. But you have a point. Most users would want to know where the plugin error happened. I'll put it in a different property.",
        "Updated in edb34684dd3f6aefa215d663df11fb7b86c024dc\r\nI passed the whole error to the property, otherwise the console.log will output with quotes and escapes.\r\n```\r\nError: foo\r\n    at file:///D:/documents/GitHub/vite/foo.mjs:2:22\r\n    at ModuleJob.run (node:internal/modules/esm/module_job:268:25)\r\n    at async onImport.tracePromise.__proto__ (node:internal/modules/esm/loader:543:26)\r\n    at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:116:5) {\r\n  runnerStack: 'Error: RunnerError\\n' +\r\n    '    at file:///D:/documents/GitHub/vite/foo.mjs:3:47\\n' +\r\n    '    at ModuleJob.run (node:internal/modules/esm/module_job:268:25)\\n' +\r\n    '    at async onImport.tracePromise.__proto__ (node:internal/modules/esm/loader:543:26)\\n' +\r\n    '    at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:116:5)'\r\n}\r\n```\r\n```\r\nError: foo\r\n    at file:///D:/documents/GitHub/vite/foo.mjs:2:22\r\n    at ModuleJob.run (node:internal/modules/esm/module_job:268:25)\r\n    at async onImport.tracePromise.__proto__ (node:internal/modules/esm/loader:543:26)\r\n    at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:116:5) {\r\n  runnerStack: Error: RunnerError\r\n      at file:///D:/documents/GitHub/vite/foo.mjs:3:47\r\n      at ModuleJob.run (node:internal/modules/esm/module_job:268:25)\r\n      at async onImport.tracePromise.__proto__ (node:internal/modules/esm/loader:543:26)\r\n      at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:116:5)\r\n}\r\n```\r\n",
        "I think we should keep the error message (`Parse failure: `...) so that `console.log(err)` shows the position and contents (#5192, #12060).",
        "I don't know when `err.loc` exsits (when it passes the previous code path) though.",
        "Thanks for checking! That makes sense."
      ],
      "vite-explicit-version-requirements": [
        "`require(ESM)` is only supported (without a flag) for Node 20.19.0+, 22.12.0.\r\nhttps://nodejs.org/docs/latest-v20.x/api/modules.html#loading-ecmascript-modules-using-require\r\nDo we want to bump the version to that version? Or would it be fine to say \"if you want to use CJS, update the Node version\"?\r\n",
        "Sounds good 👍 ",
        "`packages/vite/src/node/__tests__/package.json` was picked up for `packages/vite/src/node/__tests__/build.spec.ts` by eslint-plugin-n and because that package.json does not have `engines` field, `>=16.0.0` was used and an lint error happened for `util.stripVTCharacters`.\r\nhttps://github.com/eslint-community/eslint-plugin-n?tab=readme-ov-file#configured-nodejs-version-range\r\nI set this to make `^18.0.0 || ^20.0.0 || >=22.0.0` to be applied across the repo.\r\n"
      ],
      "vite-manage-configuration-inheritance-carefully": [
        "Probably needs a helper function that merges recursively but replaces arrays, slightly different from `mergeConfig`.",
        "Added `mergeWithDefaults` function 👍 ",
        "There's many inferred options. Is it fine to simply omit it?",
        "I'm fine with trimming down the values for the exposed object. I think the object form makes it easier for users to find out the value for each options. But given that we already need to expose `mainFields` and `conditions` separately, maybe that advantage is gone.",
        "I pushed a commit that removes the new exports so that we can separate the discussion about exposing the default values. 👍 "
      ],
      "vite-descriptive-consistent-naming": [
        "```suggestion\r\n      // normalize and rewrite accepted deps\r\n      const resolvedAcceptedDeps = new Set<string>()\r\n```\r\nWould you rename this variable now that we push ids?",
        "I think we should rename this variable as this no longer only matches `context=*` but also matches `module`. For example, `svelteScriptModuleRE`."
      ],
      "vite-clean-configuration-organization": [
        "I think this should be in the \"Linting\" section rather than \"Bundler mode\" section as it's not required for Vite. Maybe also good to add a comment that it can be disabled if desired.",
        "Ah, didn't know that. I'll remove this."
      ],
      "vite-restrict-server-access": [
        "Added a similar warning to `server.cors` for `server.allowedHosts` as well. Technically, it is safe to set `server.allowedHosts: true` if the dev server runs behind a reverse proxy (the reverse proxy needs to check the host in that case though). But I didn't mention it here as I guess that usage isn't common and setting `allowedHosts` doesn't hurt."
      ],
      "vite-clean-network-resources": [
        "I guess we should call `socket.close` and `socket.removeEventListener` to avoid memory leak.",
        "I confirmed that `Promise.allSettled` is also supported by our default modern browser target.\r\nhttps://caniuse.com/mdn-javascript_builtins_promise_allsettled"
      ],
      "vite-ensure-documentation-accuracy": [
        "```suggestion\r\n- Package manager lockfile content, e.g. `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml` or `bun.lock`\r\n```\r\nSince this list is just an example, I think it's fine to remove `bun.lockb` because `bun.lock` has been added instead.\r\n",
        "Having both feels too verbose. Since `bun.lock` is the new default for bun, I think `bun.lock` should be listed instead of `bun.lockb`.",
        "I believe this \"can also be\" was confusing as the other possibility (`true`) doesn't actually exist now.",
        "Since `root` is deprecated and does not need to be set anymore, I think we can remove `root` from all the examples and types in the docs."
      ],
      "vite-document-protocol-configurations-clearly": [
        "> Is there more information of what are the sufficient values for TLS?\r\n\r\nThe only thing I found on the Node's document is this line.\r\n\r\n> [For servers, the identity options (`pfx` or `key`/`cert`) are usually required.](https://nodejs.org/api/http2.html#http2createserveroptions-onrequesthandler:~:text=For%20servers%2C%20the%20identity%20options%20(pfx%20or%20key/cert)%20are%20usually%20required.)\r\n\r\n> Does this also mean that it's possible to only enable TLS without HTTP/2 if the values are not sufficiently provided? (regardless of proxy)\r\n\r\nNo, Vite always tries to start a HTTP/2 server with TLS enabled if `server.https` is an object if `server.proxy` is not passed. If the options are not sufficiently provided, Node.js starts the server (probably) without any cert and reject any connections.\r\n",
        "Makes sense. I'll change to the previous one."
      ]
    },
    "profile": {
      "location": "Japan",
      "company": "void (0)",
      "blog": "https://green.sapphi.red/",
      "twitter_username": "sapphi_red",
      "site_admin": false,
      "followers": 1188,
      "following": 21
    }
  },
  "mattlord": {
    "repos": [
      "vitessio/vitess"
    ],
    "entries": [
      {
        "slug": "vitess-consistent-database-apis",
        "title": "Consistent database APIs"
      },
      {
        "slug": "vitess-document-configuration-precedence",
        "title": "Document configuration precedence"
      },
      {
        "slug": "vitess-dynamic-configuration-needs-validation",
        "title": "Dynamic configuration needs validation"
      },
      {
        "slug": "vitess-explicit-nil-handling",
        "title": "Explicit nil handling"
      },
      {
        "slug": "vitess-extract-shared-code-patterns",
        "title": "Extract shared code patterns"
      },
      {
        "slug": "vitess-log-levels-and-clarity",
        "title": "Log levels and clarity"
      },
      {
        "slug": "vitess-manage-workflow-state-transitions",
        "title": "Manage workflow state transitions"
      },
      {
        "slug": "vitess-meaningful-consistent-naming",
        "title": "Meaningful consistent naming"
      },
      {
        "slug": "vitess-metric-design-best-practices",
        "title": "Metric design best practices"
      },
      {
        "slug": "vitess-optimize-memory-allocation",
        "title": "Optimize memory allocation"
      },
      {
        "slug": "vitess-prevent-concurrent-access-races",
        "title": "Prevent concurrent access races"
      },
      {
        "slug": "vitess-robust-network-handling",
        "title": "Robust network handling"
      },
      {
        "slug": "vitess-size-fields-appropriately",
        "title": "Size fields appropriately"
      },
      {
        "slug": "vitess-standardize-error-wrapping-patterns",
        "title": "Standardize error wrapping patterns"
      },
      {
        "slug": "vitess-use-parameterized-queries",
        "title": "Use parameterized queries"
      },
      {
        "slug": "vitess-use-testify-assertion-libraries",
        "title": "Use testify assertion libraries"
      }
    ],
    "comments": {
      "vitess-consistent-database-apis": [
        "Yeah, it returns an error if there's a validation failure: https://github.com/vitessio/vitess/blob/76abae45fe6c8881c5ec308d05d1b648d058fa79/go/vt/vtctl/vtctl.go#L3259-L3269\r\n\r\nWe could change that, of course, but here I'm primarily just porting these missing commands over.",
        "\"We\" here would be whomever uses and consumes this command (certainly not me to date). This PR is about migrating the missing but still used/needed commands from the old client to the new, where maintaining \"drop in compatibility\" is the only concern. Why are we discussing this here? Is there some feature gap or request? If so, we should open a feature request that lays it out and someone can pick that up at a later date.\r\n\r\nI'm actually leaning towards removing [the endtoend test code for it](https://github.com/vitessio/vitess/blob/a57ae939ec5dbddc02e9eac31e1519113e904eeb/go/test/endtoend/sharded/sharded_keyspace_test.go#L169-L172) (`ValidatePermissions*`, as I did with `CopySchemaShard`) since I cannot find anything that actually uses it today (in vitess, vitess-operator, etc):\r\n```\r\ngo/test/endtoend/sharded/sharded_keyspace_test.go:      err = clusterInstance.VtctlclientProcess.ExecuteCommand(\"ValidatePermissionsShard\", fmt.Sprintf(\"%s/%s\", keyspaceName, shard1.Name))\r\ngo/test/endtoend/sharded/sharded_keyspace_test.go:      err = clusterInstance.VtctlclientProcess.ExecuteCommand(\"ValidatePermissionsKeyspace\", keyspaceName)\r\ngo/vt/vtctl/vtctl.go:                           name:   \"ValidatePermissionsShard\",\r\ngo/vt/vtctl/vtctl.go:                           method: commandValidatePermissionsShard,\r\ngo/vt/vtctl/vtctl.go:                           name:   \"ValidatePermissionsKeyspace\",\r\ngo/vt/vtctl/vtctl.go:                           method: commandValidatePermissionsKeyspace,\r\ngo/vt/vtctl/vtctl.go:func commandValidatePermissionsShard(ctx context.Context, wr *wrangler.Wrangler, subFlags *pflag.FlagSet, args []string) error {\r\ngo/vt/vtctl/vtctl.go:           return fmt.Errorf(\"the <keyspace/shard> argument is required for the ValidatePermissionsShard command\")\r\ngo/vt/vtctl/vtctl.go:   return wr.ValidatePermissionsShard(ctx, keyspace, shard)\r\ngo/vt/vtctl/vtctl.go:func commandValidatePermissionsKeyspace(ctx context.Context, wr *wrangler.Wrangler, subFlags *pflag.FlagSet, args []string) error {\r\ngo/vt/vtctl/vtctl.go:           return fmt.Errorf(\"the <keyspace name> argument is required for the ValidatePermissionsKeyspace command\")\r\ngo/vt/vtctl/vtctl.go:   return wr.ValidatePermissionsKeyspace(ctx, keyspace)\r\ngo/vt/vtctld/vtctld.go: actionRepo.RegisterKeyspaceAction(\"ValidatePermissionsKeyspace\",\r\ngo/vt/vtctld/vtctld.go:                 return \"\", wr.ValidatePermissionsKeyspace(ctx, keyspace)\r\ngo/vt/vtctld/vtctld.go: actionRepo.RegisterShardAction(\"ValidatePermissionsShard\",\r\ngo/vt/vtctld/vtctld.go:                 return \"\", wr.ValidatePermissionsShard(ctx, keyspace, shard)\r\ngo/vt/wrangler/permissions.go:// ValidatePermissionsShard validates all the permissions are the same\r\ngo/vt/wrangler/permissions.go:func (wr *Wrangler) ValidatePermissionsShard(ctx context.Context, keyspace, shard string) error {\r\ngo/vt/wrangler/permissions.go:// ValidatePermissionsKeyspace validates all the permissions are the same\r\ngo/vt/wrangler/permissions.go:func (wr *Wrangler) ValidatePermissionsKeyspace(ctx context.Context, keyspace string) error {\r\ngo/vt/wrangler/permissions.go:          return wr.ValidatePermissionsShard(ctx, keyspace, shards[0])\r\ngo/vt/wrangler/testlib/permissions_test.go:     // run ValidatePermissionsKeyspace, this should work\r\ngo/vt/wrangler/testlib/permissions_test.go:     if err := vp.Run([]string{\"ValidatePermissionsKeyspace\", primary.Tablet.Keyspace}); err != nil {\r\ngo/vt/wrangler/testlib/permissions_test.go:             t.Fatalf(\"ValidatePermissionsKeyspace failed: %v\", err)\r\ngo/vt/wrangler/testlib/permissions_test.go:     // run ValidatePermissionsKeyspace again, this should now fail\r\ngo/vt/wrangler/testlib/permissions_test.go:     if err := vp.Run([]string{\"ValidatePermissionsKeyspace\", primary.Tablet.Keyspace}); err == nil || !strings.Contains(err.Error(), \"has an extra user\") {\r\ngo/vt/wrangler/testlib/permissions_test.go:             t.Fatalf(\"ValidatePermissionsKeyspace has unexpected err: %v\", err)\r\n```\r\n\r\nSo rather than spending time discussing how we could improve it... I think we should kill it off during the client transition as it no longer seems to serve a valid/used purpose as a client command. Unless you for some reason feel this is needed (and should be improved at a later time)?",
        "To provide some context, the `GetPermissions` client command simply returns the contents of the `mysql.user` and `mysql.db` tables as a JSON doc ([tmc](https://github.com/vitessio/vitess/blob/a57ae939ec5dbddc02e9eac31e1519113e904eeb/go/vt/vttablet/tabletmanager/rpc_actions.go#L64-L67) -> [mysqlctl](https://github.com/vitessio/vitess/blob/a57ae939ec5dbddc02e9eac31e1519113e904eeb/go/vt/mysqlctl/permissions.go#L26-L52)):\r\n```\r\n❯ vtctldclient GetPermissions zone1-100\r\n{\r\n  \"user_permissions\": [\r\n    {\r\n      \"host\": \"%\",\r\n      \"user\": \"vt_repl\",\r\n      \"password_checksum\": \"0\",\r\n      \"privileges\": {\r\n        \"Alter_priv\": \"N\",\r\n        \"Alter_routine_priv\": \"N\",\r\n        \"Create_priv\": \"N\",\r\n        \"Create_role_priv\": \"N\",\r\n        \"Create_routine_priv\": \"N\",\r\n        \"Create_tablespace_priv\": \"N\",\r\n        \"Create_tmp_table_priv\": \"N\",\r\n        \"Create_user_priv\": \"N\",\r\n        \"Create_view_priv\": \"N\",\r\n        \"Delete_priv\": \"N\",\r\n        \"Drop_priv\": \"N\",\r\n        \"Drop_role_priv\": \"N\",\r\n        \"Event_priv\": \"N\",\r\n        \"Execute_priv\": \"N\",\r\n        \"File_priv\": \"N\",\r\n        \"Grant_priv\": \"N\",\r\n        \"Index_priv\": \"N\",\r\n        \"Insert_priv\": \"N\",\r\n        \"Lock_tables_priv\": \"N\",\r\n        \"Password_require_current\": \"\",\r\n        \"Password_reuse_history\": \"\",\r\n        \"Password_reuse_time\": \"\",\r\n        \"Process_priv\": \"N\",\r\n        \"References_priv\": \"N\",\r\n        \"Reload_priv\": \"N\",\r\n        \"Repl_client_priv\": \"N\",\r\n        \"Repl_slave_priv\": \"Y\",\r\n        \"Select_priv\": \"N\",\r\n        \"Show_db_priv\": \"N\",\r\n        \"Show_view_priv\": \"N\",\r\n        \"Shutdown_priv\": \"N\",\r\n        \"Super_priv\": \"N\",\r\n        \"Trigger_priv\": \"N\",\r\n        \"Update_priv\": \"N\",\r\n        \"User_attributes\": \"\",\r\n        \"account_locked\": \"N\",\r\n        \"authentication_string\": \"\",\r\n        \"max_connections\": \"0\",\r\n        \"max_questions\": \"0\",\r\n        \"max_updates\": \"0\",\r\n        \"max_user_connections\": \"0\",\r\n        \"password_expired\": \"N\",\r\n        \"password_lifetime\": \"\",\r\n        \"plugin\": \"mysql_native_password\",\r\n        \"ssl_cipher\": \"\",\r\n        \"ssl_type\": \"\",\r\n        \"x509_issuer\": \"\",\r\n        \"x509_subject\": \"\"\r\n      }\r\n    },\r\n...\r\n```\r\n\r\nThe `ValidatePermissions*` commands then tells you if the output differs within a shard or a keyspace. I don't disagree that telling you what nodes differ and how would be useful (not in this PR, but later enhancements)... IF this is actually still useful in the broader sense today. I haven't yet seen anything to indicate that it's still used or still useful. ",
        "In looking for uses of the ValidatePermissons commands (I found none) I did find uses of CopySchemaShard. So I think I will have to port that over.",
        "One last note... FWIW the functions used for the `ValidatePermissions*` commands do include the details in the error message: https://github.com/vitessio/vitess/blob/a57ae939ec5dbddc02e9eac31e1519113e904eeb/go/vt/mysqlctl/tmutils/permissions.go#L164\r\n\r\nSo I'm going to put this comment thread to bed for now.  🙂 ",
        "I'm going to mark this as resolved for now. But I'm going to add example outputs between the clients for each command added here. For example:\r\n```\r\n❯ vtctlclient ValidatePermissionsKeyspace commerce\r\nW0101 13:58:38.218227   32307 log.go:39] Failed to read in config : Config File \"vtconfig\" Not Found in \"[/Users/matt/git/vitess]\". This is optional, and can be ignored if you are not using config files. For a detailed explanation, see https://github.com/vitessio/vitess/blob/main/doc/viper/viper.md#config-files.\r\nValidatePermissionsKeyspace Error: rpc error: code = Unknown desc = permissions diffs: zone1-0000000100 has an extra user localhost:mlord;zone1-0000000100 has an extra user localhost:mlord\r\nE0101 13:58:38.237814   32307 main.go:105] remote error: rpc error: code = Unknown desc = permissions diffs: zone1-0000000100 has an extra user localhost:mlord;zone1-0000000100 has an extra user localhost:mlord\r\n\r\n❯ vtctldclient ValidatePermissionsKeyspace commerce\r\nE0101 13:58:55.687067   32429 main.go:56] rpc error: code = Unknown desc = permissions diffs: zone1-0000000100 has an extra user localhost:mlord\r\n\r\n❯ vtctlclient ValidatePermissionsShard commerce/0\r\nW0101 13:59:55.640410   32898 log.go:39] Failed to read in config : Config File \"vtconfig\" Not Found in \"[/Users/matt/git/vitess]\". This is optional, and can be ignored if you are not using config files. For a detailed explanation, see https://github.com/vitessio/vitess/blob/main/doc/viper/viper.md#config-files.\r\nValidatePermissionsShard Error: rpc error: code = Unknown desc = permissions diffs: zone1-0000000100 has an extra user localhost:mlord;zone1-0000000100 has an extra user localhost:mlord\r\nE0101 13:59:55.655940   32898 main.go:105] remote error: rpc error: code = Unknown desc = permissions diffs: zone1-0000000100 has an extra user localhost:mlord;zone1-0000000100 has an extra user localhost:mlord\r\n\r\n❯ vtctldclient ValidatePermissionsShard commerce/0\r\nE0101 13:59:59.051856   33004 main.go:56] rpc error: code = Unknown desc = permissions diffs: zone1-0000000100 has an extra user localhost:mlord\r\n```",
        "That’s my preference (OK with either way) as well, but here I’m following the well established design choice to always return a response object in vtctldserver RPCs, even when it has no fields. Better to have a unified and predictable behavior IMO. "
      ],
      "vitess-extract-shared-code-patterns": [
        "Agreed. They seem identical other than the capability and query so we could pass both into a function.",
        "This is a new struct that has a subset of the workflow server's values, and it's not specific to lookup vindexes at all. Can you help me understand what value this provides? \r\n\r\nThe workflow server type being:\r\n```\r\ntype Server struct {\r\n\tts  *topo.Server\r\n\ttmc tmclient.TabletManagerClient\r\n\t// Limit the number of concurrent background goroutines if needed.\r\n\tsem     *semaphore.Weighted\r\n\tenv     *vtenv.Environment\r\n\toptions serverOptions\r\n}\r\n```\r\n\r\nOtherwise this LGTM. Thanks!",
        "OK, so I take that to mean this is really for a human reader/developer to try and make the code easier to reason about and manage as lookup vindex related workflow code is in its own specific file and has its own specific method receiver.\r\n\r\nIn that case, IMO we should do this:\r\n 1. Go from workflow/lookup.go to workflow/lookup_vindex.go\r\n 2. Go from workflow.lookup.prepareCreateLookup to workflow.lookupVindex.prepareCreate (the type becomes lookupVindex and we don't need that lookupvindex context in the function name as its encapsulated in the receiver type\r\n\r\nLookup is a very generic term and it's not otherwise obvious to me how it adds distinction, separation, discoverability, and clarity specifically around the VReplication backfilling work for Lookup Vindexes via the LookupVindex client command and related RPCs. A [lookup vindex](https://vitess.io/docs/reference/features/vindexes/#lookup-vindex-types) is a specific concept in Vitess, and the client command we're processing in the workflow server is [`LookupVindex`](https://vitess.io/docs/21.0/reference/programs/vtctldclient/vtctldclient_lookupvindex/) (also [here](https://vitess.io/docs/21.0/user-guides/vschema-guide/backfill-vindexes/) and [here](https://vitess.io/docs/reference/vreplication/lookupvindex/)). Otherwise I would think that workflow/lookup was about code related to looking up workflows. Does this all make sense?\r\n\r\nThanks!"
      ],
      "vitess-metric-design-best-practices": [
        "I don't think that we should include the hostname. 1) it changes across the lifespan of a vtgate vstream (we retry for the shard in vstream manager on tablet stream errors) 2) in many deployment envs it's not really helpful 3) it can explode the size of the metric over time as e.g. in k8s deployments, which are the norm today for Vitess, you regularly roll the tablet pods. IMO this kind of context is better used in log messages if we want it. I'm not dead set against it though, so I'm open to discussion.",
        "I had assumed that this was a count of vtgate vstreams rather than tablet streams. Is this really supposed to be a count of tablet streams (one per shard, per vtgate vstream)?",
        "OK, makes sense. Thanks!",
        "I don't see any value in adding 0. Am I missing something?",
        "Is Reset what you really want? It's a counter and not a gauge. I assumed that it was meant to be a counter that spanned the life of the vtgate as the description is `\"Number of vstreams that ended with errors\"`. "
      ],
      "vitess-optimize-memory-allocation": [
        "We can allocate this in one shot: `finalRes := make(map[string]string, len(qr.Rows))`",
        "Probably worth setting the initial capacity to 1."
      ],
      "vitess-explicit-nil-handling": [
        "IMO, this should only be:\r\n```\r\nif req.Message != nil {\r\n```\r\n\r\nIf someone explicitly specifies the string literal `NULL` then we should store that since we're updating the stored value if any new value was explicitly provided."
      ],
      "vitess-standardize-error-wrapping-patterns": [
        "Probably worth using vterrors.New / vterrors.Errorf here so that we at least have a code as well.",
        "Nit, but I think `vterrors.Wrapf(derr, ...)` would be better here."
      ],
      "vitess-dynamic-configuration-needs-validation": [
        "A consideration we can add in the docs somewhere -- that setting the in-memory value does not guarantee it persists as this could only be in the process memory, before being written to the FS, and the FS write may not be flushed or sync'd either (?). A brief info section in the docs could help users think about how to deal with this if e.g. they update a setting in a hundred tablets (maybe one crashes or is e.g. replaced by k8s shorter thereafter).\r\n\r\nI would guess that on process normal shutdown all of these pending in-memory changes are flushed? Although I believe we do kill the process in some cases after a period of time and in those cases we can't know for sure.\r\n\r\nThese considerations also make the trade-off with this flag value here more clear.\r\n\r\nFWIW, I don't consider this an issue in any way. I don't think too many people are even aware of `/debug/env`, we don't support a lot of config options there, and this is clearly an improvement. :-) \r\n\r\nAlthough this does make me think... what if you want to play with different settings in a running process w/o affecting the persistent config? Is there a way to do that? I suspect (don't know for sure) there are two primary uses of `/debug/env` today:\r\n1. Change setting in the running process w/o having to restart. You persisted it yourself separately. Clearly this new behavior is an improvement there.\r\n2. Try different settings in the running process to see which one seems to offer better behavior. You may or may not end up persisting a different flag value.\r\n\r\nDo we effectively lose num 2? I guess we could simply document the breaking change in that it's up to the user to reset it back to the default when they're done with their testing (since intermediate temporary test values were likely persisted in the config file). And when choosing to use a config file, you must become aware of this anyway and change your process accordingly. I think that's totally fine and reasonable FWIW. \r\n\r\nPrecedence also comes into play -- we should document that somewhere in the docs (you may already have some place) -- as if they specify the value as a cmd-line flag then that value may override anything that's persisted in the config file anyway (which they may want for a core set of flags).",
        "These are all fine. Just considerations that we can internalize and potentially document if needed. "
      ],
      "vitess-use-parameterized-queries": [
        "We should use the parser instead. So instead of `%s` here and fmt.Sprintf later on we'd have `%a` and then when building the query we use the parser and bindvars instead. You can see examples of doing it this way throughout the code base, but e.g. you can see vdiff/schama.go to see the queries with \"%a\" in them and then how those variables are used. For example:\r\n  - Query: https://github.com/vitessio/vitess/blob/54dfd6005bcdd599604d42a6771cdf5a1025d7d9/go/vt/vttablet/tabletmanager/vdiff/schema.go#L30\r\n  - Usage: https://github.com/vitessio/vitess/blob/54dfd6005bcdd599604d42a6771cdf5a1025d7d9/go/vt/vttablet/tabletmanager/vdiff/action.go#L318-L321",
        "Does this variable really need to be exported from the package? If not, we should make the first letter a lower case s.",
        "I would recommend something like this:\r\n```\r\n        // The format specifier is for any optional predicates.\r\n        query := \"SELECT variable_name, variable_value FROM performance_schema.global_status%s\"\r\n        predicates := strings.Builder{} // optional filters\r\n        if len(names) > 0 {\r\n\t\tpredicates.WriteString(\" WHERE variable_name IN (\")\r\n\t        for i, name := range names {\r\n\t\t        if i > 0 {\r\n\t\t\t        predicates.WriteByte(',')\r\n\t        \t}\r\n                        predicates.WriteString(sqltypes.EncodeStringSQL(name))\r\n                }\r\n\t        predicates.WriteByte(')')\r\n        }\r\n        qr, err := mysqld.FetchSuperQuery(ctx, fmt.Sprintf(query, predicates.String())\r\n```\r\n\r\nSelecting the `variable_name` is necessary in order to return a map, which I think we really have to in order for this to be usable with multiple values.\r\n\r\nI also think that the base query should be a const in the package (~ `getGlobalStatusQuery`).\r\n\r\nLastly, I also think that we should build a parsed query here rather than sending the raw unparsed one on. This will add some input validation to protect against intentional or not, unexpected/unwanted behaviors. For example, someone sending in something like `'); drop table mysql.user;` as a var name. For example:\r\n```\r\n\tparsed := sqlparser.BuildParsedQuery(query, predicates.String())\r\n        query, err := parsed.GenerateQuery(nil, nil)\r\n        if err != nil {\r\n             return nil, err\r\n        }\r\n        qr, err := mysqld.FetchSuperQuery(ctx, query)\r\n```\r\n\r\nRelated to that, we don't have to use Super for this query.",
        "We do. So if we do end up using the parser, which I think we should as discussed above, we can leverage that to build the IN clause. Here's an example:\r\n\r\n`... in %a ...`:\r\nhttps://github.com/vitessio/vitess/blob/main/go/vt/vttablet/tabletmanager/vdiff/table_plan.go#L39\r\n\r\nBuilding and binding the slice bind var:\r\nhttps://github.com/vitessio/vitess/blob/377e1ddc99b9b1aeeae3eddb5a6ce575d1f5fc79/go/vt/vttablet/tabletmanager/vdiff/table_plan.go#L250-L262",
        "IMO it's worth creating a `mysqld.FetchQuery` helper (doesn't have to be done here) as there should be no need to use the DBA pool for this and various other things. "
      ],
      "vitess-size-fields-appropriately": [
        "This takes us from a limit of 10,000 bytes to 16,777,215 bytes. I don't see any reason not to do this. It might make sense to step toward that by using `blob` first. That would take us from 10,000 to 65,536 bytes. Did you use `mediumblob` because your case was already over the `blob` size? Then again... we know it's possible so we might as well allow it here. `mediumblob` would cover any reasonable installation, I think.",
        "We should increase this as well since if the `pos` is beyond the limit you'll also be beyond the `stop_pos` limit which would block various VReplication features — such as VDiff — which set the `stop_pos`, with the value being a GTID snapshot from the source tablet."
      ],
      "vitess-manage-workflow-state-transitions": [
        "I think that we should update the message in the `UpdateVReplicationWorkflow` call above, which means we'll have to add support for that to the RPC as an optional proto field.",
        "I'm curious what the `complete` command gets us beyond what the `cancel` command does? Is it that `cancel` enforces that the vindex has NOT been externalized while `complete` enforces that it HAS been externalized? In either case, I would expect the command to delete the workflow if that check passes."
      ],
      "vitess-document-configuration-precedence": [
        "TiL. As long as we mark `--tablet_config` as deprecated I think it's fine."
      ],
      "vitess-robust-network-handling": [
        "We should get into the habit of using `net.JoinHostPort()` in places like this (supports ipv4 and ipv6):\r\n```\r\nddtracer.WithAgentAddr(net.JoinHostPort(host, port)),\r\n``` "
      ],
      "vitess-meaningful-consistent-naming": [
        "Nit, but `MaxValuePartition` would match the standard (Pascal) case.",
        "What's the `Nb` stand for? Number backup? If so, it would be a bit more clear if it was NB. Or maybe even just RetryNum or RetryCnt. Maybe Nb is short for number? "
      ],
      "vitess-prevent-concurrent-access-races": [
        "Nit, but IMO it's worth using a closure here to be sure the mutex is unlocked using a defer (e.g. you get a panic between the lock and unlock, and that panic is recovered up the call stack, but then we don't unlock the mutex — although in this specific case it would be fine since this mutex would go out of scope in that case).\r\n```\r\n\t\tfunc() {\r\n\t\t\tmu.Lock()\r\n\t\t\tdefer mu.Unlock()\r\n\t\t\tif len(res.Streams) > 0 && sourceKeyspace == \"\" {\r\n\t\t\t\tsourceKeyspace = res.Streams[0].Bls.Keyspace\r\n\t\t\t}\r\n\t\t\tworkflowType = res.WorkflowType\r\n\t\t\treadVReplicationWorkflowResp[tablet.Shard] = res\r\n\t\t}()\r\n```",
        "I don't think this affects the query hot path, right? If it does, then it might be worth e.g. using 1 byte for the status and using bits in there for isWriting, isBlocked, isOpen etc. so that we can use atomics for reading them, CAS for optional changes, etc. If nothing else, it's probably worth moving these to atomic.Bool so that e.g. checkAndSetIsWriting can be one atomic call:\r\n```\r\n    return w.isWriting(false, true)\r\n```\r\n\r\nIt makes the code simpler, clearer, and less contentious / efficient. ",
        "Not that it really matters here, but when you don't care about the value being passed you can use an empty struct literal which is zero bytes:\r\n```\r\nsemAcquiredChan := make(chan struct{})\r\n\r\nsemAcquiredChan <- struct{}{}\r\n```"
      ],
      "vitess-use-testify-assertion-libraries": [
        "We should use require/assert in new tests. So e.g. here it would be:\r\n```\r\nerr := f.Close()\r\nrequire.NoError(t, err)\r\n```\r\n\r\nOr even just: `require.NoError(t, f.Close())`"
      ],
      "vitess-log-levels-and-clarity": [
        "The default logger is glog. And error log messages also end up in the info log as they cascade down. Do we really want/need duplicate log messages? This is an error so I think we should improve the existing one by adding the workflow name.",
        "IMO we should take this opportunity to improve the log message:\r\n```\r\nts.Logger().Errorf(\"Cancel migration failed: could not revert denied tables / shared access: %v\", err)\r\n```\r\n\r\nI also think that we should accumulate these and return them to the caller. But we could defer that change. "
      ]
    },
    "profile": {
      "company": "@IstariDigital",
      "blog": "https://www.linkedin.com/in/mattallenlord/",
      "twitter_username": "mattalord",
      "site_admin": false,
      "followers": 43,
      "following": 3
    }
  },
  "MarshallOfSound": {
    "repos": [
      "electron/electron"
    ],
    "entries": [
      {
        "slug": "electron-api-parameter-consistency",
        "title": "API parameter consistency"
      },
      {
        "slug": "electron-avoid-redundant-operations",
        "title": "avoid redundant operations"
      },
      {
        "slug": "electron-avoid-runtime-credential-resolution",
        "title": "avoid runtime credential resolution"
      },
      {
        "slug": "electron-conditional-feature-initialization",
        "title": "conditional feature initialization"
      },
      {
        "slug": "electron-consistent-platform-identifiers",
        "title": "consistent platform identifiers"
      },
      {
        "slug": "electron-context-aware-module-loading",
        "title": "Context-aware module loading"
      },
      {
        "slug": "electron-descriptive-error-messages",
        "title": "Descriptive error messages"
      },
      {
        "slug": "electron-ensure-async-error-cleanup",
        "title": "Ensure async error cleanup"
      },
      {
        "slug": "electron-ensure-comprehensive-test-coverage",
        "title": "Ensure comprehensive test coverage"
      },
      {
        "slug": "electron-extract-reusable-workflow-components",
        "title": "extract reusable workflow components"
      },
      {
        "slug": "electron-organize-code-structure",
        "title": "organize code structure"
      },
      {
        "slug": "electron-remove-internal-apis",
        "title": "Remove internal APIs"
      },
      {
        "slug": "electron-scope-configuration-impact",
        "title": "Scope configuration impact"
      },
      {
        "slug": "electron-use-extensible-parameter-objects",
        "title": "Use extensible parameter objects"
      },
      {
        "slug": "electron-use-optional-types-safely",
        "title": "use optional types safely"
      },
      {
        "slug": "electron-validate-external-input-safely",
        "title": "validate external input safely"
      }
    ],
    "comments": {
      "electron-scope-configuration-impact": [
        "This is going to enable debugging for **all** pipelines, instead of just one. I think this secret should be a branch name or something so that the check is `current_branch == secrets.debug_branch_name` or something."
      ],
      "electron-use-optional-types-safely": [
        "> Maybe replace std::optional<WebFrameMain*> with just a WebFrameMain* that may be nullptr?\r\n\r\nI don't think the current gin converter will take `Undefined` and convert it to `nullptr` 🤔 Nor do I think it should. The `std::optional` converter was added to allow basically `null | undefined | T` conversions that previously weren't possible without manual gin incantations.\r\n\r\nGiven the above I don't think it's possible in this case to have the optional wrap a `nullptr` as the converter would never put one there, but that isn't enforced by semantics so I do see the issue still.\r\n\r\nMaybe the gin converter should go to `std::reference_wrapper<T>` instead of `T*` to indicate it's non-null nature, but thats a bit of a lift for the whole codebase ",
        "@samuelmaddock This value comes from JS, do we have existing precedent for a `frameToken` being passed around 🤔 ",
        "The problem is because the argument is optional, if I want to use a gin bound template It would need to be `std::optional<content::RenderFrameHost*>` which then has the same three state issue that @ckerr pointed out, of being `nullptr`, `nullopt` and `T*`.",
        "I opted for a `foo && foo.value()` check to ensure no nullptr in this case, I think it would be good to consider a codebase wide change to something more declarative like `std::reference_wrapper` but that requires more thought / everything doing it so for now I'll just make this bit ok"
      ],
      "electron-use-extensible-parameter-objects": [
        "Can we make this something more friendly, don't change immediately this is meant to start a discussion thread here.\r\n\r\nI'd be in favour of just a generic `BaseWindow` constructor option for a window \"name\" which we can document as a unique identifier that apps can use to identify windows, and Electron can use internally for things like this state persistance.\r\n\r\nThen change this options object to be just `enabled: true/false`.\r\n\r\nApologies if this option has already been discussed or bikeshed elsewhere.",
        "@erickzhao interested in your opinions here",
        "Is the `func` naming here consistent with something else? We can't use `function` because _reserved_ but I think this could be a case where matching `spawn(thing, args)` might be nice. E.g. `exec(fn, args, { ...opts })`"
      ],
      "electron-context-aware-module-loading": [
        "`default_app/tsconfig.json` now has `module: esnext` which means that import statements aren't translated into require calls when tsc is run.  Sandboxed preload scripts do not (as noted elsewhere) support ESM so we have to use `require` here",
        "> Writing const ... = require(...) will just give you a bunch of any which doesn't sound desirable.\r\n\r\nThis was giving me types when I was in my editor at least 🤔 Will try the funky syntax instead though 👍 ",
        "We could read it and parse but I want to remain consistent with node's loading of JSON files with things like character encoding etc.  I could copy-paste but it seemed safer to just use `import`",
        "This change isn't actually anything that will impact apps, i.e. there are no breaking changes in this PR.  Every app that currently works should continue to work the exact same way.\r\n\r\nThis change is required because `default_app/package.json` now has `type: module` which means that tsc no longer replaces import with require when it builds the files.  Sandboxed preloads don't support import though so we have to the replacement instead."
      ],
      "electron-consistent-platform-identifiers": [
        "Another note that we have `windows`, `win` and `win32`. Let's choose one and stick with it"
      ],
      "electron-avoid-redundant-operations": [
        "non-blocking-fast-follow: This is susceptible to userland modification of `resourcesPath`, unlike the impl in `node/init.ts` which captures the value before userland code runs.",
        "non-blocking: this also has a perf hit during module loading as we're reading this on every module load, we should be able to cache this in a `base::NoDestructor<std::string>` or something for future use"
      ],
      "electron-remove-internal-apis": [
        "We delete it so that app code can't somehow call it, it's just protecting our internals"
      ],
      "electron-organize-code-structure": [
        "To avoid _abuse_ can we make this truly private. i.e.\r\n\r\n`function awaitNextLoad(...`\r\n\r\n```\r\nreturn awaitNextLoad.call(this)\r\n```"
      ],
      "electron-ensure-comprehensive-test-coverage": [
        "Can you please update `generateTests` so the same \"it doesn't leak X\" tests run in a service worker environment too. They current run for sandbox + unsandbox. They should run for sandbox + unsandbox + service workers.",
        "I don't think we should release this to stable without such tests, security has to come first. I'd be OK landing this PR in exchange for raising an issue that blocks stable for adding / updating this test suite to cover the SW case."
      ],
      "electron-extract-reusable-workflow-components": [
        "I stand by my comment elsewhere that this is typically slower on unix than Just Installing",
        "If we're keeping this though we definitely shouldnt be duplicating it everywhere, extract \"install dependencies\" to a common action"
      ],
      "electron-descriptive-error-messages": [
        "```suggestion\r\n        \"Must provide frameOrigin and topFrameSite strings to `clearSharedDictionaryCacheForIsolationKey`\");\r\n```\r\n\r\nBig fan of error messages self-declaring where they come from to avoid the classic \"what even is an ENOENT\" style errors"
      ],
      "electron-api-parameter-consistency": [
        "> Cocoa would allow a non-GUID\r\n\r\nCocoa might, but IMO for ease of API across platforms it should just be a GUID on both. Then folks can even use the _same_ GUID which is nice. Saying \"this is a GUID on windows and on macOS can be any string have fun\" just feels bad. So GUID everywhere is right IMO",
        "By spec these should have an enforced minimum of 100\r\n\r\n> Specifies the width of the content area, including scrollbars. The minimum required value is 100.",
        "Are we deliberately double-converting to give better error messages? You should be able to pull these out of the `Dictionary` immediately as `GURL` via the gin converter "
      ],
      "electron-validate-external-input-safely": [
        "This is incredibly dangerous, please remove this feature from this PR as it is basically a giant security hole. Apps should only ever be told \"you are packaged\" never the inverse.",
        "This is wildly unsafe, please use an appropriate object builder or JSON implementation from `//base` or `//v8`",
        "> Have simple question: Why does we have self formed JSON into logger in case if my 3 lines of self formed JSON are so significant?\r\n\r\nAll the code you referenced is deep in Chromium not in electron code.  This code / logic in it's current state is left up to users to consume and it is our responsibility as maintainers to ensure that users are safe and secure, that takes priority over any theoretical performance differences or \"visions\".\r\n\r\nManually forming JSON with user data is **unsafe**.  We have no way to guarantee their aren't quotes, escape sequences, etc. in that data.\r\n\r\nTo be clear, in it's current state this code will not be landed."
      ],
      "electron-avoid-runtime-credential-resolution": [
        "I think the secret should actually contain the SSH keys. And those should be fetched from the infra repo.\r\n\r\nWe can in terraform use a github_secret resource and folks should have to hardcode the SSH key they want to use to connect there. That avoids the API call here and avoids any risk around `GITHUB_ACTOR` somehow being an injection vector.",
        "Separate to this, maybe we shouldn't use authorized_keys at all and instead should rely on cloudflare access zero trust rules for protecting our ssh access. We can configure this hostname as an SSH target in cloudflare zero trust and then assign IDP roles (wg-infra) as having access to that hostname."
      ],
      "electron-conditional-feature-initialization": [
        "This is going to result in the default BrowserContext _always_ getting written to disk regardless of whether it is actually used. i.e. if an app entirely uses off-the-record contexts then there is no reason for this context to be created.\r\n\r\nThis will also result in these per-window data slots being stored in a per-context directory, which for apps that reset sessions by literally `rm -rf path/to/context/dir` will result in unexpected wipes of this data.\r\n\r\nCan we make our own prefs factory for this using a path separate to the per-session storage path. This will (a) give us a more distinct on-disk file path to the per-session prefs store and (b) ensure we don't erroneously create the default context in cases we do not need it",
        "We should flip the order here, only get or create the prefs factory when this `stateId` property is set."
      ],
      "electron-ensure-async-error-cleanup": [
        "This will leave a dangling promise, this shouldn't be `async`.\r\n\r\nInstead `winsOwnedByElectronProcess` should be `Promise<Array<>>` instead of `Array`",
        "We should do `winsOwnedByElectronProcess.then((wins) => resolve(sources.concat(wins)))` in combination with my above comment, these handlers should not be `async` as nothing is handling rejections"
      ]
    },
    "profile": {
      "location": "Vancouver, Canada",
      "company": "@anthropics",
      "blog": "https://www.samuelattard.com",
      "site_admin": false,
      "followers": 1963,
      "following": 1
    }
  },
  "ClearlyClaire": {
    "repos": [
      "mastodon/mastodon"
    ],
    "entries": [
      {
        "slug": "mastodon-accessibility-interaction-security",
        "title": "accessibility interaction security"
      },
      {
        "slug": "mastodon-api-parameter-design",
        "title": "API parameter design"
      },
      {
        "slug": "mastodon-avoid-deprecated-sass-syntax",
        "title": "avoid deprecated SASS syntax"
      },
      {
        "slug": "mastodon-batch-similar-operations",
        "title": "batch similar operations"
      },
      {
        "slug": "mastodon-centralize-configuration-management",
        "title": "centralize configuration management"
      },
      {
        "slug": "mastodon-choose-appropriate-exception-types",
        "title": "Choose appropriate exception types"
      },
      {
        "slug": "mastodon-complete-translatable-sentences",
        "title": "Complete translatable sentences"
      },
      {
        "slug": "mastodon-comprehensive-authorization-checks",
        "title": "comprehensive authorization checks"
      },
      {
        "slug": "mastodon-configuration-value-safety",
        "title": "Configuration value safety"
      },
      {
        "slug": "mastodon-early-nil-validation",
        "title": "early nil validation"
      },
      {
        "slug": "mastodon-environment-variable-descriptive-naming",
        "title": "Environment variable descriptive naming"
      },
      {
        "slug": "mastodon-framework-aware-text-composition",
        "title": "Framework-aware text composition"
      },
      {
        "slug": "mastodon-leverage-existing-configuration-sources",
        "title": "leverage existing configuration sources"
      },
      {
        "slug": "mastodon-migration-data-dependencies",
        "title": "migration data dependencies"
      },
      {
        "slug": "mastodon-minimize-html-attack-surface",
        "title": "Minimize HTML attack surface"
      },
      {
        "slug": "mastodon-network-resource-limits",
        "title": "Network resource limits"
      },
      {
        "slug": "mastodon-optimize-collection-iterations",
        "title": "optimize collection iterations"
      },
      {
        "slug": "mastodon-optimize-database-queries",
        "title": "Optimize database queries"
      },
      {
        "slug": "mastodon-optimize-react-hooks",
        "title": "Optimize React hooks"
      },
      {
        "slug": "mastodon-optimize-test-organization",
        "title": "optimize test organization"
      },
      {
        "slug": "mastodon-prefer-early-returns",
        "title": "prefer early returns"
      },
      {
        "slug": "mastodon-referrer-header-privacy",
        "title": "referrer header privacy"
      },
      {
        "slug": "mastodon-review-configuration-currency",
        "title": "Review configuration currency"
      },
      {
        "slug": "mastodon-use-accessible-terminology",
        "title": "Use accessible terminology"
      },
      {
        "slug": "mastodon-use-contextually-descriptive-names",
        "title": "Use contextually descriptive names"
      },
      {
        "slug": "mastodon-use-descriptive-specific-names",
        "title": "Use descriptive specific names"
      },
      {
        "slug": "mastodon-use-semantic-naming",
        "title": "Use semantic naming"
      },
      {
        "slug": "mastodon-use-semantic-null-handling",
        "title": "Use semantic null handling"
      }
    ],
    "comments": {
      "mastodon-use-semantic-null-handling": [
        "Maybe the following might be a little easier to read?\r\n```suggestion\r\n  return accountId ?? undefined;\r\n```",
        "But overall, the previous version had an actual issue where it could return a string, `undefined` or `null` but the types were overridden so Typescript was considering only `string` and `undefined`. I think in some places (probably non-Typescript only) we rely on the difference between `undefined` and `null`, as `undefined` means “we haven't fetched it yet” and `null` means “it does not exist”.\r\n\r\nIf I'm wrong and we never make the distinction, we probably should never store `null` and instead delete from the map, simplifying the type and having `string | undefined` everywhere.",
        "We indeed do make a distinction in at least `app/javascript/mastodon/features/account_timeline/index.jsx`, with `https://mastodon.social/@bogus@example.com` displaying a 404, but `app/javascript/mastodon/features/account_featured/index.tsx`, not making that distinction, displays an infinite spinner: `https://mastodon.social/@bogus@example.com/featured`"
      ],
      "mastodon-centralize-configuration-management": [
        "One thing I'm kind of worried about is that we don't test for the environment variables' effect anymore, although they are the documented way to do this.",
        "I am not sure how clean and future-proof adding our own stuff directly in `config` is. Otherwise, this looks good.",
        "I meant the use of the top-level Rails `configuration` namespace. `omniauth` is probably safe indeed, though."
      ],
      "mastodon-optimize-react-hooks": [
        "Oh, yeah, good point for the refs, I'm always confused by that. It can't have a dependency of `[]` since `handleDocumentClick` depends on `onClose`."
      ],
      "mastodon-accessibility-interaction-security": [
        "> The `aria-describedby` property is appropriate when the associated content contains plain text. If the content is extensive, contains useful semantics, or has a complex structure requiring user navigation, use [aria-details](https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/Reference/Attributes/aria-details) instead.\r\n\r\nContent warnings are mostly textual but they are allowed to include emojis, including custom ones, so I wonder if we should use `aria-details` instead?"
      ],
      "mastodon-use-semantic-naming": [
        "I'm personally fine either ways. `MAX_STATUS_CHARS` would be more consistent with the source code, but the configuration option is intended for admins, not for developers.",
        "I'm having a look and it's not super obvious how this could be done. I mean we could have a `rule.translated_text_for(I18n.locale.to_s)` but then memoizing the correct translation would be pretty awkward imho.",
        "I went with something like that, thanks!"
      ],
      "mastodon-review-configuration-currency": [
        "This is still a discrepancy with the Ruby handling, isn't it?"
      ],
      "mastodon-choose-appropriate-exception-types": [
        "Having something called `find_or_initialize_by_*` raise an error (especially an `ActiveRecord::RecordInvalid` error) is pretty surprising, as `find_or_initialize_by` does not perform validation and does not raise.",
        "I think I would be in favor of either ignoring the issue and only errorring on save, or further restrict the existing `constraints: { id: %r{[^/]+} }` constraint in the router (which I think would look like something like `constraints: ->(req) { TagManager.instance.normalize_domain(req.params['id']).present? }`), though that is a little awkward as well.",
        "I think I would just use `find_or_initialize_by(domain: )`, I don't really see the value of the method otherwise.",
        "I think this `save!` is what causes the settings page to return a bogus 422 by raising a `ActiveRecord::RecordInvalid` instead of properly annotating the model with an error."
      ],
      "mastodon-optimize-collection-iterations": [
        "Hm… maybe something like the following? It seems to perform about the same, but is much more verbose:\r\n```suggestion\r\n    usernames = []\r\n    ids = []\r\n\r\n    uris.each do |uri|\r\n      param, value = uri_to_local_account_params(uri)\r\n\r\n      case param\r\n      when :username\r\n        usernames << value.downcase\r\n      when :id\r\n        ids << value\r\n      end\r\n    end\r\n\r\n```",
        "Yeah I suppose the following is ok:\r\n```suggestion\r\n    usernames = []\r\n    ids = []\r\n\r\n    uris.each do |uri|\r\n      param, value = uri_to_local_account_params(uri)\r\n      usernames << value.downcase if param == :username\r\n      ids << value if param == :id\r\n    end\r\n```",
        "If I understand correctly, this will recursively fetch posts even if those have already been fetched recently? I mean, the `should_fetch_replies?` is only fetched for the root post in the recursive crawling. So basically, if you check the context for a post *then* for its parent post, you'd end up doing the work twice."
      ],
      "mastodon-framework-aware-text-composition": [
        "I tried to come up with something that would work both for previous and upcoming ToS changes, since someone could log in between the time a ToS change notification was triggered and the time the new ToS apply.\r\n\r\nUpdated to have a different copy for future and past ToS changes."
      ],
      "mastodon-batch-similar-operations": [
        "Thank you for your contribution!\r\n\r\nThis could grow into a pretty large in-memory `notification_jobs_args` array. If going this way, I'd probably use `find_in_batches` and build an array for each such group instead of for all affected groups. As you pointed out, `find_each` works with the same batch size as `perform_bulk`, so this would not actually change how jobs are sent to Sidekiq, it would just put a bound on the size of the in-memory array.",
        "Looks good, thanks!",
        "Should those two calls be pipelined to avoid back-and-forth with Redis?",
        "The superclass `vite_stylesheet_tag` calls `vite_asset_path` which itself makes a lookup. Maybe it would make sense rewriting this bit to avoid doing the lookup twice (once for the path, and once for the integrity)?",
        "This will still be re-parsed on every access… maybe memoize it?\r\n```suggestion\r\n    @@frontend_translations ||= JSON.parse(FRONTEND_TRANSLATIONS)\r\n```"
      ],
      "mastodon-leverage-existing-configuration-sources": [
        "Oh, that's good to know and it would indeed be appropriate to replace the computed `jsRoot`!\r\n\r\nHowever, it's not (in general) guaranteed to be set, so I'm not sure how to handle the case where `config.root` is `undefined`.\r\n\r\nFurthermore, it would mean that `themesFile` would be defined as something like `path.resolve(config.root, '../../config/themes.yml')`, which is rather awkward and brittle.",
        "Seems like we could use `config.root` and `config.envDir`. However, they both can be `undefined`, I am not sure what the behavior of the plugin should be in this case."
      ],
      "mastodon-migration-data-dependencies": [
        "I think this needs to be moved to a pre-deployment migration. And now that we have migration breakpoints, it should be safe to do so.",
        "That is a good question. I guess the assumption now is that anyone running those will be on 4.3.0 or newer, or will have the services stopped while running the migrations. This means they should normally not lead to new entries with `thing_id` and `thing_type`. Just to be safe we could do the deletion in both a pre and post-deployment migration.\r\n\r\nThe columns removal still need to be in a post-deployment migration because the 4.3.0 code does not ignore those columns. Index changes should be safe either way I suppose, but if we are re-deleting stuff out of cautiousness in the post-deployment migrations, this means we would do the index changes there too.",
        "Those are loaded in a post-deployment migration (not sure why we did it that way), which means there is no guarantee the role exists at runtime (or in pre-deployment migrations).",
        "Yes, that's it!\r\n\r\nThough, once again, taking a step back, I would be happy with a migration-less approach that may do a N+1 once if the default role doesn't exist yet at runtime, as long as we can avoid N+1s when the role does exist.",
        "The main issue with `after_initialize` is that it would set the value to `-99` in the database, while we ideally do not want that to be persisted in the database, for index size reasons mainly."
      ],
      "mastodon-avoid-deprecated-sass-syntax": [
        "This leads to the following warning:\r\n```\r\nDEPRECATION WARNING [slash-div]: Using / for division outside of calc() is deprecated and will be removed in Dart Sass 2.0.0.\r\n\r\nRecommendation: math.div(24px - 12px, 2) or calc((24px - 12px) / 2)\r\n\r\nMore info and automated migrator: https://sass-lang.com/d/slash-div\r\n\r\n     ╷\r\n4623 │   margin: (24px - 12px) / 2;\r\n     │           ^^^^^^^^^^^^^^^^^\r\n     ╵\r\n    app/javascript/styles/mastodon/components.scss 4623:11  @use\r\n    app/javascript/styles/application.scss 16:1             root stylesheet\r\n```",
        "There's a deprecation warning for `lighten`."
      ],
      "mastodon-environment-variable-descriptive-naming": [
        "I'd suggest MAX_TOOT_CHARS or something similar, as one may want custom maximum length for other things (such as bios)."
      ],
      "mastodon-optimize-test-organization": [
        "It's a minor thing, but we're not checking empty events anywhere.",
        "Oh yeah, it's testing empty URL and empty events at the same time. Maybe it should be split in two, and made a little more explicit with comments.",
        "Sorry if I haven't been clear, but I was suggesting testing both case failures (empty events and empty URL) separately.",
        "Yeah, I suppose you're right, if the model tests cover this, it does not matter much."
      ],
      "mastodon-referrer-header-privacy": [
        "This looks ok, but things I'm worried are less uniquely identifying users, but having them inadvertently out something on them when clicking a link to an external service they are registered on (e.g. user on a LGBTQ instance logging in on a place where that could be an issue for them)"
      ],
      "mastodon-use-contextually-descriptive-names": [
        "Should we use a different name than `api`? This is pretty non-specific and confusing."
      ],
      "mastodon-minimize-html-attack-surface": [
        "Allowing `style` attributes *should* be safe, although there have been vulnerabilities in our sanitizing library in the past, and limiting the surface area should be best.\r\n\r\nAnd if we do decide to allow `style` attributes, it probably makes sense to only allow specific properties like `border`. And keep support for `frameborder` as well.",
        "Afaik the `html` bit you modified still goes through sanitization, so by using `style` instead, it will get sanitized away and won't work.",
        "I'm not sure if we want to risk adding support for `style` (it's *supposed* to be safe but significantly increases the attack surface), I'll let @Gargron decide on this.\r\n\r\nBut if we're doing this, we should very much use something like:\r\n```ruby\r\n  css: {\r\n    properties: ['border']\r\n  }\r\n```\r\n\r\nto limit attributes to a list of explicitly-vetted properties, to avoid an attacker getting creative and using confusing UI elements, and so on."
      ],
      "mastodon-api-parameter-design": [
        "If you wanted to do it like that, you'd need:\r\n```suggestion\r\n    @filter.keywords.build(resource_params.dig(:keywords_attributes, '0'))\r\n```\r\n\r\nIndeed, using `params` would raise `ActiveModel::ForbiddenAttributesError` as the params would not be vetted through `StrongParameters` (that's what `params.expect` does), and `resource_params.dig(:keywords)` is an array (well, a hash) for multiple keywords, while you are setting up only one.\r\n\r\nThat being said, this is a lot of work for just a few attributes. I wonder if we shouldn't use some bespoke parameter instead, e.g. have `/filters/new?hashtag=blah`.",
        "You would need to add handling just like you did there, e.g. `@filter.keywords.build({ 'keyword' => \"##{params['hashtag']}\", 'whole_word' => true }) if params['hashtag'].present?`"
      ],
      "mastodon-prefer-early-returns": [
        "Where is the `status` coming from in this snippet?"
      ],
      "mastodon-use-descriptive-specific-names": [
        "We have only briefly used values starting with `_:` and stopped doing so years ago, so the `value.start_with?('_:')` case would actually never occur afaik.\r\n\r\nAs for values starting with `_`, we currently have none, and would start using `_misskey_quote`, which needs to *not* be camelized.",
        "For context, this was added in #4585 for the `_:locked` attribute, which was dropped in #4767, and then renamed in #4779.",
        "Ok, why not, changed it to `start_with?('_misskey')`",
        "Maybe those scopes could use some renaming. `Fasp::Subscription.account` was a bit confusing, as on first read I did not think this was a scope, but an Account attribute.",
        "I think `category_account` and `category_content` are ever so slightly less confusing. But I don't have any better suggestion.",
        "I chose “closed federation” in opposition of “open federation”, but I have no strong feeling. I also considered `limited_federation` and `allowlist_federation` (possibly the most descriptive one here).",
        "Changed to `limited_federation`"
      ],
      "mastodon-use-accessible-terminology": [
        "This appears technically correct, however I think it is unnecessary, as `count` here is a constant (currently set to 20000). It may change in the future, but it will always be a large number.",
        "Same thing with `limit` VS `count`, and same thing with the limit being a hardcoded value (currently 25)."
      ],
      "mastodon-comprehensive-authorization-checks": [
        "Indeed, because we don't have any support or representation for private groups at the moment."
      ],
      "mastodon-complete-translatable-sentences": [
        "I am again concerned, with this panel in particular, that we are encouraging clout-chasing and popularity contests where we really should not be.\r\n\r\nIn addition, I have a few other concerns about this panel:\r\n- I'm afraid that breaking up the sentence in a few translatable strings and making layout assumptions based on the shape of the sentence is going to make that hard or even impossible to translate in some languages\r\n- “That puts you in the top X of Mastodon users” can be fairly confusing and misleading. What's the metric used here? Is that Mastodon users in general, fediverse users known to your server, or local users on your server?\r\n- (I vaguely understand what the Bernie thing is supposed to mean, but this doesn't land for me; maybe it's some kind of cultural reference I'm missing?)",
        "What if the whole thing was one translatable string with various placeholders?\r\n\r\nSomething like (untested):\r\n\r\n```jsx\r\n<FormattedMessage\r\n  id='annual_report.summary.percentile_text'\r\n  defaultMessage='<label>That puts you in the top</label><percentage /><label>of Mastodon users.</label>'\r\n  values={{\r\n   label: (text) => <div className='annual-report__summary__percentile__label'>{text}</div>,\r\n   percentage: () => <div className='annual-report__summary__percentile__number'><FormattedNumber value={percentile / 100} style='percent' maximumFractionDigits={1} /></div>,\r\n  }}\r\n/>\r\n```",
        "This is *probably* fine, but I'd be careful with this. It's usually way better to have full sentences in translatable strings, even if similar strings end up repeating themselves. This gives translators more context and helps with languages which have a different grammatical structure."
      ],
      "mastodon-optimize-database-queries": [
        "I wonder if we wouldn't be better off using raw SQL here:\r\n```suggestion\r\n  scope :matches_partially, ->(str) { where(exact: false).where(\"'%' || username || '%'\") }\r\n```\r\n\r\nI'm also slightly worried about the performance implications: if the table of blocked usernames grows big, looking up with partial matches is going to be increasingly expensive, as it will require going through the whole table. Unfortunately, I do not have any suggestion how to improve that.",
        "@Gargron both would need to go through the whole table… I guess I'd stick with the current approach for now\r\n\r\n@mjankowski I'm not against that, but it would just move those decisions to later, and my first review pass is already done, so while it would have helped, it's now a bit late.",
        "Good catch. Fixed!",
        "There is a slight (probably pretty much unobservable) overhead to checking for blocks, but the overall query is actually much faster by avoiding a sequential scan (`Account.local.where(username: local_usernames)` did not use the index we have, because the index is on lowered usernames).",
        "I suppose we could do that in a follow-up PR, yeah."
      ],
      "mastodon-network-resource-limits": [
        "I think an infinite loop is possible if an attacker builds a collection of infinitely many empty pages.",
        "I think having a low recursion limit makes sense. Every request is costly (up to a few seconds of waiting on a remote server), and the more we do in the service, the longer we will keep a worker busy. This can easily become a DoS vector.\r\n\r\nAs for paginating the context, yes, that would be extremely useful, and it could be useful in this specific case as well, but as you may have noticed this is a massively complex issue that also involves a lot of UX considerations (for instance, Mastodon's display of replies is “flattened” so the pagination cutoffs are far less obvious to represent than on say, reddit)",
        "> Checking on how we are meaning recursion here, just because activitystreams also refers to just iterating through collection pages as recursion: do we mean a limit on paging or do we mean limiting the recursion depth within a reply tree (only get e.g. `depth<n` replies)?\r\n\r\nI meant a pagination limit, not specifically a reply tree depth limit.\r\n\r\n> I would think that some invisible barrier where someone finds out later 'oh i wasn't actually seeing all the replies, i needed to click around more' would be pretty frustrating.\r\n\r\nThis is always a risk, whatever limits we have, unfortunately.",
        "Wouldn't this change require that every caller that uses persistent connection fully consume the response? Is that something we're sure we're currently doing?",
        "So this changes persistent connections from never being closed by `Request` to being closed by `Request` if they return too large of a body. I wonder if this is safe. What about only closing if it's not a persistent connection?",
        "I was not completely sure how HTTP.rb handled closed connections. It seems it silently reopens a connection, which seems safe in this case."
      ],
      "mastodon-early-nil-validation": [
        "Good catch, this would indeed be a weird edge case, but it should probably be gracefully handled rather than raising an exception and clogging queues with retried jobs which are extremely unlikely to ever proceed. Will have a look!",
        "I changed it to silently return if the post is missing.",
        "`effective_date` can be `nil`, for instance for ToS entries created before this PR. In such cases, this will raise `NoMethodError (undefined method 'past?' for nil)`.",
        "I think this should go even earlier, you can avoid the Redis lock.",
        "What about just defining `uncached = {}` just a few lines above, so that it's never `nil`?",
        "No! This would precisely not work, because the failing test passes an array as `scope`, which causes `no implicit conversion of Symbol into Integer`, an error that would remain the same with `dig`!"
      ],
      "mastodon-configuration-value-safety": [
        "afaik what we need is:\r\n```suggestion\r\n    password: <%= ENV.fetch('SMTP_PASSWORD', nil).to_json %>\r\n```\r\n\r\nas we do in `config/database.yml`"
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 403,
      "following": 0
    }
  },
  "jfagoagas": {
    "repos": [
      "prowler-cloud/prowler"
    ],
    "entries": [
      {
        "slug": "prowler-configure-observability-variables",
        "title": "Configure observability variables"
      },
      {
        "slug": "prowler-consistent-environment-variable-naming",
        "title": "Consistent environment variable naming"
      },
      {
        "slug": "prowler-document-configuration-variables",
        "title": "Document configuration variables"
      },
      {
        "slug": "prowler-endpoints-for-evolving-data",
        "title": "Endpoints for evolving data"
      },
      {
        "slug": "prowler-ensure-migration-compatibility",
        "title": "Ensure migration compatibility"
      },
      {
        "slug": "prowler-format-ai-code-identifiers",
        "title": "Format AI code identifiers"
      },
      {
        "slug": "prowler-least-privilege-principle",
        "title": "Least privilege principle"
      },
      {
        "slug": "prowler-log-exceptions-with-context",
        "title": "Log exceptions with context"
      },
      {
        "slug": "prowler-meaningful-consistent-naming",
        "title": "Meaningful consistent naming"
      },
      {
        "slug": "prowler-memory-usage-optimization",
        "title": "Memory usage optimization"
      },
      {
        "slug": "prowler-parameterize-configuration-values",
        "title": "Parameterize configuration values"
      },
      {
        "slug": "prowler-pin-github-actions-dependencies",
        "title": "Pin GitHub Actions dependencies"
      },
      {
        "slug": "prowler-precise-csp-configuration",
        "title": "Precise CSP configuration"
      },
      {
        "slug": "prowler-prioritize-code-readability",
        "title": "Prioritize code readability"
      },
      {
        "slug": "prowler-safe-attribute-access-patterns",
        "title": "Safe attribute access patterns"
      },
      {
        "slug": "prowler-secure-authentication-flows",
        "title": "Secure authentication flows"
      },
      {
        "slug": "prowler-service-layer-abstraction",
        "title": "Service layer abstraction"
      },
      {
        "slug": "prowler-specific-exception-handling",
        "title": "Specific exception handling"
      },
      {
        "slug": "prowler-task-signature-methods",
        "title": "Task signature methods"
      },
      {
        "slug": "prowler-tenant-aware-query-optimization",
        "title": "Tenant-aware query optimization"
      },
      {
        "slug": "prowler-use-configurable-default-values",
        "title": "Use configurable default values"
      },
      {
        "slug": "prowler-write-objectively",
        "title": "Write objectively"
      }
    ],
    "comments": {
      "prowler-least-privilege-principle": [
        "Hello @maxi-bee, `iam:PassRole` action could lead into privilege escalation if the resource configured is `*` or list/single role with more privileges. In this case I recommend you to use the [Mutelist](https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/mutelist/) if the affected IAM Policy is flagged as `FAIL` by Prowler because in your environment/context it is not."
      ],
      "prowler-parameterize-configuration-values": [
        "Could you please store the Powershell version in a variable?\r\n```\r\nARG POWERSHELL_VERSION=v7.5.0\r\n```",
        "I don't know how are we going to manage Powershell dependencies but we'll need to find an automated way and try to have just one source of truth. Regarding the environment variable I think it is good to have it."
      ],
      "prowler-secure-authentication-flows": [
        "We need to always redirect to the SAML endpoint, if not we will allow domain enumeration if responses are different based on existence.",
        "What's in the response body? We should find a way to prevent or mitigate it.",
        "Not sure about this step, I don't know if we should make these assumptions, like `easier and faster` or `always work with org`, for example the latter does not care about the type of authentication. Also assuming a role is not authentication but authorization, it always requires base credentials."
      ],
      "prowler-task-signature-methods": [
        "`s` or `si` as we have in other tasks?",
        "`s` or `si` as we have in other tasks?"
      ],
      "prowler-configure-observability-variables": [
        "We should add these two to have more info in all the errors:\r\n- `SENTRY_ENVIRONMENT`\r\n- `SENTRY_RELEASE`"
      ],
      "prowler-safe-attribute-access-patterns": [
        "You can also do `project_info[\"source\"].get(\"location\",\"\") to have it in one line without the `if/else`"
      ],
      "prowler-service-layer-abstraction": [
        "Since you created this, could you add it as a replacement in lines 185-192?\r\n\r\nAlso please add tests to this function using `moto`.",
        "The `Content-Type` needs to be set because in AWS by default is `binary/octet-stream`. That's another reason why I recommended you to use the current SDK's S3 methods. We can adapt whatever is needed to reutilize code.",
        "S3 integration allows you to pass the same credentials as the provider and has a `test_connection` method, so you don't need to setup the AWS provider.\r\n```suggestion\r\n            S3.test_connection(**integration.credentials)\r\n```\r\n\r\nI see the `test_connection` does not support passing the raw credentials and it should. I'm going to add support to that.",
        "Why are you not using SDK's `send_to_bucket`? We should aim to have all this login in the SDK and to adapt whatever is missing for the API to use it. If not you will need to replicate all the logic in there to handle content-type and file paths.",
        "I get that. I think we should first analyse the SDK's status prior starting the work and make all the required changes. \r\n\r\nRegarding the `content-type`, we need to set it for each file because AWS sets only the file type."
      ],
      "prowler-format-ai-code-identifiers": [
        "```suggestion\nFor example, the description of `getScanTool` is \"Fetches detailed information about a specific scan by its ID.\" If the description doesn't convey what the tool is capable of doing, LLM will not invoke the function. If the description of `getScanTool` was set to something random or not set at all, LLM will not answer queries like \"Give me the critical issues from the scan ID xxxxxxxxxxxxxxx\"\n```",
        "```suggestion\n- It uses a \"supervisor\" architecture that interacts with different agents for specialized tasks. For example, `findings_agent` can analyze detected security findings, while `overview_agent` provides a summary of connected cloud accounts.\n```"
      ],
      "prowler-prioritize-code-readability": [
        "```suggestion\r\n                return {}\r\n```\r\n\r\nI think this is missing one level indentation, right?",
        "Again, to me this is really hard to follow.",
        "To me @vicferpoy has the final word on this PR.",
        "Totally agree, I'm getting older.",
        "`if` or `elif`?"
      ],
      "prowler-use-configurable-default-values": [
        "If the default value is `in-memory` we'd need to set that default.\r\n```suggestion\r\n    prowler_db_connection = os.environ.get('PROWLER_DB_CONNECTION', \"memory://\")\r\n```",
        "Can we add an environment variable for the batch size?",
        "What about `env.int(\"DJANGO_FINDINGS_BATCH_SIZE\", 1000)`",
        "Not having `DJANGO_OUTPUT_AWS_DEFAULT_REGION` makes the endpoint to raise a `HTTP 500 Internal Server Error`. I think we should handle that too.",
        "Fixed and tested ",
        "If credentials are not configured this is raising an exception, could you please check it when you get a chance?\r\n\r\nWhen this happens the call to _upload_to_s3 within generate_outputs raises an exception and the output location is not stored nor locally.",
        "Good point, thanks for pointing that out. The only issue I see with that is that if something fails there is no fallback to the local storage."
      ],
      "prowler-write-objectively": [
        "```suggestion\n2. Provide a valid Mutelist in `YAML` format. You can see full details about Mutelist [here](../tutorials/mutelist.md))\n```",
        "True, remove it 😄 \r\n```\r\n2. Provide a valid Mutelist in `YAML` format. See full details about Mutelist [here](../tutorials/mutelist.md))\r\n```"
      ],
      "prowler-ensure-migration-compatibility": [
        "Do we want to backfill all previous findings? This blocks the app startup while updating all the findings."
      ],
      "prowler-consistent-environment-variable-naming": [
        "Why public cert and private key? I think it should be either private/public key or certificate and private key.",
        "In that case, why not to rename it to `SAML_CERTIFICATE`?",
        "I'd call it either `SAML_X509_CERT` or `SAML_CERT`, to me adding public there is confusing.",
        "It's not confusing but maybe I'm used to call it differently.",
        "All the `ARTIFACTS_*` environment variables should be prefixed with `DJANGO` because it is the convention we've been using for the ones used in Django."
      ],
      "prowler-precise-csp-configuration": [
        "Do you need to add all of this to support GTM?",
        "Thanks for the clarification 👏 "
      ],
      "prowler-endpoints-for-evolving-data": [
        "@Chan9390 we've been reviewing the PR and get to the point that we need an API endpoint to fetch compliance frameworks by provider instead of having this file where all is hardcoded because we are constantly adding new frameworks and that'd add more steps when creating them.\r\n\r\nPlease talk with the API team (@vicferpoy and @AdriiiPRodri) because they can explain to you how to create a simple endpoint to get that from some memory objects the API service has.",
        "For now we'll use the Hub, included in [`15f98d7` (#7878)](https://github.com/prowler-cloud/prowler/pull/7878/commits/15f98d79e0fc2c5c7e9ac9bddb69ff4bf24a332c)",
        "@Chan9390 we've been reviewing the PR and get to the point that we need an API endpoint to fetch checks by provider instead of having this file where all is hardcoded because we are constantly adding new checks and eventually providers and that'd add more steps when creating them.\r\n\r\nPlease talk with the API team (@vicferpoy and @AdriiiPRodri) because they can explain to you how to create a simple endpoint to get that from some memory objects the API service has."
      ],
      "prowler-document-configuration-variables": [
        "Is there any real intention of having the SQLite connection chain in the environment variable? If not I think we can just set `sqlite` because it may lead to confusion, seems that the string needs to be completed.",
        "Thanks!",
        "```suggestion\n???+ note\n    The Mutelist configuration takes effect on the next scans.\n```"
      ],
      "prowler-pin-github-actions-dependencies": [
        "SHA also here.\r\n```suggestion\r\n        uses: peter-evans/find-comment@3eae4d37986fb5a8592848f6a574fdf654e61f9e #v3.1.0\r\n```",
        "Why this?",
        "Out of curiosity, why do you need to install all these in the workflow system? ",
        "Interesting, can you create a TODO and a ticket to review this in the future once the issue is addressed?"
      ],
      "prowler-log-exceptions-with-context": [
        "You can also log this in Sentry with\r\n```python\r\nimport sentry_sdk\r\n\r\nsentry_sdk.capture_exception(exception)\r\n```",
        "I added that log line within the `generate_output` `try/except` clause, if it is not enough I can add another one here."
      ],
      "prowler-meaningful-consistent-naming": [
        "Why this change? Just curious.",
        "For me `user_mail`/`mail` and `token` are more appropriate here.",
        "Thanks for the clarification!"
      ],
      "prowler-memory-usage-optimization": [
        "Why not to add a parameter to this query?",
        "What is the point of this context manager? Is it intended for testing and benchmarking?",
        "Would not be easier to just call explicitly the GC from `main`?",
        "We are not sure about this, but we will review that later on.",
        "I thought we talked about writing findings to file in _streaming_ instead of all at once to reduce memory overhead."
      ],
      "prowler-specific-exception-handling": [
        "I forgot to point the SDK to this branch and I got this error:\r\n```\r\nworker-dev-1   | [2025-06-19 16:35:02,487: ERROR/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: S3 output upload failed for integration 8740eeb1-789c-4890-93a9-f41d8c71a742: 'S3' object has no attribute 'upload_file'\r\nworker-dev-1   | [2025-06-19 16:35:02,503: ERROR/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: S3 compliance upload failed for integration 8740eeb1-789c-4890-93a9-f41d8c71a742: 'S3' object has no attribute 'upload_file'\r\nworker-dev-1   | [2025-06-19 16:35:02,517: INFO/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: All the S3 integrations completed successfully for provider 1370501e-80a3-4b1b-aad8-bdc4ce26066f\r\n```\r\n\r\nI think we should not print this line because there was an error:\r\n```\r\nworker-dev-1   | [2025-06-19 16:35:02,517: INFO/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: All the S3 integrations completed successfully for provider 1370501e-80a3-4b1b-aad8-bdc4ce26066f\r\n```",
        "Thanks!",
        "Why this? Are we sure that this is not going to break anything?",
        "I knew that, only concerned about this changes because you know what happened in other cases, but better if this fails because something was not right.\r\n\r\nThanks!",
        "We need to handle `NoSuchKey` if the file got deleted from S3.",
        "Tested and fixed ✅  ",
        "We should also add `before_send=before_send` not to send several credential errors coming from the SDK.\r\n\r\n\r\n```python\r\nignored_exceptions = [\r\n    # Authentication Errors from AWS\r\n    \"InvalidToken\",\r\n    \"AccessDeniedException\",\r\n    \"AuthorizationErrorException\",\r\n    \"UnrecognizedClientException\",\r\n    \"UnauthorizedOperation\",\r\n    \"AuthFailure\",\r\n    \"InvalidClientTokenId\",\r\n    \"AccessDenied\",\r\n    # Shodan Check\r\n    \"No Shodan API Key\",\r\n    # For now we don't want to log the RequestLimitExceeded errors\r\n    \"RequestLimitExceeded\",\r\n    \"ThrottlingException\",\r\n    \"Rate exceeded\",\r\n    # The following comes from urllib3\r\n    # eu-west-1 -- HTTPClientError[126]: An HTTP Client raised an unhandled exception: AWSHTTPSConnectionPool(host='hostname.s3.eu-west-1.amazonaws.com', port=443): Pool is closed.\r\n    \"Pool is closed\",\r\n]\r\n\r\n\r\ndef before_send(event, hint):\r\n    \"\"\"\r\n    before_send handles the Sentry events in order to sent them or not\r\n    \"\"\"\r\n    # Ignore logs with the ignored_exceptions\r\n    # https://docs.python.org/3/library/logging.html#logrecord-objects\r\n    if \"log_record\" in hint:\r\n        log_msg = hint[\"log_record\"].msg\r\n        log_lvl = hint[\"log_record\"].levelno\r\n\r\n        # Handle Error events and discard the rest\r\n        if log_lvl == 40 and any(ignored in log_msg for ignored in ignored_exceptions):\r\n            return\r\n    return event\r\n```\r\n\r\nThis is not tested 😄 ",
        "I think this needs to be refined as we previously did\r\n```python\r\ntry:\r\n                s3_object = s3_client.get_object(Bucket=bucket_name, Key=key)\r\n            except ClientError as e:\r\n                error_code = e.response.get(\"Error\", {}).get(\"Code\")\r\n                if error_code == \"NoSuchKey\":\r\n                    return Response(\r\n                        {\"detail\": \"The scan has no reports.\"},\r\n                        status=status.HTTP_404_NOT_FOUND,\r\n                    )\r\n                return Response(\r\n                    {\"detail\": \"There is a problem with credentials.\"},\r\n                    status=status.HTTP_403_FORBIDDEN,\r\n                )\r\n```\r\n\r\nWe should not raise `HTTP 500 Internal Server Error`.",
        "I do not agree with you. A `ClientError` in `botocore`/`boto3` could not end up in an `HTTP 500 Internal Server Error` because is it a wrapper on top of different types of errors. If we are doing a `list_objects_v2` we should handle what's happening by:\r\n- Logging the error\r\n- Sending the exception to Sentry\r\n\r\nWe can maybe raise 500's for some cases but not for all the possible exceptions under `ClientError`.",
        "Thanks, I know that we need to improve the way we handle this things, but I'm not sure about it."
      ],
      "prowler-tenant-aware-query-optimization": [
        "That's concerning... why are you removing the `tenant_id` filter? It is a key part of our multi tenant system with RLS, we could not remove it unless there is a strong reason behind."
      ]
    },
    "profile": {
      "location": "Madrid, Spain",
      "company": "@prowler-cloud",
      "blog": "linkedin.com/in/jfagoagas",
      "twitter_username": "jfagoagas",
      "site_admin": false,
      "followers": 59,
      "following": 61
    }
  },
  "thaJeztah": {
    "repos": [
      "docker/compose"
    ],
    "entries": [
      {
        "slug": "compose-add-logging-without-duplication",
        "title": "Add logging without duplication"
      },
      {
        "slug": "compose-avoid-ci-resource-conflicts",
        "title": "avoid CI resource conflicts"
      },
      {
        "slug": "compose-avoid-confusing-names",
        "title": "Avoid confusing names"
      },
      {
        "slug": "compose-avoid-variable-name-conflicts",
        "title": "Avoid variable name conflicts"
      },
      {
        "slug": "compose-ci-security-boundaries",
        "title": "CI security boundaries"
      },
      {
        "slug": "compose-consistent-formatting-choices",
        "title": "consistent formatting choices"
      },
      {
        "slug": "compose-environment-variable-validation",
        "title": "Environment variable validation"
      },
      {
        "slug": "compose-evaluate-dependency-api-compatibility",
        "title": "evaluate dependency API compatibility"
      },
      {
        "slug": "compose-explicit-configuration-management",
        "title": "explicit configuration management"
      },
      {
        "slug": "compose-isolate-test-dependencies",
        "title": "Isolate test dependencies"
      },
      {
        "slug": "compose-keep-code-structure-flat",
        "title": "Keep code structure flat"
      },
      {
        "slug": "compose-minimize-credential-access-scope",
        "title": "minimize credential access scope"
      },
      {
        "slug": "compose-network-api-precision",
        "title": "Network API precision"
      },
      {
        "slug": "compose-optimize-docker-layer-caching",
        "title": "optimize Docker layer caching"
      },
      {
        "slug": "compose-precise-security-pattern-matching",
        "title": "precise security pattern matching"
      },
      {
        "slug": "compose-schema-changes-upstream-first",
        "title": "Schema changes upstream first"
      },
      {
        "slug": "compose-use-standard-api-fields",
        "title": "Use standard API fields"
      }
    ],
    "comments": {
      "compose-avoid-ci-resource-conflicts": [
        "Do we actually need `--privileged` for this container? I see it bind-mounts `docker.sock`, which means that all `docker` commands will actually run against the docker daemon running on the host (not a `dockerd` daemon running inside the container (which _would_ require privileged)",
        "Actually wondering if we need a container here at all, because it's running a container, just to run a script, that uses the `docker` CLI inside the container to start new containers on the host 🤔 "
      ],
      "compose-keep-code-structure-flat": [
        "Perhaps split the switch into a `switch direction`, and within each of those (from/to) do the further checks, which could be a nested `switch` if needed.\r\n\r\nCombining the check (especially if they're checking on the same variables, such as `index` is sometimes \"tricky\", and easy to overlook if there's things excluded or \"duplicated\".\r\n\r\nNote that instead of `||` you can also use `case <condition1>, <condition2>, <condition3> ...`, which (I think) is more common."
      ],
      "compose-consistent-formatting-choices": [
        "I'd keep the `pip install` separate; it likely won't overwrite files that were installed by `apk add`, so doing it in a separate step will add a new layer, but won't make the image bigger.\r\n\r\nAlso worth to keep the same convention as for `apk add`, and split the packages that will be installed to one-per-line, sorted alphabetically (it makes the Dockerfile longer, but can help making it more maintainable)\r\n\r\n```suggestion\r\nRUN pip install \\\r\n    tox==2.1.1 \\\r\n    virtualenv==16.2.0\r\n```\r\n\r\nIf these packages are expected to be updated individually, could even be two `RUN` lines",
        "`-f` specifies a path to the Docekrfile, so good practice to put quotes around it (even though that will likely not be hit here\r\n\r\n```suggestion\r\ndocker build -f \"${DOCKERFILE}\" -t \"${TAG}\" --target \"${DOCKER_BUILD_TARGET}\" .\r\n```"
      ],
      "compose-environment-variable-validation": [
        "Perhaps could also be worth (if compose reads the cli config) to consider either an option in `features`, or `plugins` (plugins allows plugin-specific options to be set), which would allow opt-in/opt-out of this without having to use an env-var;\r\nhttps://github.com/docker/cli/blob/9861ce90fd6b8ddca19db5f803dcbef9a583e9e1/cli/config/configfile/file.go#L42-L44\r\n\r\n```go\r\n\tPlugins              map[string]map[string]string `json:\"plugins,omitempty\"`\r\n\tAliases              map[string]string            `json:\"aliases,omitempty\"`\r\n\tFeatures             map[string]string            `json:\"features,omitempty\"`\r\n```\r\n\r\n(in addition to an env-var probably)",
        "Yeah, my thinking here was that the cli-config would more easily allow this to be set as a default, which could also allow (e.g.) it to be set through docker desktop \"settings\"."
      ],
      "compose-optimize-docker-layer-caching": [
        "I _think_ this `ENV` is only used by `script/build/linux-entrypoint`, so better to move it lower to prevent unwanted cache-busts",
        "very minor nit; this file likely doesn't change much, so could be moved before the `COPY . . `, or even before the `COPY requirements.txt` (ordering from \"less frequently changing\" to \"most frequently changing\").\r\n\r\nIf this script (and `ENTRYPOINT` below is only used for `runtime`, might consider moving both to the start of the `runtime` stage)"
      ],
      "compose-precise-security-pattern-matching": [
        "A minor optimisation could be to use `--format` (so that only the Security options are output), and/or to match `name=userns` instead of just `userns` (as that's what the daemon will return; https://github.com/moby/moby/blob/b6684a403c99aaf6be5b8ce0bef3c6650fcdcd12/daemon/info.go#L180-L182\r\n\r\n```suggestion\r\nif docker info --format '{{json .SecurityOptions}}' 2>/dev/null | grep -q 'name=userns'; then\r\n```"
      ],
      "compose-avoid-confusing-names": [
        "`Docker-Compose` (both capital) is definitely incorrect (should either be `docker-compose` (name of the binary), or `Docker Compose`). Perhaps avoid the name altogether and use something similar as the `docker` cli uses (`Print version information and quit`)\r\n\r\n```suggestion\r\n      version            Show version information and quit\r\n```\r\n\r\n(quit/exit, not sure what's clearer)? `curl` uses `quit`;\r\n\r\n```\r\n -V, --version       Show version number and quit\r\n```\r\n\r\n"
      ],
      "compose-isolate-test-dependencies": [
        "Would it make sense to have a separate module for the e2e tests, so that these test dependencies don't become a dependency for the main module?\r\n\r\nWe took that approach in containerd, where we then replaced the main module  with a path, to make sure we use the code from the branch where it's needed ; https://github.com/containerd/containerd/blob/main/integration/client/go.mod#L79\r\n",
        "Thanks! So, yes, I think the \"ideal\" at some point would be to (e.g) have a docker image with the integration tests compiled in, which could be run with a compose binary and docker socket mounted; this would also allow for (e.g.) https://github.com/moby/moby to run the latest e2e/integration tests as part of CI."
      ],
      "compose-schema-changes-upstream-first": [
        "Note that we can't update the 3.7 schema, as it's already been released, so to add this property to the schema, it probably has to be added to the upcoming 3.9 schema in https://github.com/docker/cli/blob/master/cli/compose/schema/data/config_schema_v3.9.json first"
      ],
      "compose-explicit-configuration-management": [
        "Did it automatically update this one, or did it still allow go 1.21.0 here?\r\n\r\nI generally try to treat this one the same as other dependencies; list the minimum required version, and only update if it's _impossible_ to use with older versions; see https://github.com/containerd/containerd/pull/10596#discussion_r1721294997",
        "Because 26.1.1 is lower than 26.1.3. Thank Go for inventing pseudo versions and not understanding release branches",
        "before we have a tag, you can temporarily add replace rules; see https://github.com/docker/buildx/pull/2499",
        "Do we have PRs for this in the upstream repositories? Looks like compose is using master / v0.13.x as dependency, so if we could get the fix merged in upstream, that'd be good, I think?",
        "Could we set `GO111MODULE=auto` (or `on`), or `-mod=<what is it?>` in the makefile? That way we wouldn't have to think about setting that in `docker-ce-packaging`.",
        "But.. I have to admit that I kinda agree with https://github.com/docker/compose/pull/9776#discussion_r952832902, and wonder to what extend we need to have this complicated auto-detection.\r\n\r\nI think common scenarios would be either;\r\n\r\n- `docker` is installed (and `docker buildx` would be available as well for regular installs)\r\n- it's run in GitHub actions with only `buildx` installed; in that case we can set `BUILDX_CMD=buildx`\r\n- if neither is true, then I think a `/bin/sh: docker: not found` error may be \"just fine\" (after all, we're also not doing similar things to detect if `go` is installed, or `make`, or `git`)\r\n\r\nIf we agree with the above, just a;\r\n\r\n```make\r\nBUILDX_CMD ?= docker buildx\r\n```\r\n\r\nwould cover that scenario\r\n\r\n(feedback / thoughts welcome!)\r\n",
        "Thanks! Sorry for being nit-picky there (I can see some value for auto-detection in other scenarios), just looking \"can we simplify things (within reason)\"? In the end, the repository would have a \"how to build\" with some prerequisites, so if things fail, users should just \"read the manual\" 😂 ",
        "Looks like we're also updating various dependencies here; were these needed for Go 1.17? (otherwise it's good practice to do this separately)."
      ],
      "compose-use-standard-api-fields": [
        "Wondering why you didn't use the `Status` field that's returned by the API (which is the field that's used by the `docker` CLI;\r\n\r\n```bash\r\ncurl --unix-socket /var/run/docker.sock \"http://localhost/containers/json\" | jq .\r\n```\r\n\r\n```json\r\n\r\n  {\r\n    \"Id\": \"82950c6535204a462c8a3c1f175408b456fbb91971d2d21a33ba04c8b91c74fd\",\r\n    \"Names\": [\r\n      \"/cranky_keldysh\"\r\n    ],\r\n    \"Image\": \"libnetworkbuild\",\r\n    \"ImageID\": \"sha256:bc6bbc6a0032300d8818182eff0101ecb7af2fc1fb21da6f290943c286946a1e\",\r\n    \"Command\": \"make unit-tests-local\",\r\n    \"Created\": 1573091888,\r\n    \"Ports\": [],\r\n    \"Labels\": {},\r\n    \"State\": \"running\",\r\n    \"Status\": \"Up 5 minutes\",\r\n    \"HostConfig\": {\r\n      \"NetworkMode\": \"default\"\r\n    },\r\n    \"NetworkSettings\": {\r\n      \"Networks\": {\r\n        \"bridge\": {\r\n          \"IPAMConfig\": null,\r\n          \"Links\": null,\r\n          \"Aliases\": null,\r\n          \"NetworkID\": \"5a59d43d598f910579535ffb2cbbb4d0987807d7b5593c264c83337c4220ec1a\",\r\n          \"EndpointID\": \"bf8b6b629b353988d047bcd0bbb897249d7a73a7811b2210b783338e6a975cd9\",\r\n          \"Gateway\": \"172.17.0.1\",\r\n          \"IPAddress\": \"172.17.0.5\",\r\n          \"IPPrefixLen\": 16,\r\n          \"IPv6Gateway\": \"\",\r\n          \"GlobalIPv6Address\": \"\",\r\n          \"GlobalIPv6PrefixLen\": 0,\r\n          \"MacAddress\": \"02:42:ac:11:00:05\",\r\n          \"DriverOpts\": null\r\n        }\r\n      }\r\n    },\r\n    \"Mounts\": [\r\n      {\r\n        \"Type\": \"bind\",\r\n        \"Source\": \"/Users/sebastiaan/projects/libnetwork\",\r\n        \"Destination\": \"/go/src/github.com/docker/libnetwork\",\r\n        \"Mode\": \"\",\r\n        \"RW\": true,\r\n        \"Propagation\": \"rprivate\"\r\n      }\r\n    ]\r\n  }\r\n]\r\n```",
        "should the new option deprecate the old (`--no-ansi`) one? (at least making them conflicting options, as (I think) `--no-ansi` is the equivalent of `--ansi=never`?)\r\n\r\nFor the `docker` cli, if there's an option that cannot be removed we usually _hide_ the option (to discourage use), and (depending on the case) print a deprecation warning if used (but keep it functional if needed).\r\n\r\nAlso looking if we should align the UX to the `--progress` option used on `docker build`;\r\n\r\n```\r\n      --progress string         Set type of progress output (auto, plain, tty). Use plain to show container output (default \"auto\")\r\n```"
      ],
      "compose-network-api-precision": [
        "I think the trick also requires `--opt type=none` "
      ],
      "compose-avoid-variable-name-conflicts": [
        "Perhaps it would be good to use a different variable name for this; I know that some tools (`rpm`, `deb` packaging) also set `LDFLAGS` as environment variable, and in those cases it's important to reset them.\r\n\r\nWe could use something similar a containerd, which uses `GO_LDFLAGS` (as well as some other `GO_` prefixed variables to prevent conflicts); https://github.com/containerd/containerd/blob/be91a219c2ac5e65c00bbe85c5dff0827d41958b/Makefile#L92-L102\r\n",
        "It's often a good idea to use a different name for the `ARG` than for the `ENV`. Both act in the same \"space\" (both are set as (environment) variables, which can lead to run situations where the `ENV` is always overridden by the `ARG` (I can find some examples, don't have them at hand)\r\n\r\nSo, might want to consider, e.g.;\r\n\r\n```Dockerfile\r\nARG GIT_COMMIT=unknown\r\nENV DOCKER_COMPOSE_GITSHA=$GIT_COMMIT\r\n```\r\n\r\n(or vice-versa)"
      ],
      "compose-add-logging-without-duplication": [
        "Wondering; should we log something if the service _does_ have an `image:` specified, but either doesn't have `build:` or image has a _digest_ set (to give some clue why the image for a service wasn't pushed)?"
      ],
      "compose-evaluate-dependency-api-compatibility": [
        "Do we know what patches are in this fork, and if they were rejected upstream? I know `fsnotify` has had some time where maintenance was slow, but I think it improved in that respect (and I _think_ we have some maintainers on it that are also maintainers for moby/moby)",
        "Ah, thanks! Yes, saw the comment later on, and saw that (github indicated \"10\" commits in the fork);\r\n\r\nhttps://github.com/fsnotify/fsnotify/compare/main...tilt-dev:fsnotify:main\r\n\r\nPinning to a commit from upstream SGTM (short term). I'm mostly trying to avoid having 2 forks of the same dependency (as I know we have fsnotify as dependency in our tree already).\r\n\r\nIf someone has some cycles to spare to look what patches are not (yet) in upstream, we could contribute them there.\r\n\r\n@cpuguy83 were you a maintainer on that repo? Or do I misremember that? (Otherwise I _think_ there's some familiar people on it, that we may try to reach out to to ask for a (pre-)release)."
      ],
      "compose-ci-security-boundaries": [
        "This is a 3rd party action, so potentially less \"trusted\"; wondering if this is one that we should pin to a commit? (also make sure that we evaluate the changes in the release).\r\n\r\n\r\nDiff since last (v1.9.0) v1 release (but perhaps there's been other v1 updates since it was added); https://github.com/tibdex/github-app-token/compare/v1.9.0...v2.1.0",
        "Thanks!"
      ],
      "compose-minimize-credential-access-scope": [
        "same comment as on the other open source repo; I'd prefer to have new credentials here, that are limited to just what's needed for this scan (so that we can easily rotate those if needed, and they don't provide access to things we don't want)"
      ]
    },
    "profile": {
      "location": "Netherlands",
      "company": "thaJeztah",
      "blog": "",
      "twitter_username": "thaJeztah",
      "site_admin": false,
      "followers": 1803,
      "following": 35
    }
  },
  "jacob314": {
    "repos": [
      "google-gemini/gemini-cli"
    ],
    "entries": [
      {
        "slug": "gemini-cli-add-tests-for-changes",
        "title": "add tests for changes"
      },
      {
        "slug": "gemini-cli-avoid-non-null-assertions",
        "title": "avoid non-null assertions"
      },
      {
        "slug": "gemini-cli-centralize-configuration-management",
        "title": "Centralize configuration management"
      },
      {
        "slug": "gemini-cli-choose-efficient-data-structures",
        "title": "Choose efficient data structures"
      },
      {
        "slug": "gemini-cli-document-configuration-defaults-clearly",
        "title": "Document configuration defaults clearly"
      },
      {
        "slug": "gemini-cli-ensure-comprehensive-user-documentation",
        "title": "Ensure comprehensive user documentation"
      },
      {
        "slug": "gemini-cli-implement-resource-constraints",
        "title": "implement resource constraints"
      },
      {
        "slug": "gemini-cli-maintain-naming-consistency",
        "title": "Maintain naming consistency"
      },
      {
        "slug": "gemini-cli-minimize-performance-overhead",
        "title": "minimize performance overhead"
      },
      {
        "slug": "gemini-cli-never-ignore-errors-silently",
        "title": "Never ignore errors silently"
      },
      {
        "slug": "gemini-cli-optimize-react-hooks-usage",
        "title": "optimize React hooks usage"
      },
      {
        "slug": "gemini-cli-organize-code-by-responsibility",
        "title": "organize code by responsibility"
      },
      {
        "slug": "gemini-cli-prefer-lightweight-composable-apis",
        "title": "Prefer lightweight composable APIs"
      },
      {
        "slug": "gemini-cli-prefer-settings-over-environment",
        "title": "prefer settings over environment"
      },
      {
        "slug": "gemini-cli-prevent-concurrent-state-races",
        "title": "Prevent concurrent state races"
      },
      {
        "slug": "gemini-cli-prevent-react-race-conditions",
        "title": "Prevent React race conditions"
      },
      {
        "slug": "gemini-cli-reduce-nesting-complexity",
        "title": "reduce nesting complexity"
      },
      {
        "slug": "gemini-cli-secure-input-validation",
        "title": "Secure input validation"
      },
      {
        "slug": "gemini-cli-test-behavioral-differences",
        "title": "Test behavioral differences"
      }
    ],
    "comments": {
      "gemini-cli-minimize-performance-overhead": [
        "nit: all of these cases are a bit verbose with repeated logic to track a start time, end time and then compute the duration. An alternate more terse api could be something like\r\n```\r\ntrackStartupPerformance(async () => {\r\n   await start_sandbox(sandboxConfig, memoryArgs);\r\n  },\r\n  'sandbox_startup`\r\n);\r\n```\r\nwhere the trackStartupPerformance method could have a no-op implementation if isPerformanceMonitoringActive()\r\nis false.\r\nAs a bonus that would also make it easy to connect this to the performance.measure API in debug builds so that the times are also visible on the chrome devtools timeline.\r\n",
        "please avoid timeouts in tests as slow tests slow everyone down. Can you use \r\n`await wait(1)` if a wait is really required for this to pass?",
        "comment applies to all wait calls added in the file"
      ],
      "gemini-cli-prevent-concurrent-state-races": [
        "this logic is not safe to perform here. instead use the existing api in text-buffer.ts or create one. that. is crucial to make sure this logic runs inside the reducer in text-buffer that ensures commands are executed properly and in order even when multiple key strokes are executed in the same react loop. the same concern applies to all of the other cases here that modify the state of the text. buffer. they need to instead call a method on text-buffer.ts to apply the transformation desired rather than reading the possibly stale state of the text buffer and performing operations on it.\nsorry this part of how text-buffer works is a little surprising but it is better than the alternatives where it becomes really difficult to be confident that react is dealing with the correct state.",
        "remove this helper completely as it is not safe given the fact that you want to handle multiple keystrokes one after the other so the buffer could be stale by the time you handle the second one if both keystrokes are handled in the same react event frame. sorry this logic has to be written in such a particular way to be robust but you are making changes to the trickiest part of the input system where we need to chain operations together carefully to avoid confusing user behavior.",
        "please refactor avoid this catch block. there should be ways to avoid any race condition here. if there are failure modes I would much rather have us check that this.events is empty that catch to swallow exceptions."
      ],
      "gemini-cli-centralize-configuration-management": [
        "can you put this in settings.ts / settings.json rather than adding as an env variable?\nI could also see the case for making this an env variable but would default to just surfacing it in settings.ts as that should be sufficient and it is easier if you don't need to merge settings from multiple sources.\nthe case for why this needs to be settable via settings.json is that some orgs might want to disable this for everyone in their org and that is easiest to accomplish via the existing mechanism for org level settings.json files.",
        "nit: still see reading an env variable rather than settings here.",
        "this is worth moving to its own file rather than keeping in app.tsx\nI'd like to reuse in InputPrompt and text-buffer.ts as well.\nIn addition, I'm hoping this can also be the basis to in the future configure keyboard shortcuts via a file and surface the up to date keyboard shortcuts in `/help`.",
        "this is a good place to be loading the custom themes. I suspect the call to load in ThemeDialog wasn't needed."
      ],
      "gemini-cli-optimize-react-hooks-usage": [
        "we should be able to avoid toggleVimModeRef. Let me know if you need help figuring out how to avoid it and I can dig into it. would expect you can duplicate other patterns in this file and that with the right instructions Gemini CLI can likely help you remove this without introducing circular reference issues.\ngenerally we try to avoid useRef in the code base unless we strictly have to as the logic is easier to reason about when we don't."
      ],
      "gemini-cli-ensure-comprehensive-user-documentation": [
        "I think we should add a message.\nThe message entering vim mode should tell you how to toggle between INSERT and NORMAL modes and the message leaving should clarify that you have left vim mode.",
        "love that this message is shown inline. I would still like to see a slightly longer message in the list of messages as well. that should be trivial to add removing the `// no message` comment and  adding a message with this content plus a reminder to run `/vim` again to leave the mode.\nThe message when leaving could be as simple as \"Exited Vim mode.`"
      ],
      "gemini-cli-implement-resource-constraints": [
        "Thank you for adding this! I am really excited to get analytics to understand how bad user's memory usage usage issues.\r\nHowever, every 5 seconds is a bit too frequent. ideally we could only send this when the app is in use and even then we should rate limit a bit more to perhaps only send this metric at most once every minute. Some other tools only send analytics about memory usage at most once every hour.\r\nThe easiest way to detect that the app is in use would be to have an API the CLI package can call when the user types of an additonal message is added to history. \r\n\r\nAnother complementary option is to poll ever perhaps 10 seconds fetching the RSS side but only report if the RSS is at least 5% greater than the previous high water mark for the largest RSS side. Memory usage will bounce up and down when there are garbage collection, potentially due to garbage collection that happens purely due to tracking these events so you have to be careful you only send events when memory usage has increased relative to the largest value it has ever been rather than the previous value.",
        "can you comment where the 50 ms threshold comes from? I would worry we might miss that terminals support kitty when the users machine happens to be under heavy load but I also don't love adding >50ms of delay for all users who happen to not be using a kitty protocol terminal.",
        "unshift, shift, and slice are O(n).\r\nYou might want to use \r\nhttps://yomguithereal.github.io/mnemonist/fixed-deque\r\nor a doubly linked list to make these faster. shouldn't matter that much with a max of 1000 events but might be worth doing anyway to avoid an issue if max_events is raised."
      ],
      "gemini-cli-avoid-non-null-assertions": [
        "tweak so you don't need the `!` after fileReadResult here and elsewhere. likely you can fix by a union type so fileReadResult is required when success is true. alternately add a case above that should never be it where you report an error if fileReadResult is undefined but success is true with a comment that it shouldn't occur.",
        "this change seems a little unrelated and generally agree with Gemini that it is safer to have this `throw`  unless this.contentGenerator can't possibly be null but the type system just can't figure it out."
      ],
      "gemini-cli-choose-efficient-data-structures": [
        "same comment as bellow about useing\r\nhttps://yomguithereal.github.io/mnemonist/fixed-deque\r\nor a doubly linked list to avoid O(events.length) operations. "
      ],
      "gemini-cli-reduce-nesting-complexity": [
        "nit: follow style of returning early to reduce nesting.",
        "nit: this conditional is getting a bit hard to read. as a fast follow can you refactor it to use a switch by AuthType or other technique to make it not as nested.",
        "200 is a bit of a magic number. make it a const while you are cleaning this up."
      ],
      "gemini-cli-prevent-react-race-conditions": [
        "Gemini CLI should be able to help you refactor this hook to better align with the Gemini CLI style guide. I would encourage you to do the following to help get this to align better with the GEMINI.md in the project.\nI gave Gemini CLI the following prompt\n\"analyze @vim.ts focusing on ways it might not be aligned with this repos GEMINI.md which has specific instructions on how to use React\"\n\n\n   1. **State Management and Performance:** The hook uses multiple useState calls for interdependent state variables (mode, count, pendingG, pendingD, pendingC). This creates a complex\n      state machine that can lead to unnecessary re-renders. For instance, every keypress in NORMAL mode that is part of a count (e.g., typing \"12\") triggers a state update and\n      re-render. A more performant approach would be to manage the Vim state in a single useReducer or a useRef to avoid re-renders on every minor state change.\n[Jacob's thoughts] if we can solve this with useReducer that would be ideal. If not, I would suggest useRef. I would also worry about the current design breaking down if multiple key presses are sent in quick succession before react has time to recompute. what you are doing using useRef for entering and leaving insert mode solves it for that option but does not handle the other state.\n\n   2. **Overuse of `useCallback`**: Many of the functions wrapped in useCallback have empty dependency arrays, which suggests they do not depend on component props or state. While this\n      prevents them from being recreated on every render, it also adds boilerplate and can be a sign that these functions could be defined outside the component if they do not rely on\n       component-specific data.\n\n   3. **Complex Side Effects and State Synchronization:** The hook uses a useEffect to synchronize the mode state with a modeRef. This is a workaround for accessing the current mode\n      within the handleInput callback, which has a stale closure over the mode state. This pattern is a known \"code smell\" in React and can often be avoided by using useReducer or by\n      structuring the code to pass the latest state directly to event handlers.\nMy thoughts: if you are able to useReducer for this that would be ideal but I suspect it may not be feasible for your case.\n\n[Jacob's thoughts:] Ignore suggestion 4. I think your code is quite elegant given the difficulty of the problem it is solving. \n   4. Readability and Maintainability: The handleInput function is extremely large and contains a deeply nested switch statement. This makes it difficult to read, test, and maintain.\n      Breaking this logic down into smaller, more focused functions would significantly improve the code's clarity and align with the GEMINI.md's emphasis on simplicity and\n      readability.",
        "these two utilities should also go into text-buffer.ts as they also need to be integrated in with the reducer in text-buffer otherwise we are operating on stale input in the even that multiple keystrokes were handled in the same React event loop.",
        "this also needs to be integrated into text-buffer or we will hit issues when multiple keystrokes need to be processed in the same react event loop."
      ],
      "gemini-cli-organize-code-by-responsibility": [
        "can you move this escape logic into InputPrompt rather than having App.tsx deal with functionality that is focused on input prompt?",
        "can you move this out of App.tsx and into a separate file? I'd also suggest considering making this method be less coupled from react. After you sync to the latest you might want to check out how 'unhandledRejection' is now dealing with a similar case in gemini.tsx You could then start the process of checking for updates in gemini.tsx before the react app is even started using the same mechanism to send events to the app notifying it that updates have been accepted.",
        "nit: can you move this out of gemini into autoUpdate.ts or similar. could make testing easier and gemini.tsx is one of the tragedy of the commons files like app.tsx that it would be nice to try to keep small.",
        "please add these to TextBuffer rather than InputPrompt as nothing about this is specific to the InputPrompt",
        "this logic to execute the keystroke handlers seems independent on app. I would move it to keystrokeHandler.ts (also please rename that file to align with naming schemes for non tsx files in this project.\nWould then be nice to add a basic unittest for it that verifies cases such as if key.shift  and key.ctrl are handled correctly. seems like right now key.shift is ignored.",
        "grabbing the theme from the theme manager is making all of this code a bit more complex and isn't needed as colors.ts is actually doing this behind the scenes in each getter.",
        "this logic should go one level deeper in TextBuffer rather than InputPrompt as it does not require any of the additional data (e.g. autocomplete) that InputPrompt has.",
        "Thank you! Please let me know if there is anything I can do to help. Could really use this functionality",
        "this logic should move to text-buffer.ts and operate on the lines before wrapping.\r\nlets also be sure to implement this in a way that we can support highlighting blocks like [Image 1] with custom colors in the future. I'd suggest we move these complex changes to a second PR and land this first just supporting paste that adds a user visible `@some/path/to/image.png` in the input prompt.",
        "moving to text-buffer.ts will also make it easy to test this as that part of the code base has reasonable unit tests."
      ],
      "gemini-cli-document-configuration-defaults-clearly": [
        "document that it is saved to `~/.gemini/settings.json`"
      ],
      "gemini-cli-never-ignore-errors-silently": [
        "please continue to log errors handling clipboard images so users can understand what went wrong.",
        "done",
        "one concern is that users of this setting will not be able to tell when errors have occurred. I would be more comfortable with the option if we still showed the number of errors that had occurred even when displayFooter is false."
      ],
      "gemini-cli-maintain-naming-consistency": [
        "to align with how things will look in the future when we align the input processing with what is in text-buffer.ts I think we should change this to allowing the command both with and without shift. rather than allowing both 'c' and 'C'",
        "also need to handle escaped spaces in paths. check the existing logic for autocomplete for file paths to make sure the logic is aligned.",
        "nit: can you add support for 'j' and 'k' to match the existing ink library we were previously using?"
      ],
      "gemini-cli-prefer-settings-over-environment": [
        "remove this env variable. I don't think this needs a setting but if it did, we should instead add it to settings.ts rather than checking env variables directly. that is the generally preferred way for us to manage user options that some users might want to opt into.",
        "we should also probably support force-model in \r\nsettings.ts so you can force it in your settings.json rather than via a command line argument",
        "sorry after some further thinking. Can we make this only be in settings.json to start out with? ",
        "it would make this a bit more annoying to use as settings.json is in the CLI package but what do you think of making this be in settings.ts rather than an env variable? this kindof seems like something I might want to set in settings.json. For example, maybe my project is really large so I want a specific value for it.\nin general I'm trying to keep most settings in settings.ts where feasible rather than encouraging more env variables or options in config controlled by args unless they strictly have to go that way. Happy to be convinced otherwise for this case.",
        "rather than adding this to config can we instead add it to settings.json with a property added to settings.ts.\r\nGenerally settings.json is a bette place for options that are small UI tweaks than command line flags. If there is a use case where a user wants to frequently change between showing the footer and not showing the footer then I would be open to including it in config.ts\r\n"
      ],
      "gemini-cli-prefer-lightweight-composable-apis": [
        "this doesn't seem very algined with existing code with useInput calls throughout the application and InputPrompt that does its own non-ink standard keyboard processing. Given that I would suggest a solution that is lighter weight and instead allows callers to resolve whether specific input matches a specific keybinding rather than a solution that requires taking control of all useInput in the application."
      ],
      "gemini-cli-add-tests-for-changes": [
        "please add a test case that verifies the case you fixed is resolved. for example, write a test that would have failed previously due to the race condition you fixed.",
        "please add a test for this.",
        "wow good catch that we weren't merging mcp servers correctly previously. please add a test for this to prevent regressions.",
        "there is another pull request for this but I think your fix is cleaner so lets go with what you are doing.\r\nhttps://github.com/google-gemini/gemini-cli/pull/2231#pullrequestreview-2973500668",
        "please add tests for this new code in theme manager"
      ],
      "gemini-cli-test-behavioral-differences": [
        "make these tests a little more robust by adding some text to undo and redo and verifying that the text is actually undone and redone.",
        "agree this is worth testing. also add tests for paths with escaped spaces. That is needed as filenames can contain spaces.",
        "can you add a couple more test cases?\r\nOther cases of interest:\r\nThe non wrapping text exceeds the length of the MaxSizedBox.\r\nThe non wrapping text happens to have line breaks (ugly edge case for this)\r\nThe non wrapping text  exceeds the length of the MaxSizedBox.\r\nAdd a case with multiple rows in the MaxSizedBox just for clarity of how this may look with multiple lines with `...`",
        "also add a test case with emojis or other wide characters to make sure we are now handling that case correctly.",
        "Is this test actually checking the rendered text for this case. On my phone so sorry if I am missing something."
      ],
      "gemini-cli-secure-input-validation": [
        "rather than rolling our own shell escaping, it might make sense to use an existing npm package for this.\r\nhttps://www.npmjs.com/package/shescape?activeTab=code\r\nthis package isn't that popular so I am also open to just rolling our own or trying to find a popular package."
      ]
    },
    "profile": {
      "location": "Seattle",
      "company": "Google",
      "blog": "",
      "site_admin": false,
      "followers": 205,
      "following": 37
    }
  },
  "dummdidumm": {
    "repos": [
      "sveltejs/svelte"
    ],
    "entries": [
      {
        "slug": "svelte-analyze-transitive-dependencies",
        "title": "analyze transitive dependencies"
      },
      {
        "slug": "svelte-api-flexibility-balance",
        "title": "API flexibility balance"
      },
      {
        "slug": "svelte-async-cleanup-safety",
        "title": "async cleanup safety"
      },
      {
        "slug": "svelte-choose-descriptive-names",
        "title": "Choose descriptive names"
      },
      {
        "slug": "svelte-complete-code-examples",
        "title": "Complete code examples"
      },
      {
        "slug": "svelte-defensive-error-handling",
        "title": "defensive error handling"
      },
      {
        "slug": "svelte-document-configuration-hierarchies",
        "title": "document configuration hierarchies"
      },
      {
        "slug": "svelte-documentation-clarity-standards",
        "title": "Documentation clarity standards"
      },
      {
        "slug": "svelte-measure-performance-impact",
        "title": "Measure performance impact"
      },
      {
        "slug": "svelte-multi-indicator-configuration-detection",
        "title": "Multi-indicator configuration detection"
      },
      {
        "slug": "svelte-prefer-simple-code-patterns",
        "title": "prefer simple code patterns"
      },
      {
        "slug": "svelte-prefer-testing-libraries",
        "title": "prefer testing libraries"
      },
      {
        "slug": "svelte-preserve-user-input-focus",
        "title": "preserve user input focus"
      },
      {
        "slug": "svelte-realistic-documentation-examples",
        "title": "realistic documentation examples"
      },
      {
        "slug": "svelte-runtime-html-escaping",
        "title": "Runtime HTML escaping"
      },
      {
        "slug": "svelte-state-boundary-management",
        "title": "state boundary management"
      },
      {
        "slug": "svelte-use-modern-null-safe-operators",
        "title": "use modern null-safe operators"
      },
      {
        "slug": "svelte-write-clear-test-cases",
        "title": "Write clear test cases"
      }
    ],
    "comments": {
      "svelte-api-flexibility-balance": [
        "I briefly thought about that but it seemed unnecessary - in which case would you have existing attributes on a html tag but in such a way that you know which ones to then merge them in some way? Even if, the regex for adjusting the html attributes string would be straightforward. So I opted for making the simple case more ergonomic.\r\n\r\nIt _is_ an interesting question for SvelteKit specifically though, which currently sets `lang=\"en\"` in `app.html` by default. What would we do here? (regardless of whether we return a string or an object). The easiest would be to have `lang=\"en\"` after the string and rely on browser being forgiving about it (they ignore duplicate attributes) / the user removing it in case they set it themselves in `<svelte:html>`",
        "let's play it save and make this an options argument, so we can enhance this in the future of needed"
      ],
      "svelte-document-configuration-hierarchies": [
        "```suggestion\r\n> Using an identifier or a rest element as the declarator for `$props` when compiling to custom elements without declaring `props` in the component options means that Svelte can't know which props to expose as properties on the DOM element. Consider explicitly destructuring all the props or add the `customElement.props` option.\r\n```",
        "previously the depth was unlimited. Are we confident with `3` being a right number which doesn't turn up false positives for people previously having no warning?",
        "If I were to design this from scratch I'd also take 3, it's more about people not seing a warning now which they previously didn't. But having such a deeply nested input is probably super rare. Maybe I'd go with 5.",
        "```suggestion\r\n- `svelte:options` now lets you set the `css: \"inject\"` compiler option on a per-component basis (**5.0.0-next.209**, [#12660](https://github.com/sveltejs/svelte/pull/12660))\r\n```"
      ],
      "svelte-prefer-simple-code-patterns": [
        "```suggestion\r\n\t\t\tstyle_directives.some((directive) => directive.metadata.expression.has_state);\r\n```",
        "Can we use a boolean instead of a number here? Seems like it's only used within one if block"
      ],
      "svelte-use-modern-null-safe-operators": [
        "Pretty sure you can reduce this to `/** @type {Derived} */ (reaction).parent?.f & DERIVED) !== 0`",
        "Of course you can?\r\n```js\r\nlet foo = { x: { i: 2 } };\r\nconsole.log((foo.x.i & 2) !== 0); // true\r\nconsole.log((foo.y?.i & 2) !== 0); // false\r\n```\r\nit doesn't work in the case of `=== 0` because then regardless of undefined or a number it's always 0, but it works in the `!== 0` case",
        "we should probably also adjust the call signature to allow undefined for the handler param, and adjust that for `export function event(...)` aswell ",
        "```suggestion\r\n\t\t\t? process.cwd?.()\r\n```"
      ],
      "svelte-state-boundary-management": [
        "Because it is unavoidable in some scenarios. Angular actually had a setting where writing inside effects was disallowed and they removed it because it caused so many headaches (and didn't actually help, most people just turned on the option)",
        "````suggestion\r\n\r\nThe fallback value of a prop not declared with `$bindable` is treated like a non-reactive POJO, and therefore also doesn't update the component when mutating its properties.\r\n\r\n```svelte\r\n<--- file: Child.svelte --->\r\n<script>\r\n\tlet { object = { count = 0 } } = $props();\r\n</script>\r\n<button onclick={() => {\r\n\t// has no effect if the fallback value is used\r\n\tobject.count += 1\r\n}}>\r\n\tclicks: {object.count}\r\n</button>\r\n```\r\n\r\nIn general, mutating props is discouraged, instead use callback props to make it easier to reason about state and changes to that state. If parent and child should share (and be allowed to mutate) the same object, then use the [$bindable]($bindable) rune.\r\n\r\n````",
        "```suggestion\r\n\r\n> [!NOTE] Fallback values are not turned into reactive state proxies (see [Updating props](#Updating-props) for more info)\r\n\r\n```"
      ],
      "svelte-write-clear-test-cases": [
        "what I like to do in these tests is also have another function above that passes, so that you know \"this one doesn't throw, this one does throw, as expected\"",
        "Names are a bit confusing: these imports are not broken.\r\nIn general, it's probably better to move this test into the validation test suite, where you can more clearly test \"I expect this not to fail\""
      ],
      "svelte-choose-descriptive-names": [
        "```suggestion\r\nexport default function read_expression(parser, opening_token, disallow_loose) {\r\n```",
        "I think so - I was hesitant to adopt the subscriber nomenclature because of it sounding too much like stores, but it explains the concept much better"
      ],
      "svelte-multi-indicator-configuration-detection": [
        "you should also check for `$:` statements",
        "```suggestion\r\n\t// Components not explicitly in legacy mode might be expected to be in runes mode (especially since we didn't\r\n\t// adjust this behavior until recently, which broke people's existing components), so we also bail in this case.\r\n\t// Kind of an in-between-mode.\r\n\tif (context.state.analysis.runes || context.state.analysis.maybe_runes) {\r\n```",
        "The problem with these is that I have seen these appear above typescript functions in the wild. `@type` on the other hand is a pretty strong indicator ",
        "> If they use JSDoc wouldn't they omit the lang=\"TS\"?\r\n\r\nYes, which is why there's the outer `||` in which case we see \"this is definitely TS because the lang tag is set\". This is only about the ambiguity of people using JSDoc but having a `tsconfig.json`\r\n\r\n> Makes sense. However, if someone specifies use_ts to be true or false, shouldn't that precedence over a check?\r\n\r\nIt will be set to `true` or `false` by `svelte-migrate` in https://github.com/sveltejs/kit/pull/12881 based on the presence (or absence) of tsconfig.json, which as pointed out above does not necessarily mean you're using TS, so the check is still needed"
      ],
      "svelte-defensive-error-handling": [
        "I'd rather have us warn here and add `UNKNOWN`, else people might be stuck with not using certain operators once they land if they can't upgrade to a version that supports it (people could be on v5 and we only add the operator in v6)",
        "I think that's the wrong conclusion. If something is thrown we catch it, no exceptions (ha). So I'd rather adjust the catch logic to not have a stack in that case and also adjust the types to `unknown`",
        "Just as a thought: What if we wrap non-errors in an error object and put the original on some property? Or does that mess with user expectations too much? Not sure if doing that would help you have nicer/smaller logic in the code. It would at least help with attaching a stack, though that's probably not the most important thing.\r\n(feel free to resolve the conversation again if you think what we have now is better than wrapping the error)",
        "```suggestion\r\n\t// try-catch needed because this tries to read properties of `a` and `b`,\r\n\t// which could be disallowed for example in a secure context\r\n\ttry {\r\n```",
        "```suggestion\r\n\tif (DEV) {\r\n\t\t// prevent parent/child element state being corrupted by a bad render\r\n\t\treset_elements();\r\n\t}\r\n```"
      ],
      "svelte-preserve-user-input-focus": [
        "what's the reason for this change? Can't imagine this breaking anyone to be fair",
        "Pretty sure yes because things not read in a teardown can still cause bugs as seen in https://github.com/sveltejs/svelte/issues/16072 - we might need to make props signals after all, gonna investigate that soon",
        "```suggestion\r\n\t// For objects, we apply string coercion (which might make things like $state array references in the template reactive) before diffing\r\n```"
      ],
      "svelte-async-cleanup-safety": [
        "It needs to be there and it will break if you run the tests with the \"no async\" environment flag. It was added in https://github.com/sveltejs/svelte/pull/16198 and the logic basically says \"in async mode we want to depend on state read inside and effect in which that state was also created, but not in non async mode because it's a breaking change\""
      ],
      "svelte-prefer-testing-libraries": [
        "I think it's important to have a low level example to not immediately abstract away stuff. Then they have an easier time reasoning about the abstraction",
        "The reading flow is better if first the underlying primitive is shown and then the abstraction on top - I added a code example showing how to rewrite it using the testing library, to make the benefits more obvious.",
        "- you can programmatically instantiate slots now using `createRawSnippet`\r\n- what is the problem with events/use/bind?\r\n- \"DX of programmatically intantiating components\" - what is so bad about it? I mean, yes it's not as nice as declaratively doing that, but doing `mount`/`unmount` isn't so bad? And when it comes to selecting elements in order to interact with them, it's always going to be a bit of legwork (in e2e testing, too)",
        "The way to go about all these cases is to create a wrapper component that tests the actual component - I don't see a problem with that. It's how you would have to do this in _any_ framework.\r\nFurthermore, just not documenting how to do component testing because it's cumbersome sounds wrong. Testing components in isolation is something you'd want to do especially when creating a component library, for example. I'm open to having a `> Don't overdo this as component tests are generally harder to maintain` note at the end of the section.\r\nI'll adjust the docs to mention both these things.",
        "It's explained in their [setup docs](https://testing-library.com/docs/svelte-testing-library/setup/#vitest) - they add a teardown behind the scenes. Didn't want to get into details here which would just mean repeating their docs, rather visualize how the resulting code is more robust to changes and more through the lens of a user."
      ],
      "svelte-complete-code-examples": [
        "yes, but prettier probably auto-formatted that"
      ],
      "svelte-realistic-documentation-examples": [
        "Is that code example necessary? It's taking up quite a bit of space, I think the \"hey watch out for that\" is enough, and then people can read up on how the `DataTransfer` API works (like I should have done, to not provoke this PR 😄 )",
        "Good clarification, but I think we also should have a code example showing how to do it if that's not what you want.\r\n\r\n````suggestion\r\nSince the `tooltip(content)` expression runs inside an [effect]($effect), the attachment will be destroyed and recreated whenever `content` changes. The same thing would happen for any state read _inside_ the attachment function when it first runs.\r\n\r\nIn case this is not the desired behavior, and you instead want the attachment to only be executed once on mount, then make sure to not read state eagerly inside the function body. Assuming `tippy` would be very expensive to setup and tear down (which it isn't), then you could rewrite the above example by passing a reference to the `content` variable and invoke it inside a nested effect:\r\n\r\n```svelte\r\n<!--- file: App.svelte --->\r\n<script>\r\n\timport tippy from 'tippy.js';\r\n\tlet content = $state('Hello!');\r\n\t/**\r\n\t * @param {() => string} content\r\n\t * @returns {import('svelte/attachments').Attachment}\r\n\t */\r\n\tfunction tooltip(content) {\r\n\t\treturn (element) => {\r\n\t\t\tconst tooltip = tippy(element);\r\n\t\t\t$effect(() => {\r\n\t\t\t\ttippy.setContent(content());\r\n\t\t\t});\r\n\t\t\treturn tooltip.destroy;\r\n\t\t};\r\n\t}\r\n</script>\r\n<input bind:value={content} />\r\n<button {@attach tooltip(() => content)}>\r\n\tHover me\r\n</button>\r\n```\r\n````",
        "Fine with me, I just want to have it mentioned somewhere",
        "Maybe something additional like \"note that if you mutate the value, you're mutating the original value, unless you do something like `$state.snapshot` to clone it\" here?\r\n\r\nAlso, can you copy that section over to `documentation/docs/03-runes/01-state.md`? That's where the docs for the final site live.",
        "The idea was that there's a compiler error version of this and so there's a bit of context why there's also a warning version of this (thought that's likely not very clear still, and so we can either just omit it [your suggestion] or expand on it by actually referencing the other variant)",
        "yeah that sounds good. My suggestion then (mirroring my other error text suggestion):\r\n```suggestion\r\nHTML restricts where certain elements can appear. In case of a violation the browser will 'repair' the HTML in a way that breaks Svelte's assumptions about the structure of your components. Some examples:\r\n\r\n- `<p>hello <div>world</div></p>` will result in `<p>hello </p><div>world</div><p></p>` for example (the `<div>` autoclosed the `<p>` because `<p>` cannot contain block-level elements)\r\n- `<option><div>option a</div></option>` will result in `<option>option a</option>` (the `<div>` is removed)\r\n- `<table><tr><td>cell</td></tr></table>` will result in `<table><tbody><tr><td>cell</td></tr></tbody></table>` (a `<tbody>` is auto-inserted)\r\n\r\nThis code will work when the component is rendered on the client (which is why this is a warning rather than an error), but if you use server rendering it will cause hydration to fail.\r\n```"
      ],
      "svelte-documentation-clarity-standards": [
        "I find this sentence much harder to read/parse than the previous one. Can we find some kind of middle ground between what was there before and what's proposed?",
        "```suggestion\r\n- `<svelte:component>` is now unnecessary in runes mode and therefore is deprecated (**5.0.0-next.203/217**, [#12646](https://github.com/sveltejs/svelte/pull/12646) and [#12694](https://github.com/sveltejs/svelte/pull/12694))\r\n```",
        "```suggestion\r\nWhen using custom elements, you should still use `<slot />` like before. In a future version, when Svelte removes its internal version of slots, it will leave those slots as-is, i.e. output a regular DOM tag instead of transforming it.\r\n```"
      ],
      "svelte-analyze-transitive-dependencies": [
        "```suggestion\r\nfix: don't consider children of rules when checking whether they are used or not\r\n```"
      ],
      "svelte-measure-performance-impact": [
        "Came here to say the same https://github.com/sveltejs/svelte/pull/15073#issuecomment-2604895320",
        "We can also do this, would handle the case described in https://github.com/sveltejs/svelte/pull/14116/files#r1876874053\r\n```suggestion\r\n\t\tconst previous = document.title;\r\n\t\tconst own = {};\r\n\t\t// @ts-expect-error\r\n\t\tdocument._last_title_setter = own;\r\n\t\tdocument.title = text;\r\n\r\n\t\treturn () => {\r\n\t\t\t// @ts-expect-error\r\n\t\t\tif (document._last_title_setter === own) {\r\n\t\t\t\tdocument.title = previous;\r\n\t\t\t}\r\n```",
        "Looking at how the code is generated, we should switch it up anyway - right now if your title would contain dynamic content, you would set it two times on an update - once to revert to the previous value, then right away again for the updated value. Wasteful."
      ],
      "svelte-runtime-html-escaping": [
        "```suggestion\r\nimport { escape_html } from '../../../escaping.js';\r\n```"
      ]
    },
    "profile": {
      "location": "Germany",
      "company": "Vercel",
      "blog": "",
      "twitter_username": "dummdidumm_",
      "site_admin": false,
      "followers": 671,
      "following": 0
    }
  },
  "chrisradek": {
    "repos": [
      "aws/aws-sdk-js"
    ],
    "entries": [
      {
        "slug": "aws-sdk-js-complete-configuration-type-definitions",
        "title": "Complete configuration type definitions"
      },
      {
        "slug": "aws-sdk-js-content-integrity-verification",
        "title": "Content integrity verification"
      },
      {
        "slug": "aws-sdk-js-defensive-null-checking",
        "title": "Defensive null checking"
      },
      {
        "slug": "aws-sdk-js-document-apis-completely",
        "title": "Document APIs completely"
      },
      {
        "slug": "aws-sdk-js-document-apis-thoroughly",
        "title": "Document APIs thoroughly"
      },
      {
        "slug": "aws-sdk-js-early-return-after-errors",
        "title": "Early return after errors"
      },
      {
        "slug": "aws-sdk-js-follow-established-testing-patterns",
        "title": "Follow established testing patterns"
      },
      {
        "slug": "aws-sdk-js-limit-cache-size",
        "title": "Limit cache size"
      },
      {
        "slug": "aws-sdk-js-organize-type-declarations",
        "title": "Organize type declarations"
      },
      {
        "slug": "aws-sdk-js-semantic-naming-conventions",
        "title": "Semantic naming conventions"
      },
      {
        "slug": "aws-sdk-js-semantic-type-organization",
        "title": "Semantic type organization"
      },
      {
        "slug": "aws-sdk-js-standardize-api-promise-patterns",
        "title": "Standardize API promise patterns"
      },
      {
        "slug": "aws-sdk-js-structured-test-resource-management",
        "title": "Structured test resource management"
      },
      {
        "slug": "aws-sdk-js-test-configuration-precedence",
        "title": "Test configuration precedence"
      },
      {
        "slug": "aws-sdk-js-validate-configurations-with-clarity",
        "title": "Validate configurations with clarity"
      }
    ],
    "comments": {
      "aws-sdk-js-validate-configurations-with-clarity": [
        "Is it possible to have an environment variable that is not a string in node.js?\r\nI haven't tried in Windows, but on MacOS, that doesn't seem possible.\r\n```bash\r\nFOO=bar node -e \"console.log(process.env)\"\r\n# FOO: 'bar'\r\n```\r\n```bash\r\nFOO= node -e \"console.log(process.env)\"\r\n# FOO: ''\r\n```\r\n\r\n```bash\r\nnode -e \"process.env['FOO'] = void 0; console.log(process.env)\"\r\n# FOO: 'undefined'\r\n```\r\n\r\nEven trying to set it to undefined within the process still results in the variable being a string. If you want to throw an error when this variable is set to nothing, you might want to instead check if it is an empty string.",
        "Sorry for the incoming tangent, but I have 2 thoughts here.\r\n1. I think we should update `isBrowser` to no longer depend on `process` existing:\r\nhttps://github.com/aws/aws-sdk-js/blob/29fc8d39b73a9c7c15b6bc8fb28cf5ef313691f4/lib/util.js#L39\r\nWe've heard reports of this breaking with Angular, and it doesn't really make sense that we're depending on a global that doesn't exist natively in browsers. Instead, I'd propose we update our (browser|react-native|node)_loader.js files to set the function to return true or false accordingly.\r\n\r\n2. This is less important if we do the item above, but I wonder if it would make sense to separate this function. You could have 3 new functions, each in their own file that perform one of these checks. \r\nThen you could have a separate browser/node file that exports the same function. This function would import whatever checks they need. This has the benefit of your browser build not having to import the code used to check the shared Ini file, and you could then allow the shared ini check to import the shared-ini loader directly. However, it also means more source files and might be more difficult to track."
      ],
      "aws-sdk-js-early-return-after-errors": [
        "It doesn't seem like there's a need for this function. Couldn't you store the return value directly in an object, then pass that to the callback/return it?",
        "I don't think giving the user a truncated stream is really ok. I think this test was checking the behavior the SDK exhibited, rather than what was intended.\r\n\r\n[Here](https://github.com/aws/aws-sdk-js/blob/master/lib/request.js#L606-L611), there's a check to see if the incoming data matched the content-length. It should be throwing an error when the content-length is less than the data received. When I was testing though, I discovered that when the body is larger than the content-length, node throws a `ParseError`. However, because we were swallowing these errors (due to already receiving response headers), and node.js still gave us access to the body up to the content-length, it appeared as though we could never detect when data streamed in exceeded the expected amount.",
        "Call `return` either on the same line as `callback` or right under. Right now `callback` might be triggered twice: once with an error and then once without an error a couple lines below."
      ],
      "aws-sdk-js-content-integrity-verification": [
        "I think tapping into the `data` event on a stream could cause some unintended side-effects. In node.js 0.10.x, binding a `data` listener on a stream will cause the stream to emit 'data' events as fast as it can, ignoring the back-pressure that's automatically in place when using `pipe`. If the writable stream is slow, then this could cause loss of data.\n\nI don't think this is an issue in versions of node.js >= 0.12.x, and some simple testing seems to confirm that. However, we need to work with node.js 0.10.x as well.\n\nThe current method creates a new `writable` stream that also gets piped into in order to emit the 'sendProgress' events. I know it'll require refactoring your logic but that path seems safer across all our supported versions of node.\n",
        "In node.js, the response body is likely to be a stream. It might be easier in node to create a passthrough stream that generates the md5 hash and compares against the checksum. You'll know the expected content-length since it's passed in the response headers. Your solution here should still cover the browser use-case though.",
        "If `responseStream` is an `IncomingMessage`, you'll want to call `destroy` on it. As of node 8.0.0, readable streams also have the `destroy` method, so you can check if `destroy` is a function, then call it if it is.",
        "We'll still need to perform these checks for Node.js 0.8, especially if we start turning it on by default. Unfortunately 0.8.x doesn't include `Transform` in its `stream` package, so your implementation won't work for those cases. You'll likely need to resort to `data` listeners to perform your calculations if `Transform` doesn't exist."
      ],
      "aws-sdk-js-semantic-naming-conventions": [
        "Fair point, thanks for finding that.\n",
        "Good catch!\n",
        "Sure, I don't have a strong preference either way.",
        "Can you spell out `optionalDiscoveryVersionEndpoint` (assuming that's what Disver means). `Disver` isn't a common abbreviation and coming back to this in the future it may be confusing what this means."
      ],
      "aws-sdk-js-defensive-null-checking": [
        "Just to be safe, can you also add a check that error exists, similar to the conditional before this one?\n",
        "Thanks, didn't catch this due to the mistake above.\n",
        "Can you keep the `AWS.util.isBrowser() && window.localStorage !== null` checks at the top of the try block? As it is, this will always throw an error in node.js (and possibly some 'browser' environments like electron). I know the catch block should handle this case, but I'd like to avoid throwing an error if we can predict it.",
        "It might be useful to add one more check after the if block to set params equal to itself or an empty object to protect us in the unlikely event someone passes in a value like `null`. Imagine this would be a rare edge case but we do this already for other operations.\n",
        "Just in case someone wants to enter `0`, maybe use a `typeof x === 'number'` check instead."
      ],
      "aws-sdk-js-limit-cache-size": [
        "We had 2 issues reported because this cache key wasn't unique enough:\r\nhttps://github.com/aws/aws-sdk-js/pull/1054\r\n\r\nYou may need to pass the serviceClientId as well.\r\n",
        "I think operation is only required if custom identifiers are defined for an operation. Is that not the case? Might help to keep the size of your cache down if we omitted operation if it isn't needed."
      ],
      "aws-sdk-js-test-configuration-precedence": [
        "There is a separate test that implicitly tests this, but happy to add an explicit test.",
        "Can we add a test to make sure the credentials from `~/.aws/credentials` is used preferentially over the credentials in `~/.aws/config` if the same profile exists in both files?",
        "I'm not sure this test is actually ensuring that the creds from `profile foo` are used instead of `default`.\r\n\r\nWhat do you think about spying on `AWS.STS` or `AWS.Credentials` to get the accessKeyId that was used as the source?"
      ],
      "aws-sdk-js-complete-configuration-type-definitions": [
        "Can you add `CredentialsOptions` to the `update` method in `ConfigBase` as well? This should be allowed in service configuration in addition to the global config."
      ],
      "aws-sdk-js-document-apis-completely": [
        "Grammar: Maybe change to something like\r\n> Whether to enable endpoint discovery for operations that allow optionally using an endpoint returned by the service.\r\n\r\nI couldn't find an example of what other teams were using for their docs.",
        "Is `parseFile` exposed to consumers of the SDK? If so, it should probably have some documentation, otherwise we don't need typings for it."
      ],
      "aws-sdk-js-structured-test-resource-management": [
        "Are these tests running in node.js? This should only be running in browser environments, and is using the 3rd party `Buffer` package instead of node.js' `Buffer` package. We can place browser-specific tests in a separate folder and exclude them from being run by mocha in the npm unit script.",
        "Probably want to append a timestamp to this as well to make it somewhat unique",
        "Why don't you create the bucket used by all the tests in the `before` step? We shouldn't have to create a new bucket for every single test, just this suite of tests. \r\n\r\nI also wouldn't mix the `putObject` method with `createBucket`. `putObject` is doing too much, and adds 'global' (across tests) state. For example, you don't directly pass it the bucket or key, instead relying on a closure that every test has access to (and can change). That could lead to tricky edge cases coming up later that are hard to debug.",
        "But you could also make sure the bucket is there in the `before` hook. Just call `done()` after the `waitFor` method completes. Then you also only need to create it once; I don't think there's a reason we need to create a new bucket for every test since we aren't testing any bucket-specific configurations here. Creating a new bucket with each test also creates more points of failure (rate/resource limits, for example).\r\n",
        "If you `createBucket` in the `before` hook, you can `deleteBucket` in the `after` hook!"
      ],
      "aws-sdk-js-standardize-api-promise-patterns": [
        "It looks like `constructor.name` may not work in all the browsers we support and could have issues when minifiers are used:\nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function/name#Browser_compatibility\n\nCan you do an equality check against the constructor instead?\n",
        "Actually, what do you think about giving each class that should return a promise a static method that accepts a Promise constructor, then the class can control how it should promisify itself?\n\nThe pros to that approach would be the logic for adding promises would be controlled by each class, rather than defined in a long if/switch statement within a utility function. If the method to promisify a class was named the same for all classes, you can just check if the constructor has that method then call it, instead of maintaining a list of enums to check a class against. You could still make the `promisifyMethod` a utility method if that reduces code duplication.\n",
        "The `AWS.util.promisifyMethod` function currently only works for functions that accept a callback function as the first parameter. For example, `AWS.Request.send` and `AWS.Credentials.get` both accept just a callback.\r\n\r\nThe `s3.getSignedUrl` method accepts an operation name, params, and a callback, so using this method won't work. You should be able to set `getSignedUrlPromise` to a function that returns a new Promise. This promise can simply wrap the getSignedUrl function."
      ],
      "aws-sdk-js-semantic-type-organization": [
        "Can you add tests to make sure we can explicitly set a variable to the `DynamoDB.Converter` type? Also, would be nice to verify we can access `DocumentClient.ConverterOptions`.\r\n\r\nCan be simple, like:\r\n```javascript\r\nconst converter: Converter = DynamoDB.Converter;\r\n// and a test for input with converter options\r\nconst converterOptions: DynamoDB.DocumentClient.ConverterOptions = {convertEmptyValues: true};\r\nDynamoDB.Converter.input('string', converterOptions);\r\n```\r\n\r\nHow hard would it be to also expose ConverterOptions on the Converter namespace? Just feels a little odd having to access it off the DocumentClient namespace instead."
      ],
      "aws-sdk-js-document-apis-thoroughly": [
        "Would you mind adding some docs around this new config parameter?\nSomething like this at line 98:\n\n```\n * @!attribute signatureCache\n *   @return [Boolean] whether the signature to sign requests with (overriding\n *     the API configuration) is cached. Only applies to the signature version 'v4'.\n *     Defaults to `true`.\n```\n\nand something like this at line 185:\n\n```\n   * @option options signatureCache [Boolean] whether the signature to sign\n   *   requests with (overriding the API configuration) is cached. Only applies\n   *   to the signature version 'v4'. Defaults to `true`.\n```\n",
        "Minor: Might be worth adding an example that uses tags.",
        "Can you add a comment that explains what this function is supposed to do? It looks like you're populating an object with identifiers and customer-provided values, but it took me a while to grok that and the function name isn't clear. ",
        "I think this comment is a bit misleading, as is the one for `getCacheKey`. Both imply that you're going to get a single `key` (presumably a string), but you're returning a map. I think something like the following is more clear: \r\n```javascript\r\n/**\r\n * Get custom identifiers for cache key.\r\n * Identifies custom identifiers by checking each shape's `endpointDiscoveryId` trait.\r\n */\r\n```\r\nThis would at least help me, because I kept expecting `cacheKey` later on to be a string you pass to `endpointCache.get`, but it turns out you pass in a map of elements.",
        "If you want this to appear in documentation, you also need to attach IniLoader to the AWS namespace:\r\n```javascript\r\nAWS.IniLoader = AWS.util.inherit/* ... */\r\n\r\n// optionally also export it:\r\nmodule.exports = AWS.IniLoader;\r\n```\r\n\r\nYou'll also want to add doc strings to the public methods.",
        "Can you amend this to state a URL will be returned?\r\n\r\nSomething like:\r\n\r\n> Returns a 'thenable' promise that will be resolved with a pre-signed URL."
      ],
      "aws-sdk-js-organize-type-declarations": [
        "I wanted to export all the interfaces so that users could cast a result if they needed to, but I'm not sure if that's necessary. I'll have to take a look at some other TypeScript libraries to see if that's common practice.\n\nYou should only see these interfaces when looking at the service client constructors, but not on the service client instances.\n",
        "So, I tried a few different things. Ultimately, I went with putting the exported types in a sub-namespace:\n`declare namespace SERVICE.Types {`\nWhen I put them on their own namespace, I had to explicitly import them into my app, otherwise the typescript compiler would complain:\nhttps://github.com/Microsoft/TypeScript/issues/9944\n\nI also wanted them to be exported so user's can specify a type for cases when the typescript compiler can't quite infer what a type should be. That might not be necessary once https://github.com/Microsoft/TypeScript/issues/6606 is addressed.\n"
      ],
      "aws-sdk-js-follow-established-testing-patterns": [
        "For all these tests where you're making sure we remove empty inputs, can you also add tests to verify that we don't translate empty strings/sets/buffers if `convertEmptyValues` isn't set?",
        "We have a standard way of testing list/describe operations in most of our feature tests that look like this:\r\nhttps://github.com/aws/aws-sdk-js/blob/v2.135.0/features/acm/acm.feature#L7-L10\r\n\r\nIf you follow this patten, you don't have to create your own step definitions, since cucumber will use the ones defined here:\r\nhttps://github.com/aws/aws-sdk-js/blob/v2.135.0/features/extra/hooks.js#L56"
      ]
    },
    "profile": {
      "location": "Seattle, WA",
      "blog": "",
      "site_admin": false,
      "followers": 81,
      "following": 2
    }
  },
  "Viicos": {
    "repos": [
      "pydantic/pydantic"
    ],
    "entries": [
      {
        "slug": "pydantic-avoid-shared-structure-mutation",
        "title": "Avoid shared structure mutation"
      },
      {
        "slug": "pydantic-avoid-unnecessary-operations",
        "title": "Avoid unnecessary operations"
      },
      {
        "slug": "pydantic-balance-documentation-thoroughness",
        "title": "Balance documentation thoroughness"
      },
      {
        "slug": "pydantic-cache-expensive-computations",
        "title": "Cache expensive computations"
      },
      {
        "slug": "pydantic-categorize-error-types",
        "title": "Categorize error types"
      },
      {
        "slug": "pydantic-consistent-configuration-patterns",
        "title": "Consistent configuration patterns"
      },
      {
        "slug": "pydantic-data-structure-correctness",
        "title": "Data structure correctness"
      },
      {
        "slug": "pydantic-document-configuration-relationships",
        "title": "Document configuration relationships"
      },
      {
        "slug": "pydantic-documentation-formatting-standards",
        "title": "Documentation formatting standards"
      },
      {
        "slug": "pydantic-eliminate-redundant-computation",
        "title": "Eliminate redundant computation"
      },
      {
        "slug": "pydantic-enforce-style-with-linters",
        "title": "Enforce style with linters"
      },
      {
        "slug": "pydantic-explicit-over-implicit",
        "title": "Explicit over implicit"
      },
      {
        "slug": "pydantic-maintain-code-consistency",
        "title": "Maintain code consistency"
      },
      {
        "slug": "pydantic-preserve-language-conventions",
        "title": "Preserve language conventions"
      },
      {
        "slug": "pydantic-robust-error-messaging",
        "title": "Robust error messaging"
      },
      {
        "slug": "pydantic-safe-attribute-access-pattern",
        "title": "Safe attribute access pattern"
      },
      {
        "slug": "pydantic-semantic-over-syntactic",
        "title": "Semantic over syntactic"
      },
      {
        "slug": "pydantic-simple-defaults-flexible-overrides",
        "title": "Simple defaults, flexible overrides"
      },
      {
        "slug": "pydantic-specific-types-for-performance",
        "title": "Specific types for performance"
      },
      {
        "slug": "pydantic-standardize-dependency-management",
        "title": "Standardize dependency management"
      },
      {
        "slug": "pydantic-structured-configuration-management",
        "title": "Structured configuration management"
      },
      {
        "slug": "pydantic-write-targeted-specific-tests",
        "title": "Write targeted, specific tests"
      }
    ],
    "comments": {
      "pydantic-avoid-shared-structure-mutation": [
        "This is related to what I mentioned:\r\n\r\n> the first one I could find: https://github.com/pydantic/pydantic/issues/7102. The https://github.com/pydantic/pydantic/issues/7102#issuecomment-1682288722 isn't really compelling, the user was doing things not the intended way. It also led to [a scary and hacky fix](https://github.com/pydantic/pydantic/pull/8066/files) that I would really like not having.\r\n\r\nThese removed lines had the effect of moving the ref from the inner schema of a `function-*` schema to the the `function-*` schema itself. With the following simplified test code added in the mentioned issue above:\r\n\r\n```python\r\nclass Numeric(BaseModel):\r\n    value: float\r\n\r\n    @classmethod\r\n    def __get_pydantic_core_schema__(cls, source_type, handler):\r\n        return core_schema.no_info_before_validator_function(cls.validate, handler(source_type))\r\n\r\n    @classmethod\r\n    def validate(cls, v):\r\n        ...\r\n\r\nclass OuterModel(BaseModel):\r\n    x: Numeric\r\n    y: Numeric\r\n```\r\n\r\nOn `main`, the schema of `OuterModel` would look like:\r\n\r\n<details>\r\n\r\n```python\r\n{\r\n│   'type': 'definitions',\r\n│   'schema': {\r\n│   │   'type': 'model',\r\n│   │   'cls': <class '__main__.OuterModel'>,\r\n│   │   'schema': {\r\n│   │   │   'type': 'model-fields',\r\n│   │   │   'fields': {\r\n│   │   │   │   'x': {'type': 'model-field', 'schema': {'type': 'definition-ref', 'schema_ref': '__main__.Numeric:109238506193520'}, 'metadata': {}},\r\n│   │   │   │   'y': {'type': 'model-field', 'schema': {'type': 'definition-ref', 'schema_ref': '__main__.Numeric:109238506193520'}, 'metadata': {}}\r\n│   │   │   },\r\n│   │   │   'model_name': 'OuterModel',\r\n│   │   │   'computed_fields': []\r\n│   │   },\r\n│   │   'config': {'title': 'OuterModel'},\r\n│   │   'ref': '__main__.OuterModel:109238503123056',\r\n│   │   'metadata': {'<stripped>'}\r\n│   },\r\n│   'definitions': [\r\n│   │   {\r\n│   │   │   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\n│   │   │   'schema': {\r\n│   │   │   │   'type': 'model',\r\n│   │   │   │   'cls': <class '__main__.Numeric'>,\r\n│   │   │   │   'schema': {\r\n│   │   │   │   │   'type': 'model-fields',\r\n│   │   │   │   │   'fields': {'value': {'type': 'model-field', 'schema': {'type': 'float'}, 'metadata': {}}},\r\n│   │   │   │   │   'model_name': 'Numeric',\r\n│   │   │   │   │   'computed_fields': []\r\n│   │   │   │   },\r\n│   │   │   │   'config': {'title': 'Numeric'}\r\n│   │   │   },\r\n│   │   │   'ref': '__main__.Numeric:109238506193520',\r\n│   │   │   'metadata': {'<stripped>'},\r\n│   │   │   'type': 'function-before'\r\n│   │   }\r\n│   ]\r\n}\r\n```\r\n\r\n</details>\r\n\r\nOn this PR, it looks like:\r\n\r\n<details>\r\n\r\n```python\r\n{\r\n│   'type': 'definitions',\r\n│   'schema': {\r\n│   │   'type': 'model',\r\n│   │   'cls': <class '__main__.OuterModel'>,\r\n│   │   'schema': {\r\n│   │   │   'type': 'model-fields',\r\n│   │   │   'fields': {\r\n│   │   │   │   'x': {\r\n│   │   │   │   │   'type': 'model-field',\r\n│   │   │   │   │   'schema': {\r\n│   │   │   │   │   │   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\n│   │   │   │   │   │   'schema': {\r\n│   │   │   │   │   │   │   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\n│   │   │   │   │   │   │   'schema': {'type': 'definition-ref', 'schema_ref': '__main__.Numeric:105945921898336'},\r\n│   │   │   │   │   │   │   'metadata': {'<stripped>'},\r\n│   │   │   │   │   │   │   'type': 'function-before'\r\n│   │   │   │   │   │   },\r\n│   │   │   │   │   │   'metadata': {'<stripped>'},\r\n│   │   │   │   │   │   'type': 'function-before'\r\n│   │   │   │   │   },\r\n│   │   │   │   │   'metadata': {}\r\n│   │   │   │   },\r\n│   │   │   │   'y': {\r\n│   │   │   │   │   'type': 'model-field',\r\n│   │   │   │   │   'schema': {\r\n│   │   │   │   │   │   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\n│   │   │   │   │   │   'schema': {\r\n│   │   │   │   │   │   │   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\n│   │   │   │   │   │   │   'schema': {'type': 'definition-ref', 'schema_ref': '__main__.Numeric:105945921898336'},\r\n│   │   │   │   │   │   │   'metadata': {'<stripped>'},\r\n│   │   │   │   │   │   │   'type': 'function-before'\r\n│   │   │   │   │   │   },\r\n│   │   │   │   │   │   'metadata': {'<stripped>'},\r\n│   │   │   │   │   │   'type': 'function-before'\r\n│   │   │   │   │   },\r\n│   │   │   │   │   'metadata': {}\r\n│   │   │   │   }\r\n│   │   │   },\r\n│   │   │   'model_name': 'OuterModel',\r\n│   │   │   'computed_fields': []\r\n│   │   },\r\n│   │   'config': {'title': 'OuterModel'},\r\n│   │   'ref': '__main__.OuterModel:105945922827312',\r\n│   │   'metadata': {'<stripped>'}\r\n│   },\r\n│   'definitions': [\r\n│   │   {\r\n│   │   │   'type': 'model',\r\n│   │   │   'cls': <class '__main__.Numeric'>,\r\n│   │   │   'schema': {\r\n│   │   │   │   'type': 'model-fields',\r\n│   │   │   │   'fields': {'value': {'type': 'model-field', 'schema': {'type': 'float'}, 'metadata': {}}},\r\n│   │   │   │   'model_name': 'Numeric',\r\n│   │   │   │   'computed_fields': []\r\n│   │   │   },\r\n│   │   │   'config': {'title': 'Numeric'},\r\n│   │   │   'ref': '__main__.Numeric:105945921898336'\r\n│   │   }\r\n│   ]\r\n}\r\n```\r\n\r\n</details>\r\n\r\nEssentially, the difference in these two schemas is that we don't \"move\" the ref from the inner schema to the `function-before` schemas.\r\n\r\nThe changes in this PR + removing this reference moving coincidentally make it work still.\r\n\r\nHowever, doing so was a dangerous game: on _L793_, `schema` directly comes from another model. The `pop` calls removes the reference to the schema, and mutating schemas from other models has been a known issue. \r\n\r\nYou may be wondering: why this doesn't break things in the example I gave? Surely the `pop` call should have mutated the core schema of `Numeric`. Turns out it doesn't, because `Numeric.__get_pydantic_core_schema__` does not cache the schema, so calling it will generate a new one every time (and this is what happens during the schema gen of `OuterModel`). But on a similar issue, [I mentioned](https://github.com/pydantic/pydantic/issues/10160#issuecomment-2298257506) that explicitly caching the core schema in the `__get_pydantic_core_schema__` method would resolve the user issue (as the use case was slightly different)! \r\n\r\nSo to conclude, overriding `BaseModel.__get_pydantic_core_schema__` is full of unexpected behaviors, but that's fine as officially supporting them would be a huge pain.\r\n\r\n",
        "iirc (but I'm not sure), I was able to remove it only thanks to the other changes. This won't clutter the git diff though, because it's just a removal. Probably by having a proper commit description when merging, I can add a note about this?",
        "This was present inside `_generate_schema_from_property` before, but actually I think it should come first. Whenever you call `generate_schema`, if we pass in `typing(_extensions).Self`, we need to resolve the type before trying to build the schema.\r\n\r\nI moved it at the top of `GenerateSchema.generate_schema`",
        "Oops, seems like moving it breaks things, it needs to be right after the `__get_pydantic_core_schema__` check, so I'll leave it here"
      ],
      "pydantic-standardize-dependency-management": [
        "```suggestion\r\n      - name: Install UV\r\n        uses: astral-sh/setup-uv@v5\r\n        with:\r\n            python-version: ${{ matrix.python-version }}\r\n```\r\n\r\nThe reason I had to use the setup-python action in the previous third-party test is because of the comment I added regarding the uv action. In normal circumstances (i.e. when the project isn't nested under a specific repository folder) you can just let uv setup the Python version.",
        "```suggestion\r\n    - uses: actions/setup-python@v5\r\n      with:\r\n        python-version: ${{ matrix.python-version }}\r\n```\r\n\r\nBest to be as close to the project's CI."
      ],
      "pydantic-consistent-configuration-patterns": [
        "The benefit of using the class arguments is that it is recognized by type checkers. I think this is only relevant for `frozen`, so actually I'll change the example and add an annotation about it"
      ],
      "pydantic-write-targeted-specific-tests": [
        "The test you added is unrelated to your change, which makes me believe your contribution is AI generated. If so, please state it explicitly in the PR description.\r\n\r\nYou can replace by the following test:\r\n\r\n```python\r\ndef test_private_attribute_not_skipped_during_ns_inspection() -> None:\r\n    # It is important for the enum name to start with the class name\r\n    # (it previously caused issues as we were comparing qualnames without\r\n    # taking this into account):\r\n    class Fullname(str, Enum):\r\n        pass\r\n\r\n    class Full(BaseModel):\r\n        _priv: object = Fullname\r\n\r\n    assert isinstance(Full._priv, ModelPrivateAttr)\r\n```",
        "> 1. Can we include the `SchemaError` information in this result?\r\n\r\nPytest will display the exception by default. This is the output you would get for a failing test:\r\n\r\n<details>\r\n\r\n```\r\ntests/test_json_schema.py F                                                                                                                       [100%]\r\ntests/test_json_schema.py:6476 test_fails - Failed: Failed to validate the JSON Schema against the Draft 2020-12 spec…                            [100%]\r\n======================================================================= FAILURES ========================================================================\r\n______________________________________________________________________ test_fails _______________________________________________________________________\r\n\r\nargs = (<pydantic.json_schema.GenerateJsonSchema object at 0x7b80723f5820>, {'metadata': {'pydantic_js_annotation_functions':...onSchema.__get_pydantic_json_schema__ of WithJsonSchema(json_schema={'type': 'invalid'}, mode=None)>]}, 'type': 'int'})\r\nkwargs = {'mode': 'validation'}, json_schema = {'type': 'invalid'}\r\n\r\n    def generate(*args: Any, **kwargs: Any) -> Any:\r\n        json_schema = orig_generate(*args, **kwargs)\r\n        if not request.node.get_closest_marker('skip_json_schema_validation'):\r\n            try:\r\n>               Draft202012Validator.check_schema(json_schema)\r\n\r\ntests/conftest.py:166: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\ncls = <class 'jsonschema.validators.Draft202012Validator'>, schema = {'type': 'invalid'}\r\nformat_checker = <FormatChecker checkers=['date', 'email', 'idn-email', 'idn-hostname', 'ipv4', 'ipv6', 'regex', 'uuid']>\r\n\r\n    @classmethod\r\n    def check_schema(cls, schema, format_checker=_UNSET):\r\n        Validator = validator_for(cls.META_SCHEMA, default=cls)\r\n        if format_checker is _UNSET:\r\n            format_checker = Validator.FORMAT_CHECKER\r\n        validator = Validator(\r\n            schema=cls.META_SCHEMA,\r\n            format_checker=format_checker,\r\n        )\r\n        for error in validator.iter_errors(schema):\r\n>           raise exceptions.SchemaError.create_from(error)\r\nE           jsonschema.exceptions.SchemaError: 'invalid' is not valid under any of the given schemas\r\nE           \r\nE           Failed validating 'anyOf' in metaschema['allOf'][3]['properties']['type']:\r\nE               {'anyOf': [{'$ref': '#/$defs/simpleTypes'},\r\nE                          {'type': 'array',\r\nE                           'items': {'$ref': '#/$defs/simpleTypes'},\r\nE                           'minItems': 1,\r\nE                           'uniqueItems': True}]}\r\nE           \r\nE           On schema['type']:\r\nE               'invalid'\r\n\r\n../../.pyenv/versions/3.12.4/envs/pydanticdev/lib/python3.12/site-packages/jsonschema/validators.py:317: SchemaError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n    def test_fails():\r\n>       TypeAdapter(Annotated[int, WithJsonSchema({'type': 'invalid'})]).json_schema()\r\n\r\ntests/test_json_schema.py:6478: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\npydantic/type_adapter.py:142: in wrapped\r\n    return func(self, *args, **kwargs)\r\npydantic/type_adapter.py:549: in json_schema\r\n    return schema_generator_instance.generate(self.core_schema, mode=mode)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nargs = (<pydantic.json_schema.GenerateJsonSchema object at 0x7b80723f5820>, {'metadata': {'pydantic_js_annotation_functions':...onSchema.__get_pydantic_json_schema__ of WithJsonSchema(json_schema={'type': 'invalid'}, mode=None)>]}, 'type': 'int'})\r\nkwargs = {'mode': 'validation'}, json_schema = {'type': 'invalid'}\r\n\r\n    def generate(*args: Any, **kwargs: Any) -> Any:\r\n        json_schema = orig_generate(*args, **kwargs)\r\n        if not request.node.get_closest_marker('skip_json_schema_validation'):\r\n            try:\r\n                Draft202012Validator.check_schema(json_schema)\r\n            except SchemaError:\r\n>               pytest.fail('Failed to validate the JSON Schema against the Draft 2020-12 spec')\r\nE               Failed: Failed to validate the JSON Schema against the Draft 2020-12 spec\r\n\r\ntests/conftest.py:168: Failed\r\n                                   Summary of Failures                                    \r\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━┓\r\n┃  File                       ┃  Function    ┃  Function Line  ┃  Error Line  ┃  Error   ┃\r\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━┩\r\n│  tests/test_json_schema.py  │  test_fails  │  6477           │  6478        │  Failed  │\r\n└─────────────────────────────┴──────────────┴─────────────────┴──────────────┴──────────┘\r\nResults (1.65s):\r\n         1 failed\r\n      5846 deselected\r\n\r\n```\r\n\r\n</details>\r\n\r\n> 2\\. Could we add a note about the fact that this is purely a testing feature, not a runtime `pydantic` check (at this point)?\r\n\r\nAdded.\r\n\r\n> Perhaps this is a bit excessive, but can we test this test?\r\n\r\nSeems like there are [ways to do so](https://stackoverflow.com/a/56635224), but they are pretty involved. I added a test with an expected failure.\r\n",
        "Yes can be removed after looking at the git blame."
      ],
      "pydantic-preserve-language-conventions": [
        "You mean `instanciate_by_*` only take effect on direct instantiation (i.e. `Model(...)`)?\r\n\r\nThis would really complicate the API. Using `__init__` directly is better suited when you provide the arguments directly (e.g. `Model(a=1, b='test')`). In that case, the user can simply provide the aliases (and this is what static type checkers will enforce, we have no control over it).\r\n\r\nIf you want to validate data where you don't control the provided keys, then `model_validate()` is better suited anyway: `Model.model_validate({'a': 1, 'b': 'test'})`, and you can provide `by_name=True` there.",
        "> This is intuitive and aligned with dataclass and other frameworks in statically typed languages.\r\n\r\nDataclasses don't make use of aliases, but this is something supported by the `@dataclass_transform` spec, and as per the [fields specifiers](https://typing.readthedocs.io/en/latest/spec/dataclasses.html#field-specifier-parameters) section:\r\n\r\n> `alias` is an optional str parameter that provides an alternative name for the field. This alternative name is used in the synthesized `__init__` method.\r\n\r\nBut I get your point, `Model(SomeRandomValue=147, Env_Global='test')` feels weird in Python code. The fact that type checkers will enforce aliases in `__init__` is unfortunate though.\r\n\r\nThis merits a broader discussion, currently we don't have a proper distinction between direct instantiation (`__init__`) and the `model_validate(_*)` methods when it comes to validation behavior. ",
        "```suggestion\r\nwhich is available on the [`model_dump()`][pydantic.main.BaseModel.model_dump] and\r\n[`model_dump_json()`][pydantic.main.BaseModel.model_dump_json] methods, as well as\r\nthe [`TypeAdapter`][pydantic.type_adapter.TypeAdapter] ones.\r\n```"
      ],
      "pydantic-balance-documentation-thoroughness": [
        "Probably for the serialization docs rewrite, but I don't think we should have this workaround documented. Having models serialized as something else than `dict` is uncommon, and the proposed solution isn't future proof (like in this case, where we make changes to the signature, this would break type checking for users using this workaround).",
        "I mention it in:\r\n\r\n> - `'allow'`: Providing extra data is allowed and stored in the `__pydantic_extra__` dictionary attribute.\r\n  The `__pydantic_extra__` can explicitly be annotated to provide validation for extra fields.\r\n\r\nI was trying to avoid duplication of documentation, and so the `__pydantic_extra__` example is present in the API docs. Maybe a broader discussion could be _what should live in the concepts/API docs_",
        "Can we simplify this section and the following one with a single _Similarly to Pydantic models, nested dataclasses and generics are supported_ (and we refer to the relevant model documentation)? Imo the added value is quite low here, we would expect the examples to work anyway",
        "They can just click on the `[init_typed](#init_typed)` reference, where I added a description for the setting (and same for others). Having things centralized avoids duplication, and having a heading makes it easier to link to when responding on github issues/discussions etc\r\n\r\nTo be clear, the explanation is not removed, just centralized in the configuration options section"
      ],
      "pydantic-specific-types-for-performance": [
        "This is added by me, as I think a common misconception is to use abstract containers as types thinking this will allow (in the case of `collections.abc.Sequence`) both lists and tuples to validate, while in fact this is already supported by Pydantic.",
        "I added the link. Regarding the second bullet point, not sure what you mean. Do you have a suggestion?"
      ],
      "pydantic-document-configuration-relationships": [
        "This makes me think the literal pattern would really fit better here.. If having this boolean pattern on two configuration values only introduced the inconsistency when setting both `validate_by_alias=False, validate_by_name=False`, it would be fine (I don't see why users would do so), but I won't be surprised if many users find it counter-intuitive that you also need to set `validate_by_name=True` here.\r\n\r\nI think it's worth reconsidering, cc @samuelcolvin ",
        "Also, what should happen if you set `validate_by_alias=False`, but explicitly set `by_alias=True` or `by_name=True` during validation?",
        "Yes, as discussed on Slack, thanks for summing things up here, this might be useful as a reference in case we get questions about the current API.\r\n\r\nAs we discussed as well, defaulting `validate_by_name` to `True` if `validate_by_alias` is set to `False` is postponed after this PR, and should be tackled either before 2.11 or after. Leaving this conversation unresolved so that it's easier to find it later.",
        "I think you'll have to update the `collect_config()` logic to handle both `validate_by_name` and `populate_by_name`. Mypy does static analysis so it can't be aware of the added logic in `ConfigWrapper.core_config()`.\r\n\r\nAlso, I'm not sure if the plugin already handled this, but previously if `populate_by_name` was set, the following calls were allowed:\r\n\r\n```python\r\nclass Model(BaseModel):\r\n    field: int = Field(alias='alias')\r\n\r\n    model_config = {'populate_by_name': True}\r\n\r\nModel(field=1)  # OK\r\nModel(alias=1)  # OK\r\n```\r\n\r\nIf the plugin accepted both these calls, it will probably need to be updated to disallow the following:\r\n\r\n```python\r\nclass Model(BaseModel):\r\n    field: int = Field(alias='alias')\r\n\r\n    model_config = {'validate_by_alias': True, 'validate_by_name': False}\r\n\r\nModel(field=1)  # Type checker error\r\nModel(alias=1)  # OK\r\n```"
      ],
      "pydantic-data-structure-correctness": [
        "```suggestion\r\nThis error is raised when an unhashable value is validated against a [`set`][] or a [`frozenset`][]```",
        "Actually this example is invalid from a type checking perspective, and it does makes sense in some way. I think the logic applied is \"can we end up with a stop when trying to match a value to the type?\".\r\n\r\nFor instance, with\r\n\r\n```python\r\ntype B = list[C]\r\ntype C = B | None\r\n```\r\n\r\nthe value `[None, [[None]], []]` successfully matches and we came to stop, thanks to the `| None` part. without it, there's no such value that can match the type alias without dealing with infinite recursion (hence my suggestion above).\r\n\r\nLet's go with:\r\n\r\n```suggestion\r\nFor example, this is a valid type alias:\r\n\r\n```py test=\"skip\" lint=\"skip\" upgrade=\"skip\"\r\ntype A = list[A] | None\r\n```\r\n```"
      ],
      "pydantic-explicit-over-implicit": [
        "Previously, this was a bit weird because even though we did not install any extra, `tzdata` (which is installed through the `timezone` extra) was still included in the `dev` dependency group. I've tried changing the pytest skip marker for these tests to check for the presence of the `tzdata` library but it's tricky as at the module level, some `ZoneInfo` instances are created so it still fails.\r\n\r\nIn the future, if we include new extras, we should change the name to `Test only with 'timezone' extra`",
        "So the thing is `uv` will prefer the specified version in the `.python-version` file if present, no matter the previously installed version. We currently don't have such a file, but it could be pretty bad if we end up creating one at some point, especially for the jobs with a Python version matrix: CI will only run on the version from `.python-version`, and we won't notice anything.\r\n\r\nIt's a bit unfortunate, probably using tox (and tox-uv) could help. "
      ],
      "pydantic-structured-configuration-management": [
        "The Ruff target version needs to be updated as well (`target-version = 'py39'`), I believe you'll then get new errors asking to update `typing.*` to `collections.abc.*` (e.g. for `Iterable`, etc).",
        "> Move dependency-groups section under the project one, as it is part of the [packaging specifications](https://packaging.python.org/en/latest/specifications/)."
      ],
      "pydantic-documentation-formatting-standards": [
        "Was there a reason to change these? These examples are not tested (marked as `test=\"skip\"`) so I think this isn't right now, the actual comment showing the output is most likely different.",
        "Ah then let's unify all of them using `print(<inst>)` instead of `print(repr(<inst>))`",
        "Hum I still see the `repr()` in the examples?",
        "The added newline broke the rendering. Was it added by the linter? It seems the extra level of nesting below did not have the same newline added.\r\n\r\n| Before                                                                                    | After                                                                                     |\r\n|-------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|\r\n| ![image](https://github.com/user-attachments/assets/643d5681-a822-48c8-85a9-445f25f16156) | ![image](https://github.com/user-attachments/assets/21435ae0-5eb9-47c6-9c4f-419e82811729) |",
        "These new lines are required, as otherwise the code block isn't assumed to be part of the list element.\r\n\r\n| Before                                                                                    | After                                                                                     |\r\n|-------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|\r\n| ![image](https://github.com/user-attachments/assets/0a282db3-571a-4f0c-81a7-9b8cb74beb75) | ![image](https://github.com/user-attachments/assets/0011b415-a802-4311-869a-a5c00d9df459) |\r\n\r\n(it also breaks admonitions/annotations defined after the code block)\r\n\r\nSeems like indenting the code block with two spaces (and keeping the newline, which should make the linter happy) works, could we apply this pattern?\r\n"
      ],
      "pydantic-semantic-over-syntactic": [
        "I wanted to avoid having the name depending on the capitalization of the object, and simply have `is_<name>`, but both make sense I think",
        "```suggestion\r\n    def defined_constraints(self) -> dict[str, Any]:\r\n```\r\n\r\nmaybe? set_constraints can be confusing at first, feels like this is representing an action of _setting_ something"
      ],
      "pydantic-robust-error-messaging": [
        "Yes I can probably move the check below, but I prefer enforcing the data to be provided as external users of this method don't know whether the default factory requires the argument or not, and I think it's best to unconditionally raise here (and enforcing the argument in the overload).",
        "You will have to add a `try..finally` block in case any exception happens, as otherwise the original function will have its attributes mutated.\r\n\r\nAlso, I'll have to think about it more, but are we certain that every callable that can be used with `validate_call` can have the `__qualname__`/`__annotations__` etc arguments mutated?",
        "```suggestion\r\n                       \"The default factory requires the 'validated_data' argument, which was not provided when calling 'get_default'.\"\r\n```\r\n\r\nI don't think mentioning \"this is a bug\" is correct, as users could really just call `get_default` in the wrong way.",
        "The `TypeAdapter[...]` form seems a bit weird, especially because `type_repr` seems to be `str(type)` when called from `TypeAdapter`. Probably fine as is, just wanted to note that this may lead to weird string representations (maybe `_display.display_as_type` could be used, although it is costly to compute)."
      ],
      "pydantic-simple-defaults-flexible-overrides": [
        "This is for validated function calls (see `test_unsupported_field_attribute_nested_with_function()`). We don't want to raise a warning for:\n\n```python\n@validate_call\ndef func(a: Annotated[int, Field(alias='b')]): ...\n```",
        "```suggestion\r\n        allowed_schemes=['clickhouse+native', 'clickhouse+asynch', 'clickhouse+http', 'clickhouse', 'clickhouses', 'clickhousedb'],\r\n```"
      ],
      "pydantic-cache-expensive-computations": [
        "I also need to check that `cls.__pydantic_generic_metadata__['origin']` is `None` for Pydantic models, so maybe it's best to keep the (small) duplication of code here."
      ],
      "pydantic-safe-attribute-access-pattern": [
        "This check did not really make sense in this `get_function_type_hints()`, but rather in `get_callable_return_type()` (this PR moved the check in this one), which has a smaller scope (i.e. analyzing callables used for validators/serializers functions)",
        "```suggestion\r\n    if getattr(email_validator, '__version__', '').partition('.')[0] == '2':\r\n```\r\nWith `''` as a default value, `''.partition('.')` returns `('', '', '')`",
        "Yes, what I meant is that the `hasattr` check is not necessary because of the `getattr` fallback to `''`. But I think it's fine to keep it for clarity"
      ],
      "pydantic-maintain-code-consistency": [
        "Both are equivalent, but indeed passing all the metadata directly to `_construct()` is cleaner. Applied.",
        "```suggestion\r\n    def __init_subclass__(cls) -> None:\r\n```\r\nWe can safely omit the kwargs here",
        "I inlined the logic as I couldn't find a good way to keep it in a single method as I need to raise it differently in `__delattr__`. A bit unfortunate, but at least this removes the `_check_frozen` method on the `BaseModel` class, so it avoids polluting the namespace.",
        "Why was it moved?"
      ],
      "pydantic-enforce-style-with-linters": [
        "Actually let's include `PIE790`, I think it is worth being included. I've checked `PIE804`, and I'm not sure why we get so many violations in the tests files, so fine to exclude it for now.",
        "Yes tried to find something without success, I'll look a bit more",
        "There's https://github.com/tox-dev/toml-fmt but way too much diff generated with our current file. For now let's just be careful when making edits to the pyproject.toml"
      ],
      "pydantic-eliminate-redundant-computation": [
        "`is_generic_alias` does an `isinstance()` check against `typing._GenericAlias` (e.g. `List[int]` is an instance of such a class), which isn't documented and technically private (although I don't think it will change). So it's best to avoid relying on it.\r\n\r\nIt is also a footgun as while `is_generic_alias()` works for all parameterized typing objects, it doesn't check for new unions (`is_generic_alias(int | str) == False`, but `is_generic_alias(Union[int, str]) == True`). For instance, I'm not sure we expected new unions to be skipped here:\r\n\r\nhttps://github.com/pydantic/pydantic/blob/acb0f10fda1c78441e052c57b4288bc91431f852/pydantic/_internal/_core_utils.py#L66-L74\r\n\r\nSimilarly, I've used this function here as a way to check for `type[list[int]]` forms (here `type_param` is `list[int]`):\r\n\r\nhttps://github.com/pydantic/pydantic/blob/acb0f10fda1c78441e052c57b4288bc91431f852/pydantic/_internal/_generate_schema.py#L1711-L1715\r\n\r\nThis would also match `type[Union[int, str]]`, which we actually want to support! Thankfully there's a specific check for unions just before, but this could easily be missed.\r\n\r\n---\r\n\r\nI think there are still valid use cases where you want to check if something is a generic alias (and by that I don't mean `isinstance(obj, (types.GenericAlias, typing._GenericAlias)`, but if the `obj` is a parameterized generic class -- excluding unions, typing special forms like `Literal`, `Annotated`, etc), but it's probably best to rely on `get_origin()` and the `typing_objects` check functions.\r\n",
        "```suggestion\r\n            which may not be types and thus do not have a `__module__` available\r\n```\r\n\r\nmaybe? I think the most common example is `SomeType = list[...]`, and is more common that PEP 695 type aliases. I think it's best to emphasize on the fact that most objects passed to type adapters are _instances_ (e.g. `type A = int`, `A` is instance of a `TypeAliasType`).",
        "This `display_as_type` function needs to be refactored, as it is relatively expensive to recursively check for `get_origin`, `get_args`, etc. It is currently used:\r\n- In `FieldInfo.__repr_args__`, to make a string representation of the `annotation` attribute. This can be kept.\r\n- In `get_type_ref`, called for each arg of a parametrized type. We should find a simpler way to generate a core schema reference."
      ],
      "pydantic-avoid-unnecessary-operations": [
        "I also need to check that `cls.__pydantic_generic_metadata__['origin']` is `None` for Pydantic models, so maybe it's best to keep the (small) duplication of code here."
      ],
      "pydantic-categorize-error-types": [
        "It would make sense to do so if these kind of errors happen at runtime, _after_ initial module imports and application setup (like validation errors, where it makes sense to know how to handle them).\r\n\r\nPydantic errors are just usage exceptions and it doesn't really make sense to try..catch on these ones.",
        "```suggestion\r\nWhile classes are callables themselves, `validate_call` can't be applied on them, as it needs to know about which method to use (`__init__` or `__new__`) to fetch type annotations. If you want to validate the constructor of a class, you should put `validate_call` on top of the appropriate method instead.\r\n```",
        "```suggestion\r\nAlthough you can create custom callable types in Python by implementing a `__call__` method, currently the instances of these types cannot be validated with `validate_call`. This may change in the future, but for now, you should use `validate_call` explicitly on `__call__` instead.\r\n```"
      ]
    },
    "profile": {
      "location": "Amsterdam",
      "company": "@pydantic",
      "blog": "",
      "site_admin": false,
      "followers": 76,
      "following": 3
    }
  },
  "erikgrinaker": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-balance-flexibility-with-performance",
        "title": "Balance flexibility with performance"
      },
      {
        "slug": "neon-comprehensive-code-documentation",
        "title": "Comprehensive code documentation"
      },
      {
        "slug": "neon-connection-pooling-with-pipelining",
        "title": "Connection pooling with pipelining"
      },
      {
        "slug": "neon-database-replica-promotion-safeguards",
        "title": "Database replica promotion safeguards"
      },
      {
        "slug": "neon-design-domain-specific-error-types",
        "title": "Design domain-specific error types"
      },
      {
        "slug": "neon-design-metrics-for-insights",
        "title": "Design metrics for insights"
      },
      {
        "slug": "neon-document-concurrency-design-decisions",
        "title": "Document concurrency design decisions"
      },
      {
        "slug": "neon-flexible-documented-configurations",
        "title": "Flexible documented configurations"
      },
      {
        "slug": "neon-guard-against-race-conditions",
        "title": "Guard against race conditions"
      },
      {
        "slug": "neon-handle-network-interrupts-safely",
        "title": "Handle network interrupts safely"
      },
      {
        "slug": "neon-hierarchical-semantic-naming",
        "title": "Hierarchical semantic naming"
      },
      {
        "slug": "neon-minimize-unnecessary-allocations",
        "title": "Minimize unnecessary allocations"
      },
      {
        "slug": "neon-optimize-cargo-dependencies",
        "title": "Optimize cargo dependencies"
      },
      {
        "slug": "neon-proper-option-type-usage",
        "title": "Proper Option type usage"
      },
      {
        "slug": "neon-secure-authentication-handling",
        "title": "Secure authentication handling"
      }
    ],
    "comments": {
      "neon-design-metrics-for-insights": [
        "I don't know if we can reliably do this with the current `gc_info`.\r\n\r\n`GcCutoffs.time` (PITR cutoff) will be 0 when the start of the timeline is within the PITR window (e.g. for the first 7 days of the timeline). However, `GcCutoffs.space` will be initialized with a non-zero value during tenant activation, before the PITR cutoff is computed. This means that we can't disambiguate an uninitialized PITR cutoff from one that falls below the timeline creation. As a consequence, once a timeline is e.g. 7 days old, it's PITR history size will suddenly jump from 0 to some large number.\r\n\r\nTo disambiguate the uninitialized case from the new timeline case, we'll need to change `GcCutoffs.time` from an `Lsn` to an `Option<Lsn>`. Does that sound good?\r\n\r\nFurthermore, if we can't rely on the cache for PITR cutoff monotonicity, then any PITR window increase by the user will have a retroactive effect on billing (in the case where it actually takes effect for GC, which is currently only in the case of a tenant restart/migration). I just want to confirm that we're okay with that.",
        "> I'm still a bit unsure about under which circumstances we need pitr_cutoff reported externally, vs. using the history size metric from the other PR\r\n\r\nIt was needed to guarantee monotonicity, but as you point out it doesn't guarantee that anyway, so we'll have to accept that the PITR history size can retroactively change (sometimes) when the user changes their PITR window. I'll axe it.\r\n\r\nIf we did report it externally, then we could use it to enforce monotonicity in the billing system, which presumably _does_ have access to the previous value. But then we'd have to compute the PITR history size there.",
        "https://github.com/neondatabase/neon/pull/11984"
      ],
      "neon-optimize-cargo-dependencies": [
        "This should use the license and edition from the workspace:\n\n```\nlicense.workspace = true\nedition.workspace = true\n```",
        "Many of these aren't necessary and can be removed: `tokio` (not used), `async-stream` (not used), and `futures-core` (available via `futures`).\n\nnit: we also try (with mixed success) to keep these in alphabetical order.",
        "We should use the workspace edition: `edition.workspace = true`. We'll also need to use the workspace license: `license.workspace = true`.",
        "We should use the workspace version: `tokio-util.workspace = true`.\n\nBut I don't think we actually use `tokio-util` at all here, so we can just drop it.",
        "Shouldn't this be in `/Cargo.toml` and included via workspace?",
        "Ok, let's leave a TODO to fix it."
      ],
      "neon-database-replica-promotion-safeguards": [
        "I think we should, yeah -- it may not be strictly necessary depending on what we use them for, but it's also a bit of a footgun if we don't.\n\n@VladLazar Do we have a concrete need for these to justify the cost/complexity of re-establishing all streams when promoted?",
        "I changed this to a TODO to unblock the PR. We can add it later if we have a compelling need for it."
      ],
      "neon-minimize-unnecessary-allocations": [
        "Idk. It would be possible to avoid an allocation by using a smallvec to track the response index by shard or something, but I don't think we care that much about it. This is a rare path (batch crossing shard bounds), we're already doing a bunch of network and disk IO, and there's a ton of allocations inherent in every page we're processing here.\r\n\r\nGiven that this code is rarely triggered, and important for correctness, I'd much rather try to keep it relatively simple than to squeeze out an allocation that probably doesn't matter. But let's revisit if it turns up in profiles.",
        "nit: `.clone()` is unnecessary here.",
        "Let's drop these `From<&>` implementations. We should generally only convert owned types to avoid unnecessary heap allocations, and the caller can copy or clone if necessary."
      ],
      "neon-secure-authentication-handling": [
        "Oops, thanks -- just blindly copied this over from some prototype code. Fixed.",
        "I don't think it matters much since the JWT will fail to parse and validate in that case, but sure -- updated."
      ],
      "neon-document-concurrency-design-decisions": [
        "This should specify the locking order between `index` and `inner`, to avoid deadlocks.",
        "Yeah, I don't think it's possible either, but let's write it down for our future selves. Thanks!",
        "We're relying on `inner` being append-only for this to be safe, yeah? Let's specify that as part of the locking protocol.",
        "I'd just add something like this to the comment you added in https://github.com/neondatabase/neon/commit/9377e9af65921e06e25e4cec5607150faad56a1a:\r\n\r\n> Note that `inner` is append-only, so it is not necessary to hold simultaneous locks on `index`. In particular:\r\n>\r\n> * It is safe to read and release `index` before locking and reading from `inner`.\r\n> * It is safe to write and release `inner` before locking and updating `index`.\r\n>\r\n> This avoids holding `index` locks across IO, and is crucial for avoiding read tail latency.",
        "Is this properly synchronized with `freeze()`? It's possible that the callers avoid races here by synchronizing at a higher level, but this sequence is not prevented by the API (for threads A and B):\r\n\r\n1. A: `put_batch` takes out lock on `inner` and writes.\r\n2. B: `freeze` sets `end_lsn`.\r\n3. B: `write_to_disk` blocks on `inner`.\r\n4. A: `put_batch` releases `inner`.\r\n5. B: `write_to_disk` acquires lock on `inner` and `index`.\r\n6. A: `put_batch` blocks on `index`.\r\n7. B: `write_to_disk` writes an incomplete layer to disk from a stale index.\r\n\r\nWe can avoid this by taking out a lock on `inner` and `index` in `freeze`, and mandating that `end_lsn` must either be accessed under lock or specifying the access ordering of `end_lsn` wrt. to the locks.",
        "Thanks for the clarification and comment.",
        "I think `Ordering::Relaxed` is too lax here, given this is a lock. I think we'll need `Acquire` on the `load` and `AcqRel` on the CaS or something similar. Otherwise, it's possible that subsequent operations (e.g. `ftruncate`) are reordered wrt. the lock acquisition.\n\nNot critical, since we only ever expect one caller to modify this, but if we're going to have a lock we may as well properly lock it."
      ],
      "neon-proper-option-type-usage": [
        "This shouldn't be an `Option`, as it's never set to `None`."
      ],
      "neon-balance-flexibility-with-performance": [
        "Ack, let's keep it then. Doesn't cost us anything, and it should just work.",
        "Ack, I'm sympathetic to that. This API is modeled more after the Pageserver's current capabilities than the compute's needs (see e.g. [`get_vectored`](https://github.com/neondatabase/neon/blob/cbf442292b44decf7ab7fff77658d81c51b2c93f/pageserver/src/tenant/timeline.rs#L1182-L1191) and [`get_rel_page_at_lsn_batched`](https://github.com/neondatabase/neon/blob/c1ff7db1874c6b5ff8ee134b944f1c54616ba37b/pageserver/src/pgdatadir_mapping.rs#L244-L252)). If the compute is only ever going to request contiguous page chunks at the same LSN then we can probably lean into that for optimization.\r\n\r\nHowever, this assumes that the compute can't make forward progress until the entire batch has been received. I'm not sure that's always true.\r\n\r\nLet's consider an extreme case, for illustration: we send a batch request for pages 0-100, and at page 50 we have to download a layer which takes 3 seconds. I think there are cases where it's advantageous to send back pages 0-49 first, and pages 50-100 later:\r\n\r\n* A different backend may request a single page 20, and block on the batch request.\r\n* Similarly, if the batch was a prefetch, we could eagerly populate the LFC with pages 0-49 before anyone requests them.\r\n\r\nHowever, this depends on where readers would get blocked. If the initial batch request holds a Postgres lock for the pages, then we can't unblock the waiters until we serve the entire initial batch request.\r\n\r\nAs for the response, we can return at most 4 MB in a response, but that's 512 pages and likely fine as an upper bound. It would require a bit more memory to first buffer the entire batch and then send it, instead of just streaming them, so we could consider doing in-order streams instead. But maybe it's more efficient to just dump it into a large contiguous array too and just throw it over to Postgres ~directly.\r\n\r\ncc @VladLazar @problame since you probably have opinions here too.",
        "> the pageserver doesn't currently support streaming. The current implementation collects all the deltas and images for all keys in the batch and then does walredo for all keys concurrently. We could do true streaming, but it's not trivial.\r\n\r\nYeah, but Christian wanted to leave the protocol open to send eager out-of-order responses, since it gives us more flexibility in how we optimize reads down the road.\r\n\r\nThis also has implications for how we design the request scheduler and locking in the communicator. If we bake in an assumption that we'll always get complete batches back, it may not be easy to back out of that later if we find that it causes too much contention and tail latency.\r\n\r\nSo I would like us to take a stance now on whether eager partial responses is something we should design for.\r\n\r\n> On how scattered the reads should be: hard to tell. Generally speaking, the more clustered the pages in a request are, the more predictable performance will be\r\n\r\nI think the more important point here is that when we do the batching client-side, we _know_ whether it makes sense to batch or not (i.e. whether to prioritize throughput or latency) -- Postgres will tell us via `smgrreadv`. And if Postgres asks us to batch, it will always batch as contiguous chunks, at the same LSN.\r\n\r\nI think I'm leaning towards changing `GetPageRequest` to take a `block_count` parameter specifying the number of contiguous pages to read, but keep the streaming responses to keep the door open to eager partial responses. But it depends on how the request scheduler design ends up -- if we don't leverage it there, then we may as well just return a full batch.",
        "> I think I'm leaning towards changing `GetPageRequest` to take a `block_count` parameter specifying the number of contiguous pages to read, but keep the streaming responses to keep the door open to eager partial responses.\r\n\r\nI made this change: https://github.com/neondatabase/neon/pull/11815/commits/f94d64a13cd5a630d15c8b5db1f5b48a46a6d254\r\n\r\nBut whether to send individual page responses is an open question, added a TODO for it: https://github.com/neondatabase/neon/pull/11815/commits/0f3c0696b15f2024d18c313c4521bac0b1aa13bb",
        ">With the more flexible protocol, the communicator could coalesce batches, but perhaps we don't care about that (yet?).\r\n\r\nIt's not clear that it's better to coalesce them than to send them in parallel on separate streams. The smaller the batch is, the more latency-sensitive it is likely to be.",
        "Actually, these batches won't necessarily be contiguous -- they can get fragmented by pages present in the LFC. Let's do scattered page reads after all.\r\n\r\nThey will still be for same relation -- and in the vicinity of other pages (but don't encode this latter assumption).\r\n\r\nThere's still no point returning eager responses and populating the LFC, because backends take out buffer pool locks before they read from the LFC.",
        "https://github.com/neondatabase/neon/pull/11815/commits/ddf544d167c800b5f6d5abc2b04bf559a0b189c6"
      ],
      "neon-flexible-documented-configurations": [
        "Bummer that there's no reliable way to get this from `cargo`. We could parse the `cargo metadata` output, but hardcoding it seems find until `cargo` provides a canonical way to detect this.",
        "Yeah, let's just hardcode it for now."
      ],
      "neon-connection-pooling-with-pipelining": [
        "`caller_rx` is a stream of `(page_api::GetPageRequest, ResponseSender)`. We have to pick out the `ResponseSender` and keep it around by request ID so we know where to send each response.",
        "Ah, yes, exactly. For pipelining to be effective the request must be sent onwards to the TCP stack and server. If the request isn't passed on to TCP it doesn't matter if we block here or buffer them in the channel -- they're not getting sent across the network any faster regardless, and the caller is just going to sit around and wait for the response no matter what.\r\n\r\nI'll add a comment to that effect.",
        "Hm, actually, there might be a hazard here. If the gRPC stream send blocks, we won't be reading responses (that send is nested below the select) -- so with a large enough queue depth and with TCP/gRPC backpressure, we could end up in a similar deadlock as we've seen with the current libpq implementation (both client and server are blocked on sends).\r\n\r\nI don't think that's likely given that gRPC/TCP buffers are large and queue depths are small, but it's probably prudent to use the queue depth as a buffer size here -- it's cheap insurance.",
        "Even better, we can just use an unbounded channel. https://github.com/neondatabase/neon/pull/12475/commits/ba1d816bc79d2020c03953bb544d499fec11a4ac",
        "> If compression is used, does it return the compressed or uncompressed size?\r\n\r\nTurns out this was buggy! libpq would report the compressed size, gRPC the uncompressed size. Fixed.\r\n\r\n> Is the time returned just the time it took to establish the connection, or the full time it took to process and extract the basebackup?\r\n\r\nNeither, it's the timestamp when the connection to the server was established.\r\n\r\nUpdated the comments to address both concerns.",
        "Good call, thanks. `Path::exists()` would also discard errors.",
        "We'll need to figure out stream management here. It's kind of pointless to run Pagebench without stream reuse, so we may as well do that now. Two points:\r\n\r\n1. It should be possible to use this client both with a stream/connection pool (for the communicator) and without it (for tests and benchmarks). We can either pass in a single connection and stream (tying the client's lifetime to it and preventing concurrent use), or pass in a trait that can be implemented both by the pools and a trivial raw gRPC client. This will have implications for where stream/connection multiplexing should happen. What's your take on this?\r\n\r\n2. The API here should probably be a bidirectional stream that implements the `futures::Stream` trait: return an object that can be used both to send requests and receive responses. This is flexible enough to support both pipelining (i.e. queue depth > 1), multiple responses per request (e.g. eager page emission and informational responses about layer downloads), and out-of-order responses (if we ever want them), while keeping request tracking at a higher level. The `futures::stream` module has helpers that might come in handy for implementing this. We can also keep this `get_page()` as a convenience method that sends a single request without exposing a stream, for non-performance-critical code such as tests.\r\n\r\nIt may be premature to make these decisions before we've prototyped the higher layers (pools and trackers). If we think it is, then we can punt this and implement a Pagebench transport using the raw gRPC client. But if we're reasonably confident about the final shape of this client API then we may as well build and merge it now.",
        "> have this client take a trait that has a \"get connection\" and \"get stream\"\r\n\r\nYeah, that's probably the way to go.\r\n\r\n> pagebench has a basic implementation that either reuses a stream, or creates a new one for every request\r\n\r\nLet's generalize the basic implementation -- this will also be needed by e.g. `compute_ctl` when fetching base backups via gRPC, and in other tests.\r\n\r\n> So there would be a \"send_get_page\" that returns a receiving stream, and a \"recv_get_page\" that receives from that stream.\r\n\r\nI think we should just return both the send and receive streams from `get_pages()`, since their lifetimes are linked. This is similar to how most other stream handles are returned e.g. in Tokio."
      ],
      "neon-design-domain-specific-error-types": [
        "nit: Rust errors normally start with lowercase (since they'll often get prefixed by outer error messages), and should include the inner error, i.e. `anyhow!(\"failed to convert endpoint: {e}\")`."
      ],
      "neon-guard-against-race-conditions": [
        "We'll need to take a new clock reading here, otherwise we're populating it with a stale timestamp and shortening the next backoff delay.\r\n\r\nAlternatively, update `now` at the end of the backoff loop.",
        "We'll need to take a new clock reading here, otherwise we're populating it with a stale timestamp and shortening the next backoff delay.\r\n\r\nAlternatively, update `now` at the end of the backoff loop."
      ],
      "neon-handle-network-interrupts-safely": [
        "Above, we're setting `shard->last_reconnect_time = now`. So this will measure the time since we started the connection attempt, not the time since the previous reconnection attempt, effectively increasing the delay when connection failures are slow.\r\n\r\nIf we move that below this loop, it should do the right thing.",
        "Hm, I think that's actually a pre-existing bug, since it includes the previous backoff delay in the next iteration's delay, effectively reducing the delay by 50%. That also means that once it hits the 1 second max, the next attempt will have no backoff.",
        "Above, we're setting `shard->last_reconnect_time = now`. So this will measure the time since we started the connection attempt, not the time since the previous reconnection attempt, effectively increasing the delay when connection failures are slow.\r\n\r\nIf we move that below this loop, it should do the right thing.",
        "Hm, I think that's actually a pre-existing bug, since it includes the previous backoff delay in the next iteration's delay, effectively reducing the delay by 50%. That also means that once it hits the 1 second max, the next attempt will have no backoff."
      ],
      "neon-comprehensive-code-documentation": [
        "nit: both this and `AuthInterceptor` could use a short doc comment explaining what they're for and anything callers should be aware of.",
        "Doc comments must start with `///`, otherwise they won't get picked up by IDEs and docs."
      ],
      "neon-hierarchical-semantic-naming": [
        "Renamed it to `ReadLsn`.\r\n\r\nI'm not sure `not_modified_since_lsn` is actually meaningful/useful for any other requests than `GetPage`. All requests do pass it on to `wait_or_get_last_lsn()`, and they _could_ make use of it, but I'm not sure if they actually do?\r\n\r\n@hlinnaka Wdyt, can we move `not_modified_since_lsn` onto `GetPageRequest`?",
        "Discussed in https://github.com/neondatabase/neon/pull/11815#pullrequestreview-2818183483."
      ]
    },
    "profile": {
      "location": "Oslo, Norway",
      "company": "@neondatabase",
      "blog": "",
      "site_admin": false,
      "followers": 859,
      "following": 0
    }
  },
  "mjankowski": {
    "repos": [
      "mastodon/mastodon"
    ],
    "entries": [
      {
        "slug": "mastodon-api-parameter-design",
        "title": "API parameter design"
      },
      {
        "slug": "mastodon-batch-similar-operations",
        "title": "batch similar operations"
      },
      {
        "slug": "mastodon-centralize-configuration-management",
        "title": "centralize configuration management"
      },
      {
        "slug": "mastodon-choose-appropriate-exception-types",
        "title": "Choose appropriate exception types"
      },
      {
        "slug": "mastodon-documentation-audience-appropriateness",
        "title": "Documentation audience appropriateness"
      },
      {
        "slug": "mastodon-environment-variable-descriptive-naming",
        "title": "Environment variable descriptive naming"
      },
      {
        "slug": "mastodon-extract-complex-logic",
        "title": "Extract complex logic"
      },
      {
        "slug": "mastodon-extract-view-complexity",
        "title": "Extract view complexity"
      },
      {
        "slug": "mastodon-markdown-formatting-consistency",
        "title": "markdown formatting consistency"
      },
      {
        "slug": "mastodon-migration-data-dependencies",
        "title": "migration data dependencies"
      },
      {
        "slug": "mastodon-optimize-database-queries",
        "title": "Optimize database queries"
      },
      {
        "slug": "mastodon-optimize-test-organization",
        "title": "optimize test organization"
      },
      {
        "slug": "mastodon-prefer-established-configuration-patterns",
        "title": "prefer established configuration patterns"
      },
      {
        "slug": "mastodon-use-accessible-terminology",
        "title": "Use accessible terminology"
      },
      {
        "slug": "mastodon-use-descriptive-specific-names",
        "title": "Use descriptive specific names"
      },
      {
        "slug": "mastodon-use-semantic-naming",
        "title": "Use semantic naming"
      }
    ],
    "comments": {
      "mastodon-extract-complex-logic": [
        "Instead of the local var and explicit return here, might be a little cleaner to have something like `unless fields_with_values_missing_names?(account) ...` (or whatever on naming) and relying on one path of that to add the errors and an implicit return to not add when all is well?",
        "Maybe pull these json/hash wrappers into private method?\r\n\r\n`webhook_payload(unblock)` or something?",
        "I think this is safe by itself, but also the `compact` on the end.\r\n\r\nThe entire second half of this method (how to handle when `options[:key]` isnt present) - should be a separate method.",
        "This could also probably use a method extraction, and/or delegate some of this to the user role.",
        "Minor style thing -- could pull out this 5 line block to a `non_functional_user_setup_path` (or something) private method, and keep this filter as a one-liner. The explicit `nil` return above this feels odd.\r\n"
      ],
      "mastodon-centralize-configuration-management": [
        "This is fine as first-pass, but could also be migrated to one of the relevant `config_for` sections.",
        "Future refactor here -- we have a series of open PRs to move app wide config like this into yml files that are loaded/validated once at app launch time (via `config_for`) ... if/when those are merged, this is a good setting to go into something like that as well.",
        "Is the idea that this is loaded once at launch time and would require a restart to reload the features? (ie, no dynamic changing w/out restart) - if so, could shove this into one of the `config_for` namespaces?\r\n\r\nSeparately, maybe `ENV.fetch...`?",
        "> I am not sure. I should maybe add that one of the requirements was that toggling of multiple features should be possible via a single environment variable. I believe it would be awkward to implement this as ERB within a yaml file.\r\n\r\nI agree that once you start to cross the line from \"just loading some env vars\" into \"processing strings and running ruby code\" in a yaml file it starts to feel awkward. I'm not sure where that line is exactly.\r\n\r\nSo far we mostly just have the env var loading in the yml - https://github.com/mastodon/mastodon/blob/main/config/mastodon.yml - and I think most scenarios that need additional parsing/processing do it in private methods where they are used?\r\n\r\nOne side goal of the `config_for` migration was to eventually enable https://docs.rubocop.org/rubocop-rails/cops_rails.html#railsenvironmentvariableaccess and enforce the \"configure at load time\" practice.\r\n",
        "Yup, exactly that - just the bare minimum of nudging the env load into `config_for`, and leaving any parsing/formatting/etc in the classes.",
        "Yeah... agreed it would be nice to preserve the env->config assertions while also getting the load early benefits. \n\nI wonder if there's a \"reload configs\" we could use just in spec. Will review that. ",
        "Wait ... we can just have it re-assign the config, and preserve the climate control approach ... updated here to do that in the one spec which was changed.\r\n\r\nMight do a follow-up here to add more of these for the ones previously updated. I agree that \"when I set the env var, the config is correct\" is a worthwhile thing to capture in these.",
        "Do you mean \"putting files in the `config` dir\", or \"putting values right on the top-level Rails `configuration` namespace?\r\n\r\nIn either case - I think unless the framework dropped the feature for some reason, it's probably safe. You avoid collisions because the key comes from the file name ... so as long you don't name (and load via config_for) some file that is named after an already-existing rails configuration value, its probably ok. In this specific case, I think \"omniauth\" is safe."
      ],
      "mastodon-use-semantic-naming": [
        "The \"does this user have expand CW or show all media on\" check could probably either be extracted to a view hleper we could use here, or some well named method on User (and optionally delegated from account to tighten up even more) ... `User#all_content_visible?` or something ... not sure on naming."
      ],
      "mastodon-choose-appropriate-exception-types": [
        "I'd have to contemplate whether this is viable ... but I've been meaning to look at the normalizes API for some of these ... would give us cleaner inbound normalization and help w/ some of the queries as well",
        "Is this ever actually raised?",
        "One idea here -- could we re-use `Mastodon::ValidatorError` (with a custom string)?\r\n\r\nI think this could keep the entire check at the controller level, would match existing style, and would let you remove all the new error classes, along with the changes to the service and service spec.\r\n\r\nFor an example of what I mean - look at `api/v1/filters#update` and how it raises that class (or `check_accounts_limit` in this controller, though it does not have a custom message).\r\n\r\nI suspect you could keep your request spec changes here, but move the actual check to a `before_action` in the controller (since it knows what accounts are relevant before passing in)."
      ],
      "mastodon-batch-similar-operations": [
        "I pushed before totally done, will update more."
      ],
      "mastodon-migration-data-dependencies": [
        "I see this new seed file brings in the previous list from config/settings ... I think the scenarios are:\n\n- New/fresh installs ... would either load full db structure and load from seeds, or would run full migrate and then load from seeds, which in both cases would leave them with that full set in place\n- Existing installs ... previously were relying on the default settings from yaml, would run this migration and load all those into new db table\n\nIs there an issue if that seed file changes? If we added a username to that seed file, would we need to also add a migration to add the new value? (assuming existing installs are not re-running seed load on upgrades)",
        "Yeah - was sort of thinking out loud there, and I guess it would be similar to the user role scenario.\n\nIf we ever added a new user role, we presumably would both add it to the seeds file and then run a migration to add the role. Though those are a little different because the app is more depending on all the (core) roles existing in order to function correctly, whereas the username block list is more of a default/sugggested list sort of thing, but not required to function.\n\nI guess the question is if we JUST want to correctly migrate all existing installs to preserve the list they currently have, this could just live in the migration and not a seeds file. If we also want to provide this list as starting data to every new/future install, seeds (or some other re-runnable tootctl or something) is fine.",
        "> The goal is to inherit the previous setting for both, if a previous setting existed, otherwise use the defaults (allow timeline_preview_local, prevent timeline_preview_remote).\r\n\r\nThe logic looks more/less correct to me ... I do wonder for perf reasons if a) should be \"post migrate\"? b) it might be gnarly with the settings hash storage, but could we this via direct sql w/out needing the AR class in the migration?\r\n\r\n> Reverting the setting in the down is also a bit chaotic, and also kinda unneeded because, well, do we ever do down migrations post-merge of a migration?\r\n\r\nIf the data is truly destroyed and not recoverable/rebuildable, it should raise the irreversible migration error and not just approximate something. I'd say this case we can (at least if done immediately) in fact reconstruct the before state from the after state, so its fine to keep it.\r\n\r\n",
        "Will do -- just to confirm, move **JUST** the delete line to new migration in `db/migrate`, but leave the rest (add index, remove index, remove columns) as-is in this `db/post_migrate` one?",
        "Rebased, pushed another update...\r\n\r\n- Remove previous single migration doing everything\r\n- Added new db/migrate migration to do just the delete\r\n- Added new db/post_migrate to repeat the delete and do the other ops\r\n- Other changes are same\r\n\r\nConverted to ready for review as well.",
        "I feel like I'm missing something obvious, but -- shouldnt this role already exist from the seeds?",
        "Interesting ... I'd never noticed that migration before. So currently if someone runs `bin/rails db:migrate:setup` (or some variation of \"run migrations and seeds\" via whatever commands you might use), they wind up loading all the seeds once, and then the roles seed specifically a second time during that one migration. This is basically harmless since the seed files are all idempotent and will just not do anything after the first run.\r\n\r\nI assume the reason for this migration then is that if we're going to keep the FK constraint and also have a default value, we need to be sure the row exists _before_ the migration adding the default is run, and thus can't rely on a post-migrations-run or post-schema-load db seed load?\r\n\r\n",
        "Yeah, agreed on that ... I think we safely can assume that during app runtime all migrations have been run and the seeds have been loaded? So for the purposes of \"try to stop the N+1 on users with null role values\", we can assume that the everyone user role (since loaded by seeds) does exist?\r\n\r\nIf we CANT assume that ... that's rough. Maybe an initializer or something similar to the RAILS_ENV check on boot that would just abort/warn if the record is not there (with a message to run the seeds?).\r\n\r\nIf we can assume that, I think that path of trying to mitigate with after_initialize or some other migration-less approach is at least worth trying first before resorting to this migration approach.",
        "This preference (of preferring nil in DB with model level defaults) is sort of similar to https://github.com/mastodon/mastodon/pull/31908 ... if that's truly a team pref in a general sense rather than case by case, might be good to capture that somewhere.\r\n\r\nIt's also not 100% clear to me whether the DB index issue is the prime driver on the other linked one, or if that's more about \"preserve the ability to change defaults for users who have never set something\", or something else."
      ],
      "mastodon-environment-variable-descriptive-naming": [
        "Is this intentionally part of the PR? Or was this a local change for dev/testing?\n\nSeparately, thoughts on using `REQUIRE_MULTI_FACTOR_AUTH` or something more explicit?"
      ],
      "mastodon-optimize-test-organization": [
        "Given the repeated use of same datetime values here ... could we pull them all out to private method at bottom of this spec? (and same in other spec?)",
        "Well, we're _sending_ empty events in this invalid portion ... by not checking any boxes we wind up with something like `\"events\"=>[\"\"]` in the params ... but yes, we are not explicitly asserting about the events on the page being called out as missing.\r\n\r\nI didn't pay close attention to this, but I guess the previous controller spec was doing \"url present, events missing\" and I've done \"both missing\" here.\r\n\r\nLMK if fine as-is (we have model spec about all the validations, so I generally just try to do *something* to trigger the controller invalid path, but not every exhaustive way to do that) or you'd prefer something more direct on events.",
        "Added some scattered comments to add some context about what's being exercised in a few spots.",
        "Will do ... but out of curiosity (mostly to inform future PRs along these lines) ... why?\r\n\r\nGenerally speaking I'd try to be exhaustive on all combinations of how we can make a thing invalid at the model spec level; and then at any of the controller/system/request spec level, just make sure we hit the \"invalid path\" once to assert that some errors showed up and the invalid attrs did not save a record (exception here would be if there were multiple ways to be invalid, multiple filters, other edge cases, etc - to exercise them all) ... but in a way where it sort of doesnt matter which particular combination of making it invalid we used.\r\n\r\nAgain, no objection, just want to understand rationale for future of these. Will update this.",
        "Added special case for no events selected.",
        "You might want to rebase this -- the `body_as_json` helper is no longer available (use `response.parsed_body` directly).\r\n\r\nSeparatetly, you can you combine/chain the three assertions together so we can assert multiple things about the same response (avoid multiple round trips, multiple factories, etc).",
        "I think as currently organized here, we'll wind up creating 20 factory records (5 lets across 4 examples).\r\n\r\nMaybe adjust to have one example that starts with `expect { subject.perform }...` and then chain on all the expected updates with `change` blocks?"
      ],
      "mastodon-documentation-audience-appropriateness": [
        "I believe this file is periodically regenerated (at release) and not meant to be edited ... I think it pulls from github api to grab emails, so if you change there it will eventually change here.",
        "I'm not sure this entire section is useful. Some of the descriptions here are either wrong, or sort of correct but phrased a little oddly.",
        "High level - I'd probably delete this or add it to a developer-specific doc file, not the top level README.\r\n\r\nMore specifically:\r\n\r\n- `app` has basically all the files (frontend, backend, activitypub, rest api, the core web app, db models, etc) - not just the frontend\r\n- `bin` has mostly developer-useful tools, and only sort of hosting scripts\r\n- `config` has a grab bag of stuff, not just federated hosting\r\n- `spec` has the test suite\r\n- Some of the others -- .github, log, public, streaming, db -- your descriptions are fine, but also sort of superfluous because the directory is pretty self explanatory and hopefully not confusing to anyone who needs to understand it?"
      ],
      "mastodon-extract-view-complexity": [
        "Minor style nitpicks:\r\n\r\n- Use `order(name: :asc)` to be explicit about what we want\r\n- Add an `alphabetic` (or something) scope on the model to capture the order (mirrors an existing scope on `CustomEmoji`)\r\n- Move the entire `CustomEmojiCategory...all` to a helper method, to avoid directly referring to a constant from the view (this isn't something you added, it's already like that, but now may be good time to extract)\r\n",
        "In this one and the other form/edit views ... pull out the input fields to a `_form` partial? (and then `render form` from this view)?",
        "👍\n\nIt's definitely more clearly useful in a new+edit scenario, but also imho there's a nice mental scanning/parsing benefit when updating the views.\n\nThere's also (again IMO) a nice consistency benefit to be had by just doing it everywhere. I think we currently have form partials in most but not quite all spots. ",
        "This whole case block ... 1) maybe dump into helper method, 2) are there existing i18n strings to use?"
      ],
      "mastodon-markdown-formatting-consistency": [
        "With caveats of a) the content changes are more important than this tiny style issue, b) I only sort of care about this, c) I think consistency is more important and I see you've changed it everywhere and that's nice ... let me submit a tiny vote in favor of preserving the \"footnote style\" on all the links here.\r\n\r\nIMO it makes reading/scanning the document (as a markdown document in an editor) much much easier, and it makes the diffs to change links much more targeted to a single line, while keeping the rendered output on GH or elsewhere the same.",
        "For ease of future-diff-review, readability, and to match the existing privacy policy markdown file ... hard wrap this whole file at 80 chars?"
      ],
      "mastodon-api-parameter-design": [
        "Minor naming/style things...\r\n\r\n- Instead of a local var, move this to a `return_source?` query method elsewhere in controller, and use that to pass bool value to serializer\r\n- I'd probably name this after the desired additional output, not where the request is being made from. Something like `params[:include_original] == true` or something.\r\n- Exception to that would be if we foresee *many* other fields also being required only for editors\r\n- Another avenue is that we do have 1 or 2 places where we pass in the desired fields to a serializer. Could do some sort of graphql-style ability here to ask for specific fields (from an allowed list) in a more general way for clients."
      ],
      "mastodon-use-descriptive-specific-names": [
        "Updated to use `by_language_length`",
        "Maybe `with_user_settings` or `user_preferences..` or something to capture that it's about something the user selected and not just to them? Minor thing..."
      ],
      "mastodon-use-accessible-terminology": [
        "Is \"Scunthorpe Problem\" something that is understood by most admins/moderators, and also easily translatable?"
      ],
      "mastodon-optimize-database-queries": [
        "I wonder if an easier-to-review and easier-to-merge \"v1\" of this might be to bring over JUST the exact match feature as it currently works, and have the initial change just be the \"settings yaml hash config is migrated to db backed model\" portion? (ie, leave out partial match, homoglyphs, approval allow, etc - and work through that stuff in future PRs)",
        "This `reorder(nil)` is needed because all of the initial scope creation starts with `score: :desc` and if we don't remove it, the eventual query does score, then language, then score again - and the results are wrong. Another option here would be to not start the scope with an order, and add the score desc only after this language block is handled."
      ],
      "mastodon-prefer-established-configuration-patterns": [
        "More of a question than a feedback, but -- if we are planning on migrating away from rails-ujs over time, this creates slightly more work. If there's an easy/alternate way to set up the hooks, maybe do that? If not, fine as-is and we can migrate whenever other areas migrate (or never)."
      ]
    },
    "profile": {
      "location": "CYBERSPACE",
      "company": "TCUWP",
      "blog": "http://jankowski.online/",
      "twitter_username": "jankowski",
      "site_admin": false,
      "followers": 188,
      "following": 0
    }
  },
  "ulyssessouza": {
    "repos": [
      "docker/compose"
    ],
    "entries": [
      {
        "slug": "compose-avoid-ci-resource-conflicts",
        "title": "avoid CI resource conflicts"
      },
      {
        "slug": "compose-avoid-confusing-names",
        "title": "Avoid confusing names"
      },
      {
        "slug": "compose-avoid-hardcoded-configuration-values",
        "title": "Avoid hardcoded configuration values"
      },
      {
        "slug": "compose-avoid-mutable-defaults",
        "title": "avoid mutable defaults"
      },
      {
        "slug": "compose-avoid-variable-name-conflicts",
        "title": "Avoid variable name conflicts"
      },
      {
        "slug": "compose-consistent-formatting-choices",
        "title": "consistent formatting choices"
      },
      {
        "slug": "compose-environment-variable-safety",
        "title": "Environment variable safety"
      },
      {
        "slug": "compose-environment-variable-validation",
        "title": "Environment variable validation"
      },
      {
        "slug": "compose-follow-existing-naming-patterns",
        "title": "Follow existing naming patterns"
      },
      {
        "slug": "compose-optimize-docker-layer-caching",
        "title": "optimize Docker layer caching"
      },
      {
        "slug": "compose-pin-git-dependencies",
        "title": "Pin git dependencies"
      },
      {
        "slug": "compose-precise-security-pattern-matching",
        "title": "precise security pattern matching"
      },
      {
        "slug": "compose-prefer-explicit-readability",
        "title": "prefer explicit readability"
      },
      {
        "slug": "compose-safe-collection-modification",
        "title": "Safe collection modification"
      },
      {
        "slug": "compose-use-standard-api-fields",
        "title": "Use standard API fields"
      }
    ],
    "comments": {
      "compose-avoid-hardcoded-configuration-values": [
        "You should be able to pass the `-e HOME` through the variable `COMPOSE_OPTIONS` (as described in line 11). If we remove this volume mount here, that can break peoples code that expects this mount.",
        "Ok fair enough!"
      ],
      "compose-avoid-ci-resource-conflicts": [
        "Done"
      ],
      "compose-follow-existing-naming-patterns": [
        "Sure! I felt the same while reading. Didn't change cuz it wasn't part of the PR.\r\nRenaming in 1, 2, 3..."
      ],
      "compose-pin-git-dependencies": [
        "This should be `docker==4.2.1` just after the release"
      ],
      "compose-consistent-formatting-choices": [
        "Thanks for that one!",
        "IMO the use of {} places delimiters on when the variable name ends.\r\nAs you just said, there are cases where it's necessary, so to make the code style consistent, I prefer using it. Even my IDE asks for it. And by the end of the day that's a question of taste.",
        "Here I just use the double quotes because the line before was using it.",
        "Done"
      ],
      "compose-environment-variable-validation": [
        "Yep! Adding the comment.",
        "My second option was to run the tests command line 2 times. One for each configuration of the envvar.\r\nI'm not sure if I understood what you mean by using a global variable for this.",
        "Ahhh... Finally got what you mean. The idea was to have it also configurable from outside of this `TestMain`.\r\n\r\nWith the envvar we can run just one specific test. And when running like this, `TestMain` doesn't run. So the envvar is a solution to be able to still pass this info to the test.",
        "The only alternative I see is to pass a build tag when running the test to choose in between tagged files. So we can choose which file to use, so we can use that conditionally depending on the tag.... But that really looks like over engineering and gets to the same problematic in the end..."
      ],
      "compose-environment-variable-safety": [
        "In this case the environment takes the precedence that should be the opposite.\r\nWhat if environment variable is set to `COMPOSE_COMPATIBILITY=no` and command line uses `--compatibility`?\r\n\r\nCould you please, add a test using both?",
        "Yep! The command line should have precedence over the environment variable"
      ],
      "compose-optimize-docker-layer-caching": [
        "Done",
        "Done"
      ],
      "compose-precise-security-pattern-matching": [
        "The absence of `[ ]` breaks the formatting compatibility with the others"
      ],
      "compose-prefer-explicit-readability": [
        "Not sure if I understood this line, but the following equivalent would be more readable.\r\n\r\n```suggestion\r\n        msg = 'Pulling' if not silent else None\r\n```",
        "IMHO this list comprehension seems too big.  Even looks like the  `if` is inside `for`but not respecting indentation.\r\nI think this deserves a verbose version"
      ],
      "compose-avoid-confusing-names": [
        "```suggestion\r\n      version            Show version of Docker-Compose and its dependencies\r\n```\r\nThe point here is that `Compose`, refers to the specification and `docker-compose` to the software",
        "Cool! LGTM",
        "Please, don't use shadowing here. It looks like you are making a recursive call. "
      ],
      "compose-safe-collection-modification": [
        "By commenting this line the test should fail"
      ],
      "compose-use-standard-api-fields": [
        "I agree with this. Also, if it's not running, there is no `Uptime`.\r\nChanging the `State` column to `Status` would do the job.\r\n\r\nThe only thing here is the people that could be parsing this output, but since it's not a guaranteed format, we should be fine.",
        "I think that like in `docker build` path should be mandatory. ",
        "Also, that would be great if the order here was alphabetic to reflect `docker build --help` options list behavior.",
        "The point is that in this API `path` is optional because `fileobj` can be used instead and `fileobj` is not used in your PR. Checked in https://github.com/docker/docker-py/blob/37e096f6add7e26ada3d6840ce9a9ce341bbdf23/docker/api/build.py#L125\r\n\r\n"
      ],
      "compose-avoid-mutable-defaults": [
        "```suggestion\r\n                interpolate=True, environment_file=None, enabled_profiles=None):\r\n    enabled_profiles = enabled_profiles or []\r\n```\r\n\r\nFor more info, https://stackoverflow.com/questions/1132941/least-astonishment-and-the-mutable-default-argument",
        "```suggestion\r\n    def __init__(self, name, services, client, networks=None, volumes=None, config_version=None,\r\n                 enabled_profiles=None):\r\n        enabled_profiles = enabled_profiles or []\r\n```",
        "```suggestion\r\n    additional_options = additional_options or {}:\r\n```\r\n\r\nSame for the others"
      ],
      "compose-avoid-variable-name-conflicts": [
        "Done"
      ]
    },
    "profile": {
      "location": "Berlin",
      "blog": "",
      "site_admin": false,
      "followers": 103,
      "following": 5
    }
  },
  "ThisIsMissEm": {
    "repos": [
      "mastodon/mastodon"
    ],
    "entries": [
      {
        "slug": "mastodon-api-parameter-design",
        "title": "API parameter design"
      },
      {
        "slug": "mastodon-avoid-redundant-computations",
        "title": "avoid redundant computations"
      },
      {
        "slug": "mastodon-batch-similar-operations",
        "title": "batch similar operations"
      },
      {
        "slug": "mastodon-centralize-configuration-management",
        "title": "centralize configuration management"
      },
      {
        "slug": "mastodon-choose-appropriate-exception-types",
        "title": "Choose appropriate exception types"
      },
      {
        "slug": "mastodon-comprehensive-authorization-checks",
        "title": "comprehensive authorization checks"
      },
      {
        "slug": "mastodon-consistent-route-nesting",
        "title": "consistent route nesting"
      },
      {
        "slug": "mastodon-extract-complex-logic",
        "title": "Extract complex logic"
      },
      {
        "slug": "mastodon-leverage-build-tool-automation",
        "title": "leverage build tool automation"
      },
      {
        "slug": "mastodon-migration-data-dependencies",
        "title": "migration data dependencies"
      },
      {
        "slug": "mastodon-minimize-html-attack-surface",
        "title": "Minimize HTML attack surface"
      },
      {
        "slug": "mastodon-network-resource-limits",
        "title": "Network resource limits"
      },
      {
        "slug": "mastodon-review-configuration-currency",
        "title": "Review configuration currency"
      },
      {
        "slug": "mastodon-safe-proxy-configuration",
        "title": "Safe proxy configuration"
      },
      {
        "slug": "mastodon-use-accessible-terminology",
        "title": "Use accessible terminology"
      },
      {
        "slug": "mastodon-use-descriptive-specific-names",
        "title": "Use descriptive specific names"
      },
      {
        "slug": "mastodon-use-semantic-naming",
        "title": "Use semantic naming"
      },
      {
        "slug": "mastodon-use-semantic-null-handling",
        "title": "Use semantic null handling"
      }
    ],
    "comments": {
      "mastodon-extract-complex-logic": [
        "the above block should probably be moved into a separate `def`"
      ],
      "mastodon-use-semantic-null-handling": [
        "Then you'd not need string | null as the type — an empty string is as food as null in this case: there is no description to show."
      ],
      "mastodon-centralize-configuration-management": [
        "This line is a close copy of line 54 in application_controller, would it be better to lift it up to the application configuration layer instead? e.g., a `Rails.configuration.x.fetch_mode` and then set it to `none | actors | all`?"
      ],
      "mastodon-consistent-route-nesting": [
        "This is a little clunky because I nested the notes under the instances routes, so:\r\n\r\n```\r\n/admin/instances/:instance_id/notes/:id\r\n```\r\n\r\nWhere as both Report Notes and Account Moderation Notes are top-level routes in admin:\r\n\r\n```\r\n/admin/account_moderation_notes/:id\r\n/admin/report_notes/:id\r\n```\r\n\r\nI think those probably should have been nested originally.",
        "This allows people to permalink to a specific note"
      ],
      "mastodon-use-semantic-naming": [
        "This should be deleted.. we reuse the `report_notes/report_notes` partial — we should probably rename that partial to `moderation_notes` or something, since it applies for Account Moderation Notes, Report Notes, and Instance Notes now.",
        "The localisation here for \"noop\" is \"Filtered\", which seems like the most reasonable value, because showing a moderator \"NOOP\" in the menu just doesn't really make sense, and usually this severity is used in combination with the `reject_*` fields in domain blocks, which are effectively \"filters\"",
        "Maybe, but given its a single use for now, I'd probably not add a method for it"
      ],
      "mastodon-review-configuration-currency": [
        "This entire switch is — basically the environment variables to find the root cert don't exist here (or aren't documented) so there's no way to align with ruby via DB_SSLMODE, because we require the root certificate to actually properly do this.\r\n\r\nDATABASE_URL can encode the path to that root certificate and automatically load it, which enables us to have a checkServerIdentity function ",
        "We could maybe grab the certificate path from `PGSSLROOTCERT` if set — it'd still be different, but at least then we'd be able grab it if defined.\r\n\r\nOtherwise we'd need to load from either `PGSSLROOTCERT`, or `~/.postgresql/root.crt` in the user's home directory on unix/mac, or on Microsoft Windows from `%APPDATA%\\postgresql\\root.crt`. (`%APPDATA%` may be [`os.homedir()`](https://nodejs.org/docs/latest/api/os.html#oshomedir) though I'm not sure sure).\r\n\r\nThis is based on https://www.postgresql.org/docs/current/libpq-ssl.html",
        "If none of those options exist (none are a readable file by the current user) then we would have to abort with an error, but otherwise we could \"discover\" the root certification just like libpq does.",
        "Yeah, so my understanding above was correct, here's the function in libpq that's called if `PGSSLROOTCERT` is not defined, to discover the default path of `~/.postgresql/` or `%APPDATA%\\postgresql\\`:\r\n\r\nhttps://github.com/postgres/postgres/blob/master/src/interfaces/libpq/fe-connect.c#L8123-L8175\r\n\r\nAnd then where it's called to load up the root certificate:\r\n\r\nhttps://github.com/postgres/postgres/blob/master/src/interfaces/libpq/fe-secure-openssl.c#L866-L969\r\n\r\n**Essentially let `rootCert` be `PGSSLROOTCERT` if defined, else construct a path to `/.postgresql/root.crt` relative to the home directory or `\\postgresql\\root.crt` relative to `%APPDATA%` (dependent on operating system)**\r\n\r\nFrom what I can tell `%APPDATA%` may have a few different locations, and there's no node.js API to resolve this as it's a C call to [`SHGetKnownFolderPath`](https://learn.microsoft.com/en-us/windows/win32/api/shlobj_core/nf-shlobj_core-shgetknownfolderpath) (as of windows vista). I have had someone suggest that there may be a `process.env.APPDATA` or `process.env.APP_DATA` value on windows, but I cannot verify this as I don't have a windows computer nor can I get a windows developer VM to test in (they've been unavailable on the microsoft website \"temporarily\" for the past 6 months)\r\n\r\nIf `rootCert` is `\"system\"` then that triggers using all of openssl's root certificate logic.\r\n\r\nElse, if `rootCert` has a value, do a `stats` syscall on `rootCert` and if it exists and is readable (I think the `stats` syscall returns non-zero if it's unreadable by the current user), then load that certificate via [`SSL_CTX_load_verify_locations`](https://docs.openssl.org/master/man3/SSL_CTX_load_verify_locations/).\r\n\r\nNode.js's crypto internals for tls do certificate loading slightly differently because they requiring loading the certifcate as a buffer from the filesystem first, before adding the buffer as a root cert if it's a valid certificate. But for all intents and purposes, I think these would be equivalent.\r\n\r\nSo if we just do the part in bold above, then we would have parity with libpq entirely, beyond what `pg-connection-string` offers with regards to compatibility when using just `DB_SSLMODE` without specifying a root certificate some how."
      ],
      "mastodon-choose-appropriate-exception-types": [
        "I don't think this is really the right error to raise here, open to suggestions. Would it be worth adding something more specific to `lib/exceptions.rb` ? e.g., `Mastodon::InvalidDomainError` or something?\r\n\r\nThis should probably also raise if the normalized domain is invalid, though it looks like `Addressable::URI::InvalidURIError` will be raised from `TagManager.instance.normalize_domain` in that case?",
        "oddly enough, `TagManager.instance.normalize_domain` doesn't raise if the domain is blank or otherwise malformed..\r\n\r\nSo these are currently \"valid\" domains:\r\n- `''` (returns an empty string)\r\n- `.` (returns `.`)\r\n- `a.` (returns `a`)\r\n- `.a` (returns `.a`)",
        "So should we allow it and just kick the problem down the road to when the person using the admin panel goes to save the note? It will error at some point.\r\n\r\nMaybe we should have the validation actually be on the router? Ideally reusable by all the instance admin routes?",
        "Could we maybe do a constrain that's just `\\w+(.\\w+)+` or something? i.e., any word character? Though arguably that wouldn't work with non-normalized IDN",
        "How would we feel if I kept the method but removed the exception being thrown?",
        "Have removed the method."
      ],
      "mastodon-safe-proxy-configuration": [
        "This default of a string of multiple values actually caused proxyaddr to throw a server crashing error if the req.ip property was ever accessed",
        "This is exactly how Express adds ip onto requests",
        "Having request.ip both for http & websocket would allow us to support IP Blocks in streaming as well, should we want that"
      ],
      "mastodon-batch-similar-operations": [
        "Here, if an application had say, 500 access tokens before being deleted, we'd previously publish 500 events in one redis pipeline to 500 pubsub topics.\r\n\r\nWith this change, we'd publish only one event with 500 access token IDs in the message body to a single pubsub topic.\r\n\r\nThis would apply for each multiple of 1000, so for 1500 access tokens, we'd send two publishes to a single topic."
      ],
      "mastodon-migration-data-dependencies": [
        "I wasn't sure on the exact way to do this migration, but I've tested it locally a bunch and it appears to work.\r\n\r\nThe goal is to inherit the previous setting for both, if a previous setting existed, otherwise use the defaults (allow timeline_preview_local, prevent timeline_preview_remote).\r\n\r\nReverting the setting in the down is also a bit chaotic, and also kinda unneeded because, well, do we ever do down migrations post-merge of a migration?",
        "I've been having a think about this further, and I'm not sure we actually even want these routes, instead I think we'd want like `GET /api/v1/timelines/preview/local` or something. That way we avoid all this complicated boolean logic."
      ],
      "mastodon-leverage-build-tool-automation": [
        "My understanding is that these maybe could be deleted because Vite automatically preloads? (assuming the application was structured in a way to code-split effectively)"
      ],
      "mastodon-minimize-html-attack-surface": [
        "I'm not sure how to do this safely? Apparently markdown.render returns html as a string but it's not marked as safe for usage in templates?"
      ],
      "mastodon-api-parameter-design": [
        "If I'm understanding what this code did correctly, then if the `forward_to_domains` option wasn't specified, then it'd always forward to the target_account's domain. We may want to reassess that, and instead rely on manual forwarding — I don't know if a missing API parameter should be seen as \"yes, let's forward this to a remote instance\"",
        "```suggestion\r\n        it 'removes the status and the attachment', :aggregate_failures do\r\n```",
        "This doesn't keep the report status filter, but probably should"
      ],
      "mastodon-use-descriptive-specific-names": [
        "```suggestion\r\n  def forwardable?\r\n```",
        "Arguably we could change this signature to:\r\n```suggestion\r\n  def call(report, forwarder, forward_to_domains = [])\r\n```\r\n\r\nThe options usage is just from moving code across from ReportService"
      ],
      "mastodon-use-accessible-terminology": [
        "I'm using \"Internal Notes\" as the heading to emphasize that these aren't visible to the general public.\r\n\r\nThere is also still \"private_comment\" vs \"public_comment\" on DomainBlocks, but I suspect we'll see people opt not to use private comments in the future (plus they're usually junk because of how importing domain blocks works)",
        "Could also use \"Private Notes\" maybe, or \"Moderator Notes\", idk."
      ],
      "mastodon-avoid-redundant-computations": [
        "This method previously did multiple things:\r\n- created the listener for the messages from Redis\r\n- subscribed to the topics in redis\r\n- attached a close handler in the case of http EventSource connections\r\n\r\nThis made the code difficult to reason about. It is still currently responsible for sending the message on to the output destination, but that can't really be avoided.\r\n\r\nThis does mean that for both WebSockets and EventSource connections we need to manage the pubsub manager subscriptions independently, but eventually we'll be able to replace that with a \"subscription manager\", much like WebSocket's already has, but abstracted for all connection types.\r\n\r\nThis will mean that processing a message from redis won't need N callbacks, but instead just the one that grabs the subscribed connections and sends the messages, making the streaming server lighter overall (currently we've some GC churn by constantly declaring complex functions such as this."
      ],
      "mastodon-comprehensive-authorization-checks": [
        "What about if the requested `params.account_id` Account has blocked the requestee? Yes, filtering will happen at the per-message level, but we should probably do an authorization check here too."
      ],
      "mastodon-network-resource-limits": [
        "It'd be better to probably use just a single topic, since the churn on subscribe/unsubscribe is going to be high otherwise — one typically isn't staying on a profile for a long period of time, but there are a significant number of \"view file, go back to notifications\" type interactions.\r\n\r\nBy using a per-account redis channel, we'd be encountering fairly significant subscribe/unsubscribe load on redis, so I'd advise against it. As it is, we've a proliferation of redis topics unnecessarily for other things (e.g., access token revocations), we should probably let the streaming server just do the filtering it needs.\r\n\r\nThe only case where we'd decide to do something different is if we were to use `pubsub numsub` to check if we should publish to redis at all in the first place."
      ]
    },
    "profile": {
      "location": "Berlin, Germany",
      "company": "@Unobvious-Technology / @brandedcode ",
      "blog": "https://brandedcode.com/",
      "site_admin": false,
      "followers": 619,
      "following": 121
    }
  },
  "pierrejeambrun": {
    "repos": [
      "apache/airflow"
    ],
    "entries": [
      {
        "slug": "airflow-avoid-code-duplication",
        "title": "Avoid code duplication"
      },
      {
        "slug": "airflow-component-reuse-first",
        "title": "Component reuse first"
      },
      {
        "slug": "airflow-document-public-api-boundaries",
        "title": "Document public API boundaries"
      },
      {
        "slug": "airflow-document-security-exceptions",
        "title": "Document security exceptions"
      },
      {
        "slug": "airflow-ensure-deterministic-queries",
        "title": "Ensure deterministic queries"
      },
      {
        "slug": "airflow-internationalize-ui-text",
        "title": "Internationalize ui text"
      },
      {
        "slug": "airflow-leverage-backend-api-capabilities",
        "title": "Leverage backend API capabilities"
      },
      {
        "slug": "airflow-maintain-code-consistency",
        "title": "Maintain code consistency"
      },
      {
        "slug": "airflow-parameterize-similar-tests",
        "title": "Parameterize similar tests"
      },
      {
        "slug": "airflow-standardize-api-parameter-handling",
        "title": "Standardize API parameter handling"
      },
      {
        "slug": "airflow-use-descriptive-action-names",
        "title": "Use descriptive action names"
      },
      {
        "slug": "airflow-use-guards-over-assertions",
        "title": "Use guards over assertions"
      },
      {
        "slug": "airflow-validate-configuration-source-changes",
        "title": "Validate configuration source changes"
      },
      {
        "slug": "airflow-validate-nulls-explicitly",
        "title": "Validate nulls explicitly"
      }
    ],
    "comments": {
      "airflow-document-security-exceptions": [
        "Maybe just disable the eslint rule around the iframe component. \r\n\r\nAnd also add a comment to explain why this is safe. (We are only framing trusted sources, coming from the AuthManager extra menu items. Auth Manager is part of the deployment, as per our security policy Deployment Managers are considered safe). A link to our security policy too would be great. https://airflow.apache.org/docs/apache-airflow/stable/security/security_model.html"
      ],
      "airflow-leverage-backend-api-capabilities": [
        "You don't need to do that slicing in the front-end. Endpoint is paginated, just request the first 5.",
        "I think this logic can holds a few problems, first of the endpoint `useAssetServiceGetAssets` is paginated. It means that you'll only retrieve the first 50 items, and you need a mechanism to display the remaining items (cf tables pagination component).\r\n\r\nIn addition, there is no guarantee at this point that all assets for a specified group will be on the same page, you could end up with a group missing some assets because those were not returned yet.\r\n\r\nI think the easiest approach would be to add a backend ui endpoints to list/retrieve asset groups directly. (I group would list all of it's child asset and we would paginate on groups directly). Maybe there are other approaches possible but I would need to think about it.",
        "We shouldn't do that manually in the UI. (call the endpoints with two different parameters and then merge results in the UI).\r\n\r\nThe backend should support this. (You can write a custom filter that will do the search accross both 'asset.name` and `asset.group`, or expand the `search_param_factory` and `_SearchParam` to be able to take a `list[ColumnElement]` as attribute. And perform a search across multiple columns."
      ],
      "airflow-use-guards-over-assertions": [
        "We shouldn't need a hard casting like that. \"as\"\r\n",
        "You'll be able to simplify this check once the backend always returns a list for `menuItems`"
      ],
      "airflow-validate-configuration-source-changes": [
        "You can't do that, response from `usePluginServiceGetPlugins` and `useConfig(\"plugins_extra_menu_items\")` are completely different this needs more work.\r\n\r\n\r\nYou should do the same as you did above. Check if `plugin` views is in the permissions, if it's not don't render the `<PuginMenus />` in the nav.",
        "Maybe keep `appbuilder_menu_items`, I have a PR that that will deprecte it and replace it with `external_views_items`",
        "As an admin I should see plugins if I have some defined:\r\n\r\nYour PR\r\n![Screenshot 2025-06-20 at 17 19 45](https://github.com/user-attachments/assets/3bfb5b17-da2e-4ea4-b141-a7fd7ff7341b)\r\n\r\nMain:\r\n![Screenshot 2025-06-20 at 17 22 49](https://github.com/user-attachments/assets/873492d2-0423-4faa-9dfc-04df13b83b6a)\r\n\r\n"
      ],
      "airflow-maintain-code-consistency": [
        "Here we should probably show text.\r\n\r\nThis is what we are doing for all other buttons in the header. (so we see a clear \"* Favorite\" and \"* Unfavorite\" button. Also it should probably be on the right align with the 'reparse' button.",
        "Maybe extract this into a really small fn. \"getPluginIframeRoute\" or something, in the same file.\r\n\r\nJust so the comment, and how to construct the iframe component remains in one place and not copy pasted everywhere. I'm scared of someone updating in one place and not in other occurrences. (That doens't cost much to do and would help preventing mistakes in the future)"
      ],
      "airflow-validate-nulls-explicitly": [
        "That type checking assert feels really weird. Maybe just handle the None case `if ... is None` -> `raise dag version not found`, while we wait for the follow up PR to clean that up."
      ],
      "airflow-component-reuse-first": [
        "We already do this at different places in the code. (We use chevron icons from react-icon lib instead), you can do something similar."
      ],
      "airflow-document-public-api-boundaries": [
        "I thought about that, and I think this is already handled by the plugin system. If someone wants to host the bundle in airflow, they can simply add a `fastapi_app` to extend the api-server capabilities and serve static content from anywhere they'd like. \r\n\r\n\r\n\r\nSomething similar to what we are doing for the Simple Auth Manager. That would look like this in your plugin:\r\n\r\n```python\r\n# In your plugins folder\r\napp = FastAPI()\r\n\r\ndirectory = Path(__file__).parent.joinpath(\"static\") # Folder holding static assets\r\n\r\napp.mount(\r\n    \"/static\",\r\n    StaticFiles(\r\n        directory=directory,\r\n        html=True,\r\n    ),\r\n    name=\"static_file_plugin\",\r\n)\r\n\r\n    \r\nclass AirflowServeStaticFilesPlugin(AirflowPlugin):\r\n    fastapi_apps = [\r\n        {\r\n            \"app\": app,\r\n            \"url_prefix\": \"/static-plugin\",\r\n            \"name\": \"Static plugin\",\r\n        }\r\n    ]\r\n```",
        "Maybe I can add a note in the doc for this. ",
        "![Screenshot 2025-06-26 at 15 02 09](https://github.com/user-attachments/assets/650e64de-6db0-404c-acdf-39bbb71db40a)\r\n\r\n"
      ],
      "airflow-avoid-code-duplication": [
        "for loop (because if MAX_SORT_PARAMS == 5, we don't want to do copy/past this 5 times)"
      ],
      "airflow-standardize-api-parameter-handling": [
        "No, FastAPI handles `node_ids: list[str] | None = None` natively. FastAPI chose exploded form query params, i.e `?node_ids=1&node_ids=2` you will directly receive `node_ids=[\"1\",\"2\"] in your function.",
        "Please stay consistent with the rest of the API, we don't want some endpoints doing that and some other doing something else, also this will remove the need for you to manually parse that string.",
        "The multiple query param orders will not be passed like this `order_by=-criteria1,criteria2`, but `order_by=-criteria1&order_by=criteria2` to be consistent with the `exploded` way passing query parameters list that FastAPI is defaulting too.",
        "`two` shouldn't be hardcoded but come from the `MAX_SORT_PARAMS` value.\r\n\r\nWe need a test for that.",
        "I'm not sure it's exactly equivalent. If there is no `order_by` query param specified, we shouldn't add any by default and let the query default order operate:\r\n- order_by = None => don't update the query at all and let the default ordering of the query operate.\r\n- order_by = [] or [\"\"] -> fill with [primary_key_string] (or raise validation error)"
      ],
      "airflow-parameterize-similar-tests": [
        "Can you reorganize this and put tests that sort on the same criteria next to each other. For instance `criteria1` is 'last_run_state', then `criteria2` is \"display_name\" so we can more easily compare.\r\n\r\nAlso for the second sort to take effect you need data where the first criteria is equal. I'm not sure we have this at the moment.\r\n\r\nBasically we need to tests where:\r\n`{\"order_by\": [\"criteria1\", \"criteria2\"]}` will yield a different result than `{\"order_by\": [\"criteria1\", \"-criteria2\"]}` to highlight that `criteria2` sorting is actually doing something and taken into account.",
        "1 class per endpoint. Multiple method for different test cases. We shouldn't  have two classes there."
      ],
      "airflow-use-descriptive-action-names": [
        "Good point, updated!"
      ],
      "airflow-ensure-deterministic-queries": [
        "I think this problem will be here for any state that has a `start_date` None. (no_status, scheduled, queued, etc....)",
        "Maybe we should completely remove that `where` clause. The run does not have to be 'started' to actually be considered, WDYT?",
        "Also we most likely want to add a test case for this scenario.",
        "> This function is currently only used in the /dashboard stats endpoint, so initially I handled it by allowing only QUEUED runs (which are the only valid state with start_date=None).\r\n\r\nWhat function are you talking about `generate_dag_with_latest_run_query` ? `generate_dag_with_latest_run_query` is not used in the stats endpoint but in many other dag listing endpoints.",
        "Please don't use bitwise operator, sqlalchemy exposes \"not\" \"and\" \"or\" which are clearer."
      ],
      "airflow-internationalize-ui-text": [
        "> Additionally, for languages that I am not familiar with, what is the recommended approach for handling those translations?\r\n\r\nDo not update other languages, they will fallback to english for the time being and other translation owners will be in charge to fill the missing translation before the next release."
      ]
    },
    "profile": {
      "location": "Paris",
      "company": "@astronomer",
      "blog": "",
      "site_admin": false,
      "followers": 32,
      "following": 11
    }
  },
  "apparentlymart": {
    "repos": [
      "opentofu/opentofu"
    ],
    "entries": [
      {
        "slug": "opentofu-clear-concise-documentation",
        "title": "Clear concise documentation"
      },
      {
        "slug": "opentofu-contextualize-security-findings",
        "title": "Contextualize security findings"
      },
      {
        "slug": "opentofu-craft-actionable-errors",
        "title": "Craft actionable errors"
      },
      {
        "slug": "opentofu-document-intent-and-limitations",
        "title": "Document intent and limitations"
      },
      {
        "slug": "opentofu-document-reference-standards",
        "title": "Document reference standards"
      },
      {
        "slug": "opentofu-document-with-examples",
        "title": "Document with examples"
      },
      {
        "slug": "opentofu-explicit-versus-dynamic-configurations",
        "title": "Explicit versus dynamic configurations"
      },
      {
        "slug": "opentofu-log-effectively-for-debugging",
        "title": "Log effectively for debugging"
      },
      {
        "slug": "opentofu-minimize-api-surface",
        "title": "Minimize API surface"
      },
      {
        "slug": "opentofu-names-preserve-cognitive-context",
        "title": "Names preserve cognitive context"
      },
      {
        "slug": "opentofu-prevent-backing-array-surprises",
        "title": "Prevent backing array surprises"
      },
      {
        "slug": "opentofu-proper-span-lifecycle",
        "title": "Proper span lifecycle"
      },
      {
        "slug": "opentofu-provider-instance-management",
        "title": "Provider instance management"
      },
      {
        "slug": "opentofu-safe-lock-patterns",
        "title": "Safe lock patterns"
      },
      {
        "slug": "opentofu-specify-configuration-behaviors",
        "title": "Specify configuration behaviors"
      },
      {
        "slug": "opentofu-structure-tests-thoroughly",
        "title": "Structure tests thoroughly"
      }
    ],
    "comments": {
      "opentofu-provider-instance-management": [
        "This section is only here to connect with the docs about the `providers` argument in `module` blocks (linked below), which I think is the more natural home for that information.\r\n\r\nAs the moment that's only lightly _implied_ by the words on the page, rather than explicitly stated, but I'll add an extra paragraph to the other page to make that explicit."
      ],
      "opentofu-safe-lock-patterns": [
        "When I looked at this with you quickly earlier I didn't spot that this is effectively using the `lockResults` channel as a funny sort of concurrency-safe slice, relying on the first loop writing to the channel exactly the same amount of times as the second loop reads the channel because they are both iterating over the same collection.\r\n\r\nThis seems technically correct, but potentially confusing and at risk of being broken under future maintenance. I wonder if we could split the difference and use a wait group for the generation process and but a channel to consume what's being produced so that the consumer loop is directly synchronized with the producer loop.\r\n\r\nFor example:\r\n\r\n```go\r\nlockResults := make(chan lockResult, len(platforms))\r\ngo func() {\r\n    var wg sync.WaitGroup\r\n    for _, platform := range platforms {\r\n        wg.Add(1)\r\n        go func(platform getproviders.Platform) {\r\n            // just like before, except now ending with...\r\n            wg.Done()\r\n        })\r\n    }\r\n    wg.Wait()\r\n    close(lockResults)\r\n}()\r\n\r\nfor result := range lockResults {\r\n    // Exactly the same as your current \"for range platforms\" loop\r\n}\r\n```\r\n\r\nSlightly more machinery, but maybe makes the one-to-one relationship between iterations of these loops a little more explicit, and makes it harder for inconsistencies to creep in under future maintenance.\r\n\r\n---\r\n\r\nIt seems like the function for that outer goroutine could be turned into a generic helper function that we could reuse across many problems like this, though I'd personally wait to see how many more times this comes up before adding it since we don't really have a good place to dump a general-purpose thing like this. :grinning: \r\n\r\n```go\r\n// ConcurrentEach calls f concurrently for all elements of from,\r\n// waits until all calls have returned, and then closes into\r\n// to signal completion before returning.\r\nfunc ConcurrentEach[In, Out any](from []In, into chan<- Out, f func(item In, into chan<- Out)) {\r\n  var wg wg sync.WaitGroup\r\n  for _, item := range from {\r\n    wg.Add(1)\r\n    go func(item In) {\r\n      f(item, into)\r\n      wg.Done()\r\n    }(item)\r\n  }\r\n  wg.Wait()\r\n  close(into)\r\n}\r\n```\r\n\r\n```go\r\nlockResults := make(chan lockResult, len(platforms))\r\ngo ConcurrentEach(platforms, lockResults, func (platform getproviders.Platform, lockResults chan<- lockResult) {\r\n    // ...\r\n})\r\nfor result := range lockResults {\r\n    // ...\r\n}\r\n```\r\n",
        "This seems reasonable for solving exactly the problem as stated, but it seems like if we were to move this down to just before [line 568](https://github.com/opentofu/opentofu/blob/f653350af64ce92ed5c516690f6ae089e0802492/internal/providercache/installer.go#L568) then we could rely on some of the conditionals we already have in place for the global vs. local situation, and also get locking of the _local_ directory at the same time, which is not quite as popular a request but still something I've seen folks ask about before.\r\n\r\nSpecifically I'm thinking about adding something like the following just before the line I indicated:\r\n\r\n```go\r\nunlockInstallTo, err := installTo.Lock(ctx, provider, version)\r\n// (...error handling...)\r\nvar unlockLinkTo func()\r\nif linkTo != nil {\r\n    unlockLinkTo, err = linkTo.Lock(ctx, provider, version)\r\n    // (...error handling...)\r\n}\r\n```\r\n\r\n...and then unconditionally call `unlockInstallTo` and conditionally call `unlockLinkTo` afterwards.\r\n",
        "I guess what I wrote above means that we'd be doing the `tryInstallPackageFromCacheDir` step without the lock held, which is okay as long as we're only trying to lock the global cache directory -- `tryInstallPackageFromCacheDir` only _reads_ from the global cache directory -- but would not protect the local cache directory. (Not that protecting the local cache directory was a requirement to begin with; just a \"would be nice\".)\r\n\r\nGiven that this locking strategy only applies to _writers_ to the cache directories anyway (concurrent readers can still observe partially-filled package directories), I'm now wondering instead about making `Dir.Lock` be unexported and calling it from the two methods in `dir_modify.go` as an implementation detail, which would help ensure that _all_ cache directory modifications can run concurrently and also address that concern about the various codepaths that neglect to call `unlock`, because `Dir.InstallPackage` can make sure to call `unlock` itself before returning.\r\n",
        "You are right that if we delay acquiring the lock until we get inside `Dir.InstallPackage` then that function would also need to check (immediately after it acquires the lock) whether the desired package had been installed by another process in the meantime.\r\n\r\nFor `InstallPackage` I'm imagining something like this (just a pseudocode-ish sketch, not final code):\r\n\r\n```go\r\nfunc (d *Dir) InstallPackage(ctx context.Context, meta getproviders.PackageMeta, allowedHashes []getproviders.Hash) (*getproviders.PackageAuthenticationResult, error) {\r\n\tif meta.TargetPlatform != d.targetPlatform {\r\n\t\treturn nil, fmt.Errorf(\"can't install %s package into cache directory expecting %s\", meta.TargetPlatform, d.targetPlatform)\r\n\t}\r\n\tnewPath := getproviders.UnpackedDirectoryPathForPackage(\r\n\t\td.baseDir, meta.Provider, meta.Version, d.targetPlatform,\r\n\t)\r\n\r\n\tunlock, err := d.Lock(ctx, meta.Provider, meta.Version)\r\n\t// (...error handling...)\r\n\tdefer unlock()\r\n\r\n\tif existsAndIsDirOrSymlinkToDir(newPath) {\r\n\t\td.metaCache = nil // another process has modified the cache directory\r\n\t\tif cached, err := d.ProviderVersion(meta.Provider, meta.Version); err != nil && cached != nil {\r\n\t\t\tmatches, err := cached.MatchesAnyHash(allowedHashes)\r\n\t\t\tif err == nil && matches {\r\n\t\t\t\treturn someSuccessfulResult()\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Invalidate our metaCache so that subsequent read calls will re-scan to\r\n\t// incorporate any changes we make here.\r\n\td.metaCache = nil\r\n\r\n\tlog.Printf(\"[TRACE] providercache.Dir.InstallPackage: installing %s v%s from %s\", meta.Provider, meta.Version, meta.Location)\r\n\treturn meta.Location.InstallProviderPackage(ctx, meta, newPath, allowedHashes)\r\n}\r\n```\r\n\r\nThis does admittedly have an interesting implication: if we end up at the `return someSuccessfulResult` statement here then _this process_ will not have applied any package authentication rules (from `meta.Authentication`) to the directory, meaning that it's assuming that the other process did that correctly _and_ the \"some successful result\" would not be able to include any verified hashes beyond perhaps the ones that caused `MatchesAnyHash` to return true.\r\n\r\nI think that _does_ technically match the current treatment of cache entries -- `tryInstallPackageFromCacheDir` also only checks that the checksum matches -- but is admittedly quite a different guarantee than this method was previously providing. If that seems acceptable though, I wonder if there's enough overlap between \"is the package in that _other_ cache acceptable to use?\" and \"is the package in _this_ cache acceptable to use?\" that we could factor out the relevant subset of logic from `tryInstallPackageFromCacheDir` and use it from both places.\r\n\r\nOverall here my goal is to limit the time spent holding the lock and to keep the lock handling as an implementation detail of `Dir.InstallPackage`, rather than exposing it as a separate thing that callers need to remember to handle. (and acquire the lock in `Dir.LinkFromOtherCache` too, so that full installation can't race with linking at the same location if someone has a really weirdo configuration.)\r\n",
        "It seems like there are a few places above where we end the iteration of the loop early without calling `unlock`, which I guess is probably not a huge deal since the process will eventually exit and implicitly release the lock, but it we did a little more work down the path I started in https://github.com/opentofu/opentofu/pull/2166 then we could perhaps factor the body of this loop out into a separate function and use `defer unlock()` to make sure it always gets called on every return path, so that we're not at risk of any quirky behavior in the uncommon cases where we terminate early without unlocking.\r\n\r\n(I don't think we should do that refactoring as part of this PR though, since that sort of thing is better reviewed on its own.)",
        "Oh, I see! I think I misunderstood this when I read it the first time because I didn't consider that (because this is running sequentially) there can only possibly be one outstanding `unlock` at a time, and so it's okay to use only a single `defer` to clean up only that one.\r\n\r\nFair enough!\r\n"
      ],
      "opentofu-document-with-examples": [
        "As things currently stand, there are two kinds of plugins in OpenTofu: provider plugins and provisioner plugins. \r\n\r\nProvisioner plugins are largely a legacy system preserved as a concession for those who need to use some of the previously-builtin provisioners that were removed in an earlier release. None of the automatic installation, registry protocol, and signature-checking infrastructure that we're accustomed to for provider plugins are available for the legacy provisioner plugins.\r\n\r\nSo I think it's fair to say that _as we currently stand_, \"plugin\" and \"provider\" are _effectively_ synonymous, aside from some little callout boxes in a few spots that acknowledge the legacy provisioner plugin support such as on [Managing Plugins](https://opentofu.org/docs/cli/plugins/).\r\n\r\nThe text below intentionally asks us to reflect on whether we want that to continue to be true. There are some definite advantages on the OpenTofu CLI side in being able to reuse our existing plugin distribution infrastructure and \"just\" add a new optional extension protocol to the plugins, but we've yet to research what it would look like for a provider developer to implement an additional optional protocol alongside the provider protocol if they are using the currently-prominent libraries for plugin development; the author of an RFC on this topic would presumably investigate whether it's possible to use those libraries in conjunction with a second gRPC service that's implemented in a separate library.\r\n\r\n"
      ],
      "opentofu-log-effectively-for-debugging": [
        "We don't typically expect end-users to notice messages in the logs, so if we want folks to report this as a bug then it should probably be either a normal error diagnostic or a panic.\r\n\r\nHowever, we do also ask folks to send us the trace logs when they report a strange behavior, so it would be a reasonable compromise to just remove the \"This should not happen...\" part of this message and rework this to be something we can notice in the trace logs that folks send us:\r\n\r\n```\r\n[ERROR] tofu.isCallFromRemote: no module call found in foo for bar\r\n```\r\n\r\n",
        "I think making log noise here is a tricky tradeoff because having a mixture of OpenTofu manifests and non-OpenTofu manifests in the same index is a reasonable thing to do when integrating with other conventions in the OCI ecosystem.\r\n\r\nIf someone writes a manifest where _all_ of the artifact types are incorrect for OpenTofu then the current code would handle that by saying that the provider doesn't support the target platform, which would be confusing. So I _will_ change this, but I'm thinking about a different change similar to the approach I took for selecting from the \"layers\" in the image manifest: it'll count how many of the listed manifests have the correct artifact type but the wrong platform, and also how many listed manifests have the wrong artifact type, and then at the end of the loop if we have no manifests with the correct artifact type but we have at least one with the incorrect artifact type then we can return an error message that says that the index manifest is incorrect, instead of reporting the \"platform not supported\" error.\r\n\r\n"
      ],
      "opentofu-contextualize-security-findings": [
        "Would it be possible to also include the name of the affected module in this report so that we can include it in the summary of the issues that get created?\r\n\r\nOpaque identifiers like GO-2025-3588 are hard to distinguish quickly without some other context, and so I think including the name of the module that the vulnerability relates to will make it easier for us to find specific vulnerability issues once there are many of them.\r\n\r\n(However, I must admit I'm not sure how to incorporate that extra requirement into this pipeline... making this more advanced might require writing this in a different language that has better support for manipulating data structures, and so maybe better to wait and see if we need it before making things even more complicated.)\r\n",
        "An alternative compromise I thought of is to make sure that the code which searches for an existing issue can tolerate extra text having been added manually to the issue title after it was automatically created, and then we could edit the title with a small amount of additional context after we have reviewed the report and understood what it affects, as long as we leave the vulnerability number in the issue title when we edit it. \r\n\r\nDo you think that would work?\r\n\r\n",
        "I am particularly interested in being able to include something in the issue _title_ because that makes it easier to view many issues together in the main GitHub issues list UI, and in the \"Project\" view that we currently use for situations like triage.\r\n\r\nI don't want to have to open many different tabs to find the comments that distingish a bunch of issues whose titles are all \"GO-NNNN-NNNN reported\". However, now that you've confirmed that it's okay to modify the title as long as we preserve the keywords from the search query, I think this concern is resolved for me: I imagine that the person who responds to an issue created by this automation would add more text to the title (preserving the \"GO-NNNN-NNNN reported\" prefix) summarizing the problem that the upstream advisory described.\r\n\r\n---\r\n\r\nIn the associated RFC I had been imagining that the automated system would create issues and then [we would _manually_ create advisories based on those issues](https://github.com/opentofu/opentofu/blob/754d7eb58b737d0214f5811320e59c78445e0646/rfc/20250314-security-patch-policy.md#sharing-our-conclusions) once we've reviewed them and addressed any problems. In that sense, the issue represents the need to do the work while the advisory represents the _result_ of that work.\r\n\r\nI honestly hadn't considered the possibility of directly creating a draft advisory using automation; that does seem like an interesting idea but since you've already found the API for that is annoying to work with I don't personally have any problem with retaining the original idea of using issues as the primary way for the automation to communicate with us. At least, we could start with that and see if it seems worth doing the extra work to interact with the security advisory API.\r\n"
      ],
      "opentofu-structure-tests-thoroughly": [
        "One way to avoid the potential pitfall here would be to have this function also take a `t *testing.T` argument and then use `t.TempDir()` to get a directory to place all of the files into. The Go test harness will automatically clean those up itself when the test ends.\r\n\r\nI think we can call `t.TempDir` just once before the loop and then use `filepath.Join` on each iteration to append a filename derived from `filepath.Base(file.filePath)` (assuming that `filePath` is set) to that single temporary directory, which would then have the nice advantage of the temporary files having a more meaningful filename in case they appear in any test failure messages.\r\n\r\nHaving a `*testing.T` in here also means that we can use `t.Fatal` instead of `log.Fatal` so that an error will only abort the current test, rather than aborting the entire test run. Make sure to call `t.Helper()` as the first statement in the function so that the test harness knows that it should treat this as a test helper function when it is choosing a source line to report in its generated error message.\r\n\r\n(Similar idea for the `testFile.tempFileWriter` method below too, though of course that one will need to make a separate temp directory for each file since it only works on one file at a time.)",
        "`t.TempDir` is a relatively recent addition to the Go standard library. We have _lots_ of old code that predates it being there and handles temporary directories in some other manual way, so certainly no judgement from me for this being new to you... it's pretty new to _us_ too. :grinning: \r\n\r\nIn case it's interesting for your future endeavors (since I don't think we really need it _here_), there is also the more general [`testing.T.Cleanup`](https://pkg.go.dev/testing#T.Cleanup) which arranges for an arbitrary function to be called at the conclusion of the calling test, and so this can be used for resources other than temporary directories that nonetheless ought not to outlive the scope of a single test run.\r\n"
      ],
      "opentofu-craft-actionable-errors": [
        "To me the `for_each` part of this at the end is largely irrelevant: it's not valid to refer to an alternative provider configuration that isn't declared _regardless of whether `for_each` is used or not_ so I think mentioning it is likely to cause more confusion than it saves.\r\n\r\nAs things are currently structured it's true that we'll only be reporting the problem this way when there's an instance key specified, but that's just an implementation detail of how this validation is implemented and not relevant to the end-user.\r\n\r\n(Folks reading error messages tend to think that everything they read is directly related to the problem at hand, so including extraneous details tends to cause people to try to solve the wrong problem.)\r\n",
        "Perhaps, but what are you expecting that the reader of this message would do differently based on that extra clause? It doesn't seem like it adds anything the reader needs to know in order to correct the problem. :thinking: \r\n\r\nAre you worried about the hypothetical person who has a child module specifying `configuration_aliases = [postgresql.by_db]` and is expecting that to allow referring to a multi-instance provider passed from the caller? It is true that currently that situation ends up here too, but I don't think that the extra clause you added actually helps diagnose it, because it doesn't say anything at all about `configuration_aliases`...\r\n\r\n\r\n\r\n",
        "OpenTofu error messages don't usually say \"please\", so I'd suggest the following tweak for similarity to the existing \"diagnostic voice\":\r\n\r\n> The \"deprecated\" argument must not be empty, and should provide instructions on how to migrate away from usage of this deprecated output value.\r\n\r\n\r\n\r\n",
        "The problem here is that because OCI was originally intended only for distributing container images there was originally no such thing as an \"artifact type\" -- container images were the only artifact type.\r\n\r\nSome projects adopted OCI for non-container-image artifacts before the OCI spec was updated to include the \"artifact type\" concept, and so the standard image layout for those projects does not include an explicit artifact type at all, and instead software \"guesses\" the artifact type based on other information in the manifest.\r\n\r\nThe fact that there's an optional `artifactType` property in the manifest format is an implementation detail of the protocol that I don't expect the reader of this error message to be aware of, and so I think it would be confusing to talk about it in the error message. Instead, this generic error message is attempting to talk about this in a way that's more likely to be relevant to the end-user: this is an artifact of a type we don't support. It's unfortunate that we can't say what type of artifact it is, but it does have an _implied_ artifact type (based on data in the manifest), so I think it would be confusing to say that this artifact does not have a type _at all_.\r\n\r\nThe main way someone would encounter this message is if they've selected a container image instead of an OpenTofu provider image, so this seems like a good situation for the error message to include a question-shaped suggestion:\r\n\r\n```\r\nunsupported OCI artifact type; is this a container image, rather than an OpenTofu provider?\r\n```\r\n\r\nPhrasing it as a question allows us to present a likely explanation even though we don't know for certain whether it's true. For example, the selected artifact might actually be a Helm chart since those _also_ don't use `artifactType`, but that seems considerably less likely for someone to select by accident.\r\n\r\nI'm going to change the error message to something similar to the above. Hopefully that's a reasonable compromise?\r\n",
        "I did consider this, but these digests are quite long and so can make these messages hard to read, and if we get to this point there are only a small number of explanations for this unlikely error message:\r\n\r\n1. The connection to the OCI registry server was interrupted somehow in a way that caused the received data to be truncated.\r\n2. The OCI registry server is implemented incorrectly and so is returning the wrong data.\r\n3. The OCI registry server has been compromised by an attacker and so is _intentionally_ returning the wrong data, in the hope that some client software won't actually verify the digest.\r\n\r\nOf these three explanations, only in the second case could the digest of the content we actually received be _potentially_ useful, and even then only if the reader of the error message already knows of some content with that digest that might've been somehow returned by mistake, but even that seems quite unlikely. Therefore I chose to focus this error message only on describing what OpenTofu was _intending_ to fetch, because that's an identifier that the reader of the error message can potentially try to retrieve themselves using other software to try to understand what has happened.\r\n\r\nTherefore I think the shape of the error message I wrote makes the best compromise of giving the reader the information needed to debug further, without the confusion of including two very long strings of opaque gibberish where one of them is unlikely to actually communicate anything useful.\r\n\r\nDoes that seem like a reasonable justification?",
        "By the time we reach this point the provider installer will already have reported which version it has selected for each provider -- that happens after calling `AvailableVersions` and before calling `PackageMeta` -- so we don't need to restate it in the individual error messages like this.\r\n",
        "```suggestion\r\n\t\t\t\tSummary:  \"Reference to undeclared key provider\",\r\n\t\t\t\tDetail:   fmt.Sprintf(\"There is no key_provider %q %q block declared in the encryption block.\", depType, depName),\r\n```\r\n\r\nTo make this more consistent with [the similar message for resource references](https://github.com/opentofu/opentofu/blob/35368d990955f225d0fd72ba6409563c992733e1/internal/tofu/evaluate.go#L658-L664).\r\n"
      ],
      "opentofu-explicit-versus-dynamic-configurations": [
        "I am still a little concerned that we're presumably going to need to commit to `main` every time our set of supported versions changes, thereby causing risk that we forget about it and creating clutter in the git history that's unrelated to the actual product we're building, but I also see that this changes infrequently enough that it'll probably be fine. :+1: \r\n\r\nIt's a little awkward that the `main` branch switches to tracking the next release at the time we release our first beta, but the oldest supported version reaches end of life only when we make  the _final_ release for a new series, so I guess we'll need to update this as a separate step once we release v1.10.0 final, rather than including it as part of the work to create the v1.10 branch and have `main` start to represent v1.11 development. If we find that we tend to forget to do this in future releases then I'd consider that a prompt to revisit this decision and consider a different approach, but I think it's fine to wait to see what happens.\r\n"
      ],
      "opentofu-minimize-api-surface": [
        "Does this type need to be exported?\r\n\r\nI see that it's used in some of the tests in `package tofu` but I wonder if we could encapsulate this a little better so that only `package marks` needs to know how this works internally. For example, we could export from _this_ package a function that produces a function that matches the signature used by the last argument of `tfdiags.Override`, and then use that function directly as the argument:\r\n\r\n```go\r\nfunc DeprecatedOutputDiagnosticOverride(cause DeprecationCause) func() tfdiags.DiagnosticExtraWrapper {\r\n    return func () tfdiags.DiagnosticExtraWrapper {\r\n        return &DeprecatedOutputDiagnosticExtra{\r\n            Cause: cause,\r\n         }\r\n    }\r\n}\r\n```\r\n\r\nHonestly the design of `tfdiags.Override` is kinda complicated and maybe one day we can find a way to simplify it a little so this extra function isn't needed, but hopefully something like the above could at least keep most of the details encapsulated in `package marks`?\r\n\r\n(This is not super important, so if this is particularly hard to do then I'm okay with not bothering. Just trying to think about ways to minimize how much exported API surface we have in each package.)\r\n",
        "One thing I noticed looking at this signature is that it only accepts a single file in each case. I suppose this is okay, but it's a little inconsistent with `-var-file` where we allow any mix of any number of `-var` and `-var-file` options on the command line.\r\n\r\nI expect we could extend this to support multiple `-target-file` or `-exclude-file` options later without any backward-compatibility problems and so starting with the simpler case is probably fine... but I figured I'd raise it now in case you think that the more flexible design is straightforward enough to implement immediately.\r\n\r\n"
      ],
      "opentofu-document-reference-standards": [
        "I don't think we explicitly decided this and so it's just been informal, but: historically the pattern was to link to one or more pull requests that represent the work the changelog entry described, but when we introduced the idea of tracking issues for larger projects it became easier to create just one link to the tracking issue rather than many links to all of the PRs that happened because of it. A tracking issue is not the same thing as a feature request or bug report issue despite the fact that we use the same GitHub features for all three: a tracking issue is directly describing some specific work to be done, rather than a broad problem to be solved somehow.\r\n\r\nIn simpler cases where there's only one PR associated with a single enhancement/bug issue it's less clear to me what we ought to do. The PR is a direct description of what changed and so that feels most intuitive to me given that the changelog is a log of changes and the PRs should link to their corresponding issues anyway.\r\n\r\nBut in practice our issues tend to describe changes in more end-user-oriented terms, so I can see the argument that they might be the more useful reference if you just want to know what you can do now that you could not do before. 🤔\r\n\r\nWe should probably discuss this in a different setting so we can think about the tradeoffs together as a team. I would suggest we don't block this PR on this question since we can always edit the changelog before release of we decide to do something different."
      ],
      "opentofu-document-intent-and-limitations": [
        "Unfortunately the _published_ versions of this spec seem to be available only as documents attached to GitHub releases, and the server-side for those URLs returns headers that force treating the file as a \"download\" rather than rendering it directly in the browser, so I wasn't sure how best to link to them.\r\n\r\nHowever, looking again today I notice that the specification text in the repository is also available under a git tag corresponding to the release, and so I guess this link is a plausible reference:\r\n\r\nhttps://github.com/opencontainers/distribution-spec/blob/v1.1.0/spec.md#pushing-manifests\r\n\r\n(The recommendation comes from the last paragraph of that section.)\r\n\r\nI'll push a new version with this link included. Thanks!\r\n",
        "I know this outside the scope of what you were doing here, but the comment on this `DecodeConfig` function says that it's only used in tests but I can see a call to it in `configs.loadConfigFile` that seems to disagree, so maybe worth correcting that comment while you're working in this area anyway?\r\n",
        "Suggest replacing this with some words in this function's doc comment that are clear that the scope of this function is intentionally limited only to simple references and function calls with arguments that are simple references, and that everything else always returns `false`, to make it clear that we are not expecting to continue adding to this each time HCL adds a new feature upstream.\r\n"
      ],
      "opentofu-names-preserve-cognitive-context": [
        "Very minor: In the spirit of [Naming convention for internal variables representing \"contexts\"](https://github.com/opentofu/opentofu/blob/1421849989cf4a65d72db8af107c0b5f70be8c0e/rfc/20250108-naming-convention-for-context-vars.md) (whose main motivation that we try to use `ctx` only for `context.Context` values), I suggest that we rename this `ctx` argument to `hclCtxFunc` so that it's easier to quickly understand both that it returns `*hcl.EvalContext` (rather than `context.Context`) and that it's a function that returns the context rather than the context directly.\r\n\r\n(Maybe we could rename the `ContextFunc` type to `HCLContextFunc` too for consistency, though since that's exported that might be more invasive in which case I'd suggest that we save it for a later PR.)\r\n"
      ],
      "opentofu-prevent-backing-array-surprises": [
        "While I agree with removing this `nolint` comment, I do find these two statements a little suspicious:\r\n\r\nIf `len(cmd) == cap(cmd)`  here then this would cause `encryptCommand` and `decryptCommand` to both have newly-allocated backing arrays that both share a prefix with `cmd`.\r\n\r\nBut if `len(cmd) < cap(cmd)` then I think these would both end up with the same backing array and the last item set to `--decrypt`, which is presumably wrong.\r\n\r\nBoth callers of this function seem to populate `cmd` using a composite literal:\r\n\r\nhttps://github.com/opentofu/opentofu/blob/ec4e0cf0e2cefcca1d2ace792416c4e584c440a6/internal/encryption/method/external/testmethod/testmethod.go#L58\r\n\r\nhttps://github.com/opentofu/opentofu/blob/ec4e0cf0e2cefcca1d2ace792416c4e584c440a6/internal/encryption/method/external/testmethod/testmethod.go#L76\r\n\r\n...and the Go spec doesn't seem to specify what the capacity is of a slice created with a slice literal, but some experimentation on the Go playground suggests that, at least for small slices, the composite literal syntax currently allocates a backing array exactly the right size for the length of the slice, which I think is saving us from the bug that the linter was trying to warn about here. Therefore this currently works but is at risk of becoming accidentally broken in future if the callers start building these slices in a different way.\r\n\r\nOne way to resolve this while still retaining a similar code style would be to explicitly remove any excess capacity from the source slice before appending to it, and now we've upgraded to Go 1.24 there's [a helper function that's (subjectively) more readable than the three-clause slice syntax](https://pkg.go.dev/slices#Clip):\r\n\r\n```go\r\ncmd = slices.Clip(cmd)\r\nencryptCommand := append(cmd, \"--encrypt\")\r\ndecryptCommand := append(cmd, \"--decrypt\")\r\n```"
      ],
      "opentofu-clear-concise-documentation": [
        "Let's make sure that these consistently use the │prefix on all lines -- either including it or omitting it -- since I think right now it's a little unclear whether the first line of each of these is part of the error message or something else. (OpenTofu CLI includes these markers to help sighted users more quickly notice the boundaries between different diagnostic messages, using visual hierarchy.)\r\n\r\nI'd suggest also generating these with your terminal set to about 72-ish columns wide so that the hard-wrapping will generate shorter lines that should be easier to read on narrower screens without a lot of horizontal scrolling. (I must admit I'm not sure what is a good width for the current way `opentofu.org` is laid out on narrow screens; 72 is just an arbitrary number that we previously used for some other error messages.)\r\n",
        "FWIW, the prevailing writing style for the docs avoids things like \"Please\" and prefers to just directly assert facts, instructions, or recommendations, and so I agree with the comment above and would personally broaden that to _all_ of our documentation, rather than just this one example.\r\n\r\nI don't have a strong opinion on \"use caution\" vs. \"be cautious\". They both seem valid.\r\n\r\nThe only example I could find of something _close_ to this in the existing docs is:\r\n\r\nhttps://github.com/opentofu/opentofu/blob/ff4c84055065fa2d83d318155b72aef6434d99e4/website/docs/language/modules/develop/refactoring.mdx?plain=1#L428\r\n\r\n...which does seem closer to \"use caution\" than \"be cautious\", but isn't exactly the same as either!\r\n\r\n",
        "Yes, that is correct.\r\n\r\nI was reluctant to make this text very long by giving lots of examples because the implementation is already built to give direct feedback about this if an author uses it incorrectly.\r\n\r\nThe fact that you correctly understood the intention without any additional words about it makes me think we should try too keep this as it is currently written and wait to see if others find it confusing in practice. It's been my experience that readers tend to skip unnecessarily long documentation, and so I'd like to try with only one example to see if this is sufficient before we make it longer.\r\n\r\nWhat you think?"
      ],
      "opentofu-specify-configuration-behaviors": [
        "Given that everything we add in a stable release is forever, I would strongly recommend _not_ adding things that don't have a clear use-case just because they seem easy to implement."
      ],
      "opentofu-proper-span-lifecycle": [
        "The promised behavior for ORAS-Go is that it always calls `ExecuteDone`, whether the execution succeeds or not, and [the current implementation](https://github.com/oras-project/oras-go/blob/753f8a8d98a5ed950366b5580c7edbd204fa5630/registry/remote/credentials/internal/executer/executer.go#L57-L64) seems straightforward and clearly correct in that regard, so I'm personally not concerned about this.\r\n\r\nThe Go OTel tracing documentation doesn't specify whether it's okay to call `span.End` multiple times, but if you know of some other source that promises that multiple calls are always okay then I'd be happy to _also_ include a `defer` for robustness, but in the normal case I think the `span.End` ought to be inside `ExecuteDone` because otherwise this span will be measuring more than what it claims to be measuring, and in that case it'd probably be better to just skip having it at all since it wouldn't be meaningfully different than the parent span. :thinking: \r\n"
      ]
    },
    "profile": {
      "location": "Portland, OR",
      "blog": "http://martin.atkins.me.uk/",
      "site_admin": false,
      "followers": 1395,
      "following": 41
    }
  },
  "lzchen": {
    "repos": [
      "open-telemetry/opentelemetry-python"
    ],
    "entries": [
      {
        "slug": "opentelemetry-python-adapt-for-linter-compatibility",
        "title": "Adapt for linter compatibility"
      },
      {
        "slug": "opentelemetry-python-choose-data-structures-wisely",
        "title": "Choose data structures wisely"
      },
      {
        "slug": "opentelemetry-python-configuration-source-precedence",
        "title": "Configuration source precedence"
      },
      {
        "slug": "opentelemetry-python-follow-python-naming-conventions",
        "title": "Follow Python naming conventions"
      },
      {
        "slug": "opentelemetry-python-future-proof-api-design",
        "title": "Future-proof API design"
      },
      {
        "slug": "opentelemetry-python-handle-exceptions-appropriately",
        "title": "Handle exceptions appropriately"
      },
      {
        "slug": "opentelemetry-python-maintain-consistent-naming",
        "title": "Maintain consistent naming"
      },
      {
        "slug": "opentelemetry-python-optimize-code-location-scope",
        "title": "Optimize code location scope"
      },
      {
        "slug": "opentelemetry-python-optimize-configuration-structure",
        "title": "Optimize configuration structure"
      },
      {
        "slug": "opentelemetry-python-precise-configuration-specifications",
        "title": "Precise configuration specifications"
      },
      {
        "slug": "opentelemetry-python-prevent-recursive-logging-calls",
        "title": "Prevent recursive logging calls"
      },
      {
        "slug": "opentelemetry-python-sanitize-observability-data-exports",
        "title": "Sanitize observability data exports"
      },
      {
        "slug": "opentelemetry-python-structured-changelog-documentation",
        "title": "Structured changelog documentation"
      },
      {
        "slug": "opentelemetry-python-telemetry-version-pinning",
        "title": "Telemetry version pinning"
      },
      {
        "slug": "opentelemetry-python-write-purposeful-comments",
        "title": "Write purposeful comments"
      }
    ],
    "comments": {
      "opentelemetry-python-configuration-source-precedence": [
        "Would this be as a large of a change as let's say...http semantic convention from old to new? Given that this is similarly a beta component (like the instrumentations), I am open to simply making this change and treating it as a specification change (and breaking customers) but if we feel the amount of usage also warrants an opt-in mechanism I am also open to that (although leaning towards just making the change).",
        "I think we covered this a bit in the SIG. It would be great to have this behavior be configurable with custom implementations of the distro. So instead of override a private `_initialize_components` function, we expose this through `configure` so distros can have dictate their own behavior. It also makes sense then to use `kwargs` in this case.",
        "Should this be additive or should we have the in-code configuration take priority and replace entrypoints similar to how we override env var? Would users be surprised by the \"default\" behavior?"
      ],
      "opentelemetry-python-structured-changelog-documentation": [
        "Are we leaving this blank?",
        "Nit: Include description of why we are adding `Final`"
      ],
      "opentelemetry-python-choose-data-structures-wisely": [
        "Should the increment to `measurements_seen` occur AFTER finding the index? Otherwise we tend to skip the first bucket?"
      ],
      "opentelemetry-python-prevent-recursive-logging-calls": [
        "+ 1 to this. Wdyt about using `warnings` instead of logger for other places in which are common to have recursions?"
      ],
      "opentelemetry-python-write-purposeful-comments": [
        "Could you clarify in the docstring here that `reset` is used for resetting any stateful logic after a collection cycle?",
        "Nit: Maybe put a comment here to explain this logic. We are iterating through `self.__attributes` which contains the entire set of original attributes from the recorded `Measurement` and saving only the remaining attributes after filtering out the keys from the post-view filtered attributes.",
        "Nit: Might be nice to add a comment to link to [this](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md#type-any) in the specs."
      ],
      "opentelemetry-python-future-proof-api-design": [
        "Does this mean that users must explicitly \"convert\" a histogram to a timing histogram to use it as a context manager?"
      ],
      "opentelemetry-python-telemetry-version-pinning": [
        "Maybe let's use `opentelemetry-api~=1.25`?",
        "Probably want to do `opentelemetry-api~=1.25` not `opentelemetry-api~=1.25.0`. This will help catch any bugs we release in minor versions."
      ],
      "opentelemetry-python-follow-python-naming-conventions": [
        "@vivek378521 \r\n\r\nHave you made the change for this? It still says `Optional[dict]` instead of `Attributes` for typing.",
        "Should we stick with `trace_exporter_names` instead?",
        "We call them \"trace_exporter\" because user is configuring what exporter they want under the \"trace\" flag. As well, the entry points are using `opentelemetry_traces_exporter`.",
        "As discussed in the 5/30 SIG, `traces` makes more sense because we are passing it in as a flag in `opentelemetry-instrument`, we use the term `traces_exporter` for our exporter [entrypoints](https://github.com/open-telemetry/opentelemetry-python/blob/main/exporter/opentelemetry-exporter-zipkin-json/pyproject.toml#L35) and I believe the idea was to use the signal types (traces, metrics, logs) for names."
      ],
      "opentelemetry-python-sanitize-observability-data-exports": [
        "> The resulting unit SHOULD be added to the metric as [UNIT metadata](https://github.com/OpenObservability/OpenMetrics/blob/main/specification/OpenMetrics.md#metricfamily) and as a suffix to the metric name unless the metric name already contains the unit, or the unit MUST be omitted.\r\n\r\nDoesn't the name need to have the unit appended as a suffix if it doesn't exist already in the name?",
        "> The resulting unit SHOULD be added to the metric as [UNIT metadata](https://github.com/OpenObservability/OpenMetrics/blob/main/specification/OpenMetrics.md#metricfamily) and as a suffix to the metric name unless the metric name already contains the unit, or the unit MUST be omitted.\r\n\r\nJust to clarify, we will be checking the existence of a SANITIZED and valid unit name in a sanitized and valid metric name to determine whether or not to omit the appending of the suffix correct?\r\n\r\nThe current logic makes sense to me, just wondering if there is ever a case where the unit can be found already in the metric name in which case we DON'T have to strip leading `_` from the unit name? Not a blocker, just a small question.",
        "> This may cause ambiguity in scenarios where multiple similar-named attributes share invalid characters at the same location. In such unlikely cases, if multiple key-value pairs are converted to have the same Prometheus key, the values MUST be concatenated together, separated by ;, and ordered by the lexicographical order of the original keys.\r\n\r\nAre we going to be addressing this use case in the future?",
        "Forgive me for my lack of understanding of how Prometheus backend works but it looks like they accept a list of keys and values and they expect the indices of the pairs (key, value) to line up to represent a label. I'm not sure how sorting the original dictionary solves this issue? Isn't the logic simply pairing off each key/value?"
      ],
      "opentelemetry-python-optimize-configuration-structure": [
        "Any reason why we are renaming this envs?",
        "@xrmx \r\n\r\nHas this been resolved offline? What was the verdict?"
      ],
      "opentelemetry-python-optimize-code-location-scope": [
        "I changed this so it follows more like how API modules are structured.\r\nThere is a `util` folder with now with `instrumentation.py` as a separate module."
      ],
      "opentelemetry-python-handle-exceptions-appropriately": [
        "Also perhaps link the issue in the comments.",
        "Nit: Shouldn't the exception handling occur within the `_readfile` function instead of the below?",
        "@sandy2008 \r\n\r\nI'm curious as to why we have to reraise the error at the higher level? Can't we remove the try except in `_load_credentials` just let `_read_file` handle the exception + return `None`?",
        "Could you keep the exception handling as is?",
        "Design-wise that might be the case but changing this would be a breaking change. I would suggest keeping it as is and maybe down the road we can explore user experience options via configuration."
      ],
      "opentelemetry-python-adapt-for-linter-compatibility": [
        "I'm not too familiar with jinja. What does this change do?"
      ],
      "opentelemetry-python-maintain-consistent-naming": [
        "```suggestion\r\n- Rename metric handle to bound metric instrument\r\n```"
      ],
      "opentelemetry-python-precise-configuration-specifications": [
        "Should we also ignore the previous files that we ignored like `.tox` and `.venv`?"
      ]
    },
    "profile": {
      "location": "Redmond, WA",
      "company": "Microsoft Corporation",
      "blog": "",
      "site_admin": false,
      "followers": 45,
      "following": 0
    }
  },
  "jmorganca": {
    "repos": [
      "ollama/ollama"
    ],
    "entries": [
      {
        "slug": "ollama-ai-dependency-decoupling-strategy",
        "title": "AI dependency decoupling strategy"
      },
      {
        "slug": "ollama-clear-recoverable-error-messages",
        "title": "Clear recoverable error messages"
      },
      {
        "slug": "ollama-complete-null-checks",
        "title": "Complete null checks"
      },
      {
        "slug": "ollama-comprehensive-test-coverage",
        "title": "Comprehensive test coverage"
      },
      {
        "slug": "ollama-descriptive-balanced-naming",
        "title": "Descriptive balanced naming"
      },
      {
        "slug": "ollama-extract-duplicated-code",
        "title": "Extract duplicated code"
      },
      {
        "slug": "ollama-follow-godoc-conventions",
        "title": "Follow GoDoc conventions"
      },
      {
        "slug": "ollama-guard-against-nil",
        "title": "Guard against nil"
      },
      {
        "slug": "ollama-loose-api-coupling",
        "title": "Loose API coupling"
      },
      {
        "slug": "ollama-optimize-ai-implementation-patterns",
        "title": "Optimize AI implementation patterns"
      },
      {
        "slug": "ollama-optimize-memory-allocations",
        "title": "Optimize memory allocations"
      },
      {
        "slug": "ollama-optimize-with-standard-library",
        "title": "Optimize with standard library"
      },
      {
        "slug": "ollama-platform-aware-configuration-documentation",
        "title": "Platform-aware configuration documentation"
      },
      {
        "slug": "ollama-purpose-reflecting-file-names",
        "title": "Purpose-reflecting file names"
      },
      {
        "slug": "ollama-reuse-buffers-strategically",
        "title": "Reuse buffers strategically"
      },
      {
        "slug": "ollama-use-idiomatic-go-flow",
        "title": "Use idiomatic Go flow"
      }
    ],
    "comments": {
      "ollama-reuse-buffers-strategically": [
        "this allocation can be one time?\r\n\r\n```\r\ntype weighted struct {\r\n\trng        *rand.Rand\r\n\ttransforms []transform\r\n\tbuf        []tokenInfo // reuse buffer\r\n}\r\n\r\nfunc (s *weighted) Sample(logits []float32) (int32, error) {\r\n\tif cap(s.buf) < len(logits) {\r\n\t\ts.buf = make([]tokenInfo, len(logits))\r\n\t}\r\n\ttokens := s.buf[:len(logits)]\r\n\t// rest of your logic\r\n}\r\n```"
      ],
      "ollama-complete-null-checks": [
        "@ParthSareen yes this should be in the `if (g != nullptr) {` block",
        "I would return a regular `std::string` here to avoid issues if the string gets deallocated or changed later by the caller\r\n\r\n```suggestion\r\nconst std::string ollama_vocab::token_to_piece(uint32_t token) {\r\n```"
      ],
      "ollama-descriptive-balanced-naming": [
        "Some of the variable and function names here are very long. Ideally 1 nameComponent, 2 if needed for clarity, but 3 is often a sign that better naming or structure can be used.\r\n\r\n```\r\nprefix, ok := toolPrefix(model.Template.Template)\r\nif !ok {\r\n  return nil\r\n}\r\n```",
        "```suggestion\r\n\tUseAuth = Bool(\"OLLAMA_AUTH\")\r\n```\r\n\r\nnit on potentially a more consistent name since we don't have `_USE_` anywhere else"
      ],
      "ollama-follow-godoc-conventions": [
        "I _think_ generally the doc for functions should start with the function name, e.g.\r\n\r\n```suggestion\r\n\t// SetLayer sets the active layer of the cache\r\n```",
        "More: https://go.dev/doc/comment",
        "A GoDoc comment here would be great! Since there are some preconditions to running this.",
        "What is `name` and `arguments` here? Are those the formats? I would add them to the GoDoc for this function. I'm not sure what their purposes is from reading the comment.\r\n",
        "Add comment as to why it's passed in by pointer here. Something like the length of the slice is modified to k",
        "pedantic but should we make these GoDoc comments above the fields?\r\n\r\n```\r\n//MainGPU is the gpu where xyz...\r\nMainGPU int\r\n```\r\n"
      ],
      "ollama-optimize-ai-implementation-patterns": [
        "There may be multiple eog tokens, specifically EOS or EOT"
      ],
      "ollama-purpose-reflecting-file-names": [
        "Yes, if this stays as a root level `Makefile` it should also build the project, ideally with GPU support"
      ],
      "ollama-optimize-memory-allocations": [
        "Thanks for the PR!\r\n\r\nWould it be possible to keep the argument based return value?"
      ],
      "ollama-loose-api-coupling": [
        "This is over coupling sampling with the `api.Options` type. Looser coupling is usually better. Much better that it accepts the raw float32/int values instead of passing `req` even deeper",
        "Instead of a list of transforms (`[]Transform`), why don't we just make helper functions like `topP` `topK` `softmax` and call them in the fixed order we use here (given the API is fixed)?"
      ],
      "ollama-guard-against-nil": [
        "Should we `ok` check this?",
        "As-is is ok if it can't happen",
        "Let's avoid the case where this is nil\r\n\r\n```\r\nvar vocab *model.Vocabulary\r\nif tp, ok := s.model.(model.TextProcessor); ok {\r\n  vocab = tp.Vocab()\r\n}\r\n\r\nif req.Grammar != \"\" && vocab != nil {\r\n\r\n}\r\n```",
        "Good catch. I'll remove the pointer.",
        "I wonder if there's a way we could get around the type assertion here via some other interface/struct design (maybe in a later PR as this all takes shape). Similar idea with `model.TextProcessor`."
      ],
      "ollama-use-idiomatic-go-flow": [
        "Nit: we can short circuit return above instead of the else",
        "nit/optional, but we could return early here\r\n\r\n```\r\nif c.windowSize == math.MaxInt32 {\r\n    return\r\n}\r\n\r\n...\r\n```\r\n"
      ],
      "ollama-platform-aware-configuration-documentation": [
        "It may be simpler to keep this `Windows` but to mention in a warning below acceleration libraries aren't currently supported for ARM (vs two sections)",
        "The top of the doc (perhaps a bit too easy to glance over) has a link to TDM-gcc which works pretty well.",
        "(and llvm-mingw for ARM)"
      ],
      "ollama-clear-recoverable-error-messages": [
        "```suggestion\r\nvar ErrNoVisionModel = errors.New(\"this model is missing data required for image input\")\r\n```",
        "Actually, this message is more of edge case vs a capability, it's great as is",
        "We may want to hint at missing data or similar. E.g. `this model is missing data required for image input`"
      ],
      "ollama-ai-dependency-decoupling-strategy": [
        "Yeah! Turned out to be little code, however the tradeoff is we have to keep using `llama` code for now. It's very important we work to remove this to use new engine tokenizers instead as this restricts us to loading GGUF files with specific KV keys. cc @ParthSareen "
      ],
      "ollama-comprehensive-test-coverage": [
        "This is a great start. I would take out the concept of tool tokens and have cases like:\r\n\r\n* valid prefix, invalid json\r\n* invalid prefix, valid json\r\n* valid prefix, valid json\r\n* 2+ objects\r\n\r\netc",
        "Great start on the tests! It will be quite a bit of tests, but we should also test the intermediate tool parsing steps (e.g. finding the template, finding the prefix/tool tokens) since we're bound to find edge cases in new models (vs these more top-down tests)",
        "Add tests for `0`",
        "No, we want to test this \"bottom-up\", testing 0 (or maybe a very very small number) here will ensure we have code coverage for `max(temp, 1e-7)`, which is different than testing to see if greedy is used (sample function)"
      ],
      "ollama-extract-duplicated-code": [
        "is `skipStartOfTurn` needed?"
      ],
      "ollama-optimize-with-standard-library": [
        "Given how rare this is, why don't we just use built-in sort for now. It will simplify the code a lot:\r\n\r\n```\r\n\tif k >= len(ts) {\r\n\t\tslices.SortFunc(ts, func(a, b token) int {\r\n\t\t\tswitch {\r\n\t\t\tcase a < b:\r\n\t\t\t\treturn -1\r\n\t\t\tcase a > b:\r\n\t\t\t\treturn 1\r\n\t\t\tdefault:\r\n\t\t\t\treturn 0\r\n\t\t\t}\r\n\t\t})\r\n\t\treturn ts\r\n\t}\r\n```",
        "This should be able to replace `partialSortLogits` and `sortLogits` below",
        "I know we passed on it previously, but it's worth seeing if we can use `container/heap`  ?\r\n\r\n````\r\nimport \"container/heap\"\r\n\r\n// tokenHeap implements heap.Interface\r\ntype tokenHeap []tokenInfo\r\n\r\nfunc (h tokenHeap) Len() int           { return len(h) }\r\nfunc (h tokenHeap) Less(i, j int) bool { return h[i].logit < h[j].logit } // min-heap based on logit\r\nfunc (h tokenHeap) Swap(i, j int)      { h[i], h[j] = h[j], h[i] }\r\n\r\nfunc (h *tokenHeap) Push(x any) {\r\n\t*h = append(*h, x.(tokenInfo))\r\n}\r\n\r\nfunc (h *tokenHeap) Pop() any {\r\n\told := *h\r\n\tn := len(old)\r\n\titem := (*h)[n-1]\r\n\t*h = (*h)[:n-1]\r\n\treturn item\r\n}\r\n\r\n### Explanation:\r\n- `Pop` should remove and return the last element of the slice after shrinking it.\r\n- This properly maintains the heap's invariants defined by the other methods (`Len`, `Less`, `Swap`).\r\n\r\n### Usage example:\r\n\r\n```go\r\nh := &tokenHeap{}\r\nheap.Init(h)\r\n\r\n// Push elements onto the heap\r\nheap.Push(h, tokenInfo{id: 1, logit: 0.9})\r\nheap.Push(h, tokenInfo{id: 2, logit: 1.2})\r\n\r\n// Pop smallest element\r\nsmallest := heap.Pop(h).(tokenInfo)\r\n```\r\n````",
        "Let's use https://pkg.go.dev/slices#BinarySearchFunc vs re-implementing binary search",
        "I believe you can use `slices.Indexfunc` instead of this dependency, something like:\r\n\r\n```\r\nimport \"slices\"\r\n\r\nfunc maxIdx(vals []float64) int {\r\n    if len(vals) == 0 {\r\n        return -1\r\n    }\r\n    maxVal := slices.Max(vals)\r\n    return slices.IndexFunc(vals, func(v float64) bool {\r\n        return v == maxVal\r\n    })\r\n}\r\n```"
      ]
    },
    "profile": {
      "company": "@ollama",
      "blog": "ollama.com",
      "twitter_username": "jmorgan",
      "site_admin": false,
      "followers": 2440,
      "following": 1
    }
  },
  "markdalgleish": {
    "repos": [
      "remix-run/react-router"
    ],
    "entries": [
      {
        "slug": "react-router-avoid-redundant-computations",
        "title": "avoid redundant computations"
      },
      {
        "slug": "react-router-avoid-timing-dependent-tests",
        "title": "avoid timing-dependent tests"
      },
      {
        "slug": "react-router-configuration-consistency-standards",
        "title": "configuration consistency standards"
      },
      {
        "slug": "react-router-configure-build-tools-properly",
        "title": "configure build tools properly"
      },
      {
        "slug": "react-router-dependency-version-ranges",
        "title": "dependency version ranges"
      },
      {
        "slug": "react-router-document-configuration-rationale",
        "title": "Document configuration rationale"
      },
      {
        "slug": "react-router-documentation-clarity-standards",
        "title": "documentation clarity standards"
      },
      {
        "slug": "react-router-graceful-error-handling",
        "title": "graceful error handling"
      },
      {
        "slug": "react-router-http-protocol-compliance",
        "title": "HTTP protocol compliance"
      },
      {
        "slug": "react-router-implement-recursive-safeguards",
        "title": "Implement recursive safeguards"
      },
      {
        "slug": "react-router-maintain-naming-consistency",
        "title": "maintain naming consistency"
      },
      {
        "slug": "react-router-organize-related-code-together",
        "title": "organize related code together"
      },
      {
        "slug": "react-router-prefer-explicit-readable-constructs",
        "title": "prefer explicit readable constructs"
      },
      {
        "slug": "react-router-provide-explicit-error-handling",
        "title": "Provide explicit error handling"
      },
      {
        "slug": "react-router-remove-obsolete-configuration-options",
        "title": "Remove obsolete configuration options"
      },
      {
        "slug": "react-router-simplify-configuration-setup",
        "title": "Simplify configuration setup"
      },
      {
        "slug": "react-router-typescript-configuration-setup",
        "title": "TypeScript configuration setup"
      },
      {
        "slug": "react-router-use-descriptive-semantic-names",
        "title": "Use descriptive semantic names"
      }
    ],
    "comments": {
      "react-router-avoid-timing-dependent-tests": [
        "This test was flaky in Firefox due to the order of requests not being deterministic."
      ],
      "react-router-use-descriptive-semantic-names": [
        "Updated 👍 "
      ],
      "react-router-maintain-naming-consistency": [
        "FWIW I've just been leaning into calling it `routes.ts` and `app/routes.ts` everywhere to avoid this sort of framing which can be confusing. Instead we could make sure to always link to a page with more detail about how it's more complicated than that?"
      ],
      "react-router-implement-recursive-safeguards": [
        "This code was already targeting export statements specifically, it just wasn't explicitly scoped in the code. Now that we have much more general tracing of dependencies above, we can scope this extra check much more aggressively."
      ],
      "react-router-http-protocol-compliance": [
        "Why did this need to be changed?"
      ],
      "react-router-configuration-consistency-standards": [
        "Now that this error can also be avoided by introducing a custom entry.server.tsx/jsx, I've expanded the error message to make this clear, similar to the error message following this one.",
        "@pcattori Just calling out that since the `--config` flag impacts the project root directory (and hence where routes live etc.), I think it makes sense to support this in all CLI commands for consistency, even those that don't use the Vite config.",
        "Side note, but I think this raises something I might have missed when we introduced `react-router.config.ts`, which is that the `--config` flag probably should have been updated to point to _our_ config, and any custom Vite config path should have been configured there, or via a separate `--vite-config` flag.\r\n\r\nThis is part of my rationale for this change. If our `--config` flag pointed to `react-router.config.ts`, I would want to support it here too.",
        "Note that if a root directory was explicitly provided (e.g. `react-router build my/app/dir`) this will still take precedence. We only fall back to inferring the root when this isn't provided, so anyone relying on this should maintain existing behaviour."
      ],
      "react-router-typescript-configuration-setup": [
        "```suggestion\r\nAfter we decided not to pursue \"zero-effort typesafety\" (as described above), our TypeScript plugin was already a simple passthrough that kicked off typegen as a side-effect.\r\n```"
      ],
      "react-router-graceful-error-handling": [
        "Pairing with @pcattori, we've opted to go with this to keep things moving:\r\n\r\n```ts\r\nerror.stack = process.env.NODE_ENV === \"development\" ? val.stack : \"\";\r\n```\r\n\r\nDeduping feels like a larger task given the current structure so I've left that for now."
      ],
      "react-router-documentation-clarity-standards": [
        "With Vite's RSC support currently being a plugin rather than being built-in, my original use of the word \"native\" doesn't quite apply.\r\n\r\n```suggestion\r\nReact Router provides a set of APIs for integrating with RSC-compatible bundlers, allowing you to leverage [Server Components][react-server-components-doc] and [Server Functions][react-server-functions-doc] in your React Router applications.\r\n```",
        "Nit:\r\n\r\n```suggestion\r\nThe [Route Module API][route-module] up until now has been a [Framework Mode][framework-mode] only feature. However, the `lazy` field of the RSC route config expects the same exports as the Route Module exports, unifying the APIs even further.\r\n```",
        "```suggestion\r\nServer Components can also be returned from your loaders and actions. In general, if you are using RSC to build your application, loaders are primarily useful for things like setting `status` codes or returning a `redirect`.\r\n```",
        "```suggestion\r\nDuring our experiments we realized that we could offload type-safe context to an external package. This would result in a simpler implementation within React Router and avoid the need to try to patch on type-safety to our existing `context` API which was designed as a quick escape hatch to cross the bridge from your server (i.e., `express` `req`/`res`) to the Remix handlers.\r\n```"
      ],
      "react-router-organize-related-code-together": [
        "This would have been force of habit for me, that typically when testing utils like this I prefer having the test right next to the implementation. Do you feel strongly about it always being in a `__tests__` directory?"
      ],
      "react-router-document-configuration-rationale": [
        "This is suppressing dependency warnings that only affect dev dependencies due to test code. Everything still works.",
        "This is optional, but I've opted to keep automatic `pre` and `post` script behaviour so that the repo doesn't change too much in the migration. We can revisit this later.",
        "This is needed so that `pnpm format` doesn't format the lock file. We ran into an issue in the Remix repo where CI got stuck in a loop deduping and formatting `pnpm-lock.yaml` since pnpm and Prettier didn't agree, so this ensures we don't hit that problem again."
      ],
      "react-router-remove-obsolete-configuration-options": [
        "We have a few loose ends like this lying around, so I think we can merge this as-is for now and do a broader cleanup as a separate pass."
      ],
      "react-router-avoid-redundant-computations": [
        "While looking at imports and exports with fresh eyes, I also realised there's room for an early bail out here."
      ],
      "react-router-configure-build-tools-properly": [
        "Now that we have the exports field, we can point to the `dist` folder and remove the hard-coded `install.js` and `install.d.ts` files."
      ],
      "react-router-prefer-explicit-readable-constructs": [
        "Thanks for catching this, I've pushed a refactor that cleans this up."
      ],
      "react-router-dependency-version-ranges": [
        "I think this should be `^4.0.0` so `4.0.x` and `4.1.x` are also valid.",
        "This should be `^3.28.2 || ^4.0.0` so it's backwards compatible.",
        "This was bumped because v3 is the first major version of vite-node to officially support both Vite v5 and v6: https://github.com/vitest-dev/vitest/blob/main/packages/vite-node/package.json#L89. I think it's okay we're running a beta version because we're using vite-node in a very limited capacity to run config files and we don't expose any Vite internals to consumers. However, just to be sure, we run the routes.ts tests against both v5 and v6 of Vite."
      ],
      "react-router-simplify-configuration-setup": [
        "The `modulePaths` option as configured didn't work with pnpm's folder structure. A simpler way to solve the same problem is to simply tell Jest to transform everything.",
        "Good call. I've simplified the script now so that the version specifier is required.\r\n\r\nIn the current package structure, the `isSnapshotVersion` is still required because it's used to detect experimental/nightly releases and ensure that `@remix-run/router` also gets a version update."
      ],
      "react-router-provide-explicit-error-handling": [
        "Review feedback: We should have our own error for when the file is missing rather than letting it fail within Vite when trying to load it.",
        "We should error when the default export is missing entirely."
      ]
    },
    "profile": {
      "location": "Melbourne, Australia",
      "company": "@remix-run",
      "blog": "markdalgleish.com",
      "twitter_username": "markdalgleish",
      "site_admin": false,
      "followers": 3231,
      "following": 80
    }
  },
  "jkotas": {
    "repos": [
      "dotnet/runtime"
    ],
    "entries": [
      {
        "slug": "runtime-avoid-busy-waiting",
        "title": "Avoid busy waiting"
      },
      {
        "slug": "runtime-cache-expensive-computations",
        "title": "Cache expensive computations"
      },
      {
        "slug": "runtime-centralize-platform-configurations",
        "title": "Centralize platform configurations"
      },
      {
        "slug": "runtime-choose-appropriate-error-mechanisms",
        "title": "Choose appropriate error mechanisms"
      },
      {
        "slug": "runtime-choose-descriptive-names",
        "title": "Choose descriptive names"
      },
      {
        "slug": "runtime-document-code-meaningfully",
        "title": "Document code meaningfully"
      },
      {
        "slug": "runtime-document-configuration-intent",
        "title": "Document configuration intent"
      },
      {
        "slug": "runtime-explicit-api-versioning",
        "title": "Explicit API versioning"
      },
      {
        "slug": "runtime-follow-naming-patterns",
        "title": "Follow naming patterns"
      },
      {
        "slug": "runtime-maintain-configuration-compatibility",
        "title": "Maintain configuration compatibility"
      },
      {
        "slug": "runtime-memory-barrier-pairing",
        "title": "Memory barrier pairing"
      },
      {
        "slug": "runtime-memory-ordering-matters",
        "title": "Memory ordering matters"
      },
      {
        "slug": "runtime-model-actual-hardware-costs",
        "title": "Model actual hardware costs"
      },
      {
        "slug": "runtime-names-reflect-actual-purpose",
        "title": "Names reflect actual purpose"
      },
      {
        "slug": "runtime-optimize-aligned-simd-operations",
        "title": "Optimize aligned SIMD operations"
      },
      {
        "slug": "runtime-optimize-common-paths",
        "title": "Optimize common paths"
      },
      {
        "slug": "runtime-optimize-for-readability",
        "title": "Optimize for readability"
      },
      {
        "slug": "runtime-optimize-memory-access",
        "title": "Optimize memory access"
      },
      {
        "slug": "runtime-preserve-pointer-authentication",
        "title": "Preserve pointer authentication"
      },
      {
        "slug": "runtime-simplify-code-expressions",
        "title": "Simplify code expressions"
      },
      {
        "slug": "runtime-specific-exceptions-with-context",
        "title": "Specific exceptions with context"
      },
      {
        "slug": "runtime-structure-configuration-options",
        "title": "Structure configuration options"
      }
    ],
    "comments": {
      "runtime-simplify-code-expressions": [
        "```suggestion\r\n    Volatile<bool> g_GCBridgeActive = false;\r\n```\r\nNit: It is fine to use native C/C++ bool for internal details."
      ],
      "runtime-names-reflect-actual-purpose": [
        "FWIW, the user facing property to enable event pipe (and related features) is called `EventSourceSupport` in NativeAOT (https://learn.microsoft.com/en-us/dotnet/core/deploying/native-aot/diagnostics#observability-and-telemetry).",
        "> Yeah, I find EventSourceSupport confusing for all that Event Pipe / Diagnostic Server can do.\r\n\r\nI agree that it is not the most descriptive name. On the other hand, it gets complicated to explain what works and does not work in the matrix of fine-grained combinations. It is why we have opted to have one config switch that controls it all."
      ],
      "runtime-maintain-configuration-compatibility": [
        "> The two big things are that we have some unused gaps now that could be filled in and I don't see why we need to use unique IDs across X86 and Arm64 (that is X86Base and ArmBase can't currently both be R2R ID 1).\r\n\r\nThis would make r2rdump more complicated. Right now, r2rdump just does Enum.ToString() to dump the instruction set. \r\n\r\nAlso, r2rdump is designed to be able to dump older versions of r2r images. Removing the definitions of the existing enum members breaks that. These removals should be reverted.",
        "> we should be able to do much like we do in NAOT\r\n\r\nNAOT does not have the versioning problem like R2R. There is no mixing and matching of versions, etc.",
        "> Right now we're using 60 bits for R2R, when we should only need 34 total. If we split it by platform then it's 11 for Arm64 and 23 for x64. Just given the pending work around SVE and AVX, we will likely go over 64-bits in the next release and need to take a break anyways.\r\n\r\nReadyToRunInstructionSet enum is not used to form a bit mask. Going above 64 should work just fine."
      ],
      "runtime-choose-appropriate-error-mechanisms": [
        "It is unusual to return error code via out argument, and only allow that error code to be OOM.\r\n\r\nCan we return proper error code, and return the pointer via `[out]` argument instead?\r\n\r\nThe error code can be an HRESULT or an enum specific to this call. (I know HRESULTs are Windows-specific, but they are part of BCL Exception so they are not going away x-plat, so there is not much value in trying hard to avoid them.)",
        "I think it is fine add back what you need to get this to compile. "
      ],
      "runtime-follow-naming-patterns": [
        "This method is not returning anything after this change, so `Get...` name does not fit what it does anymore. \r\n\r\nI would rename it to `PrintHelp`, `DisplayHelp` or something similar.\r\n\r\n",
        "(I have not commented on all places with this name. All of them should be fixed.)"
      ],
      "runtime-document-code-meaningfully": [
        "What's the license on the file that this was copied from?",
        "We prefer to err on the side of doing more attribution.\r\n\r\nThis is more than 100 lines copied nearly verbatim, including docs. I think it is above the threshold."
      ],
      "runtime-explicit-api-versioning": [
        "Do we need bump `EE_INTERFACE_MAJOR_VERSION` for this this?",
        "I am worried about the existing standalone GC scenarios that the GC team cares about being broken. As implemented in the PR currently, I do not think you can use new GC with old runtime, and vice versa."
      ],
      "runtime-optimize-aligned-simd-operations": [
        ">  I don't believe trying to polyfill via malloc/free is reliable as there is no guarantee that free works with arbitrary pointers:\r\n\r\nThe polyfill would have to be for both aligned alloc and free, something like this: https://github.com/dotnet/runtime/issues/33244#issuecomment-595848832"
      ],
      "runtime-optimize-for-readability": [
        "> RuntimeMethodInfo doesn't overload those operators so it doesn't matter\r\n\r\nMethodInfo does overload those operators. MethodInfo operators are going to be selected to compare RuntimeMethodInfo [quick test](https://sharplab.io/#v2:EYLgtghglgdgNAFxFANgHwAIAYAEGCMAdAEoCmAZiqQMYJQD2MA3ALABQ7EwAzggE4RaeAEw5iAVxh0wpALKkEAC3oATAJIxy9HCBzylqjVvYBvdjgt4AzHnwA2HMHr0UOACqleACglSoM/WV1TW0wAEpzSzM2S1i8AHYcMBwAXhScGHEUFFYYywBfdnygA=)"
      ],
      "runtime-optimize-common-paths": [
        "We build with both llvm and gcc successfully. What's the compiler used by SunOS that this needs fixing?",
        "Why does this compile fine with gcc on Linux?",
        "> it would be nice if NAOT could target this as the default\r\n\r\nI do not think we want to have the baseline supported piecemeal (it is unlikely we would be able to it correctly since it is impossible to test). If we want to raise the baseline, we should do it for the whole product. I would not be opposed to raising the product baseline to x86-64-v2.\r\n\r\n>  its an 18 year old baseline\r\n\r\nWhen did Intel/Amd stop selling the last processor without x86-64-v2? It is the more interesting date for this discussion.\r\n\r\n> Win11 prior to 24H2 is documented as requiring SSE4.1 otherwise\r\n\r\nWhere is it documented?"
      ],
      "runtime-centralize-platform-configurations": [
        "So this will need ifdefs controlled by configure script based on what's available on different Unix flavors. You can always do polyfill using malloc when there is no native API.",
        "Here is the existing place where all similar adjustment are concentrated today:\r\n\r\nhttps://github.com/dotnet/runtime/blob/9d771a26f058a9fa4a49850d4778bbab7aa79a22/src/libraries/Native/Unix/configure.cmake#L532-L558\r\n\r\nWe may want to move this one to this central place too."
      ],
      "runtime-cache-expensive-computations": [
        "This is an expensive way to compute a bool. Should we have a virtual property for this?",
        "Pass the result of `type.GetClassLayout()` to the worked methods to avoid recomputing it? (All except `ComputeCStructFieldLayout` needs it on some path.) It can be passed around as `in` since it is likely going grow into a large struct over time.",
        "Pre-allocate this HashSet and Dictionary to avoid re-hashing?",
        "```suggestion\r\n                symbolRemapping = new Dictionary<ISymbolNode, ISymbolNode>((int)(1.05 * (previousSymbolRemapping?.Count ?? 0)));\r\n```",
        "Otherwise, we are guaranteed to rehash the whole thing on most iterations just to add a few more elements."
      ],
      "runtime-document-configuration-intent": [
        "Are any of these documented in official docs? If they are not, I do not think we need to keep them.",
        "`IlcInstructionSet` is explicitly documented in the repo docs as [subject to change without a breaking change notice](https://github.com/dotnet/runtime/blob/main/src/coreclr/nativeaot/docs/optimizing.md).",
        "Command line arguments for direct invocation of crossgen2/ilc are not officially documented/supported."
      ],
      "runtime-memory-barrier-pairing": [
        "Do we need a counter-part barrier on the writer sides (e.g. in DynamicHelperFixup)?"
      ],
      "runtime-preserve-pointer-authentication": [
        "I see that you have `Sign with SP` at the end of the list. I am not sure whether you want to wait with tackling it as the last item. I expect that you will need to revisit and retest number of changes in this PR to make signing with SP work. ",
        "Signing return addresses with `SP` is required to deliver on security promise of PAC and it changes how things like hijacking need to be handled. I expect that you will need to redo a large part of change to make it work and then retest everything again. You can certainly do that, but I do not think it is the most efficient way to deliver this feature.",
        "Would it make sense to keep the bits on `m_pvHJRetAddr` unstripped so that the original return address is protected at all times? I think stripping the bits here creates a hole in the protection. Should we strip the bits later when we read `m_pvHJRetAddr` for stackwalking, but not when we use it to store/restore the original return address so that the return address is protected for the whole time?",
        "Once the code actually returns to the restored return address, it is going to authenticate. It should not be necessary to do an extra authentication during hijacking.",
        "> the original address is preserved so that after GC we can return to it, and the updated address would take the execution flow to desired new address\r\n\r\nRight. My point is that it would be better to keep the signed address intact through the whole process:\r\n- Delete PacStripPtr in Thread::HijackThread and store the original signed address in m_pvHJRetAddr instead \r\n- Delete PacSignPtr in Thread::UnhijackThread\r\n- Add PacStripPtr as necessary to places that read m_pvHJRetAddr and do not expect the signature in the upper bits\r\n\r\nIt will make the original return address protected while it is stored in m_pvHJRetAddr. As implemented currently, the return address is not protected while it is stored in m_pvHJRetAddr."
      ],
      "runtime-structure-configuration-options": [
        "```suggestion\r\n#if !defined(DACCESS_COMPILE) && !defined(FEATURE_CDAC_UNWINDER)`\r\n```\r\nWe have two builds of out-of-proc unwinders: DACCESS and CDAC. I think we want to take the offline path for both out-of-proc unwinders. `TARGET_ARM64` should not be needed in the condition once you do enable to the offline path for all out-of-proc unwinders.\r\n\r\n(All similar ifdefs in this file should be changed like this.)",
        "We want to be able to test interpreter-only mode on Windows and Linux even with support JIT compiled in. This should be based on some config switch - like `DOTNET_Interpter=*` that should make us to use the interpreter for all methods.",
        "@janvorli Do you have thoughts how we want to test interpreter-only mode?\r\n\r\nI am thinking we may want to introduce a new dedicated environment variable: `DOTNET_InterpreterMode`:\r\n\r\n`DOTNET_InterpreterMode=0`: default, do not use interpreter except explicit opt-in via `DOTNET_Interpreter`\r\n`DOTNET_InterpreterMode=1`: use interpreter for everything except (1) methods that have R2R compiled code and (2) all code in System.Private.CoreLib. All code in System.Private.CoreLib falls back to JIT if there is no R2R available for it. This can replace the testing mode introduced in https://github.com/dotnet/runtime/pull/116570/files#diff-3e5a329159ca5b2268e62be8a0d776b6092681e9b241210cb4d57e3454816abcR403 since it will cover code in non-entrypoint assemblies too and thus will be more comprehensive. This mode should have good balance between speed and coverage. We may want to use it for running libraries tests with interpreter eventually.\r\n`DOTNET_InterpreterMode=2`: use interpreter for everything except intrinsics. All intrinsics fallback to JIT. Implies `DOTNET_ReadyToRun=0`. I am not sure how much this will be useful in practice, but it sounds like interesting mode to have.\r\n`DOTNET_InterpreterMode=3`: use interpreter for everything, the full interpreter-only mode, no fallbacks to R2R or JIT whatsoever. Implies `DOTNET_ReadyToRun=0`, `DOTNET_EnableHWIntrinsic=0`, \r\n\r\nAn alternative is to piece together solutions from existing environment variables (`DOTNET_Interpreter`, `DOTNET_ReadyToRun`, `DOTNET_EnableHWIntrinsic`, ...), but it may be hard to make it do what we want exactly.\r\n\r\n> Should I introduce that flag as part of this PR?\r\n\r\nI do not have an opinion - it is fine with me to introduce it as part of this PR once we agree on how it should work.",
        "> I am not sure if the mode 1 would replace the testing mode for coreclr tests though, as it would also interpret the xunit stuff which I assume would be quite slow.\r\n\r\ncoreclr tests should have the xunit stuff source generated. It should not be a lot of code - it does not run reflection and other heavy lifting like regular xunit."
      ],
      "runtime-avoid-busy-waiting": [
        "Yes, somebody tried really hard to make `Volatile.Read` for 64-bit values atomic on x86, and this is the price that was paid for it."
      ],
      "runtime-choose-descriptive-names": [
        "```suggestion\r\n                            trace::error(_X(\"The application '%s' is not a managed .dll.\"), app_candidate.c_str());\r\n```\r\nI do not think managed .exes are a thing in modern .NET. None of the tooling will produce them. Should we drop `or .exe` here to avoid suggesting that managed .exes are a thing?\r\n\r\nAlternatively, we can say `is not a .NET binary.\". \"binary\" looks less Windows-specific, but also less descriptive. It won't give you a hint that you need to pass in .dll."
      ],
      "runtime-memory-ordering-matters": [
        "```suggestion\r\n        void* pvHIjackAddr = (void*)pfnHijackFunction;\r\n#if defined(TARGET_ARM64)\r\n        pvHIjackAddr = PacSignPtr(pvHIjackAddr);\r\n#endif // TARGET_ARM64\r\n        *ppvRetAddrLocation = pvHIjackAddr;\r\n```\r\nWe should avoid writing the wrong value first, and then overwriting it with the correct value. It can cause interesting race conditions."
      ],
      "runtime-specific-exceptions-with-context": [
        "\"Dynamic entrypoint allocation is not supported in the current environment.\"?",
        "Can we throw the exception immediately here instead of propagating it manually? The manual error propagation looks like a left-over from the .NET Native MRT/app split.",
        "This should be `PlatformNotSupportedException`. From .NET Framework design guidelines:\r\n\r\nDO throw PlatformNotSupportedException to indicate the operation cannot complete in the current runtime environment but could on a different runtime or operating system."
      ],
      "runtime-optimize-memory-access": [
        "Should this be done as unsigned division to match CoreCLR? \r\n\r\nSigned division is extra instructions: https://godbolt.org/z/cxxWP67n6",
        ">  Also validating that byteCount % alignment == 0 would be fairly expensive.\r\n\r\nYou do not need to use `%` once you know that the alignment is power of 2.",
        "Does the formula for the second argument need to use the alignment after it has been bumped up to `sizeof(void*)`?",
        "> basically, all MemoryMarshal.Read/Write/Cast, Unsafe.Read/ReadUnligned/Write/WriteUnligned, Unsafe.As\r\n\r\nMemoryMarshal.Cast and Unsafe.As are the only ones with the potential alignment problem, there are not that many, and we have been accepting fixes to make the core more portable (e.g. https://github.com/dotnet/runtime/pull/98812).\r\n\r\n> The only tricky cases\r\n\r\nAnd the platforms/architecture that we never heard of that Unity may run on.",
        "> it still is going to fail on a platform where such reads aren't fixed by the OS. At least on CoreCLR/NAOT\r\n\r\nIf we end up targeting a platform like that, the JIT will need to be fixed up to make `Unsafe.ReadUnaligned` work as appropriate.",
        "> I guess it's one of those cases when safe code doesn't read better than unsafe\r\n\r\nIt is because of we have not designed safe APIs that make this kind of code look good.\r\n\r\nI think that that best you can do using existing .NET APIs is approximation of \"eat the span\" pattern that's idiomatic in golang:\r\n```csharp\r\nwhile (span.Length >= 8)\r\n{\r\n   ulong v = BitConverter.ToInt64(span);\r\n\r\n   ...\r\n\r\n   span = span.Slice(8);\r\n}\r\n```\r\n\r\nNew APIs can make it look better:\r\n```\r\nwhile (BitConverter.TryRead(span, out long v))\r\n{\r\n   ...\r\n\r\n   span = span.Slice(8);\r\n}\r\n```\r\n\r\nAnother alternative is to introduce iterators that are idiomatic in Rust:\r\n```\r\n// Rust equivalent is for chunk in data.array_chunks::<8>()\r\nforeach (long v in span.IterateAsInt64())\r\n{\r\n...\r\n}\r\n```",
        "> `=> Aggregate<T, IdentityOperator<T>, Crc32Operator<T>>(x)`\r\n\r\nI get that this pattern is centralizing the unsafe code, but I do not think that it reads well."
      ],
      "runtime-model-actual-hardware-costs": [
        "This should be enabled for crossgen too.",
        "Yep, as @AndyAyersMS said. PREJIT means AOT compilation in general. R2R means specific ABI for AOT compiled code, e.g. code sequence to access generic dictionaries, etc."
      ]
    },
    "profile": {
      "company": "Microsoft",
      "blog": "",
      "site_admin": false,
      "followers": 662,
      "following": 1
    }
  },
  "the-mikedavis": {
    "repos": [
      "helix-editor/helix"
    ],
    "entries": [
      {
        "slug": "helix-api-documentation-accuracy",
        "title": "API documentation accuracy"
      },
      {
        "slug": "helix-avoid-hardcoded-configuration-values",
        "title": "avoid hardcoded configuration values"
      },
      {
        "slug": "helix-avoid-panics-gracefully",
        "title": "avoid panics gracefully"
      },
      {
        "slug": "helix-avoid-version-specific-documentation",
        "title": "avoid version-specific documentation"
      },
      {
        "slug": "helix-consistent-descriptive-naming-conventions",
        "title": "Consistent descriptive naming conventions"
      },
      {
        "slug": "helix-consistent-highlighting-patterns",
        "title": "consistent highlighting patterns"
      },
      {
        "slug": "helix-consistent-naming-conventions",
        "title": "consistent naming conventions"
      },
      {
        "slug": "helix-documentation-style-and-formatting",
        "title": "Documentation style and formatting"
      },
      {
        "slug": "helix-follow-established-conventions",
        "title": "Follow established conventions"
      },
      {
        "slug": "helix-minimize-allocations-and-syscalls",
        "title": "Minimize allocations and syscalls"
      },
      {
        "slug": "helix-omit-redundant-configuration",
        "title": "omit redundant configuration"
      },
      {
        "slug": "helix-optimize-query-performance",
        "title": "Optimize query performance"
      },
      {
        "slug": "helix-prefer-let-else-patterns",
        "title": "prefer let-else patterns"
      },
      {
        "slug": "helix-reduce-nesting-complexity",
        "title": "reduce nesting complexity"
      },
      {
        "slug": "helix-semantic-identifier-naming-patterns",
        "title": "Semantic identifier naming patterns"
      },
      {
        "slug": "helix-standardize-build-configuration-patterns",
        "title": "Standardize build configuration patterns"
      },
      {
        "slug": "helix-target-documentation-to-audience",
        "title": "Target documentation to audience"
      },
      {
        "slug": "helix-use-descriptive-names",
        "title": "use descriptive names"
      }
    ],
    "comments": {
      "helix-minimize-allocations-and-syscalls": [
        "The `to_string()` can move from the `map` in `let language_servers` to the `map` here on L349. That way we allocate only for names that match the input.",
        "```suggestion\r\n        Variable::Language => Ok(match doc.language_name() {\r\n            Some(lang) => Cow::Owned(lang.to_owned()),\r\n            None => Cow::Borrowed(\"text\"),\r\n        }),\r\n```\r\n\r\nmight as well avoid that extra allocation when there is no lang, even though it's small",
        "We should avoid reading the entire file into memory at once"
      ],
      "helix-reduce-nesting-complexity": [
        "It looks like each of these enum variants are storing the count, right? Let's separate the enum from the count and pass that as a separate parameter even if it means adding `#[allow(clippy::too_many_arguments)]` for some functions",
        "Stylewise we avoid `let` blocks because they lead to increased indentation. It's fine to let `label` be `mut` for this `for` loop scope"
      ],
      "helix-target-documentation-to-audience": [
        "For these we should tag as `comment.line.documentation` - Rust also has block doc comments ([reference](https://doc.rust-lang.org/reference/comments.html)) as\r\n\r\n```rust\r\n/**  - Outer block doc (exactly) 2 asterisks */\r\n```\r\n\r\nwhich we could tag as well"
      ],
      "helix-consistent-highlighting-patterns": [
        "```suggestion\r\n(rule (unknownDirective) @attribute) @error\r\n```\r\n\r\nFor syntax errors in the past I believe we used `error` rather than `diagnostic.error`",
        "It's fine to highlight based on exact function/type names for things like the builtin highlights, but I don't like a huge pattern like this that tries to guess the highlight. We can't figure out whether the identifier is in this situation robustly and I don't want a really big highlight like this that tries to cover common cases - it will seem inconsistent when you write a custom higher order function",
        "Generally we do not highlight `(ERROR)` since it's very noisy when typing. In the future we might introduce configuration for it if it's desired (basically prepending `(ERROR) @error` to `highlights.scm` text) but in the meantime let's remove this pattern",
        "The captures need some tweaks to match the scopes we use: https://docs.helix-editor.com/master/themes.html#syntax-highlighting\r\n\r\nFor example `boolean` should be `constant.builtin.boolean`, `number` should be `constant.numeric.integer` or `constant.numeric.float`",
        "Some of the captures in this file will need to be adapted to the ones we use: https://docs.helix-editor.com/master/themes.html#syntax-highlighting\r\n\r\nFor example `@number` should become `@constant.numeric` and `@module` should become `@namespace`",
        "I think these two should be reversed? Now the less specific highlight should be higher in the file - I think `comment.line` will always overwrite `comment.block.documentation` here",
        "```suggestion\r\n  (#match? @comment.block.documentation \"^//!\"))\r\n```\r\n\r\n`#lua-match?` is neovim specific but this regex will work for `#match?`"
      ],
      "helix-standardize-build-configuration-patterns": [
        "For the build of `packages.helix` we can use the latest stable - that's what we do in the release builds as well. We only need the MSRV toolchain for the development shell",
        "```suggestion\r\n        cargoBuildType = \"release\";\r\n```\r\n\r\nYeah let's switch back to `release`. The LTO stuff (I assume) seems to be way more expensive than I thought. I'm seeing ~60s to build on `release` and ~330s to build on `opt` :/",
        "Should we use `rustc` and `cargo` from the attrs passed to `mkHelix` instead? I'm seeing a deprecation warning on accessing these",
        "For the package build (not the shell) we don't need to follow the MSRV, it's ok to build with latest stable. We do roughly the same in CI - the build workflow for PRs checks with the MSRV and then the release workflow uses whatever is the latest stable"
      ],
      "helix-optimize-query-performance": [
        "Capturing the name as a namespace here can lead to some odd highlighting, for example\r\n\r\n```rust\r\nfn fun(param: u32) {\r\n    param::call();\r\n    param\r\n}\r\n```\r\n\r\n`call` is highlighted as `namespace`. Probably the highlighter should recognize that this capture comes from the locals queries and not attach any highlight to it, so that the regular highlights take over.",
        "I don't think it would take that large of a change - Query has functions that can be used to tell if the pattern is from the locals file or from the highlights file. I'll look into it.\r\n\r\nI'm a bit surprised that `#has-ancestor?` has poor performance since it should be bounded by the height of the tree. I suppose that can grow to be quite large though. I don't like it because it has you write node names and that is both inflexible (can't use the full pattern matching power of the query language) and error-prone since tree-sitter won't check that it's a valid node during query analysis. So I'd like to avoid adding it if possible. Some day I'll try to understand `query.c` and see if an arbitrary nesting operator is possible.",
        "```suggestion\r\n((identifier) @variable.builtin (#any-of? @variable.builtin \"this\" \"msg\" \"block\" \"tx\"))\r\n```\r\n\r\nwhen this was originally written I don't think we supported `#any-of?` but now we can replace the `#match?` with the equivalent `#any-of`"
      ],
      "helix-documentation-style-and-formatting": [
        "```suggestion\r\nwhere `key` represents what you want to style, `fg` specifies the foreground color, `bg` the background color, `underline-style` the underline style, `underline-color` the underline color (only meaningful if an underline style is enabled), and `modifiers` is a list of style modifiers. `bg`, `underline` and `modifiers` can be omitted to defer to the defaults.\r\n```"
      ],
      "helix-api-documentation-accuracy": [
        "Some of the commentary is good like the above one about dance. I'd like to avoid commenting on how well the extensions work though like these two lines since the extensions might get better over time (and then this page is out of date) and how well the emulation works might be arguable\r\n\r\n(Same for the shells section below - we can probably drop the \"comments\" column from that table)"
      ],
      "helix-avoid-panics-gracefully": [
        "Rather than a panic you can set an error in the statusline:\r\n\r\n```rust\r\nif count != 1 {\r\n    cx.editor.set_error(\"inserting multiple newlines not yet supported\");\r\n}\r\n```",
        "Instead of an `expect` let's `log::error!` when the config isn't set and `return true`"
      ],
      "helix-semantic-identifier-naming-patterns": [
        "Semantically it contains a function but syntactically it's a variable and it should be highlighted as one",
        "Same here and below on L270 about `#lua-match?`"
      ],
      "helix-use-descriptive-names": [
        "I'll add a note in the command description that it can be used to evaluate arguments with no effect",
        "This is meant to be generically about strings - we might define other specialized small strings in this module",
        "```suggestion\r\n    pub workspace_diagnostics: Vec<Severity>,\r\n```\r\n\r\nI would prefer a full name here so it's easy to read"
      ],
      "helix-follow-established-conventions": [
        "Configuring a formatter is ok but we shouldn't enable `auto-format` unless formatting for TLA+ is a standard practice\r\n\r\n```suggestion\r\n```",
        "We don't parse 3-character hex codes so this will need to be expanded to the 6 character version"
      ],
      "helix-avoid-version-specific-documentation": [
        "`+stable` is only available if you're using rustup which is not always the case, for example you might use cargo/rustc from your package manager",
        "This text may age poorly as versions change. Let's say something a bit more generic like \"The CI may use a libc version greater than what your Ubuntu/Debian/Mint version requires - in this case you can build a `.deb` from source.\" and jump right into the directions."
      ],
      "helix-prefer-let-else-patterns": [
        "rustfmt won't help us fix this yet but I think the style guide suggests (https://github.com/rust-lang/rust/blob/c8ead2e693a22fe94c6b3edeb3f49c7e6aec3912/src/doc/style-guide/src/statements.md#single-line-let-else-statements):\r\n\r\n```rs\r\nlet Some(capabilities) = self.capabilities.get() else { return false };\r\n```"
      ],
      "helix-consistent-naming-conventions": [
        "```suggestion\r\n\"markup.link.url\"             = { fg = \"markup_link_url_fg\", modifiers = [\"underlined\"] }\r\n```\r\n\r\nsmall typo here: when used in `modifiers` it should be `underlined` instead of `underline`",
        "It looks like these `selectionFG` colors should be switched to `selectionfg` to match the entry in the palette",
        "Let's call this something more specific in the `contrib` dir like `contrib/hx-deb-wrapper.sh` or something so it's clear what it's for. (I assume you can rename it back to `hx` here.)",
        "Yep that works too, I'd just like to avoid calling it \"hx\""
      ],
      "helix-omit-redundant-configuration": [
        "```suggestion\r\n```\r\n\r\nthis key can be omitted since `[]` is the default",
        "This `language-id` key can be omitted - by default the `name` will be used"
      ],
      "helix-consistent-descriptive-naming-conventions": [
        "To align with other config, `underline_styles` and `underline_color` should probably be kebab-case and to align with the modifiers, the underline styles should be snake_case"
      ],
      "helix-avoid-hardcoded-configuration-values": [
        "This debounce doesn't relate to that: it debounces large updates to the word index. Instead what you'd want would be covered by configuration for the minimum word length to trigger completion (in the new `word` module).\r\n\r\nThe way I have the completion tuned now (with constants rather than configuration) avoids word completion most of the time so to type a long word I use `C-x` more often than not. I think it's ok for the trigger length to be configurable but I think the default configuration should try to avoid aggressively completing words and word completions should be lower priority than LSP completions."
      ]
    },
    "profile": {
      "location": "NYC",
      "company": "@AWS",
      "blog": "https://the-mikedavis.github.io/",
      "site_admin": false,
      "followers": 618,
      "following": 36
    }
  },
  "gruebel": {
    "repos": [
      "bridgecrewio/checkov"
    ],
    "entries": [
      {
        "slug": "checkov-backward-compatible-parameters",
        "title": "Backward compatible parameters"
      },
      {
        "slug": "checkov-choose-optimal-algorithms",
        "title": "Choose optimal algorithms"
      },
      {
        "slug": "checkov-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "checkov-comprehensive-security-scanning",
        "title": "Comprehensive security scanning"
      },
      {
        "slug": "checkov-configure-security-scanners-completely",
        "title": "Configure security scanners completely"
      },
      {
        "slug": "checkov-consistent-naming-conventions",
        "title": "Consistent naming conventions"
      },
      {
        "slug": "checkov-document-configuration-consistently",
        "title": "Document configuration consistently"
      },
      {
        "slug": "checkov-document-configuration-options",
        "title": "Document configuration options"
      },
      {
        "slug": "checkov-ensure-dependency-compatibility",
        "title": "Ensure dependency compatibility"
      },
      {
        "slug": "checkov-meaningful-identifier-names",
        "title": "Meaningful identifier names"
      },
      {
        "slug": "checkov-precise-configuration-validation",
        "title": "Precise configuration validation"
      },
      {
        "slug": "checkov-preserve-api-compatibility",
        "title": "Preserve API compatibility"
      },
      {
        "slug": "checkov-safe-dictionary-access",
        "title": "Safe dictionary access"
      },
      {
        "slug": "checkov-safe-dictionary-navigation",
        "title": "Safe dictionary navigation"
      },
      {
        "slug": "checkov-strategic-error-handling",
        "title": "Strategic error handling"
      },
      {
        "slug": "checkov-strategic-exception-management",
        "title": "Strategic exception management"
      },
      {
        "slug": "checkov-support-all-target-environments",
        "title": "Support all target environments"
      },
      {
        "slug": "checkov-use-appropriate-logging-levels",
        "title": "Use appropriate logging levels"
      },
      {
        "slug": "checkov-use-pytest-best-practices",
        "title": "Use pytest best practices"
      },
      {
        "slug": "checkov-validate-configurations-correctly",
        "title": "Validate configurations correctly"
      },
      {
        "slug": "checkov-write-pythonic-code",
        "title": "Write pythonic code"
      }
    ],
    "comments": {
      "checkov-validate-configurations-correctly": [
        "```suggestion\r\n      operator: not_exists\r\n```\r\nshouldn't it be not set, because we don't want a public IP?",
        "```suggestion\r\n  operator: \"not_equals_ignore_case\"\r\n```\r\nit should not `true` right?",
        "you can change it to a 'not empty', but a not empty `origin_access_identity` is a broken configuration, but correct me, if I'm wrong."
      ],
      "checkov-safe-dictionary-navigation": [
        "```suggestion\r\n        if properties and isinstance(properties, dict):\r\n```\r\nlet's make sure, ew deal with a dictionary otherwise we will have a problem.",
        "```suggestion\r\n        principal = conf.get(\"principal\")\r\n        if principal and isintsance(principal, list) and isinstance(principal[0], str):\r\n            principal_parts = principal[0].split('.')\r\n```\r\nthis is a bit tricky, we need to be a bit more cautious on the types. Quite often Terraform plan files come with unexpected default values and break our checks.",
        "```suggestion\r\n    inline_suppressions_by_cve = inline_suppressions.get(\"cves\", {}).get(\"byCve\", {})\r\n    for cve_suppression in inline_suppressions_by_cve:\r\n        cve_id = cve_suppression.get(\"cveId\")\r\n        if cve_id:\r\n            cve_by_cve_map[cve_id] = cve_suppression\r\n```\r\n🙂 ",
        "`not` also works, but if it is an empty `dict` then there is no need to override it again\r\n```suggestion\r\n            if each[\"change\"][\"before\"] is None:\r\n                each[\"change\"][\"before\"] = {}\r\n            if each[\"change\"][\"after\"] is None:\r\n                each[\"change\"][\"after\"] = {}\r\n```"
      ],
      "checkov-comprehensive-security-scanning": [
        "we usually recommend to run docker with `--tty` for better output handle.",
        "```suggestion\r\n    entry: checkov -d . --framework secrets --enable-secret-scan-all-files\r\n```\r\ncould you also add the flag `--enable-secret-scan-all-files` so all the files are scanned."
      ],
      "checkov-document-configuration-consistently": [
        "move this to the `Settings` block as an env var",
        "nice catch!"
      ],
      "checkov-strategic-error-handling": [
        "@mikeurbanski1 any thoughts about raising an exception here or should we just log it and return the normal URL?",
        "do we really want to raise an exception or just log a message?",
        "in theory it could, but this would mean something went wrong during the context creation. But I will add a check and log a message."
      ],
      "checkov-choose-optimal-algorithms": [
        "you could also create a `defaultdict`, then you don't have to this check manually \r\n```python\r\ndirs_to_definitions = defaultdict(list)\r\ndirs_to_definitions[dir_path].append({tf_definition_key: tf_value})\r\n```",
        "how about making this a `set()` then you don't need to transform it to a set and back to list 🙂 ",
        "also thought the same, but I think it is a bigger change, because of the typing mismatch.",
        "```suggestion\r\n            for field, value in each[\"change\"][\"before\"].items():\r\n                if value != each[\"change\"][\"after\"].get(field):\r\n```",
        "also make sure to skip the field names `__startline__` and `__endline__`, there is aconstant `LINE_FIELD_NAMES` which stores them as a set.",
        "just double checking, this has no bad side effect, because we would create more often edges, right?",
        "if we for some reason get more than 1 `target_variables`, we didn't create an edge before and just continued. If the tests are passing, then it is probably not an issue 😄 "
      ],
      "checkov-choose-optimal-data-structures": [
        "you could also create a `defaultdict`, then you don't have to this check manually \r\n```python\r\ndirs_to_definitions = defaultdict(list)\r\ndirs_to_definitions[dir_path].append({tf_definition_key: tf_value})\r\n```",
        "how about making this a `set()` then you don't need to transform it to a set and back to list 🙂 ",
        "also thought the same, but I think it is a bigger change, because of the typing mismatch.",
        "just double checking, this has no bad side effect, because we would create more often edges, right?",
        "if we for some reason get more than 1 `target_variables`, we didn't create an edge before and just continued. If the tests are passing, then it is probably not an issue 😄 "
      ],
      "checkov-meaningful-identifier-names": [
        "yeah, sure. I just named them identical to keep the code changes minimal 😄 "
      ],
      "checkov-use-pytest-best-practices": [
        "the parent calss `TestBaseSolver` comes with a couple of convenient functions, which I didn't plan to duplicate, but I will double check what's the effort 🙂 "
      ],
      "checkov-backward-compatible-parameters": [
        "```suggestion\r\n                    messages=messages,\r\n```\r\notherwise you only send the first message, same a few lines lower"
      ],
      "checkov-write-pythonic-code": [
        "```suggestion\r\n                if mod['Key']:\r\n```\r\nit is faster to check for truthiness, which means it is not empty.",
        "```suggestion\r\n        self._address_to_tf_vertex_map = {\r\n            vertex.attributes[TF_PLAN_RESOURCE_ADDRESS]: vertex\r\n            for vertex in self.tf_graph.vertices\r\n            if vertex.block_type == BlockType.RESOURCE:\r\n        }\r\n```\r\nI think you can do it via a dict comprehension",
        "```suggestion\r\n                if 'source_arn' in conf or 'source_account' in conf:  # If either of these are set, we're good and the check should pass.\r\n```\r\nno need to explicitly add `.keys()` because it is the default when doing a lookup in a dict."
      ],
      "checkov-ensure-dependency-compatibility": [
        "this will not work. this lock file was created with Python 3.10+ therefore any CI jobs running on 3.8 or 3.9 will fail"
      ],
      "checkov-precise-configuration-validation": [
        "```suggestion\r\n      operator: not_exists\r\n```\r\nshouldn't it be not set, because we don't want a public IP?",
        "```suggestion\r\n  operator: \"not_equals_ignore_case\"\r\n```\r\nit should not `true` right?",
        "you can change it to a 'not empty', but a not empty `origin_access_identity` is a broken configuration, but correct me, if I'm wrong.",
        "please add an or block, when `shared_access_key_enabled` is set to `false` then it should pass without setting the expiration"
      ],
      "checkov-document-configuration-options": [
        "move this to the `Settings` block as an env var",
        "nice catch!"
      ],
      "checkov-preserve-api-compatibility": [
        "```suggestion\r\n                if runner.graph_manager:\r\n                    check_type_to_graph = {runner.check_type: runner.graph_manager.get_reader_endpoint()}\r\n                    return report, runner.check_type, runner.graph_manager.get_reader_endpoint()\r\n                return report, None, None\r\n```\r\nhow about returning all as a normal tuple, then you can check if both are not `None` and update `full_check_type_to_graph[check_type] = reader_endpoint`"
      ],
      "checkov-support-all-target-environments": [
        "this will not work. this lock file was created with Python 3.10+ therefore any CI jobs running on 3.8 or 3.9 will fail"
      ],
      "checkov-configure-security-scanners-completely": [
        "we usually recommend to run docker with `--tty` for better output handle.",
        "```suggestion\r\n    entry: checkov -d . --framework secrets --enable-secret-scan-all-files\r\n```\r\ncould you also add the flag `--enable-secret-scan-all-files` so all the files are scanned."
      ],
      "checkov-safe-dictionary-access": [
        "```suggestion\r\n        if properties and isinstance(properties, dict):\r\n```\r\nlet's make sure, ew deal with a dictionary otherwise we will have a problem.",
        "```suggestion\r\n        principal = conf.get(\"principal\")\r\n        if principal and isintsance(principal, list) and isinstance(principal[0], str):\r\n            principal_parts = principal[0].split('.')\r\n```\r\nthis is a bit tricky, we need to be a bit more cautious on the types. Quite often Terraform plan files come with unexpected default values and break our checks.",
        "```suggestion\r\n    inline_suppressions_by_cve = inline_suppressions.get(\"cves\", {}).get(\"byCve\", {})\r\n    for cve_suppression in inline_suppressions_by_cve:\r\n        cve_id = cve_suppression.get(\"cveId\")\r\n        if cve_id:\r\n            cve_by_cve_map[cve_id] = cve_suppression\r\n```\r\n🙂 ",
        "`not` also works, but if it is an empty `dict` then there is no need to override it again\r\n```suggestion\r\n            if each[\"change\"][\"before\"] is None:\r\n                each[\"change\"][\"before\"] = {}\r\n            if each[\"change\"][\"after\"] is None:\r\n                each[\"change\"][\"after\"] = {}\r\n```"
      ],
      "checkov-consistent-naming-conventions": [
        "you can also use here `def get_evaluated_keys(self) -> List[str]:` instead of adding them dynamically",
        "```suggestion\r\n        self.ALLOW_KUSTOMIZE_FILE_EDITS = convert_str_to_bool(os.getenv(\"CHECKOV_ALLOW_KUSTOMIZE_FILE_EDITS\", False))\r\n```\r\nplease prefix it with `CHECKOV_` to make it clear, it is an internal env var.",
        "had the same thought 😄 ",
        "yeah, sure. I just named them identical to keep the code changes minimal 😄 "
      ],
      "checkov-strategic-exception-management": [
        "@mikeurbanski1 any thoughts about raising an exception here or should we just log it and return the normal URL?",
        "do we really want to raise an exception or just log a message?",
        "in theory it could, but this would mean something went wrong during the context creation. But I will add a check and log a message."
      ],
      "checkov-use-appropriate-logging-levels": [
        "```suggestion\r\n            logging.debug(f\"OpenAI request returned: {completion}\")\r\n```\r\nor something similar to have a direct context. Also debug level is more than enough.\r\n",
        "instead of print use `logging.info(...)`",
        "just thinking about the log level, if maybe `info` is enough, depends on how critical it is to the user. "
      ]
    },
    "profile": {
      "company": "@baz-scm",
      "blog": "",
      "site_admin": false,
      "followers": 58,
      "following": 2
    }
  },
  "dsherret": {
    "repos": [
      "denoland/deno"
    ],
    "entries": [
      {
        "slug": "deno-add-comprehensive-test-coverage",
        "title": "Add comprehensive test coverage"
      },
      {
        "slug": "deno-avoid-ambiguous-naming",
        "title": "Avoid ambiguous naming"
      },
      {
        "slug": "deno-avoid-implementation-detail-leakage",
        "title": "avoid implementation detail leakage"
      },
      {
        "slug": "deno-avoid-panics-gracefully",
        "title": "avoid panics gracefully"
      },
      {
        "slug": "deno-avoid-tooling-workarounds",
        "title": "avoid tooling workarounds"
      },
      {
        "slug": "deno-choose-appropriate-algorithms",
        "title": "Choose appropriate algorithms"
      },
      {
        "slug": "deno-comprehensive-test-verification",
        "title": "comprehensive test verification"
      },
      {
        "slug": "deno-control-cache-lifecycle",
        "title": "Control cache lifecycle"
      },
      {
        "slug": "deno-defensive-null-handling",
        "title": "defensive null handling"
      },
      {
        "slug": "deno-enhance-error-message-clarity",
        "title": "Enhance error message clarity"
      },
      {
        "slug": "deno-environment-loading-order",
        "title": "environment loading order"
      },
      {
        "slug": "deno-explain-non-obvious-decisions",
        "title": "Explain non-obvious decisions"
      },
      {
        "slug": "deno-explicit-dependency-configuration",
        "title": "explicit dependency configuration"
      },
      {
        "slug": "deno-minimize-memory-allocations",
        "title": "minimize memory allocations"
      },
      {
        "slug": "deno-organize-code-structure",
        "title": "organize code structure"
      },
      {
        "slug": "deno-prefer-safe-optional-returns",
        "title": "prefer safe optional returns"
      },
      {
        "slug": "deno-use-appropriate-synchronization-mechanisms",
        "title": "Use appropriate synchronization mechanisms"
      },
      {
        "slug": "deno-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      },
      {
        "slug": "deno-validate-operation-permissions",
        "title": "Validate operation permissions"
      }
    ],
    "comments": {
      "deno-prefer-safe-optional-returns": [
        "Maybe just return `None`? dprint-plugin-typescript could change in the future and that would break this code.",
        "Prefer `deno_path_utils::url_to_file_path` and maybe handle these errors?"
      ],
      "deno-avoid-implementation-detail-leakage": [
        "Nitpick: Maybe just reexport `quinn::ConnectionError` to not leak this detail to the rest of the code?",
        "It's probably too much to expose all the way down here and also it creates a dependency on deno_graph for Deploy as well as `deno compile`. Probably we should have a trait that gets passed down here and then the CLI and deno compile can implement this separately (maybe just put it as a method on `NodeRequireLoader`)",
        "Before we merge this, we should open a PR in deno_graph to expose these and then add a todo here to remove this code.",
        "I noticed we don't need to expose this and only need to expose the options. Saves having to import one extra type in the caller."
      ],
      "deno-use-appropriate-synchronization-mechanisms": [
        "Yeah. Let's remove these.",
        "Is there a helper function we could introduce to ensure we always do this safely?",
        "I mean, something like have it panic if not done from the same thread or we just put a lock over all the usages on non-windows platforms?",
        "I kind of feel like we should be using a conditional variable here in order to kill the thread more quickly in case this is hanging shutdown, but this is probably fine.",
        "@nayeemrmn do you think we could move the init_flag and shutdown_flag into our tower-lsp fork? Might help simplify things up here."
      ],
      "deno-add-comprehensive-test-coverage": [
        "In this file there are unit tests for flag parsing. Maybe we should add some for the clean command now?"
      ],
      "deno-organize-code-structure": [
        "Instead of this being here and on `Permissions`. I think it should be a constructor parameter to `RuntimePermissionDescriptorParser`? That way the value isn't being passed around horizontally through the code.",
        "We'll need to figure out how to extract out the progress bar stuff from here before extracting this to a new crate.",
        "```suggestion\r\n  let user_data_dir = if let Some(env_var) = std::env::var_os(TEST_ENV_VAR_NAME) {\r\n    PathBuf::from(env_var)\r\n```\r\n\r\nDeja-vu. Should this code be extracted out to a reusable function?",
        "Nitpick: Maybe move this about PathTree so that the PathTrie struct w/ impl is beside each other?",
        "Nitpick: maybe use named properties at this point? (especially because it's public)",
        "Can you make this private again? See the comment above (basically when we previously had this exposed bugs would happen where people rely on this instead of also doing stuff like looking at the configuration file)"
      ],
      "deno-validate-operation-permissions": [
        "Is it ok this is accessible without permissions?",
        "I think we'll need to add write permission checks here?",
        "This might be a security issue. I think there's a reason why this isn't allowed without `-A`, but I can't remember at the moment.",
        "I looked into this a bit and I think it's more complicated than it seems. Here's some points from an old internal document:\r\n\r\n> - If users are using `\\\\.\\` paths to access things like named pipes, raw devices, those should trigger `--allow-sys` .\r\n> - Network resources should perform DNS lookup in permission checks, and the fully-resolved permission check object should include the the IP address. When we make the outgoing network requests, we must use the resolved IP.\r\n\r\nThat said, I'm not sure about what the right solution is here. At the moment, this just requires `--allow-all` permissions."
      ],
      "deno-use-descriptive-identifiers": [
        "Not sure about this name. What do other people do?"
      ],
      "deno-enhance-error-message-clarity": [
        "Maybe we should show which code did this by showing part of the stack?",
        "Yup!"
      ],
      "deno-avoid-ambiguous-naming": [
        "I see from a resolved comment that we're going to change this to an enum? Consider that or have a `parse_for_list_with_wildcards` method.",
        "Don't we usually name the unstable options in an options bag rather than parameters? Or we should be and I've been updating some of them like in https://github.com/denoland/deno/pull/29473\r\n\r\nGiven it's permissions code I think we should make it very clear what's going on here and this `true` here isn't clear.",
        "Nitpick: It might be good to have the call sites for this be more descriptive and not use boolean parameters, but it's probably fine because it's internal.",
        "Maybe: `check_specifiers(specifiers: &[ModuleSpecifier], options: CheckSpecifierOptions)`"
      ],
      "deno-avoid-panics-gracefully": [
        "Why are these warning and returning the error?",
        "I think this is doing what `error.maybe_range()` does internally? https://github.com/denoland/deno_graph/blob/961df5f16a0bb28a530612172b2f58e4482f287e/src/graph.rs#L426C14-L426C24"
      ],
      "deno-explain-non-obvious-decisions": [
        "Done, and used the enum instead.",
        "It's up to the caller. I'll add a doc comment on the option."
      ],
      "deno-choose-appropriate-algorithms": [
        "Could use strategy pattern here in the future (reporting strategy and remove strategy).",
        "Why do we need to parse an AST now? It seems we really only need the comments which the lexer provides?",
        "Should we be using `deno_semver::Version` here? https://docs.rs/deno_semver/0.7.1/deno_semver/struct.Version.html"
      ],
      "deno-minimize-memory-allocations": [
        "Can we change this to do the allocations in the constructor by precomputing the two possible values? (and preferably avoid any allocations there when `self.options.conditions` is empty, though that's not as big of a deal) I think this could return a `&[Cow<'static, str>>]`\r\n\r\nSorry, this might seem nitpicky, but it's not really. I did a lot of work removing allocations from node resolution in order to make it faster.",
        "Seems like a lot of work and allocations. Maybe we should just do something that stops after X chars on the first line? (ex. read the first 400 chars and if there's no newline then it's probably minified or maybe read all the chars and if it's over X length and only a max of 2 lines then it's minified)",
        "I think there's cloning that shouldn't be done because we have a static string here."
      ],
      "deno-avoid-tooling-workarounds": [
        "It looks like an swc syntax error.",
        "It's an swc diagnostic that's not surfaced in the playground. I'll open an swc issue.",
        "Looked into it and here's an example of the swc playground erroring: https://play.swc.rs/?version=1.11.20&code=H4sIAAAAAAAAA0vOzysuUUgrys9VsFWorrXmSq0oyC8qUUhJTUsszYHIWAMAg8l5qiUAAAA%3D&config=H4sIAAAAAAAAA1WPsQ6DMAxEd74Cee7aDp2rbv0IKzUoKCGRbSQQ4t8JkNCyxXfPd%2FFc1TV0YuBZz%2BmZhogsxOecFJl6xTEpQMajGLZR4VZcGmNgfVGDg9M3B59A5YFOoJNtt0EntEvL4YAit6R7rNxzHrgQhAqeNW9720z%2FPzLBRyaRK7ih2LeOrnVVrgQfvsNu5kt1inTUP%2BAHlbIzGKx8yuZ2WLWst%2B%2Fb9zUBAAA%3D\r\n\r\nIt's because of the `export default from` proposal.",
        "https://github.com/swc-project/swc/issues/10372 -- Using identifiers that are also contextual keywords is kind of asking to hit edge cases.",
        "We should try to move off `@ts-ignore` and instead just remove the `/** @internal */`s in TypeScript's codebase. I updated https://github.com/denoland/TypeScript/pull/15 as part of this change."
      ],
      "deno-comprehensive-test-verification": [
        "It might be better to test the full output so that the test is more resilient to change (ex. the fixes being moved somewhere else)"
      ],
      "deno-explicit-dependency-configuration": [
        "Can you revert these changes to the `deno_` crates? I don't think deno_doc is used anywhere else. I'd like to keep this here so that if we ever switch other crates to depend on this crate it's more obvious and we can have more of a discussion. The non-deno crates are fine though.\r\n\r\nAlso, the CI is failing because `cargo check` needs to be run I think."
      ],
      "deno-environment-loading-order": [
        "I think you want to move this above creating the factory and use `flags.env_file` instead. That way the changed environment variables propagate into everything created by the factory.",
        "We'll need to remove the usage of `.env` because the env file hasn't been loaded at this point. Do a search through the other code to see how it's done, but basically we need to load the env variables after loading the env file.",
        "I have a pending custom lint rule I need to implement for this: https://github.com/denoland/deno/issues/29886"
      ],
      "deno-control-cache-lifecycle": [
        "It's better to do this at a higher level so the caller can decide when to free these from the cache.",
        "Separated the caching to separate the concerns (and it allows for a long lived service and short lived cached)",
        "We should improve this to be smarter. Right now I just have it clearing before and after each op_resolve.",
        "Oh, actually I think we can just clear when the dependents and config are recreated. I will move this there.",
        "Ah, seems tricky. Yeah I'm going to switch to clear after the request is completed."
      ],
      "deno-defensive-null-handling": [
        "```suggestion\r\n  const formattedMessage = message.length === 0 ? \"\" : `${message} `;\r\n  return op_read_line_prompt(formattedMessage, `${defaultValue}`);\r\n```"
      ]
    },
    "profile": {
      "location": "Toronto, Canada",
      "company": "@denoland",
      "blog": "https://david.deno.dev",
      "twitter_username": "DavidSherret",
      "site_admin": false,
      "followers": 1991,
      "following": 25
    }
  },
  "dougwilson": {
    "repos": [
      "expressjs/express"
    ],
    "entries": [
      {
        "slug": "express-access-settings-properly",
        "title": "Access settings properly"
      },
      {
        "slug": "express-accurate-jsdoc-documentation",
        "title": "Accurate JSDoc documentation"
      },
      {
        "slug": "express-clear-array-operations",
        "title": "Clear array operations"
      },
      {
        "slug": "express-clear-intention-in-names",
        "title": "Clear intention in names"
      },
      {
        "slug": "express-enforce-null-safety-patterns",
        "title": "Enforce null safety patterns"
      },
      {
        "slug": "express-ensure-test-completion",
        "title": "Ensure test completion"
      },
      {
        "slug": "express-exclude-sensitive-configurations",
        "title": "Exclude sensitive configurations"
      },
      {
        "slug": "express-follow-standardjs-when-modifying",
        "title": "Follow StandardJS when modifying"
      },
      {
        "slug": "express-handle-streams-properly",
        "title": "Handle streams properly"
      },
      {
        "slug": "express-optimize-hot-paths",
        "title": "Optimize hot paths"
      },
      {
        "slug": "express-propagate-errors-properly",
        "title": "Propagate errors properly"
      },
      {
        "slug": "express-purposeful-style-changes",
        "title": "Purposeful style changes"
      },
      {
        "slug": "express-rest-principles-first",
        "title": "REST principles first"
      },
      {
        "slug": "express-single-source-documentation",
        "title": "Single source documentation"
      },
      {
        "slug": "express-standardize-dependency-version-notation",
        "title": "Standardize dependency version notation"
      },
      {
        "slug": "express-structured-release-workflows",
        "title": "Structured release workflows"
      },
      {
        "slug": "express-use-unique-password-salts",
        "title": "Use unique password salts"
      }
    ],
    "comments": {
      "express-rest-principles-first": [
        "I agree. The `Blob` is just a byte stream, so whatever the `type` is shouldn't be touched, as Express has no way to know what the charset is of the bytestream.",
        "Makes sense. I think that is what this pr is already doing, but if not, it shouldn't set the type if already set, which is what all the other arguments to send do.",
        "I'm not sure why this was marked resolved, as nothing was fixed and the issue @jimmywarting mentioned with adding charset is still there.",
        "We cal always be more restrictive than the spec and then loosen up later (but not the other way around), so alphanumerics would at least be a start :)\n"
      ],
      "express-enforce-null-safety-patterns": [
        "Probably don't want to reassign values linked to arguments, since it silently alters `arguments[0]`. Usually I would just name the argument `options` and this line as `var opts = options || {}`\r\n```\r\n$ node -pe '(function(foo){foo=foo||{};return arguments[0] === null}(null))'\r\nfalse\r\n```",
        "Usually not best practice to alter the object someone is passing in; the object could potentially be frozen, even, causing an exception here. `var createServer = opts.createServer || http.createServer` is probably fine (and then changing the vars below).",
        "Pleas do not use in for option detection. This makes the interface really confusing because you have to ensure that whatever you are passing in does not have that key anywhere in the prototype chain.\n",
        "Do not pass in non-booleans to the sub module, because that is just asking for undefined behavior later. The etag option only accepts true or false.\n"
      ],
      "express-standardize-dependency-version-notation": [
        "Be sure to keep the `~` symbol; if a 2.0.7 is published, then a user upgrading from a 4.17 version of a 4.18 version would end up downgrading the `proxy-addr` installed.",
        "You cannot use `^` in our `package.json` file.\n",
        "`^` is not compatible with all the base installed for 0.10.x. people would have to upgrade their npm just to use express, which is not always possible in certain enterprise environments. plus we don't only hard-pin external dependencies since it's too easy for them to break people's express otherwise\n"
      ],
      "express-clear-array-operations": [
        "`[key].concat(fns)` will inadvertinately allow for the arguments to be arrays, because it'll flatten in certain cases, i.e.:\n\n``` js\n$ node -pe '[2].concat([1,2])'\n[ 2, 1, 2 ]\n```\n"
      ],
      "express-purposeful-style-changes": [
        "I know the PR was updated already, but I figured I'll reply for just information sharing: the short answer is no, but the long answer is that PRs are accepted as long as they provide some kind of merit; for example, if there is a demonstrable perf improvement."
      ],
      "express-use-unique-password-salts": [
        "> here '10' is the salt used for encryption\r\n\r\nI thought your PR said you don't need a salt? Can you vlarify this comment and/or PR description?",
        "Gotcha. So then the comment right above this seems misleading/confusing:\r\n\r\n> A better Way to Hash Password without passing any salt using Bcrypt.Js\r\n\r\nit says \"without passing any salt\", but this line's comment says \"here '10' is the salt\""
      ],
      "express-clear-intention-in-names": [
        "`this.getHeader(\"transfer-encoding\")` should be `this.get(\"Transfer-Encoding\")`",
        "It is, but the change here is to use `this.get`, not `this.getHeader`. In addition, they are both case-insensitive, but we use the standard header casing in the code (as you can see from all the other parts).",
        "They are private, which is why they are not documented and the testing for them is tested though the public APIs that utilize them 👍 ",
        "Yes, it is curious 😀 They are before my time so cannot answer the why, lol. Even an underscore doesn't actually stop folks from using stuff, and a large project even ends up needing to be csreful modifying those things. Really only way to protect is an inside out obect, clever usage of closures, or the newer private class members. Idk if 5.x will change it, as it is late in the dev on it and probably need to scout round for the usages to make sure they are still provided in some way and truely private them in like 6.\r\n\r\nI bet people are accessing settings directly, at least. Perhaps too cache if they wanted to clear it since there is no API to do so.",
        "This should probably have a different name, otherwise people will try to `app.dispatch(req, res)` with it.\n"
      ],
      "express-handle-streams-properly": [
        "Should we add a code path here to use https://nodejs.org/api/buffer.html#blobstream when supported? Not sure how usable that is, but seems Node.js is adding the ability to make a Blob from a file which could be huge.",
        "I'm not sure why a custom write stream needs to be added, but it is missing backpressure handling, which is very important in http servers like this because the clients can stop reading if they know this is a large response, leading to a dos vector. But I would think you don't need to implement that at all, as you should be able to use `.toWeb()`/`.fromWeb()`, right? I ask because I'm not familiar with the new APIs for web streams (yet). So either we should use that or this custom stream needs to have backpressure handling added",
        "Just to note, if it doesn't work or make sense to land in the 4.x line due to back compat, that's no problem, as we have the 5.x line thay doesn't have that concern and as soon as I finish getting out a sec path for a diff module, the last 5.x pre release will be published so we can publish the final.",
        "`this.getHeader(\"transfer-encoding\")` should be `this.get(\"Transfer-Encoding\")`",
        "remove your aborted listener when done\n"
      ],
      "express-follow-standardjs-when-modifying": [
        "I would just leave this off the pr since it's a style only change.",
        "All net new code should be written in StandardJS style per https://github.com/expressjs/express/blob/master/Collaborator-Guide.md#prs-and-code-contributions",
        "Typically we do not indent these lines, keeping the `.` at the same indent as the function call. Can you revert all the re-formatting of the file that wasn't related to your changes?"
      ],
      "express-structured-release-workflows": [
        "I'm not sure if this was the way it has ever been done. Typically there are two main release flows: patch and non-patch.\n\nThe patch flow is pretty simple: it's just extremely simple updates, like typo fixes, patch dependency updates, and maybe a fix, depending on how risky it is. Every other change is always a non-patch flow.\n\nThe non-patch flow is done using a separate branch (for example `4.13`, `5.0`, etc.) and tracked using a release pull request (for example, https://github.com/expressjs/express/pull/2682, https://github.com/expressjs/express/pull/2237, etc.).\n"
      ],
      "express-accurate-jsdoc-documentation": [
        "This jsdoc does not reflect what you are doing. It documents the call as .param('id', [fn1, fn2])\n",
        "We had a PR about this right now, so we should make sure we land it right this time :) The `options` argument needs to be optional in the JSDoc here since it's optional in the code 👍 "
      ],
      "express-access-settings-properly": [
        "I would expect that if the user explicitly set the `etag` option to `res.sendFile`, it would not be overwritten by the setting.\n",
        "You should use the setting functions to access settings, not direct access to the object. Direct object access was deprecated in 3.0.\n"
      ],
      "express-single-source-documentation": [
        "Should this section link to https://github.com/expressjs/express/blob/master/Contributing.md#tc-process , just replace that part of that other document, or something else? It seems like a duplication and could easily get lost where we make a change to one but forget the other, so the less duplication probably the better.",
        "All of these \"you\"s need to be re-worded in to the third person from the second person, as the document is meant to be in the third person form."
      ],
      "express-exclude-sensitive-configurations": [
        "Since certs have an expiration date, I would suggest not checking them in and either leave it as an exercise for the user running the example to create it with the instructions you provided or use one of those npm modules that auto generate them."
      ],
      "express-ensure-test-completion": [
        "without an `else` here, a failure of the check leaves the test suite hanging. if you are testing the body contents, just use supertest to do it rather than manually checking res.body which would simplify it.",
        "Darn. That particular tests doesn't do much at all. It seems like you likely just need to blanket increase the timeout for the environment you're running in. Are you about to run the test suite like the following? `npm test -- --timeout 7500` ?",
        "Ah, I see, there is a an issue with npm passing arguments around. Any reason you are using `npm run test-ci` ? That is meant just for the usage of the GitHub Actions CI here. Why not just use `npm test` ?",
        "Since this is not an async test, you don't need to use `done` here and just throw the errors",
        "use mocha, not try-catch\n"
      ],
      "express-propagate-errors-properly": [
        "If there is an error, it should ideally be forwarded to the error handling pipeline rather than swallow and just ending the response wirh no info.",
        "It should forward the error to the error handling pipeline, not just output the error directly. May sites what users to see a customized error page and have the error written to their logs, not displayed to the end user. The error pipeline (through error handling middlewares) allows this to be customized.",
        "You probably need to remove this handlers before proceeding. The error handler itself that `next` may try to write to `res` and cause an error, and this would end up involing `next` a second time.",
        "Also on this res error, it probably means we need to stop the pipe from the blob so it doesn't keep writing to res. ",
        "I'm not sure if the examples should demonstrate that any possible error above should result in a 400. What if the data source to get the note has an issue (like cannot connect to db)? One would normally expect it to 500 rather than 400. I also wonder if these should be using the Express error handling mechanism to respond rather than demo putting a try...catch in every single handler (except `getAll` that doesn't need one for some reason)? The reason I ask is because ideally we want to demo using Express.js and it's features in our demo apps as much as possible.",
        "the aborted error should only go to `fn` if there is one; never to `next`. just change it so it doesn't go to `next` :)\n"
      ],
      "express-optimize-hot-paths": [
        "For any Node.js version where `require('stream/web')` doesn't work, this is going to cause a sync walk of the file system every time, since a `require()` failure is not added to the module cache like a success is. This would ideally be moved to only happen on the loading of this module, not on every single response write.",
        "this creating fake contexts is expensive; could use refactoring somewhere to not need to do this.\n"
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 3715,
      "following": 12
    }
  },
  "vaxerski": {
    "repos": [
      "hyprwm/Hyprland"
    ],
    "entries": [
      {
        "slug": "hyprland-api-inputoutput-validation",
        "title": "API input/output validation"
      },
      {
        "slug": "hyprland-avoid-expensive-hot-paths",
        "title": "avoid expensive hot paths"
      },
      {
        "slug": "hyprland-avoid-unintentional-defaults",
        "title": "Avoid unintentional defaults"
      },
      {
        "slug": "hyprland-configuration-consistency-validation",
        "title": "configuration consistency validation"
      },
      {
        "slug": "hyprland-consistent-naming-conventions",
        "title": "consistent naming conventions"
      },
      {
        "slug": "hyprland-dynamic-configuration-handling",
        "title": "Dynamic configuration handling"
      },
      {
        "slug": "hyprland-explain-documentation-rationale",
        "title": "Explain documentation rationale"
      },
      {
        "slug": "hyprland-maintain-clean-code-structure",
        "title": "Maintain clean code structure"
      },
      {
        "slug": "hyprland-no-braces-short-ifs",
        "title": "no braces short ifs"
      },
      {
        "slug": "hyprland-optimize-computational-efficiency",
        "title": "optimize computational efficiency"
      },
      {
        "slug": "hyprland-optimize-with-bit-manipulation",
        "title": "optimize with bit manipulation"
      },
      {
        "slug": "hyprland-optimize-workflow-triggers",
        "title": "optimize workflow triggers"
      },
      {
        "slug": "hyprland-precise-configuration-patterns",
        "title": "Precise configuration patterns"
      },
      {
        "slug": "hyprland-precise-documentation-language",
        "title": "Precise documentation language"
      },
      {
        "slug": "hyprland-prefer-managed-pointers",
        "title": "prefer managed pointers"
      },
      {
        "slug": "hyprland-prefer-stderror-code-parameters",
        "title": "prefer std::error_code parameters"
      },
      {
        "slug": "hyprland-prevent-null-dereferences",
        "title": "prevent null dereferences"
      },
      {
        "slug": "hyprland-semantic-variable-naming",
        "title": "Semantic variable naming"
      },
      {
        "slug": "hyprland-validate-environment-variables",
        "title": "validate environment variables"
      },
      {
        "slug": "hyprland-weak-pointer-callback-safety",
        "title": "weak pointer callback safety"
      }
    ],
    "comments": {
      "hyprland-semantic-variable-naming": [
        "this is a bannable offense in software development. \"duration\" without a unit specifier, that is. `durationUs` is ok if you really want to avoid µ",
        "a name more like `allow_pin_fullscreen` makes more sense intuitively\r\n\r\n+ missing documentation in configDescriptions.hpp",
        "style:\r\n these arent pointers, no `p` prefix\r\n consts are `CAPS`\r\n \r\n`CURRENTWINDOWFSMODE` for example",
        "no snake_case, we use camelCase\r\n\r\nadditionally, const values we SHOUT\r\n\r\nso `TARGETPORTION` and `POSITION`"
      ],
      "hyprland-optimize-computational-efficiency": [
        "prefer distanceSq",
        "I meant `CCompositor::ensurePersistentWorkspacesPresent`",
        "well then it should, no?",
        "that is fine ye",
        "why this change? why not just multiply it beforehand?",
        "makes sens",
        "wouldn't `std::erase_if` work better here?"
      ],
      "hyprland-optimize-workflow-triggers": [
        "I'd drop the cron. If something is skipped, we can do it manually. Especially when the number of issues grows, this can get ratelimited and slow, etc. I've already seen it with other actions.",
        "will this not just fail on regular commits? Should we just dump this to a separate workflow?",
        "like one that runs only on MRs"
      ],
      "hyprland-prevent-null-dereferences": [
        ".lock() != nullptr redundant, Hyprutils::Memory::CWeakPointer has an operator bool",
        "needs a check for nulls here. This could crash if you call `hyprctl layers` when a layer is fading out, I believe.",
        "whats with all those parentheses. Also, `== nullptr` is the same as `!var`",
        "this will crash idiot",
        "wait, lastmonitor can be null",
        "redundant != nullptr",
        "I meant only the `!= nullptr` part\r\n\r\n`if (g_pCompositor->m_pLastFocus)`",
        "m_pLastWindow still can be null. This would be a nullptr dereference aka segfault.\r\n\r\n```cpp\r\ng_pCompositor->m_pLastWindow ? g_pCompositor->m_pLastWindow->m_bIsFullscreen : false;\r\n```",
        "what?"
      ],
      "hyprland-api-inputoutput-validation": [
        "hm, this might bug out if the rest of the request has the thing. Maybe substring only the part after the first / and before the first space?",
        "you didn't but when already changing it why not improve?",
        "it's not input validation - it's making sure you choose the right value in case of e.g.\r\n\r\n```\r\n/notify blah blah /decorations\r\n```",
        "no, the /decorations is a part of the parameter. Notify for example takes a string, so the user can put whatever.",
        "missing escapeJSONStrings"
      ],
      "hyprland-explain-documentation-rationale": [
        "too long. \"This is important to avoid clutter, spam, and make the issues more readable\".",
        "If you explicitly state that failing to do so will get your issue removed they will do it I'd say"
      ],
      "hyprland-prefer-stderror-code-parameters": [
        "while we are redoing this, this can throw.\r\n\r\n```\r\nstd::error_code ec;\r\nif (std::filesystem::exists(..., ec) && !ec)\r\n```",
        "canonical can throw. Needs an `std::error_code` and handling",
        "all of this might throw so probably should be in a try catch",
        "every std::filesystem call can throw unless supplied with a std::error_code as the second arg, so the entire thing",
        "instead of try catch which kinda sucks can't you just `std::error_code e1, e2;` and pass them to the `is_*` fns?\r\n\r\nthen just `if (e1 || e2) continue`"
      ],
      "hyprland-precise-documentation-language": [
        "See I don't want to add that because imagine a scenario one day where a corporation starts using any part of my projects, and I want a cut from them. I have no leverage, as this disallows me from adding a clause prohibiting commercial use / use by commercial entities.",
        "OSS definition? Where do I define as that? You mean line 9?\r\n\r\nAlso, I believe it's more like dropping the F rather than the OSS xD\r\n\r\nHow about `license does not hinder the ability of any non-commercial entity to contribute to, redistribute or view the source code of the project.`",
        "In all honesty - based on a lot of the open source initiative points I've seen - it's a damn pipe dream and completely unrealistic for the modern world. You will _not_ get rid of the tech giants fucking people over in any way other than by law.",
        "committed, thanks!",
        "then you have an open end to the traits, and someone can argue very nasty illegal stuff are their \"trait\". Protected traits are a law term used throughout countries to make it _not_ open ended, so that this doesn't occur.",
        "  well that's how it's worded, no?\r\n  \r\n  Do not harass, attack, or in any other way discriminate against anyone, **including** for their protected traits,",
        "but why? There is a strong \"no harassment\" point. You can make this argument forever, adding traits until the end of time. We put some traits as examples, not as an exhaustive list (note the _including_ and _etc._)\r\n\r\nPeople who want to harass others will always find another reason to harass someone. (as you said, \"I've witnessed discrimination based on so many traits\")",
        "It does, but at some point it clutters the CoC, and makes people less inclined to read it because of the length.\r\n\r\nI believe we've covered enough common sources of specific targeted hate, and the rest fall under \"etc\".\r\n\r\n\"no harassment\" is still \"no harassment\", IMO.",
        "as we are the only members of the org :)"
      ],
      "hyprland-weak-pointer-callback-safety": [
        "```cpp\r\nPBUFFER->onBackendRelease([wb = WP<IHLBuffer>{PBUFFER}] {\r\n    if (wb)\r\n        wb->unlock();\r\n});\r\n```",
        "double .lock() is redundant"
      ],
      "hyprland-consistent-naming-conventions": [
        "`m_enabled`. We are moving to drop hungarian see #9061\r\n\r\nalso please initialize it",
        "no snake. `wantsEnabled`",
        "naming convention: classes have a `C` prefix, so `CXCBConnection`",
        "enums in CAPS and with their short name in the beginning, e.g.:\r\n`DIR_AUTO_UP`",
        "enums prefix `e`: `eAutoDirs`",
        "if we are rewriting this, some style things to consider.\r\n\r\nFirstly, this could be wrapped in a namespace e.g. `namespace Systemd`\r\n\r\nSecondly, no_snake, we use camelCase"
      ],
      "hyprland-maintain-clean-code-structure": [
        "this was unnecessary, please keep function bodies outside of headers",
        "all .frag files: I'd prefer them in `src/render/shaders/glsl`, then compiled to `src/render/shaders`",
        "no need to add additional includes here - my original mistake of doing that pays now.\r\n\r\nMove them to keybindmanager.cpp to shorten compile time.",
        "oh yeah I missed this... we can't have imports using src paths\r\nalways relative",
        "fixed",
        "better?\r\n",
        "why not put them inside CHyprCtl? It's a singleton. If you prefer, you can even make these static.",
        "private comes last",
        "I'd avoid global funcs tbh. Why not `namespace FontUtils`?",
        "we don't use `this` where unnecessary"
      ],
      "hyprland-validate-environment-variables": [
        "instead of this, just pass a `add_compile_definitions` in the cmake",
        "I'd **highly** recommend to get nix to set  _at least_ GIT_COMMIT_HASH, otherwise hyprpm will not work."
      ],
      "hyprland-dynamic-configuration-handling": [
        "does libinput verify the passed val? I'd clamp it.",
        "yep",
        "why are we handling this? This is handled by hyprlang, no?!",
        "I think it's just because this MR has a weird approach. Why not use the approach like for device configs? After a dyncall, or after parsing, recheck monitorv2 stuff. If anything changed, update and reload monitor mode",
        "don't do this. Instead, do `handleMonitorV2` on reload in the reload func. Here, just do:\r\n```cpp\r\ng_pEventLoopManager->doLater([this] {\r\ng_pConfigManager->m_wantsMonitorReload = true;\r\n});\r\n```",
        "this will make it unchangeable dynamically. Why not do the same as with `debug:disable_logs`?",
        "dynamically means with hyprctl.\r\n\r\n`disable_logs` is a static pointer, not a value copy.",
        "you can use `configStringToInt`, it handles all of these cases and more.\r\n\r\n`PANIM->second.internalEnabled = configStringToInt(ARGS[1]);`",
        "why not use the same `->set` flag here? instead of the entire change to how `deviceConfigExists` works?",
        "1. getDeviceConfig()->set\r\n2. I don't think so. If a device config exists, all vars belonging to it exist",
        "Again, you can get the struct. We cannot receive ptrs to device configs as they are not static IIRC.\r\n\r\nWhat is your problem? It returns the struct. The struct has a `set` property.",
        "final print of the MR looks good to me.\r\n\r\nI squash everything anyways\r\n\r\nIf there isnt anything else, I am alright with merging this as-is"
      ],
      "hyprland-precise-configuration-patterns": [
        "shouldn't this be `src/*.h*`? Otherwise it will consume all the subprojects and shit",
        "`.xml`s and `.c`s should also be excluded. Ideally it should be pattern `.h` only"
      ],
      "hyprland-prefer-managed-pointers": [
        "raw ptrs are banned in new hl code unless necessary, pass a SP",
        "generally, in new protocol impls, prefer a UP<> for m_resource. You can still take a WP<> to it.",
        "raw ptrs are banned in new code",
        "hm. I don't think they should be shared for no reason. I'll write a unique_ptr in hyprutils later today that allows weak pointers, if I don't by tomorrow remind me.\r\nI've wanted one anyways.",
        "class is boring, here you go https://github.com/hyprwm/hyprutils/commit/423c69d697f56af4f8f2de7e2279eead17901228",
        "https://github.com/hyprwm/Hyprland/pull/9143",
        "sls container should now store a UP if you rebase on main and you can make this func return a WP",
        "the reason popup and subsurface are C pointers is that they haven't yet been rewritten. Please use managed pointers (a WP here)"
      ],
      "hyprland-optimize-with-bit-manipulation": [
        "first time I see this syntax what the fuck? xD",
        "can someone verify this works tho? xD like with a small cpp tester",
        "huh. I know what a bitfield is, just never seen this syntax. Great, guess we all learn every day. Thanks!",
        "here and under, use the bitshift notation `(1 << 0)` `(1 << 1)` etc",
        "with bitfields please make all fields except 0 a `(1 << x)`",
        "it's a stylistic choice, for consistency"
      ],
      "hyprland-no-braces-short-ifs": [
        "style: no {} around short ifs",
        "no {} around short ifs",
        "needs to ignore windows that:\r\n - aren't mapped\r\n - are fading out\r\n - are XWayland OR",
        "prefer guards.\r\n\r\n```cpp\r\nif (bad)\r\n   continue;\r\n```",
        "no {}\r\n\r\nno `!= NULL`\r\n\r\nconst are CAPS\r\n\r\nyou can use auto\r\n\r\n```\r\nif (const auto CFG_ENV = getenv(\"HYPRLAND_CONFIG\"); CFG_ENV)\r\n```",
        "no {} around short ifs",
        "this is not a short if, please {}",
        "style nit: no {} around short ifs",
        "style: here and hereafter, no {} around short ifs",
        "style: no {} around short ifs",
        "no {} around short ifs",
        "no {} around short ifs",
        "style nit: no `{}` around short ifs",
        "a) no {} around short ifs\r\nb) `forceFloat != 0 && forceFloat - 1 == !PCURRENT->m_bIsFloating`",
        "also why is this even in the if, can be extracted outside of it to avoid duplicating code",
        "> It's there twice because it depends on either PWINDOW or PCURRENT, which is defined inside the outer if.\r\n\r\nright. The other can be extracted, though.",
        "> You sure you want that shortened version? Seems way less clear what the logic is.\r\n\r\nyea",
        "yeah tho tbh I don't see why the PCURRENT check is necessary, all windows in a group should have their floating state synced. You can remove the second check mostl ikely",
        "yea lgtm",
        "prefer guards\r\n```cpp\r\nif (!sls->mapped)\r\n    continue;\r\n```",
        "style: no {} around short ifs",
        "1: style, no {} around short ifs\r\n2: leak: not destroyed on .destroy",
        "RemoveBracesLLVM IIRC removes too much. We only remove around short ifs.",
        "style nit: no {} around short ifs"
      ],
      "hyprland-avoid-expensive-hot-paths": [
        "this should probably be static. It won't change, and I don't know how expensive drmGetCap is. Furthermore, you're duplicating this here and in protocolmanager. Maybe add a bool to CCompositor that is set in `::initServer`?",
        "can we skip damageBox if box.empty",
        "this check can be moved above PMONITOR finding a monitor for better optimization",
        "we should try the cached ones first instead of bothering the X server all the time",
        "Extreme performance kill. Move the variable setting to `setTabletConfigs` and store a copy inside STablet itself.\r\n\r\nUsing `getDevice*` outside `set*Config` is banned",
        "actually, no need to update if tickdelta is less than 1,1, so can you leave that in?",
        "if we are moving, rn, even if delta.x < 1 and delta.y < 1 we still process it, which is redundant",
        "tldr:\r\n```cpp\r\nif (((abs(TICKDELTA.x) < 1.f && abs(TICKDELTA.y) < 1.f) || (TIMERDELTA < MSMONITOR && canSkipUpdate)) && g_pInputManager->dragMode != MBIND_MOVE)\r\n```\r\ninto \r\n```cpp\r\nif ((abs(TICKDELTA.x) < 1.f && abs(TICKDELTA.y) < 1.f) || (TIMERDELTA < MSMONITOR && canSkipUpdate && g_pInputManager->dragMode != MBIND_MOVE))\r\n```"
      ],
      "hyprland-avoid-unintentional-defaults": [
        "you still left the (always include)",
        "actually, it's fine how it is."
      ],
      "hyprland-configuration-consistency-validation": [
        "is there any reason for this to be configurable? Shouldn't it be on by default? What are the caveats?",
        "unlikely. I'd just keep it hardcoded on",
        "you've adjusted it here, but forgot src/config/defaultConfig.hpp\r\n\r\nAlso IIRC these are in the wiki as well, so might wanna change them there too",
        "I'd rather not have this one, people usually set it in /etc/environment etc"
      ]
    },
    "profile": {
      "location": "Acheron, Hades",
      "blog": "https://vaxry.net",
      "site_admin": false,
      "followers": 2353,
      "following": 10
    }
  },
  "micalevisk": {
    "repos": [
      "nestjs/nest"
    ],
    "entries": [
      {
        "slug": "nest-avoid-testing-anti-patterns",
        "title": "Avoid testing anti-patterns"
      },
      {
        "slug": "nest-choose-meaningful-identifier-names",
        "title": "Choose meaningful identifier names"
      },
      {
        "slug": "nest-comprehensive-dependency-security-checks",
        "title": "Comprehensive dependency security checks"
      },
      {
        "slug": "nest-configurable-log-formatting",
        "title": "Configurable log formatting"
      },
      {
        "slug": "nest-descriptive-identifier-names",
        "title": "Descriptive identifier names"
      },
      {
        "slug": "nest-document-configuration-behaviors",
        "title": "Document configuration behaviors"
      },
      {
        "slug": "nest-explicit-default-configurations",
        "title": "Explicit default configurations"
      },
      {
        "slug": "nest-follow-protocol-standards",
        "title": "Follow protocol standards"
      },
      {
        "slug": "nest-http-header-management",
        "title": "HTTP header management"
      },
      {
        "slug": "nest-modern-null-safety-patterns",
        "title": "Modern null safety patterns"
      },
      {
        "slug": "nest-pin-dependency-versions",
        "title": "Pin dependency versions"
      },
      {
        "slug": "nest-proactive-dependency-security",
        "title": "Proactive dependency security"
      },
      {
        "slug": "nest-proper-asynchronous-error-handling",
        "title": "Proper asynchronous error handling"
      },
      {
        "slug": "nest-standardize-logger-configuration-patterns",
        "title": "Standardize logger configuration patterns"
      },
      {
        "slug": "nest-standardize-null-safety-patterns",
        "title": "Standardize null safety patterns"
      },
      {
        "slug": "nest-structure-behavior-driven-tests-properly",
        "title": "Structure behavior-driven tests properly"
      },
      {
        "slug": "nest-use-consistent-control-structures",
        "title": "Use consistent control structures"
      },
      {
        "slug": "nest-use-consistent-curly-braces",
        "title": "Use consistent curly braces"
      },
      {
        "slug": "nest-use-factory-providers",
        "title": "Use factory providers"
      }
    ],
    "comments": {
      "nest-use-consistent-control-structures": [
        "I prefer your way tbh. I thought `npm run lint:fix` would fix that :p"
      ],
      "nest-configurable-log-formatting": [
        "I think having this interface wouldn't be that easy to write the output that that Issue need because `pidMessage` is too tied with the default formatting and uses that `color` function\r\n\r\nCan you try to rewrite this `formatMessage` like this:\r\n\r\n```ts\r\nprotected formatMessage(\r\n  pid: number,\r\n  logLevel: string,\r\n  context: string,\r\n  timestampDiff: number,\r\n  output: string,\r\n): string {\r\n  return `` ...\r\n}\r\n```\r\n\r\nbut then we'll need to change the `updateAndGetTimestampDiff` method to extract the coloring stuff from it and make it return a number instead of string.\r\n\r\nAnd then make `formatMessage` return an string with the color applied instead of applying it on `printMessages`. But yeah, that could be a bit harsh\r\n"
      ],
      "nest-modern-null-safety-patterns": [
        "```suggestion\r\n    return pattern?.test(str);\r\n```",
        "I guess this would be better for readability:\r\n\r\n```suggestion\r\n    if (!isNil(this.min) && float < this.min) {\r\n```\r\n\r\n`import { isNil } from '../utils/shared.utils';`"
      ],
      "nest-proactive-dependency-security": [
        "is there any link where we can see that? because there were no reports on `npm audit` \r\n\r\n![image](https://github.com/user-attachments/assets/2243ed54-2069-4f3c-89b2-68160da1e913)\r\n"
      ],
      "nest-document-configuration-behaviors": [
        "@IlliaHalchun you can find the docs here: https://docs.nestjs.com/techniques/caching#use-module-globally\r\n\r\nfeel free do change it if needed",
        "```suggestion\r\n   * Defines if file parameter is optional.\r\n   * @default false\r\n   */\r\n```",
        "we can't use `@default` tag here because `KafkaOptions['options']` is being used by both client and server"
      ],
      "nest-structure-behavior-driven-tests-properly": [
        "```suggestion\r\n      await expect(catsController.findAll()).resolves.toStrictEqual([]);\r\n```\r\n\r\ncan we use this instead? https://jestjs.io/docs/tutorial-async#asyncawait",
        "```suggestion\r\n```\r\n\r\nif we're testing `CatsService#findAll`, we shouldn't mock its implementation, otherwise we end up testing nothing.\r\n\r\nInstead, you somehow should do `catService.cats = result`. I guess it's fine do write it like this:\r\n\r\n```ts\r\n// @ts-ignore\r\ncatService.cats = result\r\n```\r\n"
      ],
      "nest-avoid-testing-anti-patterns": [
        "```suggestion\r\n```\r\n\r\nif we're testing `CatsService#findAll`, we shouldn't mock its implementation, otherwise we end up testing nothing.\r\n\r\nInstead, you somehow should do `catService.cats = result`. I guess it's fine do write it like this:\r\n\r\n```ts\r\n// @ts-ignore\r\ncatService.cats = result\r\n```\r\n"
      ],
      "nest-http-header-management": [
        "Note that since the return of `response.getHeader('Content-Type')` depends on the arguments provided to `response#setHeader` then this condition could be mislead(?). For instance, if we pass any falsy value to `response.setHeader` in some controller's method, like `response.setHeader('Content-Type', '')`, we'll not receive that content type.\r\n\r\nI know this scenario is weird but since I _(as a Nest user)_ have explicity called `response.setHeader` in my code, it would be strange to receive the default value instead. What do you think?",
        "I was thinking more in HTTP client usage and where the content type value is defined dynamically (or sort of). Instead of identifying quickly, by looking into the headers sent, that the value was wrong for something that you've implemented, the dev will see another value -- maybe this could be documented, idk.\r\n\r\nBut I agree with you now. Since we're in dealing with a framework is better to apply some restrictions. ty!",
        "oh I just read how fastify handles that here: https://github.com/fastify/fastify/blob/7e18edcf76fb58dc33b842b1dba14a425dd6feba/lib/reply.js#L136-L142 looks like they **do allow** falsy values.\r\n\r\nSo the `AbstractHttpAdapter#reply` will not behave in the same way for both adapters in those edge cases. Do you guys think this could be an issue somehow? "
      ],
      "nest-standardize-logger-configuration-patterns": [
        "instead of `console.error` we could use REPL's logger that was supplied to `NestFactory.createApplicationContext` above",
        "I think having this interface wouldn't be that easy to write the output that that Issue need because `pidMessage` is too tied with the default formatting and uses that `color` function\r\n\r\nCan you try to rewrite this `formatMessage` like this:\r\n\r\n```ts\r\nprotected formatMessage(\r\n  pid: number,\r\n  logLevel: string,\r\n  context: string,\r\n  timestampDiff: number,\r\n  output: string,\r\n): string {\r\n  return `` ...\r\n}\r\n```\r\n\r\nbut then we'll need to change the `updateAndGetTimestampDiff` method to extract the coloring stuff from it and make it return a number instead of string.\r\n\r\nAnd then make `formatMessage` return an string with the color applied instead of applying it on `printMessages`. But yeah, that could be a bit harsh\r\n"
      ],
      "nest-use-consistent-curly-braces": [
        "I prefer your way tbh. I thought `npm run lint:fix` would fix that :p"
      ],
      "nest-proper-asynchronous-error-handling": [
        "```suggestion\r\n        body.getStream().once('error', (err: Error) => {\r\n```\r\n\r\nto prevent the error 'Cannot set headers after they are sent to the client' if for whatever reason the _error_ event is emitted multiple times (not sure if this is possible tho)"
      ],
      "nest-descriptive-identifier-names": [
        "to me, `isEmpty` sounds that it could be used on non-array values. But I saw that it is only being used on arrays\r\n\r\npeharps we could rename this utility to `isEmptyArray` and drop the following:\r\n\r\nhttps://github.com/nestjs/nest/blob/e1b91d02a601c03cb8d0438b32badfaae5403447/packages/common/pipes/file/parse-file.pipe.ts#L63",
        "what do you think on renaming `value` to `fileOrFiles` or `filesOrFile`",
        "oh right.\r\n\r\nCan we have `shouldFlushLogsOnOverride: boolean`, and `flushLogsOnOverride(): void` method instead? Otherwise I'll add `setFlushLogsOnOverride(value: boolean)`\r\n"
      ],
      "nest-explicit-default-configurations": [
        "```suggestion\r\n    this.rawOutputPackets = this.getOptionsProp(options, 'rawOutputPackets', false);\r\n```\r\n\r\nhttps://github.com/nestjs/nest/blob/8617ee9952f4961841c8609329de9627cd8087f9/packages/microservices/server/server.ts#L146-L151",
        "```suggestion\r\n   * Defines if file parameter is optional.\r\n   * @default false\r\n   */\r\n```",
        "we can't use `@default` tag here because `KafkaOptions['options']` is being used by both client and server"
      ],
      "nest-pin-dependency-versions": [
        "nothing much https://github.com/actions/checkout/compare/v2...v3\r\n"
      ],
      "nest-use-factory-providers": [
        "I notice another good side-effect on changing `ExternalContextCreator` and `SerializedGraph` providers to factory:\r\n\r\n`SerializedGraph#toJSON` was called 6x in a very simple nestjs app before these changes, which I think it was useless because it was only invoked due to the name `toJSON` being known as a special method for `JSON.stringify` (used by `stringify` from `fast-safe-stringify`)\r\n\r\n:partying_face: ",
        "I guess we can also suggest them to use factory providers over value providers",
        "and I'm not sure if the word _object_ here would help the end user. But yeah, this is not a log to the app, it's an internal one"
      ],
      "nest-choose-meaningful-identifier-names": [
        "I prefer the `RouteSchema` name (`route-schema.decorator.ts`). Kinda following the same convention as the others decorators ",
        "what do you think on renaming `value` to `fileOrFiles` or `filesOrFile`",
        "oh right.\r\n\r\nCan we have `shouldFlushLogsOnOverride: boolean`, and `flushLogsOnOverride(): void` method instead? Otherwise I'll add `setFlushLogsOnOverride(value: boolean)`\r\n"
      ],
      "nest-follow-protocol-standards": [
        "Note that since the return of `response.getHeader('Content-Type')` depends on the arguments provided to `response#setHeader` then this condition could be mislead(?). For instance, if we pass any falsy value to `response.setHeader` in some controller's method, like `response.setHeader('Content-Type', '')`, we'll not receive that content type.\r\n\r\nI know this scenario is weird but since I _(as a Nest user)_ have explicity called `response.setHeader` in my code, it would be strange to receive the default value instead. What do you think?",
        "I was thinking more in HTTP client usage and where the content type value is defined dynamically (or sort of). Instead of identifying quickly, by looking into the headers sent, that the value was wrong for something that you've implemented, the dev will see another value -- maybe this could be documented, idk.\r\n\r\nBut I agree with you now. Since we're in dealing with a framework is better to apply some restrictions. ty!",
        "oh I just read how fastify handles that here: https://github.com/fastify/fastify/blob/7e18edcf76fb58dc33b842b1dba14a425dd6feba/lib/reply.js#L136-L142 looks like they **do allow** falsy values.\r\n\r\nSo the `AbstractHttpAdapter#reply` will not behave in the same way for both adapters in those edge cases. Do you guys think this could be an issue somehow? "
      ],
      "nest-comprehensive-dependency-security-checks": [
        "is there any link where we can see that? because there were no reports on `npm audit` \r\n\r\n![image](https://github.com/user-attachments/assets/2243ed54-2069-4f3c-89b2-68160da1e913)\r\n"
      ],
      "nest-standardize-null-safety-patterns": [
        "```suggestion\r\n    return pattern?.test(str);\r\n```",
        "I guess this would be better for readability:\r\n\r\n```suggestion\r\n    if (!isNil(this.min) && float < this.min) {\r\n```\r\n\r\n`import { isNil } from '../utils/shared.utils';`"
      ]
    },
    "profile": {
      "location": "Manaus, Amazonas (Brazil)",
      "company": "Sr. Solutions Architect @TarkenAg",
      "blog": "https://www.linkedin.com/in/micalevisk",
      "site_admin": false,
      "followers": 337,
      "following": 54
    }
  },
  "brophdawg11": {
    "repos": [
      "remix-run/react-router"
    ],
    "entries": [
      {
        "slug": "react-router-api-backward-compatibility",
        "title": "API backward compatibility"
      },
      {
        "slug": "react-router-api-consistency-patterns",
        "title": "API consistency patterns"
      },
      {
        "slug": "react-router-api-naming-consistency",
        "title": "API naming consistency"
      },
      {
        "slug": "react-router-avoid-redundant-computations",
        "title": "avoid redundant computations"
      },
      {
        "slug": "react-router-avoid-timing-dependent-tests",
        "title": "avoid timing-dependent tests"
      },
      {
        "slug": "react-router-cancel-aborted-async-operations",
        "title": "Cancel aborted async operations"
      },
      {
        "slug": "react-router-complete-accurate-documentation",
        "title": "Complete accurate documentation"
      },
      {
        "slug": "react-router-configuration-compatibility-validation",
        "title": "configuration compatibility validation"
      },
      {
        "slug": "react-router-configuration-consistency-standards",
        "title": "configuration consistency standards"
      },
      {
        "slug": "react-router-configure-build-tools-properly",
        "title": "configure build tools properly"
      },
      {
        "slug": "react-router-configure-react-build-environments",
        "title": "Configure React build environments"
      },
      {
        "slug": "react-router-configure-rendering-modes-clearly",
        "title": "Configure rendering modes clearly"
      },
      {
        "slug": "react-router-dependency-version-ranges",
        "title": "dependency version ranges"
      },
      {
        "slug": "react-router-distinguish-falsy-vs-nullish",
        "title": "distinguish falsy vs nullish"
      },
      {
        "slug": "react-router-documentation-clarity-standards",
        "title": "documentation clarity standards"
      },
      {
        "slug": "react-router-documentation-generation-compatibility",
        "title": "documentation generation compatibility"
      },
      {
        "slug": "react-router-documentation-linking-standards",
        "title": "documentation linking standards"
      },
      {
        "slug": "react-router-extract-test-helpers",
        "title": "Extract test helpers"
      },
      {
        "slug": "react-router-graceful-error-handling",
        "title": "graceful error handling"
      },
      {
        "slug": "react-router-handle-ssr-hydration-mismatches",
        "title": "Handle SSR hydration mismatches"
      },
      {
        "slug": "react-router-hook-dependencies-stability",
        "title": "Hook dependencies stability"
      },
      {
        "slug": "react-router-http-protocol-compliance",
        "title": "HTTP protocol compliance"
      },
      {
        "slug": "react-router-maintain-naming-consistency",
        "title": "maintain naming consistency"
      },
      {
        "slug": "react-router-organize-related-code-together",
        "title": "organize related code together"
      },
      {
        "slug": "react-router-organize-test-scripts-properly",
        "title": "organize test scripts properly"
      },
      {
        "slug": "react-router-pin-problematic-dependencies",
        "title": "Pin problematic dependencies"
      },
      {
        "slug": "react-router-precise-null-type-checking",
        "title": "precise null type checking"
      },
      {
        "slug": "react-router-prefer-built-in-react-types",
        "title": "prefer built-in React types"
      },
      {
        "slug": "react-router-prefer-explicit-readable-constructs",
        "title": "prefer explicit readable constructs"
      },
      {
        "slug": "react-router-provide-explicit-error-handling",
        "title": "Provide explicit error handling"
      },
      {
        "slug": "react-router-remove-obsolete-configuration-options",
        "title": "Remove obsolete configuration options"
      },
      {
        "slug": "react-router-simplify-configuration-setup",
        "title": "Simplify configuration setup"
      },
      {
        "slug": "react-router-structure-documentation-interfaces",
        "title": "Structure documentation interfaces"
      },
      {
        "slug": "react-router-use-descriptive-semantic-names",
        "title": "Use descriptive semantic names"
      }
    ],
    "comments": {
      "react-router-documentation-generation-compatibility": [
        "Include this as a nested `dom` module underneath `react-router`",
        "Thanks!  Just got the `docs.ts` script updated to handle the new module",
        "We can lift the note above the example so it shows in the summary and then we shouldn't need to try to detect this - I updated that file in f96433f37",
        "Export this and `DOMRouterOpts` so they get picked up by typedoc"
      ],
      "react-router-avoid-timing-dependent-tests": [
        "Are we sure this test exhibits the actual bug?  I see this failure when run against `dev`:\r\n\r\n```\r\n● navigation blocking › proceeds from blocked state using browser history › proceeds after quick block of back navigation\r\n\r\n    expect(received).toBe(expected) // Object.is equality\r\n\r\n    Expected: \"/about\"\r\n    Received: \"/\"\r\n```\r\n\r\nThe reproduction for the original issue leaves you on the original blocked page (`/three`) but this test, without the fix, actually ends up going back 2 history locations?",
        "Ah ok yeah this is probably a JSDOM issue.  I'm going to test this through an integration test in a real browser in https://github.com/remix-run/remix/pull/9914 instead of trying to hack JSDOM into behaving correctly"
      ],
      "react-router-precise-null-type-checking": [
        "Adjust types based on whether the param is required/optional",
        "No longer optional - still maintains a default type of `any` though",
        "This is a type definition bug - key is not optional in the implementations of `getFetcher`/`deleteFetcher`"
      ],
      "react-router-use-descriptive-semantic-names": [
        "Rename for clarity:\r\n\r\n* `handlerContext`-> a `StaticHandlerContext` returned from `staticHandler.query()`\r\n* `requestContext` -> The `AppLoadContext` used passed to `staticHandler.query` by Remix SSR today\r\n* `routerContext` -> This new `DefaultRouterContext` used for SPAs since it's not tired to a request like the SSR use case\r\n\r\nIn shared code, `requestContext` becomes `routerContext`",
        "I wonder if we should rename this to `navigationType` in v7 to align with `useNavigationType`...?",
        "Could we rename this to something like `unblockBlockerHistoryUpdate` to align with the new approach?"
      ],
      "react-router-pin-problematic-dependencies": [
        "```suggestion\r\n        # PLEASE KEEP THIS PINNED TO 1.4.10 to avoid a regression in 1.5.*\r\n        # See https://github.com/changesets/action/issues/465\r\n        uses: changesets/action@v1.4.10\r\n```"
      ],
      "react-router-maintain-naming-consistency": [
        "This will now be auto-generated as just `HistoryRouter.md` so the URL stays stable even if we ever stabilize the API (which we won't in this case, but now it's consistent with how we handle `usePrompt` and others)",
        "Nit - we use the term \"browser\" in our code and repo playgrounds/templates.  I think the original motivation was to avoid any confusion of the SSR Server being a \"client\" of the RSC server.  I'm happy with either, but if we go with client it's probably worth updating our internal code to align.",
        "Is `pages` an explicit choice here?  Should we be consistent with https://github.com/remix-run/react-router-templates/tree/main/default/app/routes?"
      ],
      "react-router-http-protocol-compliance": [
        "Lets drop a link in here for easier lookup\r\n\r\n```suggestion\r\n  // HTTP/2 doesn't support status messages\r\n  // https://datatracker.ietf.org/doc/html/rfc7540#section-8.1.2.4\r\n```",
        "When a prerendered document redirects we fall back on an http-equiv redirect"
      ],
      "react-router-configuration-consistency-standards": [
        "We have a build time `__DEV__` constant we can swap out for dev/prod builds to allow the library to remain runtime agnostic\r\n\r\n```suggestion\r\n  if (__DEV__ && !alreadyWarned[message]) {\r\n```",
        "Parse args and set `NODE_ENV` first",
        "\"SPA Mode\" is specifically no SSR and only generating a root `index.html` - this `isSpaMode` flag is what tells the server.client not to SSR/hydrate beyond the root route.  "
      ],
      "react-router-graceful-error-handling": [
        "lol I was super confused for a bit - since we shouldn't be using this \"built-in\" hydration logic in Remix since we do our own hydration via `window.__remixContext`.  This is used to automatically hydrate from `StaticRouterProvider`'s `window.__staticRouterHydrationData` - but we specifically pass [`hydrate=false`](https://github.com/remix-run/react-router/blob/v7/packages/react-router-dom/ssr/server.tsx#L90) in the Remix SSR use case.\r\n\r\nTurns out we're just using the wrong implementation since we brought the Remix code over.  There's a dup version of this method in `ssr/errors.ts` that preserves the stack that we're not implementing since it found a local function with the same name.  \r\n\r\nThe original reason for automatically clearing stack traces is that it felt safer than assuming any DIY-SSR setups would always be stripping them on the server like Remix does so it prevented any accidental leakage of SSR stack traces.  Including the in dev-only mode felt like something advanced users could achieve via manual hydration.\r\n\r\nI think for now we can just de-dup them and maybe add a param to preserve the stack trace that we send from the Remix usage in `RouterProvider` and continue stripping in the RR case (`parseHydrationData`).\r\n\r\nI am also pretty sure the Remix usage can go away in v7 with single fetch but would need to look a bit deeper into that.  ",
        "Bubble a proper 404 to our UI if we're in a prerendered app and there is no static file on the server",
        "We have a built-in `warning` method we can use here:\r\n\r\n```suggestion\r\n      } catch (error) {\r\n          warning(\r\n            false,\r\n            \"Failed to save scroll positions in sessionStorage, <ScrollRestoration /> will not work properly.\"\r\n          );\r\n          console.error(error);\r\n      }\r\n```",
        "Yeah the idea there was to show the actual underlying error, but we could probably inline `error.message` into the warning as well if we wanted to keep just one console entry and avoid a `console.error`."
      ],
      "react-router-structure-documentation-interfaces": [
        "yeah I was hoping the signature right above showing the rest/spread would make that apparent - but we could adjust the wording to make it more explicit?\r\n\r\n<img width=\"1640\" height=\"868\" alt=\"Screenshot 2025-07-21 at 5 18 05 PM\" src=\"https://github.com/user-attachments/assets/dc3f1ffe-caea-4de2-a430-5c5c860f58d3\" />\r\n\r\n\r\nI don't like [what we're doing today](https://reactrouter.com/api/components/PrefetchPageLinks) by trying to document each individual property since we're just being a worse MDN at that point and I'd rather just link out to them.",
        "Extracted to an interface so it gets it's own docs page and to align with `DOMRouterOpts`"
      ],
      "react-router-prefer-built-in-react-types": [
        "Added a generic here so the remix layers could pass in their route type",
        "All these can go away and we just use the RR types - and users will add `context: appLoadContext` via module augmentation: https://reactrouter.com/dev/guides/upgrading/remix#step-7---update-types-for-apploadcontext"
      ],
      "react-router-configuration-compatibility-validation": [
        "```suggestion\r\nIf you need to prerender paths with dynamic/splat parameters, or you only want to prerender a subset of your static paths, you can provide an array of paths:\r\n```",
        "I might introduce `HydrateFallback` here since it can/should be used for this scenario where you have only a `clientLoader`, and no `loader`.  In that scenario, `clientLoader.hydrate` defaults to true because we _have_ to run it on hydration if we have no `loaderData` from the server.\r\n\r\nThen below you can mention:\r\n* When you provide a server `loader` in addition to a `clientLoader`, we _can_ render the during SSR using the server loader data\r\n* If if we do not want to render the route with only server loaderData, then set `clientLoader.hydrate=true` to run on hydration and provide a `HydrateFallback` to render until `clientLoader` completes"
      ],
      "react-router-documentation-clarity-standards": [
        "```suggestion\r\nExport `DefineRouteFunction` type alongside `DefineRoutesFunction`\r\n```",
        "```suggestion\r\nDisable Lazy Route Discovery for all `ssr:false` apps and not just \"SPA Mode\" because there is no runtime server to serve the search-param-configured `__manifest` requests\r\n```",
        "```suggestion\r\nWe don't actually want the about page to be nested inside of the sidebar layout. Let's move the sidebar to a layout so we can avoid applying it to the about page. Additionally, we want to avoid loading all the contacts data on the about page.\r\n```",
        "```suggestion\r\nIn React Router v7 you define your routes using the [`app/routes.ts`][routing] file. For backwards-compatibility and for folks who prefer [file-based conventions][fs-routing], you can opt-into the same \"flat routes\" convention you are using in Remix v2 via the new `@react-router/fs-routes` package:\r\n```",
        "```suggestion\r\nWhen using `v7_relativeSplatPath`, properly resolve relative paths in splat routes that are children of pathless routes\r\n```\r\n\r\nThe pull request number will be added automatically during the release process, and the PR will link to the issue number :)",
        "```suggestion\r\n\"react-router-dom\": patch\r\n```\r\n\r\nThis can just be a patch - no new runtime functionality added, just exporting a missing existing type so it's more of a type bugfix IMO",
        "```suggestion\r\nExport `NavLinkRenderProps` type for easier typing of custom `NavLink` callback\r\n```",
        "```suggestion\r\nAllow falsy `location.state` values passed to `<StaticRouter>`\r\n```"
      ],
      "react-router-extract-test-helpers": [
        "Added some helpers for these things so we can add new fields in one spot and not have to go around and touch all tests that mock out a context",
        "Added the `decodedChar` to all existing setups - even though it's the same as the `char` since we now assert matched param values against `decodedChar`",
        "Moved to partial-hydration-test and these now also run against the `react-router` `RouterProvider` along with `createMemoryRouter`"
      ],
      "react-router-organize-test-scripts-properly": [
        "This makes it easier to re-run the test without a build vis `pnpm test:integration:run` to iterate on test contents without changing source code."
      ],
      "react-router-api-backward-compatibility": [
        "This internal `_renderMatches` was forked off as a private implementation to allow us to expand the function with data-router capabilities without changing the existing `renderMatches` public API.",
        "Because `middleware` is part of the default data strategy, we have to re-implement it here in our custom data strategy and can do so using the same `runMiddlewarePipeline` API we use internally.  I'm thinking we should make some form of this public API as well for userland `dataStrategy` implementation who want to use the normal middleware. \r\n\r\nThe current API is as follows - may be leaking some implementation details we could hide in the exported version though:\r\n\r\n```js\r\nrunMiddlewarePipeline(\r\n  // Passthrough of { request, matches, context } from dataStrategy\r\n  args, \r\n\r\n  // how deep?  I.e., what is the lowest handler to run\r\n  matchIndexToRunMiddlewareTo, \r\n\r\n  // Should I bubble up a returned Response?  SSR only - always `false` in user client-side implementations\r\n  false, \r\n\r\n  // callback to run the handlers and assign results to keyedResults\r\n  // async (keyedResults: Record<string, DataStrategyResult>) { ... },\r\n\r\n  // Error callback if a middleware throws an error - assign the error to keyedResults\r\n  async (e: MiddlewareError, keyedResults: Record<string, DataStrategyResult>) { ... }  \r\n)\r\n```\r\n\r\n\r\nMaybe we could pass it as an arg to `dataStrategy`?  We could remove the boolean and handle that for them internally, and then instead of using an index we could just let them hand us the matches which they could `.slice` if they didn't want to run all the way down:\r\n\r\n```js\r\nfunction dataStrategy({ request, params, context, matches, runMiddleware }) {\r\n  return runMiddleware(\r\n    { request, params, context }, \r\n    matches,\r\n    (results) => { /* run handlers, assign to results */ },\r\n    (e, results) => { /* handle error */ },\r\n  );    \r\n})\r\n```\r\n",
        "Ah ok yeah good catch - opened a PR to fix those https://github.com/remix-run/react-router/pull/13946.  I'll cut an experimental release if you want to try it out and see if it fixes the issue.  I'll tag you to move the convo over there",
        "Use the new `runMiddleware` API to wrap middleware around our existing `dataStrategy` - instead of calling middleware independently for each fork in the `dataStrategy`"
      ],
      "react-router-documentation-linking-standards": [
        "Same for typedocs - we when we don't find a `.md` file for the `@link` we look through the typedoc json (make sure you have run `pnpm run docs` locally before testing this out to generate the typedoc JSON file)\r\n\r\n```suggestion\r\nThe {@link Blocker} object returned by the hook has the following properties:\r\n```"
      ],
      "react-router-hook-dependencies-stability": [
        "I actually think this will work - the \"fix\" is removing `state.revalidation` as a dep of the revalidate function.  `dataRouterContext.router` should remain stable for the duration of the app ",
        "Previously, when you kicked off a revalidation and `state.revalidation` went into a `loading` state, you got a new instance of the `revalidate` function",
        "Can we throw existing return types on here to ensure we keep the contract the same?\r\n\r\n```suggestion\r\nexport function useRevalidator(): {\r\n  revalidate: () => Promise<void>;\r\n  state: DataRouter[\"state\"][\"revalidation\"];\r\n} {\r\n```",
        "Can we keep this like it was so we don't couple the return value of `useRevalidator().revalidate` and `router.revalidate`?\r\n\r\n```suggestion\r\n    async revalidate() {\r\n      await dataRouterContext.router.revalidate();\r\n    },\r\n```",
        "Tests needed to be updated so they don't return promises from `useEffect`",
        "The reason this was problematic in 6.11 is that we would call navigate twice and when in a `RouterProvider` we'd delay resolving the relative path until inside the router.  And thus the second execution would re-resolve `to` against the current location (which in a on-loader scenario would already be updated from the first effect).\r\n\r\nInstead, we resolve the absolute path here in `Navigate` so that duplicate calls to navigate via the data router go to the same path - just as they do in `BrowserRouter`.",
        "For future reference we tested the unit test setups from [93ccb2b](https://github.com/remix-run/react-router/pull/10435/commits/93ccb2b5d967142270a084cdd166bcc34943fefb) in demo apps using react 16.8 and 18 and confirmed the UI was the same (even if the underlying React.StrictMode execution approaches differed).  We may try to see if we can get our test suite running against both versions in a separate undertaking.",
        "Also, I think we're no worse off than we were previously where the effect had no second param since it would have re-run every time anyway?"
      ],
      "react-router-configure-rendering-modes-clearly": [
        "Never need to prerender a manifest anymore\r\n- `ssr: true` will handle the manifest via the server handler\r\n- `ssr: false` will not have fog of war enabled",
        "This went a bit too far - `ssr:false, prerender:['/']` is an explicit opt-into prerendering the `/` route and should trigger full SSG of the `/` route and prerender past the root.",
        "Write out this spa fallback file when the user chose `ssr:false + prerender:['/']` as a way for them to load/hydrate into non-prerendered paths without a runtime server",
        "Prerender `.data` files for any routes with at least one loader"
      ],
      "react-router-distinguish-falsy-vs-nullish": [
        "While the nullish coalescing operator is the \"proper\" way to do this, we have a lint rule that doesn't allow it because when it gets transpiled away it results in a fairly large bundle bloat so we stopped using it in the code until a major version bump when we can update our tranpsilation settings and minimum browser support.  Would you mind changing this to:\r\n\r\n```suggestion\r\n    state: locationProp.state != null ? locationProp.state : null,\r\n```"
      ],
      "react-router-organize-related-code-together": [
        "unrelated to this PR, but curious why this doesn't live in `__tests__`?",
        "Not strongly - just really consistency within the repo more than preferring one approach or the other",
        "nit - but this may make sense living in `utils.ts` next to `stripBasename`",
        "Anything directly exported from `internal-export.ts` now lives in the `types/internal-export/` folder",
        "Moved these to `UNSAFE_` exports instead of deep imports",
        "We have to export a bunch of stuff as `UNSAFE` here to consume from the subpath export file to avoid duplicating any implementations",
        "All router imports come from the source files, no more `lib/router/index.ts` barrel file to re-export"
      ],
      "react-router-complete-accurate-documentation": [
        "We've specifically avoided doing this so far because it causes an extra level of nesting that we don't want the UI - it now look like these APIs should be imported from `react-router/index` and `react-router/dom-export` - both of which are incorrect...\r\n\r\n<img width=\"576\" height=\"720\" alt=\"Screenshot 2025-07-21 at 5 39 15 PM\" src=\"https://github.com/user-attachments/assets/2a317692-eae8-44fa-a5bb-883cdbd99c55\" />\r\n\r\n\r\nI don't yet know the right solution but I think it's ok to omit the dom exports from typedoc for now and just document them in the MD files.",
        "👍 You can generate them just to make sure it passes but no need to include in this PR.  I can pull the branch and generate them locally to review them.  That will save us from needing to revert them before merging."
      ],
      "react-router-handle-ssr-hydration-mismatches": [
        "Wipe out any SSR'd 404s if we now match our new client routes - we will have a hydration error/flicker but it will recover with the client render",
        "```suggestion\r\n      // no-op - no changes if we can't construct a valid URL\r\n```"
      ],
      "react-router-api-naming-consistency": [
        "```suggestion\r\nAdd `loaderData` arguments/properties alongside existing `data` arguments/properties to provide consistency and clarity between `loaderData` and `actionData` across the board\r\n - Updated types: `Route.MetaArgs`, `Route.MetaMatch`, `MetaArgs`, `MetaMatch`, `Route.ComponentProps.matches`, `UIMatch`\r\n - `@deprecated` warnings have been added to the existing `data` properties to point users to new `loaderData` properties, in preparation for removing the `data` properties in a future major release\r\n```",
        "lol yeah I should have probably grabbed my examples from those templates and not the playgrounds in our repo.  I'll merge this change and also go through and see if there's other stuff I should be bringing over too",
        "```suggestion\r\n- Log deprecation warnings for v7 flags\r\n- Add deprecation warnings to `json`/`defer` in favor of returning raw objects\r\n  - These methods will be removed in React Router v7\r\n```\r\n\r\n"
      ],
      "react-router-remove-obsolete-configuration-options": [
        "yeah it's now a useless prop that was supposed to be removed in `7.0.0` but got missed.  I think you can view it as a types bug fix since the type allows you to pass a thing that does nothing.  FWIW it's also a build-time \"break\" not a runtime functional breaking change.",
        "Confirmed with  the tam we feel this is a types bug fix.\r\n\r\nAnother advantage is the error TS will provide after this change is a _good_ thing because if you are still passing `abortDelay` in RR v7 then it's highly likely you have a functional bug in your app because your streams are not going to timeout properly.  Surfacing this TS proper error will alert you to fix that bug.",
        "I think the css-bundle package is obsolete in a `vite`-only world?"
      ],
      "react-router-cancel-aborted-async-operations": [
        "It will be up to users to abort their own promises going forward",
        "Cancel `defer()` instances created _after_ the `request` has already been aborted",
        "Cancel `defer()` instanced created _before_ the `request` is aborted"
      ],
      "react-router-avoid-redundant-computations": [
        "Track up to 1000 paths (fifo) internally to avoid re-calling `patchRoutesOnMiss` on subsequent navigations to the same path"
      ],
      "react-router-configure-react-build-environments": [
        "Set this correctly before calling the CLI so the proper version of React gets loaded (https://github.com/remix-run/react-router/issues/12078)"
      ],
      "react-router-configure-build-tools-properly": [
        "Needed to avoid TS incorrectly exporting a CJS module during the rollup esm build"
      ],
      "react-router-prefer-explicit-readable-constructs": [
        "I think I expected to see the `if (route.clientLoaderModule)` logic above moving down into `route.lazy.loader` as well?  I think it would be another split in this conditional (using if/else for readability now that it has 3 branches).  It might nicely colocate all of our loader initialization whereas we used to have 2 spots (outside/inside of `lazy`)\r\n\r\n```ts\r\n      // assume `dataRoute.lazy` is initialized as an empty object above - may not \r\n      // be the way we want to do it but it might read nicely as we build up the \r\n      // route object and let us colocate initialization logic per-field...\r\n      if (!route.hasClientLoader) {\r\n        // No `clientLoader` exists, use the `loader` to load styles and call the\r\n        // server `loader` (if it exists) in parallel with `route.lazy` execution\r\n        dataRoute.loader = (_: LoaderFunctionArgs, singleFetch?: unknown) =>\r\n          prefetchStylesAndCallHandler(() => {\r\n            return fetchServerLoader(singleFetch);\r\n          });\r\n      } else if (route.clientLoaderModule) {\r\n        // A `clientLoader` module exists, load it with route.lazy.loader\r\n        dataRoute.lazy.loader = async () => {\r\n          invariant(route.clientLoaderModule);\r\n          let { clientLoader } = await import(\r\n            /* @vite-ignore */\r\n            /* webpackIgnore: true */\r\n            route.clientLoaderModule\r\n          );\r\n          return (args: LoaderFunctionArgs, singleFetch?: unknown) =>\r\n            clientLoader({\r\n              ...args,\r\n              async serverLoader() {\r\n                preventInvalidServerHandlerCall(\"loader\", route);\r\n                return fetchServerLoader(singleFetch);\r\n              },\r\n            });\r\n        };\r\n      } else {\r\n        // No `clientLoader` module exists, load the `clientLoader` via the\r\n        // full route module\r\n        dataRoute.lazy.loader = async () => {\r\n          let { clientLoader } = await getLazyRoute();\r\n          invariant(clientLoader, \"No `clientLoader` export found\");\r\n          // This below is the same as above so may be able to be shared.  \r\n          // All that differs between this and the conditional branch above is \r\n          // where we get `clientLoader` from...\r\n          return (args: LoaderFunctionArgs, singleFetch?: unknown) =>\r\n            clientLoader({\r\n              ...args,\r\n              async serverLoader() {\r\n                preventInvalidServerHandlerCall(\"loader\", route);\r\n                return fetchServerLoader(singleFetch);\r\n              },\r\n            });\r\n        };\r\n      }\r\n```",
        "We can use `endsWith` here for a little more clarity:\r\n\r\n```suggestion\r\n    const endSlashPosition =\r\n      toPathname.length > 1 && toPathname.endsWith(\"/\")\r\n        ? toPathname.length - 1\r\n        : toPathname.length;\r\n```"
      ],
      "react-router-dependency-version-ranges": [
        "I think in the root monorepo we are safe to put everything in `dependencies` since end users never use this"
      ],
      "react-router-simplify-configuration-setup": [
        "Externalize all `node_modules` deps like we did in Remix, instead of maintaining a manual list",
        "If it's true we only use this for experimentals now, we might be able to shed some more stuff in here:\r\n\r\n* This prompt stuff (step 2) isn't necessary since we're using `--skip-prompt`, so we could remove that CLI flag and this code\r\n* The manual version bump above (`if (version == null)`) I think is no longer needed since we'll _always_ provide a version as a CLI arg?\r\n* I think all the `isSnapshotVersion` conditionals could go away since it'll only be used for those",
        "it's probably a moot point too since the router will be collapsing into react-router too so everything will be versioned identically moving forward.  We can tackle that when we remove the router package 👍 "
      ],
      "react-router-api-consistency-patterns": [
        "```suggestion\r\n  /** @deprecated Use `MetaArgs.loaderData` instead */\r\n```",
        "```suggestion\r\n   * @deprecated Use `UIMatch.loaderData` instead\r\n```",
        "Same as Remix was doing in 1.7.6: https://github.com/remix-run/remix/blob/remix%401.7.6/packages/remix-server-runtime/server.ts#L418",
        "For the moment we are not persisting `method:'POST'` as Remix did since we think that should be changed in Remix",
        "Instead of directly definig these here, have a base private `SharedSubmitOptions` that can be extended.  That way it's not easy to add something fetcher-only and accidentally leak it to non-fetcher cases.  Do this for `FetcherFormProps` as well",
        "`RedirectResult`'s contain the raw response now, so just grab location from there",
        "good call - yeah I think previously we would have handled that on the value in `result.location` and reduced it to a root relative path but that's probably missed now.  We can move the logic into a util and leverage here - search for the code comment:\r\n\r\n```\r\n// Strip off the protocol+origin for same-origin + same-basename absolute redirects\r\n```",
        "✅ ",
        "Could we build this around `redirect`?\r\n\r\n```suggestion\r\nexport const redirectWithReload: RedirectFunction = (url, init) => {\r\n  let response = redirect(url, init);\r\n  response.headers.set(\"X-Remix-Reload-Document\", \"true\");\r\n  return response;\r\n};\r\n```"
      ],
      "react-router-provide-explicit-error-handling": [
        "We can probably run this through the same utility we use elsewhere\r\n\r\n```suggestion\r\n      let error = new Error('Unhandled request')\r\n      return returnLastResortErrorResponse(error, serverMode);\r\n```",
        "All but 2 uses of `abortFetcher` were already defensive against this invariant and the 2 that weren't were the bug we're fixing, so now that they all want to be defensive we can just flatten the defensive check into this method."
      ]
    },
    "profile": {
      "location": "Delaware (but Philly at heart)",
      "company": "Staff Developer - @remix-run @Shopify",
      "blog": "http://www.brophy.org",
      "twitter_username": "brophdawg11",
      "site_admin": false,
      "followers": 765,
      "following": 6
    }
  },
  "konstin": {
    "repos": [
      "astral-sh/uv"
    ],
    "entries": [
      {
        "slug": "uv-avoid-unnecessary-constraints",
        "title": "Avoid unnecessary constraints"
      },
      {
        "slug": "uv-balance-test-performance-considerations",
        "title": "Balance test performance considerations"
      },
      {
        "slug": "uv-clear-precise-documentation",
        "title": "Clear precise documentation"
      },
      {
        "slug": "uv-declarative-constraints-over-runtime",
        "title": "Declarative constraints over runtime"
      },
      {
        "slug": "uv-enforce-strong-optional-types",
        "title": "Enforce strong optional types"
      },
      {
        "slug": "uv-environment-variable-best-practices",
        "title": "Environment variable best practices"
      },
      {
        "slug": "uv-follow-established-naming-conventions",
        "title": "Follow established naming conventions"
      },
      {
        "slug": "uv-make-errors-user-actionable",
        "title": "Make errors user actionable"
      },
      {
        "slug": "uv-mask-sensitive-tokens",
        "title": "Mask sensitive tokens"
      },
      {
        "slug": "uv-optimize-cache-sharing-strategies",
        "title": "Optimize cache sharing strategies"
      },
      {
        "slug": "uv-redact-url-credentials",
        "title": "Redact URL credentials"
      },
      {
        "slug": "uv-respect-connectivity-state",
        "title": "Respect connectivity state"
      },
      {
        "slug": "uv-secure-configuration-defaults",
        "title": "Secure configuration defaults"
      },
      {
        "slug": "uv-structure-for-readability",
        "title": "Structure for readability"
      },
      {
        "slug": "uv-test-deployment-edge-cases",
        "title": "Test deployment edge cases"
      },
      {
        "slug": "uv-use-direct-documentation-style",
        "title": "Use direct documentation style"
      }
    ],
    "comments": {
      "uv-declarative-constraints-over-runtime": [
        "Given that this is a clap bug, I don't think it's worth it introducing `Result`s in the whole resolve chain.\r\n\r\nThe exit code is the same that clap uses for parsing errors."
      ],
      "uv-use-direct-documentation-style": [
        "This sounds too complex for the guide documentation, I'd guide users towards using either a single `--bump` or a stable and an unstable bump and leave the rest to the concept/reference documentation."
      ],
      "uv-optimize-cache-sharing-strategies": [
        "CC @eifinger is this still a good example?",
        "Do you want to make a PR updating the docs? You know the intended configuration better than me."
      ],
      "uv-secure-configuration-defaults": [
        "We will add custom options, we need them for inclusions and exclusions, and from my experience in maturin and ruff we need to be able to evolve them."
      ],
      "uv-enforce-strong-optional-types": [
        "A missing `project` table should error, otherwise we risk creating an invalid `project` table with a `version` but no `name` (should be solved by `entry()` -> `get()`, too)"
      ],
      "uv-follow-established-naming-conventions": [
        "We should be using either `$HOME` or `~` throughout the document."
      ],
      "uv-environment-variable-best-practices": [
        "This should document what shape of URL is expected, is there something that GitHub considers as API root that users could exchange?"
      ],
      "uv-structure-for-readability": [
        "nit: assign this to a variable outside the tuple to make it easier to follow that this is a tuple; ideally, we'd also de-nest it a bit.",
        "It's used as the non-consuming conversion `is_extended_transient_error`, while the `From`s consume `Self`. Could be a deref or a `From` with `&Self` too, not sure what the most intuitive here is.",
        "oh, yes sure that's better!",
        "Can we DRY this up by only determining the action in the branches?"
      ],
      "uv-test-deployment-edge-cases": [
        "Can you add tests for the prefix and the target case? We have to emulate this by copying our python files and the built test binary manually, but it catches cases such as https://github.com/astral-sh/uv/pull/14184#discussion_r2160101023 and the user scheme preference.",
        "We need this to work on Windows, so we need to be lenient in the matching",
        "Should this fail with `--all`? Otherwise it sounds like a test could slip through if the env var is missing or wrong."
      ],
      "uv-avoid-unnecessary-constraints": [
        "```suggestion\r\nrequires-python = \">=3.11\"\r\n```"
      ],
      "uv-clear-precise-documentation": [
        "Should that be past tense?",
        "We should have consistent grammar between e.g. `The environment would be updated.` and `Create a new environment.` (indicative vs conditional) (unless I've missed something and we are always creating but the update is not done in a dry run?)",
        "```suggestion\n    /// Represents a lockfile and whether it needs to be created or update.\n```"
      ],
      "uv-redact-url-credentials": [
        "I would make this more generic, since we're also displaying URLs in error messages and serializing it to files, e.g.:\n\n```suggestion\n/// A [`Url`] wrapper that redacts credentials when displaying the URL.\n```",
        "I find it confusing that `LogSafeUrl` here is both working a URL that has credentials, but redacts them, and as a URL that had its credentials removed.",
        "This is something that's already in the existing code: Sometimes `Url` refers to a URL with credentials, and sometimes to the same underlying \"data\" without credentials. What about returning a `impl Display` instead of a real type from `without_credentials` to make it clearer that this is an output-only method?",
        "Do we still need this?\n\nThis question applies to all non-test `remove_credentials` calls: Do we still need explicit redaction, or does automatic redaction handle everything we need now?",
        "I'm a bit torn here, on one side seeing `***` here could be confusing cause those credentials are evidently the wrong ones, otoh I'd really want to know that there were credentials on the URL here."
      ],
      "uv-balance-test-performance-considerations": [
        "Can you merge this and the previous test? Usually I'm all for small tests, but Python downloads and installs are slow so reuse helps the test speed",
        "Does this snapshot break when someone publishes `cp311` to PyPI?"
      ],
      "uv-respect-connectivity-state": [
        "To match the Remote Git fetches:\r\n\r\n```suggestion\r\n                    \"{}{} Self-update is not possible because network connectivity is disabled (i.e., with `--offline`)\"\r\n```"
      ],
      "uv-mask-sensitive-tokens": [
        "Can you mask the token with `::add-mask::`, and also the GCP token?"
      ],
      "uv-make-errors-user-actionable": [
        "The error should by itself contain enough information to track down the problem, so I would like it to at least contain the versions.",
        "thiserror at least doesn't support it, I think that's a limitation because every `.source()` needs a type and we'd need two type for the `LibcDetectionError` error message and the variant error message, but we only have one type.",
        "I'd push the libc part down to the most detailed error, I don't expect Python devs to know about libc flavors."
      ]
    },
    "profile": {
      "location": "Munich",
      "blog": "",
      "site_admin": false,
      "followers": 305,
      "following": 0
    }
  },
  "MikeMcQuaid": {
    "repos": [
      "Homebrew/brew"
    ],
    "entries": [
      {
        "slug": "brew-avoid-variable-name-abbreviations",
        "title": "Avoid variable name abbreviations"
      },
      {
        "slug": "brew-clear-code-examples",
        "title": "Clear code examples"
      },
      {
        "slug": "brew-clear-error-recovery-paths",
        "title": "Clear error recovery paths"
      },
      {
        "slug": "brew-decouple-ci-from-code",
        "title": "Decouple CI from code"
      },
      {
        "slug": "brew-document-ci-pipeline-comprehensively",
        "title": "Document CI pipeline comprehensively"
      },
      {
        "slug": "brew-document-non-obvious-decisions",
        "title": "Document non-obvious decisions"
      },
      {
        "slug": "brew-environment-variable-safety",
        "title": "Environment variable safety"
      },
      {
        "slug": "brew-evaluate-security-control-effectiveness",
        "title": "Evaluate security control effectiveness"
      },
      {
        "slug": "brew-fail-with-messages",
        "title": "Fail with messages"
      },
      {
        "slug": "brew-follow-established-naming-patterns",
        "title": "Follow established naming patterns"
      },
      {
        "slug": "brew-follow-support-tiers",
        "title": "Follow support tiers"
      },
      {
        "slug": "brew-minimize-unnecessary-operations",
        "title": "Minimize unnecessary operations"
      },
      {
        "slug": "brew-optimize-collection-operations",
        "title": "Optimize collection operations"
      },
      {
        "slug": "brew-prefer-explicit-nil-handling",
        "title": "Prefer explicit nil handling"
      },
      {
        "slug": "brew-prefer-flags-over-conditionals",
        "title": "Prefer flags over conditionals"
      },
      {
        "slug": "brew-secure-api-url-parsing",
        "title": "Secure API URL parsing"
      },
      {
        "slug": "brew-simplify-complex-code-blocks",
        "title": "Simplify complex code blocks"
      },
      {
        "slug": "brew-standardize-api-integration-patterns",
        "title": "Standardize API integration patterns"
      },
      {
        "slug": "brew-structure-test-fixtures-clearly",
        "title": "Structure test fixtures clearly"
      },
      {
        "slug": "brew-structured-environment-configuration",
        "title": "Structured environment configuration"
      },
      {
        "slug": "brew-use-ascii-only-urls",
        "title": "Use ASCII-only URLs"
      }
    ],
    "comments": {
      "brew-evaluate-security-control-effectiveness": [
        "```suggestion\r\nNote that unlike formulae, casks do not consider the `sha256` stanza to be a meaningful security measure\r\nas maintainers cannot realistically check them for authenticity. Casks download from upstream; if a malicious\r\nactor compromised a URL, they could potentially compromise a version and make it look like an update.\r\n```"
      ],
      "brew-simplify-complex-code-blocks": [
        "```suggestion\r\n    print_stderr = if verbose && show_info\r\n      true\r\n    else\r\n      false\r\n    end\r\n```\r\nis a bit easier to read",
        "Can these be dedicated methods rather than a lambda? The method this is part of is getting very long and we don't typically use `lambda` like this just for variable scoping."
      ],
      "brew-clear-code-examples": [
        "Love this version, thanks @jvns! @colindean could you use this verbatim?\r\n\r\nThanks both!",
        "```suggestion\r\n```\r\n\r\nthis should instead rely on the rubydoc.brew.sh documentation rather than duplicating it",
        "```suggestion\r\n| `require_root`          | `false`      |  yes  |  yes  | whether the service requires root access. If true, Homebrew hints at using `sudo` on various occasions, but does not enforce it |\r\n```"
      ],
      "brew-environment-variable-safety": [
        "```suggestion\r\n  if [[ -r \"/var/tmp\" && -w \"/var/tmp\" ]]\r\n  then\r\n    HOMEBREW_DEFAULT_TEMP=\"/var/tmp\"\r\n  else\r\n    HOMEBREW_DEFAULT_TEMP=\"/tmp\"\r\n  fi\r\n```",
        "@carlocab great point, will adjust and add an explicit `PATH` suggestion"
      ],
      "brew-prefer-explicit-nil-handling": [
        "```suggestion\r\n      @name = name.presence\r\n      @version = Version.new(version) if version.present?\r\n```",
        "@abitrolly Why does it make sense to accept a blank name from the user here?",
        "Ok, I understand now, thanks.",
        "```suggestion\r\n        if (pypi_extras = extras.presence)\r\n          out += \"[#{pypi_extras.join(\",\")}]\" \r\n        end\r\n```",
        "This feels like it would be nicer to:\r\n- move this logic to an `else` in `livecheck_url_to_string`\r\n- make `livecheck_url_to_string` always return a `String` rather than a `T.nilable(String)`\r\n\r\nAs a general rule/concept: whenever you can remove `T.nilable` usage and `raise` instead: it's nicer to do so when using Sorbet.",
        "```suggestion\r\n          if (bottle = formula.bottle)\r\n```",
        "```suggestion\r\n    resource = github_packages_manifest_resource\r\n    return unless resource&.downloaded?\r\n```",
        "```suggestion\r\n    resource = github_packages_manifest_resource\r\n    return unless resource&.downloaded?\r\n```"
      ],
      "brew-avoid-variable-name-abbreviations": [
        "No idea what `uphpp` means. Please use longer variable names.",
        "```suggestion\r\n  repository=\"$(tr '[:upper:]' '[:lower:]' <<<\"${dir#*/}\")\"\r\n  repository=\"${repository#@(home|linux)brew-}\"\r\n  echo \"${user}/${repository}\"\r\n```",
        "```suggestion\r\n  user=\"$(tr '[:upper:]' '[:lower:]' <<<\"${directory%%/*}\")\"\r\n  repo=\"$(tr '[:upper:]' '[:lower:]' <<<\"${directory#*/}\")\"\r\n```"
      ],
      "brew-fail-with-messages": [
        "This shouldn't fail silently.",
        "How/when will this exit with a non-zero result? Not currently seeing any e.g. `return 1` in there?",
        "@ZhongRuoyu whoops, still not seeing it!",
        "> made sure at the call site that the function is only called when there are no `brew tap` arguments\r\n\r\nGotcha, I see that now. Might be a little clearer if this is above the wildcard commands and doesn't do `&& exit 0` but instead has `exit 0` be unconditional as there's no `return` status to check here."
      ],
      "brew-document-non-obvious-decisions": [
        "Please add a comment, and ideally turn this into a constant/variable, explaining why 100 is used otherwise it seems arbitrary.",
        "```suggestion\r\n              # maximum length of PR body is 65,536 characters so let's truncate release notes to half of that.\r\n              body = github_release_data[\"body\"].truncate(32_768)\r\n```\r\n\r\nwould be nicer to truncate based on an actual length here",
        "@bevanjkay worth handling in a follow-up I think. My suggestion would be that `--bump-synced` does not include any release notes or it handles the truncation its own way/as well.",
        "As mentioned before: I'd love to see this be a temporary stop-gap while we add a DSL. I think this needs some pretty hefty amounts of comments until then explaining why these particular formulae need to be doing what they are doing here."
      ],
      "brew-structured-environment-configuration": [
        "@Bo98 Is `HOMEBREW_TEMP` not the per-user temp? If not: ideally we'd do that and: yes, agreed.",
        "@Bo98 done!",
        "This will override e.g. `brew search` behaviour which seems undesirable.",
        "I'm game for searching internal taps but this would break searching non-internal taps."
      ],
      "brew-use-ascii-only-urls": [
        "I'd rather we be excessively strict for now, provided homebrew/core and homebrew/cask pass, and loosen it later."
      ],
      "brew-follow-support-tiers": [
        "```suggestion\r\ncommand -v brew || export PATH=\"/opt/homebrew/bin:/home/linuxbrew/.linuxbrew/bin:/usr/local/bin\"\r\ncommand -v brew && eval \"$(brew shellenv)\"\r\n```",
        "@colindean This seems sufficient to handle all platforms without error output and is short and easier to understand. \r\n\r\nFor future: can you allow maintainers to commit to your fork? Thanks."
      ],
      "brew-standardize-api-integration-patterns": [
        "Does `brew services list --json` contain the information we need? If so, would be nice to use that instead. If not, maybe it'd be nice to add that information in there.",
        "I think this needs to be more specific as to where it's used. It's not using this instead for e.g. all GitHub searches."
      ],
      "brew-secure-api-url-parsing": [
        "```suggestion\r\n        if [[ \"${UPSTREAM_REPOSITORY_URL}\" = \"https://github.com/\"* ]] && \r\n           [[ -n \"${HOMEBREW_GITHUB_API_TOKEN}\" ]]\r\n```\r\n\r\nor similar as otherwise `HOMEBREW_GITHUB_API_TOKEN` being set at all may limit this.\r\n\r\nIt may be desirable to have a single `[[ \"${UPSTREAM_REPOSITORY_URL}\" == \"https://github.com/\"* ]]` check which sets a variable e.g. `UPSTREAM_REPOSITORY_GITHUB_GLOB` or something which you can then check in these multiple places later.",
        "```suggestion\r\n        if [[ -n \"${UPSTREAM_REPOSITORY_TOKEN}\" ]]\r\n        then\r\n          CURL_GITHUB_API_ARGS=(\"--header\" \"Authorization: token ${UPSTREAM_REPOSITORY_TOKEN}\")\r\n        elif [[ -n \"${HOMEBREW_GITHUB_API_TOKEN}\" ]]\r\n        then\r\n          CURL_GITHUB_API_ARGS=(\"--header\" \"Authorization: token ${HOMEBREW_GITHUB_API_TOKEN}\")\r\n```"
      ],
      "brew-follow-established-naming-patterns": [
        "- This is a behaviour change as it'll no longer look for `archive|releases` like it did before\r\n- `m` is a poor variable name here, please use a better, longer one\r\n- I think it's worth continuing to set `user` and `repo` without `fetch`",
        "```suggestion\r\n                basename = if File.directory?(path)\r\n                  File.basename(path)\r\n                 else\r\n                   File.basename(path, \".*\")\r\n                 end\r\n                excluded_names.include?(basename)\r\n```",
        "```suggestion\r\n  def self.binary_linked_to_library?(binary, library, prefix = HOMEBREW_PREFIX)\r\n```\r\ngiven it returns a `T::Boolean`",
        "How about:\r\n```suggestion\r\n      return unless cask.livecheck?\r\n```\r\nWould that naming work?",
        "> It makes sense to me, as `#livecheck` returns a [`Livecheck` object](https://github.com/Homebrew/brew/blob/84823d80f71a01eba4febbbdf4259a687219885b/Library/Homebrew/livecheck.rb) (the DSL values), so `#livecheck?` would align with that.\r\n\r\nAnother option would be `bottle_defined?` which is the equivalent for the `bottle` block.",
        "Yup, sounds good thanks!"
      ],
      "brew-clear-error-recovery-paths": [
        "Will this not now be output even if `Homebrew::EnvConfig.github_api_token` is unset? If so, that's undesirable.",
        "I think an extra case checking if `if Homebrew::EnvConfig.github_api_token.present?` and saying `HOMEBREW_GITHUB_API_TOKEN is unset` or similar would improve this, thanks!",
        "Should better handle here where there's no group name. because `Failed setting group \"\"` doesn't seem great.",
        "```suggestion\r\n      sleep_time = 2 ** @attestation_retry_count[bottle]\r\n```\r\nwould be fine with me if you want this to take smaller jumps/be quicker to fail",
        "```suggestion\r\n    ATTESTATION_MAX_RETRIES = 2\r\n```\r\nor \r\n```suggestion\r\n    ATTESTATION_MAX_RETRIES = 3\r\n```\r\nwould be fine with me if you want to fail earlier"
      ],
      "brew-prefer-flags-over-conditionals": [
        "Can you explain why we'd want to claim HTTP2 support is present when `curl` says its not?",
        "Gotcha, thanks. I'd be tempted to just use Homebrew's `curl` in that case rather than have a shim. Can we use `uses_from_macos \"curl\", since:` instead to handle macOS 13 and below?",
        "> @MikeMcQuaid that hack (`uses_from_macos \"curl\", since: :sonoma`) would actually enable the linkage against system curl, see [this build log](https://github.com/Homebrew/homebrew-core/actions/runs/11872759852/job/33086872512).\r\n\r\n@chenrui333 on which macOS versions would it link against/not link against system `curl`? Thanks!",
        "> I know this would be only for ventura builds.\r\n\r\nFor one, non-latest macOS version this seems overkill, personally, but I'm open to thoughts from other maintainers.",
        "@chenrui333 Thanks, not worth doing for a single OS version IMO, sorry."
      ],
      "brew-structure-test-fixtures-clearly": [
        "Why was this a loop?\r\n```suggestion\r\n        result = { name: fc.name, version: fc.version }\r\n        expect(result).to eq(test.fetch(:expected))\r\n```",
        "What do you propose? `expected_name` and `expected_version`? Criticising without offering an alternative is not helpful.",
        "> The original idea is fully declarative table test fixture.\r\n\r\nThis is not something we do in Homebrew.\r\n\r\n> Dynamic enrichment of static test data with \"default values\" makes it less readable for me.\r\n\r\nAs someone who is neither an expert in Ruby nor Homebrew, readability for you is not the desired outcome here.\r\n\r\n> If there is an error that sets `head` to nil, and test expects `false`, the test won't catch it.\r\n\r\nThe Sorbet type system will catch it.\r\n\r\n> The same with version here - if we need a test that checks version is null after parsing, it should be explicitly set in fixture.\r\n\r\nI disagree.\r\n\r\n> No need to complicate the logic to repeat it for all URLs where the version is irrelevant.\r\n\r\nThe version is never irrelevant.\r\n\r\n> What could be relevant is to add a fixture entry that tests that version from params overrides the one parsed from URL.\r\n\r\nWill review a follow-up PR to do that."
      ],
      "brew-minimize-unnecessary-operations": [
        "If it's slow: be worth putting this in the loop lazily evaluated as late as possible and memoized so that we can avoid more cases where it might not need to be called at all e.g. for the last possible `next`",
        "Would be nicer to put this in `if adopt` above so it avoids reading the `source_plist` etc. unnecessarily."
      ],
      "brew-optimize-collection-operations": [
        "```suggestion\r\n        json[\"old_tokens\"] = [old_token, *json[\"old_tokens\"]].compact.uniq\r\n```",
        "```suggestion\r\n          formulae_and_casks &= excluded_autobump\r\n```\r\nor \r\n```suggestion\r\n          formulae_and_casks |= excluded_autobump\r\n```\r\nI can't remember which 😅 ",
        "That works for me also! anything that avoids a loop.",
        "```suggestion\r\n              installed_formula.deps.required.map(&:to_formula).any? { |dep| sized_formulae.include?(dep) }\r\n```\r\nany may be able to simplify this further still with `intersect?`",
        "```suggestion\r\n        installed_formula_tap_names = Formula.installed.filter_map(&:tap).uniq.reject(&:official?)\r\n```",
        "```suggestion\r\n        installed_cask_tap_names = Cask::Caskroom.casks.filter_map(&:tap).uniq.reject(&:official?)\r\n```",
        "```suggestion\r\n        @non_core_taps ||= Tap.installed.reject(&:core_tap?).reject(&:core_cask_tap?)\r\n```"
      ],
      "brew-decouple-ci-from-code": [
        "We should stop Monterey CI as soon as we start any Sequoia CI.",
        "@Bo98 Bit confused here, sorry! Can you make a code suggestion on whatever you'd want to see changed in this PR specifically before it is merged e.g. today? Thanks ❤️ ",
        "Can you elaborate a bit more on what \"separate these controlling CI \" and \"separating CI control\" means here? It's not totally clear. Thanks!",
        "Thanks for explaining. Sounds like the main blocker therefore is updating the CI runners to not use the same logic as these numbers here.",
        "> We'll probably not merge this until there's good bottle coverage.\r\n\r\nWe should merge this as soon as we're running Sequoia in CI.\r\n\r\nIt doesn't matter for end-users if we have zero Sequoia bottle coverage as we can still support it before that."
      ],
      "brew-document-ci-pipeline-comprehensively": [
        "It does a few more things, too?",
        "@Rylan12 `brew readall`, `brew test-bot --only-formulae --test-default-formula`, `brew doctor` seem like the important bits."
      ]
    },
    "profile": {
      "location": "Edinburgh, Scotland",
      "company": "@mikemcquaid",
      "blog": "https://mikemcquaid.com",
      "twitter_username": "MikeMcQuaid",
      "site_admin": false,
      "followers": 4042,
      "following": 0
    }
  },
  "BruceMacD": {
    "repos": [
      "ollama/ollama"
    ],
    "entries": [
      {
        "slug": "ollama-abstract-model-operations-cleanly",
        "title": "Abstract model operations cleanly"
      },
      {
        "slug": "ollama-ai-dependency-decoupling-strategy",
        "title": "AI dependency decoupling strategy"
      },
      {
        "slug": "ollama-ai-memory-management",
        "title": "AI memory management"
      },
      {
        "slug": "ollama-clear-recoverable-error-messages",
        "title": "Clear recoverable error messages"
      },
      {
        "slug": "ollama-complete-http-protocol-handling",
        "title": "Complete HTTP protocol handling"
      },
      {
        "slug": "ollama-descriptive-balanced-naming",
        "title": "Descriptive balanced naming"
      },
      {
        "slug": "ollama-document-synchronization-intent",
        "title": "Document synchronization intent"
      },
      {
        "slug": "ollama-follow-godoc-conventions",
        "title": "Follow GoDoc conventions"
      },
      {
        "slug": "ollama-loose-api-coupling",
        "title": "Loose API coupling"
      },
      {
        "slug": "ollama-optimize-ai-implementation-patterns",
        "title": "Optimize AI implementation patterns"
      },
      {
        "slug": "ollama-optimize-with-standard-library",
        "title": "Optimize with standard library"
      },
      {
        "slug": "ollama-path-traversal-prevention",
        "title": "Path traversal prevention"
      },
      {
        "slug": "ollama-use-environment-variables",
        "title": "Use environment variables"
      },
      {
        "slug": "ollama-use-idiomatic-go-flow",
        "title": "Use idiomatic Go flow"
      },
      {
        "slug": "ollama-use-portable-path-configurations",
        "title": "Use portable path configurations"
      }
    ],
    "comments": {
      "ollama-ai-memory-management": [
        "```suggestion\r\nBy default, Ollama uses a context window size of 2048 tokens. This can be overridden with the `OLLAMA_CONTEXT_LENGTH` environment variable. For example, to set the default context length to 8K, use: `OLLAMA_CONTEXT_LENGTH=8192 ollama serve`.\r\n```\r\n\r\nFixing the double space here.",
        "```suggestion\r\n# Preventing Out of Memory (OOM) Errors\r\n\r\nOllama is designed to automatically manage memory usage and prevent out-of-memory errors. However, in some cases, these errors might still occur. Here are solutions to help prevent these crashes:\r\n\r\n## Basic Solutions\r\n\r\n1. Decrease Context Size\r\n   - [Lower the `num_ctx` parameter](https://github.com/ollama/ollama/blob/a4f69a0191b304c204ef074ccd6523f121bfddfe/docs/modelfile.md#valid-parameters-and-values:~:text=mirostat_tau%205.0-,num_ctx,-Sets%20the%20size)\r\n   - This reduces how much text the model can process at once\r\n   - Results in lower memory usage\r\n\r\n2. Limit GPU Layer Usage\r\n   - [Reduce the number of model layers that run on your GPU](https://github.com/ollama/ollama/issues/6950#issuecomment-2373663650).\r\n   - This helps balance the workload between GPU and CPU\r\n\r\n## Advanced Solutions\r\n\r\nFor users who need additional control, the following environment variables can be configured:\r\n\r\n- [`OLLAMA_GPU_OVERHEAD`](https://github.com/ollama/ollama/blob/5f8051180e3b9aeafc153f6b5056e7358a939c88/envconfig/config.go#L237): Reserves additional GPU memory (e.g., for 512MB buffer)\r\n- [`OLLAMA_FLASH_ATTENTION`](https://github.com/ollama/ollama/blob/5f8051180e3b9aeafc153f6b5056e7358a939c88/envconfig/config.go#L236): Enables more efficient memory usage\r\n- `GGML_CUDA_ENABLE_UNIFIED_MEMORY`: Allows GPU to use CPU memory (Linux only)\r\n- [`OLLAMA_NUM_PARALLEL`](https://github.com/ollama/ollama/blob/a4f69a0191b304c204ef074ccd6523f121bfddfe/envconfig/config.go#L249): Controls parallel processing\r\n\r\nNote: Environment variable support may vary depending on your setup and system configuration.\r\n```"
      ],
      "ollama-complete-http-protocol-handling": [
        "What will happen on pulling from an http source now?"
      ],
      "ollama-descriptive-balanced-naming": [
        "```suggestion\r\n\t\tmodel, err := GetModel(n.String())\r\n\t\tif err != nil {\r\n\t\t\tslog.Warn(\"bad model details\", \"name\", n, \"error\", err)\r\n\t\t\tcontinue\r\n\t\t}\r\n```\r\n\r\nThe variable name should reference the actual type here",
        "I'd like to avoid single letter variables, this is so hard to read. Let me know what you think of how I've done Qwen2:\r\nhttps://github.com/ollama/ollama/pull/9200"
      ],
      "ollama-use-environment-variables": [
        "As far as I know Windows uses `%USERPROFILE%` rather than `~`, but good call, generally better to use os.PathSeparator "
      ],
      "ollama-use-portable-path-configurations": [
        "I believe the space after the `from-repofile` flag will cause issues, this results in the command format: `... addrepo --from-repofile= https://developer.download.nvidia.com...`"
      ],
      "ollama-follow-godoc-conventions": [
        "I know that comments aren't in other places here, but we should be documenting these interfaces and functions with comments where this serves as an implementation guide for different back-ends, and a reference for model implementers. "
      ],
      "ollama-optimize-ai-implementation-patterns": [
        "```suggestion\r\n    \"For best results, this notebook should be run on a Linux node with a GPU or an environment like Google Colab.\"\r\n```"
      ],
      "ollama-document-synchronization-intent": [
        "Looks like `Items` is returning the actual map, which could be unsafely modified by a caller. We should return a copy instead to prevent accidental misuse that creates a race condition.\r\n```\r\n// Items returns a copy of the underlying map.\r\nfunc (s *SyncMap[K, V]) Items() map[K]V {\r\n\ts.mu.RLock()\r\n\tdefer s.mu.RUnlock()\r\n\t\r\n\tresult := make(map[K]V, len(s.m))\r\n\tfor k, v := range s.m {\r\n\t\tresult[k] = v\r\n\t}\r\n\treturn result\r\n}\r\n```"
      ],
      "ollama-loose-api-coupling": [
        "Good point, opened https://github.com/ollama/ollama/pull/9324 to address this since its a confusing diff. I'll rebase this PR on top of #9324 when possible."
      ],
      "ollama-use-idiomatic-go-flow": [
        "Nice, this is easier to read. Made the change.",
        "Consider using an if statement instead of a switch with a single case, as it would be more idiomatic Go for this scenario and potentially improve readability."
      ],
      "ollama-abstract-model-operations-cleanly": [
        "```suggestion\r\n// Attention implements scaled dot-product attention for transformer models:\r\n// Attention(Q, K, V) = softmax(QK^T/√d_k)V\r\n//\r\n// Parameters:\r\n//   - ctx: Context for tensor operations\r\n//   - query: Query tensor (Q) with shape [batch, heads, seq_len_q, d_k]\r\n//   - key: Key tensor (K) with shape [batch, heads, seq_len_k, d_k]\r\n//   - value: Value tensor (V) with shape [batch, heads, seq_len_k, d_v]\r\n//   - mask: Optional attention mask. If provided, should broadcast to [batch, heads, seq_len_q, seq_len_k]\r\n//   - scale: Scaling factor, typically 1/√d_k where d_k is the key dimension\r\n//\r\n// Returns:\r\n//   Attention output with shape [batch, seq_len_q, heads, d_v]\r\nfunc Attention(ctx ml.Context, query, key, value, mask ml.Tensor, scale float64) ml.Tensor {\r\n```",
        "When will the query implement scaled dot product attention? Is this something that should handled here or in the caller?",
        "Should we be validating the tensor shapes here?",
        "That's pretty much what I was thinking, not a requirement by any means but maybe nice to have",
        "It feels less clear to me how the scaling factor is set now and how its used, maybe we should move it to a constant somewhere in the model definition",
        "I find it easier to eye-ball when looking at the model forward pass implementation when its directly associated with scaling:\r\n```go\r\nkq = kq.Scale(ctx, 1.0/math.Sqrt(float64(headDim)))\r\n```\r\nThe fact it is being passed to scale make it apparent that this is probably the scaling factor.\r\n\r\n```go\r\nkqv := nn.Attention(ctx, q, k, v, mask, 1.0/math.Sqrt(float64(headDim)))\r\n```\r\nNow I can't tell what this mathematical operation is while reading the code, I have to follow through and see what it is actually used for in the attention implementation.\r\n\r\n```go\r\nvar scaleFactor = 1.0/math.Sqrt(float64(headDim))\r\nkqv := nn.Attention(ctx, q, k, v, mask, scaleFactor)\r\n```\r\nI personally like something like this, so I can understand what is going on without leaving the current file. "
      ],
      "ollama-clear-recoverable-error-messages": [
        "It looks like this wait to the end to report the error that the download is incomplete, rather than retrying. Is that accurate? I'd rather just retry the chunk, since the download is not resumable either as far as I can tell.",
        "Added the default fallback back in, defence in depth is good"
      ],
      "ollama-path-traversal-prevention": [
        "The os.Root validation allows relative paths that stay within the root directory but rejects absolute paths and any paths that attempt to escape the root boundary. Added some test cases to demonstrate this, I don't see why we should allow the absolute paths. Let me know if I'm missing something.",
        "I agree with your assessment of the \"Relative (rooted) Paths only\" approach. Here's how I would clarify it:\r\n\r\n### Relative (rooted) Paths only:\r\n\r\nAll paths in files MUST be relative to the current directory (without any traversal components):\r\n- Paths MUST NOT start with \"/\" (absolute paths)\r\n- Paths MUST NOT start with \"./\" or \"../\" (explicit current/parent directory references)\r\n- Paths MUST NOT be just \".\" or \"..\"\r\n- Paths MUST NOT contain traversal sequences anywhere (\"/../\", \"/./\") \r\n- Paths MUST NOT end with traversal components (\"/..\") or current directory references (\"/.\")\r\n- All paths are represented in the temp directory as filepath.Join(tmpDir, relPath)\r\n- All joined temp paths are, as an added measure, checked to ensure they remain within the base directory\r\n\r\n### Valid Examples:\r\n- \"x/y/z\" - Simple nested path\r\n- \"a/b\" - Simple path with one directory\r\n- \"o/l/l/a/m/a\" - Multiple nested directories\r\n- \"file.txt\" - File in root\r\n- \"dir/file.txt\" - File in subdirectory\r\n- \"a/b/\" - Paths with trailing slash\r\n- \"a//b\" - Paths with double slash (will be normalized)\r\n\r\n### Invalid Examples:\r\n- \"/y\" - Absolute path\r\n- \"./x/y\" - Starts with current directory reference\r\n- \"../x\" - Starts with parent directory reference\r\n- \".\" - Current directory only\r\n- \"..\" - Parent directory only\r\n- \"a/../b\" - Contains traversal component\r\n- \"a/./b\" - Contains current directory reference\r\n- \"a/../../b\" - Contains traversal beyond boundaries\r\n- \"a/b/..\" - Ends with parent directory reference\r\n- \"a/b/.\" - Ends with current directory reference\r\n\r\n> Also, just to make sure it is clear: The provided paths (in whichever form we require) are always joined with the tmpDir BEFORE we pass to (*os.Root).Stat\r\n\r\nThis will not work since its an absolute path to the temp dir. `os.Root` rejects absolute paths. i've tweaked the logic to create/open the file through the root interface, which will properly validate containment regardless of whether the file exists or not.\r\n\r\n",
        "The os.Root doesn't quite work in this case, if you prefix the invalid path with a directory name, than it is seen as safe, since that results in a fs.ErrNotExist",
        "Yup, checking both since I dont want to rely on this only being called through createHandler, but want to fail fast if I can"
      ],
      "ollama-ai-dependency-decoupling-strategy": [
        "Nice, simpler than where I was going with this"
      ],
      "ollama-optimize-with-standard-library": [
        "The Go standard library has an example of how to build a priority queue with a heap. Should we try that instead of the dependency?\r\nhttps://pkg.go.dev/container/heap"
      ]
    },
    "profile": {
      "location": "San Francisco Bay Area",
      "company": "@ollama",
      "blog": "bmacd.xyz",
      "twitter_username": "_bmacd",
      "site_admin": false,
      "followers": 488,
      "following": 73
    }
  },
  "chrisduerr": {
    "repos": [
      "alacritty/alacritty"
    ],
    "entries": [
      {
        "slug": "alacritty-assess-security-trade-offs",
        "title": "Assess security trade-offs"
      },
      {
        "slug": "alacritty-avoid-unnecessary-operations",
        "title": "avoid unnecessary operations"
      },
      {
        "slug": "alacritty-avoid-unwrap-on-nullables",
        "title": "Avoid unwrap on nullables"
      },
      {
        "slug": "alacritty-centralize-workspace-dependencies",
        "title": "centralize workspace dependencies"
      },
      {
        "slug": "alacritty-choose-familiar-intuitive-names",
        "title": "Choose familiar, intuitive names"
      },
      {
        "slug": "alacritty-configuration-documentation-accuracy",
        "title": "Configuration documentation accuracy"
      },
      {
        "slug": "alacritty-configuration-validation-feedback",
        "title": "Configuration validation feedback"
      },
      {
        "slug": "alacritty-consistent-error-handling",
        "title": "consistent error handling"
      },
      {
        "slug": "alacritty-consistent-naming-conventions",
        "title": "Consistent naming conventions"
      },
      {
        "slug": "alacritty-document-configuration-specifics",
        "title": "Document configuration specifics"
      },
      {
        "slug": "alacritty-explain-code-intent",
        "title": "Explain code intent"
      },
      {
        "slug": "alacritty-follow-metadata-specifications",
        "title": "Follow metadata specifications"
      },
      {
        "slug": "alacritty-keep-documentation-together",
        "title": "Keep documentation together"
      },
      {
        "slug": "alacritty-optimize-algorithmic-efficiency",
        "title": "optimize algorithmic efficiency"
      },
      {
        "slug": "alacritty-platform-specific-api-documentation",
        "title": "Platform-specific API documentation"
      },
      {
        "slug": "alacritty-prefer-early-returns",
        "title": "prefer early returns"
      },
      {
        "slug": "alacritty-synchronize-platform-configurations",
        "title": "synchronize platform configurations"
      },
      {
        "slug": "alacritty-unsafe-code-practices",
        "title": "unsafe code practices"
      },
      {
        "slug": "alacritty-use-constraining-types",
        "title": "Use constraining types"
      },
      {
        "slug": "alacritty-use-descriptive-contextual-names",
        "title": "Use descriptive contextual names"
      },
      {
        "slug": "alacritty-user-friendly-network-descriptions",
        "title": "User-friendly network descriptions"
      },
      {
        "slug": "alacritty-write-audience-appropriate-documentation",
        "title": "Write audience-appropriate documentation"
      }
    ],
    "comments": {
      "alacritty-consistent-error-handling": [
        "Seems like the easiest solution to me?",
        "Well it's a completely separate execution path.",
        "I want to keep it this way because it gives us more control over the error messages.\r\n\r\nWe're not just trying to print errors and bail here, but instead we're trying to have nice human-readable CLI output.\r\n\r\nThat's why I did it this way, letting Rust's `?` handle error printing is usually worse.",
        "You should return an error with span information, not panic. See `src/config_deserialize/mod.rs` for reference.",
        "I don't like the idea of sidestepping error handling to cause an explicit panic, doesn't really make sense.\r\n\r\nEspecially not in here, considering there's other places this function could error out from. If anything, our error handling should be consistent?",
        "That just sounds like the `FreeConsole` code should be reworked."
      ],
      "alacritty-configuration-documentation-accuracy": [
        "We should reuse the \"looks for\"/search verbs here. There shouldn't be a config file there, it's all optional.",
        "```suggestion\r\ncan be found at _https://toml.io/en/v1.0.0_. Every _Default:_ and _Example:_\r\nentry is valid TOML and can be copied directly into the configuration file.\r\n```"
      ],
      "alacritty-avoid-unnecessary-operations": [
        "To avoid unnecessary clones.",
        "This change isn't strictly necessary, but it seems to just work so why not?",
        "But it was relying on the same thing before, just in a more verbose manner?",
        "Yeah honestly I was kinda worried that this patch would only change the mouse cursor when on a hyperlink escape, but that doesn't seem to be the case.",
        "Actually I found why this is necessary and my changes broke things:\r\n\r\nIf you're in mouse mode and hover over a URL, this will change mouse shape without shift.",
        "```suggestion\r\n        if self.frame().damage_all || selection == self.old_selection {\r\n            return;\r\n        }\r\n\r\n```",
        "If the `bg_alpha` is zero, then it won't be rendered regardless of what color it is. So when changing the background color you also need to make sure you're setting alpha again.\r\n\r\nWhy is the bg alpha 0 when it's the same as the cell background? Performance. It's unnecessary to draw a background for every cell when the background is the same as the terminal background anyway.",
        "If it's the same as the background color, we're not drawing anything anyway. So it doesn't matter what you set your `window_opacity` to. Making it *less* transparent than 0 will just cause it to be rendered, which is a waste of time since you can't see it anyway."
      ],
      "alacritty-unsafe-code-practices": [
        "I'd assume that reregister has mostly the same properties as register. Is this safety note not also true for reregister?",
        "This really doesn't seem like something Alacritty should be doing by hand. That seems like a rather poor Winit API? Especially as an unsafe interface, that's kinda unacceptable…",
        "> You'd be surprised, but we already do that. It's unsafe because it touches env variables, as in mutates the global state. You can't do that transparently, because it's again, global variables and it's all racy.\r\n\r\nYou mean without using `std::env`? Also which env is this removed from? If it's Alacritty, can't we do this in Rust?",
        "> I mean, the function simply calls std::env::remove_var that's all it does and that's the reason it's unsafe, because it removes the env which you could use for launching, so you should take care.\r\n\r\nHow is that unsafe?",
        "Mutating global state that is behind a lock is not unsafe…\r\n\r\nThat's just not how this works.",
        "Yeah that's not how the unsafe keyword is supposed to be used.",
        "There are no safety-relevant contracts that could be violated here."
      ],
      "alacritty-platform-specific-api-documentation": [
        "Is it necessary to quote thhe `--working-directory` flag here? And does `\\\"` work or does it need to be `&quot;`?",
        "Quoting the path is fine, as long as it is required and it doesn't work with a space in the path otherwise (that should be simple to test).\r\n\r\nSo if quotes around the path is required, then 2 would be the best option. There's no reason to quote something if it works without it. That just makes things unreadable.\r\n\r\nAlso single quotes might work just fine?",
        "```suggestion\r\n\tRequest compositor to blur content behind transparent windows.\r\n```\r\n\r\nWindow manager technically doesn't have the capability to do this, it's the compositor that can."
      ],
      "alacritty-follow-metadata-specifications": [
        "Yeah let's just fix it all in one PR. I'm sure @AsciiWolf doesn't mind.",
        "You can't just change the metadata license. The existing text is not available under CC0.",
        "> given that we don't use copyleft, requiring permission of the author is not needed\r\n\r\nI'm not sure what you mean by this. No matter what, we'd require permission by the author if we're changing the license.",
        "Yes, so the only option is rewriting the manifest from scratch, which I cannot do because I've been tainted. So we cannot change the license."
      ],
      "alacritty-keep-documentation-together": [
        "The different window levels should be documented, see `startup_mode` for reference.\r\n\r\nHaving the variants in parenthesis after the description is not beneficial, so that should be removed.",
        "Not a fan of having all the defaults documented in a massive blog so far from its definition.\r\n\r\nI don't want to scroll through half a manpage just to get the documentation for that one field I'm looking for. Especially because I don't even know if that documentation is still going to come. If I'm just looking at that one field, how am I supposed to know there's more?\r\n\r\nThis obviously applies to all the toml blocks.",
        "I **really** don't like that which is why I didn't write the manpage like this to begin with.\r\n\r\nI think it's a bloody mess and makes the entire manpage unreadable."
      ],
      "alacritty-document-configuration-specifics": [
        "```suggestion\r\n- Add `/etc/alacritty/alacritty.toml` fallback for system wide configuration\r\n```",
        "```suggestion\r\n- Pass `-q` to `login` on macOS if `~/.hushlogin` is present\r\n```",
        "```suggestion\r\n- Config option `window.level = \"AlwaysOnTop\"` to force Alacritty to always be the toplevel window\r\n```",
        "We don't document things for developers, we document them for users.\r\n\r\nSo the message you should be adding is that config keys are now all available under their proper name.",
        "```suggestion\r\n- Support for keybindings with dead keys\r\n```",
        "```suggestion\r\n**Alacritty versions before 0.13.0 are using YAML configuration files\r\nand all their settings were documented in the `alacritty.yml`. The example\r\n`alacritty.yml` file for each release can be found on [GitHub releases page](https://github.com/alacritty/alacritty/releases).**\r\n```"
      ],
      "alacritty-configuration-validation-feedback": [
        "What is the difference between `None` and `WindowLevel::Normal`? Either `WindowLevel::Normal` should be removed, or the option. Otherwise this seems redundant.",
        "Switched this from `cfg(unix)` to `allow(unused)` because otherwise you'd get warnings on Windows with this in your config. We don't need to change the old one since that will warn for deprecation anyway. Realistically this should have never gotten added with a `cfg` attribute.",
        "Shouldn't be a thing, but switching to a config option easily solves this too.",
        "Add it where?",
        "```suggestion\r\n                // Require at least one of hyperlink or regex to trigger hint matches.\r\n```"
      ],
      "alacritty-use-constraining-types": [
        "This function's API doesn't really make sense, the arguments here are seemingly arbitrary. This makes it look like any parameter is accepted for these parameters but that's just incorrect. The API should represent the accepted values and use enums where appropriate to prevent incorrect usage.",
        "Taking a `&[u8]` here definitely sucks, but I think constants are the best option with events that don't have any payload. Should there be a necessity for a payload at some point, pulling in a crate like `bincode` that does the translation is likely the correct choice."
      ],
      "alacritty-use-descriptive-contextual-names": [
        "The variable name is confusing, since you're storing a column count not a line number.",
        "How frequently does winit log? I could see this easily going out of control.",
        "Could go for `alacritty_winit_event` just to be precise that this isn't actually logged from winit?\r\n\r\nThere's zero reason to be brief here, since it's always used through the constant anyway.",
        "Might want to change the name because of `0x7f`? Maybe just `is_control_character` with a comment which specific control characters this function detects?",
        "`x` is not a good variable name.",
        "`x` is not a good variable name.",
        "\"normal\" is a bad variable name because there's no reference for what is normal about it.\r\n\r\nIt would probably be easier to name this variable if it is inverted.\r\n\r\nThe comment size can also probably be reduced by replacing the list with an unnumbered one and removing the extra whitespace.",
        "Honestly thinking about it `f_x` and `g_x` are pretty terrible names. Would be much easier to understand all of this by just naming them after what they actually are. Something like `top_line` and `bottom_line` maybe.\r\n\r\nThe whole `_x` thing is just confusing because it's basically referring to a variable that isn't actually passed to the line function (we just call `.next()`). It makes sense mathematically but the concept just doesn't translate.",
        "This is a bad name. It's not an equation it's an iterator. An equation would map to a function in programming so it would just be `fn f(x: i32) -> i32` or whatever.",
        "Not really a fan of naming a variable after what it is, rather than *why* it is.\r\n\r\nCalling it just `ViMotion` would be fine. If you wanted to change the name a more descriptive one would be `SerdeViMotion` which communicates *why* it is wrapped.",
        "Shouldn't use the full path here. Just import it as `WinitOptionsAsAlt` or something.",
        "Please refrain from single-character variables. There's no reason not to just use `user` here. Same applies to the other match blocks."
      ],
      "alacritty-choose-familiar-intuitive-names": [
        "At this point I do not yet want this to be the default way to open new windows, maybe never.\r\n\r\nAlso I'm not sure it's appropriate to add a binding for spawning new windows on Linux, it seems like something users likely wouldn't be that familiar with anyway so they might as well just configure it themselves.",
        "While doing so in the future might make sense, I'm not convinced it's the best option *right now*. I think this is a feature that generally would benefit from a lot of testing and even then reliability will always be worse than separate instances. So it's difficult for me to recommend it as a default to anyone. Though I suppose on macOS this is expected, so I'm not opposed to changing the default binding in the future. But I see no reason to do it now.",
        "```suggestion\r\n  # By default, these will use the inverse primary color.\r\n```\r\n\r\nSince \"inverse text\" is the usual way terminal emulators refer to swapping foreground/background, I think it's good to use the well known language here."
      ],
      "alacritty-prefer-early-returns": [
        "Early return for a lil less indentation?",
        "I mean it's 4 levels of indentation already. With kernel indentation rules that would be 32 columns of indentation alone for the deepest level. I wouldn't call that \"low\".\r\n\r\nIf there were plans to handle different causes in the future, it might be different. But I don't see any immediate reason why we **ever** would handle different causes, and if we did, we'd probably switch to a match anyway. So I fail to see how an early return isn't the best option here.",
        "This is going to be **far** cleaner if you early return on error. Just to deal with the already ridiculous amount of indentation.",
        "The else branch is just `false`, so this can probably be written as a single conditional?\r\n\r\nYou could still keep the variables extracted for documentation purposes, but they're also pretty self-documenting.",
        "```suggestion\r\n        if drop_hyperlink {\r\n            self.extra = None;\r\n        } else {\r\n            let extra = self.extra.get_or_insert(Default::default());\r\n            Arc::make_mut(extra).hyperlink = hyperlink;\r\n        }\r\n```\r\n\r\nShould avoid the extra return, just complicates control flow.",
        "I recommend putting this in an `else` block rather than `break`ing, it's way too big of a conditional block with the early return to put an early return just to skip 5 lines."
      ],
      "alacritty-explain-code-intent": [
        "```suggestion\r\n        // Attempt to make the context current, if it is not.\r\n        let mut was_context_reset = if is_current {\r\n            false\r\n        } else {\r\n            match self.context.make_current(&self.surface) {\r\n                Err(err) if err.error_kind() == ErrorKind::ContextLost => {\r\n                    info!(\"Context lost for window {:?}\", self.window.id());\r\n                    true\r\n                },\r\n                _ => false,\r\n            }\r\n        };\r\n```\r\n\r\nI think a comment here is nice to explain that our goal here is not just checking if the context was reset, but actually performing the make_current call.",
        "Maybe we should add a comment *why* one would want to do this?\r\n\r\nSo something like \"to ensure toml compatibility\" or something.",
        "This method is getting very long, the least we should be doing is adding comments on all the individual code blocks for what they do.",
        "This took me a second to figure out, I think it might be nice to be a little more verbose:\r\n\r\n```\r\n// If we can't fit the entire arrow in the cell, we cut off the tip of the arrow by drawing a rectangle between the two lines.\r\n```",
        "It's not immediately clear what this does, should be documented, probably extracted in a separate variable with a nice name.",
        "It's not immediately clear what this does, should be documented, probably extracted in a separate variable with a nice name.",
        "This should be made more clear, probably by adding a comment and newline as separation. It's not just some error handling, this is how hyperlinks are stopped."
      ],
      "alacritty-consistent-naming-conventions": [
        "```suggestion\r\n\t\tNumpad keys are prefixed by _Numpad*_: \"NumpadEnter\" | \"NumpadAdd\" |\r\n\t\t\"NumpadComma\" | \"NumpadDivide\" | \"NumpadEquals\" | \"NumpadSubtract\" |\r\n\t\t\"NumpadMultiply\" | \"Numpad[0-9]\".\r\n```"
      ],
      "alacritty-centralize-workspace-dependencies": [
        "Didn't want to migrate everything, but I think for most stuff (especially shared deps) it makes sense to just define them in the root.",
        "Going with just dirs 2 seems reasonable to me for now."
      ],
      "alacritty-assess-security-trade-offs": [
        "I mean yaml is just a security nightmare anyway. I don't see how keeping an unmaintained version would be more secure.\r\n\r\nAnd yaml probably isn't going away soon.",
        "```suggestion\r\n\tControls the ability to write to the system clipboard with the _OSC 52_\r\n\tescape sequence. While this escape sequence is useful to copy contents\r\n\tfrom the remote server, allowing any application to read from the clipboard can be easily abused while not providing significant benefits over explicitly pasting text.\r\n```\r\n\r\nMy main gripe with this comment was the \"not that necessary\" part sounding kinda imprecise. I think the new suggestion should be a little more direct."
      ],
      "alacritty-synchronize-platform-configurations": [
        "You couldn't. Until I wrote this patch to make it possible. :D \r\n\r\nCheck out the changes in `src/input.rs`, it was fairly simple so I thought that this would be the best solution.",
        "With the previous configuration double bindings would just silently be ignored, like this they're both used. If it would have caused errors previously I think this would be a far more severe change, but like this it should actually make things more transparent and less error prone.\r\n\r\nOr that was my train of thought at least. But I was trying to justify the change after the fact. :D",
        "Yeah completely agree with that sentiment. I tested it on Linux by changing it to `Control`, but it's always better to be safe than sorry. :D"
      ],
      "alacritty-avoid-unwrap-on-nullables": [
        "This unwrap does not sound right to me. This is deprecated, is there plan to remove it potentially which could cause this to fail? We shouldn't crash here under any circumstances.",
        "Seems easy enough to handle, so I don't see why we wouldn't. Unwrap's not the right choice.",
        "Just falling back to the `else` branch if anything goes wrong is fine, should be the simplest solution. I'd just early return on success probably.",
        "That's a good point. Falling back to en_US.UTF-8 is also fine or whatever you think is a reasonable default. I think the assumption here is this is almost never hit anyway, so we just don't want to crash really. Bugs would be easier to work around for users than crashes.",
        "```suggestion\r\n                // Fall back to en_US in case the country code is not available.\r\n```",
        "This would mean that an empty string is a c0 or c1 codepoint. Which I'd say is inaccurate.\r\n\r\nI also think that looking at chars/u32s is unnecessary here. Just getting `bytes()` should be fine since no valid unicode character (and `&str` is always valid unicode) can start with a C0/C1 character?",
        "This should be cleaner as `map_or`.",
        "Instead of doing `match padding.y { Some(y) => { ... }, None => {} }` it would be more ergonomic to use `if let Some(y) => { ... }`. Like this the `None => {}` part which does nothing can just be left out."
      ],
      "alacritty-optimize-algorithmic-efficiency": [
        "No need for saturating here, point.line cannot be bigger than end.line. At that point it's probably also fine to just put it all on a single line.",
        "If it's a hyperlink escape, we ALWAYS highlight regardless of the bounds. If we don't check for `is_some`, we'd only highlight within bounds.",
        "```suggestion\r\n    for (p1, p2) in f_x.zip(g_x) {\r\n```\r\n\r\nI think this should just work since `LineEquation` implements iterator and it's shorter?",
        "Cloning the padding seems like a waste. It should be perfectly fine for padding to implement `Copy`.",
        "Currently you're iterating twice to get to the point where things need to be split off. However the `char_indices` iterator is already an iterator which provides everything necessary to get to the split-off point.\r\n\r\nI'd propose something like this:\r\n```\r\n        // Remove non-alphabetical characters before the scheme\r\n        // https://tools.ietf.org/html/rfc3986#section-3.1\r\n        if let Some(index) = self.state.find(\"://\") {\r\n            let iter = self\r\n                .state\r\n                .char_indices()\r\n                .rev()\r\n                .skip_while(|(byte_index, _)| byte_index >= &index);\r\n            for (byte_index, c) in iter {\r\n                match c {\r\n                    'a'...'z' | 'A'...'Z' => (),\r\n                    _ => {\r\n                        self.state = self.state.split_off(byte_index + c.len_utf8());\r\n                        break;\r\n                    }\r\n                }\r\n            }\r\n        }\r\n```\r\n\r\nThis should work fine, though I haven't manually tested it. But the automatic tests don't seem to complain.",
        "```suggestion\r\n            if !uniq_hyperlinks.contains(&hyperlink) {\r\n                uniq_hyperlinks.insert(hyperlink.clone());\r\n                Some((cell, hyperlink))\r\n            } else {\r\n                None\r\n            }\r\n```",
        "This definitely needs a comment, seems like you use this to skip over cells that contain already existing hyperlinks, but doesn't `if uniq_hyperlinks.contains(&hyperlink) {` already do this? Seems like it's kinda duplicate work?",
        "Why use a dynamic Vec for something that will have at most two elements? Can't we just use a fixed-size slice with two elements for it?",
        "The `draw_string` method now takes an iterator, right? So do we need a `Vec` here?",
        "What's the point of this division? Wouldn't it be easier to just do `1_000_000_000 / 60_000` rather than `1_000_000 / (60_000 / 1000)`?"
      ],
      "alacritty-write-audience-appropriate-documentation": [
        "This is the user-facing changelog, so we should explain things a little bit. I'd suggest the following:\r\n\r\n```suggestion\r\n- Hide login message if `~/.hushlogin` is present\r\n```\r\n\r\nThis way we're properly documenting the effect it has on the user, rather than some internal details.",
        "```suggestion\r\n- Replaced `Options::hold` with `Options::drain_on_exit` that drains, but doesn't hold, since holding can be done outside of alacritty_terminal\r\n```\r\n\r\n\"could\" and \"easily\" are two words I'd rather not use in a changelog since they're vague and superfluous. Additionally \"user\" is kinda confusing because this is about the library consumer not the terminal user.",
        "This is a user-facing document, however this is worded in a developer-centric fashion.\r\n\r\nThis would probably be better:\r\n\r\n```suggestion\r\nNotable changes to the `alacritty_terminal` crate are documented in its\r\n[CHANGELOG](./alacritty_terminal/CHANGELOG.md).\r\n```\r\n\r\nArguably the same could be said about the documentation about the sections, but that is a bit older.",
        "I wonder if we should somehow indicate breaking changes? Do you have any opinion on this?\r\n\r\nMaybe a migration guide between versions?",
        "> you can look at the changelog we have in winit now\r\n\r\nI mean that's exactly the issue right there. A ton of changes but no clear indication which changes require my attention.\r\n\r\nI don't care if a bunch of new stuff was added or something was fixed when updating to a new version, but I **do** care if something needs manual intervention like this focus change to keep the behavior consistent.\r\n\r\nSome changelogs have ugly prefixes like `[BREAKING]` for these kind of entries, but I don't think that's particularly nice. Maybe we can add a line at the top explaining that breaking changes are bold, then use that to indicate them?",
        "> It has a big chunk of code and explanation for what you need. So it's just 2 changes where you need attention.\r\n\r\nIt doesn't though. It states nowhere that these are the only changes that require my attention. It just tells me what I need to do for those.\r\n\r\n> We can do a separate section with breaking changes.\r\n\r\nDo you suggest adding an extra section with its own Fixed/Added/etc?",
        "I like bold for breaking, it's unintrusive but still pretty clear.",
        "There's numerous issues with the default ConPTY. This section should be way more generic.\r\n\r\nIt's also pointlessly detailed, nobody cares what it's called or how it works, they just want a fix.",
        "I think this is still significantly too verbose. It should be easy to explain this in a single paragraph. It's not necessary to mention potential issues at all, if people are running into problems we just need to point them at things they can try. We also don't need to document in Alacritty how other terminal emulators behave.\r\n\r\nWhen it comes to how to get this to work, we don't need to really mention anything in the FAQ at all. I'd just add a section to the install.md."
      ],
      "alacritty-user-friendly-network-descriptions": [
        "This is a very technical description of what has been fixed (which is great for devs), but doesn't really help the user understand what has been fixed.\r\n\r\nI'd propose something more like this:\r\n`- Unicode characters at the beginning of URLs are now ignored`\r\n\r\nOf course this is just a suggestion, feel free to improve on it if you have any ideas.",
        "```suggestion\r\n- Escape sequence to set hyperlinks (`OSC 8 ; params ; URI ST`)\r\n```"
      ]
    },
    "profile": {
      "blog": "https://christianduerr.com",
      "site_admin": false,
      "followers": 264,
      "following": 7
    }
  },
  "AlexDBlack": {
    "repos": [
      "deeplearning4j/deeplearning4j"
    ],
    "entries": [
      {
        "slug": "deeplearning4j-always-secure-your-locks",
        "title": "Always secure your locks"
      },
      {
        "slug": "deeplearning4j-clean-up-your-code",
        "title": "Clean up your code"
      },
      {
        "slug": "deeplearning4j-clear-descriptive-identifiers",
        "title": "Clear descriptive identifiers"
      },
      {
        "slug": "deeplearning4j-compare-floating-point-safely",
        "title": "Compare floating-point safely"
      },
      {
        "slug": "deeplearning4j-configurable-resource-locations",
        "title": "Configurable resource locations"
      },
      {
        "slug": "deeplearning4j-descriptive-error-context",
        "title": "Descriptive error context"
      },
      {
        "slug": "deeplearning4j-document-ai-apis-completely",
        "title": "Document AI APIs completely"
      },
      {
        "slug": "deeplearning4j-document-ai-implementation-references",
        "title": "Document AI implementation references"
      },
      {
        "slug": "deeplearning4j-document-api-completely",
        "title": "Document API completely"
      },
      {
        "slug": "deeplearning4j-fail-fast-clearly",
        "title": "Fail fast clearly"
      },
      {
        "slug": "deeplearning4j-maintain-proper-capitalization",
        "title": "Maintain proper capitalization"
      },
      {
        "slug": "deeplearning4j-minimize-object-allocations",
        "title": "Minimize object allocations"
      },
      {
        "slug": "deeplearning4j-numerical-stability-practices",
        "title": "Numerical stability practices"
      },
      {
        "slug": "deeplearning4j-optimize-hardware-acceleration",
        "title": "Optimize hardware acceleration"
      },
      {
        "slug": "deeplearning4j-use-logging-best-practices",
        "title": "Use logging best practices"
      },
      {
        "slug": "deeplearning4j-use-modern-api-methods",
        "title": "Use modern API methods"
      },
      {
        "slug": "deeplearning4j-user-friendly-documentation-examples",
        "title": "User-friendly documentation examples"
      },
      {
        "slug": "deeplearning4j-validate-and-document-nulls",
        "title": "Validate and document nulls"
      }
    ],
    "comments": {
      "deeplearning4j-document-ai-implementation-references": [
        "Maybe add a comment here - seemed a little strange on first glance.\r\nI gather you want masked steps to have very large negative attention weights as a way of doing \"masked softmax\"?\r\nHow was the 1e9 chosen? (just wondering if we'll even have numerical stability issues...)",
        "conv_padding is 'truncate' mode padding values, and conv_padding_r is same mode padding, right?\r\nIf so, not sure on the -pH and -pW here... Usually with same mode, we ignore the ph and pW args and just calculate what we should actually use.\r\nhttps://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/ConvolutionMode.java#L62-L63\r\n\r\nAlso present in a bunch of other places."
      ],
      "deeplearning4j-maintain-proper-capitalization": [
        "Capitalization: `samediff` -> `SameDiff`\r\n`Tensorflow` -> `TensorFlow`\r\n`Samediff.importFrozenTF` -> `SameDiff.importFrozenTF`\r\n\r\nSame things in a few other places in the doc, I won't flag others...",
        "Technically it's a method overload, not an optional argument.\r\nLet's also mention generated name \"based on the operation name\" when not specified"
      ],
      "deeplearning4j-configurable-resource-locations": [
        "Let's make dir configurable. User might want it on SSD different to system drive.",
        "Do we want configurable location?\r\ni.e., for VMs etc with slow system  HDD but fast SSD available",
        "Also configurable location here?"
      ],
      "deeplearning4j-minimize-object-allocations": [
        "When using the Preconditions class, do this instead:\r\nThis avoids object creation/garbage unless an error is actually thrown.\r\n```\r\nPreconditions.checkArgument((labels.size(1) != preOutput.size(1)),\"Labels array numColumns (size(1) = %s) does not match output layer number of outputs (nOut = %s)\",  labels.size(1), preOutput.size(1));\r\n```",
        "This seems off.\r\nYou create a zeros array of shape [mb, inputCaps, caps, capsDimensions, 1]\r\nBut then proceed to immediately get a [mb, inputCaps, caps, 1, 1] subset from it?\r\nThat's unnecessarily inefficient. Why not make a ```[mb, inputCaps, caps, 1, 1]``` in the first place?",
        "Don't create this temporary ret array. Use ```gainParamView.assign(gainInit)``` instead.",
        "Nice regex :)\r\nA minor detail, but pattern is apparently immutable and thread safe... we can make it static? (Or make the one in BertWordPieceTokenizer static + public and use here too?)"
      ],
      "deeplearning4j-use-logging-best-practices": [
        "Not a big deal - but we can use Lombok `@Slf4j` annotation on the class to get the same thing",
        "Don't also print stack trace, log.error will already print it.\r\nAlso other instances below.",
        "I'm being picky here - can use ```@Slf4j``` annotation instead.",
        "Put exception as log.error arg"
      ],
      "deeplearning4j-always-secure-your-locks": [
        "This doesn't look safe to me.\r\nIf the semaphore available permits is anything other than 0, this will overflow.\r\n\r\nSemaphore.release:\r\n```\r\n    public void release(int permits) {\r\n        if (permits < 0) throw new IllegalArgumentException();\r\n        sync.releaseShared(permits);\r\n    }\r\n```\r\n\r\nThis ultimately ends up calling this method, on Semaphore.Sync:\r\n```\r\n        protected final boolean tryReleaseShared(int releases) {\r\n            for (;;) {\r\n                int current = getState();\r\n                int next = current + releases;\r\n                if (next < current) // overflow\r\n                    throw new Error(\"Maximum permit count exceeded\");\r\n                if (compareAndSetState(current, next))\r\n                    return true;\r\n            }\r\n        }\r\n```\r\n\r\nUnless we can guarantee that the semaphore only ever has 0 permits at this point (unlikely), this is a bug.",
        "Unlock should be in finally block?\r\nOtherwise if there's any sort of an exception, any other threads waiting on that lock could be waiting forever.",
        "Same thing - unlock in finally block in case of exception.\r\nAlso un-indent line above.\r\nSame thing for other lock uses in this class."
      ],
      "deeplearning4j-document-ai-apis-completely": [
        "inputs() is good - that just tells you placeholders. Let's note placeholders, and maybe link to other SameDiff page that explains this.\r\nAs for outputs() - that will be going away. Yes it's in 1.0.0-beta5, but it's not robust enough - it's basically \"variables that don't feed into any other ops\" - which often won't be the real predictions that users want - but rather the loss function or some irrelevant unused output.\r\n\r\nSo users should just look at the summary instead and infer the outputs from that.",
        "\"For multiple outputs, use exec() instead of execSingle(), to return a `Map<String,INDArray>` of outputs instead.\r\nAlternatively, you can use methods such as `SameDiff.output(Map<String, INDArray> placeholders, String... outputs)` to get the same output.\"",
        "Let's add a section with this, which should be good enough for now. We'll get proper coverage info up at a later date.\r\n\r\n```\r\n## Operations Coverage\r\n\r\nSameDiff's TensorFlow import is still being developed, and does not yet have support for every single operation and datatype in TensorFlow.\r\nAlmost all of the common/standard operations are importable and tested, however - including almost everything in the tf, tf.math, tf.layers, tf.losses, tf.bitwise and tf.nn namespaces. The majority of existing pretrained models out there should be importable into SameDiff.\r\n\r\nIf you run into an operation that can't be imported, feel free to open an issue here: https://github.com/eclipse/deeplearning4j/issues\r\n```",
        "DifferentialFunctionFactory is an internal detail most users will never need to touch, let's not mention this."
      ],
      "deeplearning4j-fail-fast-clearly": [
        "Good check, but this can be optimized (and is not the correct way to use preconditions). This always results in 3 objects being created (2xlong[] shape, and a String) regardless of whether an error is encountered. Instead, use this (no object creation unless an error is encountered)\r\n```\r\nPreconditions.checkState(input.rank() == 3, \"3D ... got %s\", input.rank())\r\n```",
        "Unimplemented? Get the params then don't do anything with them...\r\nIf planned to be added later, maybe add UnsupportedOperationException?\r\n(Better to get exception than obscure issues or silent failure)",
        "Couple of things here:\r\n(a) we can use Preconditions.checkArgument/checkState - less verbose\r\n(b) When throwing exceptions, I think it's good practice to include useful information. What dimension? What's the shape of the array? Without that, I need a debugger to get that information, which adds a lot of time required to fix it..."
      ],
      "deeplearning4j-use-modern-api-methods": [
        "Let's direct users to Nd4j.createFromArray instead.\r\nUnlike Nd4j.create, there's overloads for all java primitive types and Number (long, short, Long, Boolean etc), and so users are less likely to get confused with Nd4j.create(int[]), where they think the int[] is content just like Nd4j.create(double[]), but it's actually shape",
        "This is old. Nd4j.createFromArray has overloads up to 4d for all types",
        "Use `Nd4j.zeros(DataType.DOUBLE, 5)`,  no need to do it in 2 lines with array for shape.\r\nMaybe also show the import for DataType the first time it's used.",
        "Show casting example as solution to this:\r\n`INDArray x3 = x.add(x2.cast(DataType.DOUBLE))`"
      ],
      "deeplearning4j-clean-up-your-code": [
        "Add private no-arg constructor to avoid users being able to do ```new ValidationUtils()```",
        "This comment applies to all of your uses of Preconditions case: don't create strings like this.\r\nThe correct way of using preconditions class doesn't (in most cases) result in any object creation unless the precondition fails:\r\n```\r\nPreconditions.checkArgument(data >= 0, \"Values for %s must be >= 0, got %s\", paramName, data);\r\n```"
      ],
      "deeplearning4j-descriptive-error-context": [
        "Is \"no axis\" a legitimate use case? More likely to be a bug than \"I want to implement identity op via standardize\".\r\nIMO exception would be better."
      ],
      "deeplearning4j-user-friendly-documentation-examples": [
        "Let's move these two \"don't do this\" to a section at the very end. Totally fine to mention them, but we shouldn't be opening with a bunch of things to *not* do that the user might not have even thought to try...",
        "Maybe add comment for args order: (start, stop, count)",
        "IndArray -> INDArray\r\n\"operations such as\" - mention they are called reduction/accumulation operations.",
        "`.data().asDouble()` etc isn't something I want in the quickstart, it's confusing for new users due to views and f/c order (not to mention permuted arrays).\r\nInstead, direct users to toFloatVector(), toDoubleMatrix() etc\r\n",
        "Missing createFromArray, valueArrayOf, createUninitialized, scalar. Remove copy method (I didn't even realize it was a thing, users can just use assign).\r\n\r\nMaybe also consider splitting out into new subsections:\r\n- Random array creation (randn, randomBernoulli, randomBinomial, randomExponential, choice)\r\n- (De)Serialization (read, readNumpy, createFromNpy/Npz etc) + write methods\r\n\r\nAlso, generally, note that this is just a few of the ops. We have literally hundreds of ops, too many to list here... we don't want users to think that this list is it :)"
      ],
      "deeplearning4j-compare-floating-point-safely": [
        "The reason it isn't equal without the toString is datatypes.\r\nBy design we don't cast on equals.\r\nI'd recommend either specifying datatypes on all INDArrays, or on none of them.\r\nTests default to double; you are comparing a float [10,10] with double [10,10] hence the failure.",
        "Let's replace these 5 lines with: ```return Double.compareDouble(lhs.getFitness(), rhs.getFitness())``` (possibly with order switched, if that's the intention here).\r\nNote I'm assuming the fitness field is protected/private and getter via ```@Data``` was added, as per earlier review comment.\r\nSame thing for other comparator."
      ],
      "deeplearning4j-validate-and-document-nulls": [
        "General point (applies everywhere) - if a method allows null values, it should be marked as such in the docs, with a brief explanation of the behaviour for null vs. non-null.\r\nSomething along the lines:\r\n`@param workspace Optional memory workspace to define the buffer in, may be null (buffer not created in workspace)`",
        "Handling of nulls? You have some in the later methods, but not here...\r\nIf null is acceptable, we should return null here. If not, let's either add a lombok ```@NonNull``` or a ```Preconditions.checkNotNull(```. At present, nulls will give a non-useful NPE.",
        "I don't think point(null) is valid?\r\nThe way you have it implemented currently (with masks) it won't return a single value along that dimension, I think.\r\nAnd if we don't allow null, switch arg to ```int```",
        "Switch all of these array methods to var-args - ```var(int... shape)``` etc\r\nMaybe also a ```Preconditions.checkArgument(shape != null  && shape.length > 0, \"Invalid shape: %s\", shape)```"
      ],
      "deeplearning4j-clear-descriptive-identifiers": [
        "What about ```var_``` prefix instead of ```sd_var_```? It's shorter, and no less descriptive.",
        "Yeah, good point. OK, sounds good :)"
      ],
      "deeplearning4j-optimize-hardware-acceleration": [
        "Can't have this dep here even under test scope, for CI CUDA-only builds:\r\nhttps://github.com/deeplearning4j/deeplearning4j/blob/a948b1364329c7281aa2302524e2b9dcd6858cf2/nd4j/nd4j-parameter-server-parent/nd4j-parameter-server-node/pom.xml#L77"
      ],
      "deeplearning4j-numerical-stability-practices": [
        "Why addition of 1e-5? Normally this is for numerical precision, but I don't believe we risk underflowing here even if l2 norm is 0? (We get 0/1 in that case)",
        "Ah, makes sense, let's leave it then.",
        "Again:\r\nBy definition, the score calculation _must_ use the SameDiff instance. The `output = activationFn.getActivation` part is ok, but after that just call SameDiff.output providing the placeholders instead.",
        "This is a little unintuitive, but it's definitely intended (and required).\r\nConsider the following graph:\r\n```\r\ninput -> split -> (A,B)\r\nB -> lossFn\r\n```\r\nHere, A is unused (in that it doesn't contribute to the loss function). But to do split op backprop, we need gradient variables/arrays for both outputs (A and B).\r\nWe know the shape and type of dL/dA must be exactly the same as A; we also know that the loss function doesn't depend on A. Hence, dL/dA == zerosLike(A)",
        "Done."
      ],
      "deeplearning4j-document-api-completely": [
        "https://github.com/eclipse/deeplearning4j/issues/8031\r\n\r\nAlso `@see` is self-referential?\r\nShould be `#pad(INDArray, int[][], List<double[]>, PadMode)` I think? (there's other ones like this).\r\nAlso for future reference: when the signatures differ, we need to specify the default behaviour / arg values, otherwise the user has to try and dig this out of the code.\r\nA good comment would be: `As per {@link ...} with 'constantValues' being zeros (zero padding)`",
        "I get what you're going for here. But suppose the user comes directly to the argMax method (not via the argMin see/link).\r\nThey could reasonably intepret this to mean that *somehow* the minimum might be returned by this method.\r\n\r\nBetter: only talk about argmax here. And for argMin, do something like `An per {@link argMax(...)} but for minimum values`",
        "Technically not javadoc, but you might as well fix them when you see them.\r\nDeprecated methods should follow the following format (usually)\r\n1. An `@Deprecated` annotation on the method (or class if applicable)\r\n2. A `@deprecated` javadoc tag\r\n3. Javadoc tag should include what to use instead (+ why it was deprecated, if applicable)\r\n4. Usually - no other comments other than the deprecated information (to discourage users from using it at all)\r\n\r\nAlso a minor nitpick: I prefer the `{@link ...}` style directly in the `@deprecated` tag.\r\n`@see` gets rendered to javadoc as something like:\r\n```\r\nSee Also:\r\nsomeMethod(int, double)\r\n```\r\nSemantically - \"see also\" is different to \"use this instead\".",
        "Better javadoc would be good. Mainly clarify exactly what you mean by \"standardize\"",
        "As a general rule: any time we add a ```@Deprecated``` tag we should have javadoc to explain why and/or what to use instead. Even better: add a date/version (pretty sure we're going with 1.0.0-beta for next release)",
        "More javadoc - limitations, when to use.\r\nAlso how to build one (i.e., \"use PI builder configured with X, not this class\") - that might not be obvious to users at first glance (it wasn't for me)"
      ]
    },
    "profile": {
      "location": "Melbourne, Australia",
      "blog": "",
      "site_admin": false,
      "followers": 236,
      "following": 0
    }
  },
  "JohannesGaessler": {
    "repos": [
      "ggml-org/llama.cpp"
    ],
    "entries": [
      {
        "slug": "llama.cpp-ai-parameter-organization",
        "title": "AI parameter organization"
      },
      {
        "slug": "llama.cpp-api-minimalism-principle",
        "title": "API minimalism principle"
      },
      {
        "slug": "llama.cpp-consistent-clear-naming",
        "title": "consistent clear naming"
      },
      {
        "slug": "llama.cpp-consolidate-algorithmic-patterns",
        "title": "consolidate algorithmic patterns"
      },
      {
        "slug": "llama.cpp-explicit-control-flow-logic",
        "title": "explicit control flow logic"
      },
      {
        "slug": "llama.cpp-explicit-performance-handling",
        "title": "explicit performance handling"
      },
      {
        "slug": "llama.cpp-maintain-code-consistency",
        "title": "maintain code consistency"
      },
      {
        "slug": "llama.cpp-maintain-naming-consistency",
        "title": "maintain naming consistency"
      },
      {
        "slug": "llama.cpp-measure-before-optimizing",
        "title": "measure before optimizing"
      },
      {
        "slug": "llama.cpp-optimize-memory-access-patterns",
        "title": "optimize memory access patterns"
      },
      {
        "slug": "llama.cpp-prefer-const-variables",
        "title": "prefer const variables"
      },
      {
        "slug": "llama.cpp-specify-naming-formats-explicitly",
        "title": "Specify naming formats explicitly"
      },
      {
        "slug": "llama.cpp-systematic-test-coverage",
        "title": "systematic test coverage"
      },
      {
        "slug": "llama.cpp-use-strongly-typed-configurations",
        "title": "Use strongly typed configurations"
      },
      {
        "slug": "llama.cpp-validate-configuration-options-explicitly",
        "title": "validate configuration options explicitly"
      }
    ],
    "comments": {
      "llama.cpp-maintain-code-consistency": [
        "```suggestion\r\n    enum ggml_opt_optimizer {\r\n```\r\n\r\nFor consistency with the surrounding code.",
        "Move this declaration upwards so that it's in the same place as the other getters for `ggml_opt_context` (remember to also move the implementation)."
      ],
      "llama.cpp-use-strongly-typed-configurations": [
        "Can't this be determined automatically?",
        "I think an enum is better than a string.\r\n\r\nThe way I would implement the automatic detection is to try loading GGUF and Parquet first. The order shouldn't matter since the loading will fail if there is a mismatch. If both fail, load as plain text. I don't think fallback logic beyond that is needed."
      ],
      "llama.cpp-consolidate-algorithmic-patterns": [
        "```suggestion\r\n#if defined(AMD_MMA_AVAILABLE)\r\n        int64_t * xi = (int64_t *) t.x;\r\n        const int64_t * xs = (int64_t *) ((const int *) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\r\n        xi[0] = xs[0];\r\n#elif defined(NEW_MMA_AVAILABLE)\r\n```\r\n\r\nSee contributing guidelines.\r\n\r\nMore generally, if I interpret this code correctly it seems to me like it is basically the same as `load_generic` except the data is being loaded as a single 64 bit value. So I would suggest modifying that function instead.",
        "With NVIDIA the limitation for the source data in `ldmatrix` is that it needs to be a pointer to shared memory with 16 byte alignment. I was thinking about it more in terms of the memory space. In practice I think we would want to have the pointers be aligned anyways so I think it's fine to add memory alignment as a precondition for `load_generic` (the generic is more about not needing the `ldmatrix` instruction).",
        "The resulting value of 32 is correct but the way it's being calculated is wrong. The idea is that each thread moves 4 bytes so `threads_per_row` should always be equal to `WARP_SIZE` or with these changes `MMQ_TILE_NE_K`.",
        "Sorry I didn't read the PR properly. I thought `MMQ_TILE_NE_K` was the constant number of 32 bit values per row for all data types. Quite frankly I don't like the current state of things though. If the number of elements per row in a tile is a multiple of the physical warp size that is to me a sensible implementation, I don't like it being a seemingly arbitrary multiple of 32. I'm not 100% sure what would be the best solution here, maybe move `mmq_type_traits` up and define the tile shapes that way?",
        "Ultimately I currently can't think of a solution that really makes me happy. I think for this PR we can just merge it as-is and if I can later think of a better way to do it we can refactor it then (provided you'll be around for testing since I don't have access to CDNA3 hardware).",
        "I think all of these cases can be covered with `*dst = float(*src)`.",
        "Add a check for whether `src_t` and `dst_t` are the same, don't do a cast to float in that case (also covers FP16 -> FP16).",
        "Conversion between floats should not all be covered so this check can also be deduplicated by checking whether both types are either FP32, FP16, or BF16.",
        "This is fine for now but long-term I think it will be simpler (for floating-point data types) to define `__device__` functions that map from and to float rather than explicit mappings between 2 types.",
        "I think it would be better to turn `ggml_cuda_mul_mat_batched_cublas` into a template with the `ggml_type` as a template parameter.",
        "Actually, maybe turn the current function into something like `ggml_cuda_mul_mat_batched_cublas_impl` with a template parameter for the type and create a new function `ggml_cuda_mul_mat_batched_cublas` that just calls the impl function with the correct type.",
        ">ggml_get_to_fp16_nc_cuda\r\n\r\nWork with the `to_t_nc_cuda_t` template instead?\r\n\r\n>Perhaps just 3 different functions and ggml_cuda_mul_mat_batched_cublas does the dispatch based on type?\r\n\r\nI would really like to avoid this if at all possible. The cuBLAS interface should in principle be the same for FP32, FP16, and BF16. It should be possible to call cuBLAS concisely, one solution would I think be to define something like a `cublas_type_traits` template struct to get e.g. the right cuBLAS compute types (take a look at `mmq_type_traits` in `mmq.cuh` for an example of what I mean).",
        "The pointers are being cast to `char *` anyways so I think a simpler solution would be to just pass `void *` pointers.",
        "I would suggest adding something like `const bool XXX = ...` on the line above. As it is it's not clear to me what the correct interpretation of the check is. (Also it prevents the line from becoming too long.)",
        "This is not quite correct. For NVIDIA FP16 is supported on Pascal or newer (but CC 6.1 has terrible FP16 performance). So I would changing this to `supports_fast_fp16` and use that as one of the inputs for `use_fp16` (since they are only used together).\r\n\r\nAlso, I don't have a P100 available for testing, but the correct logic for CUDA and AMD should be `fast_fp16_hardware_available(cc)`. Consider extending that function for MUSA since it is also used to determine the optimal code paths for e.g. MoE models.",
        ">Would it be better to replace this with fast_fp16_hardware_available(cc) since that function is already updated in this PR?\r\n\r\nYes, that's what I meant."
      ],
      "llama.cpp-specify-naming-formats-explicitly": [
        "Preferably use an integer for the version. If you want to specify minor versions my suggestion is to either specify multiple integers or to do what NVIDIA does and specify e.g. v12.34 as something like `12034`.",
        "```suggestion\r\n    *   **Naming**: `training.tensor.{index}` (e.g., `training.tensor.0`, `training.tensor.1`, ...). No leading zeros.\r\n```\r\n\r\nWould also be fine to add leading zeros but it should be specified."
      ],
      "llama.cpp-systematic-test-coverage": [
        "You can remove this logic if the Vulkan backend simply returns that `OPT_STEP_SGD` is unsupported, the corresponding test will then simply be skipped."
      ],
      "llama.cpp-prefer-const-variables": [
        "```suggestion\r\n            const int j = j0 + tile_C::get_j(0);\r\n            const float2 dsB = __half22float2(y_dm[j*MMQ_TILE_Y_K + k01/QI8_1]);\r\n```\r\n\r\nPreferably use `const` if applicable.",
        "```suggestion\r\n    const int global_idx     = blockIdx.x * blockDim.x + threadIdx.x;\r\n    const int total_elements = batches * channels * out_h * out_w;\r\n```\r\n\r\nPreferably add const to variables unless they are actually intended to be changed later."
      ],
      "llama.cpp-optimize-memory-access-patterns": [
        "```suggestion\r\n        if constexpr (I != 64 || J != 2) {\r\n            int64_t * xi = (int64_t *) t.x;\r\n            const int64_t * xs = (int64_t *) ((const int *) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\r\n            xi[0] = xs[0];\r\n            return;\r\n        }\r\n```\r\n\r\nI think this would be simpler.",
        "I basically meant to have the instructions for loading data as 64 bit encapsulated in an `ifdef AMD_MFMA_AVAILABLE ... #endif` and to use the generic implementation if the preconditions aren't met. But if this is going to be refactored anyways it doesn't matter.",
        "```suggestion\r\n    const void * __restrict__ src0,\r\n    void * __restrict__ dst,\r\n```\r\n\r\nOn older GPUs especially you can gain some performance by telling the compiler that the pointers aren't aliased.",
        "I didn't look at the generated PTX code for this specific CUDA code but in my experience the CPU code pattern with explicit byte offsets performs comparatively poorly on GPUs. My recommendation would be to have the input and output types as template parameters and to compute the strides in units of the types (e.g. `nb01/ggml_element_size(src0)`) in host code. This is of course assuming that this kernel has a non-negligible contribution to the end-to-end performance in the first place.",
        "I didn't compare the two versions but I think it would be better to use a static block size of e.g. 256 here and to map the thread indices to tensor data indices as if the tensor was flattened. Running CUDA code with fractional warps will always result in wasted GPU resources.",
        "```suggestion\r\n    const int i01 = blockIdx.x;\r\n    const int i00 = blockIdx.y*blockDim.x + threadIdx.x;\r\n```\r\n\r\nThis is how you should organize the threads, otherwise the memory accesses are uncoalesced for `nb00 == ggml_element_size(src0)`. If `blockIdx` had the same size in all dimensions we could just do:\r\n\r\n```\r\n    const int i01 = blockIdx.y;\r\n    const int i00 = blockIdx.x*blockDim.x + threadIdx.x;\r\n```\r\n\r\nBut then we run into issues for `i01 >= 65536`. So the solution is to just swap `blockIdx.x` and `blockIdx.y` but without touching `blockdim` and `threadIdx` (since they are conceptually different)."
      ],
      "llama.cpp-explicit-performance-handling": [
        "```suggestion\r\n# Properties that are boolean and are converted to Yes/No for the table:\r\nLLAMA_BENCH_BOOL_PROPERTIES = [\"embeddings\", \"cpu_strict\", \"use_mmap\", \"no_kv_offload\", \"flash_attn\"]\r\nTEST_BACKEND_OPS_BOOL_PROPERTIES = [\"supported\", \"passed\"]\r\n```",
        "```suggestion\r\n    elif unit == \"GFLOPS\":\r\n        gflops = value\r\n    else:\r\n        assert False\r\n```"
      ],
      "llama.cpp-explicit-control-flow-logic": [
        "This logic is incorrect. Whether or not the optimizer needs buffers for the momenta is independent of whether or not buffers are needed to accumulate the gradients.",
        "```suggestion\r\n            struct ggml_tensor * opt_step;\r\n            switch (opt_ctx->optimizer_type) {\r\n                case GGML_OPT_OPTIMIZER_ADAMW:\r\n                    opt_step = ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, adamw_params);\r\n                    break;\r\n                case GGML_OPT_OPTIMIZER_SGD:\r\n                    opt_step = ggml_opt_step_sgd(opt_ctx->ctx_compute, node, grad, adamw_params);\r\n                    break;\r\n                default:\r\n                    GGML_ABORT(\"fatal error\");\r\n                    break;\r\n            }\r\n```\r\n\r\nThis is what I meant regarding separating the logic for allocation and setting `opt_step`. I don't have a problem with whether or not the code produces the correct logic, I have a problem with the implicit reasoning behind it. Whether or not the momenta should be allocated is derived from the type of optimizer, not the other way around.",
        "The optimizer type should be constant for the lifetime of `ggml_opt_context` so it's enough to just check whether the constant optimizer type needs the momenta.",
        "I think that if someone wants to change the optimizer type they should simply free the current `ggml_opt_context` and create a new one. The model weights will be unaffected and the overhead should be negligible vs. the runtime of one epoch.",
        "Add the optimizer type to `ggml_opt_params` and `ggml_opt_context`, set the value of the context during initialization, do not change it afterwards, use it for the `need_moment` logic. Does that make it clear enough?",
        "This is inconsistent with the docstring in `ggml.h`. As I outlined before for AdamW, the interface in `ggml.h` should be using the human-readable parameters. Please simply pass `alpha` and `wd` here. A derived parameter `keep` should be calculated in the backend-specific implementations for `OPT_STEP_SGD`."
      ],
      "llama.cpp-ai-parameter-organization": [
        "The number of epochs is not a property of the optimizer.",
        "It should be in \"user code\". The concept of an epoch only makes sense in conjunction with a dataset, so it should be added in functions like `ggml_opt_epoch` or `llama_opt_epoch` where there is explicit iteration over a dataset."
      ],
      "llama.cpp-api-minimalism-principle": [
        "I would say it's preferable to keep the interface simple: add the `--preview` flag, if set it always shows a fixed number of sequences, both as tokens and as text."
      ],
      "llama.cpp-measure-before-optimizing": [
        "Optimizer steps are going to be I/O bound and optimizing compute is not going to make a meaningful difference for the runtime of the steps, for the runtime of the total probram it's completely negligible. So please revert this change, I think the other variant is easier to understand.",
        "My biggest concern with the code is the amount of effort needed to maintain it, particularly when it comes to debugging and asserting that the code on master works correctly. It is quite likely that I will at some point be in a situation where a user reports bad training results and I will not know whether that is the due to a bug in ggml or due to bad hyperparamters or something similar. So it is very important to me that the data layout is consistent across multiple levels. \r\n\r\nThe correct way to implement the micro-optimization of pre-computing a parameter derived from the human-interpretable parameters is as follows:\r\n\r\n1. Pass the human-interpretable parameters to `ggml_opt_step_adamw` / `ggml_opt_step_sdg`.\r\n2. In the CUDA host code, pre-compute some derived parameters from the human-interpretable parameters.\r\n3. Change the CUDA device code to accept the derived parameters instead.\r\n\r\nThe way CUDA works is that the CPU schedules the GPU kernels in a CUDA stream and then waits for said stream to finish all kernels. Scheduling the kernels is of course much faster and it doesn't matter how fast you are as long as you are fast enough to keep the GPU busy. So adding a bit of overhead to the scheduling has essentially no impact on the runtime of a CUDA program even if you do it once per CUDA kernel launch instead of once per epoch.",
        "If performance is a concern we could maybe provide a list of EoG tokens to iterate over instead of iterating over all tokens and checking whether each one is EoG. Although I think iterating over all tokens once per request is going to be negligible vs. iterating over all tokens once per generated token as is being done for sampling."
      ],
      "llama.cpp-maintain-naming-consistency": [
        "```suggestion\r\n                    snprintf(buf, sizeof(buf), \"%6.2f kFLOP\", flops / 1e3);\r\n```\r\n\r\nNot that it was added with this PR, but the correct capitalization for a kilo FLOP is with a k.",
        "I don't understand the purpose of this line.",
        "Ah okay, I didn't see the inline `+=` in the line below. I think the code could be made easier to understand if you change the name of `step_prefix_len` to something like `step_prefix_len_orig` and move the `+=` to a separate line."
      ],
      "llama.cpp-consistent-clear-naming": [
        "```suggestion\r\n    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);\r\n```\r\n\r\nYou were right that it's fine to use either `tile_B::I` or `tile_C::J`. We should consistently use one of them.",
        "Either way is fine, it should just be consistent. I have a slight preference towards `tile_C::J` since that is what is used for the iteration over  `ntx`.",
        "Rename the arguments to e.g. `s01` to avoid confusion with the offsets in bytes. Preferably use `int64_t` since that is the data type used in host code."
      ],
      "llama.cpp-validate-configuration-options-explicitly": [
        "```suggestion\r\n        if self.tool == \"llama-bench\":\r\n            self.check_keys = set(LLAMA_BENCH_KEY_PROPERTIES + [\"build_commit\", \"test_time\", \"avg_ts\"])\r\n        elif self.tool == \"test-backend-ops\":\r\n            self.check_keys = set(TEST_BACKEND_OPS_KEY_PROPERTIES + [\"build_commit\", \"test_time\"])\r\n        else:\r\n            assert False\r\n```\r\n\r\nPreferably assert exact matches in case we ever add more tools.",
        "I would suggest setting the `--tool` CLI argument to `None` by default. If it is `None` here, use either table if it exists.",
        "I don't understand what you mean by that.",
        "It should be the same behavior by default. The default value for `--tool` is `None`. If pointed at an SQLite file with a table produced by `llama-bench`, check for the existence of table `test` and use it if it exists. Otherwise use table `test_backend_ops` if it exists. If the user explicitly specifies `--tool`, try to use the corresponding table, raise an error if it doesn't exist."
      ]
    },
    "profile": {
      "location": "Karlsruhe, Germany",
      "company": "Karlsruhe Institute of Technology",
      "blog": "",
      "site_admin": false,
      "followers": 252,
      "following": 0
    }
  },
  "justinmk": {
    "repos": [
      "neovim/neovim"
    ],
    "entries": [
      {
        "slug": "neovim-api-consistency-patterns",
        "title": "API consistency patterns"
      },
      {
        "slug": "neovim-api-extensibility-parameters",
        "title": "API extensibility parameters"
      },
      {
        "slug": "neovim-avoid-error-masking",
        "title": "avoid error masking"
      },
      {
        "slug": "neovim-avoid-lua-ternary-traps",
        "title": "avoid Lua ternary traps"
      },
      {
        "slug": "neovim-avoid-unnecessary-configuration",
        "title": "avoid unnecessary configuration"
      },
      {
        "slug": "neovim-complete-function-documentation",
        "title": "Complete function documentation"
      },
      {
        "slug": "neovim-conditional-expensive-operations",
        "title": "Conditional expensive operations"
      },
      {
        "slug": "neovim-configuration-variable-organization",
        "title": "Configuration variable organization"
      },
      {
        "slug": "neovim-consistent-algorithm-interfaces",
        "title": "consistent algorithm interfaces"
      },
      {
        "slug": "neovim-consistent-naming-conventions",
        "title": "consistent naming conventions"
      },
      {
        "slug": "neovim-consolidate-network-apis",
        "title": "consolidate network APIs"
      },
      {
        "slug": "neovim-defer-expensive-operations",
        "title": "defer expensive operations"
      },
      {
        "slug": "neovim-document-connection-scope",
        "title": "Document connection scope"
      },
      {
        "slug": "neovim-documentation-accuracy-standards",
        "title": "Documentation accuracy standards"
      },
      {
        "slug": "neovim-documentation-formatting-standards",
        "title": "Documentation formatting standards"
      },
      {
        "slug": "neovim-follow-established-api-patterns",
        "title": "Follow established API patterns"
      },
      {
        "slug": "neovim-follow-protocol-standards",
        "title": "follow protocol standards"
      },
      {
        "slug": "neovim-initialize-before-dereferencing",
        "title": "Initialize before dereferencing"
      },
      {
        "slug": "neovim-optimize-algorithmic-complexity",
        "title": "Optimize algorithmic complexity"
      },
      {
        "slug": "neovim-prefer-concise-expressions",
        "title": "prefer concise expressions"
      },
      {
        "slug": "neovim-prevent-autocommand-reentrancy",
        "title": "prevent autocommand reentrancy"
      },
      {
        "slug": "neovim-prioritize-code-readability",
        "title": "prioritize code readability"
      },
      {
        "slug": "neovim-provide-helpful-documentation-context",
        "title": "Provide helpful documentation context"
      },
      {
        "slug": "neovim-reduce-test-verbosity",
        "title": "reduce test verbosity"
      },
      {
        "slug": "neovim-reuse-concurrency-infrastructure",
        "title": "reuse concurrency infrastructure"
      },
      {
        "slug": "neovim-security-warnings-need-guidance",
        "title": "Security warnings need guidance"
      },
      {
        "slug": "neovim-semantic-naming-over-implementation",
        "title": "Semantic naming over implementation"
      },
      {
        "slug": "neovim-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "neovim-validate-early-fail-fast",
        "title": "validate early, fail fast"
      }
    ],
    "comments": {
      "neovim-documentation-formatting-standards": [
        "```suggestion\r\n• |prompt-buffer| supports multiline input/paste, undo/redo, and o/O normal\r\n  commands.\r\n```"
      ],
      "neovim-optimize-algorithmic-complexity": [
        "this has triple-nested for-loops and doesn't say even 1 comment giving a hint about what it does."
      ],
      "neovim-follow-protocol-standards": [
        "```suggestion\n  bool is_tcp = !!strrchr(server_address, ':');\n```"
      ],
      "neovim-avoid-lua-ternary-traps": [
        "ternary"
      ],
      "neovim-provide-helpful-documentation-context": [
        "does the `This fails when changes have been made` note need to be updated? Also should mention `:confirm restart` as a hint.",
        "If it checks changes by default then :confirm is never needed? It probably makes sense to *not* check changes by default, then user can opt-in to checking changes via :confirm.",
        "docs should also show a +cmd example",
        "instead of mentioning this in a different place, add a line after line 1324 like\n\n    `:EditQuery <tab>` completes injected language names.",
        "should the module `@brief` mention what \"linked editing session\" is? Not obvious to me. Also linking to the spec section helps.",
        "non-blocker: should we mention something in lsp.txt (not the private fields, but just something like \"DiagnosticRelatedInformation is supported/included\")? I looked around and the best place would be a `@brief` in `lsp/diagnostic.lua` , then it will appear in the \"overview\" part of `:help lsp-diagnostic`\r\n\r\n"
      ],
      "neovim-semantic-naming-over-implementation": [
        "is there an `on_foo` name we can use? same for the others. \r\n```suggestion\r\ndeclaration({opts}, {on_result})                    *vim.lsp.buf.declaration()*\r\n```",
        "All of the callbacks in this pr need some sort of `on_` prefix (`:help dev-naming`). We use \"callback\" way too much. ",
        "> `on_` isn't necessary for continuations.\r\n\r\nIt is much easier to communicate one invariant rule rather than multiple, subjective rules. `on_x` tends to result in much more meaningful names, while also avoiding over-use of `callback`.\r\n\r\n> What is \"too much\"?\r\n\r\n`callback` and `cb` are used in places where an `on_x` name would be more useful and intuitive. And then people see that and just do even more of it. That is why it matters: people copy what existing code is doing, so isolated decisions spread out and multiply.",
        "> leads lots of variations of the same name: `on_exit`, `on_done`, `on_finished`, `on_result`, `on_complet`\r\n\r\nIf those are the same then they should align on one name.\r\n\r\n> It's also not obvious these names are callbacks... unlike `callback`.\r\n\r\nWhat happens when there is more than one callback? Then you need a qualifier anyway. So having a common prefix is extra helpful in that case.\r\n\r\nAnything prefixed with `on_` is a callback, I don't see how that is unclear.\r\n\r\n> but the `on_x` thing just seems like a specific thing you are over pushing for everything and seems to come directly from js. \r\n\r\nNo, this decision was made after experience from early days of the project where `_cb` prefix was used. `on_` is much more readable and conventional. You made a wrong assumption/speculation here.\r\n\r\nIt's a project guideline, and continually objecting to project guidelines is really not helpful. ",
        "> > What happens when there is more than one callback?\r\n> \r\n> Then it's not a continuation.\r\n\r\nA continuation is just a special case. And naming it differently leads to confusion and debates, which is exactly what I'm trying to avoid.\r\n\r\nMany people are just going to copy `callback` even when it's not a \"continuation\" or whatever other specific circumstances. A blanket rule is easier to follow and document.\r\n\r\n> Maybe if you're used to js.\r\n\r\nIt could be named `abc_` for all I care, what matters is that it's a common consistent prefix instead of a different subjective choice everywhere. \r\n\r\nIf it's hard for you to adapt to `on_` then you'll appreciate that it's even harder to document and learn when different kinds of callbacks/handlers should be named in which way in various contexts.",
        "> If there are multiple events then `on_` can be switched to.\r\n\r\nNot if it's a field, like in the case of `nvim_create_autocmd({callback=...})` which is a case where `on_x` was clearly more meaningful and readable.\r\n\r\nAnd \"we'll just do it differently here and there\" is what I'm trying to avoid.",
        "I added it because we had no guidelines and we have things named `handler`, `callback`, `x_cb`, `on_x`, and more (js uses \"listener\" btw). What's your point? I get criticized when there are no guidelines, and when there are...\r\n\r\nHow about this compromise:\r\n\r\n- `callback` for continuations (exactly one callback param, not multiple [like uv.new_work](https://github.com/neovim/neovim/blob/4745270bf124a960ccdffdddbb6c27b7bf36ba82/runtime/doc/luvref.txt#L3600) for example),\r\n- `on_x` for everything else\r\n\r\nand we can commit to this (no debates about special cases or personal preferences in the future)? Then I will update the docs.",
        "`on_response` sure would be more meaningful here. Instead the docs have to explain it.",
        "does `on_response` make sense as the name for this? \"exit\" is an implementation detail.  the fact that curl is used internally isn't relevant and shouldn't be surfaced in the request() interface (for now)."
      ],
      "neovim-validate-early-fail-fast": [
        "also need to handle the case where the +cmd didnt cause an exit nor an error ?",
        "> We could just call `getout(0)` if we get there by logging an error message\r\n\r\nlogging *and* then call `getout` makes sense 👍 ",
        "> I was initially thinking to just match against a list of possible \"quit\" commands (and combinations) and then checking against that\n\nThat's probably good enough for now.\n\nThe \"right\" way to do things is probably to setup some sort of `on_exiting` callback which performs `remote_ui_restart` only when Nvim is about to exit, wherever the `VimLeavePre` logic is."
      ],
      "neovim-complete-function-documentation": [
        "This is a whole new blob of code that future developers must study. We really need to give hints where we can. In this case, the key idea seems to be (correct if wrong):\n\n\n```suggestion\n  -- Compute positions, set them as extmarks, and store in diagnostic._extmark_id\n  -- (used by get_logical_location to adjust positions). \n  once_buf_loaded(bufnr, function()\n```\n\nNone of this will be obvious in the future. We need to leave hints.",
        "```suggestion\n--- Can also be shown with `:EditQuery`. [:EditQuery]()\n--- `:EditQuery <tab>` completes injected language names.\n```",
        "try this\r\n\r\n```\r\n--- Can also be shown with `:EditQuery`. `:EditQuery <tab>` completes injected language names. [:EditQuery]()\r\n```\r\n\r\nbtw, I'm confused about the mention of \"injected languages\" in your docs, since it looks like `_complete()` just iterates all available parsers, not specific to \"injected languages\" for the current buffer.",
        "need a `@return` tag. maybe helpful to look at `vim.system` example https://github.com/neovim/neovim/blob/e518666f1db110abcfc899e1227da949998cdd82/runtime/lua/vim/_editor.lua#L130",
        "the wording is kind of confusing, is this rewording correct?\r\n```suggestion\r\n--- @return string|boolean|nil Response body (sync) or `true` (async) on success; `nil` on failure\r\n```",
        "is there a 1-line docstring that can give insight into what this does? E.g. \"Sets a mark unless...\"",
        "missing `@since`\r\n\r\n```suggestion\r\n--- Computes the common range shared by the given ranges.\r\n```"
      ],
      "neovim-api-consistency-patterns": [
        "this existing test shouldn't require a change, because the enhancement to serverlist() should be opt-in, i.e. the user needs to pass something like:\n\n    vim.fn.serverlist({peer=true})",
        "```suggestion\n  -- TODO: Handle this generally (like vim.ui.open()), rather than overriding gf. #7282\n```"
      ],
      "neovim-avoid-error-masking": [
        "why silent with \"!\"? that silences errors...",
        "use `vim.system():wait()`. that avoids fragile things like `vim.v.shell_error` :)",
        "`t.pcall_err`\n```suggestion\n    eq(':edit command in prompt buffer throws error',\n       t.pcall_err(api.nvim_command, 'edit'))\n```"
      ],
      "neovim-document-connection-scope": [
        "We probably need to mention \"This only works if the UI started the server initially.\" \r\n\r\nIf the UI connected to some random remote endpoint, that is out of scope for now. We could think about how to support it if anyone shows interest."
      ],
      "neovim-prefer-concise-expressions": [
        "please, use ternaries for simple cases. verbose code is a cost."
      ],
      "neovim-conditional-expensive-operations": [
        "this is a perf cost, which should only be paid if it's actually necessary (i.e. depending on the `follow` param)",
        "perf nit: vim.ts is [lazy loaded](https://github.com/neovim/neovim/blob/7cd5356a6f89a46d83bbba9b7f6496b67f054629/runtime/lua/vim/treesitter.lua#L6) so let's wrap this to avoid eager-loading it:\n```suggestion\n    complete = function(...) return vim.treesitter.language._complete(...) end,\n```",
        "> I think ideally the full `DiagnosticChanged` autocmd should only be setup on demand if the default status line is actually used. Otherwise everyone pays the cost for the string formatting.\r\n\r\nOh, should we introduce this as `vim.diagnostic.status()` (compare `vim.lsp.status()`) ?\r\n\r\nAnd yes, making this conditional on whether the default statusline is used, makes sense.",
        "> `vim.lsp.status()` displays messages from all clients for all buffers.\r\n\r\nvim.diagnostics.status() doesn't need to exactly match how lsp.status works; it's just a reference for the \"shape\" + name.\r\n\r\n\r\n\r\n> If we decided to go with `vim.diagnostic.status()` would it be used in the statusline as is i.e.\r\n\r\nI think so. And then the [default 'statusline'](https://github.com/neovim/neovim/blob/6577d72d819dde32abeacd6a72d6ba64483f7ddc/src/nvim/options.lua#L8577-L8584) will reflect this too, which is Good. If there are \"performance\" issues, those need to be fixed internally in `status()`, e.g. by debouncing or whatever."
      ],
      "neovim-documentation-accuracy-standards": [
        "the docstring looks wrong, it doesn't return anything"
      ],
      "neovim-consistent-naming-conventions": [
        "Let's stick to \"pos\" as the nonce for position/location. Jargon creep adds friction.\n\n```suggestion\nlocal function get_logical_pos(diagnostic)\n```",
        "In a description many different terms may be used to orient the reader. But in interface names, same concept should always have same name.",
        "avoid the `_cb` suffix, use an `on_` suffix. `:help dev-naming`",
        "`on_cmd` is fine",
        "The name FRProvider seems questionable. Why not name it `FoldRange` or `FoldState`? If it's just an object that does stuff, that's the typical pattern.\r\n\r\n\"Provider\" implies that there will be different backend implementations of a concept, typically platform-specific in some way. If we're only talking about polymorphism or a \"base class\", it's helpful to just name the base-class by its main purpose.",
        "> The suffix `Provider` is just to highlight that it have a common base class and contains methods.\r\n\r\nOk then \"Provider\" isn't the right name, for the reason mentioned in my above comment.\r\n\r\n`vim.lsp.folding_range.State` seems fine, but the name of the instance should probably be `foldState` or `foldRangeState`.",
        "would refresh() make sense here? that's a common name in other modules.",
        "the doc seems to imply this name:\n\n\n```suggestion\nlocal function get_confirm_update_plugins(bufnr)\n```",
        "start/end are the normal convention in the codebase\r\n\r\n```suggestion\r\n          local mark = { start_line = start_row, start_col = start_col, opts = opts }\r\n```",
        "since \"mode\" is pretty similar to \"modal\", maybe mention \"win\" in these new names. My reflex would be a prefix (`win_modal_active`, `win_cmdwin_active`, `win_modal_result`, `WinModalType`, ...) but I guess the prevalence of `cmdwin_xx` would justify `modalwin_xx` (this also complements `floatwin_xx`).",
        "handlers should be prefixed with `on_`. or if we think of this function as \"procedural\" we can just call it `show_message()` ?\r\n\r\n\r\n\r\n```suggestion\r\nlocal function on_show_message(params, ctx)\r\n```"
      ],
      "neovim-configuration-variable-organization": [
        "I like the \":\" idea to separate the variable part. (Though hopefully we don't need that in the future when we have [user-defined event keys](https://github.com/neovim/neovim/pull/34637#discussion_r2188886306).)",
        "[Rethinking this](https://github.com/neovim/neovim/pull/34388#discussion_r2196343686), can we just store the state on the client instead of creating per-client augroups?",
        "(Non-blocker; we can try it out and revisit later:) This could end up creating many top-level _lsp_...<client> vars. That's pretty noisy. It might be worth the trouble to define a single _lsp_enable table and manage things in there, e.g.\n\n```\n_lsp_enable = {\n  client = { [42] = true, [78] = true, ... },\n}\n```",
        "Actually, could we store this info on the `client` object itself? Any time the \"key\" is the client-id, it usually makes sense to store the data on the `Client`. Or is that not possible because of \"lifecycle\" requirements (maybe we need a `on_dispose` hook on the client?)",
        "Looking at this again, IIUC the per-client augroups (`get_client_augroup`) are just to enable/disable a specific client? And thus the augroup is a way to manage state. \n\nSeems like a much better pattern would be to store the state on the client itself. Could shove it in a `_foo` field for now. Then we just need 1 general `LspAttach` handler , which checks the `_foo` field on the relevant client object.",
        "> I like this because eventually we can also use a more general `features` property\r\n\r\nsame thought popped into my mind... :) related: https://github.com/neovim/neovim/issues/34659"
      ],
      "neovim-reduce-test-verbosity": [
        "this is the same screen, isn't it? just use a multiline pattern to match the contents of 1 screen, instead of multiple expect() with `unchanged=true`.\n\nsame for the other cases below.\n\n```suggestion\n      screen:expect({ any = 'Allowed.*\\n.exrc' })\n```",
        "When the literal contents are known we usually don't use match. It tends to be more readable to write the literal contents, e.g. in this case it would look like (I think):\n\n    {1:~                          }|*2",
        "oh, yeah that makes sense.",
        "test is too verbose just to check that a commmand requires an argument. that only requires ~2 lines to check. also rename this to: \n```suggestion\n  it('validation', function()\n```\n\nthen we can add more validation later.",
        "is it possible for these tests to re-use a shared function, to make them less verbose?\r\n\r\nindirection can make things harder to read, but if these tests are mostly doing the same kind of thing, then using a shared function can be much more readable because it makes it obvious what is being tested in each case.",
        "are we actually testing anything here or is this just redundantly testing the existing behavior  of extmarks?\r\n\r\nis there a less verbose way to instead (1) assert that extmarks are mapped as expected to a diagnostic, and (2) just assume that extmarks work they way they should?\r\n\r\nsame question for all the other tests.",
        "instead of a separate test for this case, let's just add an extra step to the end of the one above, something like `enew` followed by `eq('', exec_lua(return vim.diagnostic.status()`.\n\nthat saves 20 lines of code, which is valuable.",
        "note: the args are flipped :) `eq()` takes the \"expected\" value as the first arg.",
        "the earlier lines in this test already make edits (and are more readable than \"asdf\"). wouldn't it make sense just to add `prompt_getinput` assertions after those screen tests, instead of peforming new edits here ?"
      ],
      "neovim-reuse-concurrency-infrastructure": [
        "could that be a flag on `vim.system` ? want to avoid teaching people to reach for similar things in different places. "
      ],
      "neovim-defer-expensive-operations": [
        "```suggestion\r\n  `msg_history_clear` events. Performance benefit: reduces UI events traffic.\r\n```"
      ],
      "neovim-consistent-algorithm-interfaces": [
        "please search for \"predicate\", \"filter\", \"skip\", and any other related concept, in the :help docs to see which is the most common. I'm wondering if positive logic (\"match\") or negative logic (\"skip\") is more common and more intuitive. ",
        "> I borrowed the concept from `vim.fs.dir`\r\n\r\nOh, I missed that an existing `vim.fs` function already has this concept. In that case this seems fine.",
        "does returning `false` omit a directory (that matches the `vim.fs.dir` case)?\r\n```suggestion\r\n                 • {filter}? (`fun(dir: string): boolean`) Predicate that\r\n                   decides if a directory is traversed. Return true to traverse\r\n                   a directory, or false to skip. ...                   \r\n```",
        "```suggestion\r\n• |vim.fs.find()| accepts a `filter` predicate.\r\n```"
      ],
      "neovim-follow-established-api-patterns": [
        "`:help dev-naming` i would expect a `enable(enable:boolean)` pattern, not a stop() function\r\n\r\nalso agree that if we want a new module then it seems attractive to have a [format module](https://github.com/neovim/neovim/pull/34637#discussion_r2165069136) for formatting-related things. (We also discussed a `vim.lsp.workspace` module for workspace-related thing).\r\n\r\nBut ideally I think [`vim.lsp.enable` or a similar](https://github.com/neovim/neovim/pull/34637#discussion_r2166175192) top-level demuxer seems to be badly needed, instead of numerous `vim.lsp.x.enable()` things.",
        "if more filters are being added then it's time to make this a \"kwargs\" arg instead of adding more parameters.\r\n\r\nprobably `is_enabled({filter})` is best, though `is_enabled({bufnr}, {filter})` could be acceptable (and `bufnr=nil` would be \"all/any buffer\")\r\n\r\n```suggestion\r\nis_enabled({bufnr}, {filter})             *vim.lsp.inlay_hint.is_enabled()*\r\n```",
        "Isn't a filter kwargs table the common pattern? Not sure why `vim.lsp.completion.enable()` differs though.\r\n\r\n```suggestion\r\nenable({enable}, {filter})\r\n```",
        "> `semantic_tokens` doesn't either (but it also uses `start()` and `stop()` to be weird 😆\r\n\r\noh we might need to fix that. https://github.com/neovim/neovim/issues/34664\r\n\r\n> Seems like enabling/disabling features is kinda all over the place right now\r\n\r\n`:help dev-patterns` documents the expected `enable()` pattern.\r\n\r\n> will just take some more logic to handle global enabling/disabling\r\n\r\nif possible we should add a `vim._util` to help with that",
        "> supporting global enabling will add a lot of complexity (especially if we specify only one of `buf = nil` and `client_id = nil`. Is this a blocker or can it be put on hold?\r\n\r\nthere is no requirement to support any particular behavior, as long as the interface has the right pattern. i.e `enable(boolean)`",
        "Just to be clear, there is no requirement for the initial impl of this or other lsp features to fully support things like per-client enablement, etc. The only requirement is that the basic signature of `enable(enable:boolean, filter?:table)` is correct :) \r\n\r\n\"per-client\", \"per-buffer\" can be added later by expanding the `filter` kwargs."
      ],
      "neovim-avoid-unnecessary-configuration": [
        "no, special-purpose options are clusmsy and a maintenance burden.\n\nthe *actual* implementation is only a few lines long. since users have to find this documentation and add this option anyway, it's nearly equivalent to just documenting an event handler that they can copy into their config. "
      ],
      "neovim-api-extensibility-parameters": [
        "- We almost always want to accept `opts` and return `Dict`, unless we are 100% certain we won't need to expand the features in the future (which is usually wrong).\r\n- But in this case the returned value is a tabpage handle which (1) already represents an object, and (2) is symmetric with `nvim_open_win`.  So it's probably acceptable to return `Tabpage`.\r\n- For `opts` (or `config`?) I could imagine an arg that controls which tabpage to open next to (like `:[count]tabnew`).\r\n\r\n```suggestion\r\nTabpage nvim_open_tabpage(Buffer buffer, Boolean enter, Dict opts, Error *err)\r\n```",
        "FUTURE: we might want to give more control over how the server is stopped. E.g.:\r\n\r\n    :restart +qall\r\n\r\n(similar to `:help +cmd`). This also frees up \"!\" to be used for some other purpose."
      ],
      "neovim-prevent-autocommand-reentrancy": [
        "Side-note: not a blocker for this PR, but need think about how to show an error if this logic fails. E.g. `:connect bogus` currently just exits without saying why."
      ],
      "neovim-security-warnings-need-guidance": [
        "Let's adjust this to:\n```suggestion\n        .. 'Xfile is not trusted. To enable it, choose (v)iew then run `:trust`.'\n```"
      ],
      "neovim-initialize-before-dereferencing": [
        "coverity:\r\n\r\n```\r\n*** CID 554963:           (FORWARD_NULL)\r\n/src/nvim/memline.c: 3484             in findswapname()\r\n3478                 if (swap_exists_action != SEA_NONE) {\r\n3479                   kv_printf(msg, _(\"Swap file \\\"\"));\r\n3480                   kv_printf(msg, \"%s\", fhname);\r\n3481                   kv_printf(msg, _(\"\\\" already exists!\"));\r\n3482                   char *run_but = _(\"&Open Read-Only\\n&Edit anyway\\n&Recover\\n&Quit\\n&Abort\");\r\n3483                   char *but = _(\"&Open Read-Only\\n&Edit anyway\\n&Recover\\n&Delete it\\n&Quit\\n&Abort\");\r\n>>>     CID 554963:           (FORWARD_NULL)\r\n>>>     Passing null pointer \"msg.items\" to \"do_dialog\", which dereferences it.\r\n3484                   choice = (sea_choice_T)do_dialog(VIM_WARNING, _(\"VIM - ATTENTION\"), msg.items,\r\n3485                                                    proc_running ? run_but : but, 1, NULL, false);\r\n3486     \r\n3487                   // compensate for missing \"Delete it\" button\r\n3488                   choice += proc_running && choice >= 4;\r\n3489                   // pretend screen didn't scroll, need redraw anyway\r\n/src/nvim/memline.c: 3492             in findswapname()\r\n3486     \r\n3487                   // compensate for missing \"Delete it\" button\r\n3488                   choice += proc_running && choice >= 4;\r\n3489                   // pretend screen didn't scroll, need redraw anyway\r\n3490                   msg_reset_scroll();\r\n3491                 } else {\r\n>>>     CID 554963:           (FORWARD_NULL)\r\n>>>     Passing null pointer \"msg.items\" to \"msg_outtrans\", which dereferences it.\r\n3492                   msg_outtrans(msg.items, 0, false);\r\n3493                 }\r\n3494                 no_wait_return--;\r\n3495                 kv_destroy(msg);\r\n3496                 xfree(fhname);\r\n3497               }\r\n```"
      ],
      "neovim-use-descriptive-names": [
        "\"float\" is ambiguous. we should always say \"floatwin\" or win_float.\n```suggestion\nbool win_float_parse_option(WinConfig *config)\n```",
        "was this copied from vim? or generated by AI? :) we have lots of similar code like `parse_winborder`, `parse_win_config`, `parse_border_style`. why do we need even more ? \n\nand if it's really needed, the name should make it more clear that this is specifically for 'previewpopup'. The name `parse_float_option` is very unclear. A clearer name would be `parse_previewpopu_option`.\n\nAlso, should this live in `optionstr.c` ?\n",
        "@bfredl latest commit returns names like `cmdline_:` and `cmdline_/`. Is that what you were thinking?\r\n\r\nI think I'd prefer if we stuck to the standard plain `:` and `/` names, and these are simply understood (and documented) to mean \"cmdline window\". Then we have new identifiers for new window-types (e.g. `modal`, `modal-term`, ...). Normal windows are \"\" (empty string).",
        "`prompt_cur_input` ?\r\n\r\nalso a brief docstring helps when there is ambiguity. ",
        "either SGTM, slightly in favor of `prompt_gettext` since it aligns with `getcmdline()` (whereas `input*()` family of functions usually involves starting a prompt, not getting the current input?)"
      ],
      "neovim-prioritize-code-readability": [
        "this logic looks suspicious. what is the purpose of the `metadata.conceal ~= nil` check, since if `metadata.conceal=false` it will fallthrough to the latter part anyway? also, use parens for clarity, since `a and b or c and d` is not obvious.",
        "not asking for an if statement, just parens",
        "use `('...'):format()` almost always. using `..` to concatenate usually ends up being less readable.",
        "> As a modest proposal, could we perhaps increase the column threshold from 100 to like 120?\r\n\r\nPushed an update.\r\n\r\nAre we ok with the changes to other test files, or should the 120 width be limited to certain files (idk if that's possible)"
      ],
      "neovim-consolidate-network-apis": [
        "side note: could drop this and use `vim.net.request()` now ?",
        "I suppose this can be a followup, but mentioning before I forget:\r\n\r\nwe will need to define new `NVIM_TEST_INTEG` env var and document it here: https://github.com/neovim/neovim/blob/f731766474901e5e345e0ca630315ef69122e556/test/README.md?plain=1#L436\r\n\r\nIt should be disabled by default. When enabled, it enables these tests which make network calls. Can use `t.skip()` to guard these tests, or define something like `skip_integ` similar to :  https://github.com/neovim/neovim/blob/f731766474901e5e345e0ca630315ef69122e556/test/testutil.lua#L837",
        "let's also have a test that tests text response and binary response (jpg or something)"
      ]
    },
    "profile": {
      "location": "Berlin",
      "blog": "https://sink.io",
      "twitter_username": "justinmk",
      "site_admin": false,
      "followers": 1615,
      "following": 12
    }
  },
  "bolinfest": {
    "repos": [
      "openai/codex"
    ],
    "entries": [
      {
        "slug": "codex-avoid-hard-coded-configuration",
        "title": "Avoid hard-coded configuration"
      },
      {
        "slug": "codex-avoid-unnecessary-operations",
        "title": "Avoid unnecessary operations"
      },
      {
        "slug": "codex-centralize-configuration-management",
        "title": "Centralize configuration management"
      },
      {
        "slug": "codex-contextualize-dont-swallow",
        "title": "Contextualize, don't swallow"
      },
      {
        "slug": "codex-document-non-obvious-aspects",
        "title": "Document non-obvious aspects"
      },
      {
        "slug": "codex-extract-reusable-logic",
        "title": "Extract reusable logic"
      },
      {
        "slug": "codex-leverage-style-tools",
        "title": "Leverage style tools"
      },
      {
        "slug": "codex-minimize-blocking-operations",
        "title": "Minimize blocking operations"
      },
      {
        "slug": "codex-organize-code-top-down",
        "title": "Organize code top down"
      },
      {
        "slug": "codex-pin-external-action-dependencies",
        "title": "Pin external action dependencies"
      },
      {
        "slug": "codex-prevent-command-injection",
        "title": "Prevent command injection"
      },
      {
        "slug": "codex-proper-packagejson-structure",
        "title": "Proper package.json structure"
      },
      {
        "slug": "codex-provider-agnostic-api-design",
        "title": "Provider-agnostic API design"
      },
      {
        "slug": "codex-secure-cicd-pipelines",
        "title": "Secure CI/CD pipelines"
      },
      {
        "slug": "codex-semantic-naming-patterns",
        "title": "Semantic naming patterns"
      },
      {
        "slug": "codex-structure-configurations-properly",
        "title": "Structure configurations properly"
      },
      {
        "slug": "codex-workspace-version-configuration",
        "title": "Workspace version configuration"
      },
      {
        "slug": "codex-write-comprehensive-test-assertions",
        "title": "Write comprehensive test assertions"
      }
    ],
    "comments": {
      "codex-semantic-naming-patterns": [
        "Using `{}` as a placeholder in this way seems very confusing to me as a Rust person because it's not being used natively by `format!()`. Please use something like `SUMMARY_TEXT` instead so it's more obvious that something is meant to be replaced.",
        "These should not have an `openai_` prefix, but should be generally applicable to all providers, right?",
        "Similar to a comment I made on another PR, please list all of these helper functions below the tests. The tests are the most important thing in this file.",
        "Also, I would name the variable `codex_home` rather than `dir`.",
        "`fully_qualified_tool_name()` and `try_parse_fully_qualified_tool_name()` must be symmetric. It is not clear that this is the case given this implementation.\r\n\r\nSomeone told me that, empirically, the model doesn't care about the names of the functions all that much and therefore, we could SHA1 the long name or something and things would still work.\r\n\r\nAnother solution that is somewhat stateful, but more readable for users, would be to get the full list of tool names and only attempt to \"fully qualify them\" when there is a naming collision."
      ],
      "codex-organize-code-top-down": [
        "This is admittedly a nit, but personally, I would rewrite this as a `match self.tx.send(RolloutCmd::Shutdown { ack: tx_done }).await` to avoid using the `return` keyword. I try to reserve the use of `return` when you really need an \"early return\" in a long function. But if using a single expression is an option, I find that to be cleaner because then it's more \"straight line\" code (though admittedly `match` branches...).",
        "For small helper functions, particularly ones that are private to the file, please declare them _after_ the functions that use them. I strongly prefer declaring the \"most important stuff\" at the top of the file and \"details\" (which includes functions like this) at the top of the file.",
        "Is there a reason this logic isn't added to `dispatch_codex_event()` instead? Much of the reason to have the `dispatch_codex_event()` helper is to keep the length of `run()` down. In a new top-level function, there will be less indenting and the code should be easier to read, as well.",
        "Let's do this before `match hunk` since it is used in all three cases."
      ],
      "codex-avoid-hard-coded-configuration": [
        "I was thinking of going in the other direction where we would replace `--full-auto` with `--dangerously-auto-approve-everything` since the Docker container *is* the sandbox. Is your motivation to add a weaker approval mode or a stronger one?\r\n\r\nI guess from your screenshot you want to run it with `--suggest`?",
        "@fouad-openai do you want this to print something else if `--native` was passed to indicate publishing to a different tag?",
        "I believe it is possible for `ALLOWED_DOMAINS` to be the empty string if this script were run with `ALLOWED_DOMAINS=` (explicitly defining the `ALLOWED_DOMAINS` env var as the empty string), so out of an abundance of caution, maybe we should do this before `docker run`:\r\n\r\n```\r\nif [ -z \"$ALLOWED_DOMAINS\" ]; then\r\n  echo \"Error: ALLOWED_DOMAINS is empty.\"\r\n  exit 1\r\nfi\r\n```\r\n\r\n"
      ],
      "codex-secure-cicd-pipelines": [
        "Hmm, this does not appear to have seen much interest or activity:\r\n\r\nhttps://github.com/codespell-project/codespell-problem-matcher\r\n",
        "Right, I trust that it works today. What I have less confidence in is a bad actor coming in, somehow gaining control of `codespell-problem-matcher`, changing the `v1` tag to point at something else, and anyone noticing."
      ],
      "codex-centralize-configuration-management": [
        "I have been trying to eliminate support for environment variables in favor of using configuration. Can we just added a config option (prefixed with \"experimental\" like `experimental_resume`) for this?",
        "I think this could be argued either way, but I think there is a technical reason to do this as early as possible (before we know what the `Config` even is), which is that setting environment variables for the current process is not thread-safe, so it should really be done before any threads have been created.\r\n\r\nI just reworked this so that `load_dotenv()` is now called before we set up the Tokio runtime.",
        "I think we should only be looking at the `OPENAI_` environment variables for the built-in OpenAI provider, not all providers, right?",
        "Taking a step back, I'm not sure we should be honoring `OPENAI_STREAM_IDLE_TIMEOUT_MS`, `OPENAI_REQUEST_MAX_RETRIES`, or `OPENAI_STREAM_MAX_RETRIES` at all. As best I can tell, these are not \"standard\" OpenAI environment variables, but ones that we made up for Codex?\r\n\r\nI've been trying to maintain a consistency where the `Config` is the \"one true way\" to configure things, so supporting a small handful of environment variables confuses that.",
        "```suggestion\r\n    // Configure retry behavior explicitly to avoid mutating process-wide\r\n```",
        "```suggestion\r\n    // Configure retry behavior explicitly to avoid mutating process-wide\r\n```"
      ],
      "codex-write-comprehensive-test-assertions": [
        "I would just `assert_eq!()` using a string literal that, yes, is a copy of `COMPACT_SUMMARY_TEMPLATE`. Again, I would use `r#`.",
        "Maybe use `find(|t| t.get(\"name\").as_ref() == Some(\"srv.dummy\")` on `tools.iter()` or something like that and then do an `assert_eq!()` on the value returned from `find()`?",
        "For both of these tests, can we just assert the entire string/serde_json::Value that we get back? I realize this means that we will have to update this test if we change the default tools, but I think having a test that verifies _everything_ (and effectively documents what we send on the wire) is worth that maintenance cost.",
        "This is a natural thing to do, but whenever possible, please just do one big `assert_eq!()` rather than doing a bunch of piecemeal `assert_eq!()` calls, so something like:\r\n\r\n```suggestion\r\n        assert_eq!(events, vec![\r\n            Ok(ResponseEvent::OutputItemDone(ResponseItem::Message { role: \"assistant\" }),\r\n            Ok(ResponseEvent::OutputItemDone(ResponseItem::Message { role, \"assistant\" }),\r\n            Ok(ResponseEvent::Completed { response_id: \"resp1\", token_usage: None })\r\n        ]);\r\n```\r\n\r\nI think you can also potentially do an `iter` / `collect()` on `events` to convert `Vec<Result<T>>` into `Result<Vec<T>>`.\r\n\r\n",
        "For clarity, would you mind just doing `assert_eq!` on the full struct? Admittedly, I think you need to `derive(PartialEq)` on `MaybeApplyPatchVerified` and some things to do that.\r\n\r\nWhen possible, I find that to be much stronger than doing `assert_eq!` on individual fields."
      ],
      "codex-workspace-version-configuration": [
        "outside the scope of this PR, but I guess all of our `Cargo.toml` files should have `version = { workspace = true }`?"
      ],
      "codex-document-non-obvious-aspects": [
        "Should we keep this comment?",
        "Please add a docstring explaining what is being tested.",
        "Could you add docstrings for this test and the other test? Admittedly, there is a lot of code required just to setup these tests, so it's not 100% obvious what is being tested. That is, this line seems to be the key bit that is producing the behavior that we are verifying at the end of the test:\r\n\r\n```rust\r\n.set_body_raw(sse_message(\"Hello, world.\"), \"text/event-stream\")\r\n```",
        "Maybe a docstring to explain what the return value represents?",
        "I have tried to avoid these sorts of comments. For example, in `bottom_pane.rs`, we have:\r\n\r\n```rust\r\n/// Number of terminal rows consumed by the textarea border (top + bottom).\r\nconst TEXTAREA_BORDER_LINES: u16 = 2;\r\n```"
      ],
      "codex-provider-agnostic-api-design": [
        "I think it's more appropriate for these to be:\r\n\r\n```suggestion\r\nconst DEFAULT_STREAM_IDLE_TIMEOUT_MS: u64 = 300_000;\r\nconst DEFAULT_STREAM_MAX_RETRIES: u64 = 10;\r\nconst DEFAULT_REQUEST_MAX_RETRIES: u64 = 4;\r\n```",
        "Some of this code duplicates code in `core/`. Note that the model provider might only support one of Responses or the chat completions API, so it is not safe to assume the above code is an option.\r\n\r\nThat said, I recently learned that there is a _non-stateful version of the Responses API_ where  you pass input as the array of all messages the same way you do for chat completions. So perhaps we could expose a function in `core/` that takes a `Config` and a list of messages like you have here and uses the `config.model_provider` and a `Client` to make the request?\r\n\r\nAdmittedly this is complex enough that it is probably appropriate to do it in a separate PR."
      ],
      "codex-proper-packagejson-structure": [
        "This should be in `devDependencies`."
      ],
      "codex-leverage-style-tools": [
        "I'm about to run CI on this PR, but seeing this makes me suspicious that you haven't run `pnpm run format` in this folder or maybe not `pnpm run lint`? If you use VS Code and have the ESLint and Prettier extensions configured correctly, you should get alerted to these issues in the Problems pane and generally don't have to run those scripts explicitly.",
        "This should have stayed a `switch` statement because we have ESLint rules set up to ensure `switch` statements are exhaustive so that if a new variant of the `ApprovalPolicy` enum is introduced we are forced to address the existing callsites."
      ],
      "codex-structure-configurations-properly": [
        "I think we need a different name and maybe a level of two of depth.\r\n\r\nThat is, this not configuring the general \"output\" of Codex CLI. (To me, this name implies the \"output\" that I see as a user.) It is configuring something extremely specific: truncation parameters for the `shell` tool call.\r\n\r\nThis makes me wonder if should have a top level config entry named `\"tools\"` where keys are tool names that point to dictionaries of arbitrary properties for configuring that tool, so the JSON would look like:\r\n\r\n```json\r\n\"tools\": {\r\n  \"shell\": {\r\n    \"maxBytes\": 12410,\r\n    \"maxLines\": 256\r\n  }\r\n}\r\n```\r\n",
        "I don't think we should be loading the config at arbitrary places in the code. That means the config could change over the course of the run, which in some cases could be helpful, but in other cases I think could be quite dangerous because it could unintentionally change the behavior of a long-running agent.\r\n\r\nToday, `loadConfig()` is called once in `cli.tsx` (as it should be) and then it should be threaded through from there."
      ],
      "codex-extract-reusable-logic": [
        "First, we should carve out a general notification abstraction rather than duping this logic into the middle of a UI component."
      ],
      "codex-avoid-unnecessary-operations": [
        "Since the other two cases always `flush()`, there should never be anything new to `flush()` in this case, no?",
        "I would skip the `flush()`: all this needs to do is `ack.send(())`.",
        "If we are going to ad logic to control the render late, we should be doing this at a higher level in the TUI so it applies globally, no?",
        "Right, but in general, it's possible the top-level event loop gets too many `AppEvent::Redraw` requests, so why not do the throttling there?",
        "I think you want `let prompt: Cow<'a, Prompt>` if you can to avoid the `clone()`? So in the consequent, it's `Cow::Borrowed` and in the alternative, it's `Cow::Owned`?"
      ],
      "codex-pin-external-action-dependencies": [
        "This one is a bit better:\r\n\r\nhttps://github.com/codespell-project/codespell-problem-matcher",
        "Copy/paste error: that should have been https://github.com/codespell-project/actions-codespell. It has 21 forks and 80 stars, so it feels like at least some folks are keeping an eye on it.",
        "I am opting to keep both. I am saying that instead of referencing the actions by tag (`@v1`, `@v2`), I would like to reference them by full commit hash so the behavior of the action cannot change out from under us.",
        "This one is fine: this is GitHub's responsibility and if this does something malicious, the whole Internet will break anyway. So please keep this as `@v4` so it's easier to eyeball that we're doing something canonical."
      ],
      "codex-minimize-blocking-operations": [
        "Can you use tokio::Command and make this `async` instead? It's cheaper to create tokio tasks than POSIX threads. You should then update `collect_git_info()` to make all these calls in parallel.",
        "I appreciate the timeouts in `collect_git_info()`, though if I am reading it correctly, I suppose this could add ~6s to startup in the worst case? It would be nice to figure out how to make this truly async, since `RolloutRecorder::new()` is on the critical path to startup.\r\n\r\nThe challenge seems to be that we have these lines below:\r\n\r\n```rust\r\n    recorder.record_item(&meta).await?;\r\n    Ok(recorder)\r\n```\r\n\r\nThat is, we don't want `new()` to exit until the first item has recorded and now that is dependent on `collect_git_info()`. Certainly this is fixable, but the bookkeeping may be a bit ugly. What do you think?",
        "Actually, what if we move `collect_git_info(cwd).await` into the lambda passed to `tokio::task::spawn` and then ensure it is written to `file` before the `while let Some(line) = rx.recv().await` loop starts?\r\n\r\nYou could also increase the `git` timeout to 5s maybe?",
        "FYI, if you write it as a single statement, then there is no intermediate `running_requests_id_to_codex_uuid` reference that has to be _dropped_ to release the lock, so the `Drop` happens implicitly as part of the statement executing.\r\n\r\n```suggestion\r\n    running_requests_id_to_codex_uuid.lock().await.running_requests_id_to_codex_uuid.insert(request_id.clone(), session_id);\r\n```",
        "This is slightly better because it results in holding the lock for a shorter amount of time.\r\n\r\nThat extra level of scoping around the use of `self.pending_redraw.lock()` and `flag` ensures that after `*flag = true`, the lock is dropped.\r\n\r\nAlso, `Arc::clone()` is less canonical than just invoking `.clone()`, in my experience.\r\n\r\n```suggestion\r\n        {\r\n            #[allow(clippy::unwrap_used)]\r\n            let mut flag = self.pending_redraw.lock().unwrap();\r\n            if *flag {\r\n                return;\r\n            }\r\n            *flag = true;\r\n        }\r\n\r\n        let tx = self.app_event_tx.clone();\r\n        let pending_redraw = &self.pending_redraw.clone();\r\n        thread::spawn(move || {\r\n            thread::sleep(REDRAW_DEBOUNCE);\r\n            tx.send(AppEvent::Redraw);\r\n            #[allow(clippy::unwrap_used)]\r\n            let mut f = pending.lock().unwrap();\r\n            *f = false;\r\n        });\r\n```"
      ],
      "codex-prevent-command-injection": [
        "Second, we should absolutely not be using `exec()` with a single string arg that appears to be subject to injection risks. At a minimum, we should be using `spawn()` with a list of strings."
      ],
      "codex-contextualize-dont-swallow": [
        "Why do we ignore all the io errors instead of returning `io::Result<()>`?",
        "can it just be this for a string literal?\r\n\r\n```suggestion\r\n        .context(\"failed to load bash grammar\")?;\r\n```",
        "I think this is debatable whether we should proceed in this case. Certainly the program will continue to work, so I guess it's fine? On the other hand, if you are trying to test the wire protocol and you sent bad data, perhaps we should crash so you fix it?",
        "If this is happens, this is a logical error where our treesitter dependency is not set up correctly, so we should not swallow this error.",
        "Would with_context from anyhow work here?"
      ]
    },
    "profile": {
      "company": "Meta",
      "blog": "",
      "site_admin": false,
      "followers": 393,
      "following": 9
    }
  },
  "javache": {
    "repos": [
      "facebook/react-native"
    ],
    "entries": [
      {
        "slug": "react-native-avoid-expensive-allocations",
        "title": "Avoid expensive allocations"
      },
      {
        "slug": "react-native-avoid-unnecessary-allocations",
        "title": "avoid unnecessary allocations"
      },
      {
        "slug": "react-native-catch-specific-exceptions",
        "title": "catch specific exceptions"
      },
      {
        "slug": "react-native-descriptive-specific-naming",
        "title": "descriptive specific naming"
      },
      {
        "slug": "react-native-ensure-exception-safety",
        "title": "ensure exception safety"
      },
      {
        "slug": "react-native-follow-platform-naming-conventions",
        "title": "Follow platform naming conventions"
      },
      {
        "slug": "react-native-maintain-cross-platform-api-consistency",
        "title": "maintain cross-platform API consistency"
      },
      {
        "slug": "react-native-minimize-public-api-surface",
        "title": "minimize public API surface"
      },
      {
        "slug": "react-native-optimize-algorithmic-choices",
        "title": "Optimize algorithmic choices"
      },
      {
        "slug": "react-native-optimize-algorithmic-efficiency",
        "title": "optimize algorithmic efficiency"
      },
      {
        "slug": "react-native-prefer-kotlin-idioms",
        "title": "prefer Kotlin idioms"
      },
      {
        "slug": "react-native-prefer-micro-optimizations",
        "title": "Prefer micro-optimizations"
      },
      {
        "slug": "react-native-prevent-regression-crashes",
        "title": "Prevent regression crashes"
      },
      {
        "slug": "react-native-proper-synchronization-practices",
        "title": "proper synchronization practices"
      },
      {
        "slug": "react-native-simplify-redundant-logic",
        "title": "simplify redundant logic"
      },
      {
        "slug": "react-native-validate-before-accessing",
        "title": "validate before accessing"
      }
    ],
    "comments": {
      "react-native-descriptive-specific-naming": [
        "This should be made iOS-specific `isIOSCatalogAsset`?",
        "It's confusing this is using `getAndroidResourceIdentifier` in an iOS code path. If this is right, can we rename `getAndroidResourceIdentifier` to be platform-independent?\r\n\r\n```suggestion\r\n  assetFromIOSCatalog(): ResolvedAssetSource {\r\n```",
        "Could you rename the method to `getResourceIndentifier` and a `getAndroidResourceIdentifier` that just calls the original method?"
      ],
      "react-native-follow-platform-naming-conventions": [
        "nit: obj-c naming conventions means that functions named `get` return a result via a pointer arg which is not the case here.",
        "Since it's exported now, we should namespace it.\r\n\r\n`RCTGetFontWeight`"
      ],
      "react-native-prevent-regression-crashes": [
        "Is there any reason we shouldn't just always call `onResponseReceived`? It seems like a gap in the implementation.",
        "It seems that we do not set a response code for any other request through custom `mUriHandlers`, which seems incorrect. It file resolution requires an `onResponseReceived` callback, so should all others.\r\n\r\nThe code you reference in fetch talks about file:// URIs, but the logic here is specific to content://?",
        "I don't actually understand how that code affects the load of content:// URI's. As far as I can tell, the only thing it does is provide a default status for some loads. \r\n\r\nDo we rely on the status later in the response handling for blobs?",
        "Thanks for those pointers! I still think we should call `ResponseUtil.onResponseReceived` for any response provided through `mUriHandlers`."
      ],
      "react-native-prefer-micro-optimizations": [
        "std::move, since you've made a copy. Or better, keep it as a reference\r\n```suggestion\r\n      auto& dynamic = std::get<folly::dynamic>(other.value_);\r\n      value_ = dynamic;\r\n```",
        "Grab a reference, not a copy\r\n\r\n```suggestion\r\n      auto& dynamic = std::get<folly::dynamic>(value_);\r\n```"
      ],
      "react-native-avoid-unnecessary-allocations": [
        "Avoid copying the list\r\n```suggestion\r\n  const auto& children = shadowNode.getChildren();\r\n```",
        "You could also model this as a std::optional<ListOfShared> and only do the copy when you first need to. That also avoids the need for `shouldUpdateChildren`."
      ],
      "react-native-optimize-algorithmic-efficiency": [
        "Consider using androidx.collection.MutableIntSet instead to avoid Integer boxing. ",
        "Looks like this would need to be a new dependency in https://github.com/facebook/react-native/blob/main/packages/react-native/gradle/libs.versions.toml#L45 - let's add a comment to change this in the future."
      ],
      "react-native-maintain-cross-platform-api-consistency": [
        "To keep things open for other platforms we should do\r\n\r\n```suggestion\r\nreturn Platform.select({\r\n android: this.isLoadedFromFileSystem()\r\n        ? this.drawableFolderInBundle()\r\n        : this.resourceIdentifierWithoutScale(),\r\n ios: this.isCatalogAsset()\r\n        ? this.assetFromCatalog()\r\n        : this.scaledAssetURLNearBundle(),\r\n  default: this.scaledAssetURLNearBundle()\r\n  });\r\n```"
      ],
      "react-native-proper-synchronization-practices": [
        "This should happen inside a synchronized, otherwise, it can still race with `prependUIBlock`",
        "If you wrap the entire method in @synchronized no copy is necessary as concurrent modification will be impossible. \r\n\r\nDoes the `synchronized(this)` I suggested not work?"
      ],
      "react-native-validate-before-accessing": [
        "Should this be looking at `convertedObjCArg` instead?",
        "@zhongwuzw: that doesn't seem right. RCTInteropTurboModule looks at `arg`, which is the return value of the `RCTConvertTo` call (we should probably use that RCTConvertTo helper here too!) - so the equivalent would be to use `convertedObjCArg` here."
      ],
      "react-native-catch-specific-exceptions": [
        "Remove the try, since there's no catch/finally"
      ],
      "react-native-prefer-kotlin-idioms": [
        "Make this `public fun interface` so an inline lambda can be used."
      ],
      "react-native-avoid-expensive-allocations": [
        "Let's allocate this lazily? Most apps won't have transitioning views and shouldn't pay for this."
      ],
      "react-native-simplify-redundant-logic": [
        "Could we inline the onComplete callback here?\r\n\r\n```\r\nif (onComplete) {\r\n  [self callFunctionOnBufferedRuntimeExecutor:[completion](facebook::jsi::Runtime &_) { completion(); }];\r\n}\r\n```"
      ],
      "react-native-minimize-public-api-surface": [
        "This seems ok to me. As long as C++ doesn't do anything silly like promoting all callers to value copies instead of refs."
      ],
      "react-native-optimize-algorithmic-choices": [
        "Instead of a regular expression, could you use a NSCharacterSet for this?"
      ],
      "react-native-ensure-exception-safety": [
        "I think you want some logic here that catches if it's safe, but re-throws otherwise.\r\n\r\n```\r\n    NSDictionary *caughtException = nil;\r\n    BOOL shouldCatchException = isSync || optionalInternalRejectBlock;\r\n    try {\r\n      @try {\r\n        [inv invokeWithTarget:strongModule];\r\n       } @catch (NSException *exception) {\r\n         caughtException = maybeCatchException(shouldCatchException, @{\r\n                       @\"name\": exception.name,\r\n                       @\"message\": exception.reason,\r\n                       @\"stackSymbols\": exception.callStackSymbols,\r\n                       @\"stackReturnAddresses\": exception.callStackReturnAddresses,\r\n                     });\r\n       } ...\r\n    } catch { ... }\r\n\r\n    if (caughtException) {\r\n      if (isSync) {\r\n        throw convertNSDictionaryToJSError(runtime, caughtException);\r\n      } else {\r\n        optionalInternalRejectBlock(caughtException);\r\n      }\r\n    }\r\n```",
        "This still needs to go in a @finally, otherwise, we'll leak `retainedObjectsForInvocation` when exceptions are thrown.",
        "C++ exceptions shouldn't be heap allocated\r\n```suggestion\r\n    throw std::runtime_error(\r\n```"
      ]
    },
    "profile": {
      "location": "London, UK",
      "company": "Facebook",
      "blog": "http://javache.be",
      "site_admin": false,
      "followers": 599,
      "following": 46
    }
  },
  "MMeent": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-adaptive-cache-expiration-strategy",
        "title": "Adaptive cache expiration strategy"
      },
      {
        "slug": "neon-cache-performance-preservation",
        "title": "Cache performance preservation"
      },
      {
        "slug": "neon-clear-consistent-identifier-names",
        "title": "Clear consistent identifier names"
      },
      {
        "slug": "neon-configuration-context-alignment",
        "title": "Configuration context alignment"
      },
      {
        "slug": "neon-database-replica-promotion-safeguards",
        "title": "Database replica promotion safeguards"
      },
      {
        "slug": "neon-document-api-specs-completely",
        "title": "Document API specs completely"
      },
      {
        "slug": "neon-document-connection-transitions",
        "title": "Document connection transitions"
      },
      {
        "slug": "neon-escape-sql-parameters",
        "title": "Escape SQL parameters"
      },
      {
        "slug": "neon-flexible-documented-configurations",
        "title": "Flexible documented configurations"
      },
      {
        "slug": "neon-guard-against-race-conditions",
        "title": "Guard against race conditions"
      },
      {
        "slug": "neon-handle-all-error-paths",
        "title": "Handle all error paths"
      },
      {
        "slug": "neon-handle-network-interrupts-safely",
        "title": "Handle network interrupts safely"
      },
      {
        "slug": "neon-limit-concurrent-access-slots",
        "title": "Limit concurrent access slots"
      },
      {
        "slug": "neon-mind-transaction-boundaries",
        "title": "Mind transaction boundaries"
      },
      {
        "slug": "neon-minimize-unnecessary-allocations",
        "title": "Minimize unnecessary allocations"
      },
      {
        "slug": "neon-optimize-data-structures",
        "title": "Optimize data structures"
      },
      {
        "slug": "neon-prefer-opt-in-security",
        "title": "Prefer opt-in security"
      },
      {
        "slug": "neon-proactive-cache-warming",
        "title": "Proactive cache warming"
      },
      {
        "slug": "neon-proper-metrics-design",
        "title": "Proper metrics design"
      },
      {
        "slug": "neon-proper-option-type-usage",
        "title": "Proper Option type usage"
      },
      {
        "slug": "neon-scope-jwt-authentication-tokens",
        "title": "Scope JWT authentication tokens"
      }
    ],
    "comments": {
      "neon-proactive-cache-warming": [
        "I really don't like this. Let's expose plain numbers, not rounded and divided values. It'll also allow (when used as metric) to compare absolute prewarm speed between projects and endpoints even when their sizes are very different.",
        "`pages_total` + `pages_processed`, probably?\r\nIIRC, we currently also have internal counters for \"ignored\" and \"dropped\" somewhere in the metrics, to cover those pages that we don't have to load because concurrent workloads already fetched the pages or that we can't load because there are no more LFC entries available, respectively.",
        "I don't like this. S3 writes are expensive to scale, and if we're about to write every N seconds it's going to be expensive to host read replicas.\r\n\r\nI'd _expected_ CPlane-triggered writes; either during shutdown (for warming up after start) or before shutdown (for hot restart); not systematic writes.",
        "I don't think checkpoint is a good place for this; it's much too frequent on highly loaded systems.",
        "We can make it part of endpoint shutdown procedures?\r\n\r\nNote that \"i.e. it can periodically fetch fresh content from S3 and do prewarming\" won't work as you seem to expect it to, as prewarming is explicitly designed to never evict existing pages from LFC, and thus won't do much if the size of LFC doesn't change for the hot standby. It's prewarming by replacing unused pages with potentially useful pages, and explicitly not LFC state synchronization.",
        "As for hot standby, it's probably good enough to \"just\" disable the replay filter and require replay of all WAL (rather than only those records which we already have in our caches)",
        "A calculation (S3 us-east-1) based on a setting of 60s:\r\n\r\n> 1 req/min * 60 min/hr * 730 hr/mon * $0.005 /1000req (S3 POST) = $0.219 per concurrently active compute per month, or $219 per 1000 concurrently active computes per month (annualized).\r\n\r\nI'd rather keep this frequency much lower than that even, as this seems like a significant potential cost for free tier operations: It'd be 7% of the monthly cost of a tiny general-purpose compute instance (t4g.micro) at AWS.\r\n\r\nI'm also a bit concerned about dumping LFC state every so often. Dumping that state is not free, and while it isn't all _that_ expensive it does block all IO operations to the LFC for a short while. Doing the dump regularly will likely cause some (small?) amount of degraded performance.\r\n\r\nIf instead of frequently dumping this state, we should dump the state only when needed (i.e. with pending shutdown or pending restart), so that we'll only degrade performance if and when needed."
      ],
      "neon-optimize-data-structures": [
        "Suggestion: Why don't we store subxact depth in DdlHashTable, and maintain a SubtransLevel counter that we can check the DdlHashTable against?\r\n\r\nWe know we won't have to create a new DdlHashTable for level=1 when CurrentLevel=2, unless level 2's subxact is rolled back to level 1.\r\n\r\nSo the stack (of sorts) would look like `SubXactLevel=130; CurrentDHT = DHT (level=100) -> DHT (level=99) -> DHT (level=40) -> DHT (level=21) -> DHT (RootTable)` instead of `SubXactLevel=30; DHT 100 -> DHT 99 -> .. -> DHT 1 -> DHT RootTable`.\r\n\r\nWDYT?\r\n\r\nAs you can see, that will reduce the memory usage required for 1000 subxacts followed by one with DDL from 1000 DdlHashTables to just 1.",
        "Does this not have off-by-one issues? I'd prefer keeping a single counter, rather than one reset every time you push a table: you can't have more than 2^31 subxacts in one transaction, as you'd run out of XIDs.\r\n\r\nAnd in that case, you can change the table `if (CurrentDdlTable->subtrans_level > --SubtransDdlLevel)`"
      ],
      "neon-limit-concurrent-access-slots": [
        "True, but I wouldn't expect them to be working on the LFC _concurrently_. Similar to the shared buffers partition locks and the WAL writers locks, I think we only need a limited amount of concurrent slots to satisfy the workload (e.g. 128), as we only really need the lock once we're ready to start loading data.",
        "Yes, see https://github.com/neondatabase/neon/pull/10312#discussion_r1922225273 - we can limit this to a limited number of concurrent backends.\r\n\r\nNote that we'll only need a prewarm state once a backend has the prefetched data in memory (no need to hold an LFC entry for a not-yet-received prefetch entry; actually, holding that is dangerous for other backends that want to prefetch a set of pages from that chunk), and the turnaround time for prewarming is expected to be very small: it's quite unlikely you're waiting for a long time on concurrent backends to finish their work, as reasonably there are at most BLOCKS_PER_CHUNK backends that are doing IO on the LFC chunk, and each such IO probably won't take very long."
      ],
      "neon-mind-transaction-boundaries": [
        "Could you please do better connection lifetime management here? It's quite bad to see the connections get leaked.",
        "Both `CHECKPOINT` and `COMMIT` should be sufficient to flush current WAL.\r\n\r\nAdditionally, the `last_flush_lsn` is quite out-of-date by the time `pg_switch_wal()` has succeeded. Better use `SELECT pg_current_wal_insert_lsn()` and then run `CHECKPOINT`."
      ],
      "neon-escape-sql-parameters": [
        "Please make sure you correctly escape the database and user parameters of this connection string."
      ],
      "neon-database-replica-promotion-safeguards": [
        "Why does it matter whether this is a promoted replica or not? I see you use it in `walproposer.c`, but that seems like a workaround and bypass of a valid check, not a correct solution to an issue."
      ],
      "neon-minimize-unnecessary-allocations": [
        "True, but that'd add a `format!()` overhead, and I'd rather prevent that. |\r\n\r\nThis allows 'static Strings, also further reducing alloc overhead."
      ],
      "neon-document-connection-transitions": [
        "This is missing the important element where the Primary that has to be shut down first, but the (old) primary does not show up in this diagram.",
        "Oh, it didn't have the same actor labeling as those of the secondary, so I'd failed to notice this.\r\n\r\nEither way, that needs additional details, as there are some compute_ctl-postgres interactions which we need to detail on the primary as well for this to work correctly and consistently.",
        "Not \"missing\" per se, but I do find it confusing that the RFC does go into detail for one side (the promoting replica) but not the other (the primary), while both sides are critial for correct functioning of the system. I'd expected both sides to have the same amount of detail."
      ],
      "neon-configuration-context-alignment": [
        "```suggestion\r\n\t\t\t\t\t\t!AmPrewarmWorker && /* do not configure the timeout-based pumping system in prewarm workers */\r\n```"
      ],
      "neon-adaptive-cache-expiration-strategy": [
        "> or evicted from LFC after receiving the corresponding WAL record (in case of replica).\r\n\r\nThis is incorrect: WAL will be replayed for every page that's currently in the LFC or shared buffers.\r\n\r\n> Wondering if there's any interactions between the user workload and prewarm that are worth considering.\r\n> if it's available then the LFC cache might have become stale over time.\r\n\r\nThe LFC can become stale, but only if the main page is still in shared buffers. The (modified) page from buffers will at some point be written, which will make the LFC lose its stale-ness.\r\n\r\nIn any case, the stale-ness of a page in LFC doesn't (shouldn't) matter for this RFC.",
        "I'm a bit concerned about issues that would arise from deletion on a weekly schedule while using the endpoint exclusively for monthly tasks - you'd want that endpoint to have good performance."
      ],
      "neon-prefer-opt-in-security": [
        "Why is anon suddenly a default preloaded library?",
        "OK, but why does it need to be a default-on setting?\r\n\r\nI, as a user, don't want `anon` anywhere close to my data, due to its inherent nature to return the wrong data (i.e. anything but the actual stored data)."
      ],
      "neon-cache-performance-preservation": [
        "Make primary report its flushLSN to CPlane on shutdown, and have CPlane pass that on with the `/promote` call so that compute_ctl can ensure that it has received that WAL before promotion.",
        "> If I'm reading this right, there's a an availability gap between the termination and promotion\r\n\r\nThe secondary *could* be made available for read-only queries before the primary shuts down. However, we can *not* allow write queries to the secondary before the primary has shut down, so there will always be a gap where there is no writer available.\r\n\r\nThe system should be fast enough, however, that this gap is no longer than a second at most; and probably much less.",
        "> It seems like we can swap promotion and termination around while maintaining this property.\r\n\r\nNo, Vlad, we can not do that.\r\n\r\nTwo computes can Never, NEVER, _never_ both be Primary on the same timeline at the same time.  CPlane is supposed to make sure of that, and Compute will fail if it doesn't. Promotion of the replica before the original Primary shut down will cause errors, panics, data loss, shutdowns, and/or nasal deamons on the Primary, this promoting Secondary, or both.\r\n\r\nWe don't want to test that theory.\r\n\r\n\r\n> What would happen to the client in this case? Let's say we have an in-progress query when the old primary terminates.\r\n\r\nTheir session would and should get disconnected. How that client handles disconnections is not something we care about here.",
        "> More specifically, by idle I mean it would have to not write any WAL and be in some sort of consensus observer role.\r\n\r\nThat's what a hot standby is for PostgreSQL - the hot standby is not allowed to write WAL, but could stop reading and applying WAL from the primary and start writing its own WAL at a moment's notice.\r\n\r\nThe issue is that you can't promote until you've replayed all acknowledged WAL from safekeepers, because otherwise you'd fail to replay commits that the primary may have already sent acknowledgements for to its clients, essentially losing commits."
      ],
      "neon-proper-metrics-design": [
        "If a table is dropped, its count is presumably removed from the total, too."
      ],
      "neon-clear-consistent-identifier-names": [
        "I don't like using plain numbers for something that is more than just that plain number, unless context makes it abundantly clear. As Display doesn't guarantee context, the value of each type was clarified by adding the full enum tag to Display output.\r\n\r\nInstead of neutering the Display implementation, I think it's better to fix the clap usage so that it doesn't rely on `Display` for serializing strings - we implement Serialize and Deserialize for those purposes.",
        "Please rename this to `WalSegmentSize`. \r\n\r\nPostgreSQL stores data files in Segments of 1GB increments, which makes this value confusing."
      ],
      "neon-proper-option-type-usage": [
        "Shouldn't this be Option<Lsn>, given that it's only set during shutdown?"
      ],
      "neon-flexible-documented-configurations": [
        "Did you consider providing a single function for this, like the following?\r\n\r\n```suggestion\r\n+CREATE OR REPLACE FUNCTION anon.enable_transparent_masking_superuser(\r\n+  dbname TEXT,\r\n+  toggle BOOL DEFAULT = true,\r\n+)\r\n+RETURNS VOID AS\r\n+$$\r\n+BEGIN\r\n+  EXECUTE format('ALTER DATABASE %I SET anon.transparent_dynamic_masking TO %L', dbname, toggle::text);\r\n+END;\r\n+$$\r\n+  LANGUAGE plpgsql\r\n+  VOLATILE\r\n+  SECURITY DEFINER\r\n+  SET search_path=''\r\n+;\r\n```"
      ],
      "neon-handle-all-error-paths": [
        "Please add an Assert(slot->response->tag == T_NeonGetPageResponse) below the `continue` code block, so that any misplaced response in slot->response is caught, at least in debug builds.",
        "Yes, but I'd rather have that check in place here as well, in case anything changes in other places that touch slot->response.",
        "So, is this the fix, or is there another component that's been fixed with this PR?",
        "Hmm, I see. How come we're not handling termination -related errors in PG_CATCH? Because we can't recover from them?"
      ],
      "neon-scope-jwt-authentication-tokens": [
        "The RFC for this storage service https://github.com/neondatabase/neon/pull/9661 implies that compute's tokens won't contain information about which endpoint it is, so the \"S3 proxy\" (which isn't just that, and thus probably shouldn't be called that) **can't** validate that the request came from a compute with the right endpoint_id.",
        "I'm not quite sure yet about the S3 design. \r\n\r\nYes, it'll have to be tenant-prefixed, and probably Endpoint-prefixed too, but I'm not yet 100% sure if it'll also be timeline-prefixed.",
        "> I think that from the security standpoint, the tenant should be enough as the tenant is our level of multi-tenancy,\r\n\r\nI don't think that's good enough. Compute's tokens should be bound to (Tenant, Timeline, Lsn >/=), so it can't ask for data created on completely disjoint timelines in the same tenant (e.g. `a` branches into `b` and `c`; compute on `b` shouldn't be able to query data in `c`).\r\n\r\n> and we use it for storage auth already\r\n\r\nIMV that's a bad argument. Having a bad practice doesn't mean we should adapt it in new projects if we can prevent it."
      ],
      "neon-document-api-specs-completely": [
        "CPlane would be hitting this service.\r\n\r\n(Note that we're not necessarily using S3; any blob storage will do)"
      ],
      "neon-guard-against-race-conditions": [
        "Shouldn't `IsUnderPostmaster` be a better approach than these rather arbitrary conditions?\r\n\r\nI.e. \r\n```suggestion\r\n\tif (newval && *newval != '\\0' && IsUnderPostmaster && RecoveryInProgress())\r\n```",
        "Yeah, it's weird that we're checking for the availability of shmem when we know that shmem should be available exactly when we run code with `IsUnderPostmaster`. AFAIK there is no valid reason why we should be unable to access shared memory in `IsUnderPostmaster` processes.",
        "Fixed.",
        "Fixed."
      ],
      "neon-handle-network-interrupts-safely": [
        "Note that moving this call to _before_ `page_server->send() || page_server->flush()` will cause requests sent by `page_server_request` to not be pipelined with getpage requests already in the connection, and so this may have 2 RTT latency, instead of 1 RTT: 1 RTT to finish all open GetPage requests, and 1 RTT for the actual NeonRequest.\r\n\r\nIt doesn't look like the behaviour here is very different, so why not put it immediately before the `page_server->receive()` call?",
        "OK, I think I got it. \r\nIf a connection is dropped during consume_prefetch_responses, then that signal is not carried on to the synchronous request path that requested `consume_prefetch_responses`, so putting consume_prefetch_responses between `page_server->sync()` and `page_server->receive()` could cause the connection to get stuck waiting on a newly created empty connection.\r\n\r\nIf we update `consume_prefetch_responses` to return the output of wait_for(), then we can use that to determine connection state, and continue handling everything correctly.\r\n\r\n```\r\nstatic bool\r\nconsume_prefetch_responses(void)\r\n{\r\n\tif (MyPState->ring_receive < MyPState->ring_unused)\r\n\t\treturn prefetch_wait_for(MyPState->ring_unused - 1);\r\n\treturn true;\r\n}\r\n```\r\n\r\n\r\nand then, in sync request paths, you'd do the following, inside a PG_TRY block to make sure to drop connections when the request gets cancelled:\r\n\r\n```\r\n\t\twhile (!page_server->send(shard_no, &request.hdr)\r\n\t\t\t\t|| !page_server->flush(shard_no)\r\n\t\t\t\t|| !consume_prefetch_responses())\r\n\t\t{\r\n\t\t\t/*\r\n\t\t\t * Loop until we've successfully\r\n\t\t\t *  1.) Written the request into the shard's connection,\r\n\t\t\t *  2.) Flushed that request to the network, and\r\n\t\t\t *  3.) Consumed all open prefetch requests still on the line.\r\n\t\t\t */\r\n\t\t};\r\n```\r\n\r\n(the sync request path in page_server_request and communicator_read_slru_segment)",
        "Hmm, ok."
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 4,
      "following": 3
    }
  },
  "crenshaw-dev": {
    "repos": [
      "argoproj/argo-cd"
    ],
    "entries": [
      {
        "slug": "argo-cd-choose-appropriate-synchronization-primitives",
        "title": "Choose appropriate synchronization primitives"
      },
      {
        "slug": "argo-cd-comprehensive-function-documentation",
        "title": "Comprehensive function documentation"
      },
      {
        "slug": "argo-cd-configuration-ui-consistency",
        "title": "Configuration UI consistency"
      },
      {
        "slug": "argo-cd-design-extensible-apis",
        "title": "design extensible APIs"
      },
      {
        "slug": "argo-cd-follow-go-naming-conventions",
        "title": "Follow Go naming conventions"
      },
      {
        "slug": "argo-cd-optimize-algorithmic-complexity",
        "title": "optimize algorithmic complexity"
      },
      {
        "slug": "argo-cd-prefer-early-returns",
        "title": "Prefer early returns"
      },
      {
        "slug": "argo-cd-prefer-modern-react-patterns",
        "title": "prefer modern React patterns"
      },
      {
        "slug": "argo-cd-prevent-silent-failures",
        "title": "Prevent silent failures"
      },
      {
        "slug": "argo-cd-provide-comprehensive-explanations",
        "title": "Provide comprehensive explanations"
      },
      {
        "slug": "argo-cd-simplify-code-readability",
        "title": "Simplify code readability"
      },
      {
        "slug": "argo-cd-structured-logging-practices",
        "title": "structured logging practices"
      },
      {
        "slug": "argo-cd-use-descriptive-constants",
        "title": "Use descriptive constants"
      },
      {
        "slug": "argo-cd-validate-conceptual-api-types",
        "title": "validate conceptual API types"
      },
      {
        "slug": "argo-cd-validate-configuration-appropriateness",
        "title": "Validate configuration appropriateness"
      },
      {
        "slug": "argo-cd-validate-external-urls",
        "title": "validate external URLs"
      },
      {
        "slug": "argo-cd-validate-untrusted-inputs",
        "title": "Validate untrusted inputs"
      },
      {
        "slug": "argo-cd-wrap-errors-with-context",
        "title": "Wrap errors with context"
      }
    ],
    "comments": {
      "argo-cd-follow-go-naming-conventions": [
        "I think convention is to add a `_seconds` suffix to timestamp gauges."
      ],
      "argo-cd-design-extensible-apis": [
        "Would it be possible to unmarshal the patch into a map[string]any so log tools aren't forced to parse a JSON string?"
      ],
      "argo-cd-prevent-silent-failures": [
        "Should we disable the button if there's an ongoing refresh? Otherwise looks like the click gets silently ignored."
      ],
      "argo-cd-use-descriptive-constants": [
        "Why 14?",
        "```suggestion\r\n                // Show \"sha256: \" plus the first 7 actual characters of the digest.\r\n                message += ' (' + revision.substring(0, 14) + ')';\r\n```"
      ],
      "argo-cd-validate-untrusted-inputs": [
        "Do we need to be concerned about someone constructing a link that tries to access stuff they shouldn't be accessing? Do we need some validation on repo, chartName, and version beyond what already exists?",
        "I guess I'm thinking more of a malicious scenario, where a user passes `repoName=../../../etc/passwd` or similar. It's not an easy attack path, but it seems like it would be super easy to validate against.",
        "Ditto re: securejoin. If `relPath` refers to a symlink, this join could end up referring to something above `s.dest`.",
        "We should probably require that ConfigMaps outside the argocd namespace have some label advertising them as available for use as a plugin. Otherwise users might use the AppSet as a way to try to leak information from arbitrary ConfigMaps which they may or may not have access to."
      ],
      "argo-cd-prefer-modern-react-patterns": [
        "Should we have a linter for this? I see we use `<Consumer>` a lot."
      ],
      "argo-cd-validate-external-urls": [
        "Need to be super careful about injection here... do we need to do any sanitization on imageUrl?",
        "I'd suggest dropping the image URL from the UI for now and add it as a follow-up enhancement. I think we might want to enforce domain allowlists or something like that."
      ],
      "argo-cd-provide-comprehensive-explanations": [
        "I think \"rather it is the values/valuesObject merged with parameters\" is still valuable to include. Maybe just rephrased to be easier to understand."
      ],
      "argo-cd-optimize-algorithmic-complexity": [
        "While we're in here, I think we should fix this logic. Instead of constructing a single string and then comparing, we should compare the highest priority field, return if not equal, proceed to the next field if equal, etc.",
        "Copilot came up with this:\r\n\r\n```\r\nsort.Slice(newConditions, func(i, j int) bool {\r\n\tleft := newConditions[i]\r\n\tright := newConditions[j]\r\n\r\n\tif left.Type != right.Type {\r\n\t\treturn left.Type < right.Type\r\n\t}\r\n\tif left.Message != right.Message {\r\n\t\treturn left.Message < right.Message\r\n\t}\r\n\tif left.Status != right.Status {\r\n\t\treturn left.Status < right.Status\r\n\t}\r\n\tif left.Reason != right.Reason {\r\n\t\treturn left.Reason < right.Reason\r\n\t}\r\n\treturn left.LastTransitionTime.Before(right.LastTransitionTime)\r\n})\r\n```",
        "```suggestion\r\n\t\tif left.Status != right.Status {\r\n\t\t\treturn left.Status < right.Status\r\n\t\t}\r\n\t\tif left.Reason != right.Reason {\r\n\t\t\treturn left.Reason < right.Reason\r\n\t\t}\r\n\t\tif left.Message != right.Message {\r\n\t\t\treturn left.Message < right.Message\r\n\t\t}\r\n```\r\n\r\nThis should give us somewhat more stable order, since the messages are arbitrary."
      ],
      "argo-cd-comprehensive-function-documentation": [
        "I feel like this function is doing too much... the revisions and phase output parameters seem to be simple aliases of their respective fields in `app.Status.OperationState`. If we need short var names, we can just do\r\n\r\n```go\r\nopPhase := app.Status.OperationState.Phase\r\nopRevisions := app.Status.OperationState.SyncResult.GetRevisions() # would probably need a new receiver\r\n```\r\n\r\nThen this function could focus on the first return param, which seems to involve the more interesting logic.",
        "Maybe but I'm really struggling to understand what this function does due to it being so crowded.\r\n\r\nFor now I'd settle for a docstring thoroughly explaining the intent and a TODO to remove the unnecessary behavior.",
        "Not really, because I don't think I fully understand either the behavior or the intent of the function (regarding just the first output param).\r\n\r\n> alreadyAttemptedSync returns whether the most recent sync was performed against the desiredRevisions and with the same app source config which are currently set in the app.\r\n\r\nI think I basically follow that, but the actual behavior is:\r\n\r\n1) If there is no operation state, return false, because we can't confirm the above two things.\r\n2) If we have an operation state but not a sync result, we return true if and only if the phase was marked completed. This is weird, because we haven't confirmed whether the synced revisions match `desiredRevisions` or whether the source config has changed. \"Phase is completed\" wasn't even mentioned as a criteria in the docstring. Why return true if the phase is completed? There's a docstring above that `return`, but it seems to be explaining the behavior of the calling function, not the `alreadyAttemptedSync` function.\r\n3) If `newRevisionHasChanges`, return `false` if `desiredRevisions` doesn't match the synced revisions. What does `newRevisionHasChanges` mean? And why aren't we checking whether the synced source config has changed, like the function docs mentioned?\r\n4) If not `newRevisionHasChanges`, just return whether the synced source config matches current source config. But it's unclear why we're not comparing desired revisions to synced revisions. Is that already confirmed to be true because `newRevisionHasChanges` is false? If `newRevisionHasChanges` means \"desired revisions don't match synced revisions\", why are we checking that again in the previous point?",
        "> alreadyAttemptedSync is meant to help the caller understand whether an identical sync operation has been attempted, to avoid excessively retrying the exact same sync operation.\r\n>\r\n> alreadyAttemptedSync returns true if either 1) newRevisionHasChanges is true and the most recently synced revision(s) exactly match the given desiredRevisions, 2) newRevisionHasChanges is false and the most recently synced app source configuration matches exactly the current app source configuration, or 3) the most recent operation state is missing a sync result but the sync phase is completed (this can happen when there are errors that cause the sync result not to be persisted). The last case returns true, because the caller should treat such a failed sync as an attempt. \r\n>\r\n> alreadyAttemptedSync returns false if the operation state is not set at all. This happens when the app is brand new or when a new operation has started.\r\n>\r\n> alreadyAttemptedSync also returns the most recently synced revisions and the most recent sync operation's phase.\r\n>\r\n> TODO: remove the last two return parameters, since they're effectively just aliases for fields on the app object. If the nil checks are too cumbersome for the caller, they can be moved into separate utility functions to avoid crowding this one.\r\n\r\nThis would get closer. But I think the references to `newRevisionHasChanges` should be replaced with a plain-language explanation of what that parameters means, and that param should be documented.",
        "The description of `newRevisionHasChanges`. cleared up a lot for me.\r\n\r\nWould probably be good to standardize on one var name for that variable and document it in each function that uses it. But that can be a future enhancement."
      ],
      "argo-cd-prefer-early-returns": [
        "I'd short-circuit instead of nesting\r\n\r\n```go\r\nif kubeutil.IsCRD(live) {\r\n// CRDs don't get tracking annotations.\r\nreturn nil\r\n}\r\n```",
        "We can avoid the deep nesting by just short-circuiting here.\r\n\r\n```suggestion\r\n\tif !needToUpdateConditions {\r\n\t\treturn nil\r\n\t}\r\n```"
      ],
      "argo-cd-simplify-code-readability": [
        "And one more... might be worth a lint rule? :-) "
      ],
      "argo-cd-validate-configuration-appropriateness": [
        "I really don't think we should do this. imo using a fork of a lightly-maintained library is worse than just using the lightly-maintained library."
      ],
      "argo-cd-wrap-errors-with-context": [
        "```suggestion\r\n\t\treturn nil, fmt.Errorf(\"failed to initialize oci client: %w\", err)\r\n```",
        "```suggestion\r\n\t\treturn nil, \"\", fmt.Errorf(\"failed to initialize oci client: %w\", err)\r\n```",
        "Let's wrap this error for easier debugging."
      ],
      "argo-cd-structured-logging-practices": [
        "A couple suggestions:\r\n\r\n1) I'd unmarshal the patch into a map[any]any and log it as its own field, so it's parseable by log tools\r\n2) I'd exclude the patch from the event: some people put _huge_ stuff in their app spec",
        "```suggestion\r\n\t\t\t\tlogFields := log.Fields{}\r\n```\r\n\r\nProbably fine to start with an empty set and replace it if the type assertion is successful.",
        "```suggestion\r\n\t\t\tlogCtx.WithFields(applog.GetAppLogFields(&generatedApplications[i])).Errorf(\"validation error found during application validation: %s\", message)\r\n```\r\n\r\nSomething like that would get you the standard app log fields.",
        "Makes sense. At any rate, I'd go with \"application\" instead of \"app\" to match the standard: https://argo-cd.readthedocs.io/en/latest/operator-manual/security/#standard-application-log-fields",
        "added revision and repo url"
      ],
      "argo-cd-validate-conceptual-api-types": [
        "Is this necessary? As far as I can tell, OCI isn't a source \"type\" in the same way that Helm or Kustomize are. An OCI repo could contain Helm, Kustomize, Directory, or Plugin-style manifetss."
      ],
      "argo-cd-choose-appropriate-synchronization-primitives": [
        "I haven't used `atomic` much... could we just use `atomic.Int64`? Docs seem to recommend that: https://pkg.go.dev/sync/atomic#AddInt64"
      ],
      "argo-cd-configuration-ui-consistency": [
        "Could we have some unit tests for these utility functions? I think we're trying to mimic Helm's values file merging behavior. If we find later that our merge algorithms differ from Helm's, it would be good to have unit tests to help safely change our algorithm to match Helm's."
      ]
    },
    "profile": {
      "company": "Intuit",
      "blog": "",
      "site_admin": false,
      "followers": 191,
      "following": 29
    }
  },
  "tristan957": {
    "repos": [
      "ghostty-org/ghostty",
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "ghostty-centralize-configuration-values",
        "title": "Centralize configuration values"
      },
      {
        "slug": "ghostty-descriptive-consistent-naming",
        "title": "Descriptive consistent naming"
      },
      {
        "slug": "ghostty-document-configs-comprehensively",
        "title": "Document configs comprehensively"
      },
      {
        "slug": "ghostty-generate-dynamic-configurations",
        "title": "Generate dynamic configurations"
      },
      {
        "slug": "ghostty-in-tree-build-configurations",
        "title": "In-tree build configurations"
      },
      {
        "slug": "neon-avoid-flaky-tests",
        "title": "Avoid flaky tests"
      },
      {
        "slug": "neon-clear-consistent-identifier-names",
        "title": "Clear consistent identifier names"
      },
      {
        "slug": "neon-configuration-context-alignment",
        "title": "Configuration context alignment"
      },
      {
        "slug": "neon-document-parameter-choices",
        "title": "Document parameter choices"
      },
      {
        "slug": "neon-flexible-documented-configurations",
        "title": "Flexible documented configurations"
      },
      {
        "slug": "neon-hierarchical-semantic-naming",
        "title": "Hierarchical semantic naming"
      },
      {
        "slug": "neon-keep-files-focused-small",
        "title": "Keep files focused small"
      },
      {
        "slug": "neon-minimize-unnecessary-allocations",
        "title": "Minimize unnecessary allocations"
      },
      {
        "slug": "neon-proper-option-type-usage",
        "title": "Proper Option type usage"
      },
      {
        "slug": "neon-structure-endpoints-for-rest",
        "title": "Structure endpoints for REST"
      },
      {
        "slug": "neon-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      }
    ],
    "comments": {
      "neon-keep-files-focused-small": [
        "Would love to see the path pulled into a constant in a `pgbouncer` module (`compute_tools/src/pgbouncer.rs`).",
        "```suggestion\r\npub const PGBOUNCER_PIDFILE: &str = \"/tmp/pgbouncer.pid\";\r\n```\r\n\r\nAlso, just move this to a pgbouncer.rs file. It doesn't make sense to put this constant in pg_helpers.rs.",
        "Please follow pre-established pattern of the use statement at the top and avoid wildcard imports.",
        "As stated in a previous comment, please move all actual logic into the `ComputeNode` struct and just have the handlers call those functions.",
        "It seems like we have a good pattern going where the `ComputeNode` methods become small wrappers around functions in other files. I agree with this suggestion.",
        "Oh, I can't remember if it is, but that is also a good idea!"
      ],
      "ghostty-in-tree-build-configurations": [
        "> We can expand and have a .Devel variant for debugging then, similar to many GNOME apps.\r\n\r\nI think this is a good idea.",
        "Let's restrict this to certain branches, at least main for now"
      ],
      "neon-avoid-flaky-tests": [
        "I know this wasn't added in this PR, but this is a flaky test waiting to happen. I suggest using a database other than the `postgres` database for this test.",
        "Feel free to fix it in a subsequent PR."
      ],
      "ghostty-generate-dynamic-configurations": [
        "Should we add all releases?"
      ],
      "neon-minimize-unnecessary-allocations": [
        "Seems like it would be easier to implement this function in terms of `major_version_num()`, but I'll leave it up to you!",
        "This is very strange to me. At the previous call site that I reviewed, we already spawned and cloned, and now we are doing it again. Please avoid that. Then you can probably inline the `impl` version of the function."
      ],
      "ghostty-centralize-configuration-values": [
        "Instead of the linker script solution, can we just add a PR to check for the library name here?\r\n\r\nhttps://github.com/ghostty-org/ghostty/blob/95daca616db5c24d7bb37fd5a3ac2f8762bb4ead/src/build/SharedDeps.zig#L117",
        "The Zig build system doesn't allow you to pass the equivalent of `-lbz2`?",
        "Yeah I don't know enough about the Zig build system to say for sure unfortunately. Hopefully someone else can step in to assist, but linker script is still fine if it can't.",
        "I'm down for this pending what @jcollie and @pluiedev think"
      ],
      "neon-configuration-context-alignment": [
        "Thanks for catching this. I agree with you."
      ],
      "neon-document-parameter-choices": [
        "Also, please don't use an empty string for an ID."
      ],
      "neon-clear-consistent-identifier-names": [
        "One last comment. It is `pgbouncer`. That's the name of the binary. Using the correct name makes it easier to search and find references.",
        "Please be consistent about either `endpoint_storage_auth_token` or `endpoint_storage_token`."
      ],
      "neon-use-descriptive-identifiers": [
        "```suggestion\r\nPREWARM_LABEL = \"compute_ctl_lfc_prewarm_requests_total\"\r\nOFFLOAD_LABEL = \"compute_ctl_lfc_prewarm_offload_requests_total\"\r\n```\r\n\r\nCapitalize constants",
        "I typically think that booleans can be overloaded. For instance, you have to write this docstring to explain what `with_compute_ctl` means. I like to use enums in such cases.\r\n\r\n```\r\nfrom enum import Enum\r\n\r\nclass LfcQueryMethod(Enum)\r\n    COMPUTE_CTL\r\n    POSTGRES\r\n```\r\n\r\nMakes it a little more obvious. But I will leave it up to you, merely a suggestion on improving readability"
      ],
      "neon-proper-option-type-usage": [
        "This should continue to be an `Option<String>`. It's optional to pass it, and defaulting the value to empty string is meaningless.",
        "Please make `error` an `Option<String>`"
      ],
      "neon-flexible-documented-configurations": [
        "Follow-up PR here: https://github.com/neondatabase/cloud/pull/30120. It looks like we would overwrite the options sent from the control plane side. What are your thoughts on control plane vs compute_ctl changes?",
        "Seems like we may already overwrite `default_transaction_read_only=false`. I would need to verify that.",
        "Ok, I will take a look at the code and investigate. Thanks!",
        "PRs to remove the TODO:\r\n- https://github.com/neondatabase/cloud/pull/30274\r\n- https://github.com/neondatabase/neon/pull/12261\r\n\r\nAnd then the pseudocode that you wrote actually already exists at https://github.com/neondatabase/neon/blob/118e13438df173b98c83bea853e346ebbe00eab3/compute_tools/src/compute.rs#L363-L366.",
        "Ok, I missed that. I'll put up a PR for discussion.",
        "See the implementation at https://github.com/neondatabase/neon/pull/12262."
      ],
      "neon-structure-endpoints-for-rest": [
        "Instead of 4 different routes, let's consolidate down to 2.\r\n\r\n```\r\nGET /prewarm_lfc - get the status\r\nPOST /prewarm_lfc - make the request\r\n\r\nGET /prewarm_lfc_offload - get the status\r\nPOST /prewarm_lfc_offload - make the request\r\n```\r\n\r\nI assume that `offload` means to send the current state of the LFC to endpoint storage. Can we just drop the `prewarm` prefix so it is just `lfc_offload`. I don't see why `prewarm` needs to be there.\r\n\r\nIf ^ is a good suggestion to you, I'd like to see the routes changed to:\r\n\r\n```\r\nGET /lfc/prewarm\r\nPOST /lfc/prewarm\r\n\r\nGET /lfc/offload\r\nPOST /lfc/offload\r\n```",
        "No need to use JsonResponse for empty bodies. Returning the status only is perfectly fine,",
        "```suggestion\r\n        StatusCode::ACCEPTED.into_response()\r\n```",
        "Since this is an async request, `Accepted` would be the more appropriate status code."
      ],
      "ghostty-document-configs-comprehensively": [
        "Maybe it would be easier for users if we just blackholed this option and warned in the logs?"
      ],
      "ghostty-descriptive-consistent-naming": [
        "Feels like we should be more consistent with action names.\r\n\r\n```\r\nTOGGLE_SPLIT_ZOOM\r\nPWD\r\nRELOAD_CONFIG\r\nBELL\r\n```\r\n\r\nMaybe `RING_BELL` would be more consistent?"
      ],
      "neon-hierarchical-semantic-naming": [
        "New metrics related to compute should be prefixed with `compute_`",
        "I would prefer that we start prefixing Postgres metrics with `postgres_` but it isn't something that we've discussed as a team."
      ]
    },
    "profile": {
      "location": "Austin, TX",
      "company": "@databricks",
      "blog": "https://tristan.partin.io",
      "site_admin": false,
      "followers": 80,
      "following": 48
    }
  },
  "agaudreault": {
    "repos": [
      "argoproj/argo-cd"
    ],
    "entries": [
      {
        "slug": "argo-cd-api-documentation-clarity",
        "title": "API documentation clarity"
      },
      {
        "slug": "argo-cd-check-nil-before-access",
        "title": "Check nil before access"
      },
      {
        "slug": "argo-cd-choose-appropriate-synchronization-primitives",
        "title": "Choose appropriate synchronization primitives"
      },
      {
        "slug": "argo-cd-complete-configuration-examples",
        "title": "Complete configuration examples"
      },
      {
        "slug": "argo-cd-comprehensive-function-documentation",
        "title": "Comprehensive function documentation"
      },
      {
        "slug": "argo-cd-consolidate-rbac-permissions",
        "title": "consolidate RBAC permissions"
      },
      {
        "slug": "argo-cd-design-extensible-apis",
        "title": "design extensible APIs"
      },
      {
        "slug": "argo-cd-document-network-requirements",
        "title": "document network requirements"
      },
      {
        "slug": "argo-cd-document-observability-prerequisites",
        "title": "Document observability prerequisites"
      },
      {
        "slug": "argo-cd-explicit-security-controls",
        "title": "explicit security controls"
      },
      {
        "slug": "argo-cd-extract-testable-units",
        "title": "Extract testable units"
      },
      {
        "slug": "argo-cd-prefer-early-returns",
        "title": "Prefer early returns"
      },
      {
        "slug": "argo-cd-provide-comprehensive-explanations",
        "title": "Provide comprehensive explanations"
      },
      {
        "slug": "argo-cd-remove-unnecessary-elements",
        "title": "Remove unnecessary elements"
      },
      {
        "slug": "argo-cd-standardize-commit-tracing-metadata",
        "title": "standardize commit tracing metadata"
      },
      {
        "slug": "argo-cd-structured-logging-practices",
        "title": "structured logging practices"
      },
      {
        "slug": "argo-cd-use-clear-descriptive-names",
        "title": "Use clear, descriptive names"
      },
      {
        "slug": "argo-cd-use-configuration-constants",
        "title": "Use configuration constants"
      },
      {
        "slug": "argo-cd-use-descriptive-constants",
        "title": "Use descriptive constants"
      },
      {
        "slug": "argo-cd-validate-configuration-appropriateness",
        "title": "Validate configuration appropriateness"
      }
    ],
    "comments": {
      "argo-cd-consolidate-rbac-permissions": [
        "The controller already has a cluster role with the permissions in https://github.com/agaudreault/argo-cd/blob/88c3fd61daf9832f12a1766d3ff37a1521d02ca8/manifests/cluster-rbac/applicationset-controller/argocd-applicationset-controller-clusterrole.yaml#L77-L88 \r\n\r\nThese will only be given for cluster install and not namespace install. This PR should consolidate both."
      ],
      "argo-cd-design-extensible-apis": [
        "The list stats should be part of an ApplicationListResponse object specific to the server/application/application.proto.\r\n\r\n",
        "Create another method on the interface, or provide another interface param to the middleware to `GetToken(r *http.Request) string`",
        "Instead of having a new VerifyJWT method on the existing provider interface, shouldn't you have a new implementation of the provider interface for JWT token? or have a new provider totally in something like /util/jwt/provider.go"
      ],
      "argo-cd-check-nil-before-access": [
        "Based on the implementation of registerDexHandlers, if SSO is not configured, ssoClientApp will be nil. The auth middleware should handle the nil use case. Make sure to add an additional unit test to validate that (TestWithAuthMiddlewareWhenSSONotConfigured)",
        "Should always return nil when claims is missing\r\n\r\n```suggestion\r\n\tclaim, err := m.GetExpirationTime()\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to get 'exp' claim: %w\", err)\r\n\t}\r\n```",
        "`app.Spec.Destination.Server` should be received in parameters bacuse it can be `nil` if destination by name is used"
      ],
      "argo-cd-use-descriptive-constants": [
        "Maybe unrelated to this feature, but I think the usage of \"Default\" here is confusing now that inheritance happens. It is not the \"Default Service Account\", it is the \"Service Account\" to use for that destination as far as I understand.\r\n```suggestion\r\n                        <div className='columns small-5'>ServiceAccount</div>\r\n```"
      ],
      "argo-cd-use-configuration-constants": [
        "It is already possible to configure which resources update to ignore with customizations. https://argo-cd.readthedocs.io/en/stable/operator-manual/reconcile/ . Have you tried to use this feature?\r\n\r\nAdditionally, if you do not manage EndpointSlice manually, you can fully omit them from the watched resources. https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#resource-exclusioninclusion"
      ],
      "argo-cd-document-observability-prerequisites": [
        "Format table + add info block that all `argocd_github_api` commands will only be enabled when the flag is configured."
      ],
      "argo-cd-remove-unnecessary-elements": [
        "don't unnecessarily quote yaml. If quoting is necessary, use single quote."
      ],
      "argo-cd-api-documentation-clarity": [
        "```suggestion\r\n* `pullRequestState`: PullRequestState is an additional MRs filter to get only those with a certain state. By default all states. Default: \"\" (all states). Valid values: `\"\"`, `opened`, `closed`, `merged` or `locked`. (Optional)\r\n```"
      ],
      "argo-cd-use-clear-descriptive-names": [
        "nitpick on the name, but having a \"type\" be a validation regexp is not really intuitive.\r\n\r\n```suggestion\r\n            [\"format\"] = \"^[0-9]*$\",\r\n```"
      ],
      "argo-cd-provide-comprehensive-explanations": [
        "You should document the caveats for the user such as conflict with an HPA, or conflict when auto-sync is enabled and replicas are defined as code."
      ],
      "argo-cd-standardize-commit-tracing-metadata": [
        "By default, support (or not) Argocd-related-commit-type as a header. If it is not specifid, hydrator can decide to always assume git\r\n\r\n```suggestion\r\n{\r\n  \"references\": [\r\n    {\r\n      \"type\": \"commit\"\r\n      \"author\": \"Author Name <author-email>\",\r\n      \"sha\": \"<code-commit-sha>\",\r\n      \"message\": \"Commit message of the code commit\",\r\n      \"repoURL\": \"https://git.example.com/owner/repo\",\r\n      \"date\": \"2025-06-09T13:50:18-04:00\"\r\n    }\r\n  ]\r\n}\r\n```",
        "```suggestion\r\n  --trailer \"Argocd-reference-commit-message: Commit message of the code commit\" \\\r\n```"
      ],
      "argo-cd-extract-testable-units": [
        "extract all the test setup code to a function, the method should be generic enough so that it accepts a liveObj and returns only the client. You can then provide the faeClient to the \"import\" method (needs refactoring) and validate the result by doing a Get on the liveObj after the import.",
        "This test seems to re-implement the logic of the run function.\r\n\r\nThe unit test should be able to test a unit of code for which it can mock its dependencies. In this case, we want to test the NewImportCommand `Run` function. However, it is quite complex to provide a \"mock\" of the Kubernetes dependency this way. Instead, extract a function that you can test, that will receive a fake Kubernetes client and the  command arguments. This way, you can unit test that code.\r\n\r\nExample: `func (opts *importOpts) executeImport(client *dynamic.DynamicClient)`"
      ],
      "argo-cd-explicit-security-controls": [
        "Should we set the default behavior to `false` (reverse the flag) so we are \"secure\" by default. Since the Sync action has to be called by a client, users will be able to upgrade Argo CD with the new behavior without affecting their Application. \r\n\r\nIf users need tome to apply the override permissions, then they can disable the flag."
      ],
      "argo-cd-document-network-requirements": [
        "Link to the ingress documentation. The underlying user infrastructure may diverge too much from the example app.\r\n\r\n```suggestion\r\nThe api path `/api/webhook` of the `argocd-applicationset-controller` service on the `webhook` named port must be configured as part of your [ingress](./ingress.md).\r\n```",
        "You can add the fields to the example above. (for both generators)\r\n\r\n```yaml\r\n        # If true, skips validating the SCM provider's TLS certificate - useful for self-signed certificates.\r\n        insecure: true\r\n        # Reference to a ConfigMap containing trusted CA certs - useful for self-signed certificates. (optional)\r\n        caRef:\r\n          configMapName: argocd-tls-certs-cm\r\n          key: azure-devops-ca\r\n```"
      ],
      "argo-cd-comprehensive-function-documentation": [
        "out of scope of this refactor",
        "any change suggestion on the current docstring?",
        "Description updated to reflect the intent of the function and added doc for newRevisionHasChanges param usage. "
      ],
      "argo-cd-prefer-early-returns": [
        "Invert if condition to return early. It will be more readable now that the method grew in size"
      ],
      "argo-cd-validate-configuration-appropriateness": [
        "Missing a check to set `obj.spec.syncPolicy.automated.enabled = false` if it is currently true"
      ],
      "argo-cd-complete-configuration-examples": [
        "This PR should update the default kustomize manifests in `manifests/` to use that variable and mount the argocd-redis secret"
      ],
      "argo-cd-structured-logging-practices": [
        "This change would mean refactoring the variables so we have a map of Applications instead of a list. This change is outside the scope of this refactor",
        "any ways to have a logger with some context like the revision / and git repo ?\r\n\r\nIt will be hard to know what caused that error"
      ],
      "argo-cd-choose-appropriate-synchronization-primitives": [
        "I think when it timeouts, this will call `cancel()` on the context which will in turn close the `appEventCh` causing the for-range loop below to break. The last call will be `_ = printFinalStatus(app)`.\r\n\r\nThis would mean that the AfterFunc should make sure that\r\n1. It is not also calling `printFinalStatus` ✅ \r\n2. it should set `refresh = false` to make sure that the last call to `printFinalStatus` will not refresh the app.\r\n3. it should call `app, err = appClient.Get(ctx, &application.ApplicationQuery` to update the `app` (without refresh) so it is used by printFinalStatus.\r\n\r\nI haven't debugged if it is really what the execution does, but it should be testable in a unit test similar to `TestWaitOnApplicationStatus_JSON_YAML_WideOutput`.\r\n\r\nThere are also a few other problem with the code like the connection not being closed in the AfterFunc, and potential race conditions with refresh and app that might now require a lock. TBD",
        "immediately call `defer cancel()`",
        "Does the value of the timestamp really matter? Or what matters is that we received the log correctly?\r\n\r\n\"timestamp <= now\" seems more reliable. I don't think it is works mocking now, but ideally this is what should be done.",
        "updated to use struct type"
      ]
    },
    "profile": {
      "location": "Saguenay",
      "company": "@Intuit @argoproj",
      "blog": "",
      "site_admin": false,
      "followers": 26,
      "following": 1
    }
  },
  "mwilsnd": {
    "repos": [
      "maplibre/maplibre-native"
    ],
    "entries": [
      {
        "slug": "maplibre-native-buffer-bounds-validation",
        "title": "Buffer bounds validation"
      },
      {
        "slug": "maplibre-native-conditional-observability-instrumentation",
        "title": "Conditional observability instrumentation"
      },
      {
        "slug": "maplibre-native-cross-platform-ci-validation",
        "title": "Cross-platform CI validation"
      },
      {
        "slug": "maplibre-native-descriptive-named-constants",
        "title": "Descriptive named constants"
      },
      {
        "slug": "maplibre-native-document-platform-requirements",
        "title": "Document platform requirements"
      },
      {
        "slug": "maplibre-native-enforce-clear-data-ownership",
        "title": "Enforce clear data ownership"
      },
      {
        "slug": "maplibre-native-externalize-config-values",
        "title": "Externalize config values"
      },
      {
        "slug": "maplibre-native-follow-modern-c-guidelines",
        "title": "Follow modern C++ guidelines"
      },
      {
        "slug": "maplibre-native-group-related-properties",
        "title": "Group related properties"
      },
      {
        "slug": "maplibre-native-handle-errors-by-severity",
        "title": "Handle errors by severity"
      },
      {
        "slug": "maplibre-native-lock-responsibly-always",
        "title": "Lock responsibly, always"
      },
      {
        "slug": "maplibre-native-modern-c-style-practices",
        "title": "Modern C++ style practices"
      },
      {
        "slug": "maplibre-native-numerical-precision-considerations",
        "title": "Numerical precision considerations"
      },
      {
        "slug": "maplibre-native-optimize-compilation-flags",
        "title": "Optimize compilation flags"
      },
      {
        "slug": "maplibre-native-preallocate-collection-capacity",
        "title": "Preallocate collection capacity"
      },
      {
        "slug": "maplibre-native-prefer-safe-null-handling",
        "title": "Prefer safe null handling"
      },
      {
        "slug": "maplibre-native-template-instantiation-trade-offs",
        "title": "Template instantiation trade-offs"
      },
      {
        "slug": "maplibre-native-use-proper-logging",
        "title": "Use proper logging"
      }
    ],
    "comments": {
      "maplibre-native-use-proper-logging": [
        "This was changed from Error to Debug intentionally, some of the tests use a [log observer](https://github.com/maplibre/maplibre-native/blob/88917b18065f6c7ee5d11ab2e109e0fab7af6edf/src/mbgl/util/logging.cpp#L53-L56) and only [Debug is omitted](https://github.com/maplibre/maplibre-native/blob/88917b18065f6c7ee5d11ab2e109e0fab7af6edf/src/mbgl/util/logging.cpp#L79). If XOpenDisplay fails during a test using the [FixtureLogObserver](https://github.com/maplibre/maplibre-native/blob/main/test/src/mbgl/test/fixture_log_observer.hpp), the test will fail because of the error level used in this retry message."
      ],
      "maplibre-native-cross-platform-ci-validation": [
        "A couple things:\r\n- bwb is really slow, it isn't building in parallel. There might be a way to fix that though.\r\n- bwb fails to locate a provisioning profile which is why we switched to bwx. I tested building the demo app and it still fails here in bwb mode."
      ],
      "maplibre-native-buffer-bounds-validation": [
        "Do we want to check that `size + offset` is within the buffer bounds here?"
      ],
      "maplibre-native-conditional-observability-instrumentation": [
        "Could this be put behind a preprocessor macro so it only happens when built with Tracy support?"
      ],
      "maplibre-native-preallocate-collection-capacity": [
        "How often do we actually see reallocation happening here?"
      ],
      "maplibre-native-modern-c-style-practices": [
        "Yup, looks like I don't even need to specify it with Clang 16.",
        "Use braces on multi-line conditionals",
        "Prefer `static_cast<GLint>`"
      ],
      "maplibre-native-descriptive-named-constants": [
        "Could you assign these indices to names to help clarify? ie `constexpr size_t LinePropUpdateFlags = 0;` (or with an enum),",
        "Can you store this in a named variable to add context, ex `constexpr uint32_t maximumVertexBindingCount = 16;`"
      ],
      "maplibre-native-lock-responsibly-always": [
        "Can we add an option to wait forever by passing 0? Would it also make sense to reset the timeout if some events in the queue are processed but more are enqueued as a result?"
      ],
      "maplibre-native-template-instantiation-trade-offs": [
        "I think in this case (and for `addCurrentVertex`), it's probably preferable to stick with `std::function`. What I expect to happen here is each unique lambda instantiates a new instance of this template. Since there is a lot of code in this template function, each new lambda given duplicates all this code. That quickly sends the trade-off in the other direction towards `std::function` the moment more than one lambda is passed to this template. Since each lambda is unique and the template approach is an almost certain inline, that prevents the compiler from de-duping the 95+% of the function body that is otherwise identical.",
        "I'm not super happy with this, but I can't specialize a template member function that also accepts a lambda without using `std::function` to erase the lambda type.",
        "The problem stems from the polymorphism of LayerGroupBase - a template virtual function isn't possible, so we need to know what kind of layer group we're dealing with to then invoke the right instance's visit template.",
        "I don't think so. There are two aspects to overhead with `std::function`, allocation and indirection. We probably weren't incurring allocation overhead in any of these changes, but we were paying the indirection cost.\r\n\r\n`std::function`'s type erasure works by using virtual dispatch. The compiler can't inline these virtual function calls as the functions provided could vary and *possibly* carry an allocated closure.",
        "+1 extract this to a function. In particular, comment the usage of `somePrime` as the FNV prime (assuming this is where it came from).",
        "Looks good."
      ],
      "maplibre-native-optimize-compilation-flags": [
        "Build with `-Oz` here to prevent the inliner behavior from distorting the optimal size configuration. Otherwise things may get aggressively inlined which distorts the metric we're interested in.\r\n\r\nAdditionally, if we can, build only for armv8/AArch64. By default I think the xcframework should have both binaries in it (x86-64 and armv8). Bloaty gets confused by that and tends to pick one, usually x86-64 in my testing. Arm binaries are going to be bigger just by virtue of the instruction set, so make sure we're comparing arm."
      ],
      "maplibre-native-prefer-safe-null-handling": [
        "They ultimately mean the same thing, and at least in MSVC/Windows land, NULL is still used in a lot of places. I do agree though a chance to modernize while we're in here is good."
      ],
      "maplibre-native-follow-modern-c-guidelines": [
        "nit: `virtual` isn't required when the `override` keyword is present, high compiler warning levels can complain if they follow the core guidelines on [virtual, override and final](http://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines.html#c128-virtual-functions-should-specify-exactly-one-of-virtual-override-or-final)",
        "Prefer `using` over typedef",
        "syntax: `using CollisionBoxUBO = CollisionUBO;`"
      ],
      "maplibre-native-document-platform-requirements": [
        "This header won't be found for bazel builds, can you move it inside the `#if MLN_RENDER_BACKEND_OPENGL` block and additionally guard it with `defined(MLN_USE_TRACY )`?"
      ],
      "maplibre-native-handle-errors-by-severity": [
        "Should this throw? We need an allocator if we're going to render stuff.",
        "We should probably throw since assert will get dropped in release builds. Calling stubs should always terminate."
      ],
      "maplibre-native-numerical-precision-considerations": [
        "I think it's fine for now, if we start having to do this in other places we should add some test constants though."
      ],
      "maplibre-native-externalize-config-values": [
        "Should these limits be configurable at all? They could be set via build flags/preprocessor values.",
        "Can this be guarded by `#ifdef MLN_USE_TRACY `? We shouldn't try and load this extension unless we're building for instrumentation with Tracy"
      ],
      "maplibre-native-enforce-clear-data-ownership": [
        "I had to look up how to do this actually, haha. It extracts a value from the container and move it to a shared pointer inside the lambda's capture group. Doing it this way means we can keep using the iterator for the rest of the container.",
        "From when I worked on it, I believe `extract().mapped()` was my best option to prevent a copy of the element while also removing it from the container. Other options didn't look like they'd work.\r\n\r\nIt may have that lambda issue, I never noticed the other instance in `TileCache`. I also haven't seen the destructor running on the main thread. To be safe, I'd say do the same here as in `TileCache`.",
        "> Is `tiles.extract((tilesIt++)->first)` important here, vs. `tiles.extract(tilesIt++)`? It's doing an extra key search.\r\n\r\nIf it compiles and works correctly then no, I suppose it isn't.",
        ">It seems to be called somewhat rarely.\r\n\r\nIf it gets called at all then I say we should fix it, there is always a possibility of threading issues if that is allowed to happen.",
        "Have you considered what might happen if the caller holds on to the returned `string_view` while another thread (or even the same one later on) triggers a reallocation of `buffer`?\r\n\r\nIt might be prudent to couple the read lock with the returned string_view to at least reduce such a possibility.",
        "Yes, a copy would be the safest option if you feel that is acceptable. I would consider the frequency at which strings are fetched from the buffer to determine if constructing `std::string`s is an acceptable performance trade-off.",
        "We can just take a read lock here. If we don't find our string in the cache, we release the read lock and take a write lock to do our insertion."
      ],
      "maplibre-native-group-related-properties": [
        "Is the intention here to only do polylines? I think this could be made a bit more clear what \"mode\" we're in, perhaps break these parameters out to a struct and pass it to `addPolyline` instead of having them here."
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 13,
      "following": 1
    }
  },
  "fisker": {
    "repos": [
      "prettier/prettier"
    ],
    "entries": [
      {
        "slug": "prettier-add-explanatory-comments",
        "title": "Add explanatory comments"
      },
      {
        "slug": "prettier-angular-syntax-parsing",
        "title": "Angular syntax parsing"
      },
      {
        "slug": "prettier-api-documentation-clarity",
        "title": "API documentation clarity"
      },
      {
        "slug": "prettier-cache-correctness-validation",
        "title": "Cache correctness validation"
      },
      {
        "slug": "prettier-cache-invalidation-strategy",
        "title": "cache invalidation strategy"
      },
      {
        "slug": "prettier-consistent-spacing-patterns",
        "title": "consistent spacing patterns"
      },
      {
        "slug": "prettier-document-ci-workflow-rationale",
        "title": "Document CI workflow rationale"
      },
      {
        "slug": "prettier-documentation-clarity-standards",
        "title": "Documentation clarity standards"
      },
      {
        "slug": "prettier-documentation-example-consistency",
        "title": "Documentation example consistency"
      },
      {
        "slug": "prettier-ensure-semantic-naming-accuracy",
        "title": "Ensure semantic naming accuracy"
      },
      {
        "slug": "prettier-environment-specific-error-handling",
        "title": "Environment-specific error handling"
      },
      {
        "slug": "prettier-maintain-api-backward-compatibility",
        "title": "maintain API backward compatibility"
      },
      {
        "slug": "prettier-measure-performance-impacts",
        "title": "Measure performance impacts"
      },
      {
        "slug": "prettier-modern-configuration-formats",
        "title": "Modern configuration formats"
      },
      {
        "slug": "prettier-organize-tests-properly",
        "title": "Organize tests properly"
      },
      {
        "slug": "prettier-prefer-efficient-algorithms",
        "title": "prefer efficient algorithms"
      },
      {
        "slug": "prettier-refactor-complex-conditions",
        "title": "refactor complex conditions"
      },
      {
        "slug": "prettier-test-all-variations",
        "title": "Test all variations"
      },
      {
        "slug": "prettier-use-cross-platform-commands",
        "title": "Use cross-platform commands"
      },
      {
        "slug": "prettier-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "prettier-use-descriptive-variable-names",
        "title": "Use descriptive variable names"
      },
      {
        "slug": "prettier-use-example-configuration-files",
        "title": "Use example configuration files"
      },
      {
        "slug": "prettier-validate-configuration-changes",
        "title": "validate configuration changes"
      },
      {
        "slug": "prettier-validate-configuration-values",
        "title": "validate configuration values"
      },
      {
        "slug": "prettier-verify-optional-chaining-necessity",
        "title": "Verify optional chaining necessity"
      },
      {
        "slug": "prettier-vue-syntax-parsing-robustness",
        "title": "Vue syntax parsing robustness"
      }
    ],
    "comments": {
      "prettier-api-documentation-clarity": [
        "```suggestion\r\nPreviously, `languages` API for custom plugin only supported to infer parser based on the file basename or extension.\r\n\r\nPrettier main added `isSupported: (file: string) => boolean` function to allow plugin check if file is supported based on the full path (eg: files in a specific directory), the `file` parameter could be a normal path or a url string like `file:///C:/test.txt`.\r\n```",
        "`smae` -> `same`",
        "I think this signals what the arguments this function will receive, I don't think we should add this, many plugins define the following functions/methods with less parameters, we didn't mark they are optional. I'm not familiar with  TS, teach me if there is a good explanation.",
        "`parse(test)` was added by me, but I forgot where it is... ",
        "Found, https://github.com/prettier/prettier/issues/11888, we don't really support `parse(text)`."
      ],
      "prettier-environment-specific-error-handling": [
        "Make sense."
      ],
      "prettier-cache-correctness-validation": [
        "Feel not safe without file content, but that will be slow.",
        "```suggestion\r\n      // If `createFromFile()` fails, it's probably because the format of cache file changed, it happened when we release v3.5.0\r\n```",
        "Can we add a test for stdin? So we won't accidentally use cache when formatting stdin."
      ],
      "prettier-validate-configuration-changes": [
        "if we have a `tests/format/flow-repo/esproposal_decorators.ignore/foo/format.test.js`, will it be unignored?",
        "I remember that I did something strange in eslint too https://github.com/prettier/prettier/blob/1652973553677de297782643ca0263f6331eb0d3/eslint.config.js#L21-L25\r\n\r\nDoes it work for prettier?",
        "As I tested, use the same pattern as https://github.com/prettier/prettier/pull/17632#discussion_r2160047355 works too.",
        "I agree with @sosukesuzuki , since this rule autofixable, \"error\" should be used.",
        "I think current settings for `no-console` is fine too.\r\n\r\nStory behind it https://github.com/prettier/prettier/pull/10322#issuecomment-779277567",
        "Yes,\r\n\r\n```console\r\n>yarn why chalk\r\nyarn why v1.22.17\r\n[1/4] Why do we have the module \"chalk\"...?\r\n[2/4] Initialising dependency graph...\r\n[3/4] Finding dependency...\r\n[4/4] Calculating file sizes...\r\n=> Found \"chalk@5.0.0\"\r\ninfo Has been hoisted to \"chalk\"\r\ninfo This module exists because it's specified in \"dependencies\".\r\ninfo Disk size without dependencies: \"68KB\"\r\ninfo Disk size with unique dependencies: \"68KB\"\r\ninfo Disk size with transitive dependencies: \"68KB\"\r\ninfo Number of shared dependencies: 0\r\n=> Found \"eslint-formatter-friendly#chalk@2.4.2\"\r\ninfo This module exists because \"eslint-formatter-friendly\" depends on it.\r\ninfo Disk size without dependencies: \"40KB\"\r\ninfo Disk size with unique dependencies: \"96KB\"\r\ninfo Disk size with transitive dependencies: \"192KB\"\r\ninfo Number of shared dependencies: 6\r\n=> Found \"vnopts#chalk@2.4.2\"\r\ninfo This module exists because \"vnopts\" depends on it.\r\ninfo Disk size without dependencies: \"40KB\"\r\ninfo Disk size with unique dependencies: \"96KB\"\r\ninfo Disk size with transitive dependencies: \"192KB\"\r\ninfo Number of shared dependencies: 6\r\n=> Found \"npm-run-all#chalk@2.4.2\"\r\ninfo This module exists because \"npm-run-all\" depends on it.\r\ninfo Disk size without dependencies: \"40KB\"\r\ninfo Disk size with unique dependencies: \"96KB\"\r\ninfo Disk size with transitive dependencies: \"192KB\"\r\ninfo Number of shared dependencies: 6\r\n=> Found \"jest-diff#chalk@4.1.2\"\r\ninfo This module exists because \"jest-diff\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-message-util#chalk@4.1.2\"\r\ninfo This module exists because \"jest-message-util\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-snapshot#chalk@4.1.2\"\r\ninfo Reasons this module exists\r\n   - \"jest-snapshot\" depends on it\r\n   - Hoisted from \"jest-snapshot#jest-diff#chalk\"\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-util#chalk@4.1.2\"\r\ninfo This module exists because \"jest-util\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"babel-jest#chalk@4.1.2\"\r\ninfo This module exists because \"babel-jest\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"eslint#chalk@4.1.2\"\r\ninfo This module exists because \"eslint\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-watch-typeahead#chalk@4.1.2\"\r\ninfo This module exists because \"jest-watch-typeahead\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"node-actionlint#chalk@4.1.2\"\r\ninfo This module exists because \"node-actionlint\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"cspell#chalk@4.1.2\"\r\ninfo This module exists because \"cspell\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"@babel/highlight#chalk@2.4.2\"\r\ninfo This module exists because \"@babel#code-frame#@babel#highlight\" depends on it.\r\ninfo Disk size without dependencies: \"40KB\"\r\ninfo Disk size with unique dependencies: \"96KB\"\r\ninfo Disk size with transitive dependencies: \"192KB\"\r\ninfo Number of shared dependencies: 6\r\n=> Found \"@jest/core#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"@jest/transform#chalk@4.1.2\"\r\ninfo This module exists because \"jest-snapshot#@jest#transform\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"@jest/types#chalk@4.1.2\"\r\ninfo This module exists because \"@jest#globals#@jest#types\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-circus#chalk@4.1.2\"\r\ninfo This module exists because \"@prettier#jest-light-runner#jest-circus\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-cli#chalk@4.1.2\"\r\ninfo This module exists because \"jest#jest-cli\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-each#chalk@4.1.2\"\r\ninfo This module exists because \"@prettier#jest-light-runner#jest-each\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-matcher-utils#chalk@4.1.2\"\r\ninfo Reasons this module exists\r\n   - \"@types#jest#jest-matcher-utils\" depends on it\r\n   - Hoisted from \"@types#jest#jest-matcher-utils#jest-diff#chalk\"\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"snapshot-diff#chalk@4.1.2\"\r\ninfo Reasons this module exists\r\n   - \"snapshot-diff#jest-snapshot\" depends on it\r\n   - Hoisted from \"snapshot-diff#jest-snapshot#chalk\"\r\n   - Hoisted from \"snapshot-diff#jest-snapshot#jest-message-util#chalk\"\r\n   - Hoisted from \"snapshot-diff#jest-snapshot#jest-util#chalk\"\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-watcher#chalk@4.1.2\"\r\ninfo This module exists because \"jest-watch-typeahead#jest-watcher\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"@jest/console#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#@jest#console\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"@jest/reporters#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#@jest#reporters\" depends on it.info Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-config#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#jest-config\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-resolve#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#jest-resolve\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-runner#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#jest-runner\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-runtime#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#jest-runtime\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-validate#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#jest-validate\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n=> Found \"jest-jasmine2#chalk@4.1.2\"\r\ninfo This module exists because \"jest#@jest#core#jest-config#jest-jasmine2\" depends on it.\r\ninfo Disk size without dependencies: \"52KB\"\r\ninfo Disk size with unique dependencies: \"88KB\"\r\ninfo Disk size with transitive dependencies: \"184KB\"\r\ninfo Number of shared dependencies: 5\r\n```\r\n\r\n\"vnopts#chalk@2.4.2\" and \"@babel/highlight#chalk@2.4.2\" are removed in build script.\r\n\r\nhttps://github.com/prettier/prettier/pull/12162 https://github.com/prettier/prettier/pull/12182",
        "`yarn dedupe` not available in `main` branch. It uses classic yarn.",
        "But we should dedupe before merge."
      ],
      "prettier-documentation-example-consistency": [
        "```suggestion\r\nWhat is `yarn exec` doing at the start? `yarn exec prettier` runs the locally installed version of Prettier. We’ll leave off the `yarn exec` part for brevity throughout the rest of this file!\r\n```",
        "```suggestion\r\nTo run your locally installed version of Prettier, prefix the command with `npx`, `yarn exec`, `pnpm exec`, or `bun exec`, i.e. `npx prettier --help`, `yarn exec prettier --help`, `pnpm exec prettier --help`, or `bun exec prettier --help`.\r\n```",
        "Don't forget https://github.com/prettier/prettier/blob/main/website/versioned_docs/version-stable/cli.md",
        "````suggestion\r\n<!-- prettier-ignore -->\r\n```md\r\n<!-- Input (--prose-wrap=always) -->\r\nThis folder has [VHS] tape files to create gifs for the [Widget Showcase]. To run them, install VHS from main (the theme and screenshot commands are not yet released).\r\n\r\n<!-- Prettier stable -->\r\nThis folder has [VHS] tape files to create gifs for the [Widget Showcase]. To run\r\nthem, install VHS from main (the theme and screenshot commands are not yet released).\r\n\r\n<!-- Prettier main -->\r\nThis folder has [VHS] tape files to create gifs for the [Widget Showcase]. To\r\nrun them, install VHS from main (the theme and screenshot commands are not yet\r\nreleased).\r\n```\r\n````",
        "````suggestion\r\n#### Fix a bug that incorrectly strips of commas in some cases (#14476 by @seiyab)\r\n\r\n<!-- prettier-ignore -->\r\n```css\r\n/* Input */\r\n@font-face {\r\n  src: url(RobotoFlex-VariableFont_GRAD,XTRA,YOPQ,YTAS,YTDE,YTFI,YTLC,YTUC,opsz,slnt,wdth,wght.ttf);\r\n}\r\n\r\n/* Prettier stable */\r\n@font-face {\r\n  src: url(RobotoFlex-VariableFont_GRADXTRAYOPQYTASYTDEYTFIYTLCYTUCopszslntwdthwght.ttf);\r\n}\r\n\r\n/* Prettier main */\r\n@font-face {\r\n  src: url(RobotoFlex-VariableFont_GRAD,XTRA,YOPQ,YTAS,YTDE,YTFI,YTLC,YTUC,opsz,slnt,wdth,wght.ttf);\r\n}\r\n````",
        "```suggestion\r\n/** @type {import(\"prettier\").Options} */\r\n```"
      ],
      "prettier-maintain-api-backward-compatibility": [
        "`printDocToString` is a public API, let's not change the signature.",
        "I don't think we can drop support for assertions, Node.js already support import json file with that syntax, so people already use that syntax. Correct me if I'm wrong.",
        "Agree, we should be able to format runnable code.",
        "```js\r\nconst property = isNonEmptyArray(node.attributes) ? \"attributes\" : isNonEmptyArray(node.assertions) ? \"assertions\" : undefined;\r\n\r\nif (!property) {\r\n // ...\r\n}\r\n\r\nconst keyword = property === \"assertions\" || node.extra?.deprecatedAssertSyntax ? \"assert\" : \"with\";\r\n\r\n// ...\r\n```"
      ],
      "prettier-document-ci-workflow-rationale": [
        "If the tests need build, it should be in prod test.",
        "https://github.com/prettier/prettier/blob/5f7aedc1cf0a1b3b4ec2c5a5ca0c09d1e0d00660/jest.config.js#L34",
        "```suggestion\r\n    name: Node.js ${{ matrix.node }} on ${{ matrix.os }}${{ matrix.FULL_TEST && ' (Full Test)' || '' }}\r\n```"
      ],
      "prettier-documentation-clarity-standards": [
        "Don't know, but feel hard to understand \"text\" and \"code\". Maybe @kachkaev or someone else have a better idea?\r\n\r\nReminder: Please sync this https://github.com/prettier/prettier/blob/e9b8764fc4c770aaa8e5d9b982bda40a83a4cf2a/website/playground/markdown.js#L30 before merge.",
        "Maybe we should add an \"Expected output\" section?"
      ],
      "prettier-verify-optional-chaining-necessity": [
        "Will this index access cause problems for emojis or other characters\r\n? https://mathiasbynens.be/notes/javascript-unicode",
        ".at not only available on array, but we can check array and string only for now. We probably won't use TypedArray, and the transform only works for files (not packages) now. https://github.com/prettier/prettier/blob/4edb68ac50a847bef0d2968e1a1e4de643add7b0/scripts/build/transform/index.js#L13",
        "Since we are here. Let's remove this chaining, I don't think the `key` can be nullish here.\r\n\r\n```suggestion\r\n  const { name } = node.key;\r\n```"
      ],
      "prettier-validate-configuration-values": [
        "The value `0` is invalid, discussed in https://github.com/josephfrazier/editorconfig-to-prettier/issues/1 and decided to keep it, but [`tabWidth=0` crashes](https://github.com/prettier/prettier/issues/7388), I think better to ignore instead.",
        "```suggestion\r\n      \"[error] `--cache-strategy` cannot be used without `--cache`.\"\r\n```\r\n\r\nDon't miss https://github.com/prettier/prettier/pull/12800#discussion_r889630196",
        "1, they can be in different directories, they can't be simply concated. We need to know what the patterns related to.\r\n\r\n```\r\n#.ignore1\r\nfoo\r\n\r\n#dir/.ignore2\r\nbar\r\n```\r\n\r\n2. In this case\r\n\r\n```\r\n#.ignore1\r\nfoo/a.js\r\n\r\n#.ignore2\r\n!foo/*\r\nfoo/b.js\r\n```\r\n\r\nshould `foo/a.js` be ignored? I think user may expect it be ignored, because it's ignored in `.ignore1` file. But if we treat this case like\r\n\r\n```\r\n#.ignore\r\nfoo/a.js\r\n!foo/*\r\nfoo/b.js\r\n```\r\n\r\nI'm not sure what's expected.\r\n",
        "Where is this filepath come from? There is no `filepath` flag in CLI at all.",
        "`filepath` only exists when format stdin, when formatting files, it doesn't exist.",
        "See `formatFiles` function, it calls `createIgnorerFromContextOrDie` on first line. I don't think it will work."
      ],
      "prettier-measure-performance-impacts": [
        "```suggestion\r\n          !/\\S/.test(text.slice(locEnd(previousComment), locStart(node)))\r\n```",
        "JS is hard!",
        "Ha, this is the root cause... It will be hard to fix, this function mutates array because this comment https://github.com/prettier/prettier/blob/484ecde4307d3f535256ef83e9b09c5a0445bc77/src/document/printer.js#L506\r\n\r\nI try to solve this problem before, but didn't get time to finish https://github.com/prettier/prettier/pull/13315",
        "> Or is there no way to copy doc? \r\n\r\nCopy is not a good idea, the doc can be huge.\r\n\r\nLet's take my approach from #13315 . It creates mutable `fill` during print.",
        "Okay, I can take care of it, thanks for looking into the issue.",
        "Yes, it's complete, but I was waiting for a better solution :smile:",
        "```suggestion\r\n    // eslint-disable-next-line unicorn/prefer-at -- `Array#at` is slow on Node.js v16 and v18\r\n```\r\n\r\nSo we'll know when to remove it."
      ],
      "prettier-add-explanatory-comments": [
        "What kind of comment do you expect? Similar code exists\r\n\r\nhttps://github.com/prettier/prettier/blob/67e121a20b9d5d983970bdc2dcba5de2bd0f7beb/src/language-js/print/function-parameters.js#L246\r\nhttps://github.com/prettier/prettier/blob/67e121a20b9d5d983970bdc2dcba5de2bd0f7beb/src/language-js/print/object.js#L200",
        "[`e849c94`](https://github.com/prettier/prettier/pull/13735/commits/e849c94bbf45d4ceb502402fadf2ea425b5e4dc1)",
        "Can you add an example here to show what this condition supposed to match?",
        "Actually, I mean example for the code in this condition, not sure those `value-string`/`value-word` suppose to match."
      ],
      "prettier-modern-configuration-formats": [
        "How about just one ESM version?",
        "Yes.",
        "> I will create an example repository then, because [azz/prettier-config](https://github.com/azz/prettier-config) is using a json file. And the new example will also use the exports field.\r\n\r\n@azz Will you accept PR change your example to ESM?",
        "```suggestion\r\nimport usernamePrettierConfig from \"@username/prettier-config\";\r\n\r\n/**\r\n * @type {import(\"prettier\").Config}\r\n */\r\nconst config = {\r\n  ...usernamePrettierConfig,\r\n  semi: false,\r\n};\r\n\r\nexport default config;\r\n```",
        "The recommended way to use plugin should be\r\n\r\n```js\r\nimport * as prettierPluginXml ...\r\n\r\nconst config = {\r\n  singleQuote: true,\r\n  plugins: [prettierPluginXml],\r\n};\r\n```\r\n\r\nBut `vscode-prettier` cannot work since they transfer config to worker.\r\n\r\nMy suggestion is to use an absolute path/url instead, but `import.meta.resolve` is under `--experimental-import-meta-resolve` flag. Not sure what's the best way here.",
        "> I'm gonna open a separate issue there.\r\n\r\nThe issue is here in Prettier.",
        "> Fortunately import.meta.resolve by itself is not under a flag,\r\n\r\nWe still support Node.js v14.",
        "> What do you think about merging this PR without this section about sharing configurations with plugins? It could be added later in a follow up PR easily.\r\n\r\nThe example should work for everyone, give me a while to think about it.",
        "Sorry for leaving this for such a long time. Let's merge with current example.",
        "Shouldn't editor also do autocomplete to json file? https://github.com/SchemaStore/schemastore/blob/master/src/schemas/json/prettierrc.json",
        "![image](https://user-images.githubusercontent.com/172584/204206097-ddd48433-790e-4b1d-b0d0-ecebc2cff834.png)\r\n![image](https://user-images.githubusercontent.com/172584/204206163-4ba68d07-fc7c-4090-b85d-0960762d0b97.png)\r\n"
      ],
      "prettier-vue-syntax-parsing-robustness": [
        "I think this is really bad, we are walking big vue SFC just to find the root script.(I think `.walk()` can't quit).\r\nIf we want allow ts only if ts script found, I think we can found them from the root.\r\n\r\nAlso, I'm not sure if we really need this check, I like things be safe, but I think we can always pase it as ts? I don't use ts myself, I'm fine with both add or remove this check.",
        "Why this write in template original..",
        "I've fixed this case here.",
        "Added a test for your case https://github.com/prettier/prettier/pull/12113/commits/6bbf42bb043622b0e30b68c342dec0c5ae01acb6"
      ],
      "prettier-cache-invalidation-strategy": [
        "I'm not good at English, feel free to ignore.\r\n\r\nI guess this can be improved, maybe something like\r\n\r\n```\r\n#### Fix CLI crash when cache for old version exists (#17100 by @sosukesuzuki)\r\n\r\nPrettier 3.5 uses a different cache format than previous versions, Prettier stable crashes when reading existing cache file, Prettier main fixed the problem.\r\n```"
      ],
      "prettier-ensure-semantic-naming-accuracy": [
        "Can we deprecate these two methods in favor of `.key` / `.index` / `.node` getter?",
        "Where is `PropertyKey` ? Can't see."
      ],
      "prettier-angular-syntax-parsing": [
        "```suggestion\r\n  angularLetDeclaration: [],\r\n```\r\n\r\n`name` and `value` are not `Node`, they are stings."
      ],
      "prettier-use-descriptive-names": [
        "Let's rename the first parameter to `text`, this function doesn't care if it's raw or cooked.",
        "```suggestion\r\nJS (ES Module)\r\n```",
        "I don't think so, the config is a single module, why `s`? I'm a native English speaker, I'll let other maintainer decide."
      ],
      "prettier-use-descriptive-variable-names": [
        "What is `xs`? `array` should be a better name.\r\n\r\nMaybe also `chunkSize` -> `size`",
        "It's common to name as size.\r\n\r\nhttps://github.com/ryancole/chunk/blob/39631771da7e5d3eb8f43d6ebcd52905c02b46ed/src/chunk.js#L4\r\n\r\nhttps://github.com/lodash/lodash/blob/6a2cc1dfcf7634fea70d1bc5bd22db453df67b42/src/chunk.ts#L22",
        "Should we name it as something like `splitByContinuousWhitespace`?"
      ],
      "prettier-consistent-spacing-patterns": [
        "This can't be a boolean, It should be `\"line-start\" | \"line-end\"` or maybe `\"start\" | \"end\"` ?\r\n\r\nOr maybe change name to `experimentalOperatorLinebreak`, like [`operator-linebreak`](https://eslint.org/docs/latest/rules/operator-linebreak) rule.\r\n\r\n@sosukesuzuki I forgot how we decided.",
        "I'm fine with current name.",
        "I thought this PR is only add indentation back?\r\n\r\nI don't like this.",
        "`.icon-#{$size}` above didn't add space around",
        "I mean space inside `#{}`, this one has space around `$name`, but `.icon-#{$size}` don't.",
        "Looks like we are keeping spaces on stable version.\r\n\r\n**Prettier 2.1.2**\r\n[Playground link](https://prettier.io/playground/#N4Igxg9gdgLgprEAuEBiYASAhgAj-gwonAXx2AB0oDIAbCAJyRwbgBMBuKvEqq9YoOLYylavjqNmrTt1L9gQvNiWlycvJKYt2XcbyjoR68Zoj1tMvTxAAaEBAAOMAJbQAzslBYGDCAHcABR8ETxQsWn8sAE9PewAjBiwwAGs4GABlR2SXKABzZBgGAFc4ezgAW3j2NnYAGSx84qw8uAAxRgqsGFd85BAsYpgIOxAACxgK2gB1MZd4d2ywOAzQ+ZcAN3no-rB3OJBc9zgGGECkvK7kADMI4-sAK3cADwAhJNT0jKwKuDrcuA3O5lEBPZ4ZXJ5WhwACKxQg8CBtHuIGyDGODH67j2B0cDFyMGmLjYMDGyAAHAAGex4iDHaZJRz9PFwDEbQH2ACO8Pg5ycYQG7gAtFA4Ox2KNWNyXKxzi0rkhbsiQccKi5CiUVZDoXCEYDFcD7DAsPEiSSyUgAExGpIuWiQgDCEAqCpArIArKNiscAComsJKlEbUoASSgtVgGTA+OcAEFwxkYNFoUjjiQSEA)\r\n```sh\r\n--parser scss\r\n```\r\n\r\n**Input:**\r\n```scss\r\n#{$a                      } {\r\n    color: red;\r\n  }\r\n\r\n#{                      $a} {\r\n    color: red;\r\n  }\r\n#{           $a           } {\r\n    color: red;\r\n  }\r\n#{$a} {\r\n    color: red;\r\n  }\r\n```\r\n\r\n**Output:**\r\n```scss\r\n#{$a } {\r\n  color: red;\r\n}\r\n\r\n#{ $a} {\r\n  color: red;\r\n}\r\n#{ $a } {\r\n  color: red;\r\n}\r\n#{$a} {\r\n  color: red;\r\n}\r\n\r\n```"
      ],
      "prettier-organize-tests-properly": [
        "Better idea to test?",
        "Happy to see a separate test for this.",
        "If it's rejected, the test will fail, why we need that?",
        "You are right, I was not thinking right, I thought Jest will exit if one test fails."
      ],
      "prettier-refactor-complex-conditions": [
        "Why don't we destruct `value` here. Maybe also return early if `value === '' || !value.startsWith('-')` then we can check `value.charAt(1)` instead.",
        "?? ",
        "Sorry, I thought it's a `if`, but better rewrite to `if/else`\r\n\r\n```js\r\nreturn (foo && \"next\")\r\n```\r\n\r\nseems wired to me.",
        "```js\r\n  if (prevSibling &&\r\n    prevSibling.type === \"JSXExpressionContainer\" &&\r\n    prevSibling.expression.type === \"JSXEmptyExpression\" &&\r\n    prevSibling.expression.comments &&\r\n    prevSibling.expression.comments.some(\r\n      (comment) => comment.value.trim() === \"prettier-ignore\"\r\n    )) {\r\n    return \"next\";\r\n  }\r\n\r\n  return false;\r\n```",
        "```suggestion\r\n      const forceHardLine =\r\n        (parentParentParentNode.type === \"css-decl\" ||\r\n          (parentParentParentNode.type === \"css-atrule\" &&\r\n            parentParentParentNode.variable)) &&\r\n        node.groups.some((node) => node.type === \"value-comma_group\");\r\n```\r\n\r\nSame?",
        "`filter` + `some` can reduce to one `.some`.",
        "This change is not related, but the logic can be simplified to use `Array#some()`."
      ],
      "prettier-test-all-variations": [
        "[`0d3bae1` (#17679)](https://github.com/prettier/prettier/pull/17679/commits/0d3bae1b2ad5b46b07af65d93b6bb279324021a0)",
        "I think we need more tests for different `--quote-props` options."
      ],
      "prettier-use-example-configuration-files": [
        "Good point, I'm going to rename it as `settings.example.json`, and add a `prepare` script to copy to `settings.json` if it's not already exist.",
        "Turns out we can't use an extra script file to copy the file, because this will run when `yarn install prettier@prettier/prettier`",
        "yarn>=2 don't run `prepare`.",
        "I don't understand how `enableConstraintschecks` can do this. But yarn plugin should be able to do it.",
        "I'm going to use yarn plugin instead. Since we don't want add `yarn.config.cjs` for this."
      ],
      "prettier-prefer-efficient-algorithms": [
        "I didn't take a look at the AST, but are we checking if there is a space between current node and next node? How about simply compare `locEnd(iNode) !== locStart(iNextNode)`? Will it work?",
        "We already have so many hacks, one more doesn't matter.\r\n\r\n`Infinity + 1 === Infinity`",
        "`return (a.loc.start.line - b.loc.start.line) || (a.loc.start.column - b.loc.start.column)`",
        "We also have `.offset` added in https://github.com/prettier/prettier/pull/9626, so `a.loc.start.offset - b.loc.start.offset` or `locStart(a) - locStart(b)` like js printer",
        "```suggestion\r\n  return getChildren(node, options).next().done;\r\n```",
        "path.siblings, path.next, path.index should be used",
        "Same here, maybe `getCallArgumentSelector(index)`, allow index to be negative value (-1)."
      ],
      "prettier-use-cross-platform-commands": [
        "Please use `fs.writeFileSync` to write file.\r\n\r\nhttps://github.com/prettier/prettier/pull/16000#discussion_r1477057609\r\n\r\nhttps://github.com/typicode/husky/issues/1380",
        "I forgot this doesn't work on Windows...\r\n\r\nLet's use \r\n\r\n```js\r\nnode --eval \"fs.writeFileSync('.husky/pre-commit','npx lint-staged\\n')\"\r\n```\r\n\r\nSee https://github.com/prettier/prettier/blob/c8ba8dbca18858a7962184bbb3898502b9ec7cfb/docs/install.md?plain=1#L31-L40"
      ]
    },
    "profile": {
      "location": "China",
      "company": "@d0d0dotcom ",
      "blog": "https://fiskercheung.com/",
      "site_admin": false,
      "followers": 418,
      "following": 64
    }
  },
  "oliviertassinari": {
    "repos": [
      "mui/material-ui"
    ],
    "entries": [
      {
        "slug": "material-ui-consistent-component-api-patterns",
        "title": "Consistent component API patterns"
      },
      {
        "slug": "material-ui-consistent-package-naming",
        "title": "Consistent package naming"
      },
      {
        "slug": "material-ui-document-compatibility-boundaries",
        "title": "Document compatibility boundaries"
      },
      {
        "slug": "material-ui-document-implementation-decisions",
        "title": "Document implementation decisions"
      },
      {
        "slug": "material-ui-effect-hook-best-practices",
        "title": "Effect hook best practices"
      },
      {
        "slug": "material-ui-event-triggered-network-requests",
        "title": "Event-triggered network requests"
      },
      {
        "slug": "material-ui-explicit-configuration-resolution",
        "title": "Explicit configuration resolution"
      },
      {
        "slug": "material-ui-explicit-configuration-specifications",
        "title": "Explicit configuration specifications"
      },
      {
        "slug": "material-ui-follow-library-recommendations",
        "title": "Follow library recommendations"
      },
      {
        "slug": "material-ui-maintain-configuration-accuracy",
        "title": "Maintain configuration accuracy"
      },
      {
        "slug": "material-ui-meaningful-and-consistent-names",
        "title": "Meaningful and consistent names"
      },
      {
        "slug": "material-ui-nextjs-integration-patterns",
        "title": "Next.js integration patterns"
      },
      {
        "slug": "material-ui-use-design-system-tokens",
        "title": "Use design system tokens"
      },
      {
        "slug": "material-ui-use-screen-queries",
        "title": "Use screen queries"
      },
      {
        "slug": "material-ui-write-timeless-documentation",
        "title": "Write timeless documentation"
      }
    ],
    "comments": {
      "material-ui-meaningful-and-consistent-names": [
        "Usually called accumulator https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/reduce#examples, shorten `acc` in most of the codebase:\r\n\r\n```suggestion\r\n        (acc, curr) => ({\r\n          ...acc,\r\n```"
      ],
      "material-ui-effect-hook-best-practices": [
        "Moved this to wrap the maximum possible scope.",
        "> Is this operating on the same key as useLocalStorageState?\r\n\r\nIt does, but the TODO comments I have left in the code are to move to a point where the mode is to be handled by `useCurrentColorScheme` and not `useLocalStorageState`.",
        "@Janpot Yes but what I see is that the mode is a lot more integrated with MUI System once you use CSS Variables and Zero runtime than it was with Emotion (what Toolpad uses).\r\n\r\nMUI System almost needs to completely own that logic. If it's possible to separate things then I'm 💯 for it. I had this feeling that hard to understand how things work because of how closely things are linked together but I don't really see how to simplify things. We have effectively built https://github.com/pacocoursey/next-themes.\r\ncc @siriwatknp.\r\n\r\n"
      ],
      "material-ui-consistent-package-naming": [
        "Per https://www.notion.so/mui-org/engineering-mui-utils-purpose-9a9fc9da3a004864b6c4e1f4d1f24f95?pvs=4#101cbfe7b660800f9d59c9204ece4ec7?\r\n\r\n```suggestion\r\n  \"name\": \"@mui/internal-bundle-size\",\r\n```",
        "> https://www.notion.so/mui-org/engineering-mui-utils-purpose-9a9fc9da3a004864b6c4e1f4d1f24f95?pvs=4#d1699bb48f0f4ef58ca831d1ee9772ec\r\n\r\nAh right, ok, my bad, I agree, it's a different class. I propose this then: \r\n\r\n<img width=\"703\" alt=\"SCR-20250508-uatk\" src=\"https://github.com/user-attachments/assets/e49b37d8-bdf9-436b-a1bb-0dadb1b819cc\" />\r\n\r\nhttps://www.notion.so/mui-org/engineering-mui-utils-purpose-9a9fc9da3a004864b6c4e1f4d1f24f95?pvs=4#d1699bb48f0f4ef58ca831d1ee9772ec\r\n\r\nFor example, if we apply this in https://github.com/mui/mui-private/pull/867/files#diff-cdf978f995619066e7c5a60f61cb6d14debaa5f59943afdffe682a9ea53b091aR16\r\n\r\n<img width=\"520\" alt=\"SCR-20250508-ubib\" src=\"https://github.com/user-attachments/assets/02a5c4a0-4c2e-4b30-8aa2-3862d263268f\" />\r\n\r\nthis would become `@mui/private-chat-uikit`. cc @hasdfa.\r\n\r\n> I'm just removing the name altogether, problem solved \r\n\r\nOh yeah, if it has no dependents, even better 👍 "
      ],
      "material-ui-use-screen-queries": [
        "Prefer using the `screen`. We are moving tests as much as possible to rely on global queries. This is purely to keep the test environment simple. Most of the time, we don't need the notion of a container. We render one element at once on the screen.\r\n\r\n```suggestion\r\n    render(<TextareaAutosize style={{ backgroundColor: 'yellow' }} />);\r\n    const input = document.querySelector<HTMLTextAreaElement>('textarea')!;\r\n```"
      ],
      "material-ui-explicit-configuration-specifications": [
        "Because we set \"latest\" for the dependencies version, and most people who create reproduction never pin the npm packages version, having the versions set at the root should help us reproduce 2+ years old bugs when we try to iterate on some logic.",
        "> That's not very useful to recreate the installed environment\r\n\r\n@Janpot Oh yeah, agree, it only helps to know it's v4 and not v5 or v6 😆. \r\n\r\n> There will be a lockfile in the sandbox of the reproduction which should be sufficient to recreate and inspect the conditions of the bug.\r\n\r\nIn the reproduction provided in https://github.com/mui/material-ui/issues, none seems to have this.\r\n\r\n(_A side note, they all seem to prefer CodeSandbox even when we push to StackBlitz by default in the instructions, maybe some SEO lag, or something else._)",
        "Yeah, I think it would be better, but this is more time-consuming to implement. I think we can narrow the scope  down, make the PR simpler. I'm reverting the version.",
        "It looks like @jedwards1211 handled Material UI v7 support in https://github.com/jcoreio/material-ui-popup-state/issues/153, so we can make the change.\r\n\r\nIn the future, we might find surprises with breaking changes, and have to temporarily comment those lines.",
        "@jedwards1211 was there major changes that requires to update those examples? For example https://deploy-preview-46051--material-ui.netlify.app/material-ui/react-menu/#material-ui-popup-state",
        "OK, thanks. So I think that we can assume that we are good."
      ],
      "material-ui-document-implementation-decisions": [
        "Why remove codesandbox?\r\n\r\n- https://pro.similarweb.com/#/digitalsuite/websiteanalysis/overview/website-performance/*/999/1m?webSource=Total&key=codesandbox.io,stackblitz.com\r\n- https://analytics.google.com/analytics/web/?authuser=1#/analysis/p353089763/edit/KctZ977QSou7VoMyBQMaqw\r\n\r\nIt looks like the majority of our users go with codesandbox. "
      ],
      "material-ui-explicit-configuration-resolution": [
        "```suggestion\r\n  // must be loaded last: https://github.com/tailwindlabs/prettier-plugin-tailwindcss?tab=readme-ov-file#compatibility-with-other-prettier-plugins\r\n  plugins.push('prettier-plugin-tailwindcss');\r\n```"
      ],
      "material-ui-event-triggered-network-requests": [
        "To check what should be the correct behavior with the middle click",
        "How about?\r\n\r\n```suggestion\r\n    if ((inputValue === '' || !open) && event.button === 0) {\r\n```\r\n\r\n<img width=\"797\" alt=\"Screenshot 2023-03-20 at 01 51 17\" src=\"https://user-images.githubusercontent.com/3165635/226222148-a910ba0c-8a90-4436-ace1-c7b1817bde73.png\">\r\n\r\nhttps://developer.mozilla.org/en-US/docs/Web/API/MouseEvent/buttons\r\n\r\n\r\nWe use `event.button` for the select and the slider: https://github.com/mui/material-ui/pull/22557#discussion_r486698221",
        "@michaldudak In the case of `event.buttons`, I imagine that the logic would need to be `event.buttons === 1` while it's `event.button === 0` in our case. But overall, I think that the issue is:\r\n\r\n- button is active: which button was pressed on the mouse **to** trigger the event\r\n- buttons is passive: which buttons are pressed on the mouse **when** a mouse event is triggered",
        "Maybe\r\n\r\n```suggestion\r\n    // Only handle event when the main button is pressed (left click).\r\n    if ((inputValue === '' || !open) && event.button === 0) {\r\n```"
      ],
      "material-ui-document-compatibility-boundaries": [
        "That was not true, it has never been allowed, per https://mui.com/material-ui/guides/minimizing-bundle-size/#option-one-use-path-imports."
      ],
      "material-ui-use-design-system-tokens": [
        "Why does it use a custom value? Meaning, why is this not using the theme breakpoints? I think deviations from the standard deserve code comments.",
        "Rely on the design token, not hard-coded values."
      ],
      "material-ui-maintain-configuration-accuracy": [
        "Is there a singleton on this one? I would imagine that the system can be a direct dependency\r\n\r\nAlso, per https://github.com/mui/material-ui/pull/32623 we should add a peer dependent on emotion."
      ],
      "material-ui-consistent-component-api-patterns": [
        "I would imagine that SvgIcon will eventually go inside `@mui/system` because once we add a new set of icons for Joy UI 1. Will we duplicate `SvgIcon` again? 2. Won't we want to have Material UI users be able to import these new icons without loading Joy UI?",
        "But then, maybe it means that we should solve #21251 and let the developers import `SvgIcon` from the right package?\r\n\r\n```jsx\r\nimport SvgIconMd from '@mui/material/SvgIcon';\r\nimport SvgIconJoy from '@mui/joy/SvgIcon';\r\nimport EditIcon from '@mui/icons-material/Edit';\r\n\r\n<SvgIconMd>\r\n  <EditIcon />\r\n</SvgIconMd>\r\n```\r\n\r\nIt's a fallback method of FontAwesome: https://fontawesome.com/docs/web/use-with/react/add-icons#add-individual-icons-explicitly.",
        "> It is tailored specifically for Material Design icons (sizes, viewport)\r\n\r\nI don't think that these are fundamentally specific to Material Design or Joy UI. We document how it can be used with random SVG path in https://mui.com/material-ui/icons/#font-awesome and are using it for the same use case as well. IMHO, the value of this component is to apply a few better default: attributes, a11y resets, CSS resets, that developer would want. It's also to provide the MUI System style helper. ",
        "The second argument was meant to be called with a value, not an option. It's related to #23708 and #31192. Unclear what is right here."
      ],
      "material-ui-follow-library-recommendations": [
        "I'm not sure we need this e2e test in the first place. It doesn't harm but I think that the origin of the problem is that `<TextField onClick` should be registered on the root element of the text field, we can't, move it to a lower element as we did in https://github.com/mui/material-ui/pull/36892."
      ],
      "material-ui-write-timeless-documentation": [
        "@mui-org/x heads up, this is a breaking change, you need to apply the same diff once you upgrade the mono-repo. We improve the support for the translation of the docs."
      ],
      "material-ui-nextjs-integration-patterns": [
        "It looks like we can remove `passHref` now per https://nextjs.org/docs/app/api-reference/components/link and the source: https://github.com/vercel/next.js/blob/8e0acd90341d59f53447189f265c5c8f9b1e3c28/packages/next/src/client/link.tsx. \r\n\r\nI'm fixing this in https://github.com/mui/material-ui/pull/44871."
      ]
    },
    "profile": {
      "location": "Paris, France",
      "company": "MUI",
      "blog": "",
      "twitter_username": "olivtassinari",
      "site_admin": false,
      "followers": 3225,
      "following": 104
    }
  },
  "medikoo": {
    "repos": [
      "serverless/serverless"
    ],
    "entries": [
      {
        "slug": "serverless-api-clarity-and-consistency",
        "title": "API clarity and consistency"
      },
      {
        "slug": "serverless-api-schema-validation-accuracy",
        "title": "API schema validation accuracy"
      },
      {
        "slug": "serverless-configuration-examples-accuracy",
        "title": "configuration examples accuracy"
      },
      {
        "slug": "serverless-consistent-asyncawait-usage",
        "title": "consistent async/await usage"
      },
      {
        "slug": "serverless-eliminate-redundant-operations",
        "title": "Eliminate redundant operations"
      },
      {
        "slug": "serverless-maintain-configuration-standards",
        "title": "maintain configuration standards"
      },
      {
        "slug": "serverless-maintain-iam-role-isolation",
        "title": "maintain IAM role isolation"
      },
      {
        "slug": "serverless-prefer-simple-readable-patterns",
        "title": "prefer simple readable patterns"
      },
      {
        "slug": "serverless-promise-error-handling-patterns",
        "title": "Promise error handling patterns"
      },
      {
        "slug": "serverless-standardize-log-output-methods",
        "title": "standardize log output methods"
      },
      {
        "slug": "serverless-structure-configs-for-clarity",
        "title": "Structure configs for clarity"
      },
      {
        "slug": "serverless-use-descriptive-semantic-names",
        "title": "Use descriptive semantic names"
      },
      {
        "slug": "serverless-use-loose-equality-checks",
        "title": "use loose equality checks"
      },
      {
        "slug": "serverless-use-runserverless-utility",
        "title": "Use runServerless utility"
      }
    ],
    "comments": {
      "serverless-standardize-log-output-methods": [
        "Let's use `process.stdout.write` instead of `console.log` (it allow us to implement this without fooling linter).\r\n",
        "Also I think better format would be:\r\n\r\n```javascript\r\n`Serverless: ${chalk.red(`Deprecation Warning: ${message}`)\\n           ${chalk.dim(`More Info: ttps://www.serverless.com/framework/docs/deprecations/#${code}`)}}`\r\n```\r\n",
        "Let's also add new line at the end (in case of `process.stdout.write` they're not added as it's with `console.log`"
      ],
      "serverless-prefer-simple-readable-patterns": [
        "@sdas13 we definitely should stick to async/await syntax. It's way more readable and maintanable",
        "I think more natural would be to use `some` here (`find` is good when we're interested in _key_ value, and not just fact that it matches some conditions)",
        "Le'ts just write it as `return resources[key].Type === 'AWS::Lambda::LayerVersion'`",
        "Let's remove `eslint-disable` comment (ensure to construct logic in away, so we don't have to disable linter)",
        "Let's not use `reduce`, it doesn't feel as use case for it.\r\n\r\nHow about:\r\n\r\n```javascript\r\nreturn Object.values(groups).map((groupStatements) => {\r\n  const [resultStatement, ...toBeJoinedStatements] = groupStatements;\r\n  resultStatement.Resource = Array.concat(\r\n    resultStatement.Resource,\r\n    ...toBeJoinedStatements.map(({ Resource }) => Resource)\r\n  );\r\n  return resultStatement;\r\n});\r\n```",
        "Let's simply `return` (no need for `Promise.resolve()`",
        "Yes, the function either needs to return value or not.\r\n\r\nAs this function is not designed to return specific value, I think fix is to ensure we do not return last promise but just await it",
        "Let's not introce such stylistic changes (`to.equal(true)` into `to.be.true`), it brings on extra value, while increases maintenance cost"
      ],
      "serverless-structure-configs-for-clarity": [
        "Let's not put any validation here.\r\n\r\nWe've just added schema based config validation. So we should simply configure a JSON schema for those properties here: https://github.com/serverless/serverless/blob/d403d9c6d3dccdfc9f79d7c42b3cbc1a06fcfac0/lib/plugins/aws/provider/awsProvider.js#L145",
        "I think there's no need to limit that. We should treat an empty array as no EFS configured",
        "I think we need to support both `include` and `patterns`, in a way I proposed in this comment: https://github.com/serverless/serverless/pull/8581#discussion_r538467304\r\n\r\nTechnically we stand between choice:\r\n1. If user provided `patterns`, throw if any `include` and `exclude` configurations are found\r\n2. Support all three\r\n\r\nI think it's easier (and not harmful) to support all three",
        "> You also suggested to crash if both are used in comment: #8093 (comment) 😅\r\n\r\nThat's true. Sorry for confusion. At that point it was not deeply thought. Thing is that crashing is more obtrusive.\r\n\r\nFirstly the easy approach to this is to upfront investigate all `package` configs (service-level, function-level, layer-level) and crash if we find that there's `patterns` somewhere and `includes` or `excludes` elsewhere`, and that's aggressive if e.g. user is switching gradually, e.g. refactor first function one, check how it works and then refactor others. It'll be nice to not break that.\r\n\r\nNow trying to validate case by case, is confusing, as e.g. for function 1 config (which uses just `include` and `exclude`) may be valid and for function 2 (which uses just `patterns`) may be not (when on service level `include` or `exclude` is used)\r\n\r\nI assume it's not harmful to support all (user will always be approached with deprecation notice anyway), and implementation wise it's simplest approach.\r\n\r\n> The problem of supporting both include and patterns is that the order in which they are appended becomes non-evident to the user and can cause tickets simply because the user expects a different order.\r\n\r\nUser should not reason about order in such case, but simply ensure that just `patterns` is used , also again _deprecation_ notice will signal that config stands on quirky ground\r\n",
        "> Function 1 and 2 will be valid in that example.\r\n\r\nI think if we invalidate it on function level, we should definitely also not support combining `include` and `exclude` on service level with `patterns` on function level. That would be even more confusing (what's the logical reason for rejecting just one case?)\r\n\r\nAnyway have you read the arguments I've put for supporting all without throwing (?) I take it's simplest thing we can do, and really not harmful",
        "> You can use patterns OR include/exclude at the function level.\r\n> It doesn't matter if you use pattern in patterns at service and include/exclude at the function\r\n\r\nIn last comment, my point was that above feels confusing. If we already support mixing both `include` and `patterns` (coming from two configuration sources) to package given lambda, why restrict mixing of `include` and `patterns` coming from one configuration source.\r\n\r\n> However, if we mix include/exclude/pattern in the same config. We need to decide in which order to merge them.\r\n\r\nYes, I've proposed order in one of the first comments here: https://github.com/serverless/serverless/pull/8581#discussion_r538467304\r\n\r\n> It's trivial, I know, exclude -> include -> patterns, but users might expect otherwise and create confusion.\r\n\r\nUser should not investigate and debate on such order, instead should just use `pattern`. We'd allow mixing both sas temporary mean simply for ease of migration (no need to be too restrictive here)\r\n\r\n\r\n",
        "Let's not use different _object_ configurations, as they result with unspecific errors for user.\r\n\r\ne.g. if user misconfigures some property, in result will be presented with error message as: \"Unsupported configuration for 'schedule' event\" not providing a hint which property is misconfigured (it's due to fact that AJV will report validation errors for two object modes and we don't have any intelligent handling to decide which error information we should choose)\r\n\r\nSo let's pack all properties from both configuration modes under one `object` definition and in code introduce further distinction with meaningful validation error throwing if needed",
        "Configuring definition like that is slightly problematic as it'll trigger _vague_ mode for validation of other authorizers, as now any validation failure will now be reported simply with _\"Unsupported configuration format\"_ without providing a hint which exactly property is misconfigured.\r\n\r\nIt's due to fact that two versions for _object_ variant are provided, and validator won't know errors of which to report, hence it'll report just message that something is wrong.\r\n\r\n(you may also confirm on that manually).\r\n\r\nI think best way to recover is to use one object notation, and then inline ensure that for AWS IAM case it's just `type` property that provided, and throw meaningful error if any other properties are found"
      ],
      "serverless-use-loose-equality-checks": [
        "Let's simplify to `resource.Properties == null` (there's no valid reason to treat `null` and `undefined` differently)",
        "`resource.Properties == null` will catch all of those scenarios, and that's wanted",
        "Let's use `process.env[address] == null` check instead. Also we need to update it below so `''` is not overridden with null",
        "It should be: `value: process.env[address] == null ? null : process.env[address]`",
        "We should treat `null` in the same way. So let's update the condition to `!= null`",
        "Let's keep it simple as `startingPosition === 'AT_TIMESTAMP' && startingPositionTimestamp == null`",
        "Let's construct it more naturally as:\r\n\r\n```javascript\r\nevent.cloudFront.behavior.ForwardedValues ||\r\n                event.cloudFront.behavior.MaxTTL !=null ||\r\n                event.cloudFront.behavior.MinTTL != null ||\r\n                event.cloudFront.behavior.DefaultTTL !=null\r\n```\r\n",
        "Prettier doesn't do that for sure: https://prettier.io/playground/#N4Igxg9gdgLgprEAuEBLAZgAgBSYDpSZHEklwBuCMAdGADYQCuAJgGIBO0NARnABYBDcqgjtMAMnEFSM4tgpVaDFhy7Veg4aOqtRAdwHtmcZgDUBdRnADOmAD53ps5wthKmbTm41CR7agCyAgAeACqhADKYAIQAvFCMdHT2joTOsq409B6q3vy+2gGoUOFRcZgJSSlO6aSZ7ipePPla-gAicOgCiTClMfGJdACUQ5jAAL4gADQgEAAOMCJQ1sighpx6AAqGCCsoFgYAnisz3OwCYADWcDAAygIAtnARxXDIXXTWcKfnVze3cwuxQA5sgYOwrDM4A9eMxjMwIgIoMDGAJgXBdOwHgIYItkcgQN0YBBpiA+DAHnQAOp8VDwayAsBwW67OmoYQwQ4EsDWE5oZZwdgwTbnYHY94WL4zABW1mCtxBdDgAEVGBB4BLPt8QID2F92ATuAJeHRSXN2MUYFTUMwYHxkAAOAAMM3NEC+VPOcwJ5psgsopIAjmr4CL5ntCdYALRQOAmEyk9hwYOoJMitHipAfKUgL4PVBgiHa6yKlUht5ZyXamDG622+1IABMM3BAlQdBBAGEIA9MyAbABWUmML6hY17bPa8hWACSUGMsFuYAtCwAgvPbpylZqvuNxkA\r\n",
        "@godu as mentioned above, following will work perfectlly, and resembles best JS standard:\r\n\r\n```javascript\r\nif ( \r\n              event.cloudFront.behavior &&\r\n              (event.cloudFront.behavior.ForwardedValues ||\r\n                event.cloudFront.behavior.MaxTTL !=null ||\r\n                event.cloudFront.behavior.MinTTL != null ||\r\n                event.cloudFront.behavior.DefaultTTL !=null)) {\r\n```"
      ],
      "serverless-eliminate-redundant-operations": [
        "It will be nice to introduce (with other PR) a refactor where we rely on memoized [`getHashForFilePath` util](https://github.com/serverless/serverless/blob/c6b5a5f4c6f206c2015267fd7c6b2ef9780c7dfd/lib/plugins/aws/package/lib/getHashForFilePath.js)",
        "As we now need to download and inspect CF template to know which artifacts are involved in given deployment I think it'll be great to be more optimal.\r\n\r\nI wouldn't in all cases read all CF templates, but only those from which we need information. Therefore I would refolumate approach to instead of introducing `findDeployments` in current shape. Introduce:\r\n\r\n####  `resolveDeploymentsData()`\r\n\r\nWhich should return following:\r\n\r\n```javascript\r\n{\r\n  deployments: [[..], [..]] // Each item contains full paths to all files in given deployment folder\r\n  artifacts: [..]// full paths to all artifact files\r\n }\r\n```\r\n\r\n####  `resolveDeploymentsArtifacts(deploymentsData, from, to)`\r\n\r\nMethod which takes CF templates for deployments resolved as `deploymentsData.deployments.slice(from, to)` and returns (in form of set) all artifacts found referenced in those templates and in `deploymentsData.artifacts`\r\n\r\n--- \r\n\r\nHaving above, in `cleanupS3Bucket`, we can be more optimal and do as:\r\n\r\n```javascript\r\nconst deploymentsData = await resolveDeploymentsData()\r\nif (deploymentsData.deployments.length > stacksToKeepCount) {\r\n  const usedArtifacts = await resolveDeploymentsArtifacts(deploymentsData, -stacksToKeepCount);\r\n  // let's also upgrade `removeObjects` to introduce `{ Key: <path> }` wrap internally\r\n  await removeObjects([\r\n    ..._.flatten(deploymentsData.deployments.slice(0, -stacksToKeepCount),\r\n    ...deploymentsData.artifacts.filter(artifactPath => !usedArtifacts.has(artifactPath))])\r\n}\r\n```\r\n\r\nAnd in case where we upload, we may confirm on fact of artifact existence via:\r\n\r\n```javascript\r\nconst artifactBaseNames = new Set(Array.from(deploymentsData.artifacts, artifactPath = path.basename(artifactPath, \".zip\")));\r\n\r\nif (artifactBaseNames.has(artifactToUploadHash)) // already uploaded\r\n```\r\n\r\nAnd I think there are no other places we will really introduce changes\r\n\r\n\r\n\r\n\r\n\r\n",
        "> Are there any statistics that would confirm that the affected functionality (cleanup S3 bucket, rollback, deploy list) has to be optimized? \r\n\r\nI believe that we need to change how things works only for _cleanup_ operation, and there are no changes to _rollback_ and _deploy list_ planned per spec. Do you see that some are needed? If so can you please elaborate in an issue, as that definitely needs to be included in a spec then.\r\n\r\nNow when speaking strictly about _cleanup_, this change will introduce a need to download CF templates as hosted on S3. In case of large services those templates can be big. I assume that downloading those templates will be relatively fast when comparing to the deploy operation in general, but I suspect that in case of large services it still may add 0.1-0.5s of overhead to the command, which we can avoid in some scenarios.\r\n\r\nAlso do you see anything particularly challenging in what I proposed? My feeling is that this doesn't make our implementation more complex, but rather better organized.",
        "I guess it's more efficient to resolve all file names with one call and confirm against that, instead of issuing AWS request per each file name",
        "@remi00 ok, so situation with AWS SDK, is that it creates some sort of long-polling connection? and then works in similar way as e.g. SSE on browser side?\r\n\r\nWhat are the observed times of above requests when done locally? (e.g. if you read time at 59 line and then check difference at 61 or 63, what difference you will observe)\r\n\r\nNote that reusing collection of obtained names will make this logic also simpler, this try/catch looks a bit convoluted",
        "Wouldn't `item.startsWith('[') be good enough here? (relying on `JSON.parse` in filter and then repeating it in map doesn't feel optimal)",
        "It would be nice to construct this in a way, that we have two methods. First `checkIfEcRepositoryExist`, which we should issue at beginning but do not `await`,\r\nand `await` after `monitorStack` succeds, and if it returns true proceed with `removeEcrRepository()`\r\n\r\nThis will ensure we do not add time to `remove` command for services where ECR is not involved",
        "Why not _move dir contents_ ? We've downloaded to temp dir, right?"
      ],
      "serverless-use-runserverless-utility": [
        "@Lokesh-Jawale all new tests should be configured with `runServerless`. see: https://github.com/serverless/serverless/tree/main/test#unit-tests\r\n\r\nI think in tests should configure `vpc` settings for service, and then confirm in the generated template that those settings are reflected on the generated custom resource. \r\n\r\n(there's already one `runServerless` based test configured in context of this test file)",
        "We have a policy to configure all new tests with help of [`runServerless`](https://github.com/serverless/test/blob/master/docs/run-serverless.md) util.\r\n\r\nHere it'll involve some AWS SDK mocks, which easily can be configured with help of `awsRequestStubMap` option, e.g. example of such test, can be found here: https://github.com/mars-lan/serverless/blob/25e8ad1fed1ebaffc4e81b306b352a385ae4b2d9/test/unit/lib/plugins/aws/remove/lib/bucket.test.js#L156-L218",
        "I checked the commited code, but I don't see tests constructed with `runServerless`. Are you sure you've pushed that?",
        "This test file ideally if it's updated to test following:\r\n\r\n- Confirm adding log groups for functions\r\n- Confirm support for logRententionInDays\r\n- Confirm support of `provider.iam.tags\r\n- Confirm support of `provider.iam.permissionsBoundary`\r\n- Confirm support of `provider.iam.statements`\r\n- Confirm support of `provider.iam.managedPolicies`\r\n- Confirm that needed managed policy is added if `provider.vpc`\r\n\r\nAlso all tests should be done with `runServerless` util (and best if those runs are reused if possible, e.g. all IAM extensions can be tested with one run)",
        "@issea1015 those tests are currently in `lib/plugins/aws/package/lib/mergeIamTemplates.test.js` in master, and in this PR you were renaming this file and having those tests removed.\r\n\r\nThose tests should remain present in a test file that corresponds to the module which implements the functionality\r\n\r\n",
        "> The test cases for function scope IAM might have confused you. I moved them from mergeIamTemplate.test.js to compile/functions.test.js as the functionality is moved to compile/functions.js\r\n\r\nThat's good then. If functionality is tested in scope of module that implements it, then it's the way it should be :)",
        "Sorry, didn't notice that one earlier.\r\n\r\nIn new tests we always rely on `runServerless` util (we have already a tons of tests configured for it, which can self as reference).\r\n\r\nCan you update that one, so it's also `runServerless` based?",
        "Yes, that looks very good.\r\nJust use async/await (instead of `.then()`), and rather confirm on property as `expect(typeof ..).to.equal('string')`",
        "Any new tests should be based on [`runServerless`](https://github.com/serverless/test/blob/master/docs/run-serverless.md) util, e.g. see how `provider.getRegion()` is tested: https://github.com/serverless/serverless/blob/9e308bdf041c73eb2d27408ae4e03bc10fe9bf8a/test/unit/lib/plugins/aws/provider.test.js#L602-L637",
        "New tests should be configured via `runServerless` util, as e.g. tests in this block: https://github.com/serverless/serverless/blob/5d1c3f7ed4d481619d412b6b453d6eaf94b32b4a/test/unit/lib/plugins/aws/package/compile/events/websockets/index.test.js#L116-L258\r\n\r\nSee also: https://github.com/serverless/serverless/tree/main/test#unit-tests",
        "@Inqnuam all new tests need to be configured via `runServerless` util (it's the only reliable way to confirm that functionality is working as expected).\r\n\r\nYou can find such tests already configured at https://github.com/serverless/serverless/blob/544e25da567de367ec174d491462a57f46a6228c/test/unit/lib/plugins/aws/package/compile/events/alb/index.test.js and I believe you can add one created here as well.\r\n\r\nSee tests documentation: https://github.com/serverless/serverless/tree/main/test#unit-tests",
        "@Inqnuam if PR is ready for re-review, just re-request review in reviewers box, Thanks 👍 ",
        "Let's refactor this tests to rely on `runServerless` utils (we also then don't have to do any mocking)\r\n\r\nSee: https://github.com/serverless/serverless/tree/main/test#unit-tests\r\n\r\n",
        "Let's not add a test here.\r\n\r\nInstead, let's ensure it's tested in the context of https://github.com/serverless/serverless/blob/d6de3346ce962392c053f7f6480f52dcdb918624/test/unit/lib/plugins/aws/package/compile/events/stream.test.js#L1474 (properly with `runServerless` util)\r\n\r\nNote that currently test over there fail, as deprecation is presented (in tests deprecations provoke command fails - to ensure we always handle them). So I believe updating the test to new naming, will automatically create test coverage for this update",
        "This `it` should not be in context of this `describe` (e.g. it doesn't require the `before` job as configured here).\r\n\r\nI think to have it clean, we need to reorganize this describe:\r\n1. Move tests that confirm on the outcome of the first `run`, into nested `describe('basic', () => ... )`  block\r\n2. Reorganize following tests as follows:\r\n   1. If outcome of `run` is tested in context of one `it` block, it can remain to be configured with one `it`\r\n   2. If outcome of `run` is tested with multiple `it` blocks, let's wrap it into `describe`",
        "New tests should be configured purely with `runServerless` util. See: https://github.com/serverless/serverless/tree/main/test#unit-tests\r\n\r\nIn existing file we usually then introduce a new describe block for those tests (and old ones ideally at some point should be refactored to also go there), e.g. check https://github.com/serverless/serverless/blob/741847d2327e87eb95b9a2349642fac5b7e316e1/test/unit/lib/plugins/aws/package/compile/events/cloud-front.test.js#L171\r\n",
        "@joelwalden indeed! Sorry I got fooled by the not common `runServerless` setup. Ignore my comment then",
        "New tests should be configured via `runServerless` util, as e.g. tests in this block: https://github.com/serverless/serverless/blob/5d1c3f7ed4d481619d412b6b453d6eaf94b32b4a/test/unit/lib/plugins/aws/package/compile/events/websockets/index.test.js#L116-L258\r\n\r\nSee also: https://github.com/serverless/serverless/tree/main/test#unit-tests\r\n\r\nIn this test file let's add a new `describe` block, that will host those new tests",
        "Let's write tests with `runServerless` util only (see second part of this test file, and https://github.com/serverless/serverless/tree/main/test#unit-tests",
        "@pgrzesik if they require internet then definitely they should go into integration set (unit tests ideally should be runnable offline)",
        "We have a policy to configure all new tests with [`runServerless`](https://github.com/serverless/test/blob/master/docs/run-serverless.md). We have already plenty of such tests configured.\r\n\r\nI think here it'll be good to rely on [`packaging`](https://github.com/serverless/serverless/tree/master/test/fixtures/programmatic/packaging) fixture, and extend configuration with local plugins that resemble direct file and directory, and confirm that in generated artifact those files are not included",
        "No, I mean unit test (in sense that we can run it offline without dependency on AWS or any other remote instance).\r\n\r\nIdeally if test is constructed with `runServerless` util, and then contents of result artifact is confirmed, in similar way as it is done in our old packaging tests: https://github.com/serverless/serverless/blob/ce66591a688ca734eb34eff929163134d1d21ae8/test/integrationPackage/lambda-files.tests.js#L52 (they should be at some point moved to regular unit tests and refactored to rely on `runServerless`)"
      ],
      "serverless-api-schema-validation-accuracy": [
        "This pattern allows only _object_, while JSON can represent many other types (_array_, _string_ etc.)\r\n\r\nIs it  only _object_ representation that  is supported by AWS?",
        "If you're sure that AWS will crash on non object JSON input (e.g. array, or string), then I think it's fine to keep it. Otherwise I would validate JSON string simply as _string_ without constraints.",
        "Is this correct to support those two formats? AWS docs (https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-events-rule-target.html#cfn-events-rule-target-input) seem to outline only _Valid JSON text passed_ as supported format",
        "Indeed @andyjduncan I missed that it's on our side where we support those three notations.\r\n\r\nEven if it's not clear why we support some of them, to not be breaking I believe we should maintain support for these.\r\n\r\nI think we could improve a bit definition by adding `additionalProperties: false` to `Input.body` case, and add third option for object with whatever properties (then I believe we also need to switch from `oneOf` to `anyOf`, as afaik `oneOf` will fail if more than one option matches)\r\n",
        "@andyjduncan I think that looks good. On our side error will resolve to _unsupported configuration format_, which will not be detailed, but that's probably better than being silent on such structure.",
        "If understand we allow three exact collections. If it's the case then above doesn't meet that, as `AllowedMethods: [\"GET\"]` will pass.\r\n\r\nI believe we can achieve needed validation through `{ enum: [[\"GET\", \"HEAD\"], [... ], [...]] }`",
        "If I see correctly in AWS docs, for SQS maximum is `10` -> https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-eventsourcemapping.html#cfn-lambda-eventsourcemapping-batchsize",
        "In original PR I suggested it to be as:\r\n\r\n```javascript\r\nAllowedMethods: {\r\n  enum: [['GET', 'HEAD'], ['GET', 'HEAD', 'OPTIONS'], ...]\r\n}\r\n```\r\n\r\nhttps://github.com/serverless/serverless/pull/8250#discussion_r490242639\r\n\r\nI have no idea how it went other way, and why I accept it :)\r\n\r\nThis is also way simpler form as proposed now. I believe I tested above proposed approach locally. Can you confirm it works and update to that?",
        "@jede very good point. I didn't realize that. In light of that, I think it's better if we stick to what you've originally proposed"
      ],
      "serverless-promise-error-handling-patterns": [
        "Do we really want to throw here?\r\n\r\nEven if, we should use `ServerlessError` and configure it with human friendly message which will be displayed to user",
        "Question, is do we want here (1) abort the command and present user with the error message then we should throw (but as mentioned above using dedicated interface and with human friendly message)\r\nOr do we want to  (2) just show error stating that given step cannot be accomplish and let command follow with other steps (as e.g. consol integration). In this case we should just log the error, do not set `context.isConsoleDevMode: true`  and let process continue",
        "Let's use regular `Error` as it's a API for plugins, while `ServerlessError` is intended to report errors caused by users's misusage (e.g invalid configuration etc)",
        "Let's reword to `Invalid arguments for variable \"merge\" (expected exclusive list of arrays or plain objects)` and add code `INVALID_VARIABLE_MERGE_PARAMS`",
        "This should be a second callback to previous `then` (otherwise we're catching here eventual programmer error in _first_ then callback, and that can hide important issues)",
        "@fredericbarthelet I was referring strictly to lines as commented (your comment seems to dive into some other area).\r\n\r\nNote the important difference between the outcome of two below snippets:\r\n\r\n```javascript\r\nconst configuredEnvVars = {}\r\nconst potentiallyErrorneousResolution = Promise.resole(\"successValue\");\r\npotentiallyErrorneousResolution.then(resolvedValue => {\r\n  configuredEvnVars[name] = resolvedValue;\r\n}).catch(e => {\r\n  throw new Error(\"Could not resolve environment variable\")\r\n});\r\n```\r\n\r\n```javascript\r\nconst configuredEnvVars = {}\r\nconst potentiallyErrorneousResolution = Promise.resole(\"successValue\");\r\npotentiallyErrorneousResolution.then(resolvedValue => {\r\n  configuredEvnVars[name] = resolvedValue;\r\n}, e => {\r\n  throw new Error(\"Could not resolve environment variable\")\r\n});\r\n```\r\n\r\nUnfortunately erroneous `then(onSuccess).catch(onFailure)` pattern is repeated by most of promise tutorials and you find in many docs. While _if/else_ logic is resembled correctly only by `then(onSuccess, onFailure)`\r\n\r\n`catch(onFailure)` is nothing more than convenient shortcut for `then(null, onFailure)`, and ideally should be seen just as that.\r\n",
        "This should be a _second_ callback to previous `then` (not a new `catch` clause), for reasons outlined here: https://github.com/serverless/serverless/pull/8157#discussion_r481039055\r\n\r\nEventually we can make it as `catch` clause that directly follows `provider.request` above",
        "In case I commented, the only thing I proposed, is to not cover this callback:\r\n\r\n```javascript\r\nif (result) stackData.externalHttpApiEndpoint = result.ApiEndpoint;\r\n```\r\n\r\nwith added error handler.  I don't see how it can produce difference you point in screenshots. By content of callback it seems clear that it's only programmer error that will eventually surface, not AWS request error",
        "To be perfectly clear. I mean to do this:\r\n\r\n```javascript\r\n.then(id => {\r\n  return this.provider.request('ApiGatewayV2', 'getApi', { ApiId: id });\r\n})\r\n.then(result => {\r\n  stackData.externalHttpApiEndpoint = result.ApiEndpoint;\r\n}, e => {\r\n  throw new this.serverless.classes.Error(\r\n    `Could not resolve provider.httpApi.id parameter. ${e.message}`\r\n  );\r\n})\r\n```\r\n\r\ninstead of\r\n\r\n```javascript\r\n.then(id => {\r\n  return this.provider.request('ApiGatewayV2', 'getApi', { ApiId: id });\r\n})\r\n.then(result => {\r\n  stackData.externalHttpApiEndpoint = result.ApiEndpoint;\r\n})\r\n.catch(e => {\r\n  throw new this.serverless.classes.Error(\r\n    `Could not resolve provider.httpApi.id parameter. ${e.message}`\r\n  );\r\n})\r\n```",
        "Let's also wrap bucket name in parenthesis (it's our convention for adding names to error messages)"
      ],
      "serverless-maintain-iam-role-isolation": [
        "Can't we unconditionally set it as:\r\n\r\n```javascript\r\nroleArn = functionResource.Properties.Role;\r\n```\r\n\r\n?",
        "But here we're just referencing some external role, and creating one, right? Shouldn't we then always reference same role to which function is attached (?)",
        "What is the downside of always using the role to which function is attached? We anyway create different scheduler resource for each configured event",
        "It indeed may feel more convenient to simply attach to role which may not require further customisations.\r\n\r\nYet logically I think event, with no exceptions, should work in context of a role which is assigned to function.\r\nIt can be assumed as security breach if suddenly we rely on role that's dedicated for function A, when configuring the resource for function B.\r\n\r\nMany users would prefer to have IAM permissions isolated and configured per function (see https://github.com/serverless/serverless/issues/4313) So now attempting here to reach out of role that's used for function doesn't feel as right direction\r\n\r\n",
        "> However, it's my opinion that the scheduler role should not be the same as the Lambda execution role and should be customizable just like for functions.\r\n\r\nIf there's a use case for it, I think we can make it customisable, but otherwise I believe with no exceptions it should fall back to same role that's used by function"
      ],
      "serverless-maintain-configuration-standards": [
        "Let's also ensure `configValidationMode: error` (as in other service fixtures)"
      ],
      "serverless-use-descriptive-semantic-names": [
        "What's the reason for trailing `_` in the name?\r\n\r\nIf intention is to escape collision with top level var. Let's simply give more specific name, e.g. `groupStatements`",
        "Let's not create variables with trailing `_`",
        "Ideally, if we do not use uppercase variable names for not constructors.\r\n\r\nI know it's a tad of inconvenient here, but it'll be less confusing (by first glance, `Effect` suggests we deal with class constructor while it's not the case)",
        "Let's not use `_` prefixes and postfixes",
        "Let's rename it to `areDuplicateStatements` (mergeable suggests, whether it's possible to merge them with some other statement set, and that feels ok for any statement)",
        "Let's name this variable `shouldMinify`",
        "Let's use lower case.\r\n\r\nI know it'll be inconsistent with other var names, but it's other var names that should be renamed (in JS var starting with capital, suggests function constructor, so it's highly confusing).\r\n\r\nI'll be happy to take in other PR a refactor that renames all those vars to lowercase",
        "Maybe let's name this step `console-enable-dev-mode` ? \"Instrument\" feels very general.",
        "Let's stick to convention where booleans are names as questions. So, as we have `isConsole` (for general console integration) maybe  we should have `isConsoleDevMode` for dev mode integration.",
        "Let's use meaningful names, so `serviceDir` instead of `sDir` (when reading the code later, it's hard to assume  what `sDir` is about)",
        "I think the problem then is, that this test is included in `describe` of which `before` job is not relevant to it?\r\n\r\nIn such case, let's introduce two nested independent describes. \r\n\r\nAlso I think `installPlugin` can be reused among all of them",
        "Let's maintain `const` but maybe name this var: `localPluginPath`, and the other below: `externalPluginPath`",
        "I think it'll be cleaner to take that var in top `'@serverless/utils/log'` require, and name var `isInteractiveTerminal`, (`isIntereactiveSetup` is answering whether it's _interactive setup_ command that's triggered, so it's a different thing, and I don't see a collision or confusion here)",
        "I would name `entryFileRealPath` as `resolvedPath` and `originalPath` as `inputPath`",
        "In platform PR I've suggested that maybe it'll be better to give it a more agnostic name as `didCreateService`, so if eventually later we will support non-CF-related deployments, this name stays accurate.\r\n\r\nWDYT?"
      ],
      "serverless-api-clarity-and-consistency": [
        "As outlined in issue, let's stick to `invokeMode`.\r\n\r\nLet's also support AWS values literally so `RESPONSE_STREAM` and `BUFFERED`"
      ],
      "serverless-consistent-asyncawait-usage": [
        "In this scenario we need to go with regular `promise.then` syntax.\r\n\r\nAlternatively we may first refactor this function to async/await (in other PR), and then go back to this PR and rely on async/await",
        "Once we refactor it this way, ,let's not use `then`. Below logic can simply follow `awat expect..`\r\n\r\n(and let's apply same in other refactored cases)",
        "As this function stands on old bluebird promises. It'll be great to first prepare PR that refactors this to native promises and async/await (with no logic changes), and then this PR should just focus on functionality in question",
        "Thank you 👍 ",
        "Let's in first PR refactor `downloadTemplateFromRepo` function into async/await, and then introduce this change, but using _async_ fs interface",
        "Let's stick to async/await",
        "Let's remove `Promise.resolve(this)` it's ineffective",
        "It's not needed, you can write same as:\r\n\r\n```javascript\r\nasync () {\r\n  return this.getPlugins().then((plugins) => this.display(plugins));\r\n}\r\n```\r\n\r\nand actually we can already refactor to async await as:\r\n\r\n```javascript\r\nasync  list() {\r\n  await this.display(await this.getPlugins());\r\n}\r\n```",
        "If we're refactoring to async/await, let's get rid of `.then`",
        "It'll be nice to fully refactor this method to async/await (just with other commit or PR)",
        "Yes, usually in such cases I think it's good to _precede_ addition of new functionality with commit or PR that refactors updated functionality to async/await.\r\n\r\nAdding async/await syntax to not refactored parts looks confusing."
      ],
      "serverless-configuration-examples-accuracy": [
        "Runtime configuration is not required (we default to `nodejs14.x` when not set in configuration)",
        "`runtimeManagement: auto` has no effect, so I don't there's a point in showing such example (we can just write that it defaults to `auto`, which is the AWS default)",
        "It's technically a shortcut for `${opt:stage, self:provider.stage, \"dev\"}`",
        "Notation as `>=2.33` is an anti-pattern (as it matches also any major greater than 2), so while it's a valid semver range, I would not list that as an example (there's never a valid reason for such range)",
        "> This could be useful especially with v3 when you know your service is compatible with v2 and v3\r\n\r\nThat is best expressed with `2 || 3` (so 4, 5 etc are not recognized as supported)",
        "> it's not that risky since major versions are so far apart in time.\r\n\r\nIt's risky, and just that is a valid concern imo. It's unlikely for users to revisit the range, then, while many services are destined to be in service for years simply in maintenance mode.\r\n\r\nFor same reason, you never put such a range in packages published on npm. We're giving poor's man advice that way.\r\n\r\nAlso I don't understand the argument that `^2.67 || 3` is more \"hard\" to use. It looks very clean and straightforward to me. \r\n\r\n>  The thing is that not everyone is familiar with semver range constraints.\r\n\r\nIt's exactly why, we should provide a future-proof examples, that will prevent users running into issues, long term."
      ]
    },
    "profile": {
      "location": "Warsaw, Poland",
      "blog": "http://www.medikoo.com/",
      "twitter_username": "medikoo",
      "site_admin": false,
      "followers": 1379,
      "following": 40
    }
  },
  "daniel-lxs": {
    "repos": [
      "RooCodeInc/Roo-Code"
    ],
    "entries": [
      {
        "slug": "roo-code-avoid-hardcoded-configurations",
        "title": "Avoid hardcoded configurations"
      },
      {
        "slug": "roo-code-centralize-configuration-constants",
        "title": "Centralize configuration constants"
      },
      {
        "slug": "roo-code-conditional-debug-logging",
        "title": "Conditional debug logging"
      },
      {
        "slug": "roo-code-configure-with-care",
        "title": "Configure with care"
      },
      {
        "slug": "roo-code-consistent-localization-formatting",
        "title": "Consistent localization formatting"
      },
      {
        "slug": "roo-code-document-i18n-string-usage",
        "title": "Document i18n string usage"
      },
      {
        "slug": "roo-code-enforce-api-format-consistency",
        "title": "Enforce API format consistency"
      },
      {
        "slug": "roo-code-enforce-resource-usage-limits",
        "title": "Enforce resource usage limits"
      },
      {
        "slug": "roo-code-extract-reusable-patterns",
        "title": "Extract reusable patterns"
      },
      {
        "slug": "roo-code-extract-shared-code-patterns",
        "title": "Extract shared code patterns"
      },
      {
        "slug": "roo-code-internationalize-all-text",
        "title": "Internationalize all text"
      },
      {
        "slug": "roo-code-maintain-consistent-naming-patterns",
        "title": "Maintain consistent naming patterns"
      },
      {
        "slug": "roo-code-optimize-algorithm-implementations",
        "title": "Optimize algorithm implementations"
      },
      {
        "slug": "roo-code-optimize-react-components",
        "title": "Optimize React components"
      },
      {
        "slug": "roo-code-preserve-error-context-chain",
        "title": "Preserve error context chain"
      },
      {
        "slug": "roo-code-prevent-timeout-race-conditions",
        "title": "Prevent timeout race conditions"
      },
      {
        "slug": "roo-code-prevent-unnecessary-processing",
        "title": "Prevent unnecessary processing"
      },
      {
        "slug": "roo-code-protect-shared-state-access",
        "title": "Protect shared state access"
      },
      {
        "slug": "roo-code-robust-error-handling",
        "title": "Robust error handling"
      },
      {
        "slug": "roo-code-use-structured-logging",
        "title": "Use structured logging"
      },
      {
        "slug": "roo-code-validate-model-capabilities-first",
        "title": "Validate model capabilities first"
      },
      {
        "slug": "roo-code-write-resilient-test-assertions",
        "title": "Write resilient test assertions"
      }
    ],
    "comments": {
      "roo-code-document-i18n-string-usage": [
        "I see that the oversized image error uses i18n properly, but this regular image notice is hardcoded. For consistency, could we move these strings to the translation files as well?\n\nFor example:\n```typescript\nconst noticeText = dimensionsInfo\n  ? t(\"tools:readFile.imageWithDimensions\", { dimensions: dimensionsInfo, size: imageSizeInKB })\n  : t(\"tools:readFile.imageWithSize\", { size: imageSizeInKB });\n```\n\nThis would ensure all user-facing strings are translatable and maintain consistency across the codebase."
      ],
      "roo-code-preserve-error-context-chain": [
        "This error handling pattern for ENOENT is duplicated in multiple places:\r\n- Here in `safeReadFile`\r\n- In `ClineProvider.updateContent`\r\n- In `custom-instructions.ts`\r\n- In `custom-system-prompt.ts`\r\n\r\nIt would be a good idea to consolidate this on a single helper function.",
        "What happens if the base64 data is corrupted or invalid? Would it be worth wrapping this in a try-catch to handle potential errors when constructing the data URL? This could prevent the entire tool response from failing due to a single corrupted image."
      ],
      "roo-code-configure-with-care": [
        "The current implementation uses the settings values without validating whether they're within reasonable bounds. For example, if someone manually edits their settings file and sets a negative number or an extremely large value, it could lead to unexpected behavior.\n\nIt might be worth adding validation like this:\n\n```ts\nconst maxImagesPerResponse = Math.max(1, Math.min(100, state?.mcpMaxImagesPerResponse ?? 20));\nconst maxImageSizeMB = Math.max(0.1, Math.min(50, state?.mcpMaxImageSizeMB ?? 10));\n````\n\nThis helps keep the values within safe operational limits, even in cases where the settings file is corrupted or manually modified.\n\n"
      ],
      "roo-code-prevent-timeout-race-conditions": [
        "Is there a potential race condition here? If the auto-approval timeout fires just as the user is typing/submitting a response, could both the manual response and auto-approval execute? The timeout clearing happens after the condition check, so there might be a small timing window.",
        "Is there a reason the timeout clearing happens inside the `clineAskRef.current` check rather than at the beginning of `handleSendMessage`? If we clear it unconditionally at the start, we could prevent any race conditions where the timeout fires while we're processing the user's input.\n\nThis would also simplify the logic since we wouldn't need to check for specific ask types.",
        "I notice there's a potential race condition here. If the component unmounts while the countdown is active, the interval cleanup happens, but what if the auto-approval in ChatView.tsx is still waiting? Could we consider using a shared cancellation mechanism or ensuring the ChatView's timeout is also cleared when this component unmounts?"
      ],
      "roo-code-optimize-algorithm-implementations": [
        "**Important**: Starting pattern detection from length 1 could cause false positives. For example, \"AAAA\" would be detected as pattern \"A\" repeated 4 times, which overlaps with consecutive repetition detection.\n\nConsider starting from length 2 to avoid this overlap:\n```typescript\nfor (let patternLength = 2; patternLength <= maxPatternLength; patternLength++) {\n```\n\nThis ensures pattern detection only catches actual patterns like \"AB\", \"ABC\", etc., not single repeated elements."
      ],
      "roo-code-avoid-hardcoded-configurations": [
        "The script could use a few updates to make it more reliable and easier to maintain:\r\n\r\n1. **Hardcoded extension ID**: Right now, the extension ID is hardcoded as `rooveterinaryinc.roo-cline`. Instead, it should be built dynamically from the `package.json` to avoid issues if the name or publisher ever changes:\r\n\r\n   ```javascript\r\n   const publisher = packageJson.publisher\r\n   const extensionId = `${publisher}.${name}`\r\n   ```\r\n\r\n2. **Missing VSIX file check**: Before trying to install the VSIX file, it's a good idea to check that it actually exists. Otherwise, the script might fail without a clear reason. Here’s a simple check to add:\r\n\r\n   ```javascript\r\n   if (!fs.existsSync(vsixFileName)) {\r\n     console.error(`VSIX file not found: ${vsixFileName}`)\r\n     console.error(\"Make sure the build completed successfully\")\r\n     process.exit(1)\r\n   }\r\n   ```\r\n\r\nAdding these checks will make the script more robust and easier to work with in the long run.\r\n"
      ],
      "roo-code-extract-reusable-patterns": [
        "The debouncing logic is implemented directly in this component. Have you considered extracting it into a reusable custom hook? This could help improve separation of concerns.",
        "The className logic here is getting quite complex with nested ternaries. Could we simplify this by extracting the condition to a variable?\n\n```tsx\nconst showShareButton = primaryButtonText === t(\"chat:startNewTask.title\") && currentTaskItem?.id;\nconst buttonClassName = showShareButton || secondaryButtonText \n  ? \"flex-1 mr-[6px]\" \n  : \"flex-[2] mr-0\";\n```\n\nThis would make the JSX cleaner and the logic more readable.",
        "The edit mode rendering logic is quite complex with multiple helper functions. Consider extracting the edit mode UI into a separate component for better maintainability:\n\n```typescript\nconst EditModeControls: React.FC<EditModeControlsProps> = ({ ... }) => {\n  // Edit mode specific UI logic\n};\n```\n\nThis would simplify the main component and make the code more modular.",
        "Could you extract this logic into a shared function to avoid duplication? The same pattern appears in `handleSuggestionClickInRow`. Something like:\n\n```typescript\nconst markFollowUpAsAnswered = useCallback(() => {\n  const lastFollowUpMessage = messagesRef.current.findLast((msg) => msg.ask === \"followup\")\n  if (lastFollowUpMessage) {\n    setFollowUpAnswered((prev) => new Set(prev).add(lastFollowUpMessage.ts))\n  }\n}, [])\n```",
        "Since there's timeout logic here as well similar to what's on ClineProvider. Have you considered extracting this into a shared utility function or hook? Something like `useDebouncedFocus()` could handle the timeout.",
        "The `handleSave` function is quite complex with nested conditions and multiple responsibilities. Would it be cleaner to extract the group update logic into a separate utility function?\n\nFor example:\n```typescript\nfunction updateMcpGroupOptions(\n  groups: GroupEntry[],\n  allowedList: string[],\n  deniedList: string[]\n): GroupEntry[] {\n  // Group update logic here\n}\n```\n\nThis would make the code more testable and easier to understand."
      ],
      "roo-code-prevent-unnecessary-processing": [
        "This useEffect runs on every selectedMenuIndex change during keyboard navigation, which could impact performance. Consider debouncing the announcements or only announcing when navigation pauses:\n\n```typescript\nuseEffect(() => {\n  if (!showContextMenu || selectedMenuIndex < 0) return;\n  \n  const timeoutId = setTimeout(() => {\n    // announcement logic here\n  }, 100); // Small delay to avoid rapid announcements\n  \n  return () => clearTimeout(timeoutId);\n}, [showContextMenu, selectedMenuIndex, /* other deps */]);\n```",
        "Consider memoizing the dialog components to prevent unnecessary re-renders:\n\n```typescript\nconst MemoizedDeleteMessageDialog = React.memo(DeleteMessageDialog);\nconst MemoizedEditMessageDialog = React.memo(EditMessageDialog);\n```\n\nThis could improve performance, especially when the parent component re-renders frequently."
      ],
      "roo-code-enforce-resource-usage-limits": [
        "Should we consider implementing a limit on the number of images that can be returned in a single response? Without a limit, a malicious or buggy MCP server could potentially return hundreds of images, causing performance issues. Maybe add a configurable maximum (e.g., 10-20 images) and log a warning if exceeded?",
        "With the increased limit of 500,000 files, have you considered the memory implications? Each FileResult object contains path, type, and label properties. For a project with 500K files, this could consume significant memory (potentially hundreds of MB).\n\nWould it make sense to:\n1. Make this limit configurable via VSCode settings?\n2. Implement streaming or pagination for extremely large file lists?\n3. Add memory usage monitoring/warnings?",
        "The accumulator still stores the entire conversation history, which matches the original behavior, so this isn't a regression. But it could become a memory concern for long-running chats.\r\n\r\nThis PR avoids re-parsing, which is great for performance. Should we try adding a bounded accumulator or sliding window to help with memory usage?",
        "While the accumulator is reset before each `attemptApiRequest`, consider adding a maximum size check as a safety measure against edge cases where the parser might be reused without proper reset:\n\n```typescript\nprivate readonly MAX_ACCUMULATOR_SIZE = 1024 * 1024; // 1MB limit\n\nprocessChunk(chunk: string): AssistantMessageContent[] {\n    if (this.accumulator.length + chunk.length > this.MAX_ACCUMULATOR_SIZE) {\n        throw new Error('Assistant message exceeds maximum allowed size');\n    }\n    this.accumulator += chunk;\n    // ...\n}\n```\n\nThis would prevent potential memory issues if the parser state isn't properly managed in all code paths.",
        "Should we consider adding a maximum draft size limit just in case? localStorage typically has a 5-10MB quota, and very large drafts could potentially cause issues. What do you think about truncating or warning when drafts exceed a reasonable size (e.g., 100KB)?"
      ],
      "roo-code-conditional-debug-logging": [
        "I noticed some `console.log` and `console.debug` statements here. Are these intended for debugging and should they be removed before merging?"
      ],
      "roo-code-internationalize-all-text": [
        "All these announcement strings need to be translated. Consider creating a helper function that uses the translation system:\n\n```typescript\nconst getAnnouncementText = (option: ContextMenuQueryItem, index: number, total: number) => {\n  const position = t(\"chat:contextMenu.position\", { current: index + 1, total });\n  \n  switch (option.type) {\n    case ContextMenuOptionType.File:\n    case ContextMenuOptionType.OpenedFile:\n      return t(\"chat:contextMenu.announceFile\", { \n        name: option.value || option.label, \n        position \n      });\n    // ... other cases\n  }\n};\n```",
        "This instruction text should also be translated:\n\n```typescript\n<div id=\"context-menu-instructions\" className=\"sr-only\">\n  {t(\"chat:contextMenu.instructions\")}\n</div>\n```"
      ],
      "roo-code-protect-shared-state-access": [
        "I noticed that while we check `supportsImages` at the beginning of the function (line 147), this value could theoretically change if the model is switched during execution. Should we consider moving this check closer to where we actually decide to include images?\n\nThe current implementation is likely fine for most cases, but for extra safety, we could store the model info check result and use it consistently throughout the function execution.",
        "I noticed a potential race condition here. The model info is fetched during message streaming, which could cause issues if multiple concurrent requests are made before the cache is populated. \n\nWould it be safer to fetch and cache the model info during handler initialization or before starting the stream? This would ensure consistent context window information across concurrent requests.",
        "The `modifyConversation` method attempts to acquire locks on two different files through nested calls to `modifyClineMessages` and `modifyApiConversationHistory`. This pattern could lead to:\n\n1. **Deadlocks** if another process tries to acquire these locks in reverse order\n2. **Data inconsistency** if one modification succeeds but the other fails\n\nIs this intentional? The proper-lockfile documentation recommends against holding multiple locks simultaneously. Consider either:\n- Using a single lock file for both operations\n- Implementing a two-phase commit pattern\n- Documenting why this approach is safe in your specific use case",
        "I noticed a potential edge case in the error handling here. If `ripgrepOperationPromise` fails and we set it to null (line 84), but another concurrent call comes in before line 90, it might start a new operation while the first caller is still in the catch block.\n\nCould this be addressed by setting `ripgrepOperationPromise = null` after the entire try-catch block completes? Or perhaps using a more robust state management approach?",
        "Is there a potential race condition here? The draft is only restored if `!inputValue`, but what happens if the component receives an initial value before the message handler is set up? Could we miss restoring a valid draft in that case?",
        "I'm a bit concerned about potential race conditions here. You're using a 100ms timeout in the provider while ChatView uses 50ms. If someone rapidly switches between windows, multiple focus events could queue up and cause unexpected behavior.\n\nMaybe we could use the same timeout duration in both places? Or even better, implement a proper debounce mechanism to handle rapid focus changes more gracefully.",
        "Good that you're clearing the timeout in dispose(), but what happens if the view gets disposed while a timeout is still pending? The timeout would still fire and try to access this.view which might be disposed.\r\n\r\nIt might be a good idea to add a check in the timeout callback to ensure the view still exists and hasn't been disposed before trying to post a message to it."
      ],
      "roo-code-optimize-react-components": [
        "Have you considered using a simpler state management approach here? Instead of tracking all answered follow-ups in a Set, you could track just the current active follow-up question's timestamp and clear it when answered. This would avoid the need to manage a growing collection of timestamps.\n\nFor example:\n```typescript\nconst [currentFollowUpTs, setCurrentFollowUpTs] = useState<number | null>(null)\n```\n\nThen check `message.ts === currentFollowUpTs` instead of using a Set."
      ],
      "roo-code-use-structured-logging": [
        "We should clean this up before merging.\r\n\r\nThe `console.info` statements currently in the code (including the one at line 144 where `finalError` is logged) should be removed. If logging is still needed, consider using a proper logging service instead of `console` calls.",
        "I notice the error logging here uses different levels - `console.info` for connection failures (line 90) vs `console.warn` for other errors (line 92). In contrast, the LM Studio fetcher uses `console.error` for connection failures. Would it make sense to standardize the logging approach across both fetchers?"
      ],
      "roo-code-validate-model-capabilities-first": [
        "Is it intentional that we're returning image data without checking if the current AI model supports images? I noticed that `maybeRemoveImageBlocks` in `src/api/transform/image-cleaning.ts` checks `apiHandler.getModel().info.supportsImages` before processing images. Should we add a similar check here to prevent sending image data to models that can't process it?\n\nFor example, we could check the provider's capability before including images in the response:\n```typescript\nconst provider = await cline.providerRef.deref();\nconst supportsImages = provider?.apiHandler?.getModel()?.info?.supportsImages ?? false;\nconst imagesToInclude = supportsImages ? allImages : [];\n```",
        "The default dimension is set to 768 for `gemini-embedding-exp-03-07`, but this model supports dimensions of 3072, 1536, and 768. Is 768 the intended default? The smaller dimension might impact embedding quality.\n\nAlso, would it be helpful to add validation somewhere to ensure only valid dimensions (3072, 1536, or 768) are accepted for this model?"
      ],
      "roo-code-maintain-consistent-naming-patterns": [
        "I noticed the naming pattern inconsistency here. You've renamed `MAX_SEARCH_RESULTS` to `DEFAULT_MAX_SEARCH_RESULTS` (which is good!), but `SEARCH_MIN_SCORE` doesn't follow the same pattern.\n\nFor consistency, should we consider renaming `SEARCH_MIN_SCORE` to `DEFAULT_SEARCH_MIN_SCORE`? This would make it clearer that both are default values that can be overridden by user configuration."
      ],
      "roo-code-enforce-api-format-consistency": [
        "I noticed that `.avif` is included in `SUPPORTED_IMAGE_FORMATS` (line 44) but there's no corresponding MIME type mapping here. This means AVIF images will default to `image/png` which isn't correct.\n\nCould we add:\n```typescript\n\".avif\": \"image/avif\",\n```\n\nto ensure AVIF images are properly identified?",
        "The URL detection pattern might be too broad. Could a URL like `https://api.example.com/v1/embeddings-service` or `https://api.example.com/deployments-info/v1` be incorrectly classified as a full endpoint URL?\n\nWould it be safer to use more specific patterns? For example:\n- Check if the URL ends with `/embeddings` (with or without query params)\n- Use a regex pattern like `/deployments/[^/]+/embeddings` for Azure-style URLs\n\nThis would reduce the risk of false positives while maintaining compatibility with the intended services.",
        "The description says \"milliseconds\" but the value is actually stored in minutes and converted to milliseconds in the handler. This is inconsistent with the other timeout descriptions and could cause confusion. Also, the `.describe()` method isn't used elsewhere in the schema.\n```suggestion\n\topenAiApiTimeout: z.number().optional(),\n```",
        "The current implementation converts messages to plain text format, but this seems inconsistent with the original approach.\r\n\r\nThe comment on line 117 states \"Claude CLI doesn't accept JSON messages\", but looking at the [original implementation pattern](https://github.com/cline/cline/pull/4111/files#diff-c922b8fdacba11f1ecef526a5223718b0ea2c9d5aea01c653b1fd7afddea2ca5R23), it appears the CLI should accept JSON-formatted messages. The current text conversion approach:\r\n\r\n1. Loses message structure and metadata\r\n2. Makes it harder to handle complex message types\r\n3. Differs from how other providers handle messages\r\n\r\nCould you verify if the Claude Code CLI actually requires text format? If it does accept JSON, consider reverting to:\r\n```typescript\r\nconst args = [\r\n  \"-p\",\r\n  JSON.stringify(messages),\r\n  \"--system-prompt\",\r\n  systemPrompt,\r\n  // ... other args\r\n]\r\n```\r\n\r\nThis would also simplify the validation logic since JSON.stringify handles escaping automatically.",
        "The current implementation converts messages to plain text format, but this seems inconsistent with the original approach.\n\nThe comment on line 117 states \"Claude CLI doesn't accept JSON messages\", but looking at the [original implementation pattern](https://github.com/cline/cline/pull/4111/files#diff-c922b8fdacba11f1ecef526a5223718b0ea2c9d5aea01c653b1fd7afddea2ca5R23), it appears the CLI should accept JSON-formatted messages. The current text conversion approach:\n\n1. Loses message structure and metadata\n2. Makes it harder to handle complex message types\n3. Differs from how other providers handle messages\n\nCould you verify if the Claude Code CLI actually requires text format? If it does accept JSON, consider reverting to:\n```typescript\nconst args = [\n  \"-p\",\n  JSON.stringify(messages),\n  \"--system-prompt\",\n  systemPrompt,\n  // ... other args\n]\n```\n\nThis would also simplify the validation logic since JSON.stringify handles escaping automatically."
      ],
      "roo-code-consistent-localization-formatting": [
        "No, that translation is correct since it's a placeholder to signify the mode"
      ],
      "roo-code-extract-shared-code-patterns": [
        "I notice the GC logic is duplicated between lines 636-658 and 674-694. Could we extract this into a separate method to follow DRY principles? Something like:\n\n```typescript\nprivate static async runBackgroundGC(git: SimpleGit, branchName: string): Promise<void> {\n    try {\n        this.log(`[${this.name}#deleteBranch] Running gc --prune=now after deleting branch ${branchName}`);\n        git.raw([\"gc\", \"--prune=now\", \"--quiet\"])\n            .then(() => {\n                this.log(`[${this.name}#deleteBranch] Background gc --prune=now completed for branch ${branchName}`);\n            })\n            .catch((gcError) => {\n                this.log(`[${this.name}#deleteBranch] ERROR: Background gc after deleting branch ${branchName} failed: ${gcError instanceof Error ? gcError.message : String(gcError)}`);\n            });\n    } catch (e) {\n        this.log(`[${this.name}#deleteBranch] ERROR: Failed to initiate gc: ${e instanceof Error ? e.message : String(e)}`);\n    }\n}\n```",
        "There's significant code duplication between this `importConfigFromPath` function and the existing `importSettings` function in `src/core/config/importExport.ts`. \n\nCould we refactor to share the common import logic? Perhaps extract a shared function that both can use, something like:\n\n```typescript\nexport async function importConfigFromData(\n  data: unknown,\n  options: ImportOptions\n): Promise<{ success: boolean; error?: string }>\n```\n\nThis would reduce maintenance burden and ensure consistent behavior between manual and automatic imports."
      ],
      "roo-code-robust-error-handling": [
        "The error handling logic looks good, but could this be more robust? Currently it falls back to string conversion for non-Error objects. Would it be helpful to also capture additional context like the component stack or timestamp when the error occurred?",
        "The JSON.parse() here could throw an error if lastMessage.text contains invalid JSON. Could we wrap this in a try-catch block to handle potential parsing errors gracefully?\n\n```typescript\nlet followUpData;\ntry {\n  followUpData = JSON.parse(lastMessage.text || \"{}\")\n} catch (error) {\n  console.error('Failed to parse follow-up data:', error);\n  return;\n}\n```"
      ],
      "roo-code-centralize-configuration-constants": [
        "UI consideration: The component shows 8192 as the default value when no value is set. Would it be clearer to show the model's actual max tokens instead?\n\n```typescript\nconst displayValue = value ?? modelInfo?.maxTokens ?? 8192\n```\n\nThis would give users better context about what the model actually supports before they override it.",
        "I'm curious about why this is needed? I think `setApiConfigurationField` should handle these state changes by itself without having to process them manually."
      ],
      "roo-code-write-resilient-test-assertions": [
        "The test verifies Windows path conversion logic but doesn't actually test that the Windows code path uses WSL or that temporary files are created and cleaned up. Would it be valuable to add more comprehensive tests that mock the `execa` call and verify the correct command is executed with WSL on Windows?",
        "Hey, @Githubguy132010\r\nI agree, we shouldn't mock the very thing we are trying to test. Can you rewrite the test to help us properly test your implementation.\r\n\r\n"
      ]
    },
    "profile": {
      "location": "Colombia",
      "blog": "",
      "site_admin": false,
      "followers": 34,
      "following": 3
    }
  },
  "SomeoneToIgnore": {
    "repos": [
      "alacritty/alacritty",
      "zed-industries/zed"
    ],
    "entries": [
      {
        "slug": "alacritty-use-constraining-types",
        "title": "Use constraining types"
      },
      {
        "slug": "zed-background-process-blocking-operations",
        "title": "Background process blocking operations"
      },
      {
        "slug": "zed-choose-domain-specific-semantic-names",
        "title": "Choose domain-specific semantic names"
      },
      {
        "slug": "zed-consider-algorithmic-complexity",
        "title": "Consider algorithmic complexity"
      },
      {
        "slug": "zed-contextualize-dont-panic",
        "title": "Contextualize don't panic"
      },
      {
        "slug": "zed-design-interfaces-not-implementations",
        "title": "Design interfaces, not implementations"
      },
      {
        "slug": "zed-document-configuration-clearly",
        "title": "Document configuration clearly"
      },
      {
        "slug": "zed-document-configuration-constraints-clearly",
        "title": "Document configuration constraints clearly"
      },
      {
        "slug": "zed-hierarchical-configuration-organization",
        "title": "Hierarchical configuration organization"
      },
      {
        "slug": "zed-minimize-credential-exposure-lifetime",
        "title": "Minimize credential exposure lifetime"
      },
      {
        "slug": "zed-prefer-idiomatic-option-handling",
        "title": "Prefer idiomatic Option handling"
      },
      {
        "slug": "zed-prefer-rust-structural-patterns",
        "title": "Prefer Rust structural patterns"
      },
      {
        "slug": "zed-protect-render-loop-performance",
        "title": "Protect render loop performance"
      },
      {
        "slug": "zed-respect-language-specific-conventions",
        "title": "Respect language-specific conventions"
      },
      {
        "slug": "zed-scope-dependencies-appropriately",
        "title": "Scope dependencies appropriately"
      },
      {
        "slug": "zed-self-explanatory-identifier-names",
        "title": "Self-explanatory identifier names"
      },
      {
        "slug": "zed-standardize-platform-agnostic-configuration",
        "title": "Standardize platform-agnostic configuration"
      },
      {
        "slug": "zed-test-through-public-apis",
        "title": "Test through public APIs"
      }
    ],
    "comments": {
      "zed-minimize-credential-exposure-lifetime": [
        "Also, adding a code that explicitly stores credentials somewhere seems somewhat scary.\nI get it that we do `drop(askpass);` but wondering if we could somehow extract this all into:\n\n```rs\nlet (askpass, askpass_rx) = ....\n....\n\nlet Some(password) = askpass_rx.next().await else {...};\nlet socket = SshSocket::new(connection_options, &temp_dir, password)?;\n```\n\nthis way, nothing related to password ever gets stuck in memory for sure.\n\nGiven good Mikayla's ideas about deduplicating the logic with the posix impl and the fact that it managed to work without storing passwords in fields, seems reasonable?",
        "So, this is a password that we write into a file and never control the lifecycle of that tmp file.\nHow adequate and safe this is?\n\nIs there a way where we do not store the password in files and \"hardcoded\" memory such as fields?\nHaving some `rx` oneshot seems ok? \nThen we can pass that through as a \"askpass reply result\" and call once, where the password is needed?\nOr, is it possible complicate the `SSH_ASKPASS` script so that it gets the input from Zed? \nI'm sort of surprised that ssh itself cannot handle its askpass matters internally with the user."
      ],
      "zed-background-process-blocking-operations": [
        "Nope, alas.\r\n\r\n1. To start with, `pull_diagnostic` has some `.detach()` inside, so here we spawn a task that calls a synchronous function that spawns a task.\r\nThis is sort of noop and odd (even the name is odd, as it actually updates the buffer with the new diagnostics).\r\n\r\nI think it's better to have `pull_diagnostic` to return `Task<anyhow::Result<Vec<Diagnostics>>>` and inside, it will either call to LSP or to protobuf (in the remote client side), similar to how inlays are doing this.\r\n\r\n2. Then, somewhere on this level, we'll have to handle the debounces and tasks better.\r\nRight now, every time we type, we spawn a task that waits for a debounce time, then queries for diagnostics and applies them to the buffer.\r\n\r\nSo, the debounce at its current form does nothing but the delay of the potential query + edit cascade.\r\n\r\nI think the whole approach of placing the pull code here might be a be a bit off-pace.\r\nInstead, we can alter `editor.rs` and adjust around the https://github.com/zed-industries/zed/blob/d53a86b01dd3d02980938cbce1bfd74e35901dda/crates/editor/src/editor.rs#L12167 line.\r\n\r\nWe can store another task and do debounces in it first, similar to what `_scroll_cursor_center_top_bottom_task` has: https://github.com/zed-industries/zed/blob/d53a86b01dd3d02980938cbce1bfd74e35901dda/crates/editor/src/scroll/actions.rs#L77-L87\r\n\r\ndoes, but query LSP store for diagnostics instead after the timeout.\r\nThen, update the diagnostics similar to what applying the diagnostics push + calling `refresh_active_diagnostics` does today.\r\n\r\nWe're lucky that buffer, before updating its diagnostics, checks `diagnostics_timestamp` that it's a newer set.",
        "One thing the new model implies though, is the need to explicitly query multiple servers.\r\nCurrent code sets up the listeners per language server, so what looks like a single LSP query can be N (e.g. tailwind-css project that will have TS langserver + tailwind langserver + maybe prettier/biome and/or eslint servers on top).\r\n\r\nThe new way will require more code and will make it more explicit with `MultiLspQuery`, but maybe it's even better as simpler to understand?\r\n\r\n",
        "This is the core place, combining multiple new feedback from this review.\r\n\r\nhttps://github.com/user-attachments/assets/99f8f320-2b24-44cc-a926-9ef928a08fa2\r\n\r\nInteresting, I've intuitively expected that `clear` will cause a lot more issues with flickering, if the diagnostics retrieval is slow due to the amount of the diagnostics, but even on the large example it's not that bad.\r\n\r\nI think we're saved here by the fact that Zed already has issues with large amounts of diagnostics, hence we're not seeing the new ones.\r\nI would propose still, to rework this a slightly:\r\n* accumulate new state (`inline_diagnostics`) first, and mutate the state it in the very end of the task, once\r\n* pass everything that does not depend on `self` and `cx` through `cx.background_spawn(async move { ... }).await` \r\n\r\nCombined, something like\r\n```rs\r\nlet new_inlay_hints = cx.background_spawn(async move {\r\n    let mut new_inlined_diagnostics: Vec<(Anchor, InlineDiagnostic)> = Vec::new();\r\n    let mut prev_diagnostic_line = None;\r\n    for diagnostic in diagnostics {\r\n        //........\r\n        new_inlined_diagnostics..binary_search_by(|probe| {\r\n            diagnostic_anchor.cmp(&probe.0, &buffer)\r\n        });\r\n        //........\r\n    }\r\n}).await;\r\n//........\r\nself.inline_diagnostics = new_inline_diagnostics;\r\n```\r\n\r\nThis way, we'll be usually on a background thread, debounced or computing, cancelled on concequent requests.\r\nOld state will be kept, and Anchor (from Editor) -> DisplayPoint (when laying out) conversion will keep the diagnostics placed on the right lines when rendering, even after adding newlines or undoing.\r\n\r\nLarge diagnostics sets to process will cause more stale diagnostics text during fast editing, but as a somewhat inevitable trade-off, which does not flicker at least."
      ],
      "zed-prefer-idiomatic-option-handling": [
        "```suggestion\r\n            .is_some_and(|popover| popover.signature.len() > 1)\r\n```\r\n\r\nNIT",
        "> `if let Some`\r\n\r\nThis can be just `?`",
        "Here, we'd rather use the `to_string_lossy()` method, to try and show something for invalid UTF-8.",
        "`?`",
        "No, I mean for the 3rd time in a row, this is a method that returns `Option`, so you can `?` instead of `if let Some`",
        "> .unwrap();\r\n\r\nDo not ever use that, please.",
        "NIT: we could use `view.read(cx).active_editor.as_ref().is_some_and(` instead."
      ],
      "zed-prefer-rust-structural-patterns": [
        "NIT: we can do `if snapshot.mode != EditorMode::Full || scrollbars_layout.vertical.is_none()` with an early `return None` to avoid nested code and improve the readability.",
        "```suggestion\r\n```\r\n\r\nCan also do `.get_diagnostics(server_id).into_iter().flat_map(...` and avoid any extra nesting and `if let Some`.",
        "```suggestion\r\n            signature_help_task: None,\r\n```\r\n\r\nNIT: let's keep the compiler a bit less busy solving traits for simple cases? Same for the other one.",
        "```suggestion\r\n            let Some((buffer, buffer_position)) = self.buffer.read(cx).text_anchor_for_position(position, cx) else { return; }\r\n```\r\n\r\nNIT: we can destructure it right away, so that will be one less block to nest.",
        "NIT: that could be destructured in-place, without method field calls, right in the parameter declaration:\r\n\r\n```\r\npub fn create_signature_help_markdown_string(\r\n    SignatureHelp {\r\n        signatures,\r\n        active_signature,\r\n        active_parameter,\r\n    }: SignatureHelp,\r\n) -> ..."
      ],
      "zed-protect-render-loop-performance": [
        "It's not superb that we have to use this method during __rendering__ , that might happen fast, e.g. 120 times/second: and we loop over outlines in a potentially large file.\r\n\r\nWhat's more odd, is that we call `render_outline(` in a loop over every item to render, so should know whether the outline has children or not.\r\n\r\nSeems that we can rewrite it into something better.",
        "`show_diagnostics_inline` is what alters the rendering logic, so this code will \"blink\" the diagnostics, if the delay is long enough: e.g for `\"delay_ms\": 500` after an edit, the diagnostics will stop rendering and editor will \"blink\", showing none at all first and 500+Nms later coming up with the new diagnostics.\r\n\r\nWhat if, instead, we rework the approach a bit?\r\nWe'll keep the only `show_diagnostics_inline` in `Editor` (altered via project settings), remove `show_diagnostics_inline_enabled` and altering the delay logic similar to https://github.com/zed-industries/zed/pull/21463 ?\r\n\r\nThe idea is to have a single \"is it enabled?\" flat on the editor level which allows updating the collection of diagnostics to render (can be stored in the editor's element or editor itself).\r\nWhen the flag goes down, the collection is cleared and not updated until the settings change and re-enable the flag.\r\n\r\nThe diagnostics' update happen after buffer edits and on special events (e.g. editor creation), debounced (50ms default seems ok?), so we do not recalculate diagnostics needlessly on quick user typing (current part of the config seems ok syntaxically, but needs a bump from 0ms).\r\n\r\nIf there's something in the collection, it gets rendered straight away, without any complex computations.",
        "Seems like a good summarization, I indeed propose to do less work on each frame, in particular\r\n\r\n> grabs what should be currently visible, does a bit of math to translate from buffer coordinates to display coordinates, and then spits out whatever those flex box thingers are collectively known as, and paints them on screen\r\n\r\nspecifically \"does a bit of math\" part is very concerning to me: we have ~16.67ms per frame at best, and potentially very large documents with very large amount of errors, consider scenarios like project.rs from this repo which will have a particular `use` statement(s) malformed (which will trigger a lot of errors).\r\n\r\nWe also do sorting with `sorted_by_key` and all those coordinate transformations are logarithmic at best.\r\n\r\nI just cannot fathom any reason to do that bit of math if we can avoid doing that.\r\nSure, this can wait until other fixes, but has to be made eventually.",
        "This is a code with a lot of tree traversals (`display_point_to_point`, `point_to_display_point` and other coordinate transform, `diagnostics_in_range` lookup) and various allocations and whatever sorting on top.\r\n\r\nAll that has a lifecycle of `let diagnostics = self.gather_inline_diagnostics(`, and `layout.inline_diagnostics.drain()` on the other end, so sort of thrown away.\r\n\r\nThis all is done on each frame* potentially 120 times/second, and to me it seems we can try and rewrite the logic so that it scales better: traversing thousands of diagnostics with various tree-manipulations on the side does not seem fun to fit into a frame.\r\n\r\nI've mentioned a proposal in the editor.rs part: we might want to debounce any attempt to update the diagnostics data (unless it's a data purge after disabling the diagnostics); keep the old state as long as possible, swapping it to the new one which is calculated after an edit; keep renrendering that part",
        "Just to be more specific, I've got a file with a lot of diagnostics.\r\n\r\n[rust-analyzer-repro-long-diagnostics.zip](https://github.com/user-attachments/files/18411900/rust-analyzer-repro-long-diagnostics.zip)\r\n\r\nand there, I've opened the file both in `Nightly` and my local `--release` build of this branch.\r\nWaited until all diagnostics are shown, and started scrolling through the file, nothing else, profiling.\r\n\r\nBoth versions behave poorly, but the traces show drastic difference in the reasons to do so:\r\n\r\nNightly has some line layout and tree-sitter-query-related issues:\r\n![Nightly](https://github.com/user-attachments/assets/565631de-1b6a-41aa-8a24-0146f35fa880)\r\n\r\nThe branch build does all that diagnostics-related work including extra allocations, not explicitly mentioned in the comment above:\r\n![Dev](https://github.com/user-attachments/assets/9cc29382-9bca-4658-8c61-71a4e762d4b5)",
        "After new design, numbers are back to something comparable with the rest of the code for the same exaggerated example:\r\n\r\n![image](https://github.com/user-attachments/assets/72ce4f9d-1d73-427e-96be-8641b6d61ada)\r\n\r\nand the heaviest stacktrace is related to layouting again.",
        "> .exists()\r\n\r\n1. We prefer to avoid using `std::fs`-related methods and use `Fs` instead, https://github.com/zed-industries/zed/blob/5e210c083fc4d1a00c9a7dac7abf185351bb9b3e/crates/fs/src/fs.rs#L112 in this case.\r\n\r\n2. This is a rendering code, potentially called 120 times/second, calling whatever FS-related things here is madness.\r\nIs there another way we can check that file is outside of projects?\r\n\r\n3. We can be on remote, where the client will have no FS at all."
      ],
      "zed-choose-domain-specific-semantic-names": [
        "It's not just the content but some way to launch askpass it seems?\nLet's name it differently to ensure we do not pass any passwords or any other sensitive things, at least on the semantics, naming level.\n\n```suggestion\n        let askpass_script = format!(\n```\n\nAlso later, we store this into `askpass` field which seems worth of having the same name.",
        "1. This method is used only once, so let's inline it instead.\r\n\r\n2. I like how `active` word is used here though, let's rename `FocusedPane` into `Active???`\r\nAnd let's uniformy state what the PR tries to do: here, it uses `Pane` but the title of the PR is \r\n> Add setting for minimap on active buffer only\r\n\r\nI think it's the latter based on the video and the discussion, so we can use\r\n\r\n`MinimapDisplayScope::ActivePane`\r\n\r\nBut then, `MinimapDisplayScope` is also wordy, can we do `Display` or `DisplayIn` or similar?\r\n\r\nAs a last naming thought, what if we use `editor` instead of `pane` in the new code?\r\nThen it will be `DisplayIn::AllEditors | ActiveEditor` and users won't have to guess what a `pane` is, is it a `panel` or not.",
        "```suggestion\r\n    pub fn semantic_tokens(&self) -> &Arc<SemanticTheme> {\r\n```\r\n\r\n`tokens` is quite broad, esp. in a text editor domain, so let's specify which tokens are these, as LSP case is relatively niche."
      ],
      "zed-contextualize-dont-panic": [
        "```suggestion\n        fs::write(&askpass_script_path, askpass_script).await.with_context(|| format!(\"Creating askpass script at {askpass_script_path:?}\"))?;\n```\n\nNot sure about `smol`, but stdlib's FS errors are notorious to not include a flie path.\n\nE.g. if script's parent directory is not created in an analogous method, the error returned would be smth. like `Err(\"not a directory\")`.\nTo be on a safe side, seems worth to add some context.",
        "I'm concerned by the amount of new `.unwrap()`, `.expect(..)` in the new code, esp. given that this function returns `Result`.\r\nLet's not write the code that may panic here, as it's quite a core feature, queried frequently, and we're parsing things from a potentially malicious server.",
        "We can remove this nesting with\r\n\r\n```rs\r\nuse anyhow::Context as _;\r\n\r\nlet (socks_proxy, version) = parse_socks_proxy(proxy).with_context(|| format!(\"Parsing proxy url '{proxy}'\"))?;\r\n// or use `.context(\"parsing proxy url\")?` if `url` is not secure to print due to the risk of leaking the credentials.\r\n```\r\n\r\nand might be good to do `.context` on more `?` around, such as `TcpStream::connect` or maybe even `.map_err(|err| anyhow!`?",
        "I would prefer `.context`/`.with_context` over `.map_err` and `?` usage anyway as indeed, mostly the Zed \"code style\" and the type system being rather explicit itself already. ",
        "I think it's fine, thank you for the rest of the fixes.",
        "I understand that we did a Python regex match and should not fail here, but the logic may have flaws (or gain, with later contributions) causing None to be returned here.\r\n\r\nFor the sake of less panics due to unwrap/expect's in \"prod\" code, let's replace it with `log_err()?` or, `debug_panic!` if you want to pay more attention to this in debug builds.\r\n\r\nI also think we can avoid this entirely, and redo this `else if let` into `else`, then get the `let file_line` and use the regular Rust's regex (see how unit tests do that with `fn re_test` now) to match the rest of the cases, with capture values extractions?"
      ],
      "zed-self-explanatory-identifier-names": [
        "```suggestion\r\n      \"cmd-alt-=\": \"workspace::IncreaseDocksSize\",\r\n```\r\n\r\n* We have a way to define parameters in the keymap:\r\n\r\nhttps://github.com/zed-industries/zed/blob/0731097ee5780b3569980d7ba93f8fcf4eee097d/assets/keymaps/default-macos.json#L40\r\n\r\nlet's use this form for every new action that has parameters: this way we'll make it more obvious to the users that the parameter exists (the do not usually see the code), and document the default.\r\n\r\n* Let's also remove that `ByOffset` suffix from everywhere, as the `offset` parameter is clearly showing this, and `Open` part, for brevity.\r\n\r\nMaybe `Size` can also go away?",
        "* WDYT on `workspace::Increase/DecreaseActiveDock`, `workspace::Increase/DecreaseDocks` and `workspace::ResetDocks` as names?\r\n\r\n* We do have to write new entries similar to that `IncreaseBufferFontSize` example above and a comment, what `0` as a default will do.",
        "What about the previous name, `\"min_line_number_digits\"`?\r\n\r\nNot sure what a \"line number base\" is, but feels about a single line number?"
      ],
      "zed-document-configuration-clearly": [
        "```suggestion\r\n    \"context\": \"Editor && can_scroll_signature_help\",\r\n```\r\n\r\nSeems that we can simplify this by giving a more readable name and only adding this context when no completions/whatever else conditions.\r\nWDYT?",
        "This 6 is somewhat confusing.\r\nIs it a default? Then we can uncomment the value.\r\nIs it a minimum allowed? Then we can explain this fact."
      ],
      "zed-scope-dependencies-appropriately": [
        "This changes the behavior for all crates in the project, seems like an overkill for a dev build-related change.\n\nIf it intends a global change, let's extract it into a separate PR and keep this one scoped on a local dev workflow change.\nFor that, I imagine we need to add a new [feature](https://doc.rust-lang.org/cargo/reference/features.html) right inside the `remote_server` crate and use that feature when doing a dev build.",
        "IIRC you have to post this change (with another feature) right inside the `remote_server/Cargo.toml` and see if it brings the right results with `cargo tree` or your experiments.\r\nFeatures are additive, so this should work."
      ],
      "zed-document-configuration-constraints-clearly": [
        "> ///\r\n\r\nLet's remove that and mention ranges around, where it matters.",
        "More like [0.0, 1.0] range that's not anywhere here and in the `///` docs too, just in the json defaults?\r\n\r\nWhat's up with negatives, btw, are those clamped to 0.0?",
        "```suggestion\r\nNon-negative `float` values\r\n```\r\n\r\nhttps://github.com/zed-industries/zed/blob/f19e1e3b5f23ec7f64b692f4530825e0e50e5b95/crates/workspace/src/pane_group.rs#L1138-L1139",
        "* Can we use a diagnostics level here? \r\nLSP defines that there are 4 types of these: https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#diagnosticSeverity and a single boolean cannot cover them all.\r\n\r\nI think it's better to be flexible and allow users to show all or almost none of them, if the want to.\r\nWe could also move that into `inline` part of the settings?\r\n\r\n* If we do that, I wonder how useful `primary_only` would be: even now it seems redundant, have you found it useful?\r\n\r\nI feel that we remove `use_rendered` unless you have a good reason to leave it be, if we remove `primary_only`, we'll have only 3 settings knobs: on/off, diagnostics level, and diagnostics interactivity.\r\nThat set looks quite small and good to me.",
        "This is the best part of the impl to me, honestly, I wonder if we could \r\n* move it into `inline` as this settings knob is related to this part of the functionality only? (apart from the fact that it spoils f8 navigation sometimes 🙂, see another comment )\r\n* enable it by default, and explain that when disabled, same effect could be achieved with f8/shift-f8 or the new action (if we decide to leave it)",
        "I think we now have a very good set of settings and descriptions inside the default.json above, and we need to update this section respectfully.\r\n\r\nI agree with the workflow, where all toggling is made by existing f8/shift-f8 , to potentially jumpy autotoggling or special actions are needed.\r\n\r\nOne wish I used to voice in another related discussion here: we can add `\"minimum_level\"` knob `\"inline\"` section, to allow users to show only errors, only errors + warnings, etc.\r\nSimilar to people not fond of flickering, there is a notable group of people who are not fond of many popovers/colors around their code and it seems simple to add another `.filter` inside `update_inline_diagnostics` to support that + new diagnostics re-queried after level change."
      ],
      "zed-respect-language-specific-conventions": [
        "One more small change here, we need to return that trailing newline back."
      ],
      "alacritty-use-constraining-types": [
        "There seems to be no `LPDWORD` or even `DWORD` type in the `windows_sys` crate, and looks like tjeu use `*mut u32` directly:\r\n\r\n* https://docs.rs/windows-sys/latest/windows_sys/Win32/System/Threading/fn.GetExitCodeProcess.html\r\n\r\n* https://github.com/microsoft/windows-rs/blob/1f40da8ffc44adba3f9a866287108e80223d0a81/crates/libs/sys/src/Windows/Win32/System/Threading/mod.rs#L140\r\n\r\n* https://github.com/microsoft/windows-rs/blob/1f40da8ffc44adba3f9a866287108e80223d0a81/crates/libs/windows/src/Windows/Win32/System/Threading/mod.rs#L840"
      ],
      "zed-standardize-platform-agnostic-configuration": [
        "That has to have `#[serde(default = \"default_true\")]` , as the default_settings.json mention default = true (which is also good to mention here) + the user setting overrides require this field always (which is not what we want): \r\n\r\n![image](https://github.com/user-attachments/assets/fadef7dc-8010-4a93-b638-2f15b8d4fc90)\r\n",
        "The error is gone, so thank you for fixing this."
      ],
      "zed-test-through-public-apis": [
        "Super nice to see the tests on the new functionality, thank you.\r\n\r\nOne thing I really lack in them is a visual part: instead of each \"visible count\" and other digits assertion, we should do what other tests do, comparing a file+outline string representations.\r\n(all these\r\n\r\n```\r\noutline: struct OutlineEntryExcerpt\r\n  outline: id\r\n  outline: buffer_id\r\n  outline: range\"\r\n```\r\nparts).",
        "We need much more tests for something as fundamental as \"show me all highlights for a random chunk of text in the buffer\".\r\n\r\nIf this panics or somehow misbehaves, it might be very frequent and frustrating, and overall, it's very scary to merge whatever related thing.\r\n\r\nWe should cover:\r\n\r\n* overlapping ranges (both LSP overlaps and LSP + tree-sitter highlights overlap LSP ones)\r\n* multi-line LSP highlights\r\n* something else? \r\nFor inlays, we have `test_random_inlays` fuzzy test checks how text behaves for random inlays' inserts.\r\nShould we do something for highlights too?",
        "Also, all that \r\n\r\n> The delta is now expressed on these number arrays without any form of interpretation what these numbers mean. \r\n\r\nsection in the spec is somewhat scary: if I've read it correctly, we should fiddle with digits on the client, when computing token deltas?\r\n\r\nDefinitely worth testing then, if/when the delta support arrives.",
        "I think it would be better to add these cases into `test_url_regex` and ensure we match out the right thing.\r\n`valid_url_ending` is pretty local method to test instead.",
        "The idea of unit tests is to test certain contracts of a single code unit, usually by using its public API for testing.\r\nObviously, nothing forbids doing otherwise and test the private api, but unit tests also slow down development, as every adjusment might require test rewrites/fixes — private, \"pretty local\", methods are meant to be moved/removed/adjusted/etc. thus testing them is not that great.\r\n\r\nJust a general rule of thumb: you're fixing the way regex captures strings, so test that, not something that checks whether the URL \"ending\" is valid.",
        "Done, and that lead to a better way to compare chars case-insensitively, thank you.",
        "There's a lot of rendering tests, which is amazing, thank you!\r\n\r\nWhat's lacking are new ones in the `editor_tests.rs`, and after we deal with the brackets and whatever else that triggers the pop-up display, it would be fantastic to have that logic tested too."
      ],
      "zed-consider-algorithmic-complexity": [
        "To note, `dedup` is the wrong method to deduplicate things in a `Vec`, as it only works for sequential items:\r\nhttps://doc.rust-lang.org/std/vec/struct.Vec.html#method.dedup\r\n\r\nIt seems that we could try and either use a HashSet (I had to implement a few `Hash` manually for document colors for this approach), or use a method from `Itertools` for a proper deduplication, [`dedup_by`](https://docs.rs/itertools/latest/itertools/trait.Itertools.html#method.dedup_by)",
        "Ahh, traits.\r\nThank you, I would not think of this method seeing a `Vec` around.",
        "This is odd: effectively, we go over all `fetched_outlines` and for each, we we-iteratethe outlines list again to find the children.\r\n\r\nIt's a plain O(N^2) for nothing and should be fixed.\r\nWe do `outlines.retain(|outline| {` right above and can prepare a depth map for any decision of a kind.",
        "Can we change the predicate, so we don't have to clone?\r\nSeems that `F: FnMut((&TaskSourceKind, &TaskTemplate)) -> bool + 'static,` is good enough?"
      ],
      "zed-design-interfaces-not-implementations": [
        "Let's add a static `fn .. -> Option<Workspace>` for all this extractions, as we might want to invoke things headless for ~10 different actions it seems.",
        "The code below seems to show that quite explicitly, so let's remove the comment.\r\n\r\nI would also argue that this function should accept `proxy: &Url` instead and `TcpStream::connect` can be done up the stack, at a single place that calls this function.\r\nShould we do that?",
        "This works, but we can reduce the diff and stick to the builder-like API with `.when(TitleBarSettings::get_global(cx).show_branch_icon, |branch_button| ...)`.\r\n\r\nLet's try doing that?",
        "Let's consolidate this with https://github.com/zed-industries/zed/blob/1cfbfc199cee551318b89a2f35853ed43b8ac52d/crates/gpui/src/app.rs#L1434-L1440 and it's the same thing implemented for macOS.\r\n\r\nAt least, the vocabulary better match in the \"interface\" API.",
        "I mean that there are two components shared in every OS (Linux is special but has similar concepts in certain DEs, but let's omit it as not implemented in the repo).\r\n\r\n* in the OS, there's a list of entries, ordered by \"last opened\"\r\n  * that list could be altered from the OS side (different ways to do that but still)\r\n* there's a list of last opened projects in Zed, and that list can be altered from Zed and needs to be propagated to the OS side\r\n\r\nAt let's call all these entries, lists, whatnot similarly, as of now there's \"jump_list\" for Windows only and something else for mac-only.\r\nWhat also would be good, is to keep the same, single \"add an entry\"/\"set entries\" workflow in the API if possible."
      ],
      "zed-hierarchical-configuration-organization": [
        "Let's either wait for https://github.com/zed-industries/zed/pull/29494 or incorporate the settings approach, as we seem to get more and more title bar settings and we'd better start namespacing them.",
        "Sure, less knobs to have in the settings is always good, thanks."
      ]
    },
    "profile": {
      "location": "Turku, Finland",
      "company": "@zed-industries",
      "blog": "t.me/SomeoneToIgnore",
      "site_admin": false,
      "followers": 195,
      "following": 1
    }
  },
  "celestial-vault": {
    "repos": [
      "cline/cline"
    ],
    "entries": [
      {
        "slug": "cline-add-missing-error-handling",
        "title": "Add missing error handling"
      },
      {
        "slug": "cline-ai-provider-naming-clarity",
        "title": "AI provider naming clarity"
      },
      {
        "slug": "cline-avoid-premature-memoization",
        "title": "Avoid premature memoization"
      },
      {
        "slug": "cline-avoid-unnecessary-custom-hooks",
        "title": "avoid unnecessary custom hooks"
      },
      {
        "slug": "cline-break-down-large-files",
        "title": "break down large files"
      },
      {
        "slug": "cline-centralize-configuration-management",
        "title": "centralize configuration management"
      },
      {
        "slug": "cline-check-before-property-access",
        "title": "Check before property access"
      },
      {
        "slug": "cline-grpc-interface-consistency",
        "title": "gRPC interface consistency"
      },
      {
        "slug": "cline-handle-async-operations-safely",
        "title": "Handle async operations safely"
      },
      {
        "slug": "cline-prevent-unnecessary-operations",
        "title": "prevent unnecessary operations"
      },
      {
        "slug": "cline-provide-configuration-fallbacks",
        "title": "Provide configuration fallbacks"
      },
      {
        "slug": "cline-reuse-common-message-types",
        "title": "reuse common message types"
      },
      {
        "slug": "cline-use-null-meaningfully",
        "title": "Use null meaningfully"
      },
      {
        "slug": "cline-use-semantic-naming",
        "title": "use semantic naming"
      },
      {
        "slug": "cline-validate-algorithmic-correctness",
        "title": "validate algorithmic correctness"
      }
    ],
    "comments": {
      "cline-add-missing-error-handling": [
        "Are we sure this is allowed to be async?\r\n\r\nAdditionally we don't have error handling here. It would be a shame if there was a random failure and the extension failed to activate.",
        "wrap in try/catch to add debug logs to file deletion issues since this can fail",
        "Call delete all if there are no tasks left so that checkpoints get deleted too. Otherwise we are left with an awkward non-zero size"
      ],
      "cline-ai-provider-naming-clarity": [
        "Could you write in the description of this PR the connection between the Oracle provider and LightLLM? I'm confused as to why we're adding all of these LiteLLM fields and changing this seemingly unconnected provider. ",
        "Yeah let's rename it",
        "Let's change this name to be Huawei-specific, so that there's no confusion. "
      ],
      "cline-use-semantic-naming": [
        "Can we make this a constant so it's clear to later devs why we have random numbers here?",
        "@saito-sv @saoudrizwan This is not one of the GlobalFileNames constants. This is apparently the old directory name that we're checking for backwards compatibility."
      ],
      "cline-grpc-interface-consistency": [
        "@sjf Are we sure about this? Citing @Garoth 's inaugural PR, a Promise<void> is used.\r\n\r\nhttps://github.com/cline/cline/pull/3781/files#diff-66f49a5549f7ac3e91f56d3bbe46445a57bf27a19e60ecd962fd4030d4bd027d",
        "Okay sounds good",
        "@sjf So in the error case we need to also return Empty?"
      ],
      "cline-centralize-configuration-management": [
        "Moved same credentials to a config file in the shared folder.",
        "default to 4000 like before"
      ],
      "cline-provide-configuration-fallbacks": [
        "@abeatrix The migration didn't populate it?",
        "@abeatrix I changed this to use the normalized API configuration function, which we should have been using. Good catch here. This undefined state should no longer happen, regardless of the actual value of the variable on the backend.",
        "@dcbartlett Okay fair. Just didn't want it briefly flashing for users who should not see it.",
        "@dcbartlett So not clear, does your request stand?"
      ],
      "cline-break-down-large-files": [
        "If this is somehow built on top of LiteLLM, I would still recommend writing it separately and not tying it to the LiteLLM handler code. ",
        "Yeah, let's move it into it's own class so they don't get mixed up.",
        "If you were to do that, you would also avoid complexity here.",
        "Yeah let's do so",
        "This function is called in several other places in McpHub, so it's cleanest to make a new function for now."
      ],
      "cline-check-before-property-access": [
        "Have never seen an array index accessed like this. Why can we not access chunk.choices[0] as before?",
        "One thing I noticed is that we treat a lot of these as if they can't be undefined, but we allow it. We should enforce defaults"
      ],
      "cline-use-null-meaningfully": [
        "@dcbartlett @saoudrizwan Talked with Saoud, will keep this way. The reason being that if the operation fails we want it to silently fail and just return null. In this way we can just not display the size."
      ],
      "cline-validate-algorithmic-correctness": [
        "For example, if messages.length is 5 and the currentDeletedRange is undefined (nothing yet has been truncated).\r\n\r\nThen startOfRest would be 1.\r\n\r\nThen, \r\n\r\nMath.floor((messages.length - startOfRest) / 8) * 3 * 2 \r\n= floor((5 - 1) / 8) * 3 * 2 \r\n= floor(.5) * 3 * 2 \r\n= 0 * 3 * 2\r\n= 0\r\n\r\nThis incorrectly calculates the messagesToRemove to be zero.\r\n\r\n",
        "In the same example, messages.length is 5 and the currentDeletedRange is undefined.\r\n\r\nSo,\r\nMath.floor(((messages.length - startOfRest) * 3) / 4 / 2) * 2\r\n= floor(((5 - 1) * 3) / 4 / 2) * 2\r\n= floor((4 * 3) / 4 / 2) * 2\r\n= floor(12 / 4 / 2) * 2\r\n= floor(3 / 2) * 2\r\n= floor(1.5) * 2\r\n= 1 * 2\r\n= 2\r\n\r\nThus correctly calculating that we can remove two messages.",
        "With user/assistant pairs, this would look like:\r\n\r\nu, a, u, a, u  (The first message is always u and, since we call this when a request is made, the last message is always u)\r\n\r\nThen, after the truncation, we get:\r\n\r\nu, _ , _ , a, u\r\n\r\nSo this would be\r\n\r\nu, a, u",
        "Note that we can't truncate further than this without either removing the first user message or the message that the user just sent.",
        "A final note.\r\n\r\nYou can see that the original \"half\" truncation works fine because, since we can't truncate message lengths of 3 or less, we don't end up flooring to zero as we see with the \"quarter\" truncation.\r\n\r\nmessagesToRemove = Math.floor((messages.length - startOfRest) / 4) * 2\r\n\r\nTo prove this, we can assume the message length is 5 (The math would work with 4 but, as we've noticed, the length is always uneven).\r\n\r\nThen we have,\r\n\r\nfloor((messages.length - startOfRest) / 4) * 2\r\n= floor((5 - 1) / 4) * 2\r\n= floor(4 / 4) * 2\r\n= floor(1) * 2\r\n= 1 * 2\r\n= 2\r\n\r\nThus correctly calculating that we can remove 1 user/message pair when we have 5 messages."
      ],
      "cline-avoid-unnecessary-custom-hooks": [
        "We already have a \"useClickAway\" that can be imported from \"react-use\". See the ClineRulesToggleModal or ServersToggleModal for reference."
      ],
      "cline-prevent-unnecessary-operations": [
        "Debouncing input with a controlled value (useState). Adding to ApiKeyField and BaseUrl field. Also seen in the new DebouncedTextField component.",
        "Fair point"
      ],
      "cline-reuse-common-message-types": [
        "Is that necessary? I saw that but prefer just making it explicit.\r\n\r\nIf that's our standard then I can switch it\r\n\r\n@canvrno ",
        "Alright"
      ],
      "cline-avoid-premature-memoization": [
        "Why no just make this a function?",
        "Sure, but it's so light that it doesn't matter. I think we're overusing useMemo and useCallback in the code",
        "A useEffect would only run on the dependency changes\r\n"
      ],
      "cline-handle-async-operations-safely": [
        "is it okay to be setting an empty path here? In the getDetectedChromePath we're just returning empty string on error."
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 37,
      "following": 5
    }
  },
  "aqrln": {
    "repos": [
      "prisma/prisma"
    ],
    "entries": [
      {
        "slug": "prisma-accurate-method-descriptions",
        "title": "Accurate method descriptions"
      },
      {
        "slug": "prisma-api-abstraction-levels",
        "title": "API abstraction levels"
      },
      {
        "slug": "prisma-avoid-quadratic-complexity",
        "title": "avoid quadratic complexity"
      },
      {
        "slug": "prisma-avoid-unnecessary-allocations",
        "title": "Avoid unnecessary allocations"
      },
      {
        "slug": "prisma-consistent-clear-naming",
        "title": "Consistent clear naming"
      },
      {
        "slug": "prisma-consistent-error-object-usage",
        "title": "consistent error object usage"
      },
      {
        "slug": "prisma-database-provider-compatibility",
        "title": "Database provider compatibility"
      },
      {
        "slug": "prisma-dependency-classification-standards",
        "title": "dependency classification standards"
      },
      {
        "slug": "prisma-document-configuration-alternatives",
        "title": "Document configuration alternatives"
      },
      {
        "slug": "prisma-ensure-concurrent-resource-cleanup",
        "title": "Ensure concurrent resource cleanup"
      },
      {
        "slug": "prisma-explicit-null-handling",
        "title": "explicit null handling"
      },
      {
        "slug": "prisma-extract-duplicate-code",
        "title": "Extract duplicate code"
      },
      {
        "slug": "prisma-maintain-clean-ci-configuration",
        "title": "maintain clean CI configuration"
      },
      {
        "slug": "prisma-manage-output-streams-carefully",
        "title": "Manage output streams carefully"
      },
      {
        "slug": "prisma-validate-sensitive-operations",
        "title": "Validate sensitive operations"
      }
    ],
    "comments": {
      "prisma-accurate-method-descriptions": [
        "Why was the meaning of the comment changed? \"Zero or one\" sounds correct to me (i.e., the result can be `null`).",
        "although \"find one\" is probably fine too",
        "@millsp @SevInf what do you think about this one?",
        "The meaning of the comment was changed. I don't think \"one or more\" is correct for `findMany`, \"zero or more\" was correct."
      ],
      "prisma-dependency-classification-standards": [
        "Should it be a peer dependency instead? Or could we have a more open version range here?",
        "This one should just be gone, an instrumentation package must not depend on the SDK. The users likely won't even use `sdk-trace-base` directly but `sdk-trace-node` or even a third-party SDK not based on `sdk-trace-base`. In the latter case we may break completely because we'll be using a completely wrong `Span` class. Removing this depends on https://github.com/prisma/prisma/pull/25164."
      ],
      "prisma-document-configuration-alternatives": [
        "Is this how MCP is configured in Cursor as well, or is this Windsurf-specific?",
        "looks good!"
      ],
      "prisma-consistent-error-object-usage": [
        "nit\r\n```suggestion\r\n      throw adapter.errorRegistry.lookupError(e as Error) ?? e\r\n```",
        "Aso `throw adapter.errorRegistry.lookupError(e as Error)` is probably wrong, because `errorRegistry.lookupError` doesn't return an `Error`, it returns an `ErrorRecord`, which is defined as `{ error: unknown }`.\r\n\r\n```suggestion\r\n      throw adapter.errorRegistry.lookupError(e as Error)?.error ?? e\r\n```",
        "That said, it could also be seen as a defect in the API. It makes total sense why errors need to be wrapped into extra objects to be stored (an error can be any JS value including `undefined`), but I see no reason why `ErrorRecord` would need to be exposed outside of the `binder` module, it feels like it should be an implementation detail to me.",
        "I feel like the logic is backwards compared to the function signature. We throw if the specified error is not found (although the signature suggests `undefined` would be returned in this case) and only return `undefined` in the final `else` branch if the shape of the input was completely wrong and we couldn't interpret it."
      ],
      "prisma-consistent-clear-naming": [
        "I'm wondering if we really want to use a generic name consistent between adapters or if it would actually be better to specialize them based on the type name that the method returns (i.e. `getPool`, `getClient` etc)."
      ],
      "prisma-explicit-null-handling": [
        "This should be undefined on MongoDB",
        "We should probably validate it using `zod` or `@effect/schema` or whatever and not just assert the type",
        "> I don't think we use either right now, but since there's only 2 fields, maybe I can just check the schema manually\r\n\r\nSince it's in the CLI and not the Client (where bundle size matters), I wouldn't worry too much about introducing new dependencies. Introducing it now lowers the barrier to using it where appropriate in the future, as well as finding and fixing other places where we might be missing validation.\r\n\r\n> It might be worth doing the same for config\r\n\r\ntrue",
        "```suggestion\r\n      obj.currentTimeframe == null ||\r\n```\r\nto cover both `null` and `undefined`",
        "You can also expand that condition to filter out both `null` and `undefined`. I personally don't have a strong preference and the code is fine with me as it currently is too."
      ],
      "prisma-database-provider-compatibility": [
        "Shouldn't this depend on the provider now?"
      ],
      "prisma-extract-duplicate-code": [
        "This seems to be the same as in `ModelFile.ts`, should we extract this to a function?",
        "done",
        "let's maybe extract a small function to convert from `DatamodelEnum` to `SchemaEnum`"
      ],
      "prisma-manage-output-streams-carefully": [
        "This technically breaks much of the `console.log` functionality: this implementation will only work when printing primitive values and not using any format specifiers. Should it be just this instead?\r\n\r\n```suggestion\r\n  console.log = console.error.bind(console)\r\n```",
        "Also, where do those logs come from? Can we just print them to stderr in the first place if they're in our code instead of monkey-patching `console`?",
        "oh wow"
      ],
      "prisma-ensure-concurrent-resource-cleanup": [
        "@jacek-prisma done",
        "great point"
      ],
      "prisma-avoid-quadratic-complexity": [
        "I don't think it matters either way because strings are interned in JS so we should always be comparing integers anyhow (whether it's enum variants or pointers to `v8::String`) but I can change it to an enum if you'd prefer",
        "Yeah, in this case V8 (Ignition) generates identical bytecode in both cases, the only difference being `LdaConstant` emitted in the case of strings or `LdaZero`/`LdaSmi` emitted for enums. The machine code generated for ARM64 by TurboFan, however, ends up being 5 CPU instructions shorter with enums because small constants can be used as immediates:\r\n\r\n```\r\n0xffff90007424   3a4  7100067f       cmp w19, #0x1 (1)\r\n```\r\n\r\nbut strings need to be loaded from the constants table into a register first:\r\n\r\n```\r\n0xffff8dfc746c   3ac  580080f6       ldr x22, pc+4124 (addr 0x0000ffff8dfc8488)    ;; object: 0x1fde6cc134f1 <String[6]: #quoted>\r\n0xffff8dfc7470   3b0  eb16029f       cmp x20, x22\r\n```",
        "I guess there technically is tiny benefit in using enums here then 😄",
        "Important note thought that this only definitely applies to const enums (as I just wrote 0, 1, 2 in a JS file I copied the code to literally), regular non-const TypeScript enums would probably generate worse code than either of these due to an extra layer of indirection through the enum object."
      ],
      "prisma-validate-sensitive-operations": [
        "done"
      ],
      "prisma-avoid-unnecessary-allocations": [
        "```suggestion\r\n    return Buffer.from(arg.buffer, arg.byteOffset, arg.byteLength)\r\n```\r\nto share the underlying `ArrayBuffer` instead of cloning the data",
        "Probably better to inline the first `map`:\r\n\r\n```suggestion\r\n    .map((row) =>\r\n      row\r\n        .map((value, index) => mappers[index](value)))\r\n        .reduce<Record<string, unknown>>((acc, value, index) => {\r\n```\r\n\r\nThe `rows` array could be large, and `map` is not lazy in JavaScript but will allocate a whole new array each time.",
        "We don't actually need to store the array, we only need to return the last value.",
        "You should be able to construct the `Uint8Array` directly in this case, the intermediate `Buffer` is only necessary to deserialize from base64",
        "Should we maybe do the mapping in the constructor instead to avoid allocating a new array each time a new span arrives?"
      ],
      "prisma-maintain-clean-ci-configuration": [
        "We already target SQLite, especially with this change. I'd remove this comment altogether, we know we want to support all of our first-class databases.\r\n\r\n```suggestion\r\n        flavor: ['js_pg', 'js_libsql', 'js_d1', 'js_better_sqlite3']\r\n```",
        "```suggestion\r\n```\r\nNo need to specify the version here, there's a `packageManager` entry in `package.json` so it should download the right version, and, fwiw, we use pnpm 9 and not 10."
      ],
      "prisma-api-abstraction-levels": [
        "I don't think this utility should be aware of `Client` (and the need to unsoundly cast empty objects to `Client` in all tests is a good evidence of this), it's on a different level of abstraction.\r\n\r\nInstead, it should take an abstract non-nullable value to get the prototype from:\r\n```suggestion\r\nexport function createCompositeProxy<T extends object>(prototypeProvider: {}, target: T, layers: CompositeProxyLayer[]): T {\r\n```\r\n\r\nand then on line 177:\r\n\r\n```diff\r\n- getPrototypeOf: () => Object.getPrototypeOf(client._originalClient),\r\n+ getPrototypeOf: () => Object.getPrototypeOf(prototypeProvider),\r\n```\r\n\r\nThen you can pass `client._originalClient` or `this._originalClient` when you actually do have a client, or an empty object without having to cast anything in the tests.",
        "alternatively, you can pass the prototype itself, that should also be fine"
      ]
    },
    "profile": {
      "location": "Berlin, Germany",
      "company": "@prisma",
      "blog": "",
      "site_admin": false,
      "followers": 340,
      "following": 279
    }
  },
  "ZeeshanTamboli": {
    "repos": [
      "mui/material-ui"
    ],
    "entries": [
      {
        "slug": "material-ui-avoid-render-cycle-allocations",
        "title": "Avoid render cycle allocations"
      },
      {
        "slug": "material-ui-distinguish-nextjs-routers",
        "title": "Distinguish Next.js routers"
      },
      {
        "slug": "material-ui-document-design-decisions",
        "title": "Document design decisions"
      },
      {
        "slug": "material-ui-document-implementation-decisions",
        "title": "Document implementation decisions"
      },
      {
        "slug": "material-ui-effect-hook-best-practices",
        "title": "Effect hook best practices"
      },
      {
        "slug": "material-ui-follow-library-recommendations",
        "title": "Follow library recommendations"
      },
      {
        "slug": "material-ui-graceful-component-errors",
        "title": "Graceful component errors"
      },
      {
        "slug": "material-ui-isolate-dom-security-boundaries",
        "title": "Isolate DOM security boundaries"
      },
      {
        "slug": "material-ui-maintain-configuration-accuracy",
        "title": "Maintain configuration accuracy"
      },
      {
        "slug": "material-ui-meaningful-and-consistent-names",
        "title": "Meaningful and consistent names"
      },
      {
        "slug": "material-ui-parameter-interaction-design",
        "title": "Parameter interaction design"
      },
      {
        "slug": "material-ui-strict-mode-proof-hooks",
        "title": "Strict mode-proof hooks"
      },
      {
        "slug": "material-ui-test-behavior-not-implementation",
        "title": "Test behavior not implementation"
      },
      {
        "slug": "material-ui-use-direct-path-imports",
        "title": "Use direct path imports"
      },
      {
        "slug": "material-ui-use-screen-queries",
        "title": "Use screen queries"
      },
      {
        "slug": "material-ui-use-slots-for-composition",
        "title": "Use slots for composition"
      },
      {
        "slug": "material-ui-use-theme-utilities-consistently",
        "title": "Use theme utilities consistently"
      }
    ],
    "comments": {
      "material-ui-avoid-render-cycle-allocations": [
        "Makes sense. While useMemo technically works here, it is meant for **derived values** based on dependencies. Updated in https://github.com/mui/material-ui/pull/46333/commits/fd86ebd2caee0137c24e72eb2d5c83706759a317.",
        "Done in https://github.com/mui/material-ui/pull/46333/commits/acc42c28a2a484209f2196d11bb5e5e51a9e5948"
      ],
      "material-ui-meaningful-and-consistent-names": [
        "Done.",
        "Why is this type needed above in docs/src/pages/premium-themes/onepirate/modules/components/Button.tsx?",
        "This still doesn't reply my question. It's not about `a` tag here, it is the `type` attribute that is redefined and why you need to have a new TS type `ConstrainedButtonProps` which is used in `docs/src/pages/premium-themes/onepirate/modules/components/Button.tsx` file. Why `ButtonProps` can't be used there like earlier? Why is there an error with `ButtonProps` in that file?",
        "> However, this fix created another potential issue - the updated `ButtonProps` would allow any string value for the `type` attribute, not just the valid HTML button types ('button', 'submit', 'reset').\r\n\r\nWhy the updated `ButtonProps` is allowing any string value for the `type` atrribute instead of the valid HTML button types?\r\n",
        "> The error occurs because the updated ButtonProps type allows the type property to be a string | undefined (inherited from React.ButtonHTMLAttributes), which is too general.\r\n\r\nWhat you are saying is wrong. `React.ButtonHTMLAttributes` already does support `type` `\"submit\" | \"reset\" | \"button\" | undefined;`.",
        "```suggestion\r\n    const hiddenElements = getHiddenElements(container);\r\n```",
        "I'm not sure what you mean by `textareaHandle`. But you're right, it shouldn't be `event`. I've updated it to `textareaElement`. According to the documentation here: https://playwright.dev/docs/next/api/class-locator#locator-evaluate, `pageFunction` takes a element as an argument.",
        "I had the same thought. The issue is that `getTagProps` returns properties specific to a Material UI Chip (or tag?), like `disabled`, and `onDelete`. This callback is meant to be spread only when using a _custom_ Material UI Chip. Maybe we should rename it to `getChipProps`.",
        "@michaldudak @DiegoAndai I've made the changes and updated the docs. I only replaced \"tag\" in the public API. Internally, some methods and variables still use \"tag\" since the deprecated `renderTags` depends on them. Renaming them would mean duplicating methods with the same logic for `renderValue`."
      ],
      "material-ui-effect-hook-best-practices": [
        "Are you sure layout effect is absolutely necessary here?",
        "I used our util `useEnhancedEffect` instead which handles SSR as well."
      ],
      "material-ui-distinguish-nextjs-routers": [
        "This is incorrect. The difference isn’t between the Next.js pages router and app router. The `id=\"__next\"` is used by **Next.js**, while `id=\"root\"` is used by **Vite** and other SPA frameworks.\r\n\r\nIt's that we need to mention that since Next.js 13+, you need to _manually_ add `id=\"__next\"` to the root element i.e `body`. It isn't automatically added like before. For Vite, the root element typically looks like this:\r\n\r\n```html\r\n<div id=\"root\"></div>\r\n```\r\nSo, the `important` option should use `#__next` for Next.js and `#root` for Vite.",
        "> Inclusion of Troubleshooting block is okay , or needs to be removed ?\r\n\r\nIt's fine. Just need to tweak point 2 by removing the mentioning of `root` as Next.js ID."
      ],
      "material-ui-test-behavior-not-implementation": [
        "Let's not test the code implementation details.  Instead, we can verify that it does not throw an error when nested options are provided to `Autocomplete`. We should trigger the user interactions step by step as provided in the issue and check that it does not crash in the end.\r\n\r\n1. The user opens the list and selects an option.\r\n2. The user clears the selected option from input.\r\n3. The user reopens the autocomplete.\r\n4. It should not crash.\r\n\r\nAlso, please add this test in `Autocomplete` test file (`Autocomplete.test.js`). Let me know if you need any help.   ",
        "Please delete the test from the `useAutocomplete.test.js`. Any logic we have in future related to highlighting will be in the `useAutocomplete` hook only which will always be used in the Material-UI `Autocomplete` component.",
        "We could. Done."
      ],
      "material-ui-use-direct-path-imports": [
        "I'm not sure this works. Can you share a reproduction where all these steps succeed and bundle size is reduced? I think Vite uses the esm bundle by default, but I'm not certain. cc @Janpot",
        "I wouldn’t replace the entire “Option two: use a Babel plugin” section. I’d keep the original content and avoid adding all the new material."
      ],
      "material-ui-parameter-interaction-design": [
        "No need to define it again here since `useAutocomplete` types already has it and Joy UI and Material UI extends the hook's types.\r\n```suggestion\r\n```"
      ],
      "material-ui-use-screen-queries": [
        "You can use `screen.getByRole` below instead of passing `getByRole` parameter,"
      ],
      "material-ui-use-slots-for-composition": [
        "```suggestion\r\n- You should provide a tooltip title using `slotProps.tooltip.title` for each speed dial action.\r\n```"
      ],
      "material-ui-document-implementation-decisions": [
        "Added."
      ],
      "material-ui-document-design-decisions": [
        "This is internal component so maybe we don't need to document in the types that `getTabbable` prop now accepts three parameters: https://github.com/mui/material-ui/blob/master/packages/mui-material/src/Unstable_TrapFocus/FocusTrap.types.ts#L13, but it would be better to add it."
      ],
      "material-ui-maintain-configuration-accuracy": [
        "`eslint` related packages and `globals` packages isn't used anywhere. They can be removed."
      ],
      "material-ui-use-theme-utilities-consistently": [
        "According to the [docs](https://mui.com/material-ui/migration/upgrade-to-v7/#theme-behavior-changes:~:text=It%27s%20recommended%20to%20use%20the%20theme.vars.*%20as%20values%20in%20your%20styles%20to%20refer%20to%20the%20CSS%20variables%20directly%3A), it's recommended to use theme.vars.* directly in your styles.",
        "I suggest using the `styled` API, like in the Customization section, and naming the component `StyledToggleButtonGroup`. Since the `sx` prop uses `styled` under the hood, `styled` is more performant and, in my opinion, easier to read—especially with longer style definitions.",
        "I don't think all this logic is needed.  We can simply reset inherited `line-height` style using `line-height: normal`:\r\n```diff\r\n--- a/packages/mui-material/src/Chip/Chip.js\r\n+++ b/packages/mui-material/src/Chip/Chip.js\r\n@@ -89,6 +89,7 @@ const ChipRoot = styled('div', {\r\n       alignItems: 'center',\r\n       justifyContent: 'center',\r\n       height: 32,\r\n+      lineHeight: 'normal',\r\n       color: (theme.vars || theme).palette.text.primary,\r\n       backgroundColor: (theme.vars || theme).palette.action.selected,\r\n       borderRadius: 32 / 2,\r\n```\r\nDocs: https://developer.mozilla.org/en-US/docs/Web/CSS/line-height#normal"
      ],
      "material-ui-follow-library-recommendations": [
        "If you test it manually in the documentation preview you'll see that a double-click is necessary to grab the handle for resizing. However, it's essential not to release the mouse button after the second click. Simply clicking once with the mouse doesn't work. I'm using the touchpad on my laptop.\r\n\r\n_Update:_ You're correct. Left-clicking using the mouse button on touchpad and resizing while keeping the button pressed works fine. I've adjusted it to use only `mouse.down()`."
      ],
      "material-ui-isolate-dom-security-boundaries": [
        "Can you explain this part of the logic with an example? It's a little hard to follow."
      ],
      "material-ui-strict-mode-proof-hooks": [
        "You're right that the initializer inside `useState` only runs once per mount. However, in React’s Strict Mode (development only), it's **intentionally called twice** to detect impure logic. Since `registerTab` mutates internal state, calling it twice would incorrectly register the tab multiple times, shifting tab indices and breaking the selection or indicator logic.\r\n\r\nTo avoid this, we guard it with `hasRegisteredRef`, ensuring `registerTab` runs only once — even in development. In production, the guard has no effect because the initializer runs only once as expected.\r\n\r\nIdeally, the initializer should be pure (per [React docs](https://react.dev/reference/react/useState#my-initializer-or-updater-function-runs-twice)), but we intentionally break that rule here to **support SSR** — specifically, to precompute tab metadata so that the correct tab is marked selected on the first render (see [test case](https://github.com/mui/material-ui/blob/6c0f14b50dc7c86134b8bb549da47dc33bf8b06a/packages/mui-material/src/Tabs/Tabs.test.js#L901-L912)). Without it, we get hydration mismatches..\r\n\r\nI considered making `registerTab` idempotent, but that’s not feasible when we need to assign an implicit `value` based on the tab's render order. That requires incrementing a shared index counter (`childIndexRef`) — and we can’t require users to always provide explicit values to `Tab` without introducing a breaking change.\r\n\r\nThis approach strikes a balance: it ensures SSR correctness, avoids hydration issues, and works with wrapper components like `<Tooltip><Tab /></Tooltip>`, while remaining safe under React’s development behavior.\r\n\r\nOpen to suggestions if you think there's a cleaner way to achieve this.",
        "> If I understand it correctly this won't work well if you remove a tab dynamically (as there's no unregister function)\r\n\r\nIt won't. But it isn't supported even in latest version.\r\n\r\nThis PR: https://stackblitz.com/edit/ry4fan5c-t3b4771r\r\nMaster: https://stackblitz.com/edit/ry4fan5c-oqugmytq",
        "Yes, registering during the effect phase would prevent it from running on the server, which breaks SSR.\r\n\r\n> Perhaps we could register both during rendering and in an effect and make the register operation idempotent (or register conditionally if it hasn't been registered yet). This will allow the use of the unregister function in the effect cleanup.\r\n\r\nThat could work well only if tabs always have explicit `value` props. But in our case, we also support implicit values based on render order, like this:\r\n\r\n```tsx\r\nconst [tab, setTab] = React.useState(1);\r\n\r\nconst handleChange = (event, newValue) => {\r\n  setTab(newValue);\r\n};\r\n\r\nreturn (\r\n  <Tabs value={tab} onChange={handleChange}>\r\n    <Tab label=\"one\" />\r\n    <Tooltip title=\"two helper\">\r\n      <Tab label=\"two\" />\r\n    </Tooltip>\r\n  </Tabs>\r\n);\r\n```\r\n\r\nHere, tabs derive their `value` from their render position (i.e., first tab = 0, second = 1), using an internal `childIndexRef`.\r\n\r\nIf we register both during render and in an effect:\r\n\r\n* In React Strict Mode (dev only), render and effect each run twice i.e total of 4 registrations.\r\n* Even without Strict Mode, a single tab would be registered twice. (one in first render and second in effect).\r\n\r\nSo, there would be 4 child indexes.\r\n\r\nWhile `registerTab` is already idempotent when used with explicit values (via `valueToIndex.has(finalValue)`), we can't enforce `value` on Tab without introducing a breaking change.\r\n",
        "> Deriving the value from the position sounds good to me 👍🏼\r\n\r\nThis logic was already present when `cloneElement` was used. Just picked that up here.\r\n\r\n> But if we create a `value` for the position on our side, then we would have a `finalValue`, no?\r\n\r\nYes, we do generate a `finalValue` based on position internally when value isn't provided. The issue is that this implicit value depends on render order and a shared index (`childIndexRef`), which is incremented during registration.\r\n\r\nIf we allow `registerTab` to run multiple times — as suggested above, in both render and effect — the index keeps increasing, and the same tab ends up with different `finalValue`s across renders. That breaks selection and causes hydration mismatches.\r\n\r\nWith explicit `value`, we don’t rely on index state, so idempotency is safe. But for implicit values, the act of generating `finalValue` is tied to mutable state — so calling `registerTab` multiple times isn’t safe unless we move to require explicit values, which would be a breaking change.",
        "I don't think this will work.\r\n\r\n> In any subsequent register call, we use `finalValue` to identify the Tab\r\n\r\nWhy do we want to pass the same `finalValue` to the next registration call? If we want to pass this stored `finalValue`, we shouldn't call `registerTab` again in the first place (supposedly in the effect).\r\n\r\nFeel free to edit the code if you have any ideas.\r\n\r\n----\r\n\r\nEven in Base UI, they are making the `value` prop required on Tab: https://github.com/mui/base-ui/pull/2124. \r\n\r\nSupporting implicit `value` in Tab cause them issues like https://github.com/mui/base-ui/issues/1880.",
        "> Because we want to return the unregistering callback from the effect, so it's run on unmount.\r\n\r\nBut wouldn't the useEffect's setup function do nothing? It would simply return the output given as an input **always** when doing `registerTab(finalValue)`.\r\n\r\n> This is not an option for us unless we want to wait for a new major.\r\n\r\nYes, not an option now.\r\n\r\n> We're already supporting implicit value, aren't we? With or without my suggestion.\r\n\r\nYes, we are already. Just wanted to point out some issues. I thought it would help us to understand.",
        "I tried doing this in https://github.com/mui/material-ui/pull/46333/commits/7221ea83aeb156e555c031bd9b0062124a1971b6 but the tests fail. Any idea why? However, it works locally on browser.",
        "Tests pass now. The tests run with Strict Mode, which helped me catch the issue that I wasn’t decrementing the child index on tab unregistration."
      ],
      "material-ui-graceful-component-errors": [
        "A standalone `Tab` isn't functional: https://stackblitz.com/edit/j4aahksg — I don't see how it could be useful on its own."
      ]
    },
    "profile": {
      "location": "Pune, Maharashtra",
      "company": "MUI",
      "blog": "https://in.linkedin.com/in/zeeshantamboli",
      "twitter_username": "ZeeshanTamboli",
      "site_admin": false,
      "followers": 57,
      "following": 8
    }
  },
  "hiltontj": {
    "repos": [
      "influxdata/influxdb"
    ],
    "entries": [
      {
        "slug": "influxdb-avoid-flaky-test-patterns",
        "title": "Avoid flaky test patterns"
      },
      {
        "slug": "influxdb-centralize-workspace-configurations",
        "title": "Centralize workspace configurations"
      },
      {
        "slug": "influxdb-choose-appropriate-lock-primitives",
        "title": "Choose appropriate lock primitives"
      },
      {
        "slug": "influxdb-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "influxdb-clear-configuration-parameters",
        "title": "Clear configuration parameters"
      },
      {
        "slug": "influxdb-descriptive-semantic-naming",
        "title": "Descriptive semantic naming"
      },
      {
        "slug": "influxdb-document-complete-data-flows",
        "title": "Document complete data flows"
      },
      {
        "slug": "influxdb-follow-api-conventions",
        "title": "Follow API conventions"
      },
      {
        "slug": "influxdb-handle-errors-by-criticality",
        "title": "Handle errors by criticality"
      },
      {
        "slug": "influxdb-manage-complete-cache-lifecycle",
        "title": "Manage complete cache lifecycle"
      },
      {
        "slug": "influxdb-minimize-critical-path-allocations",
        "title": "Minimize critical path allocations"
      },
      {
        "slug": "influxdb-performance-conscious-metrics-implementation",
        "title": "Performance-conscious metrics implementation"
      },
      {
        "slug": "influxdb-prefer-explicit-nullability",
        "title": "Prefer explicit nullability"
      },
      {
        "slug": "influxdb-promote-code-clarity",
        "title": "Promote code clarity"
      },
      {
        "slug": "influxdb-secure-token-lifecycle",
        "title": "Secure token lifecycle"
      },
      {
        "slug": "influxdb-stable-schema-identifiers",
        "title": "Stable schema identifiers"
      },
      {
        "slug": "influxdb-use-structured-logging-fields",
        "title": "Use structured logging fields"
      }
    ],
    "comments": {
      "influxdb-secure-token-lifecycle": [
        "Is it possible to assert that the new token is assigned a new unique `TokenId` from the previous deleted token? The catalog repository type should handle this but having a check in place would be good.\r\n\r\nWe also do hard deletion in other places (LVC, DVC, and triggers) and may not have such a check in place, but I think it is worth adding for tokens here if possible, given sensitive nature.",
        "Does this work because the `_admin` token has been deleted?\r\n\r\ni.e., normally this request to create the token would require a valid auth token, but the `_admin` token has been deleted, and there are no tokens to authenticate, so the create token request is allowed.",
        "Ah, right, that makes sense. Thanks for clarifying."
      ],
      "influxdb-promote-code-clarity": [
        "Could you move this code into a helper function since it is re-used in several places.",
        "Good call, I think I could do this by passing forward the `Arc<TableDefinition>` to the `MetaCacheExec` so it only pulls the column names when needed, e.g., for the `EXPLAIN` output. The table definition is already there so should not be too tricky, and yeah - given that part of the motivation for this whole execution plan implementation is to make it readable to the operator so I will get this in.",
        "https://github.com/influxdata/influxdb/issues/25582",
        "Yep, that works! I will apply it locally.",
        "Done in https://github.com/influxdata/influxdb/pull/25389/commits/71daada9884066ec8a2f7b2a1ac33d90fbf4cd99",
        "I moved out the system table related code to its own module in https://github.com/influxdata/influxdb/pull/25166/commits/7c1f4db1eea45f8b12cc6a013260fc668c658fc9 - 🧹 definitely cleaner.\r\n\r\nNo code was changed and CI is ✅ so I will get this merged."
      ],
      "influxdb-minimize-critical-path-allocations": [
        "Since this gets called in the write path for each line, might be worth returning a `Vec<&str>` or `Vec<Arc<str>>` to avoid the string copies. Or even a slice, if possible.\r\n\r\nFurthermore, having to do the lookup by ID for the name every time could also be avoided by holding the `Arc<str>` names around in the `TableDefinition` then just iterating over those directly.\r\n\r\nDepending on how far you take it, this could lead to a substantial change if you had to change the `TableDefinition` struct, so might be better for a follow-on if that's the case.",
        "This function no longer builds a new arrow schema on _every_ call. As a result, the arrow schema will only ever need to be rebuilt when new fields are added for caches that accept new fields, _or_ never for caches that have an explicit set of value columns. Furthermore, for the explicit case, it produces the value columns by iterating directly on the cache `IndexMap`, instead of iterating over the schema. The non-explicit case still needs to iterate over the schema and do a lookup to get column ID. Therefore, I suspect the explicit case will be considerably more performant.",
        "Changing this to use a `HashMap` instead of `BTreeMap` for faster lookups."
      ],
      "influxdb-clear-configuration-parameters": [
        "The default should already have been 1 day: https://github.com/influxdata/influxdb/blob/e4cfbf71f78ce44072ec48f41b83721af6d21799/influxdb3_catalog/src/log/versions/v2.rs#L568\r\n\r\nI guess this is just for the purpose of documentation? Since, the coded default doesn't appear in the CLI output anywhere."
      ],
      "influxdb-centralize-workspace-configurations": [
        "Might be good to keep the feature set at the workspace level, then don't need to set it in each individual cargo file.",
        "To clarify, it was previously set in the workspace `Cargo.toml`. Having it there could save someone a few head scratches if they pull one of the `v3` featured crates and find things broken, and I'm not sure we would ever have a crate that depends on one of these and does not use the feature."
      ],
      "influxdb-follow-api-conventions": [
        "FWIW there is a [`Time::checked_sub` API](https://github.com/influxdata/influxdb3_core/blob/fd0e474a6c0af5ba867399d753f5df18f59907cb/iox_time/src/lib.rs#L149-L155) that you could use here to directly subtract the retention period `Duration` from `self.time_provider.now()`.",
        "Might be a case worthy of [`str::rsplit_once`](https://doc.rust-lang.org/std/primitive.str.html#method.rsplit_once) so that for, e.g., `foo:bar:apiv3_...`, it will only take `apiv3_...`.",
        "Several types are duplicated in this crate, e.g., `Format`, from the `influxdb3_server` crate. Perhaps we should pull `influxdb3_server` as a dependency of `influxdb3_client` and re-use them directly, or move the types to a central crate.\r\n\r\nBut this has bit us a couple times now.",
        "https://github.com/influxdata/influxdb/issues/24672",
        "With `reqwest`, you can use the [`query` API](https://docs.rs/reqwest/latest/reqwest/struct.RequestBuilder.html#method.query) to set the param, e.g.,\r\n```suggestion\r\n    /// Make a request to the `DELETE /api/v3/configure/database?db=foo` API\r\n    pub async fn api_v3_configure_db_delete(&self, db: impl AsRef<str> + Send) -> Result<()> {\r\n        let api_path = \"/api/v3/configure/database\";\r\n\r\n        let mut url = self.base_url.join(api_path)?;\r\n\r\n        let mut req = self.http_client.delete(url).query(&[(\"db\", db.as_ref())]);\r\n```"
      ],
      "influxdb-performance-conscious-metrics-implementation": [
        "This will likely be an issue, since the HTTP endpoint that serves prometheus (`/metrics`) assumes a single registry: https://github.com/influxdata/influxdb/blob/be25c6f52b046e57ec909b815e5471d4c6bb4f19/influxdb3_server/src/http.rs#L734-L740\r\n\r\nIs the issue that using the same registry for multiple executors causes them to overwrite each other, or contend for locks with each other?",
        "Opened https://github.com/influxdata/influxdb/issues/25696"
      ],
      "influxdb-document-complete-data-flows": [
        "A useful addition to this diagram would be to show the entry point for writes from user, i.e., where do writes go from the user (`wal buffer`?), via an arrow. Otherwise, it is not clear on the order of operations. If you could connect the numbers from the steps described below to locations / arrows on the diagram, that would be helpful."
      ],
      "influxdb-handle-errors-by-criticality": [
        "These unwraps can probably be changed to errors, but if any of these fails, it means that there is a race condition, so it might be that panicking is the right thing.",
        "FWIW - I think at most it would be `warn!`, since telemetry not sending is not a critical, _wake up your engineers in the middle of the night_ kind of error 😆 \r\n\r\nI have been fairly loose with the use of `error!` myself, which probably needs to be revisited."
      ],
      "influxdb-choose-optimal-data-structures": [
        "Using [`indexmap`](https://github.com/indexmap-rs/indexmap) makes this assertion (and the use of `insta` snapshots for serialization tests) possible.",
        "Maybe add a note on why `IndexSet` was used, i.e., for fast lookup in addition to iteration to the doc comment.",
        "Since I insert columns into the cache using the ordering of fields in the `schema` ([here](https://github.com/influxdata/influxdb/pull/25109/files#diff-6740f1021d631fa570625b9b27f8b4692fd0777c50eabcc435cda7793d0da049R155-R159)), then when producing a record batch out of the cache ([here](https://github.com/influxdata/influxdb/pull/25109/files#diff-6740f1021d631fa570625b9b27f8b4692fd0777c50eabcc435cda7793d0da049R184-R193)), `IndexMap` allows to iterate over the map directly while producing the correct order of columns for the schema.",
        "The links in that comment look to be out-of-date. But I have added docs/comments in the code to help explain this.",
        "@pauldix - while working on https://github.com/influxdata/influxdb/pull/25125 to enable caches that add new fields, I have found that the insertion order guarantee falls apart in scenarios where writes come in with different fields/orders for different key values. So, we can't rely on that, and I will be removing the comments about insertion order.\r\n\r\nI still like the use of `IndexMap` because it gives fast iteration over keys, as well as fast lookups (see [here](https://github.com/indexmap-rs/indexmap?tab=readme-ov-file#performance)), but if we want to avoid the dependency, or just optimize for lookup speed, then we could use a `HashMap` here.",
        "I guess you avoid the `entry` API on `buffered_data.database_buffers` here in order to avoid cloning `db_name` on every access?\r\n\r\nWe could consider using the `hashbrown` crate for its `HashMap` impl (which gets used here and there in IOx) and has the [`entry_ref` API](https://docs.rs/hashbrown/latest/hashbrown/struct.HashMap.html#method.entry_ref). I think there are a few places in this code where we could leverage that - I think introducing `hashbrown` can be a separate issue/PR though."
      ],
      "influxdb-use-structured-logging-fields": [
        "It would be useful to log the `db_name` and `duration` provided.",
        "It would be useful to log the `db_name`",
        "```suggestion\r\n                info!(instance_id = ?instance_id, \"catalog not found, creating new instance id\");\r\n```\r\nJust encouraging use of `tracing`'s field syntax.\r\n\r\nThis is somewhat redundant with the `info!` emitted at the caller level, but I don't think it hurts to have.",
        "This will show up in the logs as `e=<error message>`, I would either format it inline or use a more descriptive name, e.g.,\r\n```rust\r\nerror!(error = %e, \"...\");\r\n```\r\nOr rename the `e` to `error`, such that it appears as `error=<error message>` in the logs."
      ],
      "influxdb-stable-schema-identifiers": [
        "I don't know if using `enumerate` to determine the column ID is a good idea. I think that generally, columns are always appended, in which case, it is okay, but in the event that we allow for dropping columns, then this would change their order and mess up the IDs.\r\n\r\nWe probably need some way to generate the IDs, based on what was the largest already used ID for a given table, and then ensure that that ID remains fixed for the column it is applied to for all time.",
        "This could be done by flipping the hashmap to\r\n```rust\r\nHashMap<TableId, (Arc<str>, TableChunks)>\r\n```\r\nSerde would then `Serialize`/`Deserialize` it gracefully using derive.\r\n\r\nNot sure how gracefully that fits in to the broader change set but it would certainly be nice to not need the custom serialization code.\r\n\r\nIf you need to have both the name and ID in the hash key then you could define a new-type, e.g.,\r\n```rust\r\n#[derive(/* ... */, Serialize, Deserialize)]\r\nstruct WriteBatch {\r\n    /* ... */\r\n    table_chunks: HashMap<TableKey, TableChunks>,\r\n}\r\n\r\nstruct TableKey(Arc<str>, TableId);\r\n```\r\nThen, implement `Serialize`/`Deserialize` on `TableKey`, vs. having to do it for the whole `WriteBatch` type.",
        "JSON doesn't have a tuple, but `serde_json` will serialize/deserialize tuples as lists (see [here](https://docs.rs/serde_json/latest/src/serde_json/ser.rs.html#303-305)). I think part of the issue would be using `TableId` as the key in the map, since JSON doesn't support integer map keys. For that we would either need to de/serialize `TableId`s as strings, or not use JSON.",
        "Should this take the next available table ID, instead of using 0?",
        "(the same would be said for the other parsing function)",
        "Ah, I see, the code in validator is a bit confusing right now. For example, it used to update the in-memory catalog schema using a `Cow` to check that it had changes which is why this is here: https://github.com/influxdata/influxdb/blob/9c71b3ce251f32cf8f23db0a4f09873e04686c1a/influxdb3_write/src/write_buffer/validator.rs#L514-L521\r\n\r\nBut it doesn't look like it uses the cow anymore. It does still update the catalog in its state here, though, by applying the catalog batch: https://github.com/influxdata/influxdb/blob/9c71b3ce251f32cf8f23db0a4f09873e04686c1a/influxdb3_write/src/write_buffer/validator.rs#L182-L196\r\n\r\nAnd I believe that will result in the table being created, so I think it should probably be using the next ID instead of 0. Perhaps a test would help suss that out.",
        "Need to handle for collisions here, i.e., to ensure there are no duplicate fields, and also check for ordering. I don't think that race conditions are a concern since this method is invoked from a single event loop, and is processing rows from the buffer sequentially.\r\n\r\nA good test would be to have a cache on the table `foo` with keys `[t1]` and values `[f1, time]`, and then write the following LP that would add `f2` and `f3` value columns:\r\n```\r\nfoo,t1=a f1=1,f2=2,f3=3\r\nfoo,t1=b f1=1,f3=3,f2=2\r\n```\r\nAssuming that `t1=a` and `t1=b` have already been written to, and therefore each have a cache associated; each cache will have the new value columns `f2` and `f3` added, but they will be added in a different order to each respective cache:\r\n```\r\nt1=a -> f2,f3\r\nt1=b -> f3,f2\r\n```\r\nThen check that `RecordBatch`es spanning both `t1` key values can be produced and combined.",
        "Something worth noting is that the write buffer is validating the incoming writes, so we shouldn't have to worry about new fields being written with incompatible types in subsequent lines of LP. I do think ensuring that ordering discrepancies like above should be handled.",
        "https://github.com/influxdata/influxdb/pull/25125/commits/a129d003397bbb859a112980aee3de6286d1adcf switched to using `SchemaBuilder::try_merge` to prevent conflicts, and with the added test case, the above scenario will not cause issues.\r\n\r\n_Edit_: that commit did not produce the correct behaviour 🤦",
        "https://github.com/influxdata/influxdb/pull/25125/commits/2917fc149766ac349a4cb297f86ea7dfb6395797 Looks to have resolved this."
      ],
      "influxdb-manage-complete-cache-lifecycle": [
        "This test was removed because cache creation no longer behaves the same as before - previously if you made the same request to create a cache twice, the second one would succeed with a `204 NO CONTENT`, but have no effect; now, the second one fails with a `409 CONFLICT`.",
        "Yeah, only if it needs to prune would lock the inner map, but I agree, this is a little over-zealous.",
        "Each of these functions acquires their own write lock. Originally, they were called from separate places, but if they always get called at the same time like this, it might be better to do the evict and write in the same call, under the same write lock.",
        "> I think it should be changed to only do eviction on snapshot, that way we're not spending too much time on it.\r\n\r\nThat sounds reasonable.",
        "Yes, good call, this only removes the values and is not walking up and cleaning up the maps. I'll address that with the other immediate issues in a follow-on PR."
      ],
      "influxdb-prefer-explicit-nullability": [
        "This is a textbook use case for [`NonZeroUsize`](https://doc.rust-lang.org/std/num/type.NonZeroUsize.html), i.e.,\r\n```rust\r\n/// Must be greater than 0\r\n#[derive(Debug, Serialize, Eq, PartialEq, Clone, Copy)]\r\npub struct LastCacheSize(pub(crate) NonZeroUsize);",
        "No worries. We can refactor this later",
        "When I first refactored this, I had the `LastCacheKey` hold a `HashMap<Option<KeyValue>, LastCacheState>` to handle null key values, but switched to this behaviour to simplify it.\r\n\r\nI think It just needs to do that, i.e., use `Option<KeyValue>` instead of `KeyValue`, and then store the datatype in the `LastCacheKey`, because 1) key columns will always have a fixed data type, and 2) you can't rely on the `KeyValue` alone to get the data type when it is `None` (for creating `RecordBatch`es).\r\n\r\nI can open up an issue for this.",
        "The [`is_valid` function](https://docs.rs/arrow/latest/arrow/array/trait.Array.html#method.is_valid) indicates whether the value at given index is null or not - the data is still _valid_, if that makes sense. So, instead of `bail!`ing here, I think you can just `continue;` in the inner loop, leaving the cell as `Value::Null`."
      ],
      "influxdb-descriptive-semantic-naming": [
        "Addressed this in https://github.com/influxdata/influxdb/pull/25722/commits/cd51bc2beda9a23446d694ee411e409871b1de39"
      ],
      "influxdb-avoid-flaky-test-patterns": [
        "Yeah, `insta` is compelling but not the  right tool here. I think @waynr was grappling with a similar issue in clustered so I may pick his brain to see if he landed on a solution.",
        "I opened https://github.com/influxdata/influxdb/issues/25493"
      ],
      "influxdb-choose-appropriate-lock-primitives": [
        "I agree, I opened https://github.com/influxdata/influxdb/issues/25382 to look for alternatives.\r\n\r\nI think most straight-up LRU implementations will have this problem, given that they need to update the recency when getting an item (the popular [`lru` crate](https://crates.io/crates/lru/) that `clru` is based on is essentially the same API).",
        "Addressed in https://github.com/influxdata/influxdb/pull/25377/commits/ac8d7d3ba9ce25cf0dd04b4471e955e8ae4e1790",
        "Might be better to do the remove in `set_success` directly, so that it does it all under one lock.",
        "DashMap could definitely be useful here. I actually was using it at one point, so maybe I will re-introduce. I want to read more about how it works though.",
        "I switched over to using `DashMap` which hopefully will help reduce some lock contention.",
        "I changed it so that there is only one top-level lock, and now it is a three-level hashmap, so that it also stores multiple caches per table."
      ]
    },
    "profile": {
      "location": "Canada",
      "blog": "trevorjhilton.com",
      "site_admin": false,
      "followers": 10,
      "following": 6
    }
  },
  "sbrannen": {
    "repos": [
      "spring-projects/spring-framework"
    ],
    "entries": [
      {
        "slug": "spring-framework-api-boundary-null-handling",
        "title": "API boundary null handling"
      },
      {
        "slug": "spring-framework-cleanup-error-handling",
        "title": "Cleanup error handling"
      },
      {
        "slug": "spring-framework-complete-api-documentation",
        "title": "Complete API documentation"
      },
      {
        "slug": "spring-framework-consistent-style-conventions",
        "title": "Consistent style conventions"
      },
      {
        "slug": "spring-framework-database-agnostic-sql-syntax",
        "title": "Database-agnostic SQL syntax"
      },
      {
        "slug": "spring-framework-descriptive-specific-names",
        "title": "Descriptive specific names"
      },
      {
        "slug": "spring-framework-design-for-api-extension",
        "title": "Design for API extension"
      },
      {
        "slug": "spring-framework-optimize-ci-environment-configuration",
        "title": "Optimize CI environment configuration"
      },
      {
        "slug": "spring-framework-package-null-safety-annotations",
        "title": "Package null-safety annotations"
      },
      {
        "slug": "spring-framework-respect-annotation-processing-order",
        "title": "Respect annotation processing order"
      },
      {
        "slug": "spring-framework-spring-annotation-best-practices",
        "title": "Spring annotation best practices"
      },
      {
        "slug": "spring-framework-spring-code-style",
        "title": "Spring code style"
      },
      {
        "slug": "spring-framework-use-assertj-correctly",
        "title": "Use AssertJ correctly"
      },
      {
        "slug": "spring-framework-use-documentation-features-properly",
        "title": "Use documentation features properly"
      },
      {
        "slug": "spring-framework-use-environment-independent-defaults",
        "title": "Use environment-independent defaults"
      },
      {
        "slug": "spring-framework-verify-operation-semantics",
        "title": "Verify operation semantics"
      }
    ],
    "comments": {
      "spring-framework-respect-annotation-processing-order": [
        "Please move the interface search above the annotation search on the current class (i.e., directly under `if (visited.add(sourceClass)) {`).\r\n\r\nAlso, please add a test which verifies that locally declared `@Import` annotations are processed _after_ `@Import` annotations discovered on implemented interfaces. For example, a bean imported from a local `@Import` annotation should be able to override a bean imported via an `@Import` declaration on an implemented interface.",
        "`OverridingConfig` should be annotated with `@Import` (and import something that overrides the `ImportedBean`) in order for `localImportShouldOverrideInterfaceImport()` to test this use case.",
        "Before @aahlenst posted those last two comments, I was also going to point out that there is a precedent for this, namely `@DirtiesContext`.\r\n\r\nI agree that it is a bit strange for it to be possible to specify a class-level setting at the method level that simply gets ignored; however, I am not in favor of introducing yet another SQL-related annotation for this purpose. We already have quite a few. And I don't think it necessarily warrants an additional enum and annotation attribute.\r\n\r\nAs it stands in the proposal, the addition of the two new enum constants in `ExecutionPhase` makes it impossible for a user to configure conflicting phases for a given `@Sql` declaration, and that's a good thing.\r\n\r\nFWIW, I have never heard any complaints from users about the `ClassMode` and `MethodMode` dichotomy in `@DirtiesContext`, and the documentation clearly states that setting an inappropriate mode \"has no meaning\" (i.e., will be ignored).\r\n\r\nIn light of that, I am OK with the PR as-is.",
        "Yes, we can definitely throw an exception for inappropriate user configuration.\r\n\r\n`IllegalArgumentException` sounds reasonable to me.",
        "> We throw now.\r\n\r\nThanks\r\n\r\n> Just thinking aloud: If `@Sql` has this, wouldn't it be nice if `@DirtiesContext` had it, too? Or would that be too disruptive years after the introduction?\r\n\r\n`@Sql` is different because it has a single enum-based `executionPhase` attribute, which means we can easily detect a user configuration error.\r\n\r\nWhereas, `@DirtiesContext` has two mutually exclusive enum-based attributes: `classMode` and `methodMode`, and each of those has a `default` value. Thus, we cannot actually detect a user configuration error.\r\n\r\nThe only way to be able to detect a user configuration error for `@DirtiesContext` would be to introduce new `DEFAULT` enum constants in `ClassMode` and `MethodMode` and use those as the `default` attribute values going forward.\r\n\r\nHowever, that would be a breaking change for any users who had inadvertently explicitly supplied the previous default values for those attributes.\r\n\r\nPlus, it would make things a bit more cumbersome.\r\n\r\nIn light of the above, I don't think we should make any changes to `@DirtiesContext` in this regard this late in the game.\r\n"
      ],
      "spring-framework-verify-operation-semantics": [
        "That's not entirely true.\r\n\r\nSpEL does not honor all of Groovy's \"truth\" rules.\r\n\r\nThe SpEL Elvis operator checks for values that are non-null **and** non-empty (for Strings).",
        "Selection is actually supported for arrays and anything that implements `java.lang.Iterable` or `java.util.Map`.\r\n\r\nWould you like to update your PR to reflect that?"
      ],
      "spring-framework-consistent-style-conventions": [
        "Please sort dependencies alphabetically in the build script.",
        "The `RowMapper` implementation is stateless. Therefore, please store a single reference to it in a `final` field and remove the `getActorMapper()` method.",
        "Please inline the lambda expression."
      ],
      "spring-framework-use-documentation-features-properly": [
        "Yes, indeed. That's a step in the right direction. I'll rework the wording locally after merging the PR.",
        "```suggestion\r\nthe <<webflux, Spring WebFlux>> framework,\r\n```\r\n\r\nYour fix works; however, when cross referencing a section within the current file (after includes have been applied), there is no need to use the folder and file name.\r\n\r\nI'll simplify that when merging the PR."
      ],
      "spring-framework-spring-annotation-best-practices": [
        "```suggestion\r\n        private Pojo self;\r\n```\r\n\r\nUnless you are certain CGLIB proxies are being created, this would need to be `Pojo`.\r\n\r\nThough, there's no need to update this PR. I'll address that after merging."
      ],
      "spring-framework-descriptive-specific-names": [
        "```suggestion\r\n\tprivate boolean quoteIdentifiers = false;\r\n```\r\n\r\n\"Using escaping\" is too generic and could potentially conflict with a (yet unknown) future feature.\r\n\r\nLet's name this according to what it is actually used for.\r\n\r\nNote that I've updated the title of this PR to reflect that as well."
      ],
      "spring-framework-cleanup-error-handling": [
        "At a quick glance, this change is the only change in this PR that does not break existing behavior.\r\n\r\nCan you please revisit your changes and ensure that existing behavior is not altered?\r\n\r\nFor example, a try-with-resources block will close the `AutoCloseable` object, but it will not swallow exceptions thrown by the invocation of `close()`. So anywhere that we were previously intentionally swallowing exceptions such as `IOException`, we would still have to swallow those exceptions.\r\n\r\nIn addition, we would have to be able to disambiguate between an `IOException` thrown from within the try-block vs. one thrown by the invocation of `close()`.\r\n\r\nIn light of that, I don't think it makes sense to use try-with-resources in such use cases, but I'm happy to be proven wrong.",
        "Actually, I'd appreciate it if you could keep the one valid change and add inline documentation to the other locations to point out why try-with-resources should not be used there.\r\n\r\nThat will provide a big help to the team to make sure we don't accidentally switch to try-with-resources for those use cases in the future.",
        ">Actually, I'd appreciate it if you could keep the one valid change and add inline documentation to the other locations to point out why try-with-resources should not be used there.\r\n\r\nOf course if you'd prefer to simply close this PR, that's also fine.\r\n\r\nI'll leave it up to you.",
        "Thanks for making the changes. That looks good now."
      ],
      "spring-framework-use-assertj-correctly": [
        "Please replace all `assert` statements with AssertJ assertions.",
        "Please use AssertJ for assertions.\r\n\r\nThe build will fail if you attempt to use JUnit Jupiter's `Assertions` class.\r\n\r\nSpeaking of which, please run a full build locally before submitting a PR (`./gradlew check`) to ensure that the build succeeds.",
        "When we check the size of a collection using AssertJ, we prefer to use `assertThat(list).hasSize(...)`.",
        "Please replace with an AssertJ assertion along the lines of `assertThatExceptionOfType(BadSqlGrammarException.class).isThrownBy(...)`.",
        "It is not permissible to use JUnit Jupiter's `Assertions`. Doing so will actually fail the build.\r\n\r\nThus, please switch to AssertJ assertions and be sure to execute `./gradlew check` to verify that you don't have any Checkstyle violations in this regard.\r\n\r\nNote: if you want something similar to `assertAll`, you may wish to use `SoftAssertions` from AssertJ.",
        "We use AssertJ for test assertions, not Spring's `org.springframework.util.Assert` class.\r\n\r\nIn any case, I'll make that change when merging the PR.",
        "For all assertions that are expected not to throw an exception, please rework them to use the following AssertJ construct, where `{}` is the code in question.\r\n\r\n```java\r\nassertThatCode(() -> {}).doesNotThrowAnyException();\r\n```"
      ],
      "spring-framework-optimize-ci-environment-configuration": [
        "I'd also recommend using a more recent version of JDK 8, like we do in the [JUnit 5 build](https://github.com/junit-team/junit5/blob/master/.travis.yml).\n",
        "> Travis CI is using 1.8.0_65 as is... that doesn't seem so bad. In the interest of saving build time, it seems best not do sudo apt-get update && sudo apt-get install oracle-java8-installer as your junit example does. What do you think?\n\nIf Travis CI is now using JDK 8 Update 65 as the default, then I agree: that should be fine.\n\nFor the JUnit 5 build, that wasn't the case when we set up the build: Travis CI used to default to an early JDK 8 release that didn't work for us.\n"
      ],
      "spring-framework-package-null-safety-annotations": [
        "Ideally this should enforce that both `@NonNullApi` and `@NonNullFields` are present -- though I suppose that would require two regular expressions (for the \"single line\" approach).\r\n",
        "```suggestion\r\n\t\t\t<property name=\"minimum\" value=\"2\"/>\r\n\t\t\t<property name=\"maximum\" value=\"2\"/>\r\n```\r\n\r\nWe expect exactly 2 `@NonNull*` declarations, specifically `@NonNullApi` and `@NonNullFields`.",
        "```suggestion\r\n\t\t\t<property name=\"message\" value=\"package-info.java is missing required null-safety annotations @NonNullApi and @NonNullFields.\"/>\r\n```\r\n\r\nSpring's `@NonNullApi` and `@NonNullFields` annotations should never span multiple lines."
      ],
      "spring-framework-database-agnostic-sql-syntax": [
        "```suggestion\r\n\t\tString expected = \"INSERT INTO `S`.`T` (`F`, `S`) VALUES(?, ?)\";\r\n```\r\n\r\nJust a side note: the schema and table names have to be quoted independently.\r\n\r\nI've fixed this in my local branch and added integration tests with H2 to verify it."
      ],
      "spring-framework-spring-code-style": [
        "In general, contributors are required to adhere to the [Spring Framework Code Style](https://github.com/spring-projects/spring-framework/wiki/Spring-Framework-Code-Style). So please familiarize yourself with that and rework your PR.\r\n\r\nFor example, you need to undo **all** changes to import ordering and static imports throughout all changed classes.",
        "The team has not yet decided to permit `var` declarations in _production code_ yet, so please use typed variable declarations for the time being.",
        "Although I fully understand the desire to remove `public` here, historically the code base often makes constructors `public` even if the enclosing type is not `public`.\r\n\r\nLook at the other `JdkClientHttpRequest` implementation as well as neighboring classes for examples.\r\n\r\nIn light of that, I removed this change when merging the PR.",
        "We do not use static imports for utility methods in the core Spring Framework.",
        "In any case, please remove empty space after an opening parenthesis and before a closing parenthesis, since that is the standard convention within the Spring Framework codebase.\r\n\r\nAnd please make that change consistently within this PR.",
        "We don't use `var` in Spring Framework.\r\n\r\nThus, please remove all usage of `var` in this PR.",
        "```suggestion\r\n\t\tString ip = \"10.0.0.1\";\r\n```\r\n\r\nWe don't typically declare local variables as final unless there's a compelling reason.",
        "Please note that the formatting you introduced fails the build with the following.\r\n\r\n```\r\n> Task :spring-beans:checkstyleMain\r\n[ant:checkstyle] [ERROR] /.../spring-beans/src/main/java/org/springframework/beans/support/PropertyComparator.java:116: Line has leading space characters; indentation should be performed with tabs only. [RegexpSinglelineJava]\r\n[ant:checkstyle] [ERROR] /.../spring-beans/src/main/java/org/springframework/beans/support/PropertyComparator.java:117: Line has leading space characters; indentation should be performed with tabs only. [RegexpSinglelineJava]\r\n[ant:checkstyle] [ERROR] /.../spring-beans/src/main/java/org/springframework/beans/support/PropertyComparator.java:118: Line has leading space characters; indentation should be performed with tabs only. [RegexpSinglelineJava]\r\n```\r\n\r\nPlease make sure you run `./gradlew check` before submitting a PR to catch such errors locally.\r\n\r\nI'll fix the formatting before merging, so there's no need to update this PR."
      ],
      "spring-framework-api-boundary-null-handling": [
        "This breaks our null-safety contracts.\r\n\r\nIf we were to implement this method in `SimpleEvaluationContext`, we would want to return an empty list.\r\n\r\nHowever, by changing the method to a default method in `EvaluationContext`, we will no longer need to override this method (unless we decide to support custom `IndexAccessor` implementations in `SimpleEvaluationContext`).",
        "We rely on nullability constraints in such cases, and `delimiters` should never be `null`, since `@Nullable` is not declared for that method parameter.\r\n\r\nIf a user supplies a `null` value for the var-args array, the resulting `NullPointerException` should suffice to inform them of their error.",
        "```suggestion\r\n\t\tsuper(bufferFactory, new HttpHeaders(new NettyHeadersAdapter(Objects.requireNonNull(response, \"HttpServerResponse must not be null\").responseHeaders())));\r\n```\r\n\r\nAlthough it will throw a `NullPointerException` instead of an `IllegalArgumentException`, if we use `Objects.requireNonNull` we can still have a custom error message.\r\n\r\nSo let's go with that."
      ],
      "spring-framework-complete-api-documentation": [
        "```suggestion\r\n\t/**\r\n\t * Returns a concise description of this {@code HandlerMethod}, which is used\r\n\t * in log and error messages.\r\n\t * <p>The description should typically include the method signature of the\r\n\t * underlying handler method for clarity and debugging purposes.\r\n\t */\r\n```",
        "Please document the `RetryExecution` parameter in all methods in this interface.",
        "Add class-level Javadoc.",
        "Indexing it not limited to arrays.\r\n\r\nIn addition, the rest of the Javadoc in this class was copied from `PropertyAccessor` and needs to updated to discuss indexing instead of property access.",
        "In Spring Framework, we attempt to wrap Javadoc around 90 characters, and we never end a `@param` or `@return` description with a period `.` unless the description includes sentences (which we try avoid)."
      ],
      "spring-framework-use-environment-independent-defaults": [
        "Yes, that's problematic in Eclipse IDE.\r\n\r\n```\r\nThe import org.springframework.web.reactive.function.client.CoExchangeFilterFunction cannot be resolved\r\n\r\nDefaultWebClient.java\r\n\r\n/spring-webflux/src/main/java/org/springframework/web/reactive/function/client\r\n\r\nline 61\r\n```",
        "These changes break the build.\r\n\r\nPlease revert these changes.\r\n\r\n[Pattern Matching for switch](https://openjdk.org/jeps/441) did not become an \"enabled\" feature until Java 21, and we have a Java 17 baseline.\r\n\r\nMore importantly, please ensure that you actually run the build before submitting a PR. \r\n\r\n"
      ],
      "spring-framework-design-for-api-extension": [
        "Since this is a `protected` method in a `public` type, we cannot simply change the method signature.\r\n\r\nInstead, we would need to introduce an overloaded variant that accepts `Executable` and deprecate the existing variant that accepts `Member`.",
        "I have not investigated the claims in this issue or the rest of this PR; however, based on a quick glance I can say that this change is not permissible since it changes the signature of a public API."
      ]
    },
    "profile": {
      "location": "Zurich, Switzerland",
      "company": "Broadcom",
      "blog": "https://SamBrannen.com",
      "twitter_username": "Sam_Brannen",
      "site_admin": false,
      "followers": 1249,
      "following": 15
    }
  },
  "ololobus": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-adaptive-cache-expiration-strategy",
        "title": "Adaptive cache expiration strategy"
      },
      {
        "slug": "neon-avoid-flaky-tests",
        "title": "Avoid flaky tests"
      },
      {
        "slug": "neon-cache-performance-preservation",
        "title": "Cache performance preservation"
      },
      {
        "slug": "neon-clear-consistent-identifier-names",
        "title": "Clear consistent identifier names"
      },
      {
        "slug": "neon-comprehensive-code-documentation",
        "title": "Comprehensive code documentation"
      },
      {
        "slug": "neon-configuration-context-alignment",
        "title": "Configuration context alignment"
      },
      {
        "slug": "neon-design-metrics-for-insights",
        "title": "Design metrics for insights"
      },
      {
        "slug": "neon-document-api-specs-completely",
        "title": "Document API specs completely"
      },
      {
        "slug": "neon-document-connection-transitions",
        "title": "Document connection transitions"
      },
      {
        "slug": "neon-document-parameter-choices",
        "title": "Document parameter choices"
      },
      {
        "slug": "neon-flexible-documented-configurations",
        "title": "Flexible documented configurations"
      },
      {
        "slug": "neon-hierarchical-semantic-naming",
        "title": "Hierarchical semantic naming"
      },
      {
        "slug": "neon-keep-files-focused-small",
        "title": "Keep files focused small"
      },
      {
        "slug": "neon-log-level-appropriately",
        "title": "Log level appropriately"
      },
      {
        "slug": "neon-proactive-cache-warming",
        "title": "Proactive cache warming"
      },
      {
        "slug": "neon-proper-metrics-design",
        "title": "Proper metrics design"
      },
      {
        "slug": "neon-scope-jwt-authentication-tokens",
        "title": "Scope JWT authentication tokens"
      },
      {
        "slug": "neon-stage-configuration-changes-gradually",
        "title": "Stage configuration changes gradually"
      },
      {
        "slug": "neon-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      }
    ],
    "comments": {
      "neon-stage-configuration-changes-gradually": [
        "I think it shouldn't be a part of feature flags, which are meant to be temporary. Instead, it should be a part of the main spec body because it's a permanent feature/mode\r\n\r\n(This `ActivityMonitorExperimental` is a bit special, I left it for longer, because statistics-based monitoring is tricky, but we should also remove it already)"
      ],
      "neon-design-metrics-for-insights": [
        "It usually makes sense to track 2 parameters out of 3: total, failed, success, so that you can always reconstruct all 3. Because just number of requests doesn't tell us much, we care more about success/error rates. Could be a separate PR, up to you"
      ],
      "neon-keep-files-focused-small": [
        "NIT: this `compute.rs` is really huge already. Should we move all new method implementations and structs into `compute_prewarm.rs`? It will be pretty well-scoped and should improve navigation and readability. WDYT?\r\n\r\nPersonally, I was already struggling with the `compute.rs` size",
        "> It seems like we have a good pattern going where the ComputeNode methods become small wrappers around functions in other files.\r\n\r\nThis could be the way to go as well, but I was more thinking about just having a second `impl` block like\r\n```rust\r\nimpl ComputeNode {\r\n    pub async fn prewarm_status(&self) -> PrewarmStatus {...}\r\n\r\n    // The rest of prewarm methods\r\n}\r\n\r\n// The rest of prewarm structs\r\n```\r\nin a separate file `compute_prewarm.rs`. If that's possible in Rust (afaik, it's)"
      ],
      "neon-proactive-cache-warming": [
        "Makes sense, @MMeent can you suggest what the raw numbers should be? Like `target_lfc_size_pages` and `processed_lfc_pages`? Will it work?",
        "Added, thanks",
        "I mean that we just start primary from scratch with empty caches, the only option to improve the situation is to do async auto-prewarm, while already accepting new connections. So from cplane perspective, it's just a normal start from with auto-prewarm enabled",
        "Well, that's debatable whether we need auto-prewarm or not at all. It exists in vanilla Postgres. The idea is that we can prewarm caches faster when we do it intentionally vs. when user tries to prewarm by just doing their normal workload\r\n\r\nImagine cplane, it eventually accesses all non-deleted projects/endpoint/branches. If we just restart it at Neon, it will take some time to visit all objects. Yet, if we actively prewarm caches in the brackground, the chance that next project read will hit the cache will be higher, as it will be already there, even though cplane hasn't read it explicitly\r\n\r\nIn practice, it may not suite all workloads, but we cannot answer for sure until we implement it and battle-test, but imo it exists in vanilla Postgres for a reason, so there are use-cases",
        "Well, seconds is just a unit, it can be set to 5, 15 minutes. For cplane-orchestrated there is a separate API, I imagined periodic dumping to be useful for\r\ni) auto-prewarm, i.e. compute periodically dumps LFC content, so later it can be used at restart. In theory, we can only dump at graceful shutdown, but then it won't help with accidental compute crash/restart, as there might be no LFC state to warm up from\r\nii) later for having a hot standby, i.e. it can periodically fetch fresh content from S3 and do prewarming\r\n\r\nI don't want to wire too complex logic via cplane, so TBH, I don't see other options to have a robust auto-prewarm without periodic dumping of the LFC state, pg_prewarm does the same via `pg_prewarm.autoprewarm_interval`\r\n\r\n@MMeent @knizhnik do you have any specific suggestions of how we can implement it without periodic dumping?",
        "Thanks for the comment about hot standby\r\n\r\n> We can make it part of endpoint shutdown procedures?\r\n\r\nYes, this is what I meant by 'In theory, we can only dump at graceful shutdown'. That'd work in most of the cases, but what I don't like is that it doesn't cover any abnormal termination like OOM, VM restarts, etc.\r\n\r\nWith your cost estimation, dumping every 5 minutes is completely reasonable",
        "> Note that \"i.e. it can periodically fetch fresh content from S3 and do prewarming\" won't work as you seem to expect it to, as prewarming is explicitly designed to never evict existing pages from LFC, and thus won't do much if the size of LFC doesn't change for the hot standby. It's prewarming by replacing unused pages with potentially useful pages, and explicitly not LFC state synchronization.\r\n\r\nRe-reading it after a long time and now it still looks like it should work. Like\r\n\r\n1. We did prewarm once\r\n2. After some time, we fetch LFC content again and iterate over blocks to check if they are present in the LFC\r\n2.1. If block is in LFC -- good, WAL replay should keep it up-to-date\r\n2.2. If it's not in LFC -- we will fetch it from pageserver\r\n\r\nThat way, we do not need any eviction explicitly, and it will help with keeping the LFC relatively warm. Not saying that we need to do it exactly like that, I like you suggestion with switching the replay mode and replaying all pages, it's just this could be a viable alternative\r\n\r\nOr do I miss something?",
        "I was actually thinking about using the default that pg_prewarm uses -- 300s. I think it's frequent enough for this purpose. This will lower it it to ~$40 per 1k computes per month, which is good enough, imo",
        "Mentioned this default explicitly, thanks for the estimation"
      ],
      "neon-avoid-flaky-tests": [
        "Any sleep-based waiting in tests almost certainly causes flakiness. Please, rewrite it into waiting for the LFC content to appear in the remote storage. There is a generic helper in python tests for that -- `wait_until()`, see usage in other tests"
      ],
      "neon-log-level-appropriately": [
        "NIT: I'd reverse this and instead log 'Skipping pgbouncer and local_proxy termination because in dev mode' when dev_mode is true. Otherwise, we log this in real envs, but it doesn't make any sense as we log separate line when we actually send signals"
      ],
      "neon-document-connection-transitions": [
        "Primary in the diagram is the old primary, and it's shut down first, it's also mentioned in text. Or what do you mean?",
        "My intent was to keep it reasonably high-level. Do you see some important interactions missing here?",
        "This step is right after we terminate the primary, so yes, during normal termination, we can expect that at this moment primary will be already terminated and all connections to it will be closed.\r\n\r\nI added this item after talking to Stas, as he had a fair point that the old primary could be unresponsive during this promotion flow, so we will send termination and k8s resources deletion requests, but we cannot generally guarantee that it will be dead by this time. So this step is more to protect from the situation, when old connections will still be connected to the old primary\r\n\r\nSee also item 7 in failure modes. I'm not quite sure how big the problem is. Safekeepers will guarantee that there is only one running writer at a time, so it's more like a nice-to-have, than must-have feature, just to prevent unnecessary interference and side effects (I worried about some stale reads from the old primary and failing writes because safekeepers should reject them)"
      ],
      "neon-configuration-context-alignment": [
        "It should default to false",
        "@knizhnik I still see that it defaults to true"
      ],
      "neon-document-parameter-choices": [
        "Can you please add test comments clarifying what parameters mean and what are the different test modes? After quickly eyeballing the test, I cannot easily grasp the `with_compute_ctl` and why we pass `ids` as a test parameter"
      ],
      "neon-adaptive-cache-expiration-strategy": [
        "Replied here https://github.com/neondatabase/neon/pull/11294#discussion_r2006276800 and here https://github.com/neondatabase/neon/pull/11294#discussion_r2006283261 as well\r\n\r\n> Wondering if there's any interactions between the user workload and prewarm that are worth considering.\r\n> if it's available then the LFC cache might have become stale over time.\r\n\r\nThis PR https://github.com/neondatabase/neon/pull/10442 introduced additional locking when accessing LFC, so it's now considered safe to write there concurrently, so that's the base for all this work.\r\n\r\nDuring prefetch, we always request the latest pages from the pageserver. If, after loading page gets modified, then it will be either updated in LFC (in case of primary) or evicted from LFC after receiving the corresponding WAL record (in case of replica). In other words, if pages is not present in the LFC, we will fetch it from the pageserver; if someone (backend, normal client workload) tries to write it concurrently, then the access will be synchronized, and we should still get a freshness guarantee. @knizhnik or @MMeent can correct me, as I'm not fluent in the underlying mechanism, I consider it as given here\r\n\r\nThus, it should be safe to prewarm LFC concurrently with user load. The only problem is performance, I wrote about it in other comments, but anyway. Yes, if it's highly intensive workload, then prewarm can compete for storage resources with user workload, so we can consider auto-prewarm to be user-togglable feature, I wrote about it in the section about auto-prewarm concerns\r\n\r\n@VladLazar @mtyazici let me know if it makes it clearer",
        "> Logical Replication\r\n\r\nI recall that Konstantin did a POC like that. We discarded that because it only helps with keeping a warm replica, and it's not possible to implement autoprewarm with that + it bloats the WAL on safekeepers and eats the network bandwidth. It's not a big problem, as Pageservers should discard such records during ingestion, so it wont bloat the data files, but it's still nice to avoid\r\n\r\n> The idea here is to expose some LFC primitives at the SQL level on the primary. The API should allow for fetching the current state of the LFC in a way that it can be reproduced\r\n\r\nI don't remember that we considered any diffing, but otherwise it's pretty much how it works -- we have SQL funcs to dump/load caches state",
        "If we consider this compute data as non-critical, could we avoid explicit deletion completely? I was thinking about setting a TTL for perfix/bucket https://docs.aws.amazon.com/AmazonS3/latest/userguide/how-to-set-lifecycle-configuration-intro.html (never used it personally, though)\r\n\r\nThat should most likely work for prewarm/caches content. Assuming we set it to a high enough value (like 7d or 30d), if one doesn't start endpoint for that long, they likely don't care about prewarming much. For `pg_stat_statements` it's pretty much the same -- well, your perf data will expire after N days -- sounds fair. For stats it could be a bit more annoying, but again should be not critical at all\r\n\r\nAt the same time, with TTL we avoid implementing a huge piece of deletion orchestration.\r\n\r\nWhat do you think?",
        "Explicit deletion would work as well, I think, just more work on the control plane side"
      ],
      "neon-cache-performance-preservation": [
        "Not a fan of wiring even more stuff via cplane. The flushLSN should be just the last 'consensus' LSN from sefekeepers, right? Cannot compute figure out it on its own before/during promotion (kinda incomplete sync-safekeepers, just without data copying, or we can even do a normal sync-safekeepers on compute before promotion)?\r\n\r\nThat'd be much more robust and less bug-prone because it doesn't put any implicit assumptions that someone passes the right LSN to us"
      ],
      "neon-proper-metrics-design": [
        "I think it's not considered a best-practice, in the docs it's formulated like\r\n\r\n> As a rule of thumb, either the sum() or the avg() over all dimensions of a given metric should be meaningful (though not necessarily useful). If it is not meaningful, split the data up into multiple metrics.\r\n\r\nhttps://prometheus.io/docs/practices/naming/ (see other suggestions there)\r\n\r\nI suggest you split it into two separate metrics with a clear meaning"
      ],
      "neon-clear-consistent-identifier-names": [
        "NIT, but it impacts readability a lot -- `state.state` -- what state of what state? I suggest making it more clear\r\n\r\n```suggestion\r\n        compute: &Arc<ComputeNode>,\r\n```"
      ],
      "neon-use-descriptive-identifiers": [
        "Am I right that instead of these cryptic `prewarm_info[n]` you can use here `total, prewarmed, skipped` defined above?"
      ],
      "neon-flexible-documented-configurations": [
        "I wonder, should we do the same for compute_ctl connections? Especially activity monitor, it runs a bunch of queries pretty often. It probably should be enough to put this option here https://github.com/neondatabase/neon/blob/24d7c37e6ee7b730f983487351721f40922a9745/compute_tools/src/compute.rs#L362",
        "There is a TODO two lines above the place I've linked\r\nhttps://github.com/neondatabase/neon/blob/24d7c37e6ee7b730f983487351721f40922a9745/compute_tools/src/compute.rs#L358-L360\r\n\r\nI was thinking about passing all essential parameters from the compute_ctl without relying on control plane.\r\n\r\nWe can probably still keep an option for control plane to override, not sure if reversing the order here\r\n\r\nSome(options) => format!(\"{} {}\", options, EXTRA_OPTIONS)\r\n\r\nto\r\n\r\nSome(options) => format!(\"{} {}\", EXTRA_OPTIONS, options)\r\n\r\nwill work\r\n",
        "> And then the pseudocode that you wrote actually already exists at\r\n\r\nYeah, but I meant that we should swap `options, EXTRA_OPTIONS` if we want cplane values to take precedence"
      ],
      "neon-scope-jwt-authentication-tokens": [
        "I think we need to add endpoint_id to the token. It won't hurt to have this extra protection to ensure that endpoints cannot write to each other sub-paths. Any problems with adding it?",
        "@myrrc looks mostly good to me, thanks, I only have minor comments. I suggest we put it into other PR -- https://github.com/neondatabase/neon/pull/9661 as it belongs to the unlogged storage/S3 proxy RFC, not just to prewarm flow specifically\r\n\r\ncc @MMeent ",
        "If we make prefix like `/epufs/tenants/{tenant_id}/{endpoint_id|any_other_lower_level_key}/...`, we could decide whether to use tenant or tenant+endpoint pair. I think that from the security standpoint, the tenant should be enough as the tenant is our level of multi-tenancy, and we use it for storage auth already",
        "This info was added in another section, so resolving",
        "I think we could elaborate on that, i.e. that we will use JWTs with tenant+timeline IDs, which both provides good tenants isolation and adds an additional protection layer for different timelines to do not mess with each other data "
      ],
      "neon-document-api-specs-completely": [
        "Let's provide a brief API spec for this EPUFS service, i.e. what are the methods and parameters we are going to have:\r\n- PUT: tenant, timeline, endpoint, relative path, data -> json response\r\n- GET: tenant, timeline, endpoint, relative path -> file content response\r\n- DELETE: tenant [ timeline [ endpoint ] ] -> json response"
      ],
      "neon-comprehensive-code-documentation": [
        "> endpoint_id is set to None while prewarming from other endpoint, see replica promotion\r\n\r\nThis doesn't sound right. When we promote we should prewarm from another endpoint, so endpoint_id should **not** be None, right?",
        "@myrrc, please, do not merge incorrect code (including comments) into `main` with the hope of fixing it in another PR. Another PR may never happen, it might be delayed for an arbitrary amount of time, you can forget, etc.",
        "My understanding is that the difference between /// and !// is that the former applies to the following block, while the latter applies to the upper, which is frequently used for the top-level comments for the crate/module. Here, the comment applies to the struct, so /// seems applicable, or do I miss something?"
      ],
      "neon-hierarchical-semantic-naming": [
        "As discussed, I'd use `compute_pg_` prefix to indicate that it comes from Postgres. Same for both metrics",
        "I think we should drop `min` from the metric. Yes, in the view it's min_mxid across all tables in this DB (same about `frozen_xid`, note PG naming consistency), but when you take age() and sort DESC you actually get oldest as it's properly mentioned in the description.\r\n\r\nSo I guess at the end metrics could be named like\r\n- compute_pg_oldest_frozen_xid_age\r\n- compute_pg_oldest_mxid_age\r\n\r\nor something"
      ]
    },
    "profile": {
      "location": "Berlin, Germany",
      "company": "@neondatabase",
      "blog": "https://alexk.uk",
      "twitter_username": "ololobuss",
      "site_admin": false,
      "followers": 70,
      "following": 44
    }
  },
  "Narsil": {
    "repos": [
      "huggingface/tokenizers"
    ],
    "entries": [
      {
        "slug": "tokenizers-avoid-unsafe-code",
        "title": "Avoid unsafe code"
      },
      {
        "slug": "tokenizers-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "tokenizers-choose-semantically-clear-identifiers",
        "title": "Choose semantically clear identifiers"
      },
      {
        "slug": "tokenizers-consistent-api-design",
        "title": "Consistent API design"
      },
      {
        "slug": "tokenizers-document-for-comprehension",
        "title": "Document for comprehension"
      },
      {
        "slug": "tokenizers-flexible-tokenizer-implementation",
        "title": "Flexible tokenizer implementation"
      },
      {
        "slug": "tokenizers-handle-nullable-types-idiomatically",
        "title": "Handle nullable types idiomatically"
      },
      {
        "slug": "tokenizers-manage-version-constraints",
        "title": "Manage version constraints"
      },
      {
        "slug": "tokenizers-minimize-memory-allocations",
        "title": "Minimize memory allocations"
      },
      {
        "slug": "tokenizers-modular-model-components",
        "title": "Modular model components"
      },
      {
        "slug": "tokenizers-optimize-workflow-triggers",
        "title": "Optimize workflow triggers"
      },
      {
        "slug": "tokenizers-prefer-explicit-api-design",
        "title": "Prefer explicit API design"
      },
      {
        "slug": "tokenizers-prioritize-tokenizer-simplicity",
        "title": "Prioritize tokenizer simplicity"
      },
      {
        "slug": "tokenizers-purpose-indicating-descriptive-names",
        "title": "Purpose-indicating descriptive names"
      },
      {
        "slug": "tokenizers-pythonic-api-design",
        "title": "Pythonic API design"
      },
      {
        "slug": "tokenizers-return-results-not-panics",
        "title": "Return results not panics"
      },
      {
        "slug": "tokenizers-robust-workflow-configurations",
        "title": "Robust workflow configurations"
      },
      {
        "slug": "tokenizers-simplify-for-readability",
        "title": "Simplify for readability"
      },
      {
        "slug": "tokenizers-test-algorithmic-behavior",
        "title": "Test algorithmic behavior"
      },
      {
        "slug": "tokenizers-thread-safe-resource-sharing",
        "title": "Thread-safe resource sharing"
      },
      {
        "slug": "tokenizers-use-explicit-assertions",
        "title": "Use explicit assertions"
      }
    ],
    "comments": {
      "tokenizers-handle-nullable-types-idiomatically": [
        "Why `Option<bool>` ? It should be `bool` no ?\r\n\r\nThere are no optional arguments in Rust (and it's good)\r\n\r\n`unk_id` is really an Option, it's not at all forced (but it will cause errors if you haven't one and are triggering an unk).",
        "```suggestion\r\n           if let Some(max_token_length) = max_token_length{\r\n               if new_token.chars().count() > max_token_length{\r\n                   continue;\r\n               }\r\n           }\r\n```\r\n\r\nThis is more idiomatic imo.\r\n0 is NOT a special value. `None`  means ignore, `zero` does mean zero. If things starts to do weird things it's not the problem of this function, it's respecting the value which is more important.\r\n\r\nTry to switch to `usize` it makes everything easier to read, and the actual \"size\" isn't important optimization wise."
      ],
      "tokenizers-prioritize-tokenizer-simplicity": [
        "Yes I removed 1 feature:\r\n\r\nPreTokenized inputs. Which are presplitted strings (so list of strings).\r\n\r\nWhy did I remove it :\r\n\r\n- It bloats quite a lot the code (it would require rewriting the argument parsing since it doesn't work by default with the macro, or at least I wasn't able to).\r\n- It's a rarely used feature which have lots of caveats.\r\n- We can readd later, right now, running on more recent versions in reasonable time was the goal.",
        "That's not True, at least it was not in my case, all the tests were failing because the test string was not splitted through whitespace, so the added vocabulary was not handled, and those tests were failing because UNK_TOKEN was not defined.\r\n\r\nWe probably should really test the output of those methods in the full to make sure it's consistent, I'm pretty sure all those tests would have caught the missing unk tokens otherwise."
      ],
      "tokenizers-test-algorithmic-behavior": [
        "Could you assert the full list of `ids` and `tokens` (like the other test ?)",
        "So if the tests are not deterministic definitely let's not add them.\r\n\r\nHowever it's suprising that it's not determinstic though..... It shouldn't be ....\r\n\r\n▁ != _  \r\nfirst is a special unicode character (rarely used that's why google chose it)\r\nSecond is regular underscore.",
        "<unk> instead of 🤗 is indeed expected",
        "That's better, and OK I understand better why IDs are not deterministic, they essentially have the same score, so no particular reason that ids should be in a particular order."
      ],
      "tokenizers-optimize-workflow-triggers": [
        "It's in the default template for `wasm` app. I don't really see a lot of downsides of keeping them."
      ],
      "tokenizers-return-results-not-panics": [
        "Panicking is NOT okay in transformers. We should NEVER panic since we're a library.\r\n\r\nFor that there is `TryFrom` which returns `Result<T>`. However I don't think this should be done in the Rust layer but in the Python layer instead.",
        "Remove every `unwrap` and every `vec`.\r\n\r\nThere is 1 `collect` tolerated ( I think it's done that way in BPE) and it's *only* to check that ALL bytes have a token id (you're allowed to use a single `vec` or `collect` in that branch, not in the others.\r\n\r\n:)",
        "Well if the regexp doesn't match, it's strictly equivalent to matching nothing, right ?\r\n\r\nI do that to avoid adding and `unwrap` which could potential `panic!` which makes users pretty unhappy.\r\n\r\nI don't think the pathway should be taken, but if it ever is, then I think `0` is a valid default which prevents panicking and is still, correct. Wdyt ?",
        "> I understand adding Results all the way up to the public API is cumbersome but to me is the cleaner approach : notify the user there was an error / undefined / unexpected behavior and let him handle it.\r\n\r\nIn that case it's not true. Finding 0 match is different than an error.\r\nI could very well change the regex to be `\\s+` and then the `else` branch would be expected and working as intended.\r\n\r\nIs adding a `warn!(\"AddedToken with `single_word` seems to have an issue, Please report this\")` in that code path OK ?\r\nIt it warning the user, but still prevent catastrophing failure ?\r\n\r\nWe don't expect the code path to be taken, but it's not preventing the algorithm from working.",
        "No, the else clause will trigger, when `REGEXP.find()` returns `None`.\r\n\r\nThis is normal when the REGEX fails to find any match in the submitted string.\r\nThis particular brand of REGEX should match all the time (since it should match the empty string being `^\\s*` and `\\s*$`).\r\nSo I don't expect it to not match. But if it doesn't match, it's roughly the same as saying no space where found.",
        "Oh no no necessarily.\r\n\r\n`AddedTokens(\"<mask>\", lstrip=True)` means you want it to match `\"Something <mask>  else\"` -> `['Something\", \" <mask>\", \" else\"]`. Meaning you are actually capturing the left spaces in addition to your token (effectively tripping it from being seen for your model).\r\n\r\nBut `\"Something<mask>else\"` will also capture `[\"Something\", \"<mask>\", \"else\"]`. no space have been deleted.\r\nIf you want this to NOT capture you need to activate `single_word=True` too.\r\n\r\nBut both options are orthogonal and don´t  necessarily need one another (although I think in practice they are probably reasonned about in conjunction)\r\n\r\n\r\nEdit: `<mask>` is a comment in GH markdown easy to miss :D\r\n",
        "@McPatate I merged since I am starting the necessary work to release this before transformers's own release on thursday, but we can continue the discussion to make this clear.\r\n",
        "`.unwrap()` is something I tend to avoid within logic code at all if possible, since having the program panic, is never a great user experience.\r\n\r\nMy answer : https://github.com/huggingface/tokenizers/pull/919#discussion_r813862919 applies too. I think returning `Option` would be preferrable to panicking, but the result would be the same IMO, if the regex doesn't match, I just don't capture anything.",
        "I think this would go away using `aho-corasick` (it returns the ID with the match).",
        "Also there are other `unwrap` which would panic too."
      ],
      "tokenizers-modular-model-components": [
        "Why to we need this ? `encode` is enough, no ?"
      ],
      "tokenizers-manage-version-constraints": [
        "No, we need to allow for breaking changes in huggingface_hub meaning we can do `>=0.17,<0.18` instead.",
        "I would very much rather we update dependencies in separate PRs. \r\n\r\nWhen updating dependencies, I need to make sure nothing breaks upstream either.\r\nThis PR will update `pyo3` but if we could leave out the other dependencies not required for other PRs it would be easier to check. (One single PR for all other dependencies is fine)."
      ],
      "tokenizers-pythonic-api-design": [
        "As long as we're breaking signature, I would argue we have a different signature like\r\n\r\n`train(files, options=bpe_train_options)`, or `train(files, vocab_size=X, ....)` what do you think ?\r\n\r\nI like the second version better, the only trouble is the exact description of those options if going to get fuzzy pretty fast and error handling a bit hard. But it \"feels\" more pythonic, what do you think ?\r\n\r\nEither that, or if we keep the `trainer` concept, we should stick to something closer to Rust, with `trainer.train(tokenizer, files)` I actually like that last version better at this moment, the control flow feels more natural.",
        "I'm merely exposing the rust api which requires this format.\r\nIt's also what `BPE::read_files()` returns.\r\n\r\nFollowing what you said we should mostly expose the rust api, no ?\r\nShould I change the rust API ? (I don't think we should)"
      ],
      "tokenizers-document-for-comprehension": [
        "Yes, totally fine. ",
        "We probably should add a little comment here, jieba's behavior is not super straightforward here.\r\n\r\nAlso we should probably explain what this code does.\r\n\r\n```python\r\nclass JiebaPreTokenizer:\r\n    def jieba_split(self, pretoken_index, pretoken):\r\n        new_tokens = []\r\n        # Why do we need `str(normalized)?`\r\n        pretoken_string = str(pretoken)\r\n        for token, start, stop in jieba.tokenize(pretoken_string):\r\n            new_tokens.append(normalized[start:stop])\r\n        return new_tokens\r\n\r\n    def pre_tokenize(self, pretok):\r\n        # Let's call split on the PreTokenizedString to split using `self.split`\r\n        # pretok.split takes in a function, that receives `i` that is the current token index\r\n        # and `pretoken` that is a substring of th original string, that might have been normalized,\r\n        # and already cut up by previous pretokenizer. It should return a list of subtokens.\r\n        # `pretok.split` changes it inplace.\r\n\r\n        # Checkout X to see all available primitives to do a custom pretokenization\r\n        pretok.split(self.jieba_split)\r\n```"
      ],
      "tokenizers-choose-optimal-data-structures": [
        "Is the hashSet `inserted` really necessary ? `required_chars` is already a HashMap, so we shouldn't get duplicates anyway, no ?",
        "My bad, did not see that change to `Vec`",
        "Yes. It complains that `needless_collect`. Basically that we collect something that we iterate over afterwards.\r\n\r\nWe could definitely use `[allow(needless_collect)]` But I think we should try to fix them anyway. We probably also could have only one forward pass for this one.\r\n\r\nBut how does `iter().rev()` handle cache locality though ? ",
        "I fixed it to only forward pass anyway, it's just better."
      ],
      "tokenizers-simplify-for-readability": [
        "```suggestion\r\n    pub fn from_string(content: &str) -> Result<Self> {\r\n        serde_json::from_str(content)\r\n    }\r\n```\r\n\r\nSeems simpler:\r\n\r\nNo need to pass owned data, no `?`+   `Ok`.",
        "Other option which feel slightly cleaner IMO:\r\n\r\n```rust\r\n        if let Some(&id) = self.vocab.get(token) {\r\n            Ok(vec![Token {\r\n                id,\r\n                value: token.to_owned(),\r\n                offsets: (0, token.len()),\r\n            }])\r\n        } else if let Some(&unk_id) = self.vocab.get(&self.unk_token) {\r\n            Ok(vec![Token {\r\n                id: unk_id,\r\n                value: self.unk_token.to_owned(),\r\n                offsets: (0, token.len()),\r\n            }])\r\n        } else {\r\n            Err(Box::new(Error::MissingUnkToken))\r\n        }\r\n```\r\nIt's personal taste, probably gets compiled the exact same way.",
        "Perfect, I'll go ahead an merge.",
        "```suggestion\r\n            let direction = match direction.as_str(){\r\n                \"left\"  => Truncate::Left,\r\n                \"right\" => Truncate::Right,\r\n                other => panic!(\"Invalid truncation direction value : {}\", other);\r\n            };\r\n```\r\nSeems slightly more rusty (and it should compile)"
      ],
      "tokenizers-choose-semantically-clear-identifiers": [
        "The argument name is odd to me.\r\n\r\ntruncate_left([0, 1, 2], 2) -> [1, 2]\r\ntruncate_right([0, 1, 2], 2) -> [0, 1]\r\n\r\nIMO. I think from the tests that you're doing the opposite.\r\n\r\nrenaming `left` to `right` is enough as a first approximation.\r\n\r\nIn `transformers` there's actually a `direction` name which would be an enum might clarify things a bit.",
        "Please factor this code out ! :D. You shouldn't need to have this twice, Make it a simple function.\r\n\r\nAlso why change `\"First\"`  ? `\"first\"` is much more pythonic imo.",
        "```suggestion\r\nfn from_string(string: String) -> Result<PrependScheme, PyErr> {\r\n```\r\n\r\nFunctions are private by default. `pub` means public."
      ],
      "tokenizers-use-explicit-assertions": [
        "Thats a great test ! Good idea on the UTF-8 intense examples ! \r\n\r\nCould you add the explicit vocab as a result ? It makes the test more readable and more robust.\r\nLet's keep the functional part too, but having the explicit values prevents \"under the radar\" bugs, where behavior is modified unintentionally.|\r\n\r\n\r\nYou could reduce the size of those strings and `max_token_length` in order to keep things readable maybe.",
        "No 1 test, but explicit assert.\r\n\r\n`assert_eq!(tokenizer.get_vocab(), HashMapFrom([....]))`\r\n\r\nIt's really much better imo. Tests don't change that often, and I have seen many bugs be silent for too long for this lack of explicit value testing.",
        "Going along with the `#ignore` if we keep this `print` we 're not checking anything actually during tests. If possible/compatible with a fast testing iteration, we probably should actually change those into real asserts.\r\n\r\nIt hinders readability only by a slight margin I feel, but it impacts forward compat by quite a bit."
      ],
      "tokenizers-consistent-api-design": [
        "Is that really something we need to expose ?"
      ],
      "tokenizers-prefer-explicit-api-design": [
        "Just make that `PrependScheme`.\r\n\r\n`impl Into<T>` has an associated cost with it. Which is that there will be a concrete function for EVERY possible caller that might arise (here at least `PrependScheme`, `&str` and `String`. \r\n\r\nThis is nice to make an API easier to use when the concrete underlying type is relatively verbose to make, or there are many structs implementing the given trait that might be useful (something like `impl Response` in a web framework.\r\n\r\nHere everything is a simple `enum`. Importing the enum and using a variant makes the function  so that only 1 exists, prevents all kind of runtiem errors (all will become compile time errors). And since only 1 function exists, it can be inline more easily by the compiler.\r\n\r\nSimple is better here IMO",
        "TBH variant return types always give me shivers. It's usually so much better to have 2 different functions instead. Saves so much headaches for users....\r\n\r\nIf we want to save backcompat:\r\n\r\n```python\r\ndef token_to_word(token) -> int:\r\n     if self.num_sequences() > 1:\r\n         raise Exception(\"Can't use this in multiple sequences encoding, use token_to_word_seq instead\")\r\n     word, seq = token_to_word_seq(token)\r\n     return word\r\n\r\ndef token_to_word_seq(token) -> Tuple[int, int]:\r\n    ....\r\n```"
      ],
      "tokenizers-thread-safe-resource-sharing": [
        "Because it's python a python object holding a ref to a rust object.\r\n\r\nSince Python has the GIL we don't know if we're on the same thread or not, hence `Arc<RwLock<>>` To become `Send+Sync`. This is already what the `PyTrainer` owns as an object.",
        "Could you explain why all those RwLock are need ? I'm not sure why we would need them.",
        "I'm always a bit scared by adding that much `unwrap` everywhere... Do you think there's a way we could avoid them ?"
      ],
      "tokenizers-purpose-indicating-descriptive-names": [
        "I'd like to keep the dictionary format, at least for uniformity within the tests fixtures that all use dictionaries.\r\n\r\nIn terms of naming, because we might add follow ups saved tokenizers, what about `serialized_files` it makes `serialized_files[\"albert_base\"]` more understandable. I'd rather add another tokenizer in the tests than naming specifically for albert IMO. (We probably should have a handful of serialized tokenizers in those tests to make sure will load the old ones, with BertNormalizer, BPE, and so on). What do you think ?",
        "Good argument for downloading separately, and that changes the names, which makes your solution the best in the end.\r\n"
      ],
      "tokenizers-avoid-unsafe-code": [
        "Ouch, not sure we need `unsafe` in this library :)",
        "I think staying away from unsafe is probably best, even if it means 1 clone. That's my opinion at least."
      ],
      "tokenizers-flexible-tokenizer-implementation": [
        "```suggestion\r\n            tokenizer = Tokenizer(BPE(unk_token=str(unk_token), dropout=dropout, end_of_word_suffix=suffx))\r\n```\r\n\r\nno ?"
      ],
      "tokenizers-robust-workflow-configurations": [
        "Hmm it does, just moves the location of this information. Will have to check about `click` too.",
        "Done."
      ],
      "tokenizers-minimize-memory-allocations": [
        "`iter_mut()` + `for_each` makes sure there are not reallocations.\r\n\r\n`.collect()` is like `.clone()`, avoid if possible.\r\n\r\nIt might be optimized away but I would rather not count on it.",
        "Probably, but if the reduce is not itself parallel, then it's ok to have `global` be mutable.",
        "Don't clone on behalf of users, never.\r\n\r\nEither return a reference `&PrependScheme`.\r\nOr make `PrependScheme`  ` Copy`.\r\n\r\nMaking something Copy, is something you should do only when the size makes it worthwile.\r\nA reference is a pointer of size `usize` so copying usize is usually much faster than copying an entire struct.\r\nFor an enum like PrependScheme, it' s only 3 possible values, encoded internally by rust as `u8` or `usize` (Don't remember which). In any case, it's prefereable to copy the value than to pass references around (which cost a pointer dereference)\r\n",
        "Let's remove this. Vec allocs is costly, you don't want to do this.",
        "Done."
      ]
    },
    "profile": {
      "company": "@huggingface ",
      "blog": "",
      "site_admin": false,
      "followers": 794,
      "following": 29
    }
  },
  "sydney-runkle": {
    "repos": [
      "pydantic/pydantic"
    ],
    "entries": [
      {
        "slug": "pydantic-avoid-shared-structure-mutation",
        "title": "Avoid shared structure mutation"
      },
      {
        "slug": "pydantic-avoid-unnecessary-operations",
        "title": "Avoid unnecessary operations"
      },
      {
        "slug": "pydantic-balance-documentation-thoroughness",
        "title": "Balance documentation thoroughness"
      },
      {
        "slug": "pydantic-cache-expensive-computations",
        "title": "Cache expensive computations"
      },
      {
        "slug": "pydantic-categorize-error-types",
        "title": "Categorize error types"
      },
      {
        "slug": "pydantic-consistent-configuration-patterns",
        "title": "Consistent configuration patterns"
      },
      {
        "slug": "pydantic-document-code-rationale",
        "title": "Document code rationale"
      },
      {
        "slug": "pydantic-document-configuration-relationships",
        "title": "Document configuration relationships"
      },
      {
        "slug": "pydantic-eliminate-redundant-computation",
        "title": "Eliminate redundant computation"
      },
      {
        "slug": "pydantic-enforce-style-with-linters",
        "title": "Enforce style with linters"
      },
      {
        "slug": "pydantic-explicit-over-implicit",
        "title": "Explicit over implicit"
      },
      {
        "slug": "pydantic-maintain-code-consistency",
        "title": "Maintain code consistency"
      },
      {
        "slug": "pydantic-preserve-language-conventions",
        "title": "Preserve language conventions"
      },
      {
        "slug": "pydantic-robust-error-messaging",
        "title": "Robust error messaging"
      },
      {
        "slug": "pydantic-safe-attribute-access-pattern",
        "title": "Safe attribute access pattern"
      },
      {
        "slug": "pydantic-semantic-over-syntactic",
        "title": "Semantic over syntactic"
      },
      {
        "slug": "pydantic-simple-defaults-flexible-overrides",
        "title": "Simple defaults, flexible overrides"
      },
      {
        "slug": "pydantic-specific-types-for-performance",
        "title": "Specific types for performance"
      },
      {
        "slug": "pydantic-standardize-dependency-management",
        "title": "Standardize dependency management"
      },
      {
        "slug": "pydantic-structured-configuration-management",
        "title": "Structured configuration management"
      },
      {
        "slug": "pydantic-write-targeted-specific-tests",
        "title": "Write targeted, specific tests"
      }
    ],
    "comments": {
      "pydantic-avoid-shared-structure-mutation": [
        "Seems like we lost this logic - is this needed anywhere?",
        "Would it make sense to break this change out into a different PR?",
        "Just curious, why do we need to add this here now?"
      ],
      "pydantic-standardize-dependency-management": [
        "```suggestion\r\n          enable-cache: true\r\n          python-version: '3.12'\r\n```"
      ],
      "pydantic-consistent-configuration-patterns": [
        "Should we use `ConfigDict(str_max_length=10)`?",
        "Should we recommend one over the other?",
        "The default does not override the config, this only overrides config if set. Good question!",
        "I believe in the migration guide we talk about priority here (sometimes it's not intuitive). Can we make a note on that here, and add a dupe key to the merged configs and show which one is preserved in the result?"
      ],
      "pydantic-write-targeted-specific-tests": [
        "Perhaps this could be a bit more verbose:\r\n\r\n1. Can we include the `SchemaError` information in this result?\r\n2. Could we add a note about the fact that this is purely a testing feature, not a runtime `pydantic` check (at this point)?\r\n\r\nPerhaps this is a bit excessive, but can we test this test? As in, run a test within a test, and check that this is raised for invalid schema? If not, no worries, but could you document an example of a failing test on this PR just to have as a reference for the blame later?",
        "Yeah, I think having their own tests would be good so that we can avoid the conditional warning checks.",
        "Why remove this?"
      ],
      "pydantic-preserve-language-conventions": [
        "`validate_by_name` logic still applies to `__init__` :)",
        "@stevapple, feel free to open an issue with a summary of this discussion!",
        "```suggestion\r\nimport json\r\nfrom pydantic import BaseModel, computed_field\r\n\r\n\r\nclass Box(BaseModel):\r\n    width: float\r\n    height: float\r\n    depth: float\r\n\r\n    @computed_field\r\n    @property  # (1)!\r\n    def volume(self) -> float:\r\n        return self.width * self.height * self.depth\r\n\r\n\r\nprint(json.dumps(Box.model_json_schema(mode='serialization'), indent=2))\r\n\"\"\"\r\n{\r\n  \"properties\": {\r\n    \"width\": {\r\n      \"title\": \"Width\",\r\n      \"type\": \"number\"\r\n    },\r\n    \"height\": {\r\n      \"title\": \"Height\",\r\n      \"type\": \"number\"\r\n    },\r\n    \"depth\": {\r\n      \"title\": \"Depth\",\r\n      \"type\": \"number\"\r\n    },\r\n    \"volume\": {\r\n      \"readOnly\": true,\r\n      \"title\": \"Volume\",\r\n      \"type\": \"number\"\r\n    }\r\n  },\r\n  \"required\": [\r\n    \"width\",\r\n    \"height\",\r\n    \"depth\",\r\n    \"volume\"\r\n  ],\r\n  \"title\": \"Box\",\r\n  \"type\": \"object\"\r\n}\r\n\"\"\"\r\n```\r\n\r\nLooking great, just a formatting fix here."
      ],
      "pydantic-balance-documentation-thoroughness": [
        "Agreed, good call.",
        "I've opened an issue and linked this comment. See https://github.com/pydantic/pydantic/issues/11491",
        "Isn't this something worth including in the conceptual docs?",
        "Agreed.",
        "I'm fine with this as long as we link to model docs. Maybe we could add a few other notes as well like json schema, etc.",
        "I feel like the explicit specifications here were helpful - people often ask about `init_typed`..."
      ],
      "pydantic-specific-types-for-performance": [
        "I like this example overall.\r\n\r\nI think we should:\r\n* Link to the related [performance section](https://docs.pydantic.dev/latest/concepts/performance/#sequence-vs-list-or-tuple-mapping-vs-dict)\r\n* Give a bit more context re efficiency here - the more specific you can be with a type, the better (sort of in a philosophical sense)",
        "Sure - basically, in one line or two, you can explain that generally, the more specific a type, the faster validation will be, as we don't have to perform as many checks / coercion attempts."
      ],
      "pydantic-document-configuration-relationships": [
        "> Also, what should happen if you set validate_by_alias=False, but explicitly set by_alias=True or by_name=True during validation?\r\n\r\nValidation time settings always take priority, when set. This is the same with `strict`.",
        "I'm sympathetic to the literal pattern argument. If we were starting fully from scratch, I think it might make more sense. Specifically, the boolean traps can be a bit confusing. In particular, the fact that you have to set `validate_by_name=True` if `validate_by_alias=False` explicitly is a bit confusing, especially for new users.\r\n\r\nOne thing we could do to mitigate this challenge is automatically set `validate_by_name=True` if a user sets `validate_By_alias=False`.\r\n\r\nMy thoughts re why we should stick with the 2 boolean flags:\r\n\r\n* It represents less change to this setting compared to a switch to literals - there's already a lot of change going on here, and I'm hesitant to introduce a setting `type` change as well.\r\n* 2 boolean flags provide greater configurability for interaction between config and runtime settings, as you can override one behavior and not the other. It's also helpful to have unset markers for each thing. For example:\r\n\r\n```\r\nM1: validate_by_alias = True, validate_by_name = False\r\nM2: validate_by_alias = False, validate_by_name = True\r\n\r\nruntime setting: by_name = True\r\n\r\n==>\r\n\r\nM1: alias and name validation\r\nM2: name only validation\r\n```\r\n\r\nThis can't be achieved with the literal approach. Either you'd use:\r\n* `validate_by='name'`, and M1 would lose alias validation\r\n* `validate_by='name and alias'` and M2 would no longer avoid validating with alias\r\n\r\n* Autocomplete is easier with boolean flags, and the behavior is relatively intuitive\r\n\r\nAliases are one of the most common (if not the most commonly used) field tool, so I do think this decision is quite important. I also understand that if we go with bools here, we're stuck with that until at least V4. ",
        "Agreed, definitely important. I've pushed a [commit](https://github.com/pydantic/pydantic/pull/11468/commits/2341a58997683df514a7dcd27a615a5d05678f55) to make this backwards compatible.\r\n\r\nThe new behavior:\r\n\r\n```py\r\nfrom pydantic import BaseModel, ConfigDict, Field\r\n\r\nclass Model1(BaseModel):\r\n    model_config = ConfigDict(validate_by_alias=False, validate_by_name=True)\r\n\r\n    my_field: str = Field(alias='my_alias')\r\n\r\nm1 = Model1(my_field='foo')\r\n\r\nclass Model2(BaseModel):\r\n    model_config = ConfigDict(validate_by_alias=True, validate_by_name=False)\r\n\r\n    my_field: str = Field(alias='my_alias')\r\n\r\nm2 = Model2(my_alias='foo')\r\n\r\nclass Model3(BaseModel):\r\n    model_config = ConfigDict(validate_by_alias=True, validate_by_name=True)\r\n\r\n    my_field: str = Field(alias='my_alias')\r\n\r\n# for this case, we prefer the field name over the alias, as we did in the past\r\n# if populate_by_name was True\r\nm31 = Model3(my_field='foo')\r\nm32 = Model3(my_alias='foo')\r\n#> test.py:23: error: Missing named argument \"my_field\" for \"Model3\"  [call-arg]\r\n\r\nclass Model4(BaseModel):\r\n    my_field: str = Field(alias='my_alias')\r\n\r\nm4 = Model4(my_alias='foo')\r\n\r\nclass Model5(BaseModel):\r\n    model_config = ConfigDict(populate_by_name=True)\r\n\r\n    my_field: str = Field(alias='my_alias')\r\n\r\n# for this case, we prefer the field name over the alias\r\nm51 = Model5(my_field='foo')\r\nm52 = Model5(my_alias='foo')\r\n#> test.py:39: error: Missing named argument \"my_field\" for \"Model5\"  [call-arg]\r\n```",
        "Fixed this problem by consolidating the settings as you had previously :)"
      ],
      "pydantic-explicit-over-implicit": [
        "This feels cleaner to me than the way we had it before, I wasn't a fan of having `tzdata` in `dev`. Thanks!",
        "It does seem redundant..."
      ],
      "pydantic-structured-configuration-management": [
        "Will do, this is going to take a while.",
        "Just curious, why did we move this up?",
        "I think we can only remove this bound if we change the lower bound for sqlalchemy?",
        "Or just add a conditional one for 3.13"
      ],
      "pydantic-semantic-over-syntactic": [
        "Would `is_type_alias_type` make more sense?",
        "I'd prefer `is_type_alias_type` just bc it's easier to read, but it's up to you, doesn't really matter!",
        "Maybe add `Schema` to the name of this? Or `CoreSchema`?",
        "Can we make this typevar more clear, maybe like `PropertyReturnType` or `ReturnType` or something like that?",
        "Good call"
      ],
      "pydantic-robust-error-messaging": [
        "Is this true? Doesn't it only need to be not `None` when `_fields.takes_validated_data_argument(self.default_factory)` is True?",
        "Fine by me, feel free to merge then :)."
      ],
      "pydantic-simple-defaults-flexible-overrides": [
        "See changes - I've sort of gone the other way here - wrap val is the same, but `__init__` is greatly simplified 👍 "
      ],
      "pydantic-cache-expensive-computations": [
        "@MarkusSintonen did a good job at intuitively extracting some of this logic as follows:\r\n\r\n```py\r\ndef get_existing_core_schema(obj: Any) -> core_schema.CoreSchema | None:\r\n    # Only use the cached value from this _exact_ class; we don't want one from a parent class\r\n    # This is why we check `cls.__dict__` and don't use `cls.__pydantic_core_schema__` or similar.\r\n    if (\r\n        hasattr(obj, '__dict__')\r\n        and (existing_schema := obj.__dict__.get('__pydantic_core_schema__')) is not None\r\n        and not isinstance(existing_schema, MockCoreSchema)\r\n    ):\r\n        return existing_schema\r\n    return None\r\n```\r\n\r\nMaybe we could eagerly pull changes like that into this PR, given that https://github.com/pydantic/pydantic/pull/10655 isn't quite ready to merge yet?",
        "I'm ok with leaving this dupe for now",
        "Why the removal of the `defer_build` check?",
        "```suggestion\r\n                # Deleting the validator/serializer is necessary as otherwise they can get reused in\r\n                # pydantic-core. Same applies for the core schema that can be reused in schema generation.\r\n```",
        "I wouldn't expect to have a `slow_memo_handler`, given that the point of memoization is to make things fast. Maybe we could use `setattr_handler` instead?"
      ],
      "pydantic-safe-attribute-access-pattern": [
        "Do we not even need to try this?",
        "I like the version @Viicos suggested.",
        "Can you avoid the ignore with a `cast` like `metadata = cast(schema.setdefault('metadata', {})`?",
        "Can we leave this as is? What `AttributeError` might occur here?",
        "Makes sense, thanks for the improvement!"
      ],
      "pydantic-maintain-code-consistency": [
        "```suggestion\r\n            model_frozen = cls.model_config.get('frozen')\r\n            field_frozen = getattr(cls.__pydantic_fields__.get(name), 'frozen', False)\r\n            if model_frozen or field_frozen:\r\n```",
        "We should make this consistent with the above pattern.\r\n\r\nIt's hard - we're not at the 3 repetition rule here that necessitates abstraction. I'm ok with keeping this duplicated code as long as it's as consistent as possible."
      ],
      "pydantic-enforce-style-with-linters": [
        "Did you want to add a lint step to precommit that helps enforce standardization here?"
      ],
      "pydantic-document-code-rationale": [
        "As discussed in person, let's add a comment that this warning is only omitted when calling super. I think we need to make it more clear what the consequences of this deprecation are.",
        "Maybe add a note about what successfully collected means - specifically, that annotations were successfully evaluated?",
        "Let's be even more explicit here.\r\n\r\nCan we:\r\n* Explain subsituting definition refs\r\n* Explain why / how we add tagged info to discriminators?\r\n\r\nThis isn't your doing at all, but our internal `GenerateSchema` logic is poorly documented, so I think it's good to have thorough improvements as a standard for PRs here going forward.",
        "Yeah, that probably makes more sense, given that's where we do the namespace fetching anyways. Happy to move there.",
        "Might put this under a closed by default block as to not overwhelm users not using this complex functionality.",
        "Maybe the class itself, honestly. Bc it applies to both `__init__` and `rebuild`",
        "Can we add a docstring to this function and the one above? If you're switching contexts into the world of typing and types management, a little description here could really help."
      ],
      "pydantic-eliminate-redundant-computation": [
        "Why is `is_generic_alias` not included in `typing_inspection`?",
        "Good question, looking into this.",
        "So, this happens when we have a model that deferred (or failed) schema building.\r\n\r\nI think this logic needs to be more complex - should we reassign `__pydantic_core_schema__`? What about `__pydantic_validator__` and `__pydantic_serializer__`?",
        "I've updated this - we no longer do the recursive call :).",
        "Good call, done.",
        "Hmm, I don't think there's an easy way to do this - you can't easily create a `TypeAdapter` with just a core schema. Even if we could hack that together, should we?",
        "Found a relatively simple way with `TypeAdapter`. That's easiest for now, as we already have the `isinstance` conditional in the wrap validator in the custom core schema.\r\n\r\nDown the line, maybe it does make sense to just use `SchemaValidator` inside these types to perform internal validation. I'm going to write up an issue with next steps based on the feedback here, and I'll include this 👍 "
      ],
      "pydantic-avoid-unnecessary-operations": [
        "@MarkusSintonen did a good job at intuitively extracting some of this logic as follows:\r\n\r\n```py\r\ndef get_existing_core_schema(obj: Any) -> core_schema.CoreSchema | None:\r\n    # Only use the cached value from this _exact_ class; we don't want one from a parent class\r\n    # This is why we check `cls.__dict__` and don't use `cls.__pydantic_core_schema__` or similar.\r\n    if (\r\n        hasattr(obj, '__dict__')\r\n        and (existing_schema := obj.__dict__.get('__pydantic_core_schema__')) is not None\r\n        and not isinstance(existing_schema, MockCoreSchema)\r\n    ):\r\n        return existing_schema\r\n    return None\r\n```\r\n\r\nMaybe we could eagerly pull changes like that into this PR, given that https://github.com/pydantic/pydantic/pull/10655 isn't quite ready to merge yet?",
        "I'm ok with leaving this dupe for now"
      ],
      "pydantic-categorize-error-types": [
        "Maybe we could offer more clarity on usage errors, like providing an example or two"
      ]
    },
    "profile": {
      "location": "Somerville, MA",
      "company": "@langchain-ai",
      "blog": "",
      "site_admin": false,
      "followers": 356,
      "following": 51
    }
  },
  "bo156": {
    "repos": [
      "bridgecrewio/checkov"
    ],
    "entries": [
      {
        "slug": "checkov-backward-compatible-parameters",
        "title": "Backward compatible parameters"
      },
      {
        "slug": "checkov-centralize-environment-variables",
        "title": "Centralize environment variables"
      },
      {
        "slug": "checkov-choose-optimal-algorithms",
        "title": "Choose optimal algorithms"
      },
      {
        "slug": "checkov-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "checkov-consistent-naming-conventions",
        "title": "Consistent naming conventions"
      },
      {
        "slug": "checkov-ensure-dependency-compatibility",
        "title": "Ensure dependency compatibility"
      },
      {
        "slug": "checkov-extract-focused-functions",
        "title": "Extract focused functions"
      },
      {
        "slug": "checkov-meaningful-identifier-names",
        "title": "Meaningful identifier names"
      },
      {
        "slug": "checkov-preserve-api-compatibility",
        "title": "Preserve API compatibility"
      },
      {
        "slug": "checkov-strategic-error-handling",
        "title": "Strategic error handling"
      },
      {
        "slug": "checkov-strategic-exception-management",
        "title": "Strategic exception management"
      },
      {
        "slug": "checkov-support-all-target-environments",
        "title": "Support all target environments"
      },
      {
        "slug": "checkov-thorough-test-assertions",
        "title": "Thorough test assertions"
      },
      {
        "slug": "checkov-use-appropriate-logging-levels",
        "title": "Use appropriate logging levels"
      },
      {
        "slug": "checkov-use-pytest-best-practices",
        "title": "Use pytest best practices"
      },
      {
        "slug": "checkov-write-pythonic-code",
        "title": "Write pythonic code"
      }
    ],
    "comments": {
      "checkov-extract-focused-functions": [
        "looks like an almost identical copy of a previous usage, please extract to function",
        "1. do we have a const for edge label? if yes use it, if not just create a const for this str.\r\n2. extract it to a function called `create_virtual_resources_edges`, which can be called from the `create_edges` function as well",
        "can you extract this one-liner to a function to make it more readable? \r\nalso `existing_tuple` is an unclear name",
        "as this part begins to be a bit big - what do you think about splitting the `if/else` block to a new function called `_generate_graphs_list` (or something like that)?",
        "would split the creation of this dict to a function like `_build_dirs_to_definitions`"
      ],
      "checkov-centralize-environment-variables": [
        "this function has the same signature across all graph managers, adding this parameter here breaks it.\r\nassuming you don't want to add it across all frameworks, I can suggest controlling this behaviour with an env var rather than a parameter:\r\n1. Add a matching env var under the file `checkov/common/util/env_vars_config.py`, with default False.\r\n2. In the specific function you wanted to use it, check for the value (using import) and define behavior accordingly",
        "this line should be under the `env_var_config.py` rather than here (under the `common` dir with the rest of the env vars)",
        "please use `config_env_vars.py` to add new env vars to checkov :) ",
        "all env vars in checkov are handled in `checkov/common/util/env_vars_config.py`, please add it there and use from import",
        "when adding a new env_var in checkov, you should add it in `checkov/common/util/env_vars_config.py` and use it from there, allows better handling of those variables throughout the code ",
        "We also have a file called `env_vars_config` with the full state of all env vars, could you also add it there and use it instead?\r\nThis gives us a unified place to handle all env vars, and also if we need to import them in multiple places we can do it easily :)"
      ],
      "checkov-strategic-error-handling": [
        "can this part result in a `StopIteeration` being thrown?"
      ],
      "checkov-choose-optimal-algorithms": [
        "good idea - fixed",
        "Just didn't watch to change api"
      ],
      "checkov-choose-optimal-data-structures": [
        "good idea - fixed",
        "Just didn't watch to change api"
      ],
      "checkov-meaningful-identifier-names": [
        "why not use `list(set(new_value))`.\r\nalso `new_value` is a bad name, I would call it `list_updated_value` or something like that",
        "Should probably be called `TerraformJsonRunner` to avoid multiple instances of the name `Runner` in the code",
        "can you rename `each` to something like `resource` or `changed_resource`?\r\nWill be more readable in python code (even though I guess it comes from terraform conventions)",
        "Maybe change the name to `pickle_deepcopy`?\r\nI just want the IDE to complete me automatically to the correct deepcopy, \r\nto avoid mistakenly importing the wrong one.\r\nAlso, it will be clearer to new developers in the future that this is something internal"
      ],
      "checkov-use-pytest-best-practices": [
        "this will set it for the rest of the process, you can use `monkeypatch` instead to set it just for the test",
        "that's not common practice, please use mocks for the test using `monkeypatch`",
        "we have this parametrization in other places in the code, you can also use `pytest.mark.parametrize`",
        "maybe use `pytest` instead of `unittest` for new tests? "
      ],
      "checkov-backward-compatible-parameters": [
        "why do you need to send the `graph` here as well? (instead of always using `get_reader_endpoint`)",
        "sound right, hadn't though about it.",
        "fixed"
      ],
      "checkov-write-pythonic-code": [
        "why not just go over `d.values()` if you don't use the key"
      ],
      "checkov-ensure-dependency-compatibility": [
        "Note that it seems like this version doesn't support python3.8, which is still supported in checkov. Try to find a version which still has support for it"
      ],
      "checkov-preserve-api-compatibility": [
        "fixed",
        "why do you need to send the `graph` here as well? (instead of always using `get_reader_endpoint`)",
        "sound right, hadn't though about it.",
        "fixed"
      ],
      "checkov-support-all-target-environments": [
        "Note that it seems like this version doesn't support python3.8, which is still supported in checkov. Try to find a version which still has support for it",
        "why did you lock the version? it will prevent getting those updates which are important",
        "I also don't want you to change it so we will be locked, as no one will update mypy if you do so. \r\nIf you wish to separate the fixes from this PR, you can open another one which only updates mypy to the latest version and fixes the relevant issues "
      ],
      "checkov-consistent-naming-conventions": [
        "use `tuple` and `str | None` instead of `Tuple` and `Optional` - the rest of the code here already uses this syntax so no need to mix",
        "the typing here is inconsistent - can you use `str | None` instead of `Optional[str]`?",
        "done",
        "can you rename `each` to something like `resource` or `changed_resource`?\r\nWill be more readable in python code (even though I guess it comes from terraform conventions)",
        "Maybe change the name to `pickle_deepcopy`?\r\nI just want the IDE to complete me automatically to the correct deepcopy, \r\nto avoid mistakenly importing the wrong one.\r\nAlso, it will be clearer to new developers in the future that this is something internal"
      ],
      "checkov-strategic-exception-management": [
        "can this part result in a `StopIteeration` being thrown?"
      ],
      "checkov-thorough-test-assertions": [
        "I would add specific unit tests to this function as well, as this is critical logic",
        "nice test!\r\nHowever, I still think we need some more for the logic - maybe add 1 with more than 1 created module.\r\nyou can find a good example under `tests/terraform/graph/resources/modules`.\r\nThis one even utilizes different modules with a reference to other directories, so I feel it will be a very strong test.\r\n(You don't have to take the whole folder as it might be also too big, maybe just one sub directory like `stacks`)"
      ],
      "checkov-use-appropriate-logging-levels": [
        "maybe warning is better than debug? just because we won't see debug logs by default",
        "Wouldn't the log be now `Done persisting <random-number> resource_subgraph_maps`? this might confuse us as we only save 1 map. maybe instead log the bucket/key/repo or something like that"
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 1,
      "following": 2
    }
  },
  "danielroe": {
    "repos": [
      "nuxt/nuxt"
    ],
    "entries": [
      {
        "slug": "nuxt-benchmark-algorithmic-optimizations",
        "title": "Benchmark algorithmic optimizations"
      },
      {
        "slug": "nuxt-build-documentation-clarity",
        "title": "Build documentation clarity"
      },
      {
        "slug": "nuxt-cache-lifecycle-management",
        "title": "Cache lifecycle management"
      },
      {
        "slug": "nuxt-check-ssr-context",
        "title": "Check SSR context"
      },
      {
        "slug": "nuxt-conditional-component-bundling",
        "title": "conditional component bundling"
      },
      {
        "slug": "nuxt-configuration-method-selection",
        "title": "Configuration method selection"
      },
      {
        "slug": "nuxt-configuration-resolution-patterns",
        "title": "configuration resolution patterns"
      },
      {
        "slug": "nuxt-consistent-code-formatting",
        "title": "consistent code formatting"
      },
      {
        "slug": "nuxt-documentation-formatting-consistency",
        "title": "documentation formatting consistency"
      },
      {
        "slug": "nuxt-explicit-api-design",
        "title": "explicit API design"
      },
      {
        "slug": "nuxt-explicit-response-types",
        "title": "explicit response types"
      },
      {
        "slug": "nuxt-follow-vue-api-patterns",
        "title": "Follow Vue API patterns"
      },
      {
        "slug": "nuxt-handle-async-cancellation-properly",
        "title": "Handle async cancellation properly"
      },
      {
        "slug": "nuxt-optimize-array-operations",
        "title": "optimize array operations"
      },
      {
        "slug": "nuxt-optimize-cicd-configurations",
        "title": "optimize CI/CD configurations"
      },
      {
        "slug": "nuxt-organize-accessibility-attributes",
        "title": "organize accessibility attributes"
      },
      {
        "slug": "nuxt-precise-language-usage",
        "title": "precise language usage"
      },
      {
        "slug": "nuxt-prefer-null-comparisons",
        "title": "prefer != null comparisons"
      },
      {
        "slug": "nuxt-preserve-http-header-semantics",
        "title": "Preserve HTTP header semantics"
      },
      {
        "slug": "nuxt-safe-error-data-handling",
        "title": "safe error data handling"
      },
      {
        "slug": "nuxt-semantic-names-with-counters",
        "title": "semantic names with counters"
      },
      {
        "slug": "nuxt-strategic-component-loading",
        "title": "Strategic component loading"
      },
      {
        "slug": "nuxt-use-consistent-terminology",
        "title": "Use consistent terminology"
      },
      {
        "slug": "nuxt-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "nuxt-use-proper-http-utilities",
        "title": "Use proper HTTP utilities"
      },
      {
        "slug": "nuxt-validate-cicd-timing-checks",
        "title": "Validate CI/CD timing checks"
      },
      {
        "slug": "nuxt-vue-context-boundaries",
        "title": "Vue context boundaries"
      }
    ],
    "comments": {
      "nuxt-vue-context-boundaries": [
        "composables shouldn't be called at the top level ('ambiently' in JS) - this relies on being able to access the Nuxt context and will fail on the server.",
        "```suggestion\r\nCurrently, the Nuxt context is only accessible in [plugins](/docs/guide/directory-structure/plugins), [Nuxt hooks](/docs/guide/going-further/hooks), [Nuxt middleware](/docs/guide/directory-structure/middleware) (if wrapped in `defineNuxtRouteMiddleware`), and [setup functions](https://vuejs.org/api/composition-api-setup.html) (in pages and components).\r\n```",
        "```suggestion\r\nFor example, doing `export myState = ref({})` would result in state shared across requests on the server and can lead to memory leaks.\r\n```"
      ],
      "nuxt-configuration-method-selection": [
        "no, nuxt.config is only read when building/dev, not when starting a production server "
      ],
      "nuxt-follow-vue-api-patterns": [
        "I did switch the auto imports in this PR. The reason the export remains is in case someone directly imports from #app/composables/id.\n\n(We will remove this file for 4.x in a separate PR marked as a breaking change.)",
        "ideally we should not only expose this but also use `useLink` internally within the setup function, to reduce code duplication and ensure the logic remains the same",
        "This matches `RouterLink` behaviour (which exposes a 'bound' composable which other libraries, like vuetify, can use to create custom links): https://router.vuejs.org/guide/advanced/composition-api.html#useLink."
      ],
      "nuxt-configuration-resolution-patterns": [
        "I think we could probably just do:\r\n\r\n```suggestion\r\n    const extraPageMetaExtractionKeys = nuxt.options?.experimental?.extraPageMetaExtractionKeys || []\r\n```\r\n\r\n... as the issue is presumably that it's not defined, rather than set to a string or something like that",
        "not sure I understand \r\n\r\nah. I get what you're saying. that's actually the default behavior of nuxt schema if you pass a raw object",
        "```suggestion\r\n          const [srcDir, assetsDir] = await Promise.all([\r\n            get('srcDir'),\r\n            get('dir.assets'),\r\n          ])\r\n          return {\r\n            [basename(assetsDir)]: resolve(srcDir, assetsDir),\r\n            ...typeof val === 'object' ? val : {},\r\n          }\r\n```",
        "This is also shared in Nitro:\r\n\r\nhttps://github.com/unjs/nitro/blob/e4f687d32d4d6ec7bd7b011a59edb374b477fbf4/src/core/config/resolvers/runtime-config.ts#L31-L39\r\n\r\nBy doing this, the value is dropped out of `runtimeConfig` entirely and can't be overridden at runtime. See https://github.com/nuxt/nuxt/pull/18586 for more context.\r\n\r\nI recall doing it before, but if we want to allow setting these values to undefined/null values, we likely need to (re)investigate where the keys are being 'dropped' when serialised.",
        "A little tweak required.\r\n\r\nif it's not v4 compat, then by default this should be `<srcDir>/server`. In v4 compat, it should be `<rootDir>/server`. If the user provides it then in either case it should be resolved relative to `<rootDir>`.",
        "we should probably also check if `vue.script.propsDestructure` is set, for backwards-compatibility",
        "Nice find! I think:\n\n```suggestion\n        [basename(publicDir)]: resolve(srcDir, publicDir),\n```\n",
        "Previously `dir.public` was resolved relative to `srcDir`. For backwards compatibility `publicDir` is now fully resolved elsewhere in the schema, so effectively the first argument to `resolve` is ignored. But in the case a user sets it to `some-dir` this will be respected as `<srcDir>/some-dir`.",
        "I don't think we have a `nuxt.options.cacheDir` option set, so this should probably be something more like:\r\n\r\n```suggestion\r\n      $resolve: async (val, get) => val ?? resolve(await get('rootDir'), 'node_modules/.cache/vite'),\r\n```"
      ],
      "nuxt-benchmark-algorithmic-optimizations": [
        "I'm not sure this is quite right. Do we only want to scan variables if the code has comments? And I think we should respect 'scope' rather than looking for any variable anywhere in the code.\r\n\r\nIn general I'd prefer to avoid using regexp for this. I feel a parsing-based scope-tracking approach would be sounder (but we can make that performant by only parsing the code if it code contains some of the 'reserved' names - if not we have a 'fast path' without parsing)",
        "by this point `array[last]` will always be undefined because you've reduced the size of the array.\r\n\r\nhere's a functioning solution:\r\n\r\n```suggestion\r\n      const lastItem = data[data.length - 1]\r\n      if (i < --data.length) {\r\n        array[i] = lastItem\r\n```\r\n\r\nhowever, it's not faster than the existing implementation (see [benchmark](https://jsbenchmark.com/#eyJjYXNlcyI6W3siaWQiOiJXR01CMEJLVXgwbUJDYVc3NmFHSVciLCJjb2RlIjoibGV0IGEgPSBEQVRBXG5hID0gZmlsdGVyKGEsIGkgPT4gaSAlIDUwID09PSAwKVxuYSA9IGZpbHRlcihhLCBpID0-IGkgJSAxMCA9PT0gMClcbmEgPSBmaWx0ZXIoYSwgaSA9PiBpICUgMiA9PT0gMCkiLCJuYW1lIjoiZmlsdGVyIiwiZGVwZW5kZW5jaWVzIjpbXX0seyJpZCI6Ik9VSnozNU1QTkdhWVZ2eVo3S3A1UiIsImNvZGUiOiJsZXQgYSA9IERBVEFcbmEgPSBmaWx0ZXJJblBsYWNlKGEsIGkgPT4gaSAlIDUwID09PSAwKVxuYSA9IGZpbHRlckluUGxhY2UoYSwgaSA9PiBpICUgMTAgPT09IDApXG5hID0gZmlsdGVySW5QbGFjZShhLCBpID0-IGkgJSAyID09PSAwKSIsIm5hbWUiOiJmaWx0ZXJJblBsYWNlIiwiZGVwZW5kZW5jaWVzIjpbXX0seyJpZCI6InctMm8ybWt5VzBmUXZpdi1CTUdmbiIsImNvZGUiOiJsZXQgYSA9IERBVEFcbmEgPSB0dXJib0ZpbHRlckluUGxhY2UoYSwgaSA9PiBpICUgNTAgPT09IDApXG5hID0gdHVyYm9GaWx0ZXJJblBsYWNlKGEsIGkgPT4gaSAlIDEwID09PSAwKVxuYSA9IHR1cmJvRmlsdGVySW5QbGFjZShhLCBpID0-IGkgJSAyID09PSAwKSIsImRlcGVuZGVuY2llcyI6W10sIm5hbWUiOiJ0dXJib0ZpbHRlciJ9XSwiY29uZmlnIjp7Im5hbWUiOiJCYXNpYyBleGFtcGxlIiwicGFyYWxsZWwiOmZhbHNlLCJnbG9iYWxUZXN0Q29uZmlnIjp7ImRlcGVuZGVuY2llcyI6W119LCJkYXRhQ29kZSI6Imdsb2JhbFRoaXMuZmlsdGVyID0gZnVuY3Rpb24gZmlsdGVyKGRhdGEsIHByZWRpY2F0ZSkge1xuICByZXR1cm4gZGF0YS5maWx0ZXIocHJlZGljYXRlKVxufVxuXG5nbG9iYWxUaGlzLmZpbHRlckluUGxhY2UgPSBmdW5jdGlvbiBmaWx0ZXJJblBsYWNlKGRhdGEsIHByZWRpY2F0ZSkge1xuICBmb3IgKGxldCBpID0gZGF0YS5sZW5ndGg7IGktLTsgaT49MCkge1xuICAgIGlmICghcHJlZGljYXRlKGRhdGFbaV0sIGksIGRhdGEpKVxuICAgICAgZGF0YS5zcGxpY2UoaSwgMSlcbiAgfVxuICByZXR1cm4gZGF0YVxufVxuXG5nbG9iYWxUaGlzLnR1cmJvRmlsdGVySW5QbGFjZSA9IGZ1bmN0aW9uIHR1cmJvRmlsdGVyKGRhdGEsIHByZWRpY2F0ZSkge1xuICBmb3IgKGxldCBpID0gZGF0YS5sZW5ndGg7IGktLTsgaSA-PSAwKSB7XG4gICAgaWYgKCFwcmVkaWNhdGUoZGF0YVtpXSwgaSwgZGF0YSkpIHtcbiAgICAgIGNvbnN0IGxhc3RJdGVtID0gZGF0YVtkYXRhLmxlbmd0aCAtIDFdXG4gICAgICBpZiAoaSA8IC0tZGF0YS5sZW5ndGgpIGRhdGFbaV0gPSBsYXN0SXRlbTtcbiAgICB9XG4gIH1cbiAgcmV0dXJuIGRhdGE7XG59XG5yZXR1cm4gWy4uLkFycmF5KDUwMDAwKS5rZXlzKCksLi4uQXJyYXkoNTAwMDApLmtleXMoKSwuLi5BcnJheSg1MDAwMCkua2V5cygpXSJ9fQ))\r\n\r\nand @antfu is right - the performance improvement is probably not noticeable in a Nuxt app, and we do probably need to keep the order of the items in the array. (but I'd be open for metrics showing otherwise!)",
        "Unfortunately I don't think that would work with a Set - though maybe we could use an object and iterate over its keys? What do you think about perf tradeoff?"
      ],
      "nuxt-use-descriptive-names": [
        "```suggestion\n/// <reference path=\"./types/shared.d.ts\" />\n```\n\ni would also recommend renaming to shared-imports.d.ts for clarity",
        "what about just naming the flag `cookieStore` instead to identify the API that would be used? And linking to CookieStore API docs both here and in 1.experimental-features.md?",
        "Maybe better: `nodeModulesDirectories`?"
      ],
      "nuxt-use-consistent-terminology": [
        "Although technically correct (as in, it's the `UserConfig` export from `vite`), this is less intuitive for users who need to know that this configuration is _vite_ configuration. Can you think of a better way of communicating it?",
        "```suggestion\nimport App from '~/app.vue'\n```\n"
      ],
      "nuxt-safe-error-data-handling": [
        "```suggestion\n        ssrError.data = destr(ssrError.data)\n```\n"
      ],
      "nuxt-cache-lifecycle-management": [
        "I was wanting to avoid fragility in case you changed case of headers or added some additional ones that would break JS-enabled error pages (as we render in Nuxt).",
        "`Cache-Control` doesn't seem to be lowercase, fyi, in dev, though it is in prod",
        "would it resolve it just to do something like:\r\n\r\n```suggestion\r\n  return JSON.parse(JSON.stringify(extractedMeta))\r\n```",
        "yes, we'd also have to update it there too",
        "we could use klona",
        "you had some nice tests in the previous commit…. is it worth keeping them?",
        "excellent point. on it 👍 ",
        "actually, seems I already implemented at https://github.com/nuxt/nuxt/commit/4f85cff8d94c575c5f4238a4a8f8472beb1bce5d. Review very appreciated 🙏 ",
        "Agreed!"
      ],
      "nuxt-prefer-null-comparisons": [
        "@manniL I also had to check, but I think it's good:\r\n\r\n![CleanShot 2024-03-10 at 10 23 02@2x](https://github.com/nuxt/nuxt/assets/28706372/d7c719dc-5c6f-4384-bcc7-04d60037fff6)\r\n",
        "what else could it be?"
      ],
      "nuxt-explicit-api-design": [
        "in honesty I'm not yet sure about this as an API. But I also might be missing something.\r\n\r\nWhat is the situation where passing `force` wouldn't be good enough?",
        "on the force option for refresh, could we just add the force option and basically ignore hasCachedData when forced? And always respect hasCachedData when calling refresh from watch?\r\n",
        "we should probably omit hash/query if they are not present",
        "```suggestion\r\n    if (options?.replace) {\r\n      return typeof to === 'string' ? { path: to, replace: true } : { ...to, replace: true }\r\n    }\r\n    return to\r\n```",
        "the key thing that's missing from the nitro implementation is the ability to get the route rules for an arbitrary path... (or we would just use it on the server side)\r\n\r\nmight it be possible for nitro to handle a path rather than just the current event?",
        "totally on-board with aligning them",
        "What about updating the signature so we call this with `start({ force: true})` (and also below with `finish`)? This makes it more explicit and allows passing more options in future.",
        "Sorry, to clarify I don't mean passing it as an option to `useLoadingIndicator` but to the `start` and `finish` functions.",
        "The fewer options that `useLoadingIndicator` accepts, the better, because they will influence the global singleton.\r\n\r\nI think we should pass the override to start/finish and calling `_hide` or `_reset` should instead clear any timeout that is still running. (We probably should have implemented it that way in the first place.)",
        "Thank you!"
      ],
      "nuxt-consistent-code-formatting": [
        "```suggestion\r\n      hasUAVisualTransition ||\r\n      !isChangingPage(to, from)\r\n```"
      ],
      "nuxt-build-documentation-clarity": [
        "```suggestion\r\n\r\nWhen prerendering a client-rendered app, Nuxt will generate `index.html`, `200.html` and `404.html` files by default. However, if you need to prevent any (or all) of these files from being generated in your build, you can use the `'prerender:generate'` hook from [Nitro](/docs/getting-started/prerendering#prerendergenerate-nitro-hook).\r\n```"
      ],
      "nuxt-conditional-component-bundling": [
        "this implementation will result in always bundling `<NuxtPage>`, even if the `vue-router` integration isn't enabled.\r\n\r\nwe should probably protect it so that if no routes have the `isolate` metadata then there will be no changes to the bundle at all"
      ],
      "nuxt-organize-accessibility-attributes": [
        "```suggestion\n      <h1 class=\"flex flex-col gap-y-4 items-center justify-center\" aria-label=\"Nuxt {{ version }}\">\n```",
        "```suggestion\n          <svg role=\"img\" aria-label=\"Nuxt\" class=\"h-8 sm:h-12\" xmlns=\"http://www.w3.org/2000/svg\" fill=\"none\" viewBox=\"0 0 800 200\">\n```"
      ],
      "nuxt-strategic-component-loading": [
        "This will only have an effect if plugins are asynchronous. I think I would first advise avoiding any async activity in a plugin, and then fall back to making plugins `parallel`.",
        "let's start with a single page. we can update the plugins PR from Julien to augment this page + later see if we need to split.",
        "lazy hydration with `hydrate-never` will still hydrate things on client-side navigation unless it was part of the initial render.",
        "if Julien says we should omit server components for now, let's do omit that section for now.\r\n\r\nIt's still worth mentioning lazy hydration.",
        "I'm worried we might give the wrong impression, as these all take place in the same network request, with additional server components rendered with 'internal' fetches.\r\n\r\n```suggestion\r\nBe careful when nesting islands within other islands as each island adds some extra overhead.\r\n```"
      ],
      "nuxt-precise-language-usage": [
        "```suggestion\r\n- `fn`: The function to run once. It can be asynchronous.\r\n```"
      ],
      "nuxt-preserve-http-header-semantics": [
        "this is an already-serialised 'set-cookie' header, not a raw value. what we're doing here is very similar to the mergeHeaders function in h3:\r\n\r\nhttps://github.com/unjs/h3/blob/67af0575eb2078ba4c2ed1c535dfc168a4e578e6/src/response.ts#L101-L111",
        "the issue is that we want to _preserve_ the existing headers, as they are, rather than flattening them into a single `set-cookie` header.\r\n\r\nmerging to iterate, but let me know if you have any concerns 🙏 "
      ],
      "nuxt-documentation-formatting-consistency": [
        "```suggestion\r\nprerenderRoutes('/api/content/article/name-of-article')\r\n```",
        "```suggestion\r\n  ssr: false,\r\n  nitro: {\r\n    hooks: {\r\n      'prerender:generate'(route) {\r\n        const routesToSkip = ['/index.html', '/200.html', '/404.html']\r\n        if (routesToSkip.includes(route.route) ) {\r\n          route.skip = true\r\n        }\r\n      }\r\n    }\r\n  }\r\n})\r\n```",
        "```suggestion\r\nWhen prerendering a client-rendered app, Nuxt will generate `index.html`, `200.html` and `404.html` files by default. However, if you need to prevent any (or all) of these files from being generated in your build, you can use the `'prerender:generate'` hook from [Nitro](/docs/getting-started/prerendering#prerendergenerate-nitro-hook).\r\n\r\n```",
        "```suggestion\r\nIf you deploy your app to [static hosting](/docs/getting-started/deployment#static-hosting) with the `nuxi generate` or `nuxi build --prerender` commands, then by default, Nuxt will render every page as a separate static HTML file.\r\n```",
        "```suggestion\r\n  <NuxtLink :to=\"{ name: 'posts-id', params: { id: 123 } }\">\r\n```"
      ],
      "nuxt-optimize-cicd-configurations": [
        "```suggestion\r\n          ref: ${{ steps.pr.outputs.head_sha }}\r\n          fetch-depth: 1\r\n```",
        "```suggestion\n    if: ${{github.event_name == 'push' || github.repository == 'nuxt/nuxt'}}\n```\n"
      ],
      "nuxt-use-proper-http-utilities": [
        "works well!"
      ],
      "nuxt-explicit-response-types": [
        "```suggestion\r\nPrerendered API routes in production may not return the expected response headers, depending on the provider you deploy to. For example, a JSON response might be served with an `application/octet-stream` content type. In this case, you may have to specify an explicit `responseType`.\r\n```",
        "```suggestion\r\nPrerendered API routes in production may not return the expected response headers, depending on the provider you deploy to. For example, a JSON response might be served with an `application/octet-stream` content type.\r\n```",
        "```suggestion\r\nAlways manually set `responseType` when fetching prerenderered API routes.\r\n```",
        "```suggestion\r\nAlways manually set `responseType` when fetching prerendered API routes.\r\n```"
      ],
      "nuxt-validate-cicd-timing-checks": [
        "Can this be forged? ie force push an 'older' commit?"
      ],
      "nuxt-semantic-names-with-counters": [
        "If we want to support defaulting to the component name as a fetch key, as in linked issue, we will need a nested global fetchKey object to keep track of individual indices of each key. That is, the fetch state might look like:\r\n```js\r\n{\r\n  '0': { a: 'mystate' },\r\n  'MyComponent': { a: 'mystate' },\r\n  'MyOtherComponent': [{ a: 'mystate' }, { a: 'mystate2' }],\r\n  // or, better\r\n  'MyOtherComponent-0': { a: 'mystate' },\r\n  'MyOtherComponent-1': { a: 'mystate2' },\r\n  '1': { a: 'mystate' },\r\n}\r\n```\r\n\r\n(If we don't keep track of these indices separately, the same issue that triggered the original bug will reappear because the global numeric fetch key will get out of sync.)",
        "What would `getKey` API look like? Would it just return a valid incremented number unique to the string passed in?\r\n```js\r\ngetKey (keyToLookup: string): number\r\n```"
      ],
      "nuxt-check-ssr-context": [
        "cc: @huang-julien "
      ],
      "nuxt-optimize-array-operations": [
        "this array might have either 1 or 2 members - is it worth optimising it for 1?",
        "I'm pretty sure there's no benefit to switching from `.map` as this already is able to optimise by creating an array with the correct number of elements.",
        "I'm not sure this is a more performant approach... Let's skip this particular change for now.",
        "Understood. TIL. However this is build time code (while bundling up templates) and never impacts a user. I am happy to keep it as is."
      ],
      "nuxt-handle-async-cancellation-properly": [
        "I think if an abortController is triggered, then we should also track this and avoid doing anything with the finished result of an asyncData. so it shouldn't update the data/status, just as if it were cancelled.",
        "this doesn't seem to result in this hook being called:\r\n\r\n```ts\r\nexport default defineNuxtConfig({\r\n  hooks: {\r\n    close: () => {\r\n      console.log('closing things')\r\n    },\r\n  },\r\n})\r\n```"
      ]
    },
    "profile": {
      "location": "Edinburgh, UK",
      "blog": "https://roe.dev",
      "site_admin": false,
      "followers": 5182,
      "following": 167
    }
  },
  "louwers": {
    "repos": [
      "maplibre/maplibre-native"
    ],
    "entries": [
      {
        "slug": "maplibre-native-accurate-documentation-references",
        "title": "Accurate documentation references"
      },
      {
        "slug": "maplibre-native-configure-platform-specific-builds",
        "title": "Configure platform-specific builds"
      },
      {
        "slug": "maplibre-native-consistent-api-practices",
        "title": "Consistent API practices"
      },
      {
        "slug": "maplibre-native-cross-platform-ci-validation",
        "title": "Cross-platform CI validation"
      },
      {
        "slug": "maplibre-native-cross-platform-test-management",
        "title": "Cross-platform test management"
      },
      {
        "slug": "maplibre-native-descriptive-named-constants",
        "title": "Descriptive named constants"
      },
      {
        "slug": "maplibre-native-design-evolution-ready-apis",
        "title": "Design evolution-ready APIs"
      },
      {
        "slug": "maplibre-native-document-containerized-builds",
        "title": "Document containerized builds"
      },
      {
        "slug": "maplibre-native-document-public-api-completely",
        "title": "Document public API completely"
      },
      {
        "slug": "maplibre-native-dry-class-hierarchies",
        "title": "DRY class hierarchies"
      },
      {
        "slug": "maplibre-native-enforce-clear-data-ownership",
        "title": "Enforce clear data ownership"
      },
      {
        "slug": "maplibre-native-externalize-config-values",
        "title": "Externalize config values"
      },
      {
        "slug": "maplibre-native-externalize-configuration-values",
        "title": "Externalize configuration values"
      },
      {
        "slug": "maplibre-native-extract-workflow-scripts",
        "title": "Extract workflow scripts"
      },
      {
        "slug": "maplibre-native-follow-modern-c-guidelines",
        "title": "Follow modern C++ guidelines"
      },
      {
        "slug": "maplibre-native-group-related-properties",
        "title": "Group related properties"
      },
      {
        "slug": "maplibre-native-handle-errors-by-severity",
        "title": "Handle errors by severity"
      },
      {
        "slug": "maplibre-native-modern-c-style-practices",
        "title": "Modern C++ style practices"
      },
      {
        "slug": "maplibre-native-numerical-precision-considerations",
        "title": "Numerical precision considerations"
      },
      {
        "slug": "maplibre-native-optimize-compilation-flags",
        "title": "Optimize compilation flags"
      },
      {
        "slug": "maplibre-native-prefer-safe-null-handling",
        "title": "Prefer safe null handling"
      },
      {
        "slug": "maplibre-native-prefer-values-over-pointers",
        "title": "Prefer values over pointers"
      },
      {
        "slug": "maplibre-native-self-documenting-code-naming",
        "title": "Self-documenting code naming"
      },
      {
        "slug": "maplibre-native-standard-configuration-files",
        "title": "Standard configuration files"
      },
      {
        "slug": "maplibre-native-structure-documentation-effectively",
        "title": "Structure documentation effectively"
      },
      {
        "slug": "maplibre-native-style-compliant-example-code",
        "title": "Style-compliant example code"
      },
      {
        "slug": "maplibre-native-template-instantiation-trade-offs",
        "title": "Template instantiation trade-offs"
      },
      {
        "slug": "maplibre-native-use-proper-logging",
        "title": "Use proper logging"
      },
      {
        "slug": "maplibre-native-use-specific-test-assertions",
        "title": "Use specific test assertions"
      },
      {
        "slug": "maplibre-native-validate-noexcept-guarantees",
        "title": "Validate noexcept guarantees"
      },
      {
        "slug": "maplibre-native-variable-evaluation-context",
        "title": "Variable evaluation context"
      }
    ],
    "comments": {
      "maplibre-native-use-proper-logging": [
        "use mbgl::Log, remove macro",
        "There are commented out lines that print used for debugging throughout this file.\r\n\r\nPlease remove them or change them to (debug) logging.",
        "Should some logging be added in these cases?",
        "Thanks for clarifying.\r\n\r\nI didn't see any retry messages in the logs, so I didn't know if it was actually retrying."
      ],
      "maplibre-native-cross-platform-ci-validation": [
        "CI is complaining that this function is not used.",
        "It does find the provisioning profile for me now. Could we make a bug report over at https://github.com/MobileNativeFoundation/rules_xcodeproj ?\r\n\r\nBwX does not have first-class support from rules_xcodeproj and support for it might be removed altogether in the future. I will add your comment to the discussion [here](https://github.com/MobileNativeFoundation/rules_xcodeproj/discussions/2391).\r\n\r\n",
        "I made the `BUILD_MODE` configurable. You can set it to your liking in your `config.bzl`."
      ],
      "maplibre-native-document-containerized-builds": [
        "If we have to choose one, just include the command without the `___any_build_command___`, because it's nicer to have a valid command that people can copy and paste.\r\n\r\nOptionally mention that you can also run build commands directly.",
        "```suggestion\r\nYou can use a Docker container to build MapLibre Native. A `Dockerfile` that installes the required dependencies when the image is built is provided in this directory.\r\n```"
      ],
      "maplibre-native-self-documenting-code-naming": [
        "I understand `_setDirection` needs the current location so that it can do the animation in one go, but maybe consider passing the center instead of a boolean. I don't think the `_setDirection` method should have anything to do with the location manager.",
        "Looks good now, thanks!",
        "Perhaps this should (also) be called `setTileCacheEnabled(bool)` because it is a bit clearer that it actually disables the tile cache when passing `false`.",
        "`removeDrawablesIf` sounds like a much better name here."
      ],
      "maplibre-native-style-compliant-example-code": [
        "Maybe don't use this because it is an internal class to the test app. Instead use something that users could copy paste in their own apps.",
        "Yes just use a hardcoded style URL like demotiles.",
        "> Personally I'd argue for \"best practice, even if more complicated\" because people will copy and paste our examples into their code and location management is not an easy task.\r\n\r\nFully agreed. Even an example with an external dependency would be fine by me. Let's not forget the iOS MapLibre codebase is quite rusty, so it should not be surprising some things are not very modern anymore.\r\n\r\nI'm just transplanting some examples from https://github.com/mapbox/ios-sdk-examples, I'm not a veteran iOS developer like you, so I am happy for your critical eye!"
      ],
      "maplibre-native-standard-configuration-files": [
        "Toolchain is already installed on the default image."
      ],
      "maplibre-native-prefer-values-over-pointers": [
        "I think using unique pointers but then using `.get()` to get a raw pointer is dangerous because now someone might hold onto a dangling pointer.\r\n\r\nI would try to avoid using raw pointers as much as possible, in this case since `Sprite` is such a lightweight object copying is probably even OK.",
        "What is this?",
        "The return type is marked with `nonnull` and the interface with `null_resettable`, which means this getter should not return null if I understand it correctly. We should not modify this, because I think it would change the Swift API.\r\n\r\nMaybe you can return `[NSURL URLWithString:@\"local://style.json\"]` instead?"
      ],
      "maplibre-native-configure-platform-specific-builds": [
        "@1ec5 mentioned on Slack that a source-only distribution is the prefered way of distribution for Apple platforms now. For MapLibre I don't know if would apply to just the SDK (probably written in Swift in the long term) or also the core written in C++.",
        "It's easier to just clone with `--recurse-submodules`"
      ],
      "maplibre-native-modern-c-style-practices": [
        "Don't use C-style casts.",
        "Do you know structured bindings? I think this would work\r\n\r\n```\r\nfor (auto [texHandle, glyph, fontStack] : glyphsToUpload) {\r\n```",
        "Please have a look over all definitions in the PR and make sure that all immutable values are marked `const`.",
        "You should be able to default this `operator==` since we use C++20 now.",
        "Should be initialized to something.",
        "C++ Code Guidelines say yes. https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#es20-always-initialize-an-object"
      ],
      "maplibre-native-structure-documentation-effectively": [
        "I think we can assume that everyone knows JSON...\r\n\r\nWe can probably link to an explanation of GeoJSON elsewhere and focus on concepts that are specific to MapLibre Android.",
        "All code snippets (except really small ones) should be referenced so that we can be sure they compile in the future.",
        "Each section of the article should have a descriptive heading so people can easily skip to the part they are interested in.",
        "Sections can be short, that is not a problem. But we should have a heading for each part of the article that covers a different concept.",
        "Give the article a descriptive name that explains the contents of the article. \"Ways to Configure the Map\" or something similar."
      ],
      "maplibre-native-accurate-documentation-references": [
        "```suggestion\r\n     * Learn more about above properties in the [Style specification](https://maplibre.org/maplibre-style-spec/).\r\n```"
      ],
      "maplibre-native-descriptive-named-constants": [
        "Avoid magic numbers",
        "Adding comments is good, but I would also assign these numbers to a const.",
        "I think all upper case names should be reserved for macros. https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#nl9-use-all_caps-for-macro-names-only",
        "Keep it `constexpr`, but lowercase them.\r\n\r\nAlso camelcase maybe `pmtilesHeaderOffset`?"
      ],
      "maplibre-native-design-evolution-ready-apis": [
        "This is technically a breaking change right?",
        "Maybe we can deprecate this API and add a new one that has the full rendering stats?",
        "Would it be a breaking change to take a `Call.Factory` here?",
        "If it's source compatible I'd say go ahead and just change the signature, if you don't mind. 🙂 "
      ],
      "maplibre-native-use-specific-test-assertions": [
        "Needs to use `EXPECT_THROW` otherwise the test will fail it doesn't throw."
      ],
      "maplibre-native-variable-evaluation-context": [
        "Yes it is manual, but people can make mistakes. Especially when making a pre-release, which does not require a PR review for a version release."
      ],
      "maplibre-native-template-instantiation-trade-offs": [
        "Are there more lightweight `std::function` alternatives?",
        "This solution is pretty easy to understand. There may be a fancier solution but looks OK to me.",
        "I guess nameIDs are small so this doens't overflow right?\r\n\r\nShould this maybe be extracted in a function? "
      ],
      "maplibre-native-optimize-compilation-flags": [
        "Yes it contains both:\r\n\r\n```\r\n$ ls MapLibre.xcframework\r\nInfo.plist*                  ios-arm64/                   ios-arm64_x86_64-simulator/\r\n```\r\n\r\nBut I am extracting the armv8 dynamic library from the XCFramework:\r\n\r\n```\r\ncp MapLibre.xcframework/ios-arm64/MapLibre.framework/MapLibre MapLibre_dynamic\r\n```\r\n\r\n```\r\n $ file MapLibre_dynamic\r\nMapLibre_dynamic: Mach-O 64-bit dynamically linked shared library arm64\r\n```\r\n\r\nSo should be fine."
      ],
      "maplibre-native-prefer-safe-null-handling": [
        "You are moving the data member `pendingReleases` here. It's no longer valid after that.",
        "<img width=\"659\" alt=\"image\" src=\"https://github.com/user-attachments/assets/2b533993-5a56-4e51-82a7-340d5a5ae186\">\r\n<img width=\"673\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b509f10f-5280-4be1-9006-b3689361f869\">\r\n",
        "Yes I think that is OK.",
        "Maybe try passing `nullptr` here?\r\n\r\n> If zVfsName is NULL then the default VFS is returned.\r\n\r\nhttps://www.sqlite.org/c3ref/vfs_find.html",
        "The memory pointed to by the unique_ptr is not cleaned up after `.release()`.\r\n\r\n",
        "I think you can just `reset()` instead.",
        "Can you avoid using raw pointers?",
        "```suggestion\r\n                                             nullptr)) {}\r\n```\r\n\r\nMight as well update this as well.",
        "Yes I don't think there's a good reason to use `NULL` in C++ code, even when calling C libraries."
      ],
      "maplibre-native-follow-modern-c-guidelines": [
        "Prefer enum class https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#Renum-class",
        "Prefer `enum class`.",
        "Created an issue.",
        "We follow the C++ Core Guidelines, which state\r\n\r\n> [Specify the underlying type of an enumeration only when necessary](https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#Renum-underlying)\r\n\r\nEspecially in this case I think it makes sense to just use the default.",
        "There is something to be said for it, but since the space saving is minimal sticking to the default might be better.\r\n\r\nI will leave it up to your judgement!",
        "Can probably be const.",
        "Can be marked const.",
        "Maybe we can use something like https://github.com/zhihaoy/nontype_functional/blob/main/include/std23/move_only_function.h instead so we can more easily migrate to `std::move_only_function` when we can use C++23.\r\n\r\nIt's also better tested.",
        "If it's not too much hassle I would add it to `vendor/nontype_functional`. Could just be the two files we need.",
        "Can be const."
      ],
      "maplibre-native-validate-noexcept-guarantees": [
        "This can throw which would crash the program (because of `noexcept`). Is it possible to avoid allocation?",
        "If it is unavoidable we should remove `noexcept` here (and from the base class), otherwise the program will crash when an exception is thrown here.",
        "The move constructor of `type::Type` is not `noexcept`.\r\n\r\nbecause `mapbox::util::recursive_wrapper<Array>` is part of the variant\r\n\r\n```cpp\r\n    recursive_wrapper(recursive_wrapper&& operand)\r\n        : p_(new T(std::move(operand.get()))) {}\r\n```",
        "When adding `noexcept` to a templated function, the compiler won't check that the methods called on that type are really `noexcept`. It will just crash on runtime.\r\n\r\nIt is really hard to verify yourself as well (with all the deeply nested templates).\r\n\r\nWith long, complicated, templated functions, it may be worth leaving out `noexcept`. In this case we can maybe use something like:\r\n\r\n```cpp\r\n    using ReturnType = decltype(evaluated.template get<DataDrivenPaintProperty>());\r\n    static_assert(std::is_nothrow_invocable_v<decltype(&ReturnType::isConstant), ReturnType&>,\r\n                  \"isConstant() must be noexcept\");\r\n```",
        "`mbgl::util::clamp` (called somewhere down) is not `noexcept`. But it can be made `noexcept`.",
        "```suggestion\r\n    T operator()(const T& a, const T&, const double) const {\r\n```\r\n\r\nmay call throwing copy constructor",
        "```suggestion\r\nT interpolate(const T& a, const T& b, const float t) {\r\n```\r\n\r\nmay call throwing copy constructor",
        "```suggestion\r\nT interpolate(const T& a, const T& b, const double t) {\r\n```\r\n\r\nmay call throwing copy constructor",
        "`std::make_tuple` is not `noexcept`.",
        "Ah OK, but this is one compiler.\r\n\r\nI would err on the side of safety in general when it comes to `noexcept`. If it is not immediately obvious something is `noexcept`, I would not use it."
      ],
      "maplibre-native-handle-errors-by-severity": [
        "What kind of exceptions can `std::exception_ptr` contain? Can all be safely ignored? ",
        "Is the callback passed to `renderStill` called multiple times? In that case the error would get overwritten in the case of multiple errors.",
        "You need to check the return value of `query.run()`. If it returns `false` no row is available and `query.get` will throw.",
        "Maybe it's better to let it crash here instead? If these functions are not initialized MapLibre will not be able to render anything.",
        "Maybe you could wrap the check in a function that returns a bool and logs a warning if there is no context anymore?\r\n\r\nYour call, but it may save someone a frustrating debugging session."
      ],
      "maplibre-native-extract-workflow-scripts": [
        "Shellcheck detects an issue with this script:\r\n\r\n```\r\nArgument mixes string and array. Use * or separate argument\r\n```",
        "Since this is quite a large script, maybe it can be extracted in a file so it can be easily ran.",
        "Like this?",
        "Yes sounds better, thanks.",
        "I see it relies on caches. That will not speed things up because our cache is full after a single run of the workflows.\r\n\r\nGitHub promised they will allow a bigger cache than just 10GB in Q1 2025 though.",
        "I prefer to just keep it simple for now.",
        "Fixed."
      ],
      "maplibre-native-consistent-api-practices": [
        "```suggestion\r\nMultiple map instances are enabled using a unique context pointer.  A unique context pointer is passed back for every `initialize` invocation. The context pointer is released on `de_initialized` or when the library reference is destroyed.\r\n```",
        "```suggestion\r\nThe environment that consumes the FFI library is responsible for initializing its own graphics backend.  This limits a library artifact to a specific platform (Mac, Window, Linux) and runtime revision.  The required parts are passed into the FFI initialize call using an opaque data pointer (nativeWindow).  Backend/Platform build flags determine how this opaque data pointer is cast/used.\r\n```",
        "```suggestion\r\n* Introduce “Annotation” as a first class citizen of the MapLibre Native C++ Core.\r\n```\r\n\r\nSince this design proposal is for the MapLibre Native repo, it is understood that it does not apply to MapLibre GL JS.",
        "```suggestion\r\nThis proposal introduces the concept of annotations that are complex, animatable and interactive to MapLibre’s core. The first phase allows bitmap backed annotations to be rendered. The second phase will enable native platform views (iOS, Android) to be rendered directly by MapLibre Native.\r\n```",
        "```suggestion\r\nThese new annotations will provide a more flexible tool for drawing user content on the map that is more in line with what developers expect from the built-in mobile map toolkits. The focus on animations and interactivity allows developers to create differentiated map experiences that feel made for mobile.\r\n```"
      ],
      "maplibre-native-cross-platform-test-management": [
        "Maybe we can only ignore it on the platforms where it is failing?\r\n\r\nFor me it is running fine locally on Android.\r\n\r\nIf you would prefer to merge this and look into this later, let's create an issue so we don't forget.",
        "Thanks!",
        "Well, looks like it failed on the Pixel 7 Pro on CI.\r\n\r\nMight be device-dependent. Maybe best to disable it after all so we can merge this."
      ],
      "maplibre-native-numerical-precision-considerations": [
        "`std::numeric_limits<double>::epsilon()` was too small.\r\n\r\nMaybe we should define this globally somewhere."
      ],
      "maplibre-native-externalize-config-values": [
        "In any case getting rid of the magic number would be good."
      ],
      "maplibre-native-document-public-api-completely": [
        "Public APIs could use some docstrings. Adding them to private APIs also does not hurt.",
        "Everything in `include` will become part of our public API. Is this intended? Can you add some triple slash comments to indicate what this template struct is for?",
        "It may make sense to make it internal, in that case you can move it to `/src`.\r\n\r\nSome comments (and a link to cppreference) would still be a good idea in that case.",
        "These could use some Doxygen comments.\r\n\r\n```\r\n    RequestedFromCache ///< like this\r\n```",
        "Suggestion: add a triple slash comment to each enum in the PR, describing what it does.\r\n\r\nIt is also possible to add descriptions to enum values.\r\n\r\n````\r\n/// This is an enum class\r\nenum class fooenum {\r\n    FOO, ///< this is foo\r\n    BAR, ///< this is bar\r\n};\r\n````",
        "A documentation comment should be added to this class.\r\n\r\nSome (public) methods including the constructors might also benefit from documentation comments, although they are relatively straightforward."
      ],
      "maplibre-native-dry-class-hierarchies": [
        "Duplicated. I feel like this should be in a common base class.",
        "Most code in this class is identical to the GL version. They can probably use a common base class."
      ],
      "maplibre-native-enforce-clear-data-ownership": [
        "Use a lock guard? Can deadlock if anything throws I think.\r\n\r\nAlso applies to other methods.",
        "Somehow my brain can't parse this capture group."
      ],
      "maplibre-native-group-related-properties": [
        "What kind of extra properties would be relevant for `MLNPluginLayerDrawingContext` but not `MLNStyleLayerDrawingContext`?",
        "Honestly I would make `ActionJournalOptions` objects immutable if possible. You could return a new `ActionJournalOptions` if you think that is a good API."
      ],
      "maplibre-native-externalize-configuration-values": [
        "Good suggestion, but that is outside the scope of this PR, because this is also what we do on `main`:\r\n\r\n```groovy\r\n                        // Enable ccache if the user has installed it.\r\n                        if (file(\"/usr/bin/ccache\").exists()) {\r\n                            arguments \"-DANDROID_CCACHE=/usr/bin/ccache\"\r\n                        } else if (file(\"/usr/local/bin/ccache\").exists()) {\r\n                            arguments \"-DANDROID_CCACHE=/usr/local/bin/ccache\"\r\n                        }\r\n```",
        "Could you create a new issue for this? "
      ]
    },
    "profile": {
      "location": "Germany, Europe",
      "blog": "",
      "site_admin": false,
      "followers": 72,
      "following": 88
    }
  },
  "jacoblee93": {
    "repos": [
      "langchain-ai/langchainjs"
    ],
    "entries": [
      {
        "slug": "langchainjs-ai-dependency-management",
        "title": "AI dependency management"
      },
      {
        "slug": "langchainjs-avoid-hardcoded-configurations",
        "title": "Avoid hardcoded configurations"
      },
      {
        "slug": "langchainjs-chunked-data-processing",
        "title": "Chunked data processing"
      },
      {
        "slug": "langchainjs-comprehensive-ai-documentation",
        "title": "Comprehensive AI documentation"
      },
      {
        "slug": "langchainjs-consistent-naming-conventions",
        "title": "Consistent naming conventions"
      },
      {
        "slug": "langchainjs-constructor-over-setter",
        "title": "Constructor over setter"
      },
      {
        "slug": "langchainjs-dependency-classification-standards",
        "title": "Dependency classification standards"
      },
      {
        "slug": "langchainjs-follow-documentation-standards",
        "title": "Follow documentation standards"
      },
      {
        "slug": "langchainjs-platform-appropriate-environment-variables",
        "title": "Platform-appropriate environment variables"
      },
      {
        "slug": "langchainjs-prefer-nullish-coalescing",
        "title": "Prefer nullish coalescing"
      },
      {
        "slug": "langchainjs-preserve-api-backward-compatibility",
        "title": "Preserve API backward compatibility"
      },
      {
        "slug": "langchainjs-simplify-code-organization",
        "title": "Simplify code organization"
      },
      {
        "slug": "langchainjs-throw-meaningful-errors",
        "title": "Throw meaningful errors"
      },
      {
        "slug": "langchainjs-typescript-naming-standards",
        "title": "TypeScript naming standards"
      },
      {
        "slug": "langchainjs-use-comprehensive-jsdoc",
        "title": "Use comprehensive JSDoc"
      },
      {
        "slug": "langchainjs-use-database-native-types",
        "title": "Use database-native types"
      },
      {
        "slug": "langchainjs-validate-untrusted-input",
        "title": "Validate untrusted input"
      }
    ],
    "comments": {
      "langchainjs-typescript-naming-standards": [
        "Let's not take a default here\r\n\r\nAlso, we are standardizing on `model` over `modelName`",
        "By convention we capitalize types/interfaces",
        "We are standardizing as `model` instead of `modelName`"
      ],
      "langchainjs-comprehensive-ai-documentation": [
        "I would show how to initialize `.fromDocuments` as well as from an existing store",
        "Can we have this use the standard retriever template?\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/retrievers.ipynb",
        "Can you use the embeddings docs template?\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/INTEGRATIONS.md#documentation-and-integration-tests\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/text_embedding.ipynb"
      ],
      "langchainjs-use-database-native-types": [
        "Would suggest storing this as an ISO string -  not all (most?) vector stores will not deserialize dates",
        "OOC why not just support `metadataJsonColumn` vs spreading metadata into other columns?",
        "And the JSON column is just to make onboarding easier?"
      ],
      "langchainjs-follow-documentation-standards": [
        "Please make the docs pages follow this format:\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/llms.ipynb",
        "Can we make the docs follow this template?\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/vectorstores.ipynb\r\n\r\nI need to update the contributing instructions...",
        "We should definitely add docs for `JsonOutputFunctionsParser`",
        "Should we have the `## Overview` boilerplate?",
        "Should at least link to vector store conceptual docs and details table"
      ],
      "langchainjs-preserve-api-backward-compatibility": [
        "Isn't removing all of these a breaking change?",
        "Not sure what current usage is, but would love a shim for principle's sake. I can try to have a look later if you don't have time.",
        "Thank you! This is a very recent integration, but we really try to not have any breaking changes in patches unless they are completely unavoidable.",
        "This conflicts with the `ToolCall` declared in `base.ts` - you should import it like this:\r\n\r\n```ts\r\nimport { type ToolCall } from \"@langchain/core/messages/tool\";\r\n```\r\n\r\nWe will remove the old type on next breaking change.",
        "Would prefer this as a static method like this:\r\n\r\nhttps://v02.api.js.langchain.com/classes/langchain_community_vectorstores_pgvector.PGVectorStore.html#initialize\r\n\r\nSince I'm not sure this will show up well in API refs, but won't block on it",
        "Hey sorry, no I mean a single param like this so as not to break the interface:\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/vectorstores/pinecone.ts#L174"
      ],
      "langchainjs-platform-appropriate-environment-variables": [
        "JavaScript uses `process.env`",
        "Yeah let's cut these if we can't find an automatic way to import them. If we really need something Node specific we can just use `.mdx`\r\n\r\nI can also reach out to the Deno team to see if they have suggestions, we have a channel with them."
      ],
      "langchainjs-use-comprehensive-jsdoc": [
        "Let's just log this once on initial call to avoid flooding the console and add some example code",
        "Ah right... this is used in multiple places\r\n\r\n@hntrl maybe let's make a small docs page with agnostic info here?",
        "nit: Add docstring with usage example",
        "Might be nice to have chat history/vectorstore classes just accept params to instantiate an engine on init in addition to taking a full engine class for connection reuse so that the user has one less import/abstraction to worry about",
        "Ok, sounds good. Yeah definitely better to do it this way for prod, just more pieces and classes to set up for getting started"
      ],
      "langchainjs-chunked-data-processing": [
        "Why not just interact with instances of `this.backingStore` directly?\r\n\r\nWould rather just init three different instances of it or just have some kind of `.bind()` semantic if absolutely necessary - there's a lot of inheritance already\r\n\r\nIf just used for tests, put in a `utils/testing` file"
      ],
      "langchainjs-prefer-nullish-coalescing": [
        "Only if someone's passing `disableStreaming: false` right? I think fine",
        "You can just do `fields?.disableStreaming ?? true`",
        "Don't think we need the `.length` check\r\n\r\nAlso should use triple equals if at all",
        "May be slightly safer to add these fields only if they are present - not sure how Azure or other model proxies will react to adding `audio: undefined`\r\n\r\n"
      ],
      "langchainjs-dependency-classification-standards": [
        "I don't think this is necessary? There is a web built-in",
        "Should be a peer + dev dep, not a direct dependency:\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/INTEGRATIONS.md#third-party-dependencies",
        "This also should be an optional peer dep:\r\n\r\nhttps://js.langchain.com/docs/contributing/code/#adding-an-entrypoint"
      ],
      "langchainjs-throw-meaningful-errors": [
        "Good to leave `response` directly on the error object - `AsyncCaller` uses fields on it to decide retries."
      ],
      "langchainjs-constructor-over-setter": [
        "Prefer `.invoke()`",
        "Also make sure this is tested",
        "`PyPDFLoader` isn't in JS",
        "So you need to click a link every hour to use this integration?",
        "I don't fully understand from the docs -  is there a way to just get tht refresh token during setup and just use that?"
      ],
      "langchainjs-validate-untrusted-input": [
        "May be worth a small note in docs emphasizing that `tableName` and `schemaName` in these files are not escaped and to not pass in end-user input here",
        "nit: can we escape column names too/disallow nonalphanumeric?",
        "Yeah good call"
      ],
      "langchainjs-consistent-naming-conventions": [
        "Let's emphasize somewhere that this wraps Unstructured\r\n\r\nShould we call this `DropboxUnstructuredLoader` instead?",
        "Separate words with underscore:\r\n`GOOGLE_DRIVE_CREDENTIALSPATH` -> `GOOGLE_DRIVE_CREDENTIALS_PATH`"
      ],
      "langchainjs-avoid-hardcoded-configurations": [
        "Let's avoid defaults at this level when possible",
        "We should ideally let the backend set this in case they change best practices, would prefer to have things unset if it won't cause issues (which it wasn't before)",
        "Thank you, old defaults have started causing issues for OpenAI so it's top of mind right now",
        "Should we be hardcoding this here?\r\n\r\nIf it's only available in one region now, would prefer to have this configurable or even not have a default at all and just have it documented"
      ],
      "langchainjs-ai-dependency-management": [
        "Did we need to add this?",
        "We don't accept hard dependencies, see:\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/INTEGRATIONS.md "
      ],
      "langchainjs-simplify-code-organization": [
        "Can we put this in an existing entrypoint? `types` maybe?",
        "Want to avoid fragmentation"
      ]
    },
    "profile": {
      "location": "San Francisco",
      "company": "@langchain-ai",
      "blog": "https://jacobscript.dev",
      "twitter_username": "hacubu",
      "site_admin": false,
      "followers": 1274,
      "following": 0
    }
  },
  "ggerganov": {
    "repos": [
      "ggml-org/llama.cpp"
    ],
    "entries": [
      {
        "slug": "llama.cpp-ai-parameter-organization",
        "title": "AI parameter organization"
      },
      {
        "slug": "llama.cpp-api-minimalism-principle",
        "title": "API minimalism principle"
      },
      {
        "slug": "llama.cpp-choose-appropriate-error-mechanism",
        "title": "Choose appropriate error mechanism"
      },
      {
        "slug": "llama.cpp-eliminate-code-duplication",
        "title": "eliminate code duplication"
      },
      {
        "slug": "llama.cpp-enable-callback-chaining",
        "title": "Enable callback chaining"
      },
      {
        "slug": "llama.cpp-explicit-control-flow-logic",
        "title": "explicit control flow logic"
      },
      {
        "slug": "llama.cpp-follow-naming-conventions",
        "title": "Follow naming conventions"
      },
      {
        "slug": "llama.cpp-maintain-code-consistency",
        "title": "maintain code consistency"
      },
      {
        "slug": "llama.cpp-maintain-consistent-naming-patterns",
        "title": "Maintain consistent naming patterns"
      },
      {
        "slug": "llama.cpp-maintain-naming-consistency",
        "title": "maintain naming consistency"
      },
      {
        "slug": "llama.cpp-measure-algorithm-performance-impact",
        "title": "measure algorithm performance impact"
      },
      {
        "slug": "llama.cpp-measure-before-optimizing",
        "title": "measure before optimizing"
      },
      {
        "slug": "llama.cpp-metal-shared-memory-sizing",
        "title": "Metal shared memory sizing"
      },
      {
        "slug": "llama.cpp-optimize-algorithmic-complexity",
        "title": "optimize algorithmic complexity"
      },
      {
        "slug": "llama.cpp-use-environment-variables",
        "title": "Use environment variables"
      },
      {
        "slug": "llama.cpp-use-model-metadata",
        "title": "use model metadata"
      },
      {
        "slug": "llama.cpp-validate-bounds-before-access",
        "title": "validate bounds before access"
      }
    ],
    "comments": {
      "llama.cpp-use-model-metadata": [
        "This should be handled by the meta data in the GGUF model. There is a boolean field for when BOS is needed or not."
      ],
      "llama.cpp-eliminate-code-duplication": [
        "After adding the broadcast support to `ggml_set_rows()` this is not really needed anymore, but I think it's nice to have either way.",
        "Ok, will add template in a follow-up PR. For now, removed the i64 support and added TODO."
      ],
      "llama.cpp-use-environment-variables": [
        "Avoid this by checking an environment variable instead. See `GGML_SCHED_DEBUG` for an example."
      ],
      "llama.cpp-maintain-code-consistency": [
        "For long argument lists, list them on new lines to improve readibility.",
        "This method is analogous to the `build_attn_inp_` methods, so we have to model it in a similar way.\r\n\r\nReplace this method with:\r\n\r\n```c++\r\n    // similar to build_attn_inp_kv_unified()\r\n    llm_graph_input_rs * build_rs_inp() const;\r\n```\r\n\r\nIntroduce new input class:\r\n\r\n```c++\r\n// similar to llm_graph_input_attn_kv_unified\r\n// put the `s_copy` tensor in this class (similar to `kq_mask`)\r\nclass llm_graph_input_rs : public llm_graph_input_i;\r\n```\r\n\r\nIn the future, this input class could be extended with additional input tensors that are needed by the recurrent cache if necessary (similar to the attention input classes).\r\n\r\nReplace `build_recurrent_state()` and `build_rwkv_shift_load()` with overloads:\r\n\r\n```c++\r\n    // similar to build_attn()\r\n    ggml_tensor * build_rs(\r\n        llm_graph_input_rs * inp,\r\n             ggml_cgraph * gf,\r\n             ggml_tensor * s,             \r\n                 int32_t   state_size,\r\n                 int32_t   n_seqs,\r\n                    bool   avoid_copies = false) const;\r\n\r\n    // similar to build_attn()\r\n    ggml_tensor * build_rwkv_token_shift_load(\r\n        llm_graph_input_rs * inp,\r\n             ggml_cgraph * gf,\r\n      const llama_ubatch & ubatch,\r\n                     int   il) const;\r\n```\r\n\r\n",
        "When this change is applied, we have to do a similar addition for the hybrid implementation. The basic pattern is that you need to introduce a new input class similar to `llm_graph_input_attn_kv_unified` and `llm_graph_input_rs`, but this one will contain inputs for both the attention and for the recurrent state.\r\n\r\nSo probably something like:\r\n\r\n```c++\r\n// this input class will have both the input tensors needed for the attention and for\r\n// the recurrent state. see llm_graph_input_attn_kv_unified_iswa for example\r\nclass llm_graph_input_mem_hybrid : public llm_graph_input_i;\r\n```\r\n\r\nWe then add overloads for `build_attn()`, `build_rs()` and `build_rwkv_token_shift_load()`.",
        "For now it's important to follow the existing patterns even if there is some extra code duplication. We can rework the implementation if needed in separate refactor PRs.\r\n\r\nThe recommended way to avoid large duplications is how the `build_attn()` is implemented to use a helper, memory-agnostic method `build_attn_mha()`. This way, the memory-specific logic is implemented in the `build_attn()` overloads and the bulk of the remaining logic is reused by calling `build_attn_mha()`. You can apply this pattern both for `build_rs()` and `build_mambaX_layer()`.",
        "Looking at the `build_mamba_layer()` function, it appears complex, but actually it boils down to something like this:\r\n\r\n```c++\r\nbuild_mamba_layer(inp) {\r\n    rec_state = static_cast<...>(mstate);\r\n\r\n    conv = rec_state->update_conv(inp); // analogous to unified get_k() + cpy_k()\r\n    ssm  = rec_state->update_ssm (inp); // analogous to unified get_v() + cpy_v()\r\n\r\n    common = get_common(model);\r\n\r\n    do_something_with_conv(model, common, conv);\r\n    do_something_with_ssm (model, common, ssm);\r\n}\r\n```\r\n\r\nThe `get_common(model)` and `do_something_with_...(model, ...)` parts have to be extracted out of this function and remain implemented in `llama-model.cpp` because they use model tensors and do not depend on the memory module. The `update_conv()` and `update_ssm()` function have to be implemented only once in the `class llama_kv_cache_recurrent`. Internally, they can use a helper `build_recurrent_state()` function to deduplicate the code. Note that the hybrid cache will call these methods from the recurrent cache instance and won't have to implement them a second time.\r\n\r\nWhen you decompose the function like this, the implementation becomes much simpler:\r\n\r\n```c++\r\nbuild_mamba_layer(inp) {\r\n    rec_state = static_cast<...based_on_inp...>(m_state);\r\n\r\n    conv = rec_state->update_conv(inp);\r\n    ssm  = rec_state->update_ssm (inp);\r\n\r\n    return { conv, ssm };\r\n}\r\n```\r\n\r\nThis is much easier to overload multiple times for different `inp` types. I would even split it one more time:\r\n\r\n```c++\r\n// llama-graph:\r\n\r\n// one for recurrent and one for hybrid\r\nbuild_mamba_conv(inp) {\r\n    rec_state = static_cast<...based on inp...>(m_state);\r\n\r\n    conv = rec_state->update_conv(inp->get_s_copy());\r\n    \r\n    return conv;\r\n}\r\n\r\n// one for recurrent and one for hybrid\r\nbuild_mamba_ssm(inp) {\r\n    rec_state = static_cast<...based on inp...>(m_state);\r\n\r\n    ssm = rec_state->update_ssm(inp->get_s_copy());\r\n    \r\n    return ssm;\r\n}\r\n```\r\n\r\nI hope I'm not missing something, but I think this should work and should be clean.\r\n\r\nAlternatively, we can also just bring the PR to a working state any way you think makes sense and then I will try to do a refactoring pass and see how it goes. Would just need some sample test commands to experiment with."
      ],
      "llama.cpp-measure-algorithm-performance-impact": [
        "I am not convinced that we have to add this new implementation.\r\n\r\nCan you provide a reference for the PlaMo-2 tokenizer so we can understand how it differs from the existing tokenizer algorithms?",
        "Regarding the implementation here - it has to follow the existing pattern for all other tokenizers. No need to create a new `class llama_vocab_plamo2`.",
        "> ... handle multiple stop words efficiently - with grammar trigger words we may have many\r\n\r\nCan you clarify why is that and how many stop words we could have in typical use cases?",
        "I see. I don't have a good sense of the computation complexity for finding the trigger words in typical use cases. If we can show that the algorithm improves the performance in a measurable way, then it's ok. If not, we might want to fallback to some simpler brute-force approach."
      ],
      "llama.cpp-optimize-algorithmic-complexity": [
        "```suggestion\r\n#ifdef __ARM_FEATURE_MATMUL_INT8\r\n    assert((nrc == 2) || (nrc == 1));\r\n#else\r\n    assert(nrc == 1);\r\n#endif\r\n```"
      ],
      "llama.cpp-follow-naming-conventions": [
        "The convention is to match the enum name with the prefix of the values:\r\n\r\n```suggestion\r\nenum diffusion_alg {\r\n    DIFFUSION_ALG_ORIGIN       = 0,\r\n    DIFFUSION_ALG_MASKGIT_PLUS = 1,\r\n    DIFFUSION_ALG_TOPK_MARGIN  = 2,\r\n    DIFFUSION_ALG_ENTROPY      = 3,\r\n};\r\n```",
        "Add a `DEPRECATED` overload of this call to make it easier for developers to adjust to the change.",
        "Ah, we break the `llama_sampler_init_...` pattern if we do it like this. Maybe like this instead:\r\n\r\n```c\r\nllama_sampler_init_grammar(...); // same as master\r\nllama_sampler_init_grammar_lazy(...); // same as PR but without `bool lazy`?\r\n```\r\n\r\np.s. huh, naming things is difficult 😄 ",
        "Can this be simply `LLM_KV_MAMBA_RMS_NORM`? If there isn't anything very specific for Falcon H1, it's better to keep the names generic.",
        "Should this be just `ssm_d_ssm`?\r\n\r\nGenerally, I think the `mamba` prefix is not needed here. For example:\r\n\r\n```\r\nmamba_rms_norm -> ssm_rms_norm\r\nmamba_expand -> ssm_expand\r\n```\r\n\r\n@compilade Do you agree?",
        "For consistency, rename the var:\r\n\r\n```suggestion\r\n    std::string api_prefix = \"\";                                                                         // NOLINT\r\n```"
      ],
      "llama.cpp-metal-shared-memory-sizing": [
        "You can always allocate `32*sizeof(float)` bytes of shared memory for simplicity:\r\n\r\n```suggestion\r\n                    const int64_t shmem_size = 32;\r\n```\r\n\r\nNote that shared memory buffers in metal require to have size multiple of 16:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/e2b7621e7c265a6739225125cf9c534f471b3472/ggml/src/ggml-metal/ggml-metal.m#L4912-L4916\r\n\r\nBy forcing `shmem_size = 32;` you handle this requirement."
      ],
      "llama.cpp-explicit-control-flow-logic": [
        "> For example, a sequence set containing multiple seq_ids cannot be mixed with one having a seq_id in the multi-sequence set.\r\n\r\nYes, this logic here at the beginning of the function determines the unique non-overlapping sequence sets that will be contained in this ubatch:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/034b0557987c31d624854e923b505d4541399c7b/src/llama-batch.cpp#L421-L446"
      ],
      "llama.cpp-ai-parameter-organization": [
        "This is already available from the `hparams` - no need to duplicate it here.",
        "`max_length` should be removed and the existing `n_ubatch` parameter should be used instead.",
        "Normally, we don't put the `vocab_size` as `hparam`. Instead, we pick it from the `llama_vocab`. So this is likely not needed.",
        "This parameter should can be avoided.\r\n\r\nSee the logic here:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/a3403ae3502b5f4d562b727bca8c810a3b212198/src/llama-model.cpp#L529-L538\r\n\r\nAnd as an example how we apply it for the Gemma model which can have a custom attention head size like in your case:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/a3403ae3502b5f4d562b727bca8c810a3b212198/src/llama-model.cpp#L1021-L1025"
      ],
      "llama.cpp-api-minimalism-principle": [
        "What is the reason to implement this inside the `llama_context` as opposed to in the diffusion example itself? If we do so, the user would have much more control over the inference and we won't have to design around this new API.",
        "It should be moved to the example. The `libllama` API should be as minimal as possible without redundant interfaces. Wrappers are implemented in user code (e.g. `libcommon`/`examples`/`tools`).",
        "If this is the only use case of `llama_sampler_accept_str()` maybe we can simply extend `llama_sampler_init_grammar()` to accept an initial string (i.e. the trigger) and avoid extending the API.",
        "> Oh, you mean to move the trigger logic in the grammar itself?\r\n\r\nNo, I had in mind something more like this:\r\n\r\n```c\r\n// llama.h\r\n    LLAMA_API struct llama_sampler * llama_sampler_init_grammar(\r\n            const struct llama_model * model,\r\n                          const char * grammar_str,\r\n                          const char * grammar_root,\r\n                          const char * init_str); // optionally used to feed initial data to the grammar state\r\n\r\n// common/sampling.cpp\r\n    gsmpl->grmr  = llama_sampler_init_grammar(model, gsmpl->params.grammar.c_str(), \"root\", trigger.c_str());\r\n    \r\n    return true;\r\n```\r\n\r\nThe logic for triggering remains in user code."
      ],
      "llama.cpp-maintain-consistent-naming-patterns": [
        "Using `shortconv` prefix would be more consistent with the existing naming pattern:\n\n```suggestion\n    MODEL_TENSOR.SHORTCONV_CONV:            \"shortconv.{bid}.shortconv.conv\",\n    MODEL_TENSOR.SHORTCONV_INPROJ:          \"shortconv.{bid}.shortconv.in_proj\",\n    MODEL_TENSOR.SHORTCONV_OUTPROJ:         \"shortconv.{bid}.shortconv.out_proj\",\n```"
      ],
      "llama.cpp-choose-appropriate-error-mechanism": [
        "I don't understand the change - if `repack()` returns -1, we will immediately assert on line 405:\r\n\r\n```c++\r\n    auto OK            = tensor_traits->repack(tensor, data, size);\r\n\r\n    GGML_ASSERT(OK == 0);\r\n```",
        "In the compute calls, it's better to keep the `GGML_ASSERT`s. These would never get called if `repack()` has already bailed."
      ],
      "llama.cpp-measure-before-optimizing": [
        "I think it's OK to enable by default. It requires the `LLAMA_SET_ROWS` anyway for the majority of models (except cacheless embedding models such as BERT), so it will take some time before it gets actually enabled. Should be enough to spot any potential problems until then.",
        "Yes, the time for reset is microscopic. I will simplify as suggested.",
        "It's done once per completion request, at the beginning, upon processing the input json parameters.",
        "I fixed this anyway: https://github.com/ggml-org/llama.cpp/pull/14721"
      ],
      "llama.cpp-validate-bounds-before-access": [
        "I used the `.at()` method: [11ee725](https://github.com/ggml-org/llama.cpp/pull/14363/commits/11ee725a373f8a3ec8f9c8bd94cdd99e72fcd501)",
        "I missed that. Fixing.",
        "This is slightly more future-proof version:\r\n\r\n```suggestion\r\n            GGML_ASSERT(src1->type == GGML_TYPE_I32);\r\n            int64_t row_idx = ((const int32_t *)src1->data)[i];\r\n            GGML_ASSERT(row_idx >= 0 && row_idx < src0->ne[1]);\r\n```\r\n\r\nAt some point in the future we might consider changing the indices of `ggml_get_rows()` to become I64 so this assert will be helpful."
      ],
      "llama.cpp-maintain-naming-consistency": [
        "```suggestion\r\n        params.send_progress = json_value(data, \"send_progress\", false);\r\n```",
        "For consistency, all of these should either be byte offsets or element offsets, but not both.",
        "For consistency with the other arch names, use a dash instead of underscore:\r\n\r\n```suggestion\r\n    { LLM_ARCH_FALCON_H1,        \"falcon-h1\"        },\r\n```"
      ],
      "llama.cpp-enable-callback-chaining": [
        "I wonder if instead of extending the `llama_sampler` API, it would be better to pass necessary callbacks (such as `is_empty`, `accept_str`, etc.) through the `llama_sampler_init_grammar()` call."
      ]
    },
    "profile": {
      "location": "Sofia, Bulgaria",
      "company": "@ggml-org ",
      "blog": "https://ggerganov.com",
      "twitter_username": "ggerganov",
      "site_admin": false,
      "followers": 17691,
      "following": 13
    }
  },
  "spytheman": {
    "repos": [
      "vlang/v"
    ],
    "entries": [
      {
        "slug": "v-add-comprehensive-test-coverage",
        "title": "Add comprehensive test coverage"
      },
      {
        "slug": "v-avoid-breaking-api-changes",
        "title": "Avoid breaking API changes"
      },
      {
        "slug": "v-avoid-expensive-repeated-operations",
        "title": "avoid expensive repeated operations"
      },
      {
        "slug": "v-choose-efficient-data-structures",
        "title": "Choose efficient data structures"
      },
      {
        "slug": "v-clear-technical-writing",
        "title": "Clear technical writing"
      },
      {
        "slug": "v-comprehensive-validated-examples",
        "title": "comprehensive validated examples"
      },
      {
        "slug": "v-configure-socket-blocking-behavior",
        "title": "Configure socket blocking behavior"
      },
      {
        "slug": "v-document-algorithm-behavior",
        "title": "Document algorithm behavior"
      },
      {
        "slug": "v-document-cryptographic-requirements",
        "title": "Document cryptographic requirements"
      },
      {
        "slug": "v-explicit-cryptographic-parameters",
        "title": "explicit cryptographic parameters"
      },
      {
        "slug": "v-explicit-null-checks",
        "title": "explicit null checks"
      },
      {
        "slug": "v-function-documentation-standards",
        "title": "function documentation standards"
      },
      {
        "slug": "v-improve-code-clarity",
        "title": "Improve code clarity"
      },
      {
        "slug": "v-informative-error-messages",
        "title": "informative error messages"
      },
      {
        "slug": "v-runtime-configurable-defaults",
        "title": "Runtime configurable defaults"
      },
      {
        "slug": "v-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "v-use-prod-for-performance",
        "title": "use `-prod` for performance"
      }
    ],
    "comments": {
      "v-use-descriptive-names": [
        "```suggestion\r\n// Note: implementation available only on macOS, Linux and Windows. Otherwise,\r\n```",
        "The name of the exact kernel is only slightly related; the popular name for the OS/platform is more important for the callers imho.",
        "```suggestion\r\n\tstr_dollar_needs_rcbr     []bool = []\r\n```",
        "`is_` is usually used as a prefix for bools, not stacks of them, in the V codebase, and the expectation is that you can `if x.is_something {` later",
        "Renaming `file` to `file_content` is good, but `path` to `dir`, given the usage is not, since write_file expects a path or file name, not a dir in its first argument.\r\n",
        "In general, it is better to have different names than a pair of `file`/`path` used as parameters, since that can be a bit confusing, because both can mean the same or not, depending on the context.\r\n\r\nI suggest using `folder` or `dir` here, instead of `path` or use `filename` instead of `file`.",
        "Why `gen_str`?\r\nIt is a public method. I think that a longer, and more descriptive name like `generate_patch` may be better.",
        "Why is this called `icon`, but the config name is `window_icon`?\r\nImho if you make both `icon`, it will be less confusing for people, similar to how the other properties are named the same.",
        "it should return `int`, not bool",
        "The signature is ignored by the compiler currently, but is used by tooling to show usage tips and to infer the result type of the method, so it should be correct.",
        "imho `mark_var_as_used` was a clearer name",
        "That is a bit misleading, since `u8` is also a type name, and saves only 2 characters. Please use the full module name `utf8` below, and remove the module alias.",
        "It would be clearer if `s` is the actual string literal, i.e. rename `l` to `s` or to `lit`.\r\nWhat is now `s` could be `sptr` for example.",
        "Use names in English for the fields. `name` for example is perfectly fine.\r\n\r\nThe code base would become an unmaintainable mess, if everyone starts using their local names/terms :-| (in the code).",
        "the name is `get_enum_type_idx`, it should return int; if you make it return ast.Type, change the name too",
        "change the name to `typ_idx`, it is misleading otherwise",
        "or just `idx`",
        "There are no maps anywhere: the input is an array, the output is an array, the body of the function also does not use maps.\r\n\r\nA better name could be `flatten_array`?"
      ],
      "v-document-algorithm-behavior": [
        "```suggestion\r\n\t// the two shared secrets (derived by Alice, and derived by Bob), should be the same\r\n```",
        "How big (in bytes) are the derived secrets?",
        "Can we print both in the example?",
        "```suggestion\r\n    // If you wish to generate another curve, use: `pbkey, pvkey := ecdsa.generate_key(nid: .secp521r1)!`\r\n```"
      ],
      "v-choose-efficient-data-structures": [
        "there is no need for this, if you are only going to use the push/pop operations for the Stack - the builtin arrays are enough",
        "is not it a bit better if the integer comparison is before the more complex string one, due to the short-circuiting behavior of boolean expressions? ",
        "```suggestion\r\n\tmain_set map[T]bool\r\n```\r\n\r\nFor most types, like strings or pointers etc, sizeof(bool) < sizeof(T), and the map here is only used for its keys afaik.",
        "```suggestion\r\n\treturn g.main_set.keys()\r\n```",
        "imho do the intermediate calculations with i64, and only do one check at the end of the loop, right before the return of the result",
        "I am concerned about the performance for the most common case, which is currently 64 bit machines.",
        "imho use i64 (or just one power of 2 higher size if you prefer), for the cases of numbers that are < 64 bit. See also `vlib/strconv/utilities.v` and `vlib/strconv/structs.v`, for the already defined operations on `Uint128` for the 64 bit case.",
        "soft 64bit arithmetic on 32 bit machines, is still preferable in terms of performance than checking for overflows on each operation in the loop, generating errors, and then handling them in the caller (in the loop)",
        "That can get expensive for lots of files.\r\nUsing a map will scale better.",
        "Or you can just use `arrays.distinct()`.",
        "Why put `[]Any` first?\r\n\r\nThe first type is going to be used for default initialization of Any values, and []Any is a container, that is relatively more expensive to create/initialize compared to say int or bool . "
      ],
      "v-runtime-configurable-defaults": [
        "Use instead:\r\n`const indexexpr_cutoff := os.getenv_opt('VET_INDEXEXPR_CUTOFF') or { '10' }.int()`\r\n\r\nThe reason that os.getenv is better, is that with $d() those will be fixed once `v vet` is compiled, while with getenv_opt, they can be changed more conveniently at startup/runtime in most shells, with just: `VET_INDEXEXPR_CUTOFF=99 v vet .` for example.",
        "These should be customizable.\r\nAn easy way for that is with something like:\r\n`const indexexpr_cutoff = os.getenv_opt('VVET_INDEXEXPR', '10').int()`\r\nThe negative is that setting env variables on windows in cmd for one off commands is clumsier.",
        "```suggestion\r\nconst buff_size = int($d('gg_text_buff_size', 2048))\r\n```",
        "I am not sure if 2048 is a good default. Running `examples/gg/minimal.v`, it seems to result in apps that use ~8MB more RAM by default on my machine, compared to the same but with 512 (on master):\r\n```\r\ndelian     14166  3.5  0.2 141112 40524 pts/0    Sl+  14:24   0:00          |       |       \\_ /home/delian/code/v/rrrrrrrrrrrrrr\r\n```\r\nvs\r\n```\r\ndelian     14082  3.6  0.3 166648 47672 pts/0    Sl+  14:23   0:00          |       |       \\_ /home/delian/code/v/rrrrrrrrrrrrrr\r\n```\r\n\r\n\r\n",
        "On the other hand, it is certainly better in the sense that apps will not have weird artifacts and disappearing texts in much more cases, and with $d(), now people can tweak it easily, if they need something smaller or bigger.",
        "There is no need for that CLI parsing complication. You can pass the non default difftool through an environment variable, say `VDIFF_TOOL`, and it would be propagated to all the code that needs it."
      ],
      "v-add-comprehensive-test-coverage": [
        "Please add new test functions, instead of modifying the existing ones, unless there is a bug. That makes reviewing a lot easier.",
        "Please, also add another test_ function, that sets the mutable parameter to a value != none, and asserts that after calling the function too.",
        "Please add assertions too.",
        "No. Keep the existing test, and add your own if you want.\r\nReplacing things like this is bad - it removes coverage for the existing functionality of `json.raw_decode`.\r\nIt is also very sloppy - the test name is still `fn test_raw_decode_string() {` ...\r\n\r\nJust keeps things working as much as possible,\r\nand add *new tests* please.",
        "Same - keep the current code, and add *new* tests/asserts.",
        "If it acts the same, even better - decode 2 times, once the old way, and once the new way, and compare the results.",
        "this lacks tests",
        "public functions should preferably have their own tests",
        "Please also add a case for comparing `''` and `''` and also for `''` vs non empty string, and the reverse.",
        "same, please also add assertions for the cases with empty strings",
        "0 is not a good value for testing, because the default initialisation also uses 0s.\r\nUsing another value like 1, 3, or 42 makes the test more sensitive/less likely to continue to work, if eventually a value was overwritten (wrongly) by 0s in the implementation.",
        "Please keep the old _test.v file, and add a new one.\r\n\r\nRefactoring the old one (not just adding new test cases) has a non-trivial chance of masking/changing what it tested before.",
        "Please put some assertions against the contents of `g.str()` and `t.str()` too.",
        "(something like `assert g.str().contains('a: 100')` etc)"
      ],
      "v-avoid-expensive-repeated-operations": [
        "hm, that is weird ... json.encode() is used here, but in the generated code, the error is inside `json__decode__option_int` which is not used by json.encode, but by json.decode, which is not used by the main program.",
        "To me, it seems like the fix is in the wrong place - it will ensure that the `builtin` `_option_none` is always available to cgen, no matter if it is used or not.",
        "Yes, we do the same for `_option_ok` and `_option_clone` too, and they are not used by most short programs either.\r\n\r\nI think, that we should work on improving markused, so that the use of options is tracked, and add those only in cases, when they are used as well.",
        "i.e. I can merge this as it is, at the cost of a small increase in cgen output for small programs, but if we tracked the use of options, we could avoid it.",
        "put that condition first, since it is cheaper to check, compared to the string comparisons and the function calls",
        "imho, cache `c.g.fn_addr.keys()` in `c`, since calling .keys() for each call can become expensive for longer programs.",
        "If these are not modified, they can be extracted as constants too.\r\nIt will avoid allocating them for each `block()` call.",
        "```suggestion\r\n\tgroup := gname[..gname_len].bytestr()\r\n```\r\n`.bytestr()` creates a copy too, i.e. the clone() is superfluous",
        "`expr.expr.str()` is called several times in the code, it may be beneficial to store it in a local variable",
        "That seems very weird - you have already written the `{`, and then you are cutting it back.\r\n\r\nWhy not just generate different code depending on the flag, instead of doing that maneuver? ",
        "You are doing `sb.str()`, which empties `sb`, followed by `sb.writeln(fn_header)` which fills it back up.\r\nIt is needless copying.",
        "At the very least, you can call `sb.bytestr()` to get a string, without emptying it :-( .",
        "And you can do that before line 4415, so that you would not have to do the .all_before() etc.",
        "`strings.Builder` is an alias to `[]u8`, and has all the methods that are defined on `[]u8` too.",
        "why 50_000 ?",
        "I was just curious, because of the round number. You are right that it can be adjusted later if needed to avoid reallocations for the common cases.",
        "imho cache the information in another field in `ast.Struct`, about whether there are any option fields in a struct, during parsing, so that it is done just once, and then just use it here, instead of doing the loop for `any` potentially many times for each expression.",
        "```suggestion\r\n\tmut sb := strings.new_builder(s.len)\r\n```",
        "Each \\n is replacing a space (or several) -> s.len will be probably very close to the final result length.",
        "Can this check be done only inside __global declarations ?",
        "In this case, please add test cases for some of the other usages.",
        "What I do not like about this, is that it does relatively heavy operations for each cast, while the error and the results of those operations will be used only in a very limited amount of cases.\r\n\r\nAt the very least, the `c.expected_type.has_flag(.generic)` check should be done first, since it is fast.\r\n",
        "You could probably also use `to_sym.name` directly, not calling `c.table.type_to_str` just to check the first non `&` character in the result.",
        "After the .type_to_str() call, you are calling `.replace('&', '')`, giving you the same as `to_sym.name` afaik.",
        "A special case like this here:\r\n`if name == 'UTF-8' { return 65001 }` \r\nput before anything else, would be beneficial since half of the checks will be for it."
      ],
      "v-function-documentation-standards": [
        "The new public function should be documented with a comment, that `v doc` can understand.\r\nUse something like this above the header:\r\n```v\r\n// get_queryset returns ...\r\n```",
        "It is only important for `v doc`, that it starts with the name of the method, the explanation after that can be whatever makes sense.",
        "```suggestion\r\n// gen_print_reg writes a string of size n stored in r to fd\r\n```",
        "I agree.",
        "@kimshrier what do you think?",
        "That is understandable, please excuse me for adding to your cognitive load.\r\n\r\nI think we can delete that comment for now, as @blackshirt suggested, and revise it later if needed.",
        "What is index in this context, a line, a byte offset from the start, or a utf character offset from the start?\r\n\r\nImho, rewrite the comment to be more informative, instead of just expanding on the name of the field.",
        "It is better to not have any comments, than ones that just repeat the information from the names."
      ],
      "v-improve-code-clarity": [
        "Those methods have no utility at all, since they just delegate to methods of the underlying field, that could be just accessed directly, if it is public.\r\n\r\nThe code of the implementation also seems extremely verbose for what it does.\r\n```v\r\n\tif s.status.is_valid() {\r\n\t\treturn true\r\n\t} else {\r\n\t\treturn false\r\n\t}\r\n}\r\n```\r\n^ that could be just: `return s.status.is_valid()` ...\r\n\r\n",
        "why is this defining a closure, just to call it right away?\r\nimho just inline the code",
        "imho there is no need for the second argument, since it essentially duplicates what `enable_echo`, but for each key check.\r\n\r\nThat `state.c_lflag &= ~C.ICANON` can be moved inside enable_echo(false) too.",
        "I also think that it will be clearer and easier for maintenance, if we have 2 functions:\r\n`pub fn key_pressed() int {`\r\nand\r\n`pub fn key_pressed_blocking() int {`\r\ninstead of a single one, with a bool argument to choose it - the places where you want each variant, are imho different, and having a mandatory boolean argument for each callsite does not seem very ergonomic to me.",
        "This is computed over 150 lines above the place where it is used.\r\nImho move it below, near the if condition that uses it.",
        "Is there a reason why it should miss an argname ?\r\nIn all existing cases, it produces one, and imho that makes reading the generated code and the potential errors that C compilers produce, much easier.\r\n\r\nImho just always write the name.",
        "It will also eliminate the bool argument to write_fn_ptr_decl , which is true in all currently existing cases, and simplify the PR diff.",
        "`@FN` is already a string, there is no need to interpolate it."
      ],
      "v-explicit-cryptographic-parameters": [
        "afaik, `hash_config` already has a default of `.with_recommended_hash`.\r\nWhy not just skip it?",
        "i.e.:\r\n```suggestion\r\n\tsignature := pvkey.sign(message_tobe_signed)!\r\n```"
      ],
      "v-informative-error-messages": [
        "what would you want them to be?",
        "That seems a bit unclear 🤔 \r\nCan the error also show the type that the compiler thinks `v` is too?\r\nSomething like this can be less puzzling:\r\n```\r\nvlib/v/checker/tests/match_generic_case_err.vv:16:4: error: return type mismatch, it should be `int`, but it is instead `string`\r\n```",
        "(not in the same PR, but in general)",
        "especially for generic code, I think it will help a lot",
        "Nice. If you keep information about the position of the opening brace, it can be included too.",
        "imho provide information in the error, that `]` was expected too",
        "adding the actual number, that was checked so far, in the error message, would be informative"
      ],
      "v-configure-socket-blocking-behavior": [
        "```suggestion\r\n\t\t$if !net_nonblocking_sockets ? {\r\n\t\t\tconn.set_blocking(true)!\r\n\t\t}\r\n```",
        "`-d net_nonblocking_sockets` will not be defined by default, but will provide a way for people to opt in for the old behavior for their old projects, if needed.",
        "```suggestion\r\n\t\t$if !net_nonblocking_sockets ? {\r\n\t\t\tconn.set_blocking(true)!\r\n\t\t}\r\n```",
        "```suggestion\r\n\t$if !net_nonblocking_sockets ? {\r\n\t\tres.set_blocking(true)!\r\n\t}\r\n```",
        "No, it should be `res.set_blocking(true)!` , the PR is changing the default.",
        "Note that there is inversion, it is `$if !net_nonblocking_sockets ? {`, not `$if net_nonblocking_sockets ? {` .\r\nIn other words, that code inside the comptime if body, will be executed only if `-d net_nonblocking_sockets` *is not* passed, i.e. by default, which is what we want."
      ],
      "v-clear-technical-writing": [
        "```suggestion\r\nModules names in .v files, must match the name of their directory.\r\n```",
        "```suggestion\r\nbelong to the same module `abc`. They should also start with `module abc`.\r\n```",
        "```suggestion\r\nSo in `abc/def/source.v` the first line will be `module def`, and not `module abc.def`.\r\n```",
        "```suggestion\r\n`abc.def`\r\n```",
        "```suggestion\r\nRefering to a module symbol such as a function or const, only needs module name as prefix:\r\n```",
        "```suggestion\r\nA function, located in `abc/def/source.v`, is called with `def.func()`, not `abc.def.func()`\r\n```",
        "```suggestion\r\nA .v file `./abc/source.v` must start with `module abc`. All .v files in this directory \r\n```"
      ],
      "v-avoid-breaking-api-changes": [
        "That is a breaking change. Is it not needed anymore?\r\nThe commented example above suggests otherwise.\r\nCan you extract the example as a separate _test.v file, that does not start with `module ecdsa`, and thus can only access its public API?",
        "It can be added in example/ecdsa_seed_test.v, but then it will be less clear about what is it demonstrating, and linking to it specifically from the comments here will be a bit harder. That said, if you prefer it that way - sure.",
        "This is a breaking change too - `new_xof_digest` was public, and  is now both changed in name, and private.",
        "Imho keep the new private function, but also add back the public one as it was + its comments, just calling the private one."
      ],
      "v-use-prod-for-performance": [
        "`-prod` should turn on `-O3 -flto`, so perhaps just this:\r\n```suggestion\r\n`v -cc gcc -prod -cflags \"-std=c17 -march=native -mtune=native\" .`\r\n```",
        "was there a problem without `-fno-inline-small-functions`?",
        "hm, that is good to know ... so the problem is triggered by something in it + gcc's -O3 🤔 "
      ],
      "v-explicit-null-checks": [
        "Please assert that at least green_arena.data is not nil, and that green_arena.head is not nil .",
        "I think that it needs `defer { C.XCloseDisplay(display) }` too.",
        "`XOpenDisplay` is also documented to return null on failure, that should be checked too",
        "Wow ... that is a good find ... it may be better to turn `fn_decl` into `?&ast.FnDecl` to force the compiler into checking all places, where it is used unguarded",
        "although that may have a non trivial performance impact",
        "set current.next to nil right after this line",
        "You are doing `free(current)` on the next line. It will not affect the loop."
      ],
      "v-comprehensive-validated-examples": [
        "this is now out of order, and if you remove the `oksyntax` tag, `v check-md` will fail (or if the user copy/pastes it in the playground/editor).",
        "One of the major benefits of self sufficient examples, is that with them you get this:\r\n![image](https://github.com/user-attachments/assets/5607c538-074b-4c56-899c-594d0319e5d7)\r\non https://docs.vlang.io/hello-world.html .",
        "And then, it is trivial to play with them, which makes learning V easier for newbies.",
        "These were more informative compared to a whole example for just `.try_pop()`, with a comment that duplicates the code ...\r\nRemove the new example, and restore the old code.",
        "No, it is not the same information at all. The code sample before showed several channel features, while later in your example, shows just this, that is related to the channel use:\r\n```v\r\nres := ch.try_pop(mut b) // try to perform `b = <-ch`\r\n```\r\n\r\nDo delete the new code, and restore the old in this particular case, I do not want to argue about it.",
        "The other examples are mostly fine, but this one loses information, that is important for understanding how to use channels.",
        "Instead of `codeblock`, use `v` here.\r\nThe benefit is that the tool `v check-md .` will then check the example on the CI,\r\nand that way it will be kept valid, and also consistent with the rest of the V examples in documentation (formatted, vetted, etc)."
      ],
      "v-document-cryptographic-requirements": [
        "```suggestion\r\n// You should make sure, the seed bytes come from a cryptographically secure random generator,\r\n```"
      ]
    },
    "profile": {
      "location": "Sofia, Bulgaria",
      "company": "DIA Soft",
      "blog": "http://blog.bulsynt.org",
      "site_admin": false,
      "followers": 334,
      "following": 325
    }
  },
  "ndeloof": {
    "repos": [
      "docker/compose"
    ],
    "entries": [
      {
        "slug": "compose-ci-security-boundaries",
        "title": "CI security boundaries"
      },
      {
        "slug": "compose-environment-variable-validation",
        "title": "Environment variable validation"
      },
      {
        "slug": "compose-evaluate-dependency-api-compatibility",
        "title": "evaluate dependency API compatibility"
      },
      {
        "slug": "compose-follow-existing-naming-patterns",
        "title": "Follow existing naming patterns"
      },
      {
        "slug": "compose-keep-code-structure-flat",
        "title": "Keep code structure flat"
      },
      {
        "slug": "compose-maintain-documentation-consistency",
        "title": "Maintain documentation consistency"
      },
      {
        "slug": "compose-network-api-precision",
        "title": "Network API precision"
      },
      {
        "slug": "compose-prefer-explicit-readability",
        "title": "prefer explicit readability"
      },
      {
        "slug": "compose-prevent-sensitive-data-exposure",
        "title": "Prevent sensitive data exposure"
      },
      {
        "slug": "compose-prevent-unintended-ci-behaviors",
        "title": "Prevent unintended CI behaviors"
      },
      {
        "slug": "compose-safe-collection-modification",
        "title": "Safe collection modification"
      },
      {
        "slug": "compose-schema-changes-upstream-first",
        "title": "Schema changes upstream first"
      },
      {
        "slug": "compose-scope-concurrency-control-precisely",
        "title": "Scope concurrency control precisely"
      },
      {
        "slug": "compose-use-api-options-pattern",
        "title": "Use API options pattern"
      },
      {
        "slug": "compose-use-structured-logging-framework",
        "title": "Use structured logging framework"
      },
      {
        "slug": "compose-wrap-and-check-errors",
        "title": "Wrap and check errors"
      },
      {
        "slug": "compose-write-deterministic-test-assertions",
        "title": "Write deterministic test assertions"
      }
    ],
    "comments": {
      "compose-keep-code-structure-flat": [
        "nested `if`s make this a bit hard to read, maybe just split this into simpler top-level `switch` statement?\r\n\r\n```go\r\nswitch {\r\n\tcase direction == fromService && index == 0: \r\n                containers = ...\r\n\tcase index != 0:\r\n                containers = ...\r\n        default:\r\n                containers = ...\r\n\t}\r\n``` \r\n\r\n",
        "`direction = from & index = 0` has been addressed by 1st case, so here we have `(from & index > 0) || (to & index > 0)` which can be simplified as `index > 0`"
      ],
      "compose-follow-existing-naming-patterns": [
        "I'd suggest to use `-y` to align with https://github.com/docker/compose/blob/main/cmd/compose/create.go#L84",
        "As this also applies to Darwin, maybe rename `_posix` :trollface: ",
        "nit: AFAICT we don't `parse` things here, so better name this `toBuildContexts`",
        "this produces the `options` for the `ImageBuild` engine API, so ...",
        "sounds weird to me we use a distinct separator for fields, I'd prefer we use `:` everywhere\r\nThis allows an earlier version of compose to successfully parse the label, and ignore the last `:xxxx` parts (so we can add more in the future)",
        "Don't need to be exporte. Also name is bit confusing. Maybe `withSelectedServicesOnly` or something comparable?",
        "This is basically a duplicate for `projectFromLabels`, better use/adapt this existing function",
        "yes, move it to `compose.go`. Also, probably should be renamed `projectFromActualResources` or something comparable to be more explicit"
      ],
      "compose-wrap-and-check-errors": [
        "use `api.IsNotFoundError(err)`"
      ],
      "compose-write-deterministic-test-assertions": [
        "This version string is very specific to Docker Desktop and will be outdated once we release v2.34, I fear this will bring some confusion. Better use a pure fake version for testing purpose: `v9.9.9-test` for example"
      ],
      "compose-environment-variable-validation": [
        "a better place to manage this is `addProjectFlags` in cmd/compose.go, as you can set flag default value from env value (the same way we do for `--env-file`)",
        "we never used this mechanism in the past for optional/experimental docker compose features",
        "this is the way we used to load PWD/.env before we know the actual project directory. This logic is now covered by toProjectOptions"
      ],
      "compose-prefer-explicit-readability": [
        "took me time indeed to understand this syntax used in original code."
      ],
      "compose-maintain-documentation-consistency": [
        "rest of the documentation uses uppercase \"V1\" and \"V2\"\r\nplease place the compose-switch note as a separate paragraph to make it bit more visible to readers"
      ],
      "compose-safe-collection-modification": [
        "> in general modifying a map during it's iteration is bad practice and can produce unpredictable results\r\n\r\nany reference ?\r\nI was looking for official reference regarding this and only found https://github.com/golang/go/issues/9926",
        "I'm not sure I remember the detail, but deletion here is expected afaict.\r\nMaybe better create a new `Volumes` before the loop, copy those that need to be kept while iterating, then override `s.Volumes`"
      ],
      "compose-prevent-sensitive-data-exposure": [
        "service.environment may be set with a fixed value, not relying on any interpolation. Typically:\r\n```\r\ndb:\r\n    image: mysql\r\n    environment:\r\n      MYSQL_DATABASE: avatar\r\n      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db-password\r\n```\r\nthose should not prevent compose file to be published, right ?",
        "```suggestion\r\n\t\tfmt.Println(\"you are about to publish bind mounts declaration within your OCI artifact.\\n\" +\r\n\t\t\t\"only the bind mount declarations will be added to the OCI artifact (not content)\\n\" +\r\n\t\t\t\"please double check that you are not mounting potential user's sensitive directories or data\")\r\n```",
        "you MUST check each individual file in the compose project, not just the final model. Otherwise I _may_ publish:\r\ncompose.yaml\r\n```yaml\r\nservices:\r\n test\r\n   some: MY_SECRET_PASSWORD\r\n```\r\n\r\ncompose.yaml\r\n```yaml\r\nservices:\r\n test\r\n   some: !override ${ENTER_YOUR_OWN_SECRET}\r\n```\r\n\r\nPublisher expectation would be that secret is not exposed to consumer, but actually it is"
      ],
      "compose-schema-changes-upstream-first": [
        "compose-spec.json",
        "in an ideal world, this whole file should be removed from docker/compose repo and replaced by a (build-time?) reference to https://github.com/compose-spec/compose-spec/blob/master/schema/config_schema.json",
        "did you checked if the current status of the compose-spec schema already includes all those changes or if there's anything missing ?"
      ],
      "compose-scope-concurrency-control-precisely": [
        "We don't want providers to run sequentially, only the project mutation must be guarded by a mutex.",
        "could use an errorGroup (like we do in other places in compose) to capture first error and stop other goroutines by cancelable context.",
        "this isn't thread safe, would require a mutex (this is why we used a channel here)"
      ],
      "compose-prevent-unintended-ci-behaviors": [
        "we don't exec `docker buildx ...` for a standard build, but use the vendored buildx client, why not do the same here with `BuildOptions.PrintFunc`?"
      ],
      "compose-network-api-precision": [
        "This hack is also supported for long by compose (https://github.com/docker/compose/blob/v1/compose/config/config.py#L494-L498) and relies on `o=bind` and `device` option being set\r\n"
      ],
      "compose-evaluate-dependency-api-compatibility": [
        "watcher_naive relies on `SetRecursive` which doesn't exist in upstream"
      ],
      "compose-use-structured-logging-framework": [
        "should be Errorf",
        "we've been using `logrus.Warning` in other places, we should do the same here"
      ],
      "compose-ci-security-boundaries": [
        "Please don't set any kind of \"auto-fix\". CI role is to check everything is OK in contributor's commit, not to replace contributor doing things right."
      ],
      "compose-use-api-options-pattern": [
        "Repository being mandatory parameter should not be part of the `Options` struct imho",
        "This is API version 1.44, but Moby v25 (typo here)\r\nMakes me wonder moby repo could offer a map for engine -> latest API version so we don't make such mistakes :)",
        "https://github.com/docker/compose/pull/11360",
        "could make it simpler by making API client an attribute in `composeService`, initialize with `dockerCli.Client()` and override here with `NewDryRunClient`.",
        "As this is not a generic Client proxy, but dedicated to dry-run usage, we could make it simpler and just have all API methods to directly invoke `client.XX` from dockerCli's client _BUT_ the few ones where we want to bypass actual actions on docker engine.\r\nOR rename this into generic `ClientProxy`, then we can configure dry-run by setting individual func to be overriden"
      ]
    },
    "profile": {
      "location": "Rennes, France",
      "company": "Docker",
      "blog": "http://blog.loof.fr",
      "twitter_username": "ndeloof",
      "site_admin": false,
      "followers": 443,
      "following": 0
    }
  },
  "PeterSchafer": {
    "repos": [
      "snyk/cli"
    ],
    "entries": [
      {
        "slug": "cli-api-interface-design",
        "title": "API interface design"
      },
      {
        "slug": "cli-balance-concurrent-operations",
        "title": "Balance concurrent operations"
      },
      {
        "slug": "cli-comprehensive-test-coverage",
        "title": "comprehensive test coverage"
      },
      {
        "slug": "cli-configuration-naming-consistency",
        "title": "Configuration naming consistency"
      },
      {
        "slug": "cli-defensive-shell-script-configuration",
        "title": "defensive shell script configuration"
      },
      {
        "slug": "cli-document-intent-and-reasoning",
        "title": "Document intent and reasoning"
      },
      {
        "slug": "cli-graceful-error-handling",
        "title": "Graceful error handling"
      },
      {
        "slug": "cli-handle-all-errors-explicitly",
        "title": "Handle all errors explicitly"
      },
      {
        "slug": "cli-maintain-build-environment-parity",
        "title": "maintain build environment parity"
      },
      {
        "slug": "cli-maintain-cicd-boundaries",
        "title": "maintain CI/CD boundaries"
      },
      {
        "slug": "cli-optimize-ci-resource-allocation",
        "title": "optimize CI resource allocation"
      },
      {
        "slug": "cli-optimize-variable-declarations",
        "title": "Optimize variable declarations"
      },
      {
        "slug": "cli-pin-dependency-versions",
        "title": "Pin dependency versions"
      },
      {
        "slug": "cli-prevent-silent-test-failures",
        "title": "prevent silent test failures"
      },
      {
        "slug": "cli-separate-build-from-runtime",
        "title": "separate build from runtime"
      },
      {
        "slug": "cli-synchronize-configuration-values",
        "title": "synchronize configuration values"
      },
      {
        "slug": "cli-use-centralized-configuration-access",
        "title": "Use centralized configuration access"
      },
      {
        "slug": "cli-use-centralized-loggers",
        "title": "Use centralized loggers"
      },
      {
        "slug": "cli-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "cli-use-descriptive-parameter-names",
        "title": "Use descriptive parameter names"
      },
      {
        "slug": "cli-use-file-locks",
        "title": "Use file locks"
      },
      {
        "slug": "cli-use-optional-chaining",
        "title": "Use optional chaining"
      },
      {
        "slug": "cli-use-secure-hash-functions",
        "title": "Use secure hash functions"
      },
      {
        "slug": "cli-validate-environment-variables-early",
        "title": "validate environment variables early"
      },
      {
        "slug": "cli-validate-security-configurations",
        "title": "Validate security configurations"
      },
      {
        "slug": "cli-write-actionable-documentation",
        "title": "Write actionable documentation"
      }
    ],
    "comments": {
      "cli-use-descriptive-parameter-names": [
        "Question: win-something? What is it?",
        "Oh got it, it is just to trigger another case. This is a bit confusing. Can we please at least have a comment what we do here?",
        "Actually it might be better to rename the parameter from executor to something like OS, since executor is an existing construct that has a special meaning."
      ],
      "cli-defensive-shell-script-configuration": [
        "Good idea! Thank you! Please take another look!"
      ],
      "cli-use-centralized-configuration-access": [
        "Issue: with the new gaf version, the temp directory is a configuration value. Instead of using this helper function directly, we should be using the configuration now, otherwise the behaviour wouldn't be consistent.\r\n\r\n```suggestion\r\n\treturn c.globalConfig.GetString(configuration.TEMP_DIR_PATH)\r\n```",
        "Issue: with the new gaf version, the temp directory is a configuration value. Instead of using this helper function directly, we should be using the configuration now, otherwise the behaviour wouldn't be consistent.\r\n\r\n```suggestion\r\n\treturn config.GetString(configuration.TEMP_DIR_PATH)\r\n```",
        "Issue: same as above, please use the configuration value for temp dir.",
        "Suggestion: Please use the configuration to access values, see [here](https://github.com/snyk/go-application-framework/blob/main/pkg/local_workflows/auth_workflow.go#L84). This way, there is no need for the implementation to know where the values come from and how they exactly need to be named.",
        "unfortunately not, please take a closer look at the linked code snippet. "
      ],
      "cli-comprehensive-test-coverage": [
        "Suggestion: There is an opportunity to move this in a small function and write a test for it ;) ",
        "Suggestion: Use `t.Setenv()` instead of `os.Setenv()`",
        "Interesting point, to be honest I'm not completely sure about the root cause of why sleep is necessary. I just had the build failing on a circle runner while working locally and on other runners. \r\nMy assumption is that the final comparison of the modification time might not be the strongest check. Looking at time resolution etc, I was assuming that the sleep helps to ensure that modification times would be definitely different.\r\nLooking at the comment it seems misleading. I'm definitely changing the comment.",
        "done!",
        "Suggestion: extend the test to cover both cases more than 5 elements in the cache and less. maybe add second test case."
      ],
      "cli-api-interface-design": [
        "Suggestion: How about adding the CertificateLocation to ProxyInfo and thereby reducing the interface here.",
        "Without the interface it would be impossible to implement other authentication handlers in the future."
      ],
      "cli-synchronize-configuration-values": [
        "Because the documentation says so.\r\n> Update all NodeJS versions to match the NodeJS versions used in pkg."
      ],
      "cli-validate-security-configurations": [
        "Question: This looks like a change of behaviour to me. Previously if a token was specified sessionToken contained the plain token value, now it contains `token <tokenvalue>`. Is this considered in the plugins that consume the sessionToken value?",
        "Great! Thanks for the detailed explanation! "
      ],
      "cli-optimize-ci-resource-allocation": [
        "Caching only relates to the downloaded installers, they still need to be executed to install the applications. The different versions of the command install different subsets of the tools for efficiency reasons."
      ],
      "cli-balance-concurrent-operations": [
        "Suggestion: Let's be a bit more pessimistic to not increase the load too much and set it to a lower value first. Let's start with 2."
      ],
      "cli-optimize-variable-declarations": [
        "Suggestion: how about removing the port here, since the information is now available via ProxyInfo.",
        "issue: imported but not used"
      ],
      "cli-handle-all-errors-explicitly": [
        "Issue: Mimicking the gaf network stack behaviour, we need to use the error returned from the errorhandler and only if this error is not nil set the terminate header.",
        "Please use fmt.Errorf() the following way, see [the documentation](https://pkg.go.dev/fmt@go1.19.5#Errorf) for the reasoning\r\n`fmt.Errorf(\"Cache directory path is invalid: %w\", err)`",
        "done!",
        "done! used defer"
      ],
      "cli-pin-dependency-versions": [
        "Question: Did you miss to add the changes in the package.json?",
        "Thanks for updating. I do have a follow up question though, how come that the added test didn't fail? I would expect that the test doesn't succeed if the dependency is not updated.",
        "Suggestion: Personally I think that the usage of `^` is not a good practice as it means that we can't reproduce individual builds. Therefore I would like to propose that we use this chance to get rid of them step by step.",
        "withdrawn :) "
      ],
      "cli-validate-environment-variables-early": [
        "Question: What is the developer experience when this Environment Variable is not specified? Is it worth to explicitly warn when it is empty?\r\n\r\nNitpick: Maybe move this to where `LS version:` is printed 😄 ",
        "This is the error I see, right now. \r\n<img width=\"613\" alt=\"image\" src=\"https://github.com/user-attachments/assets/05a0cb4b-3219-487a-8b4a-ba2a93529b31\" />\r\n\r\nCan we make the test a bit more descriptive and actionable? I'm worried about all the asks we will be getting because someone is using a dev CLI without the URL set.\r\n",
        "Maybe we fail the command execution instead?!",
        "yes, it fails, but it seems to do a lot of things until it fails with the message right now. So a proposal could be to do a check for the variable and fail early and actionable. Does this make sense?",
        "for example [here](https://github.com/snyk/cli-extension-iac/blob/0a63172561937695026101838e018d5574618785/internal/commands/iactest/iactest.go#L69)",
        "Question: DEBUG is a very generic environment variable name. There are good chances to have this conflicting somewhere for example in our CI/CD. Should we maybe solve enabling the debug build via an explicit target?",
        "Suggestion: How about using the `.mvnrc` file to determine the major version, instead of having it hardcoded here. Take a look at https://github.com/snyk/cli/blob/master/scripts/create-build-image.sh#L9C3-L9C32 for example"
      ],
      "cli-use-optional-chaining": [
        "As I understand the case, not having vulnerabilities here is an error case. This means if they are missing, the comparison will fail. The intention is to not make this helper function crash. Appropriate error handling must and as far as I see, does happen outside."
      ],
      "cli-use-descriptive-names": [
        "nitpick: Naming variables the same way as data structures can be confusing. Maybe just rename the variables here?"
      ],
      "cli-use-centralized-loggers": [
        "We want/need the information logged! \r\nBut rather than using the debug parameter the logger actually gets disabled centrally.",
        "Suggestion: try to use DebugLogger as zerolog logger, this will be possible every except of two places. For these places you could add an additional property here.",
        "Issue: you need to use the centrally supplied logger! It is not correct to create a new one here.",
        "Suggestion: I would use both logger types from the context instead of creating a ToZeroLogDebug below. The logic is at one place and if we want to remove one day the log.Logger usage, it will be easier to find. ",
        "Issue: this needs to use a log.Logger that uses a writer to zerolog `ToZeroLogDebug`! Otherwise, the log.Default logger doesn't respect the debug configuration and logger configuration that is centrally done.",
        "Issue: this needs to use a log.Logger that uses a writer to zerolog ToZeroLogDebug! Otherwise, the log.Default logger doesn't respect the debug configuration and logger configuration that is centrally done.\r\n\r\nSame as below!",
        "Suggestion: replace fmt.Println() by c.DebugLogger.Println() and add the currentPath"
      ],
      "cli-configuration-naming-consistency": [
        "Question: is this on purpose that the log says `TEST_CONFIG_FILE` and the env var used is `SNYK_CONFIG_FILE`?",
        "Is `TEST_CONFIG_FILE` used somewhere?"
      ],
      "cli-use-file-locks": [
        "This would probably be in addition to restoring the file I assume.",
        "@bastiandoetsch we can use flock but we would need to place the lock file outside of the cache directory, which we are going to create. We can do this, just that we need to consider this in the docs about filesystem access etc.",
        "And yes, I agree, the user shouldn't be bothered at all. Per default the CLI determines the cache directory based on OS defaults, these directories exists and we just place a subfolder inside of it, which we can create relatively safe with mkdir. so per default, the assumption is that this error would never happen.\r\n\r\nIt might happen, if a user specifies a custom path.",
        "Definitely Possible! "
      ],
      "cli-graceful-error-handling": [
        "Suggestion: Let's revert this change here, since it is better to fail than send an empty string.",
        "I removed the duplicated logic.\r\nI realized that `err.jsonPayload` was already there, so I'm not changing this.",
        "Issue: From my understanding I think we need the non-pretty-print part of the original solution as well and only if this fails as well, we should fallback on the new solution. Removing the original code will cause issues for use cases of --json, which worked due to the non-pretty-print part.",
        "The associated unit test needs to be fixed and an additional one needs to be created.",
        "Question: Don't we need to move this out of the else case?"
      ],
      "cli-write-actionable-documentation": [
        "suggestion: replace directives are not necessarily a debugging step, maybe we separate them from the debugging docs and add them into a section about development in golang?"
      ],
      "cli-prevent-silent-test-failures": [
        "Suggestion: asserting against stdout is always a bit tricky as it is not a strong check or actually too strong as it breaks with minor rewrite of a message, which increases the maintenance. There is another way to check, which is using the instrumentation data, which would actually be a more complete check of the scenario.",
        "Issue: This  changes the expectation, `> 0` is not the same as `==2`.  You should be able to filter the requests and leave the expectations unchanged.",
        "Suggestion: This test should ensure that the content is a json data structure, for example by using a library like [this](https://www.npmjs.com/package/jsonparse). It should also ensure that the length of stdoutBuffer is greater 0.",
        "Suggestion: move this assert a bit up before trying to access the output file. Running the test with a non fixed CLI should fail in a planned way and not just because the file doesn't exists.",
        "Question: Shouldn't this be equal instead of contains?"
      ],
      "cli-use-secure-hash-functions": [
        "We can optimize and just do it when `--debug` is used",
        "yes, salt would not be good 😄  the token-hash is being truncated to 16 bytes to avoid any reverse engineering of the actual token, see the lines below."
      ],
      "cli-separate-build-from-runtime": [
        "Suggestion: We should build the dependencies when building the docker images and not in the script that shall just sign."
      ],
      "cli-maintain-cicd-boundaries": [
        "Issue: on main, there shouldn't be any release notes."
      ],
      "cli-maintain-build-environment-parity": [
        "Question: Do we really need a differentiation between build and build-local? I'm worried that local and CI build get out of sync at some point. I would like to see everyone using the same command to build.",
        "Just to expand the thought, I wonder if we can make the usage of the virtual environment invisible to the `make` user.",
        "it is an optimization to not always use clean-install. The pipeline itself does `npm ci` at the beginning, in this stage, we just want to install what is missing if anything."
      ],
      "cli-document-intent-and-reasoning": [
        "Nitpick: a short comment what the new flag does would be helpful for future us."
      ]
    },
    "profile": {
      "location": "Cologne ",
      "blog": "",
      "site_admin": false,
      "followers": 10,
      "following": 0
    }
  },
  "cam72cam": {
    "repos": [
      "opentofu/opentofu"
    ],
    "entries": [
      {
        "slug": "opentofu-clear-concise-documentation",
        "title": "Clear concise documentation"
      },
      {
        "slug": "opentofu-craft-actionable-errors",
        "title": "Craft actionable errors"
      },
      {
        "slug": "opentofu-document-intent-and-limitations",
        "title": "Document intent and limitations"
      },
      {
        "slug": "opentofu-document-reference-standards",
        "title": "Document reference standards"
      },
      {
        "slug": "opentofu-document-with-examples",
        "title": "Document with examples"
      },
      {
        "slug": "opentofu-minimize-api-surface",
        "title": "Minimize API surface"
      },
      {
        "slug": "opentofu-optimize-cicd-workflows",
        "title": "Optimize CI/CD workflows"
      },
      {
        "slug": "opentofu-prevent-backing-array-surprises",
        "title": "Prevent backing array surprises"
      },
      {
        "slug": "opentofu-proper-span-lifecycle",
        "title": "Proper span lifecycle"
      },
      {
        "slug": "opentofu-protect-infrastructure-secrets",
        "title": "Protect infrastructure secrets"
      },
      {
        "slug": "opentofu-provider-instance-management",
        "title": "Provider instance management"
      },
      {
        "slug": "opentofu-reduce-code-nesting",
        "title": "Reduce code nesting"
      },
      {
        "slug": "opentofu-safe-lock-patterns",
        "title": "Safe lock patterns"
      },
      {
        "slug": "opentofu-separate-configuration-lifecycles",
        "title": "Separate configuration lifecycles"
      },
      {
        "slug": "opentofu-specify-configuration-behaviors",
        "title": "Specify configuration behaviors"
      }
    ],
    "comments": {
      "opentofu-provider-instance-management": [
        "Can you explain why you chose not to use setsubtract and disabled regions here?",
        "To be more explicit, the way that this is currently written (using the same input for both provider and resource's for_each) will cause warnings in OpenTofu.  Those warnings are why we need these docs in the first place.",
        "Perhaps a note that we don't support provider instances passed into child modules without selection."
      ],
      "opentofu-protect-infrastructure-secrets": [
        "It also might be worth introducing the idea that state encryption is our currently recommended solution.\r\n\r\nIMO they are two overlapping, but not exclusive concepts\r\n\r\nAdvantages of State Encryption:\r\n* All secrets are protected, regardless of provider or resource/module configuration\r\n* Don't need to chose what is protected and what is exposed\r\n* Also protects against provider injection\r\n  - https://www.plerion.com/blog/hacking-terraform-state-for-privilege-escalation\r\n  \r\nAdvantages of Ephemeral\r\n * No keys to loose / have leaked, secrets (if identified properly) are not stored in the first place."
      ],
      "opentofu-safe-lock-patterns": [
        "Good catch! fixed in fb8afbff57",
        "The reason I added the lock earlier was to help the terragrunt (and similar) scenarios.  I'm imagining a bunch of OpenTofu inits all being started from some parent tool, all pointed at the same cache directory.\r\n\r\nThe first one to acquire the lock downloads and installs the provider before linking it to the local directory and unlocking.\r\n\r\nEvery iteration after that is then able to detect and use the downloaded provider that's in the global cache.\r\n\r\nIf we move the lock further down, they would all fail the initial tryInstallPackageFromCacheDir check and be forced to download the provider and overwrite the global cache entry.\r\n\r\nRe-reading perhaps I should understand this instead as multiple smaller \"scoped\" locks where necessary?",
        "This resulted in a subsequent PR that deals with refactoring this part of the codebase a bit more in depth.  https://github.com/opentofu/opentofu/pull/2708\r\n\r\nI'm going to merge this version as is, with the idea that we will merge or close 2708 next week.",
        "That is why I added a defer() at the top of this function to catch any stray early returns.  It's inelegant and I've left some breadcrumb comments above on why it's not being refactored in this PR.\r\n\r\nYour suggested refactoring would definitely make this much easier to parse (in a subsequent PR)",
        "For now, let's keep with the unfortunate convention of context.TODO().  Earlier on in the project, we tried to refactor a lot of the contexts to be functional throughout the codebase.  It broke quite a few things in really subtle ways.\r\n\r\nI think that using a proper context here is the right thing long term, but we will need to prioritize that separately from this work."
      ],
      "opentofu-optimize-cicd-workflows": [
        "I wonder if having this fire while engineers are online makes more sense?",
        "I don't want to get pinged all Sunday if a bunch of vulns are posted to github."
      ],
      "opentofu-document-with-examples": [
        "changed alias to provider_alias"
      ],
      "opentofu-reduce-code-nesting": [
        "We use pv below and can't omit the value here.",
        "[1a5e1ec](https://github.com/opentofu/opentofu/pull/2069/commits/1a5e1ec2d8b9938424e22ce2ff24936c96a769d4)"
      ],
      "opentofu-craft-actionable-errors": [
        "This requirement only exists for provider for_each however.  There are other paths through the code (aliases in required_providers) that don't hit this constraint, other than during for_each.",
        "I was thinking more along the lines of someone who's introducing provider for_each into an existing codebase, and is confused why they are just now seeing this message.",
        "Going to resolve for now, happy to amend with further user feedback :+1: ",
        "Done!"
      ],
      "opentofu-minimize-api-surface": [
        "This appears to be nearly identical to the previous functionality (without the tag) as public properties are already encoded in the json package.\r\n\r\nI see two potential differences:\r\n1. The casing of the names is different and should be identical to the field name as before.\r\n2. The omitempty might generate payloads that are different that what was generated previously.\r\n\r\n1 could cause some serious breakage with previous locks (assuming that we json encoded it in previous releases)\r\n\r\n2 is probably not an issue as tools should not be directly manipulating this data outside of tofu.\r\n",
        "`grep \"json.Marshal\" internal/backend/ -r` shows that backends already depend on the current field names, which means they should be preserved\r\n",
        "Marking as resolved :+1: "
      ],
      "opentofu-document-reference-standards": [
        "Can we remove these individual comparability notes and point instead at the migration guide?"
      ],
      "opentofu-document-intent-and-limitations": [
        "We typically don't suggest folks look directly at the statefile, I don't think we have these fields documented anywhere concrete.",
        "Fixed in 1c3e8c0677",
        "Fixed in e7fe712c1d7d9b97c628a4f6eb9610e650ffe54b",
        "We now list all builtin functions both without a namespace and under the core:: namespace"
      ],
      "opentofu-prevent-backing-array-surprises": [
        "Makes sense :+1: \r\nb884241ff0",
        "[b884241](https://github.com/opentofu/opentofu/pull/2577/commits/b884241ff0924238bd19c95d137166e30abf3089)",
        "Our lint rules push for a particular application of de morgans law to keep boolean operations consistent.",
        "I've gone back and forth on this.  We know we are using overlapping code paths to produce duplicate diagnostics.  I'm ok with this being a bit stricter than ConsolidateWarnings.  I think the DeepEqual is probably a bit overkill, but @ollevche's list makes sense, perhaps with the Ranges added as well.",
        "I also think that DoNotConsolidateDiagnostic can be ignored for this particular function, as it is a special purpose function only for this scenario."
      ],
      "opentofu-clear-concise-documentation": [
        "I dislike the \"Please\", I want the message to be a call to action and not gentle."
      ],
      "opentofu-specify-configuration-behaviors": [
        "This was implemented in the implementation PR with a few lines.  It's easy to back out, but I'm not sure if it's more confusing to intentionally exclude it here."
      ],
      "opentofu-separate-configuration-lifecycles": [
        "1. It's more so a \"provider block\" vs \"provider block with instance data\"\r\n2. This is true, the expansion here feels a bit weird IMO and I noticed that during the prototype phase.  This change is the \"low churn\" option, but adds cognitive load to understanding provider configs in the configs package. \r\n\r\nThere are technically 3 levels of config parsing that are all mixed together:\r\n1. hcl/json text -> configs.File (HCL loading)\r\n2. configs.File -> configs.Module (merging files)\r\n3. configs.Module -> configs.Module (static evaluation)\r\n\r\nEven before static evaluation was added, there were all sorts of odd validation bugs all throughout the code when dealing with overrides.  Static evaluation shines more of a light on this.\r\n\r\nIn this specific scenario, we would need:\r\n```go\r\n\r\n// File parsing produces\r\ntype configs.ProviderBlock struct {\r\n  // 1-1 mapping to HCL\r\n}\r\n\r\n// Module struct from Files (normal+override) produces\r\ntype configs.ProviderData struct {\r\n  // Closer to existing struct\r\n  ForEach hcl.Expression\r\n  Count hcl.Expression\r\n}\r\n// Validation is performed when constructing ProviderData\r\n\r\n// func (ProviderData) expand(StaticContext) []Provider\r\n\r\n// Expanded into one or more\r\ntype configs.Provider struct {\r\n  // Matches existing struct\r\n  \r\n  ForEachValue *cty.Value\r\n  CountValue *cty.Value\r\n  // OR\r\n  InstanceData instances.RepetitionData\r\n}\r\n```\r\n\r\n\r\nFor the purposes of this PR, we could merge the concept of ProviderBlock and ProviderData to reduce the potential set of changes as it is a less important distinction IMO.",
        "I wonder if a pattern like `alias, ok := target.ref.KnownAlias(); ok` would make more sense here?  I also stink at naming functions.",
        "Given that import blocks are only allowed in the root module, we should know the exact provider alias and can treat any unknowns here as a bug?",
        "The config is already accessed in EvaluationScope below:\r\nhttps://github.com/opentofu/opentofu/pull/1772/files?diff=split&w=1#diff-126d6884fd43891cbb56845ee6f7f51ed02895af69190815e5c694a51a3777d3L522\r\n\r\nI don't think we need to add ctx.Config and can use that same code instead."
      ],
      "opentofu-proper-span-lifecycle": [
        "Another approach that could be considered is something like https://github.com/opentofu/opentofu/pull/1878/files#diff-385ff28969dc6410989718822b4c24e033e3a10d679bdd792fd2d55148c49471R456\r\n\r\nIt's a similar situation where I didn't want to heavily refactor the loop.  It mixes defer and iterative \"actions\" in a way that's comprehensive",
        "Alternatively, this is now the second time we are running into this and should probably take the time to refactor it :D",
        "Do you still need to span.End() here?"
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 94,
      "following": 3
    }
  },
  "radoering": {
    "repos": [
      "python-poetry/poetry"
    ],
    "entries": [
      {
        "slug": "poetry-api-backwards-compatibility",
        "title": "API backwards compatibility"
      },
      {
        "slug": "poetry-avoid-redundant-tool-configuration",
        "title": "avoid redundant tool configuration"
      },
      {
        "slug": "poetry-cache-expensive-computations",
        "title": "Cache expensive computations"
      },
      {
        "slug": "poetry-clear-actionable-error-messages",
        "title": "Clear actionable error messages"
      },
      {
        "slug": "poetry-complete-config-setting-integration",
        "title": "Complete config setting integration"
      },
      {
        "slug": "poetry-configure-http-requests-properly",
        "title": "Configure HTTP requests properly"
      },
      {
        "slug": "poetry-consistent-semantic-naming",
        "title": "consistent semantic naming"
      },
      {
        "slug": "poetry-dependency-constraint-consistency",
        "title": "dependency constraint consistency"
      },
      {
        "slug": "poetry-document-configuration-clearly",
        "title": "Document configuration clearly"
      },
      {
        "slug": "poetry-documentation-clarity-standards",
        "title": "Documentation clarity standards"
      },
      {
        "slug": "poetry-explicit-configuration-specification",
        "title": "explicit configuration specification"
      },
      {
        "slug": "poetry-explicit-null-handling",
        "title": "Explicit null handling"
      },
      {
        "slug": "poetry-maintain-security-constraints",
        "title": "maintain security constraints"
      },
      {
        "slug": "poetry-manage-testing-dependencies",
        "title": "manage testing dependencies"
      },
      {
        "slug": "poetry-network-request-configuration",
        "title": "Network request configuration"
      },
      {
        "slug": "poetry-optimize-algorithmic-efficiency",
        "title": "optimize algorithmic efficiency"
      },
      {
        "slug": "poetry-parameterize-similar-tests",
        "title": "Parameterize similar tests"
      },
      {
        "slug": "poetry-pin-tool-versions-explicitly",
        "title": "Pin tool versions explicitly"
      },
      {
        "slug": "poetry-prefer-simple-readable-code",
        "title": "prefer simple readable code"
      },
      {
        "slug": "poetry-specify-tool-version-requirements",
        "title": "specify tool version requirements"
      },
      {
        "slug": "poetry-use-appropriate-logging-levels",
        "title": "Use appropriate logging levels"
      },
      {
        "slug": "poetry-use-descriptive-names",
        "title": "Use descriptive names"
      }
    ],
    "comments": {
      "poetry-explicit-configuration-specification": [
        "> @radoering is `(>=2.7,<2.8 || >=3.4)` defaults intended?\r\n\r\nI assume this has been the default since forever:\r\nhttps://github.com/python-poetry/poetry-core/blob/main/src/poetry/core/packages/project_package.py#L40-L41",
        "> What do you think?\r\n\r\nI think your observation is correct.\r\n\r\n> It is not a good default\r\n\r\nI agree. I think we should discourage not defining `project.requires-python` or `tool.poetry.dependency.python`, maybe at least print a warning in `poetry check`, but that is something for another PR.",
        "```suggestion\r\nrequires = [\"poetry-core\", \"cython\", \"setuptools\"]\r\n```",
        "Not quite correct. We shouldn't care about `default` too much here because I don't want to change default handling in this PR.",
        "Not sure if \"explicit packages\" makes sense. Maybe:\r\n\r\n```suggestion\r\nThis approach is strongly suggested for all packages that are expected to be provided only by one specific source to avoid dependency confusion attacks.\r\n```",
        "```suggestion\r\n**Omit the `url` when specifying PyPI explicitly.** Because PyPI is internally configured\r\nwith Poetry, the PyPI repository cannot be configured with a given URL. Remember, you can always use\r\n`poetry check` to ensure the validity of the `pyproject.toml` file.\r\n```",
        "```suggestion\r\n{{% warning %}}\r\nUse `--` to terminate option parsing, otherwise commands like\r\n`poetry config http-basic.custom-repo gitlab-ci-token ${GITLAB_JOB_TOKEN}`\r\nwill fail if `${GITLAB_JOB_TOKEN}` starts with a hyphen (`-`).\r\n{{% /warning%}}\r\n\r\n```"
      ],
      "poetry-network-request-configuration": [
        "I'd omit everything from _\"As this environment variable is **unstable**, ...\"_ If we replace the environment variable, IMO we will deprecate it first anyway."
      ],
      "poetry-parameterize-similar-tests": [
        "Can we merge these three `ref_spec_resolve` tests into one parameterized test?\r\n\r\nThere are still some uncovered branches in `_normalise`, e.g. specifying a `branch` as `revision`, which can be covered easily. If we add more test variants, a parameterized tests will make even more sense.",
        "This looks like a copy of `test_env_activate_prints_correct_script` that also works for Windows. Can we just edit the original test instead of creating a new one?",
        "Can you make the test `parametrized` to test passing `bytes` as well as `str`, please?",
        "Can you make the test `parametrized` to add an example with a trailing slash, e.g. `\"ssh://git@github.com/org/repo/\"`, please?",
        "```suggestion\r\ndef test_no_directory_is_passed_to_installer(tester: CommandTester, mocker: MockerFixture):\r\n```",
        "Can you move this somewhere near the other \"pass to the installer\" tests, e.g. after `test_compile_option_is_passed_to_the_installer` and add a negative test via `pytest.mark.parametrize`  (`_skip_directory` is `False` if option is not passed)",
        "Can you integrate this with `pytest.mark.parametrize` in the previous test?\r\n\r\n`skip_directory=False` -> `directory_installs`, `installations_count == 6`\r\n`skip_directory=True` -> `not directory_installs`, `installations_count == ...`",
        "```suggestion\r\n@pytest.mark.parametrize(\"command\", [\"foo\", \"foo --lock\"])\r\n@pytest.mark.parametrize((\"locked\", \"expected_docker\"), [(True, \"4.3.1\"), (False, \"4.3.2\")])\r\n```\r\n\r\nThinking about the `poetry.locker.locked(True)`, I just came up with an idea to make the test more robust.",
        "We should parametrize the test since it should make no difference if `--lock` is passed or not anymore.",
        "Is it necessary to have three similar test cases or is one parameterized test case sufficient?",
        "If you now add `packages_files` and `expected_url_reference` as parameters (like [that](https://github.com/python-poetry/poetry/blob/e31a9fc3ce5a7fc72eefa22040676bc66a35e5ad/tests/installation/test_executor.py#L247-L279)) that's what I meant. 😅"
      ],
      "poetry-clear-actionable-error-messages": [
        "The output is not so nicely formatted. It contains a raw dict, e.g.:\r\n```\r\nValidation failed: {'errors': ['project.name must match pattern ^([a-zA-Z\\\\d]|[a-zA-Z\\\\d][\\\\w.-]*[a-zA-Z\\\\d])$'], 'warnings': []}\r\n```\r\n\r\nYou may take a look at https://github.com/python-poetry/poetry-core/blob/002aa3e16f98d21645bb9a45f698b55adc40f317/src/poetry/core/factory.py#L53-L60 for better formatting.",
        "Absolutely.",
        "My comment was just about formatting the output. At this point, we get a dict with lists of messages and the output should just be nicely formatted. The content of the single messages has been created before and is fixed at this point.\r\n\r\nIn my opinion, it is too much effort to improve each possible message that comes from schema validation - or at least this is clearly out of scope of this PR. The message is the same message if you run `poetry check` on such an invalid pyproject.toml.",
        "I assume we don't need `<error>` and `Error:` when raising an error. Further, we can remove `\"You may be getting improper dependencies.\"` because now you can't install anymore.",
        "Printing an error and raising an exception is a bit like \"Two is better than one.\" IMO one should suffice.\r\n\r\nPrinting an error and returning exit code 1 is probably not easily possible here, because we're not in the `handle` method of a command. Thus, I'd propose to only raise an exception.",
        "IMO it's not required to mention that only valid groups are allowed and it's not really necessary to mention the option to which the invalid group has been provided. The name of the invalid group should be enough for the user to notice what's wrong. I'd probably just print `Group(s) not found: batman, robin`\r\n\r\nOf course, I'm not the average user so if you think that's too consise please tell me. 😄 ",
        "I just noticed that we print this error even if there is no lockfile. Maybe, we should make a distinction between \"no lockfile\" and \"inconsistent lockfile\"?",
        "```suggestion\r\n                raise ValueError(f\"Failed to parse script entry point '{script}'\") from exc\r\n```",
        "> Several lint tools recommend against interpolation when instantiating Exceptions.\r\n\r\nSorry, I don't follow. Can you give an example?\r\n\r\nMy reasoning is that you get the original exception via `from exc`. Thus, you don't need `exc.args` it in the derived exception.",
        "Thanks for the example. I read the [reasoning to EM102](https://pypi.org/project/flake8-errmsg/0.4.0/) and I'm not sure I'm convinced (especially since we are not printing the traceback by default). Normally, we are using string literals and f-strings without assigning it to a variable first, so there is no reason to deviate from that here.\r\n\r\n> Poetry does not display the stack trace so you have to add the exception info to the message so the user can see what the symptom and cause is in one message\r\n\r\nYou will see the traceback by running poetry with `-vvv`. When not in verbose mode, we shouldn't print the original error message since it is not very helpful anyway. If you think that `f\"Failed to parse script entry point '{script}'\"` is not enough information, then what about `f\"Failed to parse script entry point '{script}', ':' not found\"` or similar?\r\n\r\nBy the way, we have the same script parsing with the same issue in https://github.com/python-poetry/poetry/blob/161b19cb4e4686fc5a0a7925001534a87f6c4052/src/poetry/console/commands/run.py#L73",
        "Good point, my proposal is not accurate enough. However, I still stand by my initial statement.\r\n\r\nIMO `('not enough values to unpack (expected 2, got 1)',)  - Failed to parse script entry point '...'` is not a good error message. Without any context `not enough values to unpack` does not really help. `Failed to parse script entry point '...'` should be good enough. If you want more details you have to run `poetry install -v`. That way you will get the `not enough values to unpack` with the line where it happened."
      ],
      "poetry-use-appropriate-logging-levels": [
        "We probably should at least add a debug log with the error message.",
        "I think it might be good to add a debug log for known exceptions because otherwise it might be difficult to tell why exactly lazy-wheel failed (e.g. no content-range).",
        "```suggestion\r\n\r\n    logger.debug('None of the hash types %s is in prioritized_hash_types', hash_types)\r\n\r\n    for hash_type in hash_types:\r\n        if hash_type in non_prioritized_hash_types:\r\n            return hash_type\r\n\r\n    logger.warning('None of the hash types %s is available in hashlib', hash_types)\r\n\r\n    return None\r\n```\r\n\r\nIf there is no prioritized hash type we just \"randomly\" choose an available hash type. That's why md5 and sha1 can be omitted in `prioritized_hash_types`.\r\n\r\nIf none of the hash types is available we probably should log a warning."
      ],
      "poetry-explicit-null-handling": [
        "Probably, a None check is easier to understand. I will change it.",
        "IIRC, this is written to the cache. It shouldn't matter if it's an empty list or None, should it?",
        "I was wondering where this warning came from, but was too lazy to go into the matter. 👍\r\n\r\nSince we are handling None as well as KeyError now, we should filter this deprecation warning, shouldn't we?",
        "However, it will take quite a while: https://github.com/python/importlib_metadata/blob/700f2c7d74543e3695163d5487155b92e6f04d65/importlib_metadata/_adapters.py#L11\r\n\r\nIt seems even though https://github.com/python/importlib_metadata/pull/416 has been merged into main it's not in main!?\r\n\r\nI'd prefer a filter and an explanatory comment. Even if we forget to remove it, I assume it will not really hurt. (Just added it.) Does that make sense to you?",
        "```suggestion\r\n        assert module_name(name) == package_include.get(\"include\")\r\n```\r\n\r\n`name` is always set isn't it?\r\n\r\nIf I don't miss anything the expression wouldn't work if it wasn't set because there is a missing bracket. It had to be:\r\n\r\n```\r\nassert module_name(name) == (package_include.get(\"include\") if name else \"\")\r\n```",
        "LGTM 👍 "
      ],
      "poetry-specify-tool-version-requirements": [
        "```suggestion\r\n**Yes**. Provided that you are using `tox` >= 4, you can use it in combination with\r\nthe PEP 517 compliant build system provided by Poetry. (With tox 3, you have to set the \r\n[isolated build](https://tox.wiki/en/3.27.1/config.html#conf-isolated_build) option.)\r\n```"
      ],
      "poetry-consistent-semantic-naming": [
        "The name of the label is not consistent with our naming scheme (no spaces, but slashes). Maybe, just `test/export`?",
        "IMO, this ID/name is a bit misleading since the hook does not run `poetry update`. Maybe, `poetry-sync` would be a better name?"
      ],
      "poetry-maintain-security-constraints": [
        "Is this still necessary? Shouldn't there always be as hash now?",
        "We assumed it before this PR. If there the set of known hashes is empty, the hash is not known and we should fail for security reasons. I wouldn't weaken that constraint if it is not necessary."
      ],
      "poetry-prefer-simple-readable-code": [
        "done",
        "```suggestion\r\n        return cls.run(\"checkout\", rev, folder=target)\r\n```\r\n\r\nThis should be sufficient because `None` is handled in `run()`. (The `args` variable in line 27 can be removed then.)"
      ],
      "poetry-complete-config-setting-integration": [
        "You have to add this setting in two more places:\r\n\r\n- Some lines below in `_get_normalizer`\r\n- In `ConfigCommand.unique_config_values`",
        "```suggestion\r\n                warning = (\r\n                    \"Found deprecated key 'default' or 'secondary' in\"\r\n                    \" pyproject.toml configuration for source\"\r\n                    f\" {source.get('name')}. Please provide the key 'priority'\"\r\n                    \" instead. Accepted values are:\"\r\n                    f\" {', '.join(p.name.lower() for p in Priority)}.\"\r\n                )\r\n                io.write_error_line(\"<warning>Warning: {warning}</warning>\")\r\n```\r\n\r\nor similar.\r\n\r\nDeprecation warnings are for (plugin) developers, but that's something that's relevant for the end user."
      ],
      "poetry-manage-testing-dependencies": [
        "Fortunately, it is only a dev dependency. That is why I would say it is not that relevant for the release. In the medium term, of course, we have to replace it."
      ],
      "poetry-cache-expensive-computations": [
        "We may use `@cached_property` instead so we don't need `_installer_scheme_dict`.",
        "One more thing: Since `all_requires` is not an attribute but a property that iterates over groups and dependencies we should store it in a local variable outside of the loop so we have to calculate it just once. Further, I'd do the same for `self.option(\"top-level\")`. It's not consistent for all options, but it's done for some options at least.",
        "Maybe, we should make this a `cached_property` since this should not change during one invocation of poetry. Now, we are reading `pyvenv.cfg` once for each package."
      ],
      "poetry-dependency-constraint-consistency": [
        "1. We do this since Poetry 1.3. IIRC, the idea behind this was that we will do a Poetry release anyway even if there is just a poetry-core bugfix release.\r\n2. I think so. That is what Poetry procudes for core metadata.",
        "Shall we require `\"^1.1.1\"` because `1.1.0` is yanked?",
        "Sure about that?\r\n\r\n<html><body>\r\n<!--StartFragment-->\r\n\r\nimportlib_metadata | stdlib\r\n-- | --\r\n4.8 | 3.11\r\n4.4 | 3.10\r\n1.4 | 3.8\r\n\r\n<!--EndFragment-->\r\n</body>\r\n</html>\r\n(from https://pypi.org/project/importlib-metadata/)\r\n\r\nIf we really want to do that, we'll have to change https://github.com/python-poetry/poetry/blob/0b71e4381445bf6b6c239bf0d88c8f5dfb4a32bd/src/poetry/utils/_compat.py#L10-L11",
        "I like <3.8 even if we don't support 3.6. I suppose that's a matter of taste. 🤷 "
      ],
      "poetry-documentation-clarity-standards": [
        "I would restructure the text a bit and do not refer to `eval` but \"the eval command of your shell\" (or similar). Proposal:\r\n\r\n```suggestion\r\nThe `poetry env activate` command prints the activate command of the virtual environment to the console.\r\nYou can run the output command manually or feed it to the eval command of your shell to activate the environment.\r\nThis way you won't leave the current shell.",
        "Is \"less significant\" a common term? (I know \"least significant\" but haven't heard \"less significant\" before.)\r\n\r\nAlternatively, we could just write \"... only modifies the numbers to the right ...\".",
        "```suggestion\r\nSimilar to `--no-root` you can use `--no-directory` to skip directory path dependencies:\r\n```\r\n\r\nCan you add a sentence that mentions that skipping directory path dependencies only makes sense for certain use cases and link to the FAQ entry?"
      ],
      "poetry-avoid-redundant-tool-configuration": [
        "```suggestion\r\n```\r\n\r\nThis seems to be the default order, so we don't have to configure it.",
        "The [documentation of isort](https://pycqa.github.io/isort/docs/configuration/custom_sections_and_ordering.html) seems to be better in this point:\r\n\r\n> You can change the section order with sections option from the default of:\r\n> \r\n> `FUTURE,STDLIB,THIRDPARTY,FIRSTPARTY,LOCALFOLDER`\r\n\r\nI'm not good at Rust but after some trial and error and code spelunking, I'd say ruff does the same:\r\n[ImportType Enum](https://github.com/charliermarsh/ruff/blob/64b7280eb824d0e5f9da887e82dcda53838dd38d/crates/ruff/src/rules/isort/categorize.rs#L35C1-L41)\r\n[Default order 1](https://github.com/charliermarsh/ruff/blob/64b7280eb824d0e5f9da887e82dcda53838dd38d/crates/ruff/src/rules/isort/settings.rs#LL343C83-L343C83)\r\n[Default order 2](https://github.com/charliermarsh/ruff/blob/64b7280eb824d0e5f9da887e82dcda53838dd38d/crates/ruff/src/rules/isort/settings.rs#L351-L353)"
      ],
      "poetry-pin-tool-versions-explicitly": [
        "Good catch. Tested it in python-poetry/website#162 and adopted it."
      ],
      "poetry-configure-http-requests-properly": [
        "I'm fine with adding them if you add a comment with a link so we see where these codes come from."
      ],
      "poetry-optimize-algorithmic-efficiency": [
        "Probably not. I'm not even sure if there are real world examples for this edge case. Usually, there is just one reason. I'll change it to a `set` and add a `sorted` to the `join` (for reproducable output).",
        "```suggestion\r\n        return super().is_path_relative_to_lib(path) or (\r\n            self.includes_system_site_packages()\r\n            and SystemEnv(Path(sys.prefix)).is_path_relative_to_lib(path)\r\n        )\r\n```\r\n\r\nThat way we don't have to create a SystemEnv and call additional methods in the default case (where system site packages are not included).",
        "```suggestion\r\n\r\nnon_prioritized_hash_types = hashlib.algorithms_available - set(prioritized_hash_types)\r\n\r\n```"
      ],
      "poetry-api-backwards-compatibility": [
        "If I remember correctly, this function is used by plugin authors (not only poetry-plugin-export). Maybe, we should deprecate the parameters, keep them functional and add `priority` as keyword-only argument for now.",
        "I suppose this will break some plugins. I'd like to deprecate it first.",
        "It's not the import, I'm concerned about. It's `PyProjectTOML.file`. Correct me if I'm wrong, but plugins relying on `PyProjectTOML.file.parent` will still work if we keep `__getattr__()`.",
        "You may not have to change the import if there is no import. Let's just look at the [docs](https://python-poetry.org/docs/plugins/#generic-plugins). You have an instance of the `Poetry` class in your hand and just could do `poetry.pyproject.file.parent`. I suppose many users will not import `PyProjectTOML` or `TOMLFile` but just use the instance they get from the Poetry object."
      ],
      "poetry-use-descriptive-names": [
        "If you want to refrain from taking advantage of the fact that the list is sorted and thus from the early return if the version is not found, you can simplify the loop as follows:\r\n\r\n```\r\nif locked is not None:\r\n    for version in packages:\r\n        if version.version == locked.version:\r\n            break\r\n```",
        "I like it, but it has to be:\r\n\r\n```\r\nversion = next((p for p in packages if p.version == locked.version), None)\r\n```\r\n\r\nMaybe a bit confusing that the variable `version` is a `package`. Should we rename it to `package`?",
        "@neersighted Sorry for bothering, but I had the impression that the review of this PR has already been well advanced - maybe, the naming of the variable being the only point to be clarified before merging. (Correct me if I'm wrong.) I'd appreciate if you find some minutes so that we may finalize it.",
        "I understand the confusion.\r\n\r\nOn the one hand, `SystemEnv` is the env Poetry is installed in. `poetry env use system` tells Poetry to use it's own Python version as default for new venvs. Thus, in our wording `system` is Poetry's own env.\r\n\r\nOne the other hand, `virtualenvs.options.system-site-packages` does not refer to Poetry's own env. And most people will probably not associate `system` with Poetry's own env.\r\n\r\nNot sure, if `get_poetry_python` is a better name or if it can be confused with the project's python. At least, `system` is quite consistent in our code. No strong opinion on this. 🤷",
        "Maybe, we should rename it to `use_in_project_venv` instead. At least to me, `root_env` is a bit vague.",
        "```suggestion\r\n    cached_wheel: bool,\r\n```\r\n\r\nMaybe, we should name it more clear because it doesn't matter if the sdist is cached, only if a generated wheel is cached or not.",
        "Ah, I thought caching is only turned on/off for the generated wheel and the sdist is always cached, but it is also turned on/off for the sdist. In that case `cached` would have been fine, but `is_artifact_cached` is better. 😄 "
      ],
      "poetry-document-configuration-clearly": [
        "I always use `--sync` so that's fine for me. 😉\r\n\r\nWe could also add two hooks `poetry-install` (without sync) and `poetry-sync` (with sync). Maybe, we just add `poetry-sync` now and poetry-install later only if someone requests it?",
        "That's fine with me, too. It has the benefit that it's more flexible and we only need one hook. 🤷 ",
        "Another spot that reads more like `poetry install --sync`. Considering the descriptions of the other hooks, maybe:\r\n\r\n```suggestion\r\n  description: run poetry install to install dependencies from the lock file\r\n```"
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 27,
      "following": 0
    }
  },
  "lgrammel": {
    "repos": [
      "vercel/ai"
    ],
    "entries": [
      {
        "slug": "ai-async-error-callbacks",
        "title": "Async error callbacks"
      },
      {
        "slug": "ai-consistent-camelcase-naming",
        "title": "Consistent camelCase naming"
      },
      {
        "slug": "ai-consistent-provider-options",
        "title": "Consistent provider options"
      },
      {
        "slug": "ai-consistent-semantic-naming",
        "title": "Consistent semantic naming"
      },
      {
        "slug": "ai-document-api-schemas",
        "title": "Document API schemas"
      },
      {
        "slug": "ai-document-configuration-decisions",
        "title": "Document configuration decisions"
      },
      {
        "slug": "ai-explicit-code-organization-patterns",
        "title": "Explicit code organization patterns"
      },
      {
        "slug": "ai-flexible-api-inputs",
        "title": "Flexible API inputs"
      },
      {
        "slug": "ai-format-for-rendering-compatibility",
        "title": "Format for rendering compatibility"
      },
      {
        "slug": "ai-keep-tests-simple",
        "title": "Keep tests simple"
      },
      {
        "slug": "ai-maintain-api-naming-consistency",
        "title": "Maintain API naming consistency"
      },
      {
        "slug": "ai-optimize-ci-type-checking",
        "title": "Optimize CI type checking"
      },
      {
        "slug": "ai-place-configurations-appropriately",
        "title": "Place configurations appropriately"
      },
      {
        "slug": "ai-provide-actionable-examples",
        "title": "Provide actionable examples"
      },
      {
        "slug": "ai-test-before-documenting",
        "title": "Test before documenting"
      },
      {
        "slug": "ai-type-safe-null-handling",
        "title": "Type-safe null handling"
      },
      {
        "slug": "ai-validate-pattern-matching",
        "title": "Validate pattern matching"
      },
      {
        "slug": "ai-versioning-for-migrations",
        "title": "Versioning for migrations"
      }
    ],
    "comments": {
      "ai-flexible-api-inputs": [
        "replace tool schemas with json schemas\n\nthis affects structured outputs as well",
        "out of curiosity, what happens if the url is entered as a string? (should ideally be supported)",
        "(no need for PR updates re this comment)",
        "we have special conversion mechanisms that we should be using, no need to this build one-off, and we can also delay and do it on v5 once all providers are there to reduce # of ports.",
        "(the mechanism has been introduced / updated on v5 recently)"
      ],
      "ai-place-configurations-appropriately": [
        "can you make it a constructor parameter (in the options) for the model that is then defined in the providers? (which would have that knowledge)",
        "include the default object generation mode in `config`",
        "This seems like something we could turn into a top-level option eventually?",
        "voice and response_format are setting we have standardized. we usually do not duplicate standardized settings in the providerOptions"
      ],
      "ai-validate-pattern-matching": [
        "this can lead to issues where the wrong enum value is selected as a partial result, leading to changing enum values. i'd prefer a solution where that cannot happen (either by only returning fully matched enum values, or (more complicated) by returning enum values for which only one value is possible per prefix)",
        "What is this for? Can we do this more elegantly?",
        "interesting - looks like this is not going to work for base64? we need tests around this and make it work for base 64 as well. Also it might be good to have some way of opting (or flagging on the signatures) since this removal won't be needed for images presumably.",
        "btw, is this a separate bugfix that should be extracted into a standalone PR?"
      ],
      "ai-explicit-code-organization-patterns": [
        "The value is set in the constructor. Would prefer `private strictMode: boolean` to prevent the impression that the `false` setting here matters.",
        "avoid export `*` - we need to control what we re-export",
        "this is strange - can we directly import (from provider utils or the file)?",
        "can't you replace it with:\r\n\r\n```\r\nimport { convertArrayToReadableStream } from '@ai-sdk/provider-utils/test';\r\n```",
        "ideally for imports we avoid barrel files as much as possible and go for the original source"
      ],
      "ai-consistent-provider-options": [
        "make provider-specific. different providers have different structures. have default that matches openai",
        "this is going to be brittle as they change their APIs. Can we expose the raw body from the final result and strip this to a minimum?",
        "Please keep specs fully separate for now.",
        "I've extracted `SharedV2ProviderOptions` and `SharedV2ProviderMetadata` that we can use in all model specs: https://github.com/vercel/ai/pull/5733",
        "Please use the validation approach for provider options that we use elsewhere (so we get a type for type checking and can throw errors before the request). See https://github.com/vercel/ai/blob/main/packages/anthropic/src/anthropic-messages-language-model.ts#L122 and https://github.com/vercel/ai/blob/main/packages/anthropic/src/anthropic-messages-language-model.ts#L724 for an example"
      ],
      "ai-optimize-ci-type-checking": [
        "which one will run in CI? important that we keep full checks there",
        "where will the js files go?"
      ],
      "ai-type-safe-null-handling": [
        "`undefined` or `never`? (i have used `never` in other places for something similar)",
        "`never` is actually better ts wise imo",
        "and you might want it to be optional ",
        "most stream parts are defined in stream parts afaik. do we need to expose this? otherwise would prefer if it's consistent with the other stream parts. also, `any` is risky. is `unknown` possible?",
        "shouldnt this be automatically typed?",
        "can you add a zod schema and validate instead?",
        "avoid any",
        "also return type should be narrowed via is",
        "you can remove indent via \r\n\r\nfor (const citation of response.message.citations ?? [])",
        "can leave undefined (no `|| ''`) and then use `?.` before `startsWith`"
      ],
      "ai-document-configuration-decisions": [
        "this is a downgrade - why?",
        "we can make zod a normal dependency since this is not a lib we publish",
        "is there a risk that this breaks common js usage of the ai sdk?",
        "needed for the rsc move?"
      ],
      "ai-async-error-callbacks": [
        "Wondering if that's the desired behavior - might be good to think through use cases",
        "wonder if it's worth pushing the onError object into `consumeStream` to keep the behavior between `consumeStream` here and the general helper aligned?",
        "the helper is afaik only used internally so we can change the main `stream` object to become a parameter object \r\n\r\n```\r\n{\r\n  stream,\r\n  onError?\r\n}\r\n```"
      ],
      "ai-provide-actionable-examples": [
        "need to set expectations:\r\n\r\n- only for green-field prototypes (no migrations yet)\r\n- not for production use\r\n- please provide feedback\r\n- expect large breaking changes while in alpha"
      ],
      "ai-format-for-rendering-compatibility": [
        "this is somewhat specific. we also have a `contributing/` folder where we can have more detailed guides, architecture docs, etc. What do you think about having a `contributing/how-to-create-a-codemode.md` and then referring to the contributing directory here?"
      ],
      "ai-keep-tests-simple": [
        "instead of flags, just set up custom prepare methods or if it's a single test define the test input in the test\r\n\r\nprefer less magic / thinking in tests, often it is very helpful to be able to look at the raw input without any indirections / logic",
        "just inline the parts. unit tests should be as straightforward as possible. if you want to check that includeRawChunks is passed correctly, add a separate test for just that.",
        "an inline snapshot on content is prob easier, also checks order. in general such logic in tests is usually a code smell",
        "is this needed? might make sense to just call `new ChatStore` in the tests to be explicit.",
        "(then you can also inline some variables in the tests to see how the store would look like)",
        "this test can be split up into several tests for the individual callbacks. should also test that they are invoked with the expected objects. testing `store.getmessages` should be a separate test as well.\r\n\r\n(one `it` test should ideally only test 1 \"thing\" so they can fail individually)",
        "please stub the dates instead. see e.g. \r\n\r\nhttps://github.com/vercel/ai/blob/main/packages/ai/core/generate-text/generate-text.test.ts#L1280\r\n\r\n```ts\r\n  _internal: {\r\n    generateId = originalGenerateId,\r\n    currentDate = () => new Date(),\r\n  } = {},\r\n  ```\r\n  \r\nTests should never have any variable aspects such as dates. With stubbed values we can exactly test the passthrough."
      ],
      "ai-document-api-schemas": [
        "add model id and jsdoc?"
      ],
      "ai-test-before-documenting": [
        "While this is generally true, it is really a misleading explanation.\r\n\r\nThis happens when a provider API returns with an error. Most of the time, this means that there API cannot be used in this way, e.g. invalid message structure. The user needs to carefully read the actual error message from the provider and figure out what to do.",
        "@Und3rf10w have you tested this with the gemini api? couldn't find it in their docs",
        "I guess we can add the fs.readFile from the executable example"
      ],
      "ai-consistent-camelcase-naming": [
        "why was this changed to snake case? if that is the case in the code we should fix the code instead",
        "we usually use `camelCase` in the provider options to match conventions (even if the provider uses `snake_case` in their apis)"
      ],
      "ai-maintain-api-naming-consistency": [
        "We need to consider renaming this on the `embed` method returns as well, and potentially for `embedMany`"
      ],
      "ai-versioning-for-migrations": [
        "on v5 it does not matter, the release will be `major` regardless. ",
        "Lets have them always use a `patch` changeset - only we should be doing minor/major"
      ],
      "ai-consistent-semantic-naming": [
        "rn to `create...` (start function w/ verb)",
        "Prefer full words for generic in upper case, e.g. `CONTEXT`",
        "convention: start non-classes/types (ie regular vars like schemas) with lowercase letter",
        "wonder how far we should to here. usually we do this but it is also overhead",
        "rename to `maxParallelCalls` or `maxParallelRequests` or `maxConcurrentRequests`. If we rename to `Concurrent` here it would be good to do the same in the model spec",
        "We should return `GenerateObjectResult<TYPE>` (not the default, that's internal)",
        "also can you change `TYPE` to `RESULT` (which is more specific, a generic will always be a type)",
        "rename to `mediaType` and refer to IANA media types (see other examples esp. on v5 branch)",
        "let's fix that in `v5` and move to a pattern similar to groq where we parse all options first",
        "what is the unit? can we include it in the var name to avoid confusion, e.g. `durationInSeconds`"
      ]
    },
    "profile": {
      "company": "Vercel",
      "blog": "",
      "twitter_username": "lgrammel",
      "site_admin": false,
      "followers": 596,
      "following": 30
    }
  },
  "chris-olszewski": {
    "repos": [
      "vercel/turborepo"
    ],
    "entries": [
      {
        "slug": "turborepo-boundary-case-handling",
        "title": "Boundary case handling"
      },
      {
        "slug": "turborepo-choose-logging-levels-wisely",
        "title": "Choose logging levels wisely"
      },
      {
        "slug": "turborepo-configuration-precision-matters",
        "title": "Configuration precision matters"
      },
      {
        "slug": "turborepo-consider-config-generation-methods",
        "title": "Consider config generation methods"
      },
      {
        "slug": "turborepo-define-api-boundaries",
        "title": "Define API boundaries"
      },
      {
        "slug": "turborepo-descriptive-unambiguous-identifiers",
        "title": "Descriptive, unambiguous identifiers"
      },
      {
        "slug": "turborepo-design-ergonomic-apis",
        "title": "Design ergonomic APIs"
      },
      {
        "slug": "turborepo-document-cache-strategies",
        "title": "Document cache strategies"
      },
      {
        "slug": "turborepo-eliminate-code-duplication",
        "title": "Eliminate code duplication"
      },
      {
        "slug": "turborepo-graceful-error-recovery",
        "title": "Graceful error recovery"
      },
      {
        "slug": "turborepo-handle-errors-appropriately",
        "title": "Handle errors appropriately"
      },
      {
        "slug": "turborepo-keep-build-tooling-updated",
        "title": "Keep build tooling updated"
      },
      {
        "slug": "turborepo-know-your-implicit-configurations",
        "title": "Know your implicit configurations"
      },
      {
        "slug": "turborepo-minimize-lock-duration",
        "title": "Minimize lock duration"
      },
      {
        "slug": "turborepo-propagate-errors-with-context",
        "title": "Propagate errors with context"
      },
      {
        "slug": "turborepo-semantic-naming-conventions",
        "title": "Semantic naming conventions"
      },
      {
        "slug": "turborepo-use-functional-null-handling",
        "title": "Use functional null handling"
      },
      {
        "slug": "turborepo-use-workspace-dependencies-consistently",
        "title": "Use workspace dependencies consistently"
      },
      {
        "slug": "turborepo-validate-configurations-comprehensively",
        "title": "Validate configurations comprehensively"
      },
      {
        "slug": "turborepo-validate-performance-impact-first",
        "title": "Validate performance impact first"
      }
    ],
    "comments": {
      "turborepo-boundary-case-handling": [
        "This one is totally up to you if you think explicitly handling the underflow case is easier to reason about than leveraging modulos.\r\n```suggestion\r\n            self.selected_task_index = self.selected_task_index.checked_sub(1).unwrap_or(num_rows - 1);\r\n```",
        "I'm an `Option` addict, I love how clean the control flow that methods provide is."
      ],
      "turborepo-keep-build-tooling-updated": [
        "We should probably switch to `topo` so if users add tests to `@repo/ui` then `web#test` and `docs#test` don't block on those finishing:\r\n![graphviz(2)](https://github.com/user-attachments/assets/cc97dbfb-92dc-433b-bd23-f39a19d8f264)\r\n"
      ],
      "turborepo-know-your-implicit-configurations": [
        "No, we'll always parse the lockfile and fold the packages into a task hash."
      ],
      "turborepo-define-api-boundaries": [
        "Do we want to clarify that we do not consider unstructured terminal output as an API? For example https://github.com/vercel/turborepo/pull/10079 should not be considered a breaking change IMO. Now if it was a removal of a field from `turbo ls --output=json` it would be.",
        "> Do other CLI tools consider their stdout/stderr as semver-protected if its meant for human readability?\r\n\r\nIt depends on the output, if it's meant to be human readable, then usually no. All for omitting this since this is usually not assumed to be an API."
      ],
      "turborepo-graceful-error-recovery": [
        "Up to you since you're the one driving `query`, but returning information that we know is incorrect feels off to me. Do we need to worry about panics down the line in query around assumptions that every task has a valid package and has valid task dependencies?",
        "Unsure of why the actual diff isn't showing on the [prysk diff output](https://github.com/vercel/turborepo/actions/runs/13460494959/job/37619607217#step:9:1126), but it's missing the possible values\r\n```suggestion\r\n            Specify which tasks should continue running when an error occurs. Use \"none\" to cancel all tasks. Use \"independent-tasks-only\" to continue running independent tasks and cancel dependent ones. Use \"all\" to continue running all tasks [default: none] [possible values: none, independent-tasks-only, all]\r\n```"
      ],
      "turborepo-document-cache-strategies": [
        "Are you referring to the `Remote caching enabled` line in the run prelude? If so I can confirm that it becomes `Remote caching disabled` with `--cache=local:rw`."
      ],
      "turborepo-semantic-naming-conventions": [
        "Not blocking, but I feel like `from` or `base` is less confusing name.\r\nDefer to Git wizard @NicholasLYang, if you're finding affected packages between `HEAD` and another commit, is there a standard name for what to call that other commit?"
      ],
      "turborepo-consider-config-generation-methods": [
        "The go yaml package explicitly doesn't support controlling whitespace: https://github.com/go-yaml/yaml/issues/627\r\nHaving encoding/decoding to preserve the file was really helpful for development when diffing the original and pruned lockfile. If sacrificing that for a straightforward yaml serialization is desired we can do that. I'll quick double check that pnpm doesn't throw if parsing these formatting differences.",
        "From reading through pnpm I don't think these can collide. If a dep is marked optional then it won't appear in `dependencies`, peer deps are handled in the same manner. In the case that there's an optional peer it appears under peer with the `optional` attribute set true in `peerDependenciesMeta`"
      ],
      "turborepo-use-workspace-dependencies-consistently": [
        "I was matching the version used in [`turborepo-lib`](https://github.com/vercel/turborepo/pull/9995/files#diff-217f728c3ab310388e85c7ed8a02edc124d9b28c82b9cf623a41b94204848d41L61). I can update both to use workspace version in a followup PR.",
        "```suggestion\r\ngit2 = { workspace = true, default-features = false }\r\n```"
      ],
      "turborepo-design-ergonomic-apis": [
        "Just using `strip_prefix` from the jump lets us skip the `unwrap`\r\n```suggestion\r\n        if let Some(catalog_name) = specifier.strip_prefix(\"catalog:\") {\r\n            if let Some(catalogs) = &self.catalogs {\r\n                if let Some(catalog) = catalogs.get(catalog_name) {\r\n                    if let Some(dep) = catalog.get(name) {\r\n                        return Ok(Some(&dep.version));\r\n                    }\r\n                }\r\n            }\r\n            return Ok(None);\r\n```",
        "If you follow the above suggestion you can change this to also just take `&str`\r\n```suggestion\r\n        fn token(mut self, value: &str) -> Self {\r\n            self.output.token = Some(value.into());\r\n            self\r\n        }\r\n```",
        "Only when building the engine do we view a non-existent `turbo.json` as a non-error. I'm not sure requiring the other callers to construct the `NoTurboJSON` error type manually is worth the better ergonomics in the engine builder.",
        "I would highly suggest leveraging `#[serde(into)]` [docs](https://serde.rs/container-attrs.html#into) here instead of manually implementing this. The general pattern is create a new type like `RepoDetailsDisplay` and then create a `impl<'a> From<&'a RepositoryDetails<'a>> for RepoDetailsDisplay<'a>` which does any of the desired conversions. e.g. `(&PackageName, &AnchoredSystemPath) -> PackageDetails { name: String, path: String }`",
        "We don't need to specify an exact underlying writer type here, we can render an app regardless of what sort of stdin handles it has.\r\n\r\n```suggestion\r\nfn view<W>(app: &mut App<W>, f: &mut Frame, rows: u16, cols: u16) {\r\n```"
      ],
      "turborepo-minimize-lock-duration": [
        "We only acquire the guard in the block so we ensure we release the lock once we take the changed packages. This lets the `event_fut` make progress while `self.execute_run` is pending.",
        "`changed_packages_guard` is still held while run is being executed, this prevents any events from being handled. ",
        "We should drop the lock before we do a serial send."
      ],
      "turborepo-configuration-precision-matters": [
        "For whatever reason, double `,` with trailing commas really creates a wild output that isn't useful when snapshot. The error could be better, but ITG as the first error still highlights the double `,,`\r\n```\r\nturbo_json_parse_error                                                                                                                                                        \r\n                                                                                                                                                                              \r\n  × Failed to parse turbo.json.                                                                                                                                               \r\n  ├─▶   × expected `}` but instead found `,`                                                                                                                                  \r\n  │      ╭─[turbo.json:2:50]                                                                                                                                                  \r\n  │    1 │ {                                                                                                                                                                  \r\n  │    2 │   \"$schema\": \"https://turborepo.com/schema.json\",,                                                                                                                 \r\n  │      ·                                                  ─                                                                                                                 \r\n  │    3 │   \"ui\": \"tui\",                                                                                                                                                     \r\n  │      ╰────                                                                                                                                                                \r\n  ├─▶   × End of file expected                                                                                                                                                \r\n  │      ╭─[turbo.json:3:3]                                                                                                                                                   \r\n  │    2 │   \"$schema\": \"https://turborepo.com/schema.json\",,                                                                                                                 \r\n  │    3 │   \"ui\": \"tui\",                                                                                                                                                     \r\n  │      ·   ────                                                                                                                                                             \r\n  │    4 │   \"tasks\": {                                                                                                                                                       \r\n  │      ╰────                                                                                                                                                                \r\n  ├─▶   × End of file expected                                                                                                                                                \r\n  │      ╭─[turbo.json:3:7]\r\n  │    2 │   \"$schema\": \"https://turborepo.com/schema.json\",,\r\n  │    3 │   \"ui\": \"tui\",\r\n  │      ·       ─\r\n  │    4 │   \"tasks\": {\r\n  │      ╰────\r\n  ├─▶   × End of file expected\r\n  │      ╭─[turbo.json:3:9]\r\n  │    2 │   \"$schema\": \"https://turborepo.com/schema.json\",,\r\n  │    3 │   \"ui\": \"tui\",\r\n  │      ·         ─────\r\n  │    4 │   \"tasks\": {\r\n  │      ╰────\r\n  ├─▶   × End of file expected\r\n  │      ╭─[turbo.json:3:14]\r\n  │    2 │   \"$schema\": \"https://turborepo.com/schema.json\",,\r\n  │    3 │   \"ui\": \"tui\",\r\n  │      ·              ─\r\n  │    4 │   \"tasks\": {\r\n  │      ╰────\r\n  ├─▶   × End of file expected\r\n  │      ╭─[turbo.json:4:3]\r\n  │    3 │   \"ui\": \"tui\",\r\n  │    4 │   \"tasks\": {\r\n  │      ·   ───────\r\n  │    5 │     \"build\": {\r\n  │      ╰────\r\n  ├─▶   × End of file expected\r\n  │      ╭─[turbo.json:4:10]\r\n  │    3 │   \"ui\": \"tui\",\r\n  │    4 │   \"tasks\": {\r\n  │      ·          ─\r\n  │    5 │     \"build\": {\r\n  │      ╰────\r\n  ├─▶   × End of file expected\r\n  │       ╭─[turbo.json:4:12]\r\n  │     3 │       \"ui\": \"tui\",\r\n  │     4 │ ╭─▶   \"tasks\": {\r\n  │     5 │ │       \"build\": {\r\n  │     6 │ │         \"dependsOn\": [\"^build\"],\r\n  │     7 │ │         \"inputs\": [\"$TURBO_DEFAULT$\", \".env*\"],\r\n  │     8 │ │         \"outputs\": [\".next/**\", \"!.next/cache/**\"],\r\n  │     9 │ │       },\r\n  │    10 │ │       \"lint\": {\r\n  │    11 │ │         \"dependsOn\": [\"^lint\"]\r\n  │    12 │ │       },\r\n  │    13 │ │       \"check-types\": {\r\n  │    14 │ │         \"dependsOn\": [\"^check-types\"]\r\n  │    15 │ │       },\r\n  │    16 │ │       \"dev\": {\r\n  │    17 │ │         \"cache\": false,\r\n  │    18 │ │         \"persistent\": true\r\n  │    19 │ │       }\r\n  │    20 │ ╰─▶   }\r\n  │    21 │     }\r\n  │       ╰────\r\n  ├─▶   × End of file expected\r\n  │       ╭─[turbo.json:21:1]\r\n  │    20 │   }\r\n  │    21 │ }\r\n  │       · ─\r\n  │       ╰────\r\n  ╰─▶   × turbo.json has an incorrect type, expected an object, but received an array.\r\n```",
        "Can we make this an exact version to prevent this getting bumped and behaving unexpectedly.\r\n```suggestion\r\n    \"@types/d3-scale\": \"4.0.2\"\r\n```",
        "> I can't remember if our integration test fixtures generally use packageManager or not, but that might be useful since I know * has some weird behavior in different npm versions, especially with workspaces\r\n\r\nGood callout. With package manager requirements we now force usage of corepack for all test fixture defaulting to `npm` if one isn't specified in the fixture. Will open a ticket to make this explicit"
      ],
      "turborepo-propagate-errors-with-context": [
        "Bubble up this error so it reaches the caller and they can decide what to do with it. In our case when this error is eventually returned in `change_detector.rs` we should catch it and behave as if every package changed since we cannot identify which exact packages changed. The warning you added should be moved there with an updated message.",
        "I do not want a lossy conversion as this will cause silent failures upstream. Please report up any non-UTF8 output as an error.",
        "Generally we should prefer methods on the `turbopath`s types since we have nicer error messages. (By default Rust io errors don't include paths in them which can lead to frustrating error messages).\r\n\r\n```suggestion\r\n        preferences_file.ensure_dir()?;\r\n```",
        "We generally want to avoid `panic` here and if we do panic, we should make sure to include the underlying error.\r\n\r\nYou can use `?` operator here to bubble up the error if you add a new variant to the cli error type [here](https://github.com/vercel/turborepo/blob/main/crates/turborepo-lib/src/cli/error.rs#L21):\r\n```\r\n#[error(transparent)]\r\nInfo(#[from] info::Error),\r\n```\r\n```suggestion\r\n            info::run(base).await?;\r\n```",
        "Lets keep around the error as it could indicate some underlying issues with the system\r\n\r\n```suggestion\r\n        |e| format!(\"Cannot determine current binary: {e}\"),\r\n```",
        "Can we keep the error messaging from `map_environment` in `env.rs`? The `unwrap`s will just report `tried to unwrap None` which isn't helpful for end users.",
        "There was an existing bug where we would ignore any errors that came from resolving config options from a source. In reality since these were (mostly) eagerly calculated it only affected and validation errors from extracting config options from a `turbo.json`.\r\n\r\nWe now bubble up any errors generated by resolving config options."
      ],
      "turborepo-choose-logging-levels-wisely": [
        "Maybe add in the task id here\r\n```suggestion\r\n        debug!(\"{}: files to cache: {:?}\", self.task_id, files_to_be_cached.len());\r\n```"
      ],
      "turborepo-descriptive-unambiguous-identifiers": [
        "I find this name a little confusing. Does `clear_pinned_task` express the behavior better?\r\n```suggestion\r\n    fn update_task_selection_pinned_state(&mut self) {\r\n        // Preferences assume a pinned state when there is an active task.\r\n        // This `None` creates \"un-pinned-ness\" on the next TUI startup.\r\n        self.preferences.set_active_task(None);\r\n    }\r\n```",
        "Minor: can we rename to `create_config` instead of `config_init` if we're making this public? `config_init` naming only makes sense when we used it in `.get_or_init`",
        "Most of these variants seem more focused on why a package is included in a filter rather than \"what has changed\".  Is `PackagePresenceReason` maybe a more accurate name for this enum?",
        "Should this be `direct_dependencies`? Gives us the nice `direct_dependencies U indirect_dependencies = dependencies` relation"
      ],
      "turborepo-handle-errors-appropriately": [
        "I think we would want to abort if we are missing/can't read the root `package.json`"
      ],
      "turborepo-validate-performance-impact-first": [
        "I think this is okay since https://github.com/vercel/turborepo/pull/9123 has been merged.\r\n\r\n`vt100` used to need to iterate through every scrollback row in order to render a visible row so 1024->2048 would be a noticeable perf hit. I will do a quick profile to check if there's a noticeable impact from this change.",
        "Did a test and this is a :shipit: no noticeable perf change",
        "```suggestion\r\n                let mut map: HashMap<&AnchoredSystemPath, (GitHashes, Vec<_>)> = HashMap::with_capacity({\r\n                    let (lower, upper) = package_roots.size_hint();\r\n                    upper.unwrap_or(lower)\r\n                });\r\n```",
        "~~Nit~~ Learning opportunity:\r\nGenerally one should prefer `&str` to `&String` as the latter has to follow jump through memory twice:\r\n - first to dereference the `&String` to `String`\r\n - then to read the pointer in the `String` object that points to the actual bytes\r\n\r\n`&str` on the other hand only has the pointer to the actual bytes\r\n\r\nExceptions to the rule: You need to mutate the string",
        "This change means the function now returns an iterator over all task names instead of an vector of them. Beneficial to us as it results in far fewer allocations (4 vector allocations + 1 allocation per task) to zero allocations."
      ],
      "turborepo-validate-configurations-comprehensively": [
        "We throw if you use legacy cache options with `TURBO_CACHE`:\r\n```\r\n[0 olszewski@chriss-mbp] /Users/olszewski/code/vercel/turborepo $ TURBO_REMOTE_CACHE_READ_ONLY=1 turbo_dev @turbo/types#lint --cache=remote:rw > /dev/null\r\n WARNING  No locally installed `turbo` found. Using version: 2.2.4-canary.9.\r\nturbo 2.2.4-canary.9\r\n\r\n WARNING  TURBO_REMOTE_CACHE_READ_ONLY is deprecated and will be removed in a future major version. Use TURBO_CACHE=remote:r\r\n  x Cannot set `cache` config and other cache options (`force`, `remoteOnly`,\r\n  | `remoteCacheReadOnly`) at the same time\r\n```",
        "```suggestion\r\n            // there can also be a TURBO_TEAM, so we'll use that as well\r\n            output.team_slug = input.TURBO_TEAM;\r\n```",
        "So something I missed: when we add new keys/fields to `turbo.json` we should always test both deserialization and serialization. When we don't we often create issues ([most recent issue caused by me forgetting to test this](https://github.com/vercel/turbo/issues/8311))",
        "This is necessary due to older versions of `pnpm` not having https://github.com/pnpm/npm-conf/pull/10 which results in `pnpm` crashing if `APPDATA` isn't defined.\r\n\r\nTechnically we could have users define this instead of baking it into `turbo` itself, but considering the popularity of `pnpm` and the amount of digging it took to find that PR, it seems nice to include it."
      ],
      "turborepo-eliminate-code-duplication": [
        "Can you remove this line? This is already done by the `..Self::default()` on the following line.",
        "Not blocking, but would love to dedupe this logic with `Config::root_turbo_json_path` instead of copy-pasta. Would need to break out the behavior to a new static function.",
        "We can just borrow here instead of cloning\r\n```suggestion\r\n        let repo_root = &self.repo_root;\r\n```",
        "Up to you, but you can dedupe the spacing logic so it's only contained in `build_message_vec`:\r\n```\r\nlet build_message_vec = |footer_texts: &[&str]| -> Line {\r\n    let mut messages = Vec::new();\r\n    messages.extend_from_slice(footer_texts);\r\n```\r\n\r\n```suggestion\r\n            LayoutSections::Pane => build_message_vec(&[EXIT_INTERACTIVE_HINT]),\r\n            LayoutSections::TaskList => {\r\n                // Spaces are used to pad the footer text for aesthetics\r\n                build_message_vec(&[ENTER_INTERACTIVE_HINT, SCROLL_LOGS])\r\n            }\r\n```",
        "I don't see this used anywhere so I think it can get removed? In general I think we don't want to be changing the counts on the `ExecutionSummary` and have all modifications happen to `SummaryState`"
      ],
      "turborepo-use-functional-null-handling": [
        "We can get rid of the unwrap by converting these `Result<T,E>` to `Option<T>` since we're already not doing anything with the error type\r\n```suggestion\r\n    let package_manager = PackageJson::load(&base.repo_root.join_component(\"package.json\"))\r\n        .ok()\r\n        .and_then(|package_json| {\r\n            PackageManager::read_or_detect_package_manager(&package_json, &base.repo_root).ok()\r\n        })\r\n        .map_or_else(|| \"Not found\".to_owned(), |pm| pm.to_string());\r\n```",
        "Maybe keep this in case anyone is parsing summary/dry JSON and expecting the user to be present?"
      ]
    },
    "profile": {
      "location": "Cleveland",
      "company": "@vercel ",
      "blog": "",
      "site_admin": false,
      "followers": 71,
      "following": 0
    }
  },
  "ritchie46": {
    "repos": [
      "pola-rs/polars"
    ],
    "entries": [
      {
        "slug": "polars-appropriate-error-handling",
        "title": "Appropriate error handling"
      },
      {
        "slug": "polars-ci-workflow-configuration-best",
        "title": "CI workflow configuration best"
      },
      {
        "slug": "polars-consistent-naming-standards",
        "title": "Consistent naming standards"
      },
      {
        "slug": "polars-defer-expensive-operations",
        "title": "Defer expensive operations"
      },
      {
        "slug": "polars-evaluate-algorithmic-complexity-tradeoffs",
        "title": "Evaluate algorithmic complexity tradeoffs"
      },
      {
        "slug": "polars-explicit-configuration-precedence",
        "title": "Explicit configuration precedence"
      },
      {
        "slug": "polars-explicit-null-handling",
        "title": "Explicit null handling"
      },
      {
        "slug": "polars-extract-duplicated-code",
        "title": "Extract duplicated code"
      },
      {
        "slug": "polars-favor-clarity-over-brevity",
        "title": "Favor clarity over brevity"
      },
      {
        "slug": "polars-feature-flag-compatibility",
        "title": "Feature flag compatibility"
      },
      {
        "slug": "polars-hide-implementation-details",
        "title": "Hide implementation details"
      },
      {
        "slug": "polars-names-reveal-clear-intent",
        "title": "Names reveal clear intent"
      },
      {
        "slug": "polars-optimize-data-transformations",
        "title": "Optimize data transformations"
      },
      {
        "slug": "polars-optimize-memory-allocation-patterns",
        "title": "Optimize memory allocation patterns"
      },
      {
        "slug": "polars-organize-tests-efficiently",
        "title": "Organize tests efficiently"
      },
      {
        "slug": "polars-prevent-cryptic-errors",
        "title": "Prevent cryptic errors"
      },
      {
        "slug": "polars-prevent-deadlock-conditions",
        "title": "Prevent deadlock conditions"
      },
      {
        "slug": "polars-safe-null-handling",
        "title": "Safe null handling"
      }
    ],
    "comments": {
      "polars-organize-tests-efficiently": [
        "Can this be done in-memory with `io.BytesIO()`. We prefer in-memory tests when possible as this is faster."
      ],
      "polars-consistent-naming-standards": [
        "`use_abs_path`.\r\n\r\nWe have a convention that separate words are `_` separated. ",
        "Can we use python `None`  directly (so not part of this set.\r\n\r\nThe argument then becomes `arg: MaintainOrder | None`\r\n\r\nPS. I also think we should name it `MaintainOrderJoin`"
      ],
      "polars-defer-expensive-operations": [
        "I don't think we need an extra benchmark for this.",
        "No, but the goal isn't to hit everything with benchmarks. ",
        "I understand what benchmarks do. :) I think the change is good, but I want to get rid of the in-repo benchmarks, I am not happy with them on the shared runners. We have on-premise benchmarks running. \r\n\r\nCan you remove this?"
      ],
      "polars-ci-workflow-configuration-best": [
        "Don't publish to pypi if this fails.",
        "Don't we need to include all variants then? (E.g. is the exclude set not smaller than the include set?)",
        "Right, I think you're right. I also do need to think harder with the include set. :laughing: "
      ],
      "polars-feature-flag-compatibility": [
        "The feature is `bigidx`, not u64 idx in Rust."
      ],
      "polars-optimize-data-transformations": [
        "You don't have to:\r\n\r\n\r\n```rust\r\n matches!(\r\n        function,\r\n        FunctionExpr::Range(RangeFunction::IntRange { .. })\r\n    );\r\n```\r\n",
        "I think we should do this check during the IR::conversion, in `resolve_groupby` here:\r\n\r\nhttps://github.com/pola-rs/polars/blob/05f2abbf1b7f76f0b34c3c552fc33aa6da186561/crates/polars-plan/src/plans/conversion/dsl_to_ir.rs#L1013",
        "Ah, right. I see that we first do a `with_columns` here. We should store the `index` column on line 1155 and do the `with_column` rewrite during IR conversion.\r\n\r\nI understand it's a bit more than you anticipated. I can do the pre-work for that later if you like?",
        "Don't create `tmp_df` as what it does now.\r\n\r\nLet's say we have `pivot_df: A, B, C` and we create `value_col: A` from `pivot_df`. By concatting/hstacking we create:\r\n\r\n`tmp_df: A, A, B, C`. Where we just want to pass the context of `pivot_df` to the aggregation. So we can pass  `pivot_df` directly to `expr.evaluate`.",
        "Could you clarify how you distinct \"scanned\" from \"read\"? \r\n"
      ],
      "polars-optimize-memory-allocation-patterns": [
        "Maybe we can hoist the writer out of the while loop?"
      ],
      "polars-safe-null-handling": [
        "We can check `is_nan` on the floats.\r\n\r\nWe can make our own `NonNan` wrapper type in `polars-utils`, which does this check on entry.",
        "No need to use unsafe for a single value.",
        "No need to use unsafe for a single value.",
        "No need to use unsafe for a single value.",
        "No need to use unsafe for a single value.",
        "No need to use unsafe for a single value.",
        "Can this return `Option<bool>`? On `None` we don't know and we must continue. But on `Some<false>` we could already error early."
      ],
      "polars-explicit-null-handling": [
        "This is wrong. If our sum doesn't ignore nulls, it doesn't propagate them, but replaces them with the identity: 0.\r\n\r\nThe horizontal semantics should be the same as the vertical semantics.",
        "I mean that our `sum` is agnostic to nulls. I think we made a mistake exposing this to `sum_horizontal` as our vertical sum is agnostic to nulls.",
        "Yeah, I think you're right. Consider it an observation. ;) Will take a look a bit later. "
      ],
      "polars-explicit-configuration-precedence": [
        "We should soften this guarantee. We will try to run on this engine, but don't guarantee it. Both streaming and gpu have cpu fallbacks."
      ],
      "polars-appropriate-error-handling": [
        "If it is an implementation error on our end, we can panic.",
        "Instead of panicking, this should just raise an unsupported error. I don't want to put PR links in error messages either."
      ],
      "polars-prevent-cryptic-errors": [
        "The guard position should be at the generic location. I believe that is in `_parse_inputs_as_iterable`.",
        "Yes, that's good. In `select` we also should not accept dictionaries."
      ],
      "polars-extract-duplicated-code": [
        "Can we move this to a separate function so that implementation is separate from dispatch?",
        "Can we add a `schema: Option<Schema>` argument to `prepare_expression_for_context` to reduce duplication.",
        "This is also converted in `IR::Scan` can we factor this out into a function?",
        "Can we do that in this PR? \r\n\r\nMake a function and factor out the shared arguments?",
        "I believe this code is exactly the same as in collect. Can we put it in a function?"
      ],
      "polars-names-reveal-clear-intent": [
        "Given that our schema conflicts with the catalog schema definition. Shall we name it `create_namespace` and mention in the docstrings that we mean catalog schema's for that?\r\n"
      ],
      "polars-evaluate-algorithmic-complexity-tradeoffs": [
        "I do wonder though if it is worth the extra binary bloat. The `is_nan` will be correctly predicted on every non-nan, until we hit it. On that mis prediction we are done, so I think it doesn't really matter and we can save a monomorphized function.",
        "This allocates a new vec. We should be able to gather without reallocing.",
        "I think this should be supported. For decimals we should extract the `Int128` in both the haystack and the needle. \r\n\r\nAnd for the nested types we should convert both to the row-encoding."
      ],
      "polars-favor-clarity-over-brevity": [
        "Let's make this required keyword arguments:\r\n\r\n`self, * , dtype: PolarsDataType | type[Any], endianness: Endianness = \"little\"`"
      ],
      "polars-hide-implementation-details": [
        "We call that the shape in public API of Polars:\r\n\r\nhttps://docs.pola.rs/api/python/stable/reference/api/polars.datatypes.Array.html\r\n\r\nNit:\r\n\r\nI also would not mention that array's are sequentially nested. I want users to think of NDArrays. That we sequentially nest them is an implementation detail.",
        "The Polars public API strongly prefers full names. So this should be `\"ir\", \"physical\"`. (I think IR is common enough to not write it out. :) )"
      ],
      "polars-prevent-deadlock-conditions": [
        "We should use the `enter_polars` which handles the `allow_threads`."
      ]
    },
    "profile": {
      "location": "Utrecht",
      "blog": "https://www.ritchievink.com",
      "site_admin": false,
      "followers": 1562,
      "following": 43
    }
  },
  "kchibisov": {
    "repos": [
      "alacritty/alacritty"
    ],
    "entries": [
      {
        "slug": "alacritty-assess-security-trade-offs",
        "title": "Assess security trade-offs"
      },
      {
        "slug": "alacritty-avoid-unnecessary-operations",
        "title": "avoid unnecessary operations"
      },
      {
        "slug": "alacritty-avoid-unwrap-on-nullables",
        "title": "Avoid unwrap on nullables"
      },
      {
        "slug": "alacritty-centralize-workspace-dependencies",
        "title": "centralize workspace dependencies"
      },
      {
        "slug": "alacritty-choose-familiar-intuitive-names",
        "title": "Choose familiar, intuitive names"
      },
      {
        "slug": "alacritty-configuration-documentation-accuracy",
        "title": "Configuration documentation accuracy"
      },
      {
        "slug": "alacritty-configuration-validation-feedback",
        "title": "Configuration validation feedback"
      },
      {
        "slug": "alacritty-consistent-error-handling",
        "title": "consistent error handling"
      },
      {
        "slug": "alacritty-consistent-naming-conventions",
        "title": "Consistent naming conventions"
      },
      {
        "slug": "alacritty-document-configuration-specifics",
        "title": "Document configuration specifics"
      },
      {
        "slug": "alacritty-explain-code-intent",
        "title": "Explain code intent"
      },
      {
        "slug": "alacritty-follow-metadata-specifications",
        "title": "Follow metadata specifications"
      },
      {
        "slug": "alacritty-keep-documentation-together",
        "title": "Keep documentation together"
      },
      {
        "slug": "alacritty-optimize-algorithmic-efficiency",
        "title": "optimize algorithmic efficiency"
      },
      {
        "slug": "alacritty-platform-specific-api-documentation",
        "title": "Platform-specific API documentation"
      },
      {
        "slug": "alacritty-prefer-early-returns",
        "title": "prefer early returns"
      },
      {
        "slug": "alacritty-unsafe-code-practices",
        "title": "unsafe code practices"
      },
      {
        "slug": "alacritty-use-constraining-types",
        "title": "Use constraining types"
      },
      {
        "slug": "alacritty-use-descriptive-contextual-names",
        "title": "Use descriptive contextual names"
      },
      {
        "slug": "alacritty-write-audience-appropriate-documentation",
        "title": "Write audience-appropriate documentation"
      }
    ],
    "comments": {
      "alacritty-consistent-error-handling": [
        "No need to do any sort of panics like that it's just error prone, simply handle good value and use prefer otherwise use standard behavior.\r\n\r\nSo it'll be \r\n```rust\r\nOk(\"1\") => DisplayApiPreference::EglThenGlx(Box::new(x11::register_xlib_error_hook)), // Or wgl\r\n_ => // default\r\n```",
        "Do we really want to do `process::exit(1)` instead of doing the flow we have with any other command with returning an error? ",
        "I mean, it's a bit weird that we have normal error handling in other sub command and have lazy `eprintln` + `exit` here.",
        "That's true, but the same you could say for `msg`, yet `msg` propagates error, they are being called from the exact same `match` handler in the end.",
        "I'd still suggest to be consistent with other subcommands.",
        "there's only one place and it's an event loop builder, but I think message box won't show if you don't build event loop on windows. Not sure about that though.\r\n\r\nIt's also tricky that you need to call it before `FreeConsole`, otherwise you won't log into pty, so I don't see other place you'd through it on windows.\r\n\r\nThe good thing about side stepping panic like that, that it's visible for the users so they can report something at least. ",
        "The fatal error could be due to crash of the event loop, so it's effectively not possible to do so.\r\n\r\nOnly windows always has windows around, but not X11/Wayland, you need to somehow draw that sort of thing. In general, you should send information to crash report service, which is system log, basically, so you'll see crashes inside the `journalctl`, etc, since rust's panic doesn't do that sort of thing.\r\n\r\n",
        "You should run `FreeConsole` from inside the `fn main`, from the `match` there you should get a `let result` and then do the same panic logic there.",
        "Also, do we really need a `panic`? Can't we call some `Win32` API function with panic message and do a normal return?"
      ],
      "alacritty-configuration-documentation-accuracy": [
        "```suggestion\r\n\t\tIf the dim foreground color is not set, it is automatically\r\n```\r\n\r\nI think it's `dim`?",
        "Also, the default in the config is `None`, when you look into color.rs, so it could be changed either here or in the configuration to specify the value explicitly. It's also a bit weird that we have `Dim` colors defined in the config, but not use `None`. \r\n\r\nit's also weird that we have `Dim` colors defined in the `color.rs`, but they actually never used, but still being computed on demand. Should likely remove definition from `color.rs` for them while at it.",
        "```suggestion\r\n*position* \"None\" | { x = <integer>, y = <integer> }\r\n\r\n\tWindow startup position\r\n\r\n\tSpecified in number of pixels.\r\n\r\n\tIf the position is _\"None\"_, the window manager will handle placement.\r\n\t\r\n\tDefault: _\"None\"_\r\n```",
        "```suggestion\r\n\t\t\tUse _-1_ to apply this change to all windows.\r\n```"
      ],
      "alacritty-avoid-unnecessary-operations": [
        "why this got changed?",
        "Hm, the reason this works is because the logic is bit inconsistent. We basically set the cursor from the renderer and from this function, so that's why the code here works even though it doesn't check the regular region anyway.\r\n\r\nIn the current state of things you can remove all the logic around `hyperlink` and `::Pointer` and it'll still work. \r\n\r\nI don't really like that it's like that though since it's a bit error prone?",
        "Pretty much yes. I'm not sure what to do about this function though, because clearly the check could be removed, though, maybe there's an edge case..."
      ],
      "alacritty-unsafe-code-practices": [
        "You'd be surprised, but we already do that. It's unsafe because it touches env variables, as in mutates the global state. You can't do that transparently, because it's again, global variables and it's all racy.\r\n\r\nFor example if you try to create a window and spawn the process at the same time, winit could implicitly remove the variables, and your process won't activate, so that's why it's all explicit.",
        "I mean, the function simply calls `std::env::remove_var` that's all it does and that's the reason it's unsafe, because it removes the `env` which you could use for launching, so you should take care. Nothing will really break or you can't lead to undefined state, it's just global state mutation basically, which is fine in application code, but I think is unsafe in library code, because it could remove a state set by application.",
        "mutates a global state. The variables may change over time. I don't understand what's wrong with adding `unsafe` on a function you where you should at least read what it does before using. ",
        "The issue is that it's not like the mutation itself is unsafe, it's just that if you spawn a process using those variables from thread `A` and then try to remove them on thread `B`, you could simply remove them for `A` and it won't start up notify anything, because the tokens were simply removed.\r\n\r\nSo the extra care should be taken and set and extra synchronization on top of that should be used.",
        "Like it won't break anything or result in UB, it just you could have sometimes things launched with the wrong env, if you don't add extra `Mutex` to sync threads here, thus it was decided to have it unsafe in winit.",
        "```\r\nThe unsafe keyword has two uses:\r\n\r\n    to declare the existence of contracts the compiler can’t check (unsafe fn and unsafe trait),\r\n    and to declare that a programmer has checked that these contracts have been upheld (unsafe {} and unsafe impl, but also unsafe fn – see below).\r\n```\r\n\r\nSo it clearly a contract which compiler can't check."
      ],
      "alacritty-platform-specific-api-documentation": [
        "Maybe something like that?\r\n\r\n```suggestion\r\n    Use EGL as display API if the current platform allows it. Note that\r\n    transparency may not work with EGL on Linux/BSD.\r\n```",
        "```suggestion\r\n\tOn Wayland the general class sets the _app\\_id_, while the instance class is ignored.\r\n```"
      ],
      "alacritty-follow-metadata-specifications": [
        "I guess one that could work here is MIT, since it's all permissive and compatible with APACHE, given that we don't use copyleft, requiring permission of the author is not needed, thus even cc may work.",
        "The current text was mostly written by @chrisduerr according to git blame, with the exception on how markup is placed. I'm not sure there's a need to contact original author, if most of the lines are changed and only _standard defined markup_ was left in place, which is there just because of standard and you can not really change it. One can update `keywords`.\r\n\r\nThe copyright at the top also a bit doesn't really matter, since there's no entity like alacritty project contributors, since there's no CONTRIBUTORS.md or other entity to contact/etc. Though, on github one can say that it's contributors that popup from the contributors menu, but it was a topic of the past as well iirc.\r\n\r\nAlso, is this field mandatory? ",
        "we're not dual licensed, that's the thing if we were, that was not a problem.",
        "they just lay there, but the application of them is explicitly specified. this particular file is apache licensed anyway.",
        "The issue is that you still need to ask actually, even if they are compatible.",
        "alacritty is apache-2.0 only software, and always been. MIT is used by 2 small crates and it lays in project root only because it's simple to link to it.",
        "The person who originally submitted it, which is no longer on github and has no email, that one should be asked, as it was outlined above. we can not contant them, so nothing could be done unless the file is rewritten."
      ],
      "alacritty-keep-documentation-together": [
        "But the whole point was to make it like that so you can copy sections into the config and probably assemble the config out of man page pretty reliably?  I don't see any issue with sections like that."
      ],
      "alacritty-document-configuration-specifics": [
        "```suggestion\r\n## 0.25.0-dev\r\n\r\n### Changed\r\n\r\n- Replaced `Options::hold` with `Options::drain_on_exit` that drains, but doesn't hold, since holding could be easily done by user.\r\n```"
      ],
      "alacritty-configuration-validation-feedback": [
        "Don't set a variable like that, it's user controllable. So completely remove those lines.",
        "I actually had a setting for a year which was unused, because I've added it during `dev` and then we changed it at the last second and I forgot about it.\r\n\r\nProbably should add that we warn on unused config keys?",
        "In the `CHANGELOG`."
      ],
      "alacritty-use-constraining-types": [
        "Could you use the actual type of this value? `LPDWORD` or however windows-sys calls it.",
        "then it's fine."
      ],
      "alacritty-use-descriptive-contextual-names": [
        "This should be called just `level`, since it's in `window` section already.",
        "I could name it `winit_event`.",
        "Could you name the variables based on the location in the shape, like `top-left`, `top-right`, `mid-left`, etc?"
      ],
      "alacritty-choose-familiar-intuitive-names": [
        "I think it should be called `CreateNewWindow`, I do understand why spawn was used, but I think it's more natural to create windows and not spawn them.",
        "Could add a default binding to open a new window for each platform, like command + n for macOS. I think there's something common for such things on Windows as well. For Linux could add `ctrl + shift +n`.",
        "I disagree. I'd say that we should use `CreateWindow` for  `command + n` on macOS, since we're using `SpawnNewInstance` right now, which is not desired action for the standard hotkey on macOS.",
        "My point is that it's expected behavior on macOS, since the current one fills up the dock. But if you don't want to change the behavior we might just merge it as it is right now, since it's been working quite stable for me(I'm not sure that I've ever seen a crash on this PR).",
        "I just copied the exact same line from the different option, so I'd change both of them."
      ],
      "alacritty-prefer-early-returns": [
        "Last time the style for thing like that was the one I picked.\r\n\r\nI can do early return, but I don't think in this particular case it's really needed, because indention is low."
      ],
      "alacritty-explain-code-intent": [
        "Document why it's done like that. Like it's not really clear why you need `0`, `0` at the end."
      ],
      "alacritty-consistent-naming-conventions": [
        "it's sort of weird that we have places with `none` starting with capital later and some without. Probably should be `None` for consistency."
      ],
      "alacritty-centralize-workspace-dependencies": [
        "It seems like our deps are using dirs 2, and given that dirs are unmaintained, we should either use dirs 2, or use `dirs-next`, since you're adding one more dependency anyway.\r\n\r\nIn general, we can go for dirs 2 just to not add a dependency for now.",
        "Seems fine to me."
      ],
      "alacritty-assess-security-trade-offs": [
        "Actually, could you roll back this part of the update? We'll drop yml anyway, and I don't want to deal with this mess. It's a libyaml transpiled into rust, so basically a 'nice way to add a security nightmare' sort of things.",
        "I mean, they are both not maintained. It's just one is a massive transpilation of a C library, and the rust crate is a small safe rust parser. They're not maintained from the same year."
      ],
      "alacritty-avoid-unwrap-on-nullables": [
        "Yeah, maybe checking for empty should be better on callee side.",
        "I still can't remove the len check due to compose input, since it could be an actual string in the end of the day."
      ],
      "alacritty-optimize-algorithmic-efficiency": [
        "What the reason to check `is_some`, I think it's already covered? ",
        "Could you simply take the code which was there before and do 2 loops? The amount of bindings is very low so it's better than `vec` approach.\r\n\r\nJust do normal loop without `mouse_mode` part and set some flag to `true` when you actually triggered some binding.\r\n\r\nAnd then, `if mouse_mode && !sent_binding` do the ho the same loop, but with `SHIFT` applied? First loop won't need to `clone` even, I think.",
        "The `find_map` finds next unique hyperlink, the `while let` loop finds the end of that particular hyperlink, the point here is to compute meaningful bounds.\r\n\r\nHowever what we can do if we don't care about bounds at all is to always use `find_map` and report ranges for hint as 1 cell. That should probably make some sense, I guess?",
        "We can, but of form `Option<Vec<char>>; 2`, since we should occupy the entire it? I can adjust that, I think."
      ],
      "alacritty-write-audience-appropriate-documentation": [
        "I'd suggest migration for bigger stuff, you can look at the changelog we have in winit now. For such a small change, I don't think you need it?\r\n\r\nhttps://github.com/rust-windowing/winit/blob/master/src/changelog/unreleased.md\r\n\r\n",
        ">I mean that's exactly the issue right there. A ton of changes but no clear indication which changes require my attention.\r\n\r\nIt has a big chunk of code and explanation for what you need. So it's just 2 changes where you need attention.\r\n\r\nWe can do a separate section with breaking changes.",
        ">It doesn't though. It states nowhere that these are the only changes that require my attention. It just tells me what I need to do for those.\r\n\r\nHm, that's fair, because it'll start to warn you, since we don't remove the API. For real breaking change I just did `bold` iirc.\r\n\r\n>Do you suggest adding an extra section with its own Fixed/Added/etc?\r\n\r\nJust `Breaking`.",
        "Maybe something like that\r\n\r\n```markdown\r\n**_I'm on Windows and applications don't work like in other terminals?_**\r\n\r\nAlacritty on Windows is relying on the system libraries by default, which are\r\nquite out of date most of the time, for example your mouse might not work in\r\nvim on Windows 10. Other applications, like Windows Terminal, ship updated\r\nversions of the required libraries which are known to work better, however\r\nthey are not officially provided by the microsoft [yet].\r\n\r\nAlacritty doesn't ship those libraries, but it could still load them if they are\r\npresent in your `PATH`. Some other terminal emulators for windows do build their\r\nown copies of such libraries, so you could take those libs from them.\r\n\r\n\r\n[yet]: https://github.com/microsoft/terminal/issues/15065\r\n\r\n```"
      ]
    },
    "profile": {
      "location": "Tokyo, Japan",
      "blog": "",
      "site_admin": false,
      "followers": 149,
      "following": 4
    }
  },
  "jasnell": {
    "repos": [
      "expressjs/express",
      "nodejs/node"
    ],
    "entries": [
      {
        "slug": "express-structured-release-workflows",
        "title": "Structured release workflows"
      },
      {
        "slug": "node-await-all-promises",
        "title": "Await all promises"
      },
      {
        "slug": "node-behavior-focused-test-design",
        "title": "Behavior-focused test design"
      },
      {
        "slug": "node-benchmark-before-optimizing-code",
        "title": "Benchmark before optimizing code"
      },
      {
        "slug": "node-descriptive-function-names",
        "title": "Descriptive function names"
      },
      {
        "slug": "node-document-non-intuitive-code",
        "title": "Document non-intuitive code"
      },
      {
        "slug": "node-document-with-precise-accuracy",
        "title": "Document with precise accuracy"
      },
      {
        "slug": "node-evolve-return-values",
        "title": "Evolve return values"
      },
      {
        "slug": "node-follow-consistent-naming-patterns",
        "title": "Follow consistent naming patterns"
      },
      {
        "slug": "node-follow-naming-conventions",
        "title": "Follow naming conventions"
      },
      {
        "slug": "node-format-docs-for-readability",
        "title": "Format docs for readability"
      },
      {
        "slug": "node-idempotent-error-safe-disposers",
        "title": "Idempotent error-safe disposers"
      },
      {
        "slug": "node-informative-error-messages",
        "title": "Informative error messages"
      },
      {
        "slug": "node-limit-environment-variable-scope",
        "title": "Limit environment variable scope"
      },
      {
        "slug": "node-minimize-configuration-dependencies",
        "title": "Minimize configuration dependencies"
      },
      {
        "slug": "node-prefer-clarity-over-cleverness",
        "title": "Prefer clarity over cleverness"
      },
      {
        "slug": "node-propagate-errors-with-context",
        "title": "Propagate errors with context"
      },
      {
        "slug": "node-public-over-internal-apis",
        "title": "Public over internal APIs"
      },
      {
        "slug": "node-resource-aware-programming-patterns",
        "title": "Resource-aware programming patterns"
      },
      {
        "slug": "node-reuse-computed-values-efficiently",
        "title": "Reuse computed values efficiently"
      },
      {
        "slug": "node-scope-security-settings",
        "title": "Scope security settings"
      },
      {
        "slug": "node-standardize-null-pointer-checks",
        "title": "Standardize null pointer checks"
      },
      {
        "slug": "node-thread-safe-resource-management-patterns",
        "title": "Thread-safe resource management patterns"
      },
      {
        "slug": "node-use-appropriate-metric-types",
        "title": "Use appropriate metric types"
      },
      {
        "slug": "node-use-modern-c-features",
        "title": "Use modern C++ features"
      },
      {
        "slug": "node-validate-network-request-parameters",
        "title": "Validate network request parameters"
      },
      {
        "slug": "node-version-apis-with-care",
        "title": "Version APIs with care"
      }
    ],
    "comments": {
      "node-validate-network-request-parameters": [
        "I might have missed it elsewhere, but we need to make sure that `requestHost`, `reqOptions.port`, `reqOptions.host`, `auth` etc are validated here to not include and invalid characters (like `\\r` and \\n`) in order to prevent any kind of request smuggling.",
        "If these are checked somewhere else, then a comment in here indicating where would be helpful for people coming into this code later."
      ],
      "node-await-all-promises": [
        "```suggestion\r\n  const { promise, resolve, reject } = Promise.withResolvers();\r\n  fs.stat(filePath, { signal }, (err, stats) => {\r\n    if (err) {\r\n      return reject(err);\r\n    }\r\n    resolve(stats);\r\n  });\r\n\r\n  await assert.rejects(promise, { name: 'AbortError' });\r\n```",
        "should the read at least be awaited?",
        "It doesn't necessarily need to be done in this PR (but would need to be done before this graduates from experimental), but we should make sure that this plays well with AsyncLocalStorage. Specifically, something like the following should work:\r\n\r\n```\r\nconst als = new AsyncLocalStorage();\r\nals.run(123, () => {\r\n  navigator.locks.request('exclusive-test', async (lock) => {\r\n    assert.strictEqual(als.getStore(), 123);\r\n  });\r\n});\r\n```\r\n\r\nThis would suggest that the lock request is capable of capturing the current async context frame and restoring it when the lock has been acquired and the callback is invoked."
      ],
      "node-scope-security-settings": [
        "I think I'd much prefer this to be set as an option on an individual connection rather than programmatically impacting all connections. A cli flag is one thing because it's set by the individual running the app. This API could be set by dependencies impacting global state without the application being aware of it. Setting it per connection seems the safest. ",
        "I won't block but I'm still unconvinced this is a good thing to add."
      ],
      "node-thread-safe-resource-management-patterns": [
        "This pattern looks... odd... and likely a bit error prone. We generally prefer the use of smart pointers to explicit use of `delete` and this could use some comments around it so it's clear what is happening here.",
        "These's aren't correct here. You really should not be holding a `v8::Global<T>` inside a `v8::External`, then deleting it like this. It's fine to use indirection through another type... e.g. having the `External` hold an instance of a struct that is holding the `Global<T>`.",
        "I ought to be able to take another look this afternoon. Just catching up after two week of being out of the office ;-) ... just mentioning because I didn't want you to feel I was ignoring your pings on this ;-) "
      ],
      "node-informative-error-messages": [
        "Here I would create the error outside the `process.nextTick(...)` so that it has a useful stack trace attached to it.",
        "It would likely be worthwhile to assign a warning code to this warning so that it can be suppressed with the `--dsable-warning` CLI flag.\r\n\r\n![image](https://github.com/user-attachments/assets/ec94215e-e7f7-4729-9c02-69bf1c433d2f)",
        "That's reasonable. Updated",
        "Likely yes but I think it's reasonable to throw a more specific error here."
      ],
      "node-evolve-return-values": [
        "I don't think the `enableHelpPrinting` here is a good idea. Instead, the return value of `parseArgs` should just be capable of returning the serialized help text. Allowing for something like,\r\n\r\n```\r\nconst result = parseArgs(...);\r\nconsole.log(result.printUsage());\r\n```",
        "We can have something, yes, but having these kinds of side effects on the constructor can be problematic",
        "Hmm.. good question. I really don't know.",
        "```suggestion\r\n   return type of an existing API is a breaking change.\r\n4. When an existing API signature does not lend itself easily to supporting making\r\n    the return value disposable and a new API needs to be introduced, it is worth\r\n    considering whether the existing API should be deprecated in favor of the new.\r\n    Deprecation is never a decision to be taken lightly, however, as it can have major\r\n    ecosystem impact.\r\n```",
        "This is likely something that can be best handled in documentation than code. For instance, in code we can have it be a fully anonymous object:\r\n\r\n```js\r\nreturn {\r\n  dispose() { ... },\r\n  [Symbol.dispose]() { this.dispose(); }\r\n}\r\n```\r\n\r\nWhile in documentation is can at least *appear* to be a named interface:\r\n\r\n```markdown\r\n### `foo()`\r\n\r\n* Returns {Disposable}\r\n\r\n### `Disposable`\r\n\r\n#### `disposable.dispose()`\r\n\r\n#### `disposable[Symbol.dispose]`\r\n```\r\n\r\nSo I'd suggest that we're talking about a documentation difference here, not necessarily a coding difference.",
        "I'm not suggesting that we would leave things undocumented. I'm saying that we don't always need formal classes in the actual implementation. For some return values we can safely rely on documentation-only and leave the actual implementation as anonymous objects. We even have existing precedence for this in the current documentation. See, for instance, all of the \"Class\" documentations for Web Crypto operations here: https://nodejs.org/docs/latest/api/webcrypto.html#algorithm-parameters ... the things like `AesDerivedKeyParams` is documented as a class but, in reality, there is no actual class named `AesDerivedKeyParams` in the source.",
        "I've expanded the document to include coverage of documentation of anonymous dispoables."
      ],
      "node-propagate-errors-with-context": [
        "These might need to be handled separately. If the first call succeeds, then the first created function will take ownership over the `fulfill_holder`. If the second call then fails for whatever reason, we are deleting the `fulfill_holder` while it's external is still holding the reference to it that it assumes it owns. It would be best to separate this into two separate calls rather than aggregating them together like this. Create one, create it's external and it's function, then create the second...\r\n\r\nOr, can we at least be certain that we won't end up with a free-after-free type error when deleting these while the External is still holding them?",
        "Not something to do here, but using `Check()` here has the same issue as using `ToLocalChecked()` in that it will just crash the process rather than propagate the error. This is a common issue throughout the code, however, so not something I would block this PR on. We need to handle these better in general. \r\n\r\nIf you did want to handle this here, then changing these to check if the return value is empty then doing some proper error propagation similar to the way the ToLocal(...) results are handled would be ideal.",
        "We should avoid using `USE` for the same error propagation reasons. Calling `Then(...)` can cause a JavaScript error to be scheduled. USE would cause that to be ignored when we ought to propagate it."
      ],
      "node-behavior-focused-test-design": [
        "Nit: you can use `deepStrictEqual` to compare the full `result` to an expectation",
        "The callback passed to the `then` should be wrapped with a `common.mustCall(...)`",
        "Wrap the callback in `common.mustCall(...)`"
      ],
      "node-idempotent-error-safe-disposers": [
        "```suggestion\r\n   closed. If there is no difference in disposing in success or exception contexts,\r\n   then separate disposal methods are unnecessary.\r\n```",
        "We can definitely soften this but I think the guideline is correct. Most of the time disposal should be sync as much as possible. There are likely exceptions to that rule, of course. Will think about how to soften this a bit to say it's ok to use but should be intentional?",
        "Consider the following two cases:\r\n\r\n```\r\n{\r\n  using foo = new MyDisposable();\r\n}\r\n\r\n{\r\n  using foo = new MyDisposable();\r\n  throw new Error('boom');\r\n}\r\n```\r\n\r\nThe disposer in each case is going to be called. Unfortunately, however, the disposer has no idea if there is an exception pending or not. Let's say our `MyDisposable` has two possible modes: one is a clean, graceful async shutdown, the second is a dirty, abrupt shutdown in case of an error. Since the disposer does not know whether it is being called with a pending exception or not we cannot safely assume within the disposer that a graceful async shutdown should be used. Instead, we have to assume it is an exception case.\r\n\r\n```js\r\nclass MyDisposer {\r\n  #closed = false;\r\n  #aborted = false;\r\n\r\n  close() {\r\n    if (this.#closed) return;\r\n    this.#closed = true;\r\n  }\r\n\r\n  abort() {\r\n    this.#aborted = true;\r\n    this.close();\r\n  }\r\n\r\n  [Symbol.dispose]() {\r\n    this.abort();\r\n  }\r\n}\r\n```\r\n\r\n",
        "> ... But for these I question if ERM is really the right way to go...\r\n\r\nGenerally I think we should be leaving the decision up to users whether to use ERM or not. These guidelines are more about what we need to do for enablement. ",
        "Updated the language on this in the doc, please take another look!",
        "```suggestion\r\n   be synchronous and immediate. Avoiding async disposal is not always possible,\r\n   however, as some types of disposable objects require asynchronous cleanup.\r\n```",
        "````suggestion\r\n```\r\n\r\nThis is because of the fact that, when the disposer is called, it has no way\r\nof knowing if there is a pending exception or not and it is generally safest\r\nto assume that it is being called in an exceptional state. While some types\r\nof disposable objects make no differentiation between dispose in success\r\nand dispose in exception cases, those that do otherwise have no way of\r\ndifferentiating the conditions from within the disposer itself.\r\n````",
        "```suggestion\r\n3. It is recommended to avoid throwing errors within disposers.\r\n   If a disposer throws an exception while there is another pending\r\n   exception, then both exceptions will be wrapped in a `SupressedError`\r\n   that masks both. This makes it difficult to understand the context\r\n   in which the exceptions were thrown.\r\n```",
        "```suggestion\r\n  let disposed = false;\r\n  return {\r\n    dispose() {\r\n      if (disposed) return;\r\n      diposed = true;\r\n      console.log('Resource disposed');\r\n    }\r\n    [Symbol.dispose]() {\r\n      this.dispose();\r\n    },\r\n  };\r\n```",
        "Hmm.. I'm not sure how best to change this to address your comment here. Would you mind if we push any update on this one to another edit PR?"
      ],
      "node-document-with-precise-accuracy": [
        "As I mentioned previously this test is only actually testing the behavior when `fs.stat` is called with an already aborted `AbortSignal`... The test description above should be updated to reflect that and I would add a comment about it in here. Not sure exactly how we can reliably test aborting the call while it is in flight since file systems have such broadly different performance characteristics and often these conclude so quickly that it rarely can be caught and canceled in time.",
        "Small typo in that, otherwise looks good\r\n\r\n```\r\n// This test verifies that fs.stat immediately throws an AbortError if the provided AbortSignal\r\n// has already been canceled. This approach is used because trying to abort an fs.stat call in flight\r\n// is unreliable given that file system operations tend to complete very quickly on many platforms.\r\n```",
        "```suggestion\r\n  * @param {string[]} data\r\n```",
        "This is such an unusual pattern to see that it likely warrants some code comments in here to explain why we're setting the prototype this way (as opposed to `class DOMException extends Error`."
      ],
      "node-follow-naming-conventions": [
        "For consistency, these should use lower-case, snake-case\r\n```suggestion\r\n  V(client_id_string, \"clientId\")                                              \\\r\n```",
        "For concepts like this, I'd prefer if we adopted a naming scheme like `IsCallable`"
      ],
      "node-follow-consistent-naming-patterns": [
        "Since these aren't static methods, the `b` should be lower-case and likely needs `make format-md` or `make lint` run\r\n\r\n```suggestion\r\n### `blockList.fromJSON(value)`\r\n```",
        "I'd prefer the constructor methods here to follow a naming pattern like `createCounter`, `createTimer`, etc, both for ergonomics and for consistency (see `createHistogram(...)` in `perf_hooks`.\r\n\r\n```js\r\nconst counter = createCounter('api.calls', { service: 'web' });\r\nconst timer = createTimer('api.request.duration', { service: 'web' });\r\n```",
        "Yeah I kind of picked `use` here intentionally because of that ;-) ... wanted to prompt a discussion about it."
      ],
      "node-document-non-intuitive-code": [
        "This could use a code comment to describe the differences between `hashDigest(...)` and `xosHashDigest(...)`"
      ],
      "node-use-appropriate-metric-types": [
        "One additional type of Guage that might be useful is a `HighwaterMarkGauge` whose value only every actually increases. For instance, suppose we are tracking deltas like:\r\n\r\n```js\r\ngauge.applyDelta(10);  // Records the stored highwater mark value as 10\r\ngauge.applyDelta(-5);  // Stored highwater mark value is still 10\r\ngauge.applyDelta(10);  // Stored highwater mark value is 15\r\ngauge.applyDelta(-15);  // Stored highwater mark value is still 15\r\ngauge.applyDelta(10);  // Stored highwater mark value is still 15\r\ngauge.applyDelta(10);  // Stored highwater mark value is 20\r\n```"
      ],
      "express-structured-release-workflows": [
        "Yeah not sure an npm org would work but a shared ID for publishing releases for express related stuff would be good. The shared secrets can be managed in a private repo the way we do shared secrets for core stuff. (There may\nalready be some tool for this somewhere)\n"
      ],
      "node-resource-aware-programming-patterns": [
        "Yeah, I'll be expanding the documentation before moving this PR out of draft. But feel free to offer suggestions ;-)",
        "```suggestion\r\nAn attempt was made to read a file larger than the supported 2 GiB limit for\r\n`fs.readFile()`. This is not a limitation of `Buffer`, but an internal I/O constraint.\r\nFor handling larger files, consider using `fs.createReadStream()` to read the\r\nfile in chunks.\r\n```"
      ],
      "node-use-modern-c-features": [
        "Nit... since we're finally up on c++20, we can start using the more condensed `namespace node::inspector::protocol {` syntax where appropriate.",
        "```suggestion\r\n  enum class Mode { Shared, Exclusive };\r\n```\r\n\r\nOptional style nit.",
        "Since we are standardized on c++20 now, this can be condensed to `namespace node::worker::locks {`",
        "We really shouldn't use `new` with `v8::Global`. These can just be `v8::Global` and we make use of move semantics. Instead of using `delete` with `v8::Global`, the correct way to clear them is to use their `reset()` method"
      ],
      "node-prefer-clarity-over-cleverness": [
        "this feels almost a bit too clever. Why not simply have `_guessHandleType(fd)` return null explicitly? then your return value below could be `return handletypes[type] || type`.",
        "```suggestion\r\n\r\n  loadFromFile(savePath=\"./blocklist.json\") {\r\n```\r\n\r\nWhy make this an arrow function?\r\n\r\nInstead of building the file loading directly into `BlockList`, consider having `BlockList` instead generate a `Buffer` or be created from a `Buffer` and let the application do the file handling itself.\r\n\r\n```\r\nconst blockList = createBlockListSomehow();\r\nconst buf = blockList.save();\r\nfs.writeFileSync('blocklist.json', buf);\r\n```\r\n",
        "JSON or buffer doesn't matter so much. I'd prefer to avoid adding the node:fs uses into BlockList and would rather separate those concerns. We can greatly simplify this, for instance, by having `toJSON` and `fromJSON` methods on BlockList and handing the fs operations externally to BlockList."
      ],
      "node-reuse-computed-values-efficiently": [
        "You might consider moving the declaration for the `Local<Object> lock_info` to outside of the for loop and just reset it here so that we're reusing the same declaration on each iteration rather than creating a new one.",
        "Same here... if the `Local<Object> lock_info;` is moved outside of the for loops then it can just be reused here."
      ],
      "node-limit-environment-variable-scope": [
        "Just a nit... I'd be more comfortable with this extracting just the env vars that are specifically relevant to `proxyEnv` rather than passing the entire `process.env`"
      ],
      "node-version-apis-with-care": [
        "The case will need to be wrapped in the `#ifdef NAPI_EXPERIMENTAL` as well",
        "Ah right, I forgot that this file defines `NAPI_EXPERIMENTAL` unconditionally at the top of the file. ",
        "Likewise here. Wrap the else if here in an `#ifdef NAPI_EXPERIMENTAL`",
        "My fear here is that `apiSurface`, even as a generated file, is likely to become quite massive and difficult to manage. As is we cannot even use the github UI to review it since it says, \"29,926 additions... not shown because the diff is too large...\". Is there any way to break it up further? Maybe generate a set of files rather than one single massive one?"
      ],
      "node-standardize-null-pointer-checks": [
        "Consider having this return a `MaybeLocal<Object>` to better facilitate error propagation"
      ],
      "node-format-docs-for-readability": [
        "Check out how we handle links in the other docs in this folder. We collect them at the end of the doc and use in-doc references rather than inlining the URLs. Much more readable that way."
      ],
      "node-public-over-internal-apis": [
        "These aren't really considered internal modules, fwiw",
        "My concern would be whether or not anyone in userland could be using this, as unlikely as that may be. Should we export this through the regular `node:http` API to provide an alternative path to them?"
      ],
      "node-minimize-configuration-dependencies": [
        "Given that this is Windows only, I would generally prefer that this entire internal binding only be compiled on windows, with the non-functional stubs implemented solely in javascript on the other platforms."
      ],
      "node-benchmark-before-optimizing-code": [
        "The results are mixed and it's just not clear if the difference is worth being concerned about... some benchmarks are faster, others are slower.\r\n\r\n```\r\nstreams/compose.js n=1000                                                                       ***    -19.41 %       ±1.17%  ±1.57%  ±2.06%\r\nstreams/creation.js kind='duplex' n=50000000                                                    ***      4.39 %       ±1.62%  ±2.16%  ±2.81%\r\nstreams/creation.js kind='readable' n=50000000                                                           2.46 %       ±4.12%  ±5.48%  ±7.14%\r\nstreams/creation.js kind='transform' n=50000000                                                   *      2.21 %       ±1.69%  ±2.25%  ±2.94%\r\nstreams/creation.js kind='writable' n=50000000                                                  ***      6.72 %       ±2.15%  ±2.85%  ±3.71%\r\nstreams/destroy.js kind='duplex' n=1000000                                                              -0.50 %       ±3.36%  ±4.48%  ±5.84%\r\nstreams/destroy.js kind='readable' n=1000000                                                             1.07 %       ±2.43%  ±3.24%  ±4.22%\r\nstreams/destroy.js kind='transform' n=1000000                                                           -0.55 %       ±2.33%  ±3.10%  ±4.03%\r\nstreams/destroy.js kind='writable' n=1000000                                                             0.16 %       ±2.75%  ±3.66%  ±4.77%\r\nstreams/pipe-object-mode.js n=5000000                                                           ***      4.68 %       ±2.00%  ±2.67%  ±3.50%\r\nstreams/pipe.js n=5000000                                                                       ***     10.30 %       ±1.19%  ±1.59%  ±2.07%\r\nstreams/readable-async-iterator.js sync='no' n=100000                                                    1.07 %       ±1.92%  ±2.55%  ±3.32%\r\nstreams/readable-async-iterator.js sync='yes' n=100000                                          ***      8.45 %       ±3.80%  ±5.06%  ±6.59%\r\nstreams/readable-bigread.js n=1000                                                                       0.55 %       ±2.27%  ±3.02%  ±3.94%\r\nstreams/readable-bigunevenread.js n=1000                                                        ***     -2.12 %       ±1.13%  ±1.52%  ±1.99%\r\nstreams/readable-boundaryread.js type='buffer' n=2000                                             *      1.03 %       ±0.84%  ±1.12%  ±1.46%\r\nstreams/readable-boundaryread.js type='string' n=2000                                             *      2.08 %       ±1.69%  ±2.26%  ±2.97%\r\nstreams/readable-from.js type='array' n=10000000                                                ***     -5.20 %       ±2.30%  ±3.06%  ±3.99%\r\nstreams/readable-from.js type='async-generator' n=10000000                                               1.73 %       ±2.22%  ±2.96%  ±3.86%\r\nstreams/readable-from.js type='sync-generator-with-async-values' n=10000000                              1.88 %       ±2.05%  ±2.73%  ±3.56%\r\nstreams/readable-from.js type='sync-generator-with-sync-values' n=10000000                              -1.75 %       ±2.41%  ±3.24%  ±4.28%\r\nstreams/readable-readall.js n=5000                                                               **     -4.53 %       ±3.18%  ±4.24%  ±5.51%\r\nstreams/readable-uint8array.js kind='encoding' n=1000000                                         **      2.40 %       ±1.52%  ±2.02%  ±2.63%\r\nstreams/readable-uint8array.js kind='read' n=1000000                                            ***     -2.98 %       ±1.59%  ±2.11%  ±2.75%\r\nstreams/readable-unevenread.js n=1000                                                             *     -1.81 %       ±1.45%  ±1.93%  ±2.51%\r\nstreams/writable-manywrites.js len=1024 callback='no' writev='no' sync='no' n=100000                     1.10 %       ±3.76%  ±5.00%  ±6.51%\r\nstreams/writable-manywrites.js len=1024 callback='no' writev='no' sync='yes' n=100000           ***     27.55 %       ±9.28% ±12.36% ±16.09%\r\nstreams/writable-manywrites.js len=1024 callback='no' writev='yes' sync='no' n=100000                    2.88 %       ±5.18%  ±6.90%  ±8.98%\r\nstreams/writable-manywrites.js len=1024 callback='no' writev='yes' sync='yes' n=100000            *     11.47 %       ±9.08% ±12.08% ±15.73%\r\nstreams/writable-manywrites.js len=1024 callback='yes' writev='no' sync='no' n=100000                   -1.45 %       ±3.28%  ±4.36%  ±5.68%\r\nstreams/writable-manywrites.js len=1024 callback='yes' writev='no' sync='yes' n=100000          ***     23.32 %       ±8.66% ±11.53% ±15.01%\r\nstreams/writable-manywrites.js len=1024 callback='yes' writev='yes' sync='no' n=100000                   3.83 %       ±4.48%  ±5.96%  ±7.76%\r\nstreams/writable-manywrites.js len=1024 callback='yes' writev='yes' sync='yes' n=100000                  1.32 %       ±6.89%  ±9.17% ±11.94%\r\nstreams/writable-manywrites.js len=32768 callback='no' writev='no' sync='no' n=100000                   -1.30 %       ±3.51%  ±4.68%  ±6.09%\r\nstreams/writable-manywrites.js len=32768 callback='no' writev='no' sync='yes' n=100000          ***     30.32 %      ±10.19% ±13.57% ±17.67%\r\nstreams/writable-manywrites.js len=32768 callback='no' writev='yes' sync='no' n=100000                   0.12 %       ±3.88%  ±5.17%  ±6.73%\r\nstreams/writable-manywrites.js len=32768 callback='no' writev='yes' sync='yes' n=100000          **     13.40 %       ±9.34% ±12.47% ±16.32%\r\nstreams/writable-manywrites.js len=32768 callback='yes' writev='no' sync='no' n=100000                  -1.46 %       ±4.38%  ±5.83%  ±7.61%\r\nstreams/writable-manywrites.js len=32768 callback='yes' writev='no' sync='yes' n=100000         ***     16.67 %       ±8.21% ±10.94% ±14.28%\r\nstreams/writable-manywrites.js len=32768 callback='yes' writev='yes' sync='no' n=100000                  1.19 %       ±4.05%  ±5.39%  ±7.01%\r\nstreams/writable-manywrites.js len=32768 callback='yes' writev='yes' sync='yes' n=100000        ***     21.43 %       ±9.84% ±13.14% ±17.21%\r\nstreams/writable-uint8array.js kind='object-mode' n=50000000                                             0.64 %       ±3.66%  ±4.87%  ±6.34%\r\nstreams/writable-uint8array.js kind='write' n=50000000                                            *      3.85 %       ±3.32%  ±4.42%  ±5.76%\r\nstreams/writable-uint8array.js kind='writev' n=50000000                                           *      3.15 %       ±2.99%  ±3.98%  ±5.18%\r\n```"
      ],
      "node-descriptive-function-names": [
        "Let's name this method `toJSON` to that it works seamlessly with `JSON.stringify(...)` ",
        "For the `IntervalHistogram` (https://nodejs.org/docs/latest/api/perf_hooks.html#class-intervalhistogram-extends-histogram) we use `enable()` and `disable()`. Not critical but I'd prefer to keep the API names consistent."
      ]
    },
    "profile": {
      "location": "Clovis, California",
      "company": "@cloudflare",
      "blog": "http://jasnell.me",
      "site_admin": false,
      "followers": 1815,
      "following": 38
    }
  },
  "DimasKovas": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-clear-consistent-identifier-names",
        "title": "Clear consistent identifier names"
      },
      {
        "slug": "neon-configurable-cache-parameters",
        "title": "Configurable cache parameters"
      },
      {
        "slug": "neon-configuration-context-alignment",
        "title": "Configuration context alignment"
      },
      {
        "slug": "neon-database-before-memory",
        "title": "Database before memory"
      },
      {
        "slug": "neon-design-domain-specific-error-types",
        "title": "Design domain-specific error types"
      },
      {
        "slug": "neon-ensure-algorithm-robustness",
        "title": "Ensure algorithm robustness"
      },
      {
        "slug": "neon-environment-specific-config-defaults",
        "title": "Environment-specific config defaults"
      },
      {
        "slug": "neon-extract-and-reuse",
        "title": "Extract and reuse"
      },
      {
        "slug": "neon-handle-all-error-paths",
        "title": "Handle all error paths"
      },
      {
        "slug": "neon-hierarchical-semantic-naming",
        "title": "Hierarchical semantic naming"
      },
      {
        "slug": "neon-secure-authentication-handling",
        "title": "Secure authentication handling"
      },
      {
        "slug": "neon-stage-configuration-changes-gradually",
        "title": "Stage configuration changes gradually"
      },
      {
        "slug": "neon-structure-endpoints-for-rest",
        "title": "Structure endpoints for REST"
      },
      {
        "slug": "neon-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      }
    ],
    "comments": {
      "neon-stage-configuration-changes-gradually": [
        "Some compat tests are broken because I think it runs old computes (without `neon.safekeeper_connstrings`) with new control plane.\r\nThis kind of changes are usually done with several steps (several PRs):\r\n1. First, add new option to compute, but do not use it in CP (or use under a feature flag).\r\n2. Wait for the new option to reach the release\r\n3. Change the option in CP\r\n4. Delete old option\r\n\r\nIt's quite complicated and not fast, that's why I liked the idea of a separate `extra_conn_options`",
        "Do I understand correctly that we need to restart all computes to switch this options in control plane in production?"
      ],
      "neon-database-before-memory": [
        "Can we have a tenant only with read-only timelines?\r\nIn this case similar logic is needed in `tenant_delete_safekeepers`"
      ],
      "neon-configurable-cache-parameters": [
        "What if it already exists?\r\n\r\nIn that case we will forget that we have another entry and will never remove it.\r\n\r\nI guess we may want to store an array as an value,\r\n\r\nor, as an alternative, if we have `tokio::sync::mpsc::unbounded_channel` for deleting entries (and drain it in `select!`), then we may insert a new entry into the `data_cache` and add previous LSN (if existed) to `deletion_queue`, so the background task will delete it\r\n\r\nThe second option is probably simpler (don't need to keep an array)"
      ],
      "neon-environment-specific-config-defaults": [
        "Now I see why you didn't like \"enabled\" field :)\r\n\r\nLet's put `#[serde(default)]` on the whole `DiskUsageEvictionTaskConfig`, so all fields will be defaulted to values from `DiskUsageEvictionTaskConfig::default()`, and remove it from fields. Then `{enabled=false}` will be a valid config"
      ],
      "neon-ensure-algorithm-robustness": [
        "Well, I guess it's very unlikely to happen, but working with floating numbers, I wouldn't assume that some numbers add up to 100. There is always a precision error. Maybe returning the last value would be a better option?"
      ],
      "neon-secure-authentication-handling": [
        "Need to protect those methods with `check_permissions(&request, None)` to block these API calls with tenant-scoped tokens.",
        "It will panic if authorization header is shorter than 7 bytes, need to check. I think `.strip_prefix(\"Bearer\")` does the job in the rust-way\r\n\r\nAnd may be `trim_start()` before it also makes sense\r\n\r\n",
        "My initial comment was slightly incorrect - having a space after `Bearer` as it was in your code before ( `.strip_prefix(\"Bearer \")`) is probably more correct because otherwise, it will match header value `BearerSmth <token>`. "
      ],
      "neon-configuration-context-alignment": [
        "I think `PGC_POSTMASTER` fits better here, because we don't have code to reconnect to safekeepers when this option changes right now.\r\nOr need to implement `assign_neon_safekeeper_extra_conninfo_options` hook to restart the walproposer process if the option changes similarly to `assign_neon_safekeepers`"
      ],
      "neon-extract-and-reuse": [
        "nit: Maybe reuse `split_off_safekeepers_generation` here? Looks exactly like it"
      ],
      "neon-clear-consistent-identifier-names": [
        "nit: it's not HTTP client anymore. just `client` sounds fine to me",
        "nit: technically, we already have `posthog` in lib name, probably it's not worth it to add `PostHog` to every class name. E.g. for the client I'm OK with `PostHogClient`, because there are too many `Client`s already. But for those structs the name is already pretty long and unique."
      ],
      "neon-use-descriptive-identifiers": [
        "nit: it's more difficult to understand inverted boolean options (like \"disable\")\r\nAnd later this option converts to `kick_secondary_downloads`, which is even more frustrating\r\nLet's call it the same `kick_secondary_downloads` everywhere"
      ],
      "neon-handle-all-error-paths": [
        "If we set `safekeepers` guc to`g#123`, endptr will point to the `\\0` terminator. Here you skip the terminator and return a pointer past the string data. It will probably cause a segfault later in the code\r\n\r\nIt is an incorrect value, and cplane should never specify it, but the best practice for handling incorrect input is an error, not SIGSEGV :)\r\n\r\nNeed to check that `*endptr == ':'` "
      ],
      "neon-structure-endpoints-for-rest": [
        "nit: `/v1/feature_flag/:flag_name` looks more \"RESTful\", but I'm fine with current `?flag=my-flag` too",
        "nit: probably we can move these to `/debug/v1`, how it's done in storcon, to make it more clear that it's debug handlers"
      ],
      "neon-design-domain-specific-error-types": [
        "Is it ok that we return `None` and not real errors here and there?\r\nProbably we want to log if there is some error in evaluation because usually it means bugs",
        "nit: it's already fine (and much better than just `return None`), but adding some context to error messages would increase debugability (e.g. the name of the missing property).\r\nIt's applies to all other places as well"
      ],
      "neon-hierarchical-semantic-naming": [
        "nit: google style guide suggests using verbs in method names. `GetDbSize`, `CheckRelExists`, `GetRelSize`",
        "nit: I would rather call it `LsnInfo` or smth more specific than \"common\". I don't think it's going to be extended"
      ]
    },
    "profile": {
      "company": "@neondatabase",
      "blog": "",
      "site_admin": false,
      "followers": 15,
      "following": 13
    }
  },
  "myrrc": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-adaptive-cache-expiration-strategy",
        "title": "Adaptive cache expiration strategy"
      },
      {
        "slug": "neon-avoid-flaky-tests",
        "title": "Avoid flaky tests"
      },
      {
        "slug": "neon-clear-consistent-identifier-names",
        "title": "Clear consistent identifier names"
      },
      {
        "slug": "neon-comprehensive-code-documentation",
        "title": "Comprehensive code documentation"
      },
      {
        "slug": "neon-document-parameter-choices",
        "title": "Document parameter choices"
      },
      {
        "slug": "neon-environment-specific-config-defaults",
        "title": "Environment-specific config defaults"
      },
      {
        "slug": "neon-extract-and-reuse",
        "title": "Extract and reuse"
      },
      {
        "slug": "neon-keep-files-focused-small",
        "title": "Keep files focused small"
      },
      {
        "slug": "neon-minimize-unnecessary-allocations",
        "title": "Minimize unnecessary allocations"
      },
      {
        "slug": "neon-optimize-cargo-dependencies",
        "title": "Optimize cargo dependencies"
      },
      {
        "slug": "neon-optimize-what-matters",
        "title": "Optimize what matters"
      },
      {
        "slug": "neon-proper-metrics-design",
        "title": "Proper metrics design"
      },
      {
        "slug": "neon-proper-option-type-usage",
        "title": "Proper Option type usage"
      },
      {
        "slug": "neon-scope-jwt-authentication-tokens",
        "title": "Scope JWT authentication tokens"
      },
      {
        "slug": "neon-stage-configuration-changes-gradually",
        "title": "Stage configuration changes gradually"
      },
      {
        "slug": "neon-structure-endpoints-for-rest",
        "title": "Structure endpoints for REST"
      }
    ],
    "comments": {
      "neon-optimize-what-matters": [
        "Yeah, that's why I want to gate it",
        "A very minor thing, but as `threshold = UINT64_MAX` only for last bucket, we can loop from 0 to `NUM_QT_BUCKETS - 1`, write last iteration manually and avoid branching in assigning `bucket_le`"
      ],
      "neon-stage-configuration-changes-gradually": [
        "We need to parse GUC in case ComputeSpec's fields are missing for backward compatibility",
        "Fixed",
        "Fixed"
      ],
      "neon-keep-files-focused-small": [
        "Fixed"
      ],
      "neon-environment-specific-config-defaults": [
        "We should vendor these changes once PR gets ready for review, downloading data from Internet in tests is a bad idea"
      ],
      "neon-avoid-flaky-tests": [
        "```suggestion\r\n@pytest.mark.skipif(not USE_LFC)\r\ndef test_lfc_prewarm(neon_simple_env: NeonEnv):\r\n```"
      ],
      "neon-optimize-cargo-dependencies": [
        "I see there are a lot of Cargo.lock changes which may not be needed.\r\nTry this to possibly reduce the diff\r\n```\r\ngit checkout origin/main -- Cargo.lock\r\ncargo hakari manage-deps\r\ncargo hakari generate\r\ncargo b --locked --frozen\r\n```"
      ],
      "neon-minimize-unnecessary-allocations": [
        "nit: May we use `itertools::join` here to avoid constructing a separate Vec?",
        "Also nit: if some of privileges may already be granted, maybe we should \r\n1. Take an iter of privileges\r\n2. Filter out those contained in `already_granted`?\r\n3. If iterator is non-empty, join using itertools and grant?",
        "Fixed"
      ],
      "neon-document-parameter-choices": [
        "Fixed"
      ],
      "neon-extract-and-reuse": [
        "This check and loop are further used in other functions, can we extract them to a separate helper function?"
      ],
      "neon-adaptive-cache-expiration-strategy": [
        "We already have a service for prewarm (endpoint_storage) in neon/"
      ],
      "neon-proper-metrics-design": [
        "I thing `counter` is better since \"number of autovacuum runs\" is non-decreasing"
      ],
      "neon-clear-consistent-identifier-names": [
        "Fixed"
      ],
      "neon-proper-option-type-usage": [
        "Lsn isn't discriminated against Lsn::INVALID so Option<> would increase its size, not sure it's the best option",
        "On the other hand, making it a discriminant (or Lsn:::MAX) isn't an option as well, so I'll use your approach."
      ],
      "neon-scope-jwt-authentication-tokens": [
        "```suggestion\r\n## Authentication and authorization\r\n1. Control plane should generate a JWT token which would be used by compute for authorizing requests\r\nto S3 proxy. This token should be passed in `/compute/api/v2/computes/{id}/spec` route,\r\nparsed by `compute_ctl` as an optional field (to preserve backward compatibility) and passed as-is\r\nto proxy requests during prewarm or prewarm offload. If no token is passed, requests to proxy\r\nshould return 505 Not Supported (other services have `--dev` flags which disable token check or\r\nconfig fields which bypass authentication when absent but that's error-prone) and log the error.\r\n\r\n2. S3 proxy should have `auth_public_key.pem` similar to pageserver\r\nfor spawning an https server for compute requests (this would also further help in getting\r\nPCI-DSS certification) and verifying compute requests. The pemfile should be supplied by\r\ninfra/. As with pageserver, S3 proxy should have `/reload_auth_validation_keys` endpoint\r\nto reload the pemfile from config should it change.\r\n\r\n## Metrics\r\n\r\nIn addition to changing `compute_ctl`'s /status, proxy should provide request duration\r\nmetrics along with result server codes as labels\r\n```\r\n",
        "I believe Matthias's RFC focuses more on the high-level overview, but ok, I'll copy the comments. Btw this will probably be split into tasks for https://github.com/neondatabase/cloud/issues/24225 (see sub-issues)"
      ],
      "neon-structure-endpoints-for-rest": [
        "Fixed",
        "Fixed, made POST",
        "success doesn't allow you to omit the body"
      ],
      "neon-comprehensive-code-documentation": [
        "Yeah, a comment typo. It's None when we prewarm from ourselves' data, will fix in promotion PR.",
        "`//!`?"
      ]
    },
    "profile": {
      "company": "Neon",
      "blog": "https://myrrc.dev",
      "site_admin": false,
      "followers": 20,
      "following": 3
    }
  },
  "tannergooding": {
    "repos": [
      "dotnet/runtime"
    ],
    "entries": [
      {
        "slug": "runtime-abstract-traversal-patterns",
        "title": "Abstract traversal patterns"
      },
      {
        "slug": "runtime-avoid-busy-waiting",
        "title": "Avoid busy waiting"
      },
      {
        "slug": "runtime-centralize-platform-configurations",
        "title": "Centralize platform configurations"
      },
      {
        "slug": "runtime-choose-appropriate-error-mechanisms",
        "title": "Choose appropriate error mechanisms"
      },
      {
        "slug": "runtime-choose-descriptive-names",
        "title": "Choose descriptive names"
      },
      {
        "slug": "runtime-decompose-complex-algorithms",
        "title": "Decompose complex algorithms"
      },
      {
        "slug": "runtime-document-configuration-intent",
        "title": "Document configuration intent"
      },
      {
        "slug": "runtime-document-function-contracts",
        "title": "Document function contracts"
      },
      {
        "slug": "runtime-enable-configurable-instrumentation",
        "title": "Enable configurable instrumentation"
      },
      {
        "slug": "runtime-honor-api-contracts",
        "title": "Honor API contracts"
      },
      {
        "slug": "runtime-maintain-configuration-compatibility",
        "title": "Maintain configuration compatibility"
      },
      {
        "slug": "runtime-maintain-consistent-formatting",
        "title": "Maintain consistent formatting"
      },
      {
        "slug": "runtime-maintainable-test-structure",
        "title": "Maintainable test structure"
      },
      {
        "slug": "runtime-model-actual-hardware-costs",
        "title": "Model actual hardware costs"
      },
      {
        "slug": "runtime-optimize-aligned-simd-operations",
        "title": "Optimize aligned SIMD operations"
      },
      {
        "slug": "runtime-optimize-build-dependency-chains",
        "title": "Optimize build dependency chains"
      },
      {
        "slug": "runtime-optimize-common-paths",
        "title": "Optimize common paths"
      },
      {
        "slug": "runtime-optimize-for-readability",
        "title": "Optimize for readability"
      },
      {
        "slug": "runtime-optimize-memory-access",
        "title": "Optimize memory access"
      },
      {
        "slug": "runtime-platform-agnostic-network-apis",
        "title": "Platform-agnostic network APIs"
      },
      {
        "slug": "runtime-platform-aware-algorithm-optimization",
        "title": "Platform-aware algorithm optimization"
      },
      {
        "slug": "runtime-simplify-code-expressions",
        "title": "Simplify code expressions"
      },
      {
        "slug": "runtime-specific-exceptions-with-context",
        "title": "Specific exceptions with context"
      }
    ],
    "comments": {
      "runtime-simplify-code-expressions": [
        "I think we could collapse `IsAVXVNNIInstruction(ins) || IsAVXVNNIINT8Instruction(ins) || IsAVXVNNIINT16Instruction(ins)` down into some `IsAvxVnniFamilyInstruction(ins)` given the places that are checking them"
      ],
      "runtime-optimize-build-dependency-chains": [
        "This is the actual interesting \"bit\" of the change.\r\n\r\nEach high level folder (Arm, General, X86) has a targets file that:\r\n1. Has a dependency on the relevant test generator to ensure it is built first\r\n2. A target that runs the test generator to ensure the necessary files exist, with relevant up to date checks to ensure it doesn't run unnecessarily\r\n3. A target that has to always run as part of compilation to read the list of generated tests and include them in the list of files csc should process",
        "The actual `GenerateTests.csx` was simply moved to be part of a csproj and to take a couple inputs to help filter the tests.\r\n\r\nIt's not necessarily the cleanest change, but it is by far one of the simplest. Building a source generator or otherwise refactoring things more would result in significantly more time and churn and would block the AVX-512 work until that was completed."
      ],
      "runtime-abstract-traversal-patterns": [
        "This was the cleanest/easiest way I could think of to do this.\r\n\r\nThe general idea is that we have several transforms we want to make to LIR before containment happens (because containment complicates these transforms). However, since we're in LIR form for these nodes at this point, we need to make sure the transform is safe to do.\r\n\r\nAn alternative would be to add a `pre` pass for lowering then do a separate `post` pass for containment, but that feels like a bigger/bulkier/riskier change.\r\n\r\nThere are some other transforms that would be nice to move \"here\" longer term, like the sequential `insertps` operation folding we do; or the recognizing of `AND(x, NOT(y))` we do; neither of which we want to do in HIR because it breaks or massively complicates other optimizations (like folding and operation negation) that we do.\r\n\r\nI'd be happy to make this a separate pass for .NET 11, if we feel that is better. But for .NET 10 I think this is a less risky approach."
      ],
      "runtime-maintain-configuration-compatibility": [
        "@jkotas are we planning any other R2R breaking changes this release? If so, it would be nice to go ahead and clean up (potentially just in a follow up PR) some of the R2R values; otherwise, I can log a tracking issue for it.\r\n\r\nThe two big things are that we have some unused gaps now that could be filled in and I don't see why we need to use unique IDs across X86 and Arm64 (that is `X86Base` and `ArmBase` can't currently both be R2R ID 1).\r\n\r\nFor the latter point, the general intent of the R2R flags is to track when user code has some `if (Isa.IsSupported) { }` check that isn't taken, because we have to assume it may cause different behavior. However, on x64 some `AdvSimd.IsSupported` check can never be true, so it can't ever cause a difference in behavior and so we shouldn't need to track it at all and so the x64 vs Arm64 R2R IDs should be able to overlap, with them differentiated by a general \"target architecture\" flag instead. \r\n\r\nI believe we should be able to do much like we do in NAOT where we simply track 1 bit per ISA on the relevant architecture. In other words, I expect we could essentially simplify this to almost the same as https://github.com/dotnet/runtime/pull/115983/files#diff-37e89a49ea22a08b2d0502ca3b1716e2dd1bd68cafd3eeb3cd45f5a4406601d3R219, which would also let us avoid needing to track \"specifiable\" or not and needing unique bit entries for cases like `gfni_v512` (which is tracked as `gfni + avx512` for NAOT). We should just need to track the baseline ISA in addition to what NAOT tracks, since that can be disabled for testing user fallbacks.",
        "I'll revert for now. However, ...\r\n\r\n> This would make r2rdump more complicated. Right now, r2rdump just does Enum.ToString() to dump the instruction set.\r\n\r\nI think it would be better to make it slightly more complex by taking this break long term and adding a compat handler to r2rdump to handle older image versions.\r\n\r\nRight now we're using 60 bits for R2R, when we should only need 34 total. If we split it by platform then it's 11 for Arm64 and 23 for x64. Just given the pending work around SVE and AVX, we will likely go over 64-bits in the next release and need to take a break anyways.\r\n\r\nI think the way NAOT has this setup is more flexible and was done because it came online later and was able to handle some of the scenarios that R2R didn't have the foresight to consider.\r\n\r\n> NAOT does not have the versioning problem like R2R. There is no mixing and matching of versions, etc.\r\n\r\nRight, but I think it's also something we can handle for R2R to make things overall cleaner moving forward. Not something we have to do now, but I expect we'll be forced to deal with it in the .NET 11 timeframe\r\n\r\n",
        "Reverted to preserve the compat for .NET 10"
      ],
      "runtime-choose-appropriate-error-mechanisms": [
        "We shouldn't assert unreached here. There are always scenarios where unused values get preserved, such as min-opts, and so we should prefer the typical pattern of `if (foundUse) { use.ReplaceWith(node); } else { node->SetUnusedValue(); }`",
        "Previously the JIT would fail fast with `The JIT compiler encountered invalid IL code or an internal limitation.`\r\n\r\nNow, it will assert in debug mode but will `throw PlatformNotSupportedException` at runtime.",
        "~This was changed to not use `impUnsupportedHWIntrinsic` and instead use `gtNewMustThrowException` directly.~\r\n\r\n`impUnsupportedHWIntrinsic` was renamed to `impUnsupportedNamedIntrinsic` and moved out of `FEATURE_HW_INTRINSIC`"
      ],
      "runtime-decompose-complex-algorithms": [
        "@kg, I don't see any obvious existing handlers for what is effectively \"bitcast\" (that is the input is returned directly with no change, the API only exists to satisfy the type system).\r\n\r\nDo you have a pointer to any similar handling that might exist?",
        "I ended up following what `op_UnaryNegation` does and adding handlers that basically do memcpy to handle it.",
        "This file ended up needing a bit of a refactoring as there were some assumptions in place that don't hold when supporting additional intrinsics.\r\n\r\nIn particular, there are various intrinsics where:\r\n* one of the SIMD types may not be generic at all (`Vector2`, `Vector3`, `Vector4`)\r\n* multiple generic types exist (`As<TFrom, TTo>`)\r\n* the return type may not be a 128-bit vector (`AsVector2`, `AsVector3`)\r\n\r\nSo, what I did here was I broke this `get_common_simd_info` method into two:\r\n* `get_common_simd_info`\r\n* `get_common_simd_scalar_arg`\r\n\r\nThe former now always gets the size of the input klass and secondly determines if it is a SIMD type and what the underlying element type is if so. While the latter identifies the first non-SIMD argument, if one exists.\r\n\r\n",
        "To support methods which return a value type, but where that isn't a 128-bit vector, this explicitly gets the return type from the signature to ensure there can be no accidents",
        "In here, we consistently query the relevant simd information of the return type and if it exists the simd information of the  first parameter (this is enough to correctly handle all the cases that currently exist).\r\n\r\nThere's actually quite a bit of logic in this function that *could* be moved down into `emit_common_simd_operations`, as APIs like `AndNot` exist for `Vector128<T>` and `Vector<T>`, they may also exist for types like `Vector4` in the future. I opted to not move that down in this PR, to try and keep the total churn under control.\r\n\r\nBut, I did add some basic validation that the encountered signatures are roughly as expected to help ensure we don't hit issues in the future as new overloads are introduced or the general SIMD support in Mono is expanded."
      ],
      "runtime-document-function-contracts": [
        "Fixed."
      ],
      "runtime-optimize-aligned-simd-operations": [
        "I have it erroring if `posix_memalign` isn't available as a fallback. `posix_memalign` is a very old (`_POSIX_C_SOURCE >= 200112L || _XOPEN_SOURCE >= 600`) function and should be always available.\r\n\r\nLikewise, I don't believe trying to polyfill via `malloc`/`free` is reliable as there is no guarantee that `free` works with arbitrary pointers:\r\n> The behavior is undefined if the value of ptr does not equal a value returned earlier by malloc(), calloc(), realloc(), or aligned_alloc() (since C11).\r\n\r\nLikewise, there is no reliable way to backtrack from a given aligned `ptr` to the actual `ptr` that `malloc` did return.",
        "I'd like to see if any platforms don't actually have `posix_memalign` before falling back to storing the actual pointer before the returned pointer."
      ],
      "runtime-maintainable-test-structure": [
        "Were these intentionally added? They look to have dead code and aren't actually testing the APIs in question.\r\n\r\nWe have other tests which cover the CPUID checks"
      ],
      "runtime-optimize-for-readability": [
        "The xplat helper intrinsics support operators and so we can make this \"more readable\" by just using `x & y`.",
        "Believe so, the fixer automatically applied them.",
        "```suggestion\r\n            if (dimension is 0 or -1)\r\n```"
      ],
      "runtime-optimize-common-paths": [
        "Instructions like `pextrw` which extract to a general purpose register or like `pinsrw` which insert from a general-purpose register don't take masks.",
        "Instructions like `pmaddwd` which are `INS_TT_FULL_MEM` don't have the `Input_*Bit` flag since they cannot support embedded broadcast and only ever take the full simd size.",
        "Checks throughout the JIT have been ordered under the presumption that `varTypeUsesIntReg` is the most likely occurrence. Where mask register handling is needed, `varTypeUsesMaskReg` is the middle case since we only need 1 additional check to cover all 3 cases. This allow the last case to `assert`.\r\n\r\nThis keeps 1-check for non-xarch, minimizes the total ifdefs when mask handling is needed, and ensures we need no more than 2 checks on xarch with the common case being 1 check. Ideally this helps keep it \"pay for play\".\r\n\r\nHaving `varTypeUsesIntReg` allowed a few other checks to become simplified/improved in general.",
        "I think it'd be better to use the `AuxiliaryJitType` and mark these as `SpecialCodeGen` than to add a bunch of extra table entries. More similar to how `Gather/Scatter` get handled.\r\n\r\nIf we were to do the extra table entries, I'd rather they be moved to the bottom of the file with the other extra intrinsics that don't directly map to managed API names. That way the lookups for managed APIs remain cheaper and clearer.",
        "That's the purpose of `AuxiliaryJitType`, to give a secondary type to use.\r\n\r\n`SimdBaseJitType` is the primary type and for most intrinsics is the sole type needed to determine which instruction to use. Some special intrinsics need a secondary type and that's the purpose of `AuxiliaryJitType` so that we have a way to track it where required.",
        "Most of these ISAs aren't meaningful to test anymore, especially since we aren't doing bringup of the ISAs.\r\n\r\nFor the most part, we just care about the unique ISA paths used in corelib or the JIT, which is primarily the SSE and AVX families that are already covering cases like FMA or BMI being disabled.\r\n\r\nI think we can even get it so that we don't need to test `nossse3, nosse41, and nosse42` as well; but we'd need to do a bit of cleanup in the JIT to achieve that.\r\n\r\n-- I'd like to get our support matrix down to effectively:\r\n* x86-64-v1: This is the baseline and is currently targeted by NAOT\r\n* x86-64-v2: This is everything up through SSE4.2+POPCNT\r\n* x86-64-v2 + AVX: This tests the VEX encoding and is known to have a decent user share\r\n* x86-64-v3: This is AVX2+FMA+BMI1+BMI2\r\n* x86-64-v4: This is AVX512 and covers the EVEX encoding\r\n\r\nFor x86-64-v2:\r\n* this is what is supported by the x64 on Arm emulation provided by Apple and Windows\r\n* it would be nice if NAOT could target this as the default, as its an 18 year old baseline\r\n* this is the target required by Win11 24H2 and boot is blocked if the ISAs aren't available\r\n* Win11 prior to 24H2 is documented as requiring SSE4.1 otherwise",
        "> I do not think we want to have the baseline supported piecemeal (it is unlikely we would be able to it correctly since it is impossible to test). If we want to raise the baseline, we should do it for the whole product. I would not be opposed to raising the product baseline to x86-64-v2.\r\n\r\n👍. I think it would provide a nice simplification in the JIT and libraries for what code paths we need to support. I don't want to raise the bar \"too high\" and negatively impact a significant number of customers, but I do think that `x86-64-v2` and `armv8.1-a` are reasonable baselines at this point.\r\n\r\n> Where is it documented?\r\n\r\nIn https://learn.microsoft.com/en-us/windows-hardware/design/minimum/minimum-hardware-requirements-overview, specifically under the Windows 11 document (last updated 2021)\r\n\r\n![image](https://github.com/user-attachments/assets/ad370ba3-9e00-49fc-8683-43d08338289a)\r\n\r\nWe got the notification that SSE4.2+POPCNT in 24H2+ when we had last reached out about Arm RDM support; although there's also been some news about the boot blocker that exists that went around last year as well.\r\n\r\nAzure, AWS, Google Cloud, and other major cloud providers that specify what hardware they provide are entirely on x86-64-v3 or later. There are also other 3rd party numbers out there, such as the Steam Hardware Survey, which shows 99.78% of reporting users have x86-64-v2 (while 97.31% have AVX and 94.66% have x86-64-v3).\r\n\r\n> When did Intel/Amd stop selling the last processor without x86-64-v2? It is the more interesting date for this discussion.\r\n\r\nFor Intel the last CPUs pre x86-64-v2 were discontinued in 2013, around the time x86-64-v3 was launched. This would have been the Bonnell microarchitecture (part of the Intel Atom lineup for low end machines). The Conroe/Merom and Penryn/Wolfdale chipsets had been discontinued in 2011-2012. AMD discontinued their 10h and Bobcat lineups in 2012-2013 as well.\r\n\r\nSo if we did try to raise the baseline in .NET 11, it should only be for 12-18 year old computers which are no longer supported by the CPU manufacturers, by MacOS, or by current Windows.\r\n"
      ],
      "runtime-centralize-platform-configurations": [
        "> /Applications/Xcode_12.4.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX11.1.sdk/usr/include/malloc/_malloc.h:50:10: note: 'aligned_alloc' has been marked as being introduced in macOS 10.15 here, but the deployment target is macOS 10.13.0\r\n\r\nand then the mono failure for C99 seem potentially problematic\r\n\r\n`posix_memalign` is the \"next best\" thing but has a restriction\r\n> The address of the allocated memory will be a multiple of alignment, which must be a power of two and a multiple of sizeof(void *).\r\n\r\n`memalign` might also be available, but is considered \"obsolete\" in newer versions and is not guaranteed to work with `free` (and there is no explicit counterpart for it)\r\n\r\nhttps://www.man7.org/linux/man-pages/man3/posix_memalign.3.html",
        "We build on 10.15 saying we target 10.13 and the headers have annotations saying \"this API is only available on 10.xx+\" so using them raises a warning.\r\n\r\nSimilarly to say building for 10.0.10240 (Threshold 1) on a machine running 10.0.19043 (21H1).",
        "There is probably some other CMake magic or define checks that could be done here, but I think that can be tracked via a separate issue for follow up.\r\n\r\nThis impacts a range of Apple products (iOS, M1, OSX, etc) and may not be trivial to do correctly on all platforms.",
        "https://github.com/dotnet/runtime/issues/54296",
        "Fixed."
      ],
      "runtime-document-configuration-intent": [
        "They've been discussed in various blogs and are fairly well known in the space as the way to test fallback paths if your hardware supports the latest.\r\n\r\nI don't think it'd be the end of the world to drop them and the devs primarily using these switches could update, but didn't want to do so without discussion.\r\n\r\nWould the `--instruction-set` flags for R2R/NAOT be in the same boat? If we could remove the extras there it would also simplify things. I don't believe the flag is considered officially supported today, but does print out on the command line.",
        "Removed the config knobs and instruction-set switches"
      ],
      "runtime-avoid-busy-waiting": [
        "I know this is existing, but this is a bug/memory safety violation.\r\n\r\nConsider the case where `location` refers to actually immutable memory. In such a case, `Volatile.Read` should succeed, but in actuality, `CompareExchange` will end up writing `0` in the case the value is actually `0`, which will cause an AV.",
        "We can't for double, but we could remove the unsafe code here for `ulong` by following the same pattern that uses `Interlocked.CompareExchange` and `Interlocked.Exchange` under the ifdef."
      ],
      "runtime-platform-aware-algorithm-optimization": [
        "> to always return false\r\n\r\n`AvxVnni*.X64.IsSupported` should return false on a 32-bit machine and on a 64-bit machine it should return the same result as `AvxVnni*.IsSupported`",
        "Is this implication going to exclude the CPUID for `AVX10_VNNI_INT`?\r\n\r\nIt wasn't clear if that would only be set for `AVX10.2` or if it was allowed to be separate"
      ],
      "runtime-maintain-consistent-formatting": [
        "nit: These should be moved up near the other `AVXVNNI` instructions so they're part of the `AVX` grouping rather than strictly the `AVX512` grouping checks.",
        "```suggestion\r\n    <GenAPITargetPath>$([MSBuild]::NormalizePath('$(MSBuildProjectDirectory)', '..', 'ref', '$(AssemblyName).netcore.cs'))</GenAPITargetPath>\r\n    <!-- SA1001: Commas should not be preceded by a whitespace; needed due to ifdef -->\r\n    <NoWarn>$(NoWarn);SA1001</NoWarn>\r\n```"
      ],
      "runtime-platform-agnostic-network-apis": [
        "Unfortunately, it isn't this simple. Stuff on Unix has to go through the PAL layer so I need effectively something like the following in PAL instead:\r\n```cpp\r\n#if defined(__APPLE__)\r\n#define sincos __sincos\r\n#endif\r\n```"
      ],
      "runtime-honor-api-contracts": [
        "Sign is supposed to throw for NaN (unlike CopySign which does not). It's a historical behavior/contract of the API."
      ],
      "runtime-choose-descriptive-names": [
        "I've renamed this in `IsRedundantMov` but not everywhere else (as `/* canSkip */ value` is used for almost every call to `emitIns_Mov`).",
        "This is in a lambda which implicitly captures the `insName` variable"
      ],
      "runtime-enable-configurable-instrumentation": [
        "```suggestion\r\nRETAIL_CONFIG_DWORD_INFO(EXTERNAL_EnableAVXVNNIINT8,            W(\"EnableAVXVNNIINT8\"),         1, \"Allows AVXVNNI8+ hardware intrinsics to be disabled\")\r\nRETAIL_CONFIG_DWORD_INFO(EXTERNAL_EnableAVXVNNIINT16,           W(\"EnableAVXVNNIINT16\"),        1, \"Allows AVXVNNI16+ hardware intrinsics to be disabled\")\r\n```"
      ],
      "runtime-specific-exceptions-with-context": [
        "We weren't entirely consistent about this throughout all of our APIs. Several of them validated `s` first and several others validated `style` first.\r\n\r\nI opted for making it consistent as `s` first since that is validating the arguments \"in order\"."
      ],
      "runtime-optimize-memory-access": [
        "This is working around the lack of variable sized `ExtractMostSignificantBits`.\r\n\r\nIt's not pretty, but it works and ideally gets replaced in .NET 8 as part of the `VectorMask` work...",
        "We now expose helper intrinsics that directly operate on `ref`: `LoadUnsafe(ref T source, nuint elementOffset)`.\r\n\r\nThis helps avoid pinning, which can have measurable overhead for small counts and which can hinder the GC in the case of long inputs.\r\n\r\nIt likewise helps improve readability over the pattern we are already utilizing in parts of the BCL where we were using `Unsafe.ReadUnaligned` + `Unsafe.Add` + `Unsafe.As`.\r\n\r\n",
        "`ExtractMostSignificantBits` behaves just like `MoveMask` on x86/x64. This is also exposed by `WASM` as `bitmask`",
        "I've preserved the `Vector256` path given that it was already here and I would presume has undergone the necessary checks to ensure it is worth doing on x86/x64.\r\n\r\nArm64 doesn't support `V256` and so will only go down the `V128` codepath.",
        "This only validates the alignment is a power of 2 as that is relatively cheap. Also validating that `byteCount % alignment == 0` would be fairly expensive.",
        "I've simplified the alignment check to just `if ((alignment % (uint)sizeof(void*)) == 0)`, which will generate simply `test dl, 7` (or `test dl, 3` for 32-bit)",
        "I'll also add in a check for `byteCount` since that can just be `(byteCount & (alignment - 1)) == 0`",
        "Yes, it does. Thanks for the catch.\r\n\r\nWill update and add a test covering this scenario.",
        "Should be resolved now.",
        "`BinaryPrimitives.ReadInt64LittleEndian` is likely \"better\" than `BitConverter`, it's at least more explicit. `MemoryMarshal.Read` is likely something we want to avoid in \"safe\" code because of where it lives (even if it does bounds check/etc).\r\n\r\nToday, we basically have to pick between being \"safe\" or being \"efficient\". We can get semi-efficient for scalars, but that's potentially leaving 2-64x perf on the table, depending on platform/scenario.\r\n\r\nIf the JIT could elide bounds checks for `V128.Create(rospan)` then we have a safe way to operate on the data. We can alternatively centralize the unsafe code behind a core helper, for example we have `InvokeSpanIntoSpan` and a pattern that makes use of \"functional interfaces + generics\" to do this for `TensorPrimitives`. This at least reduces risk as the logic that accesses memory can be well tested and not duplicated everywhere. We'll never be able to get rid of *all* unsafe code either, something like `TensorPrimitives` which needs to handle large data needs to be able to pin so it gets access to non-temporal loads/stores, for example.\r\n\r\nI think our goal should therefore really be to move paths like these to use centralized helpers instead of trying to rewrite them to be \"safe\". We know they're important to perf and we know they benefit from unsafe today, so lets pull some of the `InvokeSpanIntoSpan` and `Aggregate` helpers so that cases like this are functionally rewritten to `=> Aggregate<T, IdentityOperator<T>, Crc32Operator<T>>(x)` and all the \"dangerous\" code is centralized. When the JIT finally gets support for eliding bounds checks over `V128.Create(rospan)` that one helper can be rewritten to use the safe pattern and the rest of the BCL implicitly gets it. We can add a number of `Debug.Assert`, memory access checks, and even potentially `opt-in` forced validation path (such as would lightup via a feature switch) to help ensure robustness in the interim.",
        "> I get that this pattern is centralizing the unsafe code, but I do not think that it reads well.\r\n\r\nIt doesn't today, but the long term goal would be to get something like `functional interfaces` into the language so that you can rather have something like `=> Aggregate(span, (previous, value) => Crc32(previous, value))` instead and the lambda binds to the `TOperation` given `where TOperation : IOperation<....>` so that it becomes \"prettier\"\r\n\r\nWe're not always going to be able to hit the trifecta of `safe`, `efficient`, and `pretty`. We can, however, strike a decent enough balance between them and as a long term goal push towards improving the scenarios where it falls down."
      ],
      "runtime-model-actual-hardware-costs": [
        "I don't think these costs are \"accurate\" and likely deserve more investigation in the future. It also isn't accounting for SIMD/Mask. -- We have a similar issue with some of the costs in the gen tree computation, where a lot of them were modeled around the x87 FPU and never updated to account for modern SIMD considerations.\r\n\r\nHere's the rough costs of loads:\r\n* a 32-bit integer move: `encoding: 2 bytes; execution: 2-5 cycles`\r\n* a 64-bit integer move: `encoding: 3 bytes; execution: 2-5 cycles`\r\n* a 32-bit floating-point move: `encoding 4 bytes; execution: 4-7 cycles`\r\n* a 64-bit floating-point move: `encoding 4 bytes; execution: 4-7 cycles`\r\n* a 128-bit simd move: `encoding 3-4 bytes; execution: 4-7 cycles`\r\n* a 256-bit simd move: `encoding 4-5 bytes; execution: 5-8 cycles`\r\n* a 512-bit simd move: `encoding 6 bytes; execution: 5-9 cycles`\r\n\r\nStores tend to be up to more expensive than loads. You get roughly `4-10 cycles` for simd and floating-point, but nearly `2-10 cycles` for integers\r\n\r\n",
        "This logic hasn't really been applicable in a long time. We have single instruction spill/reload regardless of SIMD size, it's just that TYP_SIMD32/64 require spilling since the upper bits are volatile and we may only spill 128-bits instead of all bits, if possible (and which is no more expensive to do).\r\n\r\nA long time ago we did have more complex logic for upper save/restore, but now we can just follow the normal callee save/restore consideration instead.",
        "> varTypeIsSIMD(tree) is redundant with tree->TypeIs(TYP_SIMD32, TYP_SIMD64).\r\n\r\n`varTypeIsSIMD(tree)` is cheaper and ensures the common case is 1 check (just checking the `varTypeClassification`).\r\n\r\n> Why have tree->OperIsHWIntrinsic() branch as well?\r\n\r\nThere are nodes that produce scalar results or small types but which still operate on V256/V512. It's an `||` condition so its handling either `node that produces a simd32/64` or `hwintrinsic node that uses simd32/64`\r\n\r\n> This will not work for tree that contain SIMD expression but do not evaluate to a SIMD/HWI.\r\n\r\nFor some arbitrary block copies or other operations, potentially. Most usages of SIMD32/64 are done via hwintrinsics, so this is getting the important bits.\r\n\r\n> The comment mentions ...such constants..., but the code does not contain checks for constants, so it is confusing.\r\n\r\nWill fix.\r\n",
        "Talked with @kunalspathak and this is currently needed as we still try to do alignment when optimizations are disabled.\r\n\r\nWe may want to revisit that since OSR + TC should handle all the important cases and aligning debug code likely isn't worth the cycles required.",
        "This covers the crossgen scenario, correct?\r\n```suggestion\r\n    const bool  isReadyToRun           = opts.IsReadyToRun();\r\n    const bool  resolveTokens          = makeInlineObservations && (isTier1 || isReadyToRun);\r\n```",
        "Will switch to that. Could I get a summary of the difference between the two for future reference?",
        "```suggestion\r\n    const bool  isPreJit               = opts.jitFlags->IsSet(JitFlags::JIT_FLAG_PREJIT);\r\n    const bool  resolveTokens          = makeInlineObservations && (isTier1 || isPreJit);\r\n```",
        "Rather than take `emitDataAlignment` which only had `None`, `Preferred`, and `Required`, we just pass in the actual alignment we want. This is normally the same as the constant size, but it isn't required to be (as is the case for `SMALL_CODE` where we want an alignment of whatever is smallest and works for the architecture).",
        "It didn't when I checked but most of the methods that involve vector constants also don't contain other constants."
      ]
    },
    "profile": {
      "location": "Lake Stevens, WA",
      "company": "Microsoft",
      "blog": "",
      "twitter_username": "tannergooding",
      "site_admin": false,
      "followers": 425,
      "following": 1
    }
  },
  "n1t0": {
    "repos": [
      "huggingface/tokenizers"
    ],
    "entries": [
      {
        "slug": "tokenizers-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "tokenizers-choose-semantically-clear-identifiers",
        "title": "Choose semantically clear identifiers"
      },
      {
        "slug": "tokenizers-consistent-api-design",
        "title": "Consistent API design"
      },
      {
        "slug": "tokenizers-flexible-tokenizer-implementation",
        "title": "Flexible tokenizer implementation"
      },
      {
        "slug": "tokenizers-handle-nullable-types-idiomatically",
        "title": "Handle nullable types idiomatically"
      },
      {
        "slug": "tokenizers-minimize-memory-allocations",
        "title": "Minimize memory allocations"
      },
      {
        "slug": "tokenizers-modular-model-components",
        "title": "Modular model components"
      },
      {
        "slug": "tokenizers-optimize-workflow-triggers",
        "title": "Optimize workflow triggers"
      },
      {
        "slug": "tokenizers-prefer-explicit-api-design",
        "title": "Prefer explicit API design"
      },
      {
        "slug": "tokenizers-prioritize-tokenizer-simplicity",
        "title": "Prioritize tokenizer simplicity"
      },
      {
        "slug": "tokenizers-purpose-indicating-descriptive-names",
        "title": "Purpose-indicating descriptive names"
      },
      {
        "slug": "tokenizers-pythonic-api-design",
        "title": "Pythonic API design"
      },
      {
        "slug": "tokenizers-return-results-not-panics",
        "title": "Return results not panics"
      },
      {
        "slug": "tokenizers-simplify-for-readability",
        "title": "Simplify for readability"
      },
      {
        "slug": "tokenizers-smart-configuration-defaults",
        "title": "Smart configuration defaults"
      },
      {
        "slug": "tokenizers-thread-safe-resource-sharing",
        "title": "Thread-safe resource sharing"
      }
    ],
    "comments": {
      "tokenizers-handle-nullable-types-idiomatically": [
        "Is there any reason for `get_as_subtype` to return `PyResult`? If not, this could be rewritten as\r\n```Rust\r\nfn get_normalizer(&self) -> Option<PyObject> {\r\n    self.tokenizer.get_normalizer().map(|n| n.get_as_subtype())\r\n}\r\n```",
        "I was wondering about those optional parameters. If we were to change the default to some value, it wouldn't be possible with the current setup to actually set them back to `None`. How do you think we could handle this?"
      ],
      "tokenizers-prioritize-tokenizer-simplicity": [
        "I don't understand why you want to add these in all the tests. The pre-tokenizer won't get called anyway because all the tokens in the tests come are handled by the `AddedVocabulary`.",
        "The added vocabulary is always handled since it happens first, so no need for a pre-tokenizer. The tests actually started to fail because the BPE is receiving the whitespace between each word, and it doesn't have an `unk_token`. With the `Whitespace` pre-tokenizer, these whitespaces are just removed.\r\n\r\n> We probably should really test the output of those methods in the full to make sure it's consistent, I'm pretty sure all those tests would have caught the missing unk tokens otherwise.\r\n\r\nThe unk token was treated this way by design. When provided we use it, otherwise the unknown tokens just get ignored. This is not a bug we didn't catch. That being said, feel free to add all the tests you find necessary, we never have enough of them :slightly_smiling_face: "
      ],
      "tokenizers-smart-configuration-defaults": [
        "What do you think about always trying to display in a notebook if `IPython` is available, and always returning the content? That way it just displays when possible, without the need to change this value."
      ],
      "tokenizers-optimize-workflow-triggers": [
        "Is this step doing something that the next one doesn't? Otherwise, since the `npm_publish` depends on `rust_publish` we can probably remove it. If one of the builds (in `rust_publish`) for any of the os/node were to fail, it would just fail the whole job anyway, right?\r\nI think it would maybe make more sense to run the `build_all` job for each commit, to make sure we don't break anything in these bindings. What do you think?",
        "Sure but the `Rust` job doesn't check that the bindings still compile after something has been modified in the main library. At each commit, I think we would like to make sure everything is still compiling: the main library, but also all the bindings.",
        "Nice thanks! You are right, at the moment it's totally useless. I don't know what would be better between having a separate workflow for the main library and each bindings, or actually having one main `Build` workflow, that takes care of building everything on each OS, mutualizing the setup parts. What do you think? (This is out of the scope of this PR though)."
      ],
      "tokenizers-return-results-not-panics": [
        "It probably would be better to make `from` return a `Result` and fail gracefully since this is a public API."
      ],
      "tokenizers-modular-model-components": [
        "As we discussed, a lot of these options might be better suited as standalone `Normalizer` or `PreTokenizer`, and removed from there. ",
        "These serve different purpose. `PreTokenizedString::tokenize` is deep inside the `encode` pipeline, and some use-cases require customization."
      ],
      "tokenizers-pythonic-api-design": [
        "The format for merges here seems overly complicated. I agree that we use this specific format internally, but it is probably unnecessary to expose this to the final user. `List[Tuple[str, str]]` is easier to construct from all the different files format that exist out there, and also let us construct the final `merges` easily. It is also probably compatible with a lot more languages.",
        "Sure I understand. The API on the Rust side has been thought with `from_file` as the main entry point though, so this aspect has to be taken care of now that we want to expose this.\r\nWe should change the Rust API accordingly, and let the BPE build the `HashMap<Pair, (u32, u32)>` by itself.",
        "Thank you for this PR @kdexd! If I remember correctly `with_added_tokens` should be a named argument and so I don't think it works if provided as a positional argument."
      ],
      "tokenizers-choose-optimal-data-structures": [
        "`inserted` has been added here because we are changing `pieces` to be a `Vec<(String, f64)>` instead of a `HashMap<String, f64>`, so we still need a way to check if a token has been inserted with `O(1)`"
      ],
      "tokenizers-simplify-for-readability": [
        "You can directly return `normalized.replace(...)` here."
      ],
      "tokenizers-choose-semantically-clear-identifiers": [
        "We can probably keep it as an argument, but I would go for an Enum too. Was thinking about the name, and I find `side` easier to understand maybe, but for consistency with padding, we might prefer `direction`. Wdyt?",
        "I'm not sure it really makes sense directly exposed on the `Tokenizer`. I'd expect this to be the number of added tokens that gets returned by `get_vocab_size(with_added_tokens=True)`.\r\n\r\nThis can simply be retrieved by doing `tokenizer.post_processor.num_added_tokens(is_pair)`",
        "I understand. My main concern is just about having `add_tokens` that does something, and `num_added_tokens` just right beside it, doing something completely different. If we find the right name, we can add it here!",
        "As discussed, let's use `num_special_tokens_to_add`"
      ],
      "tokenizers-consistent-api-design": [
        "Same here. Also, shouldn't the default value be \"right\" (false here)?",
        "See, that's where I'm confused (and why I'm wondering about `direction`). For me the current implementation only supports truncation on the right (namely at the end of the different vecs), and we want to add truncation on the left (see #779 also). ",
        "Yes, it's just for consistency. The user can manually merge multiple `Encoding`s, and this allows to manually set the sequence id for each of them."
      ],
      "tokenizers-prefer-explicit-api-design": [
        "As discussed, we'll remove the output with the sequence id, and will just keep the classic one. The user can call `token_to_sequence` when needed to determine it.",
        "I'm wondering if it makes sense to expose `vocab`, `vocab_r`, and `merges` separately. I think we should always provide either vocab/merges or nothing. What do you think?",
        "(With the `vocab_r` being built from `vocab` anyway)",
        ":+1: "
      ],
      "tokenizers-thread-safe-resource-sharing": [
        "Sure, we need them because anything in an `Arc` is immutable. These `RwLock` provides us with a way to actually mutate the `Model` here.",
        "The `unwrap` here is for the `std::sync::LockResult` that is returned by the `RwLock` when you try to access its content. The `Err` case happens when a thread that was holding the lock panicked. So since this shouldn't happen, and we don't want to recover from it, I think the `unwrap` should be ok."
      ],
      "tokenizers-purpose-indicating-descriptive-names": [
        "For consistency with current `implementations`, the constructor should probably expect file by default, and provide another way to build with in-memory data.",
        "What about `albert_tokenizer` instead of `precompiled_files` which seems off? Also, we can probably return `albert_base` directly since its a single file in this case.",
        "Yes I agree that we'll probably add more, but it might be better to keep each set of files separated though. When you require `albert-base` you don't necessarily want to download all the serialized files, and having logically separated fixtures let us do that.\r\nI don't mind keeping a dict, but I felt that with serialized tokenizers it would just add repetition (as opposed to the vocab/merges used for the others)."
      ],
      "tokenizers-flexible-tokenizer-implementation": [
        "This won't work with a lot of tokenizers that do not use `[UNK]` as their unknown token. Unfortunately there is no easy way at the moment to get the unk token directly from the tokenizer, but maybe we can use something a bit more large. Maybe something like the regex `/^(.{1}\\b)?unk(\\b.{1})?$/i` would work for now (https://regex101.com/r/zh0He9/1/)"
      ],
      "tokenizers-minimize-memory-allocations": [
        "You can probably get `&PyBytes` and then a slice, to avoid allocating a `Vec` here."
      ]
    },
    "profile": {
      "location": "New York",
      "company": "@huggingface ",
      "blog": "https://twitter.com/moi_anthony",
      "site_admin": false,
      "followers": 318,
      "following": 11
    }
  },
  "VladLazar": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-adaptive-cache-expiration-strategy",
        "title": "Adaptive cache expiration strategy"
      },
      {
        "slug": "neon-balance-flexibility-with-performance",
        "title": "Balance flexibility with performance"
      },
      {
        "slug": "neon-cache-performance-preservation",
        "title": "Cache performance preservation"
      },
      {
        "slug": "neon-comprehensive-code-documentation",
        "title": "Comprehensive code documentation"
      },
      {
        "slug": "neon-configurable-cache-parameters",
        "title": "Configurable cache parameters"
      },
      {
        "slug": "neon-connection-pooling-with-pipelining",
        "title": "Connection pooling with pipelining"
      },
      {
        "slug": "neon-database-before-memory",
        "title": "Database before memory"
      },
      {
        "slug": "neon-design-metrics-for-insights",
        "title": "Design metrics for insights"
      },
      {
        "slug": "neon-document-api-specs-completely",
        "title": "Document API specs completely"
      },
      {
        "slug": "neon-document-concurrency-design-decisions",
        "title": "Document concurrency design decisions"
      },
      {
        "slug": "neon-document-parameter-choices",
        "title": "Document parameter choices"
      },
      {
        "slug": "neon-ensure-algorithm-robustness",
        "title": "Ensure algorithm robustness"
      },
      {
        "slug": "neon-environment-specific-config-defaults",
        "title": "Environment-specific config defaults"
      },
      {
        "slug": "neon-log-level-appropriately",
        "title": "Log level appropriately"
      },
      {
        "slug": "neon-minimize-unnecessary-allocations",
        "title": "Minimize unnecessary allocations"
      },
      {
        "slug": "neon-reliable-concurrency-synchronization",
        "title": "Reliable concurrency synchronization"
      },
      {
        "slug": "neon-stage-configuration-changes-gradually",
        "title": "Stage configuration changes gradually"
      },
      {
        "slug": "neon-structure-endpoints-for-rest",
        "title": "Structure endpoints for REST"
      }
    ],
    "comments": {
      "neon-stage-configuration-changes-gradually": [
        "Why do we support propagating these configs in two ways?",
        "Should these be optional? It allows us to try it out in staging/pre-prod without deploying the new endpoint everywhere in prod."
      ],
      "neon-database-before-memory": [
        "Another general guideline: aim to update the database state __before__ the in-memory state. Otherwise, if you crash in between it will look like you time travelled since storcon rebuilds the world state from the db.",
        "`schedule_compaction_update`, yeah that should, but the name is bad :laughing: \r\n\r\nFair point. I'll just unlink here.",
        "https://github.com/neondatabase/neon/pull/12038/commits/2053e7f4a4b8f838159008d0e7841c454bda4459"
      ],
      "neon-design-metrics-for-insights": [
        "Is this metric useful at timeline granularity? If the goal is to get a sense of the overall PS background operations, you can look at `rate(pageserver_storage_operations_seconds_count)`, but that's a per-pageserver view.\r\n\r\nAsking, since this is pretty high cardinality.",
        "This metric is being bumped multiple times for the same request. I think it makes sense to remove the increment from resolve and keep just this one (this is when we actually tell the compute to reconnect).",
        "Pushed [b01579a](https://github.com/neondatabase/neon/pull/12467/commits/b01579afad6b604597c95234d59a031c8b5f2329). PTAL",
        "Not sure I understand this. If the current offloader is indeed lagging, then the call to `determine_offloader` would have already picked a different offloader. I mean `offloader_sk_id` might not actually be the current offloader."
      ],
      "neon-configurable-cache-parameters": [
        "I don't have good intuition on what the right size for this cache is. Therefore, I'd make this a tenant config since those are configurable at run-time. The data structures supports capacity resizing via `resize_capacity`.\r\n\r\nThis would give us a tool to address replicas with slow get pages due to rel size cache churn.",
        "Please plug into `Timeline::tenant_conf_updated` and resize the cache when the config changes.\r\nOtherwise, the config change still requires a restart."
      ],
      "neon-environment-specific-config-defaults": [
        "The production defaults don't make sense for the test env.\r\nThis will likely become a tenant config at some point, so we can remove this oddity then."
      ],
      "neon-minimize-unnecessary-allocations": [
        "Would be good to avoid an extra allocation here. Could track the block number in `Self::response` and put everything in the right place with `Vec::spare_capacity_mut`.",
        "Sounds good. Can optimize later if needed."
      ],
      "neon-ensure-algorithm-robustness": [
        "This isn't entirely correct. It doesn't handle:\r\n* overlapping layers\r\n* delta layers containing a full page image\r\n\r\nIn some cases it would panic and in others it would just yield an incorrect result.\r\nThe correct way of doing this is to use a neon local setup, import the tenant state from s3 and then use the HTTP endpoints to get the reconstructed page: `GET v1/tenant/<tid>-<shard-id>/timeline/<tlid>/getpage?lsn=<lsn>`.",
        "Indeed the check was wrong. Good catch.\r\n\r\nIt's not about bound checking since we don't blindly index into the jobs.\r\nIt narrows collisions down to collisions on plans with the same length.\r\n\r\n> Wonder why we wouldn't go all the way and compute a cryptographic checksum\r\n\r\nI don't think we'll have enough on-going imports at any given time to worry about collisions here.\r\nFor 1 million the collision probability is ~2.71e-08. This is without accounting for imports of the same length check.",
        "Fixed in https://github.com/neondatabase/neon/pull/11862/commits/5ec58a723b11e0370dab9c8b6cd0d64ceab414e2",
        "Done: https://github.com/neondatabase/neon/pull/11862/commits/27458a203b18510e20c57bf6feb229a171f59cb1"
      ],
      "neon-log-level-appropriately": [
        "Let's log a warning here. It's not that big of a deal for draining since we expect the node to come back, but here we're removing it. Also, these comments mention draining. Can you update them to avoid confusion?",
        "Don't think it's useful to log thousands of layers that we are keeping. We log removed layers somewhere else - that's what could actually be useful. Could also contribute to the loop taking longer than it should.",
        "Please update this log line and comment to mention that we place a tombstone instead of deleting."
      ],
      "neon-document-parameter-choices": [
        "Would be good to leave a comment about why `retry_on_failures` is required."
      ],
      "neon-document-concurrency-design-decisions": [
        "https://github.com/neondatabase/neon/pull/11937/commits/9377e9af65921e06e25e4cec5607150faad56a1a\r\n\r\nI don't think a deadlock is possible here. The only place where we grab both locks at the same time is `write_to_disk` and in that case they're read locks.",
        "> We're relying on inner being append-only for this to be safe, yeah?\r\n\r\nCorrect.\r\n\r\n> Let's specify that as part of the locking protocol.\r\n\r\nDon't see how you'd express that via the locking protocol. Can you elaborate?",
        "Makes sense. Done in https://github.com/neondatabase/neon/pull/11937/commits/721e8989b0968023d06f8b52e6b559fc6af2cded",
        "The API sucks.\r\n\r\n`freeze` and `put_batch` are synchronized via `Timeline::write_lock`.\r\nWe only `freeze` when there's no ongoing writes. There's two paths on which we freeze:\r\n1. Via the active `TimelineWriter`. This holds the `Timeline::write_lock` for its lifetime. The rolling is handled in `TimelineWriter::put_batch`. It's a `&mut self` function so can't be called from different threads.\r\n2. In the background via `Timeline::maybe_freeze_ephemeral_layer`. This only proceeds if `try_lock` on `Timeline::write_lock` succeeds (i.e. there's no active writer), hence there can be no concurrent writes.",
        "https://github.com/neondatabase/neon/pull/11855/commits/af641fe64ac386a712952f531076d476eae6555e"
      ],
      "neon-adaptive-cache-expiration-strategy": [
        "The compute is available during pre-warm, correct? Wondering if there's any interactions between the user workload and prewarm that are worth considering."
      ],
      "neon-cache-performance-preservation": [
        "We terminate the old primary before promoting the secondary. If I'm reading this right, there's a an availability gap between the termination and promotion. Is this reading correct? If so, can we avoid it?",
        "It would be great to aim for something where the happy path has no hiccups. If that's not possible, let's be explicit about it and say why.\r\n\r\n---\r\n\r\nProxy learns about about the new primary after the promotion of the new compute, so there should be no queries on it at that point (according to the diagram). It seems like we can swap promotion and termination around while maintaining this property.\r\n\r\nWhat would happen to the client in this case? Let's say we have an in-progress query when the old primary terminates. Client would probably retry and eventually get routed to the new primary.\r\n\r\n",
        "> Two computes can Never, NEVER, never both be Primary on the same timeline at the same time. CPlane is supposed to make sure of that, and Compute will fail if it doesn't. Promotion of the replica before the original Primary shut down will cause errors, panics, data loss, shutdowns, and/or nasal deamons on the Primary, this promoting Secondary, or both.\r\n\r\nAt the risk of being annoying: why? I understand it doesn't work in the case when two primaries are being written to. But what about if we could guarantee the new primary is idle? More specifically, by idle I mean it would have to not write any WAL and be in some sort of consensus observer role.\r\n\r\nI'm sure you're right about this working with the current state of afairs, but I'm curious about what the technical roadblock is."
      ],
      "neon-balance-flexibility-with-performance": [
        "> Let's consider an extreme case, for illustration: we send a batch request for pages 0-100, and at page 50 we have to download a layer which takes 3 seconds. I think there are cases where it's advantageous to send back pages 0-49 first, and pages 50-100 later:\r\n\r\nOn streaming: the pageserver doesn't currently support streaming. The current implementation collects all the deltas and images for _all_ keys in the batch and then does walredo for all keys concurrently. We could do true streaming, but it's not trivial.\r\n\r\nOn how scattered the reads should be: hard to tell. Generally speaking, the more clustered the pages in a request are, the more predictable performance will be:\r\n* in-memory layers are ingest ordered: clustering by key doesn't help much here\r\n* delta layers are ordered by (key, lsn): clustering by key helps with index reads, but for the reads themselves our ability to merge depends on how many key versions we have\r\n* in-memory layers are ordered by key: clustering keys clearly helps here\r\n\r\nOverall take:\r\n* I'm not sure streaming responses makes sense at this stage. We currently support a max of 32 keys in a batch (PS side). We could increase it, but I don't think we should go above 512 pages short term. I'd just return all the pages at once. Streaming protocol can be done in the future, but I see there's other reasons quoted for streaming.\r\n* I think it makes sense to keep the grpc spec somewhat flexible here. The implementation can enforce strict batching rules or implement heuristics based on key, lsn range.",
        "> think the more important point here is that when we do the batching client-side, we know whether it makes sense to batch or not (i.e. whether to prioritize throughput or latency)\r\n\r\nWith the more flexible protocol, the communicator could coalesce batches, but perhaps we don't care about that (yet?)."
      ],
      "neon-connection-pooling-with-pipelining": [
        "What's the point of this extra channel now? Can't we just plug `caller_rx` into `client.get_pages`?",
        "Makes sense. The extra stream still allows for pipelining as long as the previous request was pushed down the tcp pipe (response need not be received by that point)."
      ],
      "neon-structure-endpoints-for-rest": [
        "This should be `POST`.",
        "This should be:\r\n`PUT`: if the endpoint does the offload before returning\r\n`POST`: if the endpoint does the offload in the background"
      ],
      "neon-document-api-specs-completely": [
        "Let's be kind to future readers and mention why it's time consuming."
      ],
      "neon-reliable-concurrency-synchronization": [
        "Would calling `env.storage_controller.reconcile_until_idle()` to force the storage controller to run all pending reconciliations work?",
        "This will hang since `NeonPageserver.start` waits for the node to be marked as active in the storage controller by default. Re-attaching is a pre-condition for that. To avoid this, you can do:\r\n```\r\nps.stop()\r\nps.start(await_active=False)\r\n```"
      ],
      "neon-comprehensive-code-documentation": [
        "nit: a comment would be nice:\r\n* explain difference between effective and request lsn\r\n* explain that primary compute always uses `request_lsn == MAX`\r\n* hint that this is how the compute thinks about get page requests"
      ]
    },
    "profile": {
      "location": "London, United Kingdom",
      "blog": "",
      "site_admin": false,
      "followers": 16,
      "following": 4
    }
  },
  "yottta": {
    "repos": [
      "opentofu/opentofu"
    ],
    "entries": [
      {
        "slug": "opentofu-clear-concise-documentation",
        "title": "Clear concise documentation"
      },
      {
        "slug": "opentofu-clear-relationship-descriptions",
        "title": "Clear relationship descriptions"
      },
      {
        "slug": "opentofu-contextualize-security-findings",
        "title": "Contextualize security findings"
      },
      {
        "slug": "opentofu-craft-actionable-errors",
        "title": "Craft actionable errors"
      },
      {
        "slug": "opentofu-document-intent-and-limitations",
        "title": "Document intent and limitations"
      },
      {
        "slug": "opentofu-document-phased-migration-paths",
        "title": "Document phased migration paths"
      },
      {
        "slug": "opentofu-document-reference-standards",
        "title": "Document reference standards"
      },
      {
        "slug": "opentofu-explicit-versus-dynamic-configurations",
        "title": "Explicit versus dynamic configurations"
      },
      {
        "slug": "opentofu-log-effectively-for-debugging",
        "title": "Log effectively for debugging"
      },
      {
        "slug": "opentofu-minimize-api-surface",
        "title": "Minimize API surface"
      },
      {
        "slug": "opentofu-optimize-cicd-workflows",
        "title": "Optimize CI/CD workflows"
      },
      {
        "slug": "opentofu-prevent-backing-array-surprises",
        "title": "Prevent backing array surprises"
      },
      {
        "slug": "opentofu-proper-span-lifecycle",
        "title": "Proper span lifecycle"
      },
      {
        "slug": "opentofu-protect-infrastructure-secrets",
        "title": "Protect infrastructure secrets"
      },
      {
        "slug": "opentofu-review-consistency-assumptions",
        "title": "Review consistency assumptions"
      },
      {
        "slug": "opentofu-safe-lock-patterns",
        "title": "Safe lock patterns"
      },
      {
        "slug": "opentofu-separate-configuration-lifecycles",
        "title": "Separate configuration lifecycles"
      },
      {
        "slug": "opentofu-specify-configuration-behaviors",
        "title": "Specify configuration behaviors"
      }
    ],
    "comments": {
      "opentofu-protect-infrastructure-secrets": [
        "> That raises another question, we should explain how we shouldn't use this with state encryption (due to the ephemerality) because some people may think \"This is great for secrets, i can use it for my passphrase\" and have issues.\r\n\r\nI am not sure that I really get what you are suggesting here. Could you expand it more please?",
        "Please check this out: ae712fdaf7a5ba4b69cfcf0e65d960df7d301dac",
        "In 91f105241ac49c3c20ba51d52306be62c7ac4c03"
      ],
      "opentofu-clear-relationship-descriptions": [
        "Wanted to add the suggestions individually but the signoff of the commit is not working correctly from the GH UI, so I added all your suggestions in one commit: 4ee1ec72ed946b32d50637bb893c4c754d1b6e4e"
      ],
      "opentofu-safe-lock-patterns": [
        "Shouldn't `Sync` be under the the mutex lock too? This PR is changing that behaviour. Is this on purpose?",
        "`note`\r\nThis locking specific order (s3 first, dynamo second) is done this way keep the feature parity. From the tests ran locally, terraform is also doing both in case locking is enabled for s3+dynamodb and is doing those in that specific order.\r\nTerraform flow from the [localstack](https://www.localstack.cloud/) logs:\r\n```\r\nlocalstack.request.aws     : AWS iam.GetUser => 200\r\nlocalstack.request.aws     : AWS s3.ListObjectsV2 => 200\r\nlocalstack.request.aws     : AWS s3.PutObject => 200 <- acquire s3 lock\r\nlocalstack.request.aws     : AWS dynamodb.PutItem => 200 <- acquire dynamo lock\r\nlocalstack.request.aws     : AWS s3.HeadObject => 200\r\nlocalstack.request.aws     : AWS s3.GetObject => 206\r\nlocalstack.request.aws     : AWS dynamodb.GetItem => 200\r\nlocalstack.request.aws     : AWS s3.HeadBucket => 200\r\n...\r\nlocalstack.request.aws     : AWS s3.HeadBucket => 200\r\nlocalstack.request.aws     : AWS s3.GetBucketTagging => 404 (NoSuchTagSet)\r\nlocalstack.request.aws     : AWS s3.GetObject => 200\r\nlocalstack.request.aws     : AWS s3.DeleteObject => 204 <- release s3 lock\r\nlocalstack.request.aws     : AWS dynamodb.GetItem => 200\r\nlocalstack.request.aws     : AWS dynamodb.DeleteItem => 200 <- release dynamo lock\r\n```",
        "Good question! I was asking myself the same thing, but I see that `context.TODO` is used everywhere in this package.\r\nI am not aware yet, but maybe there are some plans on tackling this. Any ideas @cam72cam?"
      ],
      "opentofu-optimize-cicd-workflows": [
        "Configured this to run once a week, on Sunday morning. Also, weird minute section configuration due to this mention from [GH actions docs](https://docs.github.com/en/actions/writing-workflows/choosing-when-your-workflow-runs/events-that-trigger-workflows#schedule):\r\n> The `schedule` event can be delayed during periods of high loads of GitHub Actions workflow runs. High load times include the start of every hour. If the load is sufficiently high enough, some queued jobs may be dropped. To decrease the chance of delay, schedule your workflow to run at a different time of the hour.",
        "Perhaps we could change this later to fit our future needs.",
        "Changed in d7bd8375e8a0c9ccc46fc607eb2591595c0b865b"
      ],
      "opentofu-log-effectively-for-debugging": [
        "`suggestion`\r\nWe could at least log the error if occurs.",
        "Could we log it at least? 🤔 ",
        "`suggestion`\r\nFor being able to adjust this parsing in the future, we could have a trace log here writing the `fullName`. This way, in case somebody is encountering \"unknown\" traces, we could ask for the logs and adjust this parsing accordingly.",
        "`suggestion`\r\nA trace log maybe? Maybe somebody really wants to debug this if something is not working for them.",
        "I am thinking that for somebody that is playing around with setting this up, could upload the artifacts with a wrong artifactType and then have a hard time figuring it out.",
        "```suggestion\r\n\t\t\tlogger.Printf(\"[WARN] failed to fetch provider package; retrying\")\r\n```",
        "I would say that this is more of a warning than an info 🤔 Personally, I see INFO as an action that finished and the log needs to show the output of that. Finished in this context mean one of the following:\r\n* that it finished successfully\r\n* it finished with an error but there is a another possible success case on handling that error\r\n\r\nThe fact that an action failed and it's retried, is more of a failure, but not yet an interruptible one."
      ],
      "opentofu-review-consistency-assumptions": [
        "Really good point!\r\nFrom a quick check, I see that some already implemented it but there are still some that didn't yet.\r\nAdded a warning about it e22385a328c24a07904fa422aad15849b57ff047",
        "I used terms like \"digest\", \"md5\" or \"digest file\" for the part where OpenTofu is writing an md5 hash of the statefile.\r\nFor example, looking into the dynamoDB, there is the md5 sum of the state file. Can be easily checked like this:\r\n![image](https://github.com/user-attachments/assets/7dc94a57-407b-4531-b6db-02bc2e24d10b)\r\n\r\n\r\n[Here](https://github.com/opentofu/opentofu/blob/6614782/internal/backend/remote-state/s3/client.go#L352C24-L374) is where this is handled in OpenTofu.\r\n\r\nIn that `Digest updates` section, I am talking about this part.",
        "Updated this section due to not needed to handle this digest file anymore.\r\nThis should be handled only when the DynamoDB locking is enabled. That's due to having two sources of information when it comes to locking and it was used to ensure that the state object from the S3 bucket was not altered separately, without acquiring a lock.",
        "Added in 38f1eb921775d9ac783fffe8c8cb3d2b11a67419"
      ],
      "opentofu-contextualize-security-findings": [
        "`question`\r\nI see no label that we could assign to these issues. Any idea of one? Or should we create a new one?",
        "@cam72cam created a new label called `govulncheck` for these. Added in 4963f351b5e15206be46ce015fcb048605810576",
        "Would be possible, indeed. That's what I wanted to do initially.\r\nThough, I opted on including the workflow run url instead in the summary of the issue. ([Example of a vulnerability affecting v1.7](https://github.com/opentofu/opentofu/actions/runs/14333067260/job/40173357398#step:6:18627))\r\nThis way, whoever is working on fixing that vulnerability, will be able to inspect the findings of govulncheck.\r\n\r\nExtracting and writing entire stacktraces might be a little bit noisy on the issues, IMO, even though it will make the life easier for the one that works on solving it.\r\nAnother thing, is that the issues are reported per vulnerability and it reports all the versions that are affected, but different versions could have different stacktraces on how is calling the affected code.\r\n\r\nAnd yes, if we want to do this, would be advisable to do it in another language.",
        "Yes, that will work for sure because the `--search` argument in `gh issue list` is actually a query string, not only the title.\r\nSo whatever will be added after the title or in the description of it will not affect the search.\r\n\r\nThough, I am not sure that I understand why this would be better compared with adding a comment into the issue.\r\n\r\nBTW, had a conversation with @cam72cam and we will try to use security advisories instead of issues for this functionality. I will try to do it today and also test your idea that you suggested here.",
        "About the security advisories. Played around with the [API](https://docs.github.com/en/rest/security-advisories/repository-advisories) to see what it can do, but sadly it's quite restrictive and there is not really a  straight-forward way to test the changes. The API key that I can generate it can access only the published opentofu security advisories. Tried to test with one of my repos, but since I don't have any published/closed advisories, was returning only the draft one.\r\nEven further, the API to work with advisories is supporting filtering only on the `state` field which is not enough for our use case.\r\n\r\nTalked with @cam72cam and we are going with issues.",
        "Great arguments for the title topic! I totally agree with the issue to advisory flow.\r\nI would like to go forward with this approach and then reiterate later if we find out that the work with this flow is hard or not clear enough.\r\n\r\nThis is rising some ideas that could help with the flow for others not involved in the development of this:\r\n* I would add in the description of the ticket a note on how to work on the issue. We can have a short file documenting this and we can point to that in each ticket description, or we can have just a small note like:\r\n   > [!NOTE] \r\n   > * _Additional information can be added to the title as long as the original keywords are kept._\r\n   > * _Check also the pipeline run linked to get a better understanding of the source of the vulnerability for each version._\r\n* I would also lock the issue. What do you think about this? This kind of ticket should be handled by the core team IMO. Therefore, locking the issue will allow conversations only from the core team, avoiding pollution of the conversation and the investigation of it."
      ],
      "opentofu-craft-actionable-errors": [
        "```suggestion\r\n\t\toc := configOutputs[outputName]\r\n\t\tif prevStateOutput.Sensitive && !oc.Sensitive {\r\n\t\t\tdiags = diags.Append(&hcl.Diagnostic{\r\n\t\t\t\tSeverity: hcl.DiagWarning,\r\n\t\t\t\tSummary:  \"Output change in sensitivity\",\r\n\t\t\t\tDetail:   fmt.Sprintf(\"Sensitivity of the output %q changed. By doing so, the value will not be obfuscated anymore.\", oc.Name),\r\n\t\t\t\tSubject:  oc.DeclRange.Ptr(),\r\n\t\t\t})\r\n\t\t}\r\n```\r\n\r\nBy using a different way to initialise a diagnostic, we can provide that `Subject` field that will allow providing a better guidance to the user.\r\nThe change above will generate a warning similar to this one:\r\n![Screenshot 2025-06-02 at 15 15 32](https://github.com/user-attachments/assets/b30521cc-2517-406c-b76b-305ccee184a4)\r\nWith that indication of where the output that the warning is talking about can be located.",
        "Before\r\n![Screenshot 2025-03-17 at 11 20 04](https://github.com/user-attachments/assets/af02e9c1-e367-4df8-b169-fc264297c689)\r\nAfter\r\n![Screenshot 2025-03-17 at 11 22 52](https://github.com/user-attachments/assets/7ad66b64-607f-4c7d-b5f8-e8aca7b0efe3)\r\nI would suggest to enhance this to make it even better, like this:\r\n![Screenshot 2025-03-17 at 11 24 16](https://github.com/user-attachments/assets/d5fc77bc-1c4e-467d-b9bf-12da1e605924)\r\n```suggestion\r\n       \terrs = append([]error{fmt.Errorf(\"decryption failed for all provided methods\")}, errs...)\r\n\terrMessage := errors.Join(errs...).Error()\r\n```",
        "Indeed, that will not work as expected.\r\nYou could make use of a function like this:\r\n```golang\r\nfunc keyProvidersStack(stack []config.KeyProviderConfig) ([]string, hcl.Diagnostics) {\r\n\tres := make([]string, len(stack))\r\n\tvar diags hcl.Diagnostics\r\n\tfor i, cfg := range stack {\r\n\t\taddr, diag := cfg.Addr()\r\n\t\tdiags = diags.Extend(diag)\r\n\t\tif diag.HasErrors() {\r\n\t\t\tres[i] = \"<unknown>\"\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tres[i] = string(addr)\r\n\t}\r\n\treturn res, diags\r\n}\r\n```\r\n\r\nAnd then, before this `return` statement, you can do this:\r\n```golang\r\naddr, diags := keyprovider.NewAddr(cfg.Type, cfg.Name)\r\nstackAddrs, diag := keyProvidersStack(append(stack, cfg))\r\ndiags = diags.Extend(diag)\r\n```\r\nAnd this line will become\r\n```golang\r\nDetail: fmt.Sprintf(\"Cannot load %s due to circular reference between key providers. Stack trace %s\", addr, strings.Join(stackAddrs, \" -> \")),\r\n```\r\n\r\nEnding up in something similar to this. _Note: the message presented is not a real use case, that cannot happen._\r\n![Screenshot 2025-03-17 at 12 13 17](https://github.com/user-attachments/assets/acb90ebd-0a9c-4e76-945c-ddec0c43a479)\r\n",
        "We could have done this, but right now the key provider ([XOR](https://github.com/opentofu/opentofu/tree/main/internal/encryption/keyprovider/xor)) that could have been used to reproduce this is not included in the default allowed providers.\r\nI tested this with some slight temp modifications to the source code. ",
        "`suggestion`\r\n```suggestion\r\n\t\t\treturn desc, fmt.Errorf(\"unsupported OCI artifact type - empty\")\r\n```\r\n\r\nOr something more specific to let the user know about the actual cause? 🤔 ",
        "Good arguments for this. Thanks.\r\nAnd yes, the updated message seems more suitable for the particular situation.",
        "`suggestion`\r\n```suggestion\r\n\t\treturn nil, fmt.Errorf(\"manifest content digest (%s) does not match resolved digest %s\", gotDigest, desc.Digest)\r\n```\r\nNot sure about this 🤔 ",
        "Yes, that's more than a reasonable justification. Thanks. As I was saying, I was not sure about this suggestion.",
        "`suggestion`\r\n```suggestion\r\n\t\treturn selected, fmt.Errorf(\"ambiguous manifest has multiple descriptors for platform %s and version %s\", target, version)\r\n```"
      ],
      "opentofu-explicit-versus-dynamic-configurations": [
        "This is a first approach where I used the GH API to get all the protected branches (which are only the ones that are for the versions released of OpenTofu) and out of those exclude the the unmaintained versions.\r\nPros:\r\n* is using a reliable and well-known API that we can fully trust.\r\n\r\nCons:\r\n* manual maintenance. Quite ugly to update this.\r\n",
        "The update on the site is done automatically as seen [here](https://github.com/endoflife-date/release-data/tree/main?tab=readme-ov-file).\r\n![image](https://github.com/user-attachments/assets/01e8497d-1e49-4104-9ed8-21c39e9a4928)\r\n_That checkmark is for auto update_",
        "We are keeping only the hardcoded strategy to hold the control on what branches we run the flow against.\r\nSee [this](https://github.com/opentofu/opentofu/pull/2636#discussion_r2033011305) suggestion.",
        "This second approach is using https://endoflife.date/opentofu and is nice in terms that we don't really need to maintain this workflow anymore, once merged.\r\nPros:\r\n* no maintenance needed\r\n\r\nCons:\r\n* adds dependency on a 3rd party system",
        "We are keeping only the hardcoded strategy to hold the control on what branches we run the flow against.\r\nSee [this](https://github.com/opentofu/opentofu/pull/2636#discussion_r2033011305) suggestion.",
        "We added this section in the [CONTRIBUTING.RELEASE.md](https://github.com/opentofu/opentofu/pull/2636/files#diff-f3a80a44a166a40dd17304f62b1399e0a1477a0af3cea60dc41250f550b5ef07R248) for it.\r\nLet's see if this will be enough."
      ],
      "opentofu-minimize-api-surface": [
        "Thanks! Really good point! Really kind of you to provide some code that actually worked flawlessly.\r\nSo applied in 538dec6f1ad081f9b78bcbb614cf595869b21a6f.",
        "`question`\r\nThis was a linting error and I totally agree with it, to have the json fields explicitly named.\r\nWas there a reason till now not to have this with json tags?",
        "You identified that correctly in [one of your comments](https://github.com/opentofu/opentofu/pull/2521#discussion_r1961379081), it was excluded before.\r\nAnd now, since I am using that struct again in my new changes, I encountered the same issue. I could have used the nolint directive as well, but I would prefer to have the actual issue fixed.\r\nLet's see what others are saying.",
        "Thanks for the ideas and the exchange. Fixed both points in 26f76c25053af5028e5ffba68ca5d5e64a2d38bc in order to match the default behavior."
      ],
      "opentofu-document-reference-standards": [
        "Yeah, seen it. I had the same question when I added this. Let's wait for others.\r\nPersonally, I consider that issues should be linked, since there should be no PR without an associated issue, especially PRs that require a changelog.\r\n\r\nSaying that because in general, the issue is the place where decisions are taken in terms of design and general functionality. I know that from the PR we can navigate back to the issue, but IMO, the client facing information should be the issue (aka the functional information) and not the PR (aka the technical approach)."
      ],
      "opentofu-document-intent-and-limitations": [
        "`suggestion`\r\nCould we maybe include a url here? 🤔 \r\nMaybe? https://opencontainers.org/posts/blog/2024-03-13-image-and-distribution-1-1/#manifest-maximum-size\r\nI am not sure which is the official one."
      ],
      "opentofu-prevent-backing-array-surprises": [
        "Interesting that this conditional is rewritten in a different manner than [this](https://github.com/opentofu/opentofu/pull/2798/files#diff-b7f5b7c669ea7bb97e98d899d204d9e6bf48a00d18cb90109447eca05997704dR235) was in the current PR.\r\nAny particular reason of this difference? I am curious about the reason around having 2 different ways of writing these conditionals."
      ],
      "opentofu-clear-concise-documentation": [
        "`question`\r\nAnd if setting `include = [\"registry.opentofu.org/mycorp/*\"]` then it means that the template should include interpolation only for the `type`?\r\nIf so, I will try to add just a small mention of it.\r\n",
        "Great points! Let's go with the current shape."
      ],
      "opentofu-specify-configuration-behaviors": [
        "I am not aware of this. Could you expand it a little bit please?",
        "Oh, this is what you meant! Ok. I misread your initial comment.\r\nUpdated in 2695d6c58dc739ceae192242733ddea987cdf450",
        "3fe9247caedeef0099138111dece7a18065cda2b",
        "Added information in 11a29cb050fd95ab55b5f6d5df01bba3c73af854.\r\nA variable will not become ephemeral strictly from referencing an ephemeral value. In order for it to be able to work with ephemeral values, it needs to be configured specifically.",
        "f603db886aa3d830534b5cc68f7e415e58e3d343",
        "Applied in 6c43f94d46a40ebf22b652cb08d42bd5ba7f4179"
      ],
      "opentofu-document-phased-migration-paths": [
        "Thanks. Added 7b88c6f84ddb90024fff88eec0fa7f8f3fa7ad6f",
        "Good point! Thanks for the input. Fixed in 12adf6200ff60dfe12597285173e08990cf5b3cb"
      ],
      "opentofu-separate-configuration-lifecycles": [
        "Thanks a lot for the input! Good catches with these 2 prompt parts.\r\nWhen it comes to the prompts, I would go with the first approach. The second option, where we could skip the deprecated variables, could block some users from using the configuration correctly, so I am not sure about it.\r\n\r\nAs for the `tofu show --json planfile`, yeah, totally agree.\r\n",
        "For the json output, added the changes in here: 55cfcce17bba824b8c642da1745c86c77b788ae7 .\r\nThe results will look like this. This is what you were suggesting, right?\r\n```json\r\n    \"root_module\": {\r\n      \"module_calls\": {\r\n        \"modcall\": {\r\n          \"source\": \"./mod1\",\r\n          \"expressions\": {\r\n            \"this_is_my_variable\": {\r\n              \"constant_value\": \"given value from modcall\"\r\n            }\r\n          },\r\n          \"module\": {\r\n            \"variables\": {\r\n              \"this_is_my_variable\": {\r\n                \"default\": \"default value\",\r\n                \"description\": \"This is a variable for the old way of configuring things.\",\r\n                \"deprecated\": \"This variable will be removed on 2024-12-31. Use another_variable instead.\"\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n```\r\n----------\r\nFor prompts, added the changes in here: dd0cfdc949026216495b090a984c7753fed33e86\r\nAnd the results will look like this. Is it right? Not fully confident with these changes 🤔 \r\n![image](https://github.com/user-attachments/assets/11775498-f8ef-44f9-8d7a-057025af2be8)\r\n",
        "Thanks. Was not sure about it either.\r\nI applied that in a4c9d758df57f524f11d288127096a6353e8d5db.\r\nNow, it looks like this.\r\n<img width=\"1094\" alt=\"Screenshot 2025-03-07 at 18 17 38\" src=\"https://github.com/user-attachments/assets/2cf05f3f-03a0-4922-a9da-5b66bce2c0ac\" />\r\n"
      ],
      "opentofu-proper-span-lifecycle": [
        "Agree, but really error prone this part. Isn't there a way to extract the content of the `for` loop into a new function?\r\nIf somebody will ever come and add a new conditional `continue` or worse, a `break` or a `return`, that span will never be closed and will create a havoc in the traces.",
        "This call here should be removed. Check that `span.End()` is called a little bit down the road."
      ]
    },
    "profile": {
      "location": "Brasov, Romania",
      "blog": "",
      "site_admin": false,
      "followers": 14,
      "following": 9
    }
  },
  "kimwnasptd": {
    "repos": [
      "kubeflow/kubeflow"
    ],
    "entries": [
      {
        "slug": "kubeflow-api-structure-balance",
        "title": "API structure balance"
      },
      {
        "slug": "kubeflow-automate-style-enforcement",
        "title": "Automate style enforcement"
      },
      {
        "slug": "kubeflow-centralize-configuration-constants",
        "title": "Centralize configuration constants"
      },
      {
        "slug": "kubeflow-centralize-configuration-values",
        "title": "Centralize configuration values"
      },
      {
        "slug": "kubeflow-centralize-dependency-configurations",
        "title": "Centralize dependency configurations"
      },
      {
        "slug": "kubeflow-check-before-use",
        "title": "Check before use"
      },
      {
        "slug": "kubeflow-configurable-security-with-defaults",
        "title": "Configurable security with defaults"
      },
      {
        "slug": "kubeflow-consistent-descriptive-naming",
        "title": "Consistent descriptive naming"
      },
      {
        "slug": "kubeflow-control-header-modification",
        "title": "Control header modification"
      },
      {
        "slug": "kubeflow-descriptive-consistent-naming",
        "title": "Descriptive consistent naming"
      },
      {
        "slug": "kubeflow-document-code-thoroughly",
        "title": "Document code thoroughly"
      },
      {
        "slug": "kubeflow-document-networking-annotations",
        "title": "Document networking annotations"
      },
      {
        "slug": "kubeflow-enforce-https-protocol",
        "title": "Enforce HTTPS protocol"
      },
      {
        "slug": "kubeflow-enforce-least-privilege",
        "title": "Enforce least privilege"
      },
      {
        "slug": "kubeflow-environment-aware-configuration-design",
        "title": "Environment-aware configuration design"
      },
      {
        "slug": "kubeflow-externalize-configuration-parameters",
        "title": "Externalize configuration parameters"
      },
      {
        "slug": "kubeflow-follow-api-conventions",
        "title": "Follow API conventions"
      },
      {
        "slug": "kubeflow-go-export-naming-conventions",
        "title": "Go export naming conventions"
      },
      {
        "slug": "kubeflow-manage-configuration-changes",
        "title": "Manage configuration changes"
      },
      {
        "slug": "kubeflow-mark-ui-text-i18n",
        "title": "Mark UI text i18n"
      },
      {
        "slug": "kubeflow-match-algorithms-to-purpose",
        "title": "Match algorithms to purpose"
      },
      {
        "slug": "kubeflow-normalize-url-paths",
        "title": "Normalize URL paths"
      },
      {
        "slug": "kubeflow-optimize-container-build-configurations",
        "title": "Optimize container build configurations"
      },
      {
        "slug": "kubeflow-pin-version-dependencies",
        "title": "Pin version dependencies"
      },
      {
        "slug": "kubeflow-precise-workflow-triggers",
        "title": "Precise workflow triggers"
      },
      {
        "slug": "kubeflow-prefer-external-configuration",
        "title": "Prefer external configuration"
      },
      {
        "slug": "kubeflow-prioritize-readability-over-brevity",
        "title": "Prioritize readability over brevity"
      },
      {
        "slug": "kubeflow-private-variable-naming-convention",
        "title": "Private variable naming convention"
      },
      {
        "slug": "kubeflow-simplify-code-structure",
        "title": "Simplify code structure"
      },
      {
        "slug": "kubeflow-specific-network-access-documentation",
        "title": "Specific network access documentation"
      },
      {
        "slug": "kubeflow-standardize-build-configurations",
        "title": "Standardize build configurations"
      },
      {
        "slug": "kubeflow-standardize-makefile-patterns",
        "title": "Standardize makefile patterns"
      },
      {
        "slug": "kubeflow-standardize-network-tools",
        "title": "Standardize network tools"
      },
      {
        "slug": "kubeflow-standardize-style-scripts",
        "title": "Standardize style scripts"
      },
      {
        "slug": "kubeflow-structured-owners-files",
        "title": "Structured OWNERS files"
      },
      {
        "slug": "kubeflow-type-appropriate-default-values",
        "title": "Type-appropriate default values"
      },
      {
        "slug": "kubeflow-unique-workflow-step-names",
        "title": "Unique workflow step names"
      },
      {
        "slug": "kubeflow-use-appropriate-log-levels",
        "title": "Use appropriate log levels"
      },
      {
        "slug": "kubeflow-use-css-classes-properly",
        "title": "Use CSS classes properly"
      },
      {
        "slug": "kubeflow-use-enums-for-state",
        "title": "Use enums for state"
      },
      {
        "slug": "kubeflow-use-modern-javascript-idioms",
        "title": "Use modern JavaScript idioms"
      },
      {
        "slug": "kubeflow-use-snake-case-in-python",
        "title": "Use snake_case in Python"
      },
      {
        "slug": "kubeflow-use-table-driven-tests",
        "title": "Use table-driven tests"
      },
      {
        "slug": "kubeflow-validate-inputs-explicitly",
        "title": "Validate inputs explicitly"
      }
    ],
    "comments": {
      "kubeflow-follow-api-conventions": [
        "@DavidSpek show your comment below about the error with Unmarshal https://github.com/kubeflow/kubeflow/pull/5660#issuecomment-797378906. Writing it here to keep it in one place.\r\n\r\nI'm not sure how you used the Unmarshal function but I believe you could also try and catch the error and not let it panic. Here's a very similar code I've seen https://github.com/kubeflow/katib/blob/master/pkg/new-ui/v1beta1/backend.go#L73"
      ],
      "kubeflow-descriptive-consistent-naming": [
        "nit: Let's use `centraldashboard-angular` here as well, to make sure we use the same value everywhere.\r\n\r\nWe can revert to `centraldashboard` once we are confident with switching the web apps"
      ],
      "kubeflow-centralize-configuration-values": [
        "Could you add the urls for the icons into the environment files instead? https://github.com/kubeflow/kubeflow/tree/master/components/crud-web-apps/jupyter/frontend/src/environments\r\n\r\nLets keep these links in a central place, where we might also add other links in the future like the svgs for the main page",
        "Thanks for the ping! \r\n\r\nGood catch on the CORS issue. Lets dump the SVGs in the `static/assets` folder. And also define the links that the frontend will use in the `environment` files as mentioned above."
      ],
      "kubeflow-optimize-container-build-configurations": [
        "I understand that by omitting the GOARCH env var we let golang decide on the architecture based on the node environment (machine). \r\n\r\nI'm just trying to wrap my mind around the next step. Is it going to be the process described in https://docs.docker.com/build/building/multi-platform/ to build images that have manifests for different platforms?",
        "@lehrig this was a *very* thorough explanation of the suggested approach! I'd also suggest to cross post it in the umbrella issue, so that it's readily available for anyone wondering how to e2e approach is going to be.\r\n\r\nAlso LGTM"
      ],
      "kubeflow-validate-inputs-explicitly": [
        "let's instead make a check if `NaN` is included in the value of either `cpu` or `cpu_limit`.\r\n\r\nAnd if that's the case then the backend should raise a `BadRequest` werkzeug exception (400)",
        "Lets also add some logic here that would handle incorrect values.\r\n\r\nThe expected values here, from the request, would be:\r\n* `None`, which means `\"jupyter\"`\r\n* `\"jupyter\"`\r\n* `\"rstudio\"`\r\n* `\"vscode\"`\r\n\r\nIf this field is present in the request but has a different value from the expected ones then the backend should raise a `400` error",
        "Indeed, the frontend sends only specific values but it's a good practice for the backend to make it explicit to the user/frontend that the data it received was not formatted as expected.",
        "And also you could look at this snippet for how to raise this error\r\n\r\nhttps://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/jupyter/backend/apps/common/form.py#L31-L34\r\n\r\nEDIT: was typing it before I saw your comment above ",
        "You are not wasting anyone's time. Your efforts are as important as mine and no releases were delayed because of a question :) "
      ],
      "kubeflow-externalize-configuration-parameters": [
        "Also, ultimately we could use these CRs then to list who the contributors in the namespace are from the CentralDashboard?"
      ],
      "kubeflow-manage-configuration-changes": [
        "I'm not sure I understand what the question or suggested change is here. Could you rephrase this?",
        "I see what you mean. The `start` script should be completely removed, since the users should run the UI locally with `build:watch` for now.\r\n\r\nI'll push a commit to remove this script",
        "Added a commit that removes the unused npm script",
        "Let's exclude this change from this PR and keep the touched lines to the minimum. This will also remove the 15.000 modified lines to the `package-lock.json` because of this change.\r\n\r\nAlthough I'd like to progressively replace the use of `lodash` with `lodash-es`, but until I get to it we can remove this from the `package.json`, but lets just do it in another PR."
      ],
      "kubeflow-consistent-descriptive-naming": [
        "nit: can we make this `pvcviewer-` instead? In the labels as well we do `pvcviewers` so let's be uniform",
        "Let's rename this to `CentralDashboard-angular Frontend Tests` and also the file to `centraldb_angular_frontend_test.yaml`"
      ],
      "kubeflow-use-css-classes-properly": [
        "Don't modify html elements directly with styles, always use a class instead that wraps what the style changes are. Also try to avoid as much as possible `!important` in CSS. Instead try to use more specific css selectors.\r\n\r\nHere you could introduce and use the following CSS class for each `<mav-icon>`\r\n```css\r\n.server-type {\r\n  height: 32px;\r\n  width: 150px;\r\n}\r\n```",
        "Add a wrapper class here to give this group a bottom margin\r\n\r\n```css\r\n.server-type-wrapper {\r\n  margin-bottom: 1rem;\r\n}"
      ],
      "kubeflow-normalize-url-paths": [
        "Let's rephrase this to something like the following:\r\n\r\nWhen Istio exports Services it always expects a `/` at the end. SO we'll need to make sure the links propagated to the iframe end with a `/`",
        "Let's add a comment here to explain why we need this function, which is to handle the sometimes missing `/` from some urls"
      ],
      "kubeflow-enforce-least-privilege": [
        "Should be good to go. It would also be a bug from the current code if it couldn't operate without listing namespaces, since it shouldn't need these permissions in the first place.\r\n\r\nNote that we might need to re-introduce them though, in a future iteration when we would want the apps to be self-standing. But we need to discuss other items for this effort. But I'm also mentioning this context to keep in mind"
      ],
      "kubeflow-match-algorithms-to-purpose": [
        "Why does this need to be a `while` loop and not an `if`? \r\n\r\nAlso, shouldn't we have the condition also check if the status is \"empty\"? Since we want to have this message only when the controller hasn't set the status? \r\n\r\nElse won't it always expose the `Waiting for StatefulSet to create the underlying Pod.` message for the first 10 seconds independently of whether the status is set?",
        "We can simplify this and remove the `for` loop for the `item` var.\r\n\r\nThe `coditions` are a list of dict objects. We only need to iterate over each condition dict, from the list, and check if that condition object contains the `reason` key"
      ],
      "kubeflow-centralize-dependency-configurations": [
        "Should we drop this package and install the corresponding kserve package instead?",
        "Could you instead make this change into the common code, so that it takes effect for all the web apps?\r\n\r\nhttps://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/common/backend/setup.py#L6"
      ],
      "kubeflow-centralize-configuration-constants": [
        "That's a good idea, it will help us have all the names organized in one place.\r\nAdding it",
        "@PatrickXYS I added another commit that introduces this `config.py` file.\r\nPTAL and if you are OK I can resolve this conversation",
        "OK, I'll just omit this field entirely. There no issue by completely omitting the field and the configmap's value will be used right?"
      ],
      "kubeflow-standardize-network-tools": [
        "@nrchakradhar thank you for your comments. I'm planning on tackling the IPv6 support for all the web apps after 1.3.\r\n\r\nThe first step is to bind the IPv6 equivalent addresses as well, as you've pointed out in your links. But I want to do a good deep dive on this and test some edge cases for which I don't have the cycles right now. "
      ],
      "kubeflow-precise-workflow-triggers": [
        "Let's re-use the makefile rules here, to avoid duplication of code in makefiles and GH Actions. The layers are already cached so rebuilding the image should be very fast\r\n```bash\r\nexport TAG=$(cat releasing/version/VERSION)\r\ncd components/centraldashboard-angular\r\nmake docker-build docker-push\r\n```",
        "@apo-ger let's leave the notebook images completely out for this effort.\r\n\r\nI believe we would be able to use the Makefiles if we'd use a `REGISTRY` env var in each Makefile. But let's discuss this in a follow up PR, since this one is already getting big",
        "just the changes from this PR, and we can send another PR for the latest tag in a separate PR that:\r\n1. Uses a `REGISTRY` env var in all makefiles (let's see if we need to do more)\r\n2. Use the Makefiles to build each notebook, but with a different TAG each time like we do for the rest of the components",
        "Since we are testing manifests this time we don't want to trigger the workflows when code changes, but rather when the **manifests** change.\r\n\r\nSo in this case it will be `components/centraldashboard/manifests/**`",
        "I think you missed this one. It should be `components/profile-controller/config/**`",
        "Lets add another path here for `components/crud-web-apps/common/**`. We want the web app to be rebuild whenever we also touch the common code.",
        "Lets add another path here for `components/crud-web-apps/common/**`. We want the web app to be rebuild whenever we also touch the common code.",
        "Lets add another path here for `components/crud-web-apps/common/**`. We want the web app to be rebuild whenever we also touch the common code.",
        "Let's also trigger this workflow for the `/components/common/**` directory, since it's using common code from there"
      ],
      "kubeflow-use-enums-for-state": [
        "Thank you for the feedback @tasos-ale! \r\n\r\nThe reason I used `true` as a default value here is because if it starts as `null` then the app will think that the dashboard is not present and try to make a request to fetch the namespaces. \r\n\r\nSo in this case I take for granted the opposite, which is expect the dashboard to be present and if after the check it's missing then the apps will make the request to fetch the namespaces",
        "but indeed the enumeration is a better way to express this. I'll use this enum instead of a boolean value and the apps will fetch the namespaces only if the state is `Disconnected`",
        "pushed a commit for the described changes"
      ],
      "kubeflow-use-snake-case-in-python": [
        "Let's rename this var to `creation_timestamp`. Python is using snake_case for var names \r\nhttps://peps.python.org/pep-0008/#descriptive-naming-styles",
        "let's not use `camelCase` for Python variables. This should be `cpu_limit`",
        "same with `camelCase` as above. This should be `memory_limit`"
      ],
      "kubeflow-mark-ui-text-i18n": [
        "Pushed a commit",
        "pushed a commit",
        "Pushed a commit"
      ],
      "kubeflow-standardize-style-scripts": [
        "Why do we introduce rules for `tslint`, which is getting deprecated?\r\n\r\nLet's instead have the following 2 rules:\r\n```json\r\n    \"lint-check\": \"ng lint\",\r\n    \"lint\": \"ng lint --fix\",\r\n```\r\nsimilarly to KWA https://github.com/kubeflow/katib/blob/master/pkg/new-ui/v1beta1/frontend/package.json#L12-L13\r\n\r\nWe might also need to make some small changes to the `angular.json`. Take a look at the previous effort for this in https://github.com/kubeflow/kubeflow/pull/6464",
        "I see that we don't have this script in TWA, like we do in Katib\r\nhttps://github.com/kubeflow/katib/tree/master/pkg/new-ui/v1beta1/frontend/scripts\r\n\r\nYet the action didn't fail. Why is that?",
        "OK I see what happened. `||` means that the second command (node script in this case) will NOT be run if the first part was successful. But the node script will run if the prettier check fails.\r\n\r\nThat script is just there to instruct users to run the formatting. Let's include it in this PR"
      ],
      "kubeflow-control-header-modification": [
        "I think we should not let users arbitrarily add headers to the requests that pass from the ingress gateway. For example they could even override the ISTIO `userid-header`, the `X-Forwarded-Proto` so that the server assumes http instead of https etc.\r\n\r\nI think we should reconsider the design we take here\r\nalso cc @yanniszark who has a lot of experience on security",
        "I took a more extensive look at the docs and I propose the following solution:\r\n\r\nThe headers that are used to configure parts of R-Studio are all non-standard headers, starting with `X-*`. Such example headers are\r\n* `X-RStudio-Request`\r\n* `X-RStudio-Root-Path`\r\n* `X-Forwarded-Host`\r\n* `X-Forwarded-Proto`\r\n* `X-Auth-Token`\r\n\r\nSo I propose to go the other way around from blacklisting specific headers. Let's only allow the users to edit/set non-standard headers that start with `X-*`. This way we provide all the necessary configuration for R-Studio and at the same time don't allow the users to edit any header arbitrarily.\r\n\r\nAnd if in the future we find users that need to specifically set well defined headers for a Notebook then we can discuss based on the specific use case on how to proceed and allow users to add more headers.\r\n\r\nThis is the case for the docs I looked up to now\r\nhttps://docs.rstudio.com/ide/server-pro/access-and-security.htmlhttps://docs.rstudio.com/connect/admin/authentication/proxied/\r\nhttps://docs.rstudio.com/connect/admin/appendix/configuration/#ProxyAuth.Settings\r\n",
        "And lastly, the controller will set accordingly the `X-RStudio-Root-Path` header if the `server-type` is `rstudio`. But the user could still be able to override this by manually specifying a different value.\r\n\r\nThis way in order for a CR to support RStuido it will only need the `notebooks.kubeflow.org/server-type` annotation, which provides a good default mechanism and extensibility to the users.",
        "> Also, using X-* for headers seems to have been deprecated int 2012: see https://tools.ietf.org/html/rfc6648\r\n\r\nThis is a very good argument to make to the folks maintaining the R-Studio code :) \r\n\r\n> However, by backing this into the controller administrators cannot easily customize these values.\r\n\r\nWhat extra headers do you think they would need to set aside, from the `X-*`  headers R-Studio knows/expects?\r\n\r\n> Regarding your last comment, are you suggesting that the current mechanisms added to the controller stay as they are, but the controller itself adds the annotations to the CR when notebooks.kubeflow.org/server-type: rstudio is present?\r\n\r\nNo, the controller will not add any extra annotations if the `server-type` is present. It will only configure the vsvc accordingly.\r\nLet me try to clarify with two examples:\r\n\r\nCR 1:\r\n```yaml\r\nmetadata:\r\n  annotations:\r\n    notebooks.kubeflow.org/server-type: rstudio\r\n```\r\n\r\nIn this case the controller will see that the Notebook is for R-Studio so it will set the `/` url rewrite in the vsvc and also set the `X-RStudio-Root-Path` header in the vsvc for the prefix\r\n\r\nCR 2:\r\n```yaml\r\nmetadata:\r\n  annotations:\r\n    notebooks.kubeflow.org/server-type: rstudio\r\n    notebooks.kubeflow.org/http-rewrite-uri: /some/path\r\n```\r\n\r\nIn this case the controller will first do exactly what it would for CR 1. But then it would also detect the `notebooks.kubeflow.org/http-rewrite-uri` and use this value for the Istio rewrite, instead of the default it previously put. \r\n\r\nAlso the same approach will be used with the `notebooks.kubeflow.org/http-headers-request-set`. If this annotation is present then the controller will use the `X-*` headers mentioned in this annotations instead",
        "> Regarding the server-type thing, we can do that in the future but its not necessary right now.\r\n\r\n@thesuperzapper I can understand your urge to have this effort merged as soon as possible, but this is not a big feature so lets get it right from the beginning and not worry about it in the future. The functionality I described is almost there.\r\n\r\n> Adding a whitelist would only act to reduce the usefulness of this feature for users why might have their own container images.\r\n\r\nCould you elaborate on this? What use cases do you think we limit? With the mention mechanism, of allowing the user to set any `X-*` header they want, we still support all of the configuration options from R-Studio."
      ],
      "kubeflow-document-networking-annotations": [
        "Can you add a comment here that for images in this group the backend will be:\r\n1. Adding the `notebooks.kubeflow.org/http-rewrite-uri` annotation to the CR, to rewrite the path that ends up in the running container to be `/`\r\n\r\nThis will help exposing to the users the constraints and logic that will be applied to images that belong to that group",
        "LGTM",
        "Can you add a comment here that for images in this group the backend will be:\r\n1. Adding the `notebooks.kubeflow.org/http-headers-request-set` annotation to the CR, for setting the `X-RStudio-Root-Path: ${ISTIO_PREFIX}`  header to each request to the running container\r\n2. Adding the `notebooks.kubeflow.org/http-rewrite-uri` annotation to the CR, to rewrite the path that ends up in the running container to be `/`\r\n\r\nThis will help exposing to the users the constraints and logic that will be applied to images that belong to that group",
        "LGTM"
      ],
      "kubeflow-standardize-build-configurations": [
        "I don't like the phrasing of this sentence, as it implies that for development purposes we need to run the command directly and can't use the Makefifle.\r\n\r\nWe can use the Makefile to build Jupyter app's image by running\r\n```bash\r\nREGISTRY_PROJECT=my-repo make docker-build\r\n```\r\n\r\n[ Tensorboard's makefile uses slightly different vars but we should iron them out and have the same template when we setup the CI/CD ]"
      ],
      "kubeflow-prioritize-readability-over-brevity": [
        "I was thinking that If in the future we want to handle other cases in the PATCH request, then each one of them should be its own function and not have all the logic unfolded in the PATCH handler",
        "I was thinking that If in the future we want to handle other cases in the PATCH request, then each one of them should be its own function and not have all the logic unfolded in the PATCH handler",
        "We should not ignore the E501 warnings.\r\nIf the line is too long you could put each argument in a distinct line.",
        "nit: @lalithvaka could you add this logic into a distinct function on its own?"
      ],
      "kubeflow-prefer-external-configuration": [
        "@tasos-ale let's have this as an empty list initially, since we don't have an app that supports it at this point in time",
        "Ideally I'd like this to info (which web app supports all namespaces) to be transmitted via the common library, so that the dashboard can dynamically know if an app supports this once it loads it.\r\n\r\nBut we can do with hardcoding the urls for now. If in the future we see a bigger need for this we can look into it",
        "Do we need all of these new imports or only a subset of them?",
        "Makes sense, lets keep them then"
      ],
      "kubeflow-api-structure-balance": [
        "I'd propose to avoid introducing a handler for the InferenceServices in the common backend code.\r\n\r\nThis would mean we'd need to add more handlers for list/delete/get/logs as well, which as a result will make the MWA depend even more in this common code. I'd propose to instead keep on using the `custom_resource.py` file's handlers for the ISVC logic in the MWA",
        "Could you add a `get_age(k8s_object)` in our common [helpers.py](https://github.com/kubeflow/kubeflow/blob/e99b5e18697a15088abea12543bd4e3f180ff984/components/crud-web-apps/common/backend/kubeflow/kubeflow/crud_backend/helpers.py) file that would return a dictionary with the `uptime` and `timestamp` values? Since other web apps will want to use this convention, for returning age information to their frontends, it would be a good idea to add this logic into a common place.\r\n\r\nFor now it could only work with dict objects, like CRs, and later one we could extend it to work with class objects as well, such as PVCs for example."
      ],
      "kubeflow-pin-version-dependencies": [
        "let's remove the dependency on the `test` rule. This would require users to have a configured Go environment to build the controller. \r\n\r\nIf in the future we'd like to run these tests then we should do this inside of the Dockerfile, just like we do for the CentralDashboard",
        "When I tried applying the new CRD on a cluster with the previous CRD I got the following errors back:\r\n```\r\nThe CustomResourceDefinition \"notebooks.kubeflow.org\" is invalid:\r\n* metadata.annotations: Too long: must have at most 262144 bytes\r\n* spec.preserveUnknownFields: Invalid value: true: must be false in order to use defaults in the schema\r\n```\r\n\r\nRegarding the `preserveUnknownFIelds`, could you explicitly set it to `false`?\r\n\r\nThis is also the recommended way in the docs to have compatibility: \r\nhttps://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#field-pruning\r\nhttps://github.com/kubernetes-sigs/controller-tools/issues/476#issuecomment-691519936\r\n\r\nAlthough with a quick look at the kubebuilder docs I'm not 100% sure whether we should do this with a [marker](https://book.kubebuilder.io/reference/markers/crd-processing.html) or via the [makefile](https://book.kubebuilder.io/reference/generating-crd.html#multiple-versions)",
        "After looking at this again I propose that we manually \r\n1. add the `spec.preserveUnknownFields: false` just for this release\r\n2. remove this field in KF 1.7, since it's only needed for the `apiextensions.k8s.io/v1beta1` to `apiextensions.k8s.io/v1` transition.\r\n\r\nThe `controller-gen` does not allow us to explicitly set this to false https://book.kubebuilder.io/reference/markers/crd-processing.html\r\n\r\n",
        "This is awesome @samuelvl!\r\n\r\nOne final nit, I see that you've commented out the `crd` part in the manifests, which results in the CRD to not be included when generating `base` or `overlays/kubeflow` \r\n\r\nhttps://github.com/kubeflow/kubeflow/pull/6374/files#diff-28481732533c7c75d2d2ba504e9670d90e72ed98f999115fd92a0f8e8a5aace2R20\r\n\r\nCould you revert it back, so that the CRD is included as well? We should be ready to merge afterwards",
        "Hmm, indeed in this PR we made the CRD quite bigger since we now actually include the full PodSpec for validation.\r\n\r\nBut the `maxDescLen=0` seems like a good way to cut down significantly the size of it. The spec is currently only the PodSpec, for which descriptions are widely available so we are OK.\r\n\r\nGood job @samuelvl!",
        "I tried to `kustomize build config/crd` with a 3.2 version, since this was the one [used from manifests](https://github.com/kubeflow/manifests#prerequisites) but I got the following errors:\r\n```\r\nError: no matches for OriginalId apiextensions.k8s.io_v1beta1_CustomResourceDefinition|~X|notebooks.kubeflow.org; no matches for CurrentId apiextensions.k8s.io_v1beta1_CustomResourceDefinition|~X|notebooks.kubeflow.org; failed to find unique target for patch apiextensions.k8s.io_v1beta1_CustomResourceDefinition|notebooks.kubeflow.org\r\n``` \r\n\r\nDo you know why these occur? They were not shows in the previous iteration of the controller, so maybe something slightly changed in the autogenerated manifests?\r\n\r\nIf I use the 3.8 version from the makefile this error goes away. Also it's not shown even with 3.2 when I build the `base`, so it's not that critical. But let's try to understand why this happens. I'll try to look into it within the next days as well",
        "I agree to remove the patch altogether. The patch was making the validation more lax, by removing validation from the CPU/Memory fields.\r\n\r\n@samuelvl can you double check that creating a Notebook with the Jupyter web app also works as expected after this change? To make sure the backend is submitting objects that are valid.",
        "Perfect!"
      ],
      "kubeflow-standardize-makefile-patterns": [
        "Can you also remove this rule? We don't use `build-gcb` anywhere in the project.\r\n\r\nIt's a good chance to clean up the project",
        "Can you add a `TAG ?= $(shell git describe --tags --always)` here? This is the one we use in the web apps as well. Let's standardize on this one",
        "nit: Could you also create an `image` rule, which builds and pushes the image?\r\n\r\nWe have this for the web apps as well and would be nice to keep this convention across all components\r\nhttps://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/jupyter/Makefile#L11",
        "@elikatsis ACK, this is a very good suggestion!\r\n\r\nI'll add a commit for Volumes web app in this PR, since it's fixing the formatting for this component. I'll then open up a new issue to keep track of the uniformity in the dockerfiles for the other web apps as well."
      ],
      "kubeflow-private-variable-naming-convention": [
        "let's make this `private`, to comply with the var's name"
      ],
      "kubeflow-use-modern-javascript-idioms": [
        "Could you restructure the if statements in order to reduce the nesting levels?\r\n\r\nSomething like:\r\n```javascript\r\nif (!queryParams || !queryParams[\"ns\"]) {\r\n  return this.buildHref(href, this.queryParams);\r\n}\r\n\r\nreturn this.buildHref(href.replace('{ns}', queryParams[\"ns\"]), queryParams);\r\n```",
        "LGTM!"
      ],
      "kubeflow-enforce-https-protocol": [
        "nit: `Invalid HOST url provided, must be like https*://*` (the protocol should be https in the end)"
      ],
      "kubeflow-use-appropriate-log-levels": [
        "I agree with @yanniszark.\r\nAlthough AFAIK `logr` does not have a [Warning level](https://github.com/go-logr/logr#why-not-more-named-levels-like-warning). So we should do this an Error instead.\r\n\r\n@gilbeckers could you also make the error message more explicit as to why it couldn't find the Notebook Container? Something like: `Could not find the Notebook container. No container has the same name with the CR '{notebook-name}'. Will not update the status of the CR.`"
      ],
      "kubeflow-automate-style-enforcement": [
        "We should look into moving away from `tslint` in the backend as well, now that we've decoupled the frontend and backend code"
      ],
      "kubeflow-structured-owners-files": [
        "@TobiasGoerke why are you not an approver? :) \r\n\r\nWe want the OWNERS file to depict the folks that are driving the component. Considering that you, @apo-ger and I have worked on the proposal of this I'd create the OWNERS file like this:\r\n\r\n```yaml\r\napprovers:\r\n  - apo-ger\r\n  - kimwnasptd\r\n  - TobiasGoerke\r\n```",
        "My bad, I just copied the reviewers from the jupyter web app.\r\nLets start with an empty list of reviewers for now",
        "I think I'm missing something here, where should approvers be listed?",
        "I see. Which people should I assign as reviewers?"
      ],
      "kubeflow-configurable-security-with-defaults": [
        "Lets make it configurable then, it should be a small change.\r\n\r\nI'll make it look for a `CSRF_SAMESITE` env var with a default value to `Strict`. If the value provided by the user is not `Strict`, `Lax` or `None` then again it will default to `Strict`\r\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie#SameSite",
        "force pushed. The PR should only contain one commit responsible for the CSRF functionality.\r\nI also made the `SameSite` attribute of the cookie configurable via the `CSRF_SAMESITE` variable.\r\n\r\nI also think a good next step would be to document the ENV Vars that each web app uses in the respective READMEs. This will make clear to the users how they can configure each app.",
        "ACK! I modified the README of the common code.\r\nI also created #5483 as an umbrella issue for the web apps"
      ],
      "kubeflow-go-export-naming-conventions": [
        "small nit, shouldn't this function start from lowercase since it's not meant to be used from outside this go module?"
      ],
      "kubeflow-unique-workflow-step-names": [
        "Could you provide some more details as to why this change is needed?",
        "Makes sense",
        "@PatrickXYS to make sure I'm synced with the proposed changes; the argument is to create a common base class for building a workflow that always has a step for kaniko-build, with `--no-push`, since we essentially use it in all of the component CI files?",
        "Regarding the changes for using a different class I agree with @DavidSpek that this isn't a blocker, so let's go with this implementation for now and if in the future we find ourselves repeating too much code then lets go over this argument.\r\nDoes this sound OK to you @PatrickXYS ?\r\n",
        "> Should the registry be called access-management or should it be kfam?\r\n\r\n@yanniszark what are your thoughts on this? I guess we should stick to `access-management` for now?\r\n\r\nI'll also give a heads up to our tracking issue to add this registry as well ",
        "> @kimwnasptd I actually just ran into an issue where using the common kaniko_builder.py actually complicates things slightly. For this situation specifically it is easily fixed\r\n\r\nCould you provide some more details for this? Just so that we can identify the pattern/pain-point once we see it reoccurring "
      ],
      "kubeflow-type-appropriate-default-values": [
        "In this case the default value for the conditions should be a `[]`, instead of `\"\"` since this var is a list"
      ],
      "kubeflow-simplify-code-structure": [
        "Let's remove this function. We only need this initialization in one place and can simplify with something like:\r\n\r\n```golang\r\nstatus := v1beta1.NotebookStatus{\r\n    Conditions:     make([]v1beta1.NotebookCondition, 0),\r\n    ReadyReplicas:  sts.Status.ReadyReplicas,\r\n    ContainerState: corev1.ContainerState{},\r\n}\r\n```",
        "Could you break this if statement to the following ones:\r\n\r\n```golang\r\nif pod.Status.ContainerStatuses[i].Name != instance.Name {\r\n    continue\r\n}\r\n\r\nif pod.Status.ContainerStatuses[i].State == instance.Status.ContainerState {\r\n    continue\r\n}\r\n\r\nlog.Info(\"Updating Notebook CR state: \", \"namespace\", instance.Namespace, \"name\", instance.Name)\r\ncs := pod.Status.ContainerStatuses[i].State\r\n...\r\n```",
        "nit: You could have an `if nodename == \"\" {` here and return nil if that holds.\r\n\r\nThen you could move the code below one tab to the left, outside of an if clause, to make it a little bit more simple to read"
      ],
      "kubeflow-environment-aware-configuration-design": [
        "It makes sure the image build, and stored in docker, will use the TAG defined in the `env` and not the default one which is\r\nhttps://github.com/kubeflow/kubeflow/blob/master/components/centraldashboard/Makefile#L2",
        "> @kimwnasptd I don't like that this applies the manifests and then patches them after, this will result in a generation of the pod that fails to run, quickly followed by one with the correct image.\r\n\r\nWhy is this a hard problem? The deployment will be patched immediately, and we wait for the Deployment object either way\r\n\r\n> I don't understand what was not working with the old sed approach, which modified the manifests before applying them?\r\n\r\nPreviously the flow was:\r\n1. build the manifests\r\n    1. These manifests in `master` would have `latest` tag in images\r\n    2. In a release branch we update the tag, so it would be something like `v1.8.0-rc.0`\r\n3. `sed` was used change the image with tag `latest`, to `${{env.TAG}}`\r\n4. The above worked for `master`, which has tag `latest` but doesn't for release branches\r\n\r\nSo the script works for `master`, because `latest` image tag is used in manifests, but fails in release branches which use a different tag in manifests.",
        "Lets remove the `logo` sections in the ConfigMap. As mentioned above they add a lot of boilerplate code that most of the users won't need to configure.",
        "@DavidSpek taking your comment https://github.com/kubeflow/kubeflow/pull/5646#issuecomment-801308346 here to have everything coordinated in this place\r\n\r\n> @kimwnasptd Could we do some sort of combination of the things you describe and what is being done now? More specifically, allow the user to set the URLs for the icons in the config map. \r\n\r\nThis is the initial idea I had https://github.com/kubeflow/kubeflow/pull/5646#issuecomment-797521345, although thinking about it more I believe it will be more involved to get it working. In this case we will need to first fetch the yaml [ and the request could fail ] and then inject the urls and use them.\r\n\r\n> Due to the use of trademarks and the applicable guidelines I think it is import to make it very easy for people to remove the logos (preferably without needing to build their own image).\r\n\r\nCould you elaborate a little bit more on this? How does the trademarks and applicable guidelines fit into the picture?\r\n\r\nI agree that we should make it easy for people to swap the logos they prefer, but these users are the minority and most probably won't have a hard time to rebuild the image.",
        "From what I understand the issue is not how the icon will appear to the UI [ via the backend directly, fetched from somewhere public etc ] but the fact that we use this logo in the software. With that I mean that if someone would use the app he would end up seeing the logo, which could imply that this is endorsed from RStudio etc.\r\n\r\nAlso an issue with fetching resources in the UI directly from the public internet is that the app unable will not work in air-gaped environments, where the users would only have access to specific IPs.\r\n\r\n",
        "So judging from our current situation, where we are tied by time constraints and the trademark situation would need some communication and tip toeing into what it's acceptable I would propose the following solution to unblock ourselves:\r\n\r\n1. Don't use the logos in this PR. We could use the [group options](https://v8.material.angular.io/components/select/overview#creating-groups-of-options) from Angular material, or just use buttons with text [ would prefer the first approach tbh ] to show the images for the different servers\r\n2. Open an issue about it and involve @castrojo in this. And once we figure out what we are allowed to do with the rstudio trademark we can open a new PR and add this functionality. We've done it once here so this won't be difficult to do in the future\r\n\r\nWDYT?",
        "Let's go with using the svgs. We'll use links to fetch them from where they publicly host them. Distributions could make a small change and add their svgs to the app and simply change these links, which is something that we were OK with from the beginning. Lets just add a small section for this in the app's README.\r\n\r\nRegarding the responsibility part, I'm not a layer on this so not an expert but we are a team and all of us move forward together. We are not going to point fingers to people if things go south. And also since I'm a reviewer and agreed to move forward with this so I'm also responsible.\r\n\r\nBut if indeed people complain about the usage then we will sincerely apologize and just change this to something like we discussed above. "
      ],
      "kubeflow-check-before-use": [
        "Let's move this part completely outside of this function and treat the Pod as the StatefulSet. \r\n\r\nThe main reconciliation logic is responsible for finding the objects (Pod, StatefulSet) and our function will need to check if they are `nil` or not",
        "This nesting level can be simplified by checking the incoming `pod` object:\r\n\r\n```golang\r\nif pod == nil {\r\n    log.Info(\"No pod found. Won't update notebook conditions and containerState\")\r\n    return status, nil\r\n}\r\n\r\n```"
      ],
      "kubeflow-use-table-driven-tests": [
        "Let's change the name of this function to `createNotebookStatus` and update it's signature to:\r\n```golang\r\nfunc createNotebookStatus(r *NotebookReconciler, nb *v1beta1.Notebook,\r\n\tsts *appsv1.StatefulSet, pod *corev1.Pod, req ctrl.Request) (v1beta1.NotebookStatus, error) {\r\n```\r\n\r\nThis means that:\r\n1. We will be passing all necessary objects, statefulset, pod, as arguments\r\n2. The function will be just calculating the status\r\n\r\nThis will also allow us to write some unit tests to ensure we calculate the status as expected.",
        "Nice! Let's also include another unit test for the case where the Notebook's Pod is unschedulable"
      ],
      "kubeflow-specific-network-access-documentation": [
        "Could you instead have a list for this numbered step and expose each `kubectl port-forward` command that a user needs to run?\r\n\r\nWe can just have commands for the Services that we proxy in the webpack config",
        "Let's add this explanation at the numbered step, since it explains how the proxying works. And, as mentioned above, have the bullet list for the different proxying commands",
        ">This continuous failure to mutate the target pod may block other target pods from mutation\r\nuntil the failing process ends.\r\n\r\nCould you clarify a little bit more what you mean here? By other Pods you are not referring to other unrelated Pods in the cluster right?\r\n\r\nIIUC, as you described, the Deployment/StatefulSet will be retrying to create the Pod which will keep failing. Are there other side effects to other pods?"
      ],
      "kubeflow-document-code-thoroughly": [
        "Could you add a comment here to document that the status of the CR will be updated based on the `ContainerState` of the container that has the same name with the CR?"
      ]
    },
    "profile": {
      "location": "Athens",
      "company": "@Canonical",
      "blog": "https://www.linkedin.com/in/kimonas-sotirchos-1ba45b155/",
      "site_admin": false,
      "followers": 74,
      "following": 7
    }
  },
  "jmcdo29": {
    "repos": [
      "nestjs/nest"
    ],
    "entries": [
      {
        "slug": "nest-avoid-testing-anti-patterns",
        "title": "Avoid testing anti-patterns"
      },
      {
        "slug": "nest-benchmark-before-optimizing",
        "title": "Benchmark before optimizing"
      },
      {
        "slug": "nest-configurable-log-formatting",
        "title": "Configurable log formatting"
      },
      {
        "slug": "nest-explicit-default-configurations",
        "title": "Explicit default configurations"
      },
      {
        "slug": "nest-follow-protocol-standards",
        "title": "Follow protocol standards"
      },
      {
        "slug": "nest-http-header-management",
        "title": "HTTP header management"
      },
      {
        "slug": "nest-manage-testing-dependencies",
        "title": "Manage testing dependencies"
      },
      {
        "slug": "nest-optimize-critical-path-iterations",
        "title": "Optimize critical path iterations"
      },
      {
        "slug": "nest-parameterize-version-requirements",
        "title": "Parameterize version requirements"
      },
      {
        "slug": "nest-pin-dependency-versions",
        "title": "Pin dependency versions"
      },
      {
        "slug": "nest-preserve-api-interface-stability",
        "title": "Preserve API interface stability"
      },
      {
        "slug": "nest-preserve-public-api-stability",
        "title": "Preserve public API stability"
      },
      {
        "slug": "nest-prevent-async-race-conditions",
        "title": "Prevent async race conditions"
      },
      {
        "slug": "nest-prevent-race-conditions",
        "title": "Prevent race conditions"
      },
      {
        "slug": "nest-structure-behavior-driven-tests-properly",
        "title": "Structure behavior-driven tests properly"
      },
      {
        "slug": "nest-test-dependency-management",
        "title": "Test dependency management"
      },
      {
        "slug": "nest-use-consistent-control-structures",
        "title": "Use consistent control structures"
      },
      {
        "slug": "nest-use-consistent-curly-braces",
        "title": "Use consistent curly braces"
      },
      {
        "slug": "nest-use-factory-providers",
        "title": "Use factory providers"
      }
    ],
    "comments": {
      "nest-use-consistent-control-structures": [
        "Would you mind updating the code to wrap the internals of the if in curly braces? It leads the code to be easier to follow and modify if we need to\r\n\r\n```suggestion\r\n    if (this.flushLogsOnOverride) {\r\n      this.flushLogs();\r\n    }\r\n```",
        "If these are going to be single line ifs, would a ternary make more sense?\r\n```\r\ndata += message.type ? `event: ${message.type}\\n` : ''\r\n```\r\n",
        "Personally, I'm not a fan of inline ifs, so if we stay with using `if` statements it should be changed to\r\n```ts\r\nif (message.type) {\r\n  data += `event: ${message.type}\\n`;\r\n}\r\n```\r\nAnd this is what most of Nest's codebase does already even for single line. So ternary, or using braces as necessary should be fine. Just to stay consistent with the rest of the codebase"
      ],
      "nest-configurable-log-formatting": [
        "This is _mostly_ only used by the `ConsoleLogger`, but there are references to it in the `Injector.ts` file as well. If it was just the `ConsoleLogger` I'd say this is okay, or even checking `process.stdout.hasColors()` directly instead of needing the prototype, but I'm not sure how to handle if this check is coming from the `Injector.ts`\r\n\r\nOverall, I don't _think_ this should be an issue, my only concern would be if someone is in an environment where `hasColors()` returns `false` but they want to force the use of colors. "
      ],
      "nest-benchmark-before-optimizing": [
        "If you're going to refactor this to a raw for loop, we should do the same to the `getAll` for squeezing as much performance out as we can. \r\n\r\nSide note: do you have any benchmarks for how much of an improvement this is? Just curious if the improvement is going to be worth the readability "
      ],
      "nest-manage-testing-dependencies": [
        "Looks like it's for a new integration test. As it's in `devDeps` it should be fine"
      ],
      "nest-test-dependency-management": [
        "Looks like it's for a new integration test. As it's in `devDeps` it should be fine"
      ],
      "nest-parameterize-version-requirements": [
        "npm v9 was released on 2022-10-24 is is not compatible with node 12. Today (2022-11-09) it was set to the `@latest` tag for `npm` so our CircleCI builds started breaking. \r\n\r\n> Wednesday Nov. 9th (General Availability)\r\n> To ensure npm@9.x is considered \"non-breaking\" for Node.js LTS we will codify a set of exit criteria in collaboration with the [Release WG](https://github.com/nodejs/release)\r\n> npm@9.x will be set to the latest dist-tag (becoming the latest, maintained version of npm)\r\n> A PR will be opened to land npm@9.x in nodejs/node's main branch (exposing experimental/nightly users to this latest version)\r\n\r\nhttps://github.blog/changelog/2022-10-24-npm-v9-0-0-released/"
      ],
      "nest-prevent-async-race-conditions": [
        "Rather than mutating the client, could I add a new `args` entry and let `getPattern()` retrieve `args[2]`? Would that be more \"stable\"? ",
        "Great call by the way! I moved `getPattern` to be a method on `WsArgumentHost` instead of under the `getClient()` so that it should be unique per request as the `WsArgumentHost` already is. Tests are still passing and the interfaces are updated. Let me know if you think of any other issues with this approach :smile_cat:"
      ],
      "nest-prevent-race-conditions": [
        "Rather than mutating the client, could I add a new `args` entry and let `getPattern()` retrieve `args[2]`? Would that be more \"stable\"? ",
        "Great call by the way! I moved `getPattern` to be a method on `WsArgumentHost` instead of under the `getClient()` so that it should be unique per request as the `WsArgumentHost` already is. Tests are still passing and the interfaces are updated. Let me know if you think of any other issues with this approach :smile_cat:"
      ],
      "nest-structure-behavior-driven-tests-properly": [
        "```suggestion\r\n      expect(usersController.create(createUserDto)).resolves.toEqual({\r\n        id: 'a id',\r\n        ...createUserDto,\r\n      });\r\n      expect(usersService.create).toHaveBeenCalled();\r\n      expect(usersService.create).toHaveBeenCalledWith(createUserDto);\r\n```\r\nNo need to call `usersController.create` twice. "
      ],
      "nest-avoid-testing-anti-patterns": [
        "```suggestion\r\n      expect(usersController.create(createUserDto)).resolves.toEqual({\r\n        id: 'a id',\r\n        ...createUserDto,\r\n      });\r\n      expect(usersService.create).toHaveBeenCalled();\r\n      expect(usersService.create).toHaveBeenCalledWith(createUserDto);\r\n```\r\nNo need to call `usersController.create` twice. ",
        "There's no need to call `controller.findAll()` twice in the test. Just call it in the assertion and then do the jest check after it\r\n```suggestion\r\n      expect(controller.findAll()).resolves.toEqual([\r\n        {\r\n          name: 'Cat #1',\r\n          breed: 'Bread #1',\r\n          age: 4,\r\n        },\r\n        {\r\n          name: 'Cat #2',\r\n          breed: 'Breed #2',\r\n          age: 3,\r\n        },\r\n        {\r\n          name: 'Cat #3',\r\n          breed: 'Breed #3',\r\n          age: 2,\r\n        },\r\n      ]);\r\n      expect(service.findAll).toHaveBeenCalled();\r\n```",
        "This isn't an actual test of anything. You mock the value you want to call and assert the test just called the service method. This should instead mock the repository method used in `service.remove`"
      ],
      "nest-preserve-api-interface-stability": [
        "Adding non-optional methods to a public interface that should be implemented for adapters means that anyone who is maintaining their own adapter, say for Koa, or hyper-express, etc, now **must** make these changes or possibly have their packages broken by a transient upgrade.\r\n\r\nThis is a braking change, and while you may have been waiting for it for a while, that doesn't mean it's any more urgent to get out, as we want to avoid making several small breaking changes and would rather lump them together over a major upgrade."
      ],
      "nest-http-header-management": [
        "If you were to have a falsy header value, such as `false`, `0` or `''`, no browser that I'm aware of would know how to properly handle that response, and as such the body might be unrecoverable, or at least encoded in an unexpected way.\n\nI think think is a decent restriction to keep from giving the dev a nice foot gun, but I'd be willing to hear what the browser should do if the header was one of those values, especially if there's an RFC for it as well",
        "I assume this will automatically be `chunk`? If not, should we set it to that?"
      ],
      "nest-use-consistent-curly-braces": [
        "Would you mind updating the code to wrap the internals of the if in curly braces? It leads the code to be easier to follow and modify if we need to\r\n\r\n```suggestion\r\n    if (this.flushLogsOnOverride) {\r\n      this.flushLogs();\r\n    }\r\n```",
        "If these are going to be single line ifs, would a ternary make more sense?\r\n```\r\ndata += message.type ? `event: ${message.type}\\n` : ''\r\n```\r\n",
        "Personally, I'm not a fan of inline ifs, so if we stay with using `if` statements it should be changed to\r\n```ts\r\nif (message.type) {\r\n  data += `event: ${message.type}\\n`;\r\n}\r\n```\r\nAnd this is what most of Nest's codebase does already even for single line. So ternary, or using braces as necessary should be fine. Just to stay consistent with the rest of the codebase"
      ],
      "nest-preserve-public-api-stability": [
        "Adding non-optional methods to a public interface that should be implemented for adapters means that anyone who is maintaining their own adapter, say for Koa, or hyper-express, etc, now **must** make these changes or possibly have their packages broken by a transient upgrade.\r\n\r\nThis is a braking change, and while you may have been waiting for it for a while, that doesn't mean it's any more urgent to get out, as we want to avoid making several small breaking changes and would rather lump them together over a major upgrade."
      ],
      "nest-explicit-default-configurations": [
        "Instead of marking this as optional, could we give it a default value?\r\n\r\n```suggestion\r\n    appOptions: NestApplicationOptions = {\r\n      disableInstanceLoaderLogs: false\r\n    },\r\n  ) {\r\n    const instanceLoader = new InstanceLoader(container);\r\n\r\n    if (appOptions.disableInstanceLoaderLogs) {\r\n      instanceLoader.disableLogs();\r\n    }\r\n```\r\n\r\nAlso, do we need the full options here, or just the `disableInstanceLoaderLogs` option? We might be able to cut this down to a simple `boolean` instead of the full object"
      ],
      "nest-pin-dependency-versions": [
        "npm v9 was released on 2022-10-24 is is not compatible with node 12. Today (2022-11-09) it was set to the `@latest` tag for `npm` so our CircleCI builds started breaking. \r\n\r\n> Wednesday Nov. 9th (General Availability)\r\n> To ensure npm@9.x is considered \"non-breaking\" for Node.js LTS we will codify a set of exit criteria in collaboration with the [Release WG](https://github.com/nodejs/release)\r\n> npm@9.x will be set to the latest dist-tag (becoming the latest, maintained version of npm)\r\n> A PR will be opened to land npm@9.x in nodejs/node's main branch (exposing experimental/nightly users to this latest version)\r\n\r\nhttps://github.blog/changelog/2022-10-24-npm-v9-0-0-released/",
        "What are the differences between v2 and v3?"
      ],
      "nest-use-factory-providers": [
        "I'd much rather we direct the user to the issue rather than have them dig into the source code to find the issue reference ",
        "[This wouldn't be the first time we've had a link in the exception](https://github.com/nestjs/nest/blob/ef5344826f23b5377a2f2599cf9efda0303e1f33/packages/core/errors/messages.ts#L163)",
        "And I'm certain we've linked to issues from within our docs. If we _really_ don't want to link to the issue directly, then we should make a new section of the FAQ > Common Errors for this"
      ],
      "nest-optimize-critical-path-iterations": [
        "If you're going to refactor this to a raw for loop, we should do the same to the `getAll` for squeezing as much performance out as we can. \r\n\r\nSide note: do you have any benchmarks for how much of an improvement this is? Just curious if the improvement is going to be worth the readability "
      ],
      "nest-follow-protocol-standards": [
        "If you were to have a falsy header value, such as `false`, `0` or `''`, no browser that I'm aware of would know how to properly handle that response, and as such the body might be unrecoverable, or at least encoded in an unexpected way.\n\nI think think is a decent restriction to keep from giving the dev a nice foot gun, but I'd be willing to hear what the browser should do if the header was one of those values, especially if there's an RFC for it as well",
        "I assume this will automatically be `chunk`? If not, should we set it to that?"
      ]
    },
    "profile": {
      "location": "Concrete, WA",
      "blog": "",
      "twitter_username": "jmcdo29",
      "site_admin": false,
      "followers": 1031,
      "following": 15
    }
  },
  "joostlek": {
    "repos": [
      "home-assistant/core"
    ],
    "entries": [
      {
        "slug": "core-api-documentation-consistency",
        "title": "API documentation consistency"
      },
      {
        "slug": "core-api-response-transformation",
        "title": "API response transformation"
      },
      {
        "slug": "core-avoid-code-duplication",
        "title": "avoid code duplication"
      },
      {
        "slug": "core-batch-operations-efficiently",
        "title": "Batch operations efficiently"
      },
      {
        "slug": "core-classify-data-sensitivity",
        "title": "classify data sensitivity"
      },
      {
        "slug": "core-clear-variable-naming",
        "title": "Clear variable naming"
      },
      {
        "slug": "core-configuration-status-accuracy",
        "title": "configuration status accuracy"
      },
      {
        "slug": "core-consistent-naming-standards",
        "title": "Consistent naming standards"
      },
      {
        "slug": "core-minimize-try-block-scope",
        "title": "Minimize try block scope"
      },
      {
        "slug": "core-mock-external-dependencies-only",
        "title": "Mock external dependencies only"
      },
      {
        "slug": "core-prefer-none-over-placeholders",
        "title": "prefer None over placeholders"
      },
      {
        "slug": "core-trust-server-side-validation",
        "title": "trust server-side validation"
      },
      {
        "slug": "core-use-appropriate-logging-levels",
        "title": "Use appropriate logging levels"
      },
      {
        "slug": "core-use-configuration-constants",
        "title": "Use configuration constants"
      }
    ],
    "comments": {
      "core-trust-server-side-validation": [
        "How should and address be formatted? Or lat/lon? I would argue that these are separate actions which require separate validation"
      ],
      "core-batch-operations-efficiently": [
        "We should collect all entities during initialisation and add them at once instead of calling async add entities multiple times",
        "Switching from executor job to event loop is expensive, so let's make 1 function and call them in the same exectuor job"
      ],
      "core-api-documentation-consistency": [
        "URLs should be put in via description placeholders. You can also use markdown here"
      ],
      "core-use-appropriate-logging-levels": [
        "Ideally we only log the `Exception` with `logger.exception` as there we truly don't know what happened and need the full stack trace. With the rest I am wondering if we need to have the full stack trace every time"
      ],
      "core-configuration-status-accuracy": [
        "I think this is the place where you can explain why we pick devices instead of set up one account"
      ],
      "core-prefer-none-over-placeholders": [
        "```suggestion\r\n            model=host_data.get(\"devmodel\"),\r\n            name=data.hostname,\r\n            sw_version=host_data.get(\"fwversion\"),\r\n```\r\nrather set it to `None` than to set an arbritary name",
        "I just saw that list where you mention that we map this unknown, is that also applicable here?",
        "In that case, we should not expose `unknown` as a possible state here, from what I read here we either show `unspecified` as valid option, but if that is translated to `None` then neither can be a valid option",
        "We should map `???` to `None`"
      ],
      "core-mock-external-dependencies-only": [
        "We should not touch internals like entry.runtime_data, instead we should check the behavior of the entities that are attached",
        "We don't want to create entities manually in the tests. Instead, mock out the library and have the test setup the integration and then observe the entity via the state machine or the entity registry"
      ],
      "core-minimize-try-block-scope": [
        "only have things in the try block that can raise",
        "only have things in the try block that can raise"
      ],
      "core-consistent-naming-standards": [
        "Let's write this out, I have no clue what TM means apart from Trackmania."
      ],
      "core-use-configuration-constants": [
        "why would this be not set?"
      ],
      "core-classify-data-sensitivity": [
        "is a group ID personal info?"
      ],
      "core-clear-variable-naming": [
        "Entities should use `_attr_has_entity_name = True`",
        "no need to add the `fluss_` prefix as a unique id is unique per domain per platform"
      ],
      "core-api-response-transformation": [
        "currently this is one big string if I understand correctly. Can we make the response better and return proper JSON and use snake_case for fields. It would also be nice if we can have a few with parameters so the tests also test how that would look",
        "The thing with service responses is, is that we should not break them, because automations might depend on them. So if they don't flag when their API changes, we should add an extra translation layer to avoid breaking this in the future and have the schema in our control",
        "would it make sense to add the extra context like hours and minutes here as well?",
        "Right, but for super users it might make sense what [12, 0] mean, while for normal users it does not, so I would argue that having it in a dict with `hours` and `minutes` as key would clear this up and make this data consumable without any external information, while for the super users, they just have to use `[\"hours\"]` instead of `[0]`",
        "That would work for me as well, but then you are for sure deviating from the original format. Like, I don't know where this applies, but if you have multiple 5+ hour programs, seeing 345 always makes me calculate what that means, while 5 hours and 45 minutes already gives me this answer.\r\n\r\nBut up to you :)"
      ],
      "core-avoid-code-duplication": [
        "```suggestion\r\n    @property\r\n    def device_info(self) -> DeviceInfo:\r\n        \"\"\"Return device information for Home Assistant.\"\"\"\r\n        return DeviceInfo(\r\n            identifiers={(DOMAIN, self.device_identifier)},\r\n            manufacturer=self.device_data.get(\"manufacturer\"),\r\n            model=self.device_data.get(\"DM\"),\r\n            name=f\"{self.device_identifier} {self.device_data.get('name')}\",\r\n            serial_number=self.device_data.get(\"serial_number\"),\r\n            sw_version=self.device_data.get(\"sw_version\"),\r\n        )\r\n```\r\nIMO this belongs in an entity platform (like `sensor.py`) or a base entity (in `entity.py`) as the coordinator is resposible for fetching data, and this is a way the entity displays itself",
        "Would it maybe make sense to create a base entity that will be inherited in both the media player and the buttons? It would help deduplicate the device info logic",
        "I would reommend doing this in a separate PR",
        "Instead of having 2 classes, create a generic class that takes in an `ButtonEntityDescription`, which you can extend for Bluesound to add a `press_fn` which you execute when someone presses the button. This way we can deduplicate even less"
      ]
    },
    "profile": {
      "location": "Utrecht, The Netherlands",
      "company": "@openhomefoundation, Lekkerkerker Software Development",
      "blog": "joostlek.dev",
      "twitter_username": "joostlek",
      "site_admin": false,
      "followers": 168,
      "following": 251
    }
  },
  "treo": {
    "repos": [
      "deeplearning4j/deeplearning4j"
    ],
    "entries": [
      {
        "slug": "deeplearning4j-centralize-dependency-management",
        "title": "Centralize dependency management"
      },
      {
        "slug": "deeplearning4j-clean-up-your-code",
        "title": "Clean up your code"
      },
      {
        "slug": "deeplearning4j-clear-descriptive-identifiers",
        "title": "Clear descriptive identifiers"
      },
      {
        "slug": "deeplearning4j-compare-floating-point-safely",
        "title": "Compare floating-point safely"
      },
      {
        "slug": "deeplearning4j-configurable-resource-locations",
        "title": "Configurable resource locations"
      },
      {
        "slug": "deeplearning4j-descriptive-error-context",
        "title": "Descriptive error context"
      },
      {
        "slug": "deeplearning4j-document-ai-implementation-references",
        "title": "Document AI implementation references"
      },
      {
        "slug": "deeplearning4j-document-api-completely",
        "title": "Document API completely"
      },
      {
        "slug": "deeplearning4j-eliminate-redundant-code",
        "title": "Eliminate redundant code"
      },
      {
        "slug": "deeplearning4j-keep-configurations-current",
        "title": "Keep configurations current"
      },
      {
        "slug": "deeplearning4j-minimize-object-allocations",
        "title": "Minimize object allocations"
      },
      {
        "slug": "deeplearning4j-modular-adaptive-configurations",
        "title": "Modular adaptive configurations"
      },
      {
        "slug": "deeplearning4j-numerical-stability-practices",
        "title": "Numerical stability practices"
      },
      {
        "slug": "deeplearning4j-preserve-api-compatibility",
        "title": "Preserve API compatibility"
      },
      {
        "slug": "deeplearning4j-prevent-memory-leaks",
        "title": "Prevent memory leaks"
      },
      {
        "slug": "deeplearning4j-remove-debugging-artifacts",
        "title": "Remove debugging artifacts"
      },
      {
        "slug": "deeplearning4j-use-appropriate-logging-levels",
        "title": "Use appropriate logging levels"
      },
      {
        "slug": "deeplearning4j-use-logging-best-practices",
        "title": "Use logging best practices"
      },
      {
        "slug": "deeplearning4j-validate-and-document-nulls",
        "title": "Validate and document nulls"
      }
    ],
    "comments": {
      "deeplearning4j-document-ai-implementation-references": [
        "The 1e9 is used by the tensor2tensor package as well. Bert on the other hand uses just 1e4, while GPT-2 uses 1e10."
      ],
      "deeplearning4j-configurable-resource-locations": [
        "We've had issues with people creating uberjars and not packaging resources previously. So having something that is dependent on loading a resource may introduce new problems. \r\n\r\nIn particular, Spring Boot has a special way of class loading and resource loading. I can imagine that it may break there."
      ],
      "deeplearning4j-minimize-object-allocations": [
        "good idea, I'll change it accordingly in the createBias method as well then."
      ],
      "deeplearning4j-use-logging-best-practices": [
        "Does it make sense to keep this at info log level? or should this maybe move to the debug log level, as you did with the logging in the other branch?"
      ],
      "deeplearning4j-keep-configurations-current": [
        "Have you cleared this with eclipse EMO? I remember that they needed to clear any additional dependency or vendoring.",
        "We should probably update this to match the suggested maven version for the rest of DL4J - Or remove the maven wrapper entirely.",
        "Got an answer to that question?"
      ],
      "deeplearning4j-use-appropriate-logging-levels": [
        "printf outside of an error condition.",
        "shouldn't this be an sd_debug? Same applies to other usages of sd_printf and printIndexedBuffer in this file",
        "is this supposed to be all `sd_printf`? Or did you want `sd_debug` instead?"
      ],
      "deeplearning4j-centralize-dependency-management": [
        "If possible, let's not use rc versions.",
        "It would probably make sense to use the property defined above in all of those jmh dependencies "
      ],
      "deeplearning4j-modular-adaptive-configurations": [
        "I'd probably split this some more into a base command and extension parameters. \r\n\r\nSomething like\r\n```bash\r\nif [ \"${HELPER}\" != '' ] && [ \"${EXTENSION}\" != '' ]; then\r\n    mvn_ext=\"-Djavacpp.platform.extension=-${{ matrix.helper }}-${{ matrix.extension }} -Dlibnd4j.helper=${{ matrix.helper }} -Dlibnd4j.extension=${{ matrix.extension }}\"\r\nelif [ \"${HELPER}\" != '' ]; then\r\n    mvn_ext=\"-Djavacpp.platform.extension=-${{ matrix.helper }} -Dlibnd4j.helper=${{ matrix.helper }} -Dlibnd4j.extension=${{ matrix.extension }}\"\r\nelse\r\n    mvn_ext=\"-Djavacpp.platform.extension=${{ matrix.extension }} -Dlibnd4j.helper=${{ matrix.helper }} -Dlibnd4j.extension=${{ matrix.extension }}\"\r\nfi\r\n\r\ncommand=\"mvn -Possrh ${mvn_ext} -Dlibnd4j.buildThreads=${{ github.event.inputs.buildThreads }}  -Djavacpp.platform=linux-x86_64 -Dlibnd4j.chip=cuda --also-make -Pcuda clean --batch-mode package deploy -DskipTests\"\r\n```\r\n\r\nAnd having refactored it like that, I wonder, why do you even specify those values in the cases where they aren't defined?",
        "If you want to make the hack hackier, you can replace the version this with\r\n```\r\nsudo cp /usr/lib/gcc/x86_64-linux-gnu/`gcc --version | head -n 1 | grep -o '[^ ]*$'` /usr/lib\r\n```"
      ],
      "deeplearning4j-eliminate-redundant-code": [
        "Looks like a PR breakout bug? The constant is defined twice. ",
        "printf outside of error condition. ",
        "Any reason why this files has soo many licence headers?"
      ],
      "deeplearning4j-clean-up-your-code": [
        "left over debug prints?",
        "I guess this print is a left over from debugging?",
        "Please don't leave any commented out code around.",
        "Please don't leave commented out code around.",
        "Please don't leave any commented out code to hang around",
        "please don't leave code commented out like that.",
        "let's not have commented out code. If this is just temporary, put at least a TODO there, so it is obvious that this is still work in progress. Otherwise it is easy to miss this when you do the cleanup. ",
        "Please don't leave any commented out code behind.",
        "Can we turn this magic number into a constant? "
      ],
      "deeplearning4j-remove-debugging-artifacts": [
        "Still contains a print.",
        "left over debug printing?",
        "printf outside of error condition.",
        "printf outside of error condition.",
        "printf outside of error condition.",
        "Let's not have commented out code like that. And in particular not have a for loop doing nothing. ",
        "Unconditional printf. I guess a leftover from debugging?",
        "Please don't leave any commented out code around. Either remove it or keep it, but just commenting things out without any indication for why that is, shouldn't make it into a merged PR.",
        "Please don't leave any commented out code around"
      ],
      "deeplearning4j-descriptive-error-context": [
        "While \"Failed execution\" is a better message than \"Boom\", I wonder if we can't have something more descriptive here?\r\n\r\nAlso, Do we need to both print and throw an exception? "
      ],
      "deeplearning4j-preserve-api-compatibility": [
        "When adding new arguments, I think it makes sense to make them optional in order to keep backwards compatibility with existing code.",
        "This will probably break some backwards compatibility - at least for people that are using Op objects directly.\r\n\r\nI think we can probably accept that breakage, as most people will be using the factory methods and for them it should be transparent."
      ],
      "deeplearning4j-compare-floating-point-safely": [
        "Is 0.01 really a good default value for this? Typical epsilon values are 1e-6, 1e-7 or even 1e-12 for double. "
      ],
      "deeplearning4j-prevent-memory-leaks": [
        "Missing a delete for this one?"
      ],
      "deeplearning4j-validate-and-document-nulls": [
        "Do we need the `null` case anywhere? Or wouldn't it make more sense to handle `null` in the `outputVariables()` function, so that everything downstream doesn't need to have special case handling?"
      ],
      "deeplearning4j-clear-descriptive-identifiers": [
        "I guess a better name here would be just `releaseGilAutomatically`."
      ],
      "deeplearning4j-numerical-stability-practices": [
        "The paper authors have found that a smaller default epsilon works better:\r\n\r\n> Epsilon in AdaBelief is different from Adam (typically eps_adabelief = eps_adam*eps_adam)\r\n> ( eps of Adam in Tensorflow is 1e-7, in PyTorch is 1e-8, need to consider this when use AdaBelief in Tensorflow)\r\n\r\nSo I guess we might want to use either 1e-14 or 1e-16 here. ",
        "maybe also check that 0 <= maskTokenProb + randomTokenProb <= 1?",
        "Should this even happen? Calculating with a zero gradient, should basically be \"don't do any change at all\" and therefore we shouldn't be performing backprop at all in that case?",
        "got it. Maybe add the explanation to the comment on top?"
      ],
      "deeplearning4j-document-api-completely": [
        "It may make sense to put this as Javadoc on the individual Enums? ",
        "Again, a comment why this is empty would be useful."
      ]
    },
    "profile": {
      "location": "Mainz, Germany",
      "company": "@XpressAI",
      "blog": "https://www.dubs.tech",
      "site_admin": false,
      "followers": 99,
      "following": 4
    }
  },
  "chia7712": {
    "repos": [
      "apache/kafka"
    ],
    "entries": [
      {
        "slug": "kafka-avoid-overly-specific-examples",
        "title": "avoid overly specific examples"
      },
      {
        "slug": "kafka-avoid-unnecessary-object-creation",
        "title": "avoid unnecessary object creation"
      },
      {
        "slug": "kafka-catch-specific-exceptions",
        "title": "catch specific exceptions"
      },
      {
        "slug": "kafka-centralize-configuration-values",
        "title": "Centralize configuration values"
      },
      {
        "slug": "kafka-comprehensive-test-coverage",
        "title": "comprehensive test coverage"
      },
      {
        "slug": "kafka-condition-based-network-synchronization",
        "title": "condition-based network synchronization"
      },
      {
        "slug": "kafka-defensive-null-validation",
        "title": "Defensive null validation"
      },
      {
        "slug": "kafka-improve-code-readability",
        "title": "Improve code readability"
      },
      {
        "slug": "kafka-maintain-naming-consistency",
        "title": "maintain naming consistency"
      },
      {
        "slug": "kafka-optimize-algorithmic-complexity",
        "title": "optimize algorithmic complexity"
      },
      {
        "slug": "kafka-optimize-collection-conversions",
        "title": "Optimize collection conversions"
      },
      {
        "slug": "kafka-optimize-data-structures",
        "title": "optimize data structures"
      },
      {
        "slug": "kafka-prefer-modern-collection-apis",
        "title": "prefer modern collection APIs"
      },
      {
        "slug": "kafka-use-condition-based-waiting",
        "title": "Use condition-based waiting"
      },
      {
        "slug": "kafka-use-parameterized-logging",
        "title": "Use parameterized logging"
      },
      {
        "slug": "kafka-validate-configuration-dependencies",
        "title": "validate configuration dependencies"
      },
      {
        "slug": "kafka-validate-configurations-early",
        "title": "Validate configurations early"
      },
      {
        "slug": "kafka-validate-network-state",
        "title": "validate network state"
      }
    ],
    "comments": {
      "kafka-avoid-unnecessary-object-creation": [
        "we could reuse the `partition` instead of creating new `AlterShareGroupOffsetsResponsePartition`, right?",
        "Could you please use `Collections.unmodifiableSet` instead to avoid extra deep copy?",
        "If it is hotspot, we should not generate optional object.\r\n```java\r\n            for (int brokerId : brokers) {\r\n                var broker = image.cluster().broker(brokerId);\r\n                if (broker != null && !broker.fenced() && broker.listeners().containsKey(listenerName.value()))\r\n                    res.add(brokerId);\r\n            }\r\n```",
        "Perhaps we don't need to create this temporary list.\r\n```java\r\n        Set<String> allNames = new HashSet<>();\r\n        allNames.addAll(MirrorCheckpointConfig.CONNECTOR_CONFIG_DEF.names());\r\n        allNames.addAll(MirrorSourceConfig.CONNECTOR_CONFIG_DEF.names());\r\n        allNames.addAll(MirrorHeartbeatConfig.CONNECTOR_CONFIG_DEF.names());\r\n        return allNames;\r\n```\r\n\r\nor\r\n\r\n```java\r\n        return Stream.of(\r\n                    MirrorCheckpointConfig.CONNECTOR_CONFIG_DEF.names(),\r\n                    MirrorSourceConfig.CONNECTOR_CONFIG_DEF.names(),\r\n                    MirrorHeartbeatConfig.CONNECTOR_CONFIG_DEF.names()\r\n                )\r\n                .flatMap(Set::stream)\r\n                .collect(Collectors.toSet());\r\n```"
      ],
      "kafka-use-parameterized-logging": [
        "the last parameter could be handled well if it is an exception object. Perhaps, we could use `{}` instead of `{}:{}`?",
        "Perhaps we should add the `e`  to the logger\r\n```java\r\nLOGGER.error(\"Encountered error while deleting {}\", file.getAbsolutePath(), e);\r\n```",
        "`LOG.error` could accept exception directly. Perhaps we could replace `due to: %s` by `LOG.error(msg, ex)`?"
      ],
      "kafka-optimize-collection-conversions": [
        "This will create many temporary collections. How about `partitionState.isr.asScala.map(_.toInt).diff(outOfSyncReplicaIds)`",
        "ditto",
        "ditto"
      ],
      "kafka-maintain-naming-consistency": [
        "According to the naming, it should pass `false` instead of `true`, shouldn't it?"
      ],
      "kafka-condition-based-network-synchronization": [
        "Perhaps it could be replaced by `TestUtils.waitForPartitionMetadata(brokers, partition2.topic(), partition2.partition())`?"
      ],
      "kafka-optimize-data-structures": [
        "```java\r\n    private DescribeTopicsResult describeTopicsResult(Collection<String> topics, int numOfPartitions) {\r\n        var topicDescriptions = topics.stream().collect(Collectors.toMap(Function.identity(),\r\n                topic -> new TopicDescription(topic, false, IntStream.range(0, numOfPartitions)\r\n                .mapToObj(i -> new TopicPartitionInfo(i, null, List.of(), List.of()))\r\n                .toList())));\r\n        return AdminClientTestUtils.describeTopicsResult(topicDescriptions);\r\n    }\r\n```",
        "```java\r\n        List<Integer> offlineReplicas = new ArrayList<>(0);\r\n        for (var brokerId : partition.replicas) {\r\n            var broker = image.cluster().broker(brokerId);\r\n            if (broker == null || isReplicaOffline(partition, listenerName, broker))\r\n                offlineReplicas.add(brokerId);\r\n        }\r\n        return offlineReplicas;\r\n```",
        "If we allow reusing the previous commit when matching subsets, the returned `Map<TopicPartition, OffsetAndMetadata>` will contain unrelated topic partitions. Therefore, should we adjust the return map of `consumer#committed` to match the input?"
      ],
      "kafka-prefer-modern-collection-apis": [
        "`toMap` returns the mutable map, so `new HashMap` is unnecessary.",
        "```java\r\nreturn List.of(InternalMirrorResource.class);\r\n```"
      ],
      "kafka-improve-code-readability": [
        "```java\r\n    assertNotEquals(JoinGroupRequest.UNKNOWN_MEMBER_ID, memberId)\r\n    assertNotEquals(JoinGroupRequest.UNKNOWN_GENERATION_ID, memberEpoch)\r\n```\r\n`assertNotEquals` could show more readable messages.",
        "Could you please move those code to separate method? That can reduce the complexity and it can be test easily."
      ],
      "kafka-use-condition-based-waiting": [
        "Could you please use \"wait for condition\" instead of time-based waiting? "
      ],
      "kafka-avoid-overly-specific-examples": [
        "for another, perhaps we could use `openjdk:17` for this example to avoid using the explicit os version."
      ],
      "kafka-optimize-algorithmic-complexity": [
        "Perhaps we could leverage `iterator` to avoid iterating through all items twice.\r\n```java\r\n        var iter = finalizedFeatureLevels.keySet().iterator();\r\n        while (iter.hasNext()) {\r\n            var featureName = iter.next();\r\n            if (newFinalizedLevels.containsKey(featureName) ||\r\n                featureName.equals(MetadataVersion.FEATURE_NAME) ||\r\n                featureName.equals(KRaftVersion.FEATURE_NAME)) {\r\n                continue;\r\n            }\r\n            removeFinalizedFeatureLevelMetric(featureName);\r\n            iter.remove();\r\n        }\r\n```"
      ],
      "kafka-catch-specific-exceptions": [
        "Perhaps the log could include the exception?",
        "@k-raina catching the `Throwable` is not a major issue, so there is no hurry to include it in the 4.1.0 release. Perhaps you could address @apoorvmittal10 comment in this PR, and then we could backport it to 4.1.1  ",
        "Perhaps we could have an `unsupportedTypes` collection to filter out types according to `NoClassDefFoundError`? for example:\r\n```java\r\n\r\n                if (e instanceof NoClassDefFoundError) unsupportedTypes.add(compressionType);\r\n```\r\n`ClientTelemetryUtils.preferredCompressionType` could leverage the collection `compressionType` to avoid using  unsupported compression. for example:\r\n```java\r\n    public static CompressionType preferredCompressionType(List<CompressionType> acceptedCompressionTypes, Set<CompressionType> unsupportedTypes) {\r\n        if (acceptedCompressionTypes == null) return CompressionType.NONE;\r\n        // Broker is providing the compression types in order of preference. Grab the\r\n        // first one.\r\n        return acceptedCompressionTypes.stream()\r\n                .filter(t -> !unsupportedTypes.contains(t))\r\n                .findFirst()\r\n                .orElse(CompressionType.NONE);\r\n    }\r\n````",
        "Did you mean specify the accurate exception, like `IOException | NoClassDefFoundError`,  instead of `Exception`? \r\n\r\n> Catching Throwable like this isn't safe.\r\n\r\nCould you share the details of your concern with me?"
      ],
      "kafka-comprehensive-test-coverage": [
        "It would be useful to have a unit test to loop over 1000 times, using random input to ensure before/after methods have same output.",
        "Could you please add unit test to ensure the maximum value of capacity of buffer is equal to `maxMessageSize`?"
      ],
      "kafka-validate-configuration-dependencies": [
        "Can two brokers with two replicas fix the issue you mentioned?",
        "Perhaps we should ensure the exposed metrics are always based on current image. The benefit is the exposed finalized versions will be consistent to current image, and we won't  need to use `minimumProduction` which puts us in a weird position\r\n",
        "> If the feature does not have a finalized level, it does not have a finalizedLevel metric?\n\nYes, that is what I meant"
      ],
      "kafka-validate-network-state": [
        "if there is a topic having three partitions, and the `t-2` partition is offline, then if `partitionsToReset` is `t-0,t-1`, `filterNoneLeaderPartitions` will return `t-2`, causing the tool to fail. Is it expected?",
        "```\r\nchia7712@chia7712-ubuntu:~/project/kafka$ ./bin/kafka-consumer-groups.sh \\\r\n  --bootstrap-server 172.20.10.2:20001 \\\r\n  --reset-offsets \\\r\n  --to-earliest \\\r\n  --execute \\\r\n  --group perf-consumer-19460 \\\r\n  --topic chia:1\r\n\r\nError: Executing consumer group command failed due to The partitions \"chia-2\" have no leader\r\norg.apache.kafka.common.errors.LeaderNotAvailableException: The partitions \"chia-2\" have no leader\r\n```\r\n\r\nIt is indeed a bug that unrelated topic partitions could fail the tool.",
        "`filterNoneLeaderPartitions` needs to get fixed since it could return unrelated topic partitions.",
        "we don't need to modify the input `topicPartitions` if `filterNoneLeaderPartitions` returns the partitions having leaders."
      ],
      "kafka-defensive-null-validation": [
        "`QuorumFeatures.defaultSupportedFeatureMap` exclude the feature if the `enableUnstableVersions` is false and production version is zero. Hence, should we iterate `supportedFeatureRanges` instead of `Feature.PRODUCTION_FEATURE_NAMES` to avoid a possible NPE?"
      ],
      "kafka-validate-configurations-early": [
        "It would be cool if `FeatureCommand` and `StorageTool` could leverage the exception from `fromVersionString`. After fixing KAFKA-19545, the `MetadataVersion.fromVersionString` will be used only by tools only, so it could throw `TerseFailure` instead of `IllegalArgumentException`, which could then be directly reused by tools.",
        "umm, `TerseFailure` is in core module, which can't be accessed by tool module. Also, `StorageTool` is still in core module, so `StorageTool` and `FeatureCommand` can't share the same `terse` exception for now. I guess another simple solution is they could just reuse the exception message thrown by `fromVersionString` :)\r\n\r\n\r\n\r\n"
      ],
      "kafka-centralize-configuration-values": [
        "sounds good"
      ]
    },
    "profile": {
      "location": "Taiwan",
      "blog": "www.chia7712.tw",
      "site_admin": false,
      "followers": 202,
      "following": 5
    }
  },
  "Rich-Harris": {
    "repos": [
      "sveltejs/svelte"
    ],
    "entries": [
      {
        "slug": "svelte-analyze-transitive-dependencies",
        "title": "analyze transitive dependencies"
      },
      {
        "slug": "svelte-api-flexibility-balance",
        "title": "API flexibility balance"
      },
      {
        "slug": "svelte-async-cleanup-safety",
        "title": "async cleanup safety"
      },
      {
        "slug": "svelte-avoid-expensive-operations",
        "title": "avoid expensive operations"
      },
      {
        "slug": "svelte-avoid-unclear-abbreviations",
        "title": "avoid unclear abbreviations"
      },
      {
        "slug": "svelte-choose-descriptive-names",
        "title": "Choose descriptive names"
      },
      {
        "slug": "svelte-complete-code-examples",
        "title": "Complete code examples"
      },
      {
        "slug": "svelte-defensive-error-handling",
        "title": "defensive error handling"
      },
      {
        "slug": "svelte-document-complex-apis",
        "title": "Document complex APIs"
      },
      {
        "slug": "svelte-document-configuration-hierarchies",
        "title": "document configuration hierarchies"
      },
      {
        "slug": "svelte-document-non-obvious-behavior",
        "title": "Document non-obvious behavior"
      },
      {
        "slug": "svelte-measure-performance-impact",
        "title": "Measure performance impact"
      },
      {
        "slug": "svelte-prefer-simple-code-patterns",
        "title": "prefer simple code patterns"
      },
      {
        "slug": "svelte-prefer-testing-libraries",
        "title": "prefer testing libraries"
      },
      {
        "slug": "svelte-preserve-user-input-focus",
        "title": "preserve user input focus"
      },
      {
        "slug": "svelte-realistic-documentation-examples",
        "title": "realistic documentation examples"
      },
      {
        "slug": "svelte-runtime-html-escaping",
        "title": "Runtime HTML escaping"
      },
      {
        "slug": "svelte-simplify-and-deduplicate-code",
        "title": "Simplify and deduplicate code"
      },
      {
        "slug": "svelte-ssr-documentation-clarity",
        "title": "SSR documentation clarity"
      },
      {
        "slug": "svelte-state-boundary-management",
        "title": "state boundary management"
      },
      {
        "slug": "svelte-use-modern-null-safe-operators",
        "title": "use modern null-safe operators"
      }
    ],
    "comments": {
      "svelte-api-flexibility-balance": [
        "FWIW we don't need to expose function wrappers to get values from different modules, only to set them. we can just `export let derived_writes`",
        "Someone might have an existing classname in their HTML template, in which case giving them a string would make it awkward to combine stuff. Should we return an object instead of a string, so that they have more flexibility?",
        "it only really matters for new projects, I think, since we can't retroactively add `%htmlAttributes%` anyway. I think we just replace `lang=\"en\"` with `%htmlAttributes%` in the template project's `app.html`, and add this to the root layout:\r\n\r\n```svelte\r\n<svelte:html lang=\"en\" />\r\n```"
      ],
      "svelte-document-configuration-hierarchies": [
        "good catch, thanks",
        "```suggestion\r\n- Use a [`target`](https://www.typescriptlang.org/tsconfig/#target) of at least `ES2022`, or a `target` of at least `ES2015` alongside [`useDefineForClassFields`](https://www.typescriptlang.org/tsconfig/#useDefineForClassFields). This ensures that rune declarations on class fields are not messed with, which would break the Svelte compiler\r\n```",
        "```suggestion\r\nThere are several ways to set this option:\r\n\r\n- Globally, via the `compilerOptions.css` option in your `svelte.config.js` or the options passed to `svelte.compile`\r\n- Dynamically, using [`dynamicCompileOptions`](https://github.com/sveltejs/vite-plugin-svelte/blob/main/docs/config.md#dynamiccompileoptions) in `vite-plugin-svelte`\r\n- Per-component, with `<svelte:options css=\"injected\" />` (this will override options set any other way)\r\n```\r\n\r\n"
      ],
      "svelte-document-non-obvious-behavior": [
        "'Deeply tracks' isn't 100% clear — maybe something like this?\r\n\r\n```suggestion\r\n * Logs the arguments whenever they, or the properties they contain, change. Example:\r\n```",
        "not sure why we would need to specify 'attribute may be omitted when false' — for one thing non-boolean attributes are never `false`, they either exist or they don't\r\n\r\n```suggestion\r\n\t * Quoted/string values are represented by an array, even if they contain a single expression like `\"{x}\"`\r\n```"
      ],
      "svelte-prefer-simple-code-patterns": [
        "we can use `&&` here because trailing undefined arguments to `b.call` are ignored\r\n\r\n```suggestion\r\n\t\t\tadditional && b.object(Object.entries(additional).map(([k, v]) => b.init(k, b.literal(v))))\r\n```",
        "using an implicit return will keep the statement on a single line in many cases \r\n\r\n```suggestion\r\n\t\t\tb.arrow([], call_expression),\r\n```",
        "in my experience it's vanishingly rare that this kind of indirection makes sense compared to writing more obvious code. if we do this...\r\n\r\n```suggestion\r\n\t/**\r\n\t * @template {Node} T\r\n\t * @param {T} child\r\n\t */\r\n\tfunction insert(child) {\r\n\t\tif (last_current_element) {\r\n\t\t\tlast_current_element.children ??= [];\r\n\t\t\tlast_current_element.children.push(child);\r\n\t\t} else {\r\n\t\t\telements.push(/** @type {Element} */ (child));\r\n\t\t}\r\n\r\n\t\treturn child;\r\n\t}\r\n\r\n\tfor (let instruction of items) {\r\n\t\tswitch (instruction.kind) {\r\n\t\t\tcase 'push_element':\r\n\t\t\t\telements_stack.push(/** @type {Element} */ (last_current_element));\r\n\t\t\t\tbreak;\r\n\r\n\t\t\tcase 'pop_element':\r\n\t\t\t\telements_stack.pop();\r\n\t\t\t\tlast_current_element = elements_stack.at(-1);\r\n\t\t\t\tbreak;\r\n\r\n\t\t\tcase 'create_element':\r\n\t\t\t\tlast_current_element = insert({\r\n\t\t\t\t\tkind: 'element',\r\n\t\t\t\t\telement: /** @type {string[]} */ (instruction.args)[0]\r\n\t\t\t\t});\r\n\t\t\t\tbreak;\r\n\r\n\t\t\tcase 'create_text':\r\n\t\t\t\tinsert({\r\n\t\t\t\t\tkind: 'text',\r\n\t\t\t\t\tvalue: /** @type {string[]} */ (instruction.args)[0]\r\n\t\t\t\t});\r\n\t\t\t\tbreak;\r\n\r\n\t\t\tcase 'create_anchor':\r\n\t\t\t\tinsert({\r\n\t\t\t\t\tkind: 'anchor',\r\n\t\t\t\t\tdata: instruction.args?.[0]\r\n\t\t\t\t});\r\n\t\t\t\tbreak;\r\n\r\n\t\t\tcase 'set_prop': {\r\n\t\t\t\tconst el = /** @type {Element} */ (last_current_element);\r\n\t\t\t\tconst [prop, value] = /** @type {string[]} */ (instruction.args);\r\n\t\t\t\tel.props ??= {};\r\n\t\t\t\tel.props[prop] = value;\r\n\t\t\t\tbreak;\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n```",
        "unrelated to the PR but I hadn't noticed this code before — any reason we're using an enum here rather than a boolean? Feels like it could be this instead, with a corresponding change on line 53:\r\n\r\n```js\r\nconst dirty = signal.w_version > current_reaction.w_version || current_reaction.w_version === 0;\r\n```",
        "What's the reason to use `0 | 1` instead of booleans here? If the signature was this...\r\n\r\n```suggestion\r\n\tconst set_branch = (/** @type {(anchor: Node) => void} */ fn, flag = false) => {\r\n\t\thas_branch = true;\r\n\t\tupdate_branch(flag, fn);\r\n\t};\r\n```\r\n\r\n...then in the common case where there's no alternate, we could generate this code — a small win but it feels like it better reflects the binary nature of `if` blocks\r\n\r\n```diff\r\n$.if(node, ($$branch) => {\r\n-\tif (Math.random() < 0.5) $$branch(0, consequent);\r\n+\tif (Math.random() < 0.5) $$branch(consequent);\r\n});\r\n```",
        "storing this feels like overkill when we only need to do `status === MAYBE_DIRTY` below (no need for bitwise shenanigans)\r\n\r\n```suggestion\r\n```"
      ],
      "svelte-use-modern-null-safe-operators": [
        "```suggestion\r\n\t\tparent: parent_derived ?? active_effect\r\n```",
        "```suggestion\r\n\t\t\t\t(to_animate ??= new Set()).add(item);\r\n```",
        "```suggestion\r\n\t\t\t\t(to_animate ??= new Set()).delete(item);\r\n```",
        "```suggestion\r\n\t\t\t\t(seen ??= new Set()).add(current);\r\n```",
        "I think this whole block could become a one-liner?\r\n\r\n```js\r\n(dependency.reactions ??= new Set()).add(reaction);\r\n```",
        "this makes me a bit nervous — if this is now being called before `a` exists, then will `a` still get created (but never aborted) in the microtask?"
      ],
      "svelte-state-boundary-management": [
        "this wording feels a bit strong. if we're so adamant that you shouldn't update state inside an effect, why do we allow it? it also feels quite vague — what is the nightmare exactly? it would be good if we could articulate _why_ we warn against this"
      ],
      "svelte-simplify-and-deduplicate-code": [
        "no need to repeat this, we're extending `Value` which has this property\r\n\r\n```suggestion\r\n```"
      ],
      "svelte-choose-descriptive-names": [
        "please, anything but `clazz` 😆 \r\n\r\n```suggestion\r\nexport function to_class(value, hash, classes) {\r\n\tlet class_name = value == null ? '' : '' + value;\r\n```",
        "wondering if this is the right name for this function. `StartStopNotifier` is slightly esoteric. what if it was `this.#subscribe = createSubscriber(fn)` instead? would that make sense?",
        "this signature (`node_or_nodes`) feels like a code smell. can we change it to this?\r\n\r\n```suggestion\r\n * @param {AST.Attribute} attribute\r\n * @param {Scope} scope\r\n */\r\nexport function is_inlinable_attribute(attribute, scope) {\r\n```\r\n\r\n(note that I got rid of the `import('...')` as well — we should use `/** @import ... */` instead)",
        "Really not a fan of a function returning `false | { need_to_escape: boolean }` — it's a messy and confusing type. If a function name begins with `is_` then it ought to return a boolean; if it needs to return more granular information than that then something like `analyze_expression` would probably be better",
        "I tend to think it's better to avoid negatives in variable names (even though a few other `skip_` things have slipped in) since you have to mentally invert stuff to understand what's happening. At first I thought replacing `if (!skip_derived_source ...)` with `if (track_owner ...)` might be clearer...\r\n\r\n```suggestion\r\nexport function source(v, track_owner = true) {\r\n```\r\n\r\n...but then it occurred to me that we could even do this...\r\n\r\n```suggestion\r\nexport function source(v, owner = current_reaction) {\r\n```\r\n\r\n...and then just do this below:\r\n\r\n```js\r\nif (owner !== null && (owner.f & DERIVED) !== 0) {...}\r\n```\r\n\r\nOne less thing to check. Thoughts?",
        "made that change"
      ],
      "svelte-ssr-documentation-clarity": [
        "```suggestion\r\nIf the `{@html ...}` value changes between the server and the client, it will not be repaired during hydration, i.e. the server value will be kept. That's because change detection during hydration is expensive and usually unnecessary.\r\n```",
        "```suggestion\r\nCertain methods such as `mount` cannot be invoked while running in a server context. Avoid calling them eagerly, i.e. not during render.\r\n```"
      ],
      "svelte-defensive-error-handling": [
        "It won't work — the update is still in progress and everything gets torn. The `svelte_boundary_reset_onerror` message has an example of fixing it by waiting for a `tick()` before calling `reset()`"
      ],
      "svelte-preserve-user-input-focus": [
        "in a case like this, if you type 'potato' and then `find('p')` resolves, Svelte will replace the input contents with 'p' because that's the `query` that goes with the resolved promise. Most of the time that's what you want, but it's absolutely _not_ what you want if the input itself was the source of the change — instead you want the UI to 'catch up' to a focused input\r\n\r\n```svelte\r\n<input bind:value={query}>\r\n<p>{await find(query)}</p>\r\n```\r\n\r\nadded an explanatory comment"
      ],
      "svelte-avoid-unclear-abbreviations": [
        "I really dislike these sorts of abbreviations, they feel clunky. The convention in mathematical notation is to use the letter `k` for this sort of constant\r\n\r\n```suggestion\r\n * @param {number} k\r\n */\r\nexport function multiplier(initial, k) {\r\n```"
      ],
      "svelte-async-cleanup-safety": [
        "I think we should get rid of `simple_set`, it will be a bug magnet. Case in point: `onchange` fires [here](https://svelte.dev/playground/hello-world?version=pr-15069#H4sIAAAAAAAAE22Qy2rDMBBFf2VQu7AhJGTrOIbu-gmBugtFntQGRRKecZJi9O-V5NemC4G4d-6ZxyiMvKMoxCdqbeFpe91Ahk3H2ORiJ26dRhLF1yj418W6KAR9Tn04t6cHao7aVRL-pytrGA0HjChJ9Z3jqjY1a2RQrTQ_2MAZ3oklY3aTmjA_RT8-pSURXGBMAr89pB5wqz7uFqdmayZYAVkO52ozAmXtwv2Ap8Xw08endukbJqVQotj2AbISuO1ov_Y-zoAU91M0BeEVXINPuGQRWR62ZU3pqmWKjFo7hCtfEaZtCxhnz5cHV4WLMb5YFHFY_-3_ALWpfmCjAQAA) when it shouldn't...\r\n\r\n```js\r\nclass X {\r\n  #value = $state(1, {\r\n    onchange: () => {\r\n      changed = true;\r\n    }\r\n  });\r\n\r\n  constructor() {\r\n    this.#value = 1;\r\n  }\r\n}\r\n```\r\n\r\n...because we're missing the `!source.equals(value)` check. We could add that in, but we're likely to run into similar cases if we (for example) add forking and so on. The very slight optimisation probably isn't worth the more complicated code.\r\n\r\nOne wrinkle: while no tests fail if we replace `$.simple_set` with `$.set` (other than a harmless snapshot test), it _is_ technically a breaking change, because it would mean that private state behaves the same as public state with regards to effects:\r\n\r\n```js\r\nclass X {\r\n  value = $state(0);\r\n  #value = $state(0);\r\n\r\n  constructor() {\r\n    this.value = 1;\r\n    this.#value = 1;\r\n  }\r\n\r\n  get private_value() {\r\n    return this.#value;\r\n  }\r\n}\r\n\r\n$effect(() => {\r\n  // this creates an infinite loop\r\n  const x = new X();\r\n  x.value;\r\n});\r\n\r\n$effect(() => {\r\n  // this does not (but probably should)\r\n  const x = new X();\r\n  x.private_value;\r\n});\r\n```\r\n\r\nI think that's _probably_ okay, since a) you really shouldn't be reading a signal that you just created inside an effect (in fact perhaps we should have a dedicated error for that, rather than just the infinite loop detection, [like we do with deriveds](https://svelte.dev/playground/hello-world?version=5.23.2#H4sIAAAAAAAAE22OQQqDMBBFrxKGLiIW7dpGobveoXahZgqBNIZktBXJ3WtUKJQu5_03M38G0zwRCrii1j179U5LxlEqQpnAER5Ko4fiNgNNNnoRLHzfulib-RE1RdY2Hv_xrjeEhpYzIHznlKWqNjVpJNb1gyFWsoNEp0aUWTtxnrCyYnNUfiRPDSE_JectW3ma7pNDGpzZ5BWF6In8-9EIW81rHkRuq6UZ4ZugIDdguIcP1dCucgsBAAA=)), and b) I'd be very surprised if anyone was relying on that behaviour. But it gives me pause.\r\n\r\nIf we wanted to be super cautious about it then we could pass a bitmask as the third option instead of a boolean, where the `1` bit means 'proxy this please' and the `2` bit means 'don't mark reactions'.",
        "A reversal of my previous position: neither effect in the code above should run in a loop https://github.com/sveltejs/svelte/pull/15553\r\n\r\nIf we merge that, we can safely get rid of `simple_set` IIUC",
        "There was some drift between this and `main` after #15553, so I merged everything and removed `simple_set` in the process on the assumption that #15300 (and #15564) will happen ",
        "the whole tuple thing is a bit confusing for starters but that'll need to be fixed on main",
        "https://github.com/sveltejs/svelte/pull/16287",
        "honestly... i have no idea what the `async_mode_flag` clause is doing here. maybe @dummdidumm knows since it was apparently introduced with #16197.\r\n\r\ni deleted it to see if anything would break and it didn't, so unless we can articulate why it needs to be there my inclination is to leave it deleted"
      ],
      "svelte-prefer-testing-libraries": [
        "```suggestion\r\nWhile the process is very straightforward, it is also low level and somewhat brittle, as the precise structure of your component may change frequently. Tools like [@testing-library/svelte](https://testing-library.com/docs/svelte-testing-library/intro/) can help streamline your tests.\r\n```",
        "how does it know when to unmount it?",
        "```suggestion\r\nWhen writing component tests that involve two-way bindings, context or snippet props, it's best to create a wrapper component for your specific test and interact with that. `@testing-library/svelte` contains some [examples](https://testing-library.com/docs/svelte-testing-library/example).\r\n```"
      ],
      "svelte-document-complex-apis": [
        "I've added a bunch of JSDoc comments around the less self-explanatory stuff. No doubt there's more that could be added but I'll mark this resolved for now",
        "unrelated to this PR but I think this comment is out of date? write versions are used for everything, not just unowned deriveds.\r\n\r\nseparately, would be good to have a comment explaining what `read_version` is"
      ],
      "svelte-complete-code-examples": [
        "it doesn't make sense for `count` to not be state (this is more apparent now that we're actually showing the code). by extension, we'll need to change the filename, both here and in the import directly above\r\n\r\n```suggestion\r\n\tlet count = $state(initial);\r\n```",
        "if we have a `<script>` for the imports, we also need to declare `visible` since it's no longer implicit\r\n\r\n```suggestion\r\n  import { fade, fly } from 'svelte/transition';\r\n  \r\n  let visible = $state(false);\r\n```",
        "we also need some way to change the value of `visible`\r\n\r\n```suggestion\r\n<label>\r\n  <input type=\"checkbox\" bind:checked={visible}>\r\n  visible\r\n</label>\r\n\r\n{#if visible}\r\n```"
      ],
      "svelte-realistic-documentation-examples": [
        "even though localStorage was one of the primary motivators for this work I think we need to omit it from these docs, because it's just too hard to illustrate concisely without cutting corners:\r\n\r\n- needs to accept initial data _without_ blowing up in SSR (i.e. needs `typeof localStorage` checks etc)\r\n- needs to be composable\r\n- needs to react to `storage` events, which basically means using `createSubscriber` which is a whole thing\r\n\r\nI think a simpler and clearer example would be validation. Will push a change",
        "You could imagine someone doing something cute like\r\n\r\n```js\r\nexport const messages = [\r\n  \"reticulating splines...\",\r\n  \"generating witty dialog...\",\r\n  \"swapping time and space...\",\r\n  \"640K ought to be enough for anybody\",\r\n  \"checking the gravitational constant in your locale...\",\r\n  \"keep calm and npm install\",\r\n  \"counting backwards from Infinity\",\r\n  \"I'm sorry Dave, I can't do that.\",\r\n  \"adjusting flux capacitor...\",\r\n  \"constructing additional pylons...\",\r\n  \"rm -rf /\",\r\n];\r\n\r\nconst loading_message = $derived(messages[$effect.pending() % messages.length]);\r\n```",
        "yeah, that was my thought process too — figured this would get the point across most concisely",
        "thought about something like [this](https://svelte.dev/playground/hello-world?version=5.36.1#H4sIAAAAAAAAE3VSwY6bMBD9lamzWpltEtg9sgRpL1UvlXrorenBC8PGWmMjewiJvPx7DYSkFe0BbL95780bsGda1MhS9hWVMtAZq0rgWErCMmJrVkmFjqU_PaNzM_AGIOAX1UvTbN0RFQ3Yq3D4L7wwmlBTsGGZK6xsKN_rPSkk6A6C8IgWdnDnKOx5Ej3PRTIk1KIyPFWrC5JGgxW6NDWPwI8lskit1aCxg-_W1NIh51UEu3wm7Mkh_ZA1mpY4HysV_ybosJ2ttmS-yBOW_CmK1vBXCR7gKUnmHNRPmz68wnKHVYXFbHpp98d8jxN51PyPfp14Km4b1KXUb4Fzfz9FqcWJj6z1khTdOmTx7UPr7LUNzhqMLpQs3nd-6nnN9jmE6_O2KcM5iyfyJGzysVcKflz7LG7yEV70DpQFdqEHwafNBrLGmjeLzsFRqBZDiuWU8fTPI_j4gKTPg_6iyWGzGa38Sla35HlgDTP71Tuer3A_fUy_QlEc4MVaceaPSdTPNyDk96ITkq7XZ046qOJBNbnGwTXsfCyrsISbTHgilpJtsf8VTkKqTuqSpZVQDvvf-aQbl0wDAAA=) and discovered that a) it doesn't decrement when it should and b) there's some weird NaN action happening 🤔 \r\n\r\ninvestigating",
        "```suggestion\r\nOptionally, they can return a function that is called before the attachment re-runs, or when the element is later removed from the DOM.\r\n```",
        "```suggestion\r\nOptionally, they can return a function that is called before the attachment re-runs, or after the element is later removed from the DOM.\r\n```",
        "I think this is probably a niche enough thing that we could move it to a separate 'Controlling when attachments re-run' section further down — thoughts?",
        "```suggestion\r\nTo fix this, either silence the warning with a [`svelte-ignore`](basic-markup#Comments) comment, or ensure that the value stays the same between server and client. If you really need the value to change on hydration, you can force an update like this:\r\n```",
        "```suggestion\r\nTo fix this, either silence the warning with a [`svelte-ignore`](basic-markup#Comments) comment, or ensure that the value stays the same between server and client. If you really need the value to change on hydration, you can force an update like this:\r\n```",
        "I think it probably makes sense to talk more abstractly about form resets, since in the provided example we're not using the `form.reset()` method (at least, not explicitly)\r\n\r\n```suggestion\r\nIf an `<input>` has a `defaultValue` and is part of a form, it will revert to that value instead of the empty string when the form is reset. Note that for the initial render the value of the binding takes precedence unless it is `null` or `undefined`.\r\n```",
        "MDN [has a note about default values](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input/reset) which I think it worth reiterating here\r\n\r\n````suggestion\r\n```\r\n\r\n> [!NOTE]\r\n> Use reset buttons sparingly, and ensure that users won't accidentally click them while trying to submit the form.\r\n````",
        "'instead of the empty string' doesn't really make sense here, because the default behaviour if no option has the `selected` attribute is to select the first option. I think we can just leave that implied\r\n\r\n```suggestion\r\nYou can give the `<select>` a default value by adding a `selected` attribute to the`<option>` (or options, in the case of `<select multiple>`) that should be initially selected. If the `<select>` is part of a form, it will revert to that selection when the form is reset. Note that for the initial render the value of the binding takes precedence if it's not `undefined`.\r\n```",
        "We're in the compiler warnings section of the (hypothetical future) docs, I don't think we need to say that Svelte will issue a compiler warning — if someone is here it's most likely because they just experienced one\r\n\r\n```suggestion\r\nHTML restricts where certain elements can appear. For example, a `<p>` element cannot contain another `<p>`, or a `<div>`, or any other block-level element. The browser will 'repair' the HTML in a way that breaks Svelte's assumptions about the structure of your components. Some more examples:\r\n\r\n- `<option><div>...</div></option>` will result in `<option>...</option>` (everything except text is removed)\r\n- `<table><tr><td>...</td></tr></table>` will result in `<table><tbody><tr><td>...</td></tr></tbody></table>` (a `<tbody>` is auto-inserted)\r\n\r\nThis code will work when the component is rendered on the client, but if you use server rendering it will cause hydration to fail.\r\n```",
        "what about\r\n\r\n> This code will work when the component is rendered on the client (which is why this is a warning rather than an error), but if you use server rendering it will cause hydration to fail."
      ],
      "svelte-avoid-expensive-operations": [
        "Walking is expensive, we want to do it as infrequently as possible. Luckily we don't need this function anyway, since the necessary information exists on `value.metadata.expression.dependencies`.\r\n\r\nAs a side-note, the check in the `Identifier` visitor is insufficient. If you want to know if an identifier is a _reference_ then you can use [`is-reference`](https://github.com/Rich-Harris/is-reference/)\r\n\r\n```suggestion\r\n```",
        "...although `value.metadata.expression.dependencies` excludes globals, which means that things like `{location}` get inlined incorrectly. We may need to treat globals as bindings",
        "Still trying to wrap my head round this, it's very cryptic. If I understand correctly, we're saying that since branch effects have no dependencies and can therefore never be dirty, we can (ab)use the `MAYBE_DIRTY` flag on branches to stop traversing upwards to the root. Is that right?\r\n\r\nIf so, it feels suboptimal — it means we traverse upwards unnecessarily until we hit a branch, and it means we call `check_dirtiness` for branches unnecessarily. What if we had an `EFFECT_HAS_DIRTY_CHILDREN` flag or similar instead?",
        "> That would just mean having more code in the codebase\r\n\r\nWould it? It seems we could probably _simplify_ `process_effects` if 'this effect is/could be dirty' and 'this effect has dirty children' were separate concepts, instead of needing to check `is_branch` at numerous points inside that function. And `schedule_effect` would get simpler, especially since we wouldn't need a chunky comment explaining what's going on.\r\n\r\nThe performance benefits might be negligible but the real win would be more self-explanatory code, I think",
        "we could do this instead which would presumably be a tiny bit faster, but less readable\r\n\r\n```suggestion\r\n\t\tvar is_skippable_branch = (flags & (CLEAN | BRANCH_EFFECT)) === (CLEAN | BRANCH_EFFECT);\r\n```"
      ],
      "svelte-analyze-transitive-dependencies": [
        "```suggestion\r\nfix: treat transitive dependencies of each blocks as mutable in legacy mode if item is mutated\r\n```",
        "```suggestion\r\nfix: treat pure call expressions as potentially reactive if they reference local bindings\r\n```"
      ],
      "svelte-measure-performance-impact": [
        "it was probably me (I used to be quite fond of the `~array.indexOf` trick), and I probably wrote it before `includes` was implemented everywhere",
        "For svelte.dev it's 147, for svelte.dev/playground it's 260, for svelte.dev/tutorial it's 302. If you interact with the search box it will increase by a few hundred. Obviously if you had a very large list then it would be more, but the upshot is that we're typically talking about hundreds of operations.\r\n\r\nI don't know if this is a valid microbenchmark, but on this M1 MBP I can run the check 10,000 times (i.e. `run(1e4)` before it takes more than a single millisecond:\r\n\r\n```js\r\nfunction run(i = 1e6) {\r\n  console.time('test');\r\n  let div = document.createElement('div');\r\n  while (i--) {\r\n    div.nodeName.includes('-');\r\n    div.namespaceURI === 'http://www.w3.org/1999/xhtml';\r\n  }\r\n  console.timeEnd('test');\r\n}\r\n```\r\n\r\nSo... it's measurable, but seems pretty negligible. I don't know, what do you think?",
        "Makes no real difference \r\n\r\n```js\r\nfunction run(i = 1e6) {\r\n  let div = document.createElement('div');\r\n  document.body.append(div);\r\n  console.time('test');\r\n  while (i--) {\r\n    div.nodeName.includes('-');\r\n    div.namespaceURI === 'http://www.w3.org/1999/xhtml';\r\n  }\r\n  console.timeEnd('test');\r\n}\r\n```",
        "Try it! The results are basically the same. Though the code above isn't correct, it should be this:\r\n\r\n```js\r\nfunction run(n = 1e6) {\r\n  const els = Array(n);\r\n  let i = n;\r\n  while (i--) {\r\n    let div = document.createElement('div');\r\n    document.body.append(div);\r\n    els[i] = div;\r\n  }\r\n  console.time('test');\r\n  i = n;\r\n  while (i--) {\r\n    const div = els[i];\r\n    div.nodeName.includes('-');\r\n    div.namespaceURI === 'http://www.w3.org/1999/xhtml';\r\n  }\r\n  console.timeEnd('test');\r\n}\r\n```",
        "I think it was you who taught me that we should avoid reassigning parameters, since mutating the `arguments` object carries overhead. Should it be something like this instead?\r\n\r\n```suggestion\r\n\tvar str = value == null ? '' : typeof value === 'object' ? value + '' : value;\r\n```",
        "probably. there's still a few places where we need to expose these properties to the world (via the `STATE_SYMBOL` property) so we can't totally get rid of it, but will make that change"
      ],
      "svelte-runtime-html-escaping": [
        "The escaping [needs to happen at runtime](https://svelte.dev/playground/hello-world?version=pr-14269#H4sIAAAAAAAAE23NMQ7CMAwF0KtEXrogupc0EucgDKU1IlLqRIlbgaLcnRgY2ayn7_8L0LQiDBBCzHCAu_OYYbgU4FcUF2j-S51jPOYdPYvdpoz_fA7ESNxqQOc5ucjqQ08eLaxh2TxaMJYsN86sZFqNqtOPZLqTJd1_vyRCenG7KRKpupfbUpuQMhg4bViv9Q3ti_IKxAAAAA==), not here"
      ]
    },
    "profile": {
      "location": "NYC",
      "blog": "",
      "site_admin": false,
      "followers": 19879,
      "following": 0
    }
  },
  "pauldix": {
    "repos": [
      "influxdata/influxdb"
    ],
    "entries": [
      {
        "slug": "influxdb-avoid-flaky-test-patterns",
        "title": "Avoid flaky test patterns"
      },
      {
        "slug": "influxdb-choose-appropriate-lock-primitives",
        "title": "Choose appropriate lock primitives"
      },
      {
        "slug": "influxdb-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "influxdb-clear-configuration-parameters",
        "title": "Clear configuration parameters"
      },
      {
        "slug": "influxdb-descriptive-semantic-naming",
        "title": "Descriptive semantic naming"
      },
      {
        "slug": "influxdb-document-complete-data-flows",
        "title": "Document complete data flows"
      },
      {
        "slug": "influxdb-follow-api-conventions",
        "title": "Follow API conventions"
      },
      {
        "slug": "influxdb-handle-errors-by-criticality",
        "title": "Handle errors by criticality"
      },
      {
        "slug": "influxdb-include-explanatory-examples",
        "title": "Include explanatory examples"
      },
      {
        "slug": "influxdb-manage-complete-cache-lifecycle",
        "title": "Manage complete cache lifecycle"
      },
      {
        "slug": "influxdb-minimize-critical-path-allocations",
        "title": "Minimize critical path allocations"
      },
      {
        "slug": "influxdb-performance-conscious-metrics-implementation",
        "title": "Performance-conscious metrics implementation"
      },
      {
        "slug": "influxdb-prefer-explicit-nullability",
        "title": "Prefer explicit nullability"
      },
      {
        "slug": "influxdb-promote-code-clarity",
        "title": "Promote code clarity"
      },
      {
        "slug": "influxdb-stable-schema-identifiers",
        "title": "Stable schema identifiers"
      },
      {
        "slug": "influxdb-vet-security-critical-dependencies",
        "title": "Vet security-critical dependencies"
      }
    ],
    "comments": {
      "influxdb-promote-code-clarity": [
        "I think this would be clearer if the `cleanup_after_snapshot` method took the separate arguments (i.e. not an Option of a tuple). That way you can put the check up here. Meaning, we show that what we're doing here is flushing the buffer. And if, the return from that is `Some(...)` then it means we have a snapshot to cleanup after.\r\n\r\nSo we call the `cleanup_after_snapshot` function with this signature: `self: Arc<Self>, snapshot_finished_reciever: oneshot::Receiver<SnapshotDetails>, snapshot_info: SnapshotInfo, snapshot_permit: OwnedSemaphorePermit`.\r\n\r\nThe separation and the argument names will make it a little more clear what's going on here.",
        "Better to make this a function on `Catalog` rather than going into its internals to get it.",
        "Might be good to pull this out of the query_executor and into its own file. Create a structure for different system tables to be defined. I imagine we'll have a number of these over time."
      ],
      "influxdb-minimize-critical-path-allocations": [
        "Has this been tested with high query concurrency to verify that this isn't a performance regression?",
        "yeah, returning the series key should be as cheap as possible. Given that every table now has one, it should just be a reference that gets returned (or an Arc'd thing?)."
      ],
      "influxdb-clear-configuration-parameters": [
        "Can we rename to `query-file-limit`? I think it's better to be more descriptive about what this file limit is about.",
        "should be renamed to `snapshotted-wal-files-to-keep`. It's not the actual total number of WAL files, it's the number that have been snapshotted. So if you have 200 files that haven't been snapshotted and this setting is 50, you'd have 250 total files at that point.",
        "Not sure if we want to change this here, but I've logged #25735 to be able to create tables without specifying any fields.",
        "This could also use help that indicates what fields should look like if provided, and what the valid types are.",
        "Looking at the help for the delete commands it seems it's a bit off:\r\n\r\n```\r\ninfluxdb3 delete table -h\r\nCreate a new table in a database\r\n\r\nUsage: influxdb3 delete table [OPTIONS] --database <DATABASE_NAME> <TABLE_NAME>\r\n\r\nArguments:\r\n  <TABLE_NAME>  \r\n\r\nOptions:\r\n  -H, --host <HOST_URL>           The host URL of the running InfluxDB 3 Core server [env: INFLUXDB3_HOST_URL=] [default: http://127.0.0.1:8181]\r\n  -d, --database <DATABASE_NAME>  The database name to run the query against [env: INFLUXDB3_DATABASE_NAME=]\r\n      --token <AUTH_TOKEN>        The token for authentication with the InfluxDB 3 Core server [env: INFLUXDB3_AUTH_TOKEN=]\r\n  -h, --help                      Print help information\r\n```\r\n\r\nI'm guessing you'll need to create different config blocks for each of the actions `TableDeleteConfig`, `TableCreateConfig`, etc. I saw this for the other nouns as well.",
        "Would it be better/more user friendly to have this value in MB? "
      ],
      "influxdb-follow-api-conventions": [
        "Can we re-use whatever it is that parses the precision argument from the HTTP API? That way they're always the exact same.",
        "I don't think this should be part of the public API. The public API for the Catalog should be to add the table (through a CatalogOp) and as part of that API it should add it to the map."
      ],
      "influxdb-performance-conscious-metrics-implementation": [
        "As a follow up it would be great to add a `influxdb_write_lines_rejected_total` counter.",
        "I don't think we want to spawn on every write call. We should be able to just call add_write_metrics on the store without this.",
        "Seems like we can have this be pretty big given they're small messages. 10k?"
      ],
      "influxdb-document-complete-data-flows": [
        "You should also mention that the write request that came in had a oneshot channel created that gets called back on after the flush and the placement of that data into the queryable buffer, with then returns a success to the client.",
        "This isn't strictly true. What the WAL periods are looking for is that data written into the oldest WAL files have time stamps that fall into chunks that are no longer receiving writes. So if we have the default 10m gen1 chunks and we're always writing data with a time of \"now\" then we would only snapshot after we have the 10m chunk time go cold. If we have lagged collection, by say 1m, we won't snapshot until after that 10m wall clock time has passed + 1m, but we would only snapshot the wal files from before that time. So we'd likely leave behind 60 wal files, which is by design."
      ],
      "influxdb-handle-errors-by-criticality": [
        "Good call to change to debug 👍 ",
        "Actually, changed my mind. I do want it to crash, because the next thing it does is delete WAL files. If for some reason the snapshot details we get back aren't expected, we want to crash rather than deleting WAL files, which may be unrecoverable. ",
        "At the moment I'm not sure what we can do other than keep trying. If we're unable to persist, it means the object store is unreachable, so writes will then return errors to clients because we'll be able to persist wal files to object storage. So clients attempting to write data will start receiving errors and errors will show up in the logs. It's the equivalent of losing the disk, so not much we can do about it. Unless we decide we want to exit the program entirely.",
        "any reason this might error, can we catch this and log it?"
      ],
      "influxdb-choose-optimal-data-structures": [
        "Given these serializations have the key as a string, I don't think there's any value in having this be a map. We'd be better off just having a Vec because we want the in-memory structure to be keyed off a u64, not a string. I'd apply this down the line everywhere we're using maps in serialization.",
        "We actually want this serialized as an int, not a string. I believe you're doing this because of the maps elsewhere?",
        "Yeah, so I think we should have one data structure for serialization, which is a vec of the objects, rather than a map. That way we can have all the IDs be ints. Then when you read the serialized data into an in-memory structure for fast lookups, you put it into a map, which should be by int. We should never being lookups against the serialized structure, I think",
        "It's not clear to me why this is being used here. What's the insertion order being preserved here?",
        "yeah, that's why I'm doing it. Moving to using hashbrown sounds good."
      ],
      "influxdb-stable-schema-identifiers": [
        "Ah yes, that's a good catch. They should have a well defined ID that remains static regardless of what schema changes later happen to the table.",
        "We could have our own `TableSchema` which includes the `SchemaRef` and also includes a map of column name to id. Then the `TableSchema` is what we serialize in the catalog. We want to have the ids and not the string identifiers in the WALContents because that's much cheaper to serialize and deserialize. Also cheaper to index when inserting the data into the `WriteBuffer`.\r\n\r\nSo we don't need to update arrow, we just need a wrapper around the arrow struct where we can add our own stuff.",
        "A thought I had is that the `DbSchema` should contain the mapping of the table name to id and back. That way when a write comes in, you grab a lock on the catalog once to get the schema. Then from that point on, assuming you're not creating any new schema, you won't need to grab a lock on the catalog to validate any of the lines.\r\n\r\nYou only end up having to lock again and replace the schema you're working with if they insert a new table.\r\n\r\nIt previously worked like this to minimize grabbing locks line by line.",
        "This feels like it's going to be problematic. I'd prefer to have the name and ID be separate fields. I think the better approach would be to have the key be the int table id and then have the name be a member on the value, which could have the table name and the data.",
        "I was thinking that you want the TableId as the key in the map. Then the value will have the name (as a member). We want the map key to be an int anyway because it's way faster to lookup.",
        "So the `TableChunks` struct would have table name as part of it. Probably worth renaming TableChunks to be something more appropriate.",
        "We should move to IDs everywhere, but anything that shows up in the WAL should also have the name in it somewhere. That is, an individual WAL file should be self-describing without having to pair it with a catalog.\r\n\r\nThe other thing about IDs everywhere is that for user facing things, it should be name. For our internal systems like lookup tables, etc, it should be ids.",
        "I guess we could drop the name from the WAL part if it proves too difficult. I'm just thinking about future recoverability & debugability. Easier if the name is there.",
        "I would do it the other way. All the data stored in a map that's keyed off ID. Then have a lookup table of ID to name. Although I'm not sure we want to do that. We don't need it assuming that we have a Catalog (which in all current cases we do). So it's extra payload that increases size and decode time for downstream consumers. Maybe just leave the name out and rely only on the ID."
      ],
      "influxdb-manage-complete-cache-lifecycle": [
        "Does this just grab a read lock? Seems like we don't need to check this so frequently by default, once a second seems more than enough?",
        "I put it here for now, but I'm not sure we want to walk the last cache and evict on every single wal file flush. I think it should be changed to only do eviction on snapshot, that way we're not spending too much time on it.",
        "Logged #25223 to track",
        "The behavior I like to see on creation of things is that if the user tries to create something with all the same arguments, it returns ok, if the arguments are different, then it returns with an error that it already exists. This might be handled higher up the stack in the API layer, but I'm not sure that this error would give the caller enough information to make the determination.\r\n\r\nThe reason I opt for this behavior is that it makes automation easier where they can call create as many times as they want and it'll always succeed as long as the settings are the same.",
        "If all rows have been expired, this key should be removed from the map. Otherwise, key values that stop sending data (think ephemeral things like container id, etc) will blow up the size of the cache over time with a bunch of entry map entries. (I'm assuming I'm reading this correctly and only the values are getting removed).\r\n\r\nThis should be true walking up the tree. Each key/value should be removed from the map if all children are empty."
      ],
      "influxdb-prefer-explicit-nullability": [
        "We might want to represent null key column values in the structure? I'm not sure about this yet though. Something to think about and discuss.",
        "Yeah, still not totally sure we want to bother with this, so maybe log an issue to see if anyone cares. For now I'd leave it as is."
      ],
      "influxdb-include-explanatory-examples": [
        "This is a bit odd but if you run\r\n\r\n```\r\ninfluxdb3 create database -h\r\nCreate a new database\r\n\r\nUsage: influxdb3 create database [OPTIONS] <DATABASE_NAME>\r\n\r\nArguments:\r\n  <DATABASE_NAME>  The database name to run the query against [env: INFLUXDB3_DATABASE_NAME=]\r\n```\r\n\r\nThe text is that you're running a query against a database, when if fact you're creating a database with that name. Ideally it would say that and also include some information about valid naming (i.e. alphanumeric with underscore and dash, starting with a letter or number).",
        "Would be good to have help that indicates what this is with an example. Also guidance on what valid tag names are.",
        "Update to include information that it's a comma separated list and provide an example.",
        "Can you add a comment with some examples of what you expect this to be? e.g. \"influxdb3-3.0.1\""
      ],
      "influxdb-descriptive-semantic-naming": [
        "rename to `last_snapshotted_wal_sequence_number` for clarity. The latest wal file number will always be >= this one.",
        "`write_lines_total` is the more standard naming convention. Metric names should end in units, e.g. _seconds, _bytes, or _total for unit-less metrics. See https://prometheus.io/docs/practices/naming/",
        "Yeah, we should do that everywhere in the code where we don't have a typed time.",
        "Should this be a new type like `struct ParquetFileId(u64)`?"
      ],
      "influxdb-avoid-flaky-test-patterns": [
        "I think with these kinds of serialization things, snapshot testing might not be what we want to use anyway. We want to confirm that we can serialize and deserialize data. Later when we make updates to the types, we want to make sure we can still deserialize older data.\r\n\r\nBut if we're marshalling into RAM, it's the equality of the data members that we're looking for."
      ],
      "influxdb-choose-appropriate-lock-primitives": [
        "This definitely feels like it's going to be a problem. An RWLock would be very preferred. I guess we can wait and see though.",
        "Seems like you could separate out the recency tracking from the data map. Also, atomics!",
        "small thing, but I'd clone the path before you acquire the lock.",
        "Shouldn't these just be in the `InnerCatalog` under its lock? Otherwise you won't be able to update these maps when inserting a new database or table in and ensure the mappings go in at the same time.",
        "I think you'd be better off just having a single RwLock around this whole struct. New last caches are only added when a new table or database is created. When writes come in, they're always for multiple databases & tables, so you end up grabbing a bunch of individual locks. Also, the write lock is only obtained on the flush interval for the WAL (which for now is 10ms, but it's going to increase in a later refactoring I have planned).\r\n\r\nYou end up having to do a bunch more locking for writes and for reads with the double lock that won't really pay off.",
        "I'd wrap this in a RWLock instead as queries will hit this a ton, but writes shouldn't be that often.",
        "Shouldn't the taking of the metadata lock happen after we've serialized the data? That way, the work happens there without a lock and then we're only holding the lock to actually update the metadata and object store, rather than doing any work."
      ],
      "influxdb-vet-security-critical-dependencies": [
        "We don't want to depend on the system. We want the interpreter included in the InfluxDB binary so that the user has no other local system setup to do other than install InfluxDB. I'm guessing we'll use PyOxidizer for this, although I'm not sure it's [fully supported yet](https://github.com/indygreg/PyOxidizer/issues/324)."
      ]
    },
    "profile": {
      "location": "New York, NY",
      "company": "@influxdata ",
      "blog": "",
      "twitter_username": "pauldix",
      "site_admin": false,
      "followers": 843,
      "following": 31
    }
  },
  "wilkinsona": {
    "repos": [
      "spring-projects/spring-boot"
    ],
    "entries": [
      {
        "slug": "spring-boot-alphabetical-ordering-requirement",
        "title": "Alphabetical ordering requirement"
      },
      {
        "slug": "spring-boot-bean-lifecycle-management",
        "title": "Bean lifecycle management"
      },
      {
        "slug": "spring-boot-clear-structured-logging-documentation",
        "title": "Clear structured logging documentation"
      },
      {
        "slug": "spring-boot-concrete-bean-return-types",
        "title": "Concrete bean return types"
      },
      {
        "slug": "spring-boot-consistent-observability-data",
        "title": "Consistent observability data"
      },
      {
        "slug": "spring-boot-consistent-terminology-usage",
        "title": "Consistent terminology usage"
      },
      {
        "slug": "spring-boot-document-configuration-properties-completely",
        "title": "Document configuration properties completely"
      },
      {
        "slug": "spring-boot-documentation-clarity-principles",
        "title": "Documentation clarity principles"
      },
      {
        "slug": "spring-boot-environment-variables-best-practices",
        "title": "Environment variables best practices"
      },
      {
        "slug": "spring-boot-explicit-security-configurations",
        "title": "Explicit security configurations"
      },
      {
        "slug": "spring-boot-explicit-security-documentation",
        "title": "Explicit security documentation"
      },
      {
        "slug": "spring-boot-follow-consistent-style-conventions",
        "title": "Follow consistent style conventions"
      },
      {
        "slug": "spring-boot-include-database-specific-migration-dependencies",
        "title": "Include database-specific migration dependencies"
      },
      {
        "slug": "spring-boot-inherit-organization-security-policies",
        "title": "Inherit organization security policies"
      },
      {
        "slug": "spring-boot-maintain-consistent-naming-patterns",
        "title": "Maintain consistent naming patterns"
      },
      {
        "slug": "spring-boot-meaningful-exception-design",
        "title": "Meaningful exception design"
      },
      {
        "slug": "spring-boot-optimize-test-case-design",
        "title": "Optimize test case design"
      },
      {
        "slug": "spring-boot-preserve-api-compatibility",
        "title": "Preserve API compatibility"
      },
      {
        "slug": "spring-boot-property-description-conventions",
        "title": "Property description conventions"
      },
      {
        "slug": "spring-boot-reference-existing-configurations",
        "title": "Reference existing configurations"
      },
      {
        "slug": "spring-boot-stable-observability-components",
        "title": "Stable observability components"
      }
    ],
    "comments": {
      "spring-boot-stable-observability-components": [
        "Should we link to something stable (a tag or specific SHA) here? Perhaps https://github.com/OpenObservability/OpenMetrics/blob/v1.0.0/specification/OpenMetrics.md#exemplars."
      ],
      "spring-boot-concrete-bean-return-types": [
        "The return type of a `@Bean` method should be as specific as possible. Could this be `DefaultPulsarConsumerFactory` with `@ConditionalOnMissingBean(PulsarConsumerFactory.class)`, similar to `topicResolver` above?",
        "Yes. This provides as much type information as possible to the bean factory while allow the bean to back off when any `PulsarConsumerFactory` bean is defined.",
        "Rather than changing the return type, which deprives the bean factory of type information, it would be better to change `@ConditionalOnMissingBean` to `@ConditionalOnMissingBean(ExamplarSampler.class)`. This will cause `exemplarSampler` to back off if there's an `ExamplarEampler` bean defined, while also still telling the bean factory that `exemplarSampler` is a `DefaultExemplarSampler`.",
        "I think this could be considered a bug and, therefore, it should be handled separately. Could you please split this out into a new PR, along with a test that verifies that the auto-configured `spanReporter` bean backs off when you define a custom `Reporter` bean?",
        "To provide as much type information as possible to the bean factory, the signature should show that it returns a `B3Propagator`."
      ],
      "spring-boot-clear-structured-logging-documentation": [
        "Thanks, @quaff. I wonder if people might miss this? They could read the paragraph above and then stop, happy that they know which configuration property to set. I think it might be more noticeable if we changed the paragraph above instead to start with something like \"To enable structured logging, ensure that you aren't using custom log configuration and set the property…\". WDYT?",
        "I agree with @mhalbritter. We shouldn't over-complicate the documentation by showing the use of conditionals if they aren't always needed."
      ],
      "spring-boot-bean-lifecycle-management": [
        "Is it possible to not expose the imperative components as beans in a reactive app?",
        "> The reactive components leverage the reactive client which adapts the original client (which is imperative and auto-configured)\r\n\r\nI wonder if the original, imperative client has to be a bean in the reactive case. Perhaps it does for ease of configuration property binding and the like. Not something to worry about for this PR, but probably worth revisiting down the road before 3.2 GAs.\r\n\r\n>  there are other technologies auto-configured in Boot whose reactive components leverage the imperative ones, no?\r\n\r\nWe have some situations where there's a low-level bean that's shared between imperative and reactive (Elasticsearch client transport and the ElasticsearchClient and ReactiveElasticsearchClient that use it, for example) but I can't think of a situation where we have an imperative bean that we expect applications to use extensively that's then wrapped by a reactive bean. I may well be forgetting something though."
      ],
      "spring-boot-optimize-test-case-design": [
        "This is largely testing the underlying SpEL support. It could be simplified to test a single valid expression.",
        "This is largely testing the underlying SpEL support. It could be simplified to test a single invalid expression.",
        "+1. For this to warrant being a Docker-based test, I think it should somehow check that setting the `application_name` has had the desired effect on the Postgres side of things.\r\n\r\nI'd then prefer that we only have a single Docker-based test that checks this and that the precedence of the different sources of the application name is unit tested without involving Docker. Right now we're adding a lot to the build time and I don't think the extra test coverage warrants that increase.",
        "I don't think so. Given that all three values are passed through and then returned by `getMaxParameterCount()` without any having special treatment, I think testing with a single value would be sufficient.",
        "The paths aren't right in this example, but Framwork 6.2's `JsonContent` is an option here:\r\n\r\n```java\r\nJsonContent body = new JsonContent(entity.getBody());\r\nassertThat(body).extractingPath(\"alias\").isEqualTo(\"spring-boot-ssl-sample\");\r\nassertThat(body).extractingPath(\"status\").isEqualTo(\"EXPIRED\");\r\nassertThat(body).extractingPath(\"subject\")\r\n\t.isEqualTo(\"CN=localhost,OU=Unknown,O=Unknown,L=Unknown,ST=Unknown,C=Unknown\");\r\n```",
        "I don't feel strongly one way or the other. That said, I do wonder if the intent would be clearer with four separate methods with names that make the expectations clear. They could all then delegate to a common method that looks a lot like this one."
      ],
      "spring-boot-documentation-clarity-principles": [
        "What failure did you get? That `xref` seems to work for me.",
        "I just pushed a change to your branch. Let's see what CI makes of it.",
        "It failed. `:spring-boot-project:spring-boot-docs:antora` which pulls in the Actuator API docs worked for me locally. It's `spring-boot-project:spring-boot-actuator-autoconfigure:antora` that's failing on CI though.\r\n\r\nAny ideas, @philwebb? The reference works fine when built as part of `spring-boot-docs` but not when just building the Actuator API docs on their own.",
        "I think this was intentionally singular. If it's to change, I think it should be changed to \"container\".",
        "Oops. Yeah, I think we can just remove it as we're already in a section about Testcontainers. I think \"for Container beans\" reads better and there's enough context to know that it's a Testcontainers container.",
        "I'd just document the map-based approach.",
        "Could we use \"over\" here? I think it's more idiomatic when talking about a protocol.",
        "This looks to be unrelated to the rest of the changes. Could it please be made separately?",
        "> If you still insist, I can move it to separate PR.\r\n\r\nYes please, Artem. It sounds like we should get rid of that sentence in 2.4.x and later whereas the rest of the changes proposed here will land in 2.6."
      ],
      "spring-boot-inherit-organization-security-policies": [
        "I don't think we need this as we already inherit the policy from spring-projects/.github. We can just link to https://github.com/spring-projects/spring-boot/security/policy."
      ],
      "spring-boot-reference-existing-configurations": [
        "Is it possible to tie this to the version that's configured in `.sdkmanrc`? That's particularly important for branches other than `main` where a Java 8 install is required.",
        "To avoid the risk of editing conflicts, I think we might prefer to have this disabled too.",
        "Those docs, which is where I learned of the potential for a conflict, say that adding the badge \"can also create a concurrent editing conflict when the bot and a user try to edit the description of a pull request at the same time\"."
      ],
      "spring-boot-maintain-consistent-naming-patterns": [
        "I wonder if it would be better if we ignored that fact that it's `BatchSpanProcessor` that's the component that's doing the exporting and focussed instead on the properties affecting span export.\r\n\r\n\r\n```\r\nmanagement.tracing.opentelemetry.span.export.include-unsampled\r\nmanagement.tracing.opentelemetry.span.export.schedule-delay\r\nmanagement.tracing.opentelemetry.span.export.timeout\r\n```",
        "> However, these properties only impact the BatchSpanProcessor and cannot be applied, for instance, to the SimpleSpanProcessor or other processors.\r\n\r\nBoot doesn't auto-configure the `SimpleSpanProcessor` so hopefully that confusion will not arise. The property names make no mention of processing so hopefully no one will expect them to apply to other processors either.\r\n\r\n>  In my opinion, there should be some indication that these properties are intended for batch processing\r\n\r\nDoesn't the `max-batch-size` property (that we omitted in the property examples above) do that?\r\n\r\nThe complete set of properties would be:\r\n\r\n```\r\nmanagement.tracing.opentelemetry.span.export.include-unsampled\r\nmanagement.tracing.opentelemetry.span.export.max-batch-size\r\nmanagement.tracing.opentelemetry.span.export.max-queue-size\r\nmanagement.tracing.opentelemetry.span.export.schedule-delay\r\nmanagement.tracing.opentelemetry.span.export.timeout\r\n```\r\n\r\nI'm not totally sold on the prefix here. In part, it's the `otlp` vs `opentelemetry` problem again. I think this export may use Zipkin or OTLP depending on which of `ZipkinSpanExporter`, `OtlpHttpSpanExporter`, and `OtlpGrpcSpanExporter` has been auto-configured. We already have a couple of export-related properties for those:\r\n\r\n- `management.otlp.tracing.export.enabled`\r\n- `management.zipkin.tracing.export.enabled`",
        "I think `opentelemetry` is better here as this is configuring parts of the OpenTelemetry SDK. The protocol used for the export won't necessarily by OTLP so I think `otlp` would be inaccurate. This naming and its subtleties are more complex than we'd like but that complexity isn't of our making and there's only so much we can do to hide it.\r\n\r\n+1 for dropping `.span.` from the names.",
        "We prefer longer names for test methods that describe what's being tested. Something like `whenCustomizerBeanIsDefinedThenItIsConfiguredOnSpringLiquibase`.",
        "I think this should be called `SdkTracerProviderBuilderCustomizer`. We can take care of that as part of merging it.",
        "I think we should aim for consistency with other similar interfaces that already exist. We have many that are named `…BuilderCustomizer` because the class that they are customising is a `…Builder`. The same applies here.",
        "For consistency with `alpha-numeric -> alphanumeric`, this method should be renamed to `remainderIsNotAlphanumeric`.",
        "Is white box a recognised Wavefront term? We try to avoid white box and black box if we can as they're jargon that can confuse people, particularly those with English as a second language."
      ],
      "spring-boot-follow-consistent-style-conventions": [
        "We don't really use `Objects.equals`. This could be `return \"trust\".equals(postgresAuthMethod);` instead.",
        "We don't use `var` in Boot's code base. Please declare variables' types explicitly.",
        "We don't use `var` in Spring Boot's code.",
        "There are [several other places where we use `@Order(Ordered.LOWEST_PRECEDENCE)`. If we're going to change one, we should change them all. Personally, I don't think we should change any of them as I prefer to see the order explicitly declared.",
        "`(exporter) ->` is Spring Boot's code style and this doesn't pass Checkstyle. Please build your changes before submitting them.",
        "We're still escaping the opening `[` but the closing `]` is no longer escaped. I think the parser may cope with that but I find the expression harder to read as you have to know that the parser treats the `]` as a plain `]` rather than closing a never-opened character set. Please revert."
      ],
      "spring-boot-property-description-conventions": [
        "Given that the property's prefix is `management.tracing.brave` do we need `(tracing library)` in the description? Put another way, is there another Brave that could cause confusion?",
        "We try to have `boolean` properties start with `Whether the…`. Perhaps this could be something like this instead:\r\n\r\n> Whether the propagation type and tracing backend support sharing the span ID between client and server spans. Requires B3 propagation and a compatible backend."
      ],
      "spring-boot-meaningful-exception-design": [
        "Let's consider this separately please. If we do it, it should be applied on the servlet side as well.",
        "We think that adding the stop or destroy failure as a suppressed exception is a good idea. Would you like to open a PR for it or would you prefer that we take care of it?",
        "Thanks!",
        "I think an `IllegalStateException` or `IllegalArgumentException` (it depends if you consider `target` or `type` to be the problem) might be better here and perhaps the error message should include `target`. Something like `\"Cannot unwrap \" + target + \" to \" + type.getName()`",
        "I'd rather this failed with an NPE as the initialize method should always be present.",
        "If the field's not found something's gone really wrong. I think it would be better to fail hard.",
        "If Hikari doesn't meet expectations then I think we should just give up as we have no way of knowing that our approach is valid. We'll know, through tests, that it works with the version of Hikari in dependency management. If someone overrides that version and Hikari has changed incompatibility I think we'd be better to fail fast. That'll make the problem much easier to diagnose rather than dealing with a checkpoint failure and trying to figure out why it occurred.",
        "`NoSuchJobException` is a `JobExecutionException` so I don't think there's any need to create a new exception. Instead, I think the `try` and `catch` could be removed completely so that the method just throws the original `NoSuchJobException`."
      ],
      "spring-boot-document-configuration-properties-completely": [
        "The annotation processor the generates configuration property metadata can't handle enums due to a limitation of the annotation processing model. Please add an entry to `additional-spring-configuration-metadata.json` for this property's default value.",
        "This should be an `int` with a default value that matches that of `InMemoryWebSessionStore`. Providing a default improves the metadata that's used for auto-completion when editing application properties and YAML files. A test should be added to assert that the defaults are in sync.",
        "Unfortunately, we can't use javadoc tags in the descriptions of configuration properties as it doesn't work well with configuration property metadata generation. We'll have to use `AggregationTemporality` or even just `Aggregation temporality` instead.",
        "This default should be declared in the additional metadata as the annotation processor cannot detect default enum values."
      ],
      "spring-boot-explicit-security-documentation": [
        "This property name is incorrect. It should be `spring.security.oauth2.resourceserver.jwt.audiences`. As previously requested, please use the `configprop:spring.security.oauth2.resourceserver.jwt.audiences[]` syntax so that the name is checked as part of the build.",
        "This (line 234) should not be here. It should be part of line 226.",
        "Sorry, it's still not right and you've moved it to line 229 rather than updating 226 as I requested above. Please don't make any more changes. I'll take things from here.",
        "That looks better. Please go ahead.",
        "I agree. In the context of CSRF, it would be better to just disable Security's protection rather than ignoring entirely. This is what the old (1.x) auto-configuration for the console used to do:\r\n\r\nhttps://github.com/spring-projects/spring-boot/blob/10a5cef4ef33e7c86d18e1f92793c2aaa57d5a82/spring-boot-autoconfigure/src/main/java/org/springframework/boot/autoconfigure/h2/H2ConsoleAutoConfiguration.java#L97-L113",
        "The goal here is to consistently use the term \"expose\" but this change uses \"secret\". I think this sentence would be better if it was something like the following:\r\n\r\n> For security purposes, only the `/health` endpoint is exposed over HTTP by default."
      ],
      "spring-boot-include-database-specific-migration-dependencies": [
        "This wildcard search doesn't work for me and reports 0 results. I think we may have to link to https://documentation.red-gate.com/flyway/flyway-cli-and-api/supported-databases instead. There are also some DB-specific modules that are not named `flyway-database-*` such as the [module for MySQL](https://documentation.red-gate.com/flyway/flyway-cli-and-api/supported-databases/mysql)."
      ],
      "spring-boot-preserve-api-compatibility": [
        "This is a breaking API change. A separate method for the timeout would be better.",
        "This is a breaking API change. A separate method for the timeout would be better."
      ],
      "spring-boot-consistent-observability-data": [
        "I wonder if something like \"Duration of HTTP server request handling\" would be better here? To me, it makes it clearer that the metric is for a server handling a request from a client rather than a client making a request to a server."
      ],
      "spring-boot-environment-variables-best-practices": [
        "I'm not in favor of trying to mock environment variables so +1 for @nosan's suggestion.\r\n\r\nMore generally, where we need to test environment variables, we try to allow the `Map` to be injected for testing purposes. If third-party code doesn't allow that then I'd prefer that we accept that it can't be tested rather than relying on reflection to hack into `System.getenv`.",
        "I don't think spaces in the default makes any difference. For example, you can do this:\r\n\r\n```\r\nproperty.example=${some.other.property:The default value}\r\n```\r\n\r\nAssuming that `some.other.property` hasn't been set the value of `property.example` will default to `The default value`."
      ],
      "spring-boot-alphabetical-ordering-requirement": [
        "The modules should be listed alphabetically",
        "We try to list modules in alphabetical order."
      ],
      "spring-boot-consistent-terminology-usage": [
        "+1 for changing to \"autowire\". The Framework docs use \"autowire\" as a verb and we should follow suit, particularly as the example that follows doesn't even use `@Autowired` as it does not need to do so.",
        "Thanks for the suggestion. I think it would be better to use Kubernetes here rather than the K8S abbreviation."
      ],
      "spring-boot-explicit-security-configurations": [
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here."
      ]
    },
    "profile": {
      "location": "UK",
      "company": "Broadcom",
      "blog": "",
      "site_admin": false,
      "followers": 2181,
      "following": 0
    }
  },
  "asmorkalov": {
    "repos": [
      "opencv/opencv"
    ],
    "entries": [
      {
        "slug": "opencv-cleanup-before-errors",
        "title": "Cleanup before errors"
      },
      {
        "slug": "opencv-clear-api-contracts",
        "title": "Clear API contracts"
      },
      {
        "slug": "opencv-code-for-readability",
        "title": "Code for readability"
      },
      {
        "slug": "opencv-consistent-descriptive-naming",
        "title": "Consistent descriptive naming"
      },
      {
        "slug": "opencv-cross-platform-api-design-rules",
        "title": "Cross-platform API design rules"
      },
      {
        "slug": "opencv-document-configuration-version-requirements",
        "title": "Document configuration version requirements"
      },
      {
        "slug": "opencv-document-properly-with-references",
        "title": "Document properly with references"
      },
      {
        "slug": "opencv-feature-flag-convention",
        "title": "Feature flag convention"
      },
      {
        "slug": "opencv-framework-synchronization-practices",
        "title": "Framework synchronization practices"
      },
      {
        "slug": "opencv-maintain-build-compatibility",
        "title": "Maintain build compatibility"
      },
      {
        "slug": "opencv-maintain-code-consistency",
        "title": "Maintain code consistency"
      },
      {
        "slug": "opencv-meaningful-semantic-naming",
        "title": "Meaningful semantic naming"
      },
      {
        "slug": "opencv-optimize-container-access",
        "title": "Optimize container access"
      },
      {
        "slug": "opencv-optimize-memory-allocation-patterns",
        "title": "Optimize memory allocation patterns"
      },
      {
        "slug": "opencv-prevent-null-vulnerabilities",
        "title": "Prevent null vulnerabilities"
      },
      {
        "slug": "opencv-use-opencv-error-mechanisms",
        "title": "Use OpenCV error mechanisms"
      },
      {
        "slug": "opencv-use-optimized-functions",
        "title": "Use optimized functions"
      },
      {
        "slug": "opencv-use-proper-assertions",
        "title": "Use proper assertions"
      },
      {
        "slug": "opencv-validate-tensor-dimensions",
        "title": "Validate tensor dimensions"
      }
    ],
    "comments": {
      "opencv-optimize-container-access": [
        "just local `std::vector<>` outside of `if` should be enough here. It's empty by default also you save one new/delete pair if condition is true."
      ],
      "opencv-code-for-readability": [
        "Let's extract extension search as dedicated static function. It should make the code more readable."
      ],
      "opencv-document-properly-with-references": [
        "Please move the text to tutorial (markdown) and add references to the header file.",
        "It should go to make ccm header to be included into documentation and bindings. Please CV_EXPORTS_W to generate Java and Python bindings for it too.",
        "I propose to use the same description as for cv::solvePnpRansac with fisheye model reference and add reference to SOLVEPNP_XXX constants instead of copy."
      ],
      "opencv-document-configuration-version-requirements": [
        "I propose to add command line option, e.g. \"--strict-dependencies\" to make the check optional.",
        "I want to switch CI to the default configuration and always build SDK and AAR packages with 16kb pages support. The packages will work with both old and new memory configuration. It just tunes ELF sections alignment a bit.  The option will be ignored with NDK before 27."
      ],
      "opencv-consistent-descriptive-naming": [
        "We use all capital letters for constant names.",
        "The same note about capital letters,\r\ne.g. `COLOR_SPACE_AppleRGB` -> `COLOR_SPACE_APPLE_RGB`",
        "The name is very controversial. I propose `THRESH_DRYRUN` or something similar."
      ],
      "opencv-feature-flag-convention": [
        "Good point. Fixed.",
        "Looks like I cannot make it public for now. We get redefinition issue. The macros are defined by both HAL and IPP core. I made it private for now to exclude the redefinition issue and added note to CMake."
      ],
      "opencv-cleanup-before-errors": [
        "Looks like the assert will be disabled in regular release builds: https://en.cppreference.com/w/cpp/error/assert.\r\nWhy not just CV_Assert? It's defined in `opencv2/core/base.hpp`"
      ],
      "opencv-maintain-build-compatibility": [
        "It should break static linkage, if OpenCV is build against own zlib-ng, but not system-wide.",
        "> protobuf_generate Added in version 3.13.\r\n\r\nIt breaks build with older CMake. I propose to add CMake version check and presume the old branch for old CMake.\r\n"
      ],
      "opencv-use-opencv-error-mechanisms": [
        "Please do not use try-catch. OpenCV uses CV_Assert, CV_Check for the function input validation and and CV_LOG_DEBUG/CV_LOG_INFO for notifications. Function should not silently ignore obvious invalid inputs like not enough points or wrong data types and ranges.  ",
        "OpenCV uses CV_Assert for such purposes.",
        "please add CV_Assert with input type checks at least. The function is public API now and should handle invalid input correctly.",
        "It's still relevant.",
        "Please use CV_Error, CV_Assert, etc. See https://docs.opencv.org/5.x/db/de0/group__core__utils.html#ga5b48c333c777666e076bd7052799f891",
        "It's error. I propose to include amount of channels to error message in printf style like https://github.com/opencv/opencv/blob/c21d0ad9d08d364542bb4a6eb971ee3051ccba63/modules/imgcodecs/src/grfmt_jpeg.cpp#L771",
        "OpenCV's `imread` is exception safe. Please use CV_LOG_WARNING and analogs and return satus instead of exception."
      ],
      "opencv-cross-platform-api-design-rules": [
        "size_t is not wrapped to Python and Java correctly. Java even does not support unsigned types. Please use int instead.",
        "size_t does not work well with Java and Python bindings. let's use just int.\r\n",
        "Please use `CV_EXPORTS_W` and `CV_WRAP` to make it available from Python,  Java and other binded languages."
      ],
      "opencv-use-optimized-functions": [
        "inRange should be enough instead of it: https://docs.opencv.org/4.x/d2/de8/group__core__array.html#ga48af0ab51e36436c5d04340e036ce981",
        "No need to spit cv::Mat by channels for it. cv::sum supports channels: https://docs.opencv.org/5.x/d2/de8/group__core__array.html#ga716e10a2dd9e228e4d3c95818f106722",
        "I propose to convert Gamma correction to public function with InputArray and OutputArray. It's useful independently from the pipeline and also may be significantly optimized with universal intrinsics.",
        "CV_SIMD_WIDTH is compile time constant. It may not work correctly with _SCALABLE branch. please use `VTraits<xxx>::max_nlanes` instead. For fixed-size SIMD it works in the same way.",
        "@fengyuentau please correct me, if I'm wrong.",
        "Yes, it does not work, because `w` value is not known in compile time. `VTraits<xxx>::max_nlanes` is compile time constant. It's equal to `CV_SIMD_WIDTH` for fixed SIMD size architectures (x86). RISC-V RVV vector size is not known in compile time, but we know maximum vector length and use it for intermediate buffers to fit any feasible vector size.",
        "Please use cv::PSNR instead: https://docs.opencv.org/4.x/d2/de8/group__core__array.html#ga3119e3ea73010a6f810bb05aa36ac8d6",
        "I propose to use cv::RNG for it to make it manageable outside:\r\n- User may set seed to get deterministic behaviour\r\n- OpenCV test system fixes cv::RNG seed for each test independently."
      ],
      "opencv-prevent-null-vulnerabilities": [
        "please use std::vector, std::array or cv::AutoBuffer to prevent memory leaks in cases of parser failrue.",
        "Let's use `std::vector<>` or `cv::AutoBuffer` for locals to prevent leaks.",
        "I propose to use `std::vector` or `cv::AutoBuffer` to prevent memory leaks. ",
        "Pleas use `cv::AutoBuffer` or `std::vector` for temporary buffers instead new/delete to prevent memory leaks. `cv::AutoBuffer` is preferable, it may use stack space, if buffer is small enough."
      ],
      "opencv-use-proper-assertions": [
        "`ASSERT_FALSE(chartsRGB.empty());`",
        "`ASSERT_FALSE(chartsRGB.empty());` here and bellow.",
        "`EXPECT_FALSE(src.empty()) << Cannot open test image perf/1920x1080.png;`"
      ],
      "opencv-maintain-code-consistency": [
        "using namespace is very bad practice. It affects everything, even, if the header is not included directly.",
        "Please remove dead code, or guard it with macro, if you want to use it later."
      ],
      "opencv-validate-tensor-dimensions": [
        "```\r\nvector<Mat> channels = {\r\n    Mat(output_transposed.size[1], output_transposed.size[2], CV_32F, output_transposed.ptr<float>(0)),\r\n    Mat(output_transposed.size[1], output_transposed.size[2], CV_32F, output_transposed.ptr<float>(1)),\r\n    Mat(output_transposed.size[1], output_transposed.size[2], CV_32F, output_transposed.ptr<float>(2))\r\n};\r\n```"
      ],
      "opencv-framework-synchronization-practices": [
        "`stripeHeight = nThreads * 10;` sounds strange. More threads -> larger piece for each thread.",
        "threads are 8. It's not true:\r\n- Android build uses 2 threads by default. It's done to prevent overheating, but may be changed in future.\r\n- Linux builds use all available cores.\r\n- parallel_for_ serializes nested parallel_for_ calls. So you can easily get 1 here.",
        "We usually set granularity to some reasonable size for single thread. OpenCV uses dynamic scheduling, so all other steps are done automatically.",
        "I propose to use `num_worker_threads = cv::getNumThreads()` to make it manageable outside. See https://docs.opencv.org/4.x/db/de0/group__core__utils.html#ga2db334ec41d98da3129ef4a2342fc4d4 "
      ],
      "opencv-optimize-memory-allocation-patterns": [
        "It's more efficient to use  std::vector<int> dims(dim_count); and assign values, rather than call push_back and trigger reallocations.",
        "If I understood correctly, The m_read_buffer is resized only once here ant it's size is always 16k. The `m_read_buffer` is used in ::read() only. M.b. it's better to make it local variable there. If you need the buffer shared between readHeader and readData then it makes sense to initialize the buffer in decoder constructor.",
        "I propose to use local variable on stack. new is redundant here. Also it's not deleted afterwards.",
        "I propose to use local variable on stack. new is redundant here. Also it's not deleted afterwards."
      ],
      "opencv-clear-api-contracts": [
        "Need a warning, that GDAL does not support metadata for now.",
        "I propose to Move BRG/RGB logic into model with a flag (default to bgr). All OpenCV functions are designed for BGR. The flag allows to make optimizations without API change.",
        "The `p` parameter does not have default value like `Point p = Point()`. It means that the point is user provided value. I do not think that we sound handle `Point()`, a.k.a. (0,0) as special case.",
        "Why do we need special cases in code? User should provide location, shouldn't he/she?"
      ],
      "opencv-meaningful-semantic-naming": [
        "It makes sense to move key name to the function parameters. E.g.\r\ncheck_cmake_flag_enabled(cmake_file, \"HAVE_IPP\")\r\n\r\nThe same function may be used for KleidiCV and other dependencies.",
        "It breaks compatibility. M.b. rename the method to detectMarkersMultiDict?",
        "What if use `namespace_prefix_override` solution like for the functions? Namespace is already appended to the function names, if it's not overridden by config. It's less hacky and more obvious."
      ]
    },
    "profile": {
      "company": "OpenCV.AI",
      "blog": "",
      "site_admin": false,
      "followers": 255,
      "following": 0
    }
  },
  "AdriiiPRodri": {
    "repos": [
      "prowler-cloud/prowler"
    ],
    "entries": [
      {
        "slug": "prowler-consistent-environment-variable-naming",
        "title": "Consistent environment variable naming"
      },
      {
        "slug": "prowler-document-dependency-versioning",
        "title": "Document dependency versioning"
      },
      {
        "slug": "prowler-ensure-migration-compatibility",
        "title": "Ensure migration compatibility"
      },
      {
        "slug": "prowler-flexible-ai-model-versions",
        "title": "Flexible AI model versions"
      },
      {
        "slug": "prowler-log-exceptions-with-context",
        "title": "Log exceptions with context"
      },
      {
        "slug": "prowler-meaningful-consistent-naming",
        "title": "Meaningful consistent naming"
      },
      {
        "slug": "prowler-parameterize-configuration-values",
        "title": "Parameterize configuration values"
      },
      {
        "slug": "prowler-pin-github-actions-dependencies",
        "title": "Pin GitHub Actions dependencies"
      },
      {
        "slug": "prowler-prioritize-code-readability",
        "title": "Prioritize code readability"
      },
      {
        "slug": "prowler-secure-authentication-flows",
        "title": "Secure authentication flows"
      },
      {
        "slug": "prowler-service-layer-abstraction",
        "title": "Service layer abstraction"
      },
      {
        "slug": "prowler-specific-exception-handling",
        "title": "Specific exception handling"
      },
      {
        "slug": "prowler-task-signature-methods",
        "title": "Task signature methods"
      },
      {
        "slug": "prowler-test-authentication-dependencies",
        "title": "Test authentication dependencies"
      },
      {
        "slug": "prowler-use-configurable-default-values",
        "title": "Use configurable default values"
      }
    ],
    "comments": {
      "prowler-document-dependency-versioning": [
        "This is because Marshmallow has recently updated to 4.0 and Safety is not compatible with this version so at least for now we need to set the version to lower than 4.0",
        "Same problem, if we upgrade dependencies and Marshmallow goes to 4.0 then our checks will fail due to a compatibility issue"
      ],
      "prowler-parameterize-configuration-values": [
        "Sure",
        "I can apply this change, @jfagoagas what do you think?"
      ],
      "prowler-secure-authentication-flows": [
        "The endpoint from the library itself returns a 404 if the domain doesn’t exist, and a 200 along with the login page if it does. There’s no way to prevent enumeration if we always return a 200, the response body will still be different depending on whether the configuration exists or not",
        "An HTML page with a CSRF token the only thing I can think of is to always generate the HTML with a fake CSRF token that’s indistinguishable from a real one. There’s no standard mechanism for this since the responsibility is left to the IdP. If we want to mitigate it on our side, this is the only approach I see as feasible"
      ],
      "prowler-task-signature-methods": [
        "It needs to be .s because we require the output of the previous task. We use .si when we don’t need anything from the previous task (within a chain)",
        "It needs to be .s because we require the output of the previous task. We use .si when we don’t need anything from the previous task (within a chain)"
      ],
      "prowler-service-layer-abstraction": [
        "It hasn’t been changed because I didn’t see the need to explicitly set the content-type. We can talk about this, as it’s possible I’m missing something or we’re not fully aligned",
        "Thanks, let me know when this is available",
        "This method was created because the API uses a completely different system to manage outputs. While we could replicate what’s in the CLI, it’s not the most efficient approach for the API. So instead of introducing changes just to mirror the CLI, a new function was created. Regarding the content-type handling, in my tests, I haven’t had to worry about it, files are uploaded correctly, the type is set properly, and everything can be viewed and downloaded without any issues.\r\n\r\nWe can discuss this in more detail and reach an agreement since it’s an important decision"
      ],
      "prowler-prioritize-code-readability": [
        "If you want we can change to this version, it's the same:\r\n\r\n```\r\nklass = GenericCompliance\r\nfor condition, cls in COMPLIANCE_CLASS_MAP.get(provider_type, []):\r\n        if condition(framework_name):\r\n            klass = cls\r\n            break\r\n```",
        "Making this change implies modifying code and having to define the function get_required_permissions in all views, for me the interesting thing would be to change the name to set_required_permissions."
      ],
      "prowler-use-configurable-default-values": [
        "If you’re okay with it, I’ll continue working on this PR myself, focusing on the DB changes since we can’t pass the tokens through the URL, that approach isn’t secure. So if it sounds good to you, I’ll take it from here and update the PR accordingly. For now, this PR should have the `no-merge` label",
        "`get_s3_client is` only invoked if you have configured `DJANGO_OUTPUT_S3_AWS_OUTPUT_BUCKET`, which indicates that you want the outputs to be uploaded to S3, then it gives an exception because if a user only configures this it is understood that he is running Prowler in AWS (it is not necessary to set the credentials) or he has made a mistake because he has not set the other variables if he wants access through credentials"
      ],
      "prowler-ensure-migration-compatibility": [
        "Since enum can't have a reverse operation to return the old value I add this:\r\n\r\nreverse_sql=migrations.RunSQL.noop,"
      ],
      "prowler-consistent-environment-variable-naming": [
        "You’re right to question the naming, in this context, we actually need the private key for signing SAML requests/responses, and the X.509 certificate (not just the public key) so that the identity provider (IdP) can verify those signatures. The certificate includes the public key along with metadata and a CA signature, which is required by SAML specs",
        "We can use whatever name you prefer, it’s called that in other projects. I’m indicating that it’s public so it’s clear that the value is meant to be public and can be accessed, to make sure you shouldn’t put your private key in that variable",
        "Maybe we didn’t understand each other correctly, I’m not quite sure I follow why it’s considered confusing. The certificate is public, that’s its whole purpose by definition. Yes, it’s an X.509, but it’s meant to be publicly accessible"
      ],
      "prowler-pin-github-actions-dependencies": [
        "Due to: https://github.com/xmlsec/python-xmlsec/issues/320, we need to reinstall after the other dependencies and it should be the no-binary version because its need to be compilated in our system",
        "Sure!"
      ],
      "prowler-log-exceptions-with-context": [
        "Thanks! Added",
        "Maybe we can add some information, and remove the `continue`. What do you think?\r\n\r\n```\r\nlogger.warning(f\"Failed to generate output for finding '{finding}': {exc}\")\r\n```",
        "I didn't notice, its ok for me"
      ],
      "prowler-meaningful-consistent-naming": [
        "No, we need a value because the name is mandatory. We can define another value",
        "Ok, we can change it",
        "They use their required_permissions parameter, so the function get_required_permissions does not return anything, maybe the interesting thing would be to change the name of this function to set_required_permissions."
      ],
      "prowler-test-authentication-dependencies": [
        "I checked it at the beginning of the development and it worked and I will check it again to be sure"
      ],
      "prowler-specific-exception-handling": [
        "Good catch, I’ll push the fix later ",
        "I think that since the user can add this value it is important to verify and track what is happening, I propose a more secure version of this function with more information to detect anomalous cases:\r\n```suggestion\r\n        try:\r\n            decrypted_key = fernet.decrypt(self.api_key)\r\n            return decrypted_key.decode()\r\n        except InvalidToken:\r\n            logger.warning(\"Failed to decrypt API key: invalid token.\")\r\n            return None\r\n        except Exception as e:\r\n            logger.error(f\"Unexpected error while decrypting API key: {e}\")\r\n            return None\r\n```",
        "I set a 500 because at that point, if a ClientError really appears, the error is not on the part of the user, it is something internal to us. In my opinion the errors of type 400 are a problem of configuration or in the request, in short, a user problem. The errors 500 are our errors and should never happen but if they happen in this case it is controlled. We can discuss this and change it as we decide",
        "I will change it but in my opinion even if boto fails it is still not the user's fault and in any case it is the server that has failed. At that point there is no way the user could have caused the crash",
        "What I mean is that at this point all errors are unexpected",
        "Change applied",
        "Maybe we can change the code like this?:\r\n\r\n```suggestion\r\n        try:\r\n            result = result_queue.get(timeout=timeout) or default\r\n            error_result = error_queue.get(timeout=1)\r\n        except queue.Empty:\r\n            result = default\r\n```"
      ],
      "prowler-flexible-ai-model-versions": [
        "Hmm, I’m not sure, postgres enums aren’t easy to modify, and OpenAI keeps releasing new models, so if we hardcode this in the code or in Postgres, we’ll need to deploy a new version every time a new model comes out. I think this should go in the `.env` file instead",
        "In my opinion, in this case I would leave it as you have it, since this allows us to add new models as they become available without having to release a new version of Prowler, just by updating the .env. What do you think, @vicferpoy? I believe making this field a choice would force us to release a new version or even add migrations every time new models become available"
      ]
    },
    "profile": {
      "location": "Granada, Spain",
      "company": "@prowler-cloud ",
      "blog": "",
      "site_admin": false,
      "followers": 21,
      "following": 11
    }
  },
  "zanieb": {
    "repos": [
      "astral-sh/uv"
    ],
    "entries": [
      {
        "slug": "uv-balance-test-performance-considerations",
        "title": "Balance test performance considerations"
      },
      {
        "slug": "uv-clear-precise-documentation",
        "title": "Clear precise documentation"
      },
      {
        "slug": "uv-consistent-authentication-patterns",
        "title": "Consistent authentication patterns"
      },
      {
        "slug": "uv-declarative-constraints-over-runtime",
        "title": "Declarative constraints over runtime"
      },
      {
        "slug": "uv-enforce-strong-optional-types",
        "title": "Enforce strong optional types"
      },
      {
        "slug": "uv-environment-variable-best-practices",
        "title": "Environment variable best practices"
      },
      {
        "slug": "uv-make-errors-user-actionable",
        "title": "Make errors user actionable"
      },
      {
        "slug": "uv-names-should-be-descriptive",
        "title": "Names should be descriptive"
      },
      {
        "slug": "uv-optimize-cache-sharing-strategies",
        "title": "Optimize cache sharing strategies"
      },
      {
        "slug": "uv-optimize-cicd-commands",
        "title": "Optimize CI/CD commands"
      },
      {
        "slug": "uv-secure-configuration-defaults",
        "title": "Secure configuration defaults"
      },
      {
        "slug": "uv-short-circuit-evaluation-strategies",
        "title": "Short-circuit evaluation strategies"
      },
      {
        "slug": "uv-structure-for-readability",
        "title": "Structure for readability"
      },
      {
        "slug": "uv-test-deployment-edge-cases",
        "title": "Test deployment edge cases"
      },
      {
        "slug": "uv-use-direct-documentation-style",
        "title": "Use direct documentation style"
      }
    ],
    "comments": {
      "uv-declarative-constraints-over-runtime": [
        "Why not just store `StatusCode` directly instead of the u16?",
        "Why does it need to implement `Serialize`?",
        "Stepping up a level, why is `Index` serializable? For writing it to the TOML file? If that's it, why _would_ we need this field to be serializable? A user can't construct it via the CLI. If truly necessary, why not use a wrapper type to implement serializability?",
        "It seems nice for type safety throughout and straightforward to implement? I don't feel super strongly, but the current approach of validating at the CLI level, but retaining the u16, then revalidating later seems quite weird."
      ],
      "uv-use-direct-documentation-style": [
        "We don't say \"simply\": https://github.com/astral-sh/uv/blob/main/STYLE.md#documentation",
        "Separately, do we want to recommend `uv sync`? The idea is you can just `uv run pytest` and start working. Maybe we want to recommend `uv sync` as a way to get things ready for an IDE?",
        "That makes sense. I think we might just want to reframe it a bit, I'll take a swing at the phrasing.",
        "Elsewhere, we stylize error codes as \"HTTP XXX\" instead of \"`XXX NAME`\". I do think the name is nice, I might do \"HTTP 403 Forbidden\" though? I'd probably omit the backticks, since you refer to the error codes in the subsequent text without them.\r\n\r\nYou can probably just say \"status code\" instead of \"response status code\".\r\n\r\nGenerally, I'd avoid using \"it\" to refer to uv if it's easy. So... \"uv will stop searching if a ... is encountered\" rather than \"uv will stop searching if it encounters ...\".\r\n\r\n",
        "We avoid referring to \"Users\" like this in the documentation. Instead, I'd say \"You can configure which error codes...\" or, more typically, \"To ignore additional error codes for an index, use the `...` setting.\""
      ],
      "uv-optimize-cache-sharing-strategies": [
        "Is the parallel part particularly important here? Isn't it helpful that the cache is shared regardless of concurrency?"
      ],
      "uv-secure-configuration-defaults": [
        "```suggestion\r\nTo use the uv build backend in an existing project, add it to the `[build-system]` section in your `pyproject.toml`:\r\n```"
      ],
      "uv-enforce-strong-optional-types": [
        "`PythonMinorVersionLink::from_installation` will already return `None` on alternative implementations, and we can use `bool::then` to avoid repeating the default case here.\r\n\r\nSee https://github.com/astral-sh/uv/pull/13980"
      ],
      "uv-short-circuit-evaluation-strategies": [
        "I would expect `@latest` to scan _all_ Python interpreters and select the latest version, whereas `@any` would stop on the first interpreter.\r\n",
        "`@any` isn't sorted by version during discovery though, because we short-circuit when we find an interpreter that meets the request. Sorting only makes a difference when selecting a Python download or iterating over managed Python installations."
      ],
      "uv-environment-variable-best-practices": [
        "Thanks Ed!",
        "Similar to https://github.com/astral-sh/uv/pull/12246/files#r1999551644\r\n\r\n```suggestion\r\n    /// Only use uv-managed Python distributions.\r\n```\r\n\r\n(Sort of on the fence about \"installations\" vs \"distributions\" here but don't want users to be confused about whether we are \"installing\" here)\r\n\r\n",
        "I guess \"versions\" is actually the most consistent with other language?",
        "I wonder if we should also mention that this is equivalent to the `python-preference = \"only-managed\"` setting? I'm not sure. We might want to reference `python-preference` though unless we intend to later add a `managed-python = true | false` option to the settings to replace `python-preference`?",
        "I also think a `UV_MANAGED_PYTHON=1|0` variable might makes sense for these new options. It seems easier to use than `UV_PYTHON_PREFERENCE`.",
        "> It would be nice to use the same language consistently.\r\n\r\nYeah, but there are some places where it might be clearer to refer to installations or distributions. I think we should probably use \"versions\" when it's not confusing. It'd be good to audit other usages too, I'm sure there are more cases where we could switch to \"versions\".\r\n\r\nI think I'd prefer updating the ones here then opening an issue to standardize the language.\r\n\r\n> I'm hesitant to reference python-preference because the user might then try to learn two different ways to do the same thing \r\n\r\nSince there's not a configuration file setting for `managed-python`, I think it makes sense to explain how it's related? I wouldn't suggest the `--python-preference` CLI option.\r\n\r\n> I considered an env var, but the problem is we have a third option as a default.\r\n\r\nThinking out loud here...\r\n\r\nI assumed `UV_MANAGED_PYTHON=1` means `--managed-python` and `UV_MANAGED_PYTHON=0` means `--no-managed-python` while unset means default.\r\n\r\nLooking at some of our existing patterns, we do use `UV_NO_...` sometimes. I think most of them don't have an equivalent \"yes\" flag though. I might be okay with `UV_MANAGED_PYTHON=1` means `--managed-python` and `UV_NO_MANAGED_PYTHON=1` means `--no-managed-python`. I think `UV_MANAGED_PYTHON=0` not working would be surprising. That's how other boolean flags work today though, e.g.:\r\n\r\n```\r\n❯ UV_SYSTEM_PYTHON=0 uv python find\r\n/opt/homebrew/opt/python@3.13/bin/python3.13\r\n```\r\n\r\nbut as you said, that's for an option that's truly binary.\r\n\r\nI think I've loosely talked myself into using `UV_NO_MANAGED_PYTHON` / `UV_MANAGED_PYTHON` :)\r\n\r\n\r\n ",
        "```suggestion\r\n    /// Disable GitHub-specific requests that allow uv to skip `git fetch` in some circumstances.\r\n```"
      ],
      "uv-structure-for-readability": [
        "A dedicated `StandaloneIntepreter` type might be helpful to keep the logic about the `executable` to use in the `uv-python` crate."
      ],
      "uv-consistent-authentication-patterns": [
        "Does this work with `UV_PUBLISH_TOKEN`? Do they just ignore the username?",
        "Ah I see this is mentioned above. Maybe we can elevate that? https://github.com/astral-sh/uv/pull/14253/files#r2175124341",
        "```suggestion\r\n!!! important\r\n\r\n    If you use `--token \"$JFROG_TOKEN\"` or `UV_PUBLISH_TOKEN` with JFrog, you will receive a\r\n    401 Unauthorized error as JFrog requires an empty username but uv passes `__token__` for as\r\n    the username when `--token` is used.\r\n```"
      ],
      "uv-test-deployment-edge-cases": [
        "I'm wary of adding a new test suite for this file, it's a big increase in scope for this task.",
        "(I'll see how painful it is though, especially with https://github.com/astral-sh/uv/pull/14184#discussion_r2175069753 this gets complicated to trust without tests)"
      ],
      "uv-optimize-cicd-commands": [
        "This is a good example of the breakage we'd cause with this change. I wonder if we can craft a GitHub search that captures if a second `uv venv` is common?",
        "We could grab the JSON output, I think — but the `readarray` reads clearer to me.",
        "uhh maybe we should attest the DockerHub ones, I'm not sure how if it works tbh.",
        "I think I'd prefer to do that afterwards",
        "I'm not sure we _want_ incremental builds in the release pipeline. A clean build seems like a good property, right?"
      ],
      "uv-names-should-be-descriptive": [
        "Nit: We use `s` as the name pretty consistently in this repository, it'd be nice to retain the style there.",
        "I think it's common for us to just write `from(...) -> Option<...>`, I don't think you need to encode the `maybe` in the name when the type system captures that.",
        "Agree that's a little confusing, but we do it pretty often and I'd prioritize consistency with the rest of the codebase.",
        "I'm loosely opposed to changing this, because we use this argument name all over the place. We do use `python_request` in some places? which you could use since you'll just shadow it later? I don't have strong feelings. It seems like this does improve readability here, but may be confusing if you're coming from elsewhere.",
        "Nit: Generally we don't shorten variable names, I'd just use `tool_request` or `tool_python_request`.",
        "Can you rename this variable to match, e.g., `with_requirements`?",
        "The name isn't great... open to suggestions.",
        "I like that",
        "Nit: Similar to https://github.com/astral-sh/uv/pull/12651/files#r2027441564, it seems fine to drop the `auth_` prefix here since these are the only indexes which exist in this context.",
        "(I think this applies broadly)"
      ],
      "uv-clear-precise-documentation": [
        "I'm not sure how we'll want to reframe the documentation here.\r\n\r\nMaybe we want something like..\r\n\r\n> Run with all packages listed in the given requirements files.\r\n>\r\n> The following formats are accepted:\r\n>\r\n> - `requirements.txt`-formatted files\r\n> - `pyproject.toml`\r\n> - `setup.cfg` and `setup.py`\r\n> - `.py` files with PEP 723 metadata\r\n\r\nI'm not sure if we want to enumerate those everywhere? Are they all supported everywhere?",
        "Looks outdated :) we can probably update those holistically separately if you prefer.",
        "During upgrade, this is the directory to look for installations in, right?",
        "Should we change the doc then?",
        "Perhaps \"The directory Python installations are stored in\"?",
        "I think this is kind of confusing as written\r\n\r\n```suggestion\r\n    /// During an upgrade, uv will not uninstall outdated patch versions.\r\n```",
        "This is my first instinct\r\n\r\n```\r\n/// The environment was checked and required no updates.\r\n/// The environment was updated.\r\n/// A new environment was created.\r\n```",
        "I think I agree these should all be past-tense? Dry-run fills in the blank \"The environment would be ___\"",
        "We might want to say \"additional\" here (as discussed elsewhere). If you want to update that language all at once, it can happen here or in a separate pull requests.",
        "I think it's also fine to just say \"requirements\" without the floating (s)",
        "```suggestion\r\n    /// Whether to display the additional requirements installed with each tool.\r\n```\r\n\r\n(will require generated reference update)",
        "I would re-phrase (I think we use \"Whether\" when we only show one of the options)\r\n\r\n```suggestion\r\n    /// Disable use of uv-managed Python distributions.\r\n```\r\nI think we could say a bit more here, too, like\r\n\r\n> Instead, uv will search for a suitable Python installation on the system.\r\n\r\n\r\n",
        "See https://github.com/astral-sh/uv/pull/12246/files#r1999556537 — maybe we want to say \"Python versions\".",
        "```suggestion\r\n    /// Only show the version\r\n    ///\r\n    /// By default, uv will show the project name before the version.\r\n```"
      ],
      "uv-balance-test-performance-considerations": [
        "Given this test won't really change a lot, should we just check if commit info is available and have two snapshots? Using filters this way makes me a bit uncomfortable.",
        "They're just annoying during snapshot updates, so I try to avoid them in most cases.",
        "but if the filter is replacing basically the entire expected output, I'd either just skip the test or have a conditional.",
        "Yes, but we should squat or advocate for those names to be banned.",
        "It's just as bad, really. We try to make them reproducible wherever possible. That's mostly w.r.t. the external world though, internal changes causing snapshot changes are fine.\r\n\r\nHowever, I think this specific case is fine.",
        "You can use `.assert().success();` instead to avoid an empty snapshot.\r\n\r\nWe should test `python find --project` _before_ creating the virtual environment. In that case, we should ensure we're respecting the `requires-python` range from the child `pyproject.toml`.\r\n\r\nThen, we should test with a virtual environment.\r\n\r\nThen, we should also test we respect the `--project` root for `.python-version` file discovery too."
      ],
      "uv-make-errors-user-actionable": [
        "Huh? Why is there a reference to a workspace at all here? This doesn't seem quite right, and seems bad enough that we need to improve it here because we're recommending installing a workspace member?",
        "Yeah it seems like we may want to drop that and open an issue to follow up on it?\r\n\r\n> In general even a single package is a workspace\r\n\r\nEven if this is true internally, we shouldn't leak that to the user. Most users don't need or use workspaces.",
        "Discussion on inclusion of the wheel filename is happening at https://github.com/astral-sh/uv/pull/13437#discussion_r2132744173",
        "👍 I'm happy to hand this off to you, unless you want me to write the version display.",
        "Why did this one change? There's no policy set here.",
        "I don't think we can throw an error there though, doesn't downstream logic rely on a successful request? (i.e., in https://github.com/astral-sh/uv/pull/12667#discussion_r2029115491)\r\n\r\nPerhaps I misunderstood the intent of this PR?",
        "Ah I understand the goal now, you want to make the case where no credentials were present distinct from the case where the credentials are wrong. Sorry it took me a while to get there. I think the snapshot changes should be like..\r\n\r\n> hint: An index URL (https://pypi-proxy.fly.dev/basic-auth/simple) could not be queried due to a lack of valid authentication credentials (401 Unauthorized).\r\n\r\nto \r\n\r\n> hint: An index URL (https://pypi-proxy.fly.dev/basic-auth/simple) could not be queried due to missing credentials (401 Unauthorized)\r\n\r\nand\r\n\r\n> hint: An index URL (https://pypi-proxy.fly.dev/basic-auth/simple) could not be queried due to invalid credentials (401 Unauthorized)\r\n\r\nI think changing that error path entirely feels too risky and is hopefully unnecessary. \r\n",
        "(fwiw, I think this would have been clearer if the title was something like \"Distinguish between authentication failures due to missing vs invalid credentials\")",
        "Should this be \"Failed to determine the libc used on the current platform\"? This message seems too generic for a libc-detection specific problem.",
        "As structured, it needs to be present here or it's too broad / inaccurate. I also don't expect devs to know about libc flavors, but there should be an error above this explaining why we're looking for it. I'm also fine with it being structured differently.",
        "I think this is too specific? This can be raised for any managed Python error.\r\n\r\n```suggestion\r\n    #[error(\"Failed to discover managed Python installations\")]\r\n```\r\n\r\nIt seems misleading to suggest it's specifically related to \"matching the current platform\"."
      ]
    },
    "profile": {
      "location": "Minneapolis, MN",
      "company": "@astral-sh",
      "blog": "",
      "site_admin": false,
      "followers": 722,
      "following": 4
    }
  },
  "ste93cry": {
    "repos": [
      "getsentry/sentry-php"
    ],
    "entries": [
      {
        "slug": "sentry-php-balance-ci-test-coverage",
        "title": "Balance CI test coverage"
      },
      {
        "slug": "sentry-php-descriptive-identifier-naming",
        "title": "Descriptive identifier naming"
      },
      {
        "slug": "sentry-php-document-api-changes",
        "title": "Document API changes"
      },
      {
        "slug": "sentry-php-document-configuration-comprehensively",
        "title": "Document configuration comprehensively"
      },
      {
        "slug": "sentry-php-ensure-test-isolation",
        "title": "Ensure test isolation"
      },
      {
        "slug": "sentry-php-evolve-api-safely",
        "title": "Evolve API safely"
      },
      {
        "slug": "sentry-php-explicit-null-handling",
        "title": "Explicit null handling"
      },
      {
        "slug": "sentry-php-flexible-configuration-formats",
        "title": "Flexible configuration formats"
      },
      {
        "slug": "sentry-php-include-practical-examples",
        "title": "Include practical examples"
      },
      {
        "slug": "sentry-php-optimize-regex-patterns",
        "title": "Optimize regex patterns"
      },
      {
        "slug": "sentry-php-precise-dependency-versioning",
        "title": "Precise dependency versioning"
      },
      {
        "slug": "sentry-php-propagate-errors-with-context",
        "title": "Propagate errors with context"
      },
      {
        "slug": "sentry-php-purposeful-documentation-standards",
        "title": "Purposeful documentation standards"
      },
      {
        "slug": "sentry-php-secure-dependency-constraints",
        "title": "Secure dependency constraints"
      },
      {
        "slug": "sentry-php-split-for-better-readability",
        "title": "Split for better readability"
      },
      {
        "slug": "sentry-php-use-data-providers-effectively",
        "title": "Use data providers effectively"
      }
    ],
    "comments": {
      "sentry-php-document-api-changes": [
        "```suggestion\r\n- Make the `StacktraceBuilder` class part of the public API and add the `Client::getStacktraceBuilder()` method to build custom stacktraces (#1124)\r\n```",
        "I don't think we should as the intention of this PR is to move away from HTTPlug as much as possible, so suggesting to still use it is a countersense, isn't it?"
      ],
      "sentry-php-split-for-better-readability": [
        "Usually inline comments like this one are formatted on a single line, e.g. `/** @see ... */`",
        ":pray: format the DocBlock so that it's multiline to keep consistency with the rest of the codebase",
        "Nice to know, would you be so kind to add it to the PHPCS config to avoid further issues in the future?",
        "Please change this description to something like `Helper method to create an instance of this class from an array of data`. Also, as a convention we try to not write comments longer than 80 characters (it's a soft limit anyway, nothing enforced strictly) but if this is the case please consider splitting it into multiple lines to improve readability",
        "Please split this array into multiple lines (have a look at how other data providers are formatted)",
        "I meant the data provider of this file. You should format the arrays one item per line if possible, e.g.\r\n\r\n```php\r\n[\r\n    [\r\n        'foo @bar',\r\n        [\r\n            '@bar' => 'bar',\r\n        ],\r\n        'foo bar',\r\n    ],\r\n    [\r\n        'message' => 'foo @bar',\r\n        'params' => [\r\n            '@bar' => 'bar',\r\n        ],\r\n        'formatted' => 'foo bar',\r\n    ],\r\n]\r\n```",
        "Thank you for reporting this, some inconsistencies are indeed possible since style is not forced by PHPCS-Fixer. However I would prefer usage of the \"expanded\" version",
        "Please keep all `private` functions after `public` ones",
        "Please split this docblock into multiple lines",
        "Please split this docblock into multiple lines",
        "Please split this docblock into multiple lines",
        "Please try to split the descriptions comments so that each line length is around 80 characters at most",
        "As a \"soft\" limit usually the descriptions of the methods goes to a new line after ~ 80 chars"
      ],
      "sentry-php-precise-dependency-versioning": [
        "What's the reason for requiring the version `1.6`? If there is no reason besides mentioning the most recent version, I would leave the code as before",
        "Ok, definitely makes sense!",
        "Right, I didn't think about this. I relaxed the constraint to allow both version `2.x` and `3.x` to be installed",
        "Yes I know, but `3.x` does not support Symfony `3.4` anymore and it didn't make sense to do more work to have two distinct config files as the older version is unmaintained and we always run the tool with the newest dependencies"
      ],
      "sentry-php-ensure-test-isolation": [
        "This test description is wrong and doesn't respect what the code does. Also to just check that the handler isn't instantiated if the parameter is wrong isn't a normal PHPUnit test with `expectException` enough?",
        "Then I believe that every single test of the error handler that we have in PHPUnit should be moved to PHPT or we should somehow (e.g. with reflection) unset the singleton variable or we will leak state between multiple tests and this is not good. Since we made the handler singleton here it's the correct PR to do it, regardless of the solution we choose"
      ],
      "sentry-php-optimize-regex-patterns": [
        "We can use a non-capturing group here so that we avoid some allocations. Also, the regex should take into account the `:[number]` part before the `$`, so it should be changed to something along the lines of (untested): `(?::\\d+\\$|0x)[a-fA-F0-9]+$`",
        "The memory address is in hex format, so a stricter regex is `/0x[a-fA-F0-9]+$/`",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole"
      ],
      "sentry-php-explicit-null-handling": [
        "I would find more appropriate to check that the client is not `null` because the the real meaning of this line is to ensure that that instance is not `null` and not that it implements the interface. It is a small change indeed, but imo it conveyes to the reader the expectation in a clearer way.",
        "Can you please avoid assigning the variable in the `if` statement and check for `null` explicitly?",
        "Please typehint the return tpe as `?callable`",
        "Always be explicit about the check, so `null !== $client`",
        "Always be explicit about the check, so `null !== $client`",
        "Why having this argument to be `null` by default instead of an empty `array`? I think that if we want to leave it as-is, then we should make the argument `nullable` for consistency: it doesn't make sense that an argument cannot accept `null`, but its default value is `null`"
      ],
      "sentry-php-balance-ci-test-coverage": [
        "Why are you running PHPT tests without code coverage? As far as I remember, it worked fine and in fact Codecov now complains that the `ErrorHandler` is untested",
        "If we don't want to run the bench except for ensuring that it still works (not sure why it should not though, even in the future), wouldn't make more sense to have a separate workflow that can run every now and then?",
        "My suggestion was not related to the time spent running the step, but rather to the fact that if this step is here just to ensure that it runs, then it doesn't make sense to have it as part of the CI workflow",
        "With \"CI workflow\" I meant the workflow that runs for every push or for every PR. Do you think that running it once per week (just as an example) would still not be enough?",
        "Ok then, let's keep it as-is",
        "Instead of installing a specific version of PHP we should use the `matrix` feature of AppVeyor (similar to the one of Travis CI) to test multiple versions and fail as early as possible using `fast_finish`",
        ">That would be really slow\r\n\r\nSince Travis CI would run in parallel with AppVeyor it would not be much slower imho. The point in testing all versions is that you may have a bug in a specific version of PHP for Windows which may not exists in Linux"
      ],
      "sentry-php-use-data-providers-effectively": [
        "You should split the test into two, because only one exception can be thrown at time. Furthermore, `expectException()` is in no case cumulative. I suggest to rename this test method to `testHubAdapterThrowsExceptionOnSerialization`, and add `testHubAdapterThrowsExceptionOnUnserialization`",
        "We could hardcode the argument passed to `unserialize()` to be `O:23:\"Sentry\\State\\HubAdapter\":0:{}`",
        "Instead of having two test methods for the same thing, what about changing the `testConstructor()` method to use a data provider that returns first `null` and then a `float` and then compare those values with the result of `getTimestamp()`? You can use the `Symfony\\Bridge\\PhpUnit\\ClockMock` class to mock the current time to a specific value in the test cases that needs it. There should already be a test that does something similar somewhere in the codebase 😃 ",
        "What about merging this test and the one below using a data provider?",
        "Instad of splitting the test into two different methods, simply use the one above with a data provider that provides the input `$data` parameter to pass to the `fromArray` method and an `$expectedResult` that can be compared to what the `toArray` method returns",
        "Can we refactor the `testToArrayWithMessage` method to use a data provider and unify that test case with this one?",
        "Instead of having multiple test conditions into a single test case please use a data provider",
        "Assuming that this test case covers the case in which a `message_formatted` param is specified in the payload, please use the `sprintf` notation for the value of the `message` key and a totally different message for the value of the `formatted` key so that we can catch if the override works"
      ],
      "sentry-php-descriptive-identifier-naming": [
        "I'm not a fan of abbreviations and acronyms when writing code. What about renaming this variable to `samplingContext`?",
        "Nitpick: I think that renaming this variable to `$samplingContext` would be clearer for the reader in terms of cognitive load",
        "As previously we were serializing any `iterable` types I think that it would be more approprite to change the typehint accordingly. Also :pray: change `$array` to `$input` to keep consistency with the rest of the test methods of this class",
        "Please rename the `$key` variable to `$className`",
        "Please rename the parameter to `$data`",
        "Please rename this variable to something that better reflect what it stores (e.g. `$setMessageMethodArguments`)",
        "Please rename this variable to something that better reflect what it stores (e.g. `$setMessageMethodArguments`)",
        "Maybe the name `$handlerInstance` is clearer?"
      ],
      "sentry-php-evolve-api-safely": [
        "We cannot mark an already existing method as `@internal` or we may be tempted in the future to make changes without worrying about BC when we must instead since it was part of the public API in the past. Just leave the note instead with a link to the issue and remove the comment from the parameter instead",
        "This is a huge breaking change, which is not acceptable without releasing a new major version. Rather than adding this argument for real, you can comment it and handle it with `func_get_arg` in the implementation class to avoid the break: we already did it in the past and it worked even though it was a bit messy",
        "Although having the `clear` word in the method's name is fine for me, Unified API specs talks about a `removeUser` method so we should stick with it. But if @HazAT is fine with changing such specs (I don't know if any of the other SDKs implement this method) we may leave it as is or rename it to `clearUserContext` (but this would be inconsistent with the rest of the methods, which I would have named with the `Context` suffix by the way)",
        "I had a look at the other SDKs and I read again the documentation and I think I misunderstood what was written there. Basically, the documentation suggests as a way to remove data from a context the `remove*` method. However this method should not clear the whole context, but just remove a given item from it, so can you please update the code to selectively unset a certain key of the context?",
        "I think that you are right in saying that generally speaking the specs should be more clear on how these methods should act, because in certain parts they are and in others they are not. I suggest to go with the `removeUser` method that unsets a given key and not the whole context, as it may turn useful for example if something out of the control of the user sets some information on a parent scope and without such method you would have no way to remove it because inner scopes just inherit all data. We can add a `clearUser` context if we see that people need it",
        "I'm sorry to change one more time my mind, but since the discussion in the referenced issue may last for a long time, let's drop the `removeUser` method so that we can merge this PR as-is and if something will change in the future regarding this topic we will update the SDK accordingly",
        "You are still able to set both the SDK identifier and version using the appropriate methods `setSdkIdentifier` and `setSdkVersion`. The only thing this method did was to automate retrieving the version of a package using the `PrettyVersions` class, which can be done from the user so that we can simplify a bit our code"
      ],
      "sentry-php-propagate-errors-with-context": [
        "Instead of catching the entire code of the method we should just wrap the `wait` function call. The same should be done in the `cleanupPendingRequests` method. The strange thing is that when `false` is passed to the `wait` method no exceptions should be thrown but just returned (effectively doing the same as a `try`/`catch` block), so there is a bug somewhere in the vendor or the exception is not raised there (the other point where it could be raised is the `sendAsyncRequest` method, but since we saw that the request is not sent until `wait` is called I don't think it's the case)",
        "It probably makes more sense to catch `\\Throwable` instead of `\\Exception`",
        "This `try`/`catch` block is misleading, since it's catching any exception to rethrow with a generic message that says that the cURL client must be installed while the problem may be something else entirely. What about removing it and just let a possible error to be thrown?"
      ],
      "sentry-php-flexible-configuration-formats": [
        "While we're at it, can you please extract this variable into a `private` class constant named `HEADERS_TO_SANITIZE`?",
        "I don't have a strong contrary opinion about having a configurable set of options, although I would prefer to implement them as options of the integration itself rather than of the SDK, but since I also believe that Relay is the right tool for this job and knowing the we said these kind of things should not be in the SDKs, I will let you decide how to proceed @HazAT",
        "So, I've talked on Discord with @HazAT and we agreed to expose this option as option of the integration itself. You can take a look at the `IgnoreErrorsIntegration` class to see how we implemented something similar 😃 I will review again the code once you submit all the new changes",
        "What's the reason this option can be all those things? Wouldn't a simple `callable` be enough? If someone wants to use a class he can either code it to make it self-invokable or he can wrap the call to the class into the `callable` itself",
        "Please typehint the parameter as `?callable`",
        "Missing the validation of the option, `string|callable|object` should work"
      ],
      "sentry-php-document-configuration-comprehensively": [
        "Maybe it would be better to squash together these two lines? Otherwise, the user will first read that the default value was changed and then that the option has been deprecated and he may think that the two things are unrelated or he may be confused because it seems that they are in conflict",
        "Even better, I suggest ``Add `in_app_include` option to whitelist paths that should be marked as part of the app (#909)``"
      ],
      "sentry-php-include-practical-examples": [
        "What do you think about changing this to ``Trim the file path of the frames of the stacktrace that refer to an anonymous class according to the `prefixes` option``?",
        "I'm open to suggestions because the current entry doesn't looks clear enough to explain the change",
        "The reason I'm asking this is that it doesn't really makes sense that a file path gets trimmed from a class. What about ``Trim the file path from the anonymous class name in the stacktrace according to the `prefixes` option`` instead?",
        "There is a space at the start of the line that should not exists. Also I would change the sentence to `The suggested way to create your own instance of the client is to use the provided builder that will take care of instantiating a few dependencies like the PSR-7 factories and the HTTP client`"
      ],
      "sentry-php-secure-dependency-constraints": [
        "Please require version `^1.8.4|^2.1.1` as before: it was that way because previous versions had security issues, and in this way you are letting users install them which is bad",
        ">fair point, we had this discussion internally how we should go about external dependencies and if we should dictate which version users should have. We decided to make it on to the user to pump deps accordingly.\r\n\r\nI understand, but you should definitely dictate which version users should have in case the dependency is used directly in your code and isn't just transient. In this case, the code in this SDK actually uses a package affected by [`CVE-2022-24775`](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2022-24775), and in the end it is your package that is vulnerable, not the project of the user."
      ],
      "sentry-php-purposeful-documentation-standards": [
        "Can you please add a little description of what this function does, e.g. `Sets additional optional context data.`?",
        "Can you please add a little description of what this function does, e.g. `Gets additional optional context data.`?",
        "The description is not accurate, the fact that this object may contain the original exception is just a detail, we should describe what the `$hint` is and not what it contains",
        "There is no much value in having this annotation without a description of the parameter and of the function. Please either add them or drop the DocBlock altogether",
        "Please mark this class as `final` and add a small DocBlock to explain what it does",
        "Please add the docblock with the description of what this class does, e.g. `Default implementation of the {@HttpClientFactoryInterface} interface that uses Httplug to autodiscover the HTTP client if none is passed by the user.`",
        "Please change the description to: `Gets the callbacks used to customize how objects are serialized in the payload of the event.`",
        "Add a description to this interface: `This interface can be used to customize how an object is serialized in the payload of an event.`",
        "This sentence should describe what the option does rather than just say what it returns. I would change it to something like `Gets whether the silenced errors should be captured or not`",
        "This sentence should describe what the option does rather than just say that it sets the option named X. I would change it to something like `Sets whether the silenced errors should be captured or not`"
      ]
    },
    "profile": {
      "location": "Padova, Italy",
      "company": "@scayle",
      "blog": "http://www.stefanoarlandini.it",
      "site_admin": false,
      "followers": 56,
      "following": 260
    }
  },
  "agibsonccc": {
    "repos": [
      "deeplearning4j/deeplearning4j"
    ],
    "entries": [
      {
        "slug": "deeplearning4j-always-secure-your-locks",
        "title": "Always secure your locks"
      },
      {
        "slug": "deeplearning4j-centralize-dependency-management",
        "title": "Centralize dependency management"
      },
      {
        "slug": "deeplearning4j-clean-up-your-code",
        "title": "Clean up your code"
      },
      {
        "slug": "deeplearning4j-compare-floating-point-safely",
        "title": "Compare floating-point safely"
      },
      {
        "slug": "deeplearning4j-configurable-resource-locations",
        "title": "Configurable resource locations"
      },
      {
        "slug": "deeplearning4j-cross-platform-algorithm-optimization",
        "title": "Cross-platform algorithm optimization"
      },
      {
        "slug": "deeplearning4j-descriptive-error-context",
        "title": "Descriptive error context"
      },
      {
        "slug": "deeplearning4j-document-in-code-decisions",
        "title": "Document in-code decisions"
      },
      {
        "slug": "deeplearning4j-eliminate-redundant-code",
        "title": "Eliminate redundant code"
      },
      {
        "slug": "deeplearning4j-ensure-test-determinism",
        "title": "Ensure test determinism"
      },
      {
        "slug": "deeplearning4j-keep-configurations-current",
        "title": "Keep configurations current"
      },
      {
        "slug": "deeplearning4j-modular-adaptive-configurations",
        "title": "Modular adaptive configurations"
      },
      {
        "slug": "deeplearning4j-optimize-validation-checks",
        "title": "Optimize validation checks"
      },
      {
        "slug": "deeplearning4j-remove-debugging-artifacts",
        "title": "Remove debugging artifacts"
      },
      {
        "slug": "deeplearning4j-use-logging-best-practices",
        "title": "Use logging best practices"
      },
      {
        "slug": "deeplearning4j-validate-and-document-nulls",
        "title": "Validate and document nulls"
      }
    ],
    "comments": {
      "deeplearning4j-configurable-resource-locations": [
        "@treo I added this based on the model import code. Since I'm attempting to use the new op descriptors in the dynamic custom ops properties now, I wanted an easier/faster way to access the descriptors. The only problems I could think of with this might be graalvm, but if there are I will address those later.",
        "@treo in that case we can tell people to just override the classloader with their own. That functionality was merged a while back. I agree with you and will keep an eye on that though."
      ],
      "deeplearning4j-use-logging-best-practices": [
        "Fixed log levels."
      ],
      "deeplearning4j-keep-configurations-current": [
        "Not yet. Good point though. Let me ping them. Since it's MIT it should be ok.",
        "Mind explaining what vecpp is and ensure it's documented in the readme? My best guess is some something like a configure.in that we manipulate with some parameters?",
        "Removed.",
        "@treo MKLMKL was deprecated long ago. Look at the latest publish: https://anaconda.org/anaconda/mklml MKLDNN became onednn and is now what's shipped with oneapi.",
        "@treo removed the reference here. Feel free to check the source tree where the mkldnn compat layer is bundled: https://github.com/KonduitAI/oneDNN/blob/9cd49946cddb6936d12963d94eb1dbb2170ff3ac/include/mkldnn.h"
      ],
      "deeplearning4j-always-secure-your-locks": [
        "Updates here?"
      ],
      "deeplearning4j-ensure-test-determinism": [
        "Mind adding some comments about how important the seed is here? A log of issues we ran in to with certain tests is handy."
      ],
      "deeplearning4j-centralize-dependency-management": [
        "Done",
        "@saudet  ah good catch, jackson-core and databind are both at 2.12.2. Just pushed the change there.",
        "Create a separate version maybe jboss.netty3.version and io.netty?",
        "Mind adding dependency versions under dependency management in the top level pom? We do this to avoid dependency conflicts across modules.\r\nDo this with \r\n<dependency>\r\n            <groupId>org.bytedeco.javacpp-presets</groupId>\r\n            <artifactId>cpython-platform</artifactId>\r\n            <version>3.6-1.4.4-SNAPSHOT</version>\r\n        </dependency>\r\n        <dependency> \r\n\r\n\r\nconverting to:\r\n<dependency>\r\n            <groupId>org.bytedeco.javacpp-presets</groupId>\r\n            <artifactId>cpython-platform</artifactId>\r\n        </dependency>\r\n        <dependency>\\\r\n\r\nputting the previous declaration under a dependency management section.\r\n\r\nAlso, please version the python and javacpp versions with variables at the top level.",
        "I wonder if a <dependencyManagement> entry would be better here?"
      ],
      "deeplearning4j-modular-adaptive-configurations": [
        "@treo  that was mainly due to environment variable/GH actions debugging. I refactored based on your comments there."
      ],
      "deeplearning4j-cross-platform-algorithm-optimization": [
        "Could you layout where the in files are used and how it works?",
        "@quickwritereader  just make sure to put this in the actual code itself, maybe even in a readme.",
        "Mind adding some declarations here so we know what versions of methods are being invoked? nd4j_debug should be fine here",
        "The point was more to put this comment you just did in the comments so it's in the code and people don't have to search github to find context  :)"
      ],
      "deeplearning4j-eliminate-redundant-code": [
        "I think it just accumulated. I'll work on stripping that down."
      ],
      "deeplearning4j-clean-up-your-code": [
        "Done. Sorry missed that."
      ],
      "deeplearning4j-remove-debugging-artifacts": [
        "Fixed"
      ],
      "deeplearning4j-descriptive-error-context": [
        "Mind putting a location of the error here like: ShapeList:: error message similar to what we do in other error messages? That helps track down the problem.",
        "Are these supposed to be stubs?",
        "Mind adding error messages to these? Otherwise it will be hard to debug (especially from java)",
        "For sure! I was just mentioning that because anyone at the java level debugging something from c++ won't really want to have to click in to and read c++ source code and to try to reverse engineer what the problem is."
      ],
      "deeplearning4j-compare-floating-point-safely": [
        "Fixed"
      ],
      "deeplearning4j-validate-and-document-nulls": [
        "Null comes mostly come the Switch op where 1 branch is null and the other isn't. I'd want to think on this one a bit. I'm not clear if there's anything in specific I'd want to do with the null case beyond that."
      ],
      "deeplearning4j-document-in-code-decisions": [
        "Describing some of the steps here would also be useful. Eg: what's \"special use\" for?",
        "@quickwritereader same thing: put it in the code, not on github",
        "Mind adding a minor comment paragraph explaining the issues found here so we know why we're defaulting to our implementation in certain cases?"
      ],
      "deeplearning4j-optimize-validation-checks": [
        "For performance reasons, I would imagine us wanting all of this behind an if debug. Do you mind implementing this?",
        "For performance reasons, I would imagine us wanting all of this behind an if debug. Do you mind implementing this?"
      ]
    },
    "profile": {
      "location": "Tokyo Japan",
      "company": "http://konduit.ai/",
      "blog": "http://konduit.ai/",
      "twitter_username": "agibsonccc",
      "site_admin": false,
      "followers": 559,
      "following": 109
    }
  },
  "knizhnik": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-cache-performance-preservation",
        "title": "Cache performance preservation"
      },
      {
        "slug": "neon-comprehensive-code-documentation",
        "title": "Comprehensive code documentation"
      },
      {
        "slug": "neon-configurable-cache-parameters",
        "title": "Configurable cache parameters"
      },
      {
        "slug": "neon-configuration-context-alignment",
        "title": "Configuration context alignment"
      },
      {
        "slug": "neon-database-replica-promotion-safeguards",
        "title": "Database replica promotion safeguards"
      },
      {
        "slug": "neon-extract-and-reuse",
        "title": "Extract and reuse"
      },
      {
        "slug": "neon-guard-against-race-conditions",
        "title": "Guard against race conditions"
      },
      {
        "slug": "neon-handle-all-error-paths",
        "title": "Handle all error paths"
      },
      {
        "slug": "neon-handle-network-interrupts-safely",
        "title": "Handle network interrupts safely"
      },
      {
        "slug": "neon-limit-concurrent-access-slots",
        "title": "Limit concurrent access slots"
      },
      {
        "slug": "neon-mind-transaction-boundaries",
        "title": "Mind transaction boundaries"
      },
      {
        "slug": "neon-optimize-data-structures",
        "title": "Optimize data structures"
      },
      {
        "slug": "neon-optimize-what-matters",
        "title": "Optimize what matters"
      },
      {
        "slug": "neon-performance-test-pragmatism",
        "title": "Performance test pragmatism"
      },
      {
        "slug": "neon-proactive-cache-warming",
        "title": "Proactive cache warming"
      }
    ],
    "comments": {
      "neon-optimize-what-matters": [
        "I do not think that this code is performance critical.\r\nSo it is better to keep it simpler and straightforward rather than try to optimise it.\r\nThis cold assignment can be easily avoided by doing `metrics[NUM_QT_BUCKETS].bucket_le = INFINITY` after the loop.  But pleas notice thatI have not written this function: it is just cut&paste of existed `histogram_to_metrics` function. If it needs to be optimised, then it should be done in separate PR.\r\n"
      ],
      "neon-proactive-cache-warming": [
        "+1 for  CPlane-triggered writes.\r\nAlternatively it can done as part of checkpoint. ",
        "I think dumping it in checkpoint is the best choice."
      ],
      "neon-configurable-cache-parameters": [
        "Will do, as I wrote - this constant is just temporary thing to illustrate approach. Definitely it needs to be replaced with parameter. I just didn't want to cut&paste a lot of boilerplate code before there is some consensus of how to solve this issue (if my proposal with two cache is accepted).",
        "Done",
        "Done"
      ],
      "neon-optimize-data-structures": [
        "I thought about it but preferred simpler implementation.\r\nBut after tbhinging for a while, I come to the conclusion that storing counter in `DdlHashTable` can even simplify things:\r\n1. We do not need to create all chain of `DdlHashTable`.\r\n2. We can allocate `DdlHashTable` in CurTransactionContext and do not worry about it's deallocation.\r\nSo thank you for the advice - I implemented this approach.",
        "> Does this not have off-by-one issues? I\r\n\r\nOne is subtracted when `subtrans_level` is assigned:\r\n```\r\nnew_table->subtrans_level = SubtransDdlLevel - 1;\r\n```\r\n\r\n> I'd prefer keeping a single counter, rather than one reset every time you push a table:\r\n\r\nOk, it seems to be more clear and straightforward. Will change.",
        "it can be done simpler using sscanf:\r\n```\r\nint pos;\r\nif (sscanf(safekeepers_list, \"g#%u:%n\", generation, &pos) == 1) { \r\n     return safekeepers_list + pos;\r\n} else {\r\n     return safekeepers_list;\r\n}\r\n```"
      ],
      "neon-limit-concurrent-access-slots": [
        "It is true for prewarm, but if we are going to use the same mechanism for storing prefetch results in LFC as soon as they arrived, number of writer ca. be as larger as number of backends (`max_connections`) which can be thousands.",
        "Once again, if this mechanism is used not only for prewarm, but for normal backends, then maintaining per-bakend state is non-disarable."
      ],
      "neon-mind-transaction-boundaries": [
        "I am absolutely against using `with` construction in tests.\r\nIt is really very convenient and safe in real python database applications.\r\nBut in case of test connection lifetime management is not critical - leaking connection is not a problem, it is in any case dropped when endpoint is restarted or testis finished.\r\n\r\nThe main problem with `with` construction which cause many problems: it implicitly start new transaction and even in auto commit all code in `with` body is executed in one transaction.\r\n\r\nBut please notice that I have also added your version of the test which uses this `with` constructions for connections.\r\n"
      ],
      "neon-database-replica-promotion-safeguards": [
        "The `replica_promote` flag is used only to disarm this check:\r\n```\r\n\t\t/*\r\n\t\t * Basebackup LSN always points to the beginning of the record (not\r\n\t\t * the page), as StartupXLOG most probably wants it this way.\r\n\t\t * Safekeepers don't skip header as they need continious stream of\r\n\t\t * data, so correct LSN for comparison.\r\n\t\t */\r\n\t\tif (SkipXLogPageHeader(wp, wp->propTermStartLsn) != wp->api.get_redo_start_lsn(wp))\r\n```\r\n\r\nIn our case redo_restart corresponds to old redo_restart LSN of replica.\r\nMy first attempt to fix the problem was to explicitly set this restart LSN using `SetRedoStartLsn()`\r\nBut it is not available in PG14.\r\nSo I decided just to disarm this check.\r\nI will be pleased if you can propose better solution.\r\n",
        "May be this check is failed because I disabled recovery:\r\n```\r\n\t/*\r\n\t * Consider whether we need to assign a new timeline ID.\r\n\t *\r\n\t * If we did archive recovery, we always assign a new ID.  This handles a\r\n\t * couple of issues.  If we stopped short of the end of WAL during\r\n\t * recovery, then we are clearly generating a new timeline and must assign\r\n\t * it a unique new ID.  Even if we ran to the end, modifying the current\r\n\t * last segment is problematic because it may result in trying to\r\n\t * overwrite an already-archived copy of that segment, and we encourage\r\n\t * DBAs to make their archive_commands reject that.  We can dodge the\r\n\t * problem by making the new active segment have a new timeline ID.\r\n\t *\r\n\t * In a normal crash recovery, we can just extend the timeline we were in.\r\n\t */\r\n\tnewTLI = endOfRecoveryInfo->lastRecTLI;\r\n-\tif (ArchiveRecoveryRequested)\r\n+\tif (ArchiveRecoveryRequested && !ZenithRecoveryRequested)\r\n```\r\nbut then we should somehow address the problem with timelines.\r\nI am not fan of this hack with adding ` !ZenithRecoveryRequested` - if there some better solution I will be glad to implement it.  But still not sure that bumping timeline on replica promotion=node restart is good idea.\r\n"
      ],
      "neon-configuration-context-alignment": [
        "Done",
        "Sorry, forget to save file."
      ],
      "neon-performance-test-pragmatism": [
        "Linux is not real time OS. And we are running tests in cloud. \r\nSo, especially if query execution time is relatively small (as in this case < 1 second), it is not possible to expect that execution time with optimization always be smaller than without it. Adding such assert is just a direct road to yet another fluky test. Certainly we can repeat query several times and calculate average time.  But IMHO it is overkill.\r\nBut may be such flukyness for performance tests is not so critical as for regression tests...\r\n",
        "Ok, done"
      ],
      "neon-extract-and-reuse": [
        "Done"
      ],
      "neon-cache-performance-preservation": [
        "Postgres can write WAL not only as result of some query execution.\r\nThere many background activities which cause writing WAL: checkpoint, vacuum,..."
      ],
      "neon-handle-all-error-paths": [
        "Please notice that right now this check is performed at the moment when response is assigned to slot by `check_getpage_response` function.\r\nThis is first thing that it does and panics if it is not true.",
        "Ok, will add assertion here.",
        "There are several cases which can cause incorrect handling of errors or interrupts: when connection is not dropped or it is dropped put prefetch ring state is not reset.\r\nActually what I have done:\r\n1. Move `consume_prefetch_responses` prior to sending new (non-getpage request)\r\n2. Block interrupts while preforming disconnect (CHECK_FOR_INTERRUPTS can be called from any logging function and can cause exit from this function without actually dripping connection)\r\n3. Register `before_shmem_exit` to handle terminate backend requests which are not handled by `PG_TRY/PG_CATCH`.",
        "FATAL errors are not matched by TRY/CATCH blocks. In this case proc_exit is called directly.\r\nSo we have not closed connection but still can communicate with PS in backend cleanup code (i.e. removing temp relations)"
      ],
      "neon-guard-against-race-conditions": [
        "Done",
        "This really strange check is used to ensure that shared memory is initialized and not detached (as done by PG14 for stat process).\r\nSo checking it with `IsUnderPostmaster` is not correct.\r\nSimilar check is done in several other places in our codeL:\r\n```\r\nfile_cache-inmem.c\u0000205:\tif (!lfc_ctl || !UsedShmemSegAddr || IsParallelWorker())\r\nfile_cache.c\u0000395:\t * SIGHUP and it has access to shared memory (UsedShmemSegAddr != NULL),\r\nfile_cache.c\u0000398:\treturn lfc_ctl && MyProc && UsedShmemSegAddr && !IsParallelWorker();\r\nlibpagestore.c\u0000183:\treturn pagestore_shared && UsedShmemSegAddr;\r\nwalproposer_pg.c\u0000314:\tif (newval && *newval != '\\0' && UsedShmemSegAddr && walprop_shared && RecoveryInProgress())\r\n```\r\nMay be it will be good to somehow refactor it and use single function for it.\r\n",
        "In stat process in PG14 `IsUnderPostmaster` is true, but shared memory is detached."
      ],
      "neon-handle-network-interrupts-safely": [
        "I do not think that pipelining of non-get page requests and get page request is really important. There are only two other frequent requests: exists and nblocks, but both should be eliminated in most cases  by resize cache.\r\n\r\nIs it absolutely necessary to move `consume_prefetch_responses` before sending new non-get page request?\r\nNo, it is not. If we can enforce that in case of error:\r\n1. Connection is dropped\r\n2. Prefetch ring state it reset\r\nthan it is not needed. The original error I have found is that this code calls `page_server->disconnect` which is not resetting prefetch state. My first attempt to fix it was just to add call of `prefetch_on_ps_disconnect()` here.\r\nBut then I thought that it will be safer to consume prefetch responses before sending new request.",
        "Just to clarify: I wanted to separate prefetch state (when there are some inflight `getpage` requests) and non-prefetch state (when we do classical server request-response call).\r\n",
        "Sorry, I do not fully agree with your analysis (or at least do not completely understand it).\r\nHow `page_server->receive() ` can got stuck waiting for new connection?\r\n`pageserver_receive` is not waiting for connection. It immediately returns NULL id state is not `PS_Connected`.\r\n\r\nThe problem I have reported is different. Assume that there is some pending interrupt - i.e. statement timeout.\r\nAnd it is invoked from `CHECK_FOR_INTERRUPTS` places in one of the functions called in TRY block in `page_server_request`. Inn this case CATCH block calls `page_server->disconnect(shard_no);`. But it actually calls `pageserver_disconnect_shard` which drop connection but doesn't reset prefetch state (doesn't call `prefetch_on_ps_disconnect`).\r\n"
      ],
      "neon-comprehensive-code-documentation": [
        "Done"
      ]
    },
    "profile": {
      "location": "Cyprus, Larnaca",
      "company": "Neon Tech",
      "blog": "www.garret.ru",
      "site_admin": false,
      "followers": 146,
      "following": 0
    }
  },
  "kamilmysliwiec": {
    "repos": [
      "nestjs/nest"
    ],
    "entries": [
      {
        "slug": "nest-benchmark-before-optimizing",
        "title": "Benchmark before optimizing"
      },
      {
        "slug": "nest-choose-meaningful-identifier-names",
        "title": "Choose meaningful identifier names"
      },
      {
        "slug": "nest-configurable-log-formatting",
        "title": "Configurable log formatting"
      },
      {
        "slug": "nest-descriptive-identifier-names",
        "title": "Descriptive identifier names"
      },
      {
        "slug": "nest-document-configuration-behaviors",
        "title": "Document configuration behaviors"
      },
      {
        "slug": "nest-explicit-default-configurations",
        "title": "Explicit default configurations"
      },
      {
        "slug": "nest-follow-protocol-standards",
        "title": "Follow protocol standards"
      },
      {
        "slug": "nest-graph-based-dependency-management",
        "title": "Graph-based dependency management"
      },
      {
        "slug": "nest-http-header-management",
        "title": "HTTP header management"
      },
      {
        "slug": "nest-modern-null-safety-patterns",
        "title": "Modern null safety patterns"
      },
      {
        "slug": "nest-optimize-critical-path-iterations",
        "title": "Optimize critical path iterations"
      },
      {
        "slug": "nest-package-dependency-configuration",
        "title": "Package dependency configuration"
      },
      {
        "slug": "nest-parameterize-version-requirements",
        "title": "Parameterize version requirements"
      },
      {
        "slug": "nest-pin-dependency-versions",
        "title": "Pin dependency versions"
      },
      {
        "slug": "nest-preserve-api-interface-stability",
        "title": "Preserve API interface stability"
      },
      {
        "slug": "nest-preserve-public-api-stability",
        "title": "Preserve public API stability"
      },
      {
        "slug": "nest-prevent-async-race-conditions",
        "title": "Prevent async race conditions"
      },
      {
        "slug": "nest-prevent-race-conditions",
        "title": "Prevent race conditions"
      },
      {
        "slug": "nest-proper-asynchronous-error-handling",
        "title": "Proper asynchronous error handling"
      },
      {
        "slug": "nest-secure-hash-algorithms",
        "title": "Secure hash algorithms"
      },
      {
        "slug": "nest-standardize-logger-configuration-patterns",
        "title": "Standardize logger configuration patterns"
      },
      {
        "slug": "nest-standardize-null-safety-patterns",
        "title": "Standardize null safety patterns"
      },
      {
        "slug": "nest-strategic-dependency-configuration",
        "title": "Strategic dependency configuration"
      },
      {
        "slug": "nest-structure-behavior-driven-tests-properly",
        "title": "Structure behavior-driven tests properly"
      },
      {
        "slug": "nest-structure-exception-handling-patterns",
        "title": "Structure exception handling patterns"
      },
      {
        "slug": "nest-use-consistent-control-structures",
        "title": "Use consistent control structures"
      },
      {
        "slug": "nest-use-consistent-curly-braces",
        "title": "Use consistent curly braces"
      },
      {
        "slug": "nest-use-factory-providers",
        "title": "Use factory providers"
      },
      {
        "slug": "nest-use-secure-hash-algorithms",
        "title": "Use secure hash algorithms"
      },
      {
        "slug": "nest-use-topological-sorting",
        "title": "Use topological sorting"
      }
    ],
    "comments": {
      "nest-use-consistent-control-structures": [
        "To remain consistent with the rest of the codebase\r\n```suggestion\r\n    if (areThereNoFileIn && this.fileIsRequired) {\r\n      throw this.exceptionFactory('File is required');\r\n    }\r\n    if (!areThereNoFileIn && this.validators.length) {\r\n      await this.validateFilesOrFile(value);\r\n    }\r\n```",
        "even though `void 0` is not necessarily required, please, let's stick with `{ ... }` at least"
      ],
      "nest-configurable-log-formatting": [
        "```suggestion\r\n  json?: boolean;\r\n```",
        "similarly to log levels, we should be able to specify (globally) that the logger should use `json` format for all logs by default ",
        "I think this line:\r\n\r\n```\r\n`${pidMessage}${this.getTimestamp()} ${formattedLogLevel} ${contextMessage}${output}${timestampDiff}\\n`\r\n```\r\nshould be configurable. We could declare another protected method that receives these variables as input parameters (without colors applied). Perhaps that's actually how the `formatMessage` should look likely (we can move everything else back to the original location)",
        "This approach https://github.com/stanimirovv/nest/commit/89d97652caafc26c379ae0268229c4fc3caf3cf2# better fits our needs but we should also allow opt-in coloring the messages. In this case, we should probably declare another dedicated method (let's say `colorize`) that takes the same set of arguments as `formatMessages` and adds colors. This method should be executed from within the `formatMessages` and have `protected` modifier to make it feasible to call it (if needed) from within the custom logger implementation.",
        "Yeah, I think as long as we make `getColorByLogLevel` protected as well (to make it accessible) it could take two arguments (one being a message to format and the second - optional - color to be used that defaults to `this.getColorByLogLever()`)"
      ],
      "nest-secure-hash-algorithms": [
        "Let's do this (since it's faster)"
      ],
      "nest-benchmark-before-optimizing": [
        "Wondering how fast it is compared to @napi-rs/blake-hash 🤔 \r\n\r\nAlso, on a side note, did you have a chance to compare https://github.com/Brooooooklyn/uuid to `@lukeed/uuid` (for generating uuids)?",
        "Why not filter + map? Performance should be marginal in this case (I'd think)",
        "Still, the performance impact should be negligible. filter & map is just cleaner & easier to read, not worth the optimization in this particular case "
      ],
      "nest-package-dependency-configuration": [
        "since we load this package lazily, i'd say we could probably declare `file-type` as a peer dependency instead",
        "we should also add the `peerDependenciesMeta` entry, flagging these deps as _optional_, see example here https://github.com/nestjs/graphql/blob/master/packages/apollo/package.json#L52"
      ],
      "nest-strategic-dependency-configuration": [
        "since we load this package lazily, i'd say we could probably declare `file-type` as a peer dependency instead",
        "we should also add the `peerDependenciesMeta` entry, flagging these deps as _optional_, see example here https://github.com/nestjs/graphql/blob/master/packages/apollo/package.json#L52"
      ],
      "nest-structure-exception-handling-patterns": [
        "Not sure if I'm following. `process.exit` (force exit) shouldn't be necessary. We're also overriding the signal (the reason to kill the process) here",
        "This doesn't work even if you run `node dist/main` (no NestJS CLI involved)",
        "![image](https://github.com/user-attachments/assets/734ee5f9-ccbf-4330-a059-87879374a61e)\r\n\r\nCode \r\n\r\n```js\r\nconsole.log('Hello Node.js');\r\n\r\nprocess.on('exit', () => {\r\n  console.log('about to exit');\r\n});\r\n\r\nsetTimeout(() => {\r\n  console.log('killing');\r\n  process.kill(process.pid, 'SIGTERM');\r\n}, 1000);\r\n```\r\n\r\nNode v20",
        "Ah apologies, I forgot to include on SIGTERM listener @thomaschaaf ",
        "I still cannot reproduce your issue with the following code:\r\n\r\n```ts\r\nimport { NestFactory } from '@nestjs/core';\r\nimport { AppModule } from './app.module';\r\n\r\nasync function bootstrap() {\r\n  const app = await NestFactory.create(AppModule);\r\n  app.enableShutdownHooks();\r\n  await app.listen(3000);\r\n\r\n  console.log(`Application is running on: ${await app.getUrl()}`);\r\n\r\n  process.on('SIGTERM', async () => {\r\n    await app.close();\r\n    console.log('SIGTERM received');\r\n    process.kill(process.pid, 'SIGTERM');\r\n  });\r\n\r\n  process.on('exit', () => {\r\n    console.log('about to exit');\r\n  });\r\n\r\n  setTimeout(() => {\r\n    console.log('killing');\r\n    process.kill(process.pid, 'SIGTERM');\r\n  }, 1000);\r\n}\r\nbootstrap();\r\n```\r\n\r\nWith `node dist/main`:\r\n\r\n```\r\nApplication is running on: http://[::1]:3000\r\nkilling\r\nSIGTERM received\r\nabout to exit\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/63336e26-2562-456d-8893-e4f3a46adb64)\r\n\r\nWith `nest start`\r\n\r\n![image](https://github.com/user-attachments/assets/dd1829a9-f323-4ab6-b77c-e3fdebdb65f7)\r\n\r\nSame with `nest start --watch` (npm run start:dev)\r\n\r\n![image](https://github.com/user-attachments/assets/446e55d1-44f2-4a22-89e6-11d61279fe50)\r\n\r\n",
        "I can replicate the exact same behavior using plain Node.js (without the Nest framework/CLI), so I'm not sure if there's anything we need to change on our end.",
        "Wouldn't it be easier to define an object in which keys represent status codes and their values = classes of corresponding exceptions? e.g. `new HttpErrorByCode[statusCode](error)`?",
        "This:\r\n\r\n```typescript\r\nexport const HttpErrorByCode = {\r\n  [HttpStatus.BAD_REQUEST]: BadRequestException,\r\n  [HttpStatus.UNPROCESSABLE_ENTITY]: UnprocessableEntityException,\r\n```\r\nand:\r\n```typescript\r\nthis.exceptionFactory = (errors => new HttpErrorByCode[this.exceptionCode](errors));\r\n```\r\nshould be enough :) Any additional method/function which leverages `HttpException` is not needed. In addition, we shouldn't allow using `OK` or `CREATED` status. Let's just define an union with error status codes that can be used, e.g.:\r\n\r\n```typescript\r\nexport type ErrorHttpStatusCode = HttpStatus.BAD_REQUEST | HttpStatus.UNPROCESSABLE_ENTITY | etc.\r\n```\r\n"
      ],
      "nest-modern-null-safety-patterns": [
        "Instead of adding a new condition here, we may just swap `||` expression with `??`.",
        "What do you think about using `isNil()` util function here instead? (to exclude `null` and `undefined` values)",
        "I think `isNil()` will be sufficient here :)"
      ],
      "nest-parameterize-version-requirements": [
        "These may quickly get out of date - we will keep supporting v16 even after it's no longer a maintenance version (after September this year). That's why versions were hardcoded before "
      ],
      "nest-document-configuration-behaviors": [
        "```suggestion\r\n```\r\nRedis adapter intentionally wasn't being used in this example (it was meant to be simple and not require additional resources, e.g. Redis db)",
        "> My concern is that the adapter is included in the sample, but it is not clear how to actually use it.\r\n\r\nFair enough! Let's comment these lines out and leave a brief explanation just in case."
      ],
      "nest-prevent-async-race-conditions": [
        "I think I misunderstood what this FR was about.\r\n\r\nDurable providers should not have access to the request-specific attributes (we shouldn't merge the payload with the request) simply because they are **durable** (meaning they are not removed after the request is finished processing). Merging the payload with request objects for durable providers will lead to memory leaks and unexpected behavior of the framework",
        "```suggestion\r\n      Object.assign(args[0] ?? {}, {\r\n        getPattern: () => this.reflectCallbackPattern(callback),\r\n      });\r\n```\r\nHmm.. I'm wondering if this won't cause some unexpected issues 🤔 If socket represents a single connection (and so its instance object is shared?), there's a probability that 2 messages from that socket might be processed asynchronously (independently). \r\n\r\nExample:\r\n- Connection to server is established (socket instance is created)\r\n- Message A is emitted \r\n  - we mutate the \"socket instance\" enhancing it with the \"getPattern()\" method\r\n  - before this message is processed, some asynchronous operation is triggered in (guard/interceptor/wherever)\r\n- In the meantime, message B is emitted\r\n  - we mutate the same \"socket instance\" again replacing the previous \"getPattern()\" method\r\n- Async operation (mentioned above) completes and we're back to processing message A.\r\n- In another interceptor/guard we call `client.getPattern()` when processing message A but it's already overridden with the message B's \"getPattern\" implementation\r\n\r\n",
        "If someone unsubscribes before this promise resolves, a memory leak would occur (`this.routingMap.set(packet.id, callback);`)",
        "@guiruiz it wouldn't because `serialize` isn't async and so `this.routingMap.set` is called synchronously."
      ],
      "nest-prevent-race-conditions": [
        "```suggestion\r\n      Object.assign(args[0] ?? {}, {\r\n        getPattern: () => this.reflectCallbackPattern(callback),\r\n      });\r\n```\r\nHmm.. I'm wondering if this won't cause some unexpected issues 🤔 If socket represents a single connection (and so its instance object is shared?), there's a probability that 2 messages from that socket might be processed asynchronously (independently). \r\n\r\nExample:\r\n- Connection to server is established (socket instance is created)\r\n- Message A is emitted \r\n  - we mutate the \"socket instance\" enhancing it with the \"getPattern()\" method\r\n  - before this message is processed, some asynchronous operation is triggered in (guard/interceptor/wherever)\r\n- In the meantime, message B is emitted\r\n  - we mutate the same \"socket instance\" again replacing the previous \"getPattern()\" method\r\n- Async operation (mentioned above) completes and we're back to processing message A.\r\n- In another interceptor/guard we call `client.getPattern()` when processing message A but it's already overridden with the message B's \"getPattern\" implementation\r\n\r\n",
        "If someone unsubscribes before this promise resolves, a memory leak would occur (`this.routingMap.set(packet.id, callback);`)",
        "@guiruiz it wouldn't because `serialize` isn't async and so `this.routingMap.set` is called synchronously.",
        "If someone applies 1 middleware multiple times, we should run it multiple times = no side-effects."
      ],
      "nest-structure-behavior-driven-tests-properly": [
        "Can we wrap it within a `describe(\"valideteUser\")` block and then inside have to scenarios covered:\r\n\r\n- should return a user object when credentials are valid\r\n- should return null when credentials are invalid\r\n\r\nAnd likewise below with the `login()` method",
        "ping 🏓 ",
        "could you follow BDD style? Example:\r\n\r\n```typescript\r\ndescribe('when \"message\" is an object', () => { \r\n    it('should serialize an object', () => {\r\n    \r\n    });\r\n});\r\n```"
      ],
      "nest-preserve-api-interface-stability": [
        "This introduces a breaking change",
        "Updating a public interface with 2 additional methods that weren't there before introduces a breaking change as now every library (platform adapter) will have to update their `HttpServer` implementation as well (including these adapters that we don't control ourselves within the NestJS organization).\r\n\r\n\r\n\r\n",
        "We can alternatively update this method to be optional; in this case we won't have to wait ",
        "This interface shouldn't be modified with unessential props (it's a public API)."
      ],
      "nest-http-header-management": [
        "Why do we presume that `TEMPORARY_REDIRECT` should be the default one?",
        "Make sense now :)"
      ],
      "nest-standardize-logger-configuration-patterns": [
        "```suggestion\r\n  json?: boolean;\r\n```",
        "similarly to log levels, we should be able to specify (globally) that the logger should use `json` format for all logs by default ",
        "I think this line:\r\n\r\n```\r\n`${pidMessage}${this.getTimestamp()} ${formattedLogLevel} ${contextMessage}${output}${timestampDiff}\\n`\r\n```\r\nshould be configurable. We could declare another protected method that receives these variables as input parameters (without colors applied). Perhaps that's actually how the `formatMessage` should look likely (we can move everything else back to the original location)",
        "This approach https://github.com/stanimirovv/nest/commit/89d97652caafc26c379ae0268229c4fc3caf3cf2# better fits our needs but we should also allow opt-in coloring the messages. In this case, we should probably declare another dedicated method (let's say `colorize`) that takes the same set of arguments as `formatMessages` and adds colors. This method should be executed from within the `formatMessages` and have `protected` modifier to make it feasible to call it (if needed) from within the custom logger implementation.",
        "Yeah, I think as long as we make `getColorByLogLevel` protected as well (to make it accessible) it could take two arguments (one being a message to format and the second - optional - color to be used that defaults to `this.getColorByLogLever()`)"
      ],
      "nest-use-consistent-curly-braces": [
        "To remain consistent with the rest of the codebase\r\n```suggestion\r\n    if (areThereNoFileIn && this.fileIsRequired) {\r\n      throw this.exceptionFactory('File is required');\r\n    }\r\n    if (!areThereNoFileIn && this.validators.length) {\r\n      await this.validateFilesOrFile(value);\r\n    }\r\n```",
        "nit(style): can we use if { } instead of inline ifs? just for the sake of consistency with the rest of the codebase",
        "even though `void 0` is not necessarily required, please, let's stick with `{ ... }` at least"
      ],
      "nest-preserve-public-api-stability": [
        "This introduces a breaking change",
        "Updating a public interface with 2 additional methods that weren't there before introduces a breaking change as now every library (platform adapter) will have to update their `HttpServer` implementation as well (including these adapters that we don't control ourselves within the NestJS organization).\r\n\r\n\r\n\r\n",
        "We can alternatively update this method to be optional; in this case we won't have to wait "
      ],
      "nest-proper-asynchronous-error-handling": [
        "If this condition `err instanceof KafkaRetriableException && !isPromiseResolved` is true then Promise will both reject and resolve (which technically doesn't make much sense)",
        "Although I like the idea of introducing a new method that can be overridden, it's a breaking change, so we'd have to wait to merge this PR until the next major release. Hence, for the time being, I'd instead suggest just wrapping this logic in try..catch blocks.",
        "Sounds good to me @wSedlacek! Thanks for your work on this one 🙌 ",
        "I don't think that we should call both `.write()` and `.emit('error')` on each error."
      ],
      "nest-use-secure-hash-algorithms": [
        "Let's do this (since it's faster)"
      ],
      "nest-descriptive-identifier-names": [
        "`should` word indicates here that this method should return a boolean (and shouldn't update anything itself) - it's a common convention (is/has/should)",
        "> Can we have shouldFlushLogsOnOverride: boolean, and flushLogsOnOverride(): void method instead?\r\n\r\nSounds great @micalevisk!",
        "nit: mergePacketOptions/mergeRequesOptions? (to not confuse it with the \"client\" instance options) cc @tuxmachine ",
        "this name is not very descriptive I guess. what about `activeShutdownSignals` or something like this?"
      ],
      "nest-explicit-default-configurations": [
        "Instead of adding two extra class properties, you could use helper `getOptionsProp()` method (see example below) which also takes a default value.",
        "could be potentially removed? (just use `undefined` when not explicitly defined)"
      ],
      "nest-pin-dependency-versions": [
        "These may quickly get out of date - we will keep supporting v16 even after it's no longer a maintenance version (after September this year). That's why versions were hardcoded before "
      ],
      "nest-use-factory-providers": [
        "With this change, you won't be able to use multiple multer modules in your project (due to equal hash tokens)."
      ],
      "nest-choose-meaningful-identifier-names": [
        "`should` word indicates here that this method should return a boolean (and shouldn't update anything itself) - it's a common convention (is/has/should)",
        "> Can we have shouldFlushLogsOnOverride: boolean, and flushLogsOnOverride(): void method instead?\r\n\r\nSounds great @micalevisk!",
        "Can we make argument names somewhat more descriptive? For example, `transportOrExtras, extras`, etc.",
        "Can we rename it to \"id\" instead?\r\n```suggestion\r\n  async findOne(@Param(\"id\") id: string): Promise<Cat> {\r\n```\r\nSimilarly in the method below and in the corresponding service class",
        "this name is not very descriptive I guess. what about `activeShutdownSignals` or something like this?"
      ],
      "nest-optimize-critical-path-iterations": [
        "Why not filter + map? Performance should be marginal in this case (I'd think)",
        "Still, the performance impact should be negligible. filter & map is just cleaner & easier to read, not worth the optimization in this particular case ",
        "Let's move this line (check) outside the callback function as otherwise it would be executed per each invocation of the route. This comment applies to other changes too 🙌 "
      ],
      "nest-follow-protocol-standards": [
        "Similarly to what Fastify did, should we expose the `forceCloseConnections` flag to control that behavior?",
        "Do we have any other ideas on how we could avoid introducing a breaking change? AFAIR `forceCloseConnections` is disabled by default (in Fastify) too.\r\n\r\nPerhaps we should expose a dedicated method at the adapter class level? So then you could do \r\n```typescript\r\nconst adapter = app.getHttpAdapter() as ExpressAdapter\r\nadapter.enableForceCloseConnections(); // adapter.forceCloseConnections = true; ?\r\n```\r\nOR maybe we should auto-enable it when someone calls `enableShutdownHooks()`?\r\n\r\n\r\n",
        "What makes this problematic is the fact that breaking change forces us to release a new major version (and so we'd have to postpone merging this PR a little) :( ",
        "SGTM ✅ ",
        "I'd say that we probably shouldn't introduce non-standard HTTP status codes. Better hardcode it inside a corresponding gRPC expection.",
        "Why do we presume that `TEMPORARY_REDIRECT` should be the default one?",
        "Make sense now :)"
      ],
      "nest-graph-based-dependency-management": [
        "This would introduce a major breaking change. Modules are supposed to be sorted by distance",
        "This change may lead to very tricky side-effects. Is there any way to somehow avoid this?",
        "What about circular dependencies? 😄 ",
        "What if the single module is imported by multiple modules that will override `distance` value several times?"
      ],
      "nest-standardize-null-safety-patterns": [
        "Instead of adding a new condition here, we may just swap `||` expression with `??`.",
        "What do you think about using `isNil()` util function here instead? (to exclude `null` and `undefined` values)",
        "I think `isNil()` will be sufficient here :)"
      ],
      "nest-use-topological-sorting": [
        "This would introduce a major breaking change. Modules are supposed to be sorted by distance",
        "This change may lead to very tricky side-effects. Is there any way to somehow avoid this?",
        "What about circular dependencies? 😄 ",
        "What if the single module is imported by multiple modules that will override `distance` value several times?"
      ]
    },
    "profile": {
      "location": "Poland",
      "company": "@nestjs ",
      "blog": "https://kamilmysliwiec.com",
      "twitter_username": "kammysliwiec",
      "site_admin": false,
      "followers": 8266,
      "following": 1
    }
  },
  "logseq-cldwalker": {
    "repos": [
      "logseq/logseq"
    ],
    "entries": [
      {
        "slug": "logseq-add-explanatory-documentation",
        "title": "Add explanatory documentation"
      },
      {
        "slug": "logseq-api-input-validation",
        "title": "API input validation"
      },
      {
        "slug": "logseq-classify-configuration-properties-appropriately",
        "title": "Classify configuration properties appropriately"
      },
      {
        "slug": "logseq-configuration-option-lifecycle",
        "title": "Configuration option lifecycle"
      },
      {
        "slug": "logseq-defer-expensive-operations",
        "title": "Defer expensive operations"
      },
      {
        "slug": "logseq-ensure-semantic-naming-accuracy",
        "title": "Ensure semantic naming accuracy"
      },
      {
        "slug": "logseq-extract-reusable-hooks",
        "title": "Extract reusable hooks"
      },
      {
        "slug": "logseq-fail-fast-explicitly",
        "title": "Fail fast explicitly"
      },
      {
        "slug": "logseq-filter-nil-values-defensively",
        "title": "Filter nil values defensively"
      },
      {
        "slug": "logseq-maintain-documentation-consistency",
        "title": "Maintain documentation consistency"
      },
      {
        "slug": "logseq-multi-arity-backward-compatibility",
        "title": "Multi-arity backward compatibility"
      },
      {
        "slug": "logseq-optimize-algorithm-performance",
        "title": "optimize algorithm performance"
      },
      {
        "slug": "logseq-prevent-command-injection-vulnerabilities",
        "title": "Prevent command injection vulnerabilities"
      },
      {
        "slug": "logseq-respect-existing-formatting",
        "title": "Respect existing formatting"
      },
      {
        "slug": "logseq-separate-user-system-data",
        "title": "Separate user system data"
      },
      {
        "slug": "logseq-simplify-code-readability",
        "title": "Simplify code readability"
      },
      {
        "slug": "logseq-simplify-naming-conventions",
        "title": "Simplify naming conventions"
      }
    ],
    "comments": {
      "logseq-maintain-documentation-consistency": [
        "Looks like renaming files in this PR has broken documentation links. Worth searching and updating broken linnks"
      ],
      "logseq-multi-arity-backward-compatibility": [
        "`eval-string` is used in other places besides `<src` so this would be adding this configuration in other features that would be confusing. Let's only pass them as args from `eval-result` for now",
        "My concern wasn't arguments being passed but that other features have additional configuration that are specific to the src feature. I've addressed this",
        "Optional: It could be nice to keep this fn simpler and have it just take an entity id (eid) like db/pull or https://cljdoc.org/d/datascript/datascript/1.4.2/api/datascript.core#entity. This would mean passing in the `db/entity` arg",
        "The logseq.api also uses `resolve-input`. Since `current-block-uuid` is nil for it, these next two `cond` conditions would fail and possibly confuse plugin authors. Could you update them to also check for `current-block-uuid`?"
      ],
      "logseq-separate-user-system-data": [
        "While I'm glad this PR allows for workflows that weren't possible before e.g. classes as properties, I don't think we should be encouraging it as common practice. Doing so muddles two important concepts in the DB version and encourages poor modeling practices i.e. one identity that behaves both as a class and a property. For most users who won't use this feature, it also makes an irreversible mistake easier i.e. turning a class into a property with no ability to undo. I also noticed for large class graphs like schema.org graphs, the property dropdown is laggy because it fetches all classes.\r\n\r\nIf we still want this classes in properties behavior, I could put it behind a config option if there's no objection",
        "Using this in the builder autocompletes to meaningless strings for users. This feels buggy\r\n\r\nI haven't looked at the rest of these attributes but there is probably a lot more to be said about them...",
        "> I haven't looked at the rest of these attributes but there is probably a lot more to be said about them...\r\n\r\nI looked at the rest of these and disabled more that are implementation details that could change. I would've disabled :block/type as it's still a work in progress e.g. :block/type \"journal\" nodes tagged with `#Journal`  but left it alone since you wanted to make it queryable with https://github.com/logseq/logseq/commit/94773db6f3e17a3e4f0ca42d8e9dcf0193d9c7f5",
        "I don't think ignoring whiteboard properties by feature is a sustainable approach. Currently I'm seeing unwanted results in a query for the `type` property, see below. Even if we could find every feature that uses `:block/properties`, fix it and ensure future features filter whiteboard properties, this would break user queries if they use a whiteboard property. From lambda, I'm seeing the following whiteboard properties that could potentially break user queries (except :collapsed) :\r\n\r\n```\r\n$ bb '(->> (fs/glob \"whiteboards\" \"*.edn\") (map #(-> % str slurp edn/read-string)) (mapcat #(-> % :blocks first :block/properties keys)) distinct sort)'\r\n(:blockType :collapsed :collapsedHeight :compact :fill :id :index :isAutoResizing :ls-type :noFill :nonce :opacity :pageId :parentId :point :scale :scaleLevel :size :stroke :strokeType :strokeWidth :type)\r\n```\r\n\r\nI see a couple solutions to the problem:\r\n1. If the all the whiteboard properties are known, put them under a namespace like we did with [macros](https://github.com/logseq/logseq/pull/6105#discussion_r934914562). Then add those whiteboard properties in `gp-property/hidden-built-in-properties`\r\n2. Introduce a new block attribute just for this purpose for whiteboards e.g. `:block/whiteboard-properties`\r\n3. Introduce a new block attribute that can be used by whiteboards and third parties that have the same need to store feature-generated properties. Perhaps `:block/{non-user-properties,feature-properties,data-properties}`. Over time we could migrate some of our built-in properties to use this block attribute instead\r\n\r\nI'd suggest solution 3 or 1 depending on if all the whiteboard properties are known, as they both seem like longer term solutions.\r\n@tiensonqin I'd be interested to hear what you think of this and what solutions you see and prefer\r\n\r\n<img width=\"612\" alt=\"Screen Shot 2022-09-24 at 2 27 03 AM\" src=\"https://user-images.githubusercontent.com/97210743/192083425-6502f48c-f864-4ad7-8197-e61532050bf8.png\">\r\n",
        "Also prefer solution 1 as it made it easy to hide it from autocompletion",
        "No longer seeing query issue for new whiteboards 😄 "
      ],
      "logseq-defer-expensive-operations": [
        "Ah yep. Fixed",
        "Given how walks aren't that performant, we should remove extra ones like this if we don't need em. I'll do this"
      ],
      "logseq-optimize-algorithm-performance": [
        "For perf reasons I wish we weren't using postwalk but without a good understanding of every ast node combination, this is what we have to do",
        "`map` is lazy so this isn't being run. Would recommend `mapv` or `doseq`"
      ],
      "logseq-simplify-code-readability": [
        "Let's simplify this fn as it only needs 2 args, the filtered-count and total-count, as we can pass nil for filtered-count if filters? isn't present in the calling fn. Since most translators aren't coders would be good to show examples in comments of different strings one can see",
        "```suggestion\r\n                                       (if (= total 1) \" Linked Reference\" \" Linked References\")\r\n```\r\nLet's simplify logic to `if` and `str` for the non-coders where possible"
      ],
      "logseq-extract-reusable-hooks": [
        "@tiensonqin When I disable the two local state atoms for simple queries, it has no effect on preserving sorting state as that is handled by the block property. Do we know what local state is needed for here? The `block/*custom-query` component is used in so many contexts that I left the local atoms in order to not accidentally break something",
        "Thanks. Makes sense. Will remove"
      ],
      "logseq-api-input-validation": [
        "I see a couple `fill`s were changed to `type`. Is the latter more useful for stability or something else ? Asking so I know when to use which api call"
      ],
      "logseq-prevent-command-injection-vulnerabilities": [
        "Is there any way we could make this available to plugins? By special casing for alda here, we encourage building an alda feature in logseq, instead of as a plugin",
        "I'm concerned this list doesn't protect enough. Have we considered something like the shellquote npm lib - https://auth0.com/blog/preventing-command-injection-attacks-in-node-js-apps/#Preventing-Command-Injection?"
      ],
      "logseq-simplify-naming-conventions": [
        "I have not seen most of this section consistently adhered to and I've never seen it enforced on a PR. Personally I'd be ok with removing this section or just putting a brief mention of the header which is the only part of the commit style that is intermittently followed. Curious to hear what the rest of the team thinks"
      ],
      "logseq-respect-existing-formatting": [
        "> we recommend that you do so for code that you change/add\r\n\r\nWe would rather that contributions respect existing whitespace conventions of the files they modify. We'll eventually put this advice in a CONTRIBUTING.md, hopefully soon"
      ],
      "logseq-fail-fast-explicitly": [
        "Almost everywhere we call a worker fn we use a when-let. Is this actually necessary in most places or is it only one or two places that needed it before worker was defined and then it got copied everywhere? If it is needed in most places, would it be helpful to provide a more stable api fn that wraps the atom?",
        "Thanks for making this more consistent to use and adding explicit error handling!",
        "A comment for this check could be helpful. Unclear why we pass invalid calls through silently rather than fail explicitly",
        "Gotcha. Good to know it was useful for refactoring. Hopefully we can delete it later when there aren't any more missed deletions",
        "This isn't a change in this PR but if sqlite is missing, we should fail explicitly to the user so users are aware their data isn't being written and they could lose a lot of data silently. I can address this tomorrow if you'd like",
        "A catch here has the downsides that it gives users a misleading impression that something works when it doesn't and we don't get a sentry issue if something fails. What motivated adding a catch here?",
        "Thanks for addressing"
      ],
      "logseq-ensure-semantic-naming-accuracy": [
        "I found this name confusing as I thought they were going to be unique per view but instead it's acting like another type. Some possible suggestions: ui-type, feature-type, feature",
        "Are there any other types that won't have ref default values? Maybe :datetime? If there are other non-ref values should we generalize the name and :type e.g. scalar-default-value and :any so we don't have to keep modifying all the places this is used",
        "Can we rename this to before-scripts or scripts-before? At some point someone will also want to inject scripts at the end to override logseq js",
        "Since we're fixing a bug from #9563, I tweaked the name here since that PR also enhanced the command so that it can be used outside of query tables",
        "I think you wanted `validate-namespaces` but that doesn't usually mean modification. How about `make-valid-namespaces`?"
      ],
      "logseq-configuration-option-lifecycle": [
        "Would be good to remove these additions of default config values. We have a tech debt item to move all these out to `frontend.state`. Also, as @andelf pointed out, hardcoding default values would make global config unusable",
        "I can address in the followup commit. Breaking the basic usability of a feature is not as important as a small workflow enhancement",
        "If we only set default-config the user loses all of their graph config which would cause annoying, silent bugs e.g. certain properties aren't rendered correctly or their journals start saving in a different format",
        "Unfortunately we don't have access to both valid configs here. Since global config doesn't have validation, it's probably the global config being invalid that causes the failure. We could add the current graph config as a temporal fix but it doesn't guarantee that there still isn't an issue and leads to another issue - a poor user notification experience. These catch's are occurring in a low level fn that can be called a number of times during render, which leads to notifying the user a number of times, at a time potentially much later than when they edited the invalid config. If we don't want illegal global config we should just validate and tell the user when they have entered it"
      ],
      "logseq-filter-nil-values-defensively": [
        "Some food for thought on \"safe\" fns (a pattern that I've seen rarely outside of this codebase). Two downsides I see:\r\n- It forces all callers to handle a nil state where they previously may not have had to. This additional complexity is one reason we have `(remove nil?)` throughout our codebase\r\n- It propagates and creates secondary error states by returning nils instead of letting the error happen\r\n   For example, see the two calls of `(-> (state/get-block-id dom) uuid ...` in [the editor handler](https://github.com/logseq/logseq/pull/8816/files?diff=unified&w=1#diff-2f055ca9c790c38c4cbe601146ca7c19ad359f36a73554039581c2e5fae428d6). If el was `nil` we'd get the original error of `Cannot read properties of null (reading 'getAttribute')` but because we forgot to handle nil here we get `Assert failed: (string? s)`.  We've made it harder to determine the root cause of failure by propagating a nil. Thankfully this one is close to the original but there's no guarantee other secondary errors will be as easy to diagnose. We can of course fix these with `some->` but it's an ongoing issue we've forced on ourselves as described in the previous point\r\n                     \r\nIf wanting to stick with safe usage would recommend renaming to `get-safe-block-id` so callers know they should handle nils accordingly"
      ],
      "logseq-add-explanatory-documentation": [
        "Could be useful to document or comment somewhere when to use this option and why. I saw it in the PR but having dev docs in PRs aren't easy to find",
        "Perhaps a brief comment summarizing why there's a condition here would be helpful as I wouldn't know if I hadn't reviewed",
        "@cnrpman @andelf Would one of you be interested in adding ns docstrings for `src/electron` in a followup PR? If so, add the path here to enable linting for it"
      ],
      "logseq-classify-configuration-properties-appropriately": [
        "Great to have all these property values documented. Would be helpful to add these to `gp-property/hidden-built-in-properties` so that the graph-parser can treat them as built in properties instead of user properties. Happy to add autocompletion for property values if that's useful some time",
        "Looks like these were added to `hidden-built-in-properties`. I'm guessing that's a mistake as they wouldn't be editable",
        "When using the query builder, I noticed that the property filter now displays the new built-in properties. My guess based on previous issues is that users are not going to want to see this but we can also wait and see:\r\n<img width=\"225\" alt=\"Screen Shot 2023-04-03 at 5 57 41 PM\" src=\"https://user-images.githubusercontent.com/97210743/229636887-fad93ceb-2995-4c35-93d0-9920d1493748.png\">\r\n\r\nThis happens because these properties haven't been added to `gp-property/hidden-built-in-properties`",
        "Agree this shouldn't block the PR. I'm ok either way on addressing this later"
      ]
    },
    "profile": {
      "blog": "",
      "site_admin": false,
      "followers": 48,
      "following": 0
    }
  },
  "Darksonn": {
    "repos": [
      "tokio-rs/tokio"
    ],
    "entries": [
      {
        "slug": "tokio-clear-command-documentation",
        "title": "Clear command documentation"
      },
      {
        "slug": "tokio-code-block-formatting-standards",
        "title": "Code block formatting standards"
      },
      {
        "slug": "tokio-design-error-handling-carefully",
        "title": "Design error handling carefully"
      },
      {
        "slug": "tokio-design-flexible-apis",
        "title": "Design flexible APIs"
      },
      {
        "slug": "tokio-document-null-safety-assumptions",
        "title": "Document null safety assumptions"
      },
      {
        "slug": "tokio-fast-deterministic-tests",
        "title": "Fast deterministic tests"
      },
      {
        "slug": "tokio-flexible-consistent-api-patterns",
        "title": "Flexible consistent API patterns"
      },
      {
        "slug": "tokio-follow-import-style",
        "title": "Follow import style"
      },
      {
        "slug": "tokio-follow-naming-conventions",
        "title": "Follow naming conventions"
      },
      {
        "slug": "tokio-graceful-error-handling",
        "title": "Graceful error handling"
      },
      {
        "slug": "tokio-granular-feature-flags",
        "title": "Granular feature flags"
      },
      {
        "slug": "tokio-memory-ordering-needs-justification",
        "title": "Memory ordering needs justification"
      },
      {
        "slug": "tokio-minimize-unsafe-code",
        "title": "Minimize unsafe code"
      },
      {
        "slug": "tokio-network-api-design-consistency",
        "title": "Network API design consistency"
      },
      {
        "slug": "tokio-optimize-algorithmic-complexity",
        "title": "Optimize algorithmic complexity"
      },
      {
        "slug": "tokio-optimize-algorithmic-efficiency",
        "title": "Optimize algorithmic efficiency"
      },
      {
        "slug": "tokio-optimize-ci-job-structure",
        "title": "Optimize CI job structure"
      },
      {
        "slug": "tokio-optimize-hot-paths",
        "title": "Optimize hot paths"
      },
      {
        "slug": "tokio-optimize-job-structure",
        "title": "Optimize job structure"
      },
      {
        "slug": "tokio-optimize-memory-allocation",
        "title": "Optimize memory allocation"
      },
      {
        "slug": "tokio-organize-code-logically",
        "title": "Organize code logically"
      },
      {
        "slug": "tokio-prefer-explicit-over-concise",
        "title": "Prefer explicit over concise"
      },
      {
        "slug": "tokio-release-locks-before-waking",
        "title": "Release locks before waking"
      },
      {
        "slug": "tokio-secure-unsafe-code",
        "title": "Secure unsafe code"
      },
      {
        "slug": "tokio-simplify-configuration-flags",
        "title": "Simplify configuration flags"
      },
      {
        "slug": "tokio-socket-configuration-guidance",
        "title": "Socket configuration guidance"
      },
      {
        "slug": "tokio-structural-configuration-approaches",
        "title": "Structural configuration approaches"
      },
      {
        "slug": "tokio-structure-api-doc-blocks",
        "title": "Structure API doc blocks"
      },
      {
        "slug": "tokio-structure-conditional-compilation",
        "title": "Structure conditional compilation"
      },
      {
        "slug": "tokio-structure-feature-flags-strategically",
        "title": "Structure feature flags strategically"
      },
      {
        "slug": "tokio-test-diverse-configurations",
        "title": "Test diverse configurations"
      },
      {
        "slug": "tokio-test-production-configurations-too",
        "title": "Test production configurations too"
      },
      {
        "slug": "tokio-use-option-methods-idiomatically",
        "title": "Use Option methods idiomatically"
      },
      {
        "slug": "tokio-write-focused-single-purpose-tests",
        "title": "Write focused single-purpose tests"
      }
    ],
    "comments": {
      "tokio-optimize-hot-paths": [
        "```suggestion\r\n    #[doc(hidden)]\r\n    #[inline]\r\n    pub fn has_budget_remaining() -> bool {\r\n```"
      ],
      "tokio-optimize-memory-allocation": [
        "The intent of this `unwrap_unchecked()` is to access the memory for free, but `get_mut` has a bunch of logic to lock the weak count and so on. If we actually want to access the memory without checks, we should go through `into_raw`.",
        "I don't think we should clear the buffer. That's up to the user. I think it's better to support appending to the vector if the user wants that.",
        "There are two options that make sense to me:\r\n\r\n * Add a third argument that specifies the maximum number of elements to add to the vector. In this case, the capacity is irrelevant, and we will resize the buffer if necessary.\r\n * Or use the capacity like this:\r\n\r\n```rs\r\nif buffer.len() == buffer.capacity() {\r\n    buffer.reserve(super::BLOCK_CAP);\r\n}\r\nlet max_number_added = buffer.capacity() - buffer.len();\r\n```\r\n\r\nIn either case, I prefer that we do not clear the buffer.",
        "I was looking at this paragraph, and I think it could be improved like this:\r\n```suggestion\r\n    /// If `buffer` has unused capacity, then this call will not reserve\r\n    /// additional space in `buffer`. This means that the maximum number of\r\n    /// received messages is `buffer.capacity() - buffer.len()`. However, if\r\n    /// the capacity is equal to the length, then this call will increase the\r\n    /// capacity to make space for additional elements.\r\n```\r\nThis actually raises a question: Perhaps it makes sense to only reserve space when we return a message? This way, we don't consume extra memory until a message arrives, and if the channel gets closed, we don't reserve any space.\r\n\r\nWhat do you think?",
        "How does this change the size of `Sleep`? Could `TimerEntry` be changed to reduce the change?",
        "What happens if you just don't call `clear_entry` in [this method](https://github.com/tokio-rs/tokio/blob/ef657d23fd0f73bc73d3cc872feaceb0f8bf36b7/tokio/src/runtime/time/entry.rs#L503-L527) when it hasn't yet been registered?\r\n\r\nBased on @conradludgate's comment [here](https://github.com/tokio-rs/tokio/issues/6504#issuecomment-2073343288), it sounds like this triggers a loom failure, but that's most likely due to some path where the timer is dropped concurrently with firing or something like that. If we've never been registered with the driver, then not calling `clear_entry` should be okay."
      ],
      "tokio-simplify-configuration-flags": [
        "Could we just call it `--cfg tokio_uring`? We can have it require you to also pass `--cfg tokio_unstable`.",
        "Sorry, I meant how about we rename the flag to just `--cfg tokio_uring` instead of `--cfg tokio_unstable_uring`?",
        "Done."
      ],
      "tokio-code-block-formatting-standards": [
        "I'm happy to accept a PR that adds `--locked` to the command."
      ],
      "tokio-optimize-ci-job-structure": [
        "Please don't install multiple Rust versions in a single CI run. It causes confusion about which version of Rust is actually being used in each call. Instead, create a new CI run for this check.",
        "The miri job is starting to take a rather long time. Could you add a new job for this, instead of adding additional work to the existing job? This will allow for parallelism.",
        "That's nice. The loom jobs are already much longer than that. We could consider gating it similarly to what we did for loom tests."
      ],
      "tokio-design-error-handling-carefully": [
        "I think we probably want to just fall back to existing behavior if `stream_position` fails.\r\n```suggestion\r\n        let pos = std.stream_position().unwrap_or(0);\r\n```",
        "You could implement the `source` method here.",
        "I don't think we want two separate kinds of errors here. With `OnceCell` initialization can fail and can take a long time, so it's important to consider it to be a separate state, but with `SetOnce` the initialization state is always very short-lived and infallible, so I don't think we want to expose it to the end-user.",
        "Let's not ignore errors.\r\n```suggestion\r\n///     compress_data(reader).await?;\r\n```",
        "I like to avoid this error type in examples because it is not `Send` which means it only works in the `block_on` task.",
        "Avoiding unwrap is fine, I just prefer a different error type. E.g., it could be `Box<dyn Error+Send+Sync>`."
      ],
      "tokio-write-focused-single-purpose-tests": [
        "It is easier for me to understand the tests if you split them into many small tests rather than one or two large ones. For example, these small tests could be useful:\r\n\r\n1. `is_closed` should return true after calling `close` but still has a sender\r\n2. `is_closed` should return true after dropping all senders\r\n3. `is_closed` should return true after dropping all senders except for a weak sender\r\n4. `is_closed` should return false when there is a sender\r\n5. `is_closed` should return false when there is a sender, even if enough messages have been sent to fill the channel\r\n6. `is_closed` should return false when there is a permit (but no senders)",
        "Thanks!\r\n\r\nThat could make sense. After all, they have different implementations of the semaphore, so we aren't *just* testing the same code twice.",
        "(No need to put everything you test related to thread ids in a single test. You can make more than one.)",
        "We like to avoid sleeps in tests to reduce the amount of time it takes to run the tests. If you make the runtime use mocked time via the `start_paused` parameter, then this test will run instantly no matter what the duration is.\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/10e23d1c621ab38aadf2cefba1120494cff615f0/tokio/src/runtime/task/join.rs#L190-L205",
        "This will make the test run much much faster by using [simulated time](https://docs.rs/tokio/latest/tokio/time/fn.pause.html).\r\n```suggestion\r\n#[tokio::test(start_paused = true)]\r\n```",
        "We do not use sleeping in tests. We have 845 tests right now. Imagine how long it would take to run the tests if every single one had a 0.5 second sleep.",
        "Please include some tests for edge cases similar to the existing `empty_join` test at the bottom and the four `sync_*` tests at the top.",
        "Please add a test that uses this with `Vec<u8>` and `BytesMut` too.",
        "I'm concerned that we might need to supply the number of bytes as a separate parameter. I recall that resizable buffers are considered infinitely large by `BufMut`."
      ],
      "tokio-structure-api-doc-blocks": [
        "Markdown docs generally start with one really short summary, followed by more text.\r\n```suggestion\r\n    /// Tries to convert a `WeakSender` into a [`Sender`]. \r\n    ///\r\n    /// This will return `Some`\r\n    /// if there are other `Sender` instances alive and the channel wasn't\r\n    /// previously dropped, otherwise `None` is returned.\r\n```\r\n\\+ reflow to line length",
        "We generally have empty lines between headers, text, and code blocks.\r\n```suggestion\r\n//! # Example encoding using `LinesCodec`\r\n//!\r\n//! The following example demonstrates how to use a codec such as [`LinesCodec`] to\r\n//! write a sink of framed data. [`FramedWrite`] can be used to achieve this. Data sent\r\n//! to [`FramedWrite`] are first framed according to a specific codec, and then sent to\r\n//! an implementor of [`AsyncWrite`].\r\n//!\r\n//! ```\r\n```",
        "This looks a bit weird with the sentence starting out of nowhere. Also, it's best to have extra information after a line break as that renders more nicely in the html docs.\r\n\r\n```suggestion\r\n    /// Returns the line number for where this symbol is currently executing.\r\n    ///\r\n    /// If debuginfo is missing, this is likely to return None.\r\n```",
        "This isn't publicly visible, so it's not a big deal here, but generally the convention is that documentation should start with one *short* line. If more explanation is needed, it can go in a separate paragraph. This is because when structs are shown in documentation, the first line is used as a summary in some cases, and the summary should be short."
      ],
      "tokio-test-production-configurations-too": [
        "That is fine. We may want to consider whether it makes sense to run with both?"
      ],
      "tokio-prefer-explicit-over-concise": [
        "If we're going to define a new type for this, then we should probably call it `AsyncFdTryNewError`.\r\n\r\nBut I would also be okay with `(T, io::Error)`.",
        "This name is okay, but I think there is precedent for the name `AbortHandle` elsewhere, so we might want to prefer that name.",
        "Actually, we have a type called `AbortHandle` in Tokio, and that does something else ... so that name doesn't work.\r\n\r\nBut another option is `AbortOnDropHandle`. I'll let you pick.",
        "I agree that `sender_strong_count` and `sender_weak_count` would be better. Otherwise, this PR looks good to me.",
        "Why not? I can't immediately see what would break if we just change `get` to `T: Clone`.",
        "This seems to compile locally:\r\n```diff\r\ndiff --git a/tokio/src/task/task_local.rs b/tokio/src/task/task_local.rs\r\nindex ba58ea6a..cb9d22c6 100644\r\n--- a/tokio/src/task/task_local.rs\r\n+++ b/tokio/src/task/task_local.rs\r\n@@ -264,16 +264,16 @@ impl<T: 'static> LocalKey<T> {\r\n     }\r\n }\r\n \r\n-impl<T: Copy + 'static> LocalKey<T> {\r\n+impl<T: Clone + 'static> LocalKey<T> {\r\n     /// Returns a copy of the task-local value\r\n-    /// if the task-local value implements `Copy`.\r\n+    /// if the task-local value implements `Clone`.\r\n     ///\r\n     /// # Panics\r\n     ///\r\n     /// This function will panic if the task local doesn't have a value set.\r\n     #[track_caller]\r\n     pub fn get(&'static self) -> T {\r\n-        self.with(|v| *v)\r\n+        self.with(|v| v.clone())\r\n     }\r\n }\r\n```",
        "Is it? If so, then you changed it before I got a chance to see it, because the thing I saw originally added a new method called `clone` instead of changing the existing `get` method.\r\n\r\nEither way, I have made up my mind. I would like to change `get` instead of introducing a new method.",
        "I'm sorry for the confusion. First, I am on the Tokio core team and am authoritative in this case (but it doesn't seem like @mox692 and I disagree?). As for your other questions:\r\n\r\n> Why remove get for Copy inner types?\r\n\r\nI am not suggesting that you remove `get` for `Copy` types. All `Copy` types are also `Clone` (it's a subtrait), so you will still be able to use `get` with any `Copy` type after changing `T: Copy` to `T: Clone`.\r\n\r\n> Why not just add a similar impl for `Clone`?\r\n\r\nWe could also do that, but then `Copy` types have two identical methods that do the same thing. To me, it seems simpler dev UX to have a single method that works for both `Copy` and `Clone`. That's why I suggested it.\r\n\r\n> I couldn't just extend on `get` for `Copy + Clone` simply because my type isn't `Copy` (it's an `Arc<_>`).\r\n\r\nIf you make `get` require `T: Clone` (rather than `T: Copy + Clone`), then it also works for `Arc<_>`."
      ],
      "tokio-structural-configuration-approaches": [
        "My main feedback is also this. It may make sense to merge this into a struct with the id, or to wrap the location in something that can be zero-sized on stable, or to otherwise avoid conditional compilation everywhere this information is passed around.",
        "Does this need to be gated on `cfg_rt!`? Is the file not already gated on that?",
        "Can you add an `#[cfg_attr(not(feature = \"rt\"), allow(dead_code))]` on the module instead? The `cfg_rt!` blocks break rustfmt, so I try to avoid wrapping large blocks in them.",
        "Or maybe it'd be better to split the file into two and wrap the `mod` statement in `cfg_rt!`."
      ],
      "tokio-network-api-design-consistency": [
        "This will incorrectly return `false` if stderr is a terminal but there is an ongoing IO operation. Can we instead do this?\r\n```suggestion\r\n    /// Returns true if the descriptor/handle refers to a terminal/tty.\r\n    pub fn is_terminal(&self) -> bool {\r\n        std::io::stderr().is_terminal()\r\n```",
        "Shouldn't this also take an interest? Otherwise we will eventually be asked to add an `with_epoll_flags_and_interest`.",
        "Please add to the documentation that `EPOLLONESHOT` must not be used, and that `EPOLLET` must be set. And please add debug asserts for this.",
        "Is Linux the only OS that needs this fix? Does macOS not have abstract path names? It would be worth to look at how mio did this prior to the v1 release.",
        "Ah, ok. Does `target_os = \"linux\"` also include Android? I see that it's also available there.",
        "Why the casts?",
        "Thanks."
      ],
      "tokio-fast-deterministic-tests": [
        "We like to avoid sleeps in tests to reduce the amount of time it takes to run the tests. If you make the runtime use mocked time via the `start_paused` parameter, then this test will run instantly no matter what the duration is.\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/10e23d1c621ab38aadf2cefba1120494cff615f0/tokio/src/runtime/task/join.rs#L190-L205",
        "This will make the test run much much faster by using [simulated time](https://docs.rs/tokio/latest/tokio/time/fn.pause.html).\r\n```suggestion\r\n#[tokio::test(start_paused = true)]\r\n```",
        "We do not use sleeping in tests. We have 845 tests right now. Imagine how long it would take to run the tests if every single one had a 0.5 second sleep.",
        "Maybe this timeout should be larger?",
        "We expect tests to run with `cargo test` too, so we shouldn't use nextest specific features. Having a timeout sgtm."
      ],
      "tokio-socket-configuration-guidance": [
        "Can we improve this wording? I don't think it very clearly explains the situation. You could say something along the lines of \"Passing a listener in blocking mode is always errornous, and the behavior in that case may change in the future. For example, it could panic.\"",
        "Shouldn't this also take an interest? Otherwise we will eventually be asked to add an `with_epoll_flags_and_interest`.",
        "Please add to the documentation that `EPOLLONESHOT` must not be used, and that `EPOLLET` must be set. And please add debug asserts for this.",
        "Is Linux the only OS that needs this fix? Does macOS not have abstract path names? It would be worth to look at how mio did this prior to the v1 release.",
        "Ah, ok. Does `target_os = \"linux\"` also include Android? I see that it's also available there.",
        "Why the casts?",
        "Thanks."
      ],
      "tokio-structure-conditional-compilation": [
        "My main feedback is also this. It may make sense to merge this into a struct with the id, or to wrap the location in something that can be zero-sized on stable, or to otherwise avoid conditional compilation everywhere this information is passed around."
      ],
      "tokio-follow-naming-conventions": [
        "We shouldn't add new `self` methods to types that implement `Deref`.\r\n```suggestion\r\n    pub fn rwlock(this: &Self) -> &Arc<RwLock<T>> {\r\n```",
        "This name is okay, but I think there is precedent for the name `AbortHandle` elsewhere, so we might want to prefer that name.",
        "Actually, we have a type called `AbortHandle` in Tokio, and that does something else ... so that name doesn't work.\r\n\r\nBut another option is `AbortOnDropHandle`. I'll let you pick.",
        "According to the API guidelines, this should be called `as_socket`. It's not an expensive conversion.\r\n```suggestion\r\n    fn as_socket(&self) -> socket2::SockRef<'_> {\r\n        socket2::SockRef::from(self)\r\n    }\r\n```",
        "Why not? I can't immediately see what would break if we just change `get` to `T: Clone`.",
        "This seems to compile locally:\r\n```diff\r\ndiff --git a/tokio/src/task/task_local.rs b/tokio/src/task/task_local.rs\r\nindex ba58ea6a..cb9d22c6 100644\r\n--- a/tokio/src/task/task_local.rs\r\n+++ b/tokio/src/task/task_local.rs\r\n@@ -264,16 +264,16 @@ impl<T: 'static> LocalKey<T> {\r\n     }\r\n }\r\n \r\n-impl<T: Copy + 'static> LocalKey<T> {\r\n+impl<T: Clone + 'static> LocalKey<T> {\r\n     /// Returns a copy of the task-local value\r\n-    /// if the task-local value implements `Copy`.\r\n+    /// if the task-local value implements `Clone`.\r\n     ///\r\n     /// # Panics\r\n     ///\r\n     /// This function will panic if the task local doesn't have a value set.\r\n     #[track_caller]\r\n     pub fn get(&'static self) -> T {\r\n-        self.with(|v| *v)\r\n+        self.with(|v| v.clone())\r\n     }\r\n }\r\n```",
        "Is it? If so, then you changed it before I got a chance to see it, because the thing I saw originally added a new method called `clone` instead of changing the existing `get` method.\r\n\r\nEither way, I have made up my mind. I would like to change `get` instead of introducing a new method.",
        "I'm sorry for the confusion. First, I am on the Tokio core team and am authoritative in this case (but it doesn't seem like @mox692 and I disagree?). As for your other questions:\r\n\r\n> Why remove get for Copy inner types?\r\n\r\nI am not suggesting that you remove `get` for `Copy` types. All `Copy` types are also `Clone` (it's a subtrait), so you will still be able to use `get` with any `Copy` type after changing `T: Copy` to `T: Clone`.\r\n\r\n> Why not just add a similar impl for `Clone`?\r\n\r\nWe could also do that, but then `Copy` types have two identical methods that do the same thing. To me, it seems simpler dev UX to have a single method that works for both `Copy` and `Clone`. That's why I suggested it.\r\n\r\n> I couldn't just extend on `get` for `Copy + Clone` simply because my type isn't `Copy` (it's an `Arc<_>`).\r\n\r\nIf you make `get` require `T: Clone` (rather than `T: Copy + Clone`), then it also works for `Arc<_>`.",
        "I agree that `sender_strong_count` and `sender_weak_count` would be better. Otherwise, this PR looks good to me."
      ],
      "tokio-optimize-algorithmic-complexity": [
        "This iterates all of the buffers every time, even if we only write a few of them. If the buffers are very long and this is called in a loop, that gives quadratic performance.\r\n\r\nWe should be able to embed this logic inside the for loop instead to avoid that.",
        "No, `b.len()` is constant time. Instead, it's O(n) in the length of `bufs`, which you iterate over.",
        "I would expect that `saturating_pow` is rather expensive compared to a shift.\r\n```suggestion\r\n        let max_number = match 1.checked_shl(8 * self.length_field_len) {\r\n            Some(shl) => shl - 1,\r\n            None => u64::MAX,\r\n        };\r\n```",
        "If we're going to the effort of providing an implementation, then I think we should not go for a version that just calls `poll_next_entry` in a loop. At that point, it might as well be a generic piece of functionality that works for any `Stream`.\r\n\r\nEach call to that method generates a random number to pick a stream to start at, but I think it would make more sense to pick a random number only once, and then keep going around the loop until we have `limit` items, or until we've polled `self.entries.len()` times in a row without getting an item.",
        "Hmm. I imagine that when we get many items, we would want to get them from different streams (probably in the order they're stored). But with this strategy, whichever stream is at `start` will be preferred every time.",
        "Easiest is probably to either return the next index from `poll_one`, or to copy its implementation into `poll_next_many`.",
        "Seems simpler to just keep track of `added` from the beginning and change the while loop to `while added < limit`.",
        "Could something like this make more sense?\r\n```\r\nwheels_lock.0.iter_mut()\r\n    .filter_map(|wheel| wheel.get_mut().next_expiration_time())\r\n    .min();\r\n```\r\nThis way, we don't need to touch indexes at all.",
        "I would like to avoid having this variable at all when using biased. I don't think it can be optimized out because it gets stored as a field in the closure type.",
        "Yeah, I don't think we want the match. It duplicates the entire logic, which isn't nice. One option could be to define two helper types:\r\n```rs\r\n#[derive(Default)]\r\npub struct Rotater<const COUNT: usize> {\r\n    next: usize,\r\n}\r\n\r\nimpl<const COUNT: usize> Rotater<COUNT> {\r\n    #[inline]\r\n    pub fn num_skip(&mut self) -> usize {\r\n        let mut num_skip = self.next;\r\n        self.next += 1;\r\n        if self.next == COUNT {\r\n            self.next = 0;\r\n        }\r\n        num_skip\r\n    }\r\n}\r\n\r\n#[derive(Default)]\r\npub struct BiasedRotater {}\r\n\r\nimpl BiasedRotater {\r\n    #[inline]\r\n    pub fn num_skip(&mut self) -> usize {\r\n        0\r\n    }\r\n}\r\n```\r\nand then have the macro select which type to use:\r\n```text\r\n( biased; $($e:expr),+ $(,)?) => {\r\n    $crate::join!(@{ rotator=$crate::macros::support::BiasedRotator; () (0) } $($e,)*)\r\n};\r\n\r\n( $($e:expr),+ $(,)?) => {\r\n    $crate::join!(@{ rotator=$crate::macros::support::Rotator; () (0) } $($e,)*)\r\n};\r\n```"
      ],
      "tokio-clear-command-documentation": [
        "I'm happy to accept a PR that adds `--locked` to the command."
      ],
      "tokio-follow-import-style": [
        "Safety comments must be right before the unsafe block. Please move `let filled` before it.",
        "Which lint is this?",
        "I think this lint does not improve the code. Please add `#![allow(clippy::needless_lifetimes)]` to the top of the `lib.rs` files instead to silence it.",
        "We don't use this import style. Please split this into three `use` statements.",
        "In Tokio we usually split up imports from different modules like this:\r\n```suggestion\r\nuse std::future::Future;\r\nuse std::time::Duration;\r\n```\r\nThe brackets are only used for items in the same module.",
        "Tokio uses this convention for imports:\r\n```suggestion\r\n    os::fd::AsFd,\r\n    os::unix::io::{AsRawFd, RawFd},\r\n```"
      ],
      "tokio-document-null-safety-assumptions": [
        "This needs a safety comment to explain that this is okay because the caller guarantees that the written portion is reported correctly.\r\n\r\nAlso, it'd be nice to note that this is only correct because `self.buf.len() == 0` as asserted above.",
        "Can you add a safety comment?\r\n```\r\n// SAFETY: The memory may be uninitialized, but `rd.read` will only write to the buffer.\r\n```",
        "This unsafe block needs a safety comment. It should explain that the resulting pointer is in-bounds (or one after the end) of the `self.wakers` array.",
        "It seems like there's an extra call to `.cast()` here?",
        "Yes, using several `let` statements is good. It makes it clear what the types are."
      ],
      "tokio-test-diverse-configurations": [
        "That is fine. We may want to consider whether it makes sense to run with both?"
      ],
      "tokio-organize-code-logically": [
        "Please put them in a sub-module so they don't clutter the front page of the crate documentation.",
        "Defining an inline module like you suggested in the beginning is fine. (But the other approaches would not be considered breaking.)",
        "No reason to expose a module with one item.\r\n```suggestion\r\nmod abort_on_drop;\r\npub use abort_on_drop::AbortOnDropHandle;\r\n```",
        "Please split this into a separate file. Rustfmt doesn't work inside the macros, so I don't want large codeblocks inside them."
      ],
      "tokio-graceful-error-handling": [
        "I think we probably want to just fall back to existing behavior if `stream_position` fails.\r\n```suggestion\r\n        let pos = std.stream_position().unwrap_or(0);\r\n```",
        "Using a debug assertion with a `--cfg` to disable it seems preferable to the `eprintln!`.",
        "You could implement the `source` method here.",
        "Let's not ignore errors.\r\n```suggestion\r\n///     compress_data(reader).await?;\r\n```",
        "Can you add `#[track_caller]` and a [test for the panic location](https://github.com/tokio-rs/tokio/blob/master/tokio/tests/rt_panic.rs)?"
      ],
      "tokio-flexible-consistent-api-patterns": [
        "I think that the `poll_proceed` / `made_progress` API we use internally is good. If we're going to expose more of coop, then I think we should expose that API instead of inventing a new one.\r\n\r\nBut I wouldn't do both \"expose coop API\" and \"make select coop aware\" in one PR. I'd like two PRs for the changelog.",
        "This provides a single object that is both the reader and writer, but in practice I think people will want those to be two different objects.",
        "I don't like this name. It sounds like something that would immediately make something abort.\r\n\r\nHow about `spawn_with_drop_handle` or `spawn_with_abort_handle`?",
        "Another possibility is to make this a constructor method. Then you would type `DropHandle::spawn`.",
        "If all of the `*acquire_many` methods already allow zero permits, then I see no harm in allowing it here. I would perhaps all the method `num_permits`?"
      ],
      "tokio-structure-feature-flags-strategically": [
        "Some of these look redundant. Don't we always have libc on linux?",
        "Ok, that's fine.",
        "Hmm. I'll have to think about what implications this has for our MSRV.",
        "I'm okay with this, but I don't want to require hashbrown if you're just using the `TaskTracker`. Can you add a `join_map` feature?",
        "Would this be sufficient?\r\n```suggestion\r\njoin-map = [\"tokio/rt\", \"hashbrown\"]\r\n```",
        "We can always mark `mod task` with `#[cfg(any(rt, join_map))]`.",
        "We usually do it like this:\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/32970527633bb72fc4f01d02523484a9376ac26a/tokio/src/lib.rs#L1\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/32970527633bb72fc4f01d02523484a9376ac26a/.github/workflows/ci.yml#L1050-L1065\r\n\r\nIt may be worth to switch, but for now I would add a `#[allow(unexpected_cfgs)]` in the source code."
      ],
      "tokio-optimize-job-structure": [
        "Please don't install multiple Rust versions in a single CI run. It causes confusion about which version of Rust is actually being used in each call. Instead, create a new CI run for this check.",
        "The miri job is starting to take a rather long time. Could you add a new job for this, instead of adding additional work to the existing job? This will allow for parallelism.",
        "That's nice. The loom jobs are already much longer than that. We could consider gating it similarly to what we did for loom tests."
      ],
      "tokio-optimize-algorithmic-efficiency": [
        "This iterates all of the buffers every time, even if we only write a few of them. If the buffers are very long and this is called in a loop, that gives quadratic performance.\r\n\r\nWe should be able to embed this logic inside the for loop instead to avoid that.",
        "No, `b.len()` is constant time. Instead, it's O(n) in the length of `bufs`, which you iterate over.",
        "I would expect that `saturating_pow` is rather expensive compared to a shift.\r\n```suggestion\r\n        let max_number = match 1.checked_shl(8 * self.length_field_len) {\r\n            Some(shl) => shl - 1,\r\n            None => u64::MAX,\r\n        };\r\n```",
        "I think the better way to check this is to use this:\r\n```rs\r\nnow.saturating_duration_since(timeout) > Duration::from_millis(5)\r\n```"
      ],
      "tokio-granular-feature-flags": [
        "Hmm. I'll have to think about what implications this has for our MSRV.",
        "I'm okay with this, but I don't want to require hashbrown if you're just using the `TaskTracker`. Can you add a `join_map` feature?",
        "Would this be sufficient?\r\n```suggestion\r\njoin-map = [\"tokio/rt\", \"hashbrown\"]\r\n```",
        "We can always mark `mod task` with `#[cfg(any(rt, join_map))]`.",
        "Let me put that in its own PR so it gets a changelog entry. #6541"
      ],
      "tokio-memory-ordering-needs-justification": [
        "Wait, does this PR not eliminate all usages of SeqCst?",
        "I don't know how your detector works, but mixing SeqCst and non-SeqCst orderings is never the right answer. Never.\r\n\r\nLooking over the code, I notice that there are atomics in both the `Notify`, but also the waiters. Before I can accept a PR relaxing any of them, I'm going to have to review the code for whether it currently relies on the SeqCst guarantees to synchronize between those different atomics. ",
        "Here you have exclusive access, so just perform a normal write."
      ],
      "tokio-secure-unsafe-code": [
        "Please include a short safety comment on why each unsafe block is okay.",
        "We need to also mark `Blocking::new` as unsafe and require that `self.inner` satisfies the requirement. Otherwise this unsafe block isn't really correct.",
        "Please reduce the scope of this unsafe block. You only need it for the `&mut *tail` operation as far as I can tell.\r\n```Rust\r\nlet tail_block = unsafe { &mut *tail };\r\n```",
        "Generally, it is preferred to have a separate unsafe block for each unsafe operation, and to annotate each block with a `// SAFETY:` comment justifying its correctness."
      ],
      "tokio-minimize-unsafe-code": [
        "Please include a short safety comment on why each unsafe block is okay.",
        "Please reduce the scope of this unsafe block. You only need it for the `&mut *tail` operation as far as I can tell.\r\n```Rust\r\nlet tail_block = unsafe { &mut *tail };\r\n```",
        "Generally, it is preferred to have a separate unsafe block for each unsafe operation, and to annotate each block with a `// SAFETY:` comment justifying its correctness."
      ],
      "tokio-release-locks-before-waking": [
        "Please add logic to `rt_multi_thread_available` to detect whether `--cfg tokio_unstable` is set and emit a hard error if it is not.",
        "The error should be similar to this error:\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/2506c9fa9916a1bdffbc762f7eb2ae5c2fd23836/tokio-macros/src/entry.rs#L193-L197",
        "I think both are okay.",
        "Doesn't that also trigger other unrelated errors when it fails?\r\n\r\nWhat we do today is essentially define the `main` macro two times to pass different values to [the `rt_multi_thread` boolean](https://github.com/tokio-rs/tokio/blob/master/tokio-macros/src/entry.rs#L488), and have `lib.rs` do this:\r\n```rs\r\n#[cfg(feature = \"rt-multi-thread\")]\r\npub use tokio_macros::main;\r\n#[cfg(not(feature = \"rt-multi-thread\"))]\r\npub use tokio_macros::main_rt as main;\r\n```\r\nThis way, the macro knows based on the cfgs that apply in the main Tokio crate. We could use the same approach and have four macros.",
        "I don't think that a shutdown future is an amazing example here. That makes sense for `select!`, but you wouldn't really have a shutdown future in `join!`. Not sure what a better example would be, though.",
        "Please make sure that the section names for `try_join!` match those we have for `join!`.",
        "```suggestion\r\n        /// place the shutdown future earlier in the `try_join!` list to ensure that it is\r\n```",
        "The current logic exists to reuse the thread parker logic when `block_on` is used many times. What is the reason for not reusing it?",
        "This is proposing to add them as stable despite the `LocalSet` question?",
        "The way we usually handle this kind of thing is to move _all_ entries in the list to another list (using the guarded linked list), and then we wake entries from the guarded list in batches. Can we not do the same here?",
        "I'm worried about panics here. What happens to entries still in the guarded list?\r\n\r\nI think some other examples of this wrap the guarded list in something that removes all entries from the list without waking their wakers."
      ],
      "tokio-design-flexible-apis": [
        "Case 2 cannot happen since `SetOnce` isn't a channel that you clone. The return value can be a bare `&T`.",
        "Yeah ... I'm not sure what is the least surprising here.",
        "If the intent of this PR is a way to detect whether polling the channel will panic, then having a function that does exactly that sounds good to me.",
        "If the limit is zero, then I think it would be okay to just return 0 immediately.",
        "Hmm. I think immediately returning 0 is the least surprising behavior. After all, you asked to receive zero messages, and you got zero messages.",
        "If what is acceptable?\r\n\r\nI mainly see people be confused about length zero reads when they do this:\r\n```rs\r\nlet mut vec = Vec::with_capacity(1024);\r\nio.read(&mut vec);\r\n```\r\nand are surprised because `&mut vec` becomes a slice of length zero.\r\n\r\nBut that doesn't happen in our case.",
        "I think allowing a zero length is useful for cases where the length is computed dynamically. I've seen code along these lines:\r\n```rs\r\nlet len = io.read_u64().await? as usize;\r\nlet mut buffer = Vec::with_capacity(len);\r\nio.take(len).read_to_end(&mut buffer)?;\r\n```\r\nif there are length-zero messages, then the above makes use of `take(0)` behavior sometimes.",
        "If all of the `*acquire_many` methods already allow zero permits, then I see no harm in allowing it here. I would perhaps all the method `num_permits`?"
      ],
      "tokio-use-option-methods-idiomatically": [
        "Can you change this to only create a mutable reference if it is `None`?",
        "Yes, thanks for the suggestion."
      ]
    },
    "profile": {
      "location": "Denmark",
      "company": "Google",
      "blog": "https://ryhl.io/",
      "site_admin": false,
      "followers": 2233,
      "following": 5
    }
  }
}