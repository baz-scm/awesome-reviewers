{
  "bo156": {
    "repos": [
      "bridgecrewio/checkov"
    ],
    "entries": [
      {
        "slug": "checkov-backward-compatible-parameters",
        "title": "Backward compatible parameters"
      },
      {
        "slug": "checkov-centralize-environment-variables",
        "title": "Centralize environment variables"
      },
      {
        "slug": "checkov-choose-optimal-algorithms",
        "title": "Choose optimal algorithms"
      },
      {
        "slug": "checkov-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "checkov-consistent-naming-conventions",
        "title": "Consistent naming conventions"
      },
      {
        "slug": "checkov-ensure-dependency-compatibility",
        "title": "Ensure dependency compatibility"
      },
      {
        "slug": "checkov-extract-focused-functions",
        "title": "Extract focused functions"
      },
      {
        "slug": "checkov-meaningful-identifier-names",
        "title": "Meaningful identifier names"
      },
      {
        "slug": "checkov-preserve-api-compatibility",
        "title": "Preserve API compatibility"
      },
      {
        "slug": "checkov-strategic-error-handling",
        "title": "Strategic error handling"
      },
      {
        "slug": "checkov-strategic-exception-management",
        "title": "Strategic exception management"
      },
      {
        "slug": "checkov-support-all-target-environments",
        "title": "Support all target environments"
      },
      {
        "slug": "checkov-thorough-test-assertions",
        "title": "Thorough test assertions"
      },
      {
        "slug": "checkov-use-appropriate-logging-levels",
        "title": "Use appropriate logging levels"
      },
      {
        "slug": "checkov-use-pytest-best-practices",
        "title": "Use pytest best practices"
      },
      {
        "slug": "checkov-write-pythonic-code",
        "title": "Write pythonic code"
      }
    ],
    "comments": {
      "checkov-extract-focused-functions": [
        "looks like an almost identical copy of a previous usage, please extract to function",
        "1. do we have a const for edge label? if yes use it, if not just create a const for this str.\r\n2. extract it to a function called `create_virtual_resources_edges`, which can be called from the `create_edges` function as well",
        "can you extract this one-liner to a function to make it more readable? \r\nalso `existing_tuple` is an unclear name",
        "as this part begins to be a bit big - what do you think about splitting the `if/else` block to a new function called `_generate_graphs_list` (or something like that)?",
        "would split the creation of this dict to a function like `_build_dirs_to_definitions`"
      ],
      "checkov-centralize-environment-variables": [
        "this function has the same signature across all graph managers, adding this parameter here breaks it.\r\nassuming you don't want to add it across all frameworks, I can suggest controlling this behaviour with an env var rather than a parameter:\r\n1. Add a matching env var under the file `checkov/common/util/env_vars_config.py`, with default False.\r\n2. In the specific function you wanted to use it, check for the value (using import) and define behavior accordingly",
        "this line should be under the `env_var_config.py` rather than here (under the `common` dir with the rest of the env vars)",
        "please use `config_env_vars.py` to add new env vars to checkov :) ",
        "all env vars in checkov are handled in `checkov/common/util/env_vars_config.py`, please add it there and use from import",
        "when adding a new env_var in checkov, you should add it in `checkov/common/util/env_vars_config.py` and use it from there, allows better handling of those variables throughout the code ",
        "We also have a file called `env_vars_config` with the full state of all env vars, could you also add it there and use it instead?\r\nThis gives us a unified place to handle all env vars, and also if we need to import them in multiple places we can do it easily :)"
      ],
      "checkov-strategic-error-handling": [
        "can this part result in a `StopIteeration` being thrown?"
      ],
      "checkov-choose-optimal-algorithms": [
        "good idea - fixed",
        "Just didn't watch to change api"
      ],
      "checkov-choose-optimal-data-structures": [
        "good idea - fixed",
        "Just didn't watch to change api"
      ],
      "checkov-meaningful-identifier-names": [
        "why not use `list(set(new_value))`.\r\nalso `new_value` is a bad name, I would call it `list_updated_value` or something like that",
        "Should probably be called `TerraformJsonRunner` to avoid multiple instances of the name `Runner` in the code",
        "can you rename `each` to something like `resource` or `changed_resource`?\r\nWill be more readable in python code (even though I guess it comes from terraform conventions)",
        "Maybe change the name to `pickle_deepcopy`?\r\nI just want the IDE to complete me automatically to the correct deepcopy, \r\nto avoid mistakenly importing the wrong one.\r\nAlso, it will be clearer to new developers in the future that this is something internal"
      ],
      "checkov-use-pytest-best-practices": [
        "this will set it for the rest of the process, you can use `monkeypatch` instead to set it just for the test",
        "that's not common practice, please use mocks for the test using `monkeypatch`",
        "we have this parametrization in other places in the code, you can also use `pytest.mark.parametrize`",
        "maybe use `pytest` instead of `unittest` for new tests? "
      ],
      "checkov-backward-compatible-parameters": [
        "why do you need to send the `graph` here as well? (instead of always using `get_reader_endpoint`)",
        "sound right, hadn't though about it.",
        "fixed"
      ],
      "checkov-write-pythonic-code": [
        "why not just go over `d.values()` if you don't use the key"
      ],
      "checkov-ensure-dependency-compatibility": [
        "Note that it seems like this version doesn't support python3.8, which is still supported in checkov. Try to find a version which still has support for it"
      ],
      "checkov-preserve-api-compatibility": [
        "fixed",
        "why do you need to send the `graph` here as well? (instead of always using `get_reader_endpoint`)",
        "sound right, hadn't though about it.",
        "fixed"
      ],
      "checkov-support-all-target-environments": [
        "Note that it seems like this version doesn't support python3.8, which is still supported in checkov. Try to find a version which still has support for it",
        "why did you lock the version? it will prevent getting those updates which are important",
        "I also don't want you to change it so we will be locked, as no one will update mypy if you do so. \r\nIf you wish to separate the fixes from this PR, you can open another one which only updates mypy to the latest version and fixes the relevant issues "
      ],
      "checkov-consistent-naming-conventions": [
        "use `tuple` and `str | None` instead of `Tuple` and `Optional` - the rest of the code here already uses this syntax so no need to mix",
        "the typing here is inconsistent - can you use `str | None` instead of `Optional[str]`?",
        "done",
        "can you rename `each` to something like `resource` or `changed_resource`?\r\nWill be more readable in python code (even though I guess it comes from terraform conventions)",
        "Maybe change the name to `pickle_deepcopy`?\r\nI just want the IDE to complete me automatically to the correct deepcopy, \r\nto avoid mistakenly importing the wrong one.\r\nAlso, it will be clearer to new developers in the future that this is something internal"
      ],
      "checkov-strategic-exception-management": [
        "can this part result in a `StopIteeration` being thrown?"
      ],
      "checkov-thorough-test-assertions": [
        "I would add specific unit tests to this function as well, as this is critical logic",
        "nice test!\r\nHowever, I still think we need some more for the logic - maybe add 1 with more than 1 created module.\r\nyou can find a good example under `tests/terraform/graph/resources/modules`.\r\nThis one even utilizes different modules with a reference to other directories, so I feel it will be a very strong test.\r\n(You don't have to take the whole folder as it might be also too big, maybe just one sub directory like `stacks`)"
      ],
      "checkov-use-appropriate-logging-levels": [
        "maybe warning is better than debug? just because we won't see debug logs by default",
        "Wouldn't the log be now `Done persisting <random-number> resource_subgraph_maps`? this might confuse us as we only save 1 map. maybe instead log the bucket/key/repo or something like that"
      ]
    }
  },
  "treo": {
    "repos": [
      "deeplearning4j/deeplearning4j"
    ],
    "entries": [
      {
        "slug": "deeplearning4j-centralize-dependency-management",
        "title": "Centralize dependency management"
      },
      {
        "slug": "deeplearning4j-clean-up-your-code",
        "title": "Clean up your code"
      },
      {
        "slug": "deeplearning4j-clear-descriptive-identifiers",
        "title": "Clear descriptive identifiers"
      },
      {
        "slug": "deeplearning4j-compare-floating-point-safely",
        "title": "Compare floating-point safely"
      },
      {
        "slug": "deeplearning4j-configurable-resource-locations",
        "title": "Configurable resource locations"
      },
      {
        "slug": "deeplearning4j-descriptive-error-context",
        "title": "Descriptive error context"
      },
      {
        "slug": "deeplearning4j-document-ai-implementation-references",
        "title": "Document AI implementation references"
      },
      {
        "slug": "deeplearning4j-document-api-completely",
        "title": "Document API completely"
      },
      {
        "slug": "deeplearning4j-eliminate-redundant-code",
        "title": "Eliminate redundant code"
      },
      {
        "slug": "deeplearning4j-keep-configurations-current",
        "title": "Keep configurations current"
      },
      {
        "slug": "deeplearning4j-minimize-object-allocations",
        "title": "Minimize object allocations"
      },
      {
        "slug": "deeplearning4j-modular-adaptive-configurations",
        "title": "Modular adaptive configurations"
      },
      {
        "slug": "deeplearning4j-numerical-stability-practices",
        "title": "Numerical stability practices"
      },
      {
        "slug": "deeplearning4j-preserve-api-compatibility",
        "title": "Preserve API compatibility"
      },
      {
        "slug": "deeplearning4j-prevent-memory-leaks",
        "title": "Prevent memory leaks"
      },
      {
        "slug": "deeplearning4j-remove-debugging-artifacts",
        "title": "Remove debugging artifacts"
      },
      {
        "slug": "deeplearning4j-use-appropriate-logging-levels",
        "title": "Use appropriate logging levels"
      },
      {
        "slug": "deeplearning4j-use-logging-best-practices",
        "title": "Use logging best practices"
      },
      {
        "slug": "deeplearning4j-validate-and-document-nulls",
        "title": "Validate and document nulls"
      }
    ],
    "comments": {
      "deeplearning4j-document-ai-implementation-references": [
        "The 1e9 is used by the tensor2tensor package as well. Bert on the other hand uses just 1e4, while GPT-2 uses 1e10."
      ],
      "deeplearning4j-configurable-resource-locations": [
        "We've had issues with people creating uberjars and not packaging resources previously. So having something that is dependent on loading a resource may introduce new problems. \r\n\r\nIn particular, Spring Boot has a special way of class loading and resource loading. I can imagine that it may break there."
      ],
      "deeplearning4j-minimize-object-allocations": [
        "good idea, I'll change it accordingly in the createBias method as well then."
      ],
      "deeplearning4j-use-logging-best-practices": [
        "Does it make sense to keep this at info log level? or should this maybe move to the debug log level, as you did with the logging in the other branch?"
      ],
      "deeplearning4j-keep-configurations-current": [
        "Have you cleared this with eclipse EMO? I remember that they needed to clear any additional dependency or vendoring.",
        "We should probably update this to match the suggested maven version for the rest of DL4J - Or remove the maven wrapper entirely.",
        "Got an answer to that question?"
      ],
      "deeplearning4j-use-appropriate-logging-levels": [
        "printf outside of an error condition.",
        "shouldn't this be an sd_debug? Same applies to other usages of sd_printf and printIndexedBuffer in this file",
        "is this supposed to be all `sd_printf`? Or did you want `sd_debug` instead?"
      ],
      "deeplearning4j-centralize-dependency-management": [
        "If possible, let's not use rc versions.",
        "It would probably make sense to use the property defined above in all of those jmh dependencies "
      ],
      "deeplearning4j-modular-adaptive-configurations": [
        "I'd probably split this some more into a base command and extension parameters. \r\n\r\nSomething like\r\n```bash\r\nif [ \"${HELPER}\" != '' ] && [ \"${EXTENSION}\" != '' ]; then\r\n    mvn_ext=\"-Djavacpp.platform.extension=-${{ matrix.helper }}-${{ matrix.extension }} -Dlibnd4j.helper=${{ matrix.helper }} -Dlibnd4j.extension=${{ matrix.extension }}\"\r\nelif [ \"${HELPER}\" != '' ]; then\r\n    mvn_ext=\"-Djavacpp.platform.extension=-${{ matrix.helper }} -Dlibnd4j.helper=${{ matrix.helper }} -Dlibnd4j.extension=${{ matrix.extension }}\"\r\nelse\r\n    mvn_ext=\"-Djavacpp.platform.extension=${{ matrix.extension }} -Dlibnd4j.helper=${{ matrix.helper }} -Dlibnd4j.extension=${{ matrix.extension }}\"\r\nfi\r\n\r\ncommand=\"mvn -Possrh ${mvn_ext} -Dlibnd4j.buildThreads=${{ github.event.inputs.buildThreads }}  -Djavacpp.platform=linux-x86_64 -Dlibnd4j.chip=cuda --also-make -Pcuda clean --batch-mode package deploy -DskipTests\"\r\n```\r\n\r\nAnd having refactored it like that, I wonder, why do you even specify those values in the cases where they aren't defined?",
        "If you want to make the hack hackier, you can replace the version this with\r\n```\r\nsudo cp /usr/lib/gcc/x86_64-linux-gnu/`gcc --version | head -n 1 | grep -o '[^ ]*$'` /usr/lib\r\n```"
      ],
      "deeplearning4j-eliminate-redundant-code": [
        "Looks like a PR breakout bug? The constant is defined twice. ",
        "printf outside of error condition. ",
        "Any reason why this files has soo many licence headers?"
      ],
      "deeplearning4j-clean-up-your-code": [
        "left over debug prints?",
        "I guess this print is a left over from debugging?",
        "Please don't leave any commented out code around.",
        "Please don't leave commented out code around.",
        "Please don't leave any commented out code to hang around",
        "please don't leave code commented out like that.",
        "let's not have commented out code. If this is just temporary, put at least a TODO there, so it is obvious that this is still work in progress. Otherwise it is easy to miss this when you do the cleanup. ",
        "Please don't leave any commented out code behind.",
        "Can we turn this magic number into a constant? "
      ],
      "deeplearning4j-remove-debugging-artifacts": [
        "Still contains a print.",
        "left over debug printing?",
        "printf outside of error condition.",
        "printf outside of error condition.",
        "printf outside of error condition.",
        "Let's not have commented out code like that. And in particular not have a for loop doing nothing. ",
        "Unconditional printf. I guess a leftover from debugging?",
        "Please don't leave any commented out code around. Either remove it or keep it, but just commenting things out without any indication for why that is, shouldn't make it into a merged PR.",
        "Please don't leave any commented out code around"
      ],
      "deeplearning4j-descriptive-error-context": [
        "While \"Failed execution\" is a better message than \"Boom\", I wonder if we can't have something more descriptive here?\r\n\r\nAlso, Do we need to both print and throw an exception? "
      ],
      "deeplearning4j-preserve-api-compatibility": [
        "When adding new arguments, I think it makes sense to make them optional in order to keep backwards compatibility with existing code.",
        "This will probably break some backwards compatibility - at least for people that are using Op objects directly.\r\n\r\nI think we can probably accept that breakage, as most people will be using the factory methods and for them it should be transparent."
      ],
      "deeplearning4j-compare-floating-point-safely": [
        "Is 0.01 really a good default value for this? Typical epsilon values are 1e-6, 1e-7 or even 1e-12 for double. "
      ],
      "deeplearning4j-prevent-memory-leaks": [
        "Missing a delete for this one?"
      ],
      "deeplearning4j-validate-and-document-nulls": [
        "Do we need the `null` case anywhere? Or wouldn't it make more sense to handle `null` in the `outputVariables()` function, so that everything downstream doesn't need to have special case handling?"
      ],
      "deeplearning4j-clear-descriptive-identifiers": [
        "I guess a better name here would be just `releaseGilAutomatically`."
      ],
      "deeplearning4j-numerical-stability-practices": [
        "The paper authors have found that a smaller default epsilon works better:\r\n\r\n> Epsilon in AdaBelief is different from Adam (typically eps_adabelief = eps_adam*eps_adam)\r\n> ( eps of Adam in Tensorflow is 1e-7, in PyTorch is 1e-8, need to consider this when use AdaBelief in Tensorflow)\r\n\r\nSo I guess we might want to use either 1e-14 or 1e-16 here. ",
        "maybe also check that 0 <= maskTokenProb + randomTokenProb <= 1?",
        "Should this even happen? Calculating with a zero gradient, should basically be \"don't do any change at all\" and therefore we shouldn't be performing backprop at all in that case?",
        "got it. Maybe add the explanation to the comment on top?"
      ],
      "deeplearning4j-document-api-completely": [
        "It may make sense to put this as Javadoc on the individual Enums? ",
        "Again, a comment why this is empty would be useful."
      ]
    }
  },
  "byroot": {
    "repos": [
      "rails/rails"
    ],
    "entries": [
      {
        "slug": "rails-configure-at-proper-scope",
        "title": "Configure at proper scope"
      },
      {
        "slug": "rails-contextual-error-messages",
        "title": "Contextual error messages"
      },
      {
        "slug": "rails-database-specific-query-optimization",
        "title": "Database-specific query optimization"
      },
      {
        "slug": "rails-document-performance-implications",
        "title": "Document performance implications"
      },
      {
        "slug": "rails-efficient-data-processing",
        "title": "Efficient data processing"
      },
      {
        "slug": "rails-minimize-public-api-surface",
        "title": "Minimize public API surface"
      },
      {
        "slug": "rails-minimize-unnecessary-object-allocations",
        "title": "Minimize unnecessary object allocations"
      },
      {
        "slug": "rails-optimize-cache-headers",
        "title": "Optimize cache headers"
      },
      {
        "slug": "rails-organize-tests-for-clarity",
        "title": "Organize tests for clarity"
      },
      {
        "slug": "rails-place-configurations-appropriately",
        "title": "Place configurations appropriately"
      },
      {
        "slug": "rails-prefer-simpler-expressions",
        "title": "Prefer simpler expressions"
      },
      {
        "slug": "rails-self-documenting-identifier-names",
        "title": "Self-documenting identifier names"
      }
    ],
    "comments": {
      "rails-database-specific-query-optimization": [
        "> About a new class I don't know what is better\r\n\r\nMutating the AST prior to calling the visitor is how it's supposed to work. Now sometimes it's a bit inconvenient and it turns out simpler to do what you've done.\r\n\r\nI'd say give it a try, but feel free to push back if that sounds too messy to you.",
        "I'm not sure I understand that `||` condition, could you explain what this is trying to match for?",
        "> What this `||` checks, is whether there is any left joins on the target which we have to workaround and resort to making a subselect query instead.\r\n\r\nThat is where I'm confused, this `if` condition is for case where we don't have to fallback to a a subquery. So why `||`, wasn't this meant to be `&&`? (Sorry, I'm not the best with Arel API)"
      ],
      "rails-efficient-data-processing": [
        "This is a bug. \r\n\r\n```suggestion\r\n    @renderables_by_class = Hash.new { |h, k| h[k] = {} }\r\n```"
      ],
      "rails-document-performance-implications": [
        "I think that's where we might as misunderstood each others.\r\n\r\nI don't think there is any value in supporting 6 different checksum algorithms for S3, the goal is just to change the current MD5 for something else (and to support both concurrently to allow smooth migration).\r\n\r\nSo we shouldn't add these 5, just one of these.",
        "The performance impact really isn't that bad, and if anything, production is really where query logs are important.\r\n\r\nI think a simple note saying that prepared statements will be disabled is enough."
      ],
      "rails-minimize-unnecessary-object-allocations": [
        "The solution proposed at the end of https://github.com/rails/rails/issues/54335#issue-2807665461 doesn't allocate and is relatively simple too, but up to you.",
        "Yes, at this point the methods aren't doing much, so the bottleneck is the method call itself.\r\n\r\nHence why I went with a case with inline snippets that reduce the number of method calls.\r\n\r\nI also have a non-recursive version, that push and pop the nodes on an array, but apparently it's slower.",
        "`try` is awful for performance, we shouldn't use that. We should check the type of the node instead."
      ],
      "rails-organize-tests-for-clarity": [
        "All these \"fixture\" jobs could be defined inside the test classes themselves. ",
        "Thanks. IMHO it's much easier to understand tests when the job they rely on is just above."
      ],
      "rails-optimize-cache-headers": [
        "```suggestion\r\n      if path.start_with?(\"/assets/\")\r\n```\r\n\r\nI think just checking the directory is enough. If you put non-immutable assets in `/assets/` that's on you.",
        "What's the use case for downloading it?",
        "> Is there a downside to sticking with a more precise match?\r\n\r\nYes, the digest implementation may change, and the regexp stop matching.",
        "```suggestion\r\n        \"public, max-age=#{1.hour.to_i}\"\r\n```\r\n\r\nYou can go even lower. 1 or 5 minutes just to avoid thundering herds should be plenty.",
        "I improved the defaults with `immutable` and `stale-while-revalidate`."
      ],
      "rails-prefer-simpler-expressions": [
        "I know you just moved it over, but I wonder why this isn't just:\r\n\r\n```suggestion\r\n          unless ActiveStorage.supported_image_processing_methods.include?(method_name)\r\n```",
        "We require 3.2 now, so you can use an anonymous block\r\n\r\n```suggestion\r\n      def add_binds(binds, proc_for_binds = nil, &)\r\n```"
      ],
      "rails-minimize-public-api-surface": [
        "I think we'd rather expose a `add_middleware` method or something like that, and that should be enough as a public API.",
        "Is making the argument optional really this necessary? Allocating a `Method` instance and then its parameters isn't cheap.\r\n\r\nIt's actually very expensive once you compare it to a simple method call. ",
        "Right. This isn't a hill I'm gonna die on, and indeed in this specific case it is probably fine.\r\n\r\nI just tend to question more and more how helpful this type of DSL heavy API design really is. Aside from the performance consideration it makes the code harder to grep for / statically analyze for not a huge gain in term of readability:\r\n\r\ne.g.\r\n```ruby\r\nstep :finalize\r\n```\r\n\r\nIs only marginally nicer than:\r\n\r\n```ruby\r\nstep :finalize { finalize }\r\n```\r\n\r\nBut:\r\n\r\n  - Will be qualified of \"magic\".\r\n  - Will defeat JITs.\r\n  - Will dead code analysis harder.\r\n  - Requires you to know the continuation API well to understand which method is called. It's not immediately obvious to the uninitiated that `step :foo` does call `foo`.\r\n\r\nBut again, if you're set on it, I'm not gonna argue you forever on this.\r\n"
      ],
      "rails-self-documenting-identifier-names": [
        "I think it would be simpler if we assume `checksum_implementation` respond to `new` and `<<`.",
        "Also I wonder if `line` really makes sense as a name, as it's not just the line, but the whole location. Maybe `location` or `source_location`?"
      ],
      "rails-configure-at-proper-scope": [
        "This should be set in `activerecord/railtie.rb` like the others, otherwise it risks being overidden by users."
      ],
      "rails-contextual-error-messages": [
        "When writing error message, it's best to include the faulty value."
      ],
      "rails-place-configurations-appropriately": [
        "If this is fully global, perhaps it would be better to define it on `ActiveRecord` rather than `ActiveRecord::Base`.\r\n\r\nThis way it doesn't look like it's a per model or per db_config action.",
        "I think we could simply allow:\r\n\r\n```suggestion\r\nconfig.yjit = { stats: true }\r\n```\r\n\r\n"
      ]
    }
  },
  "Janpot": {
    "repos": [
      "mui/material-ui"
    ],
    "entries": [
      {
        "slug": "material-ui-consistent-package-naming",
        "title": "Consistent package naming"
      },
      {
        "slug": "material-ui-document-implementation-decisions",
        "title": "Document implementation decisions"
      },
      {
        "slug": "material-ui-effect-hook-best-practices",
        "title": "Effect hook best practices"
      },
      {
        "slug": "material-ui-explicit-configuration-resolution",
        "title": "Explicit configuration resolution"
      },
      {
        "slug": "material-ui-explicit-configuration-specifications",
        "title": "Explicit configuration specifications"
      },
      {
        "slug": "material-ui-maintain-configuration-accuracy",
        "title": "Maintain configuration accuracy"
      },
      {
        "slug": "material-ui-optimize-cicd-infrastructure",
        "title": "Optimize CI/CD infrastructure"
      },
      {
        "slug": "material-ui-parameterize-build-processes",
        "title": "Parameterize build processes"
      },
      {
        "slug": "material-ui-secure-api-endpoints",
        "title": "Secure API endpoints"
      },
      {
        "slug": "material-ui-standardize-build-configurations",
        "title": "Standardize build configurations"
      },
      {
        "slug": "material-ui-use-design-system-tokens",
        "title": "Use design system tokens"
      },
      {
        "slug": "material-ui-use-direct-path-imports",
        "title": "Use direct path imports"
      }
    ],
    "comments": {
      "material-ui-parameterize-build-processes": [
        "The build script creates it. The icons package is a bit special as it bakes built files into the repo. This package.json conflicts a bit with the aliasing setup for the docs. I didn't necessarily want to add a special case to the build for a single package, but we could.",
        "I'm adding a `--skipEsmPkg`. ",
        "Yes, we've used verdaccio before for testing publishes. It allowed us to also test an install from registry."
      ],
      "material-ui-effect-hook-best-practices": [
        "Is this operating on the same key as `useLocalStorageState`? Can't it just use the hook instead? I don't think the storage value should be manipulated by anything else, it will definitely lead to bugs.",
        "The way I envisioned this was\r\n\r\n```tsx\r\n// have these hooks available in the docs\r\nfunction usePaletteModeUserPreference() {\r\n  const [mode, setMode] = useLocalStorageState('colorScheme', 'system');\r\n  return { mode, setMode };\r\n}\r\n\r\nfunction useCurrentPaletteMode() {\r\n  const { mode: userMode } = usePaletteModeUserPreference();\r\n  const systemColor = useSystemColor();\r\n  return userMode === 'system' ? systemColor : userMode;\r\n}\r\n\r\n\r\n// for theme provider\r\nconst paletteMode = useCurrentPaletteMode()\r\nconst theme = paletteMode === 'dark' ? darkTheme : lightTheme;\r\n\r\n// anywhere for theme switchers\r\nconst { mode, setMode } = usePaletteModeUserPreference();\r\n<Button active={mode === 'dark'} onClick={() => setMode('light')}>dark</Button>\r\n<Button active={mode === 'light'} onClick={() => setMode('dark')}>light</Button>\r\n```\r\n\r\nThe nice thing about `useLocalStorage` is that it acts as global state, so you don't ever need to add a shared state variable somewhere and create a context for it. Just the hook works anywhere you use it, it handles state outside of React. This system worked very well for the Toolpad app theme switcher. I might be oversimplifying for the docs, I'd have to look deeper into it, there may be blockers to adopt the same way of doing a theme switcher in the core docs"
      ],
      "material-ui-consistent-package-naming": [
        "Didn't want to change too much the commands that are already in @brijeshb42 muscle memory, but happy to change it. Do you think we should also rename `@app/vite-app` then?",
        "This is not publishable, nor a dependency of other packages. it's only here to allow us to have it able to declare its own dependencies. I think the naming conventions shouldn't apply, or we should create a new category of naming. In testing there are more such packages, e.g. to be able have typescript tests run on build artifacts instead of aliases. We've been naming some of those with `@mui/test-`, in apps folder they are named with `@app/`. How do you feel about this?",
        "Ok, I'm just removing the name altogether, problem solved ðŸ™‚ ",
        "> this would become `@mui/private-chat-uikit`.\r\n\r\nIf it's marked as private in package.json, why does it need another marker in the name? I don't see the benefit, the goal with internal is to signal something to people that want to install it, but private packages aren't published in the first place.\r\nEven with internal packages, it's not like we can ignore semver, our own packages depend on them with a caret range, so they must remain backwards compatible for a whole major anyway. So the signal serves little purpose to end users other than \"there won't be guaranteed continuity across majors\"."
      ],
      "material-ui-standardize-build-configurations": [
        "Maintain backwards compatibility for the legacy `./modern` bundles. Just a re-export of the main exports, but with a `./modern` prefix",
        "it copies .cjs files from the src folder to the build folder. we use them to import cjs only modules such as `next/document`.",
        "Does it make sense to do something different in base than in core?",
        "The idea behind the structure in core was to keep support for node10 resolution. Not sure how you can achieve that with the base structure.\r\nThe current structure makes it too easy to forget properties like `sideEffects` in the subfolder package.json (as is currently hapening in base)\r\nIMO the ideal structure is a flat one with cjs and mjs file extensions. This avoids package.json lookups to determine module type. But that's a bit more difficult to achieve with our current setup.\r\n",
        "> I was surprised to find that this structure supports node10 resolution.\r\n\r\nThe type resolution yes, through `typesVersions`, I'm not sure it would actually resolve the javascript correctly under node10, I don't see how without support for the exports field. But I may be missing something",
        "> Something different in base-ui's package.json is that it lists each export individually\r\n\r\nYes, we're likely also going to move to something like that eventually. There's a `mui-src` condition that is supported by our package.json exports generator that helps with this. e.g. https://github.com/mui/material-ui/blob/7a7c530e07ae959baf1ca660053cc42da2187bbd/packages/mui-material/package.json#L109",
        "Alternatively https://www.npmjs.com/package/tsconfig",
        "> unless we want a resolved tsconfig\r\n\r\nWe're reading properties out of it, so I guess we kind of do.\r\nBut it's also something we can add in follow-up."
      ],
      "material-ui-use-direct-path-imports": [
        "I don't think the problem is tree-shaking during production builds. That should be covered regardless of whether you import top-level or not. It's mostly about avoiding importing from barrel files. This can greatly decrease performance of a dev server. A problem which also exists in vite: https://github.com/vitejs/vite/issues/8237\r\n\r\nPersonally I'm not hugely in favor for recommending babel plugins that mess with our imports. I'd just recommend upgrading to Next.js > 13.5. And really push users towards avoiding barrel imports altogether.\r\n\r\nWe could also suggest them to make vscode not auto-import from top-level:\r\n\r\n```tsx\r\n// .vscode/settings.json\r\n  \"typescript.preferences.autoImportSpecifierExcludeRegexes\": [\"^@mui/[^\\/]+$\"]\r\n```"
      ],
      "material-ui-explicit-configuration-specifications": [
        "ðŸ˜•  This will require the user to add every library that uses emotion in this list?",
        "That's not very useful to recreate the installed environment. First, our packages follow independent versioning so it can't be captured in a single version number. And second, it won't capture transitive dependencies. i.e. you may fix the @mui/material version to 7.3.6, its @mui/utils dependency is defined with caret range, so it may as well install 7.56.12 of that package. Or the bug may have been in vite.\r\nThere will be a lockfile in the sandbox of the reproduction which should be sufficient to recreate and inspect the conditions of the bug.",
        "> it only helps to know it's v4 and not v5 or v6\r\n\r\nWouldn't it make more sense then to just replace \"latest\" with \"<current-major>\"? ",
        "\r\n\r\nI had claude write a little script to search all dependencies under docs/node_modules and to find any that depend on a workspace package. It came up with this:\r\n\r\n```tsx\r\n[\r\n  \"@mui-internal/api-docs-builder\",\r\n  \"@mui/base\",\r\n  \"@mui/docs\",\r\n  \"@mui/icons-material\",\r\n  \"@mui/internal-scripts\",\r\n  \"@mui/lab\",\r\n  \"@mui/material\",\r\n  \"@mui/system\",\r\n  \"@mui/utils\",\r\n  \"@mui/x-charts\",\r\n  \"@mui/x-data-grid\",\r\n  \"@mui/x-data-grid-generator\",\r\n  \"@mui/x-data-grid-premium\",\r\n  \"@mui/x-data-grid-pro\",\r\n  \"@mui/x-date-pickers\",\r\n  \"@mui/x-date-pickers-pro\",\r\n  \"@mui/x-license\",\r\n  \"@mui/x-tree-view\",\r\n  \"@toolpad/core\",\r\n  \"material-ui-popup-state\"\r\n]\r\n```\r\n\r\nIgnoring the internal ones for a second, interesting to see `notistack` isn't in there and `material-ui-popup-state` is. Verified and it seems to check out.\r\n\r\n```suggestion\r\n            'material-ui-popup-state',\r\n```\r\n\r\n_edit: We could even automate the initialization of `hasDependencyOnRepoPackages` if we want. The script seems fast enough._"
      ],
      "material-ui-document-implementation-decisions": [
        "> Why remove codesandbox?\r\n\r\nIt's [broken](https://mui-org.slack.com/archives/C011VC970AW/p1744197923988119). And it doesn't look like they [prioritize](https://github.com/codesandbox/codesandbox-client/issues/8746) at all. I'm bringing back the codesandboxes, we can discuss removing it separately.\r\n\r\n> It looks like the majority of our users go with codesandbox.\r\n\r\nThe numbers may also be a bit skewed as in some places in the docs there is only codesandbox available (e.g. joy templates)",
        "I brought back codesandbox, we can discuss removing it separately"
      ],
      "material-ui-secure-api-endpoints": [
        "Wouldn't this allow random people to delete slack messages in our org?\r\n\r\nPerhaps this needs some form of authentication?",
        "We could configure an API key in the environment and just compare a header/queryparam against it?",
        "Great solution ðŸ‘ ",
        "Should these events be a bit more validated, e.g. make sure they are POST requests? I'm also not quite sure how Netlify deals with CORS.",
        "> Is their another reason than semantic behavior?\r\n\r\nYes, you at least need to refuse GET/HEAD requests for non-idempotent behaviour to avoid someone crafting a link for this request. Links can be followed without user interaction, e.g. bots, retries, prefetches,...\r\nI find that the easiest way is to just refuse any non-POST"
      ],
      "material-ui-optimize-cicd-infrastructure": [
        "```suggestion\r\n- **Keep technical debt in check**. You will make sure we can keep shipping features at a reasonable pace. This includes:\r\n  - Responding to outages and failing CI.\r\n  - Stabilizing and optimizing test infrastructure.\r\n  - Optimizing and improving build tools, linting tools.\r\n  - Improve packaging and publishing flow.\r\n- **Find consensus**. In response to their needs, this role aligns the teams on \"one way\" of doing things and doing it well. You'll need to communicate clearly and openly. You'll be at service to the teams DX needs.\r\n```\r\n\r\nedit: will refine this a bit more, also need to add something about security."
      ],
      "material-ui-explicit-configuration-resolution": [
        "in eslint 9 we will be able to just list the preset as an object"
      ],
      "material-ui-use-design-system-tokens": [
        "I wouldn't expect to see a hard-coded width. Isn't there a way to make the highlight follow the width of the code content? Perhaps it's in the wrong container?"
      ],
      "material-ui-maintain-configuration-accuracy": [
        "ðŸ¤¦  Totally missed this. Opening https://github.com/mui/material-ui/pull/46479"
      ]
    }
  },
  "bracesproul": {
    "repos": [
      "langchain-ai/langchainjs"
    ],
    "entries": [
      {
        "slug": "langchainjs-avoid-hardcoded-configurations",
        "title": "Avoid hardcoded configurations"
      },
      {
        "slug": "langchainjs-chunked-data-processing",
        "title": "Chunked data processing"
      },
      {
        "slug": "langchainjs-consistent-naming-conventions",
        "title": "Consistent naming conventions"
      },
      {
        "slug": "langchainjs-dependency-classification-standards",
        "title": "Dependency classification standards"
      },
      {
        "slug": "langchainjs-eliminate-redundant-code",
        "title": "Eliminate redundant code"
      },
      {
        "slug": "langchainjs-prefer-nullish-coalescing",
        "title": "Prefer nullish coalescing"
      },
      {
        "slug": "langchainjs-preserve-api-backward-compatibility",
        "title": "Preserve API backward compatibility"
      },
      {
        "slug": "langchainjs-simplify-code-organization",
        "title": "Simplify code organization"
      },
      {
        "slug": "langchainjs-throw-meaningful-errors",
        "title": "Throw meaningful errors"
      },
      {
        "slug": "langchainjs-typescript-naming-standards",
        "title": "TypeScript naming standards"
      },
      {
        "slug": "langchainjs-update-documentation-configuration",
        "title": "Update documentation configuration"
      },
      {
        "slug": "langchainjs-use-comprehensive-jsdoc",
        "title": "Use comprehensive JSDoc"
      }
    ],
    "comments": {
      "langchainjs-typescript-naming-standards": [
        "Drop the underscore if they're being used. Typically, variables prefixed with an underscore are unused, and the underscore is used to bypass a lint rule for no-unused-variables",
        "Instead of prefixing with `_` lets just make them `private`/`protected`"
      ],
      "langchainjs-preserve-api-backward-compatibility": [
        "Instead of this check it should be a required param in the `AzureMLChatParams` interface. But why are we requiring users to pass this in? Can we instead keep it optional and have a default formatter that gets used?",
        "This will be a breaking change. Instead we should overload this func so both input types are allowed, and we can narrow the type inside of the method."
      ],
      "langchainjs-use-comprehensive-jsdoc": [
        "This should be a JSDoc so devs & our api refs can get the description properly",
        "Convert to JSDoc and give more of an overview as to what this class can be used for",
        "jsdoc would be nice explaining what it's doing"
      ],
      "langchainjs-eliminate-redundant-code": [
        "Nit, defaults to 1 so this can be dropped.\r\n```suggestion\r\n    return embeddings.flat();\r\n```",
        "If this is only accessed inside the class constructor we don't need to make it a class property.\r\n```suggestion\r\n```"
      ],
      "langchainjs-chunked-data-processing": [
        "Instead of mapping over it, calling `.reduce` would likely be a better method, so we can keep all content fields. Could you push up this refactor? If not I can push up in a little"
      ],
      "langchainjs-prefer-nullish-coalescing": [
        "Prefer `??`. Please change for the rest. Also why is there `+ \"\"`?\r\n```suggestion\r\n      fields.endpointUrl ?? getEnvironmentVariable(\"AZUREML_URL\") + \"\";\r\n```",
        "this is the one area which is technically breaking",
        "ya `??` would work"
      ],
      "langchainjs-dependency-classification-standards": [
        "If we're replacing `@cloudflare/ai` with this, it needs to be moved out of dev deps and into standard deps"
      ],
      "langchainjs-throw-meaningful-errors": [
        "If it's not implemented, it should throw a not implemented error.\r\n```suggestion\r\n    throw new Error(\"AzureMLChatOnlineEndpoint._combineLLMOutput called, but is not implemented.\")\r\n```"
      ],
      "langchainjs-consistent-naming-conventions": [
        "`modelName` is deprecated, prefer `model`\r\n```suggestion\r\nYou can call all models defined on the `litellm_config.yaml`, all you need to do is change the `model` param.\r\n\r\n## Call `\"gpt-azure\"`\r\n```js\r\nimport { ChatOpenAI } from \"@langchain/openai\";\r\n\r\n\r\nconst model = new ChatOpenAI({\r\n  model: \"gpt-azure\",\r\n```",
        "Please update the rest of the `modelName` instances too."
      ],
      "langchainjs-avoid-hardcoded-configurations": [
        "We shouldn't override all if one isn't passed, instead lets do something like this:\r\n```suggestion\r\n    let id = clientId;\r\n    let secret = clientSecret;\r\n    let uri = redirectUri;\r\n    if (!id || !secret || !uri) {\r\n      id = id ?? getEnvironmentVariable(\"OUTLOOK_CLIENT_ID\");\r\n      secret = secret ??  getEnvironmentVariable(\"OUTLOOK_CLIENT_SECRET\");\r\n      uri = uri ?? getEnvironmentVariable(\"OUTLOOK_REDIRECT_URI\");\r\n    }\r\n```"
      ],
      "langchainjs-simplify-code-organization": [
        "Whats the point of wrapping this and not just calling `Npm2Yarn` inside the component?",
        "Moved to `@langchain/core/language_models/base` so it's exported from the same place as `ToolDefinition`"
      ],
      "langchainjs-update-documentation-configuration": [
        "The langchain entrypoint is deprecated, so we shouldn't be adding more features to it. Only add to the community integration"
      ]
    }
  },
  "tristan957": {
    "repos": [
      "ghostty-org/ghostty",
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "ghostty-centralize-configuration-values",
        "title": "Centralize configuration values"
      },
      {
        "slug": "ghostty-descriptive-consistent-naming",
        "title": "Descriptive consistent naming"
      },
      {
        "slug": "ghostty-document-configs-comprehensively",
        "title": "Document configs comprehensively"
      },
      {
        "slug": "ghostty-generate-dynamic-configurations",
        "title": "Generate dynamic configurations"
      },
      {
        "slug": "ghostty-in-tree-build-configurations",
        "title": "In-tree build configurations"
      },
      {
        "slug": "neon-avoid-flaky-tests",
        "title": "Avoid flaky tests"
      },
      {
        "slug": "neon-clear-consistent-identifier-names",
        "title": "Clear consistent identifier names"
      },
      {
        "slug": "neon-configuration-context-alignment",
        "title": "Configuration context alignment"
      },
      {
        "slug": "neon-document-parameter-choices",
        "title": "Document parameter choices"
      },
      {
        "slug": "neon-flexible-documented-configurations",
        "title": "Flexible documented configurations"
      },
      {
        "slug": "neon-hierarchical-semantic-naming",
        "title": "Hierarchical semantic naming"
      },
      {
        "slug": "neon-keep-files-focused-small",
        "title": "Keep files focused small"
      },
      {
        "slug": "neon-minimize-unnecessary-allocations",
        "title": "Minimize unnecessary allocations"
      },
      {
        "slug": "neon-proper-option-type-usage",
        "title": "Proper Option type usage"
      },
      {
        "slug": "neon-structure-endpoints-for-rest",
        "title": "Structure endpoints for REST"
      },
      {
        "slug": "neon-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      }
    ],
    "comments": {
      "neon-keep-files-focused-small": [
        "Would love to see the path pulled into a constant in a `pgbouncer` module (`compute_tools/src/pgbouncer.rs`).",
        "```suggestion\r\npub const PGBOUNCER_PIDFILE: &str = \"/tmp/pgbouncer.pid\";\r\n```\r\n\r\nAlso, just move this to a pgbouncer.rs file. It doesn't make sense to put this constant in pg_helpers.rs.",
        "Please follow pre-established pattern of the use statement at the top and avoid wildcard imports.",
        "As stated in a previous comment, please move all actual logic into the `ComputeNode` struct and just have the handlers call those functions.",
        "It seems like we have a good pattern going where the `ComputeNode` methods become small wrappers around functions in other files. I agree with this suggestion.",
        "Oh, I can't remember if it is, but that is also a good idea!"
      ],
      "ghostty-in-tree-build-configurations": [
        "> We can expand and have a .Devel variant for debugging then, similar to many GNOME apps.\r\n\r\nI think this is a good idea.",
        "Let's restrict this to certain branches, at least main for now"
      ],
      "neon-avoid-flaky-tests": [
        "I know this wasn't added in this PR, but this is a flaky test waiting to happen. I suggest using a database other than the `postgres` database for this test.",
        "Feel free to fix it in a subsequent PR."
      ],
      "ghostty-generate-dynamic-configurations": [
        "Should we add all releases?"
      ],
      "neon-minimize-unnecessary-allocations": [
        "Seems like it would be easier to implement this function in terms of `major_version_num()`, but I'll leave it up to you!",
        "This is very strange to me. At the previous call site that I reviewed, we already spawned and cloned, and now we are doing it again. Please avoid that. Then you can probably inline the `impl` version of the function."
      ],
      "ghostty-centralize-configuration-values": [
        "Instead of the linker script solution, can we just add a PR to check for the library name here?\r\n\r\nhttps://github.com/ghostty-org/ghostty/blob/95daca616db5c24d7bb37fd5a3ac2f8762bb4ead/src/build/SharedDeps.zig#L117",
        "The Zig build system doesn't allow you to pass the equivalent of `-lbz2`?",
        "Yeah I don't know enough about the Zig build system to say for sure unfortunately. Hopefully someone else can step in to assist, but linker script is still fine if it can't.",
        "I'm down for this pending what @jcollie and @pluiedev think"
      ],
      "neon-configuration-context-alignment": [
        "Thanks for catching this. I agree with you."
      ],
      "neon-document-parameter-choices": [
        "Also, please don't use an empty string for an ID."
      ],
      "neon-clear-consistent-identifier-names": [
        "One last comment. It is `pgbouncer`. That's the name of the binary. Using the correct name makes it easier to search and find references.",
        "Please be consistent about either `endpoint_storage_auth_token` or `endpoint_storage_token`."
      ],
      "neon-use-descriptive-identifiers": [
        "```suggestion\r\nPREWARM_LABEL = \"compute_ctl_lfc_prewarm_requests_total\"\r\nOFFLOAD_LABEL = \"compute_ctl_lfc_prewarm_offload_requests_total\"\r\n```\r\n\r\nCapitalize constants",
        "I typically think that booleans can be overloaded. For instance, you have to write this docstring to explain what `with_compute_ctl` means. I like to use enums in such cases.\r\n\r\n```\r\nfrom enum import Enum\r\n\r\nclass LfcQueryMethod(Enum)\r\n    COMPUTE_CTL\r\n    POSTGRES\r\n```\r\n\r\nMakes it a little more obvious. But I will leave it up to you, merely a suggestion on improving readability"
      ],
      "neon-proper-option-type-usage": [
        "This should continue to be an `Option<String>`. It's optional to pass it, and defaulting the value to empty string is meaningless.",
        "Please make `error` an `Option<String>`"
      ],
      "neon-flexible-documented-configurations": [
        "Follow-up PR here: https://github.com/neondatabase/cloud/pull/30120. It looks like we would overwrite the options sent from the control plane side. What are your thoughts on control plane vs compute_ctl changes?",
        "Seems like we may already overwrite `default_transaction_read_only=false`. I would need to verify that.",
        "Ok, I will take a look at the code and investigate. Thanks!",
        "PRs to remove the TODO:\r\n- https://github.com/neondatabase/cloud/pull/30274\r\n- https://github.com/neondatabase/neon/pull/12261\r\n\r\nAnd then the pseudocode that you wrote actually already exists at https://github.com/neondatabase/neon/blob/118e13438df173b98c83bea853e346ebbe00eab3/compute_tools/src/compute.rs#L363-L366.",
        "Ok, I missed that. I'll put up a PR for discussion.",
        "See the implementation at https://github.com/neondatabase/neon/pull/12262."
      ],
      "neon-structure-endpoints-for-rest": [
        "Instead of 4 different routes, let's consolidate down to 2.\r\n\r\n```\r\nGET /prewarm_lfc - get the status\r\nPOST /prewarm_lfc - make the request\r\n\r\nGET /prewarm_lfc_offload - get the status\r\nPOST /prewarm_lfc_offload - make the request\r\n```\r\n\r\nI assume that `offload` means to send the current state of the LFC to endpoint storage. Can we just drop the `prewarm` prefix so it is just `lfc_offload`. I don't see why `prewarm` needs to be there.\r\n\r\nIf ^ is a good suggestion to you, I'd like to see the routes changed to:\r\n\r\n```\r\nGET /lfc/prewarm\r\nPOST /lfc/prewarm\r\n\r\nGET /lfc/offload\r\nPOST /lfc/offload\r\n```",
        "No need to use JsonResponse for empty bodies. Returning the status only is perfectly fine,",
        "```suggestion\r\n        StatusCode::ACCEPTED.into_response()\r\n```",
        "Since this is an async request, `Accepted` would be the more appropriate status code."
      ],
      "ghostty-document-configs-comprehensively": [
        "Maybe it would be easier for users if we just blackholed this option and warned in the logs?"
      ],
      "ghostty-descriptive-consistent-naming": [
        "Feels like we should be more consistent with action names.\r\n\r\n```\r\nTOGGLE_SPLIT_ZOOM\r\nPWD\r\nRELOAD_CONFIG\r\nBELL\r\n```\r\n\r\nMaybe `RING_BELL` would be more consistent?"
      ],
      "neon-hierarchical-semantic-naming": [
        "New metrics related to compute should be prefixed with `compute_`",
        "I would prefer that we start prefixing Postgres metrics with `postgres_` but it isn't something that we've discussed as a team."
      ]
    }
  },
  "siriwatknp": {
    "repos": [
      "mui/material-ui"
    ],
    "entries": [
      {
        "slug": "material-ui-avoid-render-cycle-allocations",
        "title": "Avoid render cycle allocations"
      },
      {
        "slug": "material-ui-defensively-handle-nullables",
        "title": "Defensively handle nullables"
      },
      {
        "slug": "material-ui-distinguish-nextjs-routers",
        "title": "Distinguish Next.js routers"
      },
      {
        "slug": "material-ui-explicit-configuration-resolution",
        "title": "Explicit configuration resolution"
      },
      {
        "slug": "material-ui-explicit-configuration-specifications",
        "title": "Explicit configuration specifications"
      },
      {
        "slug": "material-ui-meaningful-and-consistent-names",
        "title": "Meaningful and consistent names"
      },
      {
        "slug": "material-ui-nextjs-integration-patterns",
        "title": "Next.js integration patterns"
      },
      {
        "slug": "material-ui-optimize-cicd-infrastructure",
        "title": "Optimize CI/CD infrastructure"
      },
      {
        "slug": "material-ui-prefer-direct-property-access",
        "title": "Prefer direct property access"
      },
      {
        "slug": "material-ui-semantic-descriptive-naming",
        "title": "Semantic descriptive naming"
      },
      {
        "slug": "material-ui-strict-mode-proof-hooks",
        "title": "Strict mode-proof hooks"
      },
      {
        "slug": "material-ui-write-user-centric-documentation-guides",
        "title": "Write user-centric documentation guides"
      }
    ],
    "comments": {
      "material-ui-avoid-render-cycle-allocations": [
        "`ColorTool` no longer need to update palette via Context, it's a lot easier and more performant to attach the CSS variables directly to html inline style."
      ],
      "material-ui-defensively-handle-nullables": [
        "Have to fix this because the type of `selected` is `boolean | undefined`.\r\n\r\nShould we loosen the type like of props? to be `() => boolean | null | undefined`?",
        ">My proposal is that we release it with in a minor version and a note.\r\n\r\nLet's do that."
      ],
      "material-ui-meaningful-and-consistent-names": [
        "```suggestion\r\n  value?: TabsProps['value'];\r\n```\r\nI think it's better to use `value` here, it's a standard name with `onChange`. The `TabsContext` will need to be exposed to public."
      ],
      "material-ui-distinguish-nextjs-routers": [
        "This is expected because we haven't release https://github.com/mui/material-ui/pull/45596.\r\n\r\nUpdated the PR description to make it clear what build to use."
      ],
      "material-ui-explicit-configuration-specifications": [
        "I believe so."
      ],
      "material-ui-optimize-cicd-infrastructure": [
        "```suggestion\r\n### Releasing a patch version\r\n\r\nA patch release could happen if there is a regression fix that could not wait for the monthly release cycle.\r\n\r\nIt goes like this:\r\n```\r\n\r\nI think it's nice to add a context on the patch release."
      ],
      "material-ui-explicit-configuration-resolution": [
        "@oliviertassinari I have to make the change to this, otherwise, https://material-ui.netlify.app/experiments/ no longer works after this PR is merged.\r\n\r\nFor the core repo, https://app.netlify.com/sites/material-ui/settings/deploys#branches, `master` branch has `process.env.CONTEXT === 'production'` (not `branch-deploy`)."
      ],
      "material-ui-semantic-descriptive-naming": [
        "I think the explicit `role: undefined` exist for specific fix.\r\n\r\nThe proper fix is to add `role: 'switch'` to `Switch` component. @sai6855 Would you mind checking on this?\r\n\r\nThis PR requires a test to ensure that the issue is fixed without introducing any regression.",
        "<img width=\"268\" alt=\"image\" src=\"https://github.com/user-attachments/assets/7e2b8724-8e0e-4cf8-b2ed-decc9c1b724c\" />\r\n\r\nI think it would be better to use label \"Open standalone demo\".\r\n\r\nWhen I first see \"Open in new tab\", I have no clue on what to expect and not sure why should I open a new tab.\r\n\r\n"
      ],
      "material-ui-prefer-direct-property-access": [
        "I think this can be explicit.\r\n```suggestion\r\n        output.className = clsx(\r\n          defaultProps.className,\r\n          props.className,\r\n```",
        "```suggestion\r\n        output.style = {\r\n          ...defaultProps.style,\r\n          ...props.style,\r\n```"
      ],
      "material-ui-write-user-centric-documentation-guides": [
        "> you should use these when both of these statements are true, right\r\n\r\nYes, that's the message I want to convey.",
        "Wow, that's a lot better."
      ],
      "material-ui-nextjs-integration-patterns": [
        "If `createEmotionServer` is not called here there will be no `cache.compat = true` which ruins the server-side rendering.",
        "In fact, it has to be\r\n```\r\notherwise the <style> tag will come with the HTML string from the server.\r\n```\r\n\r\nWe don't want the style tag to be with the HTML (it's the emotion's default behavior), instead we want all of the styles to be in the head tag."
      ],
      "material-ui-strict-mode-proof-hooks": [
        "I doubt that `props` will always be a new object on every render?\r\nsee this sandbox https://codesandbox.io/p/sandbox/react-new\r\n\r\nReact.useMemo would not help."
      ]
    }
  },
  "190n": {
    "repos": [
      "oven-sh/bun"
    ],
    "entries": [
      {
        "slug": "bun-assert-before-cast",
        "title": "Assert before cast"
      },
      {
        "slug": "bun-check-exceptions-consistently",
        "title": "Check exceptions consistently"
      },
      {
        "slug": "bun-clean-all-error-paths",
        "title": "Clean all error paths"
      },
      {
        "slug": "bun-clear-accurate-documentation",
        "title": "Clear accurate documentation"
      },
      {
        "slug": "bun-descriptive-identifier-names",
        "title": "Descriptive identifier names"
      },
      {
        "slug": "bun-deterministic-lock-management",
        "title": "Deterministic lock management"
      },
      {
        "slug": "bun-judicious-move-semantics",
        "title": "Judicious move semantics"
      },
      {
        "slug": "bun-maintain-consistent-style",
        "title": "Maintain consistent style"
      },
      {
        "slug": "bun-maintain-portable-config-values",
        "title": "Maintain portable config values"
      },
      {
        "slug": "bun-use-consistent-test-patterns",
        "title": "Use consistent test patterns"
      },
      {
        "slug": "bun-validate-nullability-explicitly",
        "title": "Validate nullability explicitly"
      },
      {
        "slug": "bun-verify-assertions-properly",
        "title": "Verify assertions properly"
      }
    ],
    "comments": {
      "bun-maintain-consistent-style": [
        "we don't need to declare `Local` 3 times, and there's no such thing as `Handle`"
      ],
      "bun-clear-accurate-documentation": [
        "```suggestion\r\n  // These must match the order of the stateNames arrays in JSReadableStream.cpp and JSWritableStream.cpp \r\n```"
      ],
      "bun-descriptive-identifier-names": [
        "if we don't change the constants to symbols i would at least rather have `isInt32AsAnyInt` and `asInt32AsAnyInt` so it doesn't break in case you manage to pass the double encoding instead of the int32 encoding.",
        "why this change?"
      ],
      "bun-assert-before-cast": [
        "- Assert the index into `stateNames` is not out of bounds (you could declare `stateNames` with `std::to_array` so that it is easy to find the length).\r\n- Assert `getDirect` doesn't return null\r\n- Call `assertNoExceptionExceptTermination`after `toInt32`",
        "Assert every `getDirect` is non-null\r\nAssert no exception except termination after all the `toXYZ()` calls\r\nMake `transform` a `JSObject*`, calling `getObject()` only once, and assert it's non-null (i.e. the value was really an object)\r\nCheck for exceptions after each `Bun__inspect` call`"
      ],
      "bun-maintain-portable-config-values": [
        "The old message is better. Bun is _not_ available for ARM64 Windows, it's just that emulating the x64 version happens to work."
      ],
      "bun-judicious-move-semantics": [
        "i don't think `mutable` is necessary (that allows you to mutate the captured values, in this case `protectedThis` and `message`, and i don't think we mutate either of them)\r\n\r\n```suggestion\r\n        context->postImmediateCppTask([protectedThis = Ref { *this }, message = WTFMove(message)](ScriptExecutionContext& ctx) {\r\n```"
      ],
      "bun-clean-all-error-paths": [
        "If dupe succeeds and put fails we leak the key\r\n```suggestion\r\n                const stable_key = try dev.allocator.dupe(u8, key);\r\n                errdefer dev.allocator.free(stable_key);\r\n                try map.put(dev.allocator, stable_key, {});\r\n\r\n```",
        "also agree on keeping `getOrPut` since this leaks the key if there was an existing entry"
      ],
      "bun-check-exceptions-consistently": [
        "Check for exceptions after `get` and after `toBoolean`"
      ],
      "bun-use-consistent-test-patterns": [
        "these tests are repetitive could we make the server in a `beforeEach` maybe?",
        "Use `fixtureURL`"
      ],
      "bun-deterministic-lock-management": [
        "add\r\n\r\n```zig\r\ndefer self.mutex.unlock();\r\ndefer self.deref();\r\n```\r\n\r\nand then we can lose the copies of this cleanup along the exit paths",
        "add\r\n\r\n```zig\r\ndefer self.mutex.unlock();\r\ndefer self.deref();\r\n```\r\n\r\nthen delete the copies along the exit paths"
      ],
      "bun-verify-assertions-properly": [
        "this is wrong, in bun `expect(...).resolves` functions as a synchronous await (though it probably shouldn't)",
        "so the AI is wrong (we're actually testing the correct behavior here; this is how Node.js behaves too) but why is it that `new Response` on these compression or decompression streams returns the input instead of the output?"
      ],
      "bun-validate-nullability-explicitly": [
        "zig pointers are non-null by default, so the compiler will assume `@intFromPtr(handle) == 0` is false and delete this check. make `handle` a nullable pointer:`handle: ?*WebWorkerLifecycleHandle`"
      ]
    }
  },
  "ZeeshanTamboli": {
    "repos": [
      "mui/material-ui"
    ],
    "entries": [
      {
        "slug": "material-ui-avoid-render-cycle-allocations",
        "title": "Avoid render cycle allocations"
      },
      {
        "slug": "material-ui-distinguish-nextjs-routers",
        "title": "Distinguish Next.js routers"
      },
      {
        "slug": "material-ui-document-design-decisions",
        "title": "Document design decisions"
      },
      {
        "slug": "material-ui-document-implementation-decisions",
        "title": "Document implementation decisions"
      },
      {
        "slug": "material-ui-effect-hook-best-practices",
        "title": "Effect hook best practices"
      },
      {
        "slug": "material-ui-follow-library-recommendations",
        "title": "Follow library recommendations"
      },
      {
        "slug": "material-ui-graceful-component-errors",
        "title": "Graceful component errors"
      },
      {
        "slug": "material-ui-isolate-dom-security-boundaries",
        "title": "Isolate DOM security boundaries"
      },
      {
        "slug": "material-ui-maintain-configuration-accuracy",
        "title": "Maintain configuration accuracy"
      },
      {
        "slug": "material-ui-meaningful-and-consistent-names",
        "title": "Meaningful and consistent names"
      },
      {
        "slug": "material-ui-parameter-interaction-design",
        "title": "Parameter interaction design"
      },
      {
        "slug": "material-ui-strict-mode-proof-hooks",
        "title": "Strict mode-proof hooks"
      },
      {
        "slug": "material-ui-test-behavior-not-implementation",
        "title": "Test behavior not implementation"
      },
      {
        "slug": "material-ui-use-direct-path-imports",
        "title": "Use direct path imports"
      },
      {
        "slug": "material-ui-use-screen-queries",
        "title": "Use screen queries"
      },
      {
        "slug": "material-ui-use-slots-for-composition",
        "title": "Use slots for composition"
      },
      {
        "slug": "material-ui-use-theme-utilities-consistently",
        "title": "Use theme utilities consistently"
      }
    ],
    "comments": {
      "material-ui-avoid-render-cycle-allocations": [
        "Makes sense. While useMemo technically works here, it is meant for **derived values** based on dependencies. Updated in https://github.com/mui/material-ui/pull/46333/commits/fd86ebd2caee0137c24e72eb2d5c83706759a317.",
        "Done in https://github.com/mui/material-ui/pull/46333/commits/acc42c28a2a484209f2196d11bb5e5e51a9e5948"
      ],
      "material-ui-meaningful-and-consistent-names": [
        "Done.",
        "Why is this type needed above in docs/src/pages/premium-themes/onepirate/modules/components/Button.tsx?",
        "This still doesn't reply my question. It's not about `a` tag here, it is the `type` attribute that is redefined and why you need to have a new TS type `ConstrainedButtonProps` which is used in `docs/src/pages/premium-themes/onepirate/modules/components/Button.tsx` file. Why `ButtonProps` can't be used there like earlier? Why is there an error with `ButtonProps` in that file?",
        "> However, this fix created another potential issue - the updated `ButtonProps` would allow any string value for the `type` attribute, not just the valid HTML button types ('button', 'submit', 'reset').\r\n\r\nWhy the updated `ButtonProps` is allowing any string value for the `type` atrribute instead of the valid HTML button types?\r\n",
        "> The error occurs because the updated ButtonProps type allows the type property to be a string | undefined (inherited from React.ButtonHTMLAttributes), which is too general.\r\n\r\nWhat you are saying is wrong. `React.ButtonHTMLAttributes` already does support `type` `\"submit\" | \"reset\" | \"button\" | undefined;`.",
        "```suggestion\r\n    const hiddenElements = getHiddenElements(container);\r\n```",
        "I'm not sure what you mean by `textareaHandle`. But you're right, it shouldn't be `event`. I've updated it to `textareaElement`. According to the documentation here: https://playwright.dev/docs/next/api/class-locator#locator-evaluate, `pageFunction` takes a element as an argument.",
        "I had the same thought. The issue is that `getTagProps` returns properties specific to a Material UI Chip (or tag?), like `disabled`, and `onDelete`. This callback is meant to be spread only when using a _custom_ Material UI Chip. Maybe we should rename it to `getChipProps`.",
        "@michaldudak @DiegoAndai I've made the changes and updated the docs. I only replaced \"tag\" in the public API. Internally, some methods and variables still use \"tag\" since the deprecated `renderTags` depends on them. Renaming them would mean duplicating methods with the same logic for `renderValue`."
      ],
      "material-ui-effect-hook-best-practices": [
        "Are you sure layout effect is absolutely necessary here?",
        "I used our util `useEnhancedEffect` instead which handles SSR as well."
      ],
      "material-ui-distinguish-nextjs-routers": [
        "This is incorrect. The difference isnâ€™t between the Next.js pages router and app router. The `id=\"__next\"` is used by **Next.js**, while `id=\"root\"` is used by **Vite** and other SPA frameworks.\r\n\r\nIt's that we need to mention that since Next.js 13+, you need to _manually_ add `id=\"__next\"` to the root element i.e `body`. It isn't automatically added like before. For Vite, the root element typically looks like this:\r\n\r\n```html\r\n<div id=\"root\"></div>\r\n```\r\nSo, the `important` option should use `#__next` for Next.js and `#root` for Vite.",
        "> Inclusion of Troubleshooting block is okay , or needs to be removed ?\r\n\r\nIt's fine. Just need to tweak point 2 by removing the mentioning of `root` as Next.js ID."
      ],
      "material-ui-test-behavior-not-implementation": [
        "Let's not test the code implementation details.  Instead, we can verify that it does not throw an error when nested options are provided to `Autocomplete`. We should trigger the user interactions step by step as provided in the issue and check that it does not crash in the end.\r\n\r\n1. The user opens the list and selects an option.\r\n2. The user clears the selected option from input.\r\n3. The user reopens the autocomplete.\r\n4. It should not crash.\r\n\r\nAlso, please add this test in `Autocomplete` test file (`Autocomplete.test.js`). Let me know if you need any help.   ",
        "Please delete the test from the `useAutocomplete.test.js`. Any logic we have in future related to highlighting will be in the `useAutocomplete` hook only which will always be used in the Material-UI `Autocomplete` component.",
        "We could. Done."
      ],
      "material-ui-use-direct-path-imports": [
        "I'm not sure this works. Can you share a reproduction where all these steps succeed and bundle size is reduced? I think Vite uses the esm bundle by default, but I'm not certain. cc @Janpot",
        "I wouldnâ€™t replace the entire â€œOption two: use a Babel pluginâ€ section. Iâ€™d keep the original content and avoid adding all the new material."
      ],
      "material-ui-parameter-interaction-design": [
        "No need to define it again here since `useAutocomplete` types already has it and Joy UI and Material UI extends the hook's types.\r\n```suggestion\r\n```"
      ],
      "material-ui-use-screen-queries": [
        "You can use `screen.getByRole` below instead of passing `getByRole` parameter,"
      ],
      "material-ui-use-slots-for-composition": [
        "```suggestion\r\n- You should provide a tooltip title using `slotProps.tooltip.title` for each speed dial action.\r\n```"
      ],
      "material-ui-document-implementation-decisions": [
        "Added."
      ],
      "material-ui-document-design-decisions": [
        "This is internal component so maybe we don't need to document in the types that `getTabbable` prop now accepts three parameters: https://github.com/mui/material-ui/blob/master/packages/mui-material/src/Unstable_TrapFocus/FocusTrap.types.ts#L13, but it would be better to add it."
      ],
      "material-ui-maintain-configuration-accuracy": [
        "`eslint` related packages and `globals` packages isn't used anywhere. They can be removed."
      ],
      "material-ui-use-theme-utilities-consistently": [
        "According to the [docs](https://mui.com/material-ui/migration/upgrade-to-v7/#theme-behavior-changes:~:text=It%27s%20recommended%20to%20use%20the%20theme.vars.*%20as%20values%20in%20your%20styles%20to%20refer%20to%20the%20CSS%20variables%20directly%3A), it's recommended to use theme.vars.* directly in your styles.",
        "I suggest using the `styled` API, like in the Customization section, and naming the component `StyledToggleButtonGroup`. Since the `sx` prop uses `styled` under the hood, `styled` is more performant and, in my opinion, easier to readâ€”especially with longer style definitions.",
        "I don't think all this logic is needed.  We can simply reset inherited `line-height` style using `line-height: normal`:\r\n```diff\r\n--- a/packages/mui-material/src/Chip/Chip.js\r\n+++ b/packages/mui-material/src/Chip/Chip.js\r\n@@ -89,6 +89,7 @@ const ChipRoot = styled('div', {\r\n       alignItems: 'center',\r\n       justifyContent: 'center',\r\n       height: 32,\r\n+      lineHeight: 'normal',\r\n       color: (theme.vars || theme).palette.text.primary,\r\n       backgroundColor: (theme.vars || theme).palette.action.selected,\r\n       borderRadius: 32 / 2,\r\n```\r\nDocs: https://developer.mozilla.org/en-US/docs/Web/CSS/line-height#normal"
      ],
      "material-ui-follow-library-recommendations": [
        "If you test it manually in the documentation preview you'll see that a double-click is necessary to grab the handle for resizing. However, it's essential not to release the mouse button after the second click. Simply clicking once with the mouse doesn't work. I'm using the touchpad on my laptop.\r\n\r\n_Update:_ You're correct. Left-clicking using the mouse button on touchpad and resizing while keeping the button pressed works fine. I've adjusted it to use only `mouse.down()`."
      ],
      "material-ui-isolate-dom-security-boundaries": [
        "Can you explain this part of the logic with an example? It's a little hard to follow."
      ],
      "material-ui-strict-mode-proof-hooks": [
        "You're right that the initializer inside `useState` only runs once per mount. However, in Reactâ€™s Strict Mode (development only), it's **intentionally called twice** to detect impure logic. Since `registerTab` mutates internal state, calling it twice would incorrectly register the tab multiple times, shifting tab indices and breaking the selection or indicator logic.\r\n\r\nTo avoid this, we guard it with `hasRegisteredRef`, ensuring `registerTab` runs only once â€” even in development. In production, the guard has no effect because the initializer runs only once as expected.\r\n\r\nIdeally, the initializer should be pure (per [React docs](https://react.dev/reference/react/useState#my-initializer-or-updater-function-runs-twice)), but we intentionally break that rule here to **support SSR** â€” specifically, to precompute tab metadata so that the correct tab is marked selected on the first render (see [test case](https://github.com/mui/material-ui/blob/6c0f14b50dc7c86134b8bb549da47dc33bf8b06a/packages/mui-material/src/Tabs/Tabs.test.js#L901-L912)). Without it, we get hydration mismatches..\r\n\r\nI considered making `registerTab` idempotent, but thatâ€™s not feasible when we need to assign an implicit `value` based on the tab's render order. That requires incrementing a shared index counter (`childIndexRef`) â€” and we canâ€™t require users to always provide explicit values to `Tab` without introducing a breaking change.\r\n\r\nThis approach strikes a balance: it ensures SSR correctness, avoids hydration issues, and works with wrapper components like `<Tooltip><Tab /></Tooltip>`, while remaining safe under Reactâ€™s development behavior.\r\n\r\nOpen to suggestions if you think there's a cleaner way to achieve this.",
        "> If I understand it correctly this won't work well if you remove a tab dynamically (as there's no unregister function)\r\n\r\nIt won't. But it isn't supported even in latest version.\r\n\r\nThis PR: https://stackblitz.com/edit/ry4fan5c-t3b4771r\r\nMaster: https://stackblitz.com/edit/ry4fan5c-oqugmytq",
        "Yes, registering during the effect phase would prevent it from running on the server, which breaks SSR.\r\n\r\n> Perhaps we could register both during rendering and in an effect and make the register operation idempotent (or register conditionally if it hasn't been registered yet). This will allow the use of the unregister function in the effect cleanup.\r\n\r\nThat could work well only if tabs always have explicit `value` props. But in our case, we also support implicit values based on render order, like this:\r\n\r\n```tsx\r\nconst [tab, setTab] = React.useState(1);\r\n\r\nconst handleChange = (event, newValue) => {\r\n  setTab(newValue);\r\n};\r\n\r\nreturn (\r\n  <Tabs value={tab} onChange={handleChange}>\r\n    <Tab label=\"one\" />\r\n    <Tooltip title=\"two helper\">\r\n      <Tab label=\"two\" />\r\n    </Tooltip>\r\n  </Tabs>\r\n);\r\n```\r\n\r\nHere, tabs derive their `value` from their render position (i.e., first tab = 0, second = 1), using an internal `childIndexRef`.\r\n\r\nIf we register both during render and in an effect:\r\n\r\n* In React Strict Mode (dev only), render and effect each run twice i.e total of 4 registrations.\r\n* Even without Strict Mode, a single tab would be registered twice. (one in first render and second in effect).\r\n\r\nSo, there would be 4 child indexes.\r\n\r\nWhile `registerTab` is already idempotent when used with explicit values (via `valueToIndex.has(finalValue)`), we can't enforce `value` on Tab without introducing a breaking change.\r\n",
        "> Deriving the value from the position sounds good to me ðŸ‘ðŸ¼\r\n\r\nThis logic was already present when `cloneElement` was used. Just picked that up here.\r\n\r\n> But if we create a `value` for the position on our side, then we would have a `finalValue`, no?\r\n\r\nYes, we do generate a `finalValue` based on position internally when value isn't provided. The issue is that this implicit value depends on render order and a shared index (`childIndexRef`), which is incremented during registration.\r\n\r\nIf we allow `registerTab` to run multiple times â€” as suggested above, in both render and effect â€” the index keeps increasing, and the same tab ends up with different `finalValue`s across renders. That breaks selection and causes hydration mismatches.\r\n\r\nWith explicit `value`, we donâ€™t rely on index state, so idempotency is safe. But for implicit values, the act of generating `finalValue` is tied to mutable state â€” so calling `registerTab` multiple times isnâ€™t safe unless we move to require explicit values, which would be a breaking change.",
        "I don't think this will work.\r\n\r\n> In any subsequent register call, we use `finalValue` to identify the Tab\r\n\r\nWhy do we want to pass the same `finalValue` to the next registration call? If we want to pass this stored `finalValue`, we shouldn't call `registerTab` again in the first place (supposedly in the effect).\r\n\r\nFeel free to edit the code if you have any ideas.\r\n\r\n----\r\n\r\nEven in Base UI, they are making the `value` prop required on Tab: https://github.com/mui/base-ui/pull/2124. \r\n\r\nSupporting implicit `value` in Tab cause them issues like https://github.com/mui/base-ui/issues/1880.",
        "> Because we want to return the unregistering callback from the effect, so it's run on unmount.\r\n\r\nBut wouldn't the useEffect's setup function do nothing? It would simply return the output given as an input **always** when doing `registerTab(finalValue)`.\r\n\r\n> This is not an option for us unless we want to wait for a new major.\r\n\r\nYes, not an option now.\r\n\r\n> We're already supporting implicit value, aren't we? With or without my suggestion.\r\n\r\nYes, we are already. Just wanted to point out some issues. I thought it would help us to understand.",
        "I tried doing this in https://github.com/mui/material-ui/pull/46333/commits/7221ea83aeb156e555c031bd9b0062124a1971b6 but the tests fail. Any idea why? However, it works locally on browser.",
        "Tests pass now. The tests run with Strict Mode, which helped me catch the issue that I wasnâ€™t decrementing the child index on tab unregistration."
      ],
      "material-ui-graceful-component-errors": [
        "A standalone `Tab` isn't functional: https://stackblitz.com/edit/j4aahksg â€” I don't see how it could be useful on its own."
      ]
    }
  },
  "daniel-lxs": {
    "repos": [
      "RooCodeInc/Roo-Code"
    ],
    "entries": [
      {
        "slug": "roo-code-avoid-hardcoded-configurations",
        "title": "Avoid hardcoded configurations"
      },
      {
        "slug": "roo-code-centralize-configuration-constants",
        "title": "Centralize configuration constants"
      },
      {
        "slug": "roo-code-conditional-debug-logging",
        "title": "Conditional debug logging"
      },
      {
        "slug": "roo-code-configure-with-care",
        "title": "Configure with care"
      },
      {
        "slug": "roo-code-consistent-localization-formatting",
        "title": "Consistent localization formatting"
      },
      {
        "slug": "roo-code-document-i18n-string-usage",
        "title": "Document i18n string usage"
      },
      {
        "slug": "roo-code-enforce-api-format-consistency",
        "title": "Enforce API format consistency"
      },
      {
        "slug": "roo-code-enforce-resource-usage-limits",
        "title": "Enforce resource usage limits"
      },
      {
        "slug": "roo-code-extract-reusable-patterns",
        "title": "Extract reusable patterns"
      },
      {
        "slug": "roo-code-extract-shared-code-patterns",
        "title": "Extract shared code patterns"
      },
      {
        "slug": "roo-code-internationalize-all-text",
        "title": "Internationalize all text"
      },
      {
        "slug": "roo-code-maintain-consistent-naming-patterns",
        "title": "Maintain consistent naming patterns"
      },
      {
        "slug": "roo-code-optimize-algorithm-implementations",
        "title": "Optimize algorithm implementations"
      },
      {
        "slug": "roo-code-optimize-react-components",
        "title": "Optimize React components"
      },
      {
        "slug": "roo-code-preserve-error-context-chain",
        "title": "Preserve error context chain"
      },
      {
        "slug": "roo-code-prevent-timeout-race-conditions",
        "title": "Prevent timeout race conditions"
      },
      {
        "slug": "roo-code-prevent-unnecessary-processing",
        "title": "Prevent unnecessary processing"
      },
      {
        "slug": "roo-code-protect-shared-state-access",
        "title": "Protect shared state access"
      },
      {
        "slug": "roo-code-robust-error-handling",
        "title": "Robust error handling"
      },
      {
        "slug": "roo-code-use-structured-logging",
        "title": "Use structured logging"
      },
      {
        "slug": "roo-code-validate-model-capabilities-first",
        "title": "Validate model capabilities first"
      },
      {
        "slug": "roo-code-write-resilient-test-assertions",
        "title": "Write resilient test assertions"
      }
    ],
    "comments": {
      "roo-code-document-i18n-string-usage": [
        "I see that the oversized image error uses i18n properly, but this regular image notice is hardcoded. For consistency, could we move these strings to the translation files as well?\n\nFor example:\n```typescript\nconst noticeText = dimensionsInfo\n  ? t(\"tools:readFile.imageWithDimensions\", { dimensions: dimensionsInfo, size: imageSizeInKB })\n  : t(\"tools:readFile.imageWithSize\", { size: imageSizeInKB });\n```\n\nThis would ensure all user-facing strings are translatable and maintain consistency across the codebase."
      ],
      "roo-code-preserve-error-context-chain": [
        "This error handling pattern for ENOENT is duplicated in multiple places:\r\n- Here in `safeReadFile`\r\n- In `ClineProvider.updateContent`\r\n- In `custom-instructions.ts`\r\n- In `custom-system-prompt.ts`\r\n\r\nIt would be a good idea to consolidate this on a single helper function.",
        "What happens if the base64 data is corrupted or invalid? Would it be worth wrapping this in a try-catch to handle potential errors when constructing the data URL? This could prevent the entire tool response from failing due to a single corrupted image."
      ],
      "roo-code-configure-with-care": [
        "The current implementation uses the settings values without validating whether they're within reasonable bounds. For example, if someone manually edits their settings file and sets a negative number or an extremely large value, it could lead to unexpected behavior.\n\nIt might be worth adding validation like this:\n\n```ts\nconst maxImagesPerResponse = Math.max(1, Math.min(100, state?.mcpMaxImagesPerResponse ?? 20));\nconst maxImageSizeMB = Math.max(0.1, Math.min(50, state?.mcpMaxImageSizeMB ?? 10));\n````\n\nThis helps keep the values within safe operational limits, even in cases where the settings file is corrupted or manually modified.\n\n"
      ],
      "roo-code-prevent-timeout-race-conditions": [
        "Is there a potential race condition here? If the auto-approval timeout fires just as the user is typing/submitting a response, could both the manual response and auto-approval execute? The timeout clearing happens after the condition check, so there might be a small timing window.",
        "Is there a reason the timeout clearing happens inside the `clineAskRef.current` check rather than at the beginning of `handleSendMessage`? If we clear it unconditionally at the start, we could prevent any race conditions where the timeout fires while we're processing the user's input.\n\nThis would also simplify the logic since we wouldn't need to check for specific ask types.",
        "I notice there's a potential race condition here. If the component unmounts while the countdown is active, the interval cleanup happens, but what if the auto-approval in ChatView.tsx is still waiting? Could we consider using a shared cancellation mechanism or ensuring the ChatView's timeout is also cleared when this component unmounts?"
      ],
      "roo-code-optimize-algorithm-implementations": [
        "**Important**: Starting pattern detection from length 1 could cause false positives. For example, \"AAAA\" would be detected as pattern \"A\" repeated 4 times, which overlaps with consecutive repetition detection.\n\nConsider starting from length 2 to avoid this overlap:\n```typescript\nfor (let patternLength = 2; patternLength <= maxPatternLength; patternLength++) {\n```\n\nThis ensures pattern detection only catches actual patterns like \"AB\", \"ABC\", etc., not single repeated elements."
      ],
      "roo-code-avoid-hardcoded-configurations": [
        "The script could use a few updates to make it more reliable and easier to maintain:\r\n\r\n1. **Hardcoded extension ID**: Right now, the extension ID is hardcoded as `rooveterinaryinc.roo-cline`. Instead, it should be built dynamically from the `package.json` to avoid issues if the name or publisher ever changes:\r\n\r\n   ```javascript\r\n   const publisher = packageJson.publisher\r\n   const extensionId = `${publisher}.${name}`\r\n   ```\r\n\r\n2. **Missing VSIX file check**: Before trying to install the VSIX file, it's a good idea to check that it actually exists. Otherwise, the script might fail without a clear reason. Hereâ€™s a simple check to add:\r\n\r\n   ```javascript\r\n   if (!fs.existsSync(vsixFileName)) {\r\n     console.error(`VSIX file not found: ${vsixFileName}`)\r\n     console.error(\"Make sure the build completed successfully\")\r\n     process.exit(1)\r\n   }\r\n   ```\r\n\r\nAdding these checks will make the script more robust and easier to work with in the long run.\r\n"
      ],
      "roo-code-extract-reusable-patterns": [
        "The debouncing logic is implemented directly in this component. Have you considered extracting it into a reusable custom hook? This could help improve separation of concerns.",
        "The className logic here is getting quite complex with nested ternaries. Could we simplify this by extracting the condition to a variable?\n\n```tsx\nconst showShareButton = primaryButtonText === t(\"chat:startNewTask.title\") && currentTaskItem?.id;\nconst buttonClassName = showShareButton || secondaryButtonText \n  ? \"flex-1 mr-[6px]\" \n  : \"flex-[2] mr-0\";\n```\n\nThis would make the JSX cleaner and the logic more readable.",
        "The edit mode rendering logic is quite complex with multiple helper functions. Consider extracting the edit mode UI into a separate component for better maintainability:\n\n```typescript\nconst EditModeControls: React.FC<EditModeControlsProps> = ({ ... }) => {\n  // Edit mode specific UI logic\n};\n```\n\nThis would simplify the main component and make the code more modular.",
        "Could you extract this logic into a shared function to avoid duplication? The same pattern appears in `handleSuggestionClickInRow`. Something like:\n\n```typescript\nconst markFollowUpAsAnswered = useCallback(() => {\n  const lastFollowUpMessage = messagesRef.current.findLast((msg) => msg.ask === \"followup\")\n  if (lastFollowUpMessage) {\n    setFollowUpAnswered((prev) => new Set(prev).add(lastFollowUpMessage.ts))\n  }\n}, [])\n```",
        "Since there's timeout logic here as well similar to what's on ClineProvider. Have you considered extracting this into a shared utility function or hook? Something like `useDebouncedFocus()` could handle the timeout.",
        "The `handleSave` function is quite complex with nested conditions and multiple responsibilities. Would it be cleaner to extract the group update logic into a separate utility function?\n\nFor example:\n```typescript\nfunction updateMcpGroupOptions(\n  groups: GroupEntry[],\n  allowedList: string[],\n  deniedList: string[]\n): GroupEntry[] {\n  // Group update logic here\n}\n```\n\nThis would make the code more testable and easier to understand."
      ],
      "roo-code-prevent-unnecessary-processing": [
        "This useEffect runs on every selectedMenuIndex change during keyboard navigation, which could impact performance. Consider debouncing the announcements or only announcing when navigation pauses:\n\n```typescript\nuseEffect(() => {\n  if (!showContextMenu || selectedMenuIndex < 0) return;\n  \n  const timeoutId = setTimeout(() => {\n    // announcement logic here\n  }, 100); // Small delay to avoid rapid announcements\n  \n  return () => clearTimeout(timeoutId);\n}, [showContextMenu, selectedMenuIndex, /* other deps */]);\n```",
        "Consider memoizing the dialog components to prevent unnecessary re-renders:\n\n```typescript\nconst MemoizedDeleteMessageDialog = React.memo(DeleteMessageDialog);\nconst MemoizedEditMessageDialog = React.memo(EditMessageDialog);\n```\n\nThis could improve performance, especially when the parent component re-renders frequently."
      ],
      "roo-code-enforce-resource-usage-limits": [
        "Should we consider implementing a limit on the number of images that can be returned in a single response? Without a limit, a malicious or buggy MCP server could potentially return hundreds of images, causing performance issues. Maybe add a configurable maximum (e.g., 10-20 images) and log a warning if exceeded?",
        "With the increased limit of 500,000 files, have you considered the memory implications? Each FileResult object contains path, type, and label properties. For a project with 500K files, this could consume significant memory (potentially hundreds of MB).\n\nWould it make sense to:\n1. Make this limit configurable via VSCode settings?\n2. Implement streaming or pagination for extremely large file lists?\n3. Add memory usage monitoring/warnings?",
        "The accumulator still stores the entire conversation history, which matches the original behavior, so this isn't a regression. But it could become a memory concern for long-running chats.\r\n\r\nThis PR avoids re-parsing, which is great for performance. Should we try adding a bounded accumulator or sliding window to help with memory usage?",
        "While the accumulator is reset before each `attemptApiRequest`, consider adding a maximum size check as a safety measure against edge cases where the parser might be reused without proper reset:\n\n```typescript\nprivate readonly MAX_ACCUMULATOR_SIZE = 1024 * 1024; // 1MB limit\n\nprocessChunk(chunk: string): AssistantMessageContent[] {\n    if (this.accumulator.length + chunk.length > this.MAX_ACCUMULATOR_SIZE) {\n        throw new Error('Assistant message exceeds maximum allowed size');\n    }\n    this.accumulator += chunk;\n    // ...\n}\n```\n\nThis would prevent potential memory issues if the parser state isn't properly managed in all code paths.",
        "Should we consider adding a maximum draft size limit just in case? localStorage typically has a 5-10MB quota, and very large drafts could potentially cause issues. What do you think about truncating or warning when drafts exceed a reasonable size (e.g., 100KB)?"
      ],
      "roo-code-conditional-debug-logging": [
        "I noticed some `console.log` and `console.debug` statements here. Are these intended for debugging and should they be removed before merging?"
      ],
      "roo-code-internationalize-all-text": [
        "All these announcement strings need to be translated. Consider creating a helper function that uses the translation system:\n\n```typescript\nconst getAnnouncementText = (option: ContextMenuQueryItem, index: number, total: number) => {\n  const position = t(\"chat:contextMenu.position\", { current: index + 1, total });\n  \n  switch (option.type) {\n    case ContextMenuOptionType.File:\n    case ContextMenuOptionType.OpenedFile:\n      return t(\"chat:contextMenu.announceFile\", { \n        name: option.value || option.label, \n        position \n      });\n    // ... other cases\n  }\n};\n```",
        "This instruction text should also be translated:\n\n```typescript\n<div id=\"context-menu-instructions\" className=\"sr-only\">\n  {t(\"chat:contextMenu.instructions\")}\n</div>\n```"
      ],
      "roo-code-protect-shared-state-access": [
        "I noticed that while we check `supportsImages` at the beginning of the function (line 147), this value could theoretically change if the model is switched during execution. Should we consider moving this check closer to where we actually decide to include images?\n\nThe current implementation is likely fine for most cases, but for extra safety, we could store the model info check result and use it consistently throughout the function execution.",
        "I noticed a potential race condition here. The model info is fetched during message streaming, which could cause issues if multiple concurrent requests are made before the cache is populated. \n\nWould it be safer to fetch and cache the model info during handler initialization or before starting the stream? This would ensure consistent context window information across concurrent requests.",
        "The `modifyConversation` method attempts to acquire locks on two different files through nested calls to `modifyClineMessages` and `modifyApiConversationHistory`. This pattern could lead to:\n\n1. **Deadlocks** if another process tries to acquire these locks in reverse order\n2. **Data inconsistency** if one modification succeeds but the other fails\n\nIs this intentional? The proper-lockfile documentation recommends against holding multiple locks simultaneously. Consider either:\n- Using a single lock file for both operations\n- Implementing a two-phase commit pattern\n- Documenting why this approach is safe in your specific use case",
        "I noticed a potential edge case in the error handling here. If `ripgrepOperationPromise` fails and we set it to null (line 84), but another concurrent call comes in before line 90, it might start a new operation while the first caller is still in the catch block.\n\nCould this be addressed by setting `ripgrepOperationPromise = null` after the entire try-catch block completes? Or perhaps using a more robust state management approach?",
        "Is there a potential race condition here? The draft is only restored if `!inputValue`, but what happens if the component receives an initial value before the message handler is set up? Could we miss restoring a valid draft in that case?",
        "I'm a bit concerned about potential race conditions here. You're using a 100ms timeout in the provider while ChatView uses 50ms. If someone rapidly switches between windows, multiple focus events could queue up and cause unexpected behavior.\n\nMaybe we could use the same timeout duration in both places? Or even better, implement a proper debounce mechanism to handle rapid focus changes more gracefully.",
        "Good that you're clearing the timeout in dispose(), but what happens if the view gets disposed while a timeout is still pending? The timeout would still fire and try to access this.view which might be disposed.\r\n\r\nIt might be a good idea to add a check in the timeout callback to ensure the view still exists and hasn't been disposed before trying to post a message to it."
      ],
      "roo-code-optimize-react-components": [
        "Have you considered using a simpler state management approach here? Instead of tracking all answered follow-ups in a Set, you could track just the current active follow-up question's timestamp and clear it when answered. This would avoid the need to manage a growing collection of timestamps.\n\nFor example:\n```typescript\nconst [currentFollowUpTs, setCurrentFollowUpTs] = useState<number | null>(null)\n```\n\nThen check `message.ts === currentFollowUpTs` instead of using a Set."
      ],
      "roo-code-use-structured-logging": [
        "We should clean this up before merging.\r\n\r\nThe `console.info` statements currently in the code (including the one at line 144 where `finalError` is logged) should be removed. If logging is still needed, consider using a proper logging service instead of `console` calls.",
        "I notice the error logging here uses different levels - `console.info` for connection failures (line 90) vs `console.warn` for other errors (line 92). In contrast, the LM Studio fetcher uses `console.error` for connection failures. Would it make sense to standardize the logging approach across both fetchers?"
      ],
      "roo-code-validate-model-capabilities-first": [
        "Is it intentional that we're returning image data without checking if the current AI model supports images? I noticed that `maybeRemoveImageBlocks` in `src/api/transform/image-cleaning.ts` checks `apiHandler.getModel().info.supportsImages` before processing images. Should we add a similar check here to prevent sending image data to models that can't process it?\n\nFor example, we could check the provider's capability before including images in the response:\n```typescript\nconst provider = await cline.providerRef.deref();\nconst supportsImages = provider?.apiHandler?.getModel()?.info?.supportsImages ?? false;\nconst imagesToInclude = supportsImages ? allImages : [];\n```",
        "The default dimension is set to 768 for `gemini-embedding-exp-03-07`, but this model supports dimensions of 3072, 1536, and 768. Is 768 the intended default? The smaller dimension might impact embedding quality.\n\nAlso, would it be helpful to add validation somewhere to ensure only valid dimensions (3072, 1536, or 768) are accepted for this model?"
      ],
      "roo-code-maintain-consistent-naming-patterns": [
        "I noticed the naming pattern inconsistency here. You've renamed `MAX_SEARCH_RESULTS` to `DEFAULT_MAX_SEARCH_RESULTS` (which is good!), but `SEARCH_MIN_SCORE` doesn't follow the same pattern.\n\nFor consistency, should we consider renaming `SEARCH_MIN_SCORE` to `DEFAULT_SEARCH_MIN_SCORE`? This would make it clearer that both are default values that can be overridden by user configuration."
      ],
      "roo-code-enforce-api-format-consistency": [
        "I noticed that `.avif` is included in `SUPPORTED_IMAGE_FORMATS` (line 44) but there's no corresponding MIME type mapping here. This means AVIF images will default to `image/png` which isn't correct.\n\nCould we add:\n```typescript\n\".avif\": \"image/avif\",\n```\n\nto ensure AVIF images are properly identified?",
        "The URL detection pattern might be too broad. Could a URL like `https://api.example.com/v1/embeddings-service` or `https://api.example.com/deployments-info/v1` be incorrectly classified as a full endpoint URL?\n\nWould it be safer to use more specific patterns? For example:\n- Check if the URL ends with `/embeddings` (with or without query params)\n- Use a regex pattern like `/deployments/[^/]+/embeddings` for Azure-style URLs\n\nThis would reduce the risk of false positives while maintaining compatibility with the intended services.",
        "The description says \"milliseconds\" but the value is actually stored in minutes and converted to milliseconds in the handler. This is inconsistent with the other timeout descriptions and could cause confusion. Also, the `.describe()` method isn't used elsewhere in the schema.\n```suggestion\n\topenAiApiTimeout: z.number().optional(),\n```",
        "The current implementation converts messages to plain text format, but this seems inconsistent with the original approach.\r\n\r\nThe comment on line 117 states \"Claude CLI doesn't accept JSON messages\", but looking at the [original implementation pattern](https://github.com/cline/cline/pull/4111/files#diff-c922b8fdacba11f1ecef526a5223718b0ea2c9d5aea01c653b1fd7afddea2ca5R23), it appears the CLI should accept JSON-formatted messages. The current text conversion approach:\r\n\r\n1. Loses message structure and metadata\r\n2. Makes it harder to handle complex message types\r\n3. Differs from how other providers handle messages\r\n\r\nCould you verify if the Claude Code CLI actually requires text format? If it does accept JSON, consider reverting to:\r\n```typescript\r\nconst args = [\r\n  \"-p\",\r\n  JSON.stringify(messages),\r\n  \"--system-prompt\",\r\n  systemPrompt,\r\n  // ... other args\r\n]\r\n```\r\n\r\nThis would also simplify the validation logic since JSON.stringify handles escaping automatically.",
        "The current implementation converts messages to plain text format, but this seems inconsistent with the original approach.\n\nThe comment on line 117 states \"Claude CLI doesn't accept JSON messages\", but looking at the [original implementation pattern](https://github.com/cline/cline/pull/4111/files#diff-c922b8fdacba11f1ecef526a5223718b0ea2c9d5aea01c653b1fd7afddea2ca5R23), it appears the CLI should accept JSON-formatted messages. The current text conversion approach:\n\n1. Loses message structure and metadata\n2. Makes it harder to handle complex message types\n3. Differs from how other providers handle messages\n\nCould you verify if the Claude Code CLI actually requires text format? If it does accept JSON, consider reverting to:\n```typescript\nconst args = [\n  \"-p\",\n  JSON.stringify(messages),\n  \"--system-prompt\",\n  systemPrompt,\n  // ... other args\n]\n```\n\nThis would also simplify the validation logic since JSON.stringify handles escaping automatically."
      ],
      "roo-code-consistent-localization-formatting": [
        "No, that translation is correct since it's a placeholder to signify the mode"
      ],
      "roo-code-extract-shared-code-patterns": [
        "I notice the GC logic is duplicated between lines 636-658 and 674-694. Could we extract this into a separate method to follow DRY principles? Something like:\n\n```typescript\nprivate static async runBackgroundGC(git: SimpleGit, branchName: string): Promise<void> {\n    try {\n        this.log(`[${this.name}#deleteBranch] Running gc --prune=now after deleting branch ${branchName}`);\n        git.raw([\"gc\", \"--prune=now\", \"--quiet\"])\n            .then(() => {\n                this.log(`[${this.name}#deleteBranch] Background gc --prune=now completed for branch ${branchName}`);\n            })\n            .catch((gcError) => {\n                this.log(`[${this.name}#deleteBranch] ERROR: Background gc after deleting branch ${branchName} failed: ${gcError instanceof Error ? gcError.message : String(gcError)}`);\n            });\n    } catch (e) {\n        this.log(`[${this.name}#deleteBranch] ERROR: Failed to initiate gc: ${e instanceof Error ? e.message : String(e)}`);\n    }\n}\n```",
        "There's significant code duplication between this `importConfigFromPath` function and the existing `importSettings` function in `src/core/config/importExport.ts`. \n\nCould we refactor to share the common import logic? Perhaps extract a shared function that both can use, something like:\n\n```typescript\nexport async function importConfigFromData(\n  data: unknown,\n  options: ImportOptions\n): Promise<{ success: boolean; error?: string }>\n```\n\nThis would reduce maintenance burden and ensure consistent behavior between manual and automatic imports."
      ],
      "roo-code-robust-error-handling": [
        "The error handling logic looks good, but could this be more robust? Currently it falls back to string conversion for non-Error objects. Would it be helpful to also capture additional context like the component stack or timestamp when the error occurred?",
        "The JSON.parse() here could throw an error if lastMessage.text contains invalid JSON. Could we wrap this in a try-catch block to handle potential parsing errors gracefully?\n\n```typescript\nlet followUpData;\ntry {\n  followUpData = JSON.parse(lastMessage.text || \"{}\")\n} catch (error) {\n  console.error('Failed to parse follow-up data:', error);\n  return;\n}\n```"
      ],
      "roo-code-centralize-configuration-constants": [
        "UI consideration: The component shows 8192 as the default value when no value is set. Would it be clearer to show the model's actual max tokens instead?\n\n```typescript\nconst displayValue = value ?? modelInfo?.maxTokens ?? 8192\n```\n\nThis would give users better context about what the model actually supports before they override it.",
        "I'm curious about why this is needed? I think `setApiConfigurationField` should handle these state changes by itself without having to process them manually."
      ],
      "roo-code-write-resilient-test-assertions": [
        "The test verifies Windows path conversion logic but doesn't actually test that the Windows code path uses WSL or that temporary files are created and cleaned up. Would it be valuable to add more comprehensive tests that mock the `execa` call and verify the correct command is executed with WSL on Windows?",
        "Hey, @Githubguy132010\r\nI agree, we shouldn't mock the very thing we are trying to test. Can you rewrite the test to help us properly test your implementation.\r\n\r\n"
      ]
    }
  },
  "chrisradek": {
    "repos": [
      "aws/aws-sdk-js"
    ],
    "entries": [
      {
        "slug": "aws-sdk-js-complete-configuration-type-definitions",
        "title": "Complete configuration type definitions"
      },
      {
        "slug": "aws-sdk-js-content-integrity-verification",
        "title": "Content integrity verification"
      },
      {
        "slug": "aws-sdk-js-defensive-null-checking",
        "title": "Defensive null checking"
      },
      {
        "slug": "aws-sdk-js-document-apis-completely",
        "title": "Document APIs completely"
      },
      {
        "slug": "aws-sdk-js-document-apis-thoroughly",
        "title": "Document APIs thoroughly"
      },
      {
        "slug": "aws-sdk-js-early-return-after-errors",
        "title": "Early return after errors"
      },
      {
        "slug": "aws-sdk-js-follow-established-testing-patterns",
        "title": "Follow established testing patterns"
      },
      {
        "slug": "aws-sdk-js-limit-cache-size",
        "title": "Limit cache size"
      },
      {
        "slug": "aws-sdk-js-organize-type-declarations",
        "title": "Organize type declarations"
      },
      {
        "slug": "aws-sdk-js-semantic-naming-conventions",
        "title": "Semantic naming conventions"
      },
      {
        "slug": "aws-sdk-js-semantic-type-organization",
        "title": "Semantic type organization"
      },
      {
        "slug": "aws-sdk-js-standardize-api-promise-patterns",
        "title": "Standardize API promise patterns"
      },
      {
        "slug": "aws-sdk-js-structured-test-resource-management",
        "title": "Structured test resource management"
      },
      {
        "slug": "aws-sdk-js-test-configuration-precedence",
        "title": "Test configuration precedence"
      },
      {
        "slug": "aws-sdk-js-validate-configurations-with-clarity",
        "title": "Validate configurations with clarity"
      }
    ],
    "comments": {
      "aws-sdk-js-validate-configurations-with-clarity": [
        "Is it possible to have an environment variable that is not a string in node.js?\r\nI haven't tried in Windows, but on MacOS, that doesn't seem possible.\r\n```bash\r\nFOO=bar node -e \"console.log(process.env)\"\r\n# FOO: 'bar'\r\n```\r\n```bash\r\nFOO= node -e \"console.log(process.env)\"\r\n# FOO: ''\r\n```\r\n\r\n```bash\r\nnode -e \"process.env['FOO'] = void 0; console.log(process.env)\"\r\n# FOO: 'undefined'\r\n```\r\n\r\nEven trying to set it to undefined within the process still results in the variable being a string. If you want to throw an error when this variable is set to nothing, you might want to instead check if it is an empty string.",
        "Sorry for the incoming tangent, but I have 2 thoughts here.\r\n1. I think we should update `isBrowser` to no longer depend on `process` existing:\r\nhttps://github.com/aws/aws-sdk-js/blob/29fc8d39b73a9c7c15b6bc8fb28cf5ef313691f4/lib/util.js#L39\r\nWe've heard reports of this breaking with Angular, and it doesn't really make sense that we're depending on a global that doesn't exist natively in browsers. Instead, I'd propose we update our (browser|react-native|node)_loader.js files to set the function to return true or false accordingly.\r\n\r\n2. This is less important if we do the item above, but I wonder if it would make sense to separate this function. You could have 3 new functions, each in their own file that perform one of these checks. \r\nThen you could have a separate browser/node file that exports the same function. This function would import whatever checks they need. This has the benefit of your browser build not having to import the code used to check the shared Ini file, and you could then allow the shared ini check to import the shared-ini loader directly. However, it also means more source files and might be more difficult to track."
      ],
      "aws-sdk-js-early-return-after-errors": [
        "It doesn't seem like there's a need for this function. Couldn't you store the return value directly in an object, then pass that to the callback/return it?",
        "I don't think giving the user a truncated stream is really ok. I think this test was checking the behavior the SDK exhibited, rather than what was intended.\r\n\r\n[Here](https://github.com/aws/aws-sdk-js/blob/master/lib/request.js#L606-L611), there's a check to see if the incoming data matched the content-length. It should be throwing an error when the content-length is less than the data received. When I was testing though, I discovered that when the body is larger than the content-length, node throws a `ParseError`. However, because we were swallowing these errors (due to already receiving response headers), and node.js still gave us access to the body up to the content-length, it appeared as though we could never detect when data streamed in exceeded the expected amount.",
        "Call `return` either on the same line as `callback` or right under. Right now `callback` might be triggered twice: once with an error and then once without an error a couple lines below."
      ],
      "aws-sdk-js-content-integrity-verification": [
        "I think tapping into the `data` event on a stream could cause some unintended side-effects. In node.js 0.10.x, binding a `data` listener on a stream will cause the stream to emit 'data' events as fast as it can, ignoring the back-pressure that's automatically in place when using `pipe`. If the writable stream is slow, then this could cause loss of data.\n\nI don't think this is an issue in versions of node.js >= 0.12.x, and some simple testing seems to confirm that. However, we need to work with node.js 0.10.x as well.\n\nThe current method creates a new `writable` stream that also gets piped into in order to emit the 'sendProgress' events. I know it'll require refactoring your logic but that path seems safer across all our supported versions of node.\n",
        "In node.js, the response body is likely to be a stream. It might be easier in node to create a passthrough stream that generates the md5 hash and compares against the checksum. You'll know the expected content-length since it's passed in the response headers. Your solution here should still cover the browser use-case though.",
        "If `responseStream` is an `IncomingMessage`, you'll want to call `destroy` on it. As of node 8.0.0, readable streams also have the `destroy` method, so you can check if `destroy` is a function, then call it if it is.",
        "We'll still need to perform these checks for Node.js 0.8, especially if we start turning it on by default. Unfortunately 0.8.x doesn't include `Transform` in its `stream` package, so your implementation won't work for those cases. You'll likely need to resort to `data` listeners to perform your calculations if `Transform` doesn't exist."
      ],
      "aws-sdk-js-semantic-naming-conventions": [
        "Fair point, thanks for finding that.\n",
        "Good catch!\n",
        "Sure, I don't have a strong preference either way.",
        "Can you spell out `optionalDiscoveryVersionEndpoint` (assuming that's what Disver means). `Disver` isn't a common abbreviation and coming back to this in the future it may be confusing what this means."
      ],
      "aws-sdk-js-defensive-null-checking": [
        "Just to be safe, can you also add a check that error exists, similar to the conditional before this one?\n",
        "Thanks, didn't catch this due to the mistake above.\n",
        "Can you keep the `AWS.util.isBrowser() && window.localStorage !== null` checks at the top of the try block? As it is, this will always throw an error in node.js (and possibly some 'browser' environments like electron). I know the catch block should handle this case, but I'd like to avoid throwing an error if we can predict it.",
        "It might be useful to add one more check after the if block to set params equal to itself or an empty object to protect us in the unlikely event someone passes in a value like `null`. Imagine this would be a rare edge case but we do this already for other operations.\n",
        "Just in case someone wants to enter `0`, maybe use a `typeof x === 'number'` check instead."
      ],
      "aws-sdk-js-limit-cache-size": [
        "We had 2 issues reported because this cache key wasn't unique enough:\r\nhttps://github.com/aws/aws-sdk-js/pull/1054\r\n\r\nYou may need to pass the serviceClientId as well.\r\n",
        "I think operation is only required if custom identifiers are defined for an operation. Is that not the case? Might help to keep the size of your cache down if we omitted operation if it isn't needed."
      ],
      "aws-sdk-js-test-configuration-precedence": [
        "There is a separate test that implicitly tests this, but happy to add an explicit test.",
        "Can we add a test to make sure the credentials from `~/.aws/credentials` is used preferentially over the credentials in `~/.aws/config` if the same profile exists in both files?",
        "I'm not sure this test is actually ensuring that the creds from `profile foo` are used instead of `default`.\r\n\r\nWhat do you think about spying on `AWS.STS` or `AWS.Credentials` to get the accessKeyId that was used as the source?"
      ],
      "aws-sdk-js-complete-configuration-type-definitions": [
        "Can you add `CredentialsOptions` to the `update` method in `ConfigBase` as well? This should be allowed in service configuration in addition to the global config."
      ],
      "aws-sdk-js-document-apis-completely": [
        "Grammar: Maybe change to something like\r\n> Whether to enable endpoint discovery for operations that allow optionally using an endpoint returned by the service.\r\n\r\nI couldn't find an example of what other teams were using for their docs.",
        "Is `parseFile` exposed to consumers of the SDK? If so, it should probably have some documentation, otherwise we don't need typings for it."
      ],
      "aws-sdk-js-structured-test-resource-management": [
        "Are these tests running in node.js? This should only be running in browser environments, and is using the 3rd party `Buffer` package instead of node.js' `Buffer` package. We can place browser-specific tests in a separate folder and exclude them from being run by mocha in the npm unit script.",
        "Probably want to append a timestamp to this as well to make it somewhat unique",
        "Why don't you create the bucket used by all the tests in the `before` step? We shouldn't have to create a new bucket for every single test, just this suite of tests. \r\n\r\nI also wouldn't mix the `putObject` method with `createBucket`. `putObject` is doing too much, and adds 'global' (across tests) state. For example, you don't directly pass it the bucket or key, instead relying on a closure that every test has access to (and can change). That could lead to tricky edge cases coming up later that are hard to debug.",
        "But you could also make sure the bucket is there in the `before` hook. Just call `done()` after the `waitFor` method completes. Then you also only need to create it once; I don't think there's a reason we need to create a new bucket for every test since we aren't testing any bucket-specific configurations here. Creating a new bucket with each test also creates more points of failure (rate/resource limits, for example).\r\n",
        "If you `createBucket` in the `before` hook, you can `deleteBucket` in the `after` hook!"
      ],
      "aws-sdk-js-standardize-api-promise-patterns": [
        "It looks like `constructor.name` may not work in all the browsers we support and could have issues when minifiers are used:\nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function/name#Browser_compatibility\n\nCan you do an equality check against the constructor instead?\n",
        "Actually, what do you think about giving each class that should return a promise a static method that accepts a Promise constructor, then the class can control how it should promisify itself?\n\nThe pros to that approach would be the logic for adding promises would be controlled by each class, rather than defined in a long if/switch statement within a utility function. If the method to promisify a class was named the same for all classes, you can just check if the constructor has that method then call it, instead of maintaining a list of enums to check a class against. You could still make the `promisifyMethod` a utility method if that reduces code duplication.\n",
        "The `AWS.util.promisifyMethod` function currently only works for functions that accept a callback function as the first parameter. For example, `AWS.Request.send` and `AWS.Credentials.get` both accept just a callback.\r\n\r\nThe `s3.getSignedUrl` method accepts an operation name, params, and a callback, so using this method won't work. You should be able to set `getSignedUrlPromise` to a function that returns a new Promise. This promise can simply wrap the getSignedUrl function."
      ],
      "aws-sdk-js-semantic-type-organization": [
        "Can you add tests to make sure we can explicitly set a variable to the `DynamoDB.Converter` type? Also, would be nice to verify we can access `DocumentClient.ConverterOptions`.\r\n\r\nCan be simple, like:\r\n```javascript\r\nconst converter: Converter = DynamoDB.Converter;\r\n// and a test for input with converter options\r\nconst converterOptions: DynamoDB.DocumentClient.ConverterOptions = {convertEmptyValues: true};\r\nDynamoDB.Converter.input('string', converterOptions);\r\n```\r\n\r\nHow hard would it be to also expose ConverterOptions on the Converter namespace? Just feels a little odd having to access it off the DocumentClient namespace instead."
      ],
      "aws-sdk-js-document-apis-thoroughly": [
        "Would you mind adding some docs around this new config parameter?\nSomething like this at line 98:\n\n```\n * @!attribute signatureCache\n *   @return [Boolean] whether the signature to sign requests with (overriding\n *     the API configuration) is cached. Only applies to the signature version 'v4'.\n *     Defaults to `true`.\n```\n\nand something like this at line 185:\n\n```\n   * @option options signatureCache [Boolean] whether the signature to sign\n   *   requests with (overriding the API configuration) is cached. Only applies\n   *   to the signature version 'v4'. Defaults to `true`.\n```\n",
        "Minor: Might be worth adding an example that uses tags.",
        "Can you add a comment that explains what this function is supposed to do? It looks like you're populating an object with identifiers and customer-provided values, but it took me a while to grok that and the function name isn't clear. ",
        "I think this comment is a bit misleading, as is the one for `getCacheKey`. Both imply that you're going to get a single `key` (presumably a string), but you're returning a map. I think something like the following is more clear: \r\n```javascript\r\n/**\r\n * Get custom identifiers for cache key.\r\n * Identifies custom identifiers by checking each shape's `endpointDiscoveryId` trait.\r\n */\r\n```\r\nThis would at least help me, because I kept expecting `cacheKey` later on to be a string you pass to `endpointCache.get`, but it turns out you pass in a map of elements.",
        "If you want this to appear in documentation, you also need to attach IniLoader to the AWS namespace:\r\n```javascript\r\nAWS.IniLoader = AWS.util.inherit/* ... */\r\n\r\n// optionally also export it:\r\nmodule.exports = AWS.IniLoader;\r\n```\r\n\r\nYou'll also want to add doc strings to the public methods.",
        "Can you amend this to state a URL will be returned?\r\n\r\nSomething like:\r\n\r\n> Returns a 'thenable' promise that will be resolved with a pre-signed URL."
      ],
      "aws-sdk-js-organize-type-declarations": [
        "I wanted to export all the interfaces so that users could cast a result if they needed to, but I'm not sure if that's necessary. I'll have to take a look at some other TypeScript libraries to see if that's common practice.\n\nYou should only see these interfaces when looking at the service client constructors, but not on the service client instances.\n",
        "So, I tried a few different things. Ultimately, I went with putting the exported types in a sub-namespace:\n`declare namespace SERVICE.Types {`\nWhen I put them on their own namespace, I had to explicitly import them into my app, otherwise the typescript compiler would complain:\nhttps://github.com/Microsoft/TypeScript/issues/9944\n\nI also wanted them to be exported so user's can specify a type for cases when the typescript compiler can't quite infer what a type should be. That might not be necessary once https://github.com/Microsoft/TypeScript/issues/6606 is addressed.\n"
      ],
      "aws-sdk-js-follow-established-testing-patterns": [
        "For all these tests where you're making sure we remove empty inputs, can you also add tests to verify that we don't translate empty strings/sets/buffers if `convertEmptyValues` isn't set?",
        "We have a standard way of testing list/describe operations in most of our feature tests that look like this:\r\nhttps://github.com/aws/aws-sdk-js/blob/v2.135.0/features/acm/acm.feature#L7-L10\r\n\r\nIf you follow this patten, you don't have to create your own step definitions, since cucumber will use the ones defined here:\r\nhttps://github.com/aws/aws-sdk-js/blob/v2.135.0/features/extra/hooks.js#L56"
      ]
    }
  },
  "ChanochShayner": {
    "repos": [
      "bridgecrewio/checkov"
    ],
    "entries": [
      {
        "slug": "checkov-azure-encryption-property-names",
        "title": "Azure encryption property names"
      },
      {
        "slug": "checkov-centralize-environment-variables",
        "title": "Centralize environment variables"
      },
      {
        "slug": "checkov-choose-optimal-algorithms",
        "title": "Choose optimal algorithms"
      },
      {
        "slug": "checkov-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "checkov-consistent-naming-conventions",
        "title": "Consistent naming conventions"
      },
      {
        "slug": "checkov-document-configuration-consistently",
        "title": "Document configuration consistently"
      },
      {
        "slug": "checkov-extract-focused-functions",
        "title": "Extract focused functions"
      },
      {
        "slug": "checkov-safe-dictionary-access",
        "title": "Safe dictionary access"
      },
      {
        "slug": "checkov-safe-dictionary-navigation",
        "title": "Safe dictionary navigation"
      },
      {
        "slug": "checkov-strategic-error-handling",
        "title": "Strategic error handling"
      },
      {
        "slug": "checkov-strategic-exception-management",
        "title": "Strategic exception management"
      },
      {
        "slug": "checkov-thorough-test-assertions",
        "title": "Thorough test assertions"
      },
      {
        "slug": "checkov-use-correct-encryption-properties",
        "title": "Use correct encryption properties"
      }
    ],
    "comments": {
      "checkov-safe-dictionary-navigation": [
        "```suggestion\r\n            if conf.get(\"properties\", {}).get(\"siteConfig\") is not None:\r\n```",
        "we need to check if the fields are in the dict before accessing them.",
        "lets check if `storage_profile` field is a dict before accessing it.  "
      ],
      "checkov-document-configuration-consistently": [
        "The severities should be - \r\n`INFO`, `LOW`, `MEDIUM`, `HIGH`, `CRITICAL`"
      ],
      "checkov-extract-focused-functions": [
        "maybe worth moving the conditions to a func - `_is_provider_key`"
      ],
      "checkov-centralize-environment-variables": [
        "```suggestion\r\n        should_allow_multi_checks_skip = strtobool(os.getenv('CHECKOV_ALLOW_SKIP_MULTIPLE_ONE_LINE', 'False'))\r\n```\r\nbool('False') equal to True.",
        "done"
      ],
      "checkov-strategic-error-handling": [
        "This is a KeyError?\r\nIf so `except KeyError`"
      ],
      "checkov-choose-optimal-algorithms": [
        "Why we will create more often edges?\r\nIf we are looking for one specific target variable we can stop the iteration."
      ],
      "checkov-choose-optimal-data-structures": [
        "Why we will create more often edges?\r\nIf we are looking for one specific target variable we can stop the iteration."
      ],
      "checkov-azure-encryption-property-names": [
        "```suggestion\r\n        \"enableDoubleEncryption\": false,\r\n```",
        "```suggestion\r\n        \"enableDoubleEncryption\": true,\r\n```"
      ],
      "checkov-use-correct-encryption-properties": [
        "```suggestion\r\n        \"enableDoubleEncryption\": false,\r\n```",
        "```suggestion\r\n        \"enableDoubleEncryption\": true,\r\n```"
      ],
      "checkov-safe-dictionary-access": [
        "```suggestion\r\n            if conf.get(\"properties\", {}).get(\"siteConfig\") is not None:\r\n```"
      ],
      "checkov-consistent-naming-conventions": [
        "```suggestion\r\n        test_files_dir = Path(__file__).parent / \"a_example_skip\"\r\n```\r\nPlease use one word in the file names."
      ],
      "checkov-strategic-exception-management": [
        "This is a KeyError?\r\nIf so `except KeyError`"
      ],
      "checkov-thorough-test-assertions": [
        "Please assert the skipped part in the reports by resource - \r\nWe want to be sure `default` and `skip_invalid` are with one skip, and `skip_more_than_one` is with 2 skips.",
        "Please add assertions for the names of the resources as well."
      ]
    }
  },
  "wilkinsona": {
    "repos": [
      "spring-projects/spring-boot"
    ],
    "entries": [
      {
        "slug": "spring-boot-alphabetical-ordering-requirement",
        "title": "Alphabetical ordering requirement"
      },
      {
        "slug": "spring-boot-bean-lifecycle-management",
        "title": "Bean lifecycle management"
      },
      {
        "slug": "spring-boot-clear-structured-logging-documentation",
        "title": "Clear structured logging documentation"
      },
      {
        "slug": "spring-boot-concrete-bean-return-types",
        "title": "Concrete bean return types"
      },
      {
        "slug": "spring-boot-consistent-observability-data",
        "title": "Consistent observability data"
      },
      {
        "slug": "spring-boot-consistent-terminology-usage",
        "title": "Consistent terminology usage"
      },
      {
        "slug": "spring-boot-document-configuration-properties-completely",
        "title": "Document configuration properties completely"
      },
      {
        "slug": "spring-boot-documentation-clarity-principles",
        "title": "Documentation clarity principles"
      },
      {
        "slug": "spring-boot-environment-variables-best-practices",
        "title": "Environment variables best practices"
      },
      {
        "slug": "spring-boot-explicit-security-configurations",
        "title": "Explicit security configurations"
      },
      {
        "slug": "spring-boot-explicit-security-documentation",
        "title": "Explicit security documentation"
      },
      {
        "slug": "spring-boot-follow-consistent-style-conventions",
        "title": "Follow consistent style conventions"
      },
      {
        "slug": "spring-boot-include-database-specific-migration-dependencies",
        "title": "Include database-specific migration dependencies"
      },
      {
        "slug": "spring-boot-inherit-organization-security-policies",
        "title": "Inherit organization security policies"
      },
      {
        "slug": "spring-boot-maintain-consistent-naming-patterns",
        "title": "Maintain consistent naming patterns"
      },
      {
        "slug": "spring-boot-meaningful-exception-design",
        "title": "Meaningful exception design"
      },
      {
        "slug": "spring-boot-optimize-test-case-design",
        "title": "Optimize test case design"
      },
      {
        "slug": "spring-boot-preserve-api-compatibility",
        "title": "Preserve API compatibility"
      },
      {
        "slug": "spring-boot-property-description-conventions",
        "title": "Property description conventions"
      },
      {
        "slug": "spring-boot-reference-existing-configurations",
        "title": "Reference existing configurations"
      },
      {
        "slug": "spring-boot-stable-observability-components",
        "title": "Stable observability components"
      }
    ],
    "comments": {
      "spring-boot-stable-observability-components": [
        "Should we link to something stable (a tag or specific SHA) here? Perhaps https://github.com/OpenObservability/OpenMetrics/blob/v1.0.0/specification/OpenMetrics.md#exemplars."
      ],
      "spring-boot-concrete-bean-return-types": [
        "The return type of a `@Bean` method should be as specific as possible. Could this be `DefaultPulsarConsumerFactory` with `@ConditionalOnMissingBean(PulsarConsumerFactory.class)`, similar to `topicResolver` above?",
        "Yes. This provides as much type information as possible to the bean factory while allow the bean to back off when any `PulsarConsumerFactory` bean is defined.",
        "Rather than changing the return type, which deprives the bean factory of type information, it would be better to change `@ConditionalOnMissingBean` to `@ConditionalOnMissingBean(ExamplarSampler.class)`. This will cause `exemplarSampler` to back off if there's an `ExamplarEampler` bean defined, while also still telling the bean factory that `exemplarSampler` is a `DefaultExemplarSampler`.",
        "I think this could be considered a bug and, therefore, it should be handled separately. Could you please split this out into a new PR, along with a test that verifies that the auto-configured `spanReporter` bean backs off when you define a custom `Reporter` bean?",
        "To provide as much type information as possible to the bean factory, the signature should show that it returns a `B3Propagator`."
      ],
      "spring-boot-clear-structured-logging-documentation": [
        "Thanks, @quaff. I wonder if people might miss this? They could read the paragraph above and then stop, happy that they know which configuration property to set. I think it might be more noticeable if we changed the paragraph above instead to start with something like \"To enable structured logging, ensure that you aren't using custom log configuration and set the propertyâ€¦\". WDYT?",
        "I agree with @mhalbritter. We shouldn't over-complicate the documentation by showing the use of conditionals if they aren't always needed."
      ],
      "spring-boot-bean-lifecycle-management": [
        "Is it possible to not expose the imperative components as beans in a reactive app?",
        "> The reactive components leverage the reactive client which adapts the original client (which is imperative and auto-configured)\r\n\r\nI wonder if the original, imperative client has to be a bean in the reactive case. Perhaps it does for ease of configuration property binding and the like. Not something to worry about for this PR, but probably worth revisiting down the road before 3.2 GAs.\r\n\r\n>  there are other technologies auto-configured in Boot whose reactive components leverage the imperative ones, no?\r\n\r\nWe have some situations where there's a low-level bean that's shared between imperative and reactive (Elasticsearch client transport and the ElasticsearchClient and ReactiveElasticsearchClient that use it, for example) but I can't think of a situation where we have an imperative bean that we expect applications to use extensively that's then wrapped by a reactive bean. I may well be forgetting something though."
      ],
      "spring-boot-optimize-test-case-design": [
        "This is largely testing the underlying SpEL support. It could be simplified to test a single valid expression.",
        "This is largely testing the underlying SpEL support. It could be simplified to test a single invalid expression.",
        "+1. For this to warrant being a Docker-based test, I think it should somehow check that setting the `application_name` has had the desired effect on the Postgres side of things.\r\n\r\nI'd then prefer that we only have a single Docker-based test that checks this and that the precedence of the different sources of the application name is unit tested without involving Docker. Right now we're adding a lot to the build time and I don't think the extra test coverage warrants that increase.",
        "I don't think so. Given that all three values are passed through and then returned by `getMaxParameterCount()` without any having special treatment, I think testing with a single value would be sufficient.",
        "The paths aren't right in this example, but Framwork 6.2's `JsonContent` is an option here:\r\n\r\n```java\r\nJsonContent body = new JsonContent(entity.getBody());\r\nassertThat(body).extractingPath(\"alias\").isEqualTo(\"spring-boot-ssl-sample\");\r\nassertThat(body).extractingPath(\"status\").isEqualTo(\"EXPIRED\");\r\nassertThat(body).extractingPath(\"subject\")\r\n\t.isEqualTo(\"CN=localhost,OU=Unknown,O=Unknown,L=Unknown,ST=Unknown,C=Unknown\");\r\n```",
        "I don't feel strongly one way or the other. That said, I do wonder if the intent would be clearer with four separate methods with names that make the expectations clear. They could all then delegate to a common method that looks a lot like this one."
      ],
      "spring-boot-documentation-clarity-principles": [
        "What failure did you get? That `xref` seems to work for me.",
        "I just pushed a change to your branch. Let's see what CI makes of it.",
        "It failed. `:spring-boot-project:spring-boot-docs:antora` which pulls in the Actuator API docs worked for me locally. It's `spring-boot-project:spring-boot-actuator-autoconfigure:antora` that's failing on CI though.\r\n\r\nAny ideas, @philwebb? The reference works fine when built as part of `spring-boot-docs` but not when just building the Actuator API docs on their own.",
        "I think this was intentionally singular. If it's to change, I think it should be changed to \"container\".",
        "Oops. Yeah, I think we can just remove it as we're already in a section about Testcontainers. I think \"for Container beans\" reads better and there's enough context to know that it's a Testcontainers container.",
        "I'd just document the map-based approach.",
        "Could we use \"over\" here? I think it's more idiomatic when talking about a protocol.",
        "This looks to be unrelated to the rest of the changes. Could it please be made separately?",
        "> If you still insist, I can move it to separate PR.\r\n\r\nYes please, Artem. It sounds like we should get rid of that sentence in 2.4.x and later whereas the rest of the changes proposed here will land in 2.6."
      ],
      "spring-boot-inherit-organization-security-policies": [
        "I don't think we need this as we already inherit the policy from spring-projects/.github. We can just link to https://github.com/spring-projects/spring-boot/security/policy."
      ],
      "spring-boot-reference-existing-configurations": [
        "Is it possible to tie this to the version that's configured in `.sdkmanrc`? That's particularly important for branches other than `main` where a Java 8 install is required.",
        "To avoid the risk of editing conflicts, I think we might prefer to have this disabled too.",
        "Those docs, which is where I learned of the potential for a conflict, say that adding the badge \"can also create a concurrent editing conflict when the bot and a user try to edit the description of a pull request at the same time\"."
      ],
      "spring-boot-maintain-consistent-naming-patterns": [
        "I wonder if it would be better if we ignored that fact that it's `BatchSpanProcessor` that's the component that's doing the exporting and focussed instead on the properties affecting span export.\r\n\r\n\r\n```\r\nmanagement.tracing.opentelemetry.span.export.include-unsampled\r\nmanagement.tracing.opentelemetry.span.export.schedule-delay\r\nmanagement.tracing.opentelemetry.span.export.timeout\r\n```",
        "> However, these properties only impact the BatchSpanProcessor and cannot be applied, for instance, to the SimpleSpanProcessor or other processors.\r\n\r\nBoot doesn't auto-configure the `SimpleSpanProcessor` so hopefully that confusion will not arise. The property names make no mention of processing so hopefully no one will expect them to apply to other processors either.\r\n\r\n>  In my opinion, there should be some indication that these properties are intended for batch processing\r\n\r\nDoesn't the `max-batch-size` property (that we omitted in the property examples above) do that?\r\n\r\nThe complete set of properties would be:\r\n\r\n```\r\nmanagement.tracing.opentelemetry.span.export.include-unsampled\r\nmanagement.tracing.opentelemetry.span.export.max-batch-size\r\nmanagement.tracing.opentelemetry.span.export.max-queue-size\r\nmanagement.tracing.opentelemetry.span.export.schedule-delay\r\nmanagement.tracing.opentelemetry.span.export.timeout\r\n```\r\n\r\nI'm not totally sold on the prefix here. In part, it's the `otlp` vs `opentelemetry` problem again. I think this export may use Zipkin or OTLP depending on which of `ZipkinSpanExporter`, `OtlpHttpSpanExporter`, and `OtlpGrpcSpanExporter` has been auto-configured. We already have a couple of export-related properties for those:\r\n\r\n- `management.otlp.tracing.export.enabled`\r\n- `management.zipkin.tracing.export.enabled`",
        "I think `opentelemetry` is better here as this is configuring parts of the OpenTelemetry SDK. The protocol used for the export won't necessarily by OTLP so I think `otlp` would be inaccurate. This naming and its subtleties are more complex than we'd like but that complexity isn't of our making and there's only so much we can do to hide it.\r\n\r\n+1 for dropping `.span.` from the names.",
        "We prefer longer names for test methods that describe what's being tested. Something like `whenCustomizerBeanIsDefinedThenItIsConfiguredOnSpringLiquibase`.",
        "I think this should be called `SdkTracerProviderBuilderCustomizer`. We can take care of that as part of merging it.",
        "I think we should aim for consistency with other similar interfaces that already exist. We have many that are named `â€¦BuilderCustomizer` because the class that they are customising is a `â€¦Builder`. The same applies here.",
        "For consistency with `alpha-numeric -> alphanumeric`, this method should be renamed to `remainderIsNotAlphanumeric`.",
        "Is white box a recognised Wavefront term? We try to avoid white box and black box if we can as they're jargon that can confuse people, particularly those with English as a second language."
      ],
      "spring-boot-follow-consistent-style-conventions": [
        "We don't really use `Objects.equals`. This could be `return \"trust\".equals(postgresAuthMethod);` instead.",
        "We don't use `var` in Boot's code base. Please declare variables' types explicitly.",
        "We don't use `var` in Spring Boot's code.",
        "There are [several other places where we use `@Order(Ordered.LOWEST_PRECEDENCE)`. If we're going to change one, we should change them all. Personally, I don't think we should change any of them as I prefer to see the order explicitly declared.",
        "`(exporter) ->` is Spring Boot's code style and this doesn't pass Checkstyle. Please build your changes before submitting them.",
        "We're still escaping the opening `[` but the closing `]` is no longer escaped. I think the parser may cope with that but I find the expression harder to read as you have to know that the parser treats the `]` as a plain `]` rather than closing a never-opened character set. Please revert."
      ],
      "spring-boot-property-description-conventions": [
        "Given that the property's prefix is `management.tracing.brave` do we need `(tracing library)` in the description? Put another way, is there another Brave that could cause confusion?",
        "We try to have `boolean` properties start with `Whether theâ€¦`. Perhaps this could be something like this instead:\r\n\r\n> Whether the propagation type and tracing backend support sharing the span ID between client and server spans. Requires B3 propagation and a compatible backend."
      ],
      "spring-boot-meaningful-exception-design": [
        "Let's consider this separately please. If we do it, it should be applied on the servlet side as well.",
        "We think that adding the stop or destroy failure as a suppressed exception is a good idea. Would you like to open a PR for it or would you prefer that we take care of it?",
        "Thanks!",
        "I think an `IllegalStateException` or `IllegalArgumentException` (it depends if you consider `target` or `type` to be the problem) might be better here and perhaps the error message should include `target`. Something like `\"Cannot unwrap \" + target + \" to \" + type.getName()`",
        "I'd rather this failed with an NPE as the initialize method should always be present.",
        "If the field's not found something's gone really wrong. I think it would be better to fail hard.",
        "If Hikari doesn't meet expectations then I think we should just give up as we have no way of knowing that our approach is valid. We'll know, through tests, that it works with the version of Hikari in dependency management. If someone overrides that version and Hikari has changed incompatibility I think we'd be better to fail fast. That'll make the problem much easier to diagnose rather than dealing with a checkpoint failure and trying to figure out why it occurred.",
        "`NoSuchJobException` is a `JobExecutionException` so I don't think there's any need to create a new exception. Instead, I think the `try` and `catch` could be removed completely so that the method just throws the original `NoSuchJobException`."
      ],
      "spring-boot-document-configuration-properties-completely": [
        "The annotation processor the generates configuration property metadata can't handle enums due to a limitation of the annotation processing model. Please add an entry to `additional-spring-configuration-metadata.json` for this property's default value.",
        "This should be an `int` with a default value that matches that of `InMemoryWebSessionStore`. Providing a default improves the metadata that's used for auto-completion when editing application properties and YAML files. A test should be added to assert that the defaults are in sync.",
        "Unfortunately, we can't use javadoc tags in the descriptions of configuration properties as it doesn't work well with configuration property metadata generation. We'll have to use `AggregationTemporality` or even just `Aggregation temporality` instead.",
        "This default should be declared in the additional metadata as the annotation processor cannot detect default enum values."
      ],
      "spring-boot-explicit-security-documentation": [
        "This property name is incorrect. It should be `spring.security.oauth2.resourceserver.jwt.audiences`. As previously requested, please use the `configprop:spring.security.oauth2.resourceserver.jwt.audiences[]` syntax so that the name is checked as part of the build.",
        "This (line 234) should not be here. It should be part of line 226.",
        "Sorry, it's still not right and you've moved it to line 229 rather than updating 226 as I requested above. Please don't make any more changes. I'll take things from here.",
        "That looks better. Please go ahead.",
        "I agree. In the context of CSRF, it would be better to just disable Security's protection rather than ignoring entirely. This is what the old (1.x) auto-configuration for the console used to do:\r\n\r\nhttps://github.com/spring-projects/spring-boot/blob/10a5cef4ef33e7c86d18e1f92793c2aaa57d5a82/spring-boot-autoconfigure/src/main/java/org/springframework/boot/autoconfigure/h2/H2ConsoleAutoConfiguration.java#L97-L113",
        "The goal here is to consistently use the term \"expose\" but this change uses \"secret\". I think this sentence would be better if it was something like the following:\r\n\r\n> For security purposes, only the `/health` endpoint is exposed over HTTP by default."
      ],
      "spring-boot-include-database-specific-migration-dependencies": [
        "This wildcard search doesn't work for me and reports 0 results. I think we may have to link to https://documentation.red-gate.com/flyway/flyway-cli-and-api/supported-databases instead. There are also some DB-specific modules that are not named `flyway-database-*` such as the [module for MySQL](https://documentation.red-gate.com/flyway/flyway-cli-and-api/supported-databases/mysql)."
      ],
      "spring-boot-preserve-api-compatibility": [
        "This is a breaking API change. A separate method for the timeout would be better.",
        "This is a breaking API change. A separate method for the timeout would be better."
      ],
      "spring-boot-consistent-observability-data": [
        "I wonder if something like \"Duration of HTTP server request handling\" would be better here? To me, it makes it clearer that the metric is for a server handling a request from a client rather than a client making a request to a server."
      ],
      "spring-boot-environment-variables-best-practices": [
        "I'm not in favor of trying to mock environment variables so +1 for @nosan's suggestion.\r\n\r\nMore generally, where we need to test environment variables, we try to allow the `Map` to be injected for testing purposes. If third-party code doesn't allow that then I'd prefer that we accept that it can't be tested rather than relying on reflection to hack into `System.getenv`.",
        "I don't think spaces in the default makes any difference. For example, you can do this:\r\n\r\n```\r\nproperty.example=${some.other.property:The default value}\r\n```\r\n\r\nAssuming that `some.other.property` hasn't been set the value of `property.example` will default to `The default value`."
      ],
      "spring-boot-alphabetical-ordering-requirement": [
        "The modules should be listed alphabetically",
        "We try to list modules in alphabetical order."
      ],
      "spring-boot-consistent-terminology-usage": [
        "+1 for changing to \"autowire\". The Framework docs use \"autowire\" as a verb and we should follow suit, particularly as the example that follows doesn't even use `@Autowired` as it does not need to do so.",
        "Thanks for the suggestion. I think it would be better to use Kubernetes here rather than the K8S abbreviation."
      ],
      "spring-boot-explicit-security-configurations": [
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here.",
        "I think `CsrfConfigurer::disable` would read better here."
      ]
    }
  },
  "sbrannen": {
    "repos": [
      "spring-projects/spring-framework"
    ],
    "entries": [
      {
        "slug": "spring-framework-api-boundary-null-handling",
        "title": "API boundary null handling"
      },
      {
        "slug": "spring-framework-cleanup-error-handling",
        "title": "Cleanup error handling"
      },
      {
        "slug": "spring-framework-complete-api-documentation",
        "title": "Complete API documentation"
      },
      {
        "slug": "spring-framework-consistent-style-conventions",
        "title": "Consistent style conventions"
      },
      {
        "slug": "spring-framework-database-agnostic-sql-syntax",
        "title": "Database-agnostic SQL syntax"
      },
      {
        "slug": "spring-framework-descriptive-specific-names",
        "title": "Descriptive specific names"
      },
      {
        "slug": "spring-framework-design-for-api-extension",
        "title": "Design for API extension"
      },
      {
        "slug": "spring-framework-optimize-ci-environment-configuration",
        "title": "Optimize CI environment configuration"
      },
      {
        "slug": "spring-framework-package-null-safety-annotations",
        "title": "Package null-safety annotations"
      },
      {
        "slug": "spring-framework-respect-annotation-processing-order",
        "title": "Respect annotation processing order"
      },
      {
        "slug": "spring-framework-spring-annotation-best-practices",
        "title": "Spring annotation best practices"
      },
      {
        "slug": "spring-framework-spring-code-style",
        "title": "Spring code style"
      },
      {
        "slug": "spring-framework-use-assertj-correctly",
        "title": "Use AssertJ correctly"
      },
      {
        "slug": "spring-framework-use-documentation-features-properly",
        "title": "Use documentation features properly"
      },
      {
        "slug": "spring-framework-use-environment-independent-defaults",
        "title": "Use environment-independent defaults"
      },
      {
        "slug": "spring-framework-verify-operation-semantics",
        "title": "Verify operation semantics"
      }
    ],
    "comments": {
      "spring-framework-respect-annotation-processing-order": [
        "Please move the interface search above the annotation search on the current class (i.e., directly under `if (visited.add(sourceClass)) {`).\r\n\r\nAlso, please add a test which verifies that locally declared `@Import` annotations are processed _after_ `@Import` annotations discovered on implemented interfaces. For example, a bean imported from a local `@Import` annotation should be able to override a bean imported via an `@Import` declaration on an implemented interface.",
        "`OverridingConfig` should be annotated with `@Import` (and import something that overrides the `ImportedBean`) in order for `localImportShouldOverrideInterfaceImport()` to test this use case.",
        "Before @aahlenst posted those last two comments, I was also going to point out that there is a precedent for this, namely `@DirtiesContext`.\r\n\r\nI agree that it is a bit strange for it to be possible to specify a class-level setting at the method level that simply gets ignored; however, I am not in favor of introducing yet another SQL-related annotation for this purpose. We already have quite a few. And I don't think it necessarily warrants an additional enum and annotation attribute.\r\n\r\nAs it stands in the proposal, the addition of the two new enum constants in `ExecutionPhase` makes it impossible for a user to configure conflicting phases for a given `@Sql` declaration, and that's a good thing.\r\n\r\nFWIW, I have never heard any complaints from users about the `ClassMode` and `MethodMode` dichotomy in `@DirtiesContext`, and the documentation clearly states that setting an inappropriate mode \"has no meaning\" (i.e., will be ignored).\r\n\r\nIn light of that, I am OK with the PR as-is.",
        "Yes, we can definitely throw an exception for inappropriate user configuration.\r\n\r\n`IllegalArgumentException` sounds reasonable to me.",
        "> We throw now.\r\n\r\nThanks\r\n\r\n> Just thinking aloud: If `@Sql` has this, wouldn't it be nice if `@DirtiesContext` had it, too? Or would that be too disruptive years after the introduction?\r\n\r\n`@Sql` is different because it has a single enum-based `executionPhase` attribute, which means we can easily detect a user configuration error.\r\n\r\nWhereas, `@DirtiesContext` has two mutually exclusive enum-based attributes: `classMode` and `methodMode`, and each of those has a `default` value. Thus, we cannot actually detect a user configuration error.\r\n\r\nThe only way to be able to detect a user configuration error for `@DirtiesContext` would be to introduce new `DEFAULT` enum constants in `ClassMode` and `MethodMode` and use those as the `default` attribute values going forward.\r\n\r\nHowever, that would be a breaking change for any users who had inadvertently explicitly supplied the previous default values for those attributes.\r\n\r\nPlus, it would make things a bit more cumbersome.\r\n\r\nIn light of the above, I don't think we should make any changes to `@DirtiesContext` in this regard this late in the game.\r\n"
      ],
      "spring-framework-verify-operation-semantics": [
        "That's not entirely true.\r\n\r\nSpEL does not honor all of Groovy's \"truth\" rules.\r\n\r\nThe SpEL Elvis operator checks for values that are non-null **and** non-empty (for Strings).",
        "Selection is actually supported for arrays and anything that implements `java.lang.Iterable` or `java.util.Map`.\r\n\r\nWould you like to update your PR to reflect that?"
      ],
      "spring-framework-consistent-style-conventions": [
        "Please sort dependencies alphabetically in the build script.",
        "The `RowMapper` implementation is stateless. Therefore, please store a single reference to it in a `final` field and remove the `getActorMapper()` method.",
        "Please inline the lambda expression."
      ],
      "spring-framework-use-documentation-features-properly": [
        "Yes, indeed. That's a step in the right direction. I'll rework the wording locally after merging the PR.",
        "```suggestion\r\nthe <<webflux, Spring WebFlux>> framework,\r\n```\r\n\r\nYour fix works; however, when cross referencing a section within the current file (after includes have been applied), there is no need to use the folder and file name.\r\n\r\nI'll simplify that when merging the PR."
      ],
      "spring-framework-spring-annotation-best-practices": [
        "```suggestion\r\n        private Pojo self;\r\n```\r\n\r\nUnless you are certain CGLIB proxies are being created, this would need to be `Pojo`.\r\n\r\nThough, there's no need to update this PR. I'll address that after merging."
      ],
      "spring-framework-descriptive-specific-names": [
        "```suggestion\r\n\tprivate boolean quoteIdentifiers = false;\r\n```\r\n\r\n\"Using escaping\" is too generic and could potentially conflict with a (yet unknown) future feature.\r\n\r\nLet's name this according to what it is actually used for.\r\n\r\nNote that I've updated the title of this PR to reflect that as well."
      ],
      "spring-framework-cleanup-error-handling": [
        "At a quick glance, this change is the only change in this PR that does not break existing behavior.\r\n\r\nCan you please revisit your changes and ensure that existing behavior is not altered?\r\n\r\nFor example, a try-with-resources block will close the `AutoCloseable` object, but it will not swallow exceptions thrown by the invocation of `close()`. So anywhere that we were previously intentionally swallowing exceptions such as `IOException`, we would still have to swallow those exceptions.\r\n\r\nIn addition, we would have to be able to disambiguate between an `IOException` thrown from within the try-block vs. one thrown by the invocation of `close()`.\r\n\r\nIn light of that, I don't think it makes sense to use try-with-resources in such use cases, but I'm happy to be proven wrong.",
        "Actually, I'd appreciate it if you could keep the one valid change and add inline documentation to the other locations to point out why try-with-resources should not be used there.\r\n\r\nThat will provide a big help to the team to make sure we don't accidentally switch to try-with-resources for those use cases in the future.",
        ">Actually, I'd appreciate it if you could keep the one valid change and add inline documentation to the other locations to point out why try-with-resources should not be used there.\r\n\r\nOf course if you'd prefer to simply close this PR, that's also fine.\r\n\r\nI'll leave it up to you.",
        "Thanks for making the changes. That looks good now."
      ],
      "spring-framework-use-assertj-correctly": [
        "Please replace all `assert` statements with AssertJ assertions.",
        "Please use AssertJ for assertions.\r\n\r\nThe build will fail if you attempt to use JUnit Jupiter's `Assertions` class.\r\n\r\nSpeaking of which, please run a full build locally before submitting a PR (`./gradlew check`) to ensure that the build succeeds.",
        "When we check the size of a collection using AssertJ, we prefer to use `assertThat(list).hasSize(...)`.",
        "Please replace with an AssertJ assertion along the lines of `assertThatExceptionOfType(BadSqlGrammarException.class).isThrownBy(...)`.",
        "It is not permissible to use JUnit Jupiter's `Assertions`. Doing so will actually fail the build.\r\n\r\nThus, please switch to AssertJ assertions and be sure to execute `./gradlew check` to verify that you don't have any Checkstyle violations in this regard.\r\n\r\nNote: if you want something similar to `assertAll`, you may wish to use `SoftAssertions` from AssertJ.",
        "We use AssertJ for test assertions, not Spring's `org.springframework.util.Assert` class.\r\n\r\nIn any case, I'll make that change when merging the PR.",
        "For all assertions that are expected not to throw an exception, please rework them to use the following AssertJ construct, where `{}` is the code in question.\r\n\r\n```java\r\nassertThatCode(() -> {}).doesNotThrowAnyException();\r\n```"
      ],
      "spring-framework-optimize-ci-environment-configuration": [
        "I'd also recommend using a more recent version of JDK 8, like we do in the [JUnit 5 build](https://github.com/junit-team/junit5/blob/master/.travis.yml).\n",
        "> Travis CI is using 1.8.0_65 as is... that doesn't seem so bad. In the interest of saving build time, it seems best not do sudo apt-get update && sudo apt-get install oracle-java8-installer as your junit example does. What do you think?\n\nIf Travis CI is now using JDK 8 Update 65 as the default, then I agree: that should be fine.\n\nFor the JUnit 5 build, that wasn't the case when we set up the build: Travis CI used to default to an early JDK 8 release that didn't work for us.\n"
      ],
      "spring-framework-package-null-safety-annotations": [
        "Ideally this should enforce that both `@NonNullApi` and `@NonNullFields` are present -- though I suppose that would require two regular expressions (for the \"single line\" approach).\r\n",
        "```suggestion\r\n\t\t\t<property name=\"minimum\" value=\"2\"/>\r\n\t\t\t<property name=\"maximum\" value=\"2\"/>\r\n```\r\n\r\nWe expect exactly 2 `@NonNull*` declarations, specifically `@NonNullApi` and `@NonNullFields`.",
        "```suggestion\r\n\t\t\t<property name=\"message\" value=\"package-info.java is missing required null-safety annotations @NonNullApi and @NonNullFields.\"/>\r\n```\r\n\r\nSpring's `@NonNullApi` and `@NonNullFields` annotations should never span multiple lines."
      ],
      "spring-framework-database-agnostic-sql-syntax": [
        "```suggestion\r\n\t\tString expected = \"INSERT INTO `S`.`T` (`F`, `S`) VALUES(?, ?)\";\r\n```\r\n\r\nJust a side note: the schema and table names have to be quoted independently.\r\n\r\nI've fixed this in my local branch and added integration tests with H2 to verify it."
      ],
      "spring-framework-spring-code-style": [
        "In general, contributors are required to adhere to the [Spring Framework Code Style](https://github.com/spring-projects/spring-framework/wiki/Spring-Framework-Code-Style). So please familiarize yourself with that and rework your PR.\r\n\r\nFor example, you need to undo **all** changes to import ordering and static imports throughout all changed classes.",
        "The team has not yet decided to permit `var` declarations in _production code_ yet, so please use typed variable declarations for the time being.",
        "Although I fully understand the desire to remove `public` here, historically the code base often makes constructors `public` even if the enclosing type is not `public`.\r\n\r\nLook at the other `JdkClientHttpRequest` implementation as well as neighboring classes for examples.\r\n\r\nIn light of that, I removed this change when merging the PR.",
        "We do not use static imports for utility methods in the core Spring Framework.",
        "In any case, please remove empty space after an opening parenthesis and before a closing parenthesis, since that is the standard convention within the Spring Framework codebase.\r\n\r\nAnd please make that change consistently within this PR.",
        "We don't use `var` in Spring Framework.\r\n\r\nThus, please remove all usage of `var` in this PR.",
        "```suggestion\r\n\t\tString ip = \"10.0.0.1\";\r\n```\r\n\r\nWe don't typically declare local variables as final unless there's a compelling reason.",
        "Please note that the formatting you introduced fails the build with the following.\r\n\r\n```\r\n> Task :spring-beans:checkstyleMain\r\n[ant:checkstyle] [ERROR] /.../spring-beans/src/main/java/org/springframework/beans/support/PropertyComparator.java:116: Line has leading space characters; indentation should be performed with tabs only. [RegexpSinglelineJava]\r\n[ant:checkstyle] [ERROR] /.../spring-beans/src/main/java/org/springframework/beans/support/PropertyComparator.java:117: Line has leading space characters; indentation should be performed with tabs only. [RegexpSinglelineJava]\r\n[ant:checkstyle] [ERROR] /.../spring-beans/src/main/java/org/springframework/beans/support/PropertyComparator.java:118: Line has leading space characters; indentation should be performed with tabs only. [RegexpSinglelineJava]\r\n```\r\n\r\nPlease make sure you run `./gradlew check` before submitting a PR to catch such errors locally.\r\n\r\nI'll fix the formatting before merging, so there's no need to update this PR."
      ],
      "spring-framework-api-boundary-null-handling": [
        "This breaks our null-safety contracts.\r\n\r\nIf we were to implement this method in `SimpleEvaluationContext`, we would want to return an empty list.\r\n\r\nHowever, by changing the method to a default method in `EvaluationContext`, we will no longer need to override this method (unless we decide to support custom `IndexAccessor` implementations in `SimpleEvaluationContext`).",
        "We rely on nullability constraints in such cases, and `delimiters` should never be `null`, since `@Nullable` is not declared for that method parameter.\r\n\r\nIf a user supplies a `null` value for the var-args array, the resulting `NullPointerException` should suffice to inform them of their error.",
        "```suggestion\r\n\t\tsuper(bufferFactory, new HttpHeaders(new NettyHeadersAdapter(Objects.requireNonNull(response, \"HttpServerResponse must not be null\").responseHeaders())));\r\n```\r\n\r\nAlthough it will throw a `NullPointerException` instead of an `IllegalArgumentException`, if we use `Objects.requireNonNull` we can still have a custom error message.\r\n\r\nSo let's go with that."
      ],
      "spring-framework-complete-api-documentation": [
        "```suggestion\r\n\t/**\r\n\t * Returns a concise description of this {@code HandlerMethod}, which is used\r\n\t * in log and error messages.\r\n\t * <p>The description should typically include the method signature of the\r\n\t * underlying handler method for clarity and debugging purposes.\r\n\t */\r\n```",
        "Please document the `RetryExecution` parameter in all methods in this interface.",
        "Add class-level Javadoc.",
        "Indexing it not limited to arrays.\r\n\r\nIn addition, the rest of the Javadoc in this class was copied from `PropertyAccessor` and needs to updated to discuss indexing instead of property access.",
        "In Spring Framework, we attempt to wrap Javadoc around 90 characters, and we never end a `@param` or `@return` description with a period `.` unless the description includes sentences (which we try avoid)."
      ],
      "spring-framework-use-environment-independent-defaults": [
        "Yes, that's problematic in Eclipse IDE.\r\n\r\n```\r\nThe import org.springframework.web.reactive.function.client.CoExchangeFilterFunction cannot be resolved\r\n\r\nDefaultWebClient.java\r\n\r\n/spring-webflux/src/main/java/org/springframework/web/reactive/function/client\r\n\r\nline 61\r\n```",
        "These changes break the build.\r\n\r\nPlease revert these changes.\r\n\r\n[Pattern Matching for switch](https://openjdk.org/jeps/441) did not become an \"enabled\" feature until Java 21, and we have a Java 17 baseline.\r\n\r\nMore importantly, please ensure that you actually run the build before submitting a PR. \r\n\r\n"
      ],
      "spring-framework-design-for-api-extension": [
        "Since this is a `protected` method in a `public` type, we cannot simply change the method signature.\r\n\r\nInstead, we would need to introduce an overloaded variant that accepts `Executable` and deprecate the existing variant that accepts `Member`.",
        "I have not investigated the claims in this issue or the rest of this PR; however, based on a quick glance I can say that this change is not permissible since it changes the signature of a public API."
      ]
    }
  },
  "yottta": {
    "repos": [
      "opentofu/opentofu"
    ],
    "entries": [
      {
        "slug": "opentofu-clear-concise-documentation",
        "title": "Clear concise documentation"
      },
      {
        "slug": "opentofu-clear-relationship-descriptions",
        "title": "Clear relationship descriptions"
      },
      {
        "slug": "opentofu-contextualize-security-findings",
        "title": "Contextualize security findings"
      },
      {
        "slug": "opentofu-craft-actionable-errors",
        "title": "Craft actionable errors"
      },
      {
        "slug": "opentofu-document-intent-and-limitations",
        "title": "Document intent and limitations"
      },
      {
        "slug": "opentofu-document-phased-migration-paths",
        "title": "Document phased migration paths"
      },
      {
        "slug": "opentofu-document-reference-standards",
        "title": "Document reference standards"
      },
      {
        "slug": "opentofu-explicit-versus-dynamic-configurations",
        "title": "Explicit versus dynamic configurations"
      },
      {
        "slug": "opentofu-log-effectively-for-debugging",
        "title": "Log effectively for debugging"
      },
      {
        "slug": "opentofu-minimize-api-surface",
        "title": "Minimize API surface"
      },
      {
        "slug": "opentofu-optimize-cicd-workflows",
        "title": "Optimize CI/CD workflows"
      },
      {
        "slug": "opentofu-prevent-backing-array-surprises",
        "title": "Prevent backing array surprises"
      },
      {
        "slug": "opentofu-proper-span-lifecycle",
        "title": "Proper span lifecycle"
      },
      {
        "slug": "opentofu-protect-infrastructure-secrets",
        "title": "Protect infrastructure secrets"
      },
      {
        "slug": "opentofu-review-consistency-assumptions",
        "title": "Review consistency assumptions"
      },
      {
        "slug": "opentofu-safe-lock-patterns",
        "title": "Safe lock patterns"
      },
      {
        "slug": "opentofu-separate-configuration-lifecycles",
        "title": "Separate configuration lifecycles"
      },
      {
        "slug": "opentofu-specify-configuration-behaviors",
        "title": "Specify configuration behaviors"
      }
    ],
    "comments": {
      "opentofu-protect-infrastructure-secrets": [
        "> That raises another question, we should explain how we shouldn't use this with state encryption (due to the ephemerality) because some people may think \"This is great for secrets, i can use it for my passphrase\" and have issues.\r\n\r\nI am not sure that I really get what you are suggesting here. Could you expand it more please?",
        "Please check this out: ae712fdaf7a5ba4b69cfcf0e65d960df7d301dac",
        "In 91f105241ac49c3c20ba51d52306be62c7ac4c03"
      ],
      "opentofu-clear-relationship-descriptions": [
        "Wanted to add the suggestions individually but the signoff of the commit is not working correctly from the GH UI, so I added all your suggestions in one commit: 4ee1ec72ed946b32d50637bb893c4c754d1b6e4e"
      ],
      "opentofu-safe-lock-patterns": [
        "Shouldn't `Sync` be under the the mutex lock too? This PR is changing that behaviour. Is this on purpose?",
        "`note`\r\nThis locking specific order (s3 first, dynamo second) is done this way keep the feature parity. From the tests ran locally, terraform is also doing both in case locking is enabled for s3+dynamodb and is doing those in that specific order.\r\nTerraform flow from the [localstack](https://www.localstack.cloud/) logs:\r\n```\r\nlocalstack.request.aws     : AWS iam.GetUser => 200\r\nlocalstack.request.aws     : AWS s3.ListObjectsV2 => 200\r\nlocalstack.request.aws     : AWS s3.PutObject => 200 <- acquire s3 lock\r\nlocalstack.request.aws     : AWS dynamodb.PutItem => 200 <- acquire dynamo lock\r\nlocalstack.request.aws     : AWS s3.HeadObject => 200\r\nlocalstack.request.aws     : AWS s3.GetObject => 206\r\nlocalstack.request.aws     : AWS dynamodb.GetItem => 200\r\nlocalstack.request.aws     : AWS s3.HeadBucket => 200\r\n...\r\nlocalstack.request.aws     : AWS s3.HeadBucket => 200\r\nlocalstack.request.aws     : AWS s3.GetBucketTagging => 404 (NoSuchTagSet)\r\nlocalstack.request.aws     : AWS s3.GetObject => 200\r\nlocalstack.request.aws     : AWS s3.DeleteObject => 204 <- release s3 lock\r\nlocalstack.request.aws     : AWS dynamodb.GetItem => 200\r\nlocalstack.request.aws     : AWS dynamodb.DeleteItem => 200 <- release dynamo lock\r\n```",
        "Good question! I was asking myself the same thing, but I see that `context.TODO` is used everywhere in this package.\r\nI am not aware yet, but maybe there are some plans on tackling this. Any ideas @cam72cam?"
      ],
      "opentofu-optimize-cicd-workflows": [
        "Configured this to run once a week, on Sunday morning. Also, weird minute section configuration due to this mention from [GH actions docs](https://docs.github.com/en/actions/writing-workflows/choosing-when-your-workflow-runs/events-that-trigger-workflows#schedule):\r\n> The `schedule` event can be delayed during periods of high loads of GitHub Actions workflow runs. High load times include the start of every hour. If the load is sufficiently high enough, some queued jobs may be dropped. To decrease the chance of delay, schedule your workflow to run at a different time of the hour.",
        "Perhaps we could change this later to fit our future needs.",
        "Changed in d7bd8375e8a0c9ccc46fc607eb2591595c0b865b"
      ],
      "opentofu-log-effectively-for-debugging": [
        "`suggestion`\r\nWe could at least log the error if occurs.",
        "Could we log it at least? ðŸ¤” ",
        "`suggestion`\r\nFor being able to adjust this parsing in the future, we could have a trace log here writing the `fullName`. This way, in case somebody is encountering \"unknown\" traces, we could ask for the logs and adjust this parsing accordingly.",
        "`suggestion`\r\nA trace log maybe? Maybe somebody really wants to debug this if something is not working for them.",
        "I am thinking that for somebody that is playing around with setting this up, could upload the artifacts with a wrong artifactType and then have a hard time figuring it out.",
        "```suggestion\r\n\t\t\tlogger.Printf(\"[WARN] failed to fetch provider package; retrying\")\r\n```",
        "I would say that this is more of a warning than an info ðŸ¤” Personally, I see INFO as an action that finished and the log needs to show the output of that. Finished in this context mean one of the following:\r\n* that it finished successfully\r\n* it finished with an error but there is a another possible success case on handling that error\r\n\r\nThe fact that an action failed and it's retried, is more of a failure, but not yet an interruptible one."
      ],
      "opentofu-review-consistency-assumptions": [
        "Really good point!\r\nFrom a quick check, I see that some already implemented it but there are still some that didn't yet.\r\nAdded a warning about it e22385a328c24a07904fa422aad15849b57ff047",
        "I used terms like \"digest\", \"md5\" or \"digest file\" for the part where OpenTofu is writing an md5 hash of the statefile.\r\nFor example, looking into the dynamoDB, there is the md5 sum of the state file. Can be easily checked like this:\r\n![image](https://github.com/user-attachments/assets/7dc94a57-407b-4531-b6db-02bc2e24d10b)\r\n\r\n\r\n[Here](https://github.com/opentofu/opentofu/blob/6614782/internal/backend/remote-state/s3/client.go#L352C24-L374) is where this is handled in OpenTofu.\r\n\r\nIn that `Digest updates` section, I am talking about this part.",
        "Updated this section due to not needed to handle this digest file anymore.\r\nThis should be handled only when the DynamoDB locking is enabled. That's due to having two sources of information when it comes to locking and it was used to ensure that the state object from the S3 bucket was not altered separately, without acquiring a lock.",
        "Added in 38f1eb921775d9ac783fffe8c8cb3d2b11a67419"
      ],
      "opentofu-contextualize-security-findings": [
        "`question`\r\nI see no label that we could assign to these issues. Any idea of one? Or should we create a new one?",
        "@cam72cam created a new label called `govulncheck` for these. Added in 4963f351b5e15206be46ce015fcb048605810576",
        "Would be possible, indeed. That's what I wanted to do initially.\r\nThough, I opted on including the workflow run url instead in the summary of the issue. ([Example of a vulnerability affecting v1.7](https://github.com/opentofu/opentofu/actions/runs/14333067260/job/40173357398#step:6:18627))\r\nThis way, whoever is working on fixing that vulnerability, will be able to inspect the findings of govulncheck.\r\n\r\nExtracting and writing entire stacktraces might be a little bit noisy on the issues, IMO, even though it will make the life easier for the one that works on solving it.\r\nAnother thing, is that the issues are reported per vulnerability and it reports all the versions that are affected, but different versions could have different stacktraces on how is calling the affected code.\r\n\r\nAnd yes, if we want to do this, would be advisable to do it in another language.",
        "Yes, that will work for sure because the `--search` argument in `gh issue list` is actually a query string, not only the title.\r\nSo whatever will be added after the title or in the description of it will not affect the search.\r\n\r\nThough, I am not sure that I understand why this would be better compared with adding a comment into the issue.\r\n\r\nBTW, had a conversation with @cam72cam and we will try to use security advisories instead of issues for this functionality. I will try to do it today and also test your idea that you suggested here.",
        "About the security advisories. Played around with the [API](https://docs.github.com/en/rest/security-advisories/repository-advisories) to see what it can do, but sadly it's quite restrictive and there is not really a  straight-forward way to test the changes. The API key that I can generate it can access only the published opentofu security advisories. Tried to test with one of my repos, but since I don't have any published/closed advisories, was returning only the draft one.\r\nEven further, the API to work with advisories is supporting filtering only on the `state` field which is not enough for our use case.\r\n\r\nTalked with @cam72cam and we are going with issues.",
        "Great arguments for the title topic! I totally agree with the issue to advisory flow.\r\nI would like to go forward with this approach and then reiterate later if we find out that the work with this flow is hard or not clear enough.\r\n\r\nThis is rising some ideas that could help with the flow for others not involved in the development of this:\r\n* I would add in the description of the ticket a note on how to work on the issue. We can have a short file documenting this and we can point to that in each ticket description, or we can have just a small note like:\r\n   > [!NOTE] \r\n   > * _Additional information can be added to the title as long as the original keywords are kept._\r\n   > * _Check also the pipeline run linked to get a better understanding of the source of the vulnerability for each version._\r\n* I would also lock the issue. What do you think about this? This kind of ticket should be handled by the core team IMO. Therefore, locking the issue will allow conversations only from the core team, avoiding pollution of the conversation and the investigation of it."
      ],
      "opentofu-craft-actionable-errors": [
        "```suggestion\r\n\t\toc := configOutputs[outputName]\r\n\t\tif prevStateOutput.Sensitive && !oc.Sensitive {\r\n\t\t\tdiags = diags.Append(&hcl.Diagnostic{\r\n\t\t\t\tSeverity: hcl.DiagWarning,\r\n\t\t\t\tSummary:  \"Output change in sensitivity\",\r\n\t\t\t\tDetail:   fmt.Sprintf(\"Sensitivity of the output %q changed. By doing so, the value will not be obfuscated anymore.\", oc.Name),\r\n\t\t\t\tSubject:  oc.DeclRange.Ptr(),\r\n\t\t\t})\r\n\t\t}\r\n```\r\n\r\nBy using a different way to initialise a diagnostic, we can provide that `Subject` field that will allow providing a better guidance to the user.\r\nThe change above will generate a warning similar to this one:\r\n![Screenshot 2025-06-02 at 15 15 32](https://github.com/user-attachments/assets/b30521cc-2517-406c-b76b-305ccee184a4)\r\nWith that indication of where the output that the warning is talking about can be located.",
        "Before\r\n![Screenshot 2025-03-17 at 11 20 04](https://github.com/user-attachments/assets/af02e9c1-e367-4df8-b169-fc264297c689)\r\nAfter\r\n![Screenshot 2025-03-17 at 11 22 52](https://github.com/user-attachments/assets/7ad66b64-607f-4c7d-b5f8-e8aca7b0efe3)\r\nI would suggest to enhance this to make it even better, like this:\r\n![Screenshot 2025-03-17 at 11 24 16](https://github.com/user-attachments/assets/d5fc77bc-1c4e-467d-b9bf-12da1e605924)\r\n```suggestion\r\n       \terrs = append([]error{fmt.Errorf(\"decryption failed for all provided methods\")}, errs...)\r\n\terrMessage := errors.Join(errs...).Error()\r\n```",
        "Indeed, that will not work as expected.\r\nYou could make use of a function like this:\r\n```golang\r\nfunc keyProvidersStack(stack []config.KeyProviderConfig) ([]string, hcl.Diagnostics) {\r\n\tres := make([]string, len(stack))\r\n\tvar diags hcl.Diagnostics\r\n\tfor i, cfg := range stack {\r\n\t\taddr, diag := cfg.Addr()\r\n\t\tdiags = diags.Extend(diag)\r\n\t\tif diag.HasErrors() {\r\n\t\t\tres[i] = \"<unknown>\"\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tres[i] = string(addr)\r\n\t}\r\n\treturn res, diags\r\n}\r\n```\r\n\r\nAnd then, before this `return` statement, you can do this:\r\n```golang\r\naddr, diags := keyprovider.NewAddr(cfg.Type, cfg.Name)\r\nstackAddrs, diag := keyProvidersStack(append(stack, cfg))\r\ndiags = diags.Extend(diag)\r\n```\r\nAnd this line will become\r\n```golang\r\nDetail: fmt.Sprintf(\"Cannot load %s due to circular reference between key providers. Stack trace %s\", addr, strings.Join(stackAddrs, \" -> \")),\r\n```\r\n\r\nEnding up in something similar to this. _Note: the message presented is not a real use case, that cannot happen._\r\n![Screenshot 2025-03-17 at 12 13 17](https://github.com/user-attachments/assets/acb90ebd-0a9c-4e76-945c-ddec0c43a479)\r\n",
        "We could have done this, but right now the key provider ([XOR](https://github.com/opentofu/opentofu/tree/main/internal/encryption/keyprovider/xor)) that could have been used to reproduce this is not included in the default allowed providers.\r\nI tested this with some slight temp modifications to the source code. ",
        "`suggestion`\r\n```suggestion\r\n\t\t\treturn desc, fmt.Errorf(\"unsupported OCI artifact type - empty\")\r\n```\r\n\r\nOr something more specific to let the user know about the actual cause? ðŸ¤” ",
        "Good arguments for this. Thanks.\r\nAnd yes, the updated message seems more suitable for the particular situation.",
        "`suggestion`\r\n```suggestion\r\n\t\treturn nil, fmt.Errorf(\"manifest content digest (%s) does not match resolved digest %s\", gotDigest, desc.Digest)\r\n```\r\nNot sure about this ðŸ¤” ",
        "Yes, that's more than a reasonable justification. Thanks. As I was saying, I was not sure about this suggestion.",
        "`suggestion`\r\n```suggestion\r\n\t\treturn selected, fmt.Errorf(\"ambiguous manifest has multiple descriptors for platform %s and version %s\", target, version)\r\n```"
      ],
      "opentofu-explicit-versus-dynamic-configurations": [
        "This is a first approach where I used the GH API to get all the protected branches (which are only the ones that are for the versions released of OpenTofu) and out of those exclude the the unmaintained versions.\r\nPros:\r\n* is using a reliable and well-known API that we can fully trust.\r\n\r\nCons:\r\n* manual maintenance. Quite ugly to update this.\r\n",
        "The update on the site is done automatically as seen [here](https://github.com/endoflife-date/release-data/tree/main?tab=readme-ov-file).\r\n![image](https://github.com/user-attachments/assets/01e8497d-1e49-4104-9ed8-21c39e9a4928)\r\n_That checkmark is for auto update_",
        "We are keeping only the hardcoded strategy to hold the control on what branches we run the flow against.\r\nSee [this](https://github.com/opentofu/opentofu/pull/2636#discussion_r2033011305) suggestion.",
        "This second approach is using https://endoflife.date/opentofu and is nice in terms that we don't really need to maintain this workflow anymore, once merged.\r\nPros:\r\n* no maintenance needed\r\n\r\nCons:\r\n* adds dependency on a 3rd party system",
        "We are keeping only the hardcoded strategy to hold the control on what branches we run the flow against.\r\nSee [this](https://github.com/opentofu/opentofu/pull/2636#discussion_r2033011305) suggestion.",
        "We added this section in the [CONTRIBUTING.RELEASE.md](https://github.com/opentofu/opentofu/pull/2636/files#diff-f3a80a44a166a40dd17304f62b1399e0a1477a0af3cea60dc41250f550b5ef07R248) for it.\r\nLet's see if this will be enough."
      ],
      "opentofu-minimize-api-surface": [
        "Thanks! Really good point! Really kind of you to provide some code that actually worked flawlessly.\r\nSo applied in 538dec6f1ad081f9b78bcbb614cf595869b21a6f.",
        "`question`\r\nThis was a linting error and I totally agree with it, to have the json fields explicitly named.\r\nWas there a reason till now not to have this with json tags?",
        "You identified that correctly in [one of your comments](https://github.com/opentofu/opentofu/pull/2521#discussion_r1961379081), it was excluded before.\r\nAnd now, since I am using that struct again in my new changes, I encountered the same issue. I could have used the nolint directive as well, but I would prefer to have the actual issue fixed.\r\nLet's see what others are saying.",
        "Thanks for the ideas and the exchange. Fixed both points in 26f76c25053af5028e5ffba68ca5d5e64a2d38bc in order to match the default behavior."
      ],
      "opentofu-document-reference-standards": [
        "Yeah, seen it. I had the same question when I added this. Let's wait for others.\r\nPersonally, I consider that issues should be linked, since there should be no PR without an associated issue, especially PRs that require a changelog.\r\n\r\nSaying that because in general, the issue is the place where decisions are taken in terms of design and general functionality. I know that from the PR we can navigate back to the issue, but IMO, the client facing information should be the issue (aka the functional information) and not the PR (aka the technical approach)."
      ],
      "opentofu-document-intent-and-limitations": [
        "`suggestion`\r\nCould we maybe include a url here? ðŸ¤” \r\nMaybe? https://opencontainers.org/posts/blog/2024-03-13-image-and-distribution-1-1/#manifest-maximum-size\r\nI am not sure which is the official one."
      ],
      "opentofu-prevent-backing-array-surprises": [
        "Interesting that this conditional is rewritten in a different manner than [this](https://github.com/opentofu/opentofu/pull/2798/files#diff-b7f5b7c669ea7bb97e98d899d204d9e6bf48a00d18cb90109447eca05997704dR235) was in the current PR.\r\nAny particular reason of this difference? I am curious about the reason around having 2 different ways of writing these conditionals."
      ],
      "opentofu-clear-concise-documentation": [
        "`question`\r\nAnd if setting `include = [\"registry.opentofu.org/mycorp/*\"]` then it means that the template should include interpolation only for the `type`?\r\nIf so, I will try to add just a small mention of it.\r\n",
        "Great points! Let's go with the current shape."
      ],
      "opentofu-specify-configuration-behaviors": [
        "I am not aware of this. Could you expand it a little bit please?",
        "Oh, this is what you meant! Ok. I misread your initial comment.\r\nUpdated in 2695d6c58dc739ceae192242733ddea987cdf450",
        "3fe9247caedeef0099138111dece7a18065cda2b",
        "Added information in 11a29cb050fd95ab55b5f6d5df01bba3c73af854.\r\nA variable will not become ephemeral strictly from referencing an ephemeral value. In order for it to be able to work with ephemeral values, it needs to be configured specifically.",
        "f603db886aa3d830534b5cc68f7e415e58e3d343",
        "Applied in 6c43f94d46a40ebf22b652cb08d42bd5ba7f4179"
      ],
      "opentofu-document-phased-migration-paths": [
        "Thanks. Added 7b88c6f84ddb90024fff88eec0fa7f8f3fa7ad6f",
        "Good point! Thanks for the input. Fixed in 12adf6200ff60dfe12597285173e08990cf5b3cb"
      ],
      "opentofu-separate-configuration-lifecycles": [
        "Thanks a lot for the input! Good catches with these 2 prompt parts.\r\nWhen it comes to the prompts, I would go with the first approach. The second option, where we could skip the deprecated variables, could block some users from using the configuration correctly, so I am not sure about it.\r\n\r\nAs for the `tofu show --json planfile`, yeah, totally agree.\r\n",
        "For the json output, added the changes in here: 55cfcce17bba824b8c642da1745c86c77b788ae7 .\r\nThe results will look like this. This is what you were suggesting, right?\r\n```json\r\n    \"root_module\": {\r\n      \"module_calls\": {\r\n        \"modcall\": {\r\n          \"source\": \"./mod1\",\r\n          \"expressions\": {\r\n            \"this_is_my_variable\": {\r\n              \"constant_value\": \"given value from modcall\"\r\n            }\r\n          },\r\n          \"module\": {\r\n            \"variables\": {\r\n              \"this_is_my_variable\": {\r\n                \"default\": \"default value\",\r\n                \"description\": \"This is a variable for the old way of configuring things.\",\r\n                \"deprecated\": \"This variable will be removed on 2024-12-31. Use another_variable instead.\"\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n```\r\n----------\r\nFor prompts, added the changes in here: dd0cfdc949026216495b090a984c7753fed33e86\r\nAnd the results will look like this. Is it right? Not fully confident with these changes ðŸ¤” \r\n![image](https://github.com/user-attachments/assets/11775498-f8ef-44f9-8d7a-057025af2be8)\r\n",
        "Thanks. Was not sure about it either.\r\nI applied that in a4c9d758df57f524f11d288127096a6353e8d5db.\r\nNow, it looks like this.\r\n<img width=\"1094\" alt=\"Screenshot 2025-03-07 at 18 17 38\" src=\"https://github.com/user-attachments/assets/2cf05f3f-03a0-4922-a9da-5b66bce2c0ac\" />\r\n"
      ],
      "opentofu-proper-span-lifecycle": [
        "Agree, but really error prone this part. Isn't there a way to extract the content of the `for` loop into a new function?\r\nIf somebody will ever come and add a new conditional `continue` or worse, a `break` or a `return`, that span will never be closed and will create a havoc in the traces.",
        "This call here should be removed. Check that `span.End()` is called a little bit down the road."
      ]
    }
  },
  "erikgrinaker": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-balance-flexibility-with-performance",
        "title": "Balance flexibility with performance"
      },
      {
        "slug": "neon-comprehensive-code-documentation",
        "title": "Comprehensive code documentation"
      },
      {
        "slug": "neon-connection-pooling-with-pipelining",
        "title": "Connection pooling with pipelining"
      },
      {
        "slug": "neon-database-replica-promotion-safeguards",
        "title": "Database replica promotion safeguards"
      },
      {
        "slug": "neon-design-domain-specific-error-types",
        "title": "Design domain-specific error types"
      },
      {
        "slug": "neon-design-metrics-for-insights",
        "title": "Design metrics for insights"
      },
      {
        "slug": "neon-document-concurrency-design-decisions",
        "title": "Document concurrency design decisions"
      },
      {
        "slug": "neon-flexible-documented-configurations",
        "title": "Flexible documented configurations"
      },
      {
        "slug": "neon-guard-against-race-conditions",
        "title": "Guard against race conditions"
      },
      {
        "slug": "neon-handle-network-interrupts-safely",
        "title": "Handle network interrupts safely"
      },
      {
        "slug": "neon-hierarchical-semantic-naming",
        "title": "Hierarchical semantic naming"
      },
      {
        "slug": "neon-minimize-unnecessary-allocations",
        "title": "Minimize unnecessary allocations"
      },
      {
        "slug": "neon-optimize-cargo-dependencies",
        "title": "Optimize cargo dependencies"
      },
      {
        "slug": "neon-proper-option-type-usage",
        "title": "Proper Option type usage"
      },
      {
        "slug": "neon-secure-authentication-handling",
        "title": "Secure authentication handling"
      }
    ],
    "comments": {
      "neon-design-metrics-for-insights": [
        "I don't know if we can reliably do this with the current `gc_info`.\r\n\r\n`GcCutoffs.time` (PITR cutoff) will be 0 when the start of the timeline is within the PITR window (e.g. for the first 7 days of the timeline). However, `GcCutoffs.space` will be initialized with a non-zero value during tenant activation, before the PITR cutoff is computed. This means that we can't disambiguate an uninitialized PITR cutoff from one that falls below the timeline creation. As a consequence, once a timeline is e.g. 7 days old, it's PITR history size will suddenly jump from 0 to some large number.\r\n\r\nTo disambiguate the uninitialized case from the new timeline case, we'll need to change `GcCutoffs.time` from an `Lsn` to an `Option<Lsn>`. Does that sound good?\r\n\r\nFurthermore, if we can't rely on the cache for PITR cutoff monotonicity, then any PITR window increase by the user will have a retroactive effect on billing (in the case where it actually takes effect for GC, which is currently only in the case of a tenant restart/migration). I just want to confirm that we're okay with that.",
        "> I'm still a bit unsure about under which circumstances we need pitr_cutoff reported externally, vs. using the history size metric from the other PR\r\n\r\nIt was needed to guarantee monotonicity, but as you point out it doesn't guarantee that anyway, so we'll have to accept that the PITR history size can retroactively change (sometimes) when the user changes their PITR window. I'll axe it.\r\n\r\nIf we did report it externally, then we could use it to enforce monotonicity in the billing system, which presumably _does_ have access to the previous value. But then we'd have to compute the PITR history size there.",
        "https://github.com/neondatabase/neon/pull/11984"
      ],
      "neon-optimize-cargo-dependencies": [
        "This should use the license and edition from the workspace:\n\n```\nlicense.workspace = true\nedition.workspace = true\n```",
        "Many of these aren't necessary and can be removed: `tokio` (not used), `async-stream` (not used), and `futures-core` (available via `futures`).\n\nnit: we also try (with mixed success) to keep these in alphabetical order.",
        "We should use the workspace edition: `edition.workspace = true`. We'll also need to use the workspace license: `license.workspace = true`.",
        "We should use the workspace version: `tokio-util.workspace = true`.\n\nBut I don't think we actually use `tokio-util` at all here, so we can just drop it.",
        "Shouldn't this be in `/Cargo.toml` and included via workspace?",
        "Ok, let's leave a TODO to fix it."
      ],
      "neon-database-replica-promotion-safeguards": [
        "I think we should, yeah -- it may not be strictly necessary depending on what we use them for, but it's also a bit of a footgun if we don't.\n\n@VladLazar Do we have a concrete need for these to justify the cost/complexity of re-establishing all streams when promoted?",
        "I changed this to a TODO to unblock the PR. We can add it later if we have a compelling need for it."
      ],
      "neon-minimize-unnecessary-allocations": [
        "Idk. It would be possible to avoid an allocation by using a smallvec to track the response index by shard or something, but I don't think we care that much about it. This is a rare path (batch crossing shard bounds), we're already doing a bunch of network and disk IO, and there's a ton of allocations inherent in every page we're processing here.\r\n\r\nGiven that this code is rarely triggered, and important for correctness, I'd much rather try to keep it relatively simple than to squeeze out an allocation that probably doesn't matter. But let's revisit if it turns up in profiles.",
        "nit: `.clone()` is unnecessary here.",
        "Let's drop these `From<&>` implementations. We should generally only convert owned types to avoid unnecessary heap allocations, and the caller can copy or clone if necessary."
      ],
      "neon-secure-authentication-handling": [
        "Oops, thanks -- just blindly copied this over from some prototype code. Fixed.",
        "I don't think it matters much since the JWT will fail to parse and validate in that case, but sure -- updated."
      ],
      "neon-document-concurrency-design-decisions": [
        "This should specify the locking order between `index` and `inner`, to avoid deadlocks.",
        "Yeah, I don't think it's possible either, but let's write it down for our future selves. Thanks!",
        "We're relying on `inner` being append-only for this to be safe, yeah? Let's specify that as part of the locking protocol.",
        "I'd just add something like this to the comment you added in https://github.com/neondatabase/neon/commit/9377e9af65921e06e25e4cec5607150faad56a1a:\r\n\r\n> Note that `inner` is append-only, so it is not necessary to hold simultaneous locks on `index`. In particular:\r\n>\r\n> * It is safe to read and release `index` before locking and reading from `inner`.\r\n> * It is safe to write and release `inner` before locking and updating `index`.\r\n>\r\n> This avoids holding `index` locks across IO, and is crucial for avoiding read tail latency.",
        "Is this properly synchronized with `freeze()`? It's possible that the callers avoid races here by synchronizing at a higher level, but this sequence is not prevented by the API (for threads A and B):\r\n\r\n1. A: `put_batch` takes out lock on `inner` and writes.\r\n2. B: `freeze` sets `end_lsn`.\r\n3. B: `write_to_disk` blocks on `inner`.\r\n4. A: `put_batch` releases `inner`.\r\n5. B: `write_to_disk` acquires lock on `inner` and `index`.\r\n6. A: `put_batch` blocks on `index`.\r\n7. B: `write_to_disk` writes an incomplete layer to disk from a stale index.\r\n\r\nWe can avoid this by taking out a lock on `inner` and `index` in `freeze`, and mandating that `end_lsn` must either be accessed under lock or specifying the access ordering of `end_lsn` wrt. to the locks.",
        "Thanks for the clarification and comment.",
        "I think `Ordering::Relaxed` is too lax here, given this is a lock. I think we'll need `Acquire` on the `load` and `AcqRel` on the CaS or something similar. Otherwise, it's possible that subsequent operations (e.g. `ftruncate`) are reordered wrt. the lock acquisition.\n\nNot critical, since we only ever expect one caller to modify this, but if we're going to have a lock we may as well properly lock it."
      ],
      "neon-proper-option-type-usage": [
        "This shouldn't be an `Option`, as it's never set to `None`."
      ],
      "neon-balance-flexibility-with-performance": [
        "Ack, let's keep it then. Doesn't cost us anything, and it should just work.",
        "Ack, I'm sympathetic to that. This API is modeled more after the Pageserver's current capabilities than the compute's needs (see e.g. [`get_vectored`](https://github.com/neondatabase/neon/blob/cbf442292b44decf7ab7fff77658d81c51b2c93f/pageserver/src/tenant/timeline.rs#L1182-L1191) and [`get_rel_page_at_lsn_batched`](https://github.com/neondatabase/neon/blob/c1ff7db1874c6b5ff8ee134b944f1c54616ba37b/pageserver/src/pgdatadir_mapping.rs#L244-L252)). If the compute is only ever going to request contiguous page chunks at the same LSN then we can probably lean into that for optimization.\r\n\r\nHowever, this assumes that the compute can't make forward progress until the entire batch has been received. I'm not sure that's always true.\r\n\r\nLet's consider an extreme case, for illustration: we send a batch request for pages 0-100, and at page 50 we have to download a layer which takes 3 seconds. I think there are cases where it's advantageous to send back pages 0-49 first, and pages 50-100 later:\r\n\r\n* A different backend may request a single page 20, and block on the batch request.\r\n* Similarly, if the batch was a prefetch, we could eagerly populate the LFC with pages 0-49 before anyone requests them.\r\n\r\nHowever, this depends on where readers would get blocked. If the initial batch request holds a Postgres lock for the pages, then we can't unblock the waiters until we serve the entire initial batch request.\r\n\r\nAs for the response, we can return at most 4 MB in a response, but that's 512 pages and likely fine as an upper bound. It would require a bit more memory to first buffer the entire batch and then send it, instead of just streaming them, so we could consider doing in-order streams instead. But maybe it's more efficient to just dump it into a large contiguous array too and just throw it over to Postgres ~directly.\r\n\r\ncc @VladLazar @problame since you probably have opinions here too.",
        "> the pageserver doesn't currently support streaming. The current implementation collects all the deltas and images for all keys in the batch and then does walredo for all keys concurrently. We could do true streaming, but it's not trivial.\r\n\r\nYeah, but Christian wanted to leave the protocol open to send eager out-of-order responses, since it gives us more flexibility in how we optimize reads down the road.\r\n\r\nThis also has implications for how we design the request scheduler and locking in the communicator. If we bake in an assumption that we'll always get complete batches back, it may not be easy to back out of that later if we find that it causes too much contention and tail latency.\r\n\r\nSo I would like us to take a stance now on whether eager partial responses is something we should design for.\r\n\r\n> On how scattered the reads should be: hard to tell. Generally speaking, the more clustered the pages in a request are, the more predictable performance will be\r\n\r\nI think the more important point here is that when we do the batching client-side, we _know_ whether it makes sense to batch or not (i.e. whether to prioritize throughput or latency) -- Postgres will tell us via `smgrreadv`. And if Postgres asks us to batch, it will always batch as contiguous chunks, at the same LSN.\r\n\r\nI think I'm leaning towards changing `GetPageRequest` to take a `block_count` parameter specifying the number of contiguous pages to read, but keep the streaming responses to keep the door open to eager partial responses. But it depends on how the request scheduler design ends up -- if we don't leverage it there, then we may as well just return a full batch.",
        "> I think I'm leaning towards changing `GetPageRequest` to take a `block_count` parameter specifying the number of contiguous pages to read, but keep the streaming responses to keep the door open to eager partial responses.\r\n\r\nI made this change: https://github.com/neondatabase/neon/pull/11815/commits/f94d64a13cd5a630d15c8b5db1f5b48a46a6d254\r\n\r\nBut whether to send individual page responses is an open question, added a TODO for it: https://github.com/neondatabase/neon/pull/11815/commits/0f3c0696b15f2024d18c313c4521bac0b1aa13bb",
        ">With the more flexible protocol, the communicator could coalesce batches, but perhaps we don't care about that (yet?).\r\n\r\nIt's not clear that it's better to coalesce them than to send them in parallel on separate streams. The smaller the batch is, the more latency-sensitive it is likely to be.",
        "Actually, these batches won't necessarily be contiguous -- they can get fragmented by pages present in the LFC. Let's do scattered page reads after all.\r\n\r\nThey will still be for same relation -- and in the vicinity of other pages (but don't encode this latter assumption).\r\n\r\nThere's still no point returning eager responses and populating the LFC, because backends take out buffer pool locks before they read from the LFC.",
        "https://github.com/neondatabase/neon/pull/11815/commits/ddf544d167c800b5f6d5abc2b04bf559a0b189c6"
      ],
      "neon-flexible-documented-configurations": [
        "Bummer that there's no reliable way to get this from `cargo`. We could parse the `cargo metadata` output, but hardcoding it seems find until `cargo` provides a canonical way to detect this.",
        "Yeah, let's just hardcode it for now."
      ],
      "neon-connection-pooling-with-pipelining": [
        "`caller_rx` is a stream of `(page_api::GetPageRequest, ResponseSender)`. We have to pick out the `ResponseSender` and keep it around by request ID so we know where to send each response.",
        "Ah, yes, exactly. For pipelining to be effective the request must be sent onwards to the TCP stack and server. If the request isn't passed on to TCP it doesn't matter if we block here or buffer them in the channel -- they're not getting sent across the network any faster regardless, and the caller is just going to sit around and wait for the response no matter what.\r\n\r\nI'll add a comment to that effect.",
        "Hm, actually, there might be a hazard here. If the gRPC stream send blocks, we won't be reading responses (that send is nested below the select) -- so with a large enough queue depth and with TCP/gRPC backpressure, we could end up in a similar deadlock as we've seen with the current libpq implementation (both client and server are blocked on sends).\r\n\r\nI don't think that's likely given that gRPC/TCP buffers are large and queue depths are small, but it's probably prudent to use the queue depth as a buffer size here -- it's cheap insurance.",
        "Even better, we can just use an unbounded channel. https://github.com/neondatabase/neon/pull/12475/commits/ba1d816bc79d2020c03953bb544d499fec11a4ac",
        "> If compression is used, does it return the compressed or uncompressed size?\r\n\r\nTurns out this was buggy! libpq would report the compressed size, gRPC the uncompressed size. Fixed.\r\n\r\n> Is the time returned just the time it took to establish the connection, or the full time it took to process and extract the basebackup?\r\n\r\nNeither, it's the timestamp when the connection to the server was established.\r\n\r\nUpdated the comments to address both concerns.",
        "Good call, thanks. `Path::exists()` would also discard errors.",
        "We'll need to figure out stream management here. It's kind of pointless to run Pagebench without stream reuse, so we may as well do that now. Two points:\r\n\r\n1. It should be possible to use this client both with a stream/connection pool (for the communicator) and without it (for tests and benchmarks). We can either pass in a single connection and stream (tying the client's lifetime to it and preventing concurrent use), or pass in a trait that can be implemented both by the pools and a trivial raw gRPC client. This will have implications for where stream/connection multiplexing should happen. What's your take on this?\r\n\r\n2. The API here should probably be a bidirectional stream that implements the `futures::Stream` trait: return an object that can be used both to send requests and receive responses. This is flexible enough to support both pipelining (i.e. queue depth > 1), multiple responses per request (e.g. eager page emission and informational responses about layer downloads), and out-of-order responses (if we ever want them), while keeping request tracking at a higher level. The `futures::stream` module has helpers that might come in handy for implementing this. We can also keep this `get_page()` as a convenience method that sends a single request without exposing a stream, for non-performance-critical code such as tests.\r\n\r\nIt may be premature to make these decisions before we've prototyped the higher layers (pools and trackers). If we think it is, then we can punt this and implement a Pagebench transport using the raw gRPC client. But if we're reasonably confident about the final shape of this client API then we may as well build and merge it now.",
        "> have this client take a trait that has a \"get connection\" and \"get stream\"\r\n\r\nYeah, that's probably the way to go.\r\n\r\n> pagebench has a basic implementation that either reuses a stream, or creates a new one for every request\r\n\r\nLet's generalize the basic implementation -- this will also be needed by e.g. `compute_ctl` when fetching base backups via gRPC, and in other tests.\r\n\r\n> So there would be a \"send_get_page\" that returns a receiving stream, and a \"recv_get_page\" that receives from that stream.\r\n\r\nI think we should just return both the send and receive streams from `get_pages()`, since their lifetimes are linked. This is similar to how most other stream handles are returned e.g. in Tokio."
      ],
      "neon-design-domain-specific-error-types": [
        "nit: Rust errors normally start with lowercase (since they'll often get prefixed by outer error messages), and should include the inner error, i.e. `anyhow!(\"failed to convert endpoint: {e}\")`."
      ],
      "neon-guard-against-race-conditions": [
        "We'll need to take a new clock reading here, otherwise we're populating it with a stale timestamp and shortening the next backoff delay.\r\n\r\nAlternatively, update `now` at the end of the backoff loop.",
        "We'll need to take a new clock reading here, otherwise we're populating it with a stale timestamp and shortening the next backoff delay.\r\n\r\nAlternatively, update `now` at the end of the backoff loop."
      ],
      "neon-handle-network-interrupts-safely": [
        "Above, we're setting `shard->last_reconnect_time = now`. So this will measure the time since we started the connection attempt, not the time since the previous reconnection attempt, effectively increasing the delay when connection failures are slow.\r\n\r\nIf we move that below this loop, it should do the right thing.",
        "Hm, I think that's actually a pre-existing bug, since it includes the previous backoff delay in the next iteration's delay, effectively reducing the delay by 50%. That also means that once it hits the 1 second max, the next attempt will have no backoff.",
        "Above, we're setting `shard->last_reconnect_time = now`. So this will measure the time since we started the connection attempt, not the time since the previous reconnection attempt, effectively increasing the delay when connection failures are slow.\r\n\r\nIf we move that below this loop, it should do the right thing.",
        "Hm, I think that's actually a pre-existing bug, since it includes the previous backoff delay in the next iteration's delay, effectively reducing the delay by 50%. That also means that once it hits the 1 second max, the next attempt will have no backoff."
      ],
      "neon-comprehensive-code-documentation": [
        "nit: both this and `AuthInterceptor` could use a short doc comment explaining what they're for and anything callers should be aware of.",
        "Doc comments must start with `///`, otherwise they won't get picked up by IDEs and docs."
      ],
      "neon-hierarchical-semantic-naming": [
        "Renamed it to `ReadLsn`.\r\n\r\nI'm not sure `not_modified_since_lsn` is actually meaningful/useful for any other requests than `GetPage`. All requests do pass it on to `wait_or_get_last_lsn()`, and they _could_ make use of it, but I'm not sure if they actually do?\r\n\r\n@hlinnaka Wdyt, can we move `not_modified_since_lsn` onto `GetPageRequest`?",
        "Discussed in https://github.com/neondatabase/neon/pull/11815#pullrequestreview-2818183483."
      ]
    }
  },
  "normanmaurer": {
    "repos": [
      "netty/netty"
    ],
    "entries": [
      {
        "slug": "netty-check-feature-compatibility",
        "title": "Check feature compatibility"
      },
      {
        "slug": "netty-configurable-over-hardcoded",
        "title": "Configurable over hardcoded"
      },
      {
        "slug": "netty-consistent-dependency-declarations",
        "title": "Consistent dependency declarations"
      },
      {
        "slug": "netty-document-public-apis",
        "title": "Document public APIs"
      },
      {
        "slug": "netty-memory-ordering-needs-barriers",
        "title": "Memory ordering needs barriers"
      },
      {
        "slug": "netty-optimize-allocation-hotspots",
        "title": "Optimize allocation hotspots"
      },
      {
        "slug": "netty-optimize-search-operations",
        "title": "Optimize search operations"
      },
      {
        "slug": "netty-preserve-backward-compatibility",
        "title": "Preserve backward compatibility"
      },
      {
        "slug": "netty-prevent-test-resource-leaks",
        "title": "Prevent test resource leaks"
      },
      {
        "slug": "netty-protect-network-buffer-lifecycle",
        "title": "Protect network buffer lifecycle"
      },
      {
        "slug": "netty-release-resources-consistently",
        "title": "Release resources consistently"
      },
      {
        "slug": "netty-semantic-over-generic-names",
        "title": "Semantic over generic names"
      },
      {
        "slug": "netty-simplify-control-flow",
        "title": "Simplify control flow"
      },
      {
        "slug": "netty-use-null-validation-utilities",
        "title": "Use null validation utilities"
      }
    ],
    "comments": {
      "netty-check-feature-compatibility": [
        "Need to fix this before we pull it in... This needs kernel 6.1",
        "Yes... Just working on it as we speak. I also need this for IORING_SETUP_SUBMIT_ALL.",
        "@dreamlike-ocean @franz1981 will use this https://github.com/netty/netty/pull/14677/files#diff-a66727a17a84c1aabaa7d4c343bb227ec6319f9e81ae2500846a9251d3a43794R389",
        "done",
        "5.18... hmm maybe you are right and we should be a bit more careful. ",
        "yeah... let me do some dance to detect if its supported or not and only if it is supported adding it. "
      ],
      "netty-protect-network-buffer-lifecycle": [
        "This is not really safe imho... The `evt` may have been released at this point already or modified in another way. You will need to convert it to a string before calling `ctx.fireUserEventTriggered(...)`.",
        "yes... the `Channel` that can be accessed via `ChannelHandlerContext` will never change. ",
        "Good point!",
        "Fixed @dreamlike-ocean PTAL again",
        "I basically put it here as based on this the select implementation might stop to hand out the id. That said I am currently trying to come up with a way to also get notified once there is space again as this might also effect the implementation ",
        "I think we only need to know when the ring is exhausted and also when the ring has something spare again. After thinking a bit more about this a user might want to stop reading when the ring has nothing left and so disable auto-read and then start again once there is something to use. ",
        "> When the buffer ring is exhausted, we only need to fill the io_uring_buf elements of the io_uring_buf_ring and then advance the tail to dynamically supplement the buffers.\r\n> \r\n> However, we need to consider how to handle the buffers that exceed the ring sizeï¼Œand the association between these buffer IDs and buffers. Perhaps using a short(buffer id) -> ByteBuf map can solve this problem?\r\n\r\nHmm not sure I get what you suggest here... Can you explain a bit more why there needs to be some special mapping ?\r\n> \r\n> I think if we want to implement this feature, we can call back the chooser when the buffering is exhausted, and the chooser will return whether buffer supplementation is required.\r\n\r\n",
        "I wonder if we also need to notify somehow once the ring has something to use again @dreamlike-ocean \r\n"
      ],
      "netty-optimize-search-operations": [
        "Maybe consider adding a comment that we +1 it as it is the \"insertion point\"",
        "nit the mask will never change so just store it as a final field in the class. ",
        "Also to ensure this all works we need to check that `entries` is a power of two in the constructor. "
      ],
      "netty-preserve-backward-compatibility": [
        "This is a breaking change... you will need to add an implementation here. \r\n\r\n```suggestion\r\n    public String readString(int length, Charset charset) {\r\n        String string = toString(readerIndex(), length, charset);\r\n        skipBytes(length);\r\n        return string;\r\n    }\r\n```"
      ],
      "netty-optimize-allocation-hotspots": [
        "hmm.. I think the enum is actually better and more clear then static ints. ",
        "Maybe I am missing something but couldn't we cache the events per Channel somehow and so reduce GC pressure ?",
        "Yes it is on a per `EventLoop` basis. So all the the `Channel`s that are using the same `EventLoop` will not run on a concurrent basis. That said it may be the easiest to just make it per `Channel` basis. If this is not good enough (and I assume it is) we could also use a `FastThreadLocal`.",
        "like I said I think re-using per Channel would be the best.",
        "yes the user will create one per `Channel` and add it to the `ChannelPipeline`.",
        "just store the event in the ChannelHandler as a field. ",
        "Add a private constructor and expose the instance as static one to reduce object allocations. This also means you will need to override `isSharable()` and return `true`.",
        "yes as long as has not state it can be sharable ",
        "Yep if we do per Channel we should create a new handler per channel ",
        "looks like the code is duplicated everywhere. Can we extract the whole `if` block in an extra method to remove code-duplication ?",
        "Generally speaking I think it is a good idea to not allocate if we can ... Just ensure the handler is not sharable anymore in this case as it has state. ",
        "```suggestion\r\n    private final AtomicReference<DomainSocketReadMode> mode = new AtomicReference<>(DomainSocketReadMode.BYTES);\r\n```\r\nOr you might consider using `AtomicReferenceFieldUpdater` to reduce memory overhead",
        "@doom369 I think this would be kind of a \"breaking change\". As we could only add a default method that return something invalid by default. ",
        "I agree I think this is not needed",
        "I like this a lot... We could also store pre-created `BufferRingExhaustedEvent`s in `bufferRing` and return the same instance for the same id. This would reduce GC a bit.",
        "We need to fix this... Like if the capacity requested is something we can no full-fill we need to return the borrowed buffer and allocate a new one from the allocator directly to back this one ",
        "as all the possible `bid`s are known we should pre-create the `Runnable`s and reuse them to reduce GC pressure.",
        "I am not sure we should do this. `sendfile(...)` does create a new pair of pipes as well every time. ",
        "```suggestion\r\n                    // As this Chunk does not belong to the mag anymore we need to decrease the used memory .\r\n                    mag.usedMemory.getAndAdd(-capacity());\r\n```"
      ],
      "netty-consistent-dependency-declarations": [
        "Let's just remove it I think ",
        "I think we also need to add it to `all/pom.xml`"
      ],
      "netty-release-resources-consistently": [
        "We also need to release the query and the content buffer, otherwise we will leak.\r\n\r\n```suggestion\r\n    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception {\r\n        DnsQuery query = (DnsQuery) msg\r\n        ByteBuf content = ctx.alloc().buffer();\r\n        try {\r\n            dohQueryEncoder.encode(ctx, query, content);\r\n\r\n            HttpRequest request = useHttpPost ? createPostRequest(content, uri) : createGetRequest(content, uri);\r\n\r\n            request.headers().set(HttpHeaderNames.HOST, dohServer.getHostName());\r\n            request.headers().set(HttpHeaderNames.ACCEPT, \"application/dns-message\");\r\n            request.headers().set(HttpHeaderNames.CONTENT_TYPE, \"application/dns-message\");\r\n\r\n            if (useHttpPost) {\r\n                request.headers().set(HttpHeaderNames.CONTENT_LENGTH, content.readableBytes());\r\n           }\r\n\r\n           ctx.write(request, promise);\r\n        } finally {\r\n            content.release();\r\n            query.release();\r\n        }\r\n    }\r\n```",
        "You will also need to do something with this `bufferPromise` in `handlerRemoved(...)`. Otherwise you may end up in a situation of this promise never been notified. ",
        "I think it would be better to fail it with an exception ",
        "This is not enough... You will need to release the message as well and fail the promise.\r\n\r\n```\r\nClosedChannelException exception = null;\r\nfor (;;) {\r\n    AbstractMap.SimpleEntry<Object, ChannelPromise> entry = queue.poll();\r\n    if (entry == null) {\r\n        break;\r\n    }\r\n    ReferenceCountedUtil.release(entry.getKey());\r\n    ChannelPromise promise = entry.getValue();\r\n    if (promise != null) {\r\n        if (exception == null) {\r\n            exception = new ClosedChannelException();\r\n        }\r\n        promise.tryFailure(exception);\r\n    }\r\n}\r\n```",
        "```suggestion\r\n            if (!IoUring.isRegisterBufferRingSupported()) {\r\n                // Close ringBuffer before throwing to ensure we release all memory on failure.\r\n                ringBuffer.close();\r\n                throw new UnsupportedOperationException(\"io_uring_register_buffer_ring is not supported\");\r\n            }\r\n            for (BufferRingConfig bufferRingConfig : bufferRingConfigs) {\r\n                try {\r\n                    registerBufferRing(bufferRingConfig);\r\n                } catch (Errors.NativeIoException e) {\r\n                    for (IoUringBufferRing bufferRing : registeredIoUringBufferRing.values()) {\r\n                        bufferRing.close();\r\n                    }\r\n                    // Close ringBuffer before throwing to ensure we release all memory on failure.\r\n                    ringBuffer.close();\r\n                    throw new UncheckedIOException(e);\r\n                }\r\n```",
        "```suggestion\r\n            throw new IllegalStateException(\"Couldn't find CompressionEncoderFactory: \" + targetContentEncoding);\r\n```"
      ],
      "netty-memory-ordering-needs-barriers": [
        "better update state before promises are notified to ensure we see the correct state in the futures.",
        "I think this can not happen",
        "As far as I can see there should be no issues",
        "I agree with @franz1981 here.  Let's make it volatile ",
        "Actually let's use an AtomicReference.",
        "I still think that AtomicReference is the safe bet. That said if we find out that it produce too much overhead we can re-evulate. I think for now we should just use i.t"
      ],
      "netty-use-null-validation-utilities": [
        "```suggestion\r\n    public DohRecordEncoder(InetSocketAddress dohServer, boolean useHttpPost, String uri) {\r\n        this.dohServer = ObjectUtil.checkNotNull(dohServer, \"dohServer\");\r\n        this.useHttpPost = useHttpPost;\r\n        this.uri = ObjectUtil.checkNotNull(uri, \"uri\");\r\n    }\r\n```"
      ],
      "netty-semantic-over-generic-names": [
        "please not use `Pair`... Just use `AbstractMap.SimpleEntry` or create your own class to hold this pair. ",
        "Use static HttpHeaderNames value "
      ],
      "netty-prevent-test-resource-leaks": [
        "@carryxyh can you also add a test to ensure buffered messages are released when the `EmbeddedChannel` is closed but these are not processed ?"
      ],
      "netty-configurable-over-hardcoded": [
        "I think we should deprecate the `public static final WebSocketClientCompressionHandler INSTANCE = new WebSocketClientCompressionHandler();` and then just add another constructor that takes the max allocation size as the constructor \r\n`",
        "This makes sense... Let's make them public. ",
        "I think it would be better to set the ringSize and cqSize with the same method as otherwise we might set this one first and then set the ring to a lower size which would not trigger the exception etc. "
      ],
      "netty-document-public-apis": [
        "Also add javadocs.",
        "As this is now public please add javadocs",
        "As this is now public please add javadocs",
        "Also as this class is public we should add javadocs etc. "
      ],
      "netty-simplify-control-flow": [
        "should this be an `unmodifiable` map ?",
        "nit: you can remove the else",
        "we usually only do `if` in this case if the other `if` returns. This way the indention is less ",
        "please use the same style we use everywhere else in netty:\r\n\r\n```\r\nfor (;;) {\r\n    Object msg = tempInboundMessages().poll()\r\n    if (msg == null) {\r\n        break;\r\n    }\r\n    pipeline.fireChannelRead(msg);\r\n}\r\n....\r\n```",
        "nit: you can remove the `else` as we return in the if block ",
        "nit: you could simplify this by using:\r\n\r\n```java\r\nif (!(msg instanceof Http2Frame) || validateFrameRead(ctx, (Http2Frame) msg) {\r\n    ctx.fireChannelRead(msg);\r\n}\r\n```"
      ]
    }
  },
  "Bo98": {
    "repos": [
      "Homebrew/brew"
    ],
    "entries": [
      {
        "slug": "brew-actions-workflow-best-practices",
        "title": "Actions workflow best practices"
      },
      {
        "slug": "brew-clear-error-recovery-paths",
        "title": "Clear error recovery paths"
      },
      {
        "slug": "brew-correct-github-actions-annotations",
        "title": "Correct GitHub Actions annotations"
      },
      {
        "slug": "brew-decouple-ci-from-code",
        "title": "Decouple CI from code"
      },
      {
        "slug": "brew-document-configuration-decisions",
        "title": "Document configuration decisions"
      },
      {
        "slug": "brew-environment-variable-safety",
        "title": "Environment variable safety"
      },
      {
        "slug": "brew-follow-established-naming-patterns",
        "title": "Follow established naming patterns"
      },
      {
        "slug": "brew-follow-support-tiers",
        "title": "Follow support tiers"
      },
      {
        "slug": "brew-minimize-unnecessary-operations",
        "title": "Minimize unnecessary operations"
      },
      {
        "slug": "brew-prefer-flags-over-conditionals",
        "title": "Prefer flags over conditionals"
      },
      {
        "slug": "brew-secure-api-url-parsing",
        "title": "Secure API URL parsing"
      },
      {
        "slug": "brew-standardize-api-integration-patterns",
        "title": "Standardize API integration patterns"
      },
      {
        "slug": "brew-structured-environment-configuration",
        "title": "Structured environment configuration"
      }
    ],
    "comments": {
      "brew-environment-variable-safety": [
        "Yeah I think the case being hit here is not this line, it's the `/usr/libexec/path_helper` line above which is new. This also eplxains why it only affects `macos-14` (`path_helper` isn't used on `macos-13`).\r\n\r\nThe reason the issue has only been seen in GitHub Actions is because of https://github.com/actions/runner-images/blob/d1db39831d5ff8fe7737d54e947bc54a1b125c31/images/macos/scripts/build/configure-shell.sh#L17 which would take the path at the time of the image build. We normally recommend `echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' > ~/.bashrc` instead, which most users use and is thus why we haven't had any other reports on this so far.",
        "For the specific line you modified: it was added for people who run with `bash -u`: https://github.com/Homebrew/brew/commit/59866d25ea39e4c5d25eb7cf78089d39752d88c3.\r\n\r\nFor the general behaviour of not using `$PATH` with `path_helper`: I agree it's not ideal. Just seems to be what Apple's `path_helper` tool outputs with no exceptions. The reason we're using `path_helper` is to support a new feature we added where people can add paths to `$(brew --prefix)/etc/paths`. I guess ideally we'd have something else that works on Linux too - we just prioritised the largest user share group for now."
      ],
      "brew-structured-environment-configuration": [
        "Might be safer to at least bring `#{Process.euid}` into the pathname or similar so we make sure the path is per-user.",
        "> Is `HOMEBREW_TEMP` not the per-user temp?\r\n\r\nHomebrew's default is global. We could potentially switch to being per-user HOMEBREW_TEMP, but it'll require custom logic for that to work on Linux (as it doesn't have a native system like macOS)."
      ],
      "brew-follow-support-tiers": [
        "```suggestion\r\n- devices using OpenCore Legacy Patcher with a Westmere or later Intel CPU\r\n```\r\n\r\nOlder than Westmere being tier 3, so could maybe add something there too.\r\n\r\nTechnically more complex with that (Westmere only matters if macOS >= 13), but macOS < 13 is tier 3 anyway so we can keep it simple here.\r\n\r\nMay become more complex again in the future if arm64 becomes relevant here.",
        "We don't really have CPU comparison functions as it's kinda tricky to do. Ideally we'd compare with `Hardware.oldest_cpu`.\r\n\r\nThough if we don't care about macOS <13, I guess a check for PCLMULQDQ (introduced with Westmere) in `sysctl machdep.cpu.features` might be a simplistic way to do so this.",
        "```suggestion\r\n- have a system `glibc` >= 2.35\r\n```"
      ],
      "brew-standardize-api-integration-patterns": [
        "`list` is a small subset of `info`. I use 4 fields from the JSON, 3 of which are not in `list`.\r\n\r\nCould maybe add them though we'd be near the point of making it not a subset anymore.\r\n\r\nI think maybe a better option is to allow `info` to take in multiple arguments: https://github.com/Homebrew/brew/pull/19565"
      ],
      "brew-document-configuration-decisions": [
        "What's this one about?\r\n\r\nI think I get why. It should just probably have a comment attached explaining.",
        "Or perhaps better: use `uuidgen` or similar. GitHub use UUID v4 in `@actions/core`.",
        "`homebrew/ubuntu18.04:master` does not exist - we only do tagged releases there so let's drop the `master` references.\r\n\r\n```suggestion\r\n            echo \"The homebrew/ubuntu18.04 image is deprecated and will soon be retired. Use homebrew/ubuntu22.04 or homebrew/ubuntu24.04 or homebrew/ubuntu20.04 or homebrew/brew.\" > .docker-deprecate\r\n```"
      ],
      "brew-actions-workflow-best-practices": [
        "```suggestion\r\n        working-directory: ${{ steps.set-up-homebrew.outputs.repository-path }}\r\n        run: |\r\n```",
        "We pin the other `codecov/codecov-action` so do we want to pin this one too?\r\n\r\n```suggestion\r\n      - uses: codecov/test-results-action@9739113ad922ea0a9abb4b2c0f8bf6a4aa8ef820 # v1.0.1\r\n```"
      ],
      "brew-secure-api-url-parsing": [
        "```suggestion\r\n      elif [[ \"${UPSTREAM_REPOSITORY_URL}\" =~ https://(([^:@/?#]+)(:([^@/?#]+))?@)github\\.com/(.*)$ ]]\r\n```\r\n\r\n- Group 1 `?` seemed unnecessary as that just reduces it down to the earlier `if` condition.\r\n- `/` and similar URL component separators should not be matched in the username password as if they appear then the `github.com` part moves from matching the host component to matching the path component of the URL, which we don't want. Probably could be stricter but this is  good enough\r\n- Escape `.` so we're only actually matching GitHub",
        "Indeed it seems we can remove that grouping"
      ],
      "brew-follow-established-naming-patterns": [
        "The old flag needs to remain as an alias for backwards compatibility"
      ],
      "brew-clear-error-recovery-paths": [
        "The only scenario that `Etc.getgrgid` returns nil is if you compile Ruby on a system without `getgrent` function.\r\n\r\nSo I guess Windows maybe? Though `chown` is no-op there so `Errno::EPERM` wouldn't even throw.",
        "Also note what I did here: https://github.com/Homebrew/brew/blob/d1e539cb8424d578e4b633f26c3cd31f9851c63e/Library/Homebrew/utils/github/api.rb#L139-L145.\r\n\r\nI guess the misconfigured NSS could apply here too?",
        "LGTM"
      ],
      "brew-prefer-flags-over-conditionals": [
        "It might be easier and cheaper to just add an `append_to_cccfg` here: https://github.com/Homebrew/brew/blob/main/Library/Homebrew/extend/os/mac/extend/ENV/super.rb#L161 gated to >= 10.13 && < 14, using a letter that isn't already used like `h` or `C` or something. Then here you can check `if [[ \"${HOMEBREW_CCCFG}\" = *h* ]]` before running most of the code here.\r\n\r\nAlso makes it more obvious we can remove it if we drop < macOS 14 support in the far future and means none of the hacks will run in macOS 14 and later which is safer in case there's edge cases we've missed."
      ],
      "brew-correct-github-actions-annotations": [
        "```suggestion\r\n        puts \"::warning::#{message_full}\" if ENV[\"GITHUB_ACTIONS\"]\r\n```",
        "```suggestion\r\n        puts \"::error::#{message_full}\" if ENV[\"GITHUB_ACTIONS\"]\r\n```",
        "```suggestion\r\n        puts \"::warning::#{message}\" if ENV[\"GITHUB_ACTIONS\"]\r\n```",
        "```suggestion\r\n        puts \"::error::#{message}\" if ENV[\"GITHUB_ACTIONS\"]\r\n```"
      ],
      "brew-minimize-unnecessary-operations": [
        "I'd move this before the dependency check (`return false if formula.class.pour_bottle_only_if != :clt_installed`). This should make it significantly faster.",
        "No, within the `Formula.all?` block. We don't need to construct the dependency tree for every formula if we're going to reject everything that doesn't match `formula.class.pour_bottle_only_if == :clt_installed` anyway.",
        "Added a suggestion diff though might be hard to read because of how GitHub formats non-trivial suggestions."
      ],
      "brew-decouple-ci-from-code": [
        "Doing it separately makes sense to me.",
        "Ideally we'd tag between doing this and dropping CI, but what we did last year that we could do here too is that we do all that at the same time (i.e. we merge this and immediately tag).\n\nThis is because this controls the build from source fallback on missing bottles, which should be done before or at the same time as dropping CI rather than after.\n\nBut yeah in terms of CI I agree with what you say.",
        "Oh if it's being merged today: we'd need to separate these controlling CI as we're probably not ready to start macOS 15 CI today, and definitely not on Intel. Carlo I think has an idea here.\r\n\r\nOtherwise, if we keep it linked to CI and decide to wait a couple days then that's also OK but will need tagging immediately after merge otherwise macOS 12 users will get \"no bottle available\" errors.\r\n\r\nI'd lean on separating CI control to avoid this confusion every year. We ideally shouldn't need to be tagging things fast nor blocking PRs like this.",
        "These variables currently not only control warnings etc but also directly control which macOS versions are used in homebrew-core CI. So if we drop macOS 12 support here then it immediately drops macOS 12 CI from Homebrew/core even when we haven't tagged yet. This is a problem because `brew install` on unbottled formulae will start failing on macOS 12 until they get the tag (and auto-update interval is now 24 hours).\r\n\r\nWe should probably separate them as while they happen typically around the same time they probably shouldn't necessarily be at the exact same minute. It should be: drop macOS 12 from brew, tag and then drop macOS 12 CI.\r\n\r\nThey used to be separate but we combined them last year and later noticed this quick-tag situation.\r\n\r\n---\r\n\r\nSimilarly, but less importantly, we can probably mark macOS 15 as supported slightly before we enable it for all Homebrew/core PRs - we can probably do so when the few SDK-tied formulae are bottled (so soon after we do pkg-config etc for Intel) given older bottles will work fine for the rest. But this doesn't really matter much.",
        "Unless someone else does, I'll try find some time tomorrow to do this. I agree this should be unblocked and merged before the end of the week for a Monday tag (or earlier if we want to allow 24 hour rollout).",
        "Thanks!",
        "We'll probably not merge this until there's good bottle coverage.\r\n\r\nBut we may need to make changes anyway as Intel will probably not be in sync with arm64 this year."
      ]
    }
  },
  "opencv-alalek": {
    "repos": [
      "opencv/opencv"
    ],
    "entries": [
      {
        "slug": "opencv-cleanup-before-errors",
        "title": "Cleanup before errors"
      },
      {
        "slug": "opencv-clear-api-contracts",
        "title": "Clear API contracts"
      },
      {
        "slug": "opencv-consistent-descriptive-naming",
        "title": "Consistent descriptive naming"
      },
      {
        "slug": "opencv-cross-platform-api-design-rules",
        "title": "Cross-platform API design rules"
      },
      {
        "slug": "opencv-document-configuration-version-requirements",
        "title": "Document configuration version requirements"
      },
      {
        "slug": "opencv-feature-flag-convention",
        "title": "Feature flag convention"
      },
      {
        "slug": "opencv-framework-synchronization-practices",
        "title": "Framework synchronization practices"
      },
      {
        "slug": "opencv-guard-optional-dependencies",
        "title": "Guard optional dependencies"
      },
      {
        "slug": "opencv-maintain-build-compatibility",
        "title": "Maintain build compatibility"
      },
      {
        "slug": "opencv-maintain-code-consistency",
        "title": "Maintain code consistency"
      },
      {
        "slug": "opencv-optimize-container-access",
        "title": "Optimize container access"
      },
      {
        "slug": "opencv-thread-safe-resource-cleanup",
        "title": "Thread-safe resource cleanup"
      },
      {
        "slug": "opencv-use-proper-assertions",
        "title": "Use proper assertions"
      }
    ],
    "comments": {
      "opencv-optimize-container-access": [
        "> inputMat ... inputMat\r\n\r\n\"In-place\" ops should not be used in general (causes extra `.clone()` call)."
      ],
      "opencv-document-configuration-version-requirements": [
        "Makes sense for \"Android NDK r27 and higher\" according to documentation.\r\n\r\nWhy do not enable that by default in the main build_sdk.py script?"
      ],
      "opencv-consistent-descriptive-naming": [
        "Just `writeTo()` ?"
      ],
      "opencv-feature-flag-convention": [
        "Perhaps `HAVE_EGL` and `HAVE_OPENGL` should be checked too.",
        "Not really.\r\n\r\nUser could specify `WITH_OPENGL=ON` but there could be no OpenCL dev packages on build system (or for build target platform in case of cross-compiling).\r\n\r\n`WITH_` / `OPENCV_ENABLE_` is a user intention (lets enable this if available).\r\nNext, `find_package()` should be executed.\r\nPreferable `try_compile()` should be run to validate dependency configuration.\r\nAfter checks above `HAVE_` variable is set to `ON` / `1`.\r\nFeature usage should be guarded by `HAVE_` checks.\r\n\r\nAll dependencies should be properly detected on configuration (cmake) stage.\r\nBuild stage (make) should not fail due to dependency misusing.",
        "`TARGET ocv.3rdparty.msmf` should be used instead of `HAVE_MSMF`.",
        "PRIVATE?\r\n\r\nThis macro is used in \"interface\" headers."
      ],
      "opencv-cleanup-before-errors": [
        "`CV_Error` doesn't call cleanup (release GIL and release `type`).\r\n\r\n```\r\nif (...)\r\n    has_error = true;\r\n\r\n... cleanup ...\r\n\r\nif (has_error)\r\n    CV_Error(...);\r\n```"
      ],
      "opencv-maintain-build-compatibility": [
        "We should not redistribute system or sysroot libraries even for static OpenCV builds. Only own 3rdparty built binaries are redistributed.\r\n\r\nWhich builder configuration does check of your modification?"
      ],
      "opencv-cross-platform-api-design-rules": [
        "We should not break HAL API/ABI.\r\nDon't modify exited function, create a new function instead.",
        "We should not pass C++ objects through low level plugin API. They are not usable in general.\r\n\r\nDifferent C++ ABIs are possible for OpenCV and plugin parts:\r\n- MSVS vs MinGW\r\n- GCC/Clang with different C++ standards\r\n- libc++ (GCC) / libstdc++ (Clang)"
      ],
      "opencv-thread-safe-resource-cleanup": [
        "Why do we need timeout?",
        "Why do we need additional \"list\" if there is already \"queue\"?\r\n\r\nAny link on \"reference implementation code\"?\r\n\r\n**UPD**: E.g,  https://stackoverflow.com/a/14450693 - there are no any extra lists."
      ],
      "opencv-use-proper-assertions": [
        "> // 0 for 4.x, 1 for 5.x\r\n\r\nDo we really to introduce support in OpenCV 4.x for functionality which has different behavior in OpenCV 5.x.",
        "Signature for `_EQ` is `ASSERT_EQ(expected_reference, actual_result);`.\r\nCorrect order of parameters are required to emit valid error messages.\r\n\r\nReference: https://github.com/opencv/opencv/blob/4.0.0/modules/ts/include/opencv2/ts/ts_gtest.h#L8196-L8200\r\n\r\n```\r\nGTEST_API_ AssertionResult EqFailure(const char* expected_expression,\r\n                                     const char* actual_expression,\r\n                                     const std::string& expected_value,\r\n                                     const std::string& actual_value,\r\n                                     bool ignoring_case);\r\n```",
        "Signature for `_EQ` is `ASSERT_EQ(expected_reference, actual_result);`.\r\nCorrect order of parameters are required to emit valid error messages.\r\n\r\nReference: https://github.com/opencv/opencv/blob/4.0.0/modules/ts/include/opencv2/ts/ts_gtest.h#L8196-L8200\r\n\r\n```\r\nGTEST_API_ AssertionResult EqFailure(const char* expected_expression,\r\n                                     const char* actual_expression,\r\n                                     const std::string& expected_value,\r\n                                     const std::string& actual_value,\r\n                                     bool ignoring_case);\r\n```"
      ],
      "opencv-maintain-code-consistency": [
        "In general it is hard to debug multi-line macros.\r\nC++ template-based approach is preferable."
      ],
      "opencv-framework-synchronization-practices": [
        "Lets try to apply `thread_local` for `UMat` variables only. No need to touch other tables (there is no problem with them).\r\n\r\nUse dedicated `initializedUMat` for that.",
        "Split initialization on 2 parts:\r\n- regular CPU buffers (like `int sdiv_table[256];`) - they don't need `thread-local` at all.\r\n- OpenCL related stuff which causes problems and needs thread-local variables (also put it under condition that this data is really needed - CPU-only processing doesn't need that at all).",
        "Proposed code locks mutex on each call. That should be properly avoided.",
        "This breaks bare metal build configurations without any threading support (see `OPENCV_DISABLE_THREAD_SUPPORT`)\r\n\r\nUse OpenCV wrappers instead.\r\n\r\nReuse `cv::getInitializationMutex()` instead of creation of new one.",
        "`std::lock_guard` => `cv::AutoLock`\r\n\r\nDid you measure performance changes due to added of `.copyTo()` calls?",
        "Please always reuse OpenCV performance tests or add them if something is missing.\r\n\r\nAvoid using of hand-written tests cases\r\n\r\n> suspicious results\r\n\r\nProvided measurement code has bug due to async execution of `cvtColor(UMat)` (code should schedule OpenCL kernels, but do not wait the final result).\r\n\r\nP.S. Mat to UMat coping is synchronous call in current API - forces wait of OpenCL execution queue."
      ],
      "opencv-guard-optional-dependencies": [
        "> ZLIB_FOUND\r\n\r\nWrong.\r\n\r\nWe should not mess with CMakes `find_package(Zlib)` from anywhere (e.g. libiff may find own ZLIB).\r\n`HAVE_ZLIB` must be used for OpenCV usage checks.",
        "> ANDROID_SUPPORT_FLEXIBLE_PAGE_SIZES\r\n\r\n`OPENCV_ANDROID_SUPPORT_FLEXIBLE_PAGE_SIZES` to avoid possible conflicts"
      ],
      "opencv-clear-api-contracts": [
        "No need to use `std::streambuf` anywhere in API.\r\nWe need to use internally just simplified class which provides seek/read **interface**.\r\n\r\nPublic API should have simple converter from this.",
        "Who performs `seek(0, 0)` between trials?\r\n\r\nPerhaps we should not allow `CAP_ANY` in this API."
      ]
    }
  },
  "zanieb": {
    "repos": [
      "astral-sh/uv"
    ],
    "entries": [
      {
        "slug": "uv-balance-test-performance-considerations",
        "title": "Balance test performance considerations"
      },
      {
        "slug": "uv-clear-precise-documentation",
        "title": "Clear precise documentation"
      },
      {
        "slug": "uv-consistent-authentication-patterns",
        "title": "Consistent authentication patterns"
      },
      {
        "slug": "uv-declarative-constraints-over-runtime",
        "title": "Declarative constraints over runtime"
      },
      {
        "slug": "uv-enforce-strong-optional-types",
        "title": "Enforce strong optional types"
      },
      {
        "slug": "uv-environment-variable-best-practices",
        "title": "Environment variable best practices"
      },
      {
        "slug": "uv-make-errors-user-actionable",
        "title": "Make errors user actionable"
      },
      {
        "slug": "uv-names-should-be-descriptive",
        "title": "Names should be descriptive"
      },
      {
        "slug": "uv-optimize-cache-sharing-strategies",
        "title": "Optimize cache sharing strategies"
      },
      {
        "slug": "uv-optimize-cicd-commands",
        "title": "Optimize CI/CD commands"
      },
      {
        "slug": "uv-secure-configuration-defaults",
        "title": "Secure configuration defaults"
      },
      {
        "slug": "uv-short-circuit-evaluation-strategies",
        "title": "Short-circuit evaluation strategies"
      },
      {
        "slug": "uv-structure-for-readability",
        "title": "Structure for readability"
      },
      {
        "slug": "uv-test-deployment-edge-cases",
        "title": "Test deployment edge cases"
      },
      {
        "slug": "uv-use-direct-documentation-style",
        "title": "Use direct documentation style"
      }
    ],
    "comments": {
      "uv-declarative-constraints-over-runtime": [
        "Why not just store `StatusCode` directly instead of the u16?",
        "Why does it need to implement `Serialize`?",
        "Stepping up a level, why is `Index` serializable? For writing it to the TOML file? If that's it, why _would_ we need this field to be serializable? A user can't construct it via the CLI. If truly necessary, why not use a wrapper type to implement serializability?",
        "It seems nice for type safety throughout and straightforward to implement? I don't feel super strongly, but the current approach of validating at the CLI level, but retaining the u16, then revalidating later seems quite weird."
      ],
      "uv-use-direct-documentation-style": [
        "We don't say \"simply\": https://github.com/astral-sh/uv/blob/main/STYLE.md#documentation",
        "Separately, do we want to recommend `uv sync`? The idea is you can just `uv run pytest` and start working. Maybe we want to recommend `uv sync` as a way to get things ready for an IDE?",
        "That makes sense. I think we might just want to reframe it a bit, I'll take a swing at the phrasing.",
        "Elsewhere, we stylize error codes as \"HTTP XXX\" instead of \"`XXX NAME`\". I do think the name is nice, I might do \"HTTP 403 Forbidden\" though? I'd probably omit the backticks, since you refer to the error codes in the subsequent text without them.\r\n\r\nYou can probably just say \"status code\" instead of \"response status code\".\r\n\r\nGenerally, I'd avoid using \"it\" to refer to uv if it's easy. So... \"uv will stop searching if a ... is encountered\" rather than \"uv will stop searching if it encounters ...\".\r\n\r\n",
        "We avoid referring to \"Users\" like this in the documentation. Instead, I'd say \"You can configure which error codes...\" or, more typically, \"To ignore additional error codes for an index, use the `...` setting.\""
      ],
      "uv-optimize-cache-sharing-strategies": [
        "Is the parallel part particularly important here? Isn't it helpful that the cache is shared regardless of concurrency?"
      ],
      "uv-secure-configuration-defaults": [
        "```suggestion\r\nTo use the uv build backend in an existing project, add it to the `[build-system]` section in your `pyproject.toml`:\r\n```"
      ],
      "uv-enforce-strong-optional-types": [
        "`PythonMinorVersionLink::from_installation` will already return `None` on alternative implementations, and we can use `bool::then` to avoid repeating the default case here.\r\n\r\nSee https://github.com/astral-sh/uv/pull/13980"
      ],
      "uv-short-circuit-evaluation-strategies": [
        "I would expect `@latest` to scan _all_ Python interpreters and select the latest version, whereas `@any` would stop on the first interpreter.\r\n",
        "`@any` isn't sorted by version during discovery though, because we short-circuit when we find an interpreter that meets the request. Sorting only makes a difference when selecting a Python download or iterating over managed Python installations."
      ],
      "uv-environment-variable-best-practices": [
        "Thanks Ed!",
        "Similar to https://github.com/astral-sh/uv/pull/12246/files#r1999551644\r\n\r\n```suggestion\r\n    /// Only use uv-managed Python distributions.\r\n```\r\n\r\n(Sort of on the fence about \"installations\" vs \"distributions\" here but don't want users to be confused about whether we are \"installing\" here)\r\n\r\n",
        "I guess \"versions\" is actually the most consistent with other language?",
        "I wonder if we should also mention that this is equivalent to the `python-preference = \"only-managed\"` setting? I'm not sure. We might want to reference `python-preference` though unless we intend to later add a `managed-python = true | false` option to the settings to replace `python-preference`?",
        "I also think a `UV_MANAGED_PYTHON=1|0` variable might makes sense for these new options. It seems easier to use than `UV_PYTHON_PREFERENCE`.",
        "> It would be nice to use the same language consistently.\r\n\r\nYeah, but there are some places where it might be clearer to refer to installations or distributions. I think we should probably use \"versions\" when it's not confusing. It'd be good to audit other usages too, I'm sure there are more cases where we could switch to \"versions\".\r\n\r\nI think I'd prefer updating the ones here then opening an issue to standardize the language.\r\n\r\n> I'm hesitant to reference python-preference because the user might then try to learn two different ways to do the same thing \r\n\r\nSince there's not a configuration file setting for `managed-python`, I think it makes sense to explain how it's related? I wouldn't suggest the `--python-preference` CLI option.\r\n\r\n> I considered an env var, but the problem is we have a third option as a default.\r\n\r\nThinking out loud here...\r\n\r\nI assumed `UV_MANAGED_PYTHON=1` means `--managed-python` and `UV_MANAGED_PYTHON=0` means `--no-managed-python` while unset means default.\r\n\r\nLooking at some of our existing patterns, we do use `UV_NO_...` sometimes. I think most of them don't have an equivalent \"yes\" flag though. I might be okay with `UV_MANAGED_PYTHON=1` means `--managed-python` and `UV_NO_MANAGED_PYTHON=1` means `--no-managed-python`. I think `UV_MANAGED_PYTHON=0` not working would be surprising. That's how other boolean flags work today though, e.g.:\r\n\r\n```\r\nâ¯ UV_SYSTEM_PYTHON=0 uv python find\r\n/opt/homebrew/opt/python@3.13/bin/python3.13\r\n```\r\n\r\nbut as you said, that's for an option that's truly binary.\r\n\r\nI think I've loosely talked myself into using `UV_NO_MANAGED_PYTHON` / `UV_MANAGED_PYTHON` :)\r\n\r\n\r\n ",
        "```suggestion\r\n    /// Disable GitHub-specific requests that allow uv to skip `git fetch` in some circumstances.\r\n```"
      ],
      "uv-structure-for-readability": [
        "A dedicated `StandaloneIntepreter` type might be helpful to keep the logic about the `executable` to use in the `uv-python` crate."
      ],
      "uv-consistent-authentication-patterns": [
        "Does this work with `UV_PUBLISH_TOKEN`? Do they just ignore the username?",
        "Ah I see this is mentioned above. Maybe we can elevate that? https://github.com/astral-sh/uv/pull/14253/files#r2175124341",
        "```suggestion\r\n!!! important\r\n\r\n    If you use `--token \"$JFROG_TOKEN\"` or `UV_PUBLISH_TOKEN` with JFrog, you will receive a\r\n    401 Unauthorized error as JFrog requires an empty username but uv passes `__token__` for as\r\n    the username when `--token` is used.\r\n```"
      ],
      "uv-test-deployment-edge-cases": [
        "I'm wary of adding a new test suite for this file, it's a big increase in scope for this task.",
        "(I'll see how painful it is though, especially with https://github.com/astral-sh/uv/pull/14184#discussion_r2175069753 this gets complicated to trust without tests)"
      ],
      "uv-optimize-cicd-commands": [
        "This is a good example of the breakage we'd cause with this change. I wonder if we can craft a GitHub search that captures if a second `uv venv` is common?",
        "We could grab the JSON output, I think â€” but the `readarray` reads clearer to me.",
        "uhh maybe we should attest the DockerHub ones, I'm not sure how if it works tbh.",
        "I think I'd prefer to do that afterwards",
        "I'm not sure we _want_ incremental builds in the release pipeline. A clean build seems like a good property, right?"
      ],
      "uv-names-should-be-descriptive": [
        "Nit: We use `s` as the name pretty consistently in this repository, it'd be nice to retain the style there.",
        "I think it's common for us to just write `from(...) -> Option<...>`, I don't think you need to encode the `maybe` in the name when the type system captures that.",
        "Agree that's a little confusing, but we do it pretty often and I'd prioritize consistency with the rest of the codebase.",
        "I'm loosely opposed to changing this, because we use this argument name all over the place. We do use `python_request` in some places? which you could use since you'll just shadow it later? I don't have strong feelings. It seems like this does improve readability here, but may be confusing if you're coming from elsewhere.",
        "Nit: Generally we don't shorten variable names, I'd just use `tool_request` or `tool_python_request`.",
        "Can you rename this variable to match, e.g., `with_requirements`?",
        "The name isn't great... open to suggestions.",
        "I like that",
        "Nit: Similar to https://github.com/astral-sh/uv/pull/12651/files#r2027441564, it seems fine to drop the `auth_` prefix here since these are the only indexes which exist in this context.",
        "(I think this applies broadly)"
      ],
      "uv-clear-precise-documentation": [
        "I'm not sure how we'll want to reframe the documentation here.\r\n\r\nMaybe we want something like..\r\n\r\n> Run with all packages listed in the given requirements files.\r\n>\r\n> The following formats are accepted:\r\n>\r\n> - `requirements.txt`-formatted files\r\n> - `pyproject.toml`\r\n> - `setup.cfg` and `setup.py`\r\n> - `.py` files with PEP 723 metadata\r\n\r\nI'm not sure if we want to enumerate those everywhere? Are they all supported everywhere?",
        "Looks outdated :) we can probably update those holistically separately if you prefer.",
        "During upgrade, this is the directory to look for installations in, right?",
        "Should we change the doc then?",
        "Perhaps \"The directory Python installations are stored in\"?",
        "I think this is kind of confusing as written\r\n\r\n```suggestion\r\n    /// During an upgrade, uv will not uninstall outdated patch versions.\r\n```",
        "This is my first instinct\r\n\r\n```\r\n/// The environment was checked and required no updates.\r\n/// The environment was updated.\r\n/// A new environment was created.\r\n```",
        "I think I agree these should all be past-tense? Dry-run fills in the blank \"The environment would be ___\"",
        "We might want to say \"additional\" here (as discussed elsewhere). If you want to update that language all at once, it can happen here or in a separate pull requests.",
        "I think it's also fine to just say \"requirements\" without the floating (s)",
        "```suggestion\r\n    /// Whether to display the additional requirements installed with each tool.\r\n```\r\n\r\n(will require generated reference update)",
        "I would re-phrase (I think we use \"Whether\" when we only show one of the options)\r\n\r\n```suggestion\r\n    /// Disable use of uv-managed Python distributions.\r\n```\r\nI think we could say a bit more here, too, like\r\n\r\n> Instead, uv will search for a suitable Python installation on the system.\r\n\r\n\r\n",
        "See https://github.com/astral-sh/uv/pull/12246/files#r1999556537 â€” maybe we want to say \"Python versions\".",
        "```suggestion\r\n    /// Only show the version\r\n    ///\r\n    /// By default, uv will show the project name before the version.\r\n```"
      ],
      "uv-balance-test-performance-considerations": [
        "Given this test won't really change a lot, should we just check if commit info is available and have two snapshots? Using filters this way makes me a bit uncomfortable.",
        "They're just annoying during snapshot updates, so I try to avoid them in most cases.",
        "but if the filter is replacing basically the entire expected output, I'd either just skip the test or have a conditional.",
        "Yes, but we should squat or advocate for those names to be banned.",
        "It's just as bad, really. We try to make them reproducible wherever possible. That's mostly w.r.t. the external world though, internal changes causing snapshot changes are fine.\r\n\r\nHowever, I think this specific case is fine.",
        "You can use `.assert().success();` instead to avoid an empty snapshot.\r\n\r\nWe should test `python find --project` _before_ creating the virtual environment. In that case, we should ensure we're respecting the `requires-python` range from the child `pyproject.toml`.\r\n\r\nThen, we should test with a virtual environment.\r\n\r\nThen, we should also test we respect the `--project` root for `.python-version` file discovery too."
      ],
      "uv-make-errors-user-actionable": [
        "Huh? Why is there a reference to a workspace at all here? This doesn't seem quite right, and seems bad enough that we need to improve it here because we're recommending installing a workspace member?",
        "Yeah it seems like we may want to drop that and open an issue to follow up on it?\r\n\r\n> In general even a single package is a workspace\r\n\r\nEven if this is true internally, we shouldn't leak that to the user. Most users don't need or use workspaces.",
        "Discussion on inclusion of the wheel filename is happening at https://github.com/astral-sh/uv/pull/13437#discussion_r2132744173",
        "ðŸ‘ I'm happy to hand this off to you, unless you want me to write the version display.",
        "Why did this one change? There's no policy set here.",
        "I don't think we can throw an error there though, doesn't downstream logic rely on a successful request? (i.e., in https://github.com/astral-sh/uv/pull/12667#discussion_r2029115491)\r\n\r\nPerhaps I misunderstood the intent of this PR?",
        "Ah I understand the goal now, you want to make the case where no credentials were present distinct from the case where the credentials are wrong. Sorry it took me a while to get there. I think the snapshot changes should be like..\r\n\r\n> hint: An index URL (https://pypi-proxy.fly.dev/basic-auth/simple) could not be queried due to a lack of valid authentication credentials (401 Unauthorized).\r\n\r\nto \r\n\r\n> hint: An index URL (https://pypi-proxy.fly.dev/basic-auth/simple) could not be queried due to missing credentials (401 Unauthorized)\r\n\r\nand\r\n\r\n> hint: An index URL (https://pypi-proxy.fly.dev/basic-auth/simple) could not be queried due to invalid credentials (401 Unauthorized)\r\n\r\nI think changing that error path entirely feels too risky and is hopefully unnecessary. \r\n",
        "(fwiw, I think this would have been clearer if the title was something like \"Distinguish between authentication failures due to missing vs invalid credentials\")",
        "Should this be \"Failed to determine the libc used on the current platform\"? This message seems too generic for a libc-detection specific problem.",
        "As structured, it needs to be present here or it's too broad / inaccurate. I also don't expect devs to know about libc flavors, but there should be an error above this explaining why we're looking for it. I'm also fine with it being structured differently.",
        "I think this is too specific? This can be raised for any managed Python error.\r\n\r\n```suggestion\r\n    #[error(\"Failed to discover managed Python installations\")]\r\n```\r\n\r\nIt seems misleading to suggest it's specifically related to \"matching the current platform\"."
      ]
    }
  },
  "jmorganca": {
    "repos": [
      "ollama/ollama"
    ],
    "entries": [
      {
        "slug": "ollama-ai-dependency-decoupling-strategy",
        "title": "AI dependency decoupling strategy"
      },
      {
        "slug": "ollama-clear-recoverable-error-messages",
        "title": "Clear recoverable error messages"
      },
      {
        "slug": "ollama-complete-null-checks",
        "title": "Complete null checks"
      },
      {
        "slug": "ollama-comprehensive-test-coverage",
        "title": "Comprehensive test coverage"
      },
      {
        "slug": "ollama-descriptive-balanced-naming",
        "title": "Descriptive balanced naming"
      },
      {
        "slug": "ollama-extract-duplicated-code",
        "title": "Extract duplicated code"
      },
      {
        "slug": "ollama-follow-godoc-conventions",
        "title": "Follow GoDoc conventions"
      },
      {
        "slug": "ollama-guard-against-nil",
        "title": "Guard against nil"
      },
      {
        "slug": "ollama-loose-api-coupling",
        "title": "Loose API coupling"
      },
      {
        "slug": "ollama-optimize-ai-implementation-patterns",
        "title": "Optimize AI implementation patterns"
      },
      {
        "slug": "ollama-optimize-memory-allocations",
        "title": "Optimize memory allocations"
      },
      {
        "slug": "ollama-optimize-with-standard-library",
        "title": "Optimize with standard library"
      },
      {
        "slug": "ollama-platform-aware-configuration-documentation",
        "title": "Platform-aware configuration documentation"
      },
      {
        "slug": "ollama-purpose-reflecting-file-names",
        "title": "Purpose-reflecting file names"
      },
      {
        "slug": "ollama-reuse-buffers-strategically",
        "title": "Reuse buffers strategically"
      },
      {
        "slug": "ollama-use-idiomatic-go-flow",
        "title": "Use idiomatic Go flow"
      }
    ],
    "comments": {
      "ollama-reuse-buffers-strategically": [
        "this allocation can be one time?\r\n\r\n```\r\ntype weighted struct {\r\n\trng        *rand.Rand\r\n\ttransforms []transform\r\n\tbuf        []tokenInfo // reuse buffer\r\n}\r\n\r\nfunc (s *weighted) Sample(logits []float32) (int32, error) {\r\n\tif cap(s.buf) < len(logits) {\r\n\t\ts.buf = make([]tokenInfo, len(logits))\r\n\t}\r\n\ttokens := s.buf[:len(logits)]\r\n\t// rest of your logic\r\n}\r\n```"
      ],
      "ollama-complete-null-checks": [
        "@ParthSareen yes this should be in the `if (g != nullptr) {` block",
        "I would return a regular `std::string` here to avoid issues if the string gets deallocated or changed later by the caller\r\n\r\n```suggestion\r\nconst std::string ollama_vocab::token_to_piece(uint32_t token) {\r\n```"
      ],
      "ollama-descriptive-balanced-naming": [
        "Some of the variable and function names here are very long. Ideally 1 nameComponent, 2 if needed for clarity, but 3 is often a sign that better naming or structure can be used.\r\n\r\n```\r\nprefix, ok := toolPrefix(model.Template.Template)\r\nif !ok {\r\n  return nil\r\n}\r\n```",
        "```suggestion\r\n\tUseAuth = Bool(\"OLLAMA_AUTH\")\r\n```\r\n\r\nnit on potentially a more consistent name since we don't have `_USE_` anywhere else"
      ],
      "ollama-follow-godoc-conventions": [
        "I _think_ generally the doc for functions should start with the function name, e.g.\r\n\r\n```suggestion\r\n\t// SetLayer sets the active layer of the cache\r\n```",
        "More: https://go.dev/doc/comment",
        "A GoDoc comment here would be great! Since there are some preconditions to running this.",
        "What is `name` and `arguments` here? Are those the formats? I would add them to the GoDoc for this function. I'm not sure what their purposes is from reading the comment.\r\n",
        "Add comment as to why it's passed in by pointer here. Something like the length of the slice is modified to k",
        "pedantic but should we make these GoDoc comments above the fields?\r\n\r\n```\r\n//MainGPU is the gpu where xyz...\r\nMainGPU int\r\n```\r\n"
      ],
      "ollama-optimize-ai-implementation-patterns": [
        "There may be multiple eog tokens, specifically EOS or EOT"
      ],
      "ollama-purpose-reflecting-file-names": [
        "Yes, if this stays as a root level `Makefile` it should also build the project, ideally with GPU support"
      ],
      "ollama-optimize-memory-allocations": [
        "Thanks for the PR!\r\n\r\nWould it be possible to keep the argument based return value?"
      ],
      "ollama-loose-api-coupling": [
        "This is over coupling sampling with the `api.Options` type. Looser coupling is usually better. Much better that it accepts the raw float32/int values instead of passing `req` even deeper",
        "Instead of a list of transforms (`[]Transform`), why don't we just make helper functions like `topP` `topK` `softmax` and call them in the fixed order we use here (given the API is fixed)?"
      ],
      "ollama-guard-against-nil": [
        "Should we `ok` check this?",
        "As-is is ok if it can't happen",
        "Let's avoid the case where this is nil\r\n\r\n```\r\nvar vocab *model.Vocabulary\r\nif tp, ok := s.model.(model.TextProcessor); ok {\r\n  vocab = tp.Vocab()\r\n}\r\n\r\nif req.Grammar != \"\" && vocab != nil {\r\n\r\n}\r\n```",
        "Good catch. I'll remove the pointer.",
        "I wonder if there's a way we could get around the type assertion here via some other interface/struct design (maybe in a later PR as this all takes shape). Similar idea with `model.TextProcessor`."
      ],
      "ollama-use-idiomatic-go-flow": [
        "Nit: we can short circuit return above instead of the else",
        "nit/optional, but we could return early here\r\n\r\n```\r\nif c.windowSize == math.MaxInt32 {\r\n    return\r\n}\r\n\r\n...\r\n```\r\n"
      ],
      "ollama-platform-aware-configuration-documentation": [
        "It may be simpler to keep this `Windows` but to mention in a warning below acceleration libraries aren't currently supported for ARM (vs two sections)",
        "The top of the doc (perhaps a bit too easy to glance over) has a link to TDM-gcc which works pretty well.",
        "(and llvm-mingw for ARM)"
      ],
      "ollama-clear-recoverable-error-messages": [
        "```suggestion\r\nvar ErrNoVisionModel = errors.New(\"this model is missing data required for image input\")\r\n```",
        "Actually, this message is more of edge case vs a capability, it's great as is",
        "We may want to hint at missing data or similar. E.g. `this model is missing data required for image input`"
      ],
      "ollama-ai-dependency-decoupling-strategy": [
        "Yeah! Turned out to be little code, however the tradeoff is we have to keep using `llama` code for now. It's very important we work to remove this to use new engine tokenizers instead as this restricts us to loading GGUF files with specific KV keys. cc @ParthSareen "
      ],
      "ollama-comprehensive-test-coverage": [
        "This is a great start. I would take out the concept of tool tokens and have cases like:\r\n\r\n* valid prefix, invalid json\r\n* invalid prefix, valid json\r\n* valid prefix, valid json\r\n* 2+ objects\r\n\r\netc",
        "Great start on the tests! It will be quite a bit of tests, but we should also test the intermediate tool parsing steps (e.g. finding the template, finding the prefix/tool tokens) since we're bound to find edge cases in new models (vs these more top-down tests)",
        "Add tests for `0`",
        "No, we want to test this \"bottom-up\", testing 0 (or maybe a very very small number) here will ensure we have code coverage for `max(temp, 1e-7)`, which is different than testing to see if greedy is used (sample function)"
      ],
      "ollama-extract-duplicated-code": [
        "is `skipStartOfTurn` needed?"
      ],
      "ollama-optimize-with-standard-library": [
        "Given how rare this is, why don't we just use built-in sort for now. It will simplify the code a lot:\r\n\r\n```\r\n\tif k >= len(ts) {\r\n\t\tslices.SortFunc(ts, func(a, b token) int {\r\n\t\t\tswitch {\r\n\t\t\tcase a < b:\r\n\t\t\t\treturn -1\r\n\t\t\tcase a > b:\r\n\t\t\t\treturn 1\r\n\t\t\tdefault:\r\n\t\t\t\treturn 0\r\n\t\t\t}\r\n\t\t})\r\n\t\treturn ts\r\n\t}\r\n```",
        "This should be able to replace `partialSortLogits` and `sortLogits` below",
        "I know we passed on it previously, but it's worth seeing if we can use `container/heap`  ?\r\n\r\n````\r\nimport \"container/heap\"\r\n\r\n// tokenHeap implements heap.Interface\r\ntype tokenHeap []tokenInfo\r\n\r\nfunc (h tokenHeap) Len() int           { return len(h) }\r\nfunc (h tokenHeap) Less(i, j int) bool { return h[i].logit < h[j].logit } // min-heap based on logit\r\nfunc (h tokenHeap) Swap(i, j int)      { h[i], h[j] = h[j], h[i] }\r\n\r\nfunc (h *tokenHeap) Push(x any) {\r\n\t*h = append(*h, x.(tokenInfo))\r\n}\r\n\r\nfunc (h *tokenHeap) Pop() any {\r\n\told := *h\r\n\tn := len(old)\r\n\titem := (*h)[n-1]\r\n\t*h = (*h)[:n-1]\r\n\treturn item\r\n}\r\n\r\n### Explanation:\r\n- `Pop` should remove and return the last element of the slice after shrinking it.\r\n- This properly maintains the heap's invariants defined by the other methods (`Len`, `Less`, `Swap`).\r\n\r\n### Usage example:\r\n\r\n```go\r\nh := &tokenHeap{}\r\nheap.Init(h)\r\n\r\n// Push elements onto the heap\r\nheap.Push(h, tokenInfo{id: 1, logit: 0.9})\r\nheap.Push(h, tokenInfo{id: 2, logit: 1.2})\r\n\r\n// Pop smallest element\r\nsmallest := heap.Pop(h).(tokenInfo)\r\n```\r\n````",
        "Let's use https://pkg.go.dev/slices#BinarySearchFunc vs re-implementing binary search",
        "I believe you can use `slices.Indexfunc` instead of this dependency, something like:\r\n\r\n```\r\nimport \"slices\"\r\n\r\nfunc maxIdx(vals []float64) int {\r\n    if len(vals) == 0 {\r\n        return -1\r\n    }\r\n    maxVal := slices.Max(vals)\r\n    return slices.IndexFunc(vals, func(v float64) bool {\r\n        return v == maxVal\r\n    })\r\n}\r\n```"
      ]
    }
  },
  "ulyssessouza": {
    "repos": [
      "docker/compose"
    ],
    "entries": [
      {
        "slug": "compose-avoid-ci-resource-conflicts",
        "title": "avoid CI resource conflicts"
      },
      {
        "slug": "compose-avoid-confusing-names",
        "title": "Avoid confusing names"
      },
      {
        "slug": "compose-avoid-hardcoded-configuration-values",
        "title": "Avoid hardcoded configuration values"
      },
      {
        "slug": "compose-avoid-mutable-defaults",
        "title": "avoid mutable defaults"
      },
      {
        "slug": "compose-avoid-variable-name-conflicts",
        "title": "Avoid variable name conflicts"
      },
      {
        "slug": "compose-consistent-formatting-choices",
        "title": "consistent formatting choices"
      },
      {
        "slug": "compose-environment-variable-safety",
        "title": "Environment variable safety"
      },
      {
        "slug": "compose-environment-variable-validation",
        "title": "Environment variable validation"
      },
      {
        "slug": "compose-follow-existing-naming-patterns",
        "title": "Follow existing naming patterns"
      },
      {
        "slug": "compose-optimize-docker-layer-caching",
        "title": "optimize Docker layer caching"
      },
      {
        "slug": "compose-pin-git-dependencies",
        "title": "Pin git dependencies"
      },
      {
        "slug": "compose-precise-security-pattern-matching",
        "title": "precise security pattern matching"
      },
      {
        "slug": "compose-prefer-explicit-readability",
        "title": "prefer explicit readability"
      },
      {
        "slug": "compose-safe-collection-modification",
        "title": "Safe collection modification"
      },
      {
        "slug": "compose-use-standard-api-fields",
        "title": "Use standard API fields"
      }
    ],
    "comments": {
      "compose-avoid-hardcoded-configuration-values": [
        "You should be able to pass the `-e HOME` through the variable `COMPOSE_OPTIONS` (as described in line 11). If we remove this volume mount here, that can break peoples code that expects this mount.",
        "Ok fair enough!"
      ],
      "compose-avoid-ci-resource-conflicts": [
        "Done"
      ],
      "compose-follow-existing-naming-patterns": [
        "Sure! I felt the same while reading. Didn't change cuz it wasn't part of the PR.\r\nRenaming in 1, 2, 3..."
      ],
      "compose-pin-git-dependencies": [
        "This should be `docker==4.2.1` just after the release"
      ],
      "compose-consistent-formatting-choices": [
        "Thanks for that one!",
        "IMO the use of {} places delimiters on when the variable name ends.\r\nAs you just said, there are cases where it's necessary, so to make the code style consistent, I prefer using it. Even my IDE asks for it. And by the end of the day that's a question of taste.",
        "Here I just use the double quotes because the line before was using it.",
        "Done"
      ],
      "compose-environment-variable-validation": [
        "Yep! Adding the comment.",
        "My second option was to run the tests command line 2 times. One for each configuration of the envvar.\r\nI'm not sure if I understood what you mean by using a global variable for this.",
        "Ahhh... Finally got what you mean. The idea was to have it also configurable from outside of this `TestMain`.\r\n\r\nWith the envvar we can run just one specific test. And when running like this, `TestMain` doesn't run. So the envvar is a solution to be able to still pass this info to the test.",
        "The only alternative I see is to pass a build tag when running the test to choose in between tagged files. So we can choose which file to use, so we can use that conditionally depending on the tag.... But that really looks like over engineering and gets to the same problematic in the end..."
      ],
      "compose-environment-variable-safety": [
        "In this case the environment takes the precedence that should be the opposite.\r\nWhat if environment variable is set to `COMPOSE_COMPATIBILITY=no` and command line uses `--compatibility`?\r\n\r\nCould you please, add a test using both?",
        "Yep! The command line should have precedence over the environment variable"
      ],
      "compose-optimize-docker-layer-caching": [
        "Done",
        "Done"
      ],
      "compose-precise-security-pattern-matching": [
        "The absence of `[ ]` breaks the formatting compatibility with the others"
      ],
      "compose-prefer-explicit-readability": [
        "Not sure if I understood this line, but the following equivalent would be more readable.\r\n\r\n```suggestion\r\n        msg = 'Pulling' if not silent else None\r\n```",
        "IMHO this list comprehension seems too big.  Even looks like the  `if` is inside `for`but not respecting indentation.\r\nI think this deserves a verbose version"
      ],
      "compose-avoid-confusing-names": [
        "```suggestion\r\n      version            Show version of Docker-Compose and its dependencies\r\n```\r\nThe point here is that `Compose`, refers to the specification and `docker-compose` to the software",
        "Cool! LGTM",
        "Please, don't use shadowing here. It looks like you are making a recursive call. "
      ],
      "compose-safe-collection-modification": [
        "By commenting this line the test should fail"
      ],
      "compose-use-standard-api-fields": [
        "I agree with this. Also, if it's not running, there is no `Uptime`.\r\nChanging the `State` column to `Status` would do the job.\r\n\r\nThe only thing here is the people that could be parsing this output, but since it's not a guaranteed format, we should be fine.",
        "I think that like in `docker build` path should be mandatory. ",
        "Also, that would be great if the order here was alphabetic to reflect `docker build --help` options list behavior.",
        "The point is that in this API `path` is optional because `fileobj` can be used instead and `fileobj` is not used in your PR. Checked in https://github.com/docker/docker-py/blob/37e096f6add7e26ada3d6840ce9a9ce341bbdf23/docker/api/build.py#L125\r\n\r\n"
      ],
      "compose-avoid-mutable-defaults": [
        "```suggestion\r\n                interpolate=True, environment_file=None, enabled_profiles=None):\r\n    enabled_profiles = enabled_profiles or []\r\n```\r\n\r\nFor more info, https://stackoverflow.com/questions/1132941/least-astonishment-and-the-mutable-default-argument",
        "```suggestion\r\n    def __init__(self, name, services, client, networks=None, volumes=None, config_version=None,\r\n                 enabled_profiles=None):\r\n        enabled_profiles = enabled_profiles or []\r\n```",
        "```suggestion\r\n    additional_options = additional_options or {}:\r\n```\r\n\r\nSame for the others"
      ],
      "compose-avoid-variable-name-conflicts": [
        "Done"
      ]
    }
  },
  "jacoblee93": {
    "repos": [
      "langchain-ai/langchainjs"
    ],
    "entries": [
      {
        "slug": "langchainjs-ai-dependency-management",
        "title": "AI dependency management"
      },
      {
        "slug": "langchainjs-avoid-hardcoded-configurations",
        "title": "Avoid hardcoded configurations"
      },
      {
        "slug": "langchainjs-chunked-data-processing",
        "title": "Chunked data processing"
      },
      {
        "slug": "langchainjs-comprehensive-ai-documentation",
        "title": "Comprehensive AI documentation"
      },
      {
        "slug": "langchainjs-consistent-naming-conventions",
        "title": "Consistent naming conventions"
      },
      {
        "slug": "langchainjs-constructor-over-setter",
        "title": "Constructor over setter"
      },
      {
        "slug": "langchainjs-dependency-classification-standards",
        "title": "Dependency classification standards"
      },
      {
        "slug": "langchainjs-follow-documentation-standards",
        "title": "Follow documentation standards"
      },
      {
        "slug": "langchainjs-platform-appropriate-environment-variables",
        "title": "Platform-appropriate environment variables"
      },
      {
        "slug": "langchainjs-prefer-nullish-coalescing",
        "title": "Prefer nullish coalescing"
      },
      {
        "slug": "langchainjs-preserve-api-backward-compatibility",
        "title": "Preserve API backward compatibility"
      },
      {
        "slug": "langchainjs-simplify-code-organization",
        "title": "Simplify code organization"
      },
      {
        "slug": "langchainjs-throw-meaningful-errors",
        "title": "Throw meaningful errors"
      },
      {
        "slug": "langchainjs-typescript-naming-standards",
        "title": "TypeScript naming standards"
      },
      {
        "slug": "langchainjs-use-comprehensive-jsdoc",
        "title": "Use comprehensive JSDoc"
      },
      {
        "slug": "langchainjs-use-database-native-types",
        "title": "Use database-native types"
      },
      {
        "slug": "langchainjs-validate-untrusted-input",
        "title": "Validate untrusted input"
      }
    ],
    "comments": {
      "langchainjs-typescript-naming-standards": [
        "Let's not take a default here\r\n\r\nAlso, we are standardizing on `model` over `modelName`",
        "By convention we capitalize types/interfaces",
        "We are standardizing as `model` instead of `modelName`"
      ],
      "langchainjs-comprehensive-ai-documentation": [
        "I would show how to initialize `.fromDocuments` as well as from an existing store",
        "Can we have this use the standard retriever template?\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/retrievers.ipynb",
        "Can you use the embeddings docs template?\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/INTEGRATIONS.md#documentation-and-integration-tests\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/text_embedding.ipynb"
      ],
      "langchainjs-use-database-native-types": [
        "Would suggest storing this as an ISO string -  not all (most?) vector stores will not deserialize dates",
        "OOC why not just support `metadataJsonColumn` vs spreading metadata into other columns?",
        "And the JSON column is just to make onboarding easier?"
      ],
      "langchainjs-follow-documentation-standards": [
        "Please make the docs pages follow this format:\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/llms.ipynb",
        "Can we make the docs follow this template?\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/vectorstores.ipynb\r\n\r\nI need to update the contributing instructions...",
        "We should definitely add docs for `JsonOutputFunctionsParser`",
        "Should we have the `## Overview` boilerplate?",
        "Should at least link to vector store conceptual docs and details table"
      ],
      "langchainjs-preserve-api-backward-compatibility": [
        "Isn't removing all of these a breaking change?",
        "Not sure what current usage is, but would love a shim for principle's sake. I can try to have a look later if you don't have time.",
        "Thank you! This is a very recent integration, but we really try to not have any breaking changes in patches unless they are completely unavoidable.",
        "This conflicts with the `ToolCall` declared in `base.ts` - you should import it like this:\r\n\r\n```ts\r\nimport { type ToolCall } from \"@langchain/core/messages/tool\";\r\n```\r\n\r\nWe will remove the old type on next breaking change.",
        "Would prefer this as a static method like this:\r\n\r\nhttps://v02.api.js.langchain.com/classes/langchain_community_vectorstores_pgvector.PGVectorStore.html#initialize\r\n\r\nSince I'm not sure this will show up well in API refs, but won't block on it",
        "Hey sorry, no I mean a single param like this so as not to break the interface:\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/vectorstores/pinecone.ts#L174"
      ],
      "langchainjs-platform-appropriate-environment-variables": [
        "JavaScript uses `process.env`",
        "Yeah let's cut these if we can't find an automatic way to import them. If we really need something Node specific we can just use `.mdx`\r\n\r\nI can also reach out to the Deno team to see if they have suggestions, we have a channel with them."
      ],
      "langchainjs-use-comprehensive-jsdoc": [
        "Let's just log this once on initial call to avoid flooding the console and add some example code",
        "Ah right... this is used in multiple places\r\n\r\n@hntrl maybe let's make a small docs page with agnostic info here?",
        "nit: Add docstring with usage example",
        "Might be nice to have chat history/vectorstore classes just accept params to instantiate an engine on init in addition to taking a full engine class for connection reuse so that the user has one less import/abstraction to worry about",
        "Ok, sounds good. Yeah definitely better to do it this way for prod, just more pieces and classes to set up for getting started"
      ],
      "langchainjs-chunked-data-processing": [
        "Why not just interact with instances of `this.backingStore` directly?\r\n\r\nWould rather just init three different instances of it or just have some kind of `.bind()` semantic if absolutely necessary - there's a lot of inheritance already\r\n\r\nIf just used for tests, put in a `utils/testing` file"
      ],
      "langchainjs-prefer-nullish-coalescing": [
        "Only if someone's passing `disableStreaming: false` right? I think fine",
        "You can just do `fields?.disableStreaming ?? true`",
        "Don't think we need the `.length` check\r\n\r\nAlso should use triple equals if at all",
        "May be slightly safer to add these fields only if they are present - not sure how Azure or other model proxies will react to adding `audio: undefined`\r\n\r\n"
      ],
      "langchainjs-dependency-classification-standards": [
        "I don't think this is necessary? There is a web built-in",
        "Should be a peer + dev dep, not a direct dependency:\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/INTEGRATIONS.md#third-party-dependencies",
        "This also should be an optional peer dep:\r\n\r\nhttps://js.langchain.com/docs/contributing/code/#adding-an-entrypoint"
      ],
      "langchainjs-throw-meaningful-errors": [
        "Good to leave `response` directly on the error object - `AsyncCaller` uses fields on it to decide retries."
      ],
      "langchainjs-constructor-over-setter": [
        "Prefer `.invoke()`",
        "Also make sure this is tested",
        "`PyPDFLoader` isn't in JS",
        "So you need to click a link every hour to use this integration?",
        "I don't fully understand from the docs -  is there a way to just get tht refresh token during setup and just use that?"
      ],
      "langchainjs-validate-untrusted-input": [
        "May be worth a small note in docs emphasizing that `tableName` and `schemaName` in these files are not escaped and to not pass in end-user input here",
        "nit: can we escape column names too/disallow nonalphanumeric?",
        "Yeah good call"
      ],
      "langchainjs-consistent-naming-conventions": [
        "Let's emphasize somewhere that this wraps Unstructured\r\n\r\nShould we call this `DropboxUnstructuredLoader` instead?",
        "Separate words with underscore:\r\n`GOOGLE_DRIVE_CREDENTIALSPATH` -> `GOOGLE_DRIVE_CREDENTIALS_PATH`"
      ],
      "langchainjs-avoid-hardcoded-configurations": [
        "Let's avoid defaults at this level when possible",
        "We should ideally let the backend set this in case they change best practices, would prefer to have things unset if it won't cause issues (which it wasn't before)",
        "Thank you, old defaults have started causing issues for OpenAI so it's top of mind right now",
        "Should we be hardcoding this here?\r\n\r\nIf it's only available in one region now, would prefer to have this configurable or even not have a default at all and just have it documented"
      ],
      "langchainjs-ai-dependency-management": [
        "Did we need to add this?",
        "We don't accept hard dependencies, see:\r\n\r\nhttps://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/INTEGRATIONS.md "
      ],
      "langchainjs-simplify-code-organization": [
        "Can we put this in an existing entrypoint? `types` maybe?",
        "Want to avoid fragmentation"
      ]
    }
  },
  "micalevisk": {
    "repos": [
      "nestjs/nest"
    ],
    "entries": [
      {
        "slug": "nest-avoid-testing-anti-patterns",
        "title": "Avoid testing anti-patterns"
      },
      {
        "slug": "nest-choose-meaningful-identifier-names",
        "title": "Choose meaningful identifier names"
      },
      {
        "slug": "nest-comprehensive-dependency-security-checks",
        "title": "Comprehensive dependency security checks"
      },
      {
        "slug": "nest-configurable-log-formatting",
        "title": "Configurable log formatting"
      },
      {
        "slug": "nest-descriptive-identifier-names",
        "title": "Descriptive identifier names"
      },
      {
        "slug": "nest-document-configuration-behaviors",
        "title": "Document configuration behaviors"
      },
      {
        "slug": "nest-explicit-default-configurations",
        "title": "Explicit default configurations"
      },
      {
        "slug": "nest-follow-protocol-standards",
        "title": "Follow protocol standards"
      },
      {
        "slug": "nest-http-header-management",
        "title": "HTTP header management"
      },
      {
        "slug": "nest-modern-null-safety-patterns",
        "title": "Modern null safety patterns"
      },
      {
        "slug": "nest-pin-dependency-versions",
        "title": "Pin dependency versions"
      },
      {
        "slug": "nest-proactive-dependency-security",
        "title": "Proactive dependency security"
      },
      {
        "slug": "nest-proper-asynchronous-error-handling",
        "title": "Proper asynchronous error handling"
      },
      {
        "slug": "nest-standardize-logger-configuration-patterns",
        "title": "Standardize logger configuration patterns"
      },
      {
        "slug": "nest-standardize-null-safety-patterns",
        "title": "Standardize null safety patterns"
      },
      {
        "slug": "nest-structure-behavior-driven-tests-properly",
        "title": "Structure behavior-driven tests properly"
      },
      {
        "slug": "nest-use-consistent-control-structures",
        "title": "Use consistent control structures"
      },
      {
        "slug": "nest-use-consistent-curly-braces",
        "title": "Use consistent curly braces"
      },
      {
        "slug": "nest-use-factory-providers",
        "title": "Use factory providers"
      }
    ],
    "comments": {
      "nest-use-consistent-control-structures": [
        "I prefer your way tbh. I thought `npm run lint:fix` would fix that :p"
      ],
      "nest-configurable-log-formatting": [
        "I think having this interface wouldn't be that easy to write the output that that Issue need because `pidMessage` is too tied with the default formatting and uses that `color` function\r\n\r\nCan you try to rewrite this `formatMessage` like this:\r\n\r\n```ts\r\nprotected formatMessage(\r\n  pid: number,\r\n  logLevel: string,\r\n  context: string,\r\n  timestampDiff: number,\r\n  output: string,\r\n): string {\r\n  return `` ...\r\n}\r\n```\r\n\r\nbut then we'll need to change the `updateAndGetTimestampDiff` method to extract the coloring stuff from it and make it return a number instead of string.\r\n\r\nAnd then make `formatMessage` return an string with the color applied instead of applying it on `printMessages`. But yeah, that could be a bit harsh\r\n"
      ],
      "nest-modern-null-safety-patterns": [
        "```suggestion\r\n    return pattern?.test(str);\r\n```",
        "I guess this would be better for readability:\r\n\r\n```suggestion\r\n    if (!isNil(this.min) && float < this.min) {\r\n```\r\n\r\n`import { isNil } from '../utils/shared.utils';`"
      ],
      "nest-proactive-dependency-security": [
        "is there any link where we can see that? because there were no reports on `npm audit` \r\n\r\n![image](https://github.com/user-attachments/assets/2243ed54-2069-4f3c-89b2-68160da1e913)\r\n"
      ],
      "nest-document-configuration-behaviors": [
        "@IlliaHalchun you can find the docs here: https://docs.nestjs.com/techniques/caching#use-module-globally\r\n\r\nfeel free do change it if needed",
        "```suggestion\r\n   * Defines if file parameter is optional.\r\n   * @default false\r\n   */\r\n```",
        "we can't use `@default` tag here because `KafkaOptions['options']` is being used by both client and server"
      ],
      "nest-structure-behavior-driven-tests-properly": [
        "```suggestion\r\n      await expect(catsController.findAll()).resolves.toStrictEqual([]);\r\n```\r\n\r\ncan we use this instead? https://jestjs.io/docs/tutorial-async#asyncawait",
        "```suggestion\r\n```\r\n\r\nif we're testing `CatsService#findAll`, we shouldn't mock its implementation, otherwise we end up testing nothing.\r\n\r\nInstead, you somehow should do `catService.cats = result`. I guess it's fine do write it like this:\r\n\r\n```ts\r\n// @ts-ignore\r\ncatService.cats = result\r\n```\r\n"
      ],
      "nest-avoid-testing-anti-patterns": [
        "```suggestion\r\n```\r\n\r\nif we're testing `CatsService#findAll`, we shouldn't mock its implementation, otherwise we end up testing nothing.\r\n\r\nInstead, you somehow should do `catService.cats = result`. I guess it's fine do write it like this:\r\n\r\n```ts\r\n// @ts-ignore\r\ncatService.cats = result\r\n```\r\n"
      ],
      "nest-http-header-management": [
        "Note that since the return of `response.getHeader('Content-Type')` depends on the arguments provided to `response#setHeader` then this condition could be mislead(?). For instance, if we pass any falsy value to `response.setHeader` in some controller's method, like `response.setHeader('Content-Type', '')`, we'll not receive that content type.\r\n\r\nI know this scenario is weird but since I _(as a Nest user)_ have explicity called `response.setHeader` in my code, it would be strange to receive the default value instead. What do you think?",
        "I was thinking more in HTTP client usage and where the content type value is defined dynamically (or sort of). Instead of identifying quickly, by looking into the headers sent, that the value was wrong for something that you've implemented, the dev will see another value -- maybe this could be documented, idk.\r\n\r\nBut I agree with you now. Since we're in dealing with a framework is better to apply some restrictions. ty!",
        "oh I just read how fastify handles that here: https://github.com/fastify/fastify/blob/7e18edcf76fb58dc33b842b1dba14a425dd6feba/lib/reply.js#L136-L142 looks like they **do allow** falsy values.\r\n\r\nSo the `AbstractHttpAdapter#reply` will not behave in the same way for both adapters in those edge cases. Do you guys think this could be an issue somehow? "
      ],
      "nest-standardize-logger-configuration-patterns": [
        "instead of `console.error` we could use REPL's logger that was supplied to `NestFactory.createApplicationContext` above",
        "I think having this interface wouldn't be that easy to write the output that that Issue need because `pidMessage` is too tied with the default formatting and uses that `color` function\r\n\r\nCan you try to rewrite this `formatMessage` like this:\r\n\r\n```ts\r\nprotected formatMessage(\r\n  pid: number,\r\n  logLevel: string,\r\n  context: string,\r\n  timestampDiff: number,\r\n  output: string,\r\n): string {\r\n  return `` ...\r\n}\r\n```\r\n\r\nbut then we'll need to change the `updateAndGetTimestampDiff` method to extract the coloring stuff from it and make it return a number instead of string.\r\n\r\nAnd then make `formatMessage` return an string with the color applied instead of applying it on `printMessages`. But yeah, that could be a bit harsh\r\n"
      ],
      "nest-use-consistent-curly-braces": [
        "I prefer your way tbh. I thought `npm run lint:fix` would fix that :p"
      ],
      "nest-proper-asynchronous-error-handling": [
        "```suggestion\r\n        body.getStream().once('error', (err: Error) => {\r\n```\r\n\r\nto prevent the error 'Cannot set headers after they are sent to the client' if for whatever reason the _error_ event is emitted multiple times (not sure if this is possible tho)"
      ],
      "nest-descriptive-identifier-names": [
        "to me, `isEmpty` sounds that it could be used on non-array values. But I saw that it is only being used on arrays\r\n\r\npeharps we could rename this utility to `isEmptyArray` and drop the following:\r\n\r\nhttps://github.com/nestjs/nest/blob/e1b91d02a601c03cb8d0438b32badfaae5403447/packages/common/pipes/file/parse-file.pipe.ts#L63",
        "what do you think on renaming `value` to `fileOrFiles` or `filesOrFile`",
        "oh right.\r\n\r\nCan we have `shouldFlushLogsOnOverride: boolean`, and `flushLogsOnOverride(): void` method instead? Otherwise I'll add `setFlushLogsOnOverride(value: boolean)`\r\n"
      ],
      "nest-explicit-default-configurations": [
        "```suggestion\r\n    this.rawOutputPackets = this.getOptionsProp(options, 'rawOutputPackets', false);\r\n```\r\n\r\nhttps://github.com/nestjs/nest/blob/8617ee9952f4961841c8609329de9627cd8087f9/packages/microservices/server/server.ts#L146-L151",
        "```suggestion\r\n   * Defines if file parameter is optional.\r\n   * @default false\r\n   */\r\n```",
        "we can't use `@default` tag here because `KafkaOptions['options']` is being used by both client and server"
      ],
      "nest-pin-dependency-versions": [
        "nothing much https://github.com/actions/checkout/compare/v2...v3\r\n"
      ],
      "nest-use-factory-providers": [
        "I notice another good side-effect on changing `ExternalContextCreator` and `SerializedGraph` providers to factory:\r\n\r\n`SerializedGraph#toJSON` was called 6x in a very simple nestjs app before these changes, which I think it was useless because it was only invoked due to the name `toJSON` being known as a special method for `JSON.stringify` (used by `stringify` from `fast-safe-stringify`)\r\n\r\n:partying_face: ",
        "I guess we can also suggest them to use factory providers over value providers",
        "and I'm not sure if the word _object_ here would help the end user. But yeah, this is not a log to the app, it's an internal one"
      ],
      "nest-choose-meaningful-identifier-names": [
        "I prefer the `RouteSchema` name (`route-schema.decorator.ts`). Kinda following the same convention as the others decorators ",
        "what do you think on renaming `value` to `fileOrFiles` or `filesOrFile`",
        "oh right.\r\n\r\nCan we have `shouldFlushLogsOnOverride: boolean`, and `flushLogsOnOverride(): void` method instead? Otherwise I'll add `setFlushLogsOnOverride(value: boolean)`\r\n"
      ],
      "nest-follow-protocol-standards": [
        "Note that since the return of `response.getHeader('Content-Type')` depends on the arguments provided to `response#setHeader` then this condition could be mislead(?). For instance, if we pass any falsy value to `response.setHeader` in some controller's method, like `response.setHeader('Content-Type', '')`, we'll not receive that content type.\r\n\r\nI know this scenario is weird but since I _(as a Nest user)_ have explicity called `response.setHeader` in my code, it would be strange to receive the default value instead. What do you think?",
        "I was thinking more in HTTP client usage and where the content type value is defined dynamically (or sort of). Instead of identifying quickly, by looking into the headers sent, that the value was wrong for something that you've implemented, the dev will see another value -- maybe this could be documented, idk.\r\n\r\nBut I agree with you now. Since we're in dealing with a framework is better to apply some restrictions. ty!",
        "oh I just read how fastify handles that here: https://github.com/fastify/fastify/blob/7e18edcf76fb58dc33b842b1dba14a425dd6feba/lib/reply.js#L136-L142 looks like they **do allow** falsy values.\r\n\r\nSo the `AbstractHttpAdapter#reply` will not behave in the same way for both adapters in those edge cases. Do you guys think this could be an issue somehow? "
      ],
      "nest-comprehensive-dependency-security-checks": [
        "is there any link where we can see that? because there were no reports on `npm audit` \r\n\r\n![image](https://github.com/user-attachments/assets/2243ed54-2069-4f3c-89b2-68160da1e913)\r\n"
      ],
      "nest-standardize-null-safety-patterns": [
        "```suggestion\r\n    return pattern?.test(str);\r\n```",
        "I guess this would be better for readability:\r\n\r\n```suggestion\r\n    if (!isNil(this.min) && float < this.min) {\r\n```\r\n\r\n`import { isNil } from '../utils/shared.utils';`"
      ]
    }
  },
  "tsmithv11": {
    "repos": [
      "bridgecrewio/checkov"
    ],
    "entries": [
      {
        "slug": "checkov-avoid-wildcard-permissions",
        "title": "Avoid wildcard permissions"
      },
      {
        "slug": "checkov-document-configuration-consistently",
        "title": "Document configuration consistently"
      },
      {
        "slug": "checkov-document-configuration-options",
        "title": "Document configuration options"
      },
      {
        "slug": "checkov-expand-iam-wildcards",
        "title": "Expand IAM wildcards"
      },
      {
        "slug": "checkov-implement-pre-commit-hooks",
        "title": "Implement pre-commit hooks"
      },
      {
        "slug": "checkov-lambda-cors-protection",
        "title": "Lambda CORS protection"
      },
      {
        "slug": "checkov-preserve-api-compatibility",
        "title": "Preserve API compatibility"
      },
      {
        "slug": "checkov-restrict-public-access",
        "title": "Restrict public access"
      },
      {
        "slug": "checkov-restrict-public-network",
        "title": "Restrict public network"
      },
      {
        "slug": "checkov-safe-dictionary-access",
        "title": "Safe dictionary access"
      },
      {
        "slug": "checkov-safe-dictionary-navigation",
        "title": "Safe dictionary navigation"
      },
      {
        "slug": "checkov-target-core-resources",
        "title": "Target core resources"
      },
      {
        "slug": "checkov-thorough-test-assertions",
        "title": "Thorough test assertions"
      }
    ],
    "comments": {
      "checkov-safe-dictionary-navigation": [
        "```suggestion\r\n                        if isinstance(condition_values, list):\r\n                            for condition_value in condition_values:\r\n```\r\n\r\nRecommend checking that it is a list"
      ],
      "checkov-document-configuration-consistently": [
        "WDYT about making it the inverse logic for this flag? IAM Action Expansion is always on unless you use extension: DISABLE_IAM_ACTION_EXPANSION?",
        "Added",
        "Good catch, @NoaAzoulay. Thanks!",
        "```suggestion\r\n| TF_REGISTRY_TOKEN      | Private registry access token (supports Terraform Cloud / Enterprise and third-party registries) |\r\n```"
      ],
      "checkov-avoid-wildcard-permissions": [
        "```suggestion\r\nBy enabling the IAM_ACTION_EXPANSION extension, additional data is added to the `Action` field of AWS Terraform resources. Every action with a `*` wildcard will be expanded to the actual actions, making it easier to query for specific permission names.\r\n```"
      ],
      "checkov-restrict-public-network": [
        "These attributes don't exist. You want to check if `block_public_security_group_rules` is true (BaseResourceValueCheck)",
        "```suggestion\r\nfrom checkov.common.models.enums import CheckCategories\r\nfrom checkov.terraform.checks.resource.base_resource_negative_value_check import BaseResourceNegativeValueCheck\r\n\r\n\r\nclass AutoScalingGroupWithPublicAccess(BaseResourceNegativeValueCheck):\r\n\r\n    def __init__(self):\r\n        name = \"Ensure AWS Auto Scaling group launch configuration doesn't have public IP address assignment enabled\"\r\n        id = \"CKV_AWS_389\"\r\n        supported_resources = ['aws_launch_configuration']\r\n        categories = [CheckCategories.NETWORKING]\r\n        super().__init__(name=name, id=id, categories=categories, supported_resources=supported_resources)\r\n\r\n    def get_forbidden_values(self):\r\n        return [True]\r\n\r\n    def get_inspected_key(self):\r\n        return \"associate_public_ip_address\"\r\n\r\n\r\ncheck = AutoScalingGroupWithPublicAccess()\r\n```",
        "```suggestion\r\nfrom checkov.terraform.checks.resource.base_resource_value_check import BaseResourceValueCheck\r\nfrom checkov.common.models.enums import CheckCategories\r\n\r\n\r\nclass EMRPubliclyAccessible(BaseResourceValueCheck):\r\n\r\n    def __init__(self):\r\n        name = \"Ensure AWS EMR block public access setting is enabled\"\r\n        id = \"CKV_AWS_390\"\r\n        supported_resources = ['aws_emr_block_public_access_configuration']\r\n        categories = [CheckCategories.NETWORKING]\r\n        super().__init__(name=name, id=id, categories=categories, supported_resources=supported_resources)\r\n\r\n    def get_inspected_key(self):\r\n        return \"block_public_security_group_rules\"\r\n\r\n\r\ncheck = EMRPubliclyAccessible()\r\n```",
        "This is the same check as CKV_AWS_56. You want instead to do the same as CKV_AWS_56 but for the resource type `aws_s3_access_point`. Include network_origin = Internet",
        "```suggestion\r\n        name = \"Ensure Azure Virtual Machine disks are configured without public network access\"\r\n```",
        "The default for [public_network_access_enabled](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/batch_account#public_network_access_enabled-1) is `true` which is bad. If this field is missing you need to check the default_action before passing/failing.\r\n\r\nLooks like you'll need to adjust the ARM one as well for this.",
        "```suggestion\r\n        name = \"Ensure that Azure Batch account public network access is disabled and account access default action is deny\"\r\n```",
        "```suggestion\r\n        name = \"Ensure Tencent Cloud mysql instances do not enable access from public networks\"\r\n```"
      ],
      "checkov-expand-iam-wildcards": [
        "```suggestion\r\nBy enabling the IAM_ACTION_EXPANSION extension, additional data is added to the `Action` field of AWS Terraform resources. Every action with a `*` wildcard will be expanded to the actual actions, making it easier to query for specific permission names.\r\n```"
      ],
      "checkov-target-core-resources": [
        "```suggestion\r\n        - \"AWS::Lambda::Function\"\r\n```",
        "```suggestion\r\n        - \"aws_lambda_function\"\r\n```"
      ],
      "checkov-document-configuration-options": [
        "```suggestion\r\nBy default, the container based pre-commit hooks use the `latest` tag. This can be overridden by declaring the version number in the entry field in the pre-commit config.\r\n\r\n```yaml\r\n    hooks:\r\n      - id: checkov_container\r\n        entry: bridgecrew/checkov:2.4.2 -d .\r\n```\r\n\r\nWe should mention how to override in the docks.",
        "Good catch, @NoaAzoulay. Thanks!",
        "```suggestion\r\nIf you have modules stored in a private repository or a private Terraform registry (hosted on Terraform Cloud, Terraform Enterprise or a third-party provider like GitLab), you can grant Checkov access by providing access tokens as environment variables. This will enable Checkov to attempt to clone and scan those modules.\r\n```",
        "```suggestion\r\n| TF_REGISTRY_TOKEN      | Private registry access token (supports Terraform Cloud / Enterprise and third-party registries) |\r\n```"
      ],
      "checkov-preserve-api-compatibility": [
        "Hi @paddymorgan84 thanks for the contribution! We still have users using azurerm version 3 (such as https://registry.terraform.io/providers/hashicorp/azurerm/3.117.0/docs/resources/container_registry), my recommendation is to modify the check to look for either attribute.",
        "Thanks, @paddymorgan84! We got a support case about this one, so I'll go ahead and adjust it, but if you can fix your others, we'd really appreciate it. Enjoy the time away"
      ],
      "checkov-safe-dictionary-access": [
        "Updated. Please take a look"
      ],
      "checkov-lambda-cors-protection": [
        "```suggestion\r\n        - \"AWS::Lambda::Function\"\r\n```",
        "```suggestion\r\n        - \"aws_lambda_function\"\r\n```"
      ],
      "checkov-thorough-test-assertions": [
        "```suggestion\r\n        supported_resources = (\"Microsoft.Web/sites/slots\", \"Microsoft.Web/sites\")\r\n```\r\n\r\nSites can also have this set: https://learn.microsoft.com/en-us/azure/templates/microsoft.web/sites?pivots=deployment-language-arm-template#siteconfig-1\r\n\r\nCan you add a UT for this?"
      ],
      "checkov-restrict-public-access": [
        "These attributes don't exist. You want to check if `block_public_security_group_rules` is true (BaseResourceValueCheck)",
        "```suggestion\r\nfrom checkov.common.models.enums import CheckCategories\r\nfrom checkov.terraform.checks.resource.base_resource_negative_value_check import BaseResourceNegativeValueCheck\r\n\r\n\r\nclass AutoScalingGroupWithPublicAccess(BaseResourceNegativeValueCheck):\r\n\r\n    def __init__(self):\r\n        name = \"Ensure AWS Auto Scaling group launch configuration doesn't have public IP address assignment enabled\"\r\n        id = \"CKV_AWS_389\"\r\n        supported_resources = ['aws_launch_configuration']\r\n        categories = [CheckCategories.NETWORKING]\r\n        super().__init__(name=name, id=id, categories=categories, supported_resources=supported_resources)\r\n\r\n    def get_forbidden_values(self):\r\n        return [True]\r\n\r\n    def get_inspected_key(self):\r\n        return \"associate_public_ip_address\"\r\n\r\n\r\ncheck = AutoScalingGroupWithPublicAccess()\r\n```",
        "```suggestion\r\nfrom checkov.terraform.checks.resource.base_resource_value_check import BaseResourceValueCheck\r\nfrom checkov.common.models.enums import CheckCategories\r\n\r\n\r\nclass EMRPubliclyAccessible(BaseResourceValueCheck):\r\n\r\n    def __init__(self):\r\n        name = \"Ensure AWS EMR block public access setting is enabled\"\r\n        id = \"CKV_AWS_390\"\r\n        supported_resources = ['aws_emr_block_public_access_configuration']\r\n        categories = [CheckCategories.NETWORKING]\r\n        super().__init__(name=name, id=id, categories=categories, supported_resources=supported_resources)\r\n\r\n    def get_inspected_key(self):\r\n        return \"block_public_security_group_rules\"\r\n\r\n\r\ncheck = EMRPubliclyAccessible()\r\n```",
        "This is the same check as CKV_AWS_56. You want instead to do the same as CKV_AWS_56 but for the resource type `aws_s3_access_point`. Include network_origin = Internet",
        "```suggestion\r\n        name = \"Ensure Azure Virtual Machine disks are configured without public network access\"\r\n```",
        "The default for [public_network_access_enabled](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/batch_account#public_network_access_enabled-1) is `true` which is bad. If this field is missing you need to check the default_action before passing/failing.\r\n\r\nLooks like you'll need to adjust the ARM one as well for this.",
        "```suggestion\r\n        name = \"Ensure that Azure Batch account public network access is disabled and account access default action is deny\"\r\n```",
        "```suggestion\r\n        name = \"Ensure Tencent Cloud mysql instances do not enable access from public networks\"\r\n```",
        "```suggestion\r\n        categories = [CheckCategories.NETWORKING]\r\n```",
        "```suggestion\r\n        categories = [CheckCategories.NETWORKING]\r\n```"
      ],
      "checkov-implement-pre-commit-hooks": [
        "```suggestion\r\nIf you want to automatically run `checkov` when files in your git repo change, [install the pre-commit binary](https://pre-commit.com/#install), and add a [.pre-commit-config.yaml file](./.pre-commit-config.yaml) to your project with content similar to the example below.\r\n```"
      ]
    }
  },
  "chris-olszewski": {
    "repos": [
      "vercel/turborepo"
    ],
    "entries": [
      {
        "slug": "turborepo-boundary-case-handling",
        "title": "Boundary case handling"
      },
      {
        "slug": "turborepo-choose-logging-levels-wisely",
        "title": "Choose logging levels wisely"
      },
      {
        "slug": "turborepo-configuration-precision-matters",
        "title": "Configuration precision matters"
      },
      {
        "slug": "turborepo-consider-config-generation-methods",
        "title": "Consider config generation methods"
      },
      {
        "slug": "turborepo-define-api-boundaries",
        "title": "Define API boundaries"
      },
      {
        "slug": "turborepo-descriptive-unambiguous-identifiers",
        "title": "Descriptive, unambiguous identifiers"
      },
      {
        "slug": "turborepo-design-ergonomic-apis",
        "title": "Design ergonomic APIs"
      },
      {
        "slug": "turborepo-document-cache-strategies",
        "title": "Document cache strategies"
      },
      {
        "slug": "turborepo-eliminate-code-duplication",
        "title": "Eliminate code duplication"
      },
      {
        "slug": "turborepo-graceful-error-recovery",
        "title": "Graceful error recovery"
      },
      {
        "slug": "turborepo-handle-errors-appropriately",
        "title": "Handle errors appropriately"
      },
      {
        "slug": "turborepo-keep-build-tooling-updated",
        "title": "Keep build tooling updated"
      },
      {
        "slug": "turborepo-know-your-implicit-configurations",
        "title": "Know your implicit configurations"
      },
      {
        "slug": "turborepo-minimize-lock-duration",
        "title": "Minimize lock duration"
      },
      {
        "slug": "turborepo-propagate-errors-with-context",
        "title": "Propagate errors with context"
      },
      {
        "slug": "turborepo-semantic-naming-conventions",
        "title": "Semantic naming conventions"
      },
      {
        "slug": "turborepo-use-functional-null-handling",
        "title": "Use functional null handling"
      },
      {
        "slug": "turborepo-use-workspace-dependencies-consistently",
        "title": "Use workspace dependencies consistently"
      },
      {
        "slug": "turborepo-validate-configurations-comprehensively",
        "title": "Validate configurations comprehensively"
      },
      {
        "slug": "turborepo-validate-performance-impact-first",
        "title": "Validate performance impact first"
      }
    ],
    "comments": {
      "turborepo-boundary-case-handling": [
        "This one is totally up to you if you think explicitly handling the underflow case is easier to reason about than leveraging modulos.\r\n```suggestion\r\n            self.selected_task_index = self.selected_task_index.checked_sub(1).unwrap_or(num_rows - 1);\r\n```",
        "I'm an `Option` addict, I love how clean the control flow that methods provide is."
      ],
      "turborepo-keep-build-tooling-updated": [
        "We should probably switch to `topo` so if users add tests to `@repo/ui` then `web#test` and `docs#test` don't block on those finishing:\r\n![graphviz(2)](https://github.com/user-attachments/assets/cc97dbfb-92dc-433b-bd23-f39a19d8f264)\r\n"
      ],
      "turborepo-know-your-implicit-configurations": [
        "No, we'll always parse the lockfile and fold the packages into a task hash."
      ],
      "turborepo-define-api-boundaries": [
        "Do we want to clarify that we do not consider unstructured terminal output as an API? For example https://github.com/vercel/turborepo/pull/10079 should not be considered a breaking change IMO. Now if it was a removal of a field from `turbo ls --output=json` it would be.",
        "> Do other CLI tools consider their stdout/stderr as semver-protected if its meant for human readability?\r\n\r\nIt depends on the output, if it's meant to be human readable, then usually no. All for omitting this since this is usually not assumed to be an API."
      ],
      "turborepo-graceful-error-recovery": [
        "Up to you since you're the one driving `query`, but returning information that we know is incorrect feels off to me. Do we need to worry about panics down the line in query around assumptions that every task has a valid package and has valid task dependencies?",
        "Unsure of why the actual diff isn't showing on the [prysk diff output](https://github.com/vercel/turborepo/actions/runs/13460494959/job/37619607217#step:9:1126), but it's missing the possible values\r\n```suggestion\r\n            Specify which tasks should continue running when an error occurs. Use \"none\" to cancel all tasks. Use \"independent-tasks-only\" to continue running independent tasks and cancel dependent ones. Use \"all\" to continue running all tasks [default: none] [possible values: none, independent-tasks-only, all]\r\n```"
      ],
      "turborepo-document-cache-strategies": [
        "Are you referring to the `Remote caching enabled` line in the run prelude? If so I can confirm that it becomes `Remote caching disabled` with `--cache=local:rw`."
      ],
      "turborepo-semantic-naming-conventions": [
        "Not blocking, but I feel like `from` or `base` is less confusing name.\r\nDefer to Git wizard @NicholasLYang, if you're finding affected packages between `HEAD` and another commit, is there a standard name for what to call that other commit?"
      ],
      "turborepo-consider-config-generation-methods": [
        "The go yaml package explicitly doesn't support controlling whitespace: https://github.com/go-yaml/yaml/issues/627\r\nHaving encoding/decoding to preserve the file was really helpful for development when diffing the original and pruned lockfile. If sacrificing that for a straightforward yaml serialization is desired we can do that. I'll quick double check that pnpm doesn't throw if parsing these formatting differences.",
        "From reading through pnpm I don't think these can collide. If a dep is marked optional then it won't appear in `dependencies`, peer deps are handled in the same manner. In the case that there's an optional peer it appears under peer with the `optional` attribute set true in `peerDependenciesMeta`"
      ],
      "turborepo-use-workspace-dependencies-consistently": [
        "I was matching the version used in [`turborepo-lib`](https://github.com/vercel/turborepo/pull/9995/files#diff-217f728c3ab310388e85c7ed8a02edc124d9b28c82b9cf623a41b94204848d41L61). I can update both to use workspace version in a followup PR.",
        "```suggestion\r\ngit2 = { workspace = true, default-features = false }\r\n```"
      ],
      "turborepo-design-ergonomic-apis": [
        "Just using `strip_prefix` from the jump lets us skip the `unwrap`\r\n```suggestion\r\n        if let Some(catalog_name) = specifier.strip_prefix(\"catalog:\") {\r\n            if let Some(catalogs) = &self.catalogs {\r\n                if let Some(catalog) = catalogs.get(catalog_name) {\r\n                    if let Some(dep) = catalog.get(name) {\r\n                        return Ok(Some(&dep.version));\r\n                    }\r\n                }\r\n            }\r\n            return Ok(None);\r\n```",
        "If you follow the above suggestion you can change this to also just take `&str`\r\n```suggestion\r\n        fn token(mut self, value: &str) -> Self {\r\n            self.output.token = Some(value.into());\r\n            self\r\n        }\r\n```",
        "Only when building the engine do we view a non-existent `turbo.json` as a non-error. I'm not sure requiring the other callers to construct the `NoTurboJSON` error type manually is worth the better ergonomics in the engine builder.",
        "I would highly suggest leveraging `#[serde(into)]` [docs](https://serde.rs/container-attrs.html#into) here instead of manually implementing this. The general pattern is create a new type like `RepoDetailsDisplay` and then create a `impl<'a> From<&'a RepositoryDetails<'a>> for RepoDetailsDisplay<'a>` which does any of the desired conversions. e.g. `(&PackageName, &AnchoredSystemPath) -> PackageDetails { name: String, path: String }`",
        "We don't need to specify an exact underlying writer type here, we can render an app regardless of what sort of stdin handles it has.\r\n\r\n```suggestion\r\nfn view<W>(app: &mut App<W>, f: &mut Frame, rows: u16, cols: u16) {\r\n```"
      ],
      "turborepo-minimize-lock-duration": [
        "We only acquire the guard in the block so we ensure we release the lock once we take the changed packages. This lets the `event_fut` make progress while `self.execute_run` is pending.",
        "`changed_packages_guard` is still held while run is being executed, this prevents any events from being handled. ",
        "We should drop the lock before we do a serial send."
      ],
      "turborepo-configuration-precision-matters": [
        "For whatever reason, double `,` with trailing commas really creates a wild output that isn't useful when snapshot. The error could be better, but ITG as the first error still highlights the double `,,`\r\n```\r\nturbo_json_parse_error                                                                                                                                                        \r\n                                                                                                                                                                              \r\n  Ã— Failed to parse turbo.json.                                                                                                                                               \r\n  â”œâ”€â–¶   Ã— expected `}` but instead found `,`                                                                                                                                  \r\n  â”‚      â•­â”€[turbo.json:2:50]                                                                                                                                                  \r\n  â”‚    1 â”‚ {                                                                                                                                                                  \r\n  â”‚    2 â”‚   \"$schema\": \"https://turborepo.com/schema.json\",,                                                                                                                 \r\n  â”‚      Â·                                                  â”€                                                                                                                 \r\n  â”‚    3 â”‚   \"ui\": \"tui\",                                                                                                                                                     \r\n  â”‚      â•°â”€â”€â”€â”€                                                                                                                                                                \r\n  â”œâ”€â–¶   Ã— End of file expected                                                                                                                                                \r\n  â”‚      â•­â”€[turbo.json:3:3]                                                                                                                                                   \r\n  â”‚    2 â”‚   \"$schema\": \"https://turborepo.com/schema.json\",,                                                                                                                 \r\n  â”‚    3 â”‚   \"ui\": \"tui\",                                                                                                                                                     \r\n  â”‚      Â·   â”€â”€â”€â”€                                                                                                                                                             \r\n  â”‚    4 â”‚   \"tasks\": {                                                                                                                                                       \r\n  â”‚      â•°â”€â”€â”€â”€                                                                                                                                                                \r\n  â”œâ”€â–¶   Ã— End of file expected                                                                                                                                                \r\n  â”‚      â•­â”€[turbo.json:3:7]\r\n  â”‚    2 â”‚   \"$schema\": \"https://turborepo.com/schema.json\",,\r\n  â”‚    3 â”‚   \"ui\": \"tui\",\r\n  â”‚      Â·       â”€\r\n  â”‚    4 â”‚   \"tasks\": {\r\n  â”‚      â•°â”€â”€â”€â”€\r\n  â”œâ”€â–¶   Ã— End of file expected\r\n  â”‚      â•­â”€[turbo.json:3:9]\r\n  â”‚    2 â”‚   \"$schema\": \"https://turborepo.com/schema.json\",,\r\n  â”‚    3 â”‚   \"ui\": \"tui\",\r\n  â”‚      Â·         â”€â”€â”€â”€â”€\r\n  â”‚    4 â”‚   \"tasks\": {\r\n  â”‚      â•°â”€â”€â”€â”€\r\n  â”œâ”€â–¶   Ã— End of file expected\r\n  â”‚      â•­â”€[turbo.json:3:14]\r\n  â”‚    2 â”‚   \"$schema\": \"https://turborepo.com/schema.json\",,\r\n  â”‚    3 â”‚   \"ui\": \"tui\",\r\n  â”‚      Â·              â”€\r\n  â”‚    4 â”‚   \"tasks\": {\r\n  â”‚      â•°â”€â”€â”€â”€\r\n  â”œâ”€â–¶   Ã— End of file expected\r\n  â”‚      â•­â”€[turbo.json:4:3]\r\n  â”‚    3 â”‚   \"ui\": \"tui\",\r\n  â”‚    4 â”‚   \"tasks\": {\r\n  â”‚      Â·   â”€â”€â”€â”€â”€â”€â”€\r\n  â”‚    5 â”‚     \"build\": {\r\n  â”‚      â•°â”€â”€â”€â”€\r\n  â”œâ”€â–¶   Ã— End of file expected\r\n  â”‚      â•­â”€[turbo.json:4:10]\r\n  â”‚    3 â”‚   \"ui\": \"tui\",\r\n  â”‚    4 â”‚   \"tasks\": {\r\n  â”‚      Â·          â”€\r\n  â”‚    5 â”‚     \"build\": {\r\n  â”‚      â•°â”€â”€â”€â”€\r\n  â”œâ”€â–¶   Ã— End of file expected\r\n  â”‚       â•­â”€[turbo.json:4:12]\r\n  â”‚     3 â”‚       \"ui\": \"tui\",\r\n  â”‚     4 â”‚ â•­â”€â–¶   \"tasks\": {\r\n  â”‚     5 â”‚ â”‚       \"build\": {\r\n  â”‚     6 â”‚ â”‚         \"dependsOn\": [\"^build\"],\r\n  â”‚     7 â”‚ â”‚         \"inputs\": [\"$TURBO_DEFAULT$\", \".env*\"],\r\n  â”‚     8 â”‚ â”‚         \"outputs\": [\".next/**\", \"!.next/cache/**\"],\r\n  â”‚     9 â”‚ â”‚       },\r\n  â”‚    10 â”‚ â”‚       \"lint\": {\r\n  â”‚    11 â”‚ â”‚         \"dependsOn\": [\"^lint\"]\r\n  â”‚    12 â”‚ â”‚       },\r\n  â”‚    13 â”‚ â”‚       \"check-types\": {\r\n  â”‚    14 â”‚ â”‚         \"dependsOn\": [\"^check-types\"]\r\n  â”‚    15 â”‚ â”‚       },\r\n  â”‚    16 â”‚ â”‚       \"dev\": {\r\n  â”‚    17 â”‚ â”‚         \"cache\": false,\r\n  â”‚    18 â”‚ â”‚         \"persistent\": true\r\n  â”‚    19 â”‚ â”‚       }\r\n  â”‚    20 â”‚ â•°â”€â–¶   }\r\n  â”‚    21 â”‚     }\r\n  â”‚       â•°â”€â”€â”€â”€\r\n  â”œâ”€â–¶   Ã— End of file expected\r\n  â”‚       â•­â”€[turbo.json:21:1]\r\n  â”‚    20 â”‚   }\r\n  â”‚    21 â”‚ }\r\n  â”‚       Â· â”€\r\n  â”‚       â•°â”€â”€â”€â”€\r\n  â•°â”€â–¶   Ã— turbo.json has an incorrect type, expected an object, but received an array.\r\n```",
        "Can we make this an exact version to prevent this getting bumped and behaving unexpectedly.\r\n```suggestion\r\n    \"@types/d3-scale\": \"4.0.2\"\r\n```",
        "> I can't remember if our integration test fixtures generally use packageManager or not, but that might be useful since I know * has some weird behavior in different npm versions, especially with workspaces\r\n\r\nGood callout. With package manager requirements we now force usage of corepack for all test fixture defaulting to `npm` if one isn't specified in the fixture. Will open a ticket to make this explicit"
      ],
      "turborepo-propagate-errors-with-context": [
        "Bubble up this error so it reaches the caller and they can decide what to do with it. In our case when this error is eventually returned in `change_detector.rs` we should catch it and behave as if every package changed since we cannot identify which exact packages changed. The warning you added should be moved there with an updated message.",
        "I do not want a lossy conversion as this will cause silent failures upstream. Please report up any non-UTF8 output as an error.",
        "Generally we should prefer methods on the `turbopath`s types since we have nicer error messages. (By default Rust io errors don't include paths in them which can lead to frustrating error messages).\r\n\r\n```suggestion\r\n        preferences_file.ensure_dir()?;\r\n```",
        "We generally want to avoid `panic` here and if we do panic, we should make sure to include the underlying error.\r\n\r\nYou can use `?` operator here to bubble up the error if you add a new variant to the cli error type [here](https://github.com/vercel/turborepo/blob/main/crates/turborepo-lib/src/cli/error.rs#L21):\r\n```\r\n#[error(transparent)]\r\nInfo(#[from] info::Error),\r\n```\r\n```suggestion\r\n            info::run(base).await?;\r\n```",
        "Lets keep around the error as it could indicate some underlying issues with the system\r\n\r\n```suggestion\r\n        |e| format!(\"Cannot determine current binary: {e}\"),\r\n```",
        "Can we keep the error messaging from `map_environment` in `env.rs`? The `unwrap`s will just report `tried to unwrap None` which isn't helpful for end users.",
        "There was an existing bug where we would ignore any errors that came from resolving config options from a source. In reality since these were (mostly) eagerly calculated it only affected and validation errors from extracting config options from a `turbo.json`.\r\n\r\nWe now bubble up any errors generated by resolving config options."
      ],
      "turborepo-choose-logging-levels-wisely": [
        "Maybe add in the task id here\r\n```suggestion\r\n        debug!(\"{}: files to cache: {:?}\", self.task_id, files_to_be_cached.len());\r\n```"
      ],
      "turborepo-descriptive-unambiguous-identifiers": [
        "I find this name a little confusing. Does `clear_pinned_task` express the behavior better?\r\n```suggestion\r\n    fn update_task_selection_pinned_state(&mut self) {\r\n        // Preferences assume a pinned state when there is an active task.\r\n        // This `None` creates \"un-pinned-ness\" on the next TUI startup.\r\n        self.preferences.set_active_task(None);\r\n    }\r\n```",
        "Minor: can we rename to `create_config` instead of `config_init` if we're making this public? `config_init` naming only makes sense when we used it in `.get_or_init`",
        "Most of these variants seem more focused on why a package is included in a filter rather than \"what has changed\".  Is `PackagePresenceReason` maybe a more accurate name for this enum?",
        "Should this be `direct_dependencies`? Gives us the nice `direct_dependencies U indirect_dependencies = dependencies` relation"
      ],
      "turborepo-handle-errors-appropriately": [
        "I think we would want to abort if we are missing/can't read the root `package.json`"
      ],
      "turborepo-validate-performance-impact-first": [
        "I think this is okay since https://github.com/vercel/turborepo/pull/9123 has been merged.\r\n\r\n`vt100` used to need to iterate through every scrollback row in order to render a visible row so 1024->2048 would be a noticeable perf hit. I will do a quick profile to check if there's a noticeable impact from this change.",
        "Did a test and this is a :shipit: no noticeable perf change",
        "```suggestion\r\n                let mut map: HashMap<&AnchoredSystemPath, (GitHashes, Vec<_>)> = HashMap::with_capacity({\r\n                    let (lower, upper) = package_roots.size_hint();\r\n                    upper.unwrap_or(lower)\r\n                });\r\n```",
        "~~Nit~~ Learning opportunity:\r\nGenerally one should prefer `&str` to `&String` as the latter has to follow jump through memory twice:\r\n - first to dereference the `&String` to `String`\r\n - then to read the pointer in the `String` object that points to the actual bytes\r\n\r\n`&str` on the other hand only has the pointer to the actual bytes\r\n\r\nExceptions to the rule: You need to mutate the string",
        "This change means the function now returns an iterator over all task names instead of an vector of them. Beneficial to us as it results in far fewer allocations (4 vector allocations + 1 allocation per task) to zero allocations."
      ],
      "turborepo-validate-configurations-comprehensively": [
        "We throw if you use legacy cache options with `TURBO_CACHE`:\r\n```\r\n[0 olszewski@chriss-mbp] /Users/olszewski/code/vercel/turborepo $ TURBO_REMOTE_CACHE_READ_ONLY=1 turbo_dev @turbo/types#lint --cache=remote:rw > /dev/null\r\n WARNING  No locally installed `turbo` found. Using version: 2.2.4-canary.9.\r\nturbo 2.2.4-canary.9\r\n\r\n WARNING  TURBO_REMOTE_CACHE_READ_ONLY is deprecated and will be removed in a future major version. Use TURBO_CACHE=remote:r\r\n  x Cannot set `cache` config and other cache options (`force`, `remoteOnly`,\r\n  | `remoteCacheReadOnly`) at the same time\r\n```",
        "```suggestion\r\n            // there can also be a TURBO_TEAM, so we'll use that as well\r\n            output.team_slug = input.TURBO_TEAM;\r\n```",
        "So something I missed: when we add new keys/fields to `turbo.json` we should always test both deserialization and serialization. When we don't we often create issues ([most recent issue caused by me forgetting to test this](https://github.com/vercel/turbo/issues/8311))",
        "This is necessary due to older versions of `pnpm` not having https://github.com/pnpm/npm-conf/pull/10 which results in `pnpm` crashing if `APPDATA` isn't defined.\r\n\r\nTechnically we could have users define this instead of baking it into `turbo` itself, but considering the popularity of `pnpm` and the amount of digging it took to find that PR, it seems nice to include it."
      ],
      "turborepo-eliminate-code-duplication": [
        "Can you remove this line? This is already done by the `..Self::default()` on the following line.",
        "Not blocking, but would love to dedupe this logic with `Config::root_turbo_json_path` instead of copy-pasta. Would need to break out the behavior to a new static function.",
        "We can just borrow here instead of cloning\r\n```suggestion\r\n        let repo_root = &self.repo_root;\r\n```",
        "Up to you, but you can dedupe the spacing logic so it's only contained in `build_message_vec`:\r\n```\r\nlet build_message_vec = |footer_texts: &[&str]| -> Line {\r\n    let mut messages = Vec::new();\r\n    messages.extend_from_slice(footer_texts);\r\n```\r\n\r\n```suggestion\r\n            LayoutSections::Pane => build_message_vec(&[EXIT_INTERACTIVE_HINT]),\r\n            LayoutSections::TaskList => {\r\n                // Spaces are used to pad the footer text for aesthetics\r\n                build_message_vec(&[ENTER_INTERACTIVE_HINT, SCROLL_LOGS])\r\n            }\r\n```",
        "I don't see this used anywhere so I think it can get removed? In general I think we don't want to be changing the counts on the `ExecutionSummary` and have all modifications happen to `SummaryState`"
      ],
      "turborepo-use-functional-null-handling": [
        "We can get rid of the unwrap by converting these `Result<T,E>` to `Option<T>` since we're already not doing anything with the error type\r\n```suggestion\r\n    let package_manager = PackageJson::load(&base.repo_root.join_component(\"package.json\"))\r\n        .ok()\r\n        .and_then(|package_json| {\r\n            PackageManager::read_or_detect_package_manager(&package_json, &base.repo_root).ok()\r\n        })\r\n        .map_or_else(|| \"Not found\".to_owned(), |pm| pm.to_string());\r\n```",
        "Maybe keep this in case anyone is parsing summary/dry JSON and expecting the user to be present?"
      ]
    }
  },
  "Viicos": {
    "repos": [
      "pydantic/pydantic"
    ],
    "entries": [
      {
        "slug": "pydantic-avoid-shared-structure-mutation",
        "title": "Avoid shared structure mutation"
      },
      {
        "slug": "pydantic-avoid-unnecessary-operations",
        "title": "Avoid unnecessary operations"
      },
      {
        "slug": "pydantic-balance-documentation-thoroughness",
        "title": "Balance documentation thoroughness"
      },
      {
        "slug": "pydantic-cache-expensive-computations",
        "title": "Cache expensive computations"
      },
      {
        "slug": "pydantic-categorize-error-types",
        "title": "Categorize error types"
      },
      {
        "slug": "pydantic-consistent-configuration-patterns",
        "title": "Consistent configuration patterns"
      },
      {
        "slug": "pydantic-data-structure-correctness",
        "title": "Data structure correctness"
      },
      {
        "slug": "pydantic-document-configuration-relationships",
        "title": "Document configuration relationships"
      },
      {
        "slug": "pydantic-documentation-formatting-standards",
        "title": "Documentation formatting standards"
      },
      {
        "slug": "pydantic-eliminate-redundant-computation",
        "title": "Eliminate redundant computation"
      },
      {
        "slug": "pydantic-enforce-style-with-linters",
        "title": "Enforce style with linters"
      },
      {
        "slug": "pydantic-explicit-over-implicit",
        "title": "Explicit over implicit"
      },
      {
        "slug": "pydantic-maintain-code-consistency",
        "title": "Maintain code consistency"
      },
      {
        "slug": "pydantic-preserve-language-conventions",
        "title": "Preserve language conventions"
      },
      {
        "slug": "pydantic-robust-error-messaging",
        "title": "Robust error messaging"
      },
      {
        "slug": "pydantic-safe-attribute-access-pattern",
        "title": "Safe attribute access pattern"
      },
      {
        "slug": "pydantic-semantic-over-syntactic",
        "title": "Semantic over syntactic"
      },
      {
        "slug": "pydantic-simple-defaults-flexible-overrides",
        "title": "Simple defaults, flexible overrides"
      },
      {
        "slug": "pydantic-specific-types-for-performance",
        "title": "Specific types for performance"
      },
      {
        "slug": "pydantic-standardize-dependency-management",
        "title": "Standardize dependency management"
      },
      {
        "slug": "pydantic-structured-configuration-management",
        "title": "Structured configuration management"
      },
      {
        "slug": "pydantic-write-targeted-specific-tests",
        "title": "Write targeted, specific tests"
      }
    ],
    "comments": {
      "pydantic-avoid-shared-structure-mutation": [
        "This is related to what I mentioned:\r\n\r\n> the first one I could find: https://github.com/pydantic/pydantic/issues/7102. The https://github.com/pydantic/pydantic/issues/7102#issuecomment-1682288722 isn't really compelling, the user was doing things not the intended way. It also led to [a scary and hacky fix](https://github.com/pydantic/pydantic/pull/8066/files) that I would really like not having.\r\n\r\nThese removed lines had the effect of moving the ref from the inner schema of a `function-*` schema to the the `function-*` schema itself. With the following simplified test code added in the mentioned issue above:\r\n\r\n```python\r\nclass Numeric(BaseModel):\r\n    value: float\r\n\r\n    @classmethod\r\n    def __get_pydantic_core_schema__(cls, source_type, handler):\r\n        return core_schema.no_info_before_validator_function(cls.validate, handler(source_type))\r\n\r\n    @classmethod\r\n    def validate(cls, v):\r\n        ...\r\n\r\nclass OuterModel(BaseModel):\r\n    x: Numeric\r\n    y: Numeric\r\n```\r\n\r\nOn `main`, the schema of `OuterModel` would look like:\r\n\r\n<details>\r\n\r\n```python\r\n{\r\nâ”‚   'type': 'definitions',\r\nâ”‚   'schema': {\r\nâ”‚   â”‚   'type': 'model',\r\nâ”‚   â”‚   'cls': <class '__main__.OuterModel'>,\r\nâ”‚   â”‚   'schema': {\r\nâ”‚   â”‚   â”‚   'type': 'model-fields',\r\nâ”‚   â”‚   â”‚   'fields': {\r\nâ”‚   â”‚   â”‚   â”‚   'x': {'type': 'model-field', 'schema': {'type': 'definition-ref', 'schema_ref': '__main__.Numeric:109238506193520'}, 'metadata': {}},\r\nâ”‚   â”‚   â”‚   â”‚   'y': {'type': 'model-field', 'schema': {'type': 'definition-ref', 'schema_ref': '__main__.Numeric:109238506193520'}, 'metadata': {}}\r\nâ”‚   â”‚   â”‚   },\r\nâ”‚   â”‚   â”‚   'model_name': 'OuterModel',\r\nâ”‚   â”‚   â”‚   'computed_fields': []\r\nâ”‚   â”‚   },\r\nâ”‚   â”‚   'config': {'title': 'OuterModel'},\r\nâ”‚   â”‚   'ref': '__main__.OuterModel:109238503123056',\r\nâ”‚   â”‚   'metadata': {'<stripped>'}\r\nâ”‚   },\r\nâ”‚   'definitions': [\r\nâ”‚   â”‚   {\r\nâ”‚   â”‚   â”‚   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\nâ”‚   â”‚   â”‚   'schema': {\r\nâ”‚   â”‚   â”‚   â”‚   'type': 'model',\r\nâ”‚   â”‚   â”‚   â”‚   'cls': <class '__main__.Numeric'>,\r\nâ”‚   â”‚   â”‚   â”‚   'schema': {\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   'type': 'model-fields',\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   'fields': {'value': {'type': 'model-field', 'schema': {'type': 'float'}, 'metadata': {}}},\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   'model_name': 'Numeric',\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   'computed_fields': []\r\nâ”‚   â”‚   â”‚   â”‚   },\r\nâ”‚   â”‚   â”‚   â”‚   'config': {'title': 'Numeric'}\r\nâ”‚   â”‚   â”‚   },\r\nâ”‚   â”‚   â”‚   'ref': '__main__.Numeric:109238506193520',\r\nâ”‚   â”‚   â”‚   'metadata': {'<stripped>'},\r\nâ”‚   â”‚   â”‚   'type': 'function-before'\r\nâ”‚   â”‚   }\r\nâ”‚   ]\r\n}\r\n```\r\n\r\n</details>\r\n\r\nOn this PR, it looks like:\r\n\r\n<details>\r\n\r\n```python\r\n{\r\nâ”‚   'type': 'definitions',\r\nâ”‚   'schema': {\r\nâ”‚   â”‚   'type': 'model',\r\nâ”‚   â”‚   'cls': <class '__main__.OuterModel'>,\r\nâ”‚   â”‚   'schema': {\r\nâ”‚   â”‚   â”‚   'type': 'model-fields',\r\nâ”‚   â”‚   â”‚   'fields': {\r\nâ”‚   â”‚   â”‚   â”‚   'x': {\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   'type': 'model-field',\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   'schema': {\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'schema': {\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'schema': {'type': 'definition-ref', 'schema_ref': '__main__.Numeric:105945921898336'},\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'metadata': {'<stripped>'},\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'type': 'function-before'\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   },\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'metadata': {'<stripped>'},\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'type': 'function-before'\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   },\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   'metadata': {}\r\nâ”‚   â”‚   â”‚   â”‚   },\r\nâ”‚   â”‚   â”‚   â”‚   'y': {\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   'type': 'model-field',\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   'schema': {\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'schema': {\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'function': {'type': 'no-info', 'function': <bound method Numeric.validate of <class '__main__.Numeric'>>},\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'schema': {'type': 'definition-ref', 'schema_ref': '__main__.Numeric:105945921898336'},\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'metadata': {'<stripped>'},\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'type': 'function-before'\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   },\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'metadata': {'<stripped>'},\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   'type': 'function-before'\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   },\r\nâ”‚   â”‚   â”‚   â”‚   â”‚   'metadata': {}\r\nâ”‚   â”‚   â”‚   â”‚   }\r\nâ”‚   â”‚   â”‚   },\r\nâ”‚   â”‚   â”‚   'model_name': 'OuterModel',\r\nâ”‚   â”‚   â”‚   'computed_fields': []\r\nâ”‚   â”‚   },\r\nâ”‚   â”‚   'config': {'title': 'OuterModel'},\r\nâ”‚   â”‚   'ref': '__main__.OuterModel:105945922827312',\r\nâ”‚   â”‚   'metadata': {'<stripped>'}\r\nâ”‚   },\r\nâ”‚   'definitions': [\r\nâ”‚   â”‚   {\r\nâ”‚   â”‚   â”‚   'type': 'model',\r\nâ”‚   â”‚   â”‚   'cls': <class '__main__.Numeric'>,\r\nâ”‚   â”‚   â”‚   'schema': {\r\nâ”‚   â”‚   â”‚   â”‚   'type': 'model-fields',\r\nâ”‚   â”‚   â”‚   â”‚   'fields': {'value': {'type': 'model-field', 'schema': {'type': 'float'}, 'metadata': {}}},\r\nâ”‚   â”‚   â”‚   â”‚   'model_name': 'Numeric',\r\nâ”‚   â”‚   â”‚   â”‚   'computed_fields': []\r\nâ”‚   â”‚   â”‚   },\r\nâ”‚   â”‚   â”‚   'config': {'title': 'Numeric'},\r\nâ”‚   â”‚   â”‚   'ref': '__main__.Numeric:105945921898336'\r\nâ”‚   â”‚   }\r\nâ”‚   ]\r\n}\r\n```\r\n\r\n</details>\r\n\r\nEssentially, the difference in these two schemas is that we don't \"move\" the ref from the inner schema to the `function-before` schemas.\r\n\r\nThe changes in this PR + removing this reference moving coincidentally make it work still.\r\n\r\nHowever, doing so was a dangerous game: on _L793_, `schema` directly comes from another model. The `pop` calls removes the reference to the schema, and mutating schemas from other models has been a known issue. \r\n\r\nYou may be wondering: why this doesn't break things in the example I gave? Surely the `pop` call should have mutated the core schema of `Numeric`. Turns out it doesn't, because `Numeric.__get_pydantic_core_schema__` does not cache the schema, so calling it will generate a new one every time (and this is what happens during the schema gen of `OuterModel`). But on a similar issue, [I mentioned](https://github.com/pydantic/pydantic/issues/10160#issuecomment-2298257506) that explicitly caching the core schema in the `__get_pydantic_core_schema__` method would resolve the user issue (as the use case was slightly different)! \r\n\r\nSo to conclude, overriding `BaseModel.__get_pydantic_core_schema__` is full of unexpected behaviors, but that's fine as officially supporting them would be a huge pain.\r\n\r\n",
        "iirc (but I'm not sure), I was able to remove it only thanks to the other changes. This won't clutter the git diff though, because it's just a removal. Probably by having a proper commit description when merging, I can add a note about this?",
        "This was present inside `_generate_schema_from_property` before, but actually I think it should come first. Whenever you call `generate_schema`, if we pass in `typing(_extensions).Self`, we need to resolve the type before trying to build the schema.\r\n\r\nI moved it at the top of `GenerateSchema.generate_schema`",
        "Oops, seems like moving it breaks things, it needs to be right after the `__get_pydantic_core_schema__` check, so I'll leave it here"
      ],
      "pydantic-standardize-dependency-management": [
        "```suggestion\r\n      - name: Install UV\r\n        uses: astral-sh/setup-uv@v5\r\n        with:\r\n            python-version: ${{ matrix.python-version }}\r\n```\r\n\r\nThe reason I had to use the setup-python action in the previous third-party test is because of the comment I added regarding the uv action. In normal circumstances (i.e. when the project isn't nested under a specific repository folder) you can just let uv setup the Python version.",
        "```suggestion\r\n    - uses: actions/setup-python@v5\r\n      with:\r\n        python-version: ${{ matrix.python-version }}\r\n```\r\n\r\nBest to be as close to the project's CI."
      ],
      "pydantic-consistent-configuration-patterns": [
        "The benefit of using the class arguments is that it is recognized by type checkers. I think this is only relevant for `frozen`, so actually I'll change the example and add an annotation about it"
      ],
      "pydantic-write-targeted-specific-tests": [
        "The test you added is unrelated to your change, which makes me believe your contribution is AI generated. If so, please state it explicitly in the PR description.\r\n\r\nYou can replace by the following test:\r\n\r\n```python\r\ndef test_private_attribute_not_skipped_during_ns_inspection() -> None:\r\n    # It is important for the enum name to start with the class name\r\n    # (it previously caused issues as we were comparing qualnames without\r\n    # taking this into account):\r\n    class Fullname(str, Enum):\r\n        pass\r\n\r\n    class Full(BaseModel):\r\n        _priv: object = Fullname\r\n\r\n    assert isinstance(Full._priv, ModelPrivateAttr)\r\n```",
        "> 1. Can we include the `SchemaError` information in this result?\r\n\r\nPytest will display the exception by default. This is the output you would get for a failing test:\r\n\r\n<details>\r\n\r\n```\r\ntests/test_json_schema.py F                                                                                                                       [100%]\r\ntests/test_json_schema.py:6476 test_fails - Failed: Failed to validate the JSON Schema against the Draft 2020-12 specâ€¦                            [100%]\r\n======================================================================= FAILURES ========================================================================\r\n______________________________________________________________________ test_fails _______________________________________________________________________\r\n\r\nargs = (<pydantic.json_schema.GenerateJsonSchema object at 0x7b80723f5820>, {'metadata': {'pydantic_js_annotation_functions':...onSchema.__get_pydantic_json_schema__ of WithJsonSchema(json_schema={'type': 'invalid'}, mode=None)>]}, 'type': 'int'})\r\nkwargs = {'mode': 'validation'}, json_schema = {'type': 'invalid'}\r\n\r\n    def generate(*args: Any, **kwargs: Any) -> Any:\r\n        json_schema = orig_generate(*args, **kwargs)\r\n        if not request.node.get_closest_marker('skip_json_schema_validation'):\r\n            try:\r\n>               Draft202012Validator.check_schema(json_schema)\r\n\r\ntests/conftest.py:166: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\ncls = <class 'jsonschema.validators.Draft202012Validator'>, schema = {'type': 'invalid'}\r\nformat_checker = <FormatChecker checkers=['date', 'email', 'idn-email', 'idn-hostname', 'ipv4', 'ipv6', 'regex', 'uuid']>\r\n\r\n    @classmethod\r\n    def check_schema(cls, schema, format_checker=_UNSET):\r\n        Validator = validator_for(cls.META_SCHEMA, default=cls)\r\n        if format_checker is _UNSET:\r\n            format_checker = Validator.FORMAT_CHECKER\r\n        validator = Validator(\r\n            schema=cls.META_SCHEMA,\r\n            format_checker=format_checker,\r\n        )\r\n        for error in validator.iter_errors(schema):\r\n>           raise exceptions.SchemaError.create_from(error)\r\nE           jsonschema.exceptions.SchemaError: 'invalid' is not valid under any of the given schemas\r\nE           \r\nE           Failed validating 'anyOf' in metaschema['allOf'][3]['properties']['type']:\r\nE               {'anyOf': [{'$ref': '#/$defs/simpleTypes'},\r\nE                          {'type': 'array',\r\nE                           'items': {'$ref': '#/$defs/simpleTypes'},\r\nE                           'minItems': 1,\r\nE                           'uniqueItems': True}]}\r\nE           \r\nE           On schema['type']:\r\nE               'invalid'\r\n\r\n../../.pyenv/versions/3.12.4/envs/pydanticdev/lib/python3.12/site-packages/jsonschema/validators.py:317: SchemaError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n    def test_fails():\r\n>       TypeAdapter(Annotated[int, WithJsonSchema({'type': 'invalid'})]).json_schema()\r\n\r\ntests/test_json_schema.py:6478: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\npydantic/type_adapter.py:142: in wrapped\r\n    return func(self, *args, **kwargs)\r\npydantic/type_adapter.py:549: in json_schema\r\n    return schema_generator_instance.generate(self.core_schema, mode=mode)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nargs = (<pydantic.json_schema.GenerateJsonSchema object at 0x7b80723f5820>, {'metadata': {'pydantic_js_annotation_functions':...onSchema.__get_pydantic_json_schema__ of WithJsonSchema(json_schema={'type': 'invalid'}, mode=None)>]}, 'type': 'int'})\r\nkwargs = {'mode': 'validation'}, json_schema = {'type': 'invalid'}\r\n\r\n    def generate(*args: Any, **kwargs: Any) -> Any:\r\n        json_schema = orig_generate(*args, **kwargs)\r\n        if not request.node.get_closest_marker('skip_json_schema_validation'):\r\n            try:\r\n                Draft202012Validator.check_schema(json_schema)\r\n            except SchemaError:\r\n>               pytest.fail('Failed to validate the JSON Schema against the Draft 2020-12 spec')\r\nE               Failed: Failed to validate the JSON Schema against the Draft 2020-12 spec\r\n\r\ntests/conftest.py:168: Failed\r\n                                   Summary of Failures                                    \r\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”“\r\nâ”ƒ  File                       â”ƒ  Function    â”ƒ  Function Line  â”ƒ  Error Line  â”ƒ  Error   â”ƒ\r\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”©\r\nâ”‚  tests/test_json_schema.py  â”‚  test_fails  â”‚  6477           â”‚  6478        â”‚  Failed  â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\nResults (1.65s):\r\n         1 failed\r\n      5846 deselected\r\n\r\n```\r\n\r\n</details>\r\n\r\n> 2\\. Could we add a note about the fact that this is purely a testing feature, not a runtime `pydantic` check (at this point)?\r\n\r\nAdded.\r\n\r\n> Perhaps this is a bit excessive, but can we test this test?\r\n\r\nSeems like there are [ways to do so](https://stackoverflow.com/a/56635224), but they are pretty involved. I added a test with an expected failure.\r\n",
        "Yes can be removed after looking at the git blame."
      ],
      "pydantic-preserve-language-conventions": [
        "You mean `instanciate_by_*` only take effect on direct instantiation (i.e. `Model(...)`)?\r\n\r\nThis would really complicate the API. Using `__init__` directly is better suited when you provide the arguments directly (e.g. `Model(a=1, b='test')`). In that case, the user can simply provide the aliases (and this is what static type checkers will enforce, we have no control over it).\r\n\r\nIf you want to validate data where you don't control the provided keys, then `model_validate()` is better suited anyway: `Model.model_validate({'a': 1, 'b': 'test'})`, and you can provide `by_name=True` there.",
        "> This is intuitive and aligned with dataclass and other frameworks in statically typed languages.\r\n\r\nDataclasses don't make use of aliases, but this is something supported by the `@dataclass_transform` spec, and as per the [fields specifiers](https://typing.readthedocs.io/en/latest/spec/dataclasses.html#field-specifier-parameters) section:\r\n\r\n> `alias` is an optional str parameter that provides an alternative name for the field. This alternative name is used in the synthesized `__init__` method.\r\n\r\nBut I get your point, `Model(SomeRandomValue=147, Env_Global='test')` feels weird in Python code. The fact that type checkers will enforce aliases in `__init__` is unfortunate though.\r\n\r\nThis merits a broader discussion, currently we don't have a proper distinction between direct instantiation (`__init__`) and the `model_validate(_*)` methods when it comes to validation behavior. ",
        "```suggestion\r\nwhich is available on the [`model_dump()`][pydantic.main.BaseModel.model_dump] and\r\n[`model_dump_json()`][pydantic.main.BaseModel.model_dump_json] methods, as well as\r\nthe [`TypeAdapter`][pydantic.type_adapter.TypeAdapter] ones.\r\n```"
      ],
      "pydantic-balance-documentation-thoroughness": [
        "Probably for the serialization docs rewrite, but I don't think we should have this workaround documented. Having models serialized as something else than `dict` is uncommon, and the proposed solution isn't future proof (like in this case, where we make changes to the signature, this would break type checking for users using this workaround).",
        "I mention it in:\r\n\r\n> - `'allow'`: Providing extra data is allowed and stored in the `__pydantic_extra__` dictionary attribute.\r\n  The `__pydantic_extra__` can explicitly be annotated to provide validation for extra fields.\r\n\r\nI was trying to avoid duplication of documentation, and so the `__pydantic_extra__` example is present in the API docs. Maybe a broader discussion could be _what should live in the concepts/API docs_",
        "Can we simplify this section and the following one with a single _Similarly to Pydantic models, nested dataclasses and generics are supported_ (and we refer to the relevant model documentation)? Imo the added value is quite low here, we would expect the examples to work anyway",
        "They can just click on the `[init_typed](#init_typed)` reference, where I added a description for the setting (and same for others). Having things centralized avoids duplication, and having a heading makes it easier to link to when responding on github issues/discussions etc\r\n\r\nTo be clear, the explanation is not removed, just centralized in the configuration options section"
      ],
      "pydantic-specific-types-for-performance": [
        "This is added by me, as I think a common misconception is to use abstract containers as types thinking this will allow (in the case of `collections.abc.Sequence`) both lists and tuples to validate, while in fact this is already supported by Pydantic.",
        "I added the link. Regarding the second bullet point, not sure what you mean. Do you have a suggestion?"
      ],
      "pydantic-document-configuration-relationships": [
        "This makes me think the literal pattern would really fit better here.. If having this boolean pattern on two configuration values only introduced the inconsistency when setting both `validate_by_alias=False, validate_by_name=False`, it would be fine (I don't see why users would do so), but I won't be surprised if many users find it counter-intuitive that you also need to set `validate_by_name=True` here.\r\n\r\nI think it's worth reconsidering, cc @samuelcolvin ",
        "Also, what should happen if you set `validate_by_alias=False`, but explicitly set `by_alias=True` or `by_name=True` during validation?",
        "Yes, as discussed on Slack, thanks for summing things up here, this might be useful as a reference in case we get questions about the current API.\r\n\r\nAs we discussed as well, defaulting `validate_by_name` to `True` if `validate_by_alias` is set to `False` is postponed after this PR, and should be tackled either before 2.11 or after. Leaving this conversation unresolved so that it's easier to find it later.",
        "I think you'll have to update the `collect_config()` logic to handle both `validate_by_name` and `populate_by_name`. Mypy does static analysis so it can't be aware of the added logic in `ConfigWrapper.core_config()`.\r\n\r\nAlso, I'm not sure if the plugin already handled this, but previously if `populate_by_name` was set, the following calls were allowed:\r\n\r\n```python\r\nclass Model(BaseModel):\r\n    field: int = Field(alias='alias')\r\n\r\n    model_config = {'populate_by_name': True}\r\n\r\nModel(field=1)  # OK\r\nModel(alias=1)  # OK\r\n```\r\n\r\nIf the plugin accepted both these calls, it will probably need to be updated to disallow the following:\r\n\r\n```python\r\nclass Model(BaseModel):\r\n    field: int = Field(alias='alias')\r\n\r\n    model_config = {'validate_by_alias': True, 'validate_by_name': False}\r\n\r\nModel(field=1)  # Type checker error\r\nModel(alias=1)  # OK\r\n```"
      ],
      "pydantic-data-structure-correctness": [
        "```suggestion\r\nThis error is raised when an unhashable value is validated against a [`set`][] or a [`frozenset`][]```",
        "Actually this example is invalid from a type checking perspective, and it does makes sense in some way. I think the logic applied is \"can we end up with a stop when trying to match a value to the type?\".\r\n\r\nFor instance, with\r\n\r\n```python\r\ntype B = list[C]\r\ntype C = B | None\r\n```\r\n\r\nthe value `[None, [[None]], []]` successfully matches and we came to stop, thanks to the `| None` part. without it, there's no such value that can match the type alias without dealing with infinite recursion (hence my suggestion above).\r\n\r\nLet's go with:\r\n\r\n```suggestion\r\nFor example, this is a valid type alias:\r\n\r\n```py test=\"skip\" lint=\"skip\" upgrade=\"skip\"\r\ntype A = list[A] | None\r\n```\r\n```"
      ],
      "pydantic-explicit-over-implicit": [
        "Previously, this was a bit weird because even though we did not install any extra, `tzdata` (which is installed through the `timezone` extra) was still included in the `dev` dependency group. I've tried changing the pytest skip marker for these tests to check for the presence of the `tzdata` library but it's tricky as at the module level, some `ZoneInfo` instances are created so it still fails.\r\n\r\nIn the future, if we include new extras, we should change the name to `Test only with 'timezone' extra`",
        "So the thing is `uv` will prefer the specified version in the `.python-version` file if present, no matter the previously installed version. We currently don't have such a file, but it could be pretty bad if we end up creating one at some point, especially for the jobs with a Python version matrix: CI will only run on the version from `.python-version`, and we won't notice anything.\r\n\r\nIt's a bit unfortunate, probably using tox (and tox-uv) could help. "
      ],
      "pydantic-structured-configuration-management": [
        "The Ruff target version needs to be updated as well (`target-version = 'py39'`), I believe you'll then get new errors asking to update `typing.*` to `collections.abc.*` (e.g. for `Iterable`, etc).",
        "> Move dependency-groups section under the project one, as it is part of the [packaging specifications](https://packaging.python.org/en/latest/specifications/)."
      ],
      "pydantic-documentation-formatting-standards": [
        "Was there a reason to change these? These examples are not tested (marked as `test=\"skip\"`) so I think this isn't right now, the actual comment showing the output is most likely different.",
        "Ah then let's unify all of them using `print(<inst>)` instead of `print(repr(<inst>))`",
        "Hum I still see the `repr()` in the examples?",
        "The added newline broke the rendering. Was it added by the linter? It seems the extra level of nesting below did not have the same newline added.\r\n\r\n| Before                                                                                    | After                                                                                     |\r\n|-------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|\r\n| ![image](https://github.com/user-attachments/assets/643d5681-a822-48c8-85a9-445f25f16156) | ![image](https://github.com/user-attachments/assets/21435ae0-5eb9-47c6-9c4f-419e82811729) |",
        "These new lines are required, as otherwise the code block isn't assumed to be part of the list element.\r\n\r\n| Before                                                                                    | After                                                                                     |\r\n|-------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|\r\n| ![image](https://github.com/user-attachments/assets/0a282db3-571a-4f0c-81a7-9b8cb74beb75) | ![image](https://github.com/user-attachments/assets/0011b415-a802-4311-869a-a5c00d9df459) |\r\n\r\n(it also breaks admonitions/annotations defined after the code block)\r\n\r\nSeems like indenting the code block with two spaces (and keeping the newline, which should make the linter happy) works, could we apply this pattern?\r\n"
      ],
      "pydantic-semantic-over-syntactic": [
        "I wanted to avoid having the name depending on the capitalization of the object, and simply have `is_<name>`, but both make sense I think",
        "```suggestion\r\n    def defined_constraints(self) -> dict[str, Any]:\r\n```\r\n\r\nmaybe? set_constraints can be confusing at first, feels like this is representing an action of _setting_ something"
      ],
      "pydantic-robust-error-messaging": [
        "Yes I can probably move the check below, but I prefer enforcing the data to be provided as external users of this method don't know whether the default factory requires the argument or not, and I think it's best to unconditionally raise here (and enforcing the argument in the overload).",
        "You will have to add a `try..finally` block in case any exception happens, as otherwise the original function will have its attributes mutated.\r\n\r\nAlso, I'll have to think about it more, but are we certain that every callable that can be used with `validate_call` can have the `__qualname__`/`__annotations__` etc arguments mutated?",
        "```suggestion\r\n                       \"The default factory requires the 'validated_data' argument, which was not provided when calling 'get_default'.\"\r\n```\r\n\r\nI don't think mentioning \"this is a bug\" is correct, as users could really just call `get_default` in the wrong way.",
        "The `TypeAdapter[...]` form seems a bit weird, especially because `type_repr` seems to be `str(type)` when called from `TypeAdapter`. Probably fine as is, just wanted to note that this may lead to weird string representations (maybe `_display.display_as_type` could be used, although it is costly to compute)."
      ],
      "pydantic-simple-defaults-flexible-overrides": [
        "This is for validated function calls (see `test_unsupported_field_attribute_nested_with_function()`). We don't want to raise a warning for:\n\n```python\n@validate_call\ndef func(a: Annotated[int, Field(alias='b')]): ...\n```",
        "```suggestion\r\n        allowed_schemes=['clickhouse+native', 'clickhouse+asynch', 'clickhouse+http', 'clickhouse', 'clickhouses', 'clickhousedb'],\r\n```"
      ],
      "pydantic-cache-expensive-computations": [
        "I also need to check that `cls.__pydantic_generic_metadata__['origin']` is `None` for Pydantic models, so maybe it's best to keep the (small) duplication of code here."
      ],
      "pydantic-safe-attribute-access-pattern": [
        "This check did not really make sense in this `get_function_type_hints()`, but rather in `get_callable_return_type()` (this PR moved the check in this one), which has a smaller scope (i.e. analyzing callables used for validators/serializers functions)",
        "```suggestion\r\n    if getattr(email_validator, '__version__', '').partition('.')[0] == '2':\r\n```\r\nWith `''` as a default value, `''.partition('.')` returns `('', '', '')`",
        "Yes, what I meant is that the `hasattr` check is not necessary because of the `getattr` fallback to `''`. But I think it's fine to keep it for clarity"
      ],
      "pydantic-maintain-code-consistency": [
        "Both are equivalent, but indeed passing all the metadata directly to `_construct()` is cleaner. Applied.",
        "```suggestion\r\n    def __init_subclass__(cls) -> None:\r\n```\r\nWe can safely omit the kwargs here",
        "I inlined the logic as I couldn't find a good way to keep it in a single method as I need to raise it differently in `__delattr__`. A bit unfortunate, but at least this removes the `_check_frozen` method on the `BaseModel` class, so it avoids polluting the namespace.",
        "Why was it moved?"
      ],
      "pydantic-enforce-style-with-linters": [
        "Actually let's include `PIE790`, I think it is worth being included. I've checked `PIE804`, and I'm not sure why we get so many violations in the tests files, so fine to exclude it for now.",
        "Yes tried to find something without success, I'll look a bit more",
        "There's https://github.com/tox-dev/toml-fmt but way too much diff generated with our current file. For now let's just be careful when making edits to the pyproject.toml"
      ],
      "pydantic-eliminate-redundant-computation": [
        "`is_generic_alias` does an `isinstance()` check against `typing._GenericAlias` (e.g. `List[int]` is an instance of such a class), which isn't documented and technically private (although I don't think it will change). So it's best to avoid relying on it.\r\n\r\nIt is also a footgun as while `is_generic_alias()` works for all parameterized typing objects, it doesn't check for new unions (`is_generic_alias(int | str) == False`, but `is_generic_alias(Union[int, str]) == True`). For instance, I'm not sure we expected new unions to be skipped here:\r\n\r\nhttps://github.com/pydantic/pydantic/blob/acb0f10fda1c78441e052c57b4288bc91431f852/pydantic/_internal/_core_utils.py#L66-L74\r\n\r\nSimilarly, I've used this function here as a way to check for `type[list[int]]` forms (here `type_param` is `list[int]`):\r\n\r\nhttps://github.com/pydantic/pydantic/blob/acb0f10fda1c78441e052c57b4288bc91431f852/pydantic/_internal/_generate_schema.py#L1711-L1715\r\n\r\nThis would also match `type[Union[int, str]]`, which we actually want to support! Thankfully there's a specific check for unions just before, but this could easily be missed.\r\n\r\n---\r\n\r\nI think there are still valid use cases where you want to check if something is a generic alias (and by that I don't mean `isinstance(obj, (types.GenericAlias, typing._GenericAlias)`, but if the `obj` is a parameterized generic class -- excluding unions, typing special forms like `Literal`, `Annotated`, etc), but it's probably best to rely on `get_origin()` and the `typing_objects` check functions.\r\n",
        "```suggestion\r\n            which may not be types and thus do not have a `__module__` available\r\n```\r\n\r\nmaybe? I think the most common example is `SomeType = list[...]`, and is more common that PEP 695 type aliases. I think it's best to emphasize on the fact that most objects passed to type adapters are _instances_ (e.g. `type A = int`, `A` is instance of a `TypeAliasType`).",
        "This `display_as_type` function needs to be refactored, as it is relatively expensive to recursively check for `get_origin`, `get_args`, etc. It is currently used:\r\n- In `FieldInfo.__repr_args__`, to make a string representation of the `annotation` attribute. This can be kept.\r\n- In `get_type_ref`, called for each arg of a parametrized type. We should find a simpler way to generate a core schema reference."
      ],
      "pydantic-avoid-unnecessary-operations": [
        "I also need to check that `cls.__pydantic_generic_metadata__['origin']` is `None` for Pydantic models, so maybe it's best to keep the (small) duplication of code here."
      ],
      "pydantic-categorize-error-types": [
        "It would make sense to do so if these kind of errors happen at runtime, _after_ initial module imports and application setup (like validation errors, where it makes sense to know how to handle them).\r\n\r\nPydantic errors are just usage exceptions and it doesn't really make sense to try..catch on these ones.",
        "```suggestion\r\nWhile classes are callables themselves, `validate_call` can't be applied on them, as it needs to know about which method to use (`__init__` or `__new__`) to fetch type annotations. If you want to validate the constructor of a class, you should put `validate_call` on top of the appropriate method instead.\r\n```",
        "```suggestion\r\nAlthough you can create custom callable types in Python by implementing a `__call__` method, currently the instances of these types cannot be validated with `validate_call`. This may change in the future, but for now, you should use `validate_call` explicitly on `__call__` instead.\r\n```"
      ]
    }
  },
  "Narsil": {
    "repos": [
      "huggingface/tokenizers"
    ],
    "entries": [
      {
        "slug": "tokenizers-avoid-unsafe-code",
        "title": "Avoid unsafe code"
      },
      {
        "slug": "tokenizers-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "tokenizers-choose-semantically-clear-identifiers",
        "title": "Choose semantically clear identifiers"
      },
      {
        "slug": "tokenizers-consistent-api-design",
        "title": "Consistent API design"
      },
      {
        "slug": "tokenizers-document-for-comprehension",
        "title": "Document for comprehension"
      },
      {
        "slug": "tokenizers-flexible-tokenizer-implementation",
        "title": "Flexible tokenizer implementation"
      },
      {
        "slug": "tokenizers-handle-nullable-types-idiomatically",
        "title": "Handle nullable types idiomatically"
      },
      {
        "slug": "tokenizers-manage-version-constraints",
        "title": "Manage version constraints"
      },
      {
        "slug": "tokenizers-minimize-memory-allocations",
        "title": "Minimize memory allocations"
      },
      {
        "slug": "tokenizers-modular-model-components",
        "title": "Modular model components"
      },
      {
        "slug": "tokenizers-optimize-workflow-triggers",
        "title": "Optimize workflow triggers"
      },
      {
        "slug": "tokenizers-prefer-explicit-api-design",
        "title": "Prefer explicit API design"
      },
      {
        "slug": "tokenizers-prioritize-tokenizer-simplicity",
        "title": "Prioritize tokenizer simplicity"
      },
      {
        "slug": "tokenizers-purpose-indicating-descriptive-names",
        "title": "Purpose-indicating descriptive names"
      },
      {
        "slug": "tokenizers-pythonic-api-design",
        "title": "Pythonic API design"
      },
      {
        "slug": "tokenizers-return-results-not-panics",
        "title": "Return results not panics"
      },
      {
        "slug": "tokenizers-robust-workflow-configurations",
        "title": "Robust workflow configurations"
      },
      {
        "slug": "tokenizers-simplify-for-readability",
        "title": "Simplify for readability"
      },
      {
        "slug": "tokenizers-test-algorithmic-behavior",
        "title": "Test algorithmic behavior"
      },
      {
        "slug": "tokenizers-thread-safe-resource-sharing",
        "title": "Thread-safe resource sharing"
      },
      {
        "slug": "tokenizers-use-explicit-assertions",
        "title": "Use explicit assertions"
      }
    ],
    "comments": {
      "tokenizers-handle-nullable-types-idiomatically": [
        "Why `Option<bool>` ? It should be `bool` no ?\r\n\r\nThere are no optional arguments in Rust (and it's good)\r\n\r\n`unk_id` is really an Option, it's not at all forced (but it will cause errors if you haven't one and are triggering an unk).",
        "```suggestion\r\n           if let Some(max_token_length) = max_token_length{\r\n               if new_token.chars().count() > max_token_length{\r\n                   continue;\r\n               }\r\n           }\r\n```\r\n\r\nThis is more idiomatic imo.\r\n0 is NOT a special value. `None`  means ignore, `zero` does mean zero. If things starts to do weird things it's not the problem of this function, it's respecting the value which is more important.\r\n\r\nTry to switch to `usize` it makes everything easier to read, and the actual \"size\" isn't important optimization wise."
      ],
      "tokenizers-prioritize-tokenizer-simplicity": [
        "Yes I removed 1 feature:\r\n\r\nPreTokenized inputs. Which are presplitted strings (so list of strings).\r\n\r\nWhy did I remove it :\r\n\r\n- It bloats quite a lot the code (it would require rewriting the argument parsing since it doesn't work by default with the macro, or at least I wasn't able to).\r\n- It's a rarely used feature which have lots of caveats.\r\n- We can readd later, right now, running on more recent versions in reasonable time was the goal.",
        "That's not True, at least it was not in my case, all the tests were failing because the test string was not splitted through whitespace, so the added vocabulary was not handled, and those tests were failing because UNK_TOKEN was not defined.\r\n\r\nWe probably should really test the output of those methods in the full to make sure it's consistent, I'm pretty sure all those tests would have caught the missing unk tokens otherwise."
      ],
      "tokenizers-test-algorithmic-behavior": [
        "Could you assert the full list of `ids` and `tokens` (like the other test ?)",
        "So if the tests are not deterministic definitely let's not add them.\r\n\r\nHowever it's suprising that it's not determinstic though..... It shouldn't be ....\r\n\r\nâ– != _  \r\nfirst is a special unicode character (rarely used that's why google chose it)\r\nSecond is regular underscore.",
        "<unk> instead of ðŸ¤— is indeed expected",
        "That's better, and OK I understand better why IDs are not deterministic, they essentially have the same score, so no particular reason that ids should be in a particular order."
      ],
      "tokenizers-optimize-workflow-triggers": [
        "It's in the default template for `wasm` app. I don't really see a lot of downsides of keeping them."
      ],
      "tokenizers-return-results-not-panics": [
        "Panicking is NOT okay in transformers. We should NEVER panic since we're a library.\r\n\r\nFor that there is `TryFrom` which returns `Result<T>`. However I don't think this should be done in the Rust layer but in the Python layer instead.",
        "Remove every `unwrap` and every `vec`.\r\n\r\nThere is 1 `collect` tolerated ( I think it's done that way in BPE) and it's *only* to check that ALL bytes have a token id (you're allowed to use a single `vec` or `collect` in that branch, not in the others.\r\n\r\n:)",
        "Well if the regexp doesn't match, it's strictly equivalent to matching nothing, right ?\r\n\r\nI do that to avoid adding and `unwrap` which could potential `panic!` which makes users pretty unhappy.\r\n\r\nI don't think the pathway should be taken, but if it ever is, then I think `0` is a valid default which prevents panicking and is still, correct. Wdyt ?",
        "> I understand adding Results all the way up to the public API is cumbersome but to me is the cleaner approach : notify the user there was an error / undefined / unexpected behavior and let him handle it.\r\n\r\nIn that case it's not true. Finding 0 match is different than an error.\r\nI could very well change the regex to be `\\s+` and then the `else` branch would be expected and working as intended.\r\n\r\nIs adding a `warn!(\"AddedToken with `single_word` seems to have an issue, Please report this\")` in that code path OK ?\r\nIt it warning the user, but still prevent catastrophing failure ?\r\n\r\nWe don't expect the code path to be taken, but it's not preventing the algorithm from working.",
        "No, the else clause will trigger, when `REGEXP.find()` returns `None`.\r\n\r\nThis is normal when the REGEX fails to find any match in the submitted string.\r\nThis particular brand of REGEX should match all the time (since it should match the empty string being `^\\s*` and `\\s*$`).\r\nSo I don't expect it to not match. But if it doesn't match, it's roughly the same as saying no space where found.",
        "Oh no no necessarily.\r\n\r\n`AddedTokens(\"<mask>\", lstrip=True)` means you want it to match `\"Something <mask>  else\"` -> `['Something\", \" <mask>\", \" else\"]`. Meaning you are actually capturing the left spaces in addition to your token (effectively tripping it from being seen for your model).\r\n\r\nBut `\"Something<mask>else\"` will also capture `[\"Something\", \"<mask>\", \"else\"]`. no space have been deleted.\r\nIf you want this to NOT capture you need to activate `single_word=True` too.\r\n\r\nBut both options are orthogonal and donÂ´t  necessarily need one another (although I think in practice they are probably reasonned about in conjunction)\r\n\r\n\r\nEdit: `<mask>` is a comment in GH markdown easy to miss :D\r\n",
        "@McPatate I merged since I am starting the necessary work to release this before transformers's own release on thursday, but we can continue the discussion to make this clear.\r\n",
        "`.unwrap()` is something I tend to avoid within logic code at all if possible, since having the program panic, is never a great user experience.\r\n\r\nMy answer : https://github.com/huggingface/tokenizers/pull/919#discussion_r813862919 applies too. I think returning `Option` would be preferrable to panicking, but the result would be the same IMO, if the regex doesn't match, I just don't capture anything.",
        "I think this would go away using `aho-corasick` (it returns the ID with the match).",
        "Also there are other `unwrap` which would panic too."
      ],
      "tokenizers-modular-model-components": [
        "Why to we need this ? `encode` is enough, no ?"
      ],
      "tokenizers-manage-version-constraints": [
        "No, we need to allow for breaking changes in huggingface_hub meaning we can do `>=0.17,<0.18` instead.",
        "I would very much rather we update dependencies in separate PRs. \r\n\r\nWhen updating dependencies, I need to make sure nothing breaks upstream either.\r\nThis PR will update `pyo3` but if we could leave out the other dependencies not required for other PRs it would be easier to check. (One single PR for all other dependencies is fine)."
      ],
      "tokenizers-pythonic-api-design": [
        "As long as we're breaking signature, I would argue we have a different signature like\r\n\r\n`train(files, options=bpe_train_options)`, or `train(files, vocab_size=X, ....)` what do you think ?\r\n\r\nI like the second version better, the only trouble is the exact description of those options if going to get fuzzy pretty fast and error handling a bit hard. But it \"feels\" more pythonic, what do you think ?\r\n\r\nEither that, or if we keep the `trainer` concept, we should stick to something closer to Rust, with `trainer.train(tokenizer, files)` I actually like that last version better at this moment, the control flow feels more natural.",
        "I'm merely exposing the rust api which requires this format.\r\nIt's also what `BPE::read_files()` returns.\r\n\r\nFollowing what you said we should mostly expose the rust api, no ?\r\nShould I change the rust API ? (I don't think we should)"
      ],
      "tokenizers-document-for-comprehension": [
        "Yes, totally fine. ",
        "We probably should add a little comment here, jieba's behavior is not super straightforward here.\r\n\r\nAlso we should probably explain what this code does.\r\n\r\n```python\r\nclass JiebaPreTokenizer:\r\n    def jieba_split(self, pretoken_index, pretoken):\r\n        new_tokens = []\r\n        # Why do we need `str(normalized)?`\r\n        pretoken_string = str(pretoken)\r\n        for token, start, stop in jieba.tokenize(pretoken_string):\r\n            new_tokens.append(normalized[start:stop])\r\n        return new_tokens\r\n\r\n    def pre_tokenize(self, pretok):\r\n        # Let's call split on the PreTokenizedString to split using `self.split`\r\n        # pretok.split takes in a function, that receives `i` that is the current token index\r\n        # and `pretoken` that is a substring of th original string, that might have been normalized,\r\n        # and already cut up by previous pretokenizer. It should return a list of subtokens.\r\n        # `pretok.split` changes it inplace.\r\n\r\n        # Checkout X to see all available primitives to do a custom pretokenization\r\n        pretok.split(self.jieba_split)\r\n```"
      ],
      "tokenizers-choose-optimal-data-structures": [
        "Is the hashSet `inserted` really necessary ? `required_chars` is already a HashMap, so we shouldn't get duplicates anyway, no ?",
        "My bad, did not see that change to `Vec`",
        "Yes. It complains that `needless_collect`. Basically that we collect something that we iterate over afterwards.\r\n\r\nWe could definitely use `[allow(needless_collect)]` But I think we should try to fix them anyway. We probably also could have only one forward pass for this one.\r\n\r\nBut how does `iter().rev()` handle cache locality though ? ",
        "I fixed it to only forward pass anyway, it's just better."
      ],
      "tokenizers-simplify-for-readability": [
        "```suggestion\r\n    pub fn from_string(content: &str) -> Result<Self> {\r\n        serde_json::from_str(content)\r\n    }\r\n```\r\n\r\nSeems simpler:\r\n\r\nNo need to pass owned data, no `?`+   `Ok`.",
        "Other option which feel slightly cleaner IMO:\r\n\r\n```rust\r\n        if let Some(&id) = self.vocab.get(token) {\r\n            Ok(vec![Token {\r\n                id,\r\n                value: token.to_owned(),\r\n                offsets: (0, token.len()),\r\n            }])\r\n        } else if let Some(&unk_id) = self.vocab.get(&self.unk_token) {\r\n            Ok(vec![Token {\r\n                id: unk_id,\r\n                value: self.unk_token.to_owned(),\r\n                offsets: (0, token.len()),\r\n            }])\r\n        } else {\r\n            Err(Box::new(Error::MissingUnkToken))\r\n        }\r\n```\r\nIt's personal taste, probably gets compiled the exact same way.",
        "Perfect, I'll go ahead an merge.",
        "```suggestion\r\n            let direction = match direction.as_str(){\r\n                \"left\"  => Truncate::Left,\r\n                \"right\" => Truncate::Right,\r\n                other => panic!(\"Invalid truncation direction value : {}\", other);\r\n            };\r\n```\r\nSeems slightly more rusty (and it should compile)"
      ],
      "tokenizers-choose-semantically-clear-identifiers": [
        "The argument name is odd to me.\r\n\r\ntruncate_left([0, 1, 2], 2) -> [1, 2]\r\ntruncate_right([0, 1, 2], 2) -> [0, 1]\r\n\r\nIMO. I think from the tests that you're doing the opposite.\r\n\r\nrenaming `left` to `right` is enough as a first approximation.\r\n\r\nIn `transformers` there's actually a `direction` name which would be an enum might clarify things a bit.",
        "Please factor this code out ! :D. You shouldn't need to have this twice, Make it a simple function.\r\n\r\nAlso why change `\"First\"`  ? `\"first\"` is much more pythonic imo.",
        "```suggestion\r\nfn from_string(string: String) -> Result<PrependScheme, PyErr> {\r\n```\r\n\r\nFunctions are private by default. `pub` means public."
      ],
      "tokenizers-use-explicit-assertions": [
        "Thats a great test ! Good idea on the UTF-8 intense examples ! \r\n\r\nCould you add the explicit vocab as a result ? It makes the test more readable and more robust.\r\nLet's keep the functional part too, but having the explicit values prevents \"under the radar\" bugs, where behavior is modified unintentionally.|\r\n\r\n\r\nYou could reduce the size of those strings and `max_token_length` in order to keep things readable maybe.",
        "No 1 test, but explicit assert.\r\n\r\n`assert_eq!(tokenizer.get_vocab(), HashMapFrom([....]))`\r\n\r\nIt's really much better imo. Tests don't change that often, and I have seen many bugs be silent for too long for this lack of explicit value testing.",
        "Going along with the `#ignore` if we keep this `print` we 're not checking anything actually during tests. If possible/compatible with a fast testing iteration, we probably should actually change those into real asserts.\r\n\r\nIt hinders readability only by a slight margin I feel, but it impacts forward compat by quite a bit."
      ],
      "tokenizers-consistent-api-design": [
        "Is that really something we need to expose ?"
      ],
      "tokenizers-prefer-explicit-api-design": [
        "Just make that `PrependScheme`.\r\n\r\n`impl Into<T>` has an associated cost with it. Which is that there will be a concrete function for EVERY possible caller that might arise (here at least `PrependScheme`, `&str` and `String`. \r\n\r\nThis is nice to make an API easier to use when the concrete underlying type is relatively verbose to make, or there are many structs implementing the given trait that might be useful (something like `impl Response` in a web framework.\r\n\r\nHere everything is a simple `enum`. Importing the enum and using a variant makes the function  so that only 1 exists, prevents all kind of runtiem errors (all will become compile time errors). And since only 1 function exists, it can be inline more easily by the compiler.\r\n\r\nSimple is better here IMO",
        "TBH variant return types always give me shivers. It's usually so much better to have 2 different functions instead. Saves so much headaches for users....\r\n\r\nIf we want to save backcompat:\r\n\r\n```python\r\ndef token_to_word(token) -> int:\r\n     if self.num_sequences() > 1:\r\n         raise Exception(\"Can't use this in multiple sequences encoding, use token_to_word_seq instead\")\r\n     word, seq = token_to_word_seq(token)\r\n     return word\r\n\r\ndef token_to_word_seq(token) -> Tuple[int, int]:\r\n    ....\r\n```"
      ],
      "tokenizers-thread-safe-resource-sharing": [
        "Because it's python a python object holding a ref to a rust object.\r\n\r\nSince Python has the GIL we don't know if we're on the same thread or not, hence `Arc<RwLock<>>` To become `Send+Sync`. This is already what the `PyTrainer` owns as an object.",
        "Could you explain why all those RwLock are need ? I'm not sure why we would need them.",
        "I'm always a bit scared by adding that much `unwrap` everywhere... Do you think there's a way we could avoid them ?"
      ],
      "tokenizers-purpose-indicating-descriptive-names": [
        "I'd like to keep the dictionary format, at least for uniformity within the tests fixtures that all use dictionaries.\r\n\r\nIn terms of naming, because we might add follow ups saved tokenizers, what about `serialized_files` it makes `serialized_files[\"albert_base\"]` more understandable. I'd rather add another tokenizer in the tests than naming specifically for albert IMO. (We probably should have a handful of serialized tokenizers in those tests to make sure will load the old ones, with BertNormalizer, BPE, and so on). What do you think ?",
        "Good argument for downloading separately, and that changes the names, which makes your solution the best in the end.\r\n"
      ],
      "tokenizers-avoid-unsafe-code": [
        "Ouch, not sure we need `unsafe` in this library :)",
        "I think staying away from unsafe is probably best, even if it means 1 clone. That's my opinion at least."
      ],
      "tokenizers-flexible-tokenizer-implementation": [
        "```suggestion\r\n            tokenizer = Tokenizer(BPE(unk_token=str(unk_token), dropout=dropout, end_of_word_suffix=suffx))\r\n```\r\n\r\nno ?"
      ],
      "tokenizers-robust-workflow-configurations": [
        "Hmm it does, just moves the location of this information. Will have to check about `click` too.",
        "Done."
      ],
      "tokenizers-minimize-memory-allocations": [
        "`iter_mut()` + `for_each` makes sure there are not reallocations.\r\n\r\n`.collect()` is like `.clone()`, avoid if possible.\r\n\r\nIt might be optimized away but I would rather not count on it.",
        "Probably, but if the reduce is not itself parallel, then it's ok to have `global` be mutable.",
        "Don't clone on behalf of users, never.\r\n\r\nEither return a reference `&PrependScheme`.\r\nOr make `PrependScheme`  ` Copy`.\r\n\r\nMaking something Copy, is something you should do only when the size makes it worthwile.\r\nA reference is a pointer of size `usize` so copying usize is usually much faster than copying an entire struct.\r\nFor an enum like PrependScheme, it' s only 3 possible values, encoded internally by rust as `u8` or `usize` (Don't remember which). In any case, it's prefereable to copy the value than to pass references around (which cost a pointer dereference)\r\n",
        "Let's remove this. Vec allocs is costly, you don't want to do this.",
        "Done."
      ]
    }
  },
  "VladLazar": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-adaptive-cache-expiration-strategy",
        "title": "Adaptive cache expiration strategy"
      },
      {
        "slug": "neon-balance-flexibility-with-performance",
        "title": "Balance flexibility with performance"
      },
      {
        "slug": "neon-cache-performance-preservation",
        "title": "Cache performance preservation"
      },
      {
        "slug": "neon-comprehensive-code-documentation",
        "title": "Comprehensive code documentation"
      },
      {
        "slug": "neon-configurable-cache-parameters",
        "title": "Configurable cache parameters"
      },
      {
        "slug": "neon-connection-pooling-with-pipelining",
        "title": "Connection pooling with pipelining"
      },
      {
        "slug": "neon-database-before-memory",
        "title": "Database before memory"
      },
      {
        "slug": "neon-design-metrics-for-insights",
        "title": "Design metrics for insights"
      },
      {
        "slug": "neon-document-api-specs-completely",
        "title": "Document API specs completely"
      },
      {
        "slug": "neon-document-concurrency-design-decisions",
        "title": "Document concurrency design decisions"
      },
      {
        "slug": "neon-document-parameter-choices",
        "title": "Document parameter choices"
      },
      {
        "slug": "neon-ensure-algorithm-robustness",
        "title": "Ensure algorithm robustness"
      },
      {
        "slug": "neon-environment-specific-config-defaults",
        "title": "Environment-specific config defaults"
      },
      {
        "slug": "neon-log-level-appropriately",
        "title": "Log level appropriately"
      },
      {
        "slug": "neon-minimize-unnecessary-allocations",
        "title": "Minimize unnecessary allocations"
      },
      {
        "slug": "neon-reliable-concurrency-synchronization",
        "title": "Reliable concurrency synchronization"
      },
      {
        "slug": "neon-stage-configuration-changes-gradually",
        "title": "Stage configuration changes gradually"
      },
      {
        "slug": "neon-structure-endpoints-for-rest",
        "title": "Structure endpoints for REST"
      }
    ],
    "comments": {
      "neon-stage-configuration-changes-gradually": [
        "Why do we support propagating these configs in two ways?",
        "Should these be optional? It allows us to try it out in staging/pre-prod without deploying the new endpoint everywhere in prod."
      ],
      "neon-database-before-memory": [
        "Another general guideline: aim to update the database state __before__ the in-memory state. Otherwise, if you crash in between it will look like you time travelled since storcon rebuilds the world state from the db.",
        "`schedule_compaction_update`, yeah that should, but the name is bad :laughing: \r\n\r\nFair point. I'll just unlink here.",
        "https://github.com/neondatabase/neon/pull/12038/commits/2053e7f4a4b8f838159008d0e7841c454bda4459"
      ],
      "neon-design-metrics-for-insights": [
        "Is this metric useful at timeline granularity? If the goal is to get a sense of the overall PS background operations, you can look at `rate(pageserver_storage_operations_seconds_count)`, but that's a per-pageserver view.\r\n\r\nAsking, since this is pretty high cardinality.",
        "This metric is being bumped multiple times for the same request. I think it makes sense to remove the increment from resolve and keep just this one (this is when we actually tell the compute to reconnect).",
        "Pushed [b01579a](https://github.com/neondatabase/neon/pull/12467/commits/b01579afad6b604597c95234d59a031c8b5f2329). PTAL",
        "Not sure I understand this. If the current offloader is indeed lagging, then the call to `determine_offloader` would have already picked a different offloader. I mean `offloader_sk_id` might not actually be the current offloader."
      ],
      "neon-configurable-cache-parameters": [
        "I don't have good intuition on what the right size for this cache is. Therefore, I'd make this a tenant config since those are configurable at run-time. The data structures supports capacity resizing via `resize_capacity`.\r\n\r\nThis would give us a tool to address replicas with slow get pages due to rel size cache churn.",
        "Please plug into `Timeline::tenant_conf_updated` and resize the cache when the config changes.\r\nOtherwise, the config change still requires a restart."
      ],
      "neon-environment-specific-config-defaults": [
        "The production defaults don't make sense for the test env.\r\nThis will likely become a tenant config at some point, so we can remove this oddity then."
      ],
      "neon-minimize-unnecessary-allocations": [
        "Would be good to avoid an extra allocation here. Could track the block number in `Self::response` and put everything in the right place with `Vec::spare_capacity_mut`.",
        "Sounds good. Can optimize later if needed."
      ],
      "neon-ensure-algorithm-robustness": [
        "This isn't entirely correct. It doesn't handle:\r\n* overlapping layers\r\n* delta layers containing a full page image\r\n\r\nIn some cases it would panic and in others it would just yield an incorrect result.\r\nThe correct way of doing this is to use a neon local setup, import the tenant state from s3 and then use the HTTP endpoints to get the reconstructed page: `GET v1/tenant/<tid>-<shard-id>/timeline/<tlid>/getpage?lsn=<lsn>`.",
        "Indeed the check was wrong. Good catch.\r\n\r\nIt's not about bound checking since we don't blindly index into the jobs.\r\nIt narrows collisions down to collisions on plans with the same length.\r\n\r\n> Wonder why we wouldn't go all the way and compute a cryptographic checksum\r\n\r\nI don't think we'll have enough on-going imports at any given time to worry about collisions here.\r\nFor 1 million the collision probability is ~2.71e-08. This is without accounting for imports of the same length check.",
        "Fixed in https://github.com/neondatabase/neon/pull/11862/commits/5ec58a723b11e0370dab9c8b6cd0d64ceab414e2",
        "Done: https://github.com/neondatabase/neon/pull/11862/commits/27458a203b18510e20c57bf6feb229a171f59cb1"
      ],
      "neon-log-level-appropriately": [
        "Let's log a warning here. It's not that big of a deal for draining since we expect the node to come back, but here we're removing it. Also, these comments mention draining. Can you update them to avoid confusion?",
        "Don't think it's useful to log thousands of layers that we are keeping. We log removed layers somewhere else - that's what could actually be useful. Could also contribute to the loop taking longer than it should.",
        "Please update this log line and comment to mention that we place a tombstone instead of deleting."
      ],
      "neon-document-parameter-choices": [
        "Would be good to leave a comment about why `retry_on_failures` is required."
      ],
      "neon-document-concurrency-design-decisions": [
        "https://github.com/neondatabase/neon/pull/11937/commits/9377e9af65921e06e25e4cec5607150faad56a1a\r\n\r\nI don't think a deadlock is possible here. The only place where we grab both locks at the same time is `write_to_disk` and in that case they're read locks.",
        "> We're relying on inner being append-only for this to be safe, yeah?\r\n\r\nCorrect.\r\n\r\n> Let's specify that as part of the locking protocol.\r\n\r\nDon't see how you'd express that via the locking protocol. Can you elaborate?",
        "Makes sense. Done in https://github.com/neondatabase/neon/pull/11937/commits/721e8989b0968023d06f8b52e6b559fc6af2cded",
        "The API sucks.\r\n\r\n`freeze` and `put_batch` are synchronized via `Timeline::write_lock`.\r\nWe only `freeze` when there's no ongoing writes. There's two paths on which we freeze:\r\n1. Via the active `TimelineWriter`. This holds the `Timeline::write_lock` for its lifetime. The rolling is handled in `TimelineWriter::put_batch`. It's a `&mut self` function so can't be called from different threads.\r\n2. In the background via `Timeline::maybe_freeze_ephemeral_layer`. This only proceeds if `try_lock` on `Timeline::write_lock` succeeds (i.e. there's no active writer), hence there can be no concurrent writes.",
        "https://github.com/neondatabase/neon/pull/11855/commits/af641fe64ac386a712952f531076d476eae6555e"
      ],
      "neon-adaptive-cache-expiration-strategy": [
        "The compute is available during pre-warm, correct? Wondering if there's any interactions between the user workload and prewarm that are worth considering."
      ],
      "neon-cache-performance-preservation": [
        "We terminate the old primary before promoting the secondary. If I'm reading this right, there's a an availability gap between the termination and promotion. Is this reading correct? If so, can we avoid it?",
        "It would be great to aim for something where the happy path has no hiccups. If that's not possible, let's be explicit about it and say why.\r\n\r\n---\r\n\r\nProxy learns about about the new primary after the promotion of the new compute, so there should be no queries on it at that point (according to the diagram). It seems like we can swap promotion and termination around while maintaining this property.\r\n\r\nWhat would happen to the client in this case? Let's say we have an in-progress query when the old primary terminates. Client would probably retry and eventually get routed to the new primary.\r\n\r\n",
        "> Two computes can Never, NEVER, never both be Primary on the same timeline at the same time. CPlane is supposed to make sure of that, and Compute will fail if it doesn't. Promotion of the replica before the original Primary shut down will cause errors, panics, data loss, shutdowns, and/or nasal deamons on the Primary, this promoting Secondary, or both.\r\n\r\nAt the risk of being annoying: why? I understand it doesn't work in the case when two primaries are being written to. But what about if we could guarantee the new primary is idle? More specifically, by idle I mean it would have to not write any WAL and be in some sort of consensus observer role.\r\n\r\nI'm sure you're right about this working with the current state of afairs, but I'm curious about what the technical roadblock is."
      ],
      "neon-balance-flexibility-with-performance": [
        "> Let's consider an extreme case, for illustration: we send a batch request for pages 0-100, and at page 50 we have to download a layer which takes 3 seconds. I think there are cases where it's advantageous to send back pages 0-49 first, and pages 50-100 later:\r\n\r\nOn streaming: the pageserver doesn't currently support streaming. The current implementation collects all the deltas and images for _all_ keys in the batch and then does walredo for all keys concurrently. We could do true streaming, but it's not trivial.\r\n\r\nOn how scattered the reads should be: hard to tell. Generally speaking, the more clustered the pages in a request are, the more predictable performance will be:\r\n* in-memory layers are ingest ordered: clustering by key doesn't help much here\r\n* delta layers are ordered by (key, lsn): clustering by key helps with index reads, but for the reads themselves our ability to merge depends on how many key versions we have\r\n* in-memory layers are ordered by key: clustering keys clearly helps here\r\n\r\nOverall take:\r\n* I'm not sure streaming responses makes sense at this stage. We currently support a max of 32 keys in a batch (PS side). We could increase it, but I don't think we should go above 512 pages short term. I'd just return all the pages at once. Streaming protocol can be done in the future, but I see there's other reasons quoted for streaming.\r\n* I think it makes sense to keep the grpc spec somewhat flexible here. The implementation can enforce strict batching rules or implement heuristics based on key, lsn range.",
        "> think the more important point here is that when we do the batching client-side, we know whether it makes sense to batch or not (i.e. whether to prioritize throughput or latency)\r\n\r\nWith the more flexible protocol, the communicator could coalesce batches, but perhaps we don't care about that (yet?)."
      ],
      "neon-connection-pooling-with-pipelining": [
        "What's the point of this extra channel now? Can't we just plug `caller_rx` into `client.get_pages`?",
        "Makes sense. The extra stream still allows for pipelining as long as the previous request was pushed down the tcp pipe (response need not be received by that point)."
      ],
      "neon-structure-endpoints-for-rest": [
        "This should be `POST`.",
        "This should be:\r\n`PUT`: if the endpoint does the offload before returning\r\n`POST`: if the endpoint does the offload in the background"
      ],
      "neon-document-api-specs-completely": [
        "Let's be kind to future readers and mention why it's time consuming."
      ],
      "neon-reliable-concurrency-synchronization": [
        "Would calling `env.storage_controller.reconcile_until_idle()` to force the storage controller to run all pending reconciliations work?",
        "This will hang since `NeonPageserver.start` waits for the node to be marked as active in the storage controller by default. Re-attaching is a pre-condition for that. To avoid this, you can do:\r\n```\r\nps.stop()\r\nps.start(await_active=False)\r\n```"
      ],
      "neon-comprehensive-code-documentation": [
        "nit: a comment would be nice:\r\n* explain difference between effective and request lsn\r\n* explain that primary compute always uses `request_lsn == MAX`\r\n* hint that this is how the compute thinks about get page requests"
      ]
    }
  },
  "DiegoAndai": {
    "repos": [
      "mui/material-ui"
    ],
    "entries": [
      {
        "slug": "material-ui-avoid-render-cycle-allocations",
        "title": "Avoid render cycle allocations"
      },
      {
        "slug": "material-ui-consistent-component-api-patterns",
        "title": "Consistent component API patterns"
      },
      {
        "slug": "material-ui-consistent-package-naming",
        "title": "Consistent package naming"
      },
      {
        "slug": "material-ui-defensively-handle-nullables",
        "title": "Defensively handle nullables"
      },
      {
        "slug": "material-ui-explicit-configuration-specifications",
        "title": "Explicit configuration specifications"
      },
      {
        "slug": "material-ui-follow-library-recommendations",
        "title": "Follow library recommendations"
      },
      {
        "slug": "material-ui-graceful-component-errors",
        "title": "Graceful component errors"
      },
      {
        "slug": "material-ui-isolate-dom-security-boundaries",
        "title": "Isolate DOM security boundaries"
      },
      {
        "slug": "material-ui-meaningful-and-consistent-names",
        "title": "Meaningful and consistent names"
      },
      {
        "slug": "material-ui-parameter-interaction-design",
        "title": "Parameter interaction design"
      },
      {
        "slug": "material-ui-parameterize-build-processes",
        "title": "Parameterize build processes"
      },
      {
        "slug": "material-ui-standardize-build-configurations",
        "title": "Standardize build configurations"
      },
      {
        "slug": "material-ui-strict-mode-proof-hooks",
        "title": "Strict mode-proof hooks"
      },
      {
        "slug": "material-ui-test-behavior-not-implementation",
        "title": "Test behavior not implementation"
      },
      {
        "slug": "material-ui-use-theme-utilities-consistently",
        "title": "Use theme utilities consistently"
      }
    ],
    "comments": {
      "material-ui-avoid-render-cycle-allocations": [
        "Should we use a ref here instead?"
      ],
      "material-ui-defensively-handle-nullables": [
        "I don't mind the `!!` to cast into boolean",
        "I would argue it's a fix as it was always supposed to be `() => boolean` (it's only `boolean` in the `CSSObjectWithVariants` type), but it wasn't actually because the type wasn't being correctly picked up.\r\n\r\nIt's one of those fixes that could be perceived as a breaking change.\r\n\r\nMaybe we could compromise with a minor release and an explanation on the changelog?",
        "This would be the last thing that we need to decide on. Similar to the `styles` now being required, this is a fix that might be perceived as breaking.\r\n\r\nMy proposal is that we release it with in a minor version and a note."
      ],
      "material-ui-parameterize-build-processes": [
        "What creates the `lib/esm/package.json` file that we need to remove? Can we stop creating it?",
        "> I didn't necessarily want to add a special case to the build for a single package, but we could.\r\n\r\nThat's ok, I was just checking ðŸ‘ðŸ¼ let's go with whatever option you think it's better."
      ],
      "material-ui-meaningful-and-consistent-names": [
        "Why call the handle `event`? Something like `textareaHandle` would be easier to understand IMO.",
        "I agree with replacing the term `tag` in our code. I think we can use `item` and explain in the docs what props are provided. We can explain in the docs that these include some props specific to Chip (`onDelete`) but other's that aren't. For example, the `data-tag-index` should be transformed to `data-item-index`, and this will be important to spread regardless of whether the component is a Chip or not."
      ],
      "material-ui-consistent-package-naming": [
        "Not strictly necessary, but if we could rename `@app/next-app` to `@app/pigment-css-next-app`, that would help clarify the ignore."
      ],
      "material-ui-test-behavior-not-implementation": [
        "Is this required?",
        "Right, but I think the `textarea` element that the `TextareaAutosize` renders should have the `textbox` role by default, doesn't it?",
        "Ah, I see. In that case I think we should do\r\n\r\n```\r\n      slotProps={{\r\n        input: {\r\n          'data-testid': 'input',\r\n        },\r\n      }}\r\n```\r\n\r\nAnd use `page.getByTestId('input')`\r\n\r\nWould that work?"
      ],
      "material-ui-standardize-build-configurations": [
        "I don't understand what's the function of this ðŸ˜… "
      ],
      "material-ui-parameter-interaction-design": [
        "If providing the `container` prop was required when using `disablePortal`, this could be simplified to \r\n\r\n```suggestion\r\nconst resolvedContainer = getContainer(container) || getDoc().body;\r\n```\r\n\r\nRight?"
      ],
      "material-ui-explicit-configuration-specifications": [
        "This makes sense, let's use `^6.0.0`"
      ],
      "material-ui-consistent-component-api-patterns": [
        "The `useSlot` hook already has merging strategies implemented, but I think the problem here is that the Autocomplete should handle the slot props better:\r\n\r\n- Refactor `Autocomplete` to use `slotProps` instead of `InputProps` and `inputProps`\r\n- Inside `Autocomplete`, merge `slotProps.input` and `slotProps.htmlInput`\r\n\r\n```js\r\nconst [HtmlInputSlot, htmlInputProps] = useSlot('htmlInput', {\r\n    elementType: 'input',\r\n    externalForwardedProps, // already defined above\r\n    additionalProps: {\r\n        disabled,\r\n        readOnly,\r\n        ...getInputProps(),\r\n    },\r\n    className: classes.input,\r\n    ownerState,\r\n  });\r\n\r\n// ...\r\n\r\n// line 730\r\n    {renderInput(\r\n     // ...\r\n     // line 778\r\n    slotProps: {\r\n        htmlInput: htmlInputProps,\r\n    },\r\n},\r\n```\r\n\r\nI haven't tested this, but it should be something similar to the code above.\r\n\r\nThis will be a complex PR, but in the end, the result should be the slots pattern properly implemented in the `Autocomplete` component.\r\n\r\nWe're still discussing if the slots pattern needs changes, cc: @aarongarciah, so this PR might require changes in the future given that decision.\r\n\r\nDoes that make sense @sai6855?",
        "> are you imagining renderInput would look something like this after refactoring?\r\n\r\nYes, exactly that\r\n\r\n> As Autocomplete component don't have any control over slotProps in highlighted line, is it possible to merge without changes to be done in userland?\r\n\r\nI don't understand the issue. Is it that `params` has a `slotProps` key as well, so it's overridden?",
        "Oh ok, I think I see the issue. The Autocomplete structure confused me. Yes, my suggestion is not correct, I was confusing `Autocomplete` slots with `TextField` slots.",
        "@sai6855 I think these should be merged in userland, this was always required, see: https://github.com/mui/material-ui/issues/43573#issuecomment-2402630864.\r\n\r\nLet's wait and see if we can move forward with migrating the usage of `InputProps`, `InputLabelProps` to slotProps in Autocomplete",
        "@sai6855 sorry for the late reply.\r\n\r\nThe path forward for this PR is what I commented here: https://github.com/mui/material-ui/issues/43573#issuecomment-2402630864. We should update Autocomplete to use `slotProps` internally to make it consistent but not merge anything on our side. We should also add documentation explaining what I explained in that same comment: \"When overriding `slotProps.input` for the Autocomplete's TextField, `params.slotProps.input` must be spread\"\r\n\r\nDoes that make sense?",
        "> If we go with above approach we would be breaking AutocompleteRenderInputParams type\r\n\r\nCould we:\r\n\r\n- Add `slotProps`\r\n- Keep and deprecate `InputProps` and `inputProps`\r\n\r\nInternally, the Autocomplete would have to forward `InputProps` and `inputProps` to `slotProps` accordingly.\r\n\r\nIf that would work, it wouldn't require a breaking change, right?"
      ],
      "material-ui-use-theme-utilities-consistently": [
        "Let's make this change\r\n\r\n```suggestion\r\n    overflowY: 'auto',\r\n```\r\n\r\nNo need for a comment ðŸ˜Š "
      ],
      "material-ui-follow-library-recommendations": [
        "Is there a particular reason a double click is needed?"
      ],
      "material-ui-isolate-dom-security-boundaries": [
        "We shouldn't do this:\r\n\r\nIf multiple modals are open, or the developer applied `aria-hidden` to one of the modal's ancestors, it's the developer responsibility to fix it.\r\n\r\nThis is not an acceptable side-effect:\r\n\r\n> [...] if a developer manually applied aria-hidden to hide certain content, removing it could lead to unintended accessibility issues.\r\n\r\n"
      ],
      "material-ui-strict-mode-proof-hooks": [
        "Using the register pattern without an unregister function sounds to me like it can introduce weird edge cases. Did you consider registering/unregistering on an effect instead?",
        "Deriving the `value` from the position sounds good to me ðŸ‘ðŸ¼ \r\n\r\n> While registerTab is already idempotent when used with explicit values (via valueToIndex.has(finalValue)), we can't enforce value on Tab without introducing a breaking change.\r\n\r\nBut if we create a `value` for the position on our side, then we would have a `finalValue`, no? ",
        "> Yes, we do generate a finalValue based on position internally when value isn't provided. The issue is that this implicit value depends on render order and a shared index (childIndexRef), which is incremented during registration.\r\n\r\nBut what if we \"store\" that value in the Tab, to be used in subsequent registrations? Wouldn't that remove the issue?\r\n\r\nSomething like:\r\n1. Tab runs register for the first time. There's no value, so we store `finalValue`\r\n2. In any subsequent register call, we use `finalValue` to identify the Tab, and thus we don't have to calculate a new `finalValue`.\r\n\r\nWould this work?",
        "> Why do we want to pass the same finalValue to the next registration call? If we want to pass this stored finalValue, we shouldn't call registerTab again in the first place (supposedly in the effect).\r\n\r\nBecause we want to return the unregistering callback from the effect, so it's run on unmount.\r\n\r\n> Even in Base UI, they are making the value prop required on Tab\r\n\r\nThis is not an option for us unless we want to wait for a new major.\r\n\r\n> Supporting implicit value in Tab cause them issues like\r\n\r\nWe're already supporting implicit value, aren't we? With or without my suggestion.",
        "> But wouldn't the useEffect's setup function do nothing?\r\n\r\nOnly on the first run of the effect. Subsequent runs would unregister (cleanup) and register back. This is the same as having register/unregister in an effect, except that the very first time we already registered inside `useState`. I don't see an issue with it if register is idempotent ðŸ¤” ",
        "Which test failed? What was the message?"
      ],
      "material-ui-graceful-component-errors": [
        "```suggestion\r\n    throw new Error('Material UI: The Tab component must be used inside a Tabs component');\r\n```",
        "I wonder if we should throw here. If it were a new component, yes, for sure, but maybe we can take a softer approach? Just to cover for users who are using the `Tab` component on its own. Or is that too cautious?",
        "We don't know how users are using it in the wild, so it's better to be conservative. Let's have a warning in development instead of throwing."
      ]
    }
  },
  "anthonyshew": {
    "repos": [
      "vercel/turborepo"
    ],
    "entries": [
      {
        "slug": "turborepo-boundary-case-handling",
        "title": "Boundary case handling"
      },
      {
        "slug": "turborepo-configuration-precision-matters",
        "title": "Configuration precision matters"
      },
      {
        "slug": "turborepo-define-api-boundaries",
        "title": "Define API boundaries"
      },
      {
        "slug": "turborepo-document-cache-strategies",
        "title": "Document cache strategies"
      },
      {
        "slug": "turborepo-document-configuration-alternatives",
        "title": "Document configuration alternatives"
      },
      {
        "slug": "turborepo-eliminate-code-duplication",
        "title": "Eliminate code duplication"
      },
      {
        "slug": "turborepo-framework-specific-entrypoints-organization",
        "title": "Framework-specific entrypoints organization"
      },
      {
        "slug": "turborepo-hybrid-monorepo-testing",
        "title": "Hybrid monorepo testing"
      },
      {
        "slug": "turborepo-keep-build-tooling-updated",
        "title": "Keep build tooling updated"
      },
      {
        "slug": "turborepo-know-your-implicit-configurations",
        "title": "Know your implicit configurations"
      },
      {
        "slug": "turborepo-link-terms-provide-examples",
        "title": "Link terms, provide examples"
      },
      {
        "slug": "turborepo-propagate-errors-with-context",
        "title": "Propagate errors with context"
      },
      {
        "slug": "turborepo-standardize-package-manager-commands",
        "title": "Standardize package manager commands"
      },
      {
        "slug": "turborepo-validate-configuration-structures",
        "title": "Validate configuration structures"
      },
      {
        "slug": "turborepo-validate-configurations-comprehensively",
        "title": "Validate configurations comprehensively"
      },
      {
        "slug": "turborepo-validate-performance-impact-first",
        "title": "Validate performance impact first"
      },
      {
        "slug": "turborepo-verify-test-commands",
        "title": "Verify test commands"
      }
    ],
    "comments": {
      "turborepo-boundary-case-handling": [
        "Classic me not knowing methods I have available."
      ],
      "turborepo-keep-build-tooling-updated": [
        "Run `npx @turbo/codemod upgrade` to upgrade to `turbo@2`. ðŸ¥³ ",
        "Run `npx @turbo/codemod upgrade` to upgrade to `turbo@2`. ðŸ¥³ "
      ],
      "turborepo-verify-test-commands": [
        "```suggestion\r\ncargo coverage -- --open\r\n```"
      ],
      "turborepo-document-configuration-alternatives": [
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```",
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```",
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```"
      ],
      "turborepo-standardize-package-manager-commands": [
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```",
        "```suggestion\r\n# Without [global `turbo`](https://turborepo.com/docs/getting-started/installation#global-installation), use your package manager\r\n```"
      ],
      "turborepo-know-your-implicit-configurations": [
        "@chris-olszewski, I actually don't know this one. Can I ignore a lockfile? I'm figuring I can't since we go parse it separately from collecting file inputs.\r\n\r\n I shouldn't be able to, but can I? I'm trying to see if I can trick it into doing so and it doesn't look like it.",
        "```suggestion\n- `turbo.json`\n- Package manager lockfiles\n```",
        "```suggestion\r\n  may want to specify a range (for example, `\">=15\"`) according to your needs.\r\n  \r\n  Additionally, for older package managers, you may need to instruct your package manager to install peer dependencies with configuration, or add the dependency to `devDependencies` as a workaround.\r\n```",
        "```suggestion\r\nTo help with incremental migration or in situations where you can't use the `packageManager` field, you may use `--dangerously-disable-package-manager-check` to opt out of this check and assume the risks of unstable lockfiles producing unpredictable behavior. When disabled, Turborepo will attempt a best-effort discovery of the intended package manager meant for the repository.\r\n```",
        "```suggestion\r\nTo help with incremental migration or in situations where you cannot use the `packageManager` field, you may use `--dangerously-disable-package-manager-check` to opt out of this check and assume the risks of unstable lockfiles producing unpredictable behavior. When disabled, Turborepo will attempt a best-effort discovery of the intended package manager meant for the repository.\r\n```"
      ],
      "turborepo-define-api-boundaries": [
        "```suggestion\r\n/* Add x-artifact-tag header to artifact download endpoint response */\r\n```",
        "My gut reaction is to say no, but I could be talked into saying yes. This has come up _once_ in my memory, or has it been more? I'm hesitant to get into specifics in these sections beyond the feature lists in the pre-stable sections. Imagine stepping onto this page with no prior context...Seeing a specific mention for unstructured terminal output would feel...odd.\r\n\r\nMaybe a broader question: Do other CLI tools consider their stdout/stderr as semver-protected if its meant for human readability? I've never looked into or thought about this...",
        "Aligned, let's omit.",
        "```suggestion\r\n        needed for self-hosted Remote Caches that implement an endpoint that dynamically creates tokens.\r\n```",
        "```suggestion\r\nNote that this example uses the [Just-in-Time Package](/repo/docs/core-concepts/internal-packages#just-in-time-packages) pattern for simplicity. It exports TypeScript directly, but you might choose to use the [Compiled Package](/repo/docs/core-concepts/internal-packages#compiled-packages) pattern instead.\r\n```"
      ],
      "turborepo-framework-specific-entrypoints-organization": [
        "```suggestion\r\n- `./next-js/link`: A customized version of [the Next.js `Link` component](https://nextjs.org/docs/app/building-your-application/routing/linking-and-navigating#link-component) with props that are preset to your organization's preferences\r\n- `./svelte/link`: A customized version of an [`a` tag for Svelte](https://svelte.dev/docs/kit/link-options) with presets.\r\n```"
      ],
      "turborepo-document-cache-strategies": [
        "I wasn't sure if `no Remote Cache activity` is correct. Can someone confirm?",
        "We talked about this to make sure and it is correct!",
        "We need to keep this information. It's important that folks know that no artifacts will be cached."
      ],
      "turborepo-configuration-precision-matters": [
        "I'm upgrading the `engines` in package.json. I don't know if this is contentious or not. I'm seeing multiple versions of CI around the repository so I'm not sure if we consider this value important or not.",
        "As it turns out, we should definitely be doing this. This is now our source of truth."
      ],
      "turborepo-propagate-errors-with-context": [
        "Would it make sense to do:\r\n\r\n```suggestion\r\n    #[error(\"Unable to persist preferences. Please file a bug report.\")]\r\n```"
      ],
      "turborepo-hybrid-monorepo-testing": [
        "```suggestion\r\nYou can combine the benefits of both approaches by implementing a hybrid solution.This approach unifies local development using Vitest's Workspace approach while preserving Turborepo's caching in CI.  This comes at the tradeoff of slightly more configuration and a mixed task running model in the repository.\r\n```",
        "```suggestion\r\n      \"outputs\": [\"coverage/**\"]\r\n```"
      ],
      "turborepo-validate-performance-impact-first": [
        "Making sure to call extra attention here: Users mentioned they felt 1024 was too small, so I doubled it here. Not sure about if there's a reason I shouldn't do this or if a different value should be used. Just did this for the sake of discussion.",
        "Interesting, muchas gracias."
      ],
      "turborepo-link-terms-provide-examples": [
        "In general, we like to provide the link the first time we mention a term. The game I like to play is: Imagine your coworker just linked you this page and you're only glancingly aware that Turborepo is in your company's codebase. You're not terribly familiar with Turborepo but you're here to learn more about it.\r\n\r\nPutting yourself in those shoes, you're likely to read this \"Environment Modes\" term and think \"Uh, what's that?\", and want to be linked to more information in that moment. You could, of course, keep reading and make it to the end of this section, but you'd probably appreciated the link sooner.\r\n\r\nSo, that said, I'd be lifting the link that's a few lines below to here, and removing the sentence on 153. Both a context and conciseness win!",
        "Ah, heard! Ignore then. ðŸ˜„ ",
        "Nice, good call."
      ],
      "turborepo-validate-configurations-comprehensively": [
        "```suggestion\r\n    /// Validates field placement to ensure root-only and package-only fields\r\n    /// are used in the correct configuration types.\r\n    ///\r\n    /// This uses an allowlist approach - ALL fields must be explicitly\r\n    /// categorized.\r\n```",
        "```suggestion\r\n#[error(\"$$ROOT$$ syntax is not allowed in globalDependencies, since globalDependencies is already relative to the root of the Workspace.\")]\r\n```",
        "```suggestion\r\n    #[error(\"\\\"$TURBO_ROOT$\\\" must be used at the start of glob.\")]\r\n```"
      ],
      "turborepo-validate-configuration-structures": [
        "```suggestion\r\n          // TODO: Our code was allowing both config files to exist. This is a bug, needs to be fixed.\r\n```",
        "No worries. Thanks for the tag. Will fix.",
        "Here's the fix: https://github.com/vercel/turborepo/pull/10105\r\n\r\nSorry about that!"
      ],
      "turborepo-eliminate-code-duplication": [
        "Some of my best \"past midnight with a screaming baby\" code, truly."
      ]
    }
  },
  "xrmx": {
    "repos": [
      "open-telemetry/opentelemetry-python"
    ],
    "entries": [
      {
        "slug": "opentelemetry-python-adapt-for-linter-compatibility",
        "title": "Adapt for linter compatibility"
      },
      {
        "slug": "opentelemetry-python-configuration-source-precedence",
        "title": "Configuration source precedence"
      },
      {
        "slug": "opentelemetry-python-explicit-ci-configurations",
        "title": "Explicit CI configurations"
      },
      {
        "slug": "opentelemetry-python-follow-python-naming-conventions",
        "title": "Follow Python naming conventions"
      },
      {
        "slug": "opentelemetry-python-future-proof-api-design",
        "title": "Future-proof API design"
      },
      {
        "slug": "opentelemetry-python-maintain-consistent-naming",
        "title": "Maintain consistent naming"
      },
      {
        "slug": "opentelemetry-python-optimize-code-location-scope",
        "title": "Optimize code location scope"
      },
      {
        "slug": "opentelemetry-python-optimize-configuration-structure",
        "title": "Optimize configuration structure"
      },
      {
        "slug": "opentelemetry-python-pin-dependency-versions",
        "title": "Pin dependency versions"
      },
      {
        "slug": "opentelemetry-python-precise-configuration-specifications",
        "title": "Precise configuration specifications"
      },
      {
        "slug": "opentelemetry-python-prevent-recursive-logging-calls",
        "title": "Prevent recursive logging calls"
      },
      {
        "slug": "opentelemetry-python-return-collections-not-none",
        "title": "Return collections not None"
      },
      {
        "slug": "opentelemetry-python-write-reliable-test-cases",
        "title": "Write reliable test cases"
      }
    ],
    "comments": {
      "opentelemetry-python-explicit-ci-configurations": [
        "Is this intentional?"
      ],
      "opentelemetry-python-configuration-source-precedence": [
        "Good question. Since we have only one value in the options I thought that it would be easier to have the caller decide the behavior and permit to override our defaults if they want or merge themselves. \r\nFor my use case I need to override the value.",
        "Implemented merging in the last commit, PTAL!"
      ],
      "opentelemetry-python-prevent-recursive-logging-calls": [
        "> Wdyt about using warnings instead of logger for other places in which are common to have recursions?\r\n\r\nWe should open an issue and add it to the logging board :)\r\n\r\n> Will this break again if someone is using `logging.captureWarnings()`? https://docs.python.org/3/library/logging.html#integration-with-the-warnings-module\r\n\r\nI guess so but at least we don't ship code using that ourself :)",
        "Opened an issue for handling this more generally here https://github.com/open-telemetry/opentelemetry-python/issues/4261"
      ],
      "opentelemetry-python-future-proof-api-design": [
        "I'm fine with that ",
        "Forgot to bring it at the SIG but pushed this since:\r\n- it has many advantages\r\n- we can revert it if it's a problem",
        "So are you suggesting to add preemptively a custom type for each instrumenting or having the specific dataclass inherit from the common be enough? In other words, is the issue about the name of the type or its structure?",
        "I guess the flat approach is for this very same reason of avoiding breaking compatibility by changing the layout of the advisory",
        "Pushed the conversion to using a flattened attribute just for the histogram in 0b6f9435bba5e444d7bc094e390060ea67631e49, left a couple of FIXMEs around, PTAL!",
        "Kept using the _MetricsHistogramAdvisory dataclass for storing the advisory params internally 32be722277aaba507951528c36eedeb5a623ccda so that mypy is happy again"
      ],
      "opentelemetry-python-write-reliable-test-cases": [
        "Hardcoded",
        "It looks like the version of pypy we are using in ci has some timer precision issues on windows. This issues could have been fixed in 7.3.12 but that release has a baseline of python 3.9.",
        "In general I agree, but here we are adding the race condition on purpose by mocking the wait condition in order to do asserts on the mock calls. Before marking it as flaky I would like to test if adding more slack in timing is helpful"
      ],
      "opentelemetry-python-follow-python-naming-conventions": [
        "Call this something more friendly like `calls` instead?",
        "I think sdk and api should match"
      ],
      "opentelemetry-python-return-collections-not-none": [
        "```suggestion\r\n            process_executable_name: Optional[str] = resource.attributes.get(\r\n```\r\n\r\nCan we assume that we'll have a string there?"
      ],
      "opentelemetry-python-optimize-configuration-structure": [
        "Can't we reuse dev-requirements.txt even if it installs a bit more stuff to avoid creating another place where to keep requirements updated?",
        "I think that is adding a lot more complexity that it is required :cry: ",
        "Question: if you change these in `commands_pre`, should you alse change the stuff with the same name in `commands`?"
      ],
      "opentelemetry-python-optimize-code-location-scope": [
        "Done"
      ],
      "opentelemetry-python-adapt-for-linter-compatibility": [
        "The issue here is that when we comment a variable as in `# MYVARIABLE: Final = 42` then pylint will warn that the following multiline comment (`\"\"\" Deprecated: ... \"\"\"`) is not used. To avoid that we are adding any eventual following comment added to the variable as inline comment (`# Deprecated: ...`) so pylint is happy"
      ],
      "opentelemetry-python-pin-dependency-versions": [
        "We tend to keep this fixed, does it work locally?",
        "Please keep it the way it is or just bump to a fixed version for reproducibility",
        "```suggestion\r\nimportlib-metadata==8.2.0\r\n```\r\n\r\nThis needs to be `==` for being reproducible",
        "If that's the case we use an older version that works everywhere, if that does not exist we use conditionals but still fixed versions please.",
        "Opened https://github.com/open-telemetry/opentelemetry-python/pull/3910"
      ],
      "opentelemetry-python-maintain-consistent-naming": [
        "```suggestion\r\n- Update sdk process resource detector `process.command_args` attribute to also include the executable itself\r\n```",
        "PROCESS_COMMAND_ARGS is a variable name, not the attribute"
      ],
      "opentelemetry-python-precise-configuration-specifications": [
        "It does by default https://docs.astral.sh/ruff/settings/#exclude",
        "```suggestion\r\n    \"importlib-metadata >= 6.0, <= 8.4.0; python_version < '3.10'\",\r\n```\r\n\r\nThis will match the code",
        "We tend to be conservative on importlib-metadata because they don't follow semantic versioning\r\n\r\n```suggestion\r\n    \"importlib-metadata >= 6.0, <= 8.2.0\",\r\n```"
      ]
    }
  },
  "dougwilson": {
    "repos": [
      "expressjs/express"
    ],
    "entries": [
      {
        "slug": "express-access-settings-properly",
        "title": "Access settings properly"
      },
      {
        "slug": "express-accurate-jsdoc-documentation",
        "title": "Accurate JSDoc documentation"
      },
      {
        "slug": "express-clear-array-operations",
        "title": "Clear array operations"
      },
      {
        "slug": "express-clear-intention-in-names",
        "title": "Clear intention in names"
      },
      {
        "slug": "express-enforce-null-safety-patterns",
        "title": "Enforce null safety patterns"
      },
      {
        "slug": "express-ensure-test-completion",
        "title": "Ensure test completion"
      },
      {
        "slug": "express-exclude-sensitive-configurations",
        "title": "Exclude sensitive configurations"
      },
      {
        "slug": "express-follow-standardjs-when-modifying",
        "title": "Follow StandardJS when modifying"
      },
      {
        "slug": "express-handle-streams-properly",
        "title": "Handle streams properly"
      },
      {
        "slug": "express-optimize-hot-paths",
        "title": "Optimize hot paths"
      },
      {
        "slug": "express-propagate-errors-properly",
        "title": "Propagate errors properly"
      },
      {
        "slug": "express-purposeful-style-changes",
        "title": "Purposeful style changes"
      },
      {
        "slug": "express-rest-principles-first",
        "title": "REST principles first"
      },
      {
        "slug": "express-single-source-documentation",
        "title": "Single source documentation"
      },
      {
        "slug": "express-standardize-dependency-version-notation",
        "title": "Standardize dependency version notation"
      },
      {
        "slug": "express-structured-release-workflows",
        "title": "Structured release workflows"
      },
      {
        "slug": "express-use-unique-password-salts",
        "title": "Use unique password salts"
      }
    ],
    "comments": {
      "express-rest-principles-first": [
        "I agree. The `Blob` is just a byte stream, so whatever the `type` is shouldn't be touched, as Express has no way to know what the charset is of the bytestream.",
        "Makes sense. I think that is what this pr is already doing, but if not, it shouldn't set the type if already set, which is what all the other arguments to send do.",
        "I'm not sure why this was marked resolved, as nothing was fixed and the issue @jimmywarting mentioned with adding charset is still there.",
        "We cal always be more restrictive than the spec and then loosen up later (but not the other way around), so alphanumerics would at least be a start :)\n"
      ],
      "express-enforce-null-safety-patterns": [
        "Probably don't want to reassign values linked to arguments, since it silently alters `arguments[0]`. Usually I would just name the argument `options` and this line as `var opts = options || {}`\r\n```\r\n$ node -pe '(function(foo){foo=foo||{};return arguments[0] === null}(null))'\r\nfalse\r\n```",
        "Usually not best practice to alter the object someone is passing in; the object could potentially be frozen, even, causing an exception here. `var createServer = opts.createServer || http.createServer` is probably fine (and then changing the vars below).",
        "Pleas do not use in for option detection. This makes the interface really confusing because you have to ensure that whatever you are passing in does not have that key anywhere in the prototype chain.\n",
        "Do not pass in non-booleans to the sub module, because that is just asking for undefined behavior later. The etag option only accepts true or false.\n"
      ],
      "express-standardize-dependency-version-notation": [
        "Be sure to keep the `~` symbol; if a 2.0.7 is published, then a user upgrading from a 4.17 version of a 4.18 version would end up downgrading the `proxy-addr` installed.",
        "You cannot use `^` in our `package.json` file.\n",
        "`^` is not compatible with all the base installed for 0.10.x. people would have to upgrade their npm just to use express, which is not always possible in certain enterprise environments. plus we don't only hard-pin external dependencies since it's too easy for them to break people's express otherwise\n"
      ],
      "express-clear-array-operations": [
        "`[key].concat(fns)` will inadvertinately allow for the arguments to be arrays, because it'll flatten in certain cases, i.e.:\n\n``` js\n$ node -pe '[2].concat([1,2])'\n[ 2, 1, 2 ]\n```\n"
      ],
      "express-purposeful-style-changes": [
        "I know the PR was updated already, but I figured I'll reply for just information sharing: the short answer is no, but the long answer is that PRs are accepted as long as they provide some kind of merit; for example, if there is a demonstrable perf improvement."
      ],
      "express-use-unique-password-salts": [
        "> here '10' is the salt used for encryption\r\n\r\nI thought your PR said you don't need a salt? Can you vlarify this comment and/or PR description?",
        "Gotcha. So then the comment right above this seems misleading/confusing:\r\n\r\n> A better Way to Hash Password without passing any salt using Bcrypt.Js\r\n\r\nit says \"without passing any salt\", but this line's comment says \"here '10' is the salt\""
      ],
      "express-clear-intention-in-names": [
        "`this.getHeader(\"transfer-encoding\")` should be `this.get(\"Transfer-Encoding\")`",
        "It is, but the change here is to use `this.get`, not `this.getHeader`. In addition, they are both case-insensitive, but we use the standard header casing in the code (as you can see from all the other parts).",
        "They are private, which is why they are not documented and the testing for them is tested though the public APIs that utilize them ðŸ‘ ",
        "Yes, it is curious ðŸ˜€ They are before my time so cannot answer the why, lol. Even an underscore doesn't actually stop folks from using stuff, and a large project even ends up needing to be csreful modifying those things. Really only way to protect is an inside out obect, clever usage of closures, or the newer private class members. Idk if 5.x will change it, as it is late in the dev on it and probably need to scout round for the usages to make sure they are still provided in some way and truely private them in like 6.\r\n\r\nI bet people are accessing settings directly, at least. Perhaps too cache if they wanted to clear it since there is no API to do so.",
        "This should probably have a different name, otherwise people will try to `app.dispatch(req, res)` with it.\n"
      ],
      "express-handle-streams-properly": [
        "Should we add a code path here to use https://nodejs.org/api/buffer.html#blobstream when supported? Not sure how usable that is, but seems Node.js is adding the ability to make a Blob from a file which could be huge.",
        "I'm not sure why a custom write stream needs to be added, but it is missing backpressure handling, which is very important in http servers like this because the clients can stop reading if they know this is a large response, leading to a dos vector. But I would think you don't need to implement that at all, as you should be able to use `.toWeb()`/`.fromWeb()`, right? I ask because I'm not familiar with the new APIs for web streams (yet). So either we should use that or this custom stream needs to have backpressure handling added",
        "Just to note, if it doesn't work or make sense to land in the 4.x line due to back compat, that's no problem, as we have the 5.x line thay doesn't have that concern and as soon as I finish getting out a sec path for a diff module, the last 5.x pre release will be published so we can publish the final.",
        "`this.getHeader(\"transfer-encoding\")` should be `this.get(\"Transfer-Encoding\")`",
        "remove your aborted listener when done\n"
      ],
      "express-follow-standardjs-when-modifying": [
        "I would just leave this off the pr since it's a style only change.",
        "All net new code should be written in StandardJS style per https://github.com/expressjs/express/blob/master/Collaborator-Guide.md#prs-and-code-contributions",
        "Typically we do not indent these lines, keeping the `.` at the same indent as the function call. Can you revert all the re-formatting of the file that wasn't related to your changes?"
      ],
      "express-structured-release-workflows": [
        "I'm not sure if this was the way it has ever been done. Typically there are two main release flows: patch and non-patch.\n\nThe patch flow is pretty simple: it's just extremely simple updates, like typo fixes, patch dependency updates, and maybe a fix, depending on how risky it is. Every other change is always a non-patch flow.\n\nThe non-patch flow is done using a separate branch (for example `4.13`, `5.0`, etc.) and tracked using a release pull request (for example, https://github.com/expressjs/express/pull/2682, https://github.com/expressjs/express/pull/2237, etc.).\n"
      ],
      "express-accurate-jsdoc-documentation": [
        "This jsdoc does not reflect what you are doing. It documents the call as .param('id', [fn1, fn2])\n",
        "We had a PR about this right now, so we should make sure we land it right this time :) The `options` argument needs to be optional in the JSDoc here since it's optional in the code ðŸ‘ "
      ],
      "express-access-settings-properly": [
        "I would expect that if the user explicitly set the `etag` option to `res.sendFile`, it would not be overwritten by the setting.\n",
        "You should use the setting functions to access settings, not direct access to the object. Direct object access was deprecated in 3.0.\n"
      ],
      "express-single-source-documentation": [
        "Should this section link to https://github.com/expressjs/express/blob/master/Contributing.md#tc-process , just replace that part of that other document, or something else? It seems like a duplication and could easily get lost where we make a change to one but forget the other, so the less duplication probably the better.",
        "All of these \"you\"s need to be re-worded in to the third person from the second person, as the document is meant to be in the third person form."
      ],
      "express-exclude-sensitive-configurations": [
        "Since certs have an expiration date, I would suggest not checking them in and either leave it as an exercise for the user running the example to create it with the instructions you provided or use one of those npm modules that auto generate them."
      ],
      "express-ensure-test-completion": [
        "without an `else` here, a failure of the check leaves the test suite hanging. if you are testing the body contents, just use supertest to do it rather than manually checking res.body which would simplify it.",
        "Darn. That particular tests doesn't do much at all. It seems like you likely just need to blanket increase the timeout for the environment you're running in. Are you about to run the test suite like the following? `npm test -- --timeout 7500` ?",
        "Ah, I see, there is a an issue with npm passing arguments around. Any reason you are using `npm run test-ci` ? That is meant just for the usage of the GitHub Actions CI here. Why not just use `npm test` ?",
        "Since this is not an async test, you don't need to use `done` here and just throw the errors",
        "use mocha, not try-catch\n"
      ],
      "express-propagate-errors-properly": [
        "If there is an error, it should ideally be forwarded to the error handling pipeline rather than swallow and just ending the response wirh no info.",
        "It should forward the error to the error handling pipeline, not just output the error directly. May sites what users to see a customized error page and have the error written to their logs, not displayed to the end user. The error pipeline (through error handling middlewares) allows this to be customized.",
        "You probably need to remove this handlers before proceeding. The error handler itself that `next` may try to write to `res` and cause an error, and this would end up involing `next` a second time.",
        "Also on this res error, it probably means we need to stop the pipe from the blob so it doesn't keep writing to res. ",
        "I'm not sure if the examples should demonstrate that any possible error above should result in a 400. What if the data source to get the note has an issue (like cannot connect to db)? One would normally expect it to 500 rather than 400. I also wonder if these should be using the Express error handling mechanism to respond rather than demo putting a try...catch in every single handler (except `getAll` that doesn't need one for some reason)? The reason I ask is because ideally we want to demo using Express.js and it's features in our demo apps as much as possible.",
        "the aborted error should only go to `fn` if there is one; never to `next`. just change it so it doesn't go to `next` :)\n"
      ],
      "express-optimize-hot-paths": [
        "For any Node.js version where `require('stream/web')` doesn't work, this is going to cause a sync walk of the file system every time, since a `require()` failure is not added to the module cache like a success is. This would ideally be moved to only happen on the loading of this module, not on every single response write.",
        "this creating fake contexts is expensive; could use refactoring somewhere to not need to do this.\n"
      ]
    }
  },
  "myrrc": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-adaptive-cache-expiration-strategy",
        "title": "Adaptive cache expiration strategy"
      },
      {
        "slug": "neon-avoid-flaky-tests",
        "title": "Avoid flaky tests"
      },
      {
        "slug": "neon-clear-consistent-identifier-names",
        "title": "Clear consistent identifier names"
      },
      {
        "slug": "neon-comprehensive-code-documentation",
        "title": "Comprehensive code documentation"
      },
      {
        "slug": "neon-document-parameter-choices",
        "title": "Document parameter choices"
      },
      {
        "slug": "neon-environment-specific-config-defaults",
        "title": "Environment-specific config defaults"
      },
      {
        "slug": "neon-extract-and-reuse",
        "title": "Extract and reuse"
      },
      {
        "slug": "neon-keep-files-focused-small",
        "title": "Keep files focused small"
      },
      {
        "slug": "neon-minimize-unnecessary-allocations",
        "title": "Minimize unnecessary allocations"
      },
      {
        "slug": "neon-optimize-cargo-dependencies",
        "title": "Optimize cargo dependencies"
      },
      {
        "slug": "neon-optimize-what-matters",
        "title": "Optimize what matters"
      },
      {
        "slug": "neon-proper-metrics-design",
        "title": "Proper metrics design"
      },
      {
        "slug": "neon-proper-option-type-usage",
        "title": "Proper Option type usage"
      },
      {
        "slug": "neon-scope-jwt-authentication-tokens",
        "title": "Scope JWT authentication tokens"
      },
      {
        "slug": "neon-stage-configuration-changes-gradually",
        "title": "Stage configuration changes gradually"
      },
      {
        "slug": "neon-structure-endpoints-for-rest",
        "title": "Structure endpoints for REST"
      }
    ],
    "comments": {
      "neon-optimize-what-matters": [
        "Yeah, that's why I want to gate it",
        "A very minor thing, but as `threshold = UINT64_MAX` only for last bucket, we can loop from 0 to `NUM_QT_BUCKETS - 1`, write last iteration manually and avoid branching in assigning `bucket_le`"
      ],
      "neon-stage-configuration-changes-gradually": [
        "We need to parse GUC in case ComputeSpec's fields are missing for backward compatibility",
        "Fixed",
        "Fixed"
      ],
      "neon-keep-files-focused-small": [
        "Fixed"
      ],
      "neon-environment-specific-config-defaults": [
        "We should vendor these changes once PR gets ready for review, downloading data from Internet in tests is a bad idea"
      ],
      "neon-avoid-flaky-tests": [
        "```suggestion\r\n@pytest.mark.skipif(not USE_LFC)\r\ndef test_lfc_prewarm(neon_simple_env: NeonEnv):\r\n```"
      ],
      "neon-optimize-cargo-dependencies": [
        "I see there are a lot of Cargo.lock changes which may not be needed.\r\nTry this to possibly reduce the diff\r\n```\r\ngit checkout origin/main -- Cargo.lock\r\ncargo hakari manage-deps\r\ncargo hakari generate\r\ncargo b --locked --frozen\r\n```"
      ],
      "neon-minimize-unnecessary-allocations": [
        "nit: May we use `itertools::join` here to avoid constructing a separate Vec?",
        "Also nit: if some of privileges may already be granted, maybe we should \r\n1. Take an iter of privileges\r\n2. Filter out those contained in `already_granted`?\r\n3. If iterator is non-empty, join using itertools and grant?",
        "Fixed"
      ],
      "neon-document-parameter-choices": [
        "Fixed"
      ],
      "neon-extract-and-reuse": [
        "This check and loop are further used in other functions, can we extract them to a separate helper function?"
      ],
      "neon-adaptive-cache-expiration-strategy": [
        "We already have a service for prewarm (endpoint_storage) in neon/"
      ],
      "neon-proper-metrics-design": [
        "I thing `counter` is better since \"number of autovacuum runs\" is non-decreasing"
      ],
      "neon-clear-consistent-identifier-names": [
        "Fixed"
      ],
      "neon-proper-option-type-usage": [
        "Lsn isn't discriminated against Lsn::INVALID so Option<> would increase its size, not sure it's the best option",
        "On the other hand, making it a discriminant (or Lsn:::MAX) isn't an option as well, so I'll use your approach."
      ],
      "neon-scope-jwt-authentication-tokens": [
        "```suggestion\r\n## Authentication and authorization\r\n1. Control plane should generate a JWT token which would be used by compute for authorizing requests\r\nto S3 proxy. This token should be passed in `/compute/api/v2/computes/{id}/spec` route,\r\nparsed by `compute_ctl` as an optional field (to preserve backward compatibility) and passed as-is\r\nto proxy requests during prewarm or prewarm offload. If no token is passed, requests to proxy\r\nshould return 505 Not Supported (other services have `--dev` flags which disable token check or\r\nconfig fields which bypass authentication when absent but that's error-prone) and log the error.\r\n\r\n2. S3 proxy should have `auth_public_key.pem` similar to pageserver\r\nfor spawning an https server for compute requests (this would also further help in getting\r\nPCI-DSS certification) and verifying compute requests. The pemfile should be supplied by\r\ninfra/. As with pageserver, S3 proxy should have `/reload_auth_validation_keys` endpoint\r\nto reload the pemfile from config should it change.\r\n\r\n## Metrics\r\n\r\nIn addition to changing `compute_ctl`'s /status, proxy should provide request duration\r\nmetrics along with result server codes as labels\r\n```\r\n",
        "I believe Matthias's RFC focuses more on the high-level overview, but ok, I'll copy the comments. Btw this will probably be split into tasks for https://github.com/neondatabase/cloud/issues/24225 (see sub-issues)"
      ],
      "neon-structure-endpoints-for-rest": [
        "Fixed",
        "Fixed, made POST",
        "success doesn't allow you to omit the body"
      ],
      "neon-comprehensive-code-documentation": [
        "Yeah, a comment typo. It's None when we prewarm from ourselves' data, will fix in promotion PR.",
        "`//!`?"
      ]
    }
  },
  "hiltontj": {
    "repos": [
      "influxdata/influxdb"
    ],
    "entries": [
      {
        "slug": "influxdb-avoid-flaky-test-patterns",
        "title": "Avoid flaky test patterns"
      },
      {
        "slug": "influxdb-centralize-workspace-configurations",
        "title": "Centralize workspace configurations"
      },
      {
        "slug": "influxdb-choose-appropriate-lock-primitives",
        "title": "Choose appropriate lock primitives"
      },
      {
        "slug": "influxdb-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "influxdb-clear-configuration-parameters",
        "title": "Clear configuration parameters"
      },
      {
        "slug": "influxdb-descriptive-semantic-naming",
        "title": "Descriptive semantic naming"
      },
      {
        "slug": "influxdb-document-complete-data-flows",
        "title": "Document complete data flows"
      },
      {
        "slug": "influxdb-follow-api-conventions",
        "title": "Follow API conventions"
      },
      {
        "slug": "influxdb-handle-errors-by-criticality",
        "title": "Handle errors by criticality"
      },
      {
        "slug": "influxdb-manage-complete-cache-lifecycle",
        "title": "Manage complete cache lifecycle"
      },
      {
        "slug": "influxdb-minimize-critical-path-allocations",
        "title": "Minimize critical path allocations"
      },
      {
        "slug": "influxdb-performance-conscious-metrics-implementation",
        "title": "Performance-conscious metrics implementation"
      },
      {
        "slug": "influxdb-prefer-explicit-nullability",
        "title": "Prefer explicit nullability"
      },
      {
        "slug": "influxdb-promote-code-clarity",
        "title": "Promote code clarity"
      },
      {
        "slug": "influxdb-secure-token-lifecycle",
        "title": "Secure token lifecycle"
      },
      {
        "slug": "influxdb-stable-schema-identifiers",
        "title": "Stable schema identifiers"
      },
      {
        "slug": "influxdb-use-structured-logging-fields",
        "title": "Use structured logging fields"
      }
    ],
    "comments": {
      "influxdb-secure-token-lifecycle": [
        "Is it possible to assert that the new token is assigned a new unique `TokenId` from the previous deleted token? The catalog repository type should handle this but having a check in place would be good.\r\n\r\nWe also do hard deletion in other places (LVC, DVC, and triggers) and may not have such a check in place, but I think it is worth adding for tokens here if possible, given sensitive nature.",
        "Does this work because the `_admin` token has been deleted?\r\n\r\ni.e., normally this request to create the token would require a valid auth token, but the `_admin` token has been deleted, and there are no tokens to authenticate, so the create token request is allowed.",
        "Ah, right, that makes sense. Thanks for clarifying."
      ],
      "influxdb-promote-code-clarity": [
        "Could you move this code into a helper function since it is re-used in several places.",
        "Good call, I think I could do this by passing forward the `Arc<TableDefinition>` to the `MetaCacheExec` so it only pulls the column names when needed, e.g., for the `EXPLAIN` output. The table definition is already there so should not be too tricky, and yeah - given that part of the motivation for this whole execution plan implementation is to make it readable to the operator so I will get this in.",
        "https://github.com/influxdata/influxdb/issues/25582",
        "Yep, that works! I will apply it locally.",
        "Done in https://github.com/influxdata/influxdb/pull/25389/commits/71daada9884066ec8a2f7b2a1ac33d90fbf4cd99",
        "I moved out the system table related code to its own module in https://github.com/influxdata/influxdb/pull/25166/commits/7c1f4db1eea45f8b12cc6a013260fc668c658fc9 - ðŸ§¹ definitely cleaner.\r\n\r\nNo code was changed and CI is âœ… so I will get this merged."
      ],
      "influxdb-minimize-critical-path-allocations": [
        "Since this gets called in the write path for each line, might be worth returning a `Vec<&str>` or `Vec<Arc<str>>` to avoid the string copies. Or even a slice, if possible.\r\n\r\nFurthermore, having to do the lookup by ID for the name every time could also be avoided by holding the `Arc<str>` names around in the `TableDefinition` then just iterating over those directly.\r\n\r\nDepending on how far you take it, this could lead to a substantial change if you had to change the `TableDefinition` struct, so might be better for a follow-on if that's the case.",
        "This function no longer builds a new arrow schema on _every_ call. As a result, the arrow schema will only ever need to be rebuilt when new fields are added for caches that accept new fields, _or_ never for caches that have an explicit set of value columns. Furthermore, for the explicit case, it produces the value columns by iterating directly on the cache `IndexMap`, instead of iterating over the schema. The non-explicit case still needs to iterate over the schema and do a lookup to get column ID. Therefore, I suspect the explicit case will be considerably more performant.",
        "Changing this to use a `HashMap` instead of `BTreeMap` for faster lookups."
      ],
      "influxdb-clear-configuration-parameters": [
        "The default should already have been 1 day: https://github.com/influxdata/influxdb/blob/e4cfbf71f78ce44072ec48f41b83721af6d21799/influxdb3_catalog/src/log/versions/v2.rs#L568\r\n\r\nI guess this is just for the purpose of documentation? Since, the coded default doesn't appear in the CLI output anywhere."
      ],
      "influxdb-centralize-workspace-configurations": [
        "Might be good to keep the feature set at the workspace level, then don't need to set it in each individual cargo file.",
        "To clarify, it was previously set in the workspace `Cargo.toml`. Having it there could save someone a few head scratches if they pull one of the `v3` featured crates and find things broken, and I'm not sure we would ever have a crate that depends on one of these and does not use the feature."
      ],
      "influxdb-follow-api-conventions": [
        "FWIW there is a [`Time::checked_sub` API](https://github.com/influxdata/influxdb3_core/blob/fd0e474a6c0af5ba867399d753f5df18f59907cb/iox_time/src/lib.rs#L149-L155) that you could use here to directly subtract the retention period `Duration` from `self.time_provider.now()`.",
        "Might be a case worthy of [`str::rsplit_once`](https://doc.rust-lang.org/std/primitive.str.html#method.rsplit_once) so that for, e.g., `foo:bar:apiv3_...`, it will only take `apiv3_...`.",
        "Several types are duplicated in this crate, e.g., `Format`, from the `influxdb3_server` crate. Perhaps we should pull `influxdb3_server` as a dependency of `influxdb3_client` and re-use them directly, or move the types to a central crate.\r\n\r\nBut this has bit us a couple times now.",
        "https://github.com/influxdata/influxdb/issues/24672",
        "With `reqwest`, you can use the [`query` API](https://docs.rs/reqwest/latest/reqwest/struct.RequestBuilder.html#method.query) to set the param, e.g.,\r\n```suggestion\r\n    /// Make a request to the `DELETE /api/v3/configure/database?db=foo` API\r\n    pub async fn api_v3_configure_db_delete(&self, db: impl AsRef<str> + Send) -> Result<()> {\r\n        let api_path = \"/api/v3/configure/database\";\r\n\r\n        let mut url = self.base_url.join(api_path)?;\r\n\r\n        let mut req = self.http_client.delete(url).query(&[(\"db\", db.as_ref())]);\r\n```"
      ],
      "influxdb-performance-conscious-metrics-implementation": [
        "This will likely be an issue, since the HTTP endpoint that serves prometheus (`/metrics`) assumes a single registry: https://github.com/influxdata/influxdb/blob/be25c6f52b046e57ec909b815e5471d4c6bb4f19/influxdb3_server/src/http.rs#L734-L740\r\n\r\nIs the issue that using the same registry for multiple executors causes them to overwrite each other, or contend for locks with each other?",
        "Opened https://github.com/influxdata/influxdb/issues/25696"
      ],
      "influxdb-document-complete-data-flows": [
        "A useful addition to this diagram would be to show the entry point for writes from user, i.e., where do writes go from the user (`wal buffer`?), via an arrow. Otherwise, it is not clear on the order of operations. If you could connect the numbers from the steps described below to locations / arrows on the diagram, that would be helpful."
      ],
      "influxdb-handle-errors-by-criticality": [
        "These unwraps can probably be changed to errors, but if any of these fails, it means that there is a race condition, so it might be that panicking is the right thing.",
        "FWIW - I think at most it would be `warn!`, since telemetry not sending is not a critical, _wake up your engineers in the middle of the night_ kind of error ðŸ˜† \r\n\r\nI have been fairly loose with the use of `error!` myself, which probably needs to be revisited."
      ],
      "influxdb-choose-optimal-data-structures": [
        "Using [`indexmap`](https://github.com/indexmap-rs/indexmap) makes this assertion (and the use of `insta` snapshots for serialization tests) possible.",
        "Maybe add a note on why `IndexSet` was used, i.e., for fast lookup in addition to iteration to the doc comment.",
        "Since I insert columns into the cache using the ordering of fields in the `schema` ([here](https://github.com/influxdata/influxdb/pull/25109/files#diff-6740f1021d631fa570625b9b27f8b4692fd0777c50eabcc435cda7793d0da049R155-R159)), then when producing a record batch out of the cache ([here](https://github.com/influxdata/influxdb/pull/25109/files#diff-6740f1021d631fa570625b9b27f8b4692fd0777c50eabcc435cda7793d0da049R184-R193)), `IndexMap` allows to iterate over the map directly while producing the correct order of columns for the schema.",
        "The links in that comment look to be out-of-date. But I have added docs/comments in the code to help explain this.",
        "@pauldix - while working on https://github.com/influxdata/influxdb/pull/25125 to enable caches that add new fields, I have found that the insertion order guarantee falls apart in scenarios where writes come in with different fields/orders for different key values. So, we can't rely on that, and I will be removing the comments about insertion order.\r\n\r\nI still like the use of `IndexMap` because it gives fast iteration over keys, as well as fast lookups (see [here](https://github.com/indexmap-rs/indexmap?tab=readme-ov-file#performance)), but if we want to avoid the dependency, or just optimize for lookup speed, then we could use a `HashMap` here.",
        "I guess you avoid the `entry` API on `buffered_data.database_buffers` here in order to avoid cloning `db_name` on every access?\r\n\r\nWe could consider using the `hashbrown` crate for its `HashMap` impl (which gets used here and there in IOx) and has the [`entry_ref` API](https://docs.rs/hashbrown/latest/hashbrown/struct.HashMap.html#method.entry_ref). I think there are a few places in this code where we could leverage that - I think introducing `hashbrown` can be a separate issue/PR though."
      ],
      "influxdb-use-structured-logging-fields": [
        "It would be useful to log the `db_name` and `duration` provided.",
        "It would be useful to log the `db_name`",
        "```suggestion\r\n                info!(instance_id = ?instance_id, \"catalog not found, creating new instance id\");\r\n```\r\nJust encouraging use of `tracing`'s field syntax.\r\n\r\nThis is somewhat redundant with the `info!` emitted at the caller level, but I don't think it hurts to have.",
        "This will show up in the logs as `e=<error message>`, I would either format it inline or use a more descriptive name, e.g.,\r\n```rust\r\nerror!(error = %e, \"...\");\r\n```\r\nOr rename the `e` to `error`, such that it appears as `error=<error message>` in the logs."
      ],
      "influxdb-stable-schema-identifiers": [
        "I don't know if using `enumerate` to determine the column ID is a good idea. I think that generally, columns are always appended, in which case, it is okay, but in the event that we allow for dropping columns, then this would change their order and mess up the IDs.\r\n\r\nWe probably need some way to generate the IDs, based on what was the largest already used ID for a given table, and then ensure that that ID remains fixed for the column it is applied to for all time.",
        "This could be done by flipping the hashmap to\r\n```rust\r\nHashMap<TableId, (Arc<str>, TableChunks)>\r\n```\r\nSerde would then `Serialize`/`Deserialize` it gracefully using derive.\r\n\r\nNot sure how gracefully that fits in to the broader change set but it would certainly be nice to not need the custom serialization code.\r\n\r\nIf you need to have both the name and ID in the hash key then you could define a new-type, e.g.,\r\n```rust\r\n#[derive(/* ... */, Serialize, Deserialize)]\r\nstruct WriteBatch {\r\n    /* ... */\r\n    table_chunks: HashMap<TableKey, TableChunks>,\r\n}\r\n\r\nstruct TableKey(Arc<str>, TableId);\r\n```\r\nThen, implement `Serialize`/`Deserialize` on `TableKey`, vs. having to do it for the whole `WriteBatch` type.",
        "JSON doesn't have a tuple, but `serde_json` will serialize/deserialize tuples as lists (see [here](https://docs.rs/serde_json/latest/src/serde_json/ser.rs.html#303-305)). I think part of the issue would be using `TableId` as the key in the map, since JSON doesn't support integer map keys. For that we would either need to de/serialize `TableId`s as strings, or not use JSON.",
        "Should this take the next available table ID, instead of using 0?",
        "(the same would be said for the other parsing function)",
        "Ah, I see, the code in validator is a bit confusing right now. For example, it used to update the in-memory catalog schema using a `Cow` to check that it had changes which is why this is here: https://github.com/influxdata/influxdb/blob/9c71b3ce251f32cf8f23db0a4f09873e04686c1a/influxdb3_write/src/write_buffer/validator.rs#L514-L521\r\n\r\nBut it doesn't look like it uses the cow anymore. It does still update the catalog in its state here, though, by applying the catalog batch: https://github.com/influxdata/influxdb/blob/9c71b3ce251f32cf8f23db0a4f09873e04686c1a/influxdb3_write/src/write_buffer/validator.rs#L182-L196\r\n\r\nAnd I believe that will result in the table being created, so I think it should probably be using the next ID instead of 0. Perhaps a test would help suss that out.",
        "Need to handle for collisions here, i.e., to ensure there are no duplicate fields, and also check for ordering. I don't think that race conditions are a concern since this method is invoked from a single event loop, and is processing rows from the buffer sequentially.\r\n\r\nA good test would be to have a cache on the table `foo` with keys `[t1]` and values `[f1, time]`, and then write the following LP that would add `f2` and `f3` value columns:\r\n```\r\nfoo,t1=a f1=1,f2=2,f3=3\r\nfoo,t1=b f1=1,f3=3,f2=2\r\n```\r\nAssuming that `t1=a` and `t1=b` have already been written to, and therefore each have a cache associated; each cache will have the new value columns `f2` and `f3` added, but they will be added in a different order to each respective cache:\r\n```\r\nt1=a -> f2,f3\r\nt1=b -> f3,f2\r\n```\r\nThen check that `RecordBatch`es spanning both `t1` key values can be produced and combined.",
        "Something worth noting is that the write buffer is validating the incoming writes, so we shouldn't have to worry about new fields being written with incompatible types in subsequent lines of LP. I do think ensuring that ordering discrepancies like above should be handled.",
        "https://github.com/influxdata/influxdb/pull/25125/commits/a129d003397bbb859a112980aee3de6286d1adcf switched to using `SchemaBuilder::try_merge` to prevent conflicts, and with the added test case, the above scenario will not cause issues.\r\n\r\n_Edit_: that commit did not produce the correct behaviour ðŸ¤¦",
        "https://github.com/influxdata/influxdb/pull/25125/commits/2917fc149766ac349a4cb297f86ea7dfb6395797 Looks to have resolved this."
      ],
      "influxdb-manage-complete-cache-lifecycle": [
        "This test was removed because cache creation no longer behaves the same as before - previously if you made the same request to create a cache twice, the second one would succeed with a `204 NO CONTENT`, but have no effect; now, the second one fails with a `409 CONFLICT`.",
        "Yeah, only if it needs to prune would lock the inner map, but I agree, this is a little over-zealous.",
        "Each of these functions acquires their own write lock. Originally, they were called from separate places, but if they always get called at the same time like this, it might be better to do the evict and write in the same call, under the same write lock.",
        "> I think it should be changed to only do eviction on snapshot, that way we're not spending too much time on it.\r\n\r\nThat sounds reasonable.",
        "Yes, good call, this only removes the values and is not walking up and cleaning up the maps. I'll address that with the other immediate issues in a follow-on PR."
      ],
      "influxdb-prefer-explicit-nullability": [
        "This is a textbook use case for [`NonZeroUsize`](https://doc.rust-lang.org/std/num/type.NonZeroUsize.html), i.e.,\r\n```rust\r\n/// Must be greater than 0\r\n#[derive(Debug, Serialize, Eq, PartialEq, Clone, Copy)]\r\npub struct LastCacheSize(pub(crate) NonZeroUsize);",
        "No worries. We can refactor this later",
        "When I first refactored this, I had the `LastCacheKey` hold a `HashMap<Option<KeyValue>, LastCacheState>` to handle null key values, but switched to this behaviour to simplify it.\r\n\r\nI think It just needs to do that, i.e., use `Option<KeyValue>` instead of `KeyValue`, and then store the datatype in the `LastCacheKey`, because 1) key columns will always have a fixed data type, and 2) you can't rely on the `KeyValue` alone to get the data type when it is `None` (for creating `RecordBatch`es).\r\n\r\nI can open up an issue for this.",
        "The [`is_valid` function](https://docs.rs/arrow/latest/arrow/array/trait.Array.html#method.is_valid) indicates whether the value at given index is null or not - the data is still _valid_, if that makes sense. So, instead of `bail!`ing here, I think you can just `continue;` in the inner loop, leaving the cell as `Value::Null`."
      ],
      "influxdb-descriptive-semantic-naming": [
        "Addressed this in https://github.com/influxdata/influxdb/pull/25722/commits/cd51bc2beda9a23446d694ee411e409871b1de39"
      ],
      "influxdb-avoid-flaky-test-patterns": [
        "Yeah, `insta` is compelling but not the  right tool here. I think @waynr was grappling with a similar issue in clustered so I may pick his brain to see if he landed on a solution.",
        "I opened https://github.com/influxdata/influxdb/issues/25493"
      ],
      "influxdb-choose-appropriate-lock-primitives": [
        "I agree, I opened https://github.com/influxdata/influxdb/issues/25382 to look for alternatives.\r\n\r\nI think most straight-up LRU implementations will have this problem, given that they need to update the recency when getting an item (the popular [`lru` crate](https://crates.io/crates/lru/) that `clru` is based on is essentially the same API).",
        "Addressed in https://github.com/influxdata/influxdb/pull/25377/commits/ac8d7d3ba9ce25cf0dd04b4471e955e8ae4e1790",
        "Might be better to do the remove in `set_success` directly, so that it does it all under one lock.",
        "DashMap could definitely be useful here. I actually was using it at one point, so maybe I will re-introduce. I want to read more about how it works though.",
        "I switched over to using `DashMap` which hopefully will help reduce some lock contention.",
        "I changed it so that there is only one top-level lock, and now it is a three-level hashmap, so that it also stores multiple caches per table."
      ]
    }
  },
  "pauldix": {
    "repos": [
      "influxdata/influxdb"
    ],
    "entries": [
      {
        "slug": "influxdb-avoid-flaky-test-patterns",
        "title": "Avoid flaky test patterns"
      },
      {
        "slug": "influxdb-choose-appropriate-lock-primitives",
        "title": "Choose appropriate lock primitives"
      },
      {
        "slug": "influxdb-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "influxdb-clear-configuration-parameters",
        "title": "Clear configuration parameters"
      },
      {
        "slug": "influxdb-descriptive-semantic-naming",
        "title": "Descriptive semantic naming"
      },
      {
        "slug": "influxdb-document-complete-data-flows",
        "title": "Document complete data flows"
      },
      {
        "slug": "influxdb-follow-api-conventions",
        "title": "Follow API conventions"
      },
      {
        "slug": "influxdb-handle-errors-by-criticality",
        "title": "Handle errors by criticality"
      },
      {
        "slug": "influxdb-include-explanatory-examples",
        "title": "Include explanatory examples"
      },
      {
        "slug": "influxdb-manage-complete-cache-lifecycle",
        "title": "Manage complete cache lifecycle"
      },
      {
        "slug": "influxdb-minimize-critical-path-allocations",
        "title": "Minimize critical path allocations"
      },
      {
        "slug": "influxdb-performance-conscious-metrics-implementation",
        "title": "Performance-conscious metrics implementation"
      },
      {
        "slug": "influxdb-prefer-explicit-nullability",
        "title": "Prefer explicit nullability"
      },
      {
        "slug": "influxdb-promote-code-clarity",
        "title": "Promote code clarity"
      },
      {
        "slug": "influxdb-stable-schema-identifiers",
        "title": "Stable schema identifiers"
      },
      {
        "slug": "influxdb-vet-security-critical-dependencies",
        "title": "Vet security-critical dependencies"
      }
    ],
    "comments": {
      "influxdb-promote-code-clarity": [
        "I think this would be clearer if the `cleanup_after_snapshot` method took the separate arguments (i.e. not an Option of a tuple). That way you can put the check up here. Meaning, we show that what we're doing here is flushing the buffer. And if, the return from that is `Some(...)` then it means we have a snapshot to cleanup after.\r\n\r\nSo we call the `cleanup_after_snapshot` function with this signature: `self: Arc<Self>, snapshot_finished_reciever: oneshot::Receiver<SnapshotDetails>, snapshot_info: SnapshotInfo, snapshot_permit: OwnedSemaphorePermit`.\r\n\r\nThe separation and the argument names will make it a little more clear what's going on here.",
        "Better to make this a function on `Catalog` rather than going into its internals to get it.",
        "Might be good to pull this out of the query_executor and into its own file. Create a structure for different system tables to be defined. I imagine we'll have a number of these over time."
      ],
      "influxdb-minimize-critical-path-allocations": [
        "Has this been tested with high query concurrency to verify that this isn't a performance regression?",
        "yeah, returning the series key should be as cheap as possible. Given that every table now has one, it should just be a reference that gets returned (or an Arc'd thing?)."
      ],
      "influxdb-clear-configuration-parameters": [
        "Can we rename to `query-file-limit`? I think it's better to be more descriptive about what this file limit is about.",
        "should be renamed to `snapshotted-wal-files-to-keep`. It's not the actual total number of WAL files, it's the number that have been snapshotted. So if you have 200 files that haven't been snapshotted and this setting is 50, you'd have 250 total files at that point.",
        "Not sure if we want to change this here, but I've logged #25735 to be able to create tables without specifying any fields.",
        "This could also use help that indicates what fields should look like if provided, and what the valid types are.",
        "Looking at the help for the delete commands it seems it's a bit off:\r\n\r\n```\r\ninfluxdb3 delete table -h\r\nCreate a new table in a database\r\n\r\nUsage: influxdb3 delete table [OPTIONS] --database <DATABASE_NAME> <TABLE_NAME>\r\n\r\nArguments:\r\n  <TABLE_NAME>  \r\n\r\nOptions:\r\n  -H, --host <HOST_URL>           The host URL of the running InfluxDB 3 Core server [env: INFLUXDB3_HOST_URL=] [default: http://127.0.0.1:8181]\r\n  -d, --database <DATABASE_NAME>  The database name to run the query against [env: INFLUXDB3_DATABASE_NAME=]\r\n      --token <AUTH_TOKEN>        The token for authentication with the InfluxDB 3 Core server [env: INFLUXDB3_AUTH_TOKEN=]\r\n  -h, --help                      Print help information\r\n```\r\n\r\nI'm guessing you'll need to create different config blocks for each of the actions `TableDeleteConfig`, `TableCreateConfig`, etc. I saw this for the other nouns as well.",
        "Would it be better/more user friendly to have this value in MB? "
      ],
      "influxdb-follow-api-conventions": [
        "Can we re-use whatever it is that parses the precision argument from the HTTP API? That way they're always the exact same.",
        "I don't think this should be part of the public API. The public API for the Catalog should be to add the table (through a CatalogOp) and as part of that API it should add it to the map."
      ],
      "influxdb-performance-conscious-metrics-implementation": [
        "As a follow up it would be great to add a `influxdb_write_lines_rejected_total` counter.",
        "I don't think we want to spawn on every write call. We should be able to just call add_write_metrics on the store without this.",
        "Seems like we can have this be pretty big given they're small messages. 10k?"
      ],
      "influxdb-document-complete-data-flows": [
        "You should also mention that the write request that came in had a oneshot channel created that gets called back on after the flush and the placement of that data into the queryable buffer, with then returns a success to the client.",
        "This isn't strictly true. What the WAL periods are looking for is that data written into the oldest WAL files have time stamps that fall into chunks that are no longer receiving writes. So if we have the default 10m gen1 chunks and we're always writing data with a time of \"now\" then we would only snapshot after we have the 10m chunk time go cold. If we have lagged collection, by say 1m, we won't snapshot until after that 10m wall clock time has passed + 1m, but we would only snapshot the wal files from before that time. So we'd likely leave behind 60 wal files, which is by design."
      ],
      "influxdb-handle-errors-by-criticality": [
        "Good call to change to debug ðŸ‘ ",
        "Actually, changed my mind. I do want it to crash, because the next thing it does is delete WAL files. If for some reason the snapshot details we get back aren't expected, we want to crash rather than deleting WAL files, which may be unrecoverable. ",
        "At the moment I'm not sure what we can do other than keep trying. If we're unable to persist, it means the object store is unreachable, so writes will then return errors to clients because we'll be able to persist wal files to object storage. So clients attempting to write data will start receiving errors and errors will show up in the logs. It's the equivalent of losing the disk, so not much we can do about it. Unless we decide we want to exit the program entirely.",
        "any reason this might error, can we catch this and log it?"
      ],
      "influxdb-choose-optimal-data-structures": [
        "Given these serializations have the key as a string, I don't think there's any value in having this be a map. We'd be better off just having a Vec because we want the in-memory structure to be keyed off a u64, not a string. I'd apply this down the line everywhere we're using maps in serialization.",
        "We actually want this serialized as an int, not a string. I believe you're doing this because of the maps elsewhere?",
        "Yeah, so I think we should have one data structure for serialization, which is a vec of the objects, rather than a map. That way we can have all the IDs be ints. Then when you read the serialized data into an in-memory structure for fast lookups, you put it into a map, which should be by int. We should never being lookups against the serialized structure, I think",
        "It's not clear to me why this is being used here. What's the insertion order being preserved here?",
        "yeah, that's why I'm doing it. Moving to using hashbrown sounds good."
      ],
      "influxdb-stable-schema-identifiers": [
        "Ah yes, that's a good catch. They should have a well defined ID that remains static regardless of what schema changes later happen to the table.",
        "We could have our own `TableSchema` which includes the `SchemaRef` and also includes a map of column name to id. Then the `TableSchema` is what we serialize in the catalog. We want to have the ids and not the string identifiers in the WALContents because that's much cheaper to serialize and deserialize. Also cheaper to index when inserting the data into the `WriteBuffer`.\r\n\r\nSo we don't need to update arrow, we just need a wrapper around the arrow struct where we can add our own stuff.",
        "A thought I had is that the `DbSchema` should contain the mapping of the table name to id and back. That way when a write comes in, you grab a lock on the catalog once to get the schema. Then from that point on, assuming you're not creating any new schema, you won't need to grab a lock on the catalog to validate any of the lines.\r\n\r\nYou only end up having to lock again and replace the schema you're working with if they insert a new table.\r\n\r\nIt previously worked like this to minimize grabbing locks line by line.",
        "This feels like it's going to be problematic. I'd prefer to have the name and ID be separate fields. I think the better approach would be to have the key be the int table id and then have the name be a member on the value, which could have the table name and the data.",
        "I was thinking that you want the TableId as the key in the map. Then the value will have the name (as a member). We want the map key to be an int anyway because it's way faster to lookup.",
        "So the `TableChunks` struct would have table name as part of it. Probably worth renaming TableChunks to be something more appropriate.",
        "We should move to IDs everywhere, but anything that shows up in the WAL should also have the name in it somewhere. That is, an individual WAL file should be self-describing without having to pair it with a catalog.\r\n\r\nThe other thing about IDs everywhere is that for user facing things, it should be name. For our internal systems like lookup tables, etc, it should be ids.",
        "I guess we could drop the name from the WAL part if it proves too difficult. I'm just thinking about future recoverability & debugability. Easier if the name is there.",
        "I would do it the other way. All the data stored in a map that's keyed off ID. Then have a lookup table of ID to name. Although I'm not sure we want to do that. We don't need it assuming that we have a Catalog (which in all current cases we do). So it's extra payload that increases size and decode time for downstream consumers. Maybe just leave the name out and rely only on the ID."
      ],
      "influxdb-manage-complete-cache-lifecycle": [
        "Does this just grab a read lock? Seems like we don't need to check this so frequently by default, once a second seems more than enough?",
        "I put it here for now, but I'm not sure we want to walk the last cache and evict on every single wal file flush. I think it should be changed to only do eviction on snapshot, that way we're not spending too much time on it.",
        "Logged #25223 to track",
        "The behavior I like to see on creation of things is that if the user tries to create something with all the same arguments, it returns ok, if the arguments are different, then it returns with an error that it already exists. This might be handled higher up the stack in the API layer, but I'm not sure that this error would give the caller enough information to make the determination.\r\n\r\nThe reason I opt for this behavior is that it makes automation easier where they can call create as many times as they want and it'll always succeed as long as the settings are the same.",
        "If all rows have been expired, this key should be removed from the map. Otherwise, key values that stop sending data (think ephemeral things like container id, etc) will blow up the size of the cache over time with a bunch of entry map entries. (I'm assuming I'm reading this correctly and only the values are getting removed).\r\n\r\nThis should be true walking up the tree. Each key/value should be removed from the map if all children are empty."
      ],
      "influxdb-prefer-explicit-nullability": [
        "We might want to represent null key column values in the structure? I'm not sure about this yet though. Something to think about and discuss.",
        "Yeah, still not totally sure we want to bother with this, so maybe log an issue to see if anyone cares. For now I'd leave it as is."
      ],
      "influxdb-include-explanatory-examples": [
        "This is a bit odd but if you run\r\n\r\n```\r\ninfluxdb3 create database -h\r\nCreate a new database\r\n\r\nUsage: influxdb3 create database [OPTIONS] <DATABASE_NAME>\r\n\r\nArguments:\r\n  <DATABASE_NAME>  The database name to run the query against [env: INFLUXDB3_DATABASE_NAME=]\r\n```\r\n\r\nThe text is that you're running a query against a database, when if fact you're creating a database with that name. Ideally it would say that and also include some information about valid naming (i.e. alphanumeric with underscore and dash, starting with a letter or number).",
        "Would be good to have help that indicates what this is with an example. Also guidance on what valid tag names are.",
        "Update to include information that it's a comma separated list and provide an example.",
        "Can you add a comment with some examples of what you expect this to be? e.g. \"influxdb3-3.0.1\""
      ],
      "influxdb-descriptive-semantic-naming": [
        "rename to `last_snapshotted_wal_sequence_number` for clarity. The latest wal file number will always be >= this one.",
        "`write_lines_total` is the more standard naming convention. Metric names should end in units, e.g. _seconds, _bytes, or _total for unit-less metrics. See https://prometheus.io/docs/practices/naming/",
        "Yeah, we should do that everywhere in the code where we don't have a typed time.",
        "Should this be a new type like `struct ParquetFileId(u64)`?"
      ],
      "influxdb-avoid-flaky-test-patterns": [
        "I think with these kinds of serialization things, snapshot testing might not be what we want to use anyway. We want to confirm that we can serialize and deserialize data. Later when we make updates to the types, we want to make sure we can still deserialize older data.\r\n\r\nBut if we're marshalling into RAM, it's the equality of the data members that we're looking for."
      ],
      "influxdb-choose-appropriate-lock-primitives": [
        "This definitely feels like it's going to be a problem. An RWLock would be very preferred. I guess we can wait and see though.",
        "Seems like you could separate out the recency tracking from the data map. Also, atomics!",
        "small thing, but I'd clone the path before you acquire the lock.",
        "Shouldn't these just be in the `InnerCatalog` under its lock? Otherwise you won't be able to update these maps when inserting a new database or table in and ensure the mappings go in at the same time.",
        "I think you'd be better off just having a single RwLock around this whole struct. New last caches are only added when a new table or database is created. When writes come in, they're always for multiple databases & tables, so you end up grabbing a bunch of individual locks. Also, the write lock is only obtained on the flush interval for the WAL (which for now is 10ms, but it's going to increase in a later refactoring I have planned).\r\n\r\nYou end up having to do a bunch more locking for writes and for reads with the double lock that won't really pay off.",
        "I'd wrap this in a RWLock instead as queries will hit this a ton, but writes shouldn't be that often.",
        "Shouldn't the taking of the metadata lock happen after we've serialized the data? That way, the work happens there without a lock and then we're only holding the lock to actually update the metadata and object store, rather than doing any work."
      ],
      "influxdb-vet-security-critical-dependencies": [
        "We don't want to depend on the system. We want the interpreter included in the InfluxDB binary so that the user has no other local system setup to do other than install InfluxDB. I'm guessing we'll use PyOxidizer for this, although I'm not sure it's [fully supported yet](https://github.com/indygreg/PyOxidizer/issues/324)."
      ]
    }
  },
  "jasnell": {
    "repos": [
      "expressjs/express",
      "nodejs/node"
    ],
    "entries": [
      {
        "slug": "express-structured-release-workflows",
        "title": "Structured release workflows"
      },
      {
        "slug": "node-await-all-promises",
        "title": "Await all promises"
      },
      {
        "slug": "node-behavior-focused-test-design",
        "title": "Behavior-focused test design"
      },
      {
        "slug": "node-benchmark-before-optimizing-code",
        "title": "Benchmark before optimizing code"
      },
      {
        "slug": "node-descriptive-function-names",
        "title": "Descriptive function names"
      },
      {
        "slug": "node-document-non-intuitive-code",
        "title": "Document non-intuitive code"
      },
      {
        "slug": "node-document-with-precise-accuracy",
        "title": "Document with precise accuracy"
      },
      {
        "slug": "node-evolve-return-values",
        "title": "Evolve return values"
      },
      {
        "slug": "node-follow-consistent-naming-patterns",
        "title": "Follow consistent naming patterns"
      },
      {
        "slug": "node-follow-naming-conventions",
        "title": "Follow naming conventions"
      },
      {
        "slug": "node-format-docs-for-readability",
        "title": "Format docs for readability"
      },
      {
        "slug": "node-idempotent-error-safe-disposers",
        "title": "Idempotent error-safe disposers"
      },
      {
        "slug": "node-informative-error-messages",
        "title": "Informative error messages"
      },
      {
        "slug": "node-limit-environment-variable-scope",
        "title": "Limit environment variable scope"
      },
      {
        "slug": "node-minimize-configuration-dependencies",
        "title": "Minimize configuration dependencies"
      },
      {
        "slug": "node-prefer-clarity-over-cleverness",
        "title": "Prefer clarity over cleverness"
      },
      {
        "slug": "node-propagate-errors-with-context",
        "title": "Propagate errors with context"
      },
      {
        "slug": "node-public-over-internal-apis",
        "title": "Public over internal APIs"
      },
      {
        "slug": "node-resource-aware-programming-patterns",
        "title": "Resource-aware programming patterns"
      },
      {
        "slug": "node-reuse-computed-values-efficiently",
        "title": "Reuse computed values efficiently"
      },
      {
        "slug": "node-scope-security-settings",
        "title": "Scope security settings"
      },
      {
        "slug": "node-standardize-null-pointer-checks",
        "title": "Standardize null pointer checks"
      },
      {
        "slug": "node-thread-safe-resource-management-patterns",
        "title": "Thread-safe resource management patterns"
      },
      {
        "slug": "node-use-appropriate-metric-types",
        "title": "Use appropriate metric types"
      },
      {
        "slug": "node-use-modern-c-features",
        "title": "Use modern C++ features"
      },
      {
        "slug": "node-validate-network-request-parameters",
        "title": "Validate network request parameters"
      },
      {
        "slug": "node-version-apis-with-care",
        "title": "Version APIs with care"
      }
    ],
    "comments": {
      "node-validate-network-request-parameters": [
        "I might have missed it elsewhere, but we need to make sure that `requestHost`, `reqOptions.port`, `reqOptions.host`, `auth` etc are validated here to not include and invalid characters (like `\\r` and \\n`) in order to prevent any kind of request smuggling.",
        "If these are checked somewhere else, then a comment in here indicating where would be helpful for people coming into this code later."
      ],
      "node-await-all-promises": [
        "```suggestion\r\n  const { promise, resolve, reject } = Promise.withResolvers();\r\n  fs.stat(filePath, { signal }, (err, stats) => {\r\n    if (err) {\r\n      return reject(err);\r\n    }\r\n    resolve(stats);\r\n  });\r\n\r\n  await assert.rejects(promise, { name: 'AbortError' });\r\n```",
        "should the read at least be awaited?",
        "It doesn't necessarily need to be done in this PR (but would need to be done before this graduates from experimental), but we should make sure that this plays well with AsyncLocalStorage. Specifically, something like the following should work:\r\n\r\n```\r\nconst als = new AsyncLocalStorage();\r\nals.run(123, () => {\r\n  navigator.locks.request('exclusive-test', async (lock) => {\r\n    assert.strictEqual(als.getStore(), 123);\r\n  });\r\n});\r\n```\r\n\r\nThis would suggest that the lock request is capable of capturing the current async context frame and restoring it when the lock has been acquired and the callback is invoked."
      ],
      "node-scope-security-settings": [
        "I think I'd much prefer this to be set as an option on an individual connection rather than programmatically impacting all connections. A cli flag is one thing because it's set by the individual running the app. This API could be set by dependencies impacting global state without the application being aware of it. Setting it per connection seems the safest. ",
        "I won't block but I'm still unconvinced this is a good thing to add."
      ],
      "node-thread-safe-resource-management-patterns": [
        "This pattern looks... odd... and likely a bit error prone. We generally prefer the use of smart pointers to explicit use of `delete` and this could use some comments around it so it's clear what is happening here.",
        "These's aren't correct here. You really should not be holding a `v8::Global<T>` inside a `v8::External`, then deleting it like this. It's fine to use indirection through another type... e.g. having the `External` hold an instance of a struct that is holding the `Global<T>`.",
        "I ought to be able to take another look this afternoon. Just catching up after two week of being out of the office ;-) ... just mentioning because I didn't want you to feel I was ignoring your pings on this ;-) "
      ],
      "node-informative-error-messages": [
        "Here I would create the error outside the `process.nextTick(...)` so that it has a useful stack trace attached to it.",
        "It would likely be worthwhile to assign a warning code to this warning so that it can be suppressed with the `--dsable-warning` CLI flag.\r\n\r\n![image](https://github.com/user-attachments/assets/ec94215e-e7f7-4729-9c02-69bf1c433d2f)",
        "That's reasonable. Updated",
        "Likely yes but I think it's reasonable to throw a more specific error here."
      ],
      "node-evolve-return-values": [
        "I don't think the `enableHelpPrinting` here is a good idea. Instead, the return value of `parseArgs` should just be capable of returning the serialized help text. Allowing for something like,\r\n\r\n```\r\nconst result = parseArgs(...);\r\nconsole.log(result.printUsage());\r\n```",
        "We can have something, yes, but having these kinds of side effects on the constructor can be problematic",
        "Hmm.. good question. I really don't know.",
        "```suggestion\r\n   return type of an existing API is a breaking change.\r\n4. When an existing API signature does not lend itself easily to supporting making\r\n    the return value disposable and a new API needs to be introduced, it is worth\r\n    considering whether the existing API should be deprecated in favor of the new.\r\n    Deprecation is never a decision to be taken lightly, however, as it can have major\r\n    ecosystem impact.\r\n```",
        "This is likely something that can be best handled in documentation than code. For instance, in code we can have it be a fully anonymous object:\r\n\r\n```js\r\nreturn {\r\n  dispose() { ... },\r\n  [Symbol.dispose]() { this.dispose(); }\r\n}\r\n```\r\n\r\nWhile in documentation is can at least *appear* to be a named interface:\r\n\r\n```markdown\r\n### `foo()`\r\n\r\n* Returns {Disposable}\r\n\r\n### `Disposable`\r\n\r\n#### `disposable.dispose()`\r\n\r\n#### `disposable[Symbol.dispose]`\r\n```\r\n\r\nSo I'd suggest that we're talking about a documentation difference here, not necessarily a coding difference.",
        "I'm not suggesting that we would leave things undocumented. I'm saying that we don't always need formal classes in the actual implementation. For some return values we can safely rely on documentation-only and leave the actual implementation as anonymous objects. We even have existing precedence for this in the current documentation. See, for instance, all of the \"Class\" documentations for Web Crypto operations here: https://nodejs.org/docs/latest/api/webcrypto.html#algorithm-parameters ... the things like `AesDerivedKeyParams` is documented as a class but, in reality, there is no actual class named `AesDerivedKeyParams` in the source.",
        "I've expanded the document to include coverage of documentation of anonymous dispoables."
      ],
      "node-propagate-errors-with-context": [
        "These might need to be handled separately. If the first call succeeds, then the first created function will take ownership over the `fulfill_holder`. If the second call then fails for whatever reason, we are deleting the `fulfill_holder` while it's external is still holding the reference to it that it assumes it owns. It would be best to separate this into two separate calls rather than aggregating them together like this. Create one, create it's external and it's function, then create the second...\r\n\r\nOr, can we at least be certain that we won't end up with a free-after-free type error when deleting these while the External is still holding them?",
        "Not something to do here, but using `Check()` here has the same issue as using `ToLocalChecked()` in that it will just crash the process rather than propagate the error. This is a common issue throughout the code, however, so not something I would block this PR on. We need to handle these better in general. \r\n\r\nIf you did want to handle this here, then changing these to check if the return value is empty then doing some proper error propagation similar to the way the ToLocal(...) results are handled would be ideal.",
        "We should avoid using `USE` for the same error propagation reasons. Calling `Then(...)` can cause a JavaScript error to be scheduled. USE would cause that to be ignored when we ought to propagate it."
      ],
      "node-behavior-focused-test-design": [
        "Nit: you can use `deepStrictEqual` to compare the full `result` to an expectation",
        "The callback passed to the `then` should be wrapped with a `common.mustCall(...)`",
        "Wrap the callback in `common.mustCall(...)`"
      ],
      "node-idempotent-error-safe-disposers": [
        "```suggestion\r\n   closed. If there is no difference in disposing in success or exception contexts,\r\n   then separate disposal methods are unnecessary.\r\n```",
        "We can definitely soften this but I think the guideline is correct. Most of the time disposal should be sync as much as possible. There are likely exceptions to that rule, of course. Will think about how to soften this a bit to say it's ok to use but should be intentional?",
        "Consider the following two cases:\r\n\r\n```\r\n{\r\n  using foo = new MyDisposable();\r\n}\r\n\r\n{\r\n  using foo = new MyDisposable();\r\n  throw new Error('boom');\r\n}\r\n```\r\n\r\nThe disposer in each case is going to be called. Unfortunately, however, the disposer has no idea if there is an exception pending or not. Let's say our `MyDisposable` has two possible modes: one is a clean, graceful async shutdown, the second is a dirty, abrupt shutdown in case of an error. Since the disposer does not know whether it is being called with a pending exception or not we cannot safely assume within the disposer that a graceful async shutdown should be used. Instead, we have to assume it is an exception case.\r\n\r\n```js\r\nclass MyDisposer {\r\n  #closed = false;\r\n  #aborted = false;\r\n\r\n  close() {\r\n    if (this.#closed) return;\r\n    this.#closed = true;\r\n  }\r\n\r\n  abort() {\r\n    this.#aborted = true;\r\n    this.close();\r\n  }\r\n\r\n  [Symbol.dispose]() {\r\n    this.abort();\r\n  }\r\n}\r\n```\r\n\r\n",
        "> ... But for these I question if ERM is really the right way to go...\r\n\r\nGenerally I think we should be leaving the decision up to users whether to use ERM or not. These guidelines are more about what we need to do for enablement. ",
        "Updated the language on this in the doc, please take another look!",
        "```suggestion\r\n   be synchronous and immediate. Avoiding async disposal is not always possible,\r\n   however, as some types of disposable objects require asynchronous cleanup.\r\n```",
        "````suggestion\r\n```\r\n\r\nThis is because of the fact that, when the disposer is called, it has no way\r\nof knowing if there is a pending exception or not and it is generally safest\r\nto assume that it is being called in an exceptional state. While some types\r\nof disposable objects make no differentiation between dispose in success\r\nand dispose in exception cases, those that do otherwise have no way of\r\ndifferentiating the conditions from within the disposer itself.\r\n````",
        "```suggestion\r\n3. It is recommended to avoid throwing errors within disposers.\r\n   If a disposer throws an exception while there is another pending\r\n   exception, then both exceptions will be wrapped in a `SupressedError`\r\n   that masks both. This makes it difficult to understand the context\r\n   in which the exceptions were thrown.\r\n```",
        "```suggestion\r\n  let disposed = false;\r\n  return {\r\n    dispose() {\r\n      if (disposed) return;\r\n      diposed = true;\r\n      console.log('Resource disposed');\r\n    }\r\n    [Symbol.dispose]() {\r\n      this.dispose();\r\n    },\r\n  };\r\n```",
        "Hmm.. I'm not sure how best to change this to address your comment here. Would you mind if we push any update on this one to another edit PR?"
      ],
      "node-document-with-precise-accuracy": [
        "As I mentioned previously this test is only actually testing the behavior when `fs.stat` is called with an already aborted `AbortSignal`... The test description above should be updated to reflect that and I would add a comment about it in here. Not sure exactly how we can reliably test aborting the call while it is in flight since file systems have such broadly different performance characteristics and often these conclude so quickly that it rarely can be caught and canceled in time.",
        "Small typo in that, otherwise looks good\r\n\r\n```\r\n// This test verifies that fs.stat immediately throws an AbortError if the provided AbortSignal\r\n// has already been canceled. This approach is used because trying to abort an fs.stat call in flight\r\n// is unreliable given that file system operations tend to complete very quickly on many platforms.\r\n```",
        "```suggestion\r\n  * @param {string[]} data\r\n```",
        "This is such an unusual pattern to see that it likely warrants some code comments in here to explain why we're setting the prototype this way (as opposed to `class DOMException extends Error`."
      ],
      "node-follow-naming-conventions": [
        "For consistency, these should use lower-case, snake-case\r\n```suggestion\r\n  V(client_id_string, \"clientId\")                                              \\\r\n```",
        "For concepts like this, I'd prefer if we adopted a naming scheme like `IsCallable`"
      ],
      "node-follow-consistent-naming-patterns": [
        "Since these aren't static methods, the `b` should be lower-case and likely needs `make format-md` or `make lint` run\r\n\r\n```suggestion\r\n### `blockList.fromJSON(value)`\r\n```",
        "I'd prefer the constructor methods here to follow a naming pattern like `createCounter`, `createTimer`, etc, both for ergonomics and for consistency (see `createHistogram(...)` in `perf_hooks`.\r\n\r\n```js\r\nconst counter = createCounter('api.calls', { service: 'web' });\r\nconst timer = createTimer('api.request.duration', { service: 'web' });\r\n```",
        "Yeah I kind of picked `use` here intentionally because of that ;-) ... wanted to prompt a discussion about it."
      ],
      "node-document-non-intuitive-code": [
        "This could use a code comment to describe the differences between `hashDigest(...)` and `xosHashDigest(...)`"
      ],
      "node-use-appropriate-metric-types": [
        "One additional type of Guage that might be useful is a `HighwaterMarkGauge` whose value only every actually increases. For instance, suppose we are tracking deltas like:\r\n\r\n```js\r\ngauge.applyDelta(10);  // Records the stored highwater mark value as 10\r\ngauge.applyDelta(-5);  // Stored highwater mark value is still 10\r\ngauge.applyDelta(10);  // Stored highwater mark value is 15\r\ngauge.applyDelta(-15);  // Stored highwater mark value is still 15\r\ngauge.applyDelta(10);  // Stored highwater mark value is still 15\r\ngauge.applyDelta(10);  // Stored highwater mark value is 20\r\n```"
      ],
      "express-structured-release-workflows": [
        "Yeah not sure an npm org would work but a shared ID for publishing releases for express related stuff would be good. The shared secrets can be managed in a private repo the way we do shared secrets for core stuff. (There may\nalready be some tool for this somewhere)\n"
      ],
      "node-resource-aware-programming-patterns": [
        "Yeah, I'll be expanding the documentation before moving this PR out of draft. But feel free to offer suggestions ;-)",
        "```suggestion\r\nAn attempt was made to read a file larger than the supported 2 GiB limit for\r\n`fs.readFile()`. This is not a limitation of `Buffer`, but an internal I/O constraint.\r\nFor handling larger files, consider using `fs.createReadStream()` to read the\r\nfile in chunks.\r\n```"
      ],
      "node-use-modern-c-features": [
        "Nit... since we're finally up on c++20, we can start using the more condensed `namespace node::inspector::protocol {` syntax where appropriate.",
        "```suggestion\r\n  enum class Mode { Shared, Exclusive };\r\n```\r\n\r\nOptional style nit.",
        "Since we are standardized on c++20 now, this can be condensed to `namespace node::worker::locks {`",
        "We really shouldn't use `new` with `v8::Global`. These can just be `v8::Global` and we make use of move semantics. Instead of using `delete` with `v8::Global`, the correct way to clear them is to use their `reset()` method"
      ],
      "node-prefer-clarity-over-cleverness": [
        "this feels almost a bit too clever. Why not simply have `_guessHandleType(fd)` return null explicitly? then your return value below could be `return handletypes[type] || type`.",
        "```suggestion\r\n\r\n  loadFromFile(savePath=\"./blocklist.json\") {\r\n```\r\n\r\nWhy make this an arrow function?\r\n\r\nInstead of building the file loading directly into `BlockList`, consider having `BlockList` instead generate a `Buffer` or be created from a `Buffer` and let the application do the file handling itself.\r\n\r\n```\r\nconst blockList = createBlockListSomehow();\r\nconst buf = blockList.save();\r\nfs.writeFileSync('blocklist.json', buf);\r\n```\r\n",
        "JSON or buffer doesn't matter so much. I'd prefer to avoid adding the node:fs uses into BlockList and would rather separate those concerns. We can greatly simplify this, for instance, by having `toJSON` and `fromJSON` methods on BlockList and handing the fs operations externally to BlockList."
      ],
      "node-reuse-computed-values-efficiently": [
        "You might consider moving the declaration for the `Local<Object> lock_info` to outside of the for loop and just reset it here so that we're reusing the same declaration on each iteration rather than creating a new one.",
        "Same here... if the `Local<Object> lock_info;` is moved outside of the for loops then it can just be reused here."
      ],
      "node-limit-environment-variable-scope": [
        "Just a nit... I'd be more comfortable with this extracting just the env vars that are specifically relevant to `proxyEnv` rather than passing the entire `process.env`"
      ],
      "node-version-apis-with-care": [
        "The case will need to be wrapped in the `#ifdef NAPI_EXPERIMENTAL` as well",
        "Ah right, I forgot that this file defines `NAPI_EXPERIMENTAL` unconditionally at the top of the file. ",
        "Likewise here. Wrap the else if here in an `#ifdef NAPI_EXPERIMENTAL`",
        "My fear here is that `apiSurface`, even as a generated file, is likely to become quite massive and difficult to manage. As is we cannot even use the github UI to review it since it says, \"29,926 additions... not shown because the diff is too large...\". Is there any way to break it up further? Maybe generate a set of files rather than one single massive one?"
      ],
      "node-standardize-null-pointer-checks": [
        "Consider having this return a `MaybeLocal<Object>` to better facilitate error propagation"
      ],
      "node-format-docs-for-readability": [
        "Check out how we handle links in the other docs in this folder. We collect them at the end of the doc and use in-doc references rather than inlining the URLs. Much more readable that way."
      ],
      "node-public-over-internal-apis": [
        "These aren't really considered internal modules, fwiw",
        "My concern would be whether or not anyone in userland could be using this, as unlikely as that may be. Should we export this through the regular `node:http` API to provide an alternative path to them?"
      ],
      "node-minimize-configuration-dependencies": [
        "Given that this is Windows only, I would generally prefer that this entire internal binding only be compiled on windows, with the non-functional stubs implemented solely in javascript on the other platforms."
      ],
      "node-benchmark-before-optimizing-code": [
        "The results are mixed and it's just not clear if the difference is worth being concerned about... some benchmarks are faster, others are slower.\r\n\r\n```\r\nstreams/compose.js n=1000                                                                       ***    -19.41 %       Â±1.17%  Â±1.57%  Â±2.06%\r\nstreams/creation.js kind='duplex' n=50000000                                                    ***      4.39 %       Â±1.62%  Â±2.16%  Â±2.81%\r\nstreams/creation.js kind='readable' n=50000000                                                           2.46 %       Â±4.12%  Â±5.48%  Â±7.14%\r\nstreams/creation.js kind='transform' n=50000000                                                   *      2.21 %       Â±1.69%  Â±2.25%  Â±2.94%\r\nstreams/creation.js kind='writable' n=50000000                                                  ***      6.72 %       Â±2.15%  Â±2.85%  Â±3.71%\r\nstreams/destroy.js kind='duplex' n=1000000                                                              -0.50 %       Â±3.36%  Â±4.48%  Â±5.84%\r\nstreams/destroy.js kind='readable' n=1000000                                                             1.07 %       Â±2.43%  Â±3.24%  Â±4.22%\r\nstreams/destroy.js kind='transform' n=1000000                                                           -0.55 %       Â±2.33%  Â±3.10%  Â±4.03%\r\nstreams/destroy.js kind='writable' n=1000000                                                             0.16 %       Â±2.75%  Â±3.66%  Â±4.77%\r\nstreams/pipe-object-mode.js n=5000000                                                           ***      4.68 %       Â±2.00%  Â±2.67%  Â±3.50%\r\nstreams/pipe.js n=5000000                                                                       ***     10.30 %       Â±1.19%  Â±1.59%  Â±2.07%\r\nstreams/readable-async-iterator.js sync='no' n=100000                                                    1.07 %       Â±1.92%  Â±2.55%  Â±3.32%\r\nstreams/readable-async-iterator.js sync='yes' n=100000                                          ***      8.45 %       Â±3.80%  Â±5.06%  Â±6.59%\r\nstreams/readable-bigread.js n=1000                                                                       0.55 %       Â±2.27%  Â±3.02%  Â±3.94%\r\nstreams/readable-bigunevenread.js n=1000                                                        ***     -2.12 %       Â±1.13%  Â±1.52%  Â±1.99%\r\nstreams/readable-boundaryread.js type='buffer' n=2000                                             *      1.03 %       Â±0.84%  Â±1.12%  Â±1.46%\r\nstreams/readable-boundaryread.js type='string' n=2000                                             *      2.08 %       Â±1.69%  Â±2.26%  Â±2.97%\r\nstreams/readable-from.js type='array' n=10000000                                                ***     -5.20 %       Â±2.30%  Â±3.06%  Â±3.99%\r\nstreams/readable-from.js type='async-generator' n=10000000                                               1.73 %       Â±2.22%  Â±2.96%  Â±3.86%\r\nstreams/readable-from.js type='sync-generator-with-async-values' n=10000000                              1.88 %       Â±2.05%  Â±2.73%  Â±3.56%\r\nstreams/readable-from.js type='sync-generator-with-sync-values' n=10000000                              -1.75 %       Â±2.41%  Â±3.24%  Â±4.28%\r\nstreams/readable-readall.js n=5000                                                               **     -4.53 %       Â±3.18%  Â±4.24%  Â±5.51%\r\nstreams/readable-uint8array.js kind='encoding' n=1000000                                         **      2.40 %       Â±1.52%  Â±2.02%  Â±2.63%\r\nstreams/readable-uint8array.js kind='read' n=1000000                                            ***     -2.98 %       Â±1.59%  Â±2.11%  Â±2.75%\r\nstreams/readable-unevenread.js n=1000                                                             *     -1.81 %       Â±1.45%  Â±1.93%  Â±2.51%\r\nstreams/writable-manywrites.js len=1024 callback='no' writev='no' sync='no' n=100000                     1.10 %       Â±3.76%  Â±5.00%  Â±6.51%\r\nstreams/writable-manywrites.js len=1024 callback='no' writev='no' sync='yes' n=100000           ***     27.55 %       Â±9.28% Â±12.36% Â±16.09%\r\nstreams/writable-manywrites.js len=1024 callback='no' writev='yes' sync='no' n=100000                    2.88 %       Â±5.18%  Â±6.90%  Â±8.98%\r\nstreams/writable-manywrites.js len=1024 callback='no' writev='yes' sync='yes' n=100000            *     11.47 %       Â±9.08% Â±12.08% Â±15.73%\r\nstreams/writable-manywrites.js len=1024 callback='yes' writev='no' sync='no' n=100000                   -1.45 %       Â±3.28%  Â±4.36%  Â±5.68%\r\nstreams/writable-manywrites.js len=1024 callback='yes' writev='no' sync='yes' n=100000          ***     23.32 %       Â±8.66% Â±11.53% Â±15.01%\r\nstreams/writable-manywrites.js len=1024 callback='yes' writev='yes' sync='no' n=100000                   3.83 %       Â±4.48%  Â±5.96%  Â±7.76%\r\nstreams/writable-manywrites.js len=1024 callback='yes' writev='yes' sync='yes' n=100000                  1.32 %       Â±6.89%  Â±9.17% Â±11.94%\r\nstreams/writable-manywrites.js len=32768 callback='no' writev='no' sync='no' n=100000                   -1.30 %       Â±3.51%  Â±4.68%  Â±6.09%\r\nstreams/writable-manywrites.js len=32768 callback='no' writev='no' sync='yes' n=100000          ***     30.32 %      Â±10.19% Â±13.57% Â±17.67%\r\nstreams/writable-manywrites.js len=32768 callback='no' writev='yes' sync='no' n=100000                   0.12 %       Â±3.88%  Â±5.17%  Â±6.73%\r\nstreams/writable-manywrites.js len=32768 callback='no' writev='yes' sync='yes' n=100000          **     13.40 %       Â±9.34% Â±12.47% Â±16.32%\r\nstreams/writable-manywrites.js len=32768 callback='yes' writev='no' sync='no' n=100000                  -1.46 %       Â±4.38%  Â±5.83%  Â±7.61%\r\nstreams/writable-manywrites.js len=32768 callback='yes' writev='no' sync='yes' n=100000         ***     16.67 %       Â±8.21% Â±10.94% Â±14.28%\r\nstreams/writable-manywrites.js len=32768 callback='yes' writev='yes' sync='no' n=100000                  1.19 %       Â±4.05%  Â±5.39%  Â±7.01%\r\nstreams/writable-manywrites.js len=32768 callback='yes' writev='yes' sync='yes' n=100000        ***     21.43 %       Â±9.84% Â±13.14% Â±17.21%\r\nstreams/writable-uint8array.js kind='object-mode' n=50000000                                             0.64 %       Â±3.66%  Â±4.87%  Â±6.34%\r\nstreams/writable-uint8array.js kind='write' n=50000000                                            *      3.85 %       Â±3.32%  Â±4.42%  Â±5.76%\r\nstreams/writable-uint8array.js kind='writev' n=50000000                                           *      3.15 %       Â±2.99%  Â±3.98%  Â±5.18%\r\n```"
      ],
      "node-descriptive-function-names": [
        "Let's name this method `toJSON` to that it works seamlessly with `JSON.stringify(...)` ",
        "For the `IntervalHistogram` (https://nodejs.org/docs/latest/api/perf_hooks.html#class-intervalhistogram-extends-histogram) we use `enable()` and `disable()`. Not critical but I'd prefer to keep the API names consistent."
      ]
    }
  },
  "bolinfest": {
    "repos": [
      "openai/codex"
    ],
    "entries": [
      {
        "slug": "codex-avoid-hard-coded-configuration",
        "title": "Avoid hard-coded configuration"
      },
      {
        "slug": "codex-avoid-unnecessary-operations",
        "title": "Avoid unnecessary operations"
      },
      {
        "slug": "codex-centralize-configuration-management",
        "title": "Centralize configuration management"
      },
      {
        "slug": "codex-contextualize-dont-swallow",
        "title": "Contextualize, don't swallow"
      },
      {
        "slug": "codex-document-non-obvious-aspects",
        "title": "Document non-obvious aspects"
      },
      {
        "slug": "codex-extract-reusable-logic",
        "title": "Extract reusable logic"
      },
      {
        "slug": "codex-leverage-style-tools",
        "title": "Leverage style tools"
      },
      {
        "slug": "codex-minimize-blocking-operations",
        "title": "Minimize blocking operations"
      },
      {
        "slug": "codex-organize-code-top-down",
        "title": "Organize code top down"
      },
      {
        "slug": "codex-pin-external-action-dependencies",
        "title": "Pin external action dependencies"
      },
      {
        "slug": "codex-prevent-command-injection",
        "title": "Prevent command injection"
      },
      {
        "slug": "codex-proper-packagejson-structure",
        "title": "Proper package.json structure"
      },
      {
        "slug": "codex-provider-agnostic-api-design",
        "title": "Provider-agnostic API design"
      },
      {
        "slug": "codex-secure-cicd-pipelines",
        "title": "Secure CI/CD pipelines"
      },
      {
        "slug": "codex-semantic-naming-patterns",
        "title": "Semantic naming patterns"
      },
      {
        "slug": "codex-structure-configurations-properly",
        "title": "Structure configurations properly"
      },
      {
        "slug": "codex-workspace-version-configuration",
        "title": "Workspace version configuration"
      },
      {
        "slug": "codex-write-comprehensive-test-assertions",
        "title": "Write comprehensive test assertions"
      }
    ],
    "comments": {
      "codex-semantic-naming-patterns": [
        "Using `{}` as a placeholder in this way seems very confusing to me as a Rust person because it's not being used natively by `format!()`. Please use something like `SUMMARY_TEXT` instead so it's more obvious that something is meant to be replaced.",
        "These should not have an `openai_` prefix, but should be generally applicable to all providers, right?",
        "Similar to a comment I made on another PR, please list all of these helper functions below the tests. The tests are the most important thing in this file.",
        "Also, I would name the variable `codex_home` rather than `dir`.",
        "`fully_qualified_tool_name()` and `try_parse_fully_qualified_tool_name()` must be symmetric. It is not clear that this is the case given this implementation.\r\n\r\nSomeone told me that, empirically, the model doesn't care about the names of the functions all that much and therefore, we could SHA1 the long name or something and things would still work.\r\n\r\nAnother solution that is somewhat stateful, but more readable for users, would be to get the full list of tool names and only attempt to \"fully qualify them\" when there is a naming collision."
      ],
      "codex-organize-code-top-down": [
        "This is admittedly a nit, but personally, I would rewrite this as a `match self.tx.send(RolloutCmd::Shutdown { ack: tx_done }).await` to avoid using the `return` keyword. I try to reserve the use of `return` when you really need an \"early return\" in a long function. But if using a single expression is an option, I find that to be cleaner because then it's more \"straight line\" code (though admittedly `match` branches...).",
        "For small helper functions, particularly ones that are private to the file, please declare them _after_ the functions that use them. I strongly prefer declaring the \"most important stuff\" at the top of the file and \"details\" (which includes functions like this) at the top of the file.",
        "Is there a reason this logic isn't added to `dispatch_codex_event()` instead? Much of the reason to have the `dispatch_codex_event()` helper is to keep the length of `run()` down. In a new top-level function, there will be less indenting and the code should be easier to read, as well.",
        "Let's do this before `match hunk` since it is used in all three cases."
      ],
      "codex-avoid-hard-coded-configuration": [
        "I was thinking of going in the other direction where we would replace `--full-auto` with `--dangerously-auto-approve-everything` since the Docker container *is* the sandbox. Is your motivation to add a weaker approval mode or a stronger one?\r\n\r\nI guess from your screenshot you want to run it with `--suggest`?",
        "@fouad-openai do you want this to print something else if `--native` was passed to indicate publishing to a different tag?",
        "I believe it is possible for `ALLOWED_DOMAINS` to be the empty string if this script were run with `ALLOWED_DOMAINS=` (explicitly defining the `ALLOWED_DOMAINS` env var as the empty string), so out of an abundance of caution, maybe we should do this before `docker run`:\r\n\r\n```\r\nif [ -z \"$ALLOWED_DOMAINS\" ]; then\r\n  echo \"Error: ALLOWED_DOMAINS is empty.\"\r\n  exit 1\r\nfi\r\n```\r\n\r\n"
      ],
      "codex-secure-cicd-pipelines": [
        "Hmm, this does not appear to have seen much interest or activity:\r\n\r\nhttps://github.com/codespell-project/codespell-problem-matcher\r\n",
        "Right, I trust that it works today. What I have less confidence in is a bad actor coming in, somehow gaining control of `codespell-problem-matcher`, changing the `v1` tag to point at something else, and anyone noticing."
      ],
      "codex-centralize-configuration-management": [
        "I have been trying to eliminate support for environment variables in favor of using configuration. Can we just added a config option (prefixed with \"experimental\" like `experimental_resume`) for this?",
        "I think this could be argued either way, but I think there is a technical reason to do this as early as possible (before we know what the `Config` even is), which is that setting environment variables for the current process is not thread-safe, so it should really be done before any threads have been created.\r\n\r\nI just reworked this so that `load_dotenv()` is now called before we set up the Tokio runtime.",
        "I think we should only be looking at the `OPENAI_` environment variables for the built-in OpenAI provider, not all providers, right?",
        "Taking a step back, I'm not sure we should be honoring `OPENAI_STREAM_IDLE_TIMEOUT_MS`, `OPENAI_REQUEST_MAX_RETRIES`, or `OPENAI_STREAM_MAX_RETRIES` at all. As best I can tell, these are not \"standard\" OpenAI environment variables, but ones that we made up for Codex?\r\n\r\nI've been trying to maintain a consistency where the `Config` is the \"one true way\" to configure things, so supporting a small handful of environment variables confuses that.",
        "```suggestion\r\n    // Configure retry behavior explicitly to avoid mutating process-wide\r\n```",
        "```suggestion\r\n    // Configure retry behavior explicitly to avoid mutating process-wide\r\n```"
      ],
      "codex-write-comprehensive-test-assertions": [
        "I would just `assert_eq!()` using a string literal that, yes, is a copy of `COMPACT_SUMMARY_TEMPLATE`. Again, I would use `r#`.",
        "Maybe use `find(|t| t.get(\"name\").as_ref() == Some(\"srv.dummy\")` on `tools.iter()` or something like that and then do an `assert_eq!()` on the value returned from `find()`?",
        "For both of these tests, can we just assert the entire string/serde_json::Value that we get back? I realize this means that we will have to update this test if we change the default tools, but I think having a test that verifies _everything_ (and effectively documents what we send on the wire) is worth that maintenance cost.",
        "This is a natural thing to do, but whenever possible, please just do one big `assert_eq!()` rather than doing a bunch of piecemeal `assert_eq!()` calls, so something like:\r\n\r\n```suggestion\r\n        assert_eq!(events, vec![\r\n            Ok(ResponseEvent::OutputItemDone(ResponseItem::Message { role: \"assistant\" }),\r\n            Ok(ResponseEvent::OutputItemDone(ResponseItem::Message { role, \"assistant\" }),\r\n            Ok(ResponseEvent::Completed { response_id: \"resp1\", token_usage: None })\r\n        ]);\r\n```\r\n\r\nI think you can also potentially do an `iter` / `collect()` on `events` to convert `Vec<Result<T>>` into `Result<Vec<T>>`.\r\n\r\n",
        "For clarity, would you mind just doing `assert_eq!` on the full struct? Admittedly, I think you need to `derive(PartialEq)` on `MaybeApplyPatchVerified` and some things to do that.\r\n\r\nWhen possible, I find that to be much stronger than doing `assert_eq!` on individual fields."
      ],
      "codex-workspace-version-configuration": [
        "outside the scope of this PR, but I guess all of our `Cargo.toml` files should have `version = { workspace = true }`?"
      ],
      "codex-document-non-obvious-aspects": [
        "Should we keep this comment?",
        "Please add a docstring explaining what is being tested.",
        "Could you add docstrings for this test and the other test? Admittedly, there is a lot of code required just to setup these tests, so it's not 100% obvious what is being tested. That is, this line seems to be the key bit that is producing the behavior that we are verifying at the end of the test:\r\n\r\n```rust\r\n.set_body_raw(sse_message(\"Hello, world.\"), \"text/event-stream\")\r\n```",
        "Maybe a docstring to explain what the return value represents?",
        "I have tried to avoid these sorts of comments. For example, in `bottom_pane.rs`, we have:\r\n\r\n```rust\r\n/// Number of terminal rows consumed by the textarea border (top + bottom).\r\nconst TEXTAREA_BORDER_LINES: u16 = 2;\r\n```"
      ],
      "codex-provider-agnostic-api-design": [
        "I think it's more appropriate for these to be:\r\n\r\n```suggestion\r\nconst DEFAULT_STREAM_IDLE_TIMEOUT_MS: u64 = 300_000;\r\nconst DEFAULT_STREAM_MAX_RETRIES: u64 = 10;\r\nconst DEFAULT_REQUEST_MAX_RETRIES: u64 = 4;\r\n```",
        "Some of this code duplicates code in `core/`. Note that the model provider might only support one of Responses or the chat completions API, so it is not safe to assume the above code is an option.\r\n\r\nThat said, I recently learned that there is a _non-stateful version of the Responses API_ where  you pass input as the array of all messages the same way you do for chat completions. So perhaps we could expose a function in `core/` that takes a `Config` and a list of messages like you have here and uses the `config.model_provider` and a `Client` to make the request?\r\n\r\nAdmittedly this is complex enough that it is probably appropriate to do it in a separate PR."
      ],
      "codex-proper-packagejson-structure": [
        "This should be in `devDependencies`."
      ],
      "codex-leverage-style-tools": [
        "I'm about to run CI on this PR, but seeing this makes me suspicious that you haven't run `pnpm run format` in this folder or maybe not `pnpm run lint`? If you use VS Code and have the ESLint and Prettier extensions configured correctly, you should get alerted to these issues in the Problems pane and generally don't have to run those scripts explicitly.",
        "This should have stayed a `switch` statement because we have ESLint rules set up to ensure `switch` statements are exhaustive so that if a new variant of the `ApprovalPolicy` enum is introduced we are forced to address the existing callsites."
      ],
      "codex-structure-configurations-properly": [
        "I think we need a different name and maybe a level of two of depth.\r\n\r\nThat is, this not configuring the general \"output\" of Codex CLI. (To me, this name implies the \"output\" that I see as a user.) It is configuring something extremely specific: truncation parameters for the `shell` tool call.\r\n\r\nThis makes me wonder if should have a top level config entry named `\"tools\"` where keys are tool names that point to dictionaries of arbitrary properties for configuring that tool, so the JSON would look like:\r\n\r\n```json\r\n\"tools\": {\r\n  \"shell\": {\r\n    \"maxBytes\": 12410,\r\n    \"maxLines\": 256\r\n  }\r\n}\r\n```\r\n",
        "I don't think we should be loading the config at arbitrary places in the code. That means the config could change over the course of the run, which in some cases could be helpful, but in other cases I think could be quite dangerous because it could unintentionally change the behavior of a long-running agent.\r\n\r\nToday, `loadConfig()` is called once in `cli.tsx` (as it should be) and then it should be threaded through from there."
      ],
      "codex-extract-reusable-logic": [
        "First, we should carve out a general notification abstraction rather than duping this logic into the middle of a UI component."
      ],
      "codex-avoid-unnecessary-operations": [
        "Since the other two cases always `flush()`, there should never be anything new to `flush()` in this case, no?",
        "I would skip the `flush()`: all this needs to do is `ack.send(())`.",
        "If we are going to ad logic to control the render late, we should be doing this at a higher level in the TUI so it applies globally, no?",
        "Right, but in general, it's possible the top-level event loop gets too many `AppEvent::Redraw` requests, so why not do the throttling there?",
        "I think you want `let prompt: Cow<'a, Prompt>` if you can to avoid the `clone()`? So in the consequent, it's `Cow::Borrowed` and in the alternative, it's `Cow::Owned`?"
      ],
      "codex-pin-external-action-dependencies": [
        "This one is a bit better:\r\n\r\nhttps://github.com/codespell-project/codespell-problem-matcher",
        "Copy/paste error: that should have been https://github.com/codespell-project/actions-codespell. It has 21 forks and 80 stars, so it feels like at least some folks are keeping an eye on it.",
        "I am opting to keep both. I am saying that instead of referencing the actions by tag (`@v1`, `@v2`), I would like to reference them by full commit hash so the behavior of the action cannot change out from under us.",
        "This one is fine: this is GitHub's responsibility and if this does something malicious, the whole Internet will break anyway. So please keep this as `@v4` so it's easier to eyeball that we're doing something canonical."
      ],
      "codex-minimize-blocking-operations": [
        "Can you use tokio::Command and make this `async` instead? It's cheaper to create tokio tasks than POSIX threads. You should then update `collect_git_info()` to make all these calls in parallel.",
        "I appreciate the timeouts in `collect_git_info()`, though if I am reading it correctly, I suppose this could add ~6s to startup in the worst case? It would be nice to figure out how to make this truly async, since `RolloutRecorder::new()` is on the critical path to startup.\r\n\r\nThe challenge seems to be that we have these lines below:\r\n\r\n```rust\r\n    recorder.record_item(&meta).await?;\r\n    Ok(recorder)\r\n```\r\n\r\nThat is, we don't want `new()` to exit until the first item has recorded and now that is dependent on `collect_git_info()`. Certainly this is fixable, but the bookkeeping may be a bit ugly. What do you think?",
        "Actually, what if we move `collect_git_info(cwd).await` into the lambda passed to `tokio::task::spawn` and then ensure it is written to `file` before the `while let Some(line) = rx.recv().await` loop starts?\r\n\r\nYou could also increase the `git` timeout to 5s maybe?",
        "FYI, if you write it as a single statement, then there is no intermediate `running_requests_id_to_codex_uuid` reference that has to be _dropped_ to release the lock, so the `Drop` happens implicitly as part of the statement executing.\r\n\r\n```suggestion\r\n    running_requests_id_to_codex_uuid.lock().await.running_requests_id_to_codex_uuid.insert(request_id.clone(), session_id);\r\n```",
        "This is slightly better because it results in holding the lock for a shorter amount of time.\r\n\r\nThat extra level of scoping around the use of `self.pending_redraw.lock()` and `flag` ensures that after `*flag = true`, the lock is dropped.\r\n\r\nAlso, `Arc::clone()` is less canonical than just invoking `.clone()`, in my experience.\r\n\r\n```suggestion\r\n        {\r\n            #[allow(clippy::unwrap_used)]\r\n            let mut flag = self.pending_redraw.lock().unwrap();\r\n            if *flag {\r\n                return;\r\n            }\r\n            *flag = true;\r\n        }\r\n\r\n        let tx = self.app_event_tx.clone();\r\n        let pending_redraw = &self.pending_redraw.clone();\r\n        thread::spawn(move || {\r\n            thread::sleep(REDRAW_DEBOUNCE);\r\n            tx.send(AppEvent::Redraw);\r\n            #[allow(clippy::unwrap_used)]\r\n            let mut f = pending.lock().unwrap();\r\n            *f = false;\r\n        });\r\n```"
      ],
      "codex-prevent-command-injection": [
        "Second, we should absolutely not be using `exec()` with a single string arg that appears to be subject to injection risks. At a minimum, we should be using `spawn()` with a list of strings."
      ],
      "codex-contextualize-dont-swallow": [
        "Why do we ignore all the io errors instead of returning `io::Result<()>`?",
        "can it just be this for a string literal?\r\n\r\n```suggestion\r\n        .context(\"failed to load bash grammar\")?;\r\n```",
        "I think this is debatable whether we should proceed in this case. Certainly the program will continue to work, so I guess it's fine? On the other hand, if you are trying to test the wire protocol and you sent bad data, perhaps we should crash so you fix it?",
        "If this is happens, this is a logical error where our treesitter dependency is not set up correctly, so we should not swallow this error.",
        "Would with_context from anyhow work here?"
      ]
    }
  },
  "crenshaw-dev": {
    "repos": [
      "argoproj/argo-cd"
    ],
    "entries": [
      {
        "slug": "argo-cd-choose-appropriate-synchronization-primitives",
        "title": "Choose appropriate synchronization primitives"
      },
      {
        "slug": "argo-cd-comprehensive-function-documentation",
        "title": "Comprehensive function documentation"
      },
      {
        "slug": "argo-cd-configuration-ui-consistency",
        "title": "Configuration UI consistency"
      },
      {
        "slug": "argo-cd-design-extensible-apis",
        "title": "design extensible APIs"
      },
      {
        "slug": "argo-cd-follow-go-naming-conventions",
        "title": "Follow Go naming conventions"
      },
      {
        "slug": "argo-cd-optimize-algorithmic-complexity",
        "title": "optimize algorithmic complexity"
      },
      {
        "slug": "argo-cd-prefer-early-returns",
        "title": "Prefer early returns"
      },
      {
        "slug": "argo-cd-prefer-modern-react-patterns",
        "title": "prefer modern React patterns"
      },
      {
        "slug": "argo-cd-prevent-silent-failures",
        "title": "Prevent silent failures"
      },
      {
        "slug": "argo-cd-provide-comprehensive-explanations",
        "title": "Provide comprehensive explanations"
      },
      {
        "slug": "argo-cd-simplify-code-readability",
        "title": "Simplify code readability"
      },
      {
        "slug": "argo-cd-structured-logging-practices",
        "title": "structured logging practices"
      },
      {
        "slug": "argo-cd-use-descriptive-constants",
        "title": "Use descriptive constants"
      },
      {
        "slug": "argo-cd-validate-conceptual-api-types",
        "title": "validate conceptual API types"
      },
      {
        "slug": "argo-cd-validate-configuration-appropriateness",
        "title": "Validate configuration appropriateness"
      },
      {
        "slug": "argo-cd-validate-external-urls",
        "title": "validate external URLs"
      },
      {
        "slug": "argo-cd-validate-untrusted-inputs",
        "title": "Validate untrusted inputs"
      },
      {
        "slug": "argo-cd-wrap-errors-with-context",
        "title": "Wrap errors with context"
      }
    ],
    "comments": {
      "argo-cd-follow-go-naming-conventions": [
        "I think convention is to add a `_seconds` suffix to timestamp gauges."
      ],
      "argo-cd-design-extensible-apis": [
        "Would it be possible to unmarshal the patch into a map[string]any so log tools aren't forced to parse a JSON string?"
      ],
      "argo-cd-prevent-silent-failures": [
        "Should we disable the button if there's an ongoing refresh? Otherwise looks like the click gets silently ignored."
      ],
      "argo-cd-use-descriptive-constants": [
        "Why 14?",
        "```suggestion\r\n                // Show \"sha256: \" plus the first 7 actual characters of the digest.\r\n                message += ' (' + revision.substring(0, 14) + ')';\r\n```"
      ],
      "argo-cd-validate-untrusted-inputs": [
        "Do we need to be concerned about someone constructing a link that tries to access stuff they shouldn't be accessing? Do we need some validation on repo, chartName, and version beyond what already exists?",
        "I guess I'm thinking more of a malicious scenario, where a user passes `repoName=../../../etc/passwd` or similar. It's not an easy attack path, but it seems like it would be super easy to validate against.",
        "Ditto re: securejoin. If `relPath` refers to a symlink, this join could end up referring to something above `s.dest`.",
        "We should probably require that ConfigMaps outside the argocd namespace have some label advertising them as available for use as a plugin. Otherwise users might use the AppSet as a way to try to leak information from arbitrary ConfigMaps which they may or may not have access to."
      ],
      "argo-cd-prefer-modern-react-patterns": [
        "Should we have a linter for this? I see we use `<Consumer>` a lot."
      ],
      "argo-cd-validate-external-urls": [
        "Need to be super careful about injection here... do we need to do any sanitization on imageUrl?",
        "I'd suggest dropping the image URL from the UI for now and add it as a follow-up enhancement. I think we might want to enforce domain allowlists or something like that."
      ],
      "argo-cd-provide-comprehensive-explanations": [
        "I think \"rather it is the values/valuesObject merged with parameters\" is still valuable to include. Maybe just rephrased to be easier to understand."
      ],
      "argo-cd-optimize-algorithmic-complexity": [
        "While we're in here, I think we should fix this logic. Instead of constructing a single string and then comparing, we should compare the highest priority field, return if not equal, proceed to the next field if equal, etc.",
        "Copilot came up with this:\r\n\r\n```\r\nsort.Slice(newConditions, func(i, j int) bool {\r\n\tleft := newConditions[i]\r\n\tright := newConditions[j]\r\n\r\n\tif left.Type != right.Type {\r\n\t\treturn left.Type < right.Type\r\n\t}\r\n\tif left.Message != right.Message {\r\n\t\treturn left.Message < right.Message\r\n\t}\r\n\tif left.Status != right.Status {\r\n\t\treturn left.Status < right.Status\r\n\t}\r\n\tif left.Reason != right.Reason {\r\n\t\treturn left.Reason < right.Reason\r\n\t}\r\n\treturn left.LastTransitionTime.Before(right.LastTransitionTime)\r\n})\r\n```",
        "```suggestion\r\n\t\tif left.Status != right.Status {\r\n\t\t\treturn left.Status < right.Status\r\n\t\t}\r\n\t\tif left.Reason != right.Reason {\r\n\t\t\treturn left.Reason < right.Reason\r\n\t\t}\r\n\t\tif left.Message != right.Message {\r\n\t\t\treturn left.Message < right.Message\r\n\t\t}\r\n```\r\n\r\nThis should give us somewhat more stable order, since the messages are arbitrary."
      ],
      "argo-cd-comprehensive-function-documentation": [
        "I feel like this function is doing too much... the revisions and phase output parameters seem to be simple aliases of their respective fields in `app.Status.OperationState`. If we need short var names, we can just do\r\n\r\n```go\r\nopPhase := app.Status.OperationState.Phase\r\nopRevisions := app.Status.OperationState.SyncResult.GetRevisions() # would probably need a new receiver\r\n```\r\n\r\nThen this function could focus on the first return param, which seems to involve the more interesting logic.",
        "Maybe but I'm really struggling to understand what this function does due to it being so crowded.\r\n\r\nFor now I'd settle for a docstring thoroughly explaining the intent and a TODO to remove the unnecessary behavior.",
        "Not really, because I don't think I fully understand either the behavior or the intent of the function (regarding just the first output param).\r\n\r\n> alreadyAttemptedSync returns whether the most recent sync was performed against the desiredRevisions and with the same app source config which are currently set in the app.\r\n\r\nI think I basically follow that, but the actual behavior is:\r\n\r\n1) If there is no operation state, return false, because we can't confirm the above two things.\r\n2) If we have an operation state but not a sync result, we return true if and only if the phase was marked completed. This is weird, because we haven't confirmed whether the synced revisions match `desiredRevisions` or whether the source config has changed. \"Phase is completed\" wasn't even mentioned as a criteria in the docstring. Why return true if the phase is completed? There's a docstring above that `return`, but it seems to be explaining the behavior of the calling function, not the `alreadyAttemptedSync` function.\r\n3) If `newRevisionHasChanges`, return `false` if `desiredRevisions` doesn't match the synced revisions. What does `newRevisionHasChanges` mean? And why aren't we checking whether the synced source config has changed, like the function docs mentioned?\r\n4) If not `newRevisionHasChanges`, just return whether the synced source config matches current source config. But it's unclear why we're not comparing desired revisions to synced revisions. Is that already confirmed to be true because `newRevisionHasChanges` is false? If `newRevisionHasChanges` means \"desired revisions don't match synced revisions\", why are we checking that again in the previous point?",
        "> alreadyAttemptedSync is meant to help the caller understand whether an identical sync operation has been attempted, to avoid excessively retrying the exact same sync operation.\r\n>\r\n> alreadyAttemptedSync returns true if either 1) newRevisionHasChanges is true and the most recently synced revision(s) exactly match the given desiredRevisions, 2) newRevisionHasChanges is false and the most recently synced app source configuration matches exactly the current app source configuration, or 3) the most recent operation state is missing a sync result but the sync phase is completed (this can happen when there are errors that cause the sync result not to be persisted). The last case returns true, because the caller should treat such a failed sync as an attempt. \r\n>\r\n> alreadyAttemptedSync returns false if the operation state is not set at all. This happens when the app is brand new or when a new operation has started.\r\n>\r\n> alreadyAttemptedSync also returns the most recently synced revisions and the most recent sync operation's phase.\r\n>\r\n> TODO: remove the last two return parameters, since they're effectively just aliases for fields on the app object. If the nil checks are too cumbersome for the caller, they can be moved into separate utility functions to avoid crowding this one.\r\n\r\nThis would get closer. But I think the references to `newRevisionHasChanges` should be replaced with a plain-language explanation of what that parameters means, and that param should be documented.",
        "The description of `newRevisionHasChanges`. cleared up a lot for me.\r\n\r\nWould probably be good to standardize on one var name for that variable and document it in each function that uses it. But that can be a future enhancement."
      ],
      "argo-cd-prefer-early-returns": [
        "I'd short-circuit instead of nesting\r\n\r\n```go\r\nif kubeutil.IsCRD(live) {\r\n// CRDs don't get tracking annotations.\r\nreturn nil\r\n}\r\n```",
        "We can avoid the deep nesting by just short-circuiting here.\r\n\r\n```suggestion\r\n\tif !needToUpdateConditions {\r\n\t\treturn nil\r\n\t}\r\n```"
      ],
      "argo-cd-simplify-code-readability": [
        "And one more... might be worth a lint rule? :-) "
      ],
      "argo-cd-validate-configuration-appropriateness": [
        "I really don't think we should do this. imo using a fork of a lightly-maintained library is worse than just using the lightly-maintained library."
      ],
      "argo-cd-wrap-errors-with-context": [
        "```suggestion\r\n\t\treturn nil, fmt.Errorf(\"failed to initialize oci client: %w\", err)\r\n```",
        "```suggestion\r\n\t\treturn nil, \"\", fmt.Errorf(\"failed to initialize oci client: %w\", err)\r\n```",
        "Let's wrap this error for easier debugging."
      ],
      "argo-cd-structured-logging-practices": [
        "A couple suggestions:\r\n\r\n1) I'd unmarshal the patch into a map[any]any and log it as its own field, so it's parseable by log tools\r\n2) I'd exclude the patch from the event: some people put _huge_ stuff in their app spec",
        "```suggestion\r\n\t\t\t\tlogFields := log.Fields{}\r\n```\r\n\r\nProbably fine to start with an empty set and replace it if the type assertion is successful.",
        "```suggestion\r\n\t\t\tlogCtx.WithFields(applog.GetAppLogFields(&generatedApplications[i])).Errorf(\"validation error found during application validation: %s\", message)\r\n```\r\n\r\nSomething like that would get you the standard app log fields.",
        "Makes sense. At any rate, I'd go with \"application\" instead of \"app\" to match the standard: https://argo-cd.readthedocs.io/en/latest/operator-manual/security/#standard-application-log-fields",
        "added revision and repo url"
      ],
      "argo-cd-validate-conceptual-api-types": [
        "Is this necessary? As far as I can tell, OCI isn't a source \"type\" in the same way that Helm or Kustomize are. An OCI repo could contain Helm, Kustomize, Directory, or Plugin-style manifetss."
      ],
      "argo-cd-choose-appropriate-synchronization-primitives": [
        "I haven't used `atomic` much... could we just use `atomic.Int64`? Docs seem to recommend that: https://pkg.go.dev/sync/atomic#AddInt64"
      ],
      "argo-cd-configuration-ui-consistency": [
        "Could we have some unit tests for these utility functions? I think we're trying to mimic Helm's values file merging behavior. If we find later that our merge algorithms differ from Helm's, it would be good to have unit tests to help safely change our algorithm to match Helm's."
      ]
    }
  },
  "gruebel": {
    "repos": [
      "bridgecrewio/checkov"
    ],
    "entries": [
      {
        "slug": "checkov-backward-compatible-parameters",
        "title": "Backward compatible parameters"
      },
      {
        "slug": "checkov-choose-optimal-algorithms",
        "title": "Choose optimal algorithms"
      },
      {
        "slug": "checkov-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "checkov-comprehensive-security-scanning",
        "title": "Comprehensive security scanning"
      },
      {
        "slug": "checkov-configure-security-scanners-completely",
        "title": "Configure security scanners completely"
      },
      {
        "slug": "checkov-consistent-naming-conventions",
        "title": "Consistent naming conventions"
      },
      {
        "slug": "checkov-document-configuration-consistently",
        "title": "Document configuration consistently"
      },
      {
        "slug": "checkov-document-configuration-options",
        "title": "Document configuration options"
      },
      {
        "slug": "checkov-ensure-dependency-compatibility",
        "title": "Ensure dependency compatibility"
      },
      {
        "slug": "checkov-meaningful-identifier-names",
        "title": "Meaningful identifier names"
      },
      {
        "slug": "checkov-precise-configuration-validation",
        "title": "Precise configuration validation"
      },
      {
        "slug": "checkov-preserve-api-compatibility",
        "title": "Preserve API compatibility"
      },
      {
        "slug": "checkov-safe-dictionary-access",
        "title": "Safe dictionary access"
      },
      {
        "slug": "checkov-safe-dictionary-navigation",
        "title": "Safe dictionary navigation"
      },
      {
        "slug": "checkov-strategic-error-handling",
        "title": "Strategic error handling"
      },
      {
        "slug": "checkov-strategic-exception-management",
        "title": "Strategic exception management"
      },
      {
        "slug": "checkov-support-all-target-environments",
        "title": "Support all target environments"
      },
      {
        "slug": "checkov-use-appropriate-logging-levels",
        "title": "Use appropriate logging levels"
      },
      {
        "slug": "checkov-use-pytest-best-practices",
        "title": "Use pytest best practices"
      },
      {
        "slug": "checkov-validate-configurations-correctly",
        "title": "Validate configurations correctly"
      },
      {
        "slug": "checkov-write-pythonic-code",
        "title": "Write pythonic code"
      }
    ],
    "comments": {
      "checkov-validate-configurations-correctly": [
        "```suggestion\r\n      operator: not_exists\r\n```\r\nshouldn't it be not set, because we don't want a public IP?",
        "```suggestion\r\n  operator: \"not_equals_ignore_case\"\r\n```\r\nit should not `true` right?",
        "you can change it to a 'not empty', but a not empty `origin_access_identity` is a broken configuration, but correct me, if I'm wrong."
      ],
      "checkov-safe-dictionary-navigation": [
        "```suggestion\r\n        if properties and isinstance(properties, dict):\r\n```\r\nlet's make sure, ew deal with a dictionary otherwise we will have a problem.",
        "```suggestion\r\n        principal = conf.get(\"principal\")\r\n        if principal and isintsance(principal, list) and isinstance(principal[0], str):\r\n            principal_parts = principal[0].split('.')\r\n```\r\nthis is a bit tricky, we need to be a bit more cautious on the types. Quite often Terraform plan files come with unexpected default values and break our checks.",
        "```suggestion\r\n    inline_suppressions_by_cve = inline_suppressions.get(\"cves\", {}).get(\"byCve\", {})\r\n    for cve_suppression in inline_suppressions_by_cve:\r\n        cve_id = cve_suppression.get(\"cveId\")\r\n        if cve_id:\r\n            cve_by_cve_map[cve_id] = cve_suppression\r\n```\r\nðŸ™‚ ",
        "`not` also works, but if it is an empty `dict` then there is no need to override it again\r\n```suggestion\r\n            if each[\"change\"][\"before\"] is None:\r\n                each[\"change\"][\"before\"] = {}\r\n            if each[\"change\"][\"after\"] is None:\r\n                each[\"change\"][\"after\"] = {}\r\n```"
      ],
      "checkov-comprehensive-security-scanning": [
        "we usually recommend to run docker with `--tty` for better output handle.",
        "```suggestion\r\n    entry: checkov -d . --framework secrets --enable-secret-scan-all-files\r\n```\r\ncould you also add the flag `--enable-secret-scan-all-files` so all the files are scanned."
      ],
      "checkov-document-configuration-consistently": [
        "move this to the `Settings` block as an env var",
        "nice catch!"
      ],
      "checkov-strategic-error-handling": [
        "@mikeurbanski1 any thoughts about raising an exception here or should we just log it and return the normal URL?",
        "do we really want to raise an exception or just log a message?",
        "in theory it could, but this would mean something went wrong during the context creation. But I will add a check and log a message."
      ],
      "checkov-choose-optimal-algorithms": [
        "you could also create a `defaultdict`, then you don't have to this check manually \r\n```python\r\ndirs_to_definitions = defaultdict(list)\r\ndirs_to_definitions[dir_path].append({tf_definition_key: tf_value})\r\n```",
        "how about making this a `set()` then you don't need to transform it to a set and back to list ðŸ™‚ ",
        "also thought the same, but I think it is a bigger change, because of the typing mismatch.",
        "```suggestion\r\n            for field, value in each[\"change\"][\"before\"].items():\r\n                if value != each[\"change\"][\"after\"].get(field):\r\n```",
        "also make sure to skip the field names `__startline__` and `__endline__`, there is aconstant `LINE_FIELD_NAMES` which stores them as a set.",
        "just double checking, this has no bad side effect, because we would create more often edges, right?",
        "if we for some reason get more than 1 `target_variables`, we didn't create an edge before and just continued. If the tests are passing, then it is probably not an issue ðŸ˜„ "
      ],
      "checkov-choose-optimal-data-structures": [
        "you could also create a `defaultdict`, then you don't have to this check manually \r\n```python\r\ndirs_to_definitions = defaultdict(list)\r\ndirs_to_definitions[dir_path].append({tf_definition_key: tf_value})\r\n```",
        "how about making this a `set()` then you don't need to transform it to a set and back to list ðŸ™‚ ",
        "also thought the same, but I think it is a bigger change, because of the typing mismatch.",
        "just double checking, this has no bad side effect, because we would create more often edges, right?",
        "if we for some reason get more than 1 `target_variables`, we didn't create an edge before and just continued. If the tests are passing, then it is probably not an issue ðŸ˜„ "
      ],
      "checkov-meaningful-identifier-names": [
        "yeah, sure. I just named them identical to keep the code changes minimal ðŸ˜„ "
      ],
      "checkov-use-pytest-best-practices": [
        "the parent calss `TestBaseSolver` comes with a couple of convenient functions, which I didn't plan to duplicate, but I will double check what's the effort ðŸ™‚ "
      ],
      "checkov-backward-compatible-parameters": [
        "```suggestion\r\n                    messages=messages,\r\n```\r\notherwise you only send the first message, same a few lines lower"
      ],
      "checkov-write-pythonic-code": [
        "```suggestion\r\n                if mod['Key']:\r\n```\r\nit is faster to check for truthiness, which means it is not empty.",
        "```suggestion\r\n        self._address_to_tf_vertex_map = {\r\n            vertex.attributes[TF_PLAN_RESOURCE_ADDRESS]: vertex\r\n            for vertex in self.tf_graph.vertices\r\n            if vertex.block_type == BlockType.RESOURCE:\r\n        }\r\n```\r\nI think you can do it via a dict comprehension",
        "```suggestion\r\n                if 'source_arn' in conf or 'source_account' in conf:  # If either of these are set, we're good and the check should pass.\r\n```\r\nno need to explicitly add `.keys()` because it is the default when doing a lookup in a dict."
      ],
      "checkov-ensure-dependency-compatibility": [
        "this will not work. this lock file was created with Python 3.10+ therefore any CI jobs running on 3.8 or 3.9 will fail"
      ],
      "checkov-precise-configuration-validation": [
        "```suggestion\r\n      operator: not_exists\r\n```\r\nshouldn't it be not set, because we don't want a public IP?",
        "```suggestion\r\n  operator: \"not_equals_ignore_case\"\r\n```\r\nit should not `true` right?",
        "you can change it to a 'not empty', but a not empty `origin_access_identity` is a broken configuration, but correct me, if I'm wrong.",
        "please add an or block, when `shared_access_key_enabled` is set to `false` then it should pass without setting the expiration"
      ],
      "checkov-document-configuration-options": [
        "move this to the `Settings` block as an env var",
        "nice catch!"
      ],
      "checkov-preserve-api-compatibility": [
        "```suggestion\r\n                if runner.graph_manager:\r\n                    check_type_to_graph = {runner.check_type: runner.graph_manager.get_reader_endpoint()}\r\n                    return report, runner.check_type, runner.graph_manager.get_reader_endpoint()\r\n                return report, None, None\r\n```\r\nhow about returning all as a normal tuple, then you can check if both are not `None` and update `full_check_type_to_graph[check_type] = reader_endpoint`"
      ],
      "checkov-support-all-target-environments": [
        "this will not work. this lock file was created with Python 3.10+ therefore any CI jobs running on 3.8 or 3.9 will fail"
      ],
      "checkov-configure-security-scanners-completely": [
        "we usually recommend to run docker with `--tty` for better output handle.",
        "```suggestion\r\n    entry: checkov -d . --framework secrets --enable-secret-scan-all-files\r\n```\r\ncould you also add the flag `--enable-secret-scan-all-files` so all the files are scanned."
      ],
      "checkov-safe-dictionary-access": [
        "```suggestion\r\n        if properties and isinstance(properties, dict):\r\n```\r\nlet's make sure, ew deal with a dictionary otherwise we will have a problem.",
        "```suggestion\r\n        principal = conf.get(\"principal\")\r\n        if principal and isintsance(principal, list) and isinstance(principal[0], str):\r\n            principal_parts = principal[0].split('.')\r\n```\r\nthis is a bit tricky, we need to be a bit more cautious on the types. Quite often Terraform plan files come with unexpected default values and break our checks.",
        "```suggestion\r\n    inline_suppressions_by_cve = inline_suppressions.get(\"cves\", {}).get(\"byCve\", {})\r\n    for cve_suppression in inline_suppressions_by_cve:\r\n        cve_id = cve_suppression.get(\"cveId\")\r\n        if cve_id:\r\n            cve_by_cve_map[cve_id] = cve_suppression\r\n```\r\nðŸ™‚ ",
        "`not` also works, but if it is an empty `dict` then there is no need to override it again\r\n```suggestion\r\n            if each[\"change\"][\"before\"] is None:\r\n                each[\"change\"][\"before\"] = {}\r\n            if each[\"change\"][\"after\"] is None:\r\n                each[\"change\"][\"after\"] = {}\r\n```"
      ],
      "checkov-consistent-naming-conventions": [
        "you can also use here `def get_evaluated_keys(self) -> List[str]:` instead of adding them dynamically",
        "```suggestion\r\n        self.ALLOW_KUSTOMIZE_FILE_EDITS = convert_str_to_bool(os.getenv(\"CHECKOV_ALLOW_KUSTOMIZE_FILE_EDITS\", False))\r\n```\r\nplease prefix it with `CHECKOV_` to make it clear, it is an internal env var.",
        "had the same thought ðŸ˜„ ",
        "yeah, sure. I just named them identical to keep the code changes minimal ðŸ˜„ "
      ],
      "checkov-strategic-exception-management": [
        "@mikeurbanski1 any thoughts about raising an exception here or should we just log it and return the normal URL?",
        "do we really want to raise an exception or just log a message?",
        "in theory it could, but this would mean something went wrong during the context creation. But I will add a check and log a message."
      ],
      "checkov-use-appropriate-logging-levels": [
        "```suggestion\r\n            logging.debug(f\"OpenAI request returned: {completion}\")\r\n```\r\nor something similar to have a direct context. Also debug level is more than enough.\r\n",
        "instead of print use `logging.info(...)`",
        "just thinking about the log level, if maybe `info` is enough, depends on how critical it is to the user. "
      ]
    }
  },
  "emdneto": {
    "repos": [
      "open-telemetry/opentelemetry-python"
    ],
    "entries": [
      {
        "slug": "opentelemetry-python-adapt-for-linter-compatibility",
        "title": "Adapt for linter compatibility"
      },
      {
        "slug": "opentelemetry-python-explicit-ci-configurations",
        "title": "Explicit CI configurations"
      },
      {
        "slug": "opentelemetry-python-follow-python-naming-conventions",
        "title": "Follow Python naming conventions"
      },
      {
        "slug": "opentelemetry-python-future-proof-api-design",
        "title": "Future-proof API design"
      },
      {
        "slug": "opentelemetry-python-handle-exceptions-appropriately",
        "title": "Handle exceptions appropriately"
      },
      {
        "slug": "opentelemetry-python-maintain-consistent-naming",
        "title": "Maintain consistent naming"
      },
      {
        "slug": "opentelemetry-python-optimize-configuration-structure",
        "title": "Optimize configuration structure"
      },
      {
        "slug": "opentelemetry-python-place-attributes-correctly",
        "title": "Place attributes correctly"
      },
      {
        "slug": "opentelemetry-python-precise-configuration-specifications",
        "title": "Precise configuration specifications"
      },
      {
        "slug": "opentelemetry-python-prevent-recursive-logging-calls",
        "title": "Prevent recursive logging calls"
      },
      {
        "slug": "opentelemetry-python-structured-changelog-documentation",
        "title": "Structured changelog documentation"
      },
      {
        "slug": "opentelemetry-python-use-proper-testing-frameworks",
        "title": "Use proper testing frameworks"
      }
    ],
    "comments": {
      "opentelemetry-python-explicit-ci-configurations": [
        "I changed this to protobuf5 to avoid conflicts with other envs like: if I run `tox -e py312-test-opentelemetry-exporter-zipkin-proto-http -r -- -ra` it also run tests for opentelemetry-proto, leading to a conflict. ",
        "The problem is in the CI we are running `run: tox -e benchmark-opentelemetry-sdk -- -k opentelemetry-sdk/benchmarks --benchmark-json=opentelemetry-sdk/output.json` but the output.json is empty. with that I was able to get the output.json with the expected data.",
        "Another option is to keep the whole command in `tox.ini` and just call `tox -e benchmark-opentelemetry-sdk` on CI "
      ],
      "opentelemetry-python-structured-changelog-documentation": [
        "Since we don't have a breaking changes section in changelog, does it make sense to add something like `[BREAKING]`  here?"
      ],
      "opentelemetry-python-prevent-recursive-logging-calls": [
        "```suggestion\r\n    # resulting in endless recursive calls that crash the program.\r\n    # See https://github.com/open-telemetry/opentelemetry-python/issues/4261 \r\n```",
        "+1 to this â¬†ï¸ ",
        "We are already using warnings https://github.com/open-telemetry/opentelemetry-python/blob/main/opentelemetry-sdk/src/opentelemetry/sdk/_logs/_internal/__init__.py#L209 in other places "
      ],
      "opentelemetry-python-future-proof-api-design": [
        "Should we change the base classes to float? Got this when running pyright\r\n```\r\n /workspaces/opentelemetry-python/exporter/opentelemetry-exporter-otlp-proto-grpc/src/opentelemetry/exporter/otlp/proto/grpc/metric_exporter/__init__.py:159:9 - error: Method \"export\" overrides class \"MetricExporter\" in an incompatible manner\r\n  Â Â Parameter 3 type mismatch: base parameter is type \"float\", override parameter is type \"int | None\"\r\n  Â Â Â Â Type \"float\" is not assignable to type \"int | None\"\r\n  Â Â Â Â Â Â \"float\" is not assignable to \"int\"\r\n  Â Â Â Â Â Â \"float\" is not assignable to \"None\" (reportIncompatibleMethodOverride)\r\n```\r\n\r\nSame for other signals",
        "I think that adding or removing methods, moving or removing required parameters, and changing a parameterâ€™s default value are breaking changes.\r\n\r\nSimilarly, there are other changes we want to make that will break usersâ€”see https://github.com/open-telemetry/opentelemetry-python/issues/4044.\r\n\r\nMaybe we can treat this as a bug fix, even though it will introduce a behavioural breaking change.\r\n\r\n>Also I could remove the timeout param to export and go ahead with the rest of this change, and we could discuss this separately.\r\n\r\nI'm ok with. Do you mean keep only the changes for the retry logic in both grpc/http exporters right?"
      ],
      "opentelemetry-python-follow-python-naming-conventions": [
        "No big deal, but noticed we are using `opentelemetry.util.types.Attributes` typing in several places for attributes typing. Maybe it makes sense to use it as well"
      ],
      "opentelemetry-python-use-proper-testing-frameworks": [
        "Yes, tests will also be checked ðŸ˜“. I agree it will be annoying. Let me see if we can exclude them. ",
        "thanks for the approval, but I'll put it in draft to write the check using griffe Python API. It will be easier to maintain.",
        "@aabmass I've wrote a simple script that uses griffe python API. We can now iterate over the breakages and implement the logic we want. Let me know wdyt",
        "How about?\r\n\r\n```suggestion\r\nexport SERVICE_ENDPOINT=http://127.0.0.1:5000/verify-tracecontext\r\npytest -k 'not test_tracestate_duplicated_keys'\r\n```",
        "Nope. It won't work ",
        "@jomcgi It works for me as well. ",
        "@jomcgi Now I think it's just a matter of adding pytest in tox.ini for the tracecontext test."
      ],
      "opentelemetry-python-optimize-configuration-structure": [
        "I think tox in newer versions is able to detect changes to requirements files. At least, locally it's working for me",
        "Right"
      ],
      "opentelemetry-python-place-attributes-correctly": [
        "```suggestion\r\n- Add attributes field in  `MeterProvider.get_meter` and `InstrumentationScope`\r\n```"
      ],
      "opentelemetry-python-handle-exceptions-appropriately": [
        "Added. Thanks"
      ],
      "opentelemetry-python-adapt-for-linter-compatibility": [
        "Lint is failing prob related to this https://github.com/protocolbuffers/protobuf/issues/10372",
        "I tested locally and `--prefer-stubs` solve the lint issue, but we need to bump pylint to 3.2.1 which breaks the lint for SDK and opentelemetry-exporter-prometheus. ",
        "@aabmass I think we have  some options:\r\n\r\n1. Fix lint separately in another PR (https://github.com/open-telemetry/opentelemetry-python/pull/4209)\r\n2. Assign all top-level globals explicitly in the generated code (will require manually or automated adding)\r\n3. Skip the lint ðŸ˜“ "
      ],
      "opentelemetry-python-maintain-consistent-naming": [
        "I also prefer `tox -e pyright`, but we are using the wording `typecheck` in -contrib repo, so let's use the same here as well"
      ],
      "opentelemetry-python-precise-configuration-specifications": [
        "Right. And for the whole file we can use something like this which also works: `# pyright: strict, reportUnusedVariable=none`"
      ]
    }
  },
  "mattlord": {
    "repos": [
      "vitessio/vitess"
    ],
    "entries": [
      {
        "slug": "vitess-consistent-database-apis",
        "title": "Consistent database APIs"
      },
      {
        "slug": "vitess-document-configuration-precedence",
        "title": "Document configuration precedence"
      },
      {
        "slug": "vitess-dynamic-configuration-needs-validation",
        "title": "Dynamic configuration needs validation"
      },
      {
        "slug": "vitess-explicit-nil-handling",
        "title": "Explicit nil handling"
      },
      {
        "slug": "vitess-extract-shared-code-patterns",
        "title": "Extract shared code patterns"
      },
      {
        "slug": "vitess-log-levels-and-clarity",
        "title": "Log levels and clarity"
      },
      {
        "slug": "vitess-manage-workflow-state-transitions",
        "title": "Manage workflow state transitions"
      },
      {
        "slug": "vitess-meaningful-consistent-naming",
        "title": "Meaningful consistent naming"
      },
      {
        "slug": "vitess-metric-design-best-practices",
        "title": "Metric design best practices"
      },
      {
        "slug": "vitess-optimize-memory-allocation",
        "title": "Optimize memory allocation"
      },
      {
        "slug": "vitess-prevent-concurrent-access-races",
        "title": "Prevent concurrent access races"
      },
      {
        "slug": "vitess-robust-network-handling",
        "title": "Robust network handling"
      },
      {
        "slug": "vitess-size-fields-appropriately",
        "title": "Size fields appropriately"
      },
      {
        "slug": "vitess-standardize-error-wrapping-patterns",
        "title": "Standardize error wrapping patterns"
      },
      {
        "slug": "vitess-use-parameterized-queries",
        "title": "Use parameterized queries"
      },
      {
        "slug": "vitess-use-testify-assertion-libraries",
        "title": "Use testify assertion libraries"
      }
    ],
    "comments": {
      "vitess-consistent-database-apis": [
        "Yeah, it returns an error if there's a validation failure: https://github.com/vitessio/vitess/blob/76abae45fe6c8881c5ec308d05d1b648d058fa79/go/vt/vtctl/vtctl.go#L3259-L3269\r\n\r\nWe could change that, of course, but here I'm primarily just porting these missing commands over.",
        "\"We\" here would be whomever uses and consumes this command (certainly not me to date). This PR is about migrating the missing but still used/needed commands from the old client to the new, where maintaining \"drop in compatibility\" is the only concern. Why are we discussing this here? Is there some feature gap or request? If so, we should open a feature request that lays it out and someone can pick that up at a later date.\r\n\r\nI'm actually leaning towards removing [the endtoend test code for it](https://github.com/vitessio/vitess/blob/a57ae939ec5dbddc02e9eac31e1519113e904eeb/go/test/endtoend/sharded/sharded_keyspace_test.go#L169-L172) (`ValidatePermissions*`, as I did with `CopySchemaShard`) since I cannot find anything that actually uses it today (in vitess, vitess-operator, etc):\r\n```\r\ngo/test/endtoend/sharded/sharded_keyspace_test.go:      err = clusterInstance.VtctlclientProcess.ExecuteCommand(\"ValidatePermissionsShard\", fmt.Sprintf(\"%s/%s\", keyspaceName, shard1.Name))\r\ngo/test/endtoend/sharded/sharded_keyspace_test.go:      err = clusterInstance.VtctlclientProcess.ExecuteCommand(\"ValidatePermissionsKeyspace\", keyspaceName)\r\ngo/vt/vtctl/vtctl.go:                           name:   \"ValidatePermissionsShard\",\r\ngo/vt/vtctl/vtctl.go:                           method: commandValidatePermissionsShard,\r\ngo/vt/vtctl/vtctl.go:                           name:   \"ValidatePermissionsKeyspace\",\r\ngo/vt/vtctl/vtctl.go:                           method: commandValidatePermissionsKeyspace,\r\ngo/vt/vtctl/vtctl.go:func commandValidatePermissionsShard(ctx context.Context, wr *wrangler.Wrangler, subFlags *pflag.FlagSet, args []string) error {\r\ngo/vt/vtctl/vtctl.go:           return fmt.Errorf(\"the <keyspace/shard> argument is required for the ValidatePermissionsShard command\")\r\ngo/vt/vtctl/vtctl.go:   return wr.ValidatePermissionsShard(ctx, keyspace, shard)\r\ngo/vt/vtctl/vtctl.go:func commandValidatePermissionsKeyspace(ctx context.Context, wr *wrangler.Wrangler, subFlags *pflag.FlagSet, args []string) error {\r\ngo/vt/vtctl/vtctl.go:           return fmt.Errorf(\"the <keyspace name> argument is required for the ValidatePermissionsKeyspace command\")\r\ngo/vt/vtctl/vtctl.go:   return wr.ValidatePermissionsKeyspace(ctx, keyspace)\r\ngo/vt/vtctld/vtctld.go: actionRepo.RegisterKeyspaceAction(\"ValidatePermissionsKeyspace\",\r\ngo/vt/vtctld/vtctld.go:                 return \"\", wr.ValidatePermissionsKeyspace(ctx, keyspace)\r\ngo/vt/vtctld/vtctld.go: actionRepo.RegisterShardAction(\"ValidatePermissionsShard\",\r\ngo/vt/vtctld/vtctld.go:                 return \"\", wr.ValidatePermissionsShard(ctx, keyspace, shard)\r\ngo/vt/wrangler/permissions.go:// ValidatePermissionsShard validates all the permissions are the same\r\ngo/vt/wrangler/permissions.go:func (wr *Wrangler) ValidatePermissionsShard(ctx context.Context, keyspace, shard string) error {\r\ngo/vt/wrangler/permissions.go:// ValidatePermissionsKeyspace validates all the permissions are the same\r\ngo/vt/wrangler/permissions.go:func (wr *Wrangler) ValidatePermissionsKeyspace(ctx context.Context, keyspace string) error {\r\ngo/vt/wrangler/permissions.go:          return wr.ValidatePermissionsShard(ctx, keyspace, shards[0])\r\ngo/vt/wrangler/testlib/permissions_test.go:     // run ValidatePermissionsKeyspace, this should work\r\ngo/vt/wrangler/testlib/permissions_test.go:     if err := vp.Run([]string{\"ValidatePermissionsKeyspace\", primary.Tablet.Keyspace}); err != nil {\r\ngo/vt/wrangler/testlib/permissions_test.go:             t.Fatalf(\"ValidatePermissionsKeyspace failed: %v\", err)\r\ngo/vt/wrangler/testlib/permissions_test.go:     // run ValidatePermissionsKeyspace again, this should now fail\r\ngo/vt/wrangler/testlib/permissions_test.go:     if err := vp.Run([]string{\"ValidatePermissionsKeyspace\", primary.Tablet.Keyspace}); err == nil || !strings.Contains(err.Error(), \"has an extra user\") {\r\ngo/vt/wrangler/testlib/permissions_test.go:             t.Fatalf(\"ValidatePermissionsKeyspace has unexpected err: %v\", err)\r\n```\r\n\r\nSo rather than spending time discussing how we could improve it... I think we should kill it off during the client transition as it no longer seems to serve a valid/used purpose as a client command. Unless you for some reason feel this is needed (and should be improved at a later time)?",
        "To provide some context, the `GetPermissions` client command simply returns the contents of the `mysql.user` and `mysql.db` tables as a JSON doc ([tmc](https://github.com/vitessio/vitess/blob/a57ae939ec5dbddc02e9eac31e1519113e904eeb/go/vt/vttablet/tabletmanager/rpc_actions.go#L64-L67) -> [mysqlctl](https://github.com/vitessio/vitess/blob/a57ae939ec5dbddc02e9eac31e1519113e904eeb/go/vt/mysqlctl/permissions.go#L26-L52)):\r\n```\r\nâ¯ vtctldclient GetPermissions zone1-100\r\n{\r\n  \"user_permissions\": [\r\n    {\r\n      \"host\": \"%\",\r\n      \"user\": \"vt_repl\",\r\n      \"password_checksum\": \"0\",\r\n      \"privileges\": {\r\n        \"Alter_priv\": \"N\",\r\n        \"Alter_routine_priv\": \"N\",\r\n        \"Create_priv\": \"N\",\r\n        \"Create_role_priv\": \"N\",\r\n        \"Create_routine_priv\": \"N\",\r\n        \"Create_tablespace_priv\": \"N\",\r\n        \"Create_tmp_table_priv\": \"N\",\r\n        \"Create_user_priv\": \"N\",\r\n        \"Create_view_priv\": \"N\",\r\n        \"Delete_priv\": \"N\",\r\n        \"Drop_priv\": \"N\",\r\n        \"Drop_role_priv\": \"N\",\r\n        \"Event_priv\": \"N\",\r\n        \"Execute_priv\": \"N\",\r\n        \"File_priv\": \"N\",\r\n        \"Grant_priv\": \"N\",\r\n        \"Index_priv\": \"N\",\r\n        \"Insert_priv\": \"N\",\r\n        \"Lock_tables_priv\": \"N\",\r\n        \"Password_require_current\": \"\",\r\n        \"Password_reuse_history\": \"\",\r\n        \"Password_reuse_time\": \"\",\r\n        \"Process_priv\": \"N\",\r\n        \"References_priv\": \"N\",\r\n        \"Reload_priv\": \"N\",\r\n        \"Repl_client_priv\": \"N\",\r\n        \"Repl_slave_priv\": \"Y\",\r\n        \"Select_priv\": \"N\",\r\n        \"Show_db_priv\": \"N\",\r\n        \"Show_view_priv\": \"N\",\r\n        \"Shutdown_priv\": \"N\",\r\n        \"Super_priv\": \"N\",\r\n        \"Trigger_priv\": \"N\",\r\n        \"Update_priv\": \"N\",\r\n        \"User_attributes\": \"\",\r\n        \"account_locked\": \"N\",\r\n        \"authentication_string\": \"\",\r\n        \"max_connections\": \"0\",\r\n        \"max_questions\": \"0\",\r\n        \"max_updates\": \"0\",\r\n        \"max_user_connections\": \"0\",\r\n        \"password_expired\": \"N\",\r\n        \"password_lifetime\": \"\",\r\n        \"plugin\": \"mysql_native_password\",\r\n        \"ssl_cipher\": \"\",\r\n        \"ssl_type\": \"\",\r\n        \"x509_issuer\": \"\",\r\n        \"x509_subject\": \"\"\r\n      }\r\n    },\r\n...\r\n```\r\n\r\nThe `ValidatePermissions*` commands then tells you if the output differs within a shard or a keyspace. I don't disagree that telling you what nodes differ and how would be useful (not in this PR, but later enhancements)... IF this is actually still useful in the broader sense today. I haven't yet seen anything to indicate that it's still used or still useful. ",
        "In looking for uses of the ValidatePermissons commands (I found none) I did find uses of CopySchemaShard. So I think I will have to port that over.",
        "One last note... FWIW the functions used for the `ValidatePermissions*` commands do include the details in the error message: https://github.com/vitessio/vitess/blob/a57ae939ec5dbddc02e9eac31e1519113e904eeb/go/vt/mysqlctl/tmutils/permissions.go#L164\r\n\r\nSo I'm going to put this comment thread to bed for now.  ðŸ™‚ ",
        "I'm going to mark this as resolved for now. But I'm going to add example outputs between the clients for each command added here. For example:\r\n```\r\nâ¯ vtctlclient ValidatePermissionsKeyspace commerce\r\nW0101 13:58:38.218227   32307 log.go:39] Failed to read in config : Config File \"vtconfig\" Not Found in \"[/Users/matt/git/vitess]\". This is optional, and can be ignored if you are not using config files. For a detailed explanation, see https://github.com/vitessio/vitess/blob/main/doc/viper/viper.md#config-files.\r\nValidatePermissionsKeyspace Error: rpc error: code = Unknown desc = permissions diffs: zone1-0000000100 has an extra user localhost:mlord;zone1-0000000100 has an extra user localhost:mlord\r\nE0101 13:58:38.237814   32307 main.go:105] remote error: rpc error: code = Unknown desc = permissions diffs: zone1-0000000100 has an extra user localhost:mlord;zone1-0000000100 has an extra user localhost:mlord\r\n\r\nâ¯ vtctldclient ValidatePermissionsKeyspace commerce\r\nE0101 13:58:55.687067   32429 main.go:56] rpc error: code = Unknown desc = permissions diffs: zone1-0000000100 has an extra user localhost:mlord\r\n\r\nâ¯ vtctlclient ValidatePermissionsShard commerce/0\r\nW0101 13:59:55.640410   32898 log.go:39] Failed to read in config : Config File \"vtconfig\" Not Found in \"[/Users/matt/git/vitess]\". This is optional, and can be ignored if you are not using config files. For a detailed explanation, see https://github.com/vitessio/vitess/blob/main/doc/viper/viper.md#config-files.\r\nValidatePermissionsShard Error: rpc error: code = Unknown desc = permissions diffs: zone1-0000000100 has an extra user localhost:mlord;zone1-0000000100 has an extra user localhost:mlord\r\nE0101 13:59:55.655940   32898 main.go:105] remote error: rpc error: code = Unknown desc = permissions diffs: zone1-0000000100 has an extra user localhost:mlord;zone1-0000000100 has an extra user localhost:mlord\r\n\r\nâ¯ vtctldclient ValidatePermissionsShard commerce/0\r\nE0101 13:59:59.051856   33004 main.go:56] rpc error: code = Unknown desc = permissions diffs: zone1-0000000100 has an extra user localhost:mlord\r\n```",
        "Thatâ€™s my preference (OK with either way) as well, but here Iâ€™m following the well established design choice to always return a response object in vtctldserver RPCs, even when it has no fields. Better to have a unified and predictable behavior IMO. "
      ],
      "vitess-extract-shared-code-patterns": [
        "Agreed. They seem identical other than the capability and query so we could pass both into a function.",
        "This is a new struct that has a subset of the workflow server's values, and it's not specific to lookup vindexes at all. Can you help me understand what value this provides? \r\n\r\nThe workflow server type being:\r\n```\r\ntype Server struct {\r\n\tts  *topo.Server\r\n\ttmc tmclient.TabletManagerClient\r\n\t// Limit the number of concurrent background goroutines if needed.\r\n\tsem     *semaphore.Weighted\r\n\tenv     *vtenv.Environment\r\n\toptions serverOptions\r\n}\r\n```\r\n\r\nOtherwise this LGTM. Thanks!",
        "OK, so I take that to mean this is really for a human reader/developer to try and make the code easier to reason about and manage as lookup vindex related workflow code is in its own specific file and has its own specific method receiver.\r\n\r\nIn that case, IMO we should do this:\r\n 1. Go from workflow/lookup.go to workflow/lookup_vindex.go\r\n 2. Go from workflow.lookup.prepareCreateLookup to workflow.lookupVindex.prepareCreate (the type becomes lookupVindex and we don't need that lookupvindex context in the function name as its encapsulated in the receiver type\r\n\r\nLookup is a very generic term and it's not otherwise obvious to me how it adds distinction, separation, discoverability, and clarity specifically around the VReplication backfilling work for Lookup Vindexes via the LookupVindex client command and related RPCs. A [lookup vindex](https://vitess.io/docs/reference/features/vindexes/#lookup-vindex-types) is a specific concept in Vitess, and the client command we're processing in the workflow server is [`LookupVindex`](https://vitess.io/docs/21.0/reference/programs/vtctldclient/vtctldclient_lookupvindex/) (also [here](https://vitess.io/docs/21.0/user-guides/vschema-guide/backfill-vindexes/) and [here](https://vitess.io/docs/reference/vreplication/lookupvindex/)). Otherwise I would think that workflow/lookup was about code related to looking up workflows. Does this all make sense?\r\n\r\nThanks!"
      ],
      "vitess-metric-design-best-practices": [
        "I don't think that we should include the hostname. 1) it changes across the lifespan of a vtgate vstream (we retry for the shard in vstream manager on tablet stream errors) 2) in many deployment envs it's not really helpful 3) it can explode the size of the metric over time as e.g. in k8s deployments, which are the norm today for Vitess, you regularly roll the tablet pods. IMO this kind of context is better used in log messages if we want it. I'm not dead set against it though, so I'm open to discussion.",
        "I had assumed that this was a count of vtgate vstreams rather than tablet streams. Is this really supposed to be a count of tablet streams (one per shard, per vtgate vstream)?",
        "OK, makes sense. Thanks!",
        "I don't see any value in adding 0. Am I missing something?",
        "Is Reset what you really want? It's a counter and not a gauge. I assumed that it was meant to be a counter that spanned the life of the vtgate as the description is `\"Number of vstreams that ended with errors\"`. "
      ],
      "vitess-optimize-memory-allocation": [
        "We can allocate this in one shot: `finalRes := make(map[string]string, len(qr.Rows))`",
        "Probably worth setting the initial capacity to 1."
      ],
      "vitess-explicit-nil-handling": [
        "IMO, this should only be:\r\n```\r\nif req.Message != nil {\r\n```\r\n\r\nIf someone explicitly specifies the string literal `NULL` then we should store that since we're updating the stored value if any new value was explicitly provided."
      ],
      "vitess-standardize-error-wrapping-patterns": [
        "Probably worth using vterrors.New / vterrors.Errorf here so that we at least have a code as well.",
        "Nit, but I think `vterrors.Wrapf(derr, ...)` would be better here."
      ],
      "vitess-dynamic-configuration-needs-validation": [
        "A consideration we can add in the docs somewhere -- that setting the in-memory value does not guarantee it persists as this could only be in the process memory, before being written to the FS, and the FS write may not be flushed or sync'd either (?). A brief info section in the docs could help users think about how to deal with this if e.g. they update a setting in a hundred tablets (maybe one crashes or is e.g. replaced by k8s shorter thereafter).\r\n\r\nI would guess that on process normal shutdown all of these pending in-memory changes are flushed? Although I believe we do kill the process in some cases after a period of time and in those cases we can't know for sure.\r\n\r\nThese considerations also make the trade-off with this flag value here more clear.\r\n\r\nFWIW, I don't consider this an issue in any way. I don't think too many people are even aware of `/debug/env`, we don't support a lot of config options there, and this is clearly an improvement. :-) \r\n\r\nAlthough this does make me think... what if you want to play with different settings in a running process w/o affecting the persistent config? Is there a way to do that? I suspect (don't know for sure) there are two primary uses of `/debug/env` today:\r\n1. Change setting in the running process w/o having to restart. You persisted it yourself separately. Clearly this new behavior is an improvement there.\r\n2. Try different settings in the running process to see which one seems to offer better behavior. You may or may not end up persisting a different flag value.\r\n\r\nDo we effectively lose num 2? I guess we could simply document the breaking change in that it's up to the user to reset it back to the default when they're done with their testing (since intermediate temporary test values were likely persisted in the config file). And when choosing to use a config file, you must become aware of this anyway and change your process accordingly. I think that's totally fine and reasonable FWIW. \r\n\r\nPrecedence also comes into play -- we should document that somewhere in the docs (you may already have some place) -- as if they specify the value as a cmd-line flag then that value may override anything that's persisted in the config file anyway (which they may want for a core set of flags).",
        "These are all fine. Just considerations that we can internalize and potentially document if needed. "
      ],
      "vitess-use-parameterized-queries": [
        "We should use the parser instead. So instead of `%s` here and fmt.Sprintf later on we'd have `%a` and then when building the query we use the parser and bindvars instead. You can see examples of doing it this way throughout the code base, but e.g. you can see vdiff/schama.go to see the queries with \"%a\" in them and then how those variables are used. For example:\r\n  - Query: https://github.com/vitessio/vitess/blob/54dfd6005bcdd599604d42a6771cdf5a1025d7d9/go/vt/vttablet/tabletmanager/vdiff/schema.go#L30\r\n  - Usage: https://github.com/vitessio/vitess/blob/54dfd6005bcdd599604d42a6771cdf5a1025d7d9/go/vt/vttablet/tabletmanager/vdiff/action.go#L318-L321",
        "Does this variable really need to be exported from the package? If not, we should make the first letter a lower case s.",
        "I would recommend something like this:\r\n```\r\n        // The format specifier is for any optional predicates.\r\n        query := \"SELECT variable_name, variable_value FROM performance_schema.global_status%s\"\r\n        predicates := strings.Builder{} // optional filters\r\n        if len(names) > 0 {\r\n\t\tpredicates.WriteString(\" WHERE variable_name IN (\")\r\n\t        for i, name := range names {\r\n\t\t        if i > 0 {\r\n\t\t\t        predicates.WriteByte(',')\r\n\t        \t}\r\n                        predicates.WriteString(sqltypes.EncodeStringSQL(name))\r\n                }\r\n\t        predicates.WriteByte(')')\r\n        }\r\n        qr, err := mysqld.FetchSuperQuery(ctx, fmt.Sprintf(query, predicates.String())\r\n```\r\n\r\nSelecting the `variable_name` is necessary in order to return a map, which I think we really have to in order for this to be usable with multiple values.\r\n\r\nI also think that the base query should be a const in the package (~ `getGlobalStatusQuery`).\r\n\r\nLastly, I also think that we should build a parsed query here rather than sending the raw unparsed one on. This will add some input validation to protect against intentional or not, unexpected/unwanted behaviors. For example, someone sending in something like `'); drop table mysql.user;` as a var name. For example:\r\n```\r\n\tparsed := sqlparser.BuildParsedQuery(query, predicates.String())\r\n        query, err := parsed.GenerateQuery(nil, nil)\r\n        if err != nil {\r\n             return nil, err\r\n        }\r\n        qr, err := mysqld.FetchSuperQuery(ctx, query)\r\n```\r\n\r\nRelated to that, we don't have to use Super for this query.",
        "We do. So if we do end up using the parser, which I think we should as discussed above, we can leverage that to build the IN clause. Here's an example:\r\n\r\n`... in %a ...`:\r\nhttps://github.com/vitessio/vitess/blob/main/go/vt/vttablet/tabletmanager/vdiff/table_plan.go#L39\r\n\r\nBuilding and binding the slice bind var:\r\nhttps://github.com/vitessio/vitess/blob/377e1ddc99b9b1aeeae3eddb5a6ce575d1f5fc79/go/vt/vttablet/tabletmanager/vdiff/table_plan.go#L250-L262",
        "IMO it's worth creating a `mysqld.FetchQuery` helper (doesn't have to be done here) as there should be no need to use the DBA pool for this and various other things. "
      ],
      "vitess-size-fields-appropriately": [
        "This takes us from a limit of 10,000 bytes to 16,777,215 bytes. I don't see any reason not to do this. It might make sense to step toward that by using `blob` first. That would take us from 10,000 to 65,536 bytes. Did you use `mediumblob` because your case was already over the `blob` size? Then again... we know it's possible so we might as well allow it here. `mediumblob` would cover any reasonable installation, I think.",
        "We should increase this as well since if the `pos` is beyond the limit you'll also be beyond the `stop_pos` limit which would block various VReplication features â€” such as VDiff â€” which set the `stop_pos`, with the value being a GTID snapshot from the source tablet."
      ],
      "vitess-manage-workflow-state-transitions": [
        "I think that we should update the message in the `UpdateVReplicationWorkflow` call above, which means we'll have to add support for that to the RPC as an optional proto field.",
        "I'm curious what the `complete` command gets us beyond what the `cancel` command does? Is it that `cancel` enforces that the vindex has NOT been externalized while `complete` enforces that it HAS been externalized? In either case, I would expect the command to delete the workflow if that check passes."
      ],
      "vitess-document-configuration-precedence": [
        "TiL. As long as we mark `--tablet_config` as deprecated I think it's fine."
      ],
      "vitess-robust-network-handling": [
        "We should get into the habit of using `net.JoinHostPort()` in places like this (supports ipv4 and ipv6):\r\n```\r\nddtracer.WithAgentAddr(net.JoinHostPort(host, port)),\r\n``` "
      ],
      "vitess-meaningful-consistent-naming": [
        "Nit, but `MaxValuePartition` would match the standard (Pascal) case.",
        "What's the `Nb` stand for? Number backup? If so, it would be a bit more clear if it was NB. Or maybe even just RetryNum or RetryCnt. Maybe Nb is short for number? "
      ],
      "vitess-prevent-concurrent-access-races": [
        "Nit, but IMO it's worth using a closure here to be sure the mutex is unlocked using a defer (e.g. you get a panic between the lock and unlock, and that panic is recovered up the call stack, but then we don't unlock the mutex â€” although in this specific case it would be fine since this mutex would go out of scope in that case).\r\n```\r\n\t\tfunc() {\r\n\t\t\tmu.Lock()\r\n\t\t\tdefer mu.Unlock()\r\n\t\t\tif len(res.Streams) > 0 && sourceKeyspace == \"\" {\r\n\t\t\t\tsourceKeyspace = res.Streams[0].Bls.Keyspace\r\n\t\t\t}\r\n\t\t\tworkflowType = res.WorkflowType\r\n\t\t\treadVReplicationWorkflowResp[tablet.Shard] = res\r\n\t\t}()\r\n```",
        "I don't think this affects the query hot path, right? If it does, then it might be worth e.g. using 1 byte for the status and using bits in there for isWriting, isBlocked, isOpen etc. so that we can use atomics for reading them, CAS for optional changes, etc. If nothing else, it's probably worth moving these to atomic.Bool so that e.g. checkAndSetIsWriting can be one atomic call:\r\n```\r\n    return w.isWriting(false, true)\r\n```\r\n\r\nIt makes the code simpler, clearer, and less contentious / efficient. ",
        "Not that it really matters here, but when you don't care about the value being passed you can use an empty struct literal which is zero bytes:\r\n```\r\nsemAcquiredChan := make(chan struct{})\r\n\r\nsemAcquiredChan <- struct{}{}\r\n```"
      ],
      "vitess-use-testify-assertion-libraries": [
        "We should use require/assert in new tests. So e.g. here it would be:\r\n```\r\nerr := f.Close()\r\nrequire.NoError(t, err)\r\n```\r\n\r\nOr even just: `require.NoError(t, f.Close())`"
      ],
      "vitess-log-levels-and-clarity": [
        "The default logger is glog. And error log messages also end up in the info log as they cascade down. Do we really want/need duplicate log messages? This is an error so I think we should improve the existing one by adding the workflow name.",
        "IMO we should take this opportunity to improve the log message:\r\n```\r\nts.Logger().Errorf(\"Cancel migration failed: could not revert denied tables / shared access: %v\", err)\r\n```\r\n\r\nI also think that we should accumulate these and return them to the caller. But we could defer that change. "
      ]
    }
  },
  "ndeloof": {
    "repos": [
      "docker/compose"
    ],
    "entries": [
      {
        "slug": "compose-ci-security-boundaries",
        "title": "CI security boundaries"
      },
      {
        "slug": "compose-environment-variable-validation",
        "title": "Environment variable validation"
      },
      {
        "slug": "compose-evaluate-dependency-api-compatibility",
        "title": "evaluate dependency API compatibility"
      },
      {
        "slug": "compose-follow-existing-naming-patterns",
        "title": "Follow existing naming patterns"
      },
      {
        "slug": "compose-keep-code-structure-flat",
        "title": "Keep code structure flat"
      },
      {
        "slug": "compose-maintain-documentation-consistency",
        "title": "Maintain documentation consistency"
      },
      {
        "slug": "compose-network-api-precision",
        "title": "Network API precision"
      },
      {
        "slug": "compose-prefer-explicit-readability",
        "title": "prefer explicit readability"
      },
      {
        "slug": "compose-prevent-sensitive-data-exposure",
        "title": "Prevent sensitive data exposure"
      },
      {
        "slug": "compose-prevent-unintended-ci-behaviors",
        "title": "Prevent unintended CI behaviors"
      },
      {
        "slug": "compose-safe-collection-modification",
        "title": "Safe collection modification"
      },
      {
        "slug": "compose-schema-changes-upstream-first",
        "title": "Schema changes upstream first"
      },
      {
        "slug": "compose-scope-concurrency-control-precisely",
        "title": "Scope concurrency control precisely"
      },
      {
        "slug": "compose-use-api-options-pattern",
        "title": "Use API options pattern"
      },
      {
        "slug": "compose-use-structured-logging-framework",
        "title": "Use structured logging framework"
      },
      {
        "slug": "compose-wrap-and-check-errors",
        "title": "Wrap and check errors"
      },
      {
        "slug": "compose-write-deterministic-test-assertions",
        "title": "Write deterministic test assertions"
      }
    ],
    "comments": {
      "compose-keep-code-structure-flat": [
        "nested `if`s make this a bit hard to read, maybe just split this into simpler top-level `switch` statement?\r\n\r\n```go\r\nswitch {\r\n\tcase direction == fromService && index == 0: \r\n                containers = ...\r\n\tcase index != 0:\r\n                containers = ...\r\n        default:\r\n                containers = ...\r\n\t}\r\n``` \r\n\r\n",
        "`direction = from & index = 0` has been addressed by 1st case, so here we have `(from & index > 0) ||Â (to & index > 0)` which can be simplified as `index > 0`"
      ],
      "compose-follow-existing-naming-patterns": [
        "I'd suggest to use `-y` to align with https://github.com/docker/compose/blob/main/cmd/compose/create.go#L84",
        "As this also applies to Darwin, maybe rename `_posix` :trollface: ",
        "nit: AFAICT we don't `parse` things here, so better name this `toBuildContexts`",
        "this produces the `options` for the `ImageBuild` engine API, so ...",
        "sounds weird to me we use a distinct separator for fields, I'd prefer we use `:` everywhere\r\nThis allows an earlier version of compose to successfully parse the label, and ignore the last `:xxxx` parts (so we can add more in the future)",
        "Don't need to be exporte. Also name is bit confusing. Maybe `withSelectedServicesOnly` or something comparable?",
        "This is basically a duplicate for `projectFromLabels`, better use/adapt this existing function",
        "yes, move it to `compose.go`. Also, probably should be renamed `projectFromActualResources` or something comparable to be more explicit"
      ],
      "compose-wrap-and-check-errors": [
        "use `api.IsNotFoundError(err)`"
      ],
      "compose-write-deterministic-test-assertions": [
        "This version string is very specific to Docker Desktop and will be outdated once we release v2.34, I fear this will bring some confusion. Better use a pure fake version for testing purpose: `v9.9.9-test` for example"
      ],
      "compose-environment-variable-validation": [
        "a better place to manage this is `addProjectFlags` in cmd/compose.go, as you can set flag default value from env value (the same way we do for `--env-file`)",
        "we never used this mechanism in the past for optional/experimental docker compose features",
        "this is the way we used to load PWD/.env before we know the actual project directory. This logic is now covered by toProjectOptions"
      ],
      "compose-prefer-explicit-readability": [
        "took me time indeed to understand this syntax used in original code."
      ],
      "compose-maintain-documentation-consistency": [
        "rest of the documentation uses uppercase \"V1\" and \"V2\"\r\nplease place the compose-switch note as a separate paragraph to make it bit more visible to readers"
      ],
      "compose-safe-collection-modification": [
        "> in general modifying a map during it's iteration is bad practice and can produce unpredictable results\r\n\r\nany reference ?\r\nI was looking for official reference regarding this and only found https://github.com/golang/go/issues/9926",
        "I'm not sure I remember the detail, but deletion here is expected afaict.\r\nMaybe better create a new `Volumes` before the loop, copy those that need to be kept while iterating, then override `s.Volumes`"
      ],
      "compose-prevent-sensitive-data-exposure": [
        "service.environment may be set with a fixed value, not relying on any interpolation. Typically:\r\n```\r\ndb:\r\n    image: mysql\r\n    environment:\r\n      MYSQL_DATABASE: avatar\r\n      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db-password\r\n```\r\nthose should not prevent compose file to be published, right ?",
        "```suggestion\r\n\t\tfmt.Println(\"you are about to publish bind mounts declaration within your OCI artifact.\\n\" +\r\n\t\t\t\"only the bind mount declarations will be added to the OCI artifact (not content)\\n\" +\r\n\t\t\t\"please double check that you are not mounting potential user's sensitive directories or data\")\r\n```",
        "you MUST check each individual file in the compose project, not just the final model. Otherwise I _may_ publish:\r\ncompose.yaml\r\n```yaml\r\nservices:\r\n test\r\n   some: MY_SECRET_PASSWORD\r\n```\r\n\r\ncompose.yaml\r\n```yaml\r\nservices:\r\n test\r\n   some: !override ${ENTER_YOUR_OWN_SECRET}\r\n```\r\n\r\nPublisher expectation would be that secret is not exposed to consumer, but actually it is"
      ],
      "compose-schema-changes-upstream-first": [
        "compose-spec.json",
        "in an ideal world, this whole file should be removed from docker/compose repo and replaced by a (build-time?) reference to https://github.com/compose-spec/compose-spec/blob/master/schema/config_schema.json",
        "did you checked if the current status of the compose-spec schema already includes all those changes or if there's anything missing ?"
      ],
      "compose-scope-concurrency-control-precisely": [
        "We don't want providers to run sequentially, only the project mutation must be guarded by a mutex.",
        "could use an errorGroup (like we do in other places in compose) to capture first error and stop other goroutines by cancelable context.",
        "this isn't thread safe, would require a mutex (this is why we used a channel here)"
      ],
      "compose-prevent-unintended-ci-behaviors": [
        "we don't exec `docker buildx ...` for a standard build, but use the vendored buildx client, why not do the same here with `BuildOptions.PrintFunc`?"
      ],
      "compose-network-api-precision": [
        "This hack is also supported for long by compose (https://github.com/docker/compose/blob/v1/compose/config/config.py#L494-L498) and relies on `o=bind` and `device` option being set\r\n"
      ],
      "compose-evaluate-dependency-api-compatibility": [
        "watcher_naive relies on `SetRecursive` which doesn't exist in upstream"
      ],
      "compose-use-structured-logging-framework": [
        "should be Errorf",
        "we've been using `logrus.Warning` in other places, we should do the same here"
      ],
      "compose-ci-security-boundaries": [
        "Please don't set any kind of \"auto-fix\". CI role is to check everything is OK in contributor's commit, not to replace contributor doing things right."
      ],
      "compose-use-api-options-pattern": [
        "Repository being mandatory parameter should not be part of the `Options` struct imho",
        "This is API version 1.44, but Moby v25 (typo here)\r\nMakes me wonder moby repo could offer a map for engine -> latest API version so we don't make such mistakes :)",
        "https://github.com/docker/compose/pull/11360",
        "could make it simpler by making API client an attribute in `composeService`, initialize with `dockerCli.Client()` and override here with `NewDryRunClient`.",
        "As this is not a generic Client proxy, but dedicated to dry-run usage, we could make it simpler and just have all API methods to directly invoke `client.XX` from dockerCli's client _BUT_ the few ones where we want to bypass actual actions on docker engine.\r\nOR rename this into generic `ClientProxy`, then we can configure dry-run by setting individual func to be overriden"
      ]
    }
  },
  "SomeoneToIgnore": {
    "repos": [
      "zed-industries/zed"
    ],
    "entries": [
      {
        "slug": "zed-background-process-blocking-operations",
        "title": "Background process blocking operations"
      },
      {
        "slug": "zed-choose-domain-specific-semantic-names",
        "title": "Choose domain-specific semantic names"
      },
      {
        "slug": "zed-consider-algorithmic-complexity",
        "title": "Consider algorithmic complexity"
      },
      {
        "slug": "zed-contextualize-dont-panic",
        "title": "Contextualize don't panic"
      },
      {
        "slug": "zed-design-interfaces-not-implementations",
        "title": "Design interfaces, not implementations"
      },
      {
        "slug": "zed-document-configuration-clearly",
        "title": "Document configuration clearly"
      },
      {
        "slug": "zed-document-configuration-constraints-clearly",
        "title": "Document configuration constraints clearly"
      },
      {
        "slug": "zed-hierarchical-configuration-organization",
        "title": "Hierarchical configuration organization"
      },
      {
        "slug": "zed-minimize-credential-exposure-lifetime",
        "title": "Minimize credential exposure lifetime"
      },
      {
        "slug": "zed-prefer-idiomatic-option-handling",
        "title": "Prefer idiomatic Option handling"
      },
      {
        "slug": "zed-prefer-rust-structural-patterns",
        "title": "Prefer Rust structural patterns"
      },
      {
        "slug": "zed-protect-render-loop-performance",
        "title": "Protect render loop performance"
      },
      {
        "slug": "zed-respect-language-specific-conventions",
        "title": "Respect language-specific conventions"
      },
      {
        "slug": "zed-scope-dependencies-appropriately",
        "title": "Scope dependencies appropriately"
      },
      {
        "slug": "zed-self-explanatory-identifier-names",
        "title": "Self-explanatory identifier names"
      },
      {
        "slug": "zed-standardize-platform-agnostic-configuration",
        "title": "Standardize platform-agnostic configuration"
      },
      {
        "slug": "zed-test-through-public-apis",
        "title": "Test through public APIs"
      }
    ],
    "comments": {
      "zed-minimize-credential-exposure-lifetime": [
        "Also, adding a code that explicitly stores credentials somewhere seems somewhat scary.\nI get it that we do `drop(askpass);` but wondering if we could somehow extract this all into:\n\n```rs\nlet (askpass, askpass_rx) = ....\n....\n\nlet Some(password) = askpass_rx.next().await else {...};\nlet socket = SshSocket::new(connection_options, &temp_dir, password)?;\n```\n\nthis way, nothing related to password ever gets stuck in memory for sure.\n\nGiven good Mikayla's ideas about deduplicating the logic with the posix impl and the fact that it managed to work without storing passwords in fields, seems reasonable?",
        "So, this is a password that we write into a file and never control the lifecycle of that tmp file.\nHow adequate and safe this is?\n\nIs there a way where we do not store the password in files and \"hardcoded\" memory such as fields?\nHaving some `rx` oneshot seems ok? \nThen we can pass that through as a \"askpass reply result\" and call once, where the password is needed?\nOr, is it possible complicate the `SSH_ASKPASS` script so that it gets the input from Zed? \nI'm sort of surprised that ssh itself cannot handle its askpass matters internally with the user."
      ],
      "zed-background-process-blocking-operations": [
        "Nope, alas.\r\n\r\n1. To start with, `pull_diagnostic` has some `.detach()` inside, so here we spawn a task that calls a synchronous function that spawns a task.\r\nThis is sort of noop and odd (even the name is odd, as it actually updates the buffer with the new diagnostics).\r\n\r\nI think it's better to have `pull_diagnostic` to return `Task<anyhow::Result<Vec<Diagnostics>>>` and inside, it will either call to LSP or to protobuf (in the remote client side), similar to how inlays are doing this.\r\n\r\n2. Then, somewhere on this level, we'll have to handle the debounces and tasks better.\r\nRight now, every time we type, we spawn a task that waits for a debounce time, then queries for diagnostics and applies them to the buffer.\r\n\r\nSo, the debounce at its current form does nothing but the delay of the potential query + edit cascade.\r\n\r\nI think the whole approach of placing the pull code here might be a be a bit off-pace.\r\nInstead, we can alter `editor.rs` and adjust around the https://github.com/zed-industries/zed/blob/d53a86b01dd3d02980938cbce1bfd74e35901dda/crates/editor/src/editor.rs#L12167 line.\r\n\r\nWe can store another task and do debounces in it first, similar to what `_scroll_cursor_center_top_bottom_task` has: https://github.com/zed-industries/zed/blob/d53a86b01dd3d02980938cbce1bfd74e35901dda/crates/editor/src/scroll/actions.rs#L77-L87\r\n\r\ndoes, but query LSP store for diagnostics instead after the timeout.\r\nThen, update the diagnostics similar to what applying the diagnostics push + calling `refresh_active_diagnostics` does today.\r\n\r\nWe're lucky that buffer, before updating its diagnostics, checks `diagnostics_timestamp` that it's a newer set.",
        "One thing the new model implies though, is the need to explicitly query multiple servers.\r\nCurrent code sets up the listeners per language server, so what looks like a single LSP query can be N (e.g. tailwind-css project that will have TS langserver + tailwind langserver + maybe prettier/biome and/or eslint servers on top).\r\n\r\nThe new way will require more code and will make it more explicit with `MultiLspQuery`, but maybe it's even better as simpler to understand?\r\n\r\n",
        "This is the core place, combining multiple new feedback from this review.\r\n\r\nhttps://github.com/user-attachments/assets/99f8f320-2b24-44cc-a926-9ef928a08fa2\r\n\r\nInteresting, I've intuitively expected that `clear` will cause a lot more issues with flickering, if the diagnostics retrieval is slow due to the amount of the diagnostics, but even on the large example it's not that bad.\r\n\r\nI think we're saved here by the fact that Zed already has issues with large amounts of diagnostics, hence we're not seeing the new ones.\r\nI would propose still, to rework this a slightly:\r\n* accumulate new state (`inline_diagnostics`) first, and mutate the state it in the very end of the task, once\r\n* pass everything that does not depend on `self` and `cx` through `cx.background_spawn(async move { ... }).await` \r\n\r\nCombined, something like\r\n```rs\r\nlet new_inlay_hints = cx.background_spawn(async move {\r\n    let mut new_inlined_diagnostics: Vec<(Anchor, InlineDiagnostic)> = Vec::new();\r\n    let mut prev_diagnostic_line = None;\r\n    for diagnostic in diagnostics {\r\n        //........\r\n        new_inlined_diagnostics..binary_search_by(|probe| {\r\n            diagnostic_anchor.cmp(&probe.0, &buffer)\r\n        });\r\n        //........\r\n    }\r\n}).await;\r\n//........\r\nself.inline_diagnostics = new_inline_diagnostics;\r\n```\r\n\r\nThis way, we'll be usually on a background thread, debounced or computing, cancelled on concequent requests.\r\nOld state will be kept, and Anchor (from Editor) -> DisplayPoint (when laying out) conversion will keep the diagnostics placed on the right lines when rendering, even after adding newlines or undoing.\r\n\r\nLarge diagnostics sets to process will cause more stale diagnostics text during fast editing, but as a somewhat inevitable trade-off, which does not flicker at least."
      ],
      "zed-prefer-idiomatic-option-handling": [
        "```suggestion\r\n            .is_some_and(|popover| popover.signature.len() > 1)\r\n```\r\n\r\nNIT",
        "> `if let Some`\r\n\r\nThis can be just `?`",
        "Here, we'd rather use the `to_string_lossy()` method, to try and show something for invalid UTF-8.",
        "`?`",
        "No, I mean for the 3rd time in a row, this is a method that returns `Option`, so you can `?` instead of `if let Some`",
        "> .unwrap();\r\n\r\nDo not ever use that, please.",
        "NIT: we could use `view.read(cx).active_editor.as_ref().is_some_and(` instead."
      ],
      "zed-prefer-rust-structural-patterns": [
        "NIT: we can do `if snapshot.mode != EditorMode::Full || scrollbars_layout.vertical.is_none()` with an early `return None` to avoid nested code and improve the readability.",
        "```suggestion\r\n```\r\n\r\nCan also do `.get_diagnostics(server_id).into_iter().flat_map(...` and avoid any extra nesting and `if let Some`.",
        "```suggestion\r\n            signature_help_task: None,\r\n```\r\n\r\nNIT: let's keep the compiler a bit less busy solving traits for simple cases? Same for the other one.",
        "```suggestion\r\n            let Some((buffer, buffer_position)) = self.buffer.read(cx).text_anchor_for_position(position, cx) else { return; }\r\n```\r\n\r\nNIT: we can destructure it right away, so that will be one less block to nest.",
        "NIT: that could be destructured in-place, without method field calls, right in the parameter declaration:\r\n\r\n```\r\npub fn create_signature_help_markdown_string(\r\n    SignatureHelp {\r\n        signatures,\r\n        active_signature,\r\n        active_parameter,\r\n    }: SignatureHelp,\r\n) -> ..."
      ],
      "zed-protect-render-loop-performance": [
        "It's not superb that we have to use this method during __rendering__ , that might happen fast, e.g. 120 times/second: and we loop over outlines in a potentially large file.\r\n\r\nWhat's more odd, is that we call `render_outline(` in a loop over every item to render, so should know whether the outline has children or not.\r\n\r\nSeems that we can rewrite it into something better.",
        "`show_diagnostics_inline` is what alters the rendering logic, so this code will \"blink\" the diagnostics, if the delay is long enough: e.g for `\"delay_ms\": 500` after an edit, the diagnostics will stop rendering and editor will \"blink\", showing none at all first and 500+Nms later coming up with the new diagnostics.\r\n\r\nWhat if, instead, we rework the approach a bit?\r\nWe'll keep the only `show_diagnostics_inline` in `Editor` (altered via project settings), remove `show_diagnostics_inline_enabled` and altering the delay logic similar to https://github.com/zed-industries/zed/pull/21463 ?\r\n\r\nThe idea is to have a single \"is it enabled?\" flat on the editor level which allows updating the collection of diagnostics to render (can be stored in the editor's element or editor itself).\r\nWhen the flag goes down, the collection is cleared and not updated until the settings change and re-enable the flag.\r\n\r\nThe diagnostics' update happen after buffer edits and on special events (e.g. editor creation), debounced (50ms default seems ok?), so we do not recalculate diagnostics needlessly on quick user typing (current part of the config seems ok syntaxically, but needs a bump from 0ms).\r\n\r\nIf there's something in the collection, it gets rendered straight away, without any complex computations.",
        "Seems like a good summarization, I indeed propose to do less work on each frame, in particular\r\n\r\n> grabs what should be currently visible, does a bit of math to translate from buffer coordinates to display coordinates, and then spits out whatever those flex box thingers are collectively known as, and paints them on screen\r\n\r\nspecifically \"does a bit of math\" part is very concerning to me:Â we have ~16.67ms per frame at best, and potentially very large documents with very large amount of errors, consider scenarios like project.rs from this repo which will have a particular `use` statement(s) malformed (which will trigger a lot of errors).\r\n\r\nWe also do sorting with `sorted_by_key` and all those coordinate transformations are logarithmic at best.\r\n\r\nI just cannot fathom any reason to do that bit of math if we can avoid doing that.\r\nSure, this can wait until other fixes, but has to be made eventually.",
        "This is a code with a lot of tree traversals (`display_point_to_point`, `point_to_display_point` and other coordinate transform, `diagnostics_in_range` lookup) and various allocations and whatever sorting on top.\r\n\r\nAll that has a lifecycle of `let diagnostics = self.gather_inline_diagnostics(`, and `layout.inline_diagnostics.drain()` on the other end, so sort of thrown away.\r\n\r\nThis all is done on each frame* potentially 120 times/second, and to me it seems we can try and rewrite the logic so that it scales better: traversing thousands of diagnostics with various tree-manipulations on the side does not seem fun to fit into a frame.\r\n\r\nI've mentioned a proposal in the editor.rs part: we might want to debounce any attempt to update the diagnostics data (unless it's a data purge after disabling the diagnostics); keep the old state as long as possible, swapping it to the new one which is calculated after an edit; keep renrendering that part",
        "Just to be more specific, I've got a file with a lot of diagnostics.\r\n\r\n[rust-analyzer-repro-long-diagnostics.zip](https://github.com/user-attachments/files/18411900/rust-analyzer-repro-long-diagnostics.zip)\r\n\r\nand there, I've opened the file both in `Nightly` and my local `--release` build of this branch.\r\nWaited until all diagnostics are shown, and started scrolling through the file, nothing else, profiling.\r\n\r\nBoth versions behave poorly, but the traces show drastic difference in the reasons to do so:\r\n\r\nNightly has some line layout and tree-sitter-query-related issues:\r\n![Nightly](https://github.com/user-attachments/assets/565631de-1b6a-41aa-8a24-0146f35fa880)\r\n\r\nThe branch build does all that diagnostics-related work including extra allocations, not explicitly mentioned in the comment above:\r\n![Dev](https://github.com/user-attachments/assets/9cc29382-9bca-4658-8c61-71a4e762d4b5)",
        "After new design, numbers are back to something comparable with the rest of the code for the same exaggerated example:\r\n\r\n![image](https://github.com/user-attachments/assets/72ce4f9d-1d73-427e-96be-8641b6d61ada)\r\n\r\nand the heaviest stacktrace is related to layouting again.",
        "> .exists()\r\n\r\n1. We prefer to avoid using `std::fs`-related methods and use `Fs` instead, https://github.com/zed-industries/zed/blob/5e210c083fc4d1a00c9a7dac7abf185351bb9b3e/crates/fs/src/fs.rs#L112 in this case.\r\n\r\n2. This is a rendering code, potentially called 120 times/second, calling whatever FS-related things here is madness.\r\nIs there another way we can check that file is outside of projects?\r\n\r\n3. We can be on remote, where the client will have no FS at all."
      ],
      "zed-choose-domain-specific-semantic-names": [
        "It's not just the content but some way to launch askpass it seems?\nLet's name it differently to ensure we do not pass any passwords or any other sensitive things, at least on the semantics, naming level.\n\n```suggestion\n        let askpass_script = format!(\n```\n\nAlso later, we store this into `askpass` field which seems worth of having the same name.",
        "1. This method is used only once, so let's inline it instead.\r\n\r\n2. I like how `active` word is used here though, let's rename `FocusedPane` into `Active???`\r\nAnd let's uniformy state what the PR tries to do: here, it uses `Pane` but the title of the PR is \r\n> Add setting for minimap on active buffer only\r\n\r\nI think it's the latter based on the video and the discussion, so we can use\r\n\r\n`MinimapDisplayScope::ActivePane`\r\n\r\nBut then, `MinimapDisplayScope` is also wordy, can we do `Display` or `DisplayIn` or similar?\r\n\r\nAs a last naming thought, what if we use `editor` instead of `pane` in the new code?\r\nThen it will be `DisplayIn::AllEditors | ActiveEditor` and users won't have to guess what a `pane` is, is it a `panel` or not.",
        "```suggestion\r\n    pub fn semantic_tokens(&self) -> &Arc<SemanticTheme> {\r\n```\r\n\r\n`tokens` is quite broad, esp. in a text editor domain, so let's specify which tokens are these, as LSP case is relatively niche."
      ],
      "zed-contextualize-dont-panic": [
        "```suggestion\n        fs::write(&askpass_script_path, askpass_script).await.with_context(|| format!(\"Creating askpass script at {askpass_script_path:?}\"))?;\n```\n\nNot sure about `smol`, but stdlib's FS errors are notorious to not include a flie path.\n\nE.g. if script's parent directory is not created in an analogous method, the error returned would be smth. like `Err(\"not a directory\")`.\nTo be on a safe side, seems worth to add some context.",
        "I'm concerned by the amount of new `.unwrap()`, `.expect(..)` in the new code, esp. given that this function returns `Result`.\r\nLet's not write the code that may panic here, as it's quite a core feature, queried frequently, and we're parsing things from a potentially malicious server.",
        "We can remove this nesting with\r\n\r\n```rs\r\nuse anyhow::Context as _;\r\n\r\nlet (socks_proxy, version) = parse_socks_proxy(proxy).with_context(|| format!(\"Parsing proxy url '{proxy}'\"))?;\r\n// or use `.context(\"parsing proxy url\")?` if `url` is not secure to print due to the risk of leaking the credentials.\r\n```\r\n\r\nand might be good to do `.context` on more `?` around, such as `TcpStream::connect` or maybe even `.map_err(|err| anyhow!`?",
        "I would prefer `.context`/`.with_context` over `.map_err` and `?` usage anyway as indeed, mostly the Zed \"code style\" and the type system being rather explicit itself already. ",
        "I think it's fine, thank you for the rest of the fixes.",
        "I understand that we did a Python regex match and should not fail here, but the logic may have flaws (or gain, with later contributions) causing None to be returned here.\r\n\r\nFor the sake of less panics due to unwrap/expect's in \"prod\" code, let's replace it with `log_err()?` or, `debug_panic!` if you want to pay more attention to this in debug builds.\r\n\r\nI also think we can avoid this entirely, and redo this `else if let` into `else`, then get the `let file_line` and use the regular Rust's regex (see how unit tests do that with `fn re_test` now) to match the rest of the cases, with capture values extractions?"
      ],
      "zed-self-explanatory-identifier-names": [
        "```suggestion\r\n      \"cmd-alt-=\": \"workspace::IncreaseDocksSize\",\r\n```\r\n\r\n* We have a way to define parameters in the keymap:\r\n\r\nhttps://github.com/zed-industries/zed/blob/0731097ee5780b3569980d7ba93f8fcf4eee097d/assets/keymaps/default-macos.json#L40\r\n\r\nlet's use this form for every new action that has parameters: this way we'll make it more obvious to the users that the parameter exists (the do not usually see the code), and document the default.\r\n\r\n* Let's also remove that `ByOffset` suffix from everywhere, as the `offset` parameter is clearly showing this, and `Open` part, for brevity.\r\n\r\nMaybe `Size` can also go away?",
        "* WDYT on `workspace::Increase/DecreaseActiveDock`, `workspace::Increase/DecreaseDocks` and `workspace::ResetDocks` as names?\r\n\r\n* We do have to write new entries similar to that `IncreaseBufferFontSize` example above and a comment, what `0` as a default will do.",
        "What about the previous name, `\"min_line_number_digits\"`?\r\n\r\nNot sure what a \"line number base\" is, but feels about a single line number?"
      ],
      "zed-document-configuration-clearly": [
        "```suggestion\r\n    \"context\": \"Editor && can_scroll_signature_help\",\r\n```\r\n\r\nSeems that we can simplify this by giving a more readable name and only adding this context when no completions/whatever else conditions.\r\nWDYT?",
        "This 6 is somewhat confusing.\r\nIs it a default? Then we can uncomment the value.\r\nIs it a minimum allowed? Then we can explain this fact."
      ],
      "zed-scope-dependencies-appropriately": [
        "This changes the behavior for all crates in the project, seems like an overkill for a dev build-related change.\n\nIf it intends a global change, let's extract it into a separate PR and keep this one scoped on a local dev workflow change.\nFor that, I imagine we need to add a new [feature](https://doc.rust-lang.org/cargo/reference/features.html) right inside the `remote_server` crate and use that feature when doing a dev build.",
        "IIRC you have to post this change (with another feature) right inside the `remote_server/Cargo.toml` and see if it brings the right results with `cargo tree` or your experiments.\r\nFeatures are additive, so this should work."
      ],
      "zed-document-configuration-constraints-clearly": [
        "> ///\r\n\r\nLet's remove that and mention ranges around, where it matters.",
        "More like [0.0, 1.0] range that's not anywhere here and in the `///` docs too, just in the json defaults?\r\n\r\nWhat's up with negatives, btw, are those clamped to 0.0?",
        "```suggestion\r\nNon-negative `float` values\r\n```\r\n\r\nhttps://github.com/zed-industries/zed/blob/f19e1e3b5f23ec7f64b692f4530825e0e50e5b95/crates/workspace/src/pane_group.rs#L1138-L1139",
        "* Can we use a diagnostics level here? \r\nLSP defines that there are 4 types of these: https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#diagnosticSeverity and a single boolean cannot cover them all.\r\n\r\nI think it's better to be flexible and allow users to show all or almost none of them, if the want to.\r\nWe could also move that into `inline` part of the settings?\r\n\r\n* If we do that, I wonder how useful `primary_only` would be: even now it seems redundant, have you found it useful?\r\n\r\nI feel that we remove `use_rendered` unless you have a good reason to leave it be, if we remove `primary_only`, we'll have only 3 settings knobs: on/off, diagnostics level, and diagnostics interactivity.\r\nThat set looks quite small and good to me.",
        "This is the best part of the impl to me, honestly, I wonder if we could \r\n* move it into `inline` as this settings knob is related to this part of the functionality only? (apart from the fact that it spoils f8 navigation sometimes ðŸ™‚, see another comment )\r\n* enable it by default, and explain that when disabled, same effect could be achieved with f8/shift-f8 or the new action (if we decide to leave it)",
        "I think we now have a very good set of settings and descriptions inside the default.json above, and we need to update this section respectfully.\r\n\r\nI agree with the workflow, where all toggling is made by existing f8/shift-f8 , to potentially jumpy autotoggling or special actions are needed.\r\n\r\nOne wish I used to voice in another related discussion here: we can add `\"minimum_level\"` knob `\"inline\"` section, to allow users to show only errors, only errors + warnings, etc.\r\nSimilar to people not fond of flickering, there is a notable group of people who are not fond of many popovers/colors around their code and it seems simple to add another `.filter` inside `update_inline_diagnostics` to support that + new diagnostics re-queried after level change."
      ],
      "zed-respect-language-specific-conventions": [
        "One more small change here, we need to return that trailing newline back."
      ],
      "zed-standardize-platform-agnostic-configuration": [
        "That has to have `#[serde(default = \"default_true\")]` , as the default_settings.json mention default = true (which is also good to mention here) + the user setting overrides require this field always (which is not what we want): \r\n\r\n![image](https://github.com/user-attachments/assets/fadef7dc-8010-4a93-b638-2f15b8d4fc90)\r\n",
        "The error is gone, so thank you for fixing this."
      ],
      "zed-test-through-public-apis": [
        "Super nice to see the tests on the new functionality, thank you.\r\n\r\nOne thing I really lack in them is a visual part: instead of each \"visible count\" and other digits assertion, we should do what other tests do, comparing a file+outline string representations.\r\n(all these\r\n\r\n```\r\noutline: struct OutlineEntryExcerpt\r\n  outline: id\r\n  outline: buffer_id\r\n  outline: range\"\r\n```\r\nparts).",
        "We need much more tests for something as fundamental as \"show me all highlights for a random chunk of text in the buffer\".\r\n\r\nIf this panics or somehow misbehaves, it might be very frequent and frustrating, and overall, it's very scary to merge whatever related thing.\r\n\r\nWe should cover:\r\n\r\n* overlapping ranges (both LSP overlaps and LSP + tree-sitter highlights overlap LSP ones)\r\n* multi-line LSP highlights\r\n* something else? \r\nFor inlays, we have `test_random_inlays` fuzzy test checks how text behaves for random inlays' inserts.\r\nShould we do something for highlights too?",
        "Also, all that \r\n\r\n> The delta is now expressed on these number arrays without any form of interpretation what these numbers mean. \r\n\r\nsection in the spec is somewhat scary: if I've read it correctly, we should fiddle with digits on the client, when computing token deltas?\r\n\r\nDefinitely worth testing then, if/when the delta support arrives.",
        "I think it would be better to add these cases into `test_url_regex` and ensure we match out the right thing.\r\n`valid_url_ending` is pretty local method to test instead.",
        "The idea of unit tests is to test certain contracts of a single code unit, usually by using its public API for testing.\r\nObviously, nothing forbids doing otherwise and test the private api, but unit tests also slow down development, as every adjusment might require test rewrites/fixes â€” private, \"pretty local\", methods are meant to be moved/removed/adjusted/etc. thus testing them is not that great.\r\n\r\nJust a general rule of thumb: you're fixing the way regex captures strings, so test that, not something that checks whether the URL \"ending\" is valid.",
        "Done, and that lead to a better way to compare chars case-insensitively, thank you.",
        "There's a lot of rendering tests, which is amazing, thank you!\r\n\r\nWhat's lacking are new ones in the `editor_tests.rs`, and after we deal with the brackets and whatever else that triggers the pop-up display, it would be fantastic to have that logic tested too."
      ],
      "zed-consider-algorithmic-complexity": [
        "To note, `dedup` is the wrong method to deduplicate things in a `Vec`, as it only works for sequential items:\r\nhttps://doc.rust-lang.org/std/vec/struct.Vec.html#method.dedup\r\n\r\nIt seems that we could try and either use a HashSet (I had to implement a few `Hash` manually for document colors for this approach), or use a method from `Itertools` for a proper deduplication, [`dedup_by`](https://docs.rs/itertools/latest/itertools/trait.Itertools.html#method.dedup_by)",
        "Ahh, traits.\r\nThank you, I would not think of this method seeing a `Vec` around.",
        "This is odd: effectively, we go over all `fetched_outlines` and for each, we we-iteratethe outlines list again to find the children.\r\n\r\nIt's a plain O(N^2) for nothing and should be fixed.\r\nWe do `outlines.retain(|outline| {` right above and can prepare a depth map for any decision of a kind.",
        "Can we change the predicate, so we don't have to clone?\r\nSeems that `F: FnMut((&TaskSourceKind, &TaskTemplate)) -> bool + 'static,` is good enough?"
      ],
      "zed-design-interfaces-not-implementations": [
        "Let's add a static `fn .. -> Option<Workspace>` for all this extractions, as we might want to invoke things headless for ~10 different actions it seems.",
        "The code below seems to show that quite explicitly, so let's remove the comment.\r\n\r\nI would also argue that this function should accept `proxy: &Url` instead and `TcpStream::connect` can be done up the stack, at a single place that calls this function.\r\nShould we do that?",
        "This works, but we can reduce the diff and stick to the builder-like API with `.when(TitleBarSettings::get_global(cx).show_branch_icon, |branch_button| ...)`.\r\n\r\nLet's try doing that?",
        "Let's consolidate this with https://github.com/zed-industries/zed/blob/1cfbfc199cee551318b89a2f35853ed43b8ac52d/crates/gpui/src/app.rs#L1434-L1440 and it's the same thing implemented for macOS.\r\n\r\nAt least, the vocabulary better match in the \"interface\" API.",
        "I mean that there are two components shared in every OS (Linux is special but has similar concepts in certain DEs, but let's omit it as not implemented in the repo).\r\n\r\n* in the OS, there's a list of entries, ordered by \"last opened\"\r\n  * that list could be altered from the OS side (different ways to do that but still)\r\n* there's a list of last opened projects in Zed, and that list can be altered from Zed and needs to be propagated to the OS side\r\n\r\nAt let's call all these entries, lists, whatnot similarly, as of now there's \"jump_list\" for Windows only and something else for mac-only.\r\nWhat also would be good, is to keep the same, single \"add an entry\"/\"set entries\" workflow in the API if possible."
      ],
      "zed-hierarchical-configuration-organization": [
        "Let's either wait for https://github.com/zed-industries/zed/pull/29494 or incorporate the settings approach, as we seem to get more and more title bar settings and we'd better start namespacing them.",
        "Sure, less knobs to have in the settings is always good, thanks."
      ]
    }
  },
  "AlexDBlack": {
    "repos": [
      "deeplearning4j/deeplearning4j"
    ],
    "entries": [
      {
        "slug": "deeplearning4j-always-secure-your-locks",
        "title": "Always secure your locks"
      },
      {
        "slug": "deeplearning4j-clean-up-your-code",
        "title": "Clean up your code"
      },
      {
        "slug": "deeplearning4j-clear-descriptive-identifiers",
        "title": "Clear descriptive identifiers"
      },
      {
        "slug": "deeplearning4j-compare-floating-point-safely",
        "title": "Compare floating-point safely"
      },
      {
        "slug": "deeplearning4j-configurable-resource-locations",
        "title": "Configurable resource locations"
      },
      {
        "slug": "deeplearning4j-descriptive-error-context",
        "title": "Descriptive error context"
      },
      {
        "slug": "deeplearning4j-document-ai-apis-completely",
        "title": "Document AI APIs completely"
      },
      {
        "slug": "deeplearning4j-document-ai-implementation-references",
        "title": "Document AI implementation references"
      },
      {
        "slug": "deeplearning4j-document-api-completely",
        "title": "Document API completely"
      },
      {
        "slug": "deeplearning4j-fail-fast-clearly",
        "title": "Fail fast clearly"
      },
      {
        "slug": "deeplearning4j-maintain-proper-capitalization",
        "title": "Maintain proper capitalization"
      },
      {
        "slug": "deeplearning4j-minimize-object-allocations",
        "title": "Minimize object allocations"
      },
      {
        "slug": "deeplearning4j-numerical-stability-practices",
        "title": "Numerical stability practices"
      },
      {
        "slug": "deeplearning4j-optimize-hardware-acceleration",
        "title": "Optimize hardware acceleration"
      },
      {
        "slug": "deeplearning4j-use-logging-best-practices",
        "title": "Use logging best practices"
      },
      {
        "slug": "deeplearning4j-use-modern-api-methods",
        "title": "Use modern API methods"
      },
      {
        "slug": "deeplearning4j-user-friendly-documentation-examples",
        "title": "User-friendly documentation examples"
      },
      {
        "slug": "deeplearning4j-validate-and-document-nulls",
        "title": "Validate and document nulls"
      }
    ],
    "comments": {
      "deeplearning4j-document-ai-implementation-references": [
        "Maybe add a comment here - seemed a little strange on first glance.\r\nI gather you want masked steps to have very large negative attention weights as a way of doing \"masked softmax\"?\r\nHow was the 1e9 chosen? (just wondering if we'll even have numerical stability issues...)",
        "conv_padding is 'truncate' mode padding values, and conv_padding_r is same mode padding, right?\r\nIf so, not sure on the -pH and -pW here... Usually with same mode, we ignore the ph and pW args and just calculate what we should actually use.\r\nhttps://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/ConvolutionMode.java#L62-L63\r\n\r\nAlso present in a bunch of other places."
      ],
      "deeplearning4j-maintain-proper-capitalization": [
        "Capitalization: `samediff` -> `SameDiff`\r\n`Tensorflow` -> `TensorFlow`\r\n`Samediff.importFrozenTF` -> `SameDiff.importFrozenTF`\r\n\r\nSame things in a few other places in the doc, I won't flag others...",
        "Technically it's a method overload, not an optional argument.\r\nLet's also mention generated name \"based on the operation name\" when not specified"
      ],
      "deeplearning4j-configurable-resource-locations": [
        "Let's make dir configurable. User might want it on SSD different to system drive.",
        "Do we want configurable location?\r\ni.e., for VMs etc with slow system  HDD but fast SSD available",
        "Also configurable location here?"
      ],
      "deeplearning4j-minimize-object-allocations": [
        "When using the Preconditions class, do this instead:\r\nThis avoids object creation/garbage unless an error is actually thrown.\r\n```\r\nPreconditions.checkArgument((labels.size(1) != preOutput.size(1)),\"Labels array numColumns (size(1) = %s) does not match output layer number of outputs (nOut = %s)\",  labels.size(1), preOutput.size(1));\r\n```",
        "This seems off.\r\nYou create a zeros array of shape [mb, inputCaps, caps, capsDimensions, 1]\r\nBut then proceed to immediately get a [mb, inputCaps, caps, 1, 1] subset from it?\r\nThat's unnecessarily inefficient. Why not make a ```[mb, inputCaps, caps, 1, 1]``` in the first place?",
        "Don't create this temporary ret array. Use ```gainParamView.assign(gainInit)``` instead.",
        "Nice regex :)\r\nA minor detail, but pattern is apparently immutable and thread safe... we can make it static? (Or make the one in BertWordPieceTokenizer static + public and use here too?)"
      ],
      "deeplearning4j-use-logging-best-practices": [
        "Not a big deal - but we can use Lombok `@Slf4j` annotation on the class to get the same thing",
        "Don't also print stack trace, log.error will already print it.\r\nAlso other instances below.",
        "I'm being picky here - can use ```@Slf4j``` annotation instead.",
        "Put exception as log.error arg"
      ],
      "deeplearning4j-always-secure-your-locks": [
        "This doesn't look safe to me.\r\nIf the semaphore available permits is anything other than 0, this will overflow.\r\n\r\nSemaphore.release:\r\n```\r\n    public void release(int permits) {\r\n        if (permits < 0) throw new IllegalArgumentException();\r\n        sync.releaseShared(permits);\r\n    }\r\n```\r\n\r\nThis ultimately ends up calling this method, on Semaphore.Sync:\r\n```\r\n        protected final boolean tryReleaseShared(int releases) {\r\n            for (;;) {\r\n                int current = getState();\r\n                int next = current + releases;\r\n                if (next < current) // overflow\r\n                    throw new Error(\"Maximum permit count exceeded\");\r\n                if (compareAndSetState(current, next))\r\n                    return true;\r\n            }\r\n        }\r\n```\r\n\r\nUnless we can guarantee that the semaphore only ever has 0 permits at this point (unlikely), this is a bug.",
        "Unlock should be in finally block?\r\nOtherwise if there's any sort of an exception, any other threads waiting on that lock could be waiting forever.",
        "Same thing - unlock in finally block in case of exception.\r\nAlso un-indent line above.\r\nSame thing for other lock uses in this class."
      ],
      "deeplearning4j-document-ai-apis-completely": [
        "inputs() is good - that just tells you placeholders. Let's note placeholders, and maybe link to other SameDiff page that explains this.\r\nAs for outputs() - that will be going away. Yes it's in 1.0.0-beta5, but it's not robust enough - it's basically \"variables that don't feed into any other ops\" - which often won't be the real predictions that users want - but rather the loss function or some irrelevant unused output.\r\n\r\nSo users should just look at the summary instead and infer the outputs from that.",
        "\"For multiple outputs, use exec() instead of execSingle(), to return a `Map<String,INDArray>` of outputs instead.\r\nAlternatively, you can use methods such as `SameDiff.output(Map<String, INDArray> placeholders, String... outputs)` to get the same output.\"",
        "Let's add a section with this, which should be good enough for now. We'll get proper coverage info up at a later date.\r\n\r\n```\r\n## Operations Coverage\r\n\r\nSameDiff's TensorFlow import is still being developed, and does not yet have support for every single operation and datatype in TensorFlow.\r\nAlmost all of the common/standard operations are importable and tested, however - including almost everything in the tf, tf.math, tf.layers, tf.losses, tf.bitwise and tf.nn namespaces. The majority of existing pretrained models out there should be importable into SameDiff.\r\n\r\nIf you run into an operation that can't be imported, feel free to open an issue here: https://github.com/eclipse/deeplearning4j/issues\r\n```",
        "DifferentialFunctionFactory is an internal detail most users will never need to touch, let's not mention this."
      ],
      "deeplearning4j-fail-fast-clearly": [
        "Good check, but this can be optimized (and is not the correct way to use preconditions). This always results in 3 objects being created (2xlong[] shape, and a String) regardless of whether an error is encountered. Instead, use this (no object creation unless an error is encountered)\r\n```\r\nPreconditions.checkState(input.rank() == 3, \"3D ... got %s\", input.rank())\r\n```",
        "Unimplemented? Get the params then don't do anything with them...\r\nIf planned to be added later, maybe add UnsupportedOperationException?\r\n(Better to get exception than obscure issues or silent failure)",
        "Couple of things here:\r\n(a) we can use Preconditions.checkArgument/checkState - less verbose\r\n(b) When throwing exceptions, I think it's good practice to include useful information. What dimension? What's the shape of the array? Without that, I need a debugger to get that information, which adds a lot of time required to fix it..."
      ],
      "deeplearning4j-use-modern-api-methods": [
        "Let's direct users to Nd4j.createFromArray instead.\r\nUnlike Nd4j.create, there's overloads for all java primitive types and Number (long, short, Long, Boolean etc), and so users are less likely to get confused with Nd4j.create(int[]), where they think the int[] is content just like Nd4j.create(double[]), but it's actually shape",
        "This is old. Nd4j.createFromArray has overloads up to 4d for all types",
        "Use `Nd4j.zeros(DataType.DOUBLE, 5)`,  no need to do it in 2 lines with array for shape.\r\nMaybe also show the import for DataType the first time it's used.",
        "Show casting example as solution to this:\r\n`INDArray x3 = x.add(x2.cast(DataType.DOUBLE))`"
      ],
      "deeplearning4j-clean-up-your-code": [
        "Add private no-arg constructor to avoid users being able to do ```new ValidationUtils()```",
        "This comment applies to all of your uses of Preconditions case: don't create strings like this.\r\nThe correct way of using preconditions class doesn't (in most cases) result in any object creation unless the precondition fails:\r\n```\r\nPreconditions.checkArgument(data >= 0, \"Values for %s must be >= 0, got %s\", paramName, data);\r\n```"
      ],
      "deeplearning4j-descriptive-error-context": [
        "Is \"no axis\" a legitimate use case? More likely to be a bug than \"I want to implement identity op via standardize\".\r\nIMO exception would be better."
      ],
      "deeplearning4j-user-friendly-documentation-examples": [
        "Let's move these two \"don't do this\" to a section at the very end. Totally fine to mention them, but we shouldn't be opening with a bunch of things to *not* do that the user might not have even thought to try...",
        "Maybe add comment for args order: (start, stop, count)",
        "IndArray -> INDArray\r\n\"operations such as\" - mention they are called reduction/accumulation operations.",
        "`.data().asDouble()` etc isn't something I want in the quickstart, it's confusing for new users due to views and f/c order (not to mention permuted arrays).\r\nInstead, direct users to toFloatVector(), toDoubleMatrix() etc\r\n",
        "Missing createFromArray, valueArrayOf, createUninitialized, scalar. Remove copy method (I didn't even realize it was a thing, users can just use assign).\r\n\r\nMaybe also consider splitting out into new subsections:\r\n- Random array creation (randn, randomBernoulli, randomBinomial, randomExponential, choice)\r\n- (De)Serialization (read, readNumpy, createFromNpy/Npz etc) + write methods\r\n\r\nAlso, generally, note that this is just a few of the ops. We have literally hundreds of ops, too many to list here... we don't want users to think that this list is it :)"
      ],
      "deeplearning4j-compare-floating-point-safely": [
        "The reason it isn't equal without the toString is datatypes.\r\nBy design we don't cast on equals.\r\nI'd recommend either specifying datatypes on all INDArrays, or on none of them.\r\nTests default to double; you are comparing a float [10,10] with double [10,10] hence the failure.",
        "Let's replace these 5 lines with: ```return Double.compareDouble(lhs.getFitness(), rhs.getFitness())``` (possibly with order switched, if that's the intention here).\r\nNote I'm assuming the fitness field is protected/private and getter via ```@Data``` was added, as per earlier review comment.\r\nSame thing for other comparator."
      ],
      "deeplearning4j-validate-and-document-nulls": [
        "General point (applies everywhere) - if a method allows null values, it should be marked as such in the docs, with a brief explanation of the behaviour for null vs. non-null.\r\nSomething along the lines:\r\n`@param workspace Optional memory workspace to define the buffer in, may be null (buffer not created in workspace)`",
        "Handling of nulls? You have some in the later methods, but not here...\r\nIf null is acceptable, we should return null here. If not, let's either add a lombok ```@NonNull``` or a ```Preconditions.checkNotNull(```. At present, nulls will give a non-useful NPE.",
        "I don't think point(null) is valid?\r\nThe way you have it implemented currently (with masks) it won't return a single value along that dimension, I think.\r\nAnd if we don't allow null, switch arg to ```int```",
        "Switch all of these array methods to var-args - ```var(int... shape)``` etc\r\nMaybe also a ```Preconditions.checkArgument(shape != null  && shape.length > 0, \"Invalid shape: %s\", shape)```"
      ],
      "deeplearning4j-clear-descriptive-identifiers": [
        "What about ```var_``` prefix instead of ```sd_var_```? It's shorter, and no less descriptive.",
        "Yeah, good point. OK, sounds good :)"
      ],
      "deeplearning4j-optimize-hardware-acceleration": [
        "Can't have this dep here even under test scope, for CI CUDA-only builds:\r\nhttps://github.com/deeplearning4j/deeplearning4j/blob/a948b1364329c7281aa2302524e2b9dcd6858cf2/nd4j/nd4j-parameter-server-parent/nd4j-parameter-server-node/pom.xml#L77"
      ],
      "deeplearning4j-numerical-stability-practices": [
        "Why addition of 1e-5? Normally this is for numerical precision, but I don't believe we risk underflowing here even if l2 norm is 0? (We get 0/1 in that case)",
        "Ah, makes sense, let's leave it then.",
        "Again:\r\nBy definition, the score calculation _must_ use the SameDiff instance. The `output = activationFn.getActivation` part is ok, but after that just call SameDiff.output providing the placeholders instead.",
        "This is a little unintuitive, but it's definitely intended (and required).\r\nConsider the following graph:\r\n```\r\ninput -> split -> (A,B)\r\nB -> lossFn\r\n```\r\nHere, A is unused (in that it doesn't contribute to the loss function). But to do split op backprop, we need gradient variables/arrays for both outputs (A and B).\r\nWe know the shape and type of dL/dA must be exactly the same as A; we also know that the loss function doesn't depend on A. Hence, dL/dA == zerosLike(A)",
        "Done."
      ],
      "deeplearning4j-document-api-completely": [
        "https://github.com/eclipse/deeplearning4j/issues/8031\r\n\r\nAlso `@see` is self-referential?\r\nShould be `#pad(INDArray, int[][], List<double[]>, PadMode)` I think? (there's other ones like this).\r\nAlso for future reference: when the signatures differ, we need to specify the default behaviour / arg values, otherwise the user has to try and dig this out of the code.\r\nA good comment would be: `As per {@link ...} with 'constantValues' being zeros (zero padding)`",
        "I get what you're going for here. But suppose the user comes directly to the argMax method (not via the argMin see/link).\r\nThey could reasonably intepret this to mean that *somehow* the minimum might be returned by this method.\r\n\r\nBetter: only talk about argmax here. And for argMin, do something like `An per {@link argMax(...)} but for minimum values`",
        "Technically not javadoc, but you might as well fix them when you see them.\r\nDeprecated methods should follow the following format (usually)\r\n1. An `@Deprecated` annotation on the method (or class if applicable)\r\n2. A `@deprecated` javadoc tag\r\n3. Javadoc tag should include what to use instead (+ why it was deprecated, if applicable)\r\n4. Usually - no other comments other than the deprecated information (to discourage users from using it at all)\r\n\r\nAlso a minor nitpick: I prefer the `{@link ...}` style directly in the `@deprecated` tag.\r\n`@see` gets rendered to javadoc as something like:\r\n```\r\nSee Also:\r\nsomeMethod(int, double)\r\n```\r\nSemantically - \"see also\" is different to \"use this instead\".",
        "Better javadoc would be good. Mainly clarify exactly what you mean by \"standardize\"",
        "As a general rule: any time we add a ```@Deprecated``` tag we should have javadoc to explain why and/or what to use instead. Even better: add a date/version (pretty sure we're going with 1.0.0-beta for next release)",
        "More javadoc - limitations, when to use.\r\nAlso how to build one (i.e., \"use PI builder configured with X, not this class\") - that might not be obvious to users at first glance (it wasn't for me)"
      ]
    }
  },
  "asmorkalov": {
    "repos": [
      "opencv/opencv"
    ],
    "entries": [
      {
        "slug": "opencv-cleanup-before-errors",
        "title": "Cleanup before errors"
      },
      {
        "slug": "opencv-clear-api-contracts",
        "title": "Clear API contracts"
      },
      {
        "slug": "opencv-code-for-readability",
        "title": "Code for readability"
      },
      {
        "slug": "opencv-consistent-descriptive-naming",
        "title": "Consistent descriptive naming"
      },
      {
        "slug": "opencv-cross-platform-api-design-rules",
        "title": "Cross-platform API design rules"
      },
      {
        "slug": "opencv-document-configuration-version-requirements",
        "title": "Document configuration version requirements"
      },
      {
        "slug": "opencv-document-properly-with-references",
        "title": "Document properly with references"
      },
      {
        "slug": "opencv-feature-flag-convention",
        "title": "Feature flag convention"
      },
      {
        "slug": "opencv-framework-synchronization-practices",
        "title": "Framework synchronization practices"
      },
      {
        "slug": "opencv-maintain-build-compatibility",
        "title": "Maintain build compatibility"
      },
      {
        "slug": "opencv-maintain-code-consistency",
        "title": "Maintain code consistency"
      },
      {
        "slug": "opencv-meaningful-semantic-naming",
        "title": "Meaningful semantic naming"
      },
      {
        "slug": "opencv-optimize-container-access",
        "title": "Optimize container access"
      },
      {
        "slug": "opencv-optimize-memory-allocation-patterns",
        "title": "Optimize memory allocation patterns"
      },
      {
        "slug": "opencv-prevent-null-vulnerabilities",
        "title": "Prevent null vulnerabilities"
      },
      {
        "slug": "opencv-use-opencv-error-mechanisms",
        "title": "Use OpenCV error mechanisms"
      },
      {
        "slug": "opencv-use-optimized-functions",
        "title": "Use optimized functions"
      },
      {
        "slug": "opencv-use-proper-assertions",
        "title": "Use proper assertions"
      },
      {
        "slug": "opencv-validate-tensor-dimensions",
        "title": "Validate tensor dimensions"
      }
    ],
    "comments": {
      "opencv-optimize-container-access": [
        "just local `std::vector<>` outside of `if` should be enough here. It's empty by default also you save one new/delete pair if condition is true."
      ],
      "opencv-code-for-readability": [
        "Let's extract extension search as dedicated static function. It should make the code more readable."
      ],
      "opencv-document-properly-with-references": [
        "Please move the text to tutorial (markdown) and add references to the header file.",
        "It should go to make ccm header to be included into documentation and bindings. Please CV_EXPORTS_W to generate Java and Python bindings for it too.",
        "I propose to use the same description as for cv::solvePnpRansac with fisheye model reference and add reference to SOLVEPNP_XXX constants instead of copy."
      ],
      "opencv-document-configuration-version-requirements": [
        "I propose to add command line option, e.g. \"--strict-dependencies\" to make the check optional.",
        "I want to switch CI to the default configuration and always build SDK and AAR packages with 16kb pages support. The packages will work with both old and new memory configuration. It just tunes ELF sections alignment a bit.  The option will be ignored with NDK before 27."
      ],
      "opencv-consistent-descriptive-naming": [
        "We use all capital letters for constant names.",
        "The same note about capital letters,\r\ne.g. `COLOR_SPACE_AppleRGB` -> `COLOR_SPACE_APPLE_RGB`",
        "The name is very controversial. I propose `THRESH_DRYRUN` or something similar."
      ],
      "opencv-feature-flag-convention": [
        "Good point. Fixed.",
        "Looks like I cannot make it public for now. We get redefinition issue. The macros are defined by both HAL and IPP core. I made it private for now to exclude the redefinition issue and added note to CMake."
      ],
      "opencv-cleanup-before-errors": [
        "Looks like the assert will be disabled in regular release builds: https://en.cppreference.com/w/cpp/error/assert.\r\nWhy not just CV_Assert? It's defined in `opencv2/core/base.hpp`"
      ],
      "opencv-maintain-build-compatibility": [
        "It should break static linkage, if OpenCV is build against own zlib-ng, but not system-wide.",
        "> protobuf_generate Added in version 3.13.\r\n\r\nIt breaks build with older CMake. I propose to add CMake version check and presume the old branch for old CMake.\r\n"
      ],
      "opencv-use-opencv-error-mechanisms": [
        "Please do not use try-catch. OpenCV uses CV_Assert, CV_Check for the function input validation and and CV_LOG_DEBUG/CV_LOG_INFO for notifications. Function should not silently ignore obvious invalid inputs like not enough points or wrong data types and ranges.  ",
        "OpenCV uses CV_Assert for such purposes.",
        "please add CV_Assert with input type checks at least. The function is public API now and should handle invalid input correctly.",
        "It's still relevant.",
        "Please use CV_Error, CV_Assert, etc. See https://docs.opencv.org/5.x/db/de0/group__core__utils.html#ga5b48c333c777666e076bd7052799f891",
        "It's error. I propose to include amount of channels to error message in printf style like https://github.com/opencv/opencv/blob/c21d0ad9d08d364542bb4a6eb971ee3051ccba63/modules/imgcodecs/src/grfmt_jpeg.cpp#L771",
        "OpenCV's `imread` is exception safe. Please use CV_LOG_WARNING and analogs and return satus instead of exception."
      ],
      "opencv-cross-platform-api-design-rules": [
        "size_t is not wrapped to Python and Java correctly. Java even does not support unsigned types. Please use int instead.",
        "size_t does not work well with Java and Python bindings. let's use just int.\r\n",
        "Please use `CV_EXPORTS_W` and `CV_WRAP` to make it available from Python,  Java and other binded languages."
      ],
      "opencv-use-optimized-functions": [
        "inRange should be enough instead of it: https://docs.opencv.org/4.x/d2/de8/group__core__array.html#ga48af0ab51e36436c5d04340e036ce981",
        "No need to spit cv::Mat by channels for it. cv::sum supports channels: https://docs.opencv.org/5.x/d2/de8/group__core__array.html#ga716e10a2dd9e228e4d3c95818f106722",
        "I propose to convert Gamma correction to public function with InputArray and OutputArray. It's useful independently from the pipeline and also may be significantly optimized with universal intrinsics.",
        "CV_SIMD_WIDTH is compile time constant. It may not work correctly with _SCALABLE branch. please use `VTraits<xxx>::max_nlanes` instead. For fixed-size SIMD it works in the same way.",
        "@fengyuentau please correct me, if I'm wrong.",
        "Yes, it does not work, because `w` value is not known in compile time. `VTraits<xxx>::max_nlanes` is compile time constant. It's equal to `CV_SIMD_WIDTH` for fixed SIMD size architectures (x86). RISC-V RVV vector size is not known in compile time, but we know maximum vector length and use it for intermediate buffers to fit any feasible vector size.",
        "Please use cv::PSNR instead: https://docs.opencv.org/4.x/d2/de8/group__core__array.html#ga3119e3ea73010a6f810bb05aa36ac8d6",
        "I propose to use cv::RNG for it to make it manageable outside:\r\n- User may set seed to get deterministic behaviour\r\n- OpenCV test system fixes cv::RNG seed for each test independently."
      ],
      "opencv-prevent-null-vulnerabilities": [
        "please use std::vector, std::array or cv::AutoBuffer to prevent memory leaks in cases of parser failrue.",
        "Let's use `std::vector<>` or `cv::AutoBuffer` for locals to prevent leaks.",
        "I propose to use `std::vector` or `cv::AutoBuffer` to prevent memory leaks. ",
        "Pleas use `cv::AutoBuffer` or `std::vector` for temporary buffers instead new/delete to prevent memory leaks. `cv::AutoBuffer` is preferable, it may use stack space, if buffer is small enough."
      ],
      "opencv-use-proper-assertions": [
        "`ASSERT_FALSE(chartsRGB.empty());`",
        "`ASSERT_FALSE(chartsRGB.empty());` here and bellow.",
        "`EXPECT_FALSE(src.empty()) << Cannot open test image perf/1920x1080.png;`"
      ],
      "opencv-maintain-code-consistency": [
        "using namespace is very bad practice. It affects everything, even, if the header is not included directly.",
        "Please remove dead code, or guard it with macro, if you want to use it later."
      ],
      "opencv-validate-tensor-dimensions": [
        "```\r\nvector<Mat> channels = {\r\n    Mat(output_transposed.size[1], output_transposed.size[2], CV_32F, output_transposed.ptr<float>(0)),\r\n    Mat(output_transposed.size[1], output_transposed.size[2], CV_32F, output_transposed.ptr<float>(1)),\r\n    Mat(output_transposed.size[1], output_transposed.size[2], CV_32F, output_transposed.ptr<float>(2))\r\n};\r\n```"
      ],
      "opencv-framework-synchronization-practices": [
        "`stripeHeight = nThreads * 10;` sounds strange. More threads -> larger piece for each thread.",
        "threads are 8. It's not true:\r\n- Android build uses 2 threads by default. It's done to prevent overheating, but may be changed in future.\r\n- Linux builds use all available cores.\r\n- parallel_for_ serializes nested parallel_for_ calls. So you can easily get 1 here.",
        "We usually set granularity to some reasonable size for single thread. OpenCV uses dynamic scheduling, so all other steps are done automatically.",
        "I propose to use `num_worker_threads = cv::getNumThreads()` to make it manageable outside. See https://docs.opencv.org/4.x/db/de0/group__core__utils.html#ga2db334ec41d98da3129ef4a2342fc4d4 "
      ],
      "opencv-optimize-memory-allocation-patterns": [
        "It's more efficient to use  std::vector<int> dims(dim_count); and assign values, rather than call push_back and trigger reallocations.",
        "If I understood correctly, The m_read_buffer is resized only once here ant it's size is always 16k. The `m_read_buffer` is used in ::read() only. M.b. it's better to make it local variable there. If you need the buffer shared between readHeader and readData then it makes sense to initialize the buffer in decoder constructor.",
        "I propose to use local variable on stack. new is redundant here. Also it's not deleted afterwards.",
        "I propose to use local variable on stack. new is redundant here. Also it's not deleted afterwards."
      ],
      "opencv-clear-api-contracts": [
        "Need a warning, that GDAL does not support metadata for now.",
        "I propose to Move BRG/RGB logic into model with a flag (default to bgr). All OpenCV functions are designed for BGR. The flag allows to make optimizations without API change.",
        "The `p` parameter does not have default value like `Point p = Point()`. It means that the point is user provided value. I do not think that we sound handle `Point()`, a.k.a. (0,0) as special case.",
        "Why do we need special cases in code? User should provide location, shouldn't he/she?"
      ],
      "opencv-meaningful-semantic-naming": [
        "It makes sense to move key name to the function parameters. E.g.\r\ncheck_cmake_flag_enabled(cmake_file, \"HAVE_IPP\")\r\n\r\nThe same function may be used for KleidiCV and other dependencies.",
        "It breaks compatibility. M.b. rename the method to detectMarkersMultiDict?",
        "What if use `namespace_prefix_override` solution like for the functions? Namespace is already appended to the function names, if it's not overridden by config. It's less hacky and more obvious."
      ]
    }
  },
  "MikeMcQuaid": {
    "repos": [
      "Homebrew/brew"
    ],
    "entries": [
      {
        "slug": "brew-avoid-variable-name-abbreviations",
        "title": "Avoid variable name abbreviations"
      },
      {
        "slug": "brew-clear-code-examples",
        "title": "Clear code examples"
      },
      {
        "slug": "brew-clear-error-recovery-paths",
        "title": "Clear error recovery paths"
      },
      {
        "slug": "brew-decouple-ci-from-code",
        "title": "Decouple CI from code"
      },
      {
        "slug": "brew-document-ci-pipeline-comprehensively",
        "title": "Document CI pipeline comprehensively"
      },
      {
        "slug": "brew-document-non-obvious-decisions",
        "title": "Document non-obvious decisions"
      },
      {
        "slug": "brew-environment-variable-safety",
        "title": "Environment variable safety"
      },
      {
        "slug": "brew-evaluate-security-control-effectiveness",
        "title": "Evaluate security control effectiveness"
      },
      {
        "slug": "brew-fail-with-messages",
        "title": "Fail with messages"
      },
      {
        "slug": "brew-follow-established-naming-patterns",
        "title": "Follow established naming patterns"
      },
      {
        "slug": "brew-follow-support-tiers",
        "title": "Follow support tiers"
      },
      {
        "slug": "brew-minimize-unnecessary-operations",
        "title": "Minimize unnecessary operations"
      },
      {
        "slug": "brew-optimize-collection-operations",
        "title": "Optimize collection operations"
      },
      {
        "slug": "brew-prefer-explicit-nil-handling",
        "title": "Prefer explicit nil handling"
      },
      {
        "slug": "brew-prefer-flags-over-conditionals",
        "title": "Prefer flags over conditionals"
      },
      {
        "slug": "brew-secure-api-url-parsing",
        "title": "Secure API URL parsing"
      },
      {
        "slug": "brew-simplify-complex-code-blocks",
        "title": "Simplify complex code blocks"
      },
      {
        "slug": "brew-standardize-api-integration-patterns",
        "title": "Standardize API integration patterns"
      },
      {
        "slug": "brew-structure-test-fixtures-clearly",
        "title": "Structure test fixtures clearly"
      },
      {
        "slug": "brew-structured-environment-configuration",
        "title": "Structured environment configuration"
      },
      {
        "slug": "brew-use-ascii-only-urls",
        "title": "Use ASCII-only URLs"
      }
    ],
    "comments": {
      "brew-evaluate-security-control-effectiveness": [
        "```suggestion\r\nNote that unlike formulae, casks do not consider the `sha256` stanza to be a meaningful security measure\r\nas maintainers cannot realistically check them for authenticity. Casks download from upstream; if a malicious\r\nactor compromised a URL, they could potentially compromise a version and make it look like an update.\r\n```"
      ],
      "brew-simplify-complex-code-blocks": [
        "```suggestion\r\n    print_stderr = if verbose && show_info\r\n      true\r\n    else\r\n      false\r\n    end\r\n```\r\nis a bit easier to read",
        "Can these be dedicated methods rather than a lambda? The method this is part of is getting very long and we don't typically use `lambda` like this just for variable scoping."
      ],
      "brew-clear-code-examples": [
        "Love this version, thanks @jvns! @colindean could you use this verbatim?\r\n\r\nThanks both!",
        "```suggestion\r\n```\r\n\r\nthis should instead rely on the rubydoc.brew.sh documentation rather than duplicating it",
        "```suggestion\r\n| `require_root`          | `false`      |  yes  |  yes  | whether the service requires root access. If true, Homebrew hints at using `sudo` on various occasions, but does not enforce it |\r\n```"
      ],
      "brew-environment-variable-safety": [
        "```suggestion\r\n  if [[ -r \"/var/tmp\" && -w \"/var/tmp\" ]]\r\n  then\r\n    HOMEBREW_DEFAULT_TEMP=\"/var/tmp\"\r\n  else\r\n    HOMEBREW_DEFAULT_TEMP=\"/tmp\"\r\n  fi\r\n```",
        "@carlocab great point, will adjust and add an explicit `PATH` suggestion"
      ],
      "brew-prefer-explicit-nil-handling": [
        "```suggestion\r\n      @name = name.presence\r\n      @version = Version.new(version) if version.present?\r\n```",
        "@abitrolly Why does it make sense to accept a blank name from the user here?",
        "Ok, I understand now, thanks.",
        "```suggestion\r\n        if (pypi_extras = extras.presence)\r\n          out += \"[#{pypi_extras.join(\",\")}]\" \r\n        end\r\n```",
        "This feels like it would be nicer to:\r\n- move this logic to an `else` in `livecheck_url_to_string`\r\n- make `livecheck_url_to_string` always return a `String` rather than a `T.nilable(String)`\r\n\r\nAs a general rule/concept: whenever you can remove `T.nilable` usage and `raise` instead: it's nicer to do so when using Sorbet.",
        "```suggestion\r\n          if (bottle = formula.bottle)\r\n```",
        "```suggestion\r\n    resource = github_packages_manifest_resource\r\n    return unless resource&.downloaded?\r\n```",
        "```suggestion\r\n    resource = github_packages_manifest_resource\r\n    return unless resource&.downloaded?\r\n```"
      ],
      "brew-avoid-variable-name-abbreviations": [
        "No idea what `uphpp` means. Please use longer variable names.",
        "```suggestion\r\n  repository=\"$(tr '[:upper:]' '[:lower:]' <<<\"${dir#*/}\")\"\r\n  repository=\"${repository#@(home|linux)brew-}\"\r\n  echo \"${user}/${repository}\"\r\n```",
        "```suggestion\r\n  user=\"$(tr '[:upper:]' '[:lower:]' <<<\"${directory%%/*}\")\"\r\n  repo=\"$(tr '[:upper:]' '[:lower:]' <<<\"${directory#*/}\")\"\r\n```"
      ],
      "brew-fail-with-messages": [
        "This shouldn't fail silently.",
        "How/when will this exit with a non-zero result? Not currently seeing any e.g. `return 1` in there?",
        "@ZhongRuoyu whoops, still not seeing it!",
        "> made sure at the call site that the function is only called when there are no `brew tap` arguments\r\n\r\nGotcha, I see that now. Might be a little clearer if this is above the wildcard commands and doesn't do `&& exit 0` but instead has `exit 0` be unconditional as there's no `return` status to check here."
      ],
      "brew-document-non-obvious-decisions": [
        "Please add a comment, and ideally turn this into a constant/variable, explaining why 100 is used otherwise it seems arbitrary.",
        "```suggestion\r\n              # maximum length of PR body is 65,536 characters so let's truncate release notes to half of that.\r\n              body = github_release_data[\"body\"].truncate(32_768)\r\n```\r\n\r\nwould be nicer to truncate based on an actual length here",
        "@bevanjkay worth handling in a follow-up I think. My suggestion would be that `--bump-synced` does not include any release notes or it handles the truncation its own way/as well.",
        "As mentioned before: I'd love to see this be a temporary stop-gap while we add a DSL. I think this needs some pretty hefty amounts of comments until then explaining why these particular formulae need to be doing what they are doing here."
      ],
      "brew-structured-environment-configuration": [
        "@Bo98 Is `HOMEBREW_TEMP` not the per-user temp? If not: ideally we'd do that and: yes, agreed.",
        "@Bo98 done!",
        "This will override e.g. `brew search` behaviour which seems undesirable.",
        "I'm game for searching internal taps but this would break searching non-internal taps."
      ],
      "brew-use-ascii-only-urls": [
        "I'd rather we be excessively strict for now, provided homebrew/core and homebrew/cask pass, and loosen it later."
      ],
      "brew-follow-support-tiers": [
        "```suggestion\r\ncommand -v brew || export PATH=\"/opt/homebrew/bin:/home/linuxbrew/.linuxbrew/bin:/usr/local/bin\"\r\ncommand -v brew && eval \"$(brew shellenv)\"\r\n```",
        "@colindean This seems sufficient to handle all platforms without error output and is short and easier to understand. \r\n\r\nFor future: can you allow maintainers to commit to your fork? Thanks."
      ],
      "brew-standardize-api-integration-patterns": [
        "Does `brew services list --json` contain the information we need? If so, would be nice to use that instead. If not, maybe it'd be nice to add that information in there.",
        "I think this needs to be more specific as to where it's used. It's not using this instead for e.g. all GitHub searches."
      ],
      "brew-secure-api-url-parsing": [
        "```suggestion\r\n        if [[ \"${UPSTREAM_REPOSITORY_URL}\" = \"https://github.com/\"* ]] && \r\n           [[ -n \"${HOMEBREW_GITHUB_API_TOKEN}\" ]]\r\n```\r\n\r\nor similar as otherwise `HOMEBREW_GITHUB_API_TOKEN` being set at all may limit this.\r\n\r\nIt may be desirable to have a single `[[ \"${UPSTREAM_REPOSITORY_URL}\" == \"https://github.com/\"* ]]` check which sets a variable e.g. `UPSTREAM_REPOSITORY_GITHUB_GLOB` or something which you can then check in these multiple places later.",
        "```suggestion\r\n        if [[ -n \"${UPSTREAM_REPOSITORY_TOKEN}\" ]]\r\n        then\r\n          CURL_GITHUB_API_ARGS=(\"--header\" \"Authorization: token ${UPSTREAM_REPOSITORY_TOKEN}\")\r\n        elif [[ -n \"${HOMEBREW_GITHUB_API_TOKEN}\" ]]\r\n        then\r\n          CURL_GITHUB_API_ARGS=(\"--header\" \"Authorization: token ${HOMEBREW_GITHUB_API_TOKEN}\")\r\n```"
      ],
      "brew-follow-established-naming-patterns": [
        "- This is a behaviour change as it'll no longer look for `archive|releases` like it did before\r\n- `m` is a poor variable name here, please use a better, longer one\r\n- I think it's worth continuing to set `user` and `repo` without `fetch`",
        "```suggestion\r\n                basename = if File.directory?(path)\r\n                  File.basename(path)\r\n                 else\r\n                   File.basename(path, \".*\")\r\n                 end\r\n                excluded_names.include?(basename)\r\n```",
        "```suggestion\r\n  def self.binary_linked_to_library?(binary, library, prefix = HOMEBREW_PREFIX)\r\n```\r\ngiven it returns a `T::Boolean`",
        "How about:\r\n```suggestion\r\n      return unless cask.livecheck?\r\n```\r\nWould that naming work?",
        "> It makes sense to me, as `#livecheck` returns a [`Livecheck` object](https://github.com/Homebrew/brew/blob/84823d80f71a01eba4febbbdf4259a687219885b/Library/Homebrew/livecheck.rb) (the DSL values), so `#livecheck?` would align with that.\r\n\r\nAnother option would be `bottle_defined?` which is the equivalent for the `bottle` block.",
        "Yup, sounds good thanks!"
      ],
      "brew-clear-error-recovery-paths": [
        "Will this not now be output even if `Homebrew::EnvConfig.github_api_token` is unset? If so, that's undesirable.",
        "I think an extra case checking if `if Homebrew::EnvConfig.github_api_token.present?` and saying `HOMEBREW_GITHUB_API_TOKEN is unset` or similar would improve this, thanks!",
        "Should better handle here where there's no group name. because `Failed setting group \"\"` doesn't seem great.",
        "```suggestion\r\n      sleep_time = 2 ** @attestation_retry_count[bottle]\r\n```\r\nwould be fine with me if you want this to take smaller jumps/be quicker to fail",
        "```suggestion\r\n    ATTESTATION_MAX_RETRIES = 2\r\n```\r\nor \r\n```suggestion\r\n    ATTESTATION_MAX_RETRIES = 3\r\n```\r\nwould be fine with me if you want to fail earlier"
      ],
      "brew-prefer-flags-over-conditionals": [
        "Can you explain why we'd want to claim HTTP2 support is present when `curl` says its not?",
        "Gotcha, thanks. I'd be tempted to just use Homebrew's `curl` in that case rather than have a shim. Can we use `uses_from_macos \"curl\", since:` instead to handle macOS 13 and below?",
        "> @MikeMcQuaid that hack (`uses_from_macos \"curl\", since: :sonoma`) would actually enable the linkage against system curl, see [this build log](https://github.com/Homebrew/homebrew-core/actions/runs/11872759852/job/33086872512).\r\n\r\n@chenrui333 on which macOS versions would it link against/not link against system `curl`? Thanks!",
        "> I know this would be only for ventura builds.\r\n\r\nFor one, non-latest macOS version this seems overkill, personally, but I'm open to thoughts from other maintainers.",
        "@chenrui333 Thanks, not worth doing for a single OS version IMO, sorry."
      ],
      "brew-structure-test-fixtures-clearly": [
        "Why was this a loop?\r\n```suggestion\r\n        result = { name: fc.name, version: fc.version }\r\n        expect(result).to eq(test.fetch(:expected))\r\n```",
        "What do you propose? `expected_name` and `expected_version`? Criticising without offering an alternative is not helpful.",
        "> The original idea is fully declarative table test fixture.\r\n\r\nThis is not something we do in Homebrew.\r\n\r\n> Dynamic enrichment of static test data with \"default values\" makes it less readable for me.\r\n\r\nAs someone who is neither an expert in Ruby nor Homebrew, readability for you is not the desired outcome here.\r\n\r\n> If there is an error that sets `head` to nil, and test expects `false`, the test won't catch it.\r\n\r\nThe Sorbet type system will catch it.\r\n\r\n> The same with version here - if we need a test that checks version is null after parsing, it should be explicitly set in fixture.\r\n\r\nI disagree.\r\n\r\n> No need to complicate the logic to repeat it for all URLs where the version is irrelevant.\r\n\r\nThe version is never irrelevant.\r\n\r\n> What could be relevant is to add a fixture entry that tests that version from params overrides the one parsed from URL.\r\n\r\nWill review a follow-up PR to do that."
      ],
      "brew-minimize-unnecessary-operations": [
        "If it's slow: be worth putting this in the loop lazily evaluated as late as possible and memoized so that we can avoid more cases where it might not need to be called at all e.g. for the last possible `next`",
        "Would be nicer to put this in `if adopt` above so it avoids reading the `source_plist` etc. unnecessarily."
      ],
      "brew-optimize-collection-operations": [
        "```suggestion\r\n        json[\"old_tokens\"] = [old_token, *json[\"old_tokens\"]].compact.uniq\r\n```",
        "```suggestion\r\n          formulae_and_casks &= excluded_autobump\r\n```\r\nor \r\n```suggestion\r\n          formulae_and_casks |= excluded_autobump\r\n```\r\nI can't remember which ðŸ˜… ",
        "That works for me also! anything that avoids a loop.",
        "```suggestion\r\n              installed_formula.deps.required.map(&:to_formula).any? { |dep| sized_formulae.include?(dep) }\r\n```\r\nany may be able to simplify this further still with `intersect?`",
        "```suggestion\r\n        installed_formula_tap_names = Formula.installed.filter_map(&:tap).uniq.reject(&:official?)\r\n```",
        "```suggestion\r\n        installed_cask_tap_names = Cask::Caskroom.casks.filter_map(&:tap).uniq.reject(&:official?)\r\n```",
        "```suggestion\r\n        @non_core_taps ||= Tap.installed.reject(&:core_tap?).reject(&:core_cask_tap?)\r\n```"
      ],
      "brew-decouple-ci-from-code": [
        "We should stop Monterey CI as soon as we start any Sequoia CI.",
        "@Bo98 Bit confused here, sorry! Can you make a code suggestion on whatever you'd want to see changed in this PR specifically before it is merged e.g. today? Thanks â¤ï¸ ",
        "Can you elaborate a bit more on what \"separate these controlling CI \" and \"separating CI control\" means here? It's not totally clear. Thanks!",
        "Thanks for explaining. Sounds like the main blocker therefore is updating the CI runners to not use the same logic as these numbers here.",
        "> We'll probably not merge this until there's good bottle coverage.\r\n\r\nWe should merge this as soon as we're running Sequoia in CI.\r\n\r\nIt doesn't matter for end-users if we have zero Sequoia bottle coverage as we can still support it before that."
      ],
      "brew-document-ci-pipeline-comprehensively": [
        "It does a few more things, too?",
        "@Rylan12 `brew readall`, `brew test-bot --only-formulae --test-default-formula`, `brew doctor` seem like the important bits."
      ]
    }
  },
  "agibsonccc": {
    "repos": [
      "deeplearning4j/deeplearning4j"
    ],
    "entries": [
      {
        "slug": "deeplearning4j-always-secure-your-locks",
        "title": "Always secure your locks"
      },
      {
        "slug": "deeplearning4j-centralize-dependency-management",
        "title": "Centralize dependency management"
      },
      {
        "slug": "deeplearning4j-clean-up-your-code",
        "title": "Clean up your code"
      },
      {
        "slug": "deeplearning4j-compare-floating-point-safely",
        "title": "Compare floating-point safely"
      },
      {
        "slug": "deeplearning4j-configurable-resource-locations",
        "title": "Configurable resource locations"
      },
      {
        "slug": "deeplearning4j-cross-platform-algorithm-optimization",
        "title": "Cross-platform algorithm optimization"
      },
      {
        "slug": "deeplearning4j-descriptive-error-context",
        "title": "Descriptive error context"
      },
      {
        "slug": "deeplearning4j-document-in-code-decisions",
        "title": "Document in-code decisions"
      },
      {
        "slug": "deeplearning4j-eliminate-redundant-code",
        "title": "Eliminate redundant code"
      },
      {
        "slug": "deeplearning4j-ensure-test-determinism",
        "title": "Ensure test determinism"
      },
      {
        "slug": "deeplearning4j-keep-configurations-current",
        "title": "Keep configurations current"
      },
      {
        "slug": "deeplearning4j-modular-adaptive-configurations",
        "title": "Modular adaptive configurations"
      },
      {
        "slug": "deeplearning4j-optimize-validation-checks",
        "title": "Optimize validation checks"
      },
      {
        "slug": "deeplearning4j-remove-debugging-artifacts",
        "title": "Remove debugging artifacts"
      },
      {
        "slug": "deeplearning4j-use-logging-best-practices",
        "title": "Use logging best practices"
      },
      {
        "slug": "deeplearning4j-validate-and-document-nulls",
        "title": "Validate and document nulls"
      }
    ],
    "comments": {
      "deeplearning4j-configurable-resource-locations": [
        "@treo I added this based on the model import code. Since I'm attempting to use the new op descriptors in the dynamic custom ops properties now, I wanted an easier/faster way to access the descriptors. The only problems I could think of with this might be graalvm, but if there are I will address those later.",
        "@treo in that case we can tell people to just override the classloader with their own. That functionality was merged a while back. I agree with you and will keep an eye on that though."
      ],
      "deeplearning4j-use-logging-best-practices": [
        "Fixed log levels."
      ],
      "deeplearning4j-keep-configurations-current": [
        "Not yet. Good point though. Let me ping them. Since it's MIT it should be ok.",
        "Mind explaining what vecpp is and ensure it's documented in the readme? My best guess is some something like a configure.in that we manipulate with some parameters?",
        "Removed.",
        "@treo MKLMKL was deprecated long ago. Look at the latest publish: https://anaconda.org/anaconda/mklml MKLDNN became onednn and is now what's shipped with oneapi.",
        "@treo removed the reference here. Feel free to check the source tree where the mkldnn compat layer is bundled: https://github.com/KonduitAI/oneDNN/blob/9cd49946cddb6936d12963d94eb1dbb2170ff3ac/include/mkldnn.h"
      ],
      "deeplearning4j-always-secure-your-locks": [
        "Updates here?"
      ],
      "deeplearning4j-ensure-test-determinism": [
        "Mind adding some comments about how important the seed is here? A log of issues we ran in to with certain tests is handy."
      ],
      "deeplearning4j-centralize-dependency-management": [
        "Done",
        "@saudet  ah good catch, jackson-core and databind are both at 2.12.2. Just pushed the change there.",
        "Create a separate version maybe jboss.netty3.version and io.netty?",
        "Mind adding dependency versions under dependency management in the top level pom? We do this to avoid dependency conflicts across modules.\r\nDo this with \r\n<dependency>\r\n            <groupId>org.bytedeco.javacpp-presets</groupId>\r\n            <artifactId>cpython-platform</artifactId>\r\n            <version>3.6-1.4.4-SNAPSHOT</version>\r\n        </dependency>\r\n        <dependency> \r\n\r\n\r\nconverting to:\r\n<dependency>\r\n            <groupId>org.bytedeco.javacpp-presets</groupId>\r\n            <artifactId>cpython-platform</artifactId>\r\n        </dependency>\r\n        <dependency>\\\r\n\r\nputting the previous declaration under a dependency management section.\r\n\r\nAlso, please version the python and javacpp versions with variables at the top level.",
        "I wonder if a <dependencyManagement> entry would be better here?"
      ],
      "deeplearning4j-modular-adaptive-configurations": [
        "@treo  that was mainly due to environment variable/GH actions debugging. I refactored based on your comments there."
      ],
      "deeplearning4j-cross-platform-algorithm-optimization": [
        "Could you layout where the in files are used and how it works?",
        "@quickwritereader  just make sure to put this in the actual code itself, maybe even in a readme.",
        "Mind adding some declarations here so we know what versions of methods are being invoked? nd4j_debug should be fine here",
        "The point was more to put this comment you just did in the comments so it's in the code and people don't have to search github to find context  :)"
      ],
      "deeplearning4j-eliminate-redundant-code": [
        "I think it just accumulated. I'll work on stripping that down."
      ],
      "deeplearning4j-clean-up-your-code": [
        "Done. Sorry missed that."
      ],
      "deeplearning4j-remove-debugging-artifacts": [
        "Fixed"
      ],
      "deeplearning4j-descriptive-error-context": [
        "Mind putting a location of the error here like: ShapeList:: error message similar to what we do in other error messages? That helps track down the problem.",
        "Are these supposed to be stubs?",
        "Mind adding error messages to these? Otherwise it will be hard to debug (especially from java)",
        "For sure! I was just mentioning that because anyone at the java level debugging something from c++ won't really want to have to click in to and read c++ source code and to try to reverse engineer what the problem is."
      ],
      "deeplearning4j-compare-floating-point-safely": [
        "Fixed"
      ],
      "deeplearning4j-validate-and-document-nulls": [
        "Null comes mostly come the Switch op where 1 branch is null and the other isn't. I'd want to think on this one a bit. I'm not clear if there's anything in specific I'd want to do with the null case beyond that."
      ],
      "deeplearning4j-document-in-code-decisions": [
        "Describing some of the steps here would also be useful. Eg: what's \"special use\" for?",
        "@quickwritereader same thing: put it in the code, not on github",
        "Mind adding a minor comment paragraph explaining the issues found here so we know why we're defaulting to our implementation in certain cases?"
      ],
      "deeplearning4j-optimize-validation-checks": [
        "For performance reasons, I would imagine us wanting all of this behind an if debug. Do you mind implementing this?",
        "For performance reasons, I would imagine us wanting all of this behind an if debug. Do you mind implementing this?"
      ]
    }
  },
  "kamilmysliwiec": {
    "repos": [
      "nestjs/nest"
    ],
    "entries": [
      {
        "slug": "nest-benchmark-before-optimizing",
        "title": "Benchmark before optimizing"
      },
      {
        "slug": "nest-choose-meaningful-identifier-names",
        "title": "Choose meaningful identifier names"
      },
      {
        "slug": "nest-configurable-log-formatting",
        "title": "Configurable log formatting"
      },
      {
        "slug": "nest-descriptive-identifier-names",
        "title": "Descriptive identifier names"
      },
      {
        "slug": "nest-document-configuration-behaviors",
        "title": "Document configuration behaviors"
      },
      {
        "slug": "nest-explicit-default-configurations",
        "title": "Explicit default configurations"
      },
      {
        "slug": "nest-follow-protocol-standards",
        "title": "Follow protocol standards"
      },
      {
        "slug": "nest-graph-based-dependency-management",
        "title": "Graph-based dependency management"
      },
      {
        "slug": "nest-http-header-management",
        "title": "HTTP header management"
      },
      {
        "slug": "nest-modern-null-safety-patterns",
        "title": "Modern null safety patterns"
      },
      {
        "slug": "nest-optimize-critical-path-iterations",
        "title": "Optimize critical path iterations"
      },
      {
        "slug": "nest-package-dependency-configuration",
        "title": "Package dependency configuration"
      },
      {
        "slug": "nest-parameterize-version-requirements",
        "title": "Parameterize version requirements"
      },
      {
        "slug": "nest-pin-dependency-versions",
        "title": "Pin dependency versions"
      },
      {
        "slug": "nest-preserve-api-interface-stability",
        "title": "Preserve API interface stability"
      },
      {
        "slug": "nest-preserve-public-api-stability",
        "title": "Preserve public API stability"
      },
      {
        "slug": "nest-prevent-async-race-conditions",
        "title": "Prevent async race conditions"
      },
      {
        "slug": "nest-prevent-race-conditions",
        "title": "Prevent race conditions"
      },
      {
        "slug": "nest-proper-asynchronous-error-handling",
        "title": "Proper asynchronous error handling"
      },
      {
        "slug": "nest-secure-hash-algorithms",
        "title": "Secure hash algorithms"
      },
      {
        "slug": "nest-standardize-logger-configuration-patterns",
        "title": "Standardize logger configuration patterns"
      },
      {
        "slug": "nest-standardize-null-safety-patterns",
        "title": "Standardize null safety patterns"
      },
      {
        "slug": "nest-strategic-dependency-configuration",
        "title": "Strategic dependency configuration"
      },
      {
        "slug": "nest-structure-behavior-driven-tests-properly",
        "title": "Structure behavior-driven tests properly"
      },
      {
        "slug": "nest-structure-exception-handling-patterns",
        "title": "Structure exception handling patterns"
      },
      {
        "slug": "nest-use-consistent-control-structures",
        "title": "Use consistent control structures"
      },
      {
        "slug": "nest-use-consistent-curly-braces",
        "title": "Use consistent curly braces"
      },
      {
        "slug": "nest-use-factory-providers",
        "title": "Use factory providers"
      },
      {
        "slug": "nest-use-secure-hash-algorithms",
        "title": "Use secure hash algorithms"
      },
      {
        "slug": "nest-use-topological-sorting",
        "title": "Use topological sorting"
      }
    ],
    "comments": {
      "nest-use-consistent-control-structures": [
        "To remain consistent with the rest of the codebase\r\n```suggestion\r\n    if (areThereNoFileIn && this.fileIsRequired) {\r\n      throw this.exceptionFactory('File is required');\r\n    }\r\n    if (!areThereNoFileIn && this.validators.length) {\r\n      await this.validateFilesOrFile(value);\r\n    }\r\n```",
        "even though `void 0` is not necessarily required, please, let's stick with `{ ... }` at least"
      ],
      "nest-configurable-log-formatting": [
        "```suggestion\r\n  json?: boolean;\r\n```",
        "similarly to log levels, we should be able to specify (globally) that the logger should use `json` format for all logs by default ",
        "I think this line:\r\n\r\n```\r\n`${pidMessage}${this.getTimestamp()} ${formattedLogLevel} ${contextMessage}${output}${timestampDiff}\\n`\r\n```\r\nshould be configurable. We could declare another protected method that receives these variables as input parameters (without colors applied). Perhaps that's actually how the `formatMessage` should look likely (we can move everything else back to the original location)",
        "This approach https://github.com/stanimirovv/nest/commit/89d97652caafc26c379ae0268229c4fc3caf3cf2# better fits our needs but we should also allow opt-in coloring the messages. In this case, we should probably declare another dedicated method (let's say `colorize`) that takes the same set of arguments as `formatMessages` and adds colors. This method should be executed from within the `formatMessages` and have `protected` modifier to make it feasible to call it (if needed) from within the custom logger implementation.",
        "Yeah, I think as long as we make `getColorByLogLevel` protected as well (to make it accessible) it could take two arguments (one being a message to format and the second - optional - color to be used that defaults to `this.getColorByLogLever()`)"
      ],
      "nest-secure-hash-algorithms": [
        "Let's do this (since it's faster)"
      ],
      "nest-benchmark-before-optimizing": [
        "Wondering how fast it is compared to @napi-rs/blake-hash ðŸ¤” \r\n\r\nAlso, on a side note, did you have a chance to compare https://github.com/Brooooooklyn/uuid to `@lukeed/uuid` (for generating uuids)?",
        "Why not filter + map? Performance should be marginal in this case (I'd think)",
        "Still, the performance impact should be negligible. filter & map is just cleaner & easier to read, not worth the optimization in this particular case "
      ],
      "nest-package-dependency-configuration": [
        "since we load this package lazily, i'd say we could probably declare `file-type` as a peer dependency instead",
        "we should also add the `peerDependenciesMeta` entry, flagging these deps as _optional_, see example here https://github.com/nestjs/graphql/blob/master/packages/apollo/package.json#L52"
      ],
      "nest-strategic-dependency-configuration": [
        "since we load this package lazily, i'd say we could probably declare `file-type` as a peer dependency instead",
        "we should also add the `peerDependenciesMeta` entry, flagging these deps as _optional_, see example here https://github.com/nestjs/graphql/blob/master/packages/apollo/package.json#L52"
      ],
      "nest-structure-exception-handling-patterns": [
        "Not sure if I'm following. `process.exit` (force exit) shouldn't be necessary. We're also overriding the signal (the reason to kill the process) here",
        "This doesn't work even if you run `node dist/main` (no NestJS CLI involved)",
        "![image](https://github.com/user-attachments/assets/734ee5f9-ccbf-4330-a059-87879374a61e)\r\n\r\nCode \r\n\r\n```js\r\nconsole.log('Hello Node.js');\r\n\r\nprocess.on('exit', () => {\r\n  console.log('about to exit');\r\n});\r\n\r\nsetTimeout(() => {\r\n  console.log('killing');\r\n  process.kill(process.pid, 'SIGTERM');\r\n}, 1000);\r\n```\r\n\r\nNode v20",
        "Ah apologies, I forgot to include on SIGTERM listener @thomaschaaf ",
        "I still cannot reproduce your issue with the following code:\r\n\r\n```ts\r\nimport { NestFactory } from '@nestjs/core';\r\nimport { AppModule } from './app.module';\r\n\r\nasync function bootstrap() {\r\n  const app = await NestFactory.create(AppModule);\r\n  app.enableShutdownHooks();\r\n  await app.listen(3000);\r\n\r\n  console.log(`Application is running on: ${await app.getUrl()}`);\r\n\r\n  process.on('SIGTERM', async () => {\r\n    await app.close();\r\n    console.log('SIGTERM received');\r\n    process.kill(process.pid, 'SIGTERM');\r\n  });\r\n\r\n  process.on('exit', () => {\r\n    console.log('about to exit');\r\n  });\r\n\r\n  setTimeout(() => {\r\n    console.log('killing');\r\n    process.kill(process.pid, 'SIGTERM');\r\n  }, 1000);\r\n}\r\nbootstrap();\r\n```\r\n\r\nWith `node dist/main`:\r\n\r\n```\r\nApplication is running on: http://[::1]:3000\r\nkilling\r\nSIGTERM received\r\nabout to exit\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/63336e26-2562-456d-8893-e4f3a46adb64)\r\n\r\nWith `nest start`\r\n\r\n![image](https://github.com/user-attachments/assets/dd1829a9-f323-4ab6-b77c-e3fdebdb65f7)\r\n\r\nSame with `nest start --watch` (npm run start:dev)\r\n\r\n![image](https://github.com/user-attachments/assets/446e55d1-44f2-4a22-89e6-11d61279fe50)\r\n\r\n",
        "I can replicate the exact same behavior using plain Node.js (without the Nest framework/CLI), so I'm not sure if there's anything we need to change on our end.",
        "Wouldn't it be easier to define an object in which keys represent status codes and their values = classes of corresponding exceptions? e.g. `new HttpErrorByCode[statusCode](error)`?",
        "This:\r\n\r\n```typescript\r\nexport const HttpErrorByCode = {\r\n  [HttpStatus.BAD_REQUEST]: BadRequestException,\r\n  [HttpStatus.UNPROCESSABLE_ENTITY]: UnprocessableEntityException,\r\n```\r\nand:\r\n```typescript\r\nthis.exceptionFactory = (errors => new HttpErrorByCode[this.exceptionCode](errors));\r\n```\r\nshould be enough :) Any additional method/function which leverages `HttpException` is not needed. In addition, we shouldn't allow using `OK` or `CREATED` status. Let's just define an union with error status codes that can be used, e.g.:\r\n\r\n```typescript\r\nexport type ErrorHttpStatusCode = HttpStatus.BAD_REQUEST | HttpStatus.UNPROCESSABLE_ENTITY | etc.\r\n```\r\n"
      ],
      "nest-modern-null-safety-patterns": [
        "Instead of adding a new condition here, we may just swap `||` expression with `??`.",
        "What do you think about using `isNil()` util function here instead? (to exclude `null` and `undefined` values)",
        "I think `isNil()` will be sufficient here :)"
      ],
      "nest-parameterize-version-requirements": [
        "These may quickly get out of date - we will keep supporting v16 even after it's no longer a maintenance version (after September this year). That's why versions were hardcoded before "
      ],
      "nest-document-configuration-behaviors": [
        "```suggestion\r\n```\r\nRedis adapter intentionally wasn't being used in this example (it was meant to be simple and not require additional resources, e.g. Redis db)",
        "> My concern is that the adapter is included in the sample, but it is not clear how to actually use it.\r\n\r\nFair enough! Let's comment these lines out and leave a brief explanation just in case."
      ],
      "nest-prevent-async-race-conditions": [
        "I think I misunderstood what this FR was about.\r\n\r\nDurable providers should not have access to the request-specific attributes (we shouldn't merge the payload with the request) simply because they are **durable** (meaning they are not removed after the request is finished processing). Merging the payload with request objects for durable providers will lead to memory leaks and unexpected behavior of the framework",
        "```suggestion\r\n      Object.assign(args[0] ?? {}, {\r\n        getPattern: () => this.reflectCallbackPattern(callback),\r\n      });\r\n```\r\nHmm.. I'm wondering if this won't cause some unexpected issues ðŸ¤” If socket represents a single connection (and so its instance object is shared?), there's a probability that 2 messages from that socket might be processed asynchronously (independently). \r\n\r\nExample:\r\n- Connection to server is established (socket instance is created)\r\n- Message A is emitted \r\n  - we mutate the \"socket instance\" enhancing it with the \"getPattern()\" method\r\n  - before this message is processed, some asynchronous operation is triggered in (guard/interceptor/wherever)\r\n- In the meantime, message B is emitted\r\n  - we mutate the same \"socket instance\" again replacing the previous \"getPattern()\" method\r\n- Async operation (mentioned above) completes and we're back to processing message A.\r\n- In another interceptor/guard we call `client.getPattern()` when processing message A but it's already overridden with the message B's \"getPattern\" implementation\r\n\r\n",
        "If someone unsubscribes before this promise resolves, a memory leak would occur (`this.routingMap.set(packet.id, callback);`)",
        "@guiruiz it wouldn't because `serialize` isn't async and so `this.routingMap.set` is called synchronously."
      ],
      "nest-prevent-race-conditions": [
        "```suggestion\r\n      Object.assign(args[0] ?? {}, {\r\n        getPattern: () => this.reflectCallbackPattern(callback),\r\n      });\r\n```\r\nHmm.. I'm wondering if this won't cause some unexpected issues ðŸ¤” If socket represents a single connection (and so its instance object is shared?), there's a probability that 2 messages from that socket might be processed asynchronously (independently). \r\n\r\nExample:\r\n- Connection to server is established (socket instance is created)\r\n- Message A is emitted \r\n  - we mutate the \"socket instance\" enhancing it with the \"getPattern()\" method\r\n  - before this message is processed, some asynchronous operation is triggered in (guard/interceptor/wherever)\r\n- In the meantime, message B is emitted\r\n  - we mutate the same \"socket instance\" again replacing the previous \"getPattern()\" method\r\n- Async operation (mentioned above) completes and we're back to processing message A.\r\n- In another interceptor/guard we call `client.getPattern()` when processing message A but it's already overridden with the message B's \"getPattern\" implementation\r\n\r\n",
        "If someone unsubscribes before this promise resolves, a memory leak would occur (`this.routingMap.set(packet.id, callback);`)",
        "@guiruiz it wouldn't because `serialize` isn't async and so `this.routingMap.set` is called synchronously.",
        "If someone applies 1 middleware multiple times, we should run it multiple times = no side-effects."
      ],
      "nest-structure-behavior-driven-tests-properly": [
        "Can we wrap it within a `describe(\"valideteUser\")` block and then inside have to scenarios covered:\r\n\r\n- should return a user object when credentials are valid\r\n- should return null when credentials are invalid\r\n\r\nAnd likewise below with the `login()` method",
        "ping ðŸ“ ",
        "could you follow BDD style? Example:\r\n\r\n```typescript\r\ndescribe('when \"message\" is an object', () => { \r\n    it('should serialize an object', () => {\r\n    \r\n    });\r\n});\r\n```"
      ],
      "nest-preserve-api-interface-stability": [
        "This introduces a breaking change",
        "Updating a public interface with 2 additional methods that weren't there before introduces a breaking change as now every library (platform adapter) will have to update their `HttpServer` implementation as well (including these adapters that we don't control ourselves within the NestJS organization).\r\n\r\n\r\n\r\n",
        "We can alternatively update this method to be optional; in this case we won't have to wait ",
        "This interface shouldn't be modified with unessential props (it's a public API)."
      ],
      "nest-http-header-management": [
        "Why do we presume that `TEMPORARY_REDIRECT` should be the default one?",
        "Make sense now :)"
      ],
      "nest-standardize-logger-configuration-patterns": [
        "```suggestion\r\n  json?: boolean;\r\n```",
        "similarly to log levels, we should be able to specify (globally) that the logger should use `json` format for all logs by default ",
        "I think this line:\r\n\r\n```\r\n`${pidMessage}${this.getTimestamp()} ${formattedLogLevel} ${contextMessage}${output}${timestampDiff}\\n`\r\n```\r\nshould be configurable. We could declare another protected method that receives these variables as input parameters (without colors applied). Perhaps that's actually how the `formatMessage` should look likely (we can move everything else back to the original location)",
        "This approach https://github.com/stanimirovv/nest/commit/89d97652caafc26c379ae0268229c4fc3caf3cf2# better fits our needs but we should also allow opt-in coloring the messages. In this case, we should probably declare another dedicated method (let's say `colorize`) that takes the same set of arguments as `formatMessages` and adds colors. This method should be executed from within the `formatMessages` and have `protected` modifier to make it feasible to call it (if needed) from within the custom logger implementation.",
        "Yeah, I think as long as we make `getColorByLogLevel` protected as well (to make it accessible) it could take two arguments (one being a message to format and the second - optional - color to be used that defaults to `this.getColorByLogLever()`)"
      ],
      "nest-use-consistent-curly-braces": [
        "To remain consistent with the rest of the codebase\r\n```suggestion\r\n    if (areThereNoFileIn && this.fileIsRequired) {\r\n      throw this.exceptionFactory('File is required');\r\n    }\r\n    if (!areThereNoFileIn && this.validators.length) {\r\n      await this.validateFilesOrFile(value);\r\n    }\r\n```",
        "nit(style): can we use if { } instead of inline ifs? just for the sake of consistency with the rest of the codebase",
        "even though `void 0` is not necessarily required, please, let's stick with `{ ... }` at least"
      ],
      "nest-preserve-public-api-stability": [
        "This introduces a breaking change",
        "Updating a public interface with 2 additional methods that weren't there before introduces a breaking change as now every library (platform adapter) will have to update their `HttpServer` implementation as well (including these adapters that we don't control ourselves within the NestJS organization).\r\n\r\n\r\n\r\n",
        "We can alternatively update this method to be optional; in this case we won't have to wait "
      ],
      "nest-proper-asynchronous-error-handling": [
        "If this condition `err instanceof KafkaRetriableException && !isPromiseResolved` is true then Promise will both reject and resolve (which technically doesn't make much sense)",
        "Although I like the idea of introducing a new method that can be overridden, it's a breaking change, so we'd have to wait to merge this PR until the next major release. Hence, for the time being, I'd instead suggest just wrapping this logic in try..catch blocks.",
        "Sounds good to me @wSedlacek! Thanks for your work on this one ðŸ™Œ ",
        "I don't think that we should call both `.write()` and `.emit('error')` on each error."
      ],
      "nest-use-secure-hash-algorithms": [
        "Let's do this (since it's faster)"
      ],
      "nest-descriptive-identifier-names": [
        "`should` word indicates here that this method should return a boolean (and shouldn't update anything itself) - it's a common convention (is/has/should)",
        "> Can we have shouldFlushLogsOnOverride: boolean, and flushLogsOnOverride(): void method instead?\r\n\r\nSounds great @micalevisk!",
        "nit: mergePacketOptions/mergeRequesOptions? (to not confuse it with the \"client\" instance options) cc @tuxmachine ",
        "this name is not very descriptive I guess. what about `activeShutdownSignals` or something like this?"
      ],
      "nest-explicit-default-configurations": [
        "Instead of adding two extra class properties, you could use helper `getOptionsProp()` method (see example below) which also takes a default value.",
        "could be potentially removed? (just use `undefined` when not explicitly defined)"
      ],
      "nest-pin-dependency-versions": [
        "These may quickly get out of date - we will keep supporting v16 even after it's no longer a maintenance version (after September this year). That's why versions were hardcoded before "
      ],
      "nest-use-factory-providers": [
        "With this change, you won't be able to use multiple multer modules in your project (due to equal hash tokens)."
      ],
      "nest-choose-meaningful-identifier-names": [
        "`should` word indicates here that this method should return a boolean (and shouldn't update anything itself) - it's a common convention (is/has/should)",
        "> Can we have shouldFlushLogsOnOverride: boolean, and flushLogsOnOverride(): void method instead?\r\n\r\nSounds great @micalevisk!",
        "Can we make argument names somewhat more descriptive? For example, `transportOrExtras, extras`, etc.",
        "Can we rename it to \"id\" instead?\r\n```suggestion\r\n  async findOne(@Param(\"id\") id: string): Promise<Cat> {\r\n```\r\nSimilarly in the method below and in the corresponding service class",
        "this name is not very descriptive I guess. what about `activeShutdownSignals` or something like this?"
      ],
      "nest-optimize-critical-path-iterations": [
        "Why not filter + map? Performance should be marginal in this case (I'd think)",
        "Still, the performance impact should be negligible. filter & map is just cleaner & easier to read, not worth the optimization in this particular case ",
        "Let's move this line (check) outside the callback function as otherwise it would be executed per each invocation of the route. This comment applies to other changes too ðŸ™Œ "
      ],
      "nest-follow-protocol-standards": [
        "Similarly to what Fastify did, should we expose the `forceCloseConnections` flag to control that behavior?",
        "Do we have any other ideas on how we could avoid introducing a breaking change? AFAIR `forceCloseConnections` is disabled by default (in Fastify) too.\r\n\r\nPerhaps we should expose a dedicated method at the adapter class level? So then you could do \r\n```typescript\r\nconst adapter = app.getHttpAdapter() as ExpressAdapter\r\nadapter.enableForceCloseConnections(); // adapter.forceCloseConnections = true; ?\r\n```\r\nOR maybe we should auto-enable it when someone calls `enableShutdownHooks()`?\r\n\r\n\r\n",
        "What makes this problematic is the fact that breaking change forces us to release a new major version (and so we'd have to postpone merging this PR a little) :( ",
        "SGTM âœ… ",
        "I'd say that we probably shouldn't introduce non-standard HTTP status codes. Better hardcode it inside a corresponding gRPC expection.",
        "Why do we presume that `TEMPORARY_REDIRECT` should be the default one?",
        "Make sense now :)"
      ],
      "nest-graph-based-dependency-management": [
        "This would introduce a major breaking change. Modules are supposed to be sorted by distance",
        "This change may lead to very tricky side-effects. Is there any way to somehow avoid this?",
        "What about circular dependencies? ðŸ˜„ ",
        "What if the single module is imported by multiple modules that will override `distance` value several times?"
      ],
      "nest-standardize-null-safety-patterns": [
        "Instead of adding a new condition here, we may just swap `||` expression with `??`.",
        "What do you think about using `isNil()` util function here instead? (to exclude `null` and `undefined` values)",
        "I think `isNil()` will be sufficient here :)"
      ],
      "nest-use-topological-sorting": [
        "This would introduce a major breaking change. Modules are supposed to be sorted by distance",
        "This change may lead to very tricky side-effects. Is there any way to somehow avoid this?",
        "What about circular dependencies? ðŸ˜„ ",
        "What if the single module is imported by multiple modules that will override `distance` value several times?"
      ]
    }
  },
  "konstin": {
    "repos": [
      "astral-sh/uv"
    ],
    "entries": [
      {
        "slug": "uv-avoid-unnecessary-constraints",
        "title": "Avoid unnecessary constraints"
      },
      {
        "slug": "uv-balance-test-performance-considerations",
        "title": "Balance test performance considerations"
      },
      {
        "slug": "uv-clear-precise-documentation",
        "title": "Clear precise documentation"
      },
      {
        "slug": "uv-declarative-constraints-over-runtime",
        "title": "Declarative constraints over runtime"
      },
      {
        "slug": "uv-enforce-strong-optional-types",
        "title": "Enforce strong optional types"
      },
      {
        "slug": "uv-environment-variable-best-practices",
        "title": "Environment variable best practices"
      },
      {
        "slug": "uv-follow-established-naming-conventions",
        "title": "Follow established naming conventions"
      },
      {
        "slug": "uv-make-errors-user-actionable",
        "title": "Make errors user actionable"
      },
      {
        "slug": "uv-mask-sensitive-tokens",
        "title": "Mask sensitive tokens"
      },
      {
        "slug": "uv-optimize-cache-sharing-strategies",
        "title": "Optimize cache sharing strategies"
      },
      {
        "slug": "uv-redact-url-credentials",
        "title": "Redact URL credentials"
      },
      {
        "slug": "uv-respect-connectivity-state",
        "title": "Respect connectivity state"
      },
      {
        "slug": "uv-secure-configuration-defaults",
        "title": "Secure configuration defaults"
      },
      {
        "slug": "uv-structure-for-readability",
        "title": "Structure for readability"
      },
      {
        "slug": "uv-test-deployment-edge-cases",
        "title": "Test deployment edge cases"
      },
      {
        "slug": "uv-use-direct-documentation-style",
        "title": "Use direct documentation style"
      }
    ],
    "comments": {
      "uv-declarative-constraints-over-runtime": [
        "Given that this is a clap bug, I don't think it's worth it introducing `Result`s in the whole resolve chain.\r\n\r\nThe exit code is the same that clap uses for parsing errors."
      ],
      "uv-use-direct-documentation-style": [
        "This sounds too complex for the guide documentation, I'd guide users towards using either a single `--bump` or a stable and an unstable bump and leave the rest to the concept/reference documentation."
      ],
      "uv-optimize-cache-sharing-strategies": [
        "CC @eifinger is this still a good example?",
        "Do you want to make a PR updating the docs? You know the intended configuration better than me."
      ],
      "uv-secure-configuration-defaults": [
        "We will add custom options, we need them for inclusions and exclusions, and from my experience in maturin and ruff we need to be able to evolve them."
      ],
      "uv-enforce-strong-optional-types": [
        "A missing `project` table should error, otherwise we risk creating an invalid `project` table with a `version` but no `name` (should be solved by `entry()` -> `get()`, too)"
      ],
      "uv-follow-established-naming-conventions": [
        "We should be using either `$HOME` or `~` throughout the document."
      ],
      "uv-environment-variable-best-practices": [
        "This should document what shape of URL is expected, is there something that GitHub considers as API root that users could exchange?"
      ],
      "uv-structure-for-readability": [
        "nit: assign this to a variable outside the tuple to make it easier to follow that this is a tuple; ideally, we'd also de-nest it a bit.",
        "It's used as the non-consuming conversion `is_extended_transient_error`, while the `From`s consume `Self`. Could be a deref or a `From` with `&Self` too, not sure what the most intuitive here is.",
        "oh, yes sure that's better!",
        "Can we DRY this up by only determining the action in the branches?"
      ],
      "uv-test-deployment-edge-cases": [
        "Can you add tests for the prefix and the target case? We have to emulate this by copying our python files and the built test binary manually, but it catches cases such as https://github.com/astral-sh/uv/pull/14184#discussion_r2160101023 and the user scheme preference.",
        "We need this to work on Windows, so we need to be lenient in the matching",
        "Should this fail with `--all`? Otherwise it sounds like a test could slip through if the env var is missing or wrong."
      ],
      "uv-avoid-unnecessary-constraints": [
        "```suggestion\r\nrequires-python = \">=3.11\"\r\n```"
      ],
      "uv-clear-precise-documentation": [
        "Should that be past tense?",
        "We should have consistent grammar between e.g. `The environment would be updated.` and `Create a new environment.` (indicative vs conditional) (unless I've missed something and we are always creating but the update is not done in a dry run?)",
        "```suggestion\n    /// Represents a lockfile and whether it needs to be created or update.\n```"
      ],
      "uv-redact-url-credentials": [
        "I would make this more generic, since we're also displaying URLs in error messages and serializing it to files, e.g.:\n\n```suggestion\n/// A [`Url`] wrapper that redacts credentials when displaying the URL.\n```",
        "I find it confusing that `LogSafeUrl` here is both working a URL that has credentials, but redacts them, and as a URL that had its credentials removed.",
        "This is something that's already in the existing code: Sometimes `Url` refers to a URL with credentials, and sometimes to the same underlying \"data\" without credentials. What about returning a `impl Display` instead of a real type from `without_credentials` to make it clearer that this is an output-only method?",
        "Do we still need this?\n\nThis question applies to all non-test `remove_credentials` calls: Do we still need explicit redaction, or does automatic redaction handle everything we need now?",
        "I'm a bit torn here, on one side seeing `***` here could be confusing cause those credentials are evidently the wrong ones, otoh I'd really want to know that there were credentials on the URL here."
      ],
      "uv-balance-test-performance-considerations": [
        "Can you merge this and the previous test? Usually I'm all for small tests, but Python downloads and installs are slow so reuse helps the test speed",
        "Does this snapshot break when someone publishes `cp311` to PyPI?"
      ],
      "uv-respect-connectivity-state": [
        "To match the Remote Git fetches:\r\n\r\n```suggestion\r\n                    \"{}{} Self-update is not possible because network connectivity is disabled (i.e., with `--offline`)\"\r\n```"
      ],
      "uv-mask-sensitive-tokens": [
        "Can you mask the token with `::add-mask::`, and also the GCP token?"
      ],
      "uv-make-errors-user-actionable": [
        "The error should by itself contain enough information to track down the problem, so I would like it to at least contain the versions.",
        "thiserror at least doesn't support it, I think that's a limitation because every `.source()` needs a type and we'd need two type for the `LibcDetectionError` error message and the variant error message, but we only have one type.",
        "I'd push the libc part down to the most detailed error, I don't expect Python devs to know about libc flavors."
      ]
    }
  },
  "milas": {
    "repos": [
      "docker/compose"
    ],
    "entries": [
      {
        "slug": "compose-configuration-documentation-clarity",
        "title": "Configuration documentation clarity"
      },
      {
        "slug": "compose-environment-variable-validation",
        "title": "Environment variable validation"
      },
      {
        "slug": "compose-evaluate-dependency-api-compatibility",
        "title": "evaluate dependency API compatibility"
      },
      {
        "slug": "compose-follow-existing-naming-patterns",
        "title": "Follow existing naming patterns"
      },
      {
        "slug": "compose-keep-code-structure-flat",
        "title": "Keep code structure flat"
      },
      {
        "slug": "compose-maintain-documentation-consistency",
        "title": "Maintain documentation consistency"
      },
      {
        "slug": "compose-network-api-precision",
        "title": "Network API precision"
      },
      {
        "slug": "compose-pin-build-dependency-versions",
        "title": "Pin build dependency versions"
      },
      {
        "slug": "compose-prevent-unintended-ci-behaviors",
        "title": "Prevent unintended CI behaviors"
      },
      {
        "slug": "compose-scope-concurrency-control-precisely",
        "title": "Scope concurrency control precisely"
      },
      {
        "slug": "compose-use-api-options-pattern",
        "title": "Use API options pattern"
      },
      {
        "slug": "compose-validate-before-value-use",
        "title": "Validate before value use"
      },
      {
        "slug": "compose-wrap-and-check-errors",
        "title": "Wrap and check errors"
      }
    ],
    "comments": {
      "compose-keep-code-structure-flat": [
        "Nit: it'd be a bit clearer if it was something like\r\n\r\n```go\r\nif len(buildOptions.Platforms) == 0 {\r\n   // docker load\r\n} else { \r\n  // image out\r\n}\r\n```\r\n\r\nMostly to make it clear that they're mutually exclusive / an intentional override."
      ],
      "compose-validate-before-value-use": [
        "Changed! I didn't take the GitHub suggestion as-is because I renamed the func to be `valueOrDefault` since it's âœ¨ generic âœ¨ now"
      ],
      "compose-follow-existing-naming-patterns": [
        "For consistency with `compose logs` & `docker logs`, looks like this should be `timestamps` (plural)\r\n\r\nhttps://github.com/docker/cli/blob/990674901b4fdd0bd2d3f91aa84ad3efe225f6fb/cli/command/container/logs.go#L49\r\n\r\nhttps://github.com/docker/compose/blob/0368f190303b28e142f983257171a6f554749313/cmd/compose/logs.go#L60",
        "@glours Should we make this `-` to align with other resources (compose-spec/compose-go#294)?",
        "Oh, I see this is actually not a change in this PR, as this logic existed perviously as `getImageName` and is just exported now, so don't need to address before merging this even if we want to change",
        "I'd propose renaming this to `GetImageNameOrDefault` since it reads `service.Image` if populated and then falls back to producing one. Particularly now that it's exported ðŸ˜‰ ",
        "Perhaps rename `classicBuildOptions()`",
        "Nit: `s/dockerCli/streams`",
        "Nit (not new to your PR ðŸ˜… ): can we rename these? They're very confusing right now.\r\n\r\nI'd recommend:\r\n * `env` -> `cmdEnv` (would match `cmdLineVar` that you added)\r\n * `projectEnv` -> `serviceOverrideEnv`  (it's 100% NOT the \"project\" env which makes this very confusing)"
      ],
      "compose-wrap-and-check-errors": [
        "It seems better to let the error propagate here and fail the command. Since there's nothing like a project lock (which causes problems as your comment points out ðŸ™ˆ), if state is changing underneath us, we should just fail IMO.\r\n\r\nAlternatively, I think we should at least not have `ensureNetwork` call itself recursively -- `409 Conflict` is unfortunately pretty generic (even in Moby land), so I'm sure there's some possibility of getting this into an infinite loop. I think returning the error and having something like `ensureNetworkWithRetries(name, retries)` that calls `ensureNetwork(name)` would be safer/easier to reason about",
        "lol you beat me by 3 mins!"
      ],
      "compose-pin-build-dependency-versions": [
        "```suggestion\r\nARG GO_VERSION=1.21.0\r\n```\r\n\r\nWe've generally been pinning to the patch as well, right? \r\n\r\nThat said...I'm fine with using the rolling tag (`1.21`) if you want. Despite the `Host` issues recently, Go patch upgrades are generally very safe IMO, so up to you!"
      ],
      "compose-environment-variable-validation": [
        "I think we should put this behind a flag for now, e.g. `COMPOSE_EXPERIMENTAL_INCLUDE_REMOTE=1`.\r\n\r\nIt's not part of the spec at the moment, so I don't think it should be enabled by default until we've done that and ironed out ambiguities, etc.",
        "```suggestion\r\n\tif cacheHome := os.GetEnv(\"XDG_CACHE_HOME\"); cacheHome != \"\" {\r\n\t\tbase = cacheHome\r\n```\r\nUnlikely, but if it's set and empty, we should still fallback to `~/.cache`",
        "See compose-spec/compose-go#446 - it was reading `COMPOSE_PROFILES` in the load, it's now done via this option up-front for consistency & correct layering",
        "Nit: Given the complexity/subtlety, I think a comment here is warranted, e.g. `// overwrite the process env with merged OS + env file results` or something\r\n\r\nFor context: I thought this was a regression at first since OS/shell has higher precedence than env file, but then I realized that `GetEnvFromFile` takes in the OS/shell env and handles the precedence/merging, which is why it's safe/necessary to overwrite everything here"
      ],
      "compose-maintain-documentation-consistency": [
        "```suggestion\r\nPull requests must be cleanly rebased on top of the base branch without multiple branches\r\n```\r\nIn this context, `master` is a branch name. However, the primary branch is actually `v2` now, so I think this can be slightly more generic.",
        "```suggestion\r\n4. Include code comments. Tell us the why, the history and the context.\r\n```\r\nI realize it's unusual for \"comment\" to be used as a verb, but \"comment on the code\" implies e.g. using GitHub rather than inline code comments"
      ],
      "compose-scope-concurrency-control-precisely": [
        "Since we don't know if the timer has fired here, we need to stop it/drain first:\r\n```go\r\nif !timer.Stop() {\r\n\t<-timer.C\r\n}\r\ntimer.Reset(interval)\r\n```\r\n\r\nhttps://pkg.go.dev/time#Timer.Reset"
      ],
      "compose-prevent-unintended-ci-behaviors": [
        "Shouldn't this be `type: \"docker\"` + `\"load\": \"true\"` like the other one?\r\n\r\nI believe this is legitimately trying to push to registry:\r\n```shell\r\n$ docker compose build hello\r\n...\r\n => ERROR exporting to image                                                                                                                           0.7s\r\n => => exporting layers                                                                                                                                0.1s\r\n => => exporting manifest sha256:90bf7923cd97b6dec8f53161c7df1e6d2587ec52ccc060273cdc6b6ae8bd6c9c                                                      0.0s\r\n => => exporting config sha256:ec0a66e1fe3ca40d1042b63f3f82a0ff10189f86c6b68c1dd2b6f3213ac7057a                                                        0.0s\r\n => => exporting manifest sha256:8a1a9bea74c44f39ab83dd4a831d74845c1f4eda2c3f2a5c230e0988fa554236                                                      0.0s\r\n => => exporting config sha256:296e0f515945635ccc81e78c428bdf2b341a725f8ad033afbff0db2d0615aec1                                                        0.0s\r\n => => exporting manifest list sha256:87e79b4bda828912aba9e22edd32010ab65b56da406c307bf60b61e79616b8cb                                                 0.0s\r\n => => pushing layers                                                                                                                                  0.5s\r\n------\r\n > exporting to image:\r\n------\r\nfailed to solve: server message: insufficient_scope: authorization failed\r\n```",
        "I just realized it's doing this because you can't export multi-platform images with the `docker` export type\r\n\r\nI wonder if we need a `--output` flag on `build` with a default of `type=docker` that behaves the same as the buildx flag, so it can error out if you try to do `docker compose build --output=docker` on a multi-platform image.",
        "And TIL that `docker compose push` already exists ðŸ¤” \r\n\r\nBut of course, it does not build:\r\n```shell\r\n$ docker compose push\r\nAn image does not exist locally with the tag: milas/hello\r\n```"
      ],
      "compose-network-api-precision": [
        "This is unnecessary - `NetworkList` with a name filter works reliably, it's `NetworkInspect` that does prefix matching"
      ],
      "compose-evaluate-dependency-api-compatibility": [
        "See #10954 - will need to swap things to `github.com/distribution/reference` to keep this from reappearing.",
        "https://github.com/tilt-dev/fsnotify#notable-changes\r\n\r\nThat's not _quite_ accurate anymore, since, as you noted, `fsnotify/fsnotify` has had more activity recently.\r\n\r\nIn fact, I believe all the important changes have been upstreamed:\r\n* Ignore file attribute change events on Windows: `fsnotify#520`\r\n* Configurable Windows buffer size: `fsnotify#521`\r\n* Recursive Windows watcher support: `fsnotify#540`\r\n\r\nHowever, they're all on `main` still - no release of `fsnotify/fsnotify` contains them yet. Also, not a _huge_ deal, but there's some API differences in the upstreamed implementations, most notably in the format for Windows recursive paths.\r\n\r\n---\r\n\r\nAll that said, I feel good about using upstream and pinning to a Git hash for now; that'll certainly make any future improvements more straightforward."
      ],
      "compose-use-api-options-pattern": [
        "Minor: we often end up refactoring the API methods to add an options object later...maybe worth having `PublishOptions` with a `Repository string` field upfront",
        "e.g. I'm guessing this will need `Quiet bool` at some point"
      ],
      "compose-configuration-documentation-clarity": [
        "```suggestion\r\n    Setting the `COMPOSE_MENU` environment variable to `false` disables the helper menu when running `docker compose up`\r\n    in attached mode. Alternatively, you can also run `docker compose up --menu=false` to disable the helper menu.\r\n```",
        "```suggestion\r\n    Many Compose subcommands can be run without a Compose file by passing\r\n    the project name.\r\n```",
        "```suggestion\r\n    `COMPOSE_PROJECT_NAME` environment variable does the same as the `-p` flag,\r\n    and `COMPOSE_PROFILES` environment variable is equivalent to the `--profiles` flag.\r\n```",
        "```suggestion\r\n    If a flag is set via the command line, the associated environment variable is ignored.\r\n```"
      ]
    }
  },
  "tannergooding": {
    "repos": [
      "dotnet/runtime"
    ],
    "entries": [
      {
        "slug": "runtime-abstract-traversal-patterns",
        "title": "Abstract traversal patterns"
      },
      {
        "slug": "runtime-avoid-busy-waiting",
        "title": "Avoid busy waiting"
      },
      {
        "slug": "runtime-centralize-platform-configurations",
        "title": "Centralize platform configurations"
      },
      {
        "slug": "runtime-choose-appropriate-error-mechanisms",
        "title": "Choose appropriate error mechanisms"
      },
      {
        "slug": "runtime-choose-descriptive-names",
        "title": "Choose descriptive names"
      },
      {
        "slug": "runtime-decompose-complex-algorithms",
        "title": "Decompose complex algorithms"
      },
      {
        "slug": "runtime-document-configuration-intent",
        "title": "Document configuration intent"
      },
      {
        "slug": "runtime-document-function-contracts",
        "title": "Document function contracts"
      },
      {
        "slug": "runtime-enable-configurable-instrumentation",
        "title": "Enable configurable instrumentation"
      },
      {
        "slug": "runtime-honor-api-contracts",
        "title": "Honor API contracts"
      },
      {
        "slug": "runtime-maintain-configuration-compatibility",
        "title": "Maintain configuration compatibility"
      },
      {
        "slug": "runtime-maintain-consistent-formatting",
        "title": "Maintain consistent formatting"
      },
      {
        "slug": "runtime-maintainable-test-structure",
        "title": "Maintainable test structure"
      },
      {
        "slug": "runtime-model-actual-hardware-costs",
        "title": "Model actual hardware costs"
      },
      {
        "slug": "runtime-optimize-aligned-simd-operations",
        "title": "Optimize aligned SIMD operations"
      },
      {
        "slug": "runtime-optimize-build-dependency-chains",
        "title": "Optimize build dependency chains"
      },
      {
        "slug": "runtime-optimize-common-paths",
        "title": "Optimize common paths"
      },
      {
        "slug": "runtime-optimize-for-readability",
        "title": "Optimize for readability"
      },
      {
        "slug": "runtime-optimize-memory-access",
        "title": "Optimize memory access"
      },
      {
        "slug": "runtime-platform-agnostic-network-apis",
        "title": "Platform-agnostic network APIs"
      },
      {
        "slug": "runtime-platform-aware-algorithm-optimization",
        "title": "Platform-aware algorithm optimization"
      },
      {
        "slug": "runtime-simplify-code-expressions",
        "title": "Simplify code expressions"
      },
      {
        "slug": "runtime-specific-exceptions-with-context",
        "title": "Specific exceptions with context"
      }
    ],
    "comments": {
      "runtime-simplify-code-expressions": [
        "I think we could collapse `IsAVXVNNIInstruction(ins) || IsAVXVNNIINT8Instruction(ins) || IsAVXVNNIINT16Instruction(ins)` down into some `IsAvxVnniFamilyInstruction(ins)` given the places that are checking them"
      ],
      "runtime-optimize-build-dependency-chains": [
        "This is the actual interesting \"bit\" of the change.\r\n\r\nEach high level folder (Arm, General, X86) has a targets file that:\r\n1. Has a dependency on the relevant test generator to ensure it is built first\r\n2. A target that runs the test generator to ensure the necessary files exist, with relevant up to date checks to ensure it doesn't run unnecessarily\r\n3. A target that has to always run as part of compilation to read the list of generated tests and include them in the list of files csc should process",
        "The actual `GenerateTests.csx` was simply moved to be part of a csproj and to take a couple inputs to help filter the tests.\r\n\r\nIt's not necessarily the cleanest change, but it is by far one of the simplest. Building a source generator or otherwise refactoring things more would result in significantly more time and churn and would block the AVX-512 work until that was completed."
      ],
      "runtime-abstract-traversal-patterns": [
        "This was the cleanest/easiest way I could think of to do this.\r\n\r\nThe general idea is that we have several transforms we want to make to LIR before containment happens (because containment complicates these transforms). However, since we're in LIR form for these nodes at this point, we need to make sure the transform is safe to do.\r\n\r\nAn alternative would be to add a `pre` pass for lowering then do a separate `post` pass for containment, but that feels like a bigger/bulkier/riskier change.\r\n\r\nThere are some other transforms that would be nice to move \"here\" longer term, like the sequential `insertps` operation folding we do; or the recognizing of `AND(x, NOT(y))` we do; neither of which we want to do in HIR because it breaks or massively complicates other optimizations (like folding and operation negation) that we do.\r\n\r\nI'd be happy to make this a separate pass for .NET 11, if we feel that is better. But for .NET 10 I think this is a less risky approach."
      ],
      "runtime-maintain-configuration-compatibility": [
        "@jkotas are we planning any other R2R breaking changes this release? If so, it would be nice to go ahead and clean up (potentially just in a follow up PR) some of the R2R values; otherwise, I can log a tracking issue for it.\r\n\r\nThe two big things are that we have some unused gaps now that could be filled in and I don't see why we need to use unique IDs across X86 and Arm64 (that is `X86Base` and `ArmBase` can't currently both be R2R ID 1).\r\n\r\nFor the latter point, the general intent of the R2R flags is to track when user code has some `if (Isa.IsSupported) { }` check that isn't taken, because we have to assume it may cause different behavior. However, on x64 some `AdvSimd.IsSupported` check can never be true, so it can't ever cause a difference in behavior and so we shouldn't need to track it at all and so the x64 vs Arm64 R2R IDs should be able to overlap, with them differentiated by a general \"target architecture\" flag instead. \r\n\r\nI believe we should be able to do much like we do in NAOT where we simply track 1 bit per ISA on the relevant architecture. In other words, I expect we could essentially simplify this to almost the same as https://github.com/dotnet/runtime/pull/115983/files#diff-37e89a49ea22a08b2d0502ca3b1716e2dd1bd68cafd3eeb3cd45f5a4406601d3R219, which would also let us avoid needing to track \"specifiable\" or not and needing unique bit entries for cases like `gfni_v512` (which is tracked as `gfni + avx512` for NAOT). We should just need to track the baseline ISA in addition to what NAOT tracks, since that can be disabled for testing user fallbacks.",
        "I'll revert for now. However, ...\r\n\r\n> This would make r2rdump more complicated. Right now, r2rdump just does Enum.ToString() to dump the instruction set.\r\n\r\nI think it would be better to make it slightly more complex by taking this break long term and adding a compat handler to r2rdump to handle older image versions.\r\n\r\nRight now we're using 60 bits for R2R, when we should only need 34 total. If we split it by platform then it's 11 for Arm64 and 23 for x64. Just given the pending work around SVE and AVX, we will likely go over 64-bits in the next release and need to take a break anyways.\r\n\r\nI think the way NAOT has this setup is more flexible and was done because it came online later and was able to handle some of the scenarios that R2R didn't have the foresight to consider.\r\n\r\n> NAOT does not have the versioning problem like R2R. There is no mixing and matching of versions, etc.\r\n\r\nRight, but I think it's also something we can handle for R2R to make things overall cleaner moving forward. Not something we have to do now, but I expect we'll be forced to deal with it in the .NET 11 timeframe\r\n\r\n",
        "Reverted to preserve the compat for .NET 10"
      ],
      "runtime-choose-appropriate-error-mechanisms": [
        "We shouldn't assert unreached here. There are always scenarios where unused values get preserved, such as min-opts, and so we should prefer the typical pattern of `if (foundUse) { use.ReplaceWith(node); } else { node->SetUnusedValue(); }`",
        "Previously the JIT would fail fast with `The JIT compiler encountered invalid IL code or an internal limitation.`\r\n\r\nNow, it will assert in debug mode but will `throw PlatformNotSupportedException` at runtime.",
        "~This was changed to not use `impUnsupportedHWIntrinsic` and instead use `gtNewMustThrowException` directly.~\r\n\r\n`impUnsupportedHWIntrinsic` was renamed to `impUnsupportedNamedIntrinsic` and moved out of `FEATURE_HW_INTRINSIC`"
      ],
      "runtime-decompose-complex-algorithms": [
        "@kg, I don't see any obvious existing handlers for what is effectively \"bitcast\" (that is the input is returned directly with no change, the API only exists to satisfy the type system).\r\n\r\nDo you have a pointer to any similar handling that might exist?",
        "I ended up following what `op_UnaryNegation` does and adding handlers that basically do memcpy to handle it.",
        "This file ended up needing a bit of a refactoring as there were some assumptions in place that don't hold when supporting additional intrinsics.\r\n\r\nIn particular, there are various intrinsics where:\r\n* one of the SIMD types may not be generic at all (`Vector2`, `Vector3`, `Vector4`)\r\n* multiple generic types exist (`As<TFrom, TTo>`)\r\n* the return type may not be a 128-bit vector (`AsVector2`, `AsVector3`)\r\n\r\nSo, what I did here was I broke this `get_common_simd_info` method into two:\r\n* `get_common_simd_info`\r\n* `get_common_simd_scalar_arg`\r\n\r\nThe former now always gets the size of the input klass and secondly determines if it is a SIMD type and what the underlying element type is if so. While the latter identifies the first non-SIMD argument, if one exists.\r\n\r\n",
        "To support methods which return a value type, but where that isn't a 128-bit vector, this explicitly gets the return type from the signature to ensure there can be no accidents",
        "In here, we consistently query the relevant simd information of the return type and if it exists the simd information of the  first parameter (this is enough to correctly handle all the cases that currently exist).\r\n\r\nThere's actually quite a bit of logic in this function that *could* be moved down into `emit_common_simd_operations`, as APIs like `AndNot` exist for `Vector128<T>` and `Vector<T>`, they may also exist for types like `Vector4` in the future. I opted to not move that down in this PR, to try and keep the total churn under control.\r\n\r\nBut, I did add some basic validation that the encountered signatures are roughly as expected to help ensure we don't hit issues in the future as new overloads are introduced or the general SIMD support in Mono is expanded."
      ],
      "runtime-document-function-contracts": [
        "Fixed."
      ],
      "runtime-optimize-aligned-simd-operations": [
        "I have it erroring if `posix_memalign` isn't available as a fallback. `posix_memalign` is a very old (`_POSIX_C_SOURCE >= 200112L || _XOPEN_SOURCE >= 600`) function and should be always available.\r\n\r\nLikewise, I don't believe trying to polyfill via `malloc`/`free` is reliable as there is no guarantee that `free` works with arbitrary pointers:\r\n> The behavior is undefined if the value of ptr does not equal a value returned earlier by malloc(), calloc(), realloc(), or aligned_alloc() (since C11).\r\n\r\nLikewise, there is no reliable way to backtrack from a given aligned `ptr` to the actual `ptr` that `malloc` did return.",
        "I'd like to see if any platforms don't actually have `posix_memalign` before falling back to storing the actual pointer before the returned pointer."
      ],
      "runtime-maintainable-test-structure": [
        "Were these intentionally added? They look to have dead code and aren't actually testing the APIs in question.\r\n\r\nWe have other tests which cover the CPUID checks"
      ],
      "runtime-optimize-for-readability": [
        "The xplat helper intrinsics support operators and so we can make this \"more readable\" by just using `x & y`.",
        "Believe so, the fixer automatically applied them.",
        "```suggestion\r\n            if (dimension is 0 or -1)\r\n```"
      ],
      "runtime-optimize-common-paths": [
        "Instructions like `pextrw` which extract to a general purpose register or like `pinsrw` which insert from a general-purpose register don't take masks.",
        "Instructions like `pmaddwd` which are `INS_TT_FULL_MEM` don't have the `Input_*Bit` flag since they cannot support embedded broadcast and only ever take the full simd size.",
        "Checks throughout the JIT have been ordered under the presumption that `varTypeUsesIntReg` is the most likely occurrence. Where mask register handling is needed, `varTypeUsesMaskReg` is the middle case since we only need 1 additional check to cover all 3 cases. This allow the last case to `assert`.\r\n\r\nThis keeps 1-check for non-xarch, minimizes the total ifdefs when mask handling is needed, and ensures we need no more than 2 checks on xarch with the common case being 1 check. Ideally this helps keep it \"pay for play\".\r\n\r\nHaving `varTypeUsesIntReg` allowed a few other checks to become simplified/improved in general.",
        "I think it'd be better to use the `AuxiliaryJitType` and mark these as `SpecialCodeGen` than to add a bunch of extra table entries. More similar to how `Gather/Scatter` get handled.\r\n\r\nIf we were to do the extra table entries, I'd rather they be moved to the bottom of the file with the other extra intrinsics that don't directly map to managed API names. That way the lookups for managed APIs remain cheaper and clearer.",
        "That's the purpose of `AuxiliaryJitType`, to give a secondary type to use.\r\n\r\n`SimdBaseJitType` is the primary type and for most intrinsics is the sole type needed to determine which instruction to use. Some special intrinsics need a secondary type and that's the purpose of `AuxiliaryJitType` so that we have a way to track it where required.",
        "Most of these ISAs aren't meaningful to test anymore, especially since we aren't doing bringup of the ISAs.\r\n\r\nFor the most part, we just care about the unique ISA paths used in corelib or the JIT, which is primarily the SSE and AVX families that are already covering cases like FMA or BMI being disabled.\r\n\r\nI think we can even get it so that we don't need to test `nossse3, nosse41, and nosse42` as well; but we'd need to do a bit of cleanup in the JIT to achieve that.\r\n\r\n-- I'd like to get our support matrix down to effectively:\r\n* x86-64-v1: This is the baseline and is currently targeted by NAOT\r\n* x86-64-v2: This is everything up through SSE4.2+POPCNT\r\n* x86-64-v2 + AVX: This tests the VEX encoding and is known to have a decent user share\r\n* x86-64-v3: This is AVX2+FMA+BMI1+BMI2\r\n* x86-64-v4: This is AVX512 and covers the EVEX encoding\r\n\r\nFor x86-64-v2:\r\n* this is what is supported by the x64 on Arm emulation provided by Apple and Windows\r\n* it would be nice if NAOT could target this as the default, as its an 18 year old baseline\r\n* this is the target required by Win11 24H2 and boot is blocked if the ISAs aren't available\r\n* Win11 prior to 24H2 is documented as requiring SSE4.1 otherwise",
        "> I do not think we want to have the baseline supported piecemeal (it is unlikely we would be able to it correctly since it is impossible to test). If we want to raise the baseline, we should do it for the whole product. I would not be opposed to raising the product baseline to x86-64-v2.\r\n\r\nðŸ‘. I think it would provide a nice simplification in the JIT and libraries for what code paths we need to support. I don't want to raise the bar \"too high\" and negatively impact a significant number of customers, but I do think that `x86-64-v2` and `armv8.1-a` are reasonable baselines at this point.\r\n\r\n> Where is it documented?\r\n\r\nIn https://learn.microsoft.com/en-us/windows-hardware/design/minimum/minimum-hardware-requirements-overview, specifically under the Windows 11 document (last updated 2021)\r\n\r\n![image](https://github.com/user-attachments/assets/ad370ba3-9e00-49fc-8683-43d08338289a)\r\n\r\nWe got the notification that SSE4.2+POPCNT in 24H2+ when we had last reached out about Arm RDM support; although there's also been some news about the boot blocker that exists that went around last year as well.\r\n\r\nAzure, AWS, Google Cloud, and other major cloud providers that specify what hardware they provide are entirely on x86-64-v3 or later. There are also other 3rd party numbers out there, such as the Steam Hardware Survey, which shows 99.78% of reporting users have x86-64-v2 (while 97.31% have AVX and 94.66% have x86-64-v3).\r\n\r\n> When did Intel/Amd stop selling the last processor without x86-64-v2? It is the more interesting date for this discussion.\r\n\r\nFor Intel the last CPUs pre x86-64-v2 were discontinued in 2013, around the time x86-64-v3 was launched. This would have been the Bonnell microarchitecture (part of the Intel Atom lineup for low end machines). The Conroe/Merom and Penryn/Wolfdale chipsets had been discontinued in 2011-2012. AMD discontinued their 10h and Bobcat lineups in 2012-2013 as well.\r\n\r\nSo if we did try to raise the baseline in .NET 11, it should only be for 12-18 year old computers which are no longer supported by the CPU manufacturers, by MacOS, or by current Windows.\r\n"
      ],
      "runtime-centralize-platform-configurations": [
        "> /Applications/Xcode_12.4.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX11.1.sdk/usr/include/malloc/_malloc.h:50:10: note: 'aligned_alloc' has been marked as being introduced in macOS 10.15 here, but the deployment target is macOS 10.13.0\r\n\r\nand then the mono failure for C99 seem potentially problematic\r\n\r\n`posix_memalign` is the \"next best\" thing but has a restriction\r\n> The address of the allocated memory will be a multiple of alignment, which must be a power of two and a multiple of sizeof(void *).\r\n\r\n`memalign` might also be available, but is considered \"obsolete\" in newer versions and is not guaranteed to work with `free` (and there is no explicit counterpart for it)\r\n\r\nhttps://www.man7.org/linux/man-pages/man3/posix_memalign.3.html",
        "We build on 10.15 saying we target 10.13 and the headers have annotations saying \"this API is only available on 10.xx+\" so using them raises a warning.\r\n\r\nSimilarly to say building for 10.0.10240 (Threshold 1) on a machine running 10.0.19043 (21H1).",
        "There is probably some other CMake magic or define checks that could be done here, but I think that can be tracked via a separate issue for follow up.\r\n\r\nThis impacts a range of Apple products (iOS, M1, OSX, etc) and may not be trivial to do correctly on all platforms.",
        "https://github.com/dotnet/runtime/issues/54296",
        "Fixed."
      ],
      "runtime-document-configuration-intent": [
        "They've been discussed in various blogs and are fairly well known in the space as the way to test fallback paths if your hardware supports the latest.\r\n\r\nI don't think it'd be the end of the world to drop them and the devs primarily using these switches could update, but didn't want to do so without discussion.\r\n\r\nWould the `--instruction-set` flags for R2R/NAOT be in the same boat? If we could remove the extras there it would also simplify things. I don't believe the flag is considered officially supported today, but does print out on the command line.",
        "Removed the config knobs and instruction-set switches"
      ],
      "runtime-avoid-busy-waiting": [
        "I know this is existing, but this is a bug/memory safety violation.\r\n\r\nConsider the case where `location` refers to actually immutable memory. In such a case, `Volatile.Read` should succeed, but in actuality, `CompareExchange` will end up writing `0` in the case the value is actually `0`, which will cause an AV.",
        "We can't for double, but we could remove the unsafe code here for `ulong` by following the same pattern that uses `Interlocked.CompareExchange` and `Interlocked.Exchange` under the ifdef."
      ],
      "runtime-platform-aware-algorithm-optimization": [
        "> to always return false\r\n\r\n`AvxVnni*.X64.IsSupported` should return false on a 32-bit machine and on a 64-bit machine it should return the same result as `AvxVnni*.IsSupported`",
        "Is this implication going to exclude the CPUID for `AVX10_VNNI_INT`?\r\n\r\nIt wasn't clear if that would only be set for `AVX10.2` or if it was allowed to be separate"
      ],
      "runtime-maintain-consistent-formatting": [
        "nit: These should be moved up near the other `AVXVNNI` instructions so they're part of the `AVX` grouping rather than strictly the `AVX512` grouping checks.",
        "```suggestion\r\n    <GenAPITargetPath>$([MSBuild]::NormalizePath('$(MSBuildProjectDirectory)', '..', 'ref', '$(AssemblyName).netcore.cs'))</GenAPITargetPath>\r\n    <!-- SA1001: Commas should not be preceded by a whitespace; needed due to ifdef -->\r\n    <NoWarn>$(NoWarn);SA1001</NoWarn>\r\n```"
      ],
      "runtime-platform-agnostic-network-apis": [
        "Unfortunately, it isn't this simple. Stuff on Unix has to go through the PAL layer so I need effectively something like the following in PAL instead:\r\n```cpp\r\n#if defined(__APPLE__)\r\n#define sincos __sincos\r\n#endif\r\n```"
      ],
      "runtime-honor-api-contracts": [
        "Sign is supposed to throw for NaN (unlike CopySign which does not). It's a historical behavior/contract of the API."
      ],
      "runtime-choose-descriptive-names": [
        "I've renamed this in `IsRedundantMov` but not everywhere else (as `/* canSkip */ value` is used for almost every call to `emitIns_Mov`).",
        "This is in a lambda which implicitly captures the `insName` variable"
      ],
      "runtime-enable-configurable-instrumentation": [
        "```suggestion\r\nRETAIL_CONFIG_DWORD_INFO(EXTERNAL_EnableAVXVNNIINT8,            W(\"EnableAVXVNNIINT8\"),         1, \"Allows AVXVNNI8+ hardware intrinsics to be disabled\")\r\nRETAIL_CONFIG_DWORD_INFO(EXTERNAL_EnableAVXVNNIINT16,           W(\"EnableAVXVNNIINT16\"),        1, \"Allows AVXVNNI16+ hardware intrinsics to be disabled\")\r\n```"
      ],
      "runtime-specific-exceptions-with-context": [
        "We weren't entirely consistent about this throughout all of our APIs. Several of them validated `s` first and several others validated `style` first.\r\n\r\nI opted for making it consistent as `s` first since that is validating the arguments \"in order\"."
      ],
      "runtime-optimize-memory-access": [
        "This is working around the lack of variable sized `ExtractMostSignificantBits`.\r\n\r\nIt's not pretty, but it works and ideally gets replaced in .NET 8 as part of the `VectorMask` work...",
        "We now expose helper intrinsics that directly operate on `ref`: `LoadUnsafe(ref T source, nuint elementOffset)`.\r\n\r\nThis helps avoid pinning, which can have measurable overhead for small counts and which can hinder the GC in the case of long inputs.\r\n\r\nIt likewise helps improve readability over the pattern we are already utilizing in parts of the BCL where we were using `Unsafe.ReadUnaligned` + `Unsafe.Add` + `Unsafe.As`.\r\n\r\n",
        "`ExtractMostSignificantBits` behaves just like `MoveMask` on x86/x64. This is also exposed by `WASM` as `bitmask`",
        "I've preserved the `Vector256` path given that it was already here and I would presume has undergone the necessary checks to ensure it is worth doing on x86/x64.\r\n\r\nArm64 doesn't support `V256` and so will only go down the `V128` codepath.",
        "This only validates the alignment is a power of 2 as that is relatively cheap. Also validating that `byteCount % alignment == 0` would be fairly expensive.",
        "I've simplified the alignment check to just `if ((alignment % (uint)sizeof(void*)) == 0)`, which will generate simply `test dl, 7` (or `test dl, 3` for 32-bit)",
        "I'll also add in a check for `byteCount` since that can just be `(byteCount & (alignment - 1)) == 0`",
        "Yes, it does. Thanks for the catch.\r\n\r\nWill update and add a test covering this scenario.",
        "Should be resolved now.",
        "`BinaryPrimitives.ReadInt64LittleEndian` is likely \"better\" than `BitConverter`, it's at least more explicit. `MemoryMarshal.Read` is likely something we want to avoid in \"safe\" code because of where it lives (even if it does bounds check/etc).\r\n\r\nToday, we basically have to pick between being \"safe\" or being \"efficient\". We can get semi-efficient for scalars, but that's potentially leaving 2-64x perf on the table, depending on platform/scenario.\r\n\r\nIf the JIT could elide bounds checks for `V128.Create(rospan)` then we have a safe way to operate on the data. We can alternatively centralize the unsafe code behind a core helper, for example we have `InvokeSpanIntoSpan` and a pattern that makes use of \"functional interfaces + generics\" to do this for `TensorPrimitives`. This at least reduces risk as the logic that accesses memory can be well tested and not duplicated everywhere. We'll never be able to get rid of *all* unsafe code either, something like `TensorPrimitives` which needs to handle large data needs to be able to pin so it gets access to non-temporal loads/stores, for example.\r\n\r\nI think our goal should therefore really be to move paths like these to use centralized helpers instead of trying to rewrite them to be \"safe\". We know they're important to perf and we know they benefit from unsafe today, so lets pull some of the `InvokeSpanIntoSpan` and `Aggregate` helpers so that cases like this are functionally rewritten to `=> Aggregate<T, IdentityOperator<T>, Crc32Operator<T>>(x)` and all the \"dangerous\" code is centralized. When the JIT finally gets support for eliding bounds checks over `V128.Create(rospan)` that one helper can be rewritten to use the safe pattern and the rest of the BCL implicitly gets it. We can add a number of `Debug.Assert`, memory access checks, and even potentially `opt-in` forced validation path (such as would lightup via a feature switch) to help ensure robustness in the interim.",
        "> I get that this pattern is centralizing the unsafe code, but I do not think that it reads well.\r\n\r\nIt doesn't today, but the long term goal would be to get something like `functional interfaces` into the language so that you can rather have something like `=> Aggregate(span, (previous, value) => Crc32(previous, value))` instead and the lambda binds to the `TOperation` given `where TOperation : IOperation<....>` so that it becomes \"prettier\"\r\n\r\nWe're not always going to be able to hit the trifecta of `safe`, `efficient`, and `pretty`. We can, however, strike a decent enough balance between them and as a long term goal push towards improving the scenarios where it falls down."
      ],
      "runtime-model-actual-hardware-costs": [
        "I don't think these costs are \"accurate\" and likely deserve more investigation in the future. It also isn't accounting for SIMD/Mask. -- We have a similar issue with some of the costs in the gen tree computation, where a lot of them were modeled around the x87 FPU and never updated to account for modern SIMD considerations.\r\n\r\nHere's the rough costs of loads:\r\n* a 32-bit integer move: `encoding: 2 bytes; execution: 2-5 cycles`\r\n* a 64-bit integer move: `encoding: 3 bytes; execution: 2-5 cycles`\r\n* a 32-bit floating-point move: `encoding 4 bytes; execution: 4-7 cycles`\r\n* a 64-bit floating-point move: `encoding 4 bytes; execution: 4-7 cycles`\r\n* a 128-bit simd move: `encoding 3-4 bytes; execution: 4-7 cycles`\r\n* a 256-bit simd move: `encoding 4-5 bytes; execution: 5-8 cycles`\r\n* a 512-bit simd move: `encoding 6 bytes; execution: 5-9 cycles`\r\n\r\nStores tend to be up to more expensive than loads. You get roughly `4-10 cycles` for simd and floating-point, but nearly `2-10 cycles` for integers\r\n\r\n",
        "This logic hasn't really been applicable in a long time. We have single instruction spill/reload regardless of SIMD size, it's just that TYP_SIMD32/64 require spilling since the upper bits are volatile and we may only spill 128-bits instead of all bits, if possible (and which is no more expensive to do).\r\n\r\nA long time ago we did have more complex logic for upper save/restore, but now we can just follow the normal callee save/restore consideration instead.",
        "> varTypeIsSIMD(tree) is redundant with tree->TypeIs(TYP_SIMD32, TYP_SIMD64).\r\n\r\n`varTypeIsSIMD(tree)` is cheaper and ensures the common case is 1 check (just checking the `varTypeClassification`).\r\n\r\n> Why have tree->OperIsHWIntrinsic() branch as well?\r\n\r\nThere are nodes that produce scalar results or small types but which still operate on V256/V512. It's an `||` condition so its handling either `node that produces a simd32/64` or `hwintrinsic node that uses simd32/64`\r\n\r\n> This will not work for tree that contain SIMD expression but do not evaluate to a SIMD/HWI.\r\n\r\nFor some arbitrary block copies or other operations, potentially. Most usages of SIMD32/64 are done via hwintrinsics, so this is getting the important bits.\r\n\r\n> The comment mentions ...such constants..., but the code does not contain checks for constants, so it is confusing.\r\n\r\nWill fix.\r\n",
        "Talked with @kunalspathak and this is currently needed as we still try to do alignment when optimizations are disabled.\r\n\r\nWe may want to revisit that since OSR + TC should handle all the important cases and aligning debug code likely isn't worth the cycles required.",
        "This covers the crossgen scenario, correct?\r\n```suggestion\r\n    const bool  isReadyToRun           = opts.IsReadyToRun();\r\n    const bool  resolveTokens          = makeInlineObservations && (isTier1 || isReadyToRun);\r\n```",
        "Will switch to that. Could I get a summary of the difference between the two for future reference?",
        "```suggestion\r\n    const bool  isPreJit               = opts.jitFlags->IsSet(JitFlags::JIT_FLAG_PREJIT);\r\n    const bool  resolveTokens          = makeInlineObservations && (isTier1 || isPreJit);\r\n```",
        "Rather than take `emitDataAlignment` which only had `None`, `Preferred`, and `Required`, we just pass in the actual alignment we want. This is normally the same as the constant size, but it isn't required to be (as is the case for `SMALL_CODE` where we want an alignment of whatever is smallest and works for the architecture).",
        "It didn't when I checked but most of the methods that involve vector constants also don't contain other constants."
      ]
    }
  },
  "PeterSchafer": {
    "repos": [
      "snyk/cli"
    ],
    "entries": [
      {
        "slug": "cli-api-interface-design",
        "title": "API interface design"
      },
      {
        "slug": "cli-balance-concurrent-operations",
        "title": "Balance concurrent operations"
      },
      {
        "slug": "cli-comprehensive-test-coverage",
        "title": "comprehensive test coverage"
      },
      {
        "slug": "cli-configuration-naming-consistency",
        "title": "Configuration naming consistency"
      },
      {
        "slug": "cli-defensive-shell-script-configuration",
        "title": "defensive shell script configuration"
      },
      {
        "slug": "cli-document-intent-and-reasoning",
        "title": "Document intent and reasoning"
      },
      {
        "slug": "cli-graceful-error-handling",
        "title": "Graceful error handling"
      },
      {
        "slug": "cli-handle-all-errors-explicitly",
        "title": "Handle all errors explicitly"
      },
      {
        "slug": "cli-maintain-build-environment-parity",
        "title": "maintain build environment parity"
      },
      {
        "slug": "cli-maintain-cicd-boundaries",
        "title": "maintain CI/CD boundaries"
      },
      {
        "slug": "cli-optimize-ci-resource-allocation",
        "title": "optimize CI resource allocation"
      },
      {
        "slug": "cli-optimize-variable-declarations",
        "title": "Optimize variable declarations"
      },
      {
        "slug": "cli-pin-dependency-versions",
        "title": "Pin dependency versions"
      },
      {
        "slug": "cli-prevent-silent-test-failures",
        "title": "prevent silent test failures"
      },
      {
        "slug": "cli-separate-build-from-runtime",
        "title": "separate build from runtime"
      },
      {
        "slug": "cli-synchronize-configuration-values",
        "title": "synchronize configuration values"
      },
      {
        "slug": "cli-use-centralized-configuration-access",
        "title": "Use centralized configuration access"
      },
      {
        "slug": "cli-use-centralized-loggers",
        "title": "Use centralized loggers"
      },
      {
        "slug": "cli-use-descriptive-names",
        "title": "Use descriptive names"
      },
      {
        "slug": "cli-use-descriptive-parameter-names",
        "title": "Use descriptive parameter names"
      },
      {
        "slug": "cli-use-file-locks",
        "title": "Use file locks"
      },
      {
        "slug": "cli-use-optional-chaining",
        "title": "Use optional chaining"
      },
      {
        "slug": "cli-use-secure-hash-functions",
        "title": "Use secure hash functions"
      },
      {
        "slug": "cli-validate-environment-variables-early",
        "title": "validate environment variables early"
      },
      {
        "slug": "cli-validate-security-configurations",
        "title": "Validate security configurations"
      },
      {
        "slug": "cli-write-actionable-documentation",
        "title": "Write actionable documentation"
      }
    ],
    "comments": {
      "cli-use-descriptive-parameter-names": [
        "Question: win-something? What is it?",
        "Oh got it, it is just to trigger another case. This is a bit confusing. Can we please at least have a comment what we do here?",
        "Actually it might be better to rename the parameter from executor to something like OS, since executor is an existing construct that has a special meaning."
      ],
      "cli-defensive-shell-script-configuration": [
        "Good idea! Thank you! Please take another look!"
      ],
      "cli-use-centralized-configuration-access": [
        "Issue: with the new gaf version, the temp directory is a configuration value. Instead of using this helper function directly, we should be using the configuration now, otherwise the behaviour wouldn't be consistent.\r\n\r\n```suggestion\r\n\treturn c.globalConfig.GetString(configuration.TEMP_DIR_PATH)\r\n```",
        "Issue: with the new gaf version, the temp directory is a configuration value. Instead of using this helper function directly, we should be using the configuration now, otherwise the behaviour wouldn't be consistent.\r\n\r\n```suggestion\r\n\treturn config.GetString(configuration.TEMP_DIR_PATH)\r\n```",
        "Issue: same as above, please use the configuration value for temp dir.",
        "Suggestion: Please use the configuration to access values, see [here](https://github.com/snyk/go-application-framework/blob/main/pkg/local_workflows/auth_workflow.go#L84). This way, there is no need for the implementation to know where the values come from and how they exactly need to be named.",
        "unfortunately not, please take a closer look at the linked code snippet. "
      ],
      "cli-comprehensive-test-coverage": [
        "Suggestion: There is an opportunity to move this in a small function and write a test for it ;) ",
        "Suggestion: Use `t.Setenv()` instead of `os.Setenv()`",
        "Interesting point, to be honest I'm not completely sure about the root cause of why sleep is necessary. I just had the build failing on a circle runner while working locally and on other runners. \r\nMy assumption is that the final comparison of the modification time might not be the strongest check. Looking at time resolution etc, I was assuming that the sleep helps to ensure that modification times would be definitely different.\r\nLooking at the comment it seems misleading. I'm definitely changing the comment.",
        "done!",
        "Suggestion: extend the test to cover both cases more than 5 elements in the cache and less. maybe add second test case."
      ],
      "cli-api-interface-design": [
        "Suggestion: How about adding the CertificateLocation to ProxyInfo and thereby reducing the interface here.",
        "Without the interface it would be impossible to implement other authentication handlers in the future."
      ],
      "cli-synchronize-configuration-values": [
        "Because the documentation says so.\r\n> Update all NodeJS versions to match the NodeJS versions used in pkg."
      ],
      "cli-validate-security-configurations": [
        "Question: This looks like a change of behaviour to me. Previously if a token was specified sessionToken contained the plain token value, now it contains `token <tokenvalue>`. Is this considered in the plugins that consume the sessionToken value?",
        "Great! Thanks for the detailed explanation! "
      ],
      "cli-optimize-ci-resource-allocation": [
        "Caching only relates to the downloaded installers, they still need to be executed to install the applications. The different versions of the command install different subsets of the tools for efficiency reasons."
      ],
      "cli-balance-concurrent-operations": [
        "Suggestion: Let's be a bit more pessimistic to not increase the load too much and set it to a lower value first. Let's start with 2."
      ],
      "cli-optimize-variable-declarations": [
        "Suggestion: how about removing the port here, since the information is now available via ProxyInfo.",
        "issue: imported but not used"
      ],
      "cli-handle-all-errors-explicitly": [
        "Issue: Mimicking the gaf network stack behaviour, we need to use the error returned from the errorhandler and only if this error is not nil set the terminate header.",
        "Please use fmt.Errorf() the following way, see [the documentation](https://pkg.go.dev/fmt@go1.19.5#Errorf) for the reasoning\r\n`fmt.Errorf(\"Cache directory path is invalid: %w\", err)`",
        "done!",
        "done! used defer"
      ],
      "cli-pin-dependency-versions": [
        "Question: Did you miss to add the changes in the package.json?",
        "Thanks for updating. I do have a follow up question though, how come that the added test didn't fail? I would expect that the test doesn't succeed if the dependency is not updated.",
        "Suggestion: Personally I think that the usage of `^` is not a good practice as it means that we can't reproduce individual builds. Therefore I would like to propose that we use this chance to get rid of them step by step.",
        "withdrawn :) "
      ],
      "cli-validate-environment-variables-early": [
        "Question: What is the developer experience when this Environment Variable is not specified? Is it worth to explicitly warn when it is empty?\r\n\r\nNitpick: Maybe move this to where `LS version:` is printed ðŸ˜„ ",
        "This is the error I see, right now. \r\n<img width=\"613\" alt=\"image\" src=\"https://github.com/user-attachments/assets/05a0cb4b-3219-487a-8b4a-ba2a93529b31\" />\r\n\r\nCan we make the test a bit more descriptive and actionable? I'm worried about all the asks we will be getting because someone is using a dev CLI without the URL set.\r\n",
        "Maybe we fail the command execution instead?!",
        "yes, it fails, but it seems to do a lot of things until it fails with the message right now. So a proposal could be to do a check for the variable and fail early and actionable. Does this make sense?",
        "for example [here](https://github.com/snyk/cli-extension-iac/blob/0a63172561937695026101838e018d5574618785/internal/commands/iactest/iactest.go#L69)",
        "Question: DEBUG is a very generic environment variable name. There are good chances to have this conflicting somewhere for example in our CI/CD. Should we maybe solve enabling the debug build via an explicit target?",
        "Suggestion: How about using the `.mvnrc` file to determine the major version, instead of having it hardcoded here. Take a look at https://github.com/snyk/cli/blob/master/scripts/create-build-image.sh#L9C3-L9C32 for example"
      ],
      "cli-use-optional-chaining": [
        "As I understand the case, not having vulnerabilities here is an error case. This means if they are missing, the comparison will fail. The intention is to not make this helper function crash. Appropriate error handling must and as far as I see, does happen outside."
      ],
      "cli-use-descriptive-names": [
        "nitpick: Naming variables the same way as data structures can be confusing. Maybe just rename the variables here?"
      ],
      "cli-use-centralized-loggers": [
        "We want/need the information logged! \r\nBut rather than using the debug parameter the logger actually gets disabled centrally.",
        "Suggestion: try to use DebugLogger as zerolog logger, this will be possible every except of two places. For these places you could add an additional property here.",
        "Issue: you need to use the centrally supplied logger! It is not correct to create a new one here.",
        "Suggestion: I would use both logger types from the context instead of creating a ToZeroLogDebug below. The logic is at one place and if we want to remove one day the log.Logger usage, it will be easier to find. ",
        "Issue: this needs to use a log.Logger that uses a writer to zerolog `ToZeroLogDebug`! Otherwise, the log.Default logger doesn't respect the debug configuration and logger configuration that is centrally done.",
        "Issue: this needs to use a log.Logger that uses a writer to zerolog ToZeroLogDebug! Otherwise, the log.Default logger doesn't respect the debug configuration and logger configuration that is centrally done.\r\n\r\nSame as below!",
        "Suggestion: replace fmt.Println() by c.DebugLogger.Println() and add the currentPath"
      ],
      "cli-configuration-naming-consistency": [
        "Question: is this on purpose that the log says `TEST_CONFIG_FILE` and the env var used is `SNYK_CONFIG_FILE`?",
        "Is `TEST_CONFIG_FILE` used somewhere?"
      ],
      "cli-use-file-locks": [
        "This would probably be in addition to restoring the file I assume.",
        "@bastiandoetsch we can use flock but we would need to place the lock file outside of the cache directory, which we are going to create. We can do this, just that we need to consider this in the docs about filesystem access etc.",
        "And yes, I agree, the user shouldn't be bothered at all. Per default the CLI determines the cache directory based on OS defaults, these directories exists and we just place a subfolder inside of it, which we can create relatively safe with mkdir. so per default, the assumption is that this error would never happen.\r\n\r\nIt might happen, if a user specifies a custom path.",
        "Definitely Possible! "
      ],
      "cli-graceful-error-handling": [
        "Suggestion: Let's revert this change here, since it is better to fail than send an empty string.",
        "I removed the duplicated logic.\r\nI realized that `err.jsonPayload` was already there, so I'm not changing this.",
        "Issue: From my understanding I think we need the non-pretty-print part of the original solution as well and only if this fails as well, we should fallback on the new solution. Removing the original code will cause issues for use cases of --json, which worked due to the non-pretty-print part.",
        "The associated unit test needs to be fixed and an additional one needs to be created.",
        "Question: Don't we need to move this out of the else case?"
      ],
      "cli-write-actionable-documentation": [
        "suggestion: replace directives are not necessarily a debugging step, maybe we separate them from the debugging docs and add them into a section about development in golang?"
      ],
      "cli-prevent-silent-test-failures": [
        "Suggestion: asserting against stdout is always a bit tricky as it is not a strong check or actually too strong as it breaks with minor rewrite of a message, which increases the maintenance. There is another way to check, which is using the instrumentation data, which would actually be a more complete check of the scenario.",
        "Issue: This  changes the expectation, `> 0` is not the same as `==2`.  You should be able to filter the requests and leave the expectations unchanged.",
        "Suggestion: This test should ensure that the content is a json data structure, for example by using a library like [this](https://www.npmjs.com/package/jsonparse). It should also ensure that the length of stdoutBuffer is greater 0.",
        "Suggestion: move this assert a bit up before trying to access the output file. Running the test with a non fixed CLI should fail in a planned way and not just because the file doesn't exists.",
        "Question: Shouldn't this be equal instead of contains?"
      ],
      "cli-use-secure-hash-functions": [
        "We can optimize and just do it when `--debug` is used",
        "yes, salt would not be good ðŸ˜„  the token-hash is being truncated to 16 bytes to avoid any reverse engineering of the actual token, see the lines below."
      ],
      "cli-separate-build-from-runtime": [
        "Suggestion: We should build the dependencies when building the docker images and not in the script that shall just sign."
      ],
      "cli-maintain-cicd-boundaries": [
        "Issue: on main, there shouldn't be any release notes."
      ],
      "cli-maintain-build-environment-parity": [
        "Question: Do we really need a differentiation between build and build-local? I'm worried that local and CI build get out of sync at some point. I would like to see everyone using the same command to build.",
        "Just to expand the thought, I wonder if we can make the usage of the virtual environment invisible to the `make` user.",
        "it is an optimization to not always use clean-install. The pipeline itself does `npm ci` at the beginning, in this stage, we just want to install what is missing if anything."
      ],
      "cli-document-intent-and-reasoning": [
        "Nitpick: a short comment what the new flag does would be helpful for future us."
      ]
    }
  },
  "louwers": {
    "repos": [
      "maplibre/maplibre-native"
    ],
    "entries": [
      {
        "slug": "maplibre-native-accurate-documentation-references",
        "title": "Accurate documentation references"
      },
      {
        "slug": "maplibre-native-configure-platform-specific-builds",
        "title": "Configure platform-specific builds"
      },
      {
        "slug": "maplibre-native-consistent-api-practices",
        "title": "Consistent API practices"
      },
      {
        "slug": "maplibre-native-cross-platform-ci-validation",
        "title": "Cross-platform CI validation"
      },
      {
        "slug": "maplibre-native-cross-platform-test-management",
        "title": "Cross-platform test management"
      },
      {
        "slug": "maplibre-native-descriptive-named-constants",
        "title": "Descriptive named constants"
      },
      {
        "slug": "maplibre-native-design-evolution-ready-apis",
        "title": "Design evolution-ready APIs"
      },
      {
        "slug": "maplibre-native-document-containerized-builds",
        "title": "Document containerized builds"
      },
      {
        "slug": "maplibre-native-document-public-api-completely",
        "title": "Document public API completely"
      },
      {
        "slug": "maplibre-native-dry-class-hierarchies",
        "title": "DRY class hierarchies"
      },
      {
        "slug": "maplibre-native-enforce-clear-data-ownership",
        "title": "Enforce clear data ownership"
      },
      {
        "slug": "maplibre-native-externalize-config-values",
        "title": "Externalize config values"
      },
      {
        "slug": "maplibre-native-externalize-configuration-values",
        "title": "Externalize configuration values"
      },
      {
        "slug": "maplibre-native-extract-workflow-scripts",
        "title": "Extract workflow scripts"
      },
      {
        "slug": "maplibre-native-follow-modern-c-guidelines",
        "title": "Follow modern C++ guidelines"
      },
      {
        "slug": "maplibre-native-group-related-properties",
        "title": "Group related properties"
      },
      {
        "slug": "maplibre-native-handle-errors-by-severity",
        "title": "Handle errors by severity"
      },
      {
        "slug": "maplibre-native-modern-c-style-practices",
        "title": "Modern C++ style practices"
      },
      {
        "slug": "maplibre-native-numerical-precision-considerations",
        "title": "Numerical precision considerations"
      },
      {
        "slug": "maplibre-native-optimize-compilation-flags",
        "title": "Optimize compilation flags"
      },
      {
        "slug": "maplibre-native-prefer-safe-null-handling",
        "title": "Prefer safe null handling"
      },
      {
        "slug": "maplibre-native-prefer-values-over-pointers",
        "title": "Prefer values over pointers"
      },
      {
        "slug": "maplibre-native-self-documenting-code-naming",
        "title": "Self-documenting code naming"
      },
      {
        "slug": "maplibre-native-standard-configuration-files",
        "title": "Standard configuration files"
      },
      {
        "slug": "maplibre-native-structure-documentation-effectively",
        "title": "Structure documentation effectively"
      },
      {
        "slug": "maplibre-native-style-compliant-example-code",
        "title": "Style-compliant example code"
      },
      {
        "slug": "maplibre-native-template-instantiation-trade-offs",
        "title": "Template instantiation trade-offs"
      },
      {
        "slug": "maplibre-native-use-proper-logging",
        "title": "Use proper logging"
      },
      {
        "slug": "maplibre-native-use-specific-test-assertions",
        "title": "Use specific test assertions"
      },
      {
        "slug": "maplibre-native-validate-noexcept-guarantees",
        "title": "Validate noexcept guarantees"
      },
      {
        "slug": "maplibre-native-variable-evaluation-context",
        "title": "Variable evaluation context"
      }
    ],
    "comments": {
      "maplibre-native-use-proper-logging": [
        "use mbgl::Log, remove macro",
        "There are commented out lines that print used for debugging throughout this file.\r\n\r\nPlease remove them or change them to (debug) logging.",
        "Should some logging be added in these cases?",
        "Thanks for clarifying.\r\n\r\nI didn't see any retry messages in the logs, so I didn't know if it was actually retrying."
      ],
      "maplibre-native-cross-platform-ci-validation": [
        "CI is complaining that this function is not used.",
        "It does find the provisioning profile for me now. Could we make a bug report over at https://github.com/MobileNativeFoundation/rules_xcodeproj ?\r\n\r\nBwX does not have first-class support from rules_xcodeproj and support for it might be removed altogether in the future. I will add your comment to the discussion [here](https://github.com/MobileNativeFoundation/rules_xcodeproj/discussions/2391).\r\n\r\n",
        "I made the `BUILD_MODE` configurable. You can set it to your liking in your `config.bzl`."
      ],
      "maplibre-native-document-containerized-builds": [
        "If we have to choose one, just include the command without the `___any_build_command___`, because it's nicer to have a valid command that people can copy and paste.\r\n\r\nOptionally mention that you can also run build commands directly.",
        "```suggestion\r\nYou can use a Docker container to build MapLibre Native. A `Dockerfile` that installes the required dependencies when the image is built is provided in this directory.\r\n```"
      ],
      "maplibre-native-self-documenting-code-naming": [
        "I understand `_setDirection` needs the current location so that it can do the animation in one go, but maybe consider passing the center instead of a boolean. I don't think the `_setDirection` method should have anything to do with the location manager.",
        "Looks good now, thanks!",
        "Perhaps this should (also) be called `setTileCacheEnabled(bool)` because it is a bit clearer that it actually disables the tile cache when passing `false`.",
        "`removeDrawablesIf` sounds like a much better name here."
      ],
      "maplibre-native-style-compliant-example-code": [
        "Maybe don't use this because it is an internal class to the test app. Instead use something that users could copy paste in their own apps.",
        "Yes just use a hardcoded style URL like demotiles.",
        "> Personally I'd argue for \"best practice, even if more complicated\" because people will copy and paste our examples into their code and location management is not an easy task.\r\n\r\nFully agreed. Even an example with an external dependency would be fine by me. Let's not forget the iOS MapLibre codebase is quite rusty, so it should not be surprising some things are not very modern anymore.\r\n\r\nI'm just transplanting some examples from https://github.com/mapbox/ios-sdk-examples, I'm not a veteran iOS developer like you, so I am happy for your critical eye!"
      ],
      "maplibre-native-standard-configuration-files": [
        "Toolchain is already installed on the default image."
      ],
      "maplibre-native-prefer-values-over-pointers": [
        "I think using unique pointers but then using `.get()` to get a raw pointer is dangerous because now someone might hold onto a dangling pointer.\r\n\r\nI would try to avoid using raw pointers as much as possible, in this case since `Sprite` is such a lightweight object copying is probably even OK.",
        "What is this?",
        "The return type is marked with `nonnull` and the interface with `null_resettable`, which means this getter should not return null if I understand it correctly. We should not modify this, because I think it would change the Swift API.\r\n\r\nMaybe you can return `[NSURL URLWithString:@\"local://style.json\"]` instead?"
      ],
      "maplibre-native-configure-platform-specific-builds": [
        "@1ec5 mentioned on Slack that a source-only distribution is the prefered way of distribution for Apple platforms now. For MapLibre I don't know if would apply to just the SDK (probably written in Swift in the long term) or also the core written in C++.",
        "It's easier to just clone with `--recurse-submodules`"
      ],
      "maplibre-native-modern-c-style-practices": [
        "Don't use C-style casts.",
        "Do you know structured bindings? I think this would work\r\n\r\n```\r\nfor (auto [texHandle, glyph, fontStack] : glyphsToUpload) {\r\n```",
        "Please have a look over all definitions in the PR and make sure that all immutable values are marked `const`.",
        "You should be able to default this `operator==` since we use C++20 now.",
        "Should be initialized to something.",
        "C++ Code Guidelines say yes. https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#es20-always-initialize-an-object"
      ],
      "maplibre-native-structure-documentation-effectively": [
        "I think we can assume that everyone knows JSON...\r\n\r\nWe can probably link to an explanation of GeoJSON elsewhere and focus on concepts that are specific to MapLibre Android.",
        "All code snippets (except really small ones) should be referenced so that we can be sure they compile in the future.",
        "Each section of the article should have a descriptive heading so people can easily skip to the part they are interested in.",
        "Sections can be short, that is not a problem. But we should have a heading for each part of the article that covers a different concept.",
        "Give the article a descriptive name that explains the contents of the article. \"Ways to Configure the Map\" or something similar."
      ],
      "maplibre-native-accurate-documentation-references": [
        "```suggestion\r\n     * Learn more about above properties in the [Style specification](https://maplibre.org/maplibre-style-spec/).\r\n```"
      ],
      "maplibre-native-descriptive-named-constants": [
        "Avoid magic numbers",
        "Adding comments is good, but I would also assign these numbers to a const.",
        "I think all upper case names should be reserved for macros. https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#nl9-use-all_caps-for-macro-names-only",
        "Keep it `constexpr`, but lowercase them.\r\n\r\nAlso camelcase maybe `pmtilesHeaderOffset`?"
      ],
      "maplibre-native-design-evolution-ready-apis": [
        "This is technically a breaking change right?",
        "Maybe we can deprecate this API and add a new one that has the full rendering stats?",
        "Would it be a breaking change to take a `Call.Factory` here?",
        "If it's source compatible I'd say go ahead and just change the signature, if you don't mind. ðŸ™‚ "
      ],
      "maplibre-native-use-specific-test-assertions": [
        "Needs to use `EXPECT_THROW` otherwise the test will fail it doesn't throw."
      ],
      "maplibre-native-variable-evaluation-context": [
        "Yes it is manual, but people can make mistakes. Especially when making a pre-release, which does not require a PR review for a version release."
      ],
      "maplibre-native-template-instantiation-trade-offs": [
        "Are there more lightweight `std::function` alternatives?",
        "This solution is pretty easy to understand. There may be a fancier solution but looks OK to me.",
        "I guess nameIDs are small so this doens't overflow right?\r\n\r\nShould this maybe be extracted in a function? "
      ],
      "maplibre-native-optimize-compilation-flags": [
        "Yes it contains both:\r\n\r\n```\r\n$ ls MapLibre.xcframework\r\nInfo.plist*                  ios-arm64/                   ios-arm64_x86_64-simulator/\r\n```\r\n\r\nBut I am extracting the armv8 dynamic library from the XCFramework:\r\n\r\n```\r\ncp MapLibre.xcframework/ios-arm64/MapLibre.framework/MapLibre MapLibre_dynamic\r\n```\r\n\r\n```\r\n $ file MapLibre_dynamic\r\nMapLibre_dynamic: Mach-O 64-bit dynamically linked shared library arm64\r\n```\r\n\r\nSo should be fine."
      ],
      "maplibre-native-prefer-safe-null-handling": [
        "You are moving the data member `pendingReleases` here. It's no longer valid after that.",
        "<img width=\"659\" alt=\"image\" src=\"https://github.com/user-attachments/assets/2b533993-5a56-4e51-82a7-340d5a5ae186\">\r\n<img width=\"673\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b509f10f-5280-4be1-9006-b3689361f869\">\r\n",
        "Yes I think that is OK.",
        "Maybe try passing `nullptr` here?\r\n\r\n> If zVfsName is NULL then the default VFS is returned.\r\n\r\nhttps://www.sqlite.org/c3ref/vfs_find.html",
        "The memory pointed to by the unique_ptr is not cleaned up after `.release()`.\r\n\r\n",
        "I think you can just `reset()` instead.",
        "Can you avoid using raw pointers?",
        "```suggestion\r\n                                             nullptr)) {}\r\n```\r\n\r\nMight as well update this as well.",
        "Yes I don't think there's a good reason to use `NULL` in C++ code, even when calling C libraries."
      ],
      "maplibre-native-follow-modern-c-guidelines": [
        "Prefer enum class https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#Renum-class",
        "Prefer `enum class`.",
        "Created an issue.",
        "We follow the C++ Core Guidelines, which state\r\n\r\n> [Specify the underlying type of an enumeration only when necessary](https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#Renum-underlying)\r\n\r\nEspecially in this case I think it makes sense to just use the default.",
        "There is something to be said for it, but since the space saving is minimal sticking to the default might be better.\r\n\r\nI will leave it up to your judgement!",
        "Can probably be const.",
        "Can be marked const.",
        "Maybe we can use something like https://github.com/zhihaoy/nontype_functional/blob/main/include/std23/move_only_function.h instead so we can more easily migrate to `std::move_only_function` when we can use C++23.\r\n\r\nIt's also better tested.",
        "If it's not too much hassle I would add it to `vendor/nontype_functional`. Could just be the two files we need.",
        "Can be const."
      ],
      "maplibre-native-validate-noexcept-guarantees": [
        "This can throw which would crash the program (because of `noexcept`). Is it possible to avoid allocation?",
        "If it is unavoidable we should remove `noexcept` here (and from the base class), otherwise the program will crash when an exception is thrown here.",
        "The move constructor of `type::Type` is not `noexcept`.\r\n\r\nbecause `mapbox::util::recursive_wrapper<Array>` is part of the variant\r\n\r\n```cpp\r\n    recursive_wrapper(recursive_wrapper&& operand)\r\n        : p_(new T(std::move(operand.get()))) {}\r\n```",
        "When adding `noexcept` to a templated function, the compiler won't check that the methods called on that type are really `noexcept`. It will just crash on runtime.\r\n\r\nIt is really hard to verify yourself as well (with all the deeply nested templates).\r\n\r\nWith long, complicated, templated functions, it may be worth leaving out `noexcept`. In this case we can maybe use something like:\r\n\r\n```cpp\r\n    using ReturnType = decltype(evaluated.template get<DataDrivenPaintProperty>());\r\n    static_assert(std::is_nothrow_invocable_v<decltype(&ReturnType::isConstant), ReturnType&>,\r\n                  \"isConstant() must be noexcept\");\r\n```",
        "`mbgl::util::clamp` (called somewhere down) is not `noexcept`. But it can be made `noexcept`.",
        "```suggestion\r\n    T operator()(const T& a, const T&, const double) const {\r\n```\r\n\r\nmay call throwing copy constructor",
        "```suggestion\r\nT interpolate(const T& a, const T& b, const float t) {\r\n```\r\n\r\nmay call throwing copy constructor",
        "```suggestion\r\nT interpolate(const T& a, const T& b, const double t) {\r\n```\r\n\r\nmay call throwing copy constructor",
        "`std::make_tuple` is not `noexcept`.",
        "Ah OK, but this is one compiler.\r\n\r\nI would err on the side of safety in general when it comes to `noexcept`. If it is not immediately obvious something is `noexcept`, I would not use it."
      ],
      "maplibre-native-handle-errors-by-severity": [
        "What kind of exceptions can `std::exception_ptr` contain? Can all be safely ignored? ",
        "Is the callback passed to `renderStill` called multiple times? In that case the error would get overwritten in the case of multiple errors.",
        "You need to check the return value of `query.run()`. If it returns `false` no row is available and `query.get` will throw.",
        "Maybe it's better to let it crash here instead? If these functions are not initialized MapLibre will not be able to render anything.",
        "Maybe you could wrap the check in a function that returns a bool and logs a warning if there is no context anymore?\r\n\r\nYour call, but it may save someone a frustrating debugging session."
      ],
      "maplibre-native-extract-workflow-scripts": [
        "Shellcheck detects an issue with this script:\r\n\r\n```\r\nArgument mixes string and array. Use * or separate argument\r\n```",
        "Since this is quite a large script, maybe it can be extracted in a file so it can be easily ran.",
        "Like this?",
        "Yes sounds better, thanks.",
        "I see it relies on caches. That will not speed things up because our cache is full after a single run of the workflows.\r\n\r\nGitHub promised they will allow a bigger cache than just 10GB in Q1 2025 though.",
        "I prefer to just keep it simple for now.",
        "Fixed."
      ],
      "maplibre-native-consistent-api-practices": [
        "```suggestion\r\nMultiple map instances are enabled using a unique context pointer.  A unique context pointer is passed back for every `initialize` invocation. The context pointer is released on `de_initialized` or when the library reference is destroyed.\r\n```",
        "```suggestion\r\nThe environment that consumes the FFI library is responsible for initializing its own graphics backend.  This limits a library artifact to a specific platform (Mac, Window, Linux) and runtime revision.  The required parts are passed into the FFI initialize call using an opaque data pointer (nativeWindow).  Backend/Platform build flags determine how this opaque data pointer is cast/used.\r\n```",
        "```suggestion\r\n* Introduce â€œAnnotationâ€ as a first class citizen of the MapLibre Native C++ Core.\r\n```\r\n\r\nSince this design proposal is for the MapLibre Native repo, it is understood that it does not apply to MapLibre GL JS.",
        "```suggestion\r\nThis proposal introduces the concept of annotations that are complex, animatable and interactive to MapLibreâ€™s core. The first phase allows bitmap backed annotations to be rendered. The second phase will enable native platform views (iOS, Android) to be rendered directly by MapLibre Native.\r\n```",
        "```suggestion\r\nThese new annotations will provide a more flexible tool for drawing user content on the map that is more in line with what developers expect from the built-in mobile map toolkits. The focus on animations and interactivity allows developers to create differentiated map experiences that feel made for mobile.\r\n```"
      ],
      "maplibre-native-cross-platform-test-management": [
        "Maybe we can only ignore it on the platforms where it is failing?\r\n\r\nFor me it is running fine locally on Android.\r\n\r\nIf you would prefer to merge this and look into this later, let's create an issue so we don't forget.",
        "Thanks!",
        "Well, looks like it failed on the Pixel 7 Pro on CI.\r\n\r\nMight be device-dependent. Maybe best to disable it after all so we can merge this."
      ],
      "maplibre-native-numerical-precision-considerations": [
        "`std::numeric_limits<double>::epsilon()` was too small.\r\n\r\nMaybe we should define this globally somewhere."
      ],
      "maplibre-native-externalize-config-values": [
        "In any case getting rid of the magic number would be good."
      ],
      "maplibre-native-document-public-api-completely": [
        "Public APIs could use some docstrings. Adding them to private APIs also does not hurt.",
        "Everything in `include` will become part of our public API. Is this intended? Can you add some triple slash comments to indicate what this template struct is for?",
        "It may make sense to make it internal, in that case you can move it to `/src`.\r\n\r\nSome comments (and a link to cppreference) would still be a good idea in that case.",
        "These could use some Doxygen comments.\r\n\r\n```\r\n    RequestedFromCache ///< like this\r\n```",
        "Suggestion: add a triple slash comment to each enum in the PR, describing what it does.\r\n\r\nIt is also possible to add descriptions to enum values.\r\n\r\n````\r\n/// This is an enum class\r\nenum class fooenum {\r\n    FOO, ///< this is foo\r\n    BAR, ///< this is bar\r\n};\r\n````",
        "A documentation comment should be added to this class.\r\n\r\nSome (public) methods including the constructors might also benefit from documentation comments, although they are relatively straightforward."
      ],
      "maplibre-native-dry-class-hierarchies": [
        "Duplicated. I feel like this should be in a common base class.",
        "Most code in this class is identical to the GL version. They can probably use a common base class."
      ],
      "maplibre-native-enforce-clear-data-ownership": [
        "Use a lock guard? Can deadlock if anything throws I think.\r\n\r\nAlso applies to other methods.",
        "Somehow my brain can't parse this capture group."
      ],
      "maplibre-native-group-related-properties": [
        "What kind of extra properties would be relevant for `MLNPluginLayerDrawingContext` but not `MLNStyleLayerDrawingContext`?",
        "Honestly I would make `ActionJournalOptions` objects immutable if possible. You could return a new `ActionJournalOptions` if you think that is a good API."
      ],
      "maplibre-native-externalize-configuration-values": [
        "Good suggestion, but that is outside the scope of this PR, because this is also what we do on `main`:\r\n\r\n```groovy\r\n                        // Enable ccache if the user has installed it.\r\n                        if (file(\"/usr/bin/ccache\").exists()) {\r\n                            arguments \"-DANDROID_CCACHE=/usr/bin/ccache\"\r\n                        } else if (file(\"/usr/local/bin/ccache\").exists()) {\r\n                            arguments \"-DANDROID_CCACHE=/usr/local/bin/ccache\"\r\n                        }\r\n```",
        "Could you create a new issue for this? "
      ]
    }
  },
  "davidspek": {
    "repos": [
      "kubeflow/kubeflow"
    ],
    "entries": [
      {
        "slug": "kubeflow-centralize-configuration-values",
        "title": "Centralize configuration values"
      },
      {
        "slug": "kubeflow-control-header-modification",
        "title": "Control header modification"
      },
      {
        "slug": "kubeflow-descriptive-consistent-naming",
        "title": "Descriptive consistent naming"
      },
      {
        "slug": "kubeflow-document-networking-annotations",
        "title": "Document networking annotations"
      },
      {
        "slug": "kubeflow-environment-aware-configuration-design",
        "title": "Environment-aware configuration design"
      },
      {
        "slug": "kubeflow-externalize-configuration-parameters",
        "title": "Externalize configuration parameters"
      },
      {
        "slug": "kubeflow-follow-api-conventions",
        "title": "Follow API conventions"
      },
      {
        "slug": "kubeflow-go-export-naming-conventions",
        "title": "Go export naming conventions"
      },
      {
        "slug": "kubeflow-manage-configuration-changes",
        "title": "Manage configuration changes"
      },
      {
        "slug": "kubeflow-standardize-network-tools",
        "title": "Standardize network tools"
      },
      {
        "slug": "kubeflow-structured-documentation-with-examples",
        "title": "Structured documentation with examples"
      },
      {
        "slug": "kubeflow-unique-workflow-step-names",
        "title": "Unique workflow step names"
      },
      {
        "slug": "kubeflow-use-snake-case-in-python",
        "title": "Use snake_case in Python"
      },
      {
        "slug": "kubeflow-validate-inputs-explicitly",
        "title": "Validate inputs explicitly"
      }
    ],
    "comments": {
      "kubeflow-follow-api-conventions": [
        "I have no idea how to start implementing this, and I don't believe there is enough time to do so anymore either. I'm mainly following what is already being done in the code elsewhere. If you want to improve the code, I'm happy to include it in the PR. ",
        "@yanniszark Thanks for the example. I just noticed the actual latest changes were not pushed yesterday. I just pushed them and I believe the error handling from you example is included in there. However, the controller crashing is still a problem.  "
      ],
      "kubeflow-descriptive-consistent-naming": [
        "Small nit, this name stands out compared to the rest. People not familiar with the code might now know what `KFAM` means. Not a blocker for the PR but `Access Management` might be better to use. "
      ],
      "kubeflow-centralize-configuration-values": [
        "Will new languages need to be added to the `currentLanguage.match(/en|fr/)` section once the JSON files are added?",
        "Just to be clear from what was discussed above, should the URL in the environment be were the SVG is hosted publicly, or should that first be downloaded by the backend and placed in the `static/assets/` directory, or have we decided to just add the SVGs in the static folder by default? Using the public URL directly means I'll need to deal with CORS. ",
        "@kimwnasptd Friendly ping on this in case you missed it, so I know what steps I need to take to get this resolved. ",
        "Cool! This shouldn't take long to implement then."
      ],
      "kubeflow-validate-inputs-explicitly": [
        "Does adding error logic make sense here? The frontend doesn't rely on user input so unless there is a MITM change to the request a wrong input cannot happen. Also `none` cannot happen, as one of the toggles is always selected. It's not as if the JWA backend is decoupled from the frontend (they exist in the same container and are built together). \r\nRelated to this, how would I go about causing a 400 error specifically in the backend code?",
        "Should I handle the error with `log.error`?",
        "Thanks for the link. Sorry for wasting your time on this, I should have looked at the code a bit better and I would have found it. "
      ],
      "kubeflow-externalize-configuration-parameters": [
        "While not directly relevant to this document, this very situation has been the cause of issues in the past so I do think this should be stated somewhere. I agree, it is a bit out of place here. I'll leave it in here for the moment until while I think of what to do with it. If needed, it can quickly be removed before merging. \r\n\r\nI'm thinking a `Best practices and considerations` section might be on option (in this README, as well as on the website, which I'm guessing will be mostly copy/paste from here with some formatting changes). "
      ],
      "kubeflow-manage-configuration-changes": [
        "In the line above the base-href is `/tensorboard/` instead of `/tensorboards/` which it is here. ",
        "There is an inconsistency in the usage of `--base-href /tensorboard/ --deploy-url /tensorboard/\"` on Line 6 and `--base-href /tensorboards/` on Line 7."
      ],
      "kubeflow-standardize-network-tools": [
        "@nrchakradhar Good point. These same entrypoint is being used by the Jupyter-Web-App. Do you have suggestions how to make this IPv6 compatible?"
      ],
      "kubeflow-use-snake-case-in-python": [
        "My bad, this was not my intention."
      ],
      "kubeflow-control-header-modification": [
        "I believe if we change the method to `add` instead of `set` the header only gets added if it is not already present. ",
        "If this is a security issue, I think https://github.com/kubeflow/kubeflow/issues/5588 is a much larger problem. ",
        "@kimwnasptd I don't think this is a big problem, as the rest of the virtual service is not touched. So all the user can do is mess up the requests going to the service he already has access to, I don't think this would actually enable somebody to be able to access a notebook in another namespace (unlike the issue linked above). But I am all about learning so I'd like to know the actual dangers/problems this could have. \r\n\r\nDon't forget, users are already able to can already create any virtual service spec they want in their namespace. If there is a security issue with any of this, allowing users to create or edit virtual services would be the actual issue. ",
        "@kimwnasptd To continue on your last comment below, there are a large amount of header configurations that can be used with RStudio. `X-RStudio-Root-Path` is one of course, `X-RStudio-Request` is another. However, much more can be set with headers: https://docs.rstudio.com/connect/admin/appendix/configuration/#ProxyAuth.Settings. I could be mistaken, but I thought you could also configure or use custom headers with JupyterLab as well. I will need to look at it further tomorrow morning, however, I'm sure there are other applications for which setting headers to configure them is supported and would be useful. I think with the URI rewriting, the configurable request headers, and allowing to set the application port (not included here but I do have a PR open for this), the notebook controller will be compatible with most web-applications a user could want to deploy. \r\n\r\nHowever, seeing as the issue described here is an issue that is already present, I do not think this is a blocker. Fixing the underlying issue will be a larger task and not something that will be done before the 1.3 release. ",
        "I added a blacklist, so I believe this issue is now resolved.",
        "@kimwnasptd A white-list rather than a blacklist would indeed be a possibility. However, by backing this into the controller administrators cannot easily customize these values. Also, using `X-*` for headers seems to have been deprecated int 2012: see https://tools.ietf.org/html/rfc6648. There might be a way to make this configurable, but I'm not sure this can be done on time. \r\n\r\nRegarding your last comment, are you suggesting that the current mechanisms added to the controller stay as they are, but the controller itself adds the annotations to the CR when `notebooks.kubeflow.org/server-type: rstudio` is present? Because I don't think this would work, as the controller will forcefully change the annotations if they are edited. I think what you are proposing, is essentially already what is being done except for that it is done in the frontend/backend rather than the controller. Users even looking into this feature are probably already aware, or capable of, figuring out how to change the virtual service spec. ",
        "@kimwnasptd This is now implemented, as well as a whitelist instead of a blacklist"
      ],
      "kubeflow-document-networking-annotations": [
        "How about:\r\n```yaml\r\n# The annotation `notebooks.kubeflow.org/http-rewrite-uri: /`\r\n# is applied to notebook in this group, configuring\r\n# the Istio rewrite for containers that host their web UI at `/`\r\n```",
        "How about:\r\n```yaml\r\n# The annotation `notebooks.kubeflow.org/http-rewrite-uri: /`\r\n# is applied to notebook in this group, configuring\r\n# the Istio rewrite for containers that host their web UI at `/`\r\n# The annotation `notebooks.kubeflow.org/http-headers-request-set`\r\n# is applied to notebook in this group, configuring Istio\r\n# to add the `X-RStudio-Root-Path` header to requests\r\n```"
      ],
      "kubeflow-go-export-naming-conventions": [
        "How do you want it to return only the bool?"
      ],
      "kubeflow-unique-workflow-step-names": [
        "Otherwise there are 2 steps in the workflow (this workflow builds 2 images) that are both named `kaniko-build-push`. I wasn't sure if it would like that so I tested it and Argo does not :). ",
        "The same thing is done here BTW: https://github.com/kubeflow/kubeflow/pull/5626/files#diff-a58831ea15b3ba92fe93b2196dc2867189dc87c3836c617833dbdf652ad22fd0R205",
        "Here you can see what the workflow I just linked looks like when it runs. \r\nhttps://argo.kubeflow-testing.com/workflows/kubeflow-test-infra/kubeflow-kubeflow-presubmit-nb-j-pt-5626-5feb726-6224-63fc?tab=workflow&nodeId=kubeflow-kubeflow-presubmit-nb-j-pt-5626-5feb726-6224-63fc-2134638194",
        "I'm not sure it makes sense for these CI files, as I am assuming they will be extended with more tests similar to https://github.com/kubeflow/kubeflow/blob/master/py/kubeflow/kubeflow/ci/common_ui_tests.py",
        "@kimwnasptd What is your thought on this?",
        "@kimwnasptd I'm also not completely clear what the proposed implementation would look like. Either way, I'm not sure if this is a blocker as it testing if images can be built seems like something that should be added sooner rather than later. This way we would have discovered that the admission-webhook was broken before https://github.com/kubeflow/kubeflow/pull/5661.\r\nAlso, https://github.com/kubeflow/kubeflow/issues/5482#issuecomment-791906596 still needs to be resolved. Should the registry be called `access-management` or should it be `kfam`?",
        "I think this can be dealt with in the future when the tests for each component get expanded. For now, I think it is most important images start getting built and the PRs that are being merged for the release are at least tested if they break the image building. ",
        "@kimwnasptd I actually just ran into an issue where using the common `kaniko_builder.py` actually complicates things slightly. For this situation specifically it is easily fixed, however; for the CI testing I think it could just complicate things with little added value. ",
        "I need to build 2 dockerfiles that exist in the same folder. For that, I just added a `second_dockerfile` and `second_destination` to the build function in `kaniko_builder.py`. I also added a small UUID tag for the task name in the workflow in the `create_kaniko_task` function in `workflow_utils.py` so that each kaniko building step has a unique name in the workflow. So for building the images I don't think it is a big deal at this point. ",
        "I think it's better to be consistent in the naming scheme and use the folder name just like every other component. So `access-management`. ",
        "@yanniszark @kimwnasptd I think this discussion should move to https://github.com/kubeflow/kubeflow/issues/5675. Then this PR can be merged and I will update all the registry naming in a new PR depending on the outcome of https://github.com/kubeflow/kubeflow/issues/5675."
      ],
      "kubeflow-environment-aware-configuration-design": [
        "This was very briefly discussed on above. However, the RStudio guidelines for using their trademark are very strict (Jupyter is the complete opposite and definitely not a problem, VSCode is still unknown). For RStudio, the only permitted use that could apply is: \"Your project deploys RStudio products in unmodified form supplied by RStudio, PBC **for internal use only**;\". In other words, EKF can probably not use the RStudio logo (you build your own images anyways, but still), and any vendor that offers Kubeflow-as-a-Service (StatCan, though it might pass as internal for now) or provides a distribution. These uses would all need written approval from RStudio to use their logo in the UI. And VSCode is still unknown at this point. \r\nThis is the reason why I would like to have a solution that doesn't require building a new image, but rather just changing a configmap or something similar. This logo issue is something that will also be faced if the Spawner UI becomes more configurable in the future (the admin can create whatever groups of images he wants, and the logos will need to come from somewhere). ",
        "@kimwnasptd I think I just thought of the solution for this. If we use links for the logos that direct to the SVGs hosted in this repository, we can swap the SVGs for something else if using the logos turns out to be a problem. I believe if this would the propagate to all running installations as well (if the cache of the original SVG is cleared). Does this make sense?",
        "Pulling it from the repo wouldn't solve your first point, but it does blur the distribution and way the logo is implemented in the software a bit. It could even directly link to the original source of the RStudio SVG. Indeed, for airgaped environment this might pose a slight problem. However, I think that people in that situation could build their own image (and might already be doing so as they would have a private registry as well). Also, I don't think the functionality of the JWA would actually break, the button would just be empty. \n\nI think the other problem with this is decision is who is responsible for the decision to include the logos or not and who would be responsible if RStudio isn't happy. If I'm the person responsible since it is my PR that adds their logo, I'm fine with that (and would prefer this actually). If I can be the person responsible for this, I'd go for adding the logos and remove them if there is a complaint. I think an argument can be made that the logo is being used according to the guidelines, if internal use is interpreted as \"the kubeflow/wg-notebooks developers deploying unmodified RStudio binaries supplied by RStudio to test their functionality in Kubeflow during development\". ",
        "I only just saw your second comment. Personally, I would then be more in favor of using the button toggle that is there now, but using an SVG with text in it for VSCode and RStudio. This still makes it rather easy for users to customize if they want the actual logos (which they might be allowed to use in their situation). It also allows us to keep using the Jupyter logo (as we can definitely use that one), and works together with https://github.com/kubeflow/kubeflow/pull/5646#pullrequestreview-614539090 as well (assuming I get this implemented quickly). "
      ],
      "kubeflow-structured-documentation-with-examples": [
        "Yeah, I will look into this. I'm used to scientific writing so tend to default to the passive voice. "
      ]
    }
  },
  "AdriiiPRodri": {
    "repos": [
      "prowler-cloud/prowler"
    ],
    "entries": [
      {
        "slug": "prowler-consistent-environment-variable-naming",
        "title": "Consistent environment variable naming"
      },
      {
        "slug": "prowler-document-dependency-versioning",
        "title": "Document dependency versioning"
      },
      {
        "slug": "prowler-ensure-migration-compatibility",
        "title": "Ensure migration compatibility"
      },
      {
        "slug": "prowler-flexible-ai-model-versions",
        "title": "Flexible AI model versions"
      },
      {
        "slug": "prowler-log-exceptions-with-context",
        "title": "Log exceptions with context"
      },
      {
        "slug": "prowler-meaningful-consistent-naming",
        "title": "Meaningful consistent naming"
      },
      {
        "slug": "prowler-parameterize-configuration-values",
        "title": "Parameterize configuration values"
      },
      {
        "slug": "prowler-pin-github-actions-dependencies",
        "title": "Pin GitHub Actions dependencies"
      },
      {
        "slug": "prowler-prioritize-code-readability",
        "title": "Prioritize code readability"
      },
      {
        "slug": "prowler-secure-authentication-flows",
        "title": "Secure authentication flows"
      },
      {
        "slug": "prowler-service-layer-abstraction",
        "title": "Service layer abstraction"
      },
      {
        "slug": "prowler-specific-exception-handling",
        "title": "Specific exception handling"
      },
      {
        "slug": "prowler-task-signature-methods",
        "title": "Task signature methods"
      },
      {
        "slug": "prowler-test-authentication-dependencies",
        "title": "Test authentication dependencies"
      },
      {
        "slug": "prowler-use-configurable-default-values",
        "title": "Use configurable default values"
      }
    ],
    "comments": {
      "prowler-document-dependency-versioning": [
        "This is because Marshmallow has recently updated to 4.0 and Safety is not compatible with this version so at least for now we need to set the version to lower than 4.0",
        "Same problem, if we upgrade dependencies and Marshmallow goes to 4.0 then our checks will fail due to a compatibility issue"
      ],
      "prowler-parameterize-configuration-values": [
        "Sure",
        "I can apply this change, @jfagoagas what do you think?"
      ],
      "prowler-secure-authentication-flows": [
        "The endpoint from the library itself returns a 404 if the domain doesnâ€™t exist, and a 200 along with the login page if it does. Thereâ€™s no way to prevent enumeration if we always return a 200, the response body will still be different depending on whether the configuration exists or not",
        "An HTML page with a CSRF token the only thing I can think of is to always generate the HTML with a fake CSRF token thatâ€™s indistinguishable from a real one. Thereâ€™s no standard mechanism for this since the responsibility is left to the IdP. If we want to mitigate it on our side, this is the only approach I see as feasible"
      ],
      "prowler-task-signature-methods": [
        "It needs to be .s because we require the output of the previous task. We use .si when we donâ€™t need anything from the previous task (within a chain)",
        "It needs to be .s because we require the output of the previous task. We use .si when we donâ€™t need anything from the previous task (within a chain)"
      ],
      "prowler-service-layer-abstraction": [
        "It hasnâ€™t been changed because I didnâ€™t see the need to explicitly set the content-type. We can talk about this, as itâ€™s possible Iâ€™m missing something or weâ€™re not fully aligned",
        "Thanks, let me know when this is available",
        "This method was created because the API uses a completely different system to manage outputs. While we could replicate whatâ€™s in the CLI, itâ€™s not the most efficient approach for the API. So instead of introducing changes just to mirror the CLI, a new function was created. Regarding the content-type handling, in my tests, I havenâ€™t had to worry about it, files are uploaded correctly, the type is set properly, and everything can be viewed and downloaded without any issues.\r\n\r\nWe can discuss this in more detail and reach an agreement since itâ€™s an important decision"
      ],
      "prowler-prioritize-code-readability": [
        "If you want we can change to this version, it's the same:\r\n\r\n```\r\nklass = GenericCompliance\r\nfor condition, cls in COMPLIANCE_CLASS_MAP.get(provider_type, []):\r\n        if condition(framework_name):\r\n            klass = cls\r\n            break\r\n```",
        "Making this change implies modifying code and having to define the function get_required_permissions in all views, for me the interesting thing would be to change the name to set_required_permissions."
      ],
      "prowler-use-configurable-default-values": [
        "If youâ€™re okay with it, Iâ€™ll continue working on this PR myself, focusing on the DB changes since we canâ€™t pass the tokens through the URL, that approach isnâ€™t secure. So if it sounds good to you, Iâ€™ll take it from here and update the PR accordingly. For now, this PR should have the `no-merge` label",
        "`get_s3_client is` only invoked if you have configured `DJANGO_OUTPUT_S3_AWS_OUTPUT_BUCKET`, which indicates that you want the outputs to be uploaded to S3, then it gives an exception because if a user only configures this it is understood that he is running Prowler in AWS (it is not necessary to set the credentials) or he has made a mistake because he has not set the other variables if he wants access through credentials"
      ],
      "prowler-ensure-migration-compatibility": [
        "Since enum can't have a reverse operation to return the old value I add this:\r\n\r\nreverse_sql=migrations.RunSQL.noop,"
      ],
      "prowler-consistent-environment-variable-naming": [
        "Youâ€™re right to question the naming, in this context, we actually need the private key for signing SAML requests/responses, and the X.509 certificate (not just the public key) so that the identity provider (IdP) can verify those signatures. The certificate includes the public key along with metadata and a CA signature, which is required by SAML specs",
        "We can use whatever name you prefer, itâ€™s called that in other projects. Iâ€™m indicating that itâ€™s public so itâ€™s clear that the value is meant to be public and can be accessed, to make sure you shouldnâ€™t put your private key in that variable",
        "Maybe we didnâ€™t understand each other correctly, Iâ€™m not quite sure I follow why itâ€™s considered confusing. The certificate is public, thatâ€™s its whole purpose by definition. Yes, itâ€™s an X.509, but itâ€™s meant to be publicly accessible"
      ],
      "prowler-pin-github-actions-dependencies": [
        "Due to: https://github.com/xmlsec/python-xmlsec/issues/320, we need to reinstall after the other dependencies and it should be the no-binary version because its need to be compilated in our system",
        "Sure!"
      ],
      "prowler-log-exceptions-with-context": [
        "Thanks! Added",
        "Maybe we can add some information, and remove the `continue`. What do you think?\r\n\r\n```\r\nlogger.warning(f\"Failed to generate output for finding '{finding}': {exc}\")\r\n```",
        "I didn't notice, its ok for me"
      ],
      "prowler-meaningful-consistent-naming": [
        "No, we need a value because the name is mandatory. We can define another value",
        "Ok, we can change it",
        "They use their required_permissions parameter, so the function get_required_permissions does not return anything, maybe the interesting thing would be to change the name of this function to set_required_permissions."
      ],
      "prowler-test-authentication-dependencies": [
        "I checked it at the beginning of the development and it worked and I will check it again to be sure"
      ],
      "prowler-specific-exception-handling": [
        "Good catch, Iâ€™ll push the fix later ",
        "I think that since the user can add this value it is important to verify and track what is happening, I propose a more secure version of this function with more information to detect anomalous cases:\r\n```suggestion\r\n        try:\r\n            decrypted_key = fernet.decrypt(self.api_key)\r\n            return decrypted_key.decode()\r\n        except InvalidToken:\r\n            logger.warning(\"Failed to decrypt API key: invalid token.\")\r\n            return None\r\n        except Exception as e:\r\n            logger.error(f\"Unexpected error while decrypting API key: {e}\")\r\n            return None\r\n```",
        "I set a 500 because at that point, if a ClientError really appears, the error is not on the part of the user, it is something internal to us. In my opinion the errors of type 400 are a problem of configuration or in the request, in short, a user problem. The errors 500 are our errors and should never happen but if they happen in this case it is controlled. We can discuss this and change it as we decide",
        "I will change it but in my opinion even if boto fails it is still not the user's fault and in any case it is the server that has failed. At that point there is no way the user could have caused the crash",
        "What I mean is that at this point all errors are unexpected",
        "Change applied",
        "Maybe we can change the code like this?:\r\n\r\n```suggestion\r\n        try:\r\n            result = result_queue.get(timeout=timeout) or default\r\n            error_result = error_queue.get(timeout=1)\r\n        except queue.Empty:\r\n            result = default\r\n```"
      ],
      "prowler-flexible-ai-model-versions": [
        "Hmm, Iâ€™m not sure, postgres enums arenâ€™t easy to modify, and OpenAI keeps releasing new models, so if we hardcode this in the code or in Postgres, weâ€™ll need to deploy a new version every time a new model comes out. I think this should go in the `.env` file instead",
        "In my opinion, in this case I would leave it as you have it, since this allows us to add new models as they become available without having to release a new version of Prowler, just by updating the .env. What do you think, @vicferpoy? I believe making this field a choice would force us to release a new version or even add migrations every time new models become available"
      ]
    }
  },
  "lucasgomide": {
    "repos": [
      "crewAIInc/crewAI"
    ],
    "entries": [
      {
        "slug": "crewai-clear-ai-component-interfaces",
        "title": "Clear AI component interfaces"
      },
      {
        "slug": "crewai-consistent-configuration-declarations",
        "title": "Consistent configuration declarations"
      },
      {
        "slug": "crewai-default-none-not-empty",
        "title": "Default None not empty"
      },
      {
        "slug": "crewai-explicit-over-implicit",
        "title": "Explicit over implicit"
      },
      {
        "slug": "crewai-fail-securely-by-default",
        "title": "Fail securely by default"
      },
      {
        "slug": "crewai-functional-code-examples",
        "title": "Functional code examples"
      },
      {
        "slug": "crewai-model-agnostic-ai-code",
        "title": "Model-agnostic AI code"
      },
      {
        "slug": "crewai-prefer-pythonic-simplicity",
        "title": "Prefer pythonic simplicity"
      },
      {
        "slug": "crewai-semantic-naming-patterns",
        "title": "Semantic naming patterns"
      },
      {
        "slug": "crewai-structure-errors-with-intent",
        "title": "Structure errors with intent"
      },
      {
        "slug": "crewai-test-behavior-not-calls",
        "title": "Test behavior not calls"
      },
      {
        "slug": "crewai-thread-safety-first",
        "title": "Thread safety first"
      },
      {
        "slug": "crewai-validate-configurations-up-front",
        "title": "Validate configurations up front"
      }
    ],
    "comments": {
      "crewai-validate-configurations-up-front": [
        "You have couple of issues here\n1. If the parameter `api_key` was provided you have to use it even those GOOGLE/GEMINI_API_KEY have been set.\n2. move this to a dedicated method and call it on init method liek \n```python\ndef get_api_key(api_key: str):\n   if is_gemini_model(self.model):\n       return api_key or self._get_gemini_api_key_evn_vars()\n   return api_key\n\nself.api_key = def get_api_key(fallback=api_key):\n```",
        "What about requiring this env var during Agent initialization? Logging a clear error message could make the issue much easier to debug"
      ],
      "crewai-semantic-naming-patterns": [
        "good call, I didn't notice that. I'm going to rename that, tks",
        "done"
      ],
      "crewai-functional-code-examples": [
        "what about adding a section to talk about `required method` likely the BaseAgentAdapter.. just to try keep the same \"pattern\""
      ],
      "crewai-prefer-pythonic-simplicity": [
        "You can drop the `else` since you are throwing an error if `not FOUNDRY_AVAILABLE` ",
        "A few suggestions to make it a little more readable\r\n```suggestion\r\n        try:\r\n            self.parent_flow = None\r\n            for frame_info in stack:\r\n                candidate = frame_info.frame.f_locals.get(\"self\")\r\n                if isinstance(candidate, Flow):\r\n                    self.parent_flow = candidate\r\n                    break\r\n             return self   \r\n        finally:\r\n            del stack\r\n```"
      ],
      "crewai-structure-errors-with-intent": [
        "what about use the same `error` variable in the event error and exception message?\r\n\r\n```python\r\nerror=f\"Task '{task.description}' execution timed out after {self.max_execution_time} seconds. Consider increasing max_execution_time or optimizing the task.\"\r\n\r\nAgentExecutionErrorEvent(..., error=error)\r\nraise TimetouError(error)\r\n```",
        "Actually I just notice that you can just propagate the error message \r\n```python\r\nerror = str(e)\r\n```",
        "Consider moving the `return` to try block. While is valid is not common use finally with return "
      ],
      "crewai-consistent-configuration-declarations": [
        "Youâ€™re saying the supported Python versions are 3.10 and 3.11, but we also fully support 3.12."
      ],
      "crewai-thread-safety-first": [
        "the following code seems a bit too verbose.. maybe we could use a simpler approach with `join + is_alive`, instead of `wait + join + set` \r\n\r\nhere is a suggestion - needs to test better:\r\n\r\n```python\r\n    result = None\r\n    error = None\r\n\r\n    def target():\r\n        try:\r\n            result = self._execute_task_without_timeout(task, context, tools)\r\n        except Exception as e:\r\n            error = e\r\n\r\n    thread = threading.Thread(target=target, daemon=True)\r\n    thread.start()\r\n    thread.join(timeout=timeout)\r\n\r\n    if thread.is_alive():\r\n        self._logger.log(\"warning\", f\"Task timed out after {timeout}s\")\r\n        raise ...\r\n\r\n    if error:\r\n        self._logger.log(\"error\", f\"Task failed: {str(error)}\")\r\n        raise error\r\n\r\n    return result.get(\"output\", \"\")\r\n```"
      ],
      "crewai-default-none-not-empty": [
        "usually it's not recommend set mutable default argument like [], {} because they will be shared across  all instances of the class (i don't think you wanna that) \r\n\r\nso i'd recommend do it instead\r\n```suggestion\r\ndef __init__(self):\r\n        self.original_tools: List[BaseTool] = []\r\n        self.converted_tools: List[Any] = []\r\n```\r\n",
        "[here's an articles about that](https://docs.python-guide.org/writing/gotchas/)",
        "same comment related to use mutable default argument",
        "```suggestion\r\ntools: Optional[List[BaseTool]] = None,\r\n```"
      ],
      "crewai-clear-ai-component-interfaces": [
        "What about `LLMOutputValidator`\r\n",
        "@greysonlalonde I forgot to resolve this conversation before merging. I'm heading to renaming it right now",
        "@gvieira @greysonlalonde \r\nhttps://github.com/crewAIInc/crewAI/pull/2731"
      ],
      "crewai-explicit-over-implicit": [
        "For your use case, I'd recommend encouraging users to set only long_term_memory instead of configuring all the individual memory attributes.",
        "should the `-kn` reset the memory from agents also? Just bring this topic up for discussing here"
      ],
      "crewai-model-agnostic-ai-code": [
        "Following the `langgraph_adapter.py` as a reference, you could define the LLM during initialization like this: llm = llm or self.model",
        "since we are converting all system message to assistant, do you need to touch at this point?\r\ndo you have any docs reference about that?"
      ],
      "crewai-test-behavior-not-calls": [
        "could we improve those tests? I mean you are testing the method will be called but what happens under the hood (that is what is matter) is not beign tested. \r\nCould we use a custom `tool`, add some sleep there?",
        "Yeah you can mock this way, just remember to run with your model API KEY\r\n\r\nI was thinking in add a tool with `result_as_answer=True` like that\r\n\r\n```\r\n@tool(\"what amazing tool\", result_as_answer=True)\r\ndef my_tool(...):\r\n   sleep(5)\r\n   return \"ok\"\r\n\r\nAgent(\r\n  tools[my_tool]\r\n)\r\n```\r\n\r\nI think it will be easier and more explicit to validate that the agent is awaiting a response for more than X seconds",
        "great work!\r\n\r\n\r\nJust add a few test to cover line 122 & 119, pls",
        "The CI tests and type checker are falling. Could u handle that?",
        "@Dev-Khant Your added test still not testing what should be testing.\r\nTo ensure that I removed `hasattr(self.memory, \"llm\")` clause from `mem0_storage` and ran your added test, still passing. \r\n\r\nIt is happens because you added test `test_search_with_llm` is mocking the `user_memory.storage`. \r\n\r\nHere's a few suggestions to fix it:\r\n\r\n1. You have to cover mem0_storage, so add a test into `test_mem0_storage.py`\r\n2. Add test to cover search and save methods \r\n3. Use `unittest.mock.patch` to mock the `memory.save` and `memory.search` calls. Example: `patch(\"mem0.Memory.search\") as mock_method`\r\n4. use `assert_called_once_with` to ensure the propagated parameters. Example: `mock_method.assert_called_once_with(agent_id=, metadata=, ...)`\r\n\r\nLet me know if you need help with those test"
      ],
      "crewai-fail-securely-by-default": [
        "great points!\r\nThe main point is about the \"magic\" that is: \"just say what the guardrail must do\".. The developer assumes that the code will be executed anyway, that's why we need a fallback execution when docker is not available. \r\n\r\nI also share with this kind of execution. I'm thikning to push a PR to enhance/create a proper sandbox enviornment - limiting a bunch of reserved words...\r\n\r\nRegarding to your thoughts, I gonna add more warnings logs and add an attribute `self.unsafe_mode` to the class. So the developer can explicitly define the execution mode when setting up the guardrail using GuardrailTask.\r\n",
        "exactly what I was planning to use haha",
        "[here is](https://github.com/crewAIInc/crewAI-tools/pull/281)"
      ]
    }
  },
  "pohly": {
    "repos": [
      "kubernetes/kubernetes"
    ],
    "entries": [
      {
        "slug": "kubernetes-api-field-documentation",
        "title": "API field documentation"
      },
      {
        "slug": "kubernetes-avoid-unnecessary-work",
        "title": "Avoid unnecessary work"
      },
      {
        "slug": "kubernetes-benchmark-performance-changes",
        "title": "Benchmark performance changes"
      },
      {
        "slug": "kubernetes-clear-field-documentation",
        "title": "Clear field documentation"
      },
      {
        "slug": "kubernetes-comprehensive-test-coverage",
        "title": "Comprehensive test coverage"
      },
      {
        "slug": "kubernetes-feature-gate-configuration-management",
        "title": "Feature gate configuration management"
      },
      {
        "slug": "kubernetes-feature-gate-field-preservation",
        "title": "Feature gate field preservation"
      },
      {
        "slug": "kubernetes-implement-proper-observability",
        "title": "implement proper observability"
      },
      {
        "slug": "kubernetes-minimize-rbac-permissions",
        "title": "minimize RBAC permissions"
      },
      {
        "slug": "kubernetes-optimize-search-algorithms",
        "title": "Optimize search algorithms"
      },
      {
        "slug": "kubernetes-prefer-early-null-returns",
        "title": "Prefer early null returns"
      },
      {
        "slug": "kubernetes-simplify-code-structure",
        "title": "simplify code structure"
      },
      {
        "slug": "kubernetes-use-semantically-clear-names",
        "title": "Use semantically clear names"
      },
      {
        "slug": "kubernetes-wrap-errors-meaningfully",
        "title": "Wrap errors meaningfully"
      }
    ],
    "comments": {
      "kubernetes-benchmark-performance-changes": [
        "Having a variant which measures performance would be useful. It doesn't need to run in the CI, defining it for manual invocation is sufficient.\r\n\r\nI'm genuinely curious: how do results compare between using and not using mixins?"
      ],
      "kubernetes-implement-proper-observability": [
        "Yes, that seems appropriate. There are two event APIs, we should use the same as the rest of the scheduler. See https://kubernetes.slack.com/archives/C20HH14P7/p1740748575144709",
        "> What the EventSource should be here; you used Scheduling it looks like. Do we fire events in other places and what do we use for event source there?\r\n\r\nI have to defer to SIG Scheduling here. I don't know what precedence we have for a scheduler plugin to emit events.\r\n\r\n/cc @dom4ha @sanposhiho \r\n\r\n> Should make the involved object the pod or the claim? Users are more likely to describe the pod, but the claim is more accurate.\r\n\r\nTricky. Quite a while ago I asked similar questions and got as answer that users should know that they need to dig down into the dependency chain. The main reason is that they already need to do that for pods: when a Deployment doesn't become ready, there are no events posted for it that explain why pods are not starting. The user has to describe the pods, not the Deployment. That would be an argument in favor of emitting for the claim.\r\n\r\nBut this is coming from the scheduler, which is processing pods. So pod would also make sense and is arguably easier for a user to find.\r\n\r\n\r\n\r\n"
      ],
      "kubernetes-clear-field-documentation": [
        "\"ResourceClaims\" is the wrong level - it can also be allocated multiple times in the same ResourceClaim, in different requests.\r\n\r\n```suggestion\r\n\t// AllowMultipleAllocations marks whether the device is allowed to be allocated multiple times.\r\n```\r\n\r\nLet's fix the language without going into whether \"multiple times\" is in different claims.\r\n",
        "Also, claims don't allocate. The scheduler allocates, not the claim.\r\n",
        "This comment doesn't add any information, let's at least be more verbose.\r\n```suggestion\r\n\t// The name of the container requesting resources.\r\n```",
        "The doc comment doesn't actually tell me anything about what a `requestMapping` is. This method and the logic around `podScalarResources` remain a mystery to me and completely lack comments in the body which explain what the code does and why.\r\n\r\n `podScalarResources` is document as \"May have extended resource backed by DRA.\" which is very vague. It's apparently a sum of all resources requested anywhere in the pod? At least say that.\r\n\r\nThis looks like an area where we need lots of dedicated unit tests which exercise the different ways how a pod may request resources. For example, I saw some comments in the called code about \"pod level resources\", but no corresponding unit tests. Should those be handled?",
        "The explanation is recursive - one has to know what mixins are to understand the field. How about:\r\n\r\n\"Mixins provides common definitions that can be used for devices and counter sets in the ResourceSlice.\"\r\n"
      ],
      "kubernetes-feature-gate-configuration-management": [
        "But `[FeatureGate:DynamicResourceAllocation]` is just informational and not wrong. It's not used for skipping tests, unless someone uses a non-default SKIP expression. I prefer to keep it.\r\n",
        "OTOH, I can also just replace it with plain text \"Dynamic Resource Allocation\". It doesn't need to be injected via `framework.WithFeatureGate` and (more importantly!) at some point it **cannot** be injected that way because the feature gate will get removed eventually - might as well change the test definition now because the GA feature shouldn't get turned off for testing.\r\n",
        "Removal is pending in https://github.com/kubernetes/kubernetes/pull/132706.\r\n",
        "Please don't add this manually. The file is also used in jobs which want to run just with core DRA.\r\n\r\nThe alpha job will add it for you automatically, so your tests will still run.\r\n",
        "Yes. ResourceClaim status was also enabled by default at beta. Such beta's then don't do much unless the DynamicResourceAllocation feature gate gets enabled, but that's fine.\r\n"
      ],
      "kubernetes-comprehensive-test-coverage": [
        "Wasn't the decision to favor devices without binding conditions? That's not in the code yet, is it?\r\n",
        "I had started writing an integration test for this and then simplified it so that it only uses a single device. You can find the ground work (reusable also for other features) in https://github.com/kubernetes/kubernetes/pull/131869. The Device Binding Conditions test is in https://github.com/pohly/kubernetes/commits/dra-device-binding-conditions/\r\n\r\nI think I outlined in some previous message all the various edge cases that can occur once we consider more complex scenarios (multiple pods referencing the same claim; binding a pod fails after updating the ResourceClaim; and probably more that I don't remember right now). Those are scenarios which need integration tests. A unit test alone is not enough because we might make incorrect assumptions about what the right behavior of the plugin needs to be and what the scheduler then does. Having those tests before merging would be nice, otherwise they are needed before beta and then should be listed in the KEP's test plan.",
        "There's a commented out \"without-binding\" device. My expectation was that the first claim would allocate that, but it ended up getting \"with-binding\" instead.\r\n\r\n",
        "> I wasn't expecting these to be mixed in a single ResourceSlice. As it stands, each slice is evaluated for its own BindingConditions, so this doesn't work for this test.\r\n\r\nWe could make that assumption, as long as we document it properly. It might even make sense if it keeps the implementation simpler - I need to look at that.\r\n\r\n",
        "Please cover the following scenarios:\r\n- one container, one extended resource\r\n- one container, three different extended resources\r\n- three containers, one extended resource each\r\n- three containers, with one/two/three resources.\r\n\r\nThis is relevant for the `listMap` handling of `RequestMapping`. Needs to be an E2E test to verify the full flow, including kubelet. The test must verify that each container gets the right devices injected. I don't remember whether we already inject some env variable representing the device name. If not, then you can add that in the CDI file generated by the test/e2e/dra/test-driver, for example by setting `DRA_DEVICE_<driver name>_<pool name>_<device name>=1`. Then you can check the allocation result, determine which env variables should be set, and check them. Make sure that no unexpected env variables are set.\r\n",
        "Unit tests for this, please. Keep this comment open until the new unit tests have been reviewed.\r\n\r\n",
        ":+1:, but see https://github.com/kubernetes/kubernetes/pull/130653/files#r2219055313."
      ],
      "kubernetes-feature-gate-field-preservation": [
        "Yes, always keep the fields if used anywhere in the old object. Think of adding a label to a ResourceClaim: that should not remove existing fields.\r\n",
        "Don't drop the fields if the old slice already had them. Otherwise adding a label to an existing slice would wipe out the fields.\r\n\r\nI'm torn about checking both features here. On the one hand, we don't want to allow setting fields that the cluster is not configured to support, so we have to check both. On the other hand the fields are said to be feature-gated only by DRADeviceBindingConditions.\r\n\r\nI'm leaning towards updating the field comments to:\r\n```\r\n\t// This is an alpha field and requires enabling the DRADeviceBindingConditions and DRAResourceClaimDeviceStatus\r\n\t// feature gates.\r\n\t//\r\n\t// +optional\r\n\t// +featureGate=DRADeviceBindingConditions,DRAResourceClaimDeviceStatus\r\n```\r\n\r\nBut I need to ask around regarding this, it is unusual.",
        "Jordan confirmed on Slack that the comment that I had proposed above is correct. Please add it.\r\n",
        "Feature gated fields must be dropped if the feature gate is disabled and the field is not set already.\r\n\r\nSee https://github.com/kubernetes/kubernetes/blob/9822e51403e46cc0aeae49cf64eae653eff29f9e/pkg/registry/resource/resourceclaim/strategy.go#L212-L345 for how that is done for ResourceClaim. You need to add similar code to https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/resource/deviceclass/strategy.go and to https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/core/pod/strategy.go for `ExtendedResourceClaimStatus` - currently both is missing."
      ],
      "kubernetes-prefer-early-null-returns": [
        "```suggestion\r\n\tif len(bindingConditions) == 0 && len(bindingFailureConditions) > 0 {\r\n```\r\n\r\nThis avoids the assumption that a non-nil slice is automatically non-empty. It also shows the intent more clearly.\r\n\r\n"
      ],
      "kubernetes-simplify-code-structure": [
        "Let's replace with a function `extendedResourceName(index int)`. Then you can replace some code below with a for loop instead of using cut-and-paste.\r\n",
        "```suggestion\r\n\t\t\tpod.Spec.Containers[2].Name = \"container2\"\r\n```\r\n\r\nBut really, please use a for loop instead to avoid such cut-and-paste errors. Same applies to several other places above and below.\r\n\r\n\r\n"
      ],
      "kubernetes-use-semantically-clear-names": [
        "@sunya-ch: please go through all opens in the KEP discussion and copy each into a separate thread attached to an appropriate line.",
        "Not resolved yet.\r\n",
        "`hasDeviceBindingStatus` could be anything. Perhaps rename to `isPodReadyForBinding`, which is symmetric with `isClaimReadyForBinding`?\r\n\r\n",
        "Please, *document* your methods and avoid appending just `Extended`. That's way to generic.\r\n\r\nBut in this case you don't even need these methods. The caller can instead do:\r\n```\r\nb := drautils.NewBuilder(f, driver)\r\nb.extended = true\r\n...\r\n```\r\n\r\nThen when `b.setUp` runs, it uses the non-default `b.extended`.",
        "\"extended\" how?\r\n\r\nPlease, document code that you are adding and try to pick descriptive names.\r\n\r\nHow about `extended` -> `useExtendedResourceName` and `extendedMultiNodes` -> `simulateDevicePlugin`?\r\n\r\n\r\n"
      ],
      "kubernetes-wrap-errors-meaningfully": [
        "```suggestion\r\n\t\t// Returning an error here causes another scheduling attempt.\r\n\t\t// In that next attempt, PreFilter will detect the timeout or\r\n\t\t// error and try to recover.\r\n\t\treturn statusError(logger, err)\r\n```"
      ],
      "kubernetes-avoid-unnecessary-work": [
        "This gets emitted every five second (polling interval in wait.PollUntilContextTimeout). That seems a bit excessive.\r\n\r\nWouldn't it be enough to emit once, when starting to wait?",
        "```suggestion\r\n\r\n\t\t\t// Performance optimization: skip the for loop if the feature is off.\r\n\t\t\t// Not needed for correctness because if the feature is off, the selected\r\n\t\t\t// device should not have binding conditions.\r\n\t\t\tif a.features.DeviceBinding {\r\n```",
        "Is this something that we can check once when gathering pools?\r\n\r\nJust an idea for a future optimization. As usual we first have to have it working, add a benchmark, then test whether it's worth it.\r\n",
        "Or do what I did originally and expand mixins when converting to the internal representation? Did you discard that approach out of concerns about memory consumption (probably higher than when keeping this disaggregated)?\r\n\r\nOne advantage of the conversion approach is that it's much easier to review because all support for mixins is in one place. We could start with that approach, ensure that we have a large number of unit tests, then do something more complicated.",
        "Some handling of mixins is done here in all cases, some more inside `selectorsMatch`. The second part seems to be same work regardless of whether selectors from the class or from the request are checked. Was the motivation to only do more expensive work on demand?\r\n\r\nThen perhaps go one step further and do both (i.e. also this lookup of mixins here) only when `selectorsMatch` really needs it? You could create a helper struct which contains `device.MixinRefs`, `slice.Spec.Mixins.Device`, a field to cache the result, and then has a helper function which computes the result or returns the cached value. Pass that to `selectorsMatch` and then the overall work will only be done once.\r\n\r\nThat's also a bit easier to review because it's all in one place.\r\n\r\n"
      ],
      "kubernetes-api-field-documentation": [
        "So the question is about the content of `CapacitySharingPolicy`? We can make `CapacitySharingPolicy.Default resource.Quantity` (currently required) a pointer and optional and mutually exclusive with a new `CapacitySharingPolicy.Name *string`, but then we also should not allow the other fields. I think this interdependency between fields is frowned upon.\r\n\r\nA better approach would be to add an alternative to `SharingPolicy *CapacitySharingPolicy` here, something like `SharingPolicyRef *string` or `SharingPolicyName *string`. But naming is hard: the API convention is that `*Ref` is for a full-blown reference to some external resource, and `*Name` is for resources of a certain type, referenced by their name. I don't have a good proposal.\r\n\r\n",
        "I'm okay with\r\n```\r\ndevices:\r\n  requests: # for devices\r\n  ...\r\n    resources:\r\n      requests: # for resources which must be provided by those devices\r\n ```\r\n \r\n I was worried about potential confusion when explaining the API, but that can be addressed by clearly distinguishing which kind of requests are meant.\r\n",
        "From https://github.com/kubernetes/kubernetes/pull/130160#issuecomment-2664589113:\r\n\r\n> Transferring AllocatedDeviceStatus from the allocator to the scheduler plugin\r\n\r\nLooking at how this is implemented in the allocator I started to wonder whether it really makes sense to add these fields here and not in the AllocationResult.\r\n\r\n* The AllocationResult is immutable. The AllocatedDeviceStatus isn't. I think we want these fields to be immutable.\r\n* Conceptually, they are set once during allocation.\r\n\r\n=> I think these fields belong into `DeviceRequestAllocationResult`.\r\n\r\nThis will also keep the allocator API simple.",
        "I was reminded that `metav1.Duration` is not suitable for REST APIs because of the encoding/decoding overhead and because non-Go clients may have problems parsing a the string, so it has to be `something<Unit>` after all:\r\n\r\n```suggestion\r\n\tBindingTimeoutSeconds *int64\r\n```\r\n\r\n",
        "I don't remember: do we really need this in the status? Who acts on it there?\r\n\r\nIf no, remove it?\r\nIf yes, add +featureGate.\r\n",
        "But does the composable DRA driver need to read the claim.status.allocation.usageRestrictedToNode field for that? I don't think so.\r\n\r\nIt knows that it needs to do something because of the binding conditions that it has to modify. It knows the node name from the node selector. That's all that it needs.\r\n",
        "Worded differently: UsageRestrictedToNode is needed in the spec and needs to be handled by the scheduler. It is not needed in the status.\r\n"
      ],
      "kubernetes-optimize-search-algorithms": [
        "What we really want is \"order all devices in the pool\" according to the triplet `binding conditions yes/no, name of slice, index within slice`.\r\n\r\nThe \"name of the slice\" is just there to ensure that we don't compare device indices from different slices, which would be meaningless. The name is random, so driver's cannot rely on that to prefer some devices over others.\r\n\r\nIf a driver does not use binding conditions, then this is equivalent to the current \"look at random slices, iterate over devices in them in the order chosen by the DRA driver\".\r\n\r\nIf a driver uses binding conditions, then it can publish some devices with binding conditions in one slice and others without binding conditions in another and it will work as intended. The sorting in the allocator wouldn't be necessary if all devices were in the same slice, but we cannot assume that.\r\n\r\n",
        "Maintaining such a list is a change of how the allocator currently iterates over devices and also adds overhead. So as a less intrusive change, sorting slices based on \"some device uses binding conditions\" and not sorting within the slice should be sufficient.\r\n\r\nIt's just an approximation, but DRA driver authors can make it work for them by not mixing devices with and without binding conditions in the same slice and if they need to do that, use only a single slice with devices using binding conditions come last.\r\n\r\nI had violated that constraint in https://github.com/kubernetes/kubernetes/pull/130160#discussion_r2101956530 and didn't get the expected outcome. But if we clearly document this for driver authors, they can avoid such a mistake, so I am fine with the original implementation.",
        "```suggestion\r\n\t\t\t\t\tif device.Name == internal.id.Device {\r\n\t\t\t\t\t\tallocationResult.Devices.Results[i].BindingConditions = device.Basic.BindingConditions\r\n\t\t\t\t\t\tallocationResult.Devices.Results[i].BindingFailureConditions = device.Basic.BindingFailureConditions\r\n\t\t\t\t\t\tallocationResult.Devices.Results[i].BindingTimeoutSeconds = device.Basic.BindingTimeoutSeconds\r\n\t\t\t\t\t\tbreak\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n```\r\n\r\nOnce we have the device, we don't need to keep searching, do we?"
      ],
      "kubernetes-minimize-rbac-permissions": [
        "Fixed with a cleanup commit \"DRA RBAC: fix kube-scheduler bootstrap policy\", then updating the following \"DRA: graduate DynamicResourceAllocation feature to GA\" commit accordingly.\r\n"
      ]
    }
  },
  "tpowell-progress": {
    "repos": [
      "chef/chef"
    ],
    "entries": [
      {
        "slug": "chef-choose-semantic-algorithms",
        "title": "Choose semantic algorithms"
      },
      {
        "slug": "chef-document-configuration-decisions",
        "title": "Document configuration decisions"
      },
      {
        "slug": "chef-document-with-examples",
        "title": "Document with examples"
      },
      {
        "slug": "chef-explicit-configuration-over-implicit",
        "title": "Explicit configuration over implicit"
      },
      {
        "slug": "chef-externalize-configuration-values",
        "title": "Externalize configuration values"
      },
      {
        "slug": "chef-extract-and-organize-methods",
        "title": "Extract and organize methods"
      },
      {
        "slug": "chef-fail-fast-principle",
        "title": "Fail fast principle"
      },
      {
        "slug": "chef-fail-gracefully-always",
        "title": "Fail gracefully always"
      },
      {
        "slug": "chef-memoize-expensive-operations",
        "title": "Memoize expensive operations"
      },
      {
        "slug": "chef-prefer-guard-clauses",
        "title": "Prefer guard clauses"
      },
      {
        "slug": "chef-secure-network-operations",
        "title": "Secure network operations"
      },
      {
        "slug": "chef-structure-cicd-scripts",
        "title": "Structure CI/CD scripts"
      },
      {
        "slug": "chef-verify-automated-documentation",
        "title": "Verify automated documentation"
      }
    ],
    "comments": {
      "chef-extract-and-organize-methods": [
        "Can we extract these two options into their own methods? Maybe something like `inline_powershell_wrapper...` and `shell_out_wrapper...`? The method is a bit long to start with, so the if/else is harder to read due to the length.",
        "Thinking this can potentially be deduplicated by some sort of templating or builder logic so that the behavior stays consistent between them.  h/t: @dafyddcrosby "
      ],
      "chef-verify-automated-documentation": [
        "should be `commingle`"
      ],
      "chef-structure-cicd-scripts": [
        "```suggestion\r\nfor platform in ${omnibus_build_platforms[@]}; do\r\n  if [[ $platform != *\"windows\"* ]]; then\r\n    cat <<- NONWINDOWS\r\n- label: \":hammer_and_wrench::docker: $platform\"\r\n  retry:\r\n    automatic:\r\n      limit: 1\r\n  key: build-$platform\r\n  agents:\r\n    queue: default-privileged\r\n  plugins:\r\n  - docker#v3.5.0:\r\n      image: chefes/omnibus-toolchain-$platform:$OMNIBUS_TOOLCHAIN_VERSION\r\n      privileged: true\r\n      propagate-environment: true\r\n      environment:\r\n        - RPM_SIGNING_KEY\r\n        - CHEF_FOUNDATION_VERSION\r\n  commands:\r\n    - ./.expeditor/scripts/omnibus_chef_build.sh\r\n  timeout_in_minutes: 60\r\nNONWINDOWS\r\n  else\r\n    cat <<- WINDOWS\r\n- label: \":hammer_and_wrench::windows: $platform\"\r\n  retry:\r\n    automatic:\r\n      limit: 1\r\n  key: build-$platform\r\n  agents:\r\n    queue: default-$platform-privileged\r\n  plugins:\r\n  - docker#v3.5.0:\r\n      image: chefes/omnibus-toolchain-$platform:$OMNIBUS_TOOLCHAIN_VERSION\r\n      shell:\r\n      - powershell\r\n      - \"-Command\"\r\n      propagate-environment: true\r\n      environment:\r\n        - CHEF_FOUNDATION_VERSION\r\n        - BUILDKITE_AGENT_ACCESS_TOKEN\r\n        - AWS_ACCESS_KEY_ID\r\n        - AWS_SECRET_ACCESS_KEY\r\n        - AWS_SESSION_TOKEN\r\n      volumes:\r\n        - \"c:\\\\buildkite-agent:c:\\\\buildkite-agent\"\r\n  commands:\r\n    - ./.expeditor/scripts/omnibus_chef_build.ps1\r\n  timeout_in_minutes: 60\r\nWINDOWS\r\n  fi\r\ndone\r\n```"
      ],
      "chef-externalize-configuration-values": [
        "Feels like these env vars should be set in Buildkite itself.",
        "We don't need to be pointing at a raw endpoint in the code. It's likely unstable, and we probably don't want to expose it, regardless. Needs to be injected via secrets or env just to not have to have a pull request (that will have failing pipelines due to this value being stale) to update this value.",
        "Secrets / env value instead of statically defined in code.\r\n",
        "Secrets / env value instead of statically defined in code.\r\n"
      ],
      "chef-memoize-expensive-operations": [
        "@jaymzjulian ok, one thing I'm not understanding in the current context is... is there ever an `(array length > 1)` scenario, and if so, what happens then?"
      ],
      "chef-document-with-examples": [
        "This feels like it needs more documentation.",
        "`, description: \"describe the property here\"` (see `lib/chef/resource.rb` for this value and also `introduced: \"...\"`)\r\n\r\nAlso add an example above using it.",
        "@IanMadd the apt package manager treats any specified package name as a regex, so we definitely need to be able to describe a regular expression in the documentation, and without the visual of the anchors, I don't think it's going to be as obvious what this setting accomplishes."
      ],
      "chef-choose-semantic-algorithms": [
        "This is potentially problematic and not necessarily deterministic in that it will likely pick the most recent install vs. probably the intended \"most recent version\" for this logic. Further, although unlikely, any mtime update of the base package directory will place that package at the front of the line for being chosen, so if you renamed the directory or something for a test, you might end up forcing that version to always be chosen?",
        "@jaymzh I'll raise you a `Gem::Dependency.new('', '< 1.4.0').match?('', get_choco_version)`",
        "If we're just comparing two *specific* gem versions, `Gem::Version` sounds good to me. If we're looking to have more general version specifiers (`~>`, etc...), it might still work, but we might want to consider how the user would expect things to work in context.",
        "Suggest splitting on the `':'` and grabbing the first `n` fields rejoined with `':'` ... It's a little bit harder to sort out how many fields are being grabbed with the regex especially with most of this under the capture group, and the regex isn't performing significant validation, etc..."
      ],
      "chef-document-configuration-decisions": [
        "Done.",
        "Done."
      ],
      "chef-explicit-configuration-over-implicit": [
        "Per our discussion, this is actually being used directly by the Chocolatey `install.ps1` script and the environment variable can be used to redirect the standard installer to an alternate location.",
        "@johnmccrae can we made the description more clear that it's the choco installer itself looking at these variables?"
      ],
      "chef-fail-gracefully-always": [
        "Looks like [`open_filename`](https://github.com/chef/ffi-libarchive/blob/545d3948e7a78835da48737c1490516905fad2e6/lib/ffi-libarchive/writer.rb#L5) takes a block, and so this would be cleaner and automatically `ensure` closure if these calls were converted to that pattern.",
        "Go ahead and remove the parens and comment that we want an error if only one of (proxy_user, proxy_password) are true."
      ],
      "chef-prefer-guard-clauses": [
        "```suggestion\r\n      return if current_record.nil?\r\n      \r\n      current_record.status = :skipped\r\n      current_record.conditional = conditional\r\n```",
        "Use a guard instead of `unless` block."
      ],
      "chef-secure-network-operations": [
        "```suggestion\r\n          # note that Invoke-Expression is being called on the downloaded script (outer parens), not triggering the script download (inner parens)\r\n          powershell_exec(\"Invoke-Expression ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\").error!\r\n```"
      ],
      "chef-fail-fast-principle": [
        "shouldn't you be able to just match on leading `'validate/'`"
      ]
    }
  },
  "beorn7": {
    "repos": [
      "prometheus/prometheus"
    ],
    "entries": [
      {
        "slug": "prometheus-avoid-subjective-language",
        "title": "avoid subjective language"
      },
      {
        "slug": "prometheus-configuration-mutual-exclusivity-validation",
        "title": "Configuration mutual exclusivity validation"
      },
      {
        "slug": "prometheus-consistent-descriptive-naming",
        "title": "Consistent descriptive naming"
      },
      {
        "slug": "prometheus-consistent-parameter-naming",
        "title": "consistent parameter naming"
      },
      {
        "slug": "prometheus-document-test-tool-options",
        "title": "document test tool options"
      },
      {
        "slug": "prometheus-ensure-mathematical-correctness",
        "title": "ensure mathematical correctness"
      },
      {
        "slug": "prometheus-explicit-test-assertions",
        "title": "explicit test assertions"
      },
      {
        "slug": "prometheus-improve-code-readability",
        "title": "improve code readability"
      },
      {
        "slug": "prometheus-maintain-api-consistency",
        "title": "maintain API consistency"
      },
      {
        "slug": "prometheus-metrics-registration-lifecycle",
        "title": "metrics registration lifecycle"
      },
      {
        "slug": "prometheus-simplify-complex-algorithms",
        "title": "simplify complex algorithms"
      },
      {
        "slug": "prometheus-special-value-handling",
        "title": "special value handling"
      },
      {
        "slug": "prometheus-test-practical-monitoring-scenarios",
        "title": "test practical monitoring scenarios"
      },
      {
        "slug": "prometheus-write-meaningful-documentation",
        "title": "Write meaningful documentation"
      }
    ],
    "comments": {
      "prometheus-document-test-tool-options": [
        "I feel we need a bit more explanation here. How about the following:\r\n\r\n```suggestion\r\nThe `--mode` flag controls how expectations are migrated:\r\n- `strict`: Strictly migrates all expectations to the new syntax.\r\n  This is probably more verbose than intended because the old syntax\r\n  implied many constraints that are often not needed.\r\n- `basic`: Like `strict` but never creates `no_info` and `no_warn`\r\n  expectations. This can be a good starting point to manually add \r\n  `no_info` and `no_warn` expectations and/or remove `info` and \r\n  `warn` expectations as needed.\r\n- `tolerant`: Only creates `expect fail` and `expect ordered` where\r\n  appropriate. All desired expectations about presence or absence \r\n  of `info` and `warn` have to be added manually.\r\n  \r\nAll three modes create valid passing tests from previously passing tests.\r\n`basic` and `tolerant` just test fewer expectations than the previous tests.\r\n```"
      ],
      "prometheus-write-meaningful-documentation": [
        "```suggestion\r\n\t// Register remote storage metrics in custom registry.\r\n```",
        "But maybe this comment is not needed at all. It just spells out what the code is clearly doing. No reason to explain the code here.",
        "I think this is the right approach in general, but let's be a bit more compact here. (We don't need to mention that this is for promtool, and neither that we are talking to Prometheus, and we don't have to say \"please\" here, this is supposed to be short command line help.) And no need for additional line breaks here. How about this:\r\n```suggestion\r\nconst httpConfigFileDescription = \"HTTP client configuration file, see details at https://prometheus.io/docs/prometheus/latest/configuration/promtool\"\r\n```",
        "This should have a doc comment, explaining what it is doing and what for it is needed.",
        "Nit: `mode` should be just a boolean, e.g. `strictConversion bool` (unless we introduce a third mode (see below) in which case this should be an enum (or whatever Go calls an enumâ€¦)).",
        "Now that this is exported, better give it a meaningful doc comment. I would probably recycle most of the stuff that was used for `NewHistogramStatsIterator` below and make the latter much shorter.",
        "While I like readable comments in proper English, this one is a bit too verbose.\r\n\r\nA general good advice is that code should not be an English version of the Go code. You can assume that the reader of your code knows Go well (maybe even better than English ;).\r\n\r\nIf you move the code below into a function `rules.ParseFiles`, your comment here can be its doc comment, but it should still focus more on _what_ the function does rather than _how_.",
        "I think this is obscure enough to deserve a doc comment.\r\n\r\nCurrently, I don't understand the logic that checks `ts`. If it doesn't match, the method returns 0, but how does that help me? 0 could as well be the actual value â€“ how do I know that the `ts` did not match?"
      ],
      "prometheus-avoid-subjective-language": [
        "Typo `additinal`. And maybe let's avoid the word `keyword` which has many technical implementation. We avoided it when documenting `start()` and `end()`, too. Following the wording of the latter, maybe write the following:\r\n\r\n```suggestion\r\nAdditionally, `step()` can be used in duration expressions. For a range\r\nquery, it resolves to the step width of the range query. For an instant query, it resolves to `0s`. \r\n```\r\n\r\n",
        "Nit: It is confusing to say that an \"info level annotation\" is \"warning\" about something, as there is also a \"warning level annotation\".\r\n\r\n```suggestion\r\n* If a native histogram with standard exponential buckets has `NaN`\r\n  observations and the quantile falls into one of the existing exponential\r\n  buckets, the result is skewed towards higher values due to `NaN`\r\n  observations treated as `+Inf`. This is flagged with an info level\r\n  annotation.\r\n* If a native histogram with standard exponential buckets has `NaN`\r\n  observations and the quantile falls above all of the existing exponential\r\n  buckets, `NaN` is returned. This is flagged with an info level annotation.\r\n```\r\n",
        "I think we should word this a bit less compact for easier readability and also mention the zero bucket. How about:\r\n\r\n\"`histogram_stddev(v instant-vector)` returns the estimated standard deviation of observations for each histogram sample in `v`, assuming that each observation had a value equal to the mean value of the boundaries of its bucket. For the zero buckets and for buckets with custom boundaries, the arithmetic mean is used. For the usual exponential buckets, the geometric mean is used. Float samples are ignored and do not show up in the returned vector.\"",
        "Maybe even better:\r\n\r\n\"`histogram_stddev(v instant-vector)` returns the estimated standard deviation of observations for each histogram sample in `v`. For this estimation, all observations in a bucket are assumed to\r\nhave the value of the mean of the bucket boundaries. For the zero bucket and for buckets with custom boundaries, the arithmetic mean is used. For the usual exponential buckets, the geometric mean is used. Float samples are ignored and do not show up in the returned vector.\""
      ],
      "prometheus-maintain-api-consistency": [
        "Again, it feels weird to expose a non-exported type in an exported method. The additional `AddRaw` also creates a lot of friction for the caller. Can we leave the old API intact (just with the one `Add(error)` method) and do the right thing behind the scenes with type assertions and `error.As`? (Does the latter work with interfaces?)",
        "To stay more in line with the other functions (i.e. deal with the Prometheus types everywhere but offer a conversion functions for Go types), let's simply enable negative durations above in `parseDuration` and instead of `parseGoDuration`, offer a `toDuration` functions similar to the existing `toTime` function."
      ],
      "prometheus-ensure-mathematical-correctness": [
        "To elaborate on my comment:\r\n\r\nThese should all have `sum:...` fields with plausible values.\r\n\r\n`cbd` already has a `sum:...` field, but its value is not plausible. If we assume that the observations are more or less in the middle of each bucket (and around 2.5 for the first bucket and not much above 20 for the +Inf bucket), the sum should be around 170.",
        "Here the last sample in the range coincides with the boundary of the range. Let's change this to have something in between, e.g.\r\n```suggestion\r\neval instant at 54m30s increase(metric[90m])\r\n```\r\n\r\n(Spoiler: The values below change, but somehow the buckets still sum up to the count.)",
        "For the record, here I have replaced the tests with the one from #16825, including enlightening comments. I think this demonstrates the benefits and problems of this POC quite well.\r\n\r\n```suggestion\r\n# Tests to demonstrate how an extrapolation below zero is prevented for both float counters and native histograms.\r\n# (The float counter result and the resulting count of the histogram is the same.)\r\n\r\nload 1m\r\n  metric{type=\"histogram\"} {{schema:0 count:15 sum:25 buckets:[5 10]}} {{schema:0 count:2490 sum:75 buckets:[15 2475]}}x55\r\n  metric{type=\"counter\"} 15 2490x55\r\n\r\n# End of range coincides with sample. Zero point of count is reached within the range.\r\neval instant at 55m increase(metric[90m])\r\n    {type=\"histogram\"} {{count:2490 sum:50.303030303030305 counter_reset_hint:gauge buckets:[15 2475]}}\r\n    {type=\"counter\"} 2490\r\n\r\n# End of range does not coincide with sample. Zero point of count is reached within the range.\r\neval instant at 54m30s increase(metric[90m])\r\n    {type=\"histogram\"} {{count:2512.9166666666665 sum:50.76599326599326 counter_reset_hint:gauge buckets:[15.092592592592593 2497.8240740740744]}}\r\n    {type=\"counter\"} 2512.9166666666665\r\n    \r\n# End of range coincides with sample. Zero point of count is reached outside of (i.e. before) the range.\r\n# This means no change of extrapolation is required for the histogram count (and neither for the float counter),\r\n# however, the 2nd bucket's extrapolation will reach zero within the range. This is corrected here, but at the\r\n# price of an inconsistent histogram (the sum of the buckets is 2488.75 and not 2486.25).\r\neval instant at 55m increase(metric[55m15s])\r\n    {type=\"histogram\"} {{count:2486.25 sum:50.227272727272734 counter_reset_hint:gauge buckets:[13.75 2475]}}\r\n    {type=\"counter\"} 2486.25\r\n\r\n# End of range does not coincide with sample. Zero point of count is reached outside of (i.e. before) the range.\r\n# This means no change of extrapolation is required for the histogram count (and neither for the float counter),\r\n# however, the 2nd bucket's extrapolation will reach zero within the range. This is corrected here, but at the\r\n# price of an inconsistent histogram (the sum of the buckets is 2511.7361111111113 and not 2509.375).\r\neval instant at 54m30s increase(metric[54m45s])\r\n    {type=\"histogram\"} {{count:2509.375 sum:50.69444444444444 counter_reset_hint:gauge buckets:[13.912037037037038 2497.8240740740744]}}\r\n    {type=\"counter\"} 2509.375\r\n```",
        "I think he is.\r\n\r\nAnd it even works because the direct mean calculation yields precise results in these cases. I have added those tests, with a comment that they have no tolerance. Should they trip at some point on some hardware, we know that they might not be tolerant enough.",
        "I'll add avg tests, too.",
        "And I'll add @bboreham's \"never compare floats for exact equality\" rule to the comment."
      ],
      "prometheus-explicit-test-assertions": [
        "Which means we should add an `expect no_info` to the test up in line 487 to demonstrate that normally, there is no info annotation.",
        "```suggestion\r\n\texpect no_warn\r\n\t{path=\"/f\"} {{schema:-53 sum:0.01 count:0.01 custom_values:[5 10] buckets:[0.01]}}\r\n```\r\n\r\nAnd similar to all the other tests based on this same data set. This section is all about when a warning is added and when not, so we should be explicit here about not having a warning for all the cases that do not get a warning annotated.",
        "Since we are explicitly interested in this _not_ creating an annotation, an `expect no_info` would make sense here.",
        "For better comparison with the tests below (where an info annotation appears), let's put an `expect no_info` here. (Remember that the old `eval` behavior was to always assert that there is no annotation whatsoever. The new behavior is more tolerant, but we still want to assert the absence of an annotations where we feel it makes sense.)"
      ],
      "prometheus-consistent-descriptive-naming": [
        "I would just call it `registry`. No reason to export anything in `main.go`, and the word `default` seems out of place as it is the intent to _not_ use the default registry.",
        "This isn't a good name. The function doesn't return a time, but a duration. The relevant difference to the existing `parseDuration` is that the Go `Duration` type is returned, not just the number of seconds since epoch.\r\n\r\nSo maybe `parseGoDuration` would be better?",
        "Maybe let's invert this and go with the more common term \"const\"? E.g. `constParam`. (The \"is\" in there sounds a bit Java-esque to me.)",
        "I'm not a great fan of abbreviations that do not abbreviate much but still create ambiguity (has this to do with historic values?).\r\n\r\nLet's call this `simpleHistogramFunc` or if we want to be short, do `simpleHFunc`.\r\n\r\nMaybe we should also rename the existing `simpleFunc` to `simpleFloatFunc` or `simpleFFunc`.\r\n\r\nMy personal preference would be `simpleHistogramFunc` and `simpleFloatFunc`."
      ],
      "prometheus-metrics-registration-lifecycle": [
        "The good thing with the custom registry is that it doesn't even have a Go collector pre-registered, so you can remove the `if` altogether and register the custom Go collector unconditionally.",
        "You also need to register a `NewProcessCollector`. That's the other collector that is registered with the default registry, see https://pkg.go.dev/github.com/prometheus/client_golang/prometheus#pkg-variables",
        "This will avoid registration errors, but it will also mean that the later created metrics will never get exposed (because they are not part of any registry).\r\n\r\nThe problem here is that two slightly different SD mechanisms want to share the same metrics. We would probably design this differently from scratch, but now that we have this situation, let's keep it for now.\r\n\r\nTo handle this problem, you have to use this pattern: https://pkg.go.dev/github.com/prometheus/client_golang/prometheus#example-AlreadyRegisteredError\r\n\r\nIn this way, you can retrieve the already registered metric from the registry and return it as part of `zookeeperMetrics` instead of the newly created one.",
        "The usage of the `AlreadyRegisteredError` pattern below looks good, but you still need to get rid of the `Once.Do`. Otherwise the old metrics won't be used.",
        "This needs a different comment now, see suggestion below.\r\n\r\n```suggestion\r\n\t// For historical reasons, both ServerSet and Nerve SD share the same zookeeper metrics.\r\n\t// To not cause double registration problems, if both SD mechanisms are instantiated with\r\n\t// the same registry, we are handling the AlreadyRegisteredError accordingly below.\r\n\t// TODO: Consider separate zookeeper metrics for both SD mechanisms in the future.\r\n```",
        "Yes, we should definitely aim for separating those metrics one day.\r\n\r\nUnregistering is a bit of a problem, I guess, once both ServerSet and Nerve SD are used at the same time. Maybe let's not do the unregistering in this case (but leave a comment why that is the case and that we should one day separate the metrics for both SDs).",
        "It's actually not just about the unregistering. `NewReadClient` is called once per RemoteReadConfig, se we'll get a panic here once we have more than one RemoteReadConfig.\r\n\r\nThe metrics are, however, curried with `remote_name` and `url` labels, so we can use this to distinguish them between the different clients. You could use something like `prometheus.WrapRegistererWith(prometheus.Labels{remoteName: name, endpoint: conf.URL.String()}, reg)` and then use local instances of the three metrics without the `remote_name` and `url` labels.\r\n\r\nOr you can do what @ptodev suggested and manage the registration of the three metrics outside of `NewReadClient` and pass in the already registered metrics (maybe in a struct) as a parameter of `NewReadClient`.\r\n\r\nI would think that the former is cleaner and more idiomatic, but it's also more refactoring work.",
        "Maybe here we don't need to worry. We already have metrics being registered in `func NewWriteStorage` with the custom registry and do not bother about unregistering. We only have one storage, so this should be fine for now.",
        "My suspicion is that this will panic if we reload the config. Check how `NewReadClient` is called in a loop in storage/remote/storage.go. This will work fine the first time, but whenever we reload the config, we will try to register the same metrics again.\r\n\r\nThis should be easy to test, just run a Prometheus server with a remote read config and send it a SIGHUP.\r\n\r\nThe remedy would be to un-register the metrics before creating new read clients. This will also help to get rid of metrics belonging to a remote read config that got removed from the config.",
        "Thanks for your efforts (and apologies again for the delay in reviewing).\r\n\r\nThe problem now is that the metrics are always recreated, so we get a counter reset on all counters each time we reload the config. (Which is a side effect of the current code always creating new remote clients when the config is reloaded. Other sub-systems use more logic around this to avoid that.)\r\n\r\nThe code is now tracking the configs (to unregister if a config goes away for good) and the metrics (while the metrics are also tracked in the registry anyway). This seems a lot of effort for limited effect, that even comes with the annoyance of counter resets on config reloads, even if there are no relevant changes.\r\n\r\nI'm currently not sure how to solve this better. I have to think about it and will come back to you with ideas once I find the time."
      ],
      "prometheus-consistent-parameter-naming": [
        "```suggestion\r\n`quantile(Ï†, v)` calculates the Ï†-quantile, the value that ranks at number Ï†*N among\r\n```\r\n\r\nIsn't it better to use the same character consistently? (Some readers might not even know how to read \"Ï†\", so they won't get the connection to \"phi\").",
        "```suggestion\r\n* `quantile(Ï†, v)` (calculate Ï†-quantile (0 â‰¤ Ï† â‰¤ 1) over dimensions)\r\n```\r\n\r\nIsn't it better to use the same character consistently? (Some readers might not even know how to read \"Ï†\", so they won't get the connection to \"phi\")."
      ],
      "prometheus-simplify-complex-algorithms": [
        "I think I understand now what you are up to here, but having two different levels of merging is complex enough so that we need to explain this better here. So there is the exported `Merge` to merge two instances of `Annotations` with each other, but then there is this unexported `merge` to merge one individual annotation with another one.",
        "```suggestion\r\n\tif isCounter && resultHistogram != nil && resultHistogram.Count > 0 {\r\n```\r\n\r\nSum can be negative. That's fine.",
        "Why Sum? If at all, it should be Count.\r\nBut I don't understand the logic of the \"proportion of the rate\" in the first place.\r\n\r\nMaybe I haven't wrapped my head sufficiently far around it. But broadly, I think we have to do something different depending on whether we have clamped the durationToStart above.\r\n\r\nIf `durationToZero > durationToStart`, we only have to manipulate the buckets if they would be extrapolated below zero at `durationToStart`.\r\n\r\nIf `durationToZero == durationToStart`, we have to manipulate all the buckets.\r\n\r\nAfter the manipulation, the manipulated buckets (only) should extrapolate te exactly zero at `durationToStart`.\r\n\r\nI can try to cobble together the equation, but maybe this is already enough to put you on track (or put me on track if I have missed something).",
        "Not quite sure.\r\n\r\nMaking all the schemas match is part of the normal rate calculation anyway. First we find the largest common schema, merge buckets in higher-res histograms to match that schema, and only then we start to do the math on all those histograms that now have the same schema.\r\n\r\nThe extrapolation logic only happens in that second part, so different schemas should not be an issue.",
        "Oooh, this is a good catch. Since we effectively ignore the 1st histogram, if there is a counter reset between the 1st and 2nd, we also allow an incompatible bucket layout between the 1st and 2nd histogram.\r\n\r\nHowever, I think we cannot just take the 2nd histogram as the 1st and go on.\r\n\r\nWe do have a similar situation with floats, just that we don't have incompatible buckets there (obviously). While we effectively ignore the value of the 1st float (if there is a counter reset between the 1st and 2nd float), we _still_ take the 1st float into account when checking for extrapolation below zero. And that's actually important. A counter reset sets the counter to zero at some point (in this case between the 1st and 2nd float), but this also means that the counter was greater than zero _before_ the reset. We have to limit extrapolation only if the extrapolation would go below zero when extrapolating from the value of the _1st_ sample.\r\n\r\nSo we need to do the same for histograms. However, if the bucket layout is incompatible, we cannot take buckets into account.\r\n\r\n\"Luckily\", I had a lot of thoughts about all of this, inspired by your code in this PR, and by now I believe, we should actually only limit extrapolation based on the count, and not on the buckets, so we don't even have to take buckets into account.",
        "After a lot of thinking about this, I ultimately came to the conclusion that we should not manipulate buckets after all. Sorry for laying this false trail. (Your work was not in vein, because I would not have been able to think about it in this way without it.)\r\n\r\nIn short, we must keep histograms consistent. For one, that means that the sum of all buckets is equal to the count. (Unless there are NaN observations, but that's a story for another day.) If we scale individual buckets, we have to adjust the count accordingly. And that's what you are doing here. However, I think that creates more harm than good. An individual bucket with some weird behavior might throw off the extrapolation for the whole histogram. I also think there is value if the count alone behaves as we are used to with counters.\r\n\r\nAt the end of the day, this whole heuristics is only a little tweak to avoid overestimating rates and increases. I think using just the count for that is simple, easy to reason with, and probably just right in most cases. In particular, in the new brave world with created-at timestamps, we will have synthetic samples that are zero everywhere (in the count and sum and all buckets), and we'll then stop extrapolation exactly an that sample, which is an exact and perfect result.\r\n\r\nLong story short, I think this code can all go, and we are all good with the simple code above that does the cut-off based on the count.",
        "Thanks for the drawing. I was about to do that myself, but then stopped myself when I thought I had understood the problem sufficiently. Let's see how far this holds.\r\n\r\nI see a problem with your example: You have put the end of the range at the same time as the 2nd (and last) sample in the range. However, that is generally not the case. The range usually goes on a bit after the last sample. If you modify your example accordingly, I'm pretty sure that you won't arrive at a consistent histogram, i.e. where the sum of the buckets is still the same as the count. (This is the problem that @gen1321  fought with in the current state of the code, where he modified the count value to again match the sum of the buckets.)\r\n\r\nWith your lucky coincidence, the outcome of the `increase` operation is essentially the same as the last point in the range, i.e. `count=75 bucket1=15 bucket2=60`. But if you go on a bit to the right with the slopes you have calculated, you'll run into a deviation between the count and the sum of the buckets.\r\n\r\nBut I think you are up to something here. I guess the following could be a working algorithm:\r\n\r\n1. Calculate the zero point solely based on the count, for all the reasons stated, including your very good added point about the optimization for `histogram_count(rate(foo[1m]))`.\r\n2. At that zero point, we virtually add a zero sample, where count and sum and all buckets are zero.\r\n3. Now we effectively redo the whole calculation including the new zero point but extrapolate from the zero point to the previously determined end point (usually the end of the range).\r\n\r\n(This is just the high level description. The detailed flow has to be different for best efficiency.)\r\n\r\nWDYT?",
        "> I wanted to keep the calculation simpler by not taking into account the right hand side.\r\n\r\nYou have to explain to me how to do that if the right boundary of the range does not coincide with a sample.\r\n\r\n> If the duration to zero is outside the duration to start, an individual bucket may still extrapolate to less than zero because it's slope could be steeper and zero point fall inside the duration to start. I assume you wouldn't insert a zero sample in this case. My algorithm I guess could still work by looking at the duration to zero for each bucket.\r\n\r\nIndeed no zero point if count doesn't extrapolate below zero within the relevant range.\r\n\r\nHowever, it will be hard to manipulate individual buckets and still keep the histogram consistent. See what @gen1321 tried in this PR (which attempts to also manipulate count, but that's what we definitely don't want, for all the reasons stated). \r\n\r\n",
        "I fully agree with @krajorama's description of the first two options. I think both are spot on. I will look into @krajorama's POC ASAP (ideally right now, if nothing happens that distracts me or I faint because of the heat :sweat:).\r\n\r\nIdeally, we find an option now that is likely to stay (so we can declare NH stable). (Alternatively, we do something preliminary now and come back to this once it is the last thing that keeps us from declaring stability. Then we can still decide to change our mind or to make an exception from the stability guarantees (\"Will change extrapolation behavior later\"). Needless to say that I would prefer to not make that exception. But I'm getting ahead of ourselves.)",
        "> my only concern is things like custom schemas, resets with different layout, etc. All of those\r\n> make approaches with bucket manipulation really complex.\r\n\r\nI believe that these problems are (with the current state of things) separate from the extrapolation (fortunately). Currently, if there is a mix of buckets with NHCB and standard exponential buckets, we don't return anything (and warn about it). If there are different NHCB bucket boundaries, we don't return anything either. Finally, if we have different standard exponential buckets, we first convert all involved histograms to the lowest-resolution common schema. In other words: Once the rate calculation kicks in, all histograms have the same bucket schema.",
        "About the counter reset between the first two samples: This is actually easy to deal with because we effectively assume a zero sample at the time of the first sample in that case.\r\n(I guess @krajorama note in the POC is only there to let us know that those special cases aren't handled yet, not that these special cases are problematic.)",
        "> About the counter reset between the first two samples: This is actually easy to deal with because we effectively assume a zero sample at the time of the first sample in that case.\r\n\r\nI have to correct myself. This _is_ actually a problem. While it's true that we assume that right after the 1st sample, everything was reset to zero, we will _not_ stop extrapolation there. In the float case, we take the value of the 1st sample and go down from there. We could still stop with extrapolation if we hit zero from that higher starting point, but we generally do not stop extrapolation at the time of the 1st sample.\r\n\r\nTherefore, if we want to take into account individual buckets, we either have to stop ignoring the bucket layout for the 1st sample, or we have to treat this specific case (counter reset between 1st and 2nd sample AND 1st and 2nd sample have incompatible buckets) specially by not checking for individual buckets in this case.",
        "Not sure if I fully understand. Could you check the commit I have just pushed to see if it is what you are fishing for?",
        "While looking at this, I had another thought how we might want to encapsulate the param handling a bit better. Instead of this local function and some other bells and whistles, how about having an `fParams` interface like this:\r\n\r\n```Go\r\ntype fParams interface  {\r\n\tNext(ts int64) float64\r\n\tMax() float64\r\n\tMin() float64\r\n\tHasAnyNaN() bool\r\n}\r\n```\r\n\r\nThere could be one implementation for the variable param series, and one for the constant parameter. The former would pre-populate internal fields to to be used for `Max`, `Min`, `HasAnyNaN`. The latter would store one float value and return it always (or just do `math.IsNaN` for the const value).\r\n\r\nThen you could do many operations in an efficient way without using separate `if` statements everywhere.\r\n\r\n"
      ],
      "prometheus-configuration-mutual-exclusivity-validation": [
        "The comment hasn't been addressed yet."
      ],
      "prometheus-special-value-handling": [
        "```suggestion\r\n\r\n`quantile` only works with float samples. Histogram samples\r\nin the input vector are ignored, flagged by an info-level annotation.\r\n\r\n`NaN` is considered the smallest possible value.\r\n```\r\n\r\nThis was missing before, but needs to be added anyway. I realized that the `NaN` handling was not only undocumented, but also untested. See https://github.com/prometheus/prometheus/pull/16847 .",
        "Maybe don't give this special fringe case a whole itemized list (with just one item...). Idea to be more compact but add more explanation about the meaning of the returned value>\r\n\r\n```suggestion\r\nSpecial case for native histograms with standard exponential buckets:\r\n`NaN` observations are considered outside of any buckets in this case.  `histogram_fraction(-Inf, +Inf, b)` effectively\r\nreturns the fraction of non-`NaN` observations and may therefore be\r\nless than 1.\r\n```"
      ],
      "prometheus-improve-code-readability": [
        "Let's break the overly long line, maybe like this:\r\n\r\n```suggestion\r\nfunc NewZookeeperTreeCache(\r\n\tconn *zk.Conn, path string, \r\n\tevents chan ZookeeperTreeCacheEvent, \r\n\tlogger *slog.Logger, \r\n\tfailureCounter prometheus.Counter, numWatchers prometheus.Gauge,\r\n) *ZookeeperTreeCache {\r\n```",
        "Again, named returns might be helpful."
      ],
      "prometheus-test-practical-monitoring-scenarios": [
        "Let's also add something like `count_over_time(metric1_total[step()])` as the most common usage?",
        "This can be replaced by `present_over_time(metric[30m])-1 and metric or present_over_time(metric[30m])` (and correspondingly below). It might be a bit wordy, but we are also addressing a niche use case here.\r\n\r\nFurthermore, I don't think that a function returning 0 or 1 is good for alerting. If you want to alert on disappearing time series, you want an expression that contains exactly the disappearing series and nothing else, something like `present_over_time(some_metric[1h]) unless present_over_time`, see also my comment on the proposal."
      ]
    }
  },
  "BruceMacD": {
    "repos": [
      "ollama/ollama"
    ],
    "entries": [
      {
        "slug": "ollama-abstract-model-operations-cleanly",
        "title": "Abstract model operations cleanly"
      },
      {
        "slug": "ollama-ai-dependency-decoupling-strategy",
        "title": "AI dependency decoupling strategy"
      },
      {
        "slug": "ollama-ai-memory-management",
        "title": "AI memory management"
      },
      {
        "slug": "ollama-clear-recoverable-error-messages",
        "title": "Clear recoverable error messages"
      },
      {
        "slug": "ollama-complete-http-protocol-handling",
        "title": "Complete HTTP protocol handling"
      },
      {
        "slug": "ollama-descriptive-balanced-naming",
        "title": "Descriptive balanced naming"
      },
      {
        "slug": "ollama-document-synchronization-intent",
        "title": "Document synchronization intent"
      },
      {
        "slug": "ollama-follow-godoc-conventions",
        "title": "Follow GoDoc conventions"
      },
      {
        "slug": "ollama-loose-api-coupling",
        "title": "Loose API coupling"
      },
      {
        "slug": "ollama-optimize-ai-implementation-patterns",
        "title": "Optimize AI implementation patterns"
      },
      {
        "slug": "ollama-optimize-with-standard-library",
        "title": "Optimize with standard library"
      },
      {
        "slug": "ollama-path-traversal-prevention",
        "title": "Path traversal prevention"
      },
      {
        "slug": "ollama-use-environment-variables",
        "title": "Use environment variables"
      },
      {
        "slug": "ollama-use-idiomatic-go-flow",
        "title": "Use idiomatic Go flow"
      },
      {
        "slug": "ollama-use-portable-path-configurations",
        "title": "Use portable path configurations"
      }
    ],
    "comments": {
      "ollama-ai-memory-management": [
        "```suggestion\r\nBy default, Ollama uses a context window size of 2048 tokens. This can be overridden with the `OLLAMA_CONTEXT_LENGTH` environment variable. For example, to set the default context length to 8K, use: `OLLAMA_CONTEXT_LENGTH=8192 ollama serve`.\r\n```\r\n\r\nFixing the double space here.",
        "```suggestion\r\n# Preventing Out of Memory (OOM) Errors\r\n\r\nOllama is designed to automatically manage memory usage and prevent out-of-memory errors. However, in some cases, these errors might still occur. Here are solutions to help prevent these crashes:\r\n\r\n## Basic Solutions\r\n\r\n1. Decrease Context Size\r\n   - [Lower the `num_ctx` parameter](https://github.com/ollama/ollama/blob/a4f69a0191b304c204ef074ccd6523f121bfddfe/docs/modelfile.md#valid-parameters-and-values:~:text=mirostat_tau%205.0-,num_ctx,-Sets%20the%20size)\r\n   - This reduces how much text the model can process at once\r\n   - Results in lower memory usage\r\n\r\n2. Limit GPU Layer Usage\r\n   - [Reduce the number of model layers that run on your GPU](https://github.com/ollama/ollama/issues/6950#issuecomment-2373663650).\r\n   - This helps balance the workload between GPU and CPU\r\n\r\n## Advanced Solutions\r\n\r\nFor users who need additional control, the following environment variables can be configured:\r\n\r\n- [`OLLAMA_GPU_OVERHEAD`](https://github.com/ollama/ollama/blob/5f8051180e3b9aeafc153f6b5056e7358a939c88/envconfig/config.go#L237): Reserves additional GPU memory (e.g., for 512MB buffer)\r\n- [`OLLAMA_FLASH_ATTENTION`](https://github.com/ollama/ollama/blob/5f8051180e3b9aeafc153f6b5056e7358a939c88/envconfig/config.go#L236): Enables more efficient memory usage\r\n- `GGML_CUDA_ENABLE_UNIFIED_MEMORY`: Allows GPU to use CPU memory (Linux only)\r\n- [`OLLAMA_NUM_PARALLEL`](https://github.com/ollama/ollama/blob/a4f69a0191b304c204ef074ccd6523f121bfddfe/envconfig/config.go#L249): Controls parallel processing\r\n\r\nNote: Environment variable support may vary depending on your setup and system configuration.\r\n```"
      ],
      "ollama-complete-http-protocol-handling": [
        "What will happen on pulling from an http source now?"
      ],
      "ollama-descriptive-balanced-naming": [
        "```suggestion\r\n\t\tmodel, err := GetModel(n.String())\r\n\t\tif err != nil {\r\n\t\t\tslog.Warn(\"bad model details\", \"name\", n, \"error\", err)\r\n\t\t\tcontinue\r\n\t\t}\r\n```\r\n\r\nThe variable name should reference the actual type here",
        "I'd like to avoid single letter variables, this is so hard to read. Let me know what you think of how I've done Qwen2:\r\nhttps://github.com/ollama/ollama/pull/9200"
      ],
      "ollama-use-environment-variables": [
        "As far as I know Windows uses `%USERPROFILE%` rather than `~`, but good call, generally better to use os.PathSeparator "
      ],
      "ollama-use-portable-path-configurations": [
        "I believe the space after the `from-repofile` flag will cause issues, this results in the command format: `... addrepo --from-repofile= https://developer.download.nvidia.com...`"
      ],
      "ollama-follow-godoc-conventions": [
        "I know that comments aren't in other places here, but we should be documenting these interfaces and functions with comments where this serves as an implementation guide for different back-ends, and a reference for model implementers. "
      ],
      "ollama-optimize-ai-implementation-patterns": [
        "```suggestion\r\n    \"For best results, this notebook should be run on a Linux node with a GPU or an environment like Google Colab.\"\r\n```"
      ],
      "ollama-document-synchronization-intent": [
        "Looks like `Items` is returning the actual map, which could be unsafely modified by a caller. We should return a copy instead to prevent accidental misuse that creates a race condition.\r\n```\r\n// Items returns a copy of the underlying map.\r\nfunc (s *SyncMap[K, V]) Items() map[K]V {\r\n\ts.mu.RLock()\r\n\tdefer s.mu.RUnlock()\r\n\t\r\n\tresult := make(map[K]V, len(s.m))\r\n\tfor k, v := range s.m {\r\n\t\tresult[k] = v\r\n\t}\r\n\treturn result\r\n}\r\n```"
      ],
      "ollama-loose-api-coupling": [
        "Good point, opened https://github.com/ollama/ollama/pull/9324 to address this since its a confusing diff. I'll rebase this PR on top of #9324 when possible."
      ],
      "ollama-use-idiomatic-go-flow": [
        "Nice, this is easier to read. Made the change.",
        "Consider using an if statement instead of a switch with a single case, as it would be more idiomatic Go for this scenario and potentially improve readability."
      ],
      "ollama-abstract-model-operations-cleanly": [
        "```suggestion\r\n// Attention implements scaled dot-product attention for transformer models:\r\n// Attention(Q, K, V) = softmax(QK^T/âˆšd_k)V\r\n//\r\n// Parameters:\r\n//   - ctx: Context for tensor operations\r\n//   - query: Query tensor (Q) with shape [batch, heads, seq_len_q, d_k]\r\n//   - key: Key tensor (K) with shape [batch, heads, seq_len_k, d_k]\r\n//   - value: Value tensor (V) with shape [batch, heads, seq_len_k, d_v]\r\n//   - mask: Optional attention mask. If provided, should broadcast to [batch, heads, seq_len_q, seq_len_k]\r\n//   - scale: Scaling factor, typically 1/âˆšd_k where d_k is the key dimension\r\n//\r\n// Returns:\r\n//   Attention output with shape [batch, seq_len_q, heads, d_v]\r\nfunc Attention(ctx ml.Context, query, key, value, mask ml.Tensor, scale float64) ml.Tensor {\r\n```",
        "When will the query implement scaled dot product attention? Is this something that should handled here or in the caller?",
        "Should we be validating the tensor shapes here?",
        "That's pretty much what I was thinking, not a requirement by any means but maybe nice to have",
        "It feels less clear to me how the scaling factor is set now and how its used, maybe we should move it to a constant somewhere in the model definition",
        "I find it easier to eye-ball when looking at the model forward pass implementation when its directly associated with scaling:\r\n```go\r\nkq = kq.Scale(ctx, 1.0/math.Sqrt(float64(headDim)))\r\n```\r\nThe fact it is being passed to scale make it apparent that this is probably the scaling factor.\r\n\r\n```go\r\nkqv := nn.Attention(ctx, q, k, v, mask, 1.0/math.Sqrt(float64(headDim)))\r\n```\r\nNow I can't tell what this mathematical operation is while reading the code, I have to follow through and see what it is actually used for in the attention implementation.\r\n\r\n```go\r\nvar scaleFactor = 1.0/math.Sqrt(float64(headDim))\r\nkqv := nn.Attention(ctx, q, k, v, mask, scaleFactor)\r\n```\r\nI personally like something like this, so I can understand what is going on without leaving the current file. "
      ],
      "ollama-clear-recoverable-error-messages": [
        "It looks like this wait to the end to report the error that the download is incomplete, rather than retrying. Is that accurate? I'd rather just retry the chunk, since the download is not resumable either as far as I can tell.",
        "Added the default fallback back in, defence in depth is good"
      ],
      "ollama-path-traversal-prevention": [
        "The os.Root validation allows relative paths that stay within the root directory but rejects absolute paths and any paths that attempt to escape the root boundary. Added some test cases to demonstrate this, I don't see why we should allow the absolute paths. Let me know if I'm missing something.",
        "I agree with your assessment of the \"Relative (rooted) Paths only\" approach. Here's how I would clarify it:\r\n\r\n### Relative (rooted) Paths only:\r\n\r\nAll paths in files MUST be relative to the current directory (without any traversal components):\r\n- Paths MUST NOT start with \"/\" (absolute paths)\r\n- Paths MUST NOT start with \"./\" or \"../\" (explicit current/parent directory references)\r\n- Paths MUST NOT be just \".\" or \"..\"\r\n- Paths MUST NOT contain traversal sequences anywhere (\"/../\", \"/./\") \r\n- Paths MUST NOT end with traversal components (\"/..\") or current directory references (\"/.\")\r\n- All paths are represented in the temp directory as filepath.Join(tmpDir, relPath)\r\n- All joined temp paths are, as an added measure, checked to ensure they remain within the base directory\r\n\r\n### Valid Examples:\r\n- \"x/y/z\" - Simple nested path\r\n- \"a/b\" - Simple path with one directory\r\n- \"o/l/l/a/m/a\" - Multiple nested directories\r\n- \"file.txt\" - File in root\r\n- \"dir/file.txt\" - File in subdirectory\r\n- \"a/b/\" - Paths with trailing slash\r\n- \"a//b\" - Paths with double slash (will be normalized)\r\n\r\n### Invalid Examples:\r\n- \"/y\" - Absolute path\r\n- \"./x/y\" - Starts with current directory reference\r\n- \"../x\" - Starts with parent directory reference\r\n- \".\" - Current directory only\r\n- \"..\" - Parent directory only\r\n- \"a/../b\" - Contains traversal component\r\n- \"a/./b\" - Contains current directory reference\r\n- \"a/../../b\" - Contains traversal beyond boundaries\r\n- \"a/b/..\" - Ends with parent directory reference\r\n- \"a/b/.\" - Ends with current directory reference\r\n\r\n> Also, just to make sure it is clear: The provided paths (in whichever form we require) are always joined with the tmpDir BEFORE we pass to (*os.Root).Stat\r\n\r\nThis will not work since its an absolute path to the temp dir. `os.Root` rejects absolute paths. i've tweaked the logic to create/open the file through the root interface, which will properly validate containment regardless of whether the file exists or not.\r\n\r\n",
        "The os.Root doesn't quite work in this case, if you prefix the invalid path with a directory name, than it is seen as safe, since that results in a fs.ErrNotExist",
        "Yup, checking both since I dont want to rely on this only being called through createHandler, but want to fail fast if I can"
      ],
      "ollama-ai-dependency-decoupling-strategy": [
        "Nice, simpler than where I was going with this"
      ],
      "ollama-optimize-with-standard-library": [
        "The Go standard library has an example of how to build a priority queue with a heap. Should we try that instead of the dependency?\r\nhttps://pkg.go.dev/container/heap"
      ]
    }
  },
  "bastiandoetsch": {
    "repos": [
      "snyk/cli"
    ],
    "entries": [
      {
        "slug": "cli-avoid-hardcoded-configuration-values",
        "title": "avoid hardcoded configuration values"
      },
      {
        "slug": "cli-comprehensive-test-coverage",
        "title": "comprehensive test coverage"
      },
      {
        "slug": "cli-defensive-shell-script-configuration",
        "title": "defensive shell script configuration"
      },
      {
        "slug": "cli-handle-all-errors-explicitly",
        "title": "Handle all errors explicitly"
      },
      {
        "slug": "cli-optimize-variable-declarations",
        "title": "Optimize variable declarations"
      },
      {
        "slug": "cli-use-centralized-loggers",
        "title": "Use centralized loggers"
      },
      {
        "slug": "cli-use-descriptive-identifier-names",
        "title": "Use descriptive identifier names"
      },
      {
        "slug": "cli-use-descriptive-parameter-names",
        "title": "Use descriptive parameter names"
      },
      {
        "slug": "cli-use-file-locks",
        "title": "Use file locks"
      },
      {
        "slug": "cli-use-optional-chaining",
        "title": "Use optional chaining"
      },
      {
        "slug": "cli-use-secure-hash-functions",
        "title": "Use secure hash functions"
      },
      {
        "slug": "cli-validate-security-configurations",
        "title": "Validate security configurations"
      },
      {
        "slug": "cli-verify-downloaded-file-integrity",
        "title": "verify downloaded file integrity"
      }
    ],
    "comments": {
      "cli-use-descriptive-parameter-names": [
        "yeah, already thought about it and then forgot :)",
        "this should be named differently - it's not obvious what it is. Suggestion: `go_download_base_url`"
      ],
      "cli-defensive-shell-script-configuration": [
        "mach doch einfach ein `set -ex` wenn du die kommandos mit ausgegeben haben willst :)"
      ],
      "cli-use-descriptive-identifier-names": [
        "suggestion (non-blocking): let's rename `GetGlobalConfiguration` to `GetGlobalConfigurationOptions` in a follow-up PR."
      ],
      "cli-comprehensive-test-coverage": [
        "How about using `assert.Eventually` instead of sleeping?"
      ],
      "cli-verify-downloaded-file-integrity": [
        "This should most likely check the shasum?",
        "Shasum checking would be awesome!",
        "Also, as we're talking about FIPS functionality, do the go providers provide GPG keys to check? ",
        "Lets do gpg in a follow-up PR/Story, using aharc-coded public keys in config.yaml and validate on each platform using the given key."
      ],
      "cli-validate-security-configurations": [
        "Does it also handle the case when neither bearer nor token is given?"
      ],
      "cli-optimize-variable-declarations": [
        "suggestion (non-blocking): this doesn't need to be a number, could just be a boolean, which would make the logic I think easier to understand. "
      ],
      "cli-handle-all-errors-explicitly": [
        "this could be a `deferred` call so that it is executed even if a panic/error occurs",
        "see above - why not cleaning up with defer?",
        "The cachedir could even clean itself up, if you pass T into the function and then register the cleanup as cleanup function."
      ],
      "cli-use-optional-chaining": [
        "suggestion: not a javascript expert, but maybe only call `.length` after checking if it actually is not `undefined`?",
        "suggestion: to get the same advice as in VSCode & Eclipse, you could even extend it more - see: https://github.com/snyk/snyk-ls/blob/a52f9d047ea29210636d8bb01ef9ea9d1233f4b2/infrastructure/oss/issue_html.go#L146"
      ],
      "cli-use-centralized-loggers": [
        "suggestion: this should use the logger :)"
      ],
      "cli-use-file-locks": [
        "I think you don't even need to check the type of error here - any error would be critical at this point and a regeneration should be done.",
        "I'm wondering if setting an exclusive flock on the cert file while the process is running would be appropriate.",
        "Hm. I wonder how we can automatically solve this. The user shouldn't be bothered about the cache-directory, as they are normally not creating it at all.",
        "Why can't we use mkdirall in conjunction with a file lock (flock library)?",
        "Maybe a lockfile in the temp dir?"
      ],
      "cli-use-secure-hash-functions": [
        "yes, but they are not secure hash functions. speed of sha256 is more than fast enough, and it's not compromised. MD5, CRC32 are not remotely secure and not guaranteed to be unidirectional anymore (or CRC has never been).",
        "Maybe a random salt should be used on top to prevent attacks via rainbow tables. But yeah, just guard it with `--debug` and add a random UUID as salt.",
        "As it's not meant to change - maybe the random salt per instantiation is not possible."
      ],
      "cli-avoid-hardcoded-configuration-values": [
        "Adding \"You may want to consider if using docker is possible for using Snyk.\" could be a good helper"
      ]
    }
  },
  "jmcdo29": {
    "repos": [
      "nestjs/nest"
    ],
    "entries": [
      {
        "slug": "nest-avoid-testing-anti-patterns",
        "title": "Avoid testing anti-patterns"
      },
      {
        "slug": "nest-benchmark-before-optimizing",
        "title": "Benchmark before optimizing"
      },
      {
        "slug": "nest-configurable-log-formatting",
        "title": "Configurable log formatting"
      },
      {
        "slug": "nest-explicit-default-configurations",
        "title": "Explicit default configurations"
      },
      {
        "slug": "nest-follow-protocol-standards",
        "title": "Follow protocol standards"
      },
      {
        "slug": "nest-http-header-management",
        "title": "HTTP header management"
      },
      {
        "slug": "nest-manage-testing-dependencies",
        "title": "Manage testing dependencies"
      },
      {
        "slug": "nest-optimize-critical-path-iterations",
        "title": "Optimize critical path iterations"
      },
      {
        "slug": "nest-parameterize-version-requirements",
        "title": "Parameterize version requirements"
      },
      {
        "slug": "nest-pin-dependency-versions",
        "title": "Pin dependency versions"
      },
      {
        "slug": "nest-preserve-api-interface-stability",
        "title": "Preserve API interface stability"
      },
      {
        "slug": "nest-preserve-public-api-stability",
        "title": "Preserve public API stability"
      },
      {
        "slug": "nest-prevent-async-race-conditions",
        "title": "Prevent async race conditions"
      },
      {
        "slug": "nest-prevent-race-conditions",
        "title": "Prevent race conditions"
      },
      {
        "slug": "nest-structure-behavior-driven-tests-properly",
        "title": "Structure behavior-driven tests properly"
      },
      {
        "slug": "nest-test-dependency-management",
        "title": "Test dependency management"
      },
      {
        "slug": "nest-use-consistent-control-structures",
        "title": "Use consistent control structures"
      },
      {
        "slug": "nest-use-consistent-curly-braces",
        "title": "Use consistent curly braces"
      },
      {
        "slug": "nest-use-factory-providers",
        "title": "Use factory providers"
      }
    ],
    "comments": {
      "nest-use-consistent-control-structures": [
        "Would you mind updating the code to wrap the internals of the if in curly braces? It leads the code to be easier to follow and modify if we need to\r\n\r\n```suggestion\r\n    if (this.flushLogsOnOverride) {\r\n      this.flushLogs();\r\n    }\r\n```",
        "If these are going to be single line ifs, would a ternary make more sense?\r\n```\r\ndata += message.type ? `event: ${message.type}\\n` : ''\r\n```\r\n",
        "Personally, I'm not a fan of inline ifs, so if we stay with using `if` statements it should be changed to\r\n```ts\r\nif (message.type) {\r\n  data += `event: ${message.type}\\n`;\r\n}\r\n```\r\nAnd this is what most of Nest's codebase does already even for single line. So ternary, or using braces as necessary should be fine. Just to stay consistent with the rest of the codebase"
      ],
      "nest-configurable-log-formatting": [
        "This is _mostly_ only used by the `ConsoleLogger`, but there are references to it in the `Injector.ts` file as well. If it was just the `ConsoleLogger` I'd say this is okay, or even checking `process.stdout.hasColors()` directly instead of needing the prototype, but I'm not sure how to handle if this check is coming from the `Injector.ts`\r\n\r\nOverall, I don't _think_ this should be an issue, my only concern would be if someone is in an environment where `hasColors()` returns `false` but they want to force the use of colors. "
      ],
      "nest-benchmark-before-optimizing": [
        "If you're going to refactor this to a raw for loop, we should do the same to the `getAll` for squeezing as much performance out as we can. \r\n\r\nSide note: do you have any benchmarks for how much of an improvement this is? Just curious if the improvement is going to be worth the readability "
      ],
      "nest-manage-testing-dependencies": [
        "Looks like it's for a new integration test. As it's in `devDeps` it should be fine"
      ],
      "nest-test-dependency-management": [
        "Looks like it's for a new integration test. As it's in `devDeps` it should be fine"
      ],
      "nest-parameterize-version-requirements": [
        "npm v9 was released on 2022-10-24 is is not compatible with node 12. Today (2022-11-09) it was set to the `@latest` tag for `npm` so our CircleCI builds started breaking. \r\n\r\n> Wednesday Nov. 9th (General Availability)\r\n> To ensure npm@9.x is considered \"non-breaking\" for Node.js LTS we will codify a set of exit criteria in collaboration with the [Release WG](https://github.com/nodejs/release)\r\n> npm@9.x will be set to the latest dist-tag (becoming the latest, maintained version of npm)\r\n> A PR will be opened to land npm@9.x in nodejs/node's main branch (exposing experimental/nightly users to this latest version)\r\n\r\nhttps://github.blog/changelog/2022-10-24-npm-v9-0-0-released/"
      ],
      "nest-prevent-async-race-conditions": [
        "Rather than mutating the client, could I add a new `args` entry and let `getPattern()` retrieve `args[2]`? Would that be more \"stable\"? ",
        "Great call by the way! I moved `getPattern` to be a method on `WsArgumentHost` instead of under the `getClient()` so that it should be unique per request as the `WsArgumentHost` already is. Tests are still passing and the interfaces are updated. Let me know if you think of any other issues with this approach :smile_cat:"
      ],
      "nest-prevent-race-conditions": [
        "Rather than mutating the client, could I add a new `args` entry and let `getPattern()` retrieve `args[2]`? Would that be more \"stable\"? ",
        "Great call by the way! I moved `getPattern` to be a method on `WsArgumentHost` instead of under the `getClient()` so that it should be unique per request as the `WsArgumentHost` already is. Tests are still passing and the interfaces are updated. Let me know if you think of any other issues with this approach :smile_cat:"
      ],
      "nest-structure-behavior-driven-tests-properly": [
        "```suggestion\r\n      expect(usersController.create(createUserDto)).resolves.toEqual({\r\n        id: 'a id',\r\n        ...createUserDto,\r\n      });\r\n      expect(usersService.create).toHaveBeenCalled();\r\n      expect(usersService.create).toHaveBeenCalledWith(createUserDto);\r\n```\r\nNo need to call `usersController.create` twice. "
      ],
      "nest-avoid-testing-anti-patterns": [
        "```suggestion\r\n      expect(usersController.create(createUserDto)).resolves.toEqual({\r\n        id: 'a id',\r\n        ...createUserDto,\r\n      });\r\n      expect(usersService.create).toHaveBeenCalled();\r\n      expect(usersService.create).toHaveBeenCalledWith(createUserDto);\r\n```\r\nNo need to call `usersController.create` twice. ",
        "There's no need to call `controller.findAll()` twice in the test. Just call it in the assertion and then do the jest check after it\r\n```suggestion\r\n      expect(controller.findAll()).resolves.toEqual([\r\n        {\r\n          name: 'Cat #1',\r\n          breed: 'Bread #1',\r\n          age: 4,\r\n        },\r\n        {\r\n          name: 'Cat #2',\r\n          breed: 'Breed #2',\r\n          age: 3,\r\n        },\r\n        {\r\n          name: 'Cat #3',\r\n          breed: 'Breed #3',\r\n          age: 2,\r\n        },\r\n      ]);\r\n      expect(service.findAll).toHaveBeenCalled();\r\n```",
        "This isn't an actual test of anything. You mock the value you want to call and assert the test just called the service method. This should instead mock the repository method used in `service.remove`"
      ],
      "nest-preserve-api-interface-stability": [
        "Adding non-optional methods to a public interface that should be implemented for adapters means that anyone who is maintaining their own adapter, say for Koa, or hyper-express, etc, now **must** make these changes or possibly have their packages broken by a transient upgrade.\r\n\r\nThis is a braking change, and while you may have been waiting for it for a while, that doesn't mean it's any more urgent to get out, as we want to avoid making several small breaking changes and would rather lump them together over a major upgrade."
      ],
      "nest-http-header-management": [
        "If you were to have a falsy header value, such as `false`, `0` or `''`, no browser that I'm aware of would know how to properly handle that response, and as such the body might be unrecoverable, or at least encoded in an unexpected way.\n\nI think think is a decent restriction to keep from giving the dev a nice foot gun, but I'd be willing to hear what the browser should do if the header was one of those values, especially if there's an RFC for it as well",
        "I assume this will automatically be `chunk`? If not, should we set it to that?"
      ],
      "nest-use-consistent-curly-braces": [
        "Would you mind updating the code to wrap the internals of the if in curly braces? It leads the code to be easier to follow and modify if we need to\r\n\r\n```suggestion\r\n    if (this.flushLogsOnOverride) {\r\n      this.flushLogs();\r\n    }\r\n```",
        "If these are going to be single line ifs, would a ternary make more sense?\r\n```\r\ndata += message.type ? `event: ${message.type}\\n` : ''\r\n```\r\n",
        "Personally, I'm not a fan of inline ifs, so if we stay with using `if` statements it should be changed to\r\n```ts\r\nif (message.type) {\r\n  data += `event: ${message.type}\\n`;\r\n}\r\n```\r\nAnd this is what most of Nest's codebase does already even for single line. So ternary, or using braces as necessary should be fine. Just to stay consistent with the rest of the codebase"
      ],
      "nest-preserve-public-api-stability": [
        "Adding non-optional methods to a public interface that should be implemented for adapters means that anyone who is maintaining their own adapter, say for Koa, or hyper-express, etc, now **must** make these changes or possibly have their packages broken by a transient upgrade.\r\n\r\nThis is a braking change, and while you may have been waiting for it for a while, that doesn't mean it's any more urgent to get out, as we want to avoid making several small breaking changes and would rather lump them together over a major upgrade."
      ],
      "nest-explicit-default-configurations": [
        "Instead of marking this as optional, could we give it a default value?\r\n\r\n```suggestion\r\n    appOptions: NestApplicationOptions = {\r\n      disableInstanceLoaderLogs: false\r\n    },\r\n  ) {\r\n    const instanceLoader = new InstanceLoader(container);\r\n\r\n    if (appOptions.disableInstanceLoaderLogs) {\r\n      instanceLoader.disableLogs();\r\n    }\r\n```\r\n\r\nAlso, do we need the full options here, or just the `disableInstanceLoaderLogs` option? We might be able to cut this down to a simple `boolean` instead of the full object"
      ],
      "nest-pin-dependency-versions": [
        "npm v9 was released on 2022-10-24 is is not compatible with node 12. Today (2022-11-09) it was set to the `@latest` tag for `npm` so our CircleCI builds started breaking. \r\n\r\n> Wednesday Nov. 9th (General Availability)\r\n> To ensure npm@9.x is considered \"non-breaking\" for Node.js LTS we will codify a set of exit criteria in collaboration with the [Release WG](https://github.com/nodejs/release)\r\n> npm@9.x will be set to the latest dist-tag (becoming the latest, maintained version of npm)\r\n> A PR will be opened to land npm@9.x in nodejs/node's main branch (exposing experimental/nightly users to this latest version)\r\n\r\nhttps://github.blog/changelog/2022-10-24-npm-v9-0-0-released/",
        "What are the differences between v2 and v3?"
      ],
      "nest-use-factory-providers": [
        "I'd much rather we direct the user to the issue rather than have them dig into the source code to find the issue reference ",
        "[This wouldn't be the first time we've had a link in the exception](https://github.com/nestjs/nest/blob/ef5344826f23b5377a2f2599cf9efda0303e1f33/packages/core/errors/messages.ts#L163)",
        "And I'm certain we've linked to issues from within our docs. If we _really_ don't want to link to the issue directly, then we should make a new section of the FAQ > Common Errors for this"
      ],
      "nest-optimize-critical-path-iterations": [
        "If you're going to refactor this to a raw for loop, we should do the same to the `getAll` for squeezing as much performance out as we can. \r\n\r\nSide note: do you have any benchmarks for how much of an improvement this is? Just curious if the improvement is going to be worth the readability "
      ],
      "nest-follow-protocol-standards": [
        "If you were to have a falsy header value, such as `false`, `0` or `''`, no browser that I'm aware of would know how to properly handle that response, and as such the body might be unrecoverable, or at least encoded in an unexpected way.\n\nI think think is a decent restriction to keep from giving the dev a nice foot gun, but I'd be willing to hear what the browser should do if the header was one of those values, especially if there's an RFC for it as well",
        "I assume this will automatically be `chunk`? If not, should we set it to that?"
      ]
    }
  },
  "thaJeztah": {
    "repos": [
      "docker/compose"
    ],
    "entries": [
      {
        "slug": "compose-add-logging-without-duplication",
        "title": "Add logging without duplication"
      },
      {
        "slug": "compose-avoid-ci-resource-conflicts",
        "title": "avoid CI resource conflicts"
      },
      {
        "slug": "compose-avoid-confusing-names",
        "title": "Avoid confusing names"
      },
      {
        "slug": "compose-avoid-variable-name-conflicts",
        "title": "Avoid variable name conflicts"
      },
      {
        "slug": "compose-ci-security-boundaries",
        "title": "CI security boundaries"
      },
      {
        "slug": "compose-consistent-formatting-choices",
        "title": "consistent formatting choices"
      },
      {
        "slug": "compose-environment-variable-validation",
        "title": "Environment variable validation"
      },
      {
        "slug": "compose-evaluate-dependency-api-compatibility",
        "title": "evaluate dependency API compatibility"
      },
      {
        "slug": "compose-explicit-configuration-management",
        "title": "explicit configuration management"
      },
      {
        "slug": "compose-isolate-test-dependencies",
        "title": "Isolate test dependencies"
      },
      {
        "slug": "compose-keep-code-structure-flat",
        "title": "Keep code structure flat"
      },
      {
        "slug": "compose-minimize-credential-access-scope",
        "title": "minimize credential access scope"
      },
      {
        "slug": "compose-network-api-precision",
        "title": "Network API precision"
      },
      {
        "slug": "compose-optimize-docker-layer-caching",
        "title": "optimize Docker layer caching"
      },
      {
        "slug": "compose-precise-security-pattern-matching",
        "title": "precise security pattern matching"
      },
      {
        "slug": "compose-schema-changes-upstream-first",
        "title": "Schema changes upstream first"
      },
      {
        "slug": "compose-use-standard-api-fields",
        "title": "Use standard API fields"
      }
    ],
    "comments": {
      "compose-avoid-ci-resource-conflicts": [
        "Do we actually need `--privileged` for this container? I see it bind-mounts `docker.sock`, which means that all `docker` commands will actually run against the docker daemon running on the host (not a `dockerd` daemon running inside the container (which _would_ require privileged)",
        "Actually wondering if we need a container here at all, because it's running a container, just to run a script, that uses the `docker` CLI inside the container to start new containers on the host ðŸ¤” "
      ],
      "compose-keep-code-structure-flat": [
        "Perhaps split the switch into a `switch direction`, and within each of those (from/to) do the further checks, which could be a nested `switch` if needed.\r\n\r\nCombining the check (especially if they're checking on the same variables, such as `index` is sometimes \"tricky\", and easy to overlook if there's things excluded or \"duplicated\".\r\n\r\nNote that instead of `||` you can also use `case <condition1>, <condition2>, <condition3> ...`, which (I think) is more common."
      ],
      "compose-consistent-formatting-choices": [
        "I'd keep the `pip install` separate; it likely won't overwrite files that were installed by `apk add`, so doing it in a separate step will add a new layer, but won't make the image bigger.\r\n\r\nAlso worth to keep the same convention as for `apk add`, and split the packages that will be installed to one-per-line, sorted alphabetically (it makes the Dockerfile longer, but can help making it more maintainable)\r\n\r\n```suggestion\r\nRUN pip install \\\r\n    tox==2.1.1 \\\r\n    virtualenv==16.2.0\r\n```\r\n\r\nIf these packages are expected to be updated individually, could even be two `RUN` lines",
        "`-f` specifies a path to the Docekrfile, so good practice to put quotes around it (even though that will likely not be hit here\r\n\r\n```suggestion\r\ndocker build -f \"${DOCKERFILE}\" -t \"${TAG}\" --target \"${DOCKER_BUILD_TARGET}\" .\r\n```"
      ],
      "compose-environment-variable-validation": [
        "Perhaps could also be worth (if compose reads the cli config) to consider either an option in `features`, or `plugins` (plugins allows plugin-specific options to be set), which would allow opt-in/opt-out of this without having to use an env-var;\r\nhttps://github.com/docker/cli/blob/9861ce90fd6b8ddca19db5f803dcbef9a583e9e1/cli/config/configfile/file.go#L42-L44\r\n\r\n```go\r\n\tPlugins              map[string]map[string]string `json:\"plugins,omitempty\"`\r\n\tAliases              map[string]string            `json:\"aliases,omitempty\"`\r\n\tFeatures             map[string]string            `json:\"features,omitempty\"`\r\n```\r\n\r\n(in addition to an env-var probably)",
        "Yeah, my thinking here was that the cli-config would more easily allow this to be set as a default, which could also allow (e.g.) it to be set through docker desktop \"settings\"."
      ],
      "compose-optimize-docker-layer-caching": [
        "I _think_ this `ENV` is only used by `script/build/linux-entrypoint`, so better to move it lower to prevent unwanted cache-busts",
        "very minor nit; this file likely doesn't change much, so could be moved before the `COPY . . `, or even before the `COPY requirements.txt` (ordering from \"less frequently changing\" to \"most frequently changing\").\r\n\r\nIf this script (and `ENTRYPOINT` below is only used for `runtime`, might consider moving both to the start of the `runtime` stage)"
      ],
      "compose-precise-security-pattern-matching": [
        "A minor optimisation could be to use `--format` (so that only the Security options are output), and/or to match `name=userns` instead of just `userns` (as that's what the daemon will return; https://github.com/moby/moby/blob/b6684a403c99aaf6be5b8ce0bef3c6650fcdcd12/daemon/info.go#L180-L182\r\n\r\n```suggestion\r\nif docker info --format '{{json .SecurityOptions}}' 2>/dev/null | grep -q 'name=userns'; then\r\n```"
      ],
      "compose-avoid-confusing-names": [
        "`Docker-Compose` (both capital) is definitely incorrect (should either be `docker-compose` (name of the binary), or `Docker Compose`). Perhaps avoid the name altogether and use something similar as the `docker` cli uses (`Print version information and quit`)\r\n\r\n```suggestion\r\n      version            Show version information and quit\r\n```\r\n\r\n(quit/exit, not sure what's clearer)? `curl` uses `quit`;\r\n\r\n```\r\n -V, --version       Show version number and quit\r\n```\r\n\r\n"
      ],
      "compose-isolate-test-dependencies": [
        "Would it make sense to have a separate module for the e2e tests, so that these test dependencies don't become a dependency for the main module?\r\n\r\nWe took that approach in containerd, where we then replaced the main module  with a path, to make sure we use the code from the branch where it's needed ; https://github.com/containerd/containerd/blob/main/integration/client/go.mod#L79\r\n",
        "Thanks! So, yes, I think the \"ideal\" at some point would be to (e.g) have a docker image with the integration tests compiled in, which could be run with a compose binary and docker socket mounted; this would also allow for (e.g.) https://github.com/moby/moby to run the latest e2e/integration tests as part of CI."
      ],
      "compose-schema-changes-upstream-first": [
        "Note that we can't update the 3.7 schema, as it's already been released, so to add this property to the schema, it probably has to be added to the upcoming 3.9 schema in https://github.com/docker/cli/blob/master/cli/compose/schema/data/config_schema_v3.9.json first"
      ],
      "compose-explicit-configuration-management": [
        "Did it automatically update this one, or did it still allow go 1.21.0 here?\r\n\r\nI generally try to treat this one the same as other dependencies; list the minimum required version, and only update if it's _impossible_ to use with older versions; see https://github.com/containerd/containerd/pull/10596#discussion_r1721294997",
        "Because 26.1.1 is lower than 26.1.3. Thank Go for inventing pseudo versions and not understanding release branches",
        "before we have a tag, you can temporarily add replace rules; see https://github.com/docker/buildx/pull/2499",
        "Do we have PRs for this in the upstream repositories? Looks like compose is using master / v0.13.x as dependency, so if we could get the fix merged in upstream, that'd be good, I think?",
        "Could we set `GO111MODULE=auto` (or `on`), or `-mod=<what is it?>` in the makefile? That way we wouldn't have to think about setting that in `docker-ce-packaging`.",
        "But.. I have to admit that I kinda agree with https://github.com/docker/compose/pull/9776#discussion_r952832902, and wonder to what extend we need to have this complicated auto-detection.\r\n\r\nI think common scenarios would be either;\r\n\r\n- `docker` is installed (and `docker buildx` would be available as well for regular installs)\r\n- it's run in GitHub actions with only `buildx` installed; in that case we can set `BUILDX_CMD=buildx`\r\n- if neither is true, then I think a `/bin/sh: docker: not found` error may be \"just fine\" (after all, we're also not doing similar things to detect if `go` is installed, or `make`, or `git`)\r\n\r\nIf we agree with the above, just a;\r\n\r\n```make\r\nBUILDX_CMD ?= docker buildx\r\n```\r\n\r\nwould cover that scenario\r\n\r\n(feedback / thoughts welcome!)\r\n",
        "Thanks! Sorry for being nit-picky there (I can see some value for auto-detection in other scenarios), just looking \"can we simplify things (within reason)\"? In the end, the repository would have a \"how to build\" with some prerequisites, so if things fail, users should just \"read the manual\" ðŸ˜‚ ",
        "Looks like we're also updating various dependencies here; were these needed for Go 1.17? (otherwise it's good practice to do this separately)."
      ],
      "compose-use-standard-api-fields": [
        "Wondering why you didn't use the `Status` field that's returned by the API (which is the field that's used by the `docker` CLI;\r\n\r\n```bash\r\ncurl --unix-socket /var/run/docker.sock \"http://localhost/containers/json\" | jq .\r\n```\r\n\r\n```json\r\n\r\n  {\r\n    \"Id\": \"82950c6535204a462c8a3c1f175408b456fbb91971d2d21a33ba04c8b91c74fd\",\r\n    \"Names\": [\r\n      \"/cranky_keldysh\"\r\n    ],\r\n    \"Image\": \"libnetworkbuild\",\r\n    \"ImageID\": \"sha256:bc6bbc6a0032300d8818182eff0101ecb7af2fc1fb21da6f290943c286946a1e\",\r\n    \"Command\": \"make unit-tests-local\",\r\n    \"Created\": 1573091888,\r\n    \"Ports\": [],\r\n    \"Labels\": {},\r\n    \"State\": \"running\",\r\n    \"Status\": \"Up 5 minutes\",\r\n    \"HostConfig\": {\r\n      \"NetworkMode\": \"default\"\r\n    },\r\n    \"NetworkSettings\": {\r\n      \"Networks\": {\r\n        \"bridge\": {\r\n          \"IPAMConfig\": null,\r\n          \"Links\": null,\r\n          \"Aliases\": null,\r\n          \"NetworkID\": \"5a59d43d598f910579535ffb2cbbb4d0987807d7b5593c264c83337c4220ec1a\",\r\n          \"EndpointID\": \"bf8b6b629b353988d047bcd0bbb897249d7a73a7811b2210b783338e6a975cd9\",\r\n          \"Gateway\": \"172.17.0.1\",\r\n          \"IPAddress\": \"172.17.0.5\",\r\n          \"IPPrefixLen\": 16,\r\n          \"IPv6Gateway\": \"\",\r\n          \"GlobalIPv6Address\": \"\",\r\n          \"GlobalIPv6PrefixLen\": 0,\r\n          \"MacAddress\": \"02:42:ac:11:00:05\",\r\n          \"DriverOpts\": null\r\n        }\r\n      }\r\n    },\r\n    \"Mounts\": [\r\n      {\r\n        \"Type\": \"bind\",\r\n        \"Source\": \"/Users/sebastiaan/projects/libnetwork\",\r\n        \"Destination\": \"/go/src/github.com/docker/libnetwork\",\r\n        \"Mode\": \"\",\r\n        \"RW\": true,\r\n        \"Propagation\": \"rprivate\"\r\n      }\r\n    ]\r\n  }\r\n]\r\n```",
        "should the new option deprecate the old (`--no-ansi`) one? (at least making them conflicting options, as (I think) `--no-ansi` is the equivalent of `--ansi=never`?)\r\n\r\nFor the `docker` cli, if there's an option that cannot be removed we usually _hide_ the option (to discourage use), and (depending on the case) print a deprecation warning if used (but keep it functional if needed).\r\n\r\nAlso looking if we should align the UX to the `--progress` option used on `docker build`;\r\n\r\n```\r\n      --progress string         Set type of progress output (auto, plain, tty). Use plain to show container output (default \"auto\")\r\n```"
      ],
      "compose-network-api-precision": [
        "I think the trick also requires `--opt type=none` "
      ],
      "compose-avoid-variable-name-conflicts": [
        "Perhaps it would be good to use a different variable name for this; I know that some tools (`rpm`, `deb` packaging) also set `LDFLAGS` as environment variable, and in those cases it's important to reset them.\r\n\r\nWe could use something similar a containerd, which uses `GO_LDFLAGS` (as well as some other `GO_` prefixed variables to prevent conflicts); https://github.com/containerd/containerd/blob/be91a219c2ac5e65c00bbe85c5dff0827d41958b/Makefile#L92-L102\r\n",
        "It's often a good idea to use a different name for the `ARG` than for the `ENV`. Both act in the same \"space\" (both are set as (environment) variables, which can lead to run situations where the `ENV` is always overridden by the `ARG` (I can find some examples, don't have them at hand)\r\n\r\nSo, might want to consider, e.g.;\r\n\r\n```Dockerfile\r\nARG GIT_COMMIT=unknown\r\nENV DOCKER_COMPOSE_GITSHA=$GIT_COMMIT\r\n```\r\n\r\n(or vice-versa)"
      ],
      "compose-add-logging-without-duplication": [
        "Wondering; should we log something if the service _does_ have an `image:` specified, but either doesn't have `build:` or image has a _digest_ set (to give some clue why the image for a service wasn't pushed)?"
      ],
      "compose-evaluate-dependency-api-compatibility": [
        "Do we know what patches are in this fork, and if they were rejected upstream? I know `fsnotify` has had some time where maintenance was slow, but I think it improved in that respect (and I _think_ we have some maintainers on it that are also maintainers for moby/moby)",
        "Ah, thanks! Yes, saw the comment later on, and saw that (github indicated \"10\" commits in the fork);\r\n\r\nhttps://github.com/fsnotify/fsnotify/compare/main...tilt-dev:fsnotify:main\r\n\r\nPinning to a commit from upstream SGTM (short term). I'm mostly trying to avoid having 2 forks of the same dependency (as I know we have fsnotify as dependency in our tree already).\r\n\r\nIf someone has some cycles to spare to look what patches are not (yet) in upstream, we could contribute them there.\r\n\r\n@cpuguy83 were you a maintainer on that repo? Or do I misremember that? (Otherwise I _think_ there's some familiar people on it, that we may try to reach out to to ask for a (pre-)release)."
      ],
      "compose-ci-security-boundaries": [
        "This is a 3rd party action, so potentially less \"trusted\"; wondering if this is one that we should pin to a commit? (also make sure that we evaluate the changes in the release).\r\n\r\n\r\nDiff since last (v1.9.0) v1 release (but perhaps there's been other v1 updates since it was added); https://github.com/tibdex/github-app-token/compare/v1.9.0...v2.1.0",
        "Thanks!"
      ],
      "compose-minimize-credential-access-scope": [
        "same comment as on the other open source repo; I'd prefer to have new credentials here, that are limited to just what's needed for this scan (so that we can easily rotate those if needed, and they don't provide access to things we don't want)"
      ]
    }
  },
  "agaudreault": {
    "repos": [
      "argoproj/argo-cd"
    ],
    "entries": [
      {
        "slug": "argo-cd-api-documentation-clarity",
        "title": "API documentation clarity"
      },
      {
        "slug": "argo-cd-check-nil-before-access",
        "title": "Check nil before access"
      },
      {
        "slug": "argo-cd-choose-appropriate-synchronization-primitives",
        "title": "Choose appropriate synchronization primitives"
      },
      {
        "slug": "argo-cd-complete-configuration-examples",
        "title": "Complete configuration examples"
      },
      {
        "slug": "argo-cd-comprehensive-function-documentation",
        "title": "Comprehensive function documentation"
      },
      {
        "slug": "argo-cd-consolidate-rbac-permissions",
        "title": "consolidate RBAC permissions"
      },
      {
        "slug": "argo-cd-design-extensible-apis",
        "title": "design extensible APIs"
      },
      {
        "slug": "argo-cd-document-network-requirements",
        "title": "document network requirements"
      },
      {
        "slug": "argo-cd-document-observability-prerequisites",
        "title": "Document observability prerequisites"
      },
      {
        "slug": "argo-cd-explicit-security-controls",
        "title": "explicit security controls"
      },
      {
        "slug": "argo-cd-extract-testable-units",
        "title": "Extract testable units"
      },
      {
        "slug": "argo-cd-prefer-early-returns",
        "title": "Prefer early returns"
      },
      {
        "slug": "argo-cd-provide-comprehensive-explanations",
        "title": "Provide comprehensive explanations"
      },
      {
        "slug": "argo-cd-remove-unnecessary-elements",
        "title": "Remove unnecessary elements"
      },
      {
        "slug": "argo-cd-standardize-commit-tracing-metadata",
        "title": "standardize commit tracing metadata"
      },
      {
        "slug": "argo-cd-structured-logging-practices",
        "title": "structured logging practices"
      },
      {
        "slug": "argo-cd-use-clear-descriptive-names",
        "title": "Use clear, descriptive names"
      },
      {
        "slug": "argo-cd-use-configuration-constants",
        "title": "Use configuration constants"
      },
      {
        "slug": "argo-cd-use-descriptive-constants",
        "title": "Use descriptive constants"
      },
      {
        "slug": "argo-cd-validate-configuration-appropriateness",
        "title": "Validate configuration appropriateness"
      }
    ],
    "comments": {
      "argo-cd-consolidate-rbac-permissions": [
        "The controller already has a cluster role with the permissions in https://github.com/agaudreault/argo-cd/blob/88c3fd61daf9832f12a1766d3ff37a1521d02ca8/manifests/cluster-rbac/applicationset-controller/argocd-applicationset-controller-clusterrole.yaml#L77-L88 \r\n\r\nThese will only be given for cluster install and not namespace install. This PR should consolidate both."
      ],
      "argo-cd-design-extensible-apis": [
        "The list stats should be part of an ApplicationListResponse object specific to the server/application/application.proto.\r\n\r\n",
        "Create another method on the interface, or provide another interface param to the middleware to `GetToken(r *http.Request) string`",
        "Instead of having a new VerifyJWT method on the existing provider interface, shouldn't you have a new implementation of the provider interface for JWT token? or have a new provider totally in something like /util/jwt/provider.go"
      ],
      "argo-cd-check-nil-before-access": [
        "Based on the implementation of registerDexHandlers, if SSO is not configured, ssoClientApp will be nil. The auth middleware should handle the nil use case. Make sure to add an additional unit test to validate that (TestWithAuthMiddlewareWhenSSONotConfigured)",
        "Should always return nil when claims is missing\r\n\r\n```suggestion\r\n\tclaim, err := m.GetExpirationTime()\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to get 'exp' claim: %w\", err)\r\n\t}\r\n```",
        "`app.Spec.Destination.Server` should be received in parameters bacuse it can be `nil` if destination by name is used"
      ],
      "argo-cd-use-descriptive-constants": [
        "Maybe unrelated to this feature, but I think the usage of \"Default\" here is confusing now that inheritance happens. It is not the \"Default Service Account\", it is the \"Service Account\" to use for that destination as far as I understand.\r\n```suggestion\r\n                        <div className='columns small-5'>ServiceAccount</div>\r\n```"
      ],
      "argo-cd-use-configuration-constants": [
        "It is already possible to configure which resources update to ignore with customizations. https://argo-cd.readthedocs.io/en/stable/operator-manual/reconcile/ . Have you tried to use this feature?\r\n\r\nAdditionally, if you do not manage EndpointSlice manually, you can fully omit them from the watched resources. https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#resource-exclusioninclusion"
      ],
      "argo-cd-document-observability-prerequisites": [
        "Format table + add info block that all `argocd_github_api` commands will only be enabled when the flag is configured."
      ],
      "argo-cd-remove-unnecessary-elements": [
        "don't unnecessarily quote yaml. If quoting is necessary, use single quote."
      ],
      "argo-cd-api-documentation-clarity": [
        "```suggestion\r\n* `pullRequestState`: PullRequestState is an additional MRs filter to get only those with a certain state. By default all states. Default: \"\" (all states). Valid values: `\"\"`, `opened`, `closed`, `merged` or `locked`. (Optional)\r\n```"
      ],
      "argo-cd-use-clear-descriptive-names": [
        "nitpick on the name, but having a \"type\" be a validation regexp is not really intuitive.\r\n\r\n```suggestion\r\n            [\"format\"] = \"^[0-9]*$\",\r\n```"
      ],
      "argo-cd-provide-comprehensive-explanations": [
        "You should document the caveats for the user such as conflict with an HPA, or conflict when auto-sync is enabled and replicas are defined as code."
      ],
      "argo-cd-standardize-commit-tracing-metadata": [
        "By default, support (or not) Argocd-related-commit-type as a header. If it is not specifid, hydrator can decide to always assume git\r\n\r\n```suggestion\r\n{\r\n  \"references\": [\r\n    {\r\n      \"type\": \"commit\"\r\n      \"author\": \"Author Name <author-email>\",\r\n      \"sha\": \"<code-commit-sha>\",\r\n      \"message\": \"Commit message of the code commit\",\r\n      \"repoURL\": \"https://git.example.com/owner/repo\",\r\n      \"date\": \"2025-06-09T13:50:18-04:00\"\r\n    }\r\n  ]\r\n}\r\n```",
        "```suggestion\r\n  --trailer \"Argocd-reference-commit-message: Commit message of the code commit\" \\\r\n```"
      ],
      "argo-cd-extract-testable-units": [
        "extract all the test setup code to a function, the method should be generic enough so that it accepts a liveObj and returns only the client. You can then provide the faeClient to the \"import\" method (needs refactoring) and validate the result by doing a Get on the liveObj after the import.",
        "This test seems to re-implement the logic of the run function.\r\n\r\nThe unit test should be able to test a unit of code for which it can mock its dependencies. In this case, we want to test the NewImportCommand `Run` function. However, it is quite complex to provide a \"mock\" of the Kubernetes dependency this way. Instead, extract a function that you can test, that will receive a fake Kubernetes client and the  command arguments. This way, you can unit test that code.\r\n\r\nExample: `func (opts *importOpts) executeImport(client *dynamic.DynamicClient)`"
      ],
      "argo-cd-explicit-security-controls": [
        "Should we set the default behavior to `false` (reverse the flag) so we are \"secure\" by default. Since the Sync action has to be called by a client, users will be able to upgrade Argo CD with the new behavior without affecting their Application. \r\n\r\nIf users need tome to apply the override permissions, then they can disable the flag."
      ],
      "argo-cd-document-network-requirements": [
        "Link to the ingress documentation. The underlying user infrastructure may diverge too much from the example app.\r\n\r\n```suggestion\r\nThe api path `/api/webhook` of the `argocd-applicationset-controller` service on the `webhook` named port must be configured as part of your [ingress](./ingress.md).\r\n```",
        "You can add the fields to the example above. (for both generators)\r\n\r\n```yaml\r\n        # If true, skips validating the SCM provider's TLS certificate - useful for self-signed certificates.\r\n        insecure: true\r\n        # Reference to a ConfigMap containing trusted CA certs - useful for self-signed certificates. (optional)\r\n        caRef:\r\n          configMapName: argocd-tls-certs-cm\r\n          key: azure-devops-ca\r\n```"
      ],
      "argo-cd-comprehensive-function-documentation": [
        "out of scope of this refactor",
        "any change suggestion on the current docstring?",
        "Description updated to reflect the intent of the function and added doc for newRevisionHasChanges param usage. "
      ],
      "argo-cd-prefer-early-returns": [
        "Invert if condition to return early. It will be more readable now that the method grew in size"
      ],
      "argo-cd-validate-configuration-appropriateness": [
        "Missing a check to set `obj.spec.syncPolicy.automated.enabled = false` if it is currently true"
      ],
      "argo-cd-complete-configuration-examples": [
        "This PR should update the default kustomize manifests in `manifests/` to use that variable and mount the argocd-redis secret"
      ],
      "argo-cd-structured-logging-practices": [
        "This change would mean refactoring the variables so we have a map of Applications instead of a list. This change is outside the scope of this refactor",
        "any ways to have a logger with some context like the revision / and git repo ?\r\n\r\nIt will be hard to know what caused that error"
      ],
      "argo-cd-choose-appropriate-synchronization-primitives": [
        "I think when it timeouts, this will call `cancel()` on the context which will in turn close the `appEventCh` causing the for-range loop below to break. The last call will be `_ = printFinalStatus(app)`.\r\n\r\nThis would mean that the AfterFunc should make sure that\r\n1. It is not also calling `printFinalStatus` âœ… \r\n2. it should set `refresh = false` to make sure that the last call to `printFinalStatus` will not refresh the app.\r\n3. it should call `app, err = appClient.Get(ctx, &application.ApplicationQuery` to update the `app` (without refresh) so it is used by printFinalStatus.\r\n\r\nI haven't debugged if it is really what the execution does, but it should be testable in a unit test similar to `TestWaitOnApplicationStatus_JSON_YAML_WideOutput`.\r\n\r\nThere are also a few other problem with the code like the connection not being closed in the AfterFunc, and potential race conditions with refresh and app that might now require a lock. TBD",
        "immediately call `defer cancel()`",
        "Does the value of the timestamp really matter? Or what matters is that we received the log correctly?\r\n\r\n\"timestamp <= now\" seems more reliable. I don't think it is works mocking now, but ideally this is what should be done.",
        "updated to use struct type"
      ]
    }
  },
  "apparentlymart": {
    "repos": [
      "opentofu/opentofu"
    ],
    "entries": [
      {
        "slug": "opentofu-clear-concise-documentation",
        "title": "Clear concise documentation"
      },
      {
        "slug": "opentofu-contextualize-security-findings",
        "title": "Contextualize security findings"
      },
      {
        "slug": "opentofu-craft-actionable-errors",
        "title": "Craft actionable errors"
      },
      {
        "slug": "opentofu-document-intent-and-limitations",
        "title": "Document intent and limitations"
      },
      {
        "slug": "opentofu-document-reference-standards",
        "title": "Document reference standards"
      },
      {
        "slug": "opentofu-document-with-examples",
        "title": "Document with examples"
      },
      {
        "slug": "opentofu-explicit-versus-dynamic-configurations",
        "title": "Explicit versus dynamic configurations"
      },
      {
        "slug": "opentofu-log-effectively-for-debugging",
        "title": "Log effectively for debugging"
      },
      {
        "slug": "opentofu-minimize-api-surface",
        "title": "Minimize API surface"
      },
      {
        "slug": "opentofu-names-preserve-cognitive-context",
        "title": "Names preserve cognitive context"
      },
      {
        "slug": "opentofu-prevent-backing-array-surprises",
        "title": "Prevent backing array surprises"
      },
      {
        "slug": "opentofu-proper-span-lifecycle",
        "title": "Proper span lifecycle"
      },
      {
        "slug": "opentofu-provider-instance-management",
        "title": "Provider instance management"
      },
      {
        "slug": "opentofu-safe-lock-patterns",
        "title": "Safe lock patterns"
      },
      {
        "slug": "opentofu-specify-configuration-behaviors",
        "title": "Specify configuration behaviors"
      },
      {
        "slug": "opentofu-structure-tests-thoroughly",
        "title": "Structure tests thoroughly"
      }
    ],
    "comments": {
      "opentofu-provider-instance-management": [
        "This section is only here to connect with the docs about the `providers` argument in `module` blocks (linked below), which I think is the more natural home for that information.\r\n\r\nAs the moment that's only lightly _implied_ by the words on the page, rather than explicitly stated, but I'll add an extra paragraph to the other page to make that explicit."
      ],
      "opentofu-safe-lock-patterns": [
        "When I looked at this with you quickly earlier I didn't spot that this is effectively using the `lockResults` channel as a funny sort of concurrency-safe slice, relying on the first loop writing to the channel exactly the same amount of times as the second loop reads the channel because they are both iterating over the same collection.\r\n\r\nThis seems technically correct, but potentially confusing and at risk of being broken under future maintenance. I wonder if we could split the difference and use a wait group for the generation process and but a channel to consume what's being produced so that the consumer loop is directly synchronized with the producer loop.\r\n\r\nFor example:\r\n\r\n```go\r\nlockResults := make(chan lockResult, len(platforms))\r\ngo func() {\r\n    var wg sync.WaitGroup\r\n    for _, platform := range platforms {\r\n        wg.Add(1)\r\n        go func(platform getproviders.Platform) {\r\n            // just like before, except now ending with...\r\n            wg.Done()\r\n        })\r\n    }\r\n    wg.Wait()\r\n    close(lockResults)\r\n}()\r\n\r\nfor result := range lockResults {\r\n    // Exactly the same as your current \"for range platforms\" loop\r\n}\r\n```\r\n\r\nSlightly more machinery, but maybe makes the one-to-one relationship between iterations of these loops a little more explicit, and makes it harder for inconsistencies to creep in under future maintenance.\r\n\r\n---\r\n\r\nIt seems like the function for that outer goroutine could be turned into a generic helper function that we could reuse across many problems like this, though I'd personally wait to see how many more times this comes up before adding it since we don't really have a good place to dump a general-purpose thing like this. :grinning: \r\n\r\n```go\r\n// ConcurrentEach calls f concurrently for all elements of from,\r\n// waits until all calls have returned, and then closes into\r\n// to signal completion before returning.\r\nfunc ConcurrentEach[In, Out any](from []In, into chan<- Out, f func(item In, into chan<- Out)) {\r\n  var wg wg sync.WaitGroup\r\n  for _, item := range from {\r\n    wg.Add(1)\r\n    go func(item In) {\r\n      f(item, into)\r\n      wg.Done()\r\n    }(item)\r\n  }\r\n  wg.Wait()\r\n  close(into)\r\n}\r\n```\r\n\r\n```go\r\nlockResults := make(chan lockResult, len(platforms))\r\ngo ConcurrentEach(platforms, lockResults, func (platform getproviders.Platform, lockResults chan<- lockResult) {\r\n    // ...\r\n})\r\nfor result := range lockResults {\r\n    // ...\r\n}\r\n```\r\n",
        "This seems reasonable for solving exactly the problem as stated, but it seems like if we were to move this down to just before [line 568](https://github.com/opentofu/opentofu/blob/f653350af64ce92ed5c516690f6ae089e0802492/internal/providercache/installer.go#L568) then we could rely on some of the conditionals we already have in place for the global vs. local situation, and also get locking of the _local_ directory at the same time, which is not quite as popular a request but still something I've seen folks ask about before.\r\n\r\nSpecifically I'm thinking about adding something like the following just before the line I indicated:\r\n\r\n```go\r\nunlockInstallTo, err := installTo.Lock(ctx, provider, version)\r\n// (...error handling...)\r\nvar unlockLinkTo func()\r\nif linkTo != nil {\r\n    unlockLinkTo, err = linkTo.Lock(ctx, provider, version)\r\n    // (...error handling...)\r\n}\r\n```\r\n\r\n...and then unconditionally call `unlockInstallTo` and conditionally call `unlockLinkTo` afterwards.\r\n",
        "I guess what I wrote above means that we'd be doing the `tryInstallPackageFromCacheDir` step without the lock held, which is okay as long as we're only trying to lock the global cache directory -- `tryInstallPackageFromCacheDir` only _reads_ from the global cache directory -- but would not protect the local cache directory. (Not that protecting the local cache directory was a requirement to begin with; just a \"would be nice\".)\r\n\r\nGiven that this locking strategy only applies to _writers_ to the cache directories anyway (concurrent readers can still observe partially-filled package directories), I'm now wondering instead about making `Dir.Lock` be unexported and calling it from the two methods in `dir_modify.go` as an implementation detail, which would help ensure that _all_ cache directory modifications can run concurrently and also address that concern about the various codepaths that neglect to call `unlock`, because `Dir.InstallPackage` can make sure to call `unlock` itself before returning.\r\n",
        "You are right that if we delay acquiring the lock until we get inside `Dir.InstallPackage` then that function would also need to check (immediately after it acquires the lock) whether the desired package had been installed by another process in the meantime.\r\n\r\nFor `InstallPackage` I'm imagining something like this (just a pseudocode-ish sketch, not final code):\r\n\r\n```go\r\nfunc (d *Dir) InstallPackage(ctx context.Context, meta getproviders.PackageMeta, allowedHashes []getproviders.Hash) (*getproviders.PackageAuthenticationResult, error) {\r\n\tif meta.TargetPlatform != d.targetPlatform {\r\n\t\treturn nil, fmt.Errorf(\"can't install %s package into cache directory expecting %s\", meta.TargetPlatform, d.targetPlatform)\r\n\t}\r\n\tnewPath := getproviders.UnpackedDirectoryPathForPackage(\r\n\t\td.baseDir, meta.Provider, meta.Version, d.targetPlatform,\r\n\t)\r\n\r\n\tunlock, err := d.Lock(ctx, meta.Provider, meta.Version)\r\n\t// (...error handling...)\r\n\tdefer unlock()\r\n\r\n\tif existsAndIsDirOrSymlinkToDir(newPath) {\r\n\t\td.metaCache = nil // another process has modified the cache directory\r\n\t\tif cached, err := d.ProviderVersion(meta.Provider, meta.Version); err != nil && cached != nil {\r\n\t\t\tmatches, err := cached.MatchesAnyHash(allowedHashes)\r\n\t\t\tif err == nil && matches {\r\n\t\t\t\treturn someSuccessfulResult()\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Invalidate our metaCache so that subsequent read calls will re-scan to\r\n\t// incorporate any changes we make here.\r\n\td.metaCache = nil\r\n\r\n\tlog.Printf(\"[TRACE] providercache.Dir.InstallPackage: installing %s v%s from %s\", meta.Provider, meta.Version, meta.Location)\r\n\treturn meta.Location.InstallProviderPackage(ctx, meta, newPath, allowedHashes)\r\n}\r\n```\r\n\r\nThis does admittedly have an interesting implication: if we end up at the `return someSuccessfulResult` statement here then _this process_ will not have applied any package authentication rules (from `meta.Authentication`) to the directory, meaning that it's assuming that the other process did that correctly _and_ the \"some successful result\" would not be able to include any verified hashes beyond perhaps the ones that caused `MatchesAnyHash` to return true.\r\n\r\nI think that _does_ technically match the current treatment of cache entries -- `tryInstallPackageFromCacheDir` also only checks that the checksum matches -- but is admittedly quite a different guarantee than this method was previously providing. If that seems acceptable though, I wonder if there's enough overlap between \"is the package in that _other_ cache acceptable to use?\" and \"is the package in _this_ cache acceptable to use?\" that we could factor out the relevant subset of logic from `tryInstallPackageFromCacheDir` and use it from both places.\r\n\r\nOverall here my goal is to limit the time spent holding the lock and to keep the lock handling as an implementation detail of `Dir.InstallPackage`, rather than exposing it as a separate thing that callers need to remember to handle. (and acquire the lock in `Dir.LinkFromOtherCache` too, so that full installation can't race with linking at the same location if someone has a really weirdo configuration.)\r\n",
        "It seems like there are a few places above where we end the iteration of the loop early without calling `unlock`, which I guess is probably not a huge deal since the process will eventually exit and implicitly release the lock, but it we did a little more work down the path I started in https://github.com/opentofu/opentofu/pull/2166 then we could perhaps factor the body of this loop out into a separate function and use `defer unlock()` to make sure it always gets called on every return path, so that we're not at risk of any quirky behavior in the uncommon cases where we terminate early without unlocking.\r\n\r\n(I don't think we should do that refactoring as part of this PR though, since that sort of thing is better reviewed on its own.)",
        "Oh, I see! I think I misunderstood this when I read it the first time because I didn't consider that (because this is running sequentially) there can only possibly be one outstanding `unlock` at a time, and so it's okay to use only a single `defer` to clean up only that one.\r\n\r\nFair enough!\r\n"
      ],
      "opentofu-document-with-examples": [
        "As things currently stand, there are two kinds of plugins in OpenTofu: provider plugins and provisioner plugins. \r\n\r\nProvisioner plugins are largely a legacy system preserved as a concession for those who need to use some of the previously-builtin provisioners that were removed in an earlier release. None of the automatic installation, registry protocol, and signature-checking infrastructure that we're accustomed to for provider plugins are available for the legacy provisioner plugins.\r\n\r\nSo I think it's fair to say that _as we currently stand_, \"plugin\" and \"provider\" are _effectively_ synonymous, aside from some little callout boxes in a few spots that acknowledge the legacy provisioner plugin support such as on [Managing Plugins](https://opentofu.org/docs/cli/plugins/).\r\n\r\nThe text below intentionally asks us to reflect on whether we want that to continue to be true. There are some definite advantages on the OpenTofu CLI side in being able to reuse our existing plugin distribution infrastructure and \"just\" add a new optional extension protocol to the plugins, but we've yet to research what it would look like for a provider developer to implement an additional optional protocol alongside the provider protocol if they are using the currently-prominent libraries for plugin development; the author of an RFC on this topic would presumably investigate whether it's possible to use those libraries in conjunction with a second gRPC service that's implemented in a separate library.\r\n\r\n"
      ],
      "opentofu-log-effectively-for-debugging": [
        "We don't typically expect end-users to notice messages in the logs, so if we want folks to report this as a bug then it should probably be either a normal error diagnostic or a panic.\r\n\r\nHowever, we do also ask folks to send us the trace logs when they report a strange behavior, so it would be a reasonable compromise to just remove the \"This should not happen...\" part of this message and rework this to be something we can notice in the trace logs that folks send us:\r\n\r\n```\r\n[ERROR] tofu.isCallFromRemote: no module call found in foo for bar\r\n```\r\n\r\n",
        "I think making log noise here is a tricky tradeoff because having a mixture of OpenTofu manifests and non-OpenTofu manifests in the same index is a reasonable thing to do when integrating with other conventions in the OCI ecosystem.\r\n\r\nIf someone writes a manifest where _all_ of the artifact types are incorrect for OpenTofu then the current code would handle that by saying that the provider doesn't support the target platform, which would be confusing. So I _will_ change this, but I'm thinking about a different change similar to the approach I took for selecting from the \"layers\" in the image manifest: it'll count how many of the listed manifests have the correct artifact type but the wrong platform, and also how many listed manifests have the wrong artifact type, and then at the end of the loop if we have no manifests with the correct artifact type but we have at least one with the incorrect artifact type then we can return an error message that says that the index manifest is incorrect, instead of reporting the \"platform not supported\" error.\r\n\r\n"
      ],
      "opentofu-contextualize-security-findings": [
        "Would it be possible to also include the name of the affected module in this report so that we can include it in the summary of the issues that get created?\r\n\r\nOpaque identifiers like GO-2025-3588 are hard to distinguish quickly without some other context, and so I think including the name of the module that the vulnerability relates to will make it easier for us to find specific vulnerability issues once there are many of them.\r\n\r\n(However, I must admit I'm not sure how to incorporate that extra requirement into this pipeline... making this more advanced might require writing this in a different language that has better support for manipulating data structures, and so maybe better to wait and see if we need it before making things even more complicated.)\r\n",
        "An alternative compromise I thought of is to make sure that the code which searches for an existing issue can tolerate extra text having been added manually to the issue title after it was automatically created, and then we could edit the title with a small amount of additional context after we have reviewed the report and understood what it affects, as long as we leave the vulnerability number in the issue title when we edit it. \r\n\r\nDo you think that would work?\r\n\r\n",
        "I am particularly interested in being able to include something in the issue _title_ because that makes it easier to view many issues together in the main GitHub issues list UI, and in the \"Project\" view that we currently use for situations like triage.\r\n\r\nI don't want to have to open many different tabs to find the comments that distingish a bunch of issues whose titles are all \"GO-NNNN-NNNN reported\". However, now that you've confirmed that it's okay to modify the title as long as we preserve the keywords from the search query, I think this concern is resolved for me: I imagine that the person who responds to an issue created by this automation would add more text to the title (preserving the \"GO-NNNN-NNNN reported\" prefix) summarizing the problem that the upstream advisory described.\r\n\r\n---\r\n\r\nIn the associated RFC I had been imagining that the automated system would create issues and then [we would _manually_ create advisories based on those issues](https://github.com/opentofu/opentofu/blob/754d7eb58b737d0214f5811320e59c78445e0646/rfc/20250314-security-patch-policy.md#sharing-our-conclusions) once we've reviewed them and addressed any problems. In that sense, the issue represents the need to do the work while the advisory represents the _result_ of that work.\r\n\r\nI honestly hadn't considered the possibility of directly creating a draft advisory using automation; that does seem like an interesting idea but since you've already found the API for that is annoying to work with I don't personally have any problem with retaining the original idea of using issues as the primary way for the automation to communicate with us. At least, we could start with that and see if it seems worth doing the extra work to interact with the security advisory API.\r\n"
      ],
      "opentofu-structure-tests-thoroughly": [
        "One way to avoid the potential pitfall here would be to have this function also take a `t *testing.T` argument and then use `t.TempDir()` to get a directory to place all of the files into. The Go test harness will automatically clean those up itself when the test ends.\r\n\r\nI think we can call `t.TempDir` just once before the loop and then use `filepath.Join` on each iteration to append a filename derived from `filepath.Base(file.filePath)` (assuming that `filePath` is set) to that single temporary directory, which would then have the nice advantage of the temporary files having a more meaningful filename in case they appear in any test failure messages.\r\n\r\nHaving a `*testing.T` in here also means that we can use `t.Fatal` instead of `log.Fatal` so that an error will only abort the current test, rather than aborting the entire test run. Make sure to call `t.Helper()` as the first statement in the function so that the test harness knows that it should treat this as a test helper function when it is choosing a source line to report in its generated error message.\r\n\r\n(Similar idea for the `testFile.tempFileWriter` method below too, though of course that one will need to make a separate temp directory for each file since it only works on one file at a time.)",
        "`t.TempDir` is a relatively recent addition to the Go standard library. We have _lots_ of old code that predates it being there and handles temporary directories in some other manual way, so certainly no judgement from me for this being new to you... it's pretty new to _us_ too. :grinning: \r\n\r\nIn case it's interesting for your future endeavors (since I don't think we really need it _here_), there is also the more general [`testing.T.Cleanup`](https://pkg.go.dev/testing#T.Cleanup) which arranges for an arbitrary function to be called at the conclusion of the calling test, and so this can be used for resources other than temporary directories that nonetheless ought not to outlive the scope of a single test run.\r\n"
      ],
      "opentofu-craft-actionable-errors": [
        "To me the `for_each` part of this at the end is largely irrelevant: it's not valid to refer to an alternative provider configuration that isn't declared _regardless of whether `for_each` is used or not_ so I think mentioning it is likely to cause more confusion than it saves.\r\n\r\nAs things are currently structured it's true that we'll only be reporting the problem this way when there's an instance key specified, but that's just an implementation detail of how this validation is implemented and not relevant to the end-user.\r\n\r\n(Folks reading error messages tend to think that everything they read is directly related to the problem at hand, so including extraneous details tends to cause people to try to solve the wrong problem.)\r\n",
        "Perhaps, but what are you expecting that the reader of this message would do differently based on that extra clause? It doesn't seem like it adds anything the reader needs to know in order to correct the problem. :thinking: \r\n\r\nAre you worried about the hypothetical person who has a child module specifying `configuration_aliases = [postgresql.by_db]` and is expecting that to allow referring to a multi-instance provider passed from the caller? It is true that currently that situation ends up here too, but I don't think that the extra clause you added actually helps diagnose it, because it doesn't say anything at all about `configuration_aliases`...\r\n\r\n\r\n\r\n",
        "OpenTofu error messages don't usually say \"please\", so I'd suggest the following tweak for similarity to the existing \"diagnostic voice\":\r\n\r\n> The \"deprecated\" argument must not be empty, and should provide instructions on how to migrate away from usage of this deprecated output value.\r\n\r\n\r\n\r\n",
        "The problem here is that because OCI was originally intended only for distributing container images there was originally no such thing as an \"artifact type\" -- container images were the only artifact type.\r\n\r\nSome projects adopted OCI for non-container-image artifacts before the OCI spec was updated to include the \"artifact type\" concept, and so the standard image layout for those projects does not include an explicit artifact type at all, and instead software \"guesses\" the artifact type based on other information in the manifest.\r\n\r\nThe fact that there's an optional `artifactType` property in the manifest format is an implementation detail of the protocol that I don't expect the reader of this error message to be aware of, and so I think it would be confusing to talk about it in the error message. Instead, this generic error message is attempting to talk about this in a way that's more likely to be relevant to the end-user: this is an artifact of a type we don't support. It's unfortunate that we can't say what type of artifact it is, but it does have an _implied_ artifact type (based on data in the manifest), so I think it would be confusing to say that this artifact does not have a type _at all_.\r\n\r\nThe main way someone would encounter this message is if they've selected a container image instead of an OpenTofu provider image, so this seems like a good situation for the error message to include a question-shaped suggestion:\r\n\r\n```\r\nunsupported OCI artifact type; is this a container image, rather than an OpenTofu provider?\r\n```\r\n\r\nPhrasing it as a question allows us to present a likely explanation even though we don't know for certain whether it's true. For example, the selected artifact might actually be a Helm chart since those _also_ don't use `artifactType`, but that seems considerably less likely for someone to select by accident.\r\n\r\nI'm going to change the error message to something similar to the above. Hopefully that's a reasonable compromise?\r\n",
        "I did consider this, but these digests are quite long and so can make these messages hard to read, and if we get to this point there are only a small number of explanations for this unlikely error message:\r\n\r\n1. The connection to the OCI registry server was interrupted somehow in a way that caused the received data to be truncated.\r\n2. The OCI registry server is implemented incorrectly and so is returning the wrong data.\r\n3. The OCI registry server has been compromised by an attacker and so is _intentionally_ returning the wrong data, in the hope that some client software won't actually verify the digest.\r\n\r\nOf these three explanations, only in the second case could the digest of the content we actually received be _potentially_ useful, and even then only if the reader of the error message already knows of some content with that digest that might've been somehow returned by mistake, but even that seems quite unlikely. Therefore I chose to focus this error message only on describing what OpenTofu was _intending_ to fetch, because that's an identifier that the reader of the error message can potentially try to retrieve themselves using other software to try to understand what has happened.\r\n\r\nTherefore I think the shape of the error message I wrote makes the best compromise of giving the reader the information needed to debug further, without the confusion of including two very long strings of opaque gibberish where one of them is unlikely to actually communicate anything useful.\r\n\r\nDoes that seem like a reasonable justification?",
        "By the time we reach this point the provider installer will already have reported which version it has selected for each provider -- that happens after calling `AvailableVersions` and before calling `PackageMeta` -- so we don't need to restate it in the individual error messages like this.\r\n",
        "```suggestion\r\n\t\t\t\tSummary:  \"Reference to undeclared key provider\",\r\n\t\t\t\tDetail:   fmt.Sprintf(\"There is no key_provider %q %q block declared in the encryption block.\", depType, depName),\r\n```\r\n\r\nTo make this more consistent with [the similar message for resource references](https://github.com/opentofu/opentofu/blob/35368d990955f225d0fd72ba6409563c992733e1/internal/tofu/evaluate.go#L658-L664).\r\n"
      ],
      "opentofu-explicit-versus-dynamic-configurations": [
        "I am still a little concerned that we're presumably going to need to commit to `main` every time our set of supported versions changes, thereby causing risk that we forget about it and creating clutter in the git history that's unrelated to the actual product we're building, but I also see that this changes infrequently enough that it'll probably be fine. :+1: \r\n\r\nIt's a little awkward that the `main` branch switches to tracking the next release at the time we release our first beta, but the oldest supported version reaches end of life only when we make  the _final_ release for a new series, so I guess we'll need to update this as a separate step once we release v1.10.0 final, rather than including it as part of the work to create the v1.10 branch and have `main` start to represent v1.11 development. If we find that we tend to forget to do this in future releases then I'd consider that a prompt to revisit this decision and consider a different approach, but I think it's fine to wait to see what happens.\r\n"
      ],
      "opentofu-minimize-api-surface": [
        "Does this type need to be exported?\r\n\r\nI see that it's used in some of the tests in `package tofu` but I wonder if we could encapsulate this a little better so that only `package marks` needs to know how this works internally. For example, we could export from _this_ package a function that produces a function that matches the signature used by the last argument of `tfdiags.Override`, and then use that function directly as the argument:\r\n\r\n```go\r\nfunc DeprecatedOutputDiagnosticOverride(cause DeprecationCause) func() tfdiags.DiagnosticExtraWrapper {\r\n    return func () tfdiags.DiagnosticExtraWrapper {\r\n        return &DeprecatedOutputDiagnosticExtra{\r\n            Cause: cause,\r\n         }\r\n    }\r\n}\r\n```\r\n\r\nHonestly the design of `tfdiags.Override` is kinda complicated and maybe one day we can find a way to simplify it a little so this extra function isn't needed, but hopefully something like the above could at least keep most of the details encapsulated in `package marks`?\r\n\r\n(This is not super important, so if this is particularly hard to do then I'm okay with not bothering. Just trying to think about ways to minimize how much exported API surface we have in each package.)\r\n",
        "One thing I noticed looking at this signature is that it only accepts a single file in each case. I suppose this is okay, but it's a little inconsistent with `-var-file` where we allow any mix of any number of `-var` and `-var-file` options on the command line.\r\n\r\nI expect we could extend this to support multiple `-target-file` or `-exclude-file` options later without any backward-compatibility problems and so starting with the simpler case is probably fine... but I figured I'd raise it now in case you think that the more flexible design is straightforward enough to implement immediately.\r\n\r\n"
      ],
      "opentofu-document-reference-standards": [
        "I don't think we explicitly decided this and so it's just been informal, but: historically the pattern was to link to one or more pull requests that represent the work the changelog entry described, but when we introduced the idea of tracking issues for larger projects it became easier to create just one link to the tracking issue rather than many links to all of the PRs that happened because of it. A tracking issue is not the same thing as a feature request or bug report issue despite the fact that we use the same GitHub features for all three: a tracking issue is directly describing some specific work to be done, rather than a broad problem to be solved somehow.\r\n\r\nIn simpler cases where there's only one PR associated with a single enhancement/bug issue it's less clear to me what we ought to do. The PR is a direct description of what changed and so that feels most intuitive to me given that the changelog is a log of changes and the PRs should link to their corresponding issues anyway.\r\n\r\nBut in practice our issues tend to describe changes in more end-user-oriented terms, so I can see the argument that they might be the more useful reference if you just want to know what you can do now that you could not do before. ðŸ¤”\r\n\r\nWe should probably discuss this in a different setting so we can think about the tradeoffs together as a team. I would suggest we don't block this PR on this question since we can always edit the changelog before release of we decide to do something different."
      ],
      "opentofu-document-intent-and-limitations": [
        "Unfortunately the _published_ versions of this spec seem to be available only as documents attached to GitHub releases, and the server-side for those URLs returns headers that force treating the file as a \"download\" rather than rendering it directly in the browser, so I wasn't sure how best to link to them.\r\n\r\nHowever, looking again today I notice that the specification text in the repository is also available under a git tag corresponding to the release, and so I guess this link is a plausible reference:\r\n\r\nhttps://github.com/opencontainers/distribution-spec/blob/v1.1.0/spec.md#pushing-manifests\r\n\r\n(The recommendation comes from the last paragraph of that section.)\r\n\r\nI'll push a new version with this link included. Thanks!\r\n",
        "I know this outside the scope of what you were doing here, but the comment on this `DecodeConfig` function says that it's only used in tests but I can see a call to it in `configs.loadConfigFile` that seems to disagree, so maybe worth correcting that comment while you're working in this area anyway?\r\n",
        "Suggest replacing this with some words in this function's doc comment that are clear that the scope of this function is intentionally limited only to simple references and function calls with arguments that are simple references, and that everything else always returns `false`, to make it clear that we are not expecting to continue adding to this each time HCL adds a new feature upstream.\r\n"
      ],
      "opentofu-names-preserve-cognitive-context": [
        "Very minor: In the spirit of [Naming convention for internal variables representing \"contexts\"](https://github.com/opentofu/opentofu/blob/1421849989cf4a65d72db8af107c0b5f70be8c0e/rfc/20250108-naming-convention-for-context-vars.md) (whose main motivation that we try to use `ctx` only for `context.Context` values), I suggest that we rename this `ctx` argument to `hclCtxFunc` so that it's easier to quickly understand both that it returns `*hcl.EvalContext` (rather than `context.Context`) and that it's a function that returns the context rather than the context directly.\r\n\r\n(Maybe we could rename the `ContextFunc` type to `HCLContextFunc` too for consistency, though since that's exported that might be more invasive in which case I'd suggest that we save it for a later PR.)\r\n"
      ],
      "opentofu-prevent-backing-array-surprises": [
        "While I agree with removing this `nolint` comment, I do find these two statements a little suspicious:\r\n\r\nIf `len(cmd) == cap(cmd)`  here then this would cause `encryptCommand` and `decryptCommand` to both have newly-allocated backing arrays that both share a prefix with `cmd`.\r\n\r\nBut if `len(cmd) < cap(cmd)` then I think these would both end up with the same backing array and the last item set to `--decrypt`, which is presumably wrong.\r\n\r\nBoth callers of this function seem to populate `cmd` using a composite literal:\r\n\r\nhttps://github.com/opentofu/opentofu/blob/ec4e0cf0e2cefcca1d2ace792416c4e584c440a6/internal/encryption/method/external/testmethod/testmethod.go#L58\r\n\r\nhttps://github.com/opentofu/opentofu/blob/ec4e0cf0e2cefcca1d2ace792416c4e584c440a6/internal/encryption/method/external/testmethod/testmethod.go#L76\r\n\r\n...and the Go spec doesn't seem to specify what the capacity is of a slice created with a slice literal, but some experimentation on the Go playground suggests that, at least for small slices, the composite literal syntax currently allocates a backing array exactly the right size for the length of the slice, which I think is saving us from the bug that the linter was trying to warn about here. Therefore this currently works but is at risk of becoming accidentally broken in future if the callers start building these slices in a different way.\r\n\r\nOne way to resolve this while still retaining a similar code style would be to explicitly remove any excess capacity from the source slice before appending to it, and now we've upgraded to Go 1.24 there's [a helper function that's (subjectively) more readable than the three-clause slice syntax](https://pkg.go.dev/slices#Clip):\r\n\r\n```go\r\ncmd = slices.Clip(cmd)\r\nencryptCommand := append(cmd, \"--encrypt\")\r\ndecryptCommand := append(cmd, \"--decrypt\")\r\n```"
      ],
      "opentofu-clear-concise-documentation": [
        "Let's make sure that these consistently use the â”‚prefix on all lines -- either including it or omitting it -- since I think right now it's a little unclear whether the first line of each of these is part of the error message or something else. (OpenTofu CLI includes these markers to help sighted users more quickly notice the boundaries between different diagnostic messages, using visual hierarchy.)\r\n\r\nI'd suggest also generating these with your terminal set to about 72-ish columns wide so that the hard-wrapping will generate shorter lines that should be easier to read on narrower screens without a lot of horizontal scrolling. (I must admit I'm not sure what is a good width for the current way `opentofu.org` is laid out on narrow screens; 72 is just an arbitrary number that we previously used for some other error messages.)\r\n",
        "FWIW, the prevailing writing style for the docs avoids things like \"Please\" and prefers to just directly assert facts, instructions, or recommendations, and so I agree with the comment above and would personally broaden that to _all_ of our documentation, rather than just this one example.\r\n\r\nI don't have a strong opinion on \"use caution\" vs. \"be cautious\". They both seem valid.\r\n\r\nThe only example I could find of something _close_ to this in the existing docs is:\r\n\r\nhttps://github.com/opentofu/opentofu/blob/ff4c84055065fa2d83d318155b72aef6434d99e4/website/docs/language/modules/develop/refactoring.mdx?plain=1#L428\r\n\r\n...which does seem closer to \"use caution\" than \"be cautious\", but isn't exactly the same as either!\r\n\r\n",
        "Yes, that is correct.\r\n\r\nI was reluctant to make this text very long by giving lots of examples because the implementation is already built to give direct feedback about this if an author uses it incorrectly.\r\n\r\nThe fact that you correctly understood the intention without any additional words about it makes me think we should try too keep this as it is currently written and wait to see if others find it confusing in practice. It's been my experience that readers tend to skip unnecessarily long documentation, and so I'd like to try with only one example to see if this is sufficient before we make it longer.\r\n\r\nWhat you think?"
      ],
      "opentofu-specify-configuration-behaviors": [
        "Given that everything we add in a stable release is forever, I would strongly recommend _not_ adding things that don't have a clear use-case just because they seem easy to implement."
      ],
      "opentofu-proper-span-lifecycle": [
        "The promised behavior for ORAS-Go is that it always calls `ExecuteDone`, whether the execution succeeds or not, and [the current implementation](https://github.com/oras-project/oras-go/blob/753f8a8d98a5ed950366b5580c7edbd204fa5630/registry/remote/credentials/internal/executer/executer.go#L57-L64) seems straightforward and clearly correct in that regard, so I'm personally not concerned about this.\r\n\r\nThe Go OTel tracing documentation doesn't specify whether it's okay to call `span.End` multiple times, but if you know of some other source that promises that multiple calls are always okay then I'd be happy to _also_ include a `defer` for robustness, but in the normal case I think the `span.End` ought to be inside `ExecuteDone` because otherwise this span will be measuring more than what it claims to be measuring, and in that case it'd probably be better to just skip having it at all since it wouldn't be meaningfully different than the parent span. :thinking: \r\n"
      ]
    }
  },
  "n1t0": {
    "repos": [
      "huggingface/tokenizers"
    ],
    "entries": [
      {
        "slug": "tokenizers-choose-optimal-data-structures",
        "title": "Choose optimal data structures"
      },
      {
        "slug": "tokenizers-choose-semantically-clear-identifiers",
        "title": "Choose semantically clear identifiers"
      },
      {
        "slug": "tokenizers-consistent-api-design",
        "title": "Consistent API design"
      },
      {
        "slug": "tokenizers-flexible-tokenizer-implementation",
        "title": "Flexible tokenizer implementation"
      },
      {
        "slug": "tokenizers-handle-nullable-types-idiomatically",
        "title": "Handle nullable types idiomatically"
      },
      {
        "slug": "tokenizers-minimize-memory-allocations",
        "title": "Minimize memory allocations"
      },
      {
        "slug": "tokenizers-modular-model-components",
        "title": "Modular model components"
      },
      {
        "slug": "tokenizers-optimize-workflow-triggers",
        "title": "Optimize workflow triggers"
      },
      {
        "slug": "tokenizers-prefer-explicit-api-design",
        "title": "Prefer explicit API design"
      },
      {
        "slug": "tokenizers-prioritize-tokenizer-simplicity",
        "title": "Prioritize tokenizer simplicity"
      },
      {
        "slug": "tokenizers-purpose-indicating-descriptive-names",
        "title": "Purpose-indicating descriptive names"
      },
      {
        "slug": "tokenizers-pythonic-api-design",
        "title": "Pythonic API design"
      },
      {
        "slug": "tokenizers-return-results-not-panics",
        "title": "Return results not panics"
      },
      {
        "slug": "tokenizers-simplify-for-readability",
        "title": "Simplify for readability"
      },
      {
        "slug": "tokenizers-smart-configuration-defaults",
        "title": "Smart configuration defaults"
      },
      {
        "slug": "tokenizers-thread-safe-resource-sharing",
        "title": "Thread-safe resource sharing"
      }
    ],
    "comments": {
      "tokenizers-handle-nullable-types-idiomatically": [
        "Is there any reason for `get_as_subtype` to return `PyResult`? If not, this could be rewritten as\r\n```Rust\r\nfn get_normalizer(&self) -> Option<PyObject> {\r\n    self.tokenizer.get_normalizer().map(|n| n.get_as_subtype())\r\n}\r\n```",
        "I was wondering about those optional parameters. If we were to change the default to some value, it wouldn't be possible with the current setup to actually set them back to `None`. How do you think we could handle this?"
      ],
      "tokenizers-prioritize-tokenizer-simplicity": [
        "I don't understand why you want to add these in all the tests. The pre-tokenizer won't get called anyway because all the tokens in the tests come are handled by the `AddedVocabulary`.",
        "The added vocabulary is always handled since it happens first, so no need for a pre-tokenizer. The tests actually started to fail because the BPE is receiving the whitespace between each word, and it doesn't have an `unk_token`. With the `Whitespace` pre-tokenizer, these whitespaces are just removed.\r\n\r\n> We probably should really test the output of those methods in the full to make sure it's consistent, I'm pretty sure all those tests would have caught the missing unk tokens otherwise.\r\n\r\nThe unk token was treated this way by design. When provided we use it, otherwise the unknown tokens just get ignored. This is not a bug we didn't catch. That being said, feel free to add all the tests you find necessary, we never have enough of them :slightly_smiling_face: "
      ],
      "tokenizers-smart-configuration-defaults": [
        "What do you think about always trying to display in a notebook if `IPython` is available, and always returning the content? That way it just displays when possible, without the need to change this value."
      ],
      "tokenizers-optimize-workflow-triggers": [
        "Is this step doing something that the next one doesn't? Otherwise, since the `npm_publish` depends on `rust_publish` we can probably remove it. If one of the builds (in `rust_publish`) for any of the os/node were to fail, it would just fail the whole job anyway, right?\r\nI think it would maybe make more sense to run the `build_all` job for each commit, to make sure we don't break anything in these bindings. What do you think?",
        "Sure but the `Rust` job doesn't check that the bindings still compile after something has been modified in the main library. At each commit, I think we would like to make sure everything is still compiling: the main library, but also all the bindings.",
        "Nice thanks! You are right, at the moment it's totally useless. I don't know what would be better between having a separate workflow for the main library and each bindings, or actually having one main `Build` workflow, that takes care of building everything on each OS, mutualizing the setup parts. What do you think? (This is out of the scope of this PR though)."
      ],
      "tokenizers-return-results-not-panics": [
        "It probably would be better to make `from` return a `Result` and fail gracefully since this is a public API."
      ],
      "tokenizers-modular-model-components": [
        "As we discussed, a lot of these options might be better suited as standalone `Normalizer` or `PreTokenizer`, and removed from there. ",
        "These serve different purpose. `PreTokenizedString::tokenize` is deep inside the `encode` pipeline, and some use-cases require customization."
      ],
      "tokenizers-pythonic-api-design": [
        "The format for merges here seems overly complicated. I agree that we use this specific format internally, but it is probably unnecessary to expose this to the final user. `List[Tuple[str, str]]` is easier to construct from all the different files format that exist out there, and also let us construct the final `merges` easily. It is also probably compatible with a lot more languages.",
        "Sure I understand. The API on the Rust side has been thought with `from_file` as the main entry point though, so this aspect has to be taken care of now that we want to expose this.\r\nWe should change the Rust API accordingly, and let the BPE build the `HashMap<Pair, (u32, u32)>` by itself.",
        "Thank you for this PR @kdexd! If I remember correctly `with_added_tokens` should be a named argument and so I don't think it works if provided as a positional argument."
      ],
      "tokenizers-choose-optimal-data-structures": [
        "`inserted` has been added here because we are changing `pieces` to be a `Vec<(String, f64)>` instead of a `HashMap<String, f64>`, so we still need a way to check if a token has been inserted with `O(1)`"
      ],
      "tokenizers-simplify-for-readability": [
        "You can directly return `normalized.replace(...)` here."
      ],
      "tokenizers-choose-semantically-clear-identifiers": [
        "We can probably keep it as an argument, but I would go for an Enum too. Was thinking about the name, and I find `side` easier to understand maybe, but for consistency with padding, we might prefer `direction`. Wdyt?",
        "I'm not sure it really makes sense directly exposed on the `Tokenizer`. I'd expect this to be the number of added tokens that gets returned by `get_vocab_size(with_added_tokens=True)`.\r\n\r\nThis can simply be retrieved by doing `tokenizer.post_processor.num_added_tokens(is_pair)`",
        "I understand. My main concern is just about having `add_tokens` that does something, and `num_added_tokens` just right beside it, doing something completely different. If we find the right name, we can add it here!",
        "As discussed, let's use `num_special_tokens_to_add`"
      ],
      "tokenizers-consistent-api-design": [
        "Same here. Also, shouldn't the default value be \"right\" (false here)?",
        "See, that's where I'm confused (and why I'm wondering about `direction`). For me the current implementation only supports truncation on the right (namely at the end of the different vecs), and we want to add truncation on the left (see #779 also). ",
        "Yes, it's just for consistency. The user can manually merge multiple `Encoding`s, and this allows to manually set the sequence id for each of them."
      ],
      "tokenizers-prefer-explicit-api-design": [
        "As discussed, we'll remove the output with the sequence id, and will just keep the classic one. The user can call `token_to_sequence` when needed to determine it.",
        "I'm wondering if it makes sense to expose `vocab`, `vocab_r`, and `merges` separately. I think we should always provide either vocab/merges or nothing. What do you think?",
        "(With the `vocab_r` being built from `vocab` anyway)",
        ":+1: "
      ],
      "tokenizers-thread-safe-resource-sharing": [
        "Sure, we need them because anything in an `Arc` is immutable. These `RwLock` provides us with a way to actually mutate the `Model` here.",
        "The `unwrap` here is for the `std::sync::LockResult` that is returned by the `RwLock` when you try to access its content. The `Err` case happens when a thread that was holding the lock panicked. So since this shouldn't happen, and we don't want to recover from it, I think the `unwrap` should be ok."
      ],
      "tokenizers-purpose-indicating-descriptive-names": [
        "For consistency with current `implementations`, the constructor should probably expect file by default, and provide another way to build with in-memory data.",
        "What about `albert_tokenizer` instead of `precompiled_files` which seems off? Also, we can probably return `albert_base` directly since its a single file in this case.",
        "Yes I agree that we'll probably add more, but it might be better to keep each set of files separated though. When you require `albert-base` you don't necessarily want to download all the serialized files, and having logically separated fixtures let us do that.\r\nI don't mind keeping a dict, but I felt that with serialized tokenizers it would just add repetition (as opposed to the vocab/merges used for the others)."
      ],
      "tokenizers-flexible-tokenizer-implementation": [
        "This won't work with a lot of tokenizers that do not use `[UNK]` as their unknown token. Unfortunately there is no easy way at the moment to get the unk token directly from the tokenizer, but maybe we can use something a bit more large. Maybe something like the regex `/^(.{1}\\b)?unk(\\b.{1})?$/i` would work for now (https://regex101.com/r/zh0He9/1/)"
      ],
      "tokenizers-minimize-memory-allocations": [
        "You can probably get `&PyBytes` and then a slice, to avoid allocating a `Vec` here."
      ]
    }
  },
  "Patrick-Erichsen": {
    "repos": [
      "continuedev/continue"
    ],
    "entries": [
      {
        "slug": "continue-choose-clear-semantic-names",
        "title": "Choose clear semantic names"
      },
      {
        "slug": "continue-clean-code-formatting-rules",
        "title": "Clean code formatting rules"
      },
      {
        "slug": "continue-document-configurations-completely",
        "title": "Document configurations completely"
      },
      {
        "slug": "continue-extract-into-helper-functions",
        "title": "Extract into helper functions"
      },
      {
        "slug": "continue-include-concrete-examples",
        "title": "Include concrete examples"
      },
      {
        "slug": "continue-logging-levels-hierarchy",
        "title": "Logging levels hierarchy"
      },
      {
        "slug": "continue-prevent-async-deadlocks",
        "title": "Prevent async deadlocks"
      },
      {
        "slug": "continue-safe-property-access",
        "title": "Safe property access"
      },
      {
        "slug": "continue-semantically-consistent-naming",
        "title": "Semantically consistent naming"
      },
      {
        "slug": "continue-standardize-llm-configurations",
        "title": "Standardize LLM configurations"
      },
      {
        "slug": "continue-use-asyncawait-pattern",
        "title": "Use async/await pattern"
      },
      {
        "slug": "continue-use-established-configuration-patterns",
        "title": "Use established configuration patterns"
      },
      {
        "slug": "continue-working-configuration-examples",
        "title": "Working configuration examples"
      }
    ],
    "comments": {
      "continue-working-configuration-examples": [
        "```suggestion\r\n  \"description\": \"Generate a commit message for staged changes\",\r\n  \"params\": { \"includeUnstaged\": true }\r\n```\r\n\r\nAlso, I think making a note in the text above this JSON block that the default behavior is to ignore unstaged changes would be helpful.\r\n\r\nShows the LLM a `git diff` of your current staged changes and asks it to generate a commit message. \r\n\r\n> If `includeUnstaged` is set to true, then unstaged changes are also included in the prompt, otherwise only staged changes are included.\"",
        "```suggestion\r\nTo open `config.json`, you can click the \"gear\" icon in the bottom right corner of the Continue Chat sidebar. When editing this file, you can see the available options through autocomplete hints as you type, or check the reference below.\r\n```\r\n\r\nThe wording of this sounds good but `IntelliSense` is a VS Code specific term. "
      ],
      "continue-prevent-async-deadlocks": [
        "The core fix was removing this clause. My hypothesis is that we had a race condition where the `toolCallStates` was getting cleared before we reached this point, which should indicate that the tool call has completed and we should continue streaming, but we were instead returning false.\r\n\r\nThe only reason this `shouldContinueStreaming` is because with the new parallel tool calling support, we want to pause streaming until _all_ tools are complete. Previously this check didn't exist at all, we just always resumed streaming at the end of the thunk."
      ],
      "continue-safe-property-access": [
        "`no-negated-condition`\r\n\r\n```suggestion\r\n        onTitleClick={item.content ? handleTitleClick : undefined}\r\n```"
      ],
      "continue-clean-code-formatting-rules": [
        "Would probably +1 removing the comment, hard to read"
      ],
      "continue-document-configurations-completely": [
        "```suggestion\r\nContinue stores it's data in the `~/.continue` directory (%USERPROFILE%\\.continue` on Windows).\r\n\r\nIf you'd like to perform a clean reset of the extension, including removing all configuration files, indices, etc, you can remove this directory, uninstall, and then reinstall. \r\n```"
      ],
      "continue-standardize-llm-configurations": [
        "```suggestion\r\n- This depends on the model provider written inside the `config.yaml`. If the model provider is `mock`, it will use `MockLLM`, and if it uses `test`, it will use `TestLLM`.\r\n```",
        "```suggestion\r\n        \"uploadImage\": true\r\n```"
      ],
      "continue-use-established-configuration-patterns": [
        "This solves a bug where the reasoning toggle wasn't rendered when using a hub assistant because the provider name is always `continue-proxy` for all models when using the hub.",
        "We have a file with typings for all of our localStorage accessors: https://github.com/continuedev/continue/blob/main/gui/src/util/localStorage.ts\r\n\r\nSo I think we should add this there and update to use the `getLocalStorage` fn."
      ],
      "continue-extract-into-helper-functions": [
        "another nit but since the `if` clause has a return we could remove the `else` clause here and avoid the unnecessary nesting",
        "Could we pull this into a `CompactionService` or something to get it out of `core/core.ts`? Could also write tests more easily for it.",
        "This function is quite large - could we pull it out into a util and break it down to make it easier to read/test? Again, another scenario where the surrounding code isn't particularly clean, but trying to keep things more maintainable going forwards",
        "We don't have linting for this but it's a bit easier to read the file if we keep all these const declarations towards the top of the function before the hooks, eg `useWebviewListener`s",
        "This all looks solid but there is just so much logic here that I think we really need to break this out into some utils that are smaller/easier to read/more testable. Again, the code around isn't the cleanest, but it would be really helpful for maintenance/debugging."
      ],
      "continue-semantically-consistent-naming": [
        "I would lean towards a more generic `warningItem` prop here to enable more flexibility for the consuming component to use whatever it wants. \r\n\r\nAlthough you could make the argument that we actually want to limit variability here and limit it to just the exclamation triangle, but in that case `onWarningText` isn't semantically clear to me since the \"on\" prefix is typically for event handlers, so I'd call it just `warningText`"
      ],
      "continue-use-asyncawait-pattern": [
        "Could we use `async/await` here for readability/consistency with the rest of the codebase?",
        "We should try to use `async`/`await` instead of callback chaining. Assuming this was AI generated code, could be a good rule ðŸ‘€ "
      ],
      "continue-logging-levels-hierarchy": [
        "```suggestion\r\n    console.debug(\"Required fields values:\", required);\r\n```\r\n\r\nWe should document this somewhere but using `console.debug` logs to reduce noise"
      ],
      "continue-choose-clear-semantic-names": [
        "Is this supposed to be the budget for the number of tokens to allocate to thinking? If so I think a more declarative `tokenBudget` would be clearer, took me a second to understand."
      ],
      "continue-include-concrete-examples": [
        "These docs should probably live in `docs/docs/chat/how-to-customize` since Apply is moreso part of Chat rather than Edit. \r\n\r\nAlso, I went ahead and created a `rules` block on the Hub so that users can easily opt-in to this behavior by installing the block: https://hub.continue.dev/continuedev/unified-diff-apply\r\n\r\nMind updating the docs here to reflect that? ",
        "This is missing an actual example of the prompt templating"
      ]
    }
  },
  "ste93cry": {
    "repos": [
      "getsentry/sentry-php"
    ],
    "entries": [
      {
        "slug": "sentry-php-balance-ci-test-coverage",
        "title": "Balance CI test coverage"
      },
      {
        "slug": "sentry-php-descriptive-identifier-naming",
        "title": "Descriptive identifier naming"
      },
      {
        "slug": "sentry-php-document-api-changes",
        "title": "Document API changes"
      },
      {
        "slug": "sentry-php-document-configuration-comprehensively",
        "title": "Document configuration comprehensively"
      },
      {
        "slug": "sentry-php-ensure-test-isolation",
        "title": "Ensure test isolation"
      },
      {
        "slug": "sentry-php-evolve-api-safely",
        "title": "Evolve API safely"
      },
      {
        "slug": "sentry-php-explicit-null-handling",
        "title": "Explicit null handling"
      },
      {
        "slug": "sentry-php-flexible-configuration-formats",
        "title": "Flexible configuration formats"
      },
      {
        "slug": "sentry-php-include-practical-examples",
        "title": "Include practical examples"
      },
      {
        "slug": "sentry-php-optimize-regex-patterns",
        "title": "Optimize regex patterns"
      },
      {
        "slug": "sentry-php-precise-dependency-versioning",
        "title": "Precise dependency versioning"
      },
      {
        "slug": "sentry-php-propagate-errors-with-context",
        "title": "Propagate errors with context"
      },
      {
        "slug": "sentry-php-purposeful-documentation-standards",
        "title": "Purposeful documentation standards"
      },
      {
        "slug": "sentry-php-secure-dependency-constraints",
        "title": "Secure dependency constraints"
      },
      {
        "slug": "sentry-php-split-for-better-readability",
        "title": "Split for better readability"
      },
      {
        "slug": "sentry-php-use-data-providers-effectively",
        "title": "Use data providers effectively"
      }
    ],
    "comments": {
      "sentry-php-document-api-changes": [
        "```suggestion\r\n- Make the `StacktraceBuilder` class part of the public API and add the `Client::getStacktraceBuilder()` method to build custom stacktraces (#1124)\r\n```",
        "I don't think we should as the intention of this PR is to move away from HTTPlug as much as possible, so suggesting to still use it is a countersense, isn't it?"
      ],
      "sentry-php-split-for-better-readability": [
        "Usually inline comments like this one are formatted on a single line, e.g. `/** @see ... */`",
        ":pray: format the DocBlock so that it's multiline to keep consistency with the rest of the codebase",
        "Nice to know, would you be so kind to add it to the PHPCS config to avoid further issues in the future?",
        "Please change this description to something like `Helper method to create an instance of this class from an array of data`. Also, as a convention we try to not write comments longer than 80 characters (it's a soft limit anyway, nothing enforced strictly) but if this is the case please consider splitting it into multiple lines to improve readability",
        "Please split this array into multiple lines (have a look at how other data providers are formatted)",
        "I meant the data provider of this file. You should format the arrays one item per line if possible, e.g.\r\n\r\n```php\r\n[\r\n    [\r\n        'foo @bar',\r\n        [\r\n            '@bar' => 'bar',\r\n        ],\r\n        'foo bar',\r\n    ],\r\n    [\r\n        'message' => 'foo @bar',\r\n        'params' => [\r\n            '@bar' => 'bar',\r\n        ],\r\n        'formatted' => 'foo bar',\r\n    ],\r\n]\r\n```",
        "Thank you for reporting this, some inconsistencies are indeed possible since style is not forced by PHPCS-Fixer. However I would prefer usage of the \"expanded\" version",
        "Please keep all `private` functions after `public` ones",
        "Please split this docblock into multiple lines",
        "Please split this docblock into multiple lines",
        "Please split this docblock into multiple lines",
        "Please try to split the descriptions comments so that each line length is around 80 characters at most",
        "As a \"soft\" limit usually the descriptions of the methods goes to a new line after ~ 80 chars"
      ],
      "sentry-php-precise-dependency-versioning": [
        "What's the reason for requiring the version `1.6`? If there is no reason besides mentioning the most recent version, I would leave the code as before",
        "Ok, definitely makes sense!",
        "Right, I didn't think about this. I relaxed the constraint to allow both version `2.x` and `3.x` to be installed",
        "Yes I know, but `3.x` does not support Symfony `3.4` anymore and it didn't make sense to do more work to have two distinct config files as the older version is unmaintained and we always run the tool with the newest dependencies"
      ],
      "sentry-php-ensure-test-isolation": [
        "This test description is wrong and doesn't respect what the code does. Also to just check that the handler isn't instantiated if the parameter is wrong isn't a normal PHPUnit test with `expectException` enough?",
        "Then I believe that every single test of the error handler that we have in PHPUnit should be moved to PHPT or we should somehow (e.g. with reflection) unset the singleton variable or we will leak state between multiple tests and this is not good. Since we made the handler singleton here it's the correct PR to do it, regardless of the solution we choose"
      ],
      "sentry-php-optimize-regex-patterns": [
        "We can use a non-capturing group here so that we avoid some allocations. Also, the regex should take into account the `:[number]` part before the `$`, so it should be changed to something along the lines of (untested): `(?::\\d+\\$|0x)[a-fA-F0-9]+$`",
        "The memory address is in hex format, so a stricter regex is `/0x[a-fA-F0-9]+$/`",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole",
        "As this is a regex now, you should escape all special characters like the dot. Also, please use `^` and `$` to make sure that the matching is done against the string as a whole"
      ],
      "sentry-php-explicit-null-handling": [
        "I would find more appropriate to check that the client is not `null` because the the real meaning of this line is to ensure that that instance is not `null` and not that it implements the interface. It is a small change indeed, but imo it conveyes to the reader the expectation in a clearer way.",
        "Can you please avoid assigning the variable in the `if` statement and check for `null` explicitly?",
        "Please typehint the return tpe as `?callable`",
        "Always be explicit about the check, so `null !== $client`",
        "Always be explicit about the check, so `null !== $client`",
        "Why having this argument to be `null` by default instead of an empty `array`? I think that if we want to leave it as-is, then we should make the argument `nullable` for consistency: it doesn't make sense that an argument cannot accept `null`, but its default value is `null`"
      ],
      "sentry-php-balance-ci-test-coverage": [
        "Why are you running PHPT tests without code coverage? As far as I remember, it worked fine and in fact Codecov now complains that the `ErrorHandler` is untested",
        "If we don't want to run the bench except for ensuring that it still works (not sure why it should not though, even in the future), wouldn't make more sense to have a separate workflow that can run every now and then?",
        "My suggestion was not related to the time spent running the step, but rather to the fact that if this step is here just to ensure that it runs, then it doesn't make sense to have it as part of the CI workflow",
        "With \"CI workflow\" I meant the workflow that runs for every push or for every PR. Do you think that running it once per week (just as an example) would still not be enough?",
        "Ok then, let's keep it as-is",
        "Instead of installing a specific version of PHP we should use the `matrix` feature of AppVeyor (similar to the one of Travis CI) to test multiple versions and fail as early as possible using `fast_finish`",
        ">That would be really slow\r\n\r\nSince Travis CI would run in parallel with AppVeyor it would not be much slower imho. The point in testing all versions is that you may have a bug in a specific version of PHP for Windows which may not exists in Linux"
      ],
      "sentry-php-use-data-providers-effectively": [
        "You should split the test into two, because only one exception can be thrown at time. Furthermore, `expectException()` is in no case cumulative. I suggest to rename this test method to `testHubAdapterThrowsExceptionOnSerialization`, and add `testHubAdapterThrowsExceptionOnUnserialization`",
        "We could hardcode the argument passed to `unserialize()` to be `O:23:\"Sentry\\State\\HubAdapter\":0:{}`",
        "Instead of having two test methods for the same thing, what about changing the `testConstructor()` method to use a data provider that returns first `null` and then a `float` and then compare those values with the result of `getTimestamp()`? You can use the `Symfony\\Bridge\\PhpUnit\\ClockMock` class to mock the current time to a specific value in the test cases that needs it. There should already be a test that does something similar somewhere in the codebase ðŸ˜ƒ ",
        "What about merging this test and the one below using a data provider?",
        "Instad of splitting the test into two different methods, simply use the one above with a data provider that provides the input `$data` parameter to pass to the `fromArray` method and an `$expectedResult` that can be compared to what the `toArray` method returns",
        "Can we refactor the `testToArrayWithMessage` method to use a data provider and unify that test case with this one?",
        "Instead of having multiple test conditions into a single test case please use a data provider",
        "Assuming that this test case covers the case in which a `message_formatted` param is specified in the payload, please use the `sprintf` notation for the value of the `message` key and a totally different message for the value of the `formatted` key so that we can catch if the override works"
      ],
      "sentry-php-descriptive-identifier-naming": [
        "I'm not a fan of abbreviations and acronyms when writing code. What about renaming this variable to `samplingContext`?",
        "Nitpick: I think that renaming this variable to `$samplingContext` would be clearer for the reader in terms of cognitive load",
        "As previously we were serializing any `iterable` types I think that it would be more approprite to change the typehint accordingly. Also :pray: change `$array` to `$input` to keep consistency with the rest of the test methods of this class",
        "Please rename the `$key` variable to `$className`",
        "Please rename the parameter to `$data`",
        "Please rename this variable to something that better reflect what it stores (e.g. `$setMessageMethodArguments`)",
        "Please rename this variable to something that better reflect what it stores (e.g. `$setMessageMethodArguments`)",
        "Maybe the name `$handlerInstance` is clearer?"
      ],
      "sentry-php-evolve-api-safely": [
        "We cannot mark an already existing method as `@internal` or we may be tempted in the future to make changes without worrying about BC when we must instead since it was part of the public API in the past. Just leave the note instead with a link to the issue and remove the comment from the parameter instead",
        "This is a huge breaking change, which is not acceptable without releasing a new major version. Rather than adding this argument for real, you can comment it and handle it with `func_get_arg` in the implementation class to avoid the break: we already did it in the past and it worked even though it was a bit messy",
        "Although having the `clear` word in the method's name is fine for me, Unified API specs talks about a `removeUser` method so we should stick with it. But if @HazAT is fine with changing such specs (I don't know if any of the other SDKs implement this method) we may leave it as is or rename it to `clearUserContext` (but this would be inconsistent with the rest of the methods, which I would have named with the `Context` suffix by the way)",
        "I had a look at the other SDKs and I read again the documentation and I think I misunderstood what was written there. Basically, the documentation suggests as a way to remove data from a context the `remove*` method. However this method should not clear the whole context, but just remove a given item from it, so can you please update the code to selectively unset a certain key of the context?",
        "I think that you are right in saying that generally speaking the specs should be more clear on how these methods should act, because in certain parts they are and in others they are not. I suggest to go with the `removeUser` method that unsets a given key and not the whole context, as it may turn useful for example if something out of the control of the user sets some information on a parent scope and without such method you would have no way to remove it because inner scopes just inherit all data. We can add a `clearUser` context if we see that people need it",
        "I'm sorry to change one more time my mind, but since the discussion in the referenced issue may last for a long time, let's drop the `removeUser` method so that we can merge this PR as-is and if something will change in the future regarding this topic we will update the SDK accordingly",
        "You are still able to set both the SDK identifier and version using the appropriate methods `setSdkIdentifier` and `setSdkVersion`. The only thing this method did was to automate retrieving the version of a package using the `PrettyVersions` class, which can be done from the user so that we can simplify a bit our code"
      ],
      "sentry-php-propagate-errors-with-context": [
        "Instead of catching the entire code of the method we should just wrap the `wait` function call. The same should be done in the `cleanupPendingRequests` method. The strange thing is that when `false` is passed to the `wait` method no exceptions should be thrown but just returned (effectively doing the same as a `try`/`catch` block), so there is a bug somewhere in the vendor or the exception is not raised there (the other point where it could be raised is the `sendAsyncRequest` method, but since we saw that the request is not sent until `wait` is called I don't think it's the case)",
        "It probably makes more sense to catch `\\Throwable` instead of `\\Exception`",
        "This `try`/`catch` block is misleading, since it's catching any exception to rethrow with a generic message that says that the cURL client must be installed while the problem may be something else entirely. What about removing it and just let a possible error to be thrown?"
      ],
      "sentry-php-flexible-configuration-formats": [
        "While we're at it, can you please extract this variable into a `private` class constant named `HEADERS_TO_SANITIZE`?",
        "I don't have a strong contrary opinion about having a configurable set of options, although I would prefer to implement them as options of the integration itself rather than of the SDK, but since I also believe that Relay is the right tool for this job and knowing the we said these kind of things should not be in the SDKs, I will let you decide how to proceed @HazAT",
        "So, I've talked on Discord with @HazAT and we agreed to expose this option as option of the integration itself. You can take a look at the `IgnoreErrorsIntegration` class to see how we implemented something similar ðŸ˜ƒ I will review again the code once you submit all the new changes",
        "What's the reason this option can be all those things? Wouldn't a simple `callable` be enough? If someone wants to use a class he can either code it to make it self-invokable or he can wrap the call to the class into the `callable` itself",
        "Please typehint the parameter as `?callable`",
        "Missing the validation of the option, `string|callable|object` should work"
      ],
      "sentry-php-document-configuration-comprehensively": [
        "Maybe it would be better to squash together these two lines? Otherwise, the user will first read that the default value was changed and then that the option has been deprecated and he may think that the two things are unrelated or he may be confused because it seems that they are in conflict",
        "Even better, I suggest ``Add `in_app_include` option to whitelist paths that should be marked as part of the app (#909)``"
      ],
      "sentry-php-include-practical-examples": [
        "What do you think about changing this to ``Trim the file path of the frames of the stacktrace that refer to an anonymous class according to the `prefixes` option``?",
        "I'm open to suggestions because the current entry doesn't looks clear enough to explain the change",
        "The reason I'm asking this is that it doesn't really makes sense that a file path gets trimmed from a class. What about ``Trim the file path from the anonymous class name in the stacktrace according to the `prefixes` option`` instead?",
        "There is a space at the start of the line that should not exists. Also I would change the sentence to `The suggested way to create your own instance of the client is to use the provided builder that will take care of instantiating a few dependencies like the PSR-7 factories and the HTTP client`"
      ],
      "sentry-php-secure-dependency-constraints": [
        "Please require version `^1.8.4|^2.1.1` as before: it was that way because previous versions had security issues, and in this way you are letting users install them which is bad",
        ">fair point, we had this discussion internally how we should go about external dependencies and if we should dictate which version users should have. We decided to make it on to the user to pump deps accordingly.\r\n\r\nI understand, but you should definitely dictate which version users should have in case the dependency is used directly in your code and isn't just transient. In this case, the code in this SDK actually uses a package affected by [`CVE-2022-24775`](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2022-24775), and in the end it is your package that is vulnerable, not the project of the user."
      ],
      "sentry-php-purposeful-documentation-standards": [
        "Can you please add a little description of what this function does, e.g. `Sets additional optional context data.`?",
        "Can you please add a little description of what this function does, e.g. `Gets additional optional context data.`?",
        "The description is not accurate, the fact that this object may contain the original exception is just a detail, we should describe what the `$hint` is and not what it contains",
        "There is no much value in having this annotation without a description of the parameter and of the function. Please either add them or drop the DocBlock altogether",
        "Please mark this class as `final` and add a small DocBlock to explain what it does",
        "Please add the docblock with the description of what this class does, e.g. `Default implementation of the {@HttpClientFactoryInterface} interface that uses Httplug to autodiscover the HTTP client if none is passed by the user.`",
        "Please change the description to: `Gets the callbacks used to customize how objects are serialized in the payload of the event.`",
        "Add a description to this interface: `This interface can be used to customize how an object is serialized in the payload of an event.`",
        "This sentence should describe what the option does rather than just say what it returns. I would change it to something like `Gets whether the silenced errors should be captured or not`",
        "This sentence should describe what the option does rather than just say that it sets the option named X. I would change it to something like `Sets whether the silenced errors should be captured or not`"
      ]
    }
  },
  "praveen-influx": {
    "repos": [
      "influxdata/influxdb"
    ],
    "entries": [
      {
        "slug": "influxdb-avoid-flaky-test-patterns",
        "title": "Avoid flaky test patterns"
      },
      {
        "slug": "influxdb-centralize-workspace-configurations",
        "title": "Centralize workspace configurations"
      },
      {
        "slug": "influxdb-choose-appropriate-lock-primitives",
        "title": "Choose appropriate lock primitives"
      },
      {
        "slug": "influxdb-clear-configuration-parameters",
        "title": "Clear configuration parameters"
      },
      {
        "slug": "influxdb-descriptive-semantic-naming",
        "title": "Descriptive semantic naming"
      },
      {
        "slug": "influxdb-document-complete-data-flows",
        "title": "Document complete data flows"
      },
      {
        "slug": "influxdb-document-versioning-strategies",
        "title": "Document versioning strategies"
      },
      {
        "slug": "influxdb-follow-api-conventions",
        "title": "Follow API conventions"
      },
      {
        "slug": "influxdb-handle-errors-by-criticality",
        "title": "Handle errors by criticality"
      },
      {
        "slug": "influxdb-minimize-critical-path-allocations",
        "title": "Minimize critical path allocations"
      },
      {
        "slug": "influxdb-performance-conscious-metrics-implementation",
        "title": "Performance-conscious metrics implementation"
      },
      {
        "slug": "influxdb-promote-code-clarity",
        "title": "Promote code clarity"
      },
      {
        "slug": "influxdb-secure-token-lifecycle",
        "title": "Secure token lifecycle"
      }
    ],
    "comments": {
      "influxdb-secure-token-lifecycle": [
        "Yes sure - I'll setup an API test for that.",
        "Yes - but not exactly for the reason you mentioned there. Currently we keep admin token creation endpoint open but there's no way to create more than one admin token which is basically the guard at the moment. So, in this test because we've deleted the previous one, new one can be added in it's place.\r\n\r\nThis behavior would likely have to change to support updates to tokens,  you still won't be able to create more than one token with `_admin` name but allow multiple _named_ admin tokens. "
      ],
      "influxdb-promote-code-clarity": [
        "Yes - good point, I'll follow that up. I was incrementally moving code around without breaking the tests, I can take it further and make it a clearer boundary between flushing WAL buffer and running a snapshot.",
        "Addressed it in [6619e9e](https://github.com/influxdata/influxdb/pull/25727/commits/6619e9eccea9bd7f794864a2a4284a4705b2ef25)",
        "I like we have a custom display to plug into `explain` output. Maybe we could swap column name for id here? \r\n\r\n```\r\npredicates=[[0 IN (us-east)], [1 IN (a,b)]]\r\n```\r\nprobably reads better (imo) when its\r\n\r\n```\r\npredicates=[[region IN (us-east)], [host IN (a,b)]]\r\n```\r\n\r\nJust an UX thing, when there's many columns involved this might be a little difficult to understand. If `col_id` is useful in `explain`, then we could _add_ that info here like `region$0` or some variation of it?"
      ],
      "influxdb-minimize-critical-path-allocations": [
        "This `snapshot_chunks_iter` produces `SnapshotChunk` lazily, uses the chunk to create `PersistJob` and then moves it to `TableBuffer`'s `snapshotting_chunks`. Because there's a write lock on this buffer above, it is ok to remove the key and then add it back. Previously the `snapshotting_chunks` was cloned and this avoids the cloning. ",
        "> are you just referring to it being added to the snapshotting chunks?\r\n\r\nYes - I can see how I've confused you, I should rephrase that. It is added (as opposed to added back) to the `snapshotting_chunks`. I was trying to imply it's added _back_ to `TableBuffer`s `snapshotting_chunks`. \r\n\r\nThe previous operation was \r\n- to remove those keys from the map\r\n- convert them to record batches\r\n- hold it in `snapshotting_chunks`, \r\n- clone it and return the copy back to create persist jobs out of it.\r\n\r\nNow, iterator holds all the keys to remove\r\n- each call to `next()` converts it to record batch and yields it\r\n- this loop creates persist job\r\n- then adds it to `snapshotting_chunks` at the end\r\n\r\nThe main thing is there's a write lock for this operation and hence both these operations should leave `TableBuffer` in same state.",
        "Yes - it has been, if I'm making further changes I'd probably need this to be perf tested again.",
        "It is a good point, previously we were replacing the `snapshotting_chunks` each time - it was creating a vec each time to replace original one, so I changed it to reclaim the memory. But now that I'm looping through and adding them, I can reuse it. I didn't revisit this code after I made the change to use the iterator. I'll change it and run through the profiler.  "
      ],
      "influxdb-clear-configuration-parameters": [
        "Just a minor thing, setting 432 here directly as a default here could've worked?"
      ],
      "influxdb-centralize-workspace-configurations": [
        "We've been using root `Cargo.toml` to pin the version of a library. This could possibly be moved there too?"
      ],
      "influxdb-follow-api-conventions": [
        "I could be misinterpreting the [RFC](https://datatracker.ietf.org/doc/html/rfc7617), I thought `:` is not allowed in username? \r\n\r\n> For the user-id, recipients MUST support all characters defined in\r\n   the \"UsernameCasePreserved\" profile defined in Section 3.3 of\r\n   [RFC7613], with the exception of the colon (\":\") character.\r\n   \r\nI'll still change this implementation but I interpreted that RFC section as clients won't be able to set \":\" in username part of the `user-pass` pair.",
        "~I cannot request review @jdstrand~ , I've reverted back and made sure if there's more than one `:` it returns a `MalformedRequest` error."
      ],
      "influxdb-performance-conscious-metrics-implementation": [
        "It runs into panic, \r\n```\r\n2025-06-02T14:49:39.205230Z ERROR panic_logging: Thread panic panic_type=\"unknown\" panic_message=\"More than one execution pool created: previously existing instrument\" panic_file=\"/home/praveen/.cargo/git/checkouts/influxdb3_core-2ede6fca005e1dcf/fd0e474/iox_query/src/exec.rs\" panic_line=281 panic_column=9\r\n\r\nthread 'main' panicked at /home/praveen/.cargo/git/checkouts/influxdb3_core-2ede6fca005e1dcf/fd0e474/iox_query/src/exec.rs:281:9:\r\nMore than one execution pool created: previously existing instrument\r\nstack backtrace:\r\n   0:     0x629f44b5e172 - <std::sys::backtrace::BacktraceLock::print::DisplayBacktrace as core::fmt::Display>::fmt::hc04c8f544ab24d66\r\n   1:     0x629f44b8eb63 - core::fmt::write::hfe57b7174b7d8eab\r\n   2:     0x629f44b595a3 - std::io::Write::write_fmt::h154385efa8565236\r\n   3:     0x629f44b5dfc2 - std::sys::backtrace::BacktraceLock::print::h0c8f24e22f5873a8\r\n   4:     0x629f44b5f24c - std::panicking::default_hook::{{closure}}::hd07d57e6a602c8e4\r\n   5:     0x629f44b5f04f - std::panicking::default_hook::h63d12f7d95bd91ed\r\n   6:     0x629f3fd807db - panic_logging::SendPanicsToTracing::new_inner::{{closure}}::h4f1478e3035af477\r\n   7:     0x629f44b5fd43 - std::panicking::rust_panic_with_hook::h33b18b24045abff4\r\n   8:     0x629f44b5f9c6 - std::panicking::begin_panic_handler::{{closure}}::hf8313cc2fd0126bc\r\n   9:     0x629f44b5e679 - std::sys::backtrace::__rust_end_short_backtrace::h57fe07c8aea5c98a\r\n  10:     0x629f44b5f68d - __rustc[95feac21a9532783]::rust_begin_unwind\r\n  11:     0x629f44b8bac0 - core::panicking::panic_fmt::hd54fb667be51beea\r\n  12:     0x629f414b6cb7 - iox_query::exec::Executor::new_with_config_and_executor::h3ef1059edcb25ade\r\n  13:     0x629f3f99fd9c - influxdb3::commands::serve::command::{{closure}}::h2cdf5ca9df83df25\r\n  14:     0x629f3f9b6fcb - influxdb3::main::{{closure}}::hc953cfc298ca6770\r\n  15:     0x629f3f987b39 - tokio::runtime::park::CachedParkThread::block_on::h51b18ac33f8a0e4d\r\n  16:     0x629f3fb2a7bf - tokio::runtime::runtime::Runtime::block_on::h9eb33b87acb6fa53\r\n  17:     0x629f3fc20d55 - influxdb3::main::h75a268e75e689bc6\r\n  18:     0x629f3fcbb256 - std::sys::backtrace::__rust_begin_short_backtrace::h5b4e77177edb3cca\r\n  19:     0x629f3fab4321 - std::rt::lang_start::{{closure}}::hc69eb1d94c6de306\r\n  20:     0x629f44b4e080 - std::rt::lang_start_internal::h418648f91f5be3a1\r\n  21:     0x629f3fc3b19d - main\r\n  22:     0x7ed71c33d488 - <unknown>\r\n  23:     0x7ed71c33d54c - __libc_start_main\r\n  24:     0x629f3f95b325 - _start\r\n  25:                0x0 - <unknown>\r\n2025-06-02T14:49:39.295724Z  WARN executor: DedicatedExecutor dropped without calling shutdown()\r\n2025-06-02T14:49:39.296308Z  WARN executor: DedicatedExecutor dropped without calling shutdown()\r\n\r\n```\r\n\r\nI can look into the panic and see if I can address that in a different way if this is going to mess downstream consumers.",
        "I looked into the code that runs into panic \r\n\r\nhttps://github.com/influxdata/influxdb3_core/blob/fd0e474a6c0af5ba867399d753f5df18f59907cb/iox_query/src/exec.rs#L268-L284\r\n\r\nIt looks like there is an assumption that you violate single memory pool -> executor relationship if the \"datafusion_pool\" is already registered. Even though we don't break that relationship, i.e in this branch there are two executors and each has it's own memory pool so the relationship is still correct but because registry is shared it runs into this error. \r\n\r\nI need to spend a bit more time to see if I can create the executor outside without hooking it up to metrics to start with (or use a different name for instrument \"datafusion_write_pool\") and then experiment with how it's reporting.",
        "Yea - I've hard coded it to 10k for now",
        "@mgattozzi - thanks, I was hoping the metrics gathering will get out of the critical path for any writes/reads when following the message passing model. If you think that having lock on the store directly will be quick enough anyway, happy to change the implementation to expose the `TelemetryStore` directly instead of going through the `TelemetryHandle`."
      ],
      "influxdb-document-complete-data-flows": [
        "Good point - I'll try to link the steps to the diagram and add the incoming writes as well."
      ],
      "influxdb-handle-errors-by-criticality": [
        "True - it'll stay as debug :smile: "
      ],
      "influxdb-document-versioning-strategies": [
        "Really useful, thanks - I was going to stick this somewhere in the docs too to help people understand the necessity of tagging this way.\r\n\r\n```\r\nv3.M.m-B[.Q.q.b]  # eg, v3.0.0-0.beta.1.0 or v3.0.0-1\r\n | | | |  | | |\r\n | | | |  | |  ----------> package build number for 'quality' (no influxdb3 code changes; start at 0)\r\n | | | |  |  ------------> 'quality' release number (has influxdb3 code changes; start at 1)\r\n | | | |   --------------> release quality (optional, if specified, one of alpha, beta, rc)\r\n | | |  -----------------> package build number (no influxdb3 code changes; 0 if specifying 'quality', otherwise start at 1)\r\n | |  -------------------> influxdb3 micro version (has non-breaking influxdb3 code changes)\r\n |  ---------------------> influxdb3 minor version (has breaking influxdb3 code changes)\r\n  -----------------------> major version (has hugely breaking code changes)\r\n```"
      ],
      "influxdb-descriptive-semantic-naming": [
        "Thanks for embedding the unit `ns`, really helps when reading the code without full context. "
      ],
      "influxdb-avoid-flaky-test-patterns": [
        "Is it `failure()` because process starts and hangs around till 500 millis and then shuts down? Just wondered if it could've used the `TestServer` to spin up the server and got hold of the stdout for your assertions. If you'd already looked into it and it didn't work for you that's fine, but it has the mechanism built in to kill the process after the test without `timeout`.",
        "I did think the test happens after killing the process as well with the timeout - just wasn't sure if there was easy wins in extending `TestServer`.  ",
        "I think using `predicates` is fine but I've been following the existing pattern of using `assert_contains!` and `assert_not_contains!` macros from `test_helpers` just to keep it consistent. If those macros are lacking in functionality for your use case, definitely keep `predicates` in.",
        "Yea - it'd have had to wait till server started fully. I think we already loop through in `TestServer` till it hits the last line to capture the address. So, you could just extend `TestServer` to hold all the lines it's seen during the startup and access it in the tests. When `TestServer::spawn().await` completes, that means server is fully up. Then you can just loop through and check the line you wanted to assert is there. \r\n\r\nSee if that works, if not go ahead and merge this PR - the point about `TestServer` is not really critical, I just wanted to check if you'd seen it and still thought it's not fit for your test. ",
        "Not really, it's the timeouts that stood out. The initial thought was `TestServer` already abstracts the server starting part already and it also hooks into server logs and then the assertions stood out as well. But as I said, both aren't critical they're part of test infra, we can ship it.",
        "It's just for the assertion https://github.com/influxdata/influxdb/blob/dc1537edc5151ce8c74cafcf08b8913cfedd5129/influxdb3_telemetry/src/store.rs#L325. \r\n\r\nThe timer is started when a new instance is created and if I immediately assert on duration I get 0, I added that sleep to make it actually assert on a non zero value. There is nothing running in the background."
      ],
      "influxdb-choose-appropriate-lock-primitives": [
        "I wonder using something like [SkipMap](https://github.com/crossbeam-rs/crossbeam/blob/master/crossbeam-skiplist/src/lib.rs) (if order is relevant) or [DashMap](https://github.com/xacrimon/dashmap/) (if order is not relevant) sort of implementations will work better so that `Path` (key) lookup locks are distributed. Two paths that are not related might have lock contention in this implementation, which could possibly be avoided?\r\n\r\nI haven't looked at the call site, so I could be missing something but it'd be really nice if the contention for lock is scoped really to two identical paths."
      ]
    }
  },
  "jcollie": {
    "repos": [
      "ghostty-org/ghostty"
    ],
    "entries": [
      {
        "slug": "ghostty-centralize-configuration-values",
        "title": "Centralize configuration values"
      },
      {
        "slug": "ghostty-code-structure-clarity",
        "title": "Code structure clarity"
      },
      {
        "slug": "ghostty-define-explicit-error-sets",
        "title": "Define explicit error sets"
      },
      {
        "slug": "ghostty-descriptive-consistent-naming",
        "title": "Descriptive consistent naming"
      },
      {
        "slug": "ghostty-document-configs-comprehensively",
        "title": "Document configs comprehensively"
      },
      {
        "slug": "ghostty-encapsulate-implementation-details",
        "title": "Encapsulate implementation details"
      },
      {
        "slug": "ghostty-generate-dynamic-configurations",
        "title": "Generate dynamic configurations"
      },
      {
        "slug": "ghostty-in-tree-build-configurations",
        "title": "In-tree build configurations"
      },
      {
        "slug": "ghostty-optimize-comparison-patterns-efficiently",
        "title": "Optimize comparison patterns efficiently"
      },
      {
        "slug": "ghostty-pipeline-friendly-script-design",
        "title": "Pipeline-friendly script design"
      },
      {
        "slug": "ghostty-prevent-unnecessary-memory-operations",
        "title": "Prevent unnecessary memory operations"
      },
      {
        "slug": "ghostty-use-if-unwrap-with-optionals",
        "title": "Use if-unwrap with optionals"
      }
    ],
    "comments": {
      "ghostty-code-structure-clarity": [
        "Done.",
        "Done."
      ],
      "ghostty-use-if-unwrap-with-optionals": [
        "The Zig way to deal with optionals in a situation like this is to \"strip\" the optional like this:\r\n\r\n```\r\nconst current_background_image = self.current_background_image orelse return;\r\n```\r\nThen you can refer to `current_background_image` without needing the awkward `.?` syntax.",
        "I'd recommend:\r\n```\r\nif (self.current_background_image) |current_background_image| {\r\n  switch (current_background_image) {\r\n```"
      ],
      "ghostty-encapsulate-implementation-details": [
        "New code that uses the raw C GTK interfaces is a non-starter, _expecially_ outside `src/gtk`. If you're not sure how to implement the interface that Mitchell described let us know and we can help.",
        "CI never got a chance to try macOS builds because of other errors, but this is not going to compile on macOS.",
        "Fixed."
      ],
      "ghostty-pipeline-friendly-script-design": [
        "Integrating this into the build system will be easier if this just outputs to stdout.",
        "Integrating this into the build system will be easier if it reads the source from stdin."
      ],
      "ghostty-in-tree-build-configurations": [
        "Fixed."
      ],
      "ghostty-generate-dynamic-configurations": [
        "Including the actual release notes creates a catch 22 situation. Would certainly be easy to add a link to the release notes on the website, since that's easily predictable.",
        "I'd say that we might want to generate the appdata at build time. That way the release version etc can all automatically reflect the version number and build time."
      ],
      "ghostty-optimize-comparison-patterns-efficiently": [
        "My preference when dealing with C 'booleans' is to compare to `!= 0` as any non-zero value is considered 'true'.",
        "Fixed."
      ],
      "ghostty-define-explicit-error-sets": [
        "Fixed."
      ],
      "ghostty-centralize-configuration-values": [
        "Hmm, if developing Gnome/GTK were the primary object sure, but since this is about developing Ghostty we should probably stick to a released version of Gnome. 48 is very close to release so if there was like a 48-rc channel I'd be OK with that.",
        "Fixed."
      ],
      "ghostty-prevent-unnecessary-memory-operations": [
        "Allocations removed."
      ],
      "ghostty-document-configs-comprehensively": [
        "I'd bet that a lot of users will never see the logs, so I think that we need something noisier than that. I think that options that just silently do nothing will frustrate more people that don't read the release notes. I think that it's going to become increasingly more common to have people that install Ghostty from their package manager and launch it from a GUI desktop icon so they may not even know where to look for the logs.",
        "A better name would be `gtk-gdk-disable-color-mgmt`. Any GTK-specific config entries should be prefixed with `gtk`.\r\n",
        "I'd expand the comment here and explain the purpose and effect of the setting a little more clearly (including the default and under what situations you'd want to enable this). While fine for an inline code comment, this will appear in user-facing documentation.",
        "I agree in general that defaulting to \"doing the right thing\" automatically is good, wouldn't this necessitate conditioning based on both the GTK version _and_ the Plasma version? We'd need to add detection for fixed versions of GTK (easy if we know the versions) as well as KDE version (not sure how hard this would be for a GTK app).",
        "This looks good, but is there a corresponding GTK bug that we can link to?",
        "No, I think that mentioning support should be on a per-feature basis as there may be features that are not supported by all platforms.",
        "That would only work if they are generating their configs to account for differences in systems, for example if system A is Linux then their home directory is probably `/home/<username>` but on macOS it's probably `/Users/<username>`. I can agree that this logic should probably be abstracted and centralized.",
        "Hmm it's all a bit convoluted but it looks like `config-file`, `custom-shader`, and `gtk-custom-css` are always treated relative to the directory of the file that they are referenced from, or in the case of CLI args, the current working directory. Re-using that logic I think would be fine.",
        "I pushed PR #6622 to refactor RepeatablePath and add NonRepeatablePath (because I don't think it makes any sense to have multiple audio files). This way `bell-audio` has the same behavior as other configuration entries with regards to relative directories or expanding `~/`.",
        "I think that it would be better to introduce a `background-blur` boolean to enable/disable background blur and then `background-blur-radius` would then only have an effect on macOS. Or make `background-blur-radius` an optional that defaults to `null`, but I think that the explicit boolean would be clearer."
      ],
      "ghostty-descriptive-consistent-naming": [
        "As above, add `toggle-` prefix.",
        "Add prefix `toggle-`.",
        "#7087 switched to RING_BELL",
        "Same as `true` above, except call it `none`. This matches the protocol more directly.",
        "Enums values in Zig should be lower case unless there's a specific reason not to.",
        "Fixed."
      ]
    }
  },
  "cam72cam": {
    "repos": [
      "opentofu/opentofu"
    ],
    "entries": [
      {
        "slug": "opentofu-clear-concise-documentation",
        "title": "Clear concise documentation"
      },
      {
        "slug": "opentofu-craft-actionable-errors",
        "title": "Craft actionable errors"
      },
      {
        "slug": "opentofu-document-intent-and-limitations",
        "title": "Document intent and limitations"
      },
      {
        "slug": "opentofu-document-reference-standards",
        "title": "Document reference standards"
      },
      {
        "slug": "opentofu-document-with-examples",
        "title": "Document with examples"
      },
      {
        "slug": "opentofu-minimize-api-surface",
        "title": "Minimize API surface"
      },
      {
        "slug": "opentofu-optimize-cicd-workflows",
        "title": "Optimize CI/CD workflows"
      },
      {
        "slug": "opentofu-prevent-backing-array-surprises",
        "title": "Prevent backing array surprises"
      },
      {
        "slug": "opentofu-proper-span-lifecycle",
        "title": "Proper span lifecycle"
      },
      {
        "slug": "opentofu-protect-infrastructure-secrets",
        "title": "Protect infrastructure secrets"
      },
      {
        "slug": "opentofu-provider-instance-management",
        "title": "Provider instance management"
      },
      {
        "slug": "opentofu-reduce-code-nesting",
        "title": "Reduce code nesting"
      },
      {
        "slug": "opentofu-safe-lock-patterns",
        "title": "Safe lock patterns"
      },
      {
        "slug": "opentofu-separate-configuration-lifecycles",
        "title": "Separate configuration lifecycles"
      },
      {
        "slug": "opentofu-specify-configuration-behaviors",
        "title": "Specify configuration behaviors"
      }
    ],
    "comments": {
      "opentofu-provider-instance-management": [
        "Can you explain why you chose not to use setsubtract and disabled regions here?",
        "To be more explicit, the way that this is currently written (using the same input for both provider and resource's for_each) will cause warnings in OpenTofu.  Those warnings are why we need these docs in the first place.",
        "Perhaps a note that we don't support provider instances passed into child modules without selection."
      ],
      "opentofu-protect-infrastructure-secrets": [
        "It also might be worth introducing the idea that state encryption is our currently recommended solution.\r\n\r\nIMO they are two overlapping, but not exclusive concepts\r\n\r\nAdvantages of State Encryption:\r\n* All secrets are protected, regardless of provider or resource/module configuration\r\n* Don't need to chose what is protected and what is exposed\r\n* Also protects against provider injection\r\n  - https://www.plerion.com/blog/hacking-terraform-state-for-privilege-escalation\r\n  \r\nAdvantages of Ephemeral\r\n * No keys to loose / have leaked, secrets (if identified properly) are not stored in the first place."
      ],
      "opentofu-safe-lock-patterns": [
        "Good catch! fixed in fb8afbff57",
        "The reason I added the lock earlier was to help the terragrunt (and similar) scenarios.  I'm imagining a bunch of OpenTofu inits all being started from some parent tool, all pointed at the same cache directory.\r\n\r\nThe first one to acquire the lock downloads and installs the provider before linking it to the local directory and unlocking.\r\n\r\nEvery iteration after that is then able to detect and use the downloaded provider that's in the global cache.\r\n\r\nIf we move the lock further down, they would all fail the initial tryInstallPackageFromCacheDir check and be forced to download the provider and overwrite the global cache entry.\r\n\r\nRe-reading perhaps I should understand this instead as multiple smaller \"scoped\" locks where necessary?",
        "This resulted in a subsequent PR that deals with refactoring this part of the codebase a bit more in depth.  https://github.com/opentofu/opentofu/pull/2708\r\n\r\nI'm going to merge this version as is, with the idea that we will merge or close 2708 next week.",
        "That is why I added a defer() at the top of this function to catch any stray early returns.  It's inelegant and I've left some breadcrumb comments above on why it's not being refactored in this PR.\r\n\r\nYour suggested refactoring would definitely make this much easier to parse (in a subsequent PR)",
        "For now, let's keep with the unfortunate convention of context.TODO().  Earlier on in the project, we tried to refactor a lot of the contexts to be functional throughout the codebase.  It broke quite a few things in really subtle ways.\r\n\r\nI think that using a proper context here is the right thing long term, but we will need to prioritize that separately from this work."
      ],
      "opentofu-optimize-cicd-workflows": [
        "I wonder if having this fire while engineers are online makes more sense?",
        "I don't want to get pinged all Sunday if a bunch of vulns are posted to github."
      ],
      "opentofu-document-with-examples": [
        "changed alias to provider_alias"
      ],
      "opentofu-reduce-code-nesting": [
        "We use pv below and can't omit the value here.",
        "[1a5e1ec](https://github.com/opentofu/opentofu/pull/2069/commits/1a5e1ec2d8b9938424e22ce2ff24936c96a769d4)"
      ],
      "opentofu-craft-actionable-errors": [
        "This requirement only exists for provider for_each however.  There are other paths through the code (aliases in required_providers) that don't hit this constraint, other than during for_each.",
        "I was thinking more along the lines of someone who's introducing provider for_each into an existing codebase, and is confused why they are just now seeing this message.",
        "Going to resolve for now, happy to amend with further user feedback :+1: ",
        "Done!"
      ],
      "opentofu-minimize-api-surface": [
        "This appears to be nearly identical to the previous functionality (without the tag) as public properties are already encoded in the json package.\r\n\r\nI see two potential differences:\r\n1. The casing of the names is different and should be identical to the field name as before.\r\n2. The omitempty might generate payloads that are different that what was generated previously.\r\n\r\n1 could cause some serious breakage with previous locks (assuming that we json encoded it in previous releases)\r\n\r\n2 is probably not an issue as tools should not be directly manipulating this data outside of tofu.\r\n",
        "`grep \"json.Marshal\" internal/backend/ -r` shows that backends already depend on the current field names, which means they should be preserved\r\n",
        "Marking as resolved :+1: "
      ],
      "opentofu-document-reference-standards": [
        "Can we remove these individual comparability notes and point instead at the migration guide?"
      ],
      "opentofu-document-intent-and-limitations": [
        "We typically don't suggest folks look directly at the statefile, I don't think we have these fields documented anywhere concrete.",
        "Fixed in 1c3e8c0677",
        "Fixed in e7fe712c1d7d9b97c628a4f6eb9610e650ffe54b",
        "We now list all builtin functions both without a namespace and under the core:: namespace"
      ],
      "opentofu-prevent-backing-array-surprises": [
        "Makes sense :+1: \r\nb884241ff0",
        "[b884241](https://github.com/opentofu/opentofu/pull/2577/commits/b884241ff0924238bd19c95d137166e30abf3089)",
        "Our lint rules push for a particular application of de morgans law to keep boolean operations consistent.",
        "I've gone back and forth on this.  We know we are using overlapping code paths to produce duplicate diagnostics.  I'm ok with this being a bit stricter than ConsolidateWarnings.  I think the DeepEqual is probably a bit overkill, but @ollevche's list makes sense, perhaps with the Ranges added as well.",
        "I also think that DoNotConsolidateDiagnostic can be ignored for this particular function, as it is a special purpose function only for this scenario."
      ],
      "opentofu-clear-concise-documentation": [
        "I dislike the \"Please\", I want the message to be a call to action and not gentle."
      ],
      "opentofu-specify-configuration-behaviors": [
        "This was implemented in the implementation PR with a few lines.  It's easy to back out, but I'm not sure if it's more confusing to intentionally exclude it here."
      ],
      "opentofu-separate-configuration-lifecycles": [
        "1. It's more so a \"provider block\" vs \"provider block with instance data\"\r\n2. This is true, the expansion here feels a bit weird IMO and I noticed that during the prototype phase.  This change is the \"low churn\" option, but adds cognitive load to understanding provider configs in the configs package. \r\n\r\nThere are technically 3 levels of config parsing that are all mixed together:\r\n1. hcl/json text -> configs.File (HCL loading)\r\n2. configs.File -> configs.Module (merging files)\r\n3. configs.Module -> configs.Module (static evaluation)\r\n\r\nEven before static evaluation was added, there were all sorts of odd validation bugs all throughout the code when dealing with overrides.  Static evaluation shines more of a light on this.\r\n\r\nIn this specific scenario, we would need:\r\n```go\r\n\r\n// File parsing produces\r\ntype configs.ProviderBlock struct {\r\n  // 1-1 mapping to HCL\r\n}\r\n\r\n// Module struct from Files (normal+override) produces\r\ntype configs.ProviderData struct {\r\n  // Closer to existing struct\r\n  ForEach hcl.Expression\r\n  Count hcl.Expression\r\n}\r\n// Validation is performed when constructing ProviderData\r\n\r\n// func (ProviderData) expand(StaticContext) []Provider\r\n\r\n// Expanded into one or more\r\ntype configs.Provider struct {\r\n  // Matches existing struct\r\n  \r\n  ForEachValue *cty.Value\r\n  CountValue *cty.Value\r\n  // OR\r\n  InstanceData instances.RepetitionData\r\n}\r\n```\r\n\r\n\r\nFor the purposes of this PR, we could merge the concept of ProviderBlock and ProviderData to reduce the potential set of changes as it is a less important distinction IMO.",
        "I wonder if a pattern like `alias, ok := target.ref.KnownAlias(); ok` would make more sense here?  I also stink at naming functions.",
        "Given that import blocks are only allowed in the root module, we should know the exact provider alias and can treat any unknowns here as a bug?",
        "The config is already accessed in EvaluationScope below:\r\nhttps://github.com/opentofu/opentofu/pull/1772/files?diff=split&w=1#diff-126d6884fd43891cbb56845ee6f7f51ed02895af69190815e5c694a51a3777d3L522\r\n\r\nI don't think we need to add ctx.Config and can use that same code instead."
      ],
      "opentofu-proper-span-lifecycle": [
        "Another approach that could be considered is something like https://github.com/opentofu/opentofu/pull/1878/files#diff-385ff28969dc6410989718822b4c24e033e3a10d679bdd792fd2d55148c49471R456\r\n\r\nIt's a similar situation where I didn't want to heavily refactor the loop.  It mixes defer and iterative \"actions\" in a way that's comprehensive",
        "Alternatively, this is now the second time we are running into this and should probably take the time to refactor it :D",
        "Do you still need to span.End() here?"
      ]
    }
  },
  "mitchellh": {
    "repos": [
      "ghostty-org/ghostty"
    ],
    "entries": [
      {
        "slug": "ghostty-centralize-configuration-management",
        "title": "Centralize configuration management"
      },
      {
        "slug": "ghostty-centralize-configuration-values",
        "title": "Centralize configuration values"
      },
      {
        "slug": "ghostty-code-structure-clarity",
        "title": "Code structure clarity"
      },
      {
        "slug": "ghostty-define-explicit-error-sets",
        "title": "Define explicit error sets"
      },
      {
        "slug": "ghostty-descriptive-consistent-naming",
        "title": "Descriptive consistent naming"
      },
      {
        "slug": "ghostty-document-configs-comprehensively",
        "title": "Document configs comprehensively"
      },
      {
        "slug": "ghostty-in-tree-build-configurations",
        "title": "In-tree build configurations"
      },
      {
        "slug": "ghostty-prefer-flat-control-flow",
        "title": "Prefer flat control flow"
      },
      {
        "slug": "ghostty-prefer-safe-optional-handling",
        "title": "Prefer safe optional handling"
      },
      {
        "slug": "ghostty-prevent-unnecessary-memory-operations",
        "title": "Prevent unnecessary memory operations"
      },
      {
        "slug": "ghostty-semantic-name-clarity",
        "title": "Semantic name clarity"
      },
      {
        "slug": "ghostty-structure-user-documentation",
        "title": "Structure user documentation"
      },
      {
        "slug": "ghostty-use-if-unwrap-with-optionals",
        "title": "Use if-unwrap with optionals"
      }
    ],
    "comments": {
      "ghostty-prefer-flat-control-flow": [
        "Thanks, this was AI slop lol",
        "This is fine, but you could also do:\r\n\r\n`guard case .split(let c) = node else { return }`\r\n\r\n(Obviously... /sarcasm lol)"
      ],
      "ghostty-code-structure-clarity": [
        "Can you add a type signature here? I find with big blocks like this its good practice to prevent future mistyping if we refactor or something. And it just helps self-document a bit",
        "Ah ah, duh. No it's okay, maybe a comment then to note it. ",
        "Given the duplication here with `RepeatablePath`, I'd like to see this pulled out to a shared common function. It'll probably require some parameter changes. This is especially necessary since we just merged home path expansion and this is now missing it.",
        "I think we should nest all the of these in a single `ColorOperation` struct.\r\n\r\n```zig\r\npub const ColorOperation = struct {\r\n  pub const Source = ...;\r\n  pub const List = ...;\r\n  pub const Kind = ...;\r\n}\r\n```",
        "Style: can do `.{` here since the result type is well known.\r\n\r\nIf you want to get really fancy you don't need the block in the `for` either since this is a single expression, but we mix that up all over the codebase.",
        "I think it'd be an improvement to separate these into separate functions: `init` and `initQuickTerminal`. It'll make it easier to grep for when looking for things that setup a quick terminal and I think is generally nicer than a modal bool on a public interface.\r\n\r\n(Internally, they probably drive back to the same non-pub `initPrivate` or w/e).",
        "If `setup` is always called directly after `init`, then we should combine `init` and `setup` into a fail-able `init`."
      ],
      "ghostty-use-if-unwrap-with-optionals": [
        "I think it's more idiomatic to do a `!= null` if you're throwing away the value anyways."
      ],
      "ghostty-in-tree-build-configurations": [
        "We probably only need to test amd64, I can't see a scenario where one works and one fails since what we're really protecting against with this test is package versions."
      ],
      "ghostty-define-explicit-error-sets": [
        "Having bunch of `catch` in this is nasty and makes it so `err defer` can't be used (if we even wanted to). I recommend the following pattern instead:\r\n\r\n```zig\r\nfn openUrlLinuxThread(...) void {\r\n    // Handle error once\r\n    openUrlLinuxThreadError(...) catch |err| {}\r\n}\r\n\r\nfn openUrlLinuxThreadError(...) !void {\r\n    // Return errors like normal, use errdefer.\r\n}\r\n```",
        "Pretty sure this can have an explicit error set. Eyeballed it down and it looks clean.",
        "I think this can trivially have a defined error set too. No need to dive all the way into `pkg/opengl`, can just set it here."
      ],
      "ghostty-structure-user-documentation": [
        "I think both this and the clear command need a more detailed synopsis. This makes it sound like it'll magically list hosts but really it's shell-integration specific. Perhaps something more like \"List hosts cached for SSH shell integration\""
      ],
      "ghostty-prefer-safe-optional-handling": [
        "There needs to be a guard here that the object is within the `surfaceTree` otherwise toggle maximize will maximize all open terminal windows. ðŸ˜„ \r\n\r\nThere should be at least one other action that has this check, search for `contains`."
      ],
      "ghostty-centralize-configuration-values": [
        "Let's pass the Zig version in here as well and put that as a top-level workflow env. The number of places we hardcode the Zig version we use is too damn high. \r\n\r\nLonger term it'd probably be better to make a `setup` job that extracts it from our source code (eventually the build.zig.zon but we don't use that yet)."
      ],
      "ghostty-prevent-unnecessary-memory-operations": [
        "Allocating when we don't have to on systems like posix feels bad. We shouldn't allocate on posix."
      ],
      "ghostty-document-configs-comprehensively": [
        "Again, normally I'm very much in the camp of bending over backwards to support backwards compatibility. In this case, we've explicitly warned for a couple releases now that this is coming and that we will be breaking compatibility, so I think in this case, we should hard remove as @jcollie has done and have the config warnings popup come up.",
        "I disagree about introducing a new config. macOS uses 0 to disable blur. We should do the same here. The uglier part is that the magnitude of value doesn't matter for Linux.\r\n\r\nI'd propose introduce a new type here `BackgroundBlurRadius` that wraps a `u8` but parses `false` to 0 and `true` to something else (I recommend 20, since that's a reasonable value for macOS). That way this works on both macOS and Linux with \"true\" but also can be set to a specific number to have an effect on macOS and still enable on Linux.\r\n\r\nAnd ultimately, this keeps a single, backwards compatible config. ",
        "That's right. Let me think about this a bit more. I'm not ready to accept two configs.",
        "I agree with this. I think we need to introduce a mechanism to rename. I can work on that. For the sake of this PR, please override `background-blur-radius`. We'll add the mechanism to rename and do the rename in a follow up PR. I don't want to block this for you."
      ],
      "ghostty-centralize-configuration-management": [
        "I think it's less confusing generally to keep these derived configs as dumb as possible. I recommend changing this to just copy the `macosHidden` value instead.",
        "This isn't correct a static value. We should be using the default size for the window that might be set with the config. \r\n\r\nThere's a lot of logic in `TerminalWindow.windowDidLoad` I think around this. That should probably be extracted in some way so it can be reused by AppD."
      ],
      "ghostty-semantic-name-clarity": [
        "I think having an inverted bool here makes the cognitive load slightly higher versus matching the config. I'd change this instead to `titled` and invert all the logic so we match the config.",
        "I agree with this. Thank you.",
        "I try to follow AppKit's notification naming convention of roughly a noun phrase followed by a past participle adjective. i.e. `[Subject] [Event (past participle)]`.\r\n\r\nSo, we should do `ghosttyFullscreenDidToggle`. \r\n\r\nThere's an inconsistency here because the notifications were never fully converted to this style (we should do that in a future PR) but for new stuff we should follow it."
      ],
      "ghostty-descriptive-consistent-naming": [
        "I think this naming doesn't match the pattern of other variants. It should be `GHOSTTY_FULLSCREEN_NON_NATIVE_TITLED_VISIBLE_MENU` probably along with all the other enums and so on throughout this PR.",
        "Rename to `Dir`, usually singular. "
      ]
    }
  },
  "Yantrio": {
    "repos": [
      "opentofu/opentofu"
    ],
    "entries": [
      {
        "slug": "opentofu-clear-concise-documentation",
        "title": "Clear concise documentation"
      },
      {
        "slug": "opentofu-defensive-null-handling",
        "title": "Defensive null handling"
      },
      {
        "slug": "opentofu-document-intent-and-limitations",
        "title": "Document intent and limitations"
      },
      {
        "slug": "opentofu-document-reference-standards",
        "title": "Document reference standards"
      },
      {
        "slug": "opentofu-document-with-examples",
        "title": "Document with examples"
      },
      {
        "slug": "opentofu-log-effectively-for-debugging",
        "title": "Log effectively for debugging"
      },
      {
        "slug": "opentofu-minimize-api-surface",
        "title": "Minimize API surface"
      },
      {
        "slug": "opentofu-optimize-cicd-workflows",
        "title": "Optimize CI/CD workflows"
      },
      {
        "slug": "opentofu-prevent-backing-array-surprises",
        "title": "Prevent backing array surprises"
      },
      {
        "slug": "opentofu-proper-span-lifecycle",
        "title": "Proper span lifecycle"
      },
      {
        "slug": "opentofu-protect-infrastructure-secrets",
        "title": "Protect infrastructure secrets"
      },
      {
        "slug": "opentofu-secure-encryption-key-backups",
        "title": "Secure encryption key backups"
      },
      {
        "slug": "opentofu-specify-configuration-behaviors",
        "title": "Specify configuration behaviors"
      },
      {
        "slug": "opentofu-structure-tests-thoroughly",
        "title": "Structure tests thoroughly"
      }
    ],
    "comments": {
      "opentofu-protect-infrastructure-secrets": [
        "It may be worth mentioning here that the statefile and the planfiles store information in plaintext. Secrets are only sanitized by the user interface intended for humans but those files do store sensitive information",
        "That raises another question, we should explain how we shouldn't use this with state encryption (due to the ephemerality) because some people may think \"This is great for secrets, i can use it for my passphrase\" and have issues.\r\n",
        "```suggestion\r\nAt the time of writing this RFC, write-only attributes are supported by a low number of providers and resources.\r\n```"
      ],
      "opentofu-optimize-cicd-workflows": [
        "I think we should also ignore the `docs` folder"
      ],
      "opentofu-document-with-examples": [
        "I wonder if we require a small bit of background here on the usage of the word `plugin` vs the usage of the word `provider` and how that relates to backends as plugins. \r\n\r\nThis will help us understand if we're pitching something new, or something as part of the provider SDK/framework etc.",
        "I think maybe we should specify an example here?  its not clear what `alias` is here."
      ],
      "opentofu-log-effectively-for-debugging": [
        "food for thought: Do you think it would be nice to include the number of retries here?\r\n\r\nMaybe something like \r\n```go\r\nlogger.Printf(\"[INFO] failed to fetch provider package; retrying attempt %s/%s\", i, maxHTTPPackageRetryCount)\r\n```",
        "Should we also log in the case of an error? (ie, if someone provided a string instead of a number here, or a negative value."
      ],
      "opentofu-structure-tests-thoroughly": [
        "this isn't testing the possible failure case where we want an error but it's actually returned 0 diags.\r\n\r\nI think something like \r\n```go\r\nif tc.wantErr != == && len(diags) == 0 {\r\n  t.Fatalf(\"expected error but got none\")\r\n}\r\n```\r\n\r\nshould cover us\r\n",
        "I think we should add them, mainly because it would help us identify regressions in the future, especially if eval_for_each_test.go somehow gets refactored. I would rather have some duplication of tests if it gives us faith that we're testing properly."
      ],
      "opentofu-minimize-api-surface": [
        "what if the API returned a nil marker back (not an empty string?)"
      ],
      "opentofu-document-reference-standards": [
        "```suggestion\r\nCheck out our [Contributing FAQ](contributing/FAQ.md) and [Development Guide](contributing/DEVELOPING.md) for more detailed information.\r\n```"
      ],
      "opentofu-defensive-null-handling": [
        "```suggestion\r\n\t\t\tSummary:  \"Import block 'to' address contains an invalid key\",\r\n\t\t\tDetail:   \"Import block contained a resource address using an index which is null. Please make sure the expression for the index is not null\",\r\n```"
      ],
      "opentofu-document-intent-and-limitations": [
        "I think for our future selves it may be best if we could document why this is how it is.\r\n\r\nWould you mind adding a small comment here explaining why its 777 and not 755 please?"
      ],
      "opentofu-prevent-backing-array-surprises": [
        "Can you add a comment to explain why we want to iterate over the attribute types in a sorted manner please?"
      ],
      "opentofu-clear-concise-documentation": [
        "I think both forms are correct here, telling someone to \"use caution\" and \"use with caution\" are both valid.\r\n\r\nI would actually propose a third alternative to make things worse: `Please be cautious when referencing....`"
      ],
      "opentofu-specify-configuration-behaviors": [
        "If a variable is used deep within a nest of modules, how do you feel is best to help the end user running something like `tofu validate` find that variable in the code? Do we link to the file, do we try and construct an address for this variable? ",
        "We should reference that work has already been done to decouple the idea of something having a mark and something being sensitive.",
        "https://github.com/opentofu/opentofu/pull/2503 This PR. \r\n\r\nPreviously in the codebase there was only one type of mark. So you could check if something was sensitive by ensuring that it was marked or not. \r\n\r\nThis had to be refactored a little to allow multiple marks (first deprecated, and now this)",
        "do we also need to introduce some checks/tests to ensure that functions such as `sensitive()` do not accidentally strip the ephemeral mark too?",
        "let's not forget to document here that variables that take in ephemeral marked values should also be implicitly marked as ephemeral",
        "```suggestion\r\n> If any information in the configuration of an ephemeral resource is unknown during the `plan` phase, OpenTofu will defer the provisioning of the resource for the `apply` phase. This means that inconsistency can occur between the plan and the apply phase.\r\n```"
      ],
      "opentofu-secure-encryption-key-backups": [
        "```suggestion\r\nFinally, before enabling encryption, please exercise your disaster recovery plan and make a temporary backup of your unencrypted state file. Also, make sure you have backups of your keys. Once you enable encryption, OpenTofu cannot read your state file without the correct key.\r\n\r\n```"
      ],
      "opentofu-proper-span-lifecycle": [
        "Because this is inside the loop of `for provider, version := range need {`",
        "I didn't want to do large refactorings of existing code in this codebase, plus i feel like this sets a good example of how we should be handling spans in loops anyway.",
        "I'll rebase ontop of this PR once it's in: https://github.com/opentofu/opentofu/pull/2695",
        "Yup! I missed this one. thanks.",
        "Is there a situation where `ExecuteDone` doesn't get fired here and we need to maybe defer ending the span anyway?",
        "That's fine by me then :D "
      ]
    }
  },
  "jfagoagas": {
    "repos": [
      "prowler-cloud/prowler"
    ],
    "entries": [
      {
        "slug": "prowler-configure-observability-variables",
        "title": "Configure observability variables"
      },
      {
        "slug": "prowler-consistent-environment-variable-naming",
        "title": "Consistent environment variable naming"
      },
      {
        "slug": "prowler-document-configuration-variables",
        "title": "Document configuration variables"
      },
      {
        "slug": "prowler-endpoints-for-evolving-data",
        "title": "Endpoints for evolving data"
      },
      {
        "slug": "prowler-ensure-migration-compatibility",
        "title": "Ensure migration compatibility"
      },
      {
        "slug": "prowler-format-ai-code-identifiers",
        "title": "Format AI code identifiers"
      },
      {
        "slug": "prowler-least-privilege-principle",
        "title": "Least privilege principle"
      },
      {
        "slug": "prowler-log-exceptions-with-context",
        "title": "Log exceptions with context"
      },
      {
        "slug": "prowler-meaningful-consistent-naming",
        "title": "Meaningful consistent naming"
      },
      {
        "slug": "prowler-memory-usage-optimization",
        "title": "Memory usage optimization"
      },
      {
        "slug": "prowler-parameterize-configuration-values",
        "title": "Parameterize configuration values"
      },
      {
        "slug": "prowler-pin-github-actions-dependencies",
        "title": "Pin GitHub Actions dependencies"
      },
      {
        "slug": "prowler-precise-csp-configuration",
        "title": "Precise CSP configuration"
      },
      {
        "slug": "prowler-prioritize-code-readability",
        "title": "Prioritize code readability"
      },
      {
        "slug": "prowler-safe-attribute-access-patterns",
        "title": "Safe attribute access patterns"
      },
      {
        "slug": "prowler-secure-authentication-flows",
        "title": "Secure authentication flows"
      },
      {
        "slug": "prowler-service-layer-abstraction",
        "title": "Service layer abstraction"
      },
      {
        "slug": "prowler-specific-exception-handling",
        "title": "Specific exception handling"
      },
      {
        "slug": "prowler-task-signature-methods",
        "title": "Task signature methods"
      },
      {
        "slug": "prowler-tenant-aware-query-optimization",
        "title": "Tenant-aware query optimization"
      },
      {
        "slug": "prowler-use-configurable-default-values",
        "title": "Use configurable default values"
      },
      {
        "slug": "prowler-write-objectively",
        "title": "Write objectively"
      }
    ],
    "comments": {
      "prowler-least-privilege-principle": [
        "Hello @maxi-bee, `iam:PassRole` action could lead into privilege escalation if the resource configured is `*` or list/single role with more privileges. In this case I recommend you to use the [Mutelist](https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/mutelist/) if the affected IAM Policy is flagged as `FAIL` by Prowler because in your environment/context it is not."
      ],
      "prowler-parameterize-configuration-values": [
        "Could you please store the Powershell version in a variable?\r\n```\r\nARG POWERSHELL_VERSION=v7.5.0\r\n```",
        "I don't know how are we going to manage Powershell dependencies but we'll need to find an automated way and try to have just one source of truth. Regarding the environment variable I think it is good to have it."
      ],
      "prowler-secure-authentication-flows": [
        "We need to always redirect to the SAML endpoint, if not we will allow domain enumeration if responses are different based on existence.",
        "What's in the response body? We should find a way to prevent or mitigate it.",
        "Not sure about this step, I don't know if we should make these assumptions, like `easier and faster` or `always work with org`, for example the latter does not care about the type of authentication. Also assuming a role is not authentication but authorization, it always requires base credentials."
      ],
      "prowler-task-signature-methods": [
        "`s` or `si` as we have in other tasks?",
        "`s` or `si` as we have in other tasks?"
      ],
      "prowler-configure-observability-variables": [
        "We should add these two to have more info in all the errors:\r\n- `SENTRY_ENVIRONMENT`\r\n- `SENTRY_RELEASE`"
      ],
      "prowler-safe-attribute-access-patterns": [
        "You can also do `project_info[\"source\"].get(\"location\",\"\") to have it in one line without the `if/else`"
      ],
      "prowler-service-layer-abstraction": [
        "Since you created this, could you add it as a replacement in lines 185-192?\r\n\r\nAlso please add tests to this function using `moto`.",
        "The `Content-Type` needs to be set because in AWS by default is `binary/octet-stream`. That's another reason why I recommended you to use the current SDK's S3 methods. We can adapt whatever is needed to reutilize code.",
        "S3 integration allows you to pass the same credentials as the provider and has a `test_connection` method, so you don't need to setup the AWS provider.\r\n```suggestion\r\n            S3.test_connection(**integration.credentials)\r\n```\r\n\r\nI see the `test_connection` does not support passing the raw credentials and it should. I'm going to add support to that.",
        "Why are you not using SDK's `send_to_bucket`? We should aim to have all this login in the SDK and to adapt whatever is missing for the API to use it. If not you will need to replicate all the logic in there to handle content-type and file paths.",
        "I get that. I think we should first analyse the SDK's status prior starting the work and make all the required changes. \r\n\r\nRegarding the `content-type`, we need to set it for each file because AWS sets only the file type."
      ],
      "prowler-format-ai-code-identifiers": [
        "```suggestion\nFor example, the description of `getScanTool` is \"Fetches detailed information about a specific scan by its ID.\" If the description doesn't convey what the tool is capable of doing, LLM will not invoke the function. If the description of `getScanTool` was set to something random or not set at all, LLM will not answer queries like \"Give me the critical issues from the scan ID xxxxxxxxxxxxxxx\"\n```",
        "```suggestion\n- It uses a \"supervisor\" architecture that interacts with different agents for specialized tasks. For example, `findings_agent` can analyze detected security findings, while `overview_agent` provides a summary of connected cloud accounts.\n```"
      ],
      "prowler-prioritize-code-readability": [
        "```suggestion\r\n                return {}\r\n```\r\n\r\nI think this is missing one level indentation, right?",
        "Again, to me this is really hard to follow.",
        "To me @vicferpoy has the final word on this PR.",
        "Totally agree, I'm getting older.",
        "`if` or `elif`?"
      ],
      "prowler-use-configurable-default-values": [
        "If the default value is `in-memory` we'd need to set that default.\r\n```suggestion\r\n    prowler_db_connection = os.environ.get('PROWLER_DB_CONNECTION', \"memory://\")\r\n```",
        "Can we add an environment variable for the batch size?",
        "What about `env.int(\"DJANGO_FINDINGS_BATCH_SIZE\", 1000)`",
        "Not having `DJANGO_OUTPUT_AWS_DEFAULT_REGION` makes the endpoint to raise a `HTTP 500 Internal Server Error`. I think we should handle that too.",
        "Fixed and tested ",
        "If credentials are not configured this is raising an exception, could you please check it when you get a chance?\r\n\r\nWhen this happens the call to _upload_to_s3 within generate_outputs raises an exception and the output location is not stored nor locally.",
        "Good point, thanks for pointing that out. The only issue I see with that is that if something fails there is no fallback to the local storage."
      ],
      "prowler-write-objectively": [
        "```suggestion\n2. Provide a valid Mutelist in `YAML` format. You can see full details about Mutelist [here](../tutorials/mutelist.md))\n```",
        "True, remove it ðŸ˜„ \r\n```\r\n2. Provide a valid Mutelist in `YAML` format. See full details about Mutelist [here](../tutorials/mutelist.md))\r\n```"
      ],
      "prowler-ensure-migration-compatibility": [
        "Do we want to backfill all previous findings? This blocks the app startup while updating all the findings."
      ],
      "prowler-consistent-environment-variable-naming": [
        "Why public cert and private key? I think it should be either private/public key or certificate and private key.",
        "In that case, why not to rename it to `SAML_CERTIFICATE`?",
        "I'd call it either `SAML_X509_CERT` or `SAML_CERT`, to me adding public there is confusing.",
        "It's not confusing but maybe I'm used to call it differently.",
        "All the `ARTIFACTS_*` environment variables should be prefixed with `DJANGO` because it is the convention we've been using for the ones used in Django."
      ],
      "prowler-precise-csp-configuration": [
        "Do you need to add all of this to support GTM?",
        "Thanks for the clarification ðŸ‘ "
      ],
      "prowler-endpoints-for-evolving-data": [
        "@Chan9390 we've been reviewing the PR and get to the point that we need an API endpoint to fetch compliance frameworks by provider instead of having this file where all is hardcoded because we are constantly adding new frameworks and that'd add more steps when creating them.\r\n\r\nPlease talk with the API team (@vicferpoy and @AdriiiPRodri) because they can explain to you how to create a simple endpoint to get that from some memory objects the API service has.",
        "For now we'll use the Hub, included in [`15f98d7` (#7878)](https://github.com/prowler-cloud/prowler/pull/7878/commits/15f98d79e0fc2c5c7e9ac9bddb69ff4bf24a332c)",
        "@Chan9390 we've been reviewing the PR and get to the point that we need an API endpoint to fetch checks by provider instead of having this file where all is hardcoded because we are constantly adding new checks and eventually providers and that'd add more steps when creating them.\r\n\r\nPlease talk with the API team (@vicferpoy and @AdriiiPRodri) because they can explain to you how to create a simple endpoint to get that from some memory objects the API service has."
      ],
      "prowler-document-configuration-variables": [
        "Is there any real intention of having the SQLite connection chain in the environment variable? If not I think we can just set `sqlite` because it may lead to confusion, seems that the string needs to be completed.",
        "Thanks!",
        "```suggestion\n???+ note\n    The Mutelist configuration takes effect on the next scans.\n```"
      ],
      "prowler-pin-github-actions-dependencies": [
        "SHA also here.\r\n```suggestion\r\n        uses: peter-evans/find-comment@3eae4d37986fb5a8592848f6a574fdf654e61f9e #v3.1.0\r\n```",
        "Why this?",
        "Out of curiosity, why do you need to install all these in the workflow system? ",
        "Interesting, can you create a TODO and a ticket to review this in the future once the issue is addressed?"
      ],
      "prowler-log-exceptions-with-context": [
        "You can also log this in Sentry with\r\n```python\r\nimport sentry_sdk\r\n\r\nsentry_sdk.capture_exception(exception)\r\n```",
        "I added that log line within the `generate_output` `try/except` clause, if it is not enough I can add another one here."
      ],
      "prowler-meaningful-consistent-naming": [
        "Why this change? Just curious.",
        "For me `user_mail`/`mail` and `token` are more appropriate here.",
        "Thanks for the clarification!"
      ],
      "prowler-memory-usage-optimization": [
        "Why not to add a parameter to this query?",
        "What is the point of this context manager? Is it intended for testing and benchmarking?",
        "Would not be easier to just call explicitly the GC from `main`?",
        "We are not sure about this, but we will review that later on.",
        "I thought we talked about writing findings to file in _streaming_ instead of all at once to reduce memory overhead."
      ],
      "prowler-specific-exception-handling": [
        "I forgot to point the SDK to this branch and I got this error:\r\n```\r\nworker-dev-1   | [2025-06-19 16:35:02,487: ERROR/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: S3 output upload failed for integration 8740eeb1-789c-4890-93a9-f41d8c71a742: 'S3' object has no attribute 'upload_file'\r\nworker-dev-1   | [2025-06-19 16:35:02,503: ERROR/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: S3 compliance upload failed for integration 8740eeb1-789c-4890-93a9-f41d8c71a742: 'S3' object has no attribute 'upload_file'\r\nworker-dev-1   | [2025-06-19 16:35:02,517: INFO/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: All the S3 integrations completed successfully for provider 1370501e-80a3-4b1b-aad8-bdc4ce26066f\r\n```\r\n\r\nI think we should not print this line because there was an error:\r\n```\r\nworker-dev-1   | [2025-06-19 16:35:02,517: INFO/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: All the S3 integrations completed successfully for provider 1370501e-80a3-4b1b-aad8-bdc4ce26066f\r\n```",
        "Thanks!",
        "Why this? Are we sure that this is not going to break anything?",
        "I knew that, only concerned about this changes because you know what happened in other cases, but better if this fails because something was not right.\r\n\r\nThanks!",
        "We need to handle `NoSuchKey` if the file got deleted from S3.",
        "Tested and fixed âœ…  ",
        "We should also add `before_send=before_send` not to send several credential errors coming from the SDK.\r\n\r\n\r\n```python\r\nignored_exceptions = [\r\n    # Authentication Errors from AWS\r\n    \"InvalidToken\",\r\n    \"AccessDeniedException\",\r\n    \"AuthorizationErrorException\",\r\n    \"UnrecognizedClientException\",\r\n    \"UnauthorizedOperation\",\r\n    \"AuthFailure\",\r\n    \"InvalidClientTokenId\",\r\n    \"AccessDenied\",\r\n    # Shodan Check\r\n    \"No Shodan API Key\",\r\n    # For now we don't want to log the RequestLimitExceeded errors\r\n    \"RequestLimitExceeded\",\r\n    \"ThrottlingException\",\r\n    \"Rate exceeded\",\r\n    # The following comes from urllib3\r\n    # eu-west-1 -- HTTPClientError[126]: An HTTP Client raised an unhandled exception: AWSHTTPSConnectionPool(host='hostname.s3.eu-west-1.amazonaws.com', port=443): Pool is closed.\r\n    \"Pool is closed\",\r\n]\r\n\r\n\r\ndef before_send(event, hint):\r\n    \"\"\"\r\n    before_send handles the Sentry events in order to sent them or not\r\n    \"\"\"\r\n    # Ignore logs with the ignored_exceptions\r\n    # https://docs.python.org/3/library/logging.html#logrecord-objects\r\n    if \"log_record\" in hint:\r\n        log_msg = hint[\"log_record\"].msg\r\n        log_lvl = hint[\"log_record\"].levelno\r\n\r\n        # Handle Error events and discard the rest\r\n        if log_lvl == 40 and any(ignored in log_msg for ignored in ignored_exceptions):\r\n            return\r\n    return event\r\n```\r\n\r\nThis is not tested ðŸ˜„ ",
        "I think this needs to be refined as we previously did\r\n```python\r\ntry:\r\n                s3_object = s3_client.get_object(Bucket=bucket_name, Key=key)\r\n            except ClientError as e:\r\n                error_code = e.response.get(\"Error\", {}).get(\"Code\")\r\n                if error_code == \"NoSuchKey\":\r\n                    return Response(\r\n                        {\"detail\": \"The scan has no reports.\"},\r\n                        status=status.HTTP_404_NOT_FOUND,\r\n                    )\r\n                return Response(\r\n                    {\"detail\": \"There is a problem with credentials.\"},\r\n                    status=status.HTTP_403_FORBIDDEN,\r\n                )\r\n```\r\n\r\nWe should not raise `HTTP 500 Internal Server Error`.",
        "I do not agree with you. A `ClientError` in `botocore`/`boto3` could not end up in an `HTTP 500 Internal Server Error` because is it a wrapper on top of different types of errors. If we are doing a `list_objects_v2` we should handle what's happening by:\r\n- Logging the error\r\n- Sending the exception to Sentry\r\n\r\nWe can maybe raise 500's for some cases but not for all the possible exceptions under `ClientError`.",
        "Thanks, I know that we need to improve the way we handle this things, but I'm not sure about it."
      ],
      "prowler-tenant-aware-query-optimization": [
        "That's concerning... why are you removing the `tenant_id` filter? It is a key part of our multi tenant system with RLS, we could not remove it unless there is a strong reason behind."
      ]
    }
  },
  "nameexhaustion": {
    "repos": [
      "pola-rs/polars"
    ],
    "entries": [
      {
        "slug": "polars-appropriate-error-handling",
        "title": "Appropriate error handling"
      },
      {
        "slug": "polars-choose-appropriate-abstractions",
        "title": "Choose appropriate abstractions"
      },
      {
        "slug": "polars-database-api-abstraction",
        "title": "Database API abstraction"
      },
      {
        "slug": "polars-defer-expensive-operations",
        "title": "Defer expensive operations"
      },
      {
        "slug": "polars-evaluate-algorithmic-complexity-tradeoffs",
        "title": "Evaluate algorithmic complexity tradeoffs"
      },
      {
        "slug": "polars-explicit-null-handling",
        "title": "Explicit null handling"
      },
      {
        "slug": "polars-hide-implementation-details",
        "title": "Hide implementation details"
      },
      {
        "slug": "polars-names-reveal-clear-intent",
        "title": "Names reveal clear intent"
      },
      {
        "slug": "polars-optimize-data-transformations",
        "title": "Optimize data transformations"
      },
      {
        "slug": "polars-optimize-memory-allocation-patterns",
        "title": "Optimize memory allocation patterns"
      },
      {
        "slug": "polars-prevent-cryptic-errors",
        "title": "Prevent cryptic errors"
      },
      {
        "slug": "polars-prevent-deadlock-conditions",
        "title": "Prevent deadlock conditions"
      },
      {
        "slug": "polars-redact-sensitive-credentials",
        "title": "Redact sensitive credentials"
      },
      {
        "slug": "polars-safe-null-handling",
        "title": "Safe null handling"
      }
    ],
    "comments": {
      "polars-defer-expensive-operations": [
        "We currently eagerly query the dataset for the snapshot ID, but this PR defers all IO operations until collection time.\r\n\r\nNote I had to change the exception type because we are going through the Rust, I plan to change it back after https://github.com/pola-rs/polars/issues/22410.\r\n",
        "Side note: On EC2 I got ~13x vs ~150x for `factor`\r\n"
      ],
      "polars-redact-sensitive-credentials": [
        "Authentication will now prioritize Entra ID (via the Python-side `DefaultAzureCredential` if `azure.identity` is installed). ~~Using the storage account key will only happen if we get a permission error (above).~~\r\n"
      ],
      "polars-optimize-data-transformations": [
        "It won't if `reorder_columns` is a no-op, but in general this final clear_schema is just to be safe in terms of all the other things we did above.\r\n\r\nI'd prefer to just have it there, it helps me sleep at night ðŸ˜„.\r\n"
      ],
      "polars-optimize-memory-allocation-patterns": [
        "Remove this `collect::<Vec<_>>()`, this allocates per-row and is unnecessary - `try_push` accepts a generic `IntoIterator`.\r\n",
        "This function should be removed. From what I can see it's used in 2 places:\r\n* In a `polars_ensure!` - it's enough to check that the inner type is a primitive type\r\n* For `element_size`, you can get this from `std::mem::size_of::<T>()` inside the function instead of passing it in.\r\n"
      ],
      "polars-choose-appropriate-abstractions": [
        "Why `OsStr`? I think cloud paths should always use `String` / `Box<str>`, all of the APIs we call into also expect an `&str` so it would be good to have this validation happen once at construction.\r\n",
        "Ah\r\n\r\nI am currently having the caller decide which operations to push into the reader based on this. But I can maybe change it to make it so that the reader gets to decide what it takes. But probably in a follow-up PR.\r\n"
      ],
      "polars-safe-null-handling": [
        "~~The allocation length here should be sourced as `num_valid_rows * (element_size * size)` `len() * (element_size * size)` instead of from the first element. I would also recommend having a `let width = element_size * size` earlier in the function.~~\r\n\r\nThis currently also ends up allocating a 0-length array if the first element is NULL.\r\n",
        "for catch-all we need to at least follow the `join_produces_null` rule\r\n",
        "Casting nullable List -> FixedSizeList, used a gather to ensure the width of the null slots - have updated this to use `Growable` instead.\r\n"
      ],
      "polars-explicit-null-handling": [
        "```suggestion\r\n            Note that rows of the binary array where the length does not match the width of the output array will become NULL.\r\n```",
        "Please also document that when reinterpreting to `Array`s, it is not allowed to reinterpret to more than 1 nesting level and recommend to instead `reshape()` separately afterwards.\r\n"
      ],
      "polars-appropriate-error-handling": [
        "nit: this should either be just `panic!()` or `unreachable!()`, or actually print out what the exact value is. I personally prefer adding in `unreachable!()`'s with a comment line, as that means we don't compile in an extra error string message into the binary while ensuring a future developer can go to this line and see why we panic.\r\n",
        "> Should this not just be an unwrap. If this goes wrong on release, I would rather have a hard-error than wrong data. (https://github.com/pola-rs/polars/pull/21812#discussion_r2002741431)",
        "make this only panic in debug builds, and in release we clear it\r\n",
        "drive-by - I noticed we currently silently ignore `slice()` after aggregations - I've made this raise instead.\r\n"
      ],
      "polars-prevent-cryptic-errors": [
        "This should check for an exact instance, otherwise we get cryptic error messages `has no attribute _p`.\r\n"
      ],
      "polars-database-api-abstraction": [
        "@ion-elgreco , can you check this one? I get an error writing empty tables on deltalake 0.21.0\r\n",
        "I see - thanks for the tip!\r\n"
      ],
      "polars-names-reveal-clear-intent": [
        "This was my mistake - `array_width` is the same as `size`. I would suggest removing this line and directly renaming it above - \r\n\r\n```rust\r\n- let ArrowDataType::FixedSizeList(_, size)\r\n+ let ArrowDataType::FixedSizeList(_, array_width)\r\n```\r\n",
        "This is not code from this PR, but `Cast` is a terrible name for a trait - it should be renamed to something more descriptive e.g. `FromBytes`. We should also add a docstring explaining that this is a utility trait to allow accessing `from_(le/be)_bytes`.\r\n",
        "I can update it. Do you also want it changed on the Python API, or just on the Rust-side functions?\r\n\r\nThe \"schema\" word is also used in multiple places (e.g. `struct SchemaInfo`, or `schema_info: &str` in function arguments) - if we want to be consistent we would need to rename all of those places.\r\n"
      ],
      "polars-evaluate-algorithmic-complexity-tradeoffs": [
        "We should use `tot_eq` from the `TotalEq` trait for comparisons - then we won't need the extra branch above for NaNs as well.\r\n"
      ],
      "polars-hide-implementation-details": [
        "```suggestion\r\n        target: str | Table,\r\n```\r\n\r\nThe `IcebergDataset` class we have right now was designed for internal use, I'd rather avoid exposing it to users if possible.\r\n"
      ],
      "polars-prevent-deadlock-conditions": [
        "If the consume token is sent into the linearizer, we can end up in a deadlock state for a `scan->sink` query  - given 2 workers from the source node (1 and 2):\r\n\r\n* Worker 1 inserts the consume token into the linearizer buffer\r\n* The linearizer is waiting for an insertion from worker 2\r\n* Worker 2 is waiting for the channel of the next source phase\r\n* Worker 1 is waiting for the consume token to be dropped while holding a ref to the current source phase, preventing it from ending",
        "Wrapping this `join_task` `AbortOnDropHandle` here doesn't cancel all of the previous tasks - the handles for those tasks should be individually wrapped in `AbortOnDropHandle` as soon as they are created.\r\n"
      ]
    }
  },
  "Darksonn": {
    "repos": [
      "tokio-rs/tokio"
    ],
    "entries": [
      {
        "slug": "tokio-clear-command-documentation",
        "title": "Clear command documentation"
      },
      {
        "slug": "tokio-code-block-formatting-standards",
        "title": "Code block formatting standards"
      },
      {
        "slug": "tokio-design-error-handling-carefully",
        "title": "Design error handling carefully"
      },
      {
        "slug": "tokio-design-flexible-apis",
        "title": "Design flexible APIs"
      },
      {
        "slug": "tokio-document-null-safety-assumptions",
        "title": "Document null safety assumptions"
      },
      {
        "slug": "tokio-fast-deterministic-tests",
        "title": "Fast deterministic tests"
      },
      {
        "slug": "tokio-flexible-consistent-api-patterns",
        "title": "Flexible consistent API patterns"
      },
      {
        "slug": "tokio-follow-import-style",
        "title": "Follow import style"
      },
      {
        "slug": "tokio-follow-naming-conventions",
        "title": "Follow naming conventions"
      },
      {
        "slug": "tokio-graceful-error-handling",
        "title": "Graceful error handling"
      },
      {
        "slug": "tokio-granular-feature-flags",
        "title": "Granular feature flags"
      },
      {
        "slug": "tokio-memory-ordering-needs-justification",
        "title": "Memory ordering needs justification"
      },
      {
        "slug": "tokio-minimize-unsafe-code",
        "title": "Minimize unsafe code"
      },
      {
        "slug": "tokio-network-api-design-consistency",
        "title": "Network API design consistency"
      },
      {
        "slug": "tokio-optimize-algorithmic-complexity",
        "title": "Optimize algorithmic complexity"
      },
      {
        "slug": "tokio-optimize-algorithmic-efficiency",
        "title": "Optimize algorithmic efficiency"
      },
      {
        "slug": "tokio-optimize-ci-job-structure",
        "title": "Optimize CI job structure"
      },
      {
        "slug": "tokio-optimize-hot-paths",
        "title": "Optimize hot paths"
      },
      {
        "slug": "tokio-optimize-job-structure",
        "title": "Optimize job structure"
      },
      {
        "slug": "tokio-optimize-memory-allocation",
        "title": "Optimize memory allocation"
      },
      {
        "slug": "tokio-organize-code-logically",
        "title": "Organize code logically"
      },
      {
        "slug": "tokio-prefer-explicit-over-concise",
        "title": "Prefer explicit over concise"
      },
      {
        "slug": "tokio-release-locks-before-waking",
        "title": "Release locks before waking"
      },
      {
        "slug": "tokio-secure-unsafe-code",
        "title": "Secure unsafe code"
      },
      {
        "slug": "tokio-simplify-configuration-flags",
        "title": "Simplify configuration flags"
      },
      {
        "slug": "tokio-socket-configuration-guidance",
        "title": "Socket configuration guidance"
      },
      {
        "slug": "tokio-structural-configuration-approaches",
        "title": "Structural configuration approaches"
      },
      {
        "slug": "tokio-structure-api-doc-blocks",
        "title": "Structure API doc blocks"
      },
      {
        "slug": "tokio-structure-conditional-compilation",
        "title": "Structure conditional compilation"
      },
      {
        "slug": "tokio-structure-feature-flags-strategically",
        "title": "Structure feature flags strategically"
      },
      {
        "slug": "tokio-test-diverse-configurations",
        "title": "Test diverse configurations"
      },
      {
        "slug": "tokio-test-production-configurations-too",
        "title": "Test production configurations too"
      },
      {
        "slug": "tokio-use-option-methods-idiomatically",
        "title": "Use Option methods idiomatically"
      },
      {
        "slug": "tokio-write-focused-single-purpose-tests",
        "title": "Write focused single-purpose tests"
      }
    ],
    "comments": {
      "tokio-optimize-hot-paths": [
        "```suggestion\r\n    #[doc(hidden)]\r\n    #[inline]\r\n    pub fn has_budget_remaining() -> bool {\r\n```"
      ],
      "tokio-optimize-memory-allocation": [
        "The intent of this `unwrap_unchecked()` is to access the memory for free, but `get_mut` has a bunch of logic to lock the weak count and so on. If we actually want to access the memory without checks, we should go through `into_raw`.",
        "I don't think we should clear the buffer. That's up to the user. I think it's better to support appending to the vector if the user wants that.",
        "There are two options that make sense to me:\r\n\r\n * Add a third argument that specifies the maximum number of elements to add to the vector. In this case, the capacity is irrelevant, and we will resize the buffer if necessary.\r\n * Or use the capacity like this:\r\n\r\n```rs\r\nif buffer.len() == buffer.capacity() {\r\n    buffer.reserve(super::BLOCK_CAP);\r\n}\r\nlet max_number_added = buffer.capacity() - buffer.len();\r\n```\r\n\r\nIn either case, I prefer that we do not clear the buffer.",
        "I was looking at this paragraph, and I think it could be improved like this:\r\n```suggestion\r\n    /// If `buffer` has unused capacity, then this call will not reserve\r\n    /// additional space in `buffer`. This means that the maximum number of\r\n    /// received messages is `buffer.capacity() - buffer.len()`. However, if\r\n    /// the capacity is equal to the length, then this call will increase the\r\n    /// capacity to make space for additional elements.\r\n```\r\nThis actually raises a question: Perhaps it makes sense to only reserve space when we return a message? This way, we don't consume extra memory until a message arrives, and if the channel gets closed, we don't reserve any space.\r\n\r\nWhat do you think?",
        "How does this change the size of `Sleep`? Could `TimerEntry` be changed to reduce the change?",
        "What happens if you just don't call `clear_entry` in [this method](https://github.com/tokio-rs/tokio/blob/ef657d23fd0f73bc73d3cc872feaceb0f8bf36b7/tokio/src/runtime/time/entry.rs#L503-L527) when it hasn't yet been registered?\r\n\r\nBased on @conradludgate's comment [here](https://github.com/tokio-rs/tokio/issues/6504#issuecomment-2073343288), it sounds like this triggers a loom failure, but that's most likely due to some path where the timer is dropped concurrently with firing or something like that. If we've never been registered with the driver, then not calling `clear_entry` should be okay."
      ],
      "tokio-simplify-configuration-flags": [
        "Could we just call it `--cfg tokio_uring`? We can have it require you to also pass `--cfg tokio_unstable`.",
        "Sorry, I meant how about we rename the flag to just `--cfg tokio_uring` instead of `--cfg tokio_unstable_uring`?",
        "Done."
      ],
      "tokio-code-block-formatting-standards": [
        "I'm happy to accept a PR that adds `--locked` to the command."
      ],
      "tokio-optimize-ci-job-structure": [
        "Please don't install multiple Rust versions in a single CI run. It causes confusion about which version of Rust is actually being used in each call. Instead, create a new CI run for this check.",
        "The miri job is starting to take a rather long time. Could you add a new job for this, instead of adding additional work to the existing job? This will allow for parallelism.",
        "That's nice. The loom jobs are already much longer than that. We could consider gating it similarly to what we did for loom tests."
      ],
      "tokio-design-error-handling-carefully": [
        "I think we probably want to just fall back to existing behavior if `stream_position` fails.\r\n```suggestion\r\n        let pos = std.stream_position().unwrap_or(0);\r\n```",
        "You could implement the `source` method here.",
        "I don't think we want two separate kinds of errors here. With `OnceCell` initialization can fail and can take a long time, so it's important to consider it to be a separate state, but with `SetOnce` the initialization state is always very short-lived and infallible, so I don't think we want to expose it to the end-user.",
        "Let's not ignore errors.\r\n```suggestion\r\n///     compress_data(reader).await?;\r\n```",
        "I like to avoid this error type in examples because it is not `Send` which means it only works in the `block_on` task.",
        "Avoiding unwrap is fine, I just prefer a different error type. E.g., it could be `Box<dyn Error+Send+Sync>`."
      ],
      "tokio-write-focused-single-purpose-tests": [
        "It is easier for me to understand the tests if you split them into many small tests rather than one or two large ones. For example, these small tests could be useful:\r\n\r\n1. `is_closed` should return true after calling `close` but still has a sender\r\n2. `is_closed` should return true after dropping all senders\r\n3. `is_closed` should return true after dropping all senders except for a weak sender\r\n4. `is_closed` should return false when there is a sender\r\n5. `is_closed` should return false when there is a sender, even if enough messages have been sent to fill the channel\r\n6. `is_closed` should return false when there is a permit (but no senders)",
        "Thanks!\r\n\r\nThat could make sense. After all, they have different implementations of the semaphore, so we aren't *just* testing the same code twice.",
        "(No need to put everything you test related to thread ids in a single test. You can make more than one.)",
        "We like to avoid sleeps in tests to reduce the amount of time it takes to run the tests. If you make the runtime use mocked time via the `start_paused` parameter, then this test will run instantly no matter what the duration is.\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/10e23d1c621ab38aadf2cefba1120494cff615f0/tokio/src/runtime/task/join.rs#L190-L205",
        "This will make the test run much much faster by using [simulated time](https://docs.rs/tokio/latest/tokio/time/fn.pause.html).\r\n```suggestion\r\n#[tokio::test(start_paused = true)]\r\n```",
        "We do not use sleeping in tests. We have 845 tests right now. Imagine how long it would take to run the tests if every single one had a 0.5 second sleep.",
        "Please include some tests for edge cases similar to the existing `empty_join` test at the bottom and the four `sync_*` tests at the top.",
        "Please add a test that uses this with `Vec<u8>` and `BytesMut` too.",
        "I'm concerned that we might need to supply the number of bytes as a separate parameter. I recall that resizable buffers are considered infinitely large by `BufMut`."
      ],
      "tokio-structure-api-doc-blocks": [
        "Markdown docs generally start with one really short summary, followed by more text.\r\n```suggestion\r\n    /// Tries to convert a `WeakSender` into a [`Sender`]. \r\n    ///\r\n    /// This will return `Some`\r\n    /// if there are other `Sender` instances alive and the channel wasn't\r\n    /// previously dropped, otherwise `None` is returned.\r\n```\r\n\\+ reflow to line length",
        "We generally have empty lines between headers, text, and code blocks.\r\n```suggestion\r\n//! # Example encoding using `LinesCodec`\r\n//!\r\n//! The following example demonstrates how to use a codec such as [`LinesCodec`] to\r\n//! write a sink of framed data. [`FramedWrite`] can be used to achieve this. Data sent\r\n//! to [`FramedWrite`] are first framed according to a specific codec, and then sent to\r\n//! an implementor of [`AsyncWrite`].\r\n//!\r\n//! ```\r\n```",
        "This looks a bit weird with the sentence starting out of nowhere. Also, it's best to have extra information after a line break as that renders more nicely in the html docs.\r\n\r\n```suggestion\r\n    /// Returns the line number for where this symbol is currently executing.\r\n    ///\r\n    /// If debuginfo is missing, this is likely to return None.\r\n```",
        "This isn't publicly visible, so it's not a big deal here, but generally the convention is that documentation should start with one *short* line. If more explanation is needed, it can go in a separate paragraph. This is because when structs are shown in documentation, the first line is used as a summary in some cases, and the summary should be short."
      ],
      "tokio-test-production-configurations-too": [
        "That is fine. We may want to consider whether it makes sense to run with both?"
      ],
      "tokio-prefer-explicit-over-concise": [
        "If we're going to define a new type for this, then we should probably call it `AsyncFdTryNewError`.\r\n\r\nBut I would also be okay with `(T, io::Error)`.",
        "This name is okay, but I think there is precedent for the name `AbortHandle` elsewhere, so we might want to prefer that name.",
        "Actually, we have a type called `AbortHandle` in Tokio, and that does something else ... so that name doesn't work.\r\n\r\nBut another option is `AbortOnDropHandle`. I'll let you pick.",
        "I agree that `sender_strong_count` and `sender_weak_count` would be better. Otherwise, this PR looks good to me.",
        "Why not? I can't immediately see what would break if we just change `get` to `T: Clone`.",
        "This seems to compile locally:\r\n```diff\r\ndiff --git a/tokio/src/task/task_local.rs b/tokio/src/task/task_local.rs\r\nindex ba58ea6a..cb9d22c6 100644\r\n--- a/tokio/src/task/task_local.rs\r\n+++ b/tokio/src/task/task_local.rs\r\n@@ -264,16 +264,16 @@ impl<T: 'static> LocalKey<T> {\r\n     }\r\n }\r\n \r\n-impl<T: Copy + 'static> LocalKey<T> {\r\n+impl<T: Clone + 'static> LocalKey<T> {\r\n     /// Returns a copy of the task-local value\r\n-    /// if the task-local value implements `Copy`.\r\n+    /// if the task-local value implements `Clone`.\r\n     ///\r\n     /// # Panics\r\n     ///\r\n     /// This function will panic if the task local doesn't have a value set.\r\n     #[track_caller]\r\n     pub fn get(&'static self) -> T {\r\n-        self.with(|v| *v)\r\n+        self.with(|v| v.clone())\r\n     }\r\n }\r\n```",
        "Is it? If so, then you changed it before I got a chance to see it, because the thing I saw originally added a new method called `clone` instead of changing the existing `get` method.\r\n\r\nEither way, I have made up my mind. I would like to change `get` instead of introducing a new method.",
        "I'm sorry for the confusion. First, I am on the Tokio core team and am authoritative in this case (but it doesn't seem like @mox692 and I disagree?). As for your other questions:\r\n\r\n> Why remove get for Copy inner types?\r\n\r\nI am not suggesting that you remove `get` for `Copy` types. All `Copy` types are also `Clone` (it's a subtrait), so you will still be able to use `get` with any `Copy` type after changing `T: Copy` to `T: Clone`.\r\n\r\n> Why not just add a similar impl for `Clone`?\r\n\r\nWe could also do that, but then `Copy` types have two identical methods that do the same thing. To me, it seems simpler dev UX to have a single method that works for both `Copy` and `Clone`. That's why I suggested it.\r\n\r\n> I couldn't just extend on `get` for `Copy + Clone` simply because my type isn't `Copy` (it's an `Arc<_>`).\r\n\r\nIf you make `get` require `T: Clone` (rather than `T: Copy + Clone`), then it also works for `Arc<_>`."
      ],
      "tokio-structural-configuration-approaches": [
        "My main feedback is also this. It may make sense to merge this into a struct with the id, or to wrap the location in something that can be zero-sized on stable, or to otherwise avoid conditional compilation everywhere this information is passed around.",
        "Does this need to be gated on `cfg_rt!`? Is the file not already gated on that?",
        "Can you add an `#[cfg_attr(not(feature = \"rt\"), allow(dead_code))]` on the module instead? The `cfg_rt!` blocks break rustfmt, so I try to avoid wrapping large blocks in them.",
        "Or maybe it'd be better to split the file into two and wrap the `mod` statement in `cfg_rt!`."
      ],
      "tokio-network-api-design-consistency": [
        "This will incorrectly return `false` if stderr is a terminal but there is an ongoing IO operation. Can we instead do this?\r\n```suggestion\r\n    /// Returns true if the descriptor/handle refers to a terminal/tty.\r\n    pub fn is_terminal(&self) -> bool {\r\n        std::io::stderr().is_terminal()\r\n```",
        "Shouldn't this also take an interest? Otherwise we will eventually be asked to add an `with_epoll_flags_and_interest`.",
        "Please add to the documentation that `EPOLLONESHOT` must not be used, and that `EPOLLET` must be set. And please add debug asserts for this.",
        "Is Linux the only OS that needs this fix? Does macOS not have abstract path names? It would be worth to look at how mio did this prior to the v1 release.",
        "Ah, ok. Does `target_os = \"linux\"` also include Android? I see that it's also available there.",
        "Why the casts?",
        "Thanks."
      ],
      "tokio-fast-deterministic-tests": [
        "We like to avoid sleeps in tests to reduce the amount of time it takes to run the tests. If you make the runtime use mocked time via the `start_paused` parameter, then this test will run instantly no matter what the duration is.\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/10e23d1c621ab38aadf2cefba1120494cff615f0/tokio/src/runtime/task/join.rs#L190-L205",
        "This will make the test run much much faster by using [simulated time](https://docs.rs/tokio/latest/tokio/time/fn.pause.html).\r\n```suggestion\r\n#[tokio::test(start_paused = true)]\r\n```",
        "We do not use sleeping in tests. We have 845 tests right now. Imagine how long it would take to run the tests if every single one had a 0.5 second sleep.",
        "Maybe this timeout should be larger?",
        "We expect tests to run with `cargo test` too, so we shouldn't use nextest specific features. Having a timeout sgtm."
      ],
      "tokio-socket-configuration-guidance": [
        "Can we improve this wording? I don't think it very clearly explains the situation. You could say something along the lines of \"Passing a listener in blocking mode is always errornous, and the behavior in that case may change in the future. For example, it could panic.\"",
        "Shouldn't this also take an interest? Otherwise we will eventually be asked to add an `with_epoll_flags_and_interest`.",
        "Please add to the documentation that `EPOLLONESHOT` must not be used, and that `EPOLLET` must be set. And please add debug asserts for this.",
        "Is Linux the only OS that needs this fix? Does macOS not have abstract path names? It would be worth to look at how mio did this prior to the v1 release.",
        "Ah, ok. Does `target_os = \"linux\"` also include Android? I see that it's also available there.",
        "Why the casts?",
        "Thanks."
      ],
      "tokio-structure-conditional-compilation": [
        "My main feedback is also this. It may make sense to merge this into a struct with the id, or to wrap the location in something that can be zero-sized on stable, or to otherwise avoid conditional compilation everywhere this information is passed around."
      ],
      "tokio-follow-naming-conventions": [
        "We shouldn't add new `self` methods to types that implement `Deref`.\r\n```suggestion\r\n    pub fn rwlock(this: &Self) -> &Arc<RwLock<T>> {\r\n```",
        "This name is okay, but I think there is precedent for the name `AbortHandle` elsewhere, so we might want to prefer that name.",
        "Actually, we have a type called `AbortHandle` in Tokio, and that does something else ... so that name doesn't work.\r\n\r\nBut another option is `AbortOnDropHandle`. I'll let you pick.",
        "According to the API guidelines, this should be called `as_socket`. It's not an expensive conversion.\r\n```suggestion\r\n    fn as_socket(&self) -> socket2::SockRef<'_> {\r\n        socket2::SockRef::from(self)\r\n    }\r\n```",
        "Why not? I can't immediately see what would break if we just change `get` to `T: Clone`.",
        "This seems to compile locally:\r\n```diff\r\ndiff --git a/tokio/src/task/task_local.rs b/tokio/src/task/task_local.rs\r\nindex ba58ea6a..cb9d22c6 100644\r\n--- a/tokio/src/task/task_local.rs\r\n+++ b/tokio/src/task/task_local.rs\r\n@@ -264,16 +264,16 @@ impl<T: 'static> LocalKey<T> {\r\n     }\r\n }\r\n \r\n-impl<T: Copy + 'static> LocalKey<T> {\r\n+impl<T: Clone + 'static> LocalKey<T> {\r\n     /// Returns a copy of the task-local value\r\n-    /// if the task-local value implements `Copy`.\r\n+    /// if the task-local value implements `Clone`.\r\n     ///\r\n     /// # Panics\r\n     ///\r\n     /// This function will panic if the task local doesn't have a value set.\r\n     #[track_caller]\r\n     pub fn get(&'static self) -> T {\r\n-        self.with(|v| *v)\r\n+        self.with(|v| v.clone())\r\n     }\r\n }\r\n```",
        "Is it? If so, then you changed it before I got a chance to see it, because the thing I saw originally added a new method called `clone` instead of changing the existing `get` method.\r\n\r\nEither way, I have made up my mind. I would like to change `get` instead of introducing a new method.",
        "I'm sorry for the confusion. First, I am on the Tokio core team and am authoritative in this case (but it doesn't seem like @mox692 and I disagree?). As for your other questions:\r\n\r\n> Why remove get for Copy inner types?\r\n\r\nI am not suggesting that you remove `get` for `Copy` types. All `Copy` types are also `Clone` (it's a subtrait), so you will still be able to use `get` with any `Copy` type after changing `T: Copy` to `T: Clone`.\r\n\r\n> Why not just add a similar impl for `Clone`?\r\n\r\nWe could also do that, but then `Copy` types have two identical methods that do the same thing. To me, it seems simpler dev UX to have a single method that works for both `Copy` and `Clone`. That's why I suggested it.\r\n\r\n> I couldn't just extend on `get` for `Copy + Clone` simply because my type isn't `Copy` (it's an `Arc<_>`).\r\n\r\nIf you make `get` require `T: Clone` (rather than `T: Copy + Clone`), then it also works for `Arc<_>`.",
        "I agree that `sender_strong_count` and `sender_weak_count` would be better. Otherwise, this PR looks good to me."
      ],
      "tokio-optimize-algorithmic-complexity": [
        "This iterates all of the buffers every time, even if we only write a few of them. If the buffers are very long and this is called in a loop, that gives quadratic performance.\r\n\r\nWe should be able to embed this logic inside the for loop instead to avoid that.",
        "No, `b.len()` is constant time. Instead, it's O(n) in the length of `bufs`, which you iterate over.",
        "I would expect that `saturating_pow` is rather expensive compared to a shift.\r\n```suggestion\r\n        let max_number = match 1.checked_shl(8 * self.length_field_len) {\r\n            Some(shl) => shl - 1,\r\n            None => u64::MAX,\r\n        };\r\n```",
        "If we're going to the effort of providing an implementation, then I think we should not go for a version that just calls `poll_next_entry` in a loop. At that point, it might as well be a generic piece of functionality that works for any `Stream`.\r\n\r\nEach call to that method generates a random number to pick a stream to start at, but I think it would make more sense to pick a random number only once, and then keep going around the loop until we have `limit` items, or until we've polled `self.entries.len()` times in a row without getting an item.",
        "Hmm. I imagine that when we get many items, we would want to get them from different streams (probably in the order they're stored). But with this strategy, whichever stream is at `start` will be preferred every time.",
        "Easiest is probably to either return the next index from `poll_one`, or to copy its implementation into `poll_next_many`.",
        "Seems simpler to just keep track of `added` from the beginning and change the while loop to `while added < limit`.",
        "Could something like this make more sense?\r\n```\r\nwheels_lock.0.iter_mut()\r\n    .filter_map(|wheel| wheel.get_mut().next_expiration_time())\r\n    .min();\r\n```\r\nThis way, we don't need to touch indexes at all.",
        "I would like to avoid having this variable at all when using biased. I don't think it can be optimized out because it gets stored as a field in the closure type.",
        "Yeah, I don't think we want the match. It duplicates the entire logic, which isn't nice. One option could be to define two helper types:\r\n```rs\r\n#[derive(Default)]\r\npub struct Rotater<const COUNT: usize> {\r\n    next: usize,\r\n}\r\n\r\nimpl<const COUNT: usize> Rotater<COUNT> {\r\n    #[inline]\r\n    pub fn num_skip(&mut self) -> usize {\r\n        let mut num_skip = self.next;\r\n        self.next += 1;\r\n        if self.next == COUNT {\r\n            self.next = 0;\r\n        }\r\n        num_skip\r\n    }\r\n}\r\n\r\n#[derive(Default)]\r\npub struct BiasedRotater {}\r\n\r\nimpl BiasedRotater {\r\n    #[inline]\r\n    pub fn num_skip(&mut self) -> usize {\r\n        0\r\n    }\r\n}\r\n```\r\nand then have the macro select which type to use:\r\n```text\r\n( biased; $($e:expr),+ $(,)?) => {\r\n    $crate::join!(@{ rotator=$crate::macros::support::BiasedRotator; () (0) } $($e,)*)\r\n};\r\n\r\n( $($e:expr),+ $(,)?) => {\r\n    $crate::join!(@{ rotator=$crate::macros::support::Rotator; () (0) } $($e,)*)\r\n};\r\n```"
      ],
      "tokio-clear-command-documentation": [
        "I'm happy to accept a PR that adds `--locked` to the command."
      ],
      "tokio-follow-import-style": [
        "Safety comments must be right before the unsafe block. Please move `let filled` before it.",
        "Which lint is this?",
        "I think this lint does not improve the code. Please add `#![allow(clippy::needless_lifetimes)]` to the top of the `lib.rs` files instead to silence it.",
        "We don't use this import style. Please split this into three `use` statements.",
        "In Tokio we usually split up imports from different modules like this:\r\n```suggestion\r\nuse std::future::Future;\r\nuse std::time::Duration;\r\n```\r\nThe brackets are only used for items in the same module.",
        "Tokio uses this convention for imports:\r\n```suggestion\r\n    os::fd::AsFd,\r\n    os::unix::io::{AsRawFd, RawFd},\r\n```"
      ],
      "tokio-document-null-safety-assumptions": [
        "This needs a safety comment to explain that this is okay because the caller guarantees that the written portion is reported correctly.\r\n\r\nAlso, it'd be nice to note that this is only correct because `self.buf.len() == 0` as asserted above.",
        "Can you add a safety comment?\r\n```\r\n// SAFETY: The memory may be uninitialized, but `rd.read` will only write to the buffer.\r\n```",
        "This unsafe block needs a safety comment. It should explain that the resulting pointer is in-bounds (or one after the end) of the `self.wakers` array.",
        "It seems like there's an extra call to `.cast()` here?",
        "Yes, using several `let` statements is good. It makes it clear what the types are."
      ],
      "tokio-test-diverse-configurations": [
        "That is fine. We may want to consider whether it makes sense to run with both?"
      ],
      "tokio-organize-code-logically": [
        "Please put them in a sub-module so they don't clutter the front page of the crate documentation.",
        "Defining an inline module like you suggested in the beginning is fine. (But the other approaches would not be considered breaking.)",
        "No reason to expose a module with one item.\r\n```suggestion\r\nmod abort_on_drop;\r\npub use abort_on_drop::AbortOnDropHandle;\r\n```",
        "Please split this into a separate file. Rustfmt doesn't work inside the macros, so I don't want large codeblocks inside them."
      ],
      "tokio-graceful-error-handling": [
        "I think we probably want to just fall back to existing behavior if `stream_position` fails.\r\n```suggestion\r\n        let pos = std.stream_position().unwrap_or(0);\r\n```",
        "Using a debug assertion with a `--cfg` to disable it seems preferable to the `eprintln!`.",
        "You could implement the `source` method here.",
        "Let's not ignore errors.\r\n```suggestion\r\n///     compress_data(reader).await?;\r\n```",
        "Can you add `#[track_caller]` and a [test for the panic location](https://github.com/tokio-rs/tokio/blob/master/tokio/tests/rt_panic.rs)?"
      ],
      "tokio-flexible-consistent-api-patterns": [
        "I think that the `poll_proceed` / `made_progress` API we use internally is good. If we're going to expose more of coop, then I think we should expose that API instead of inventing a new one.\r\n\r\nBut I wouldn't do both \"expose coop API\" and \"make select coop aware\" in one PR. I'd like two PRs for the changelog.",
        "This provides a single object that is both the reader and writer, but in practice I think people will want those to be two different objects.",
        "I don't like this name. It sounds like something that would immediately make something abort.\r\n\r\nHow about `spawn_with_drop_handle` or `spawn_with_abort_handle`?",
        "Another possibility is to make this a constructor method. Then you would type `DropHandle::spawn`.",
        "If all of the `*acquire_many` methods already allow zero permits, then I see no harm in allowing it here. I would perhaps all the method `num_permits`?"
      ],
      "tokio-structure-feature-flags-strategically": [
        "Some of these look redundant. Don't we always have libc on linux?",
        "Ok, that's fine.",
        "Hmm. I'll have to think about what implications this has for our MSRV.",
        "I'm okay with this, but I don't want to require hashbrown if you're just using the `TaskTracker`. Can you add a `join_map` feature?",
        "Would this be sufficient?\r\n```suggestion\r\njoin-map = [\"tokio/rt\", \"hashbrown\"]\r\n```",
        "We can always mark `mod task` with `#[cfg(any(rt, join_map))]`.",
        "We usually do it like this:\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/32970527633bb72fc4f01d02523484a9376ac26a/tokio/src/lib.rs#L1\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/32970527633bb72fc4f01d02523484a9376ac26a/.github/workflows/ci.yml#L1050-L1065\r\n\r\nIt may be worth to switch, but for now I would add a `#[allow(unexpected_cfgs)]` in the source code."
      ],
      "tokio-optimize-job-structure": [
        "Please don't install multiple Rust versions in a single CI run. It causes confusion about which version of Rust is actually being used in each call. Instead, create a new CI run for this check.",
        "The miri job is starting to take a rather long time. Could you add a new job for this, instead of adding additional work to the existing job? This will allow for parallelism.",
        "That's nice. The loom jobs are already much longer than that. We could consider gating it similarly to what we did for loom tests."
      ],
      "tokio-optimize-algorithmic-efficiency": [
        "This iterates all of the buffers every time, even if we only write a few of them. If the buffers are very long and this is called in a loop, that gives quadratic performance.\r\n\r\nWe should be able to embed this logic inside the for loop instead to avoid that.",
        "No, `b.len()` is constant time. Instead, it's O(n) in the length of `bufs`, which you iterate over.",
        "I would expect that `saturating_pow` is rather expensive compared to a shift.\r\n```suggestion\r\n        let max_number = match 1.checked_shl(8 * self.length_field_len) {\r\n            Some(shl) => shl - 1,\r\n            None => u64::MAX,\r\n        };\r\n```",
        "I think the better way to check this is to use this:\r\n```rs\r\nnow.saturating_duration_since(timeout) > Duration::from_millis(5)\r\n```"
      ],
      "tokio-granular-feature-flags": [
        "Hmm. I'll have to think about what implications this has for our MSRV.",
        "I'm okay with this, but I don't want to require hashbrown if you're just using the `TaskTracker`. Can you add a `join_map` feature?",
        "Would this be sufficient?\r\n```suggestion\r\njoin-map = [\"tokio/rt\", \"hashbrown\"]\r\n```",
        "We can always mark `mod task` with `#[cfg(any(rt, join_map))]`.",
        "Let me put that in its own PR so it gets a changelog entry. #6541"
      ],
      "tokio-memory-ordering-needs-justification": [
        "Wait, does this PR not eliminate all usages of SeqCst?",
        "I don't know how your detector works, but mixing SeqCst and non-SeqCst orderings is never the right answer. Never.\r\n\r\nLooking over the code, I notice that there are atomics in both the `Notify`, but also the waiters. Before I can accept a PR relaxing any of them, I'm going to have to review the code for whether it currently relies on the SeqCst guarantees to synchronize between those different atomics. ",
        "Here you have exclusive access, so just perform a normal write."
      ],
      "tokio-secure-unsafe-code": [
        "Please include a short safety comment on why each unsafe block is okay.",
        "We need to also mark `Blocking::new` as unsafe and require that `self.inner` satisfies the requirement. Otherwise this unsafe block isn't really correct.",
        "Please reduce the scope of this unsafe block. You only need it for the `&mut *tail` operation as far as I can tell.\r\n```Rust\r\nlet tail_block = unsafe { &mut *tail };\r\n```",
        "Generally, it is preferred to have a separate unsafe block for each unsafe operation, and to annotate each block with a `// SAFETY:` comment justifying its correctness."
      ],
      "tokio-minimize-unsafe-code": [
        "Please include a short safety comment on why each unsafe block is okay.",
        "Please reduce the scope of this unsafe block. You only need it for the `&mut *tail` operation as far as I can tell.\r\n```Rust\r\nlet tail_block = unsafe { &mut *tail };\r\n```",
        "Generally, it is preferred to have a separate unsafe block for each unsafe operation, and to annotate each block with a `// SAFETY:` comment justifying its correctness."
      ],
      "tokio-release-locks-before-waking": [
        "Please add logic to `rt_multi_thread_available` to detect whether `--cfg tokio_unstable` is set and emit a hard error if it is not.",
        "The error should be similar to this error:\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/2506c9fa9916a1bdffbc762f7eb2ae5c2fd23836/tokio-macros/src/entry.rs#L193-L197",
        "I think both are okay.",
        "Doesn't that also trigger other unrelated errors when it fails?\r\n\r\nWhat we do today is essentially define the `main` macro two times to pass different values to [the `rt_multi_thread` boolean](https://github.com/tokio-rs/tokio/blob/master/tokio-macros/src/entry.rs#L488), and have `lib.rs` do this:\r\n```rs\r\n#[cfg(feature = \"rt-multi-thread\")]\r\npub use tokio_macros::main;\r\n#[cfg(not(feature = \"rt-multi-thread\"))]\r\npub use tokio_macros::main_rt as main;\r\n```\r\nThis way, the macro knows based on the cfgs that apply in the main Tokio crate. We could use the same approach and have four macros.",
        "I don't think that a shutdown future is an amazing example here. That makes sense for `select!`, but you wouldn't really have a shutdown future in `join!`. Not sure what a better example would be, though.",
        "Please make sure that the section names for `try_join!` match those we have for `join!`.",
        "```suggestion\r\n        /// place the shutdown future earlier in the `try_join!` list to ensure that it is\r\n```",
        "The current logic exists to reuse the thread parker logic when `block_on` is used many times. What is the reason for not reusing it?",
        "This is proposing to add them as stable despite the `LocalSet` question?",
        "The way we usually handle this kind of thing is to move _all_ entries in the list to another list (using the guarded linked list), and then we wake entries from the guarded list in batches. Can we not do the same here?",
        "I'm worried about panics here. What happens to entries still in the guarded list?\r\n\r\nI think some other examples of this wrap the guarded list in something that removes all entries from the list without waking their wakers."
      ],
      "tokio-design-flexible-apis": [
        "Case 2 cannot happen since `SetOnce` isn't a channel that you clone. The return value can be a bare `&T`.",
        "Yeah ... I'm not sure what is the least surprising here.",
        "If the intent of this PR is a way to detect whether polling the channel will panic, then having a function that does exactly that sounds good to me.",
        "If the limit is zero, then I think it would be okay to just return 0 immediately.",
        "Hmm. I think immediately returning 0 is the least surprising behavior. After all, you asked to receive zero messages, and you got zero messages.",
        "If what is acceptable?\r\n\r\nI mainly see people be confused about length zero reads when they do this:\r\n```rs\r\nlet mut vec = Vec::with_capacity(1024);\r\nio.read(&mut vec);\r\n```\r\nand are surprised because `&mut vec` becomes a slice of length zero.\r\n\r\nBut that doesn't happen in our case.",
        "I think allowing a zero length is useful for cases where the length is computed dynamically. I've seen code along these lines:\r\n```rs\r\nlet len = io.read_u64().await? as usize;\r\nlet mut buffer = Vec::with_capacity(len);\r\nio.take(len).read_to_end(&mut buffer)?;\r\n```\r\nif there are length-zero messages, then the above makes use of `take(0)` behavior sometimes.",
        "If all of the `*acquire_many` methods already allow zero permits, then I see no harm in allowing it here. I would perhaps all the method `num_permits`?"
      ],
      "tokio-use-option-methods-idiomatically": [
        "Can you change this to only create a mutable reference if it is `None`?",
        "Yes, thanks for the suggestion."
      ]
    }
  },
  "jaymzh": {
    "repos": [
      "chef/chef"
    ],
    "entries": [
      {
        "slug": "chef-choose-semantic-algorithms",
        "title": "Choose semantic algorithms"
      },
      {
        "slug": "chef-consistent-descriptive-naming-patterns",
        "title": "Consistent descriptive naming patterns"
      },
      {
        "slug": "chef-document-configuration-completely",
        "title": "Document configuration completely"
      },
      {
        "slug": "chef-document-configuration-decisions",
        "title": "Document configuration decisions"
      },
      {
        "slug": "chef-document-with-examples",
        "title": "Document with examples"
      },
      {
        "slug": "chef-explicit-configuration-over-implicit",
        "title": "Explicit configuration over implicit"
      },
      {
        "slug": "chef-extract-and-organize-methods",
        "title": "Extract and organize methods"
      },
      {
        "slug": "chef-fail-fast-principle",
        "title": "Fail fast principle"
      },
      {
        "slug": "chef-fail-gracefully-always",
        "title": "Fail gracefully always"
      },
      {
        "slug": "chef-memoize-expensive-operations",
        "title": "Memoize expensive operations"
      },
      {
        "slug": "chef-prefer-guard-clauses",
        "title": "Prefer guard clauses"
      },
      {
        "slug": "chef-secure-credential-management",
        "title": "Secure credential management"
      },
      {
        "slug": "chef-secure-network-operations",
        "title": "Secure network operations"
      }
    ],
    "comments": {
      "chef-extract-and-organize-methods": [
        "rather than repeat this line/logic over and over, please extract this to a method (and I\"m reversing the log here per my previous comment):\r\n\r\n```ruby\r\ndef resolved_package(pkg)\r\n  new_resource.anchor_package_regex ? \"^#{pkg}$\" : pkg\r\nend\r\n```"
      ],
      "chef-memoize-expensive-operations": [
        "This needs to be memoized. Probably something like:\r\n\r\n```ruby\r\ndef choco_ver\r\n  @choco_ver ||= begin\r\n    powershell_exec!(\"choco --version\").result[0].to_i\r\n  end\r\nend\r\n\r\ndef query_command\r\n  choco_version == 1 ? 'list' : 'search'\r\nend\r\n```",
        "@tas50 because calling losetup 93048234092834 times is bad. I asked for that."
      ],
      "chef-document-with-examples": [
        "maybe add some examples, `... (such as 'ssh' or 'winrm')`",
        "yes please. I thought there was a `:help` option to properties that was used for docs...",
        "```suggestion\r\n      In this example, `anchor_package_regex true` helps prevent that by adding `^` and `$` anchors around the package name. Example: `lua5.3` would be `^lua5.3$`.\r\n```",
        "That's insane. Things take regular expressions in Chef, how do you document that? Can we escape it? Regex.escape? There has to be some way to do that. We'll file a docs Issue here for that, lets get this solved so we can document our software.",
        "https://github.com/chef/chef/issues/13908",
        "This looks really nice. One thing that would be useful though would be to add a comment for each `when` that has an example line you're matching."
      ],
      "chef-choose-semantic-algorithms": [
        "This will break again when 1.5 comes out. We should be a bit more thorough here. Maybe something like...\r\n\r\n```suggestion\r\n          v_bits = get_choco_version.split('.')\r\n          # 1.4+ and 2+\r\n          if v_bits[0].to_i > 1 || v_bits[1] > 3\r\n            return \"search\"\r\n          end\r\n          \"list\"\r\n```",
        "Does it conform to gem versions? We have other version helpers if not. Gem versions are very strict.",
        "This only fixes... part of the bug.\r\n\r\nIt will ensure that if you try to purge `['a', 'b', 'c']` and `c` isn't a known package, that it won't try to purge `c`... which is good...\r\n\r\nBut the actual bug is that if `a` is installed, it'll automatically try to uninstall `b`, and `c` even if they are not installed.\r\n\r\nSo imagine:\r\n* a - installed\r\n* b - uninstalled, but available\r\n* c - uninstalled and unavailable\r\n\r\nCurrent behavior is Chef will try to purge `a`, `b` and `c`\r\n\r\nWith your PR it'll just try to purge `a` and `b`\r\n\r\nBut the *right* behavior is that it should only try to purge `a`."
      ],
      "chef-secure-credential-management": [
        "This used  to be (the equivalent of):\r\n```ruby\r\npassword_blob.each do |secret|\r\n  if secret[:name] == \"PfxPass\"\r\n    password = secret[:data]\r\n    break\r\n  end\r\nend\r\n```\r\n\r\nYour code here will, I think, overwrite `password` with non-PfxPass",
        "Shouldn't we want to use the same password here from when it was an encrypted string? Isn't that what the key is encrypted with? If we make a new one, we just break the users' ability to use their key? Perhaps I'm mis-reading something, but it feels like we should just properly encrypted the existing password."
      ],
      "chef-document-configuration-decisions": [
        "```suggestion\r\n    # back compat for pre-unified-/usr distros, do not add new OSes\r\n    - remote: sudo ln -s /usr/bin/install /bin/install\r\n```"
      ],
      "chef-explicit-configuration-over-implicit": [
        "We don't usually consult env vars from cookbooks.  It breaks idempotency... Two prior with different environments running this could move the install back and forth between different versions. I highly highly recommend against this."
      ],
      "chef-fail-gracefully-always": [
        "we still need to remove the local tmpfile, even if we're not in targetmode, just like we did before. "
      ],
      "chef-prefer-guard-clauses": [
        "You can just do\r\n```suggestion\r\n        if new_resource.download_url\r\n```\r\n\r\nRuby treats that was defined, not nil, not false.",
        "do you nee to call `.compact` here so you don't return a bunch of `nil`s?"
      ],
      "chef-secure-network-operations": [
        "This downloads it, but it doesn't run the ps1 that's downloaded... So it's not installed... I think."
      ],
      "chef-consistent-descriptive-naming-patterns": [
        "this _is_ cleanpath, so just call cleanpath.\r\n\r\nAnd you can add the ! there if you want."
      ],
      "chef-document-configuration-completely": [
        "```suggestion\r\nWhen making a PR, if you need to add or remove gems, it should be done using the `--conservative` flag:\r\n\r\n```shell\r\ngem install [gem_name] --conservative\r\ngem update [gem_name] --conservative`\r\n```\r\n\r\nIf a manual PR updates the `Gemfile.lock`, please include the full output of your `gem` command in the commit message an PR description.\r\n\r\n```"
      ],
      "chef-fail-fast-principle": [
        "I don't think this file is related to the backport?"
      ]
    }
  },
  "sapphi-red": {
    "repos": [
      "vitejs/vite"
    ],
    "entries": [
      {
        "slug": "vite-break-down-complex-functions",
        "title": "Break down complex functions"
      },
      {
        "slug": "vite-clean-configuration-organization",
        "title": "Clean configuration organization"
      },
      {
        "slug": "vite-clean-network-resources",
        "title": "Clean network resources"
      },
      {
        "slug": "vite-complete-deployment-commands",
        "title": "Complete deployment commands"
      },
      {
        "slug": "vite-descriptive-consistent-naming",
        "title": "Descriptive consistent naming"
      },
      {
        "slug": "vite-document-code-purposefully",
        "title": "Document code purposefully"
      },
      {
        "slug": "vite-document-protocol-configurations-clearly",
        "title": "Document protocol configurations clearly"
      },
      {
        "slug": "vite-ensure-documentation-accuracy",
        "title": "Ensure documentation accuracy"
      },
      {
        "slug": "vite-environment-variable-management",
        "title": "Environment variable management"
      },
      {
        "slug": "vite-escape-html-content-properly",
        "title": "Escape HTML content properly"
      },
      {
        "slug": "vite-evolve-apis-with-compatibility",
        "title": "Evolve APIs with compatibility"
      },
      {
        "slug": "vite-explicit-version-requirements",
        "title": "Explicit version requirements"
      },
      {
        "slug": "vite-manage-configuration-inheritance-carefully",
        "title": "Manage configuration inheritance carefully"
      },
      {
        "slug": "vite-minimize-memory-allocations",
        "title": "Minimize memory allocations"
      },
      {
        "slug": "vite-optimize-glob-operations",
        "title": "Optimize glob operations"
      },
      {
        "slug": "vite-permission-hierarchy-awareness",
        "title": "Permission hierarchy awareness"
      },
      {
        "slug": "vite-precise-documentation-language",
        "title": "Precise documentation language"
      },
      {
        "slug": "vite-propagate-errors-with-context",
        "title": "Propagate errors with context"
      },
      {
        "slug": "vite-react-transformation-tool-clarity",
        "title": "React transformation tool clarity"
      },
      {
        "slug": "vite-restrict-server-access",
        "title": "Restrict server access"
      },
      {
        "slug": "vite-runtime-agnostic-api-design",
        "title": "Runtime-agnostic API design"
      },
      {
        "slug": "vite-secure-workflow-permissions",
        "title": "Secure workflow permissions"
      },
      {
        "slug": "vite-separate-configuration-responsibilities",
        "title": "Separate configuration responsibilities"
      },
      {
        "slug": "vite-vue-component-import-handling",
        "title": "Vue component import handling"
      }
    ],
    "comments": {
      "vite-precise-documentation-language": [
        "I replaced \"files generated by create-vite\" with \"files generated from those files\" so that we don't say anything about files generated by the redirected CLIs (e.g. create-vue, create svelte)."
      ],
      "vite-separate-configuration-responsibilities": [
        "```suggestion\r\n      \"fileMatch\": [\"packages\\/create-vite\\/src\\/index\\\\.ts$\"],\r\n```",
        "Updated ðŸ‘ "
      ],
      "vite-minimize-memory-allocations": [
        "```suggestion\r\n                      size: Buffer.byteLength(chunk.code),\r\n```\r\n`Buffer.byteLength` should be performant than `Buffer.from().length` as it doesn't require the whole converted value to be held in the memory.\r\n",
        "Good catch! Instead of doing that, I made the `deepClone` to be call only once before merging with defaults (ffb92fdf743026c98edd8830e0868878db36f060, 2d6bc9cc73f15e4b7e551af649bc69c3265e4e0d). This way, the properties of defaults that would be overridden with the values will be cloned unnecessarily compared to your suggestion, but I think that's negligible.\r\n\r\n\r\n\r\n",
        "It was mainly for `RegExp`. I put the `structuredClone` in a condition ðŸ‘ (38162a0a1603da5de06d683d97a1b175289caeeb, e4364e59fbd760c2f7f6e63a2cc137720cd2690d)."
      ],
      "vite-react-transformation-tool-clarity": [
        "```suggestion\r\n### `@vitejs/plugin-react-oxc`\r\n\r\nWhen using `@vitejs/plugin-react` or `@vitejs/plugin-react-swc`, you can switch to the `@vitejs/plugin-react-oxc` plugin, which uses Oxc for JSX/TSX transformation instead of esbuild. It is designed to be a drop-in replacement, providing better build performance and aligning with the underlying architecture of `rolldown-vite`.\r\n\r\nBe aware that you can only switch to `@vitejs/plugin-react-oxc` if you are not using any Babel or SWC plugins (including the React compiler), or mutate the SWC options.\r\n```",
        "> which uses Oxc for JSX/TSX transformation instead of esbuild.\r\n\r\nThis isn't correct. These two plugins use transformers as shown in the table below. The difference is what handles the react-refresh transformation.\r\n\r\n- plugin-react (without plugins)\r\n  - dev (rollup-vite): uses babel for react-refresh transform + uses esbuild for JSX/TSX transform\r\n  - dev (rolldown-vite): uses babel for react-refresh transform + uses Oxc for JSX/TSX transform\r\n  - build (rollup-vite): uses esbuild for JSX/TSX transform\r\n  - build (rolldown-vite): uses Oxc for JSX/TSX transform\r\n- plugin-react-swc (without plugins)\r\n  - dev (rollup-vite): uses SWC for react-refresh and JSX/TSX transform\r\n  - dev (rolldown-vite): uses SWC for react-refresh and JSX/TSX transform\r\n  - build (rollup-vite): uses esbuild for JSX/TSX transform\r\n  - build (rolldown-vite): uses Oxc for JSX/TSX transform\r\n- plugin-react-oxc\r\n  - dev (rolldown-vite): uses Oxc for react-refresh and JSX/TSX transform\r\n  - build (rolldown-vite): uses Oxc for JSX/TSX transform"
      ],
      "vite-document-code-purposefully": [
        "```suggestion\r\n  /** @internal */\r\n  hostname: Hostname\r\n```\r\nI'd like to mark this as a internal type.",
        "```suggestion\r\n              // only limit to these extensions because:\r\n              // - for the `@import`/`@use`s written in file loaded by `load` function,\r\n              //   the `canonicalize` function of that `importer` is called first\r\n              // - the `load` function of an importer is only called for the importer\r\n              //   that returned a non-null result from its `canonicalize` function\r\n              (resolved.endsWith('.css') ||\r\n                resolved.endsWith('.scss') ||\r\n                resolved.endsWith('.sass'))\r\n```\r\nLet's add a comment that explains why we need to limit the extension here."
      ],
      "vite-optimize-glob-operations": [
        "We need the returned `files` to be absolute. I thought we can utilize the `cwd` option like:\r\n```ts\r\nconst files = globSync(globPattern, {\r\n  cwd: path.resolve(path.dirname(id), dir),\r\n  absolute: true,\r\n  expandDirectories: false,\r\n  ignore: ['**/node_modules/**'],\r\n})\r\n```\r\nI think this is what @SuperchupuDev suggested.\r\n",
        "Given that the problem is that `toAbsoluteGlob` escaping the characters unnecessary, how about not using that function?\r\nChecking the `toAbsoluteGlob` function, most of the code is handling about globs and escaping the paths. I think it would be trimmed down to:\r\n```js\r\n// replace `normalizeGlobPattern` + `toAbsoluteGlob` with\r\nconst dir = importer ? dirname(importer) : root\r\nconst normalized = rawPattern[0] === '/' ? posix.join(root, rawPattern.slice(1)) : posix.join(dir, rawPattern)\r\n\r\n// pass it to `newRawPattern`\r\nlet newRawPattern = posix.relative(posix.dirname(importer), normalized)\r\n```\r\n"
      ],
      "vite-environment-variable-management": [
        "```suggestion\r\nThe directory from which `.env` files are loaded. Can be an absolute path, or a path relative to the project root. `false` will disable the `.env` file loading.\r\n\r\nSee [here](/guide/env-and-mode#env-files) for more about environment files.\r\n```\r\nI think it doesn't have to be a warning.\r\n",
        "The current change would make this example broken (`env.APP_ENV` will be `undefined`).\r\n\r\n```suggestion\r\n  // Set the third parameter to 'APP_' to load envs with the `APP_` prefix.\r\n  // If necessary, you can set the optional third parameter to '' to load all env regardless of the `VITE_` prefix.\r\n  const env = loadEnv(mode, process.cwd(), 'APP_')\r\n```\r\n",
        "```suggestion\r\nThe variables declared in an env file for a specific mode (e.g. `.env.production`) will take higher priority than the ones in a generic one (e.g. `.env`).\r\n```\r\nThe fact that `.env` is always loaded is written above in the code block. Does changing the sentence to clarify that the priority is talking about the variables rather than the files make things clear?\r\n",
        "> When running with a specific mode, Vite will always load `.env` and `.env.local` in addition to the mode-specific `.env.[mode]` file. Variables declared in mode-specific files will take precedence over those in generic files, but variables defined only in .env or .env.local will still be available in the environment.\r\n\r\nLooks good to me. I think we can remove the `When running with a specific mode, ` as Vite always have a mode set (it's `development` in dev and `production` in build by default).\r\n"
      ],
      "vite-vue-component-import-handling": [
        "yes\r\nhttps://vite.dev/changes/hotupdate-hook.html#:~:text=handle%20additional%20watch%20events%20with%20create%20and%20delete."
      ],
      "vite-permission-hierarchy-awareness": [
        "I guess this diff is not needed. `permissions.triage` is always true for the ones that has `permissions.write`/`permissions.admin`."
      ],
      "vite-break-down-complex-functions": [
        "I think splitting the function into 3 like this will make the function more easier to understand\r\n```ts\r\nconst toStyleSheetLinkTag = (\r\n  file: string,\r\n  toOutputPath: (filename: string) => string,\r\n): HtmlTagDescriptor => ({\r\n  tag: 'link',\r\n  attrs: {\r\n    rel: 'stylesheet',\r\n    crossorigin: true,\r\n    href: toOutputPath(file),\r\n  },\r\n})\r\n\r\nconst getCssFilesForChunk = (\r\n  chunk: OutputChunk,\r\n  seenChunks: Set<string> = new Set(),\r\n  seenCss: Set<string> = new Set(),\r\n): string[] => {\r\n  if (seenChunks.has(chunk.fileName)) {\r\n    return []\r\n  }\r\n  seenChunks.add(chunk.fileName)\r\n\r\n  if (analyzedChunk.has(chunk)) {\r\n    return analyzedChunk.get(chunk)!\r\n  }\r\n\r\n  const files: string[] = []\r\n  chunk.imports.forEach((file) => {\r\n    const importee = bundle[file]\r\n    if (importee?.type === 'chunk') {\r\n      files.push(...getCssFilesForChunk(importee, seenChunks, seenCss))\r\n    }\r\n  })\r\n  analyzedChunk.set(chunk, files)\r\n\r\n  chunk.viteMetadata!.importedCss.forEach((file) => {\r\n    if (!seenCss.has(file)) {\r\n      seenCss.add(file)\r\n      files.push(file)\r\n    }\r\n  })\r\n\r\n  return files\r\n}\r\n\r\nconst getCssTagsForChunk = (\r\n  chunk: OutputChunk,\r\n  toOutputPath: (filename: string) => string,\r\n) =>\r\n  getCssFilesForChunk(chunk).map((file) =>\r\n    toStyleSheetLinkTag(file, toOutputPath),\r\n  )\r\n```\r\nI think `analyzedChunk` should be renamed to `analyzedImportedCssFiles` in this case.",
        "Ah, yeah, `getCssFilesForChunk` should be\r\n```diff\r\nconst getCssFilesForChunk = (\r\n  chunk: OutputChunk,\r\n  seenChunks: Set<string> = new Set(),\r\n  seenCss: Set<string> = new Set(),\r\n): string[] => {\r\n  if (seenChunks.has(chunk.fileName)) {\r\n    return []\r\n  }\r\n  seenChunks.add(chunk.fileName)\r\n\r\n  if (analyzedChunk.has(chunk)) {\r\n+    const files = analyzedChunk.get(chunk)!\r\n+    const additionals = files.filter(file => !seenCss.has(file))\r\n+    additionals.forEach(file => seenCss.add(file))\r\n+    return additionals\r\n-    return analyzedChunk.get(chunk)!\r\n  }\r\n\r\n  const files: string[] = []\r\n  chunk.imports.forEach((file) => {\r\n    const importee = bundle[file]\r\n    if (importee?.type === 'chunk') {\r\n      files.push(...getCssFilesForChunk(importee, seenChunks, seenCss))\r\n    }\r\n  })\r\n  analyzedChunk.set(chunk, files)\r\n\r\n  chunk.viteMetadata!.importedCss.forEach((file) => {\r\n    if (!seenCss.has(file)) {\r\n      seenCss.add(file)\r\n      files.push(file)\r\n    }\r\n  })\r\n\r\n  return files\r\n}\r\n```"
      ],
      "vite-complete-deployment-commands": [
        "```suggestion\r\n   - **Build Command**: `npm install && npm run build`\r\n```",
        "Note for others: `npm install` is required\r\nhttps://docs.render.com/deploy-sveltekit#manual-deploy\r\n"
      ],
      "vite-secure-workflow-permissions": [
        "Good catch ðŸ™ ",
        "Let's move this step and the \"React Based on Permissions\" step to the top so that nothing other than the reaction would happen for users without permission."
      ],
      "vite-escape-html-content-properly": [
        "Checking [the example](https://github.com/component/escape-html#:~:text=console.dir(%27%3Cinput%20name%3D%22full_name%22%20value%3D%22%27%20%2B%20escapeHtml(fullName)%20%2B%20%27%22%3E%27)), I think it should be\r\n```suggestion\r\n      res += ` ${key}=\"${escapeHtml(attrs[key])}\"`\r\n```\r\notherwise, `JSON.stringify` will do unnecessary escapes."
      ],
      "vite-runtime-agnostic-api-design": [
        "I've pushed a commit that replaces them with `CustomDevEnvironment` and `.runEntrypoint` so that it doesn't look like it can satisfy the `FetchableDevEnvironment` interface."
      ],
      "vite-evolve-apis-with-compatibility": [
        "I think we can deprecate these two methods as well. When migrated from `ssrLoadModule` to `ModuleRunner`, these APIs won't need to be called anymore as the `ModuleRunner` will interpret sourcemaps and rewrite the error stacks."
      ],
      "vite-propagate-errors-with-context": [
        "I think this should be\r\n```suggestion\r\n    if (!fs.existsSync(srcFile)) {\r\n```\r\nAlso I think it'd nice to avoid this `existsSync` call and add an `try-catch` to `fs.statSync` instead. `fs.statSync` throwed `ENOENT` on my machine (both Windows and WSL), did `fs.statSync` return a result on your machine?\r\n",
        "We need to wrap the whole handler with `try-catch` and call `next(error)` in the `catch`, so that `next` is called  when an error happens asynchronously.",
        "I think that is the expected behavior.",
        "For `send`s that were called while connecting the error would be voided and won't be able to catch by the user. In Vite 5, the error happened for the `send` call later that was called (which is confusing I guess).\r\nhttps://github.com/vitejs/vite/blob/c54c860f9d90e4074e5321648f9c5ee9fbda7038/packages/vite/src/shared/hmr.ts#L180-L190\r\n\r\n",
        "I think the order itself is natural as this is how the cause property works. But you have a point. Most users would want to know where the plugin error happened. I'll put it in a different property.",
        "Updated in edb34684dd3f6aefa215d663df11fb7b86c024dc\r\nI passed the whole error to the property, otherwise the console.log will output with quotes and escapes.\r\n```\r\nError: foo\r\n    at file:///D:/documents/GitHub/vite/foo.mjs:2:22\r\n    at ModuleJob.run (node:internal/modules/esm/module_job:268:25)\r\n    at async onImport.tracePromise.__proto__ (node:internal/modules/esm/loader:543:26)\r\n    at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:116:5) {\r\n  runnerStack: 'Error: RunnerError\\n' +\r\n    '    at file:///D:/documents/GitHub/vite/foo.mjs:3:47\\n' +\r\n    '    at ModuleJob.run (node:internal/modules/esm/module_job:268:25)\\n' +\r\n    '    at async onImport.tracePromise.__proto__ (node:internal/modules/esm/loader:543:26)\\n' +\r\n    '    at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:116:5)'\r\n}\r\n```\r\n```\r\nError: foo\r\n    at file:///D:/documents/GitHub/vite/foo.mjs:2:22\r\n    at ModuleJob.run (node:internal/modules/esm/module_job:268:25)\r\n    at async onImport.tracePromise.__proto__ (node:internal/modules/esm/loader:543:26)\r\n    at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:116:5) {\r\n  runnerStack: Error: RunnerError\r\n      at file:///D:/documents/GitHub/vite/foo.mjs:3:47\r\n      at ModuleJob.run (node:internal/modules/esm/module_job:268:25)\r\n      at async onImport.tracePromise.__proto__ (node:internal/modules/esm/loader:543:26)\r\n      at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:116:5)\r\n}\r\n```\r\n",
        "I think we should keep the error message (`Parse failure: `...) so that `console.log(err)` shows the position and contents (#5192, #12060).",
        "I don't know when `err.loc` exsits (when it passes the previous code path) though.",
        "Thanks for checking! That makes sense."
      ],
      "vite-explicit-version-requirements": [
        "`require(ESM)` is only supported (without a flag) for Node 20.19.0+, 22.12.0.\r\nhttps://nodejs.org/docs/latest-v20.x/api/modules.html#loading-ecmascript-modules-using-require\r\nDo we want to bump the version to that version? Or would it be fine to say \"if you want to use CJS, update the Node version\"?\r\n",
        "Sounds good ðŸ‘ ",
        "`packages/vite/src/node/__tests__/package.json` was picked up for `packages/vite/src/node/__tests__/build.spec.ts` by eslint-plugin-n and because that package.json does not have `engines` field, `>=16.0.0` was used and an lint error happened for `util.stripVTCharacters`.\r\nhttps://github.com/eslint-community/eslint-plugin-n?tab=readme-ov-file#configured-nodejs-version-range\r\nI set this to make `^18.0.0 || ^20.0.0 || >=22.0.0` to be applied across the repo.\r\n"
      ],
      "vite-manage-configuration-inheritance-carefully": [
        "Probably needs a helper function that merges recursively but replaces arrays, slightly different from `mergeConfig`.",
        "Added `mergeWithDefaults` function ðŸ‘ ",
        "There's many inferred options. Is it fine to simply omit it?",
        "I'm fine with trimming down the values for the exposed object. I think the object form makes it easier for users to find out the value for each options. But given that we already need to expose `mainFields` and `conditions` separately, maybe that advantage is gone.",
        "I pushed a commit that removes the new exports so that we can separate the discussion about exposing the default values. ðŸ‘ "
      ],
      "vite-descriptive-consistent-naming": [
        "```suggestion\r\n      // normalize and rewrite accepted deps\r\n      const resolvedAcceptedDeps = new Set<string>()\r\n```\r\nWould you rename this variable now that we push ids?",
        "I think we should rename this variable as this no longer only matches `context=*` but also matches `module`. For example, `svelteScriptModuleRE`."
      ],
      "vite-clean-configuration-organization": [
        "I think this should be in the \"Linting\" section rather than \"Bundler mode\" section as it's not required for Vite. Maybe also good to add a comment that it can be disabled if desired.",
        "Ah, didn't know that. I'll remove this."
      ],
      "vite-restrict-server-access": [
        "Added a similar warning to `server.cors` for `server.allowedHosts` as well. Technically, it is safe to set `server.allowedHosts: true` if the dev server runs behind a reverse proxy (the reverse proxy needs to check the host in that case though). But I didn't mention it here as I guess that usage isn't common and setting `allowedHosts` doesn't hurt."
      ],
      "vite-clean-network-resources": [
        "I guess we should call `socket.close` and `socket.removeEventListener` to avoid memory leak.",
        "I confirmed that `Promise.allSettled` is also supported by our default modern browser target.\r\nhttps://caniuse.com/mdn-javascript_builtins_promise_allsettled"
      ],
      "vite-ensure-documentation-accuracy": [
        "```suggestion\r\n- Package manager lockfile content, e.g. `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml` or `bun.lock`\r\n```\r\nSince this list is just an example, I think it's fine to remove `bun.lockb` because `bun.lock` has been added instead.\r\n",
        "Having both feels too verbose. Since `bun.lock` is the new default for bun, I think `bun.lock` should be listed instead of `bun.lockb`.",
        "I believe this \"can also be\" was confusing as the other possibility (`true`) doesn't actually exist now.",
        "Since `root` is deprecated and does not need to be set anymore, I think we can remove `root` from all the examples and types in the docs."
      ],
      "vite-document-protocol-configurations-clearly": [
        "> Is there more information of what are the sufficient values for TLS?\r\n\r\nThe only thing I found on the Node's document is this line.\r\n\r\n> [For servers, the identity options (`pfx` or `key`/`cert`) are usually required.](https://nodejs.org/api/http2.html#http2createserveroptions-onrequesthandler:~:text=For%20servers%2C%20the%20identity%20options%20(pfx%20or%20key/cert)%20are%20usually%20required.)\r\n\r\n> Does this also mean that it's possible to only enable TLS without HTTP/2 if the values are not sufficiently provided? (regardless of proxy)\r\n\r\nNo, Vite always tries to start a HTTP/2 server with TLS enabled if `server.https` is an object if `server.proxy` is not passed. If the options are not sufficiently provided, Node.js starts the server (probably) without any cert and reject any connections.\r\n",
        "Makes sense. I'll change to the previous one."
      ]
    }
  },
  "MMeent": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-adaptive-cache-expiration-strategy",
        "title": "Adaptive cache expiration strategy"
      },
      {
        "slug": "neon-cache-performance-preservation",
        "title": "Cache performance preservation"
      },
      {
        "slug": "neon-clear-consistent-identifier-names",
        "title": "Clear consistent identifier names"
      },
      {
        "slug": "neon-configuration-context-alignment",
        "title": "Configuration context alignment"
      },
      {
        "slug": "neon-database-replica-promotion-safeguards",
        "title": "Database replica promotion safeguards"
      },
      {
        "slug": "neon-document-api-specs-completely",
        "title": "Document API specs completely"
      },
      {
        "slug": "neon-document-connection-transitions",
        "title": "Document connection transitions"
      },
      {
        "slug": "neon-escape-sql-parameters",
        "title": "Escape SQL parameters"
      },
      {
        "slug": "neon-flexible-documented-configurations",
        "title": "Flexible documented configurations"
      },
      {
        "slug": "neon-guard-against-race-conditions",
        "title": "Guard against race conditions"
      },
      {
        "slug": "neon-handle-all-error-paths",
        "title": "Handle all error paths"
      },
      {
        "slug": "neon-handle-network-interrupts-safely",
        "title": "Handle network interrupts safely"
      },
      {
        "slug": "neon-limit-concurrent-access-slots",
        "title": "Limit concurrent access slots"
      },
      {
        "slug": "neon-mind-transaction-boundaries",
        "title": "Mind transaction boundaries"
      },
      {
        "slug": "neon-minimize-unnecessary-allocations",
        "title": "Minimize unnecessary allocations"
      },
      {
        "slug": "neon-optimize-data-structures",
        "title": "Optimize data structures"
      },
      {
        "slug": "neon-prefer-opt-in-security",
        "title": "Prefer opt-in security"
      },
      {
        "slug": "neon-proactive-cache-warming",
        "title": "Proactive cache warming"
      },
      {
        "slug": "neon-proper-metrics-design",
        "title": "Proper metrics design"
      },
      {
        "slug": "neon-proper-option-type-usage",
        "title": "Proper Option type usage"
      },
      {
        "slug": "neon-scope-jwt-authentication-tokens",
        "title": "Scope JWT authentication tokens"
      }
    ],
    "comments": {
      "neon-proactive-cache-warming": [
        "I really don't like this. Let's expose plain numbers, not rounded and divided values. It'll also allow (when used as metric) to compare absolute prewarm speed between projects and endpoints even when their sizes are very different.",
        "`pages_total` + `pages_processed`, probably?\r\nIIRC, we currently also have internal counters for \"ignored\" and \"dropped\" somewhere in the metrics, to cover those pages that we don't have to load because concurrent workloads already fetched the pages or that we can't load because there are no more LFC entries available, respectively.",
        "I don't like this. S3 writes are expensive to scale, and if we're about to write every N seconds it's going to be expensive to host read replicas.\r\n\r\nI'd _expected_ CPlane-triggered writes; either during shutdown (for warming up after start) or before shutdown (for hot restart); not systematic writes.",
        "I don't think checkpoint is a good place for this; it's much too frequent on highly loaded systems.",
        "We can make it part of endpoint shutdown procedures?\r\n\r\nNote that \"i.e. it can periodically fetch fresh content from S3 and do prewarming\" won't work as you seem to expect it to, as prewarming is explicitly designed to never evict existing pages from LFC, and thus won't do much if the size of LFC doesn't change for the hot standby. It's prewarming by replacing unused pages with potentially useful pages, and explicitly not LFC state synchronization.",
        "As for hot standby, it's probably good enough to \"just\" disable the replay filter and require replay of all WAL (rather than only those records which we already have in our caches)",
        "A calculation (S3 us-east-1) based on a setting of 60s:\r\n\r\n> 1 req/min * 60 min/hr * 730 hr/mon * $0.005 /1000req (S3 POST) = $0.219 per concurrently active compute per month, or $219 per 1000 concurrently active computes per month (annualized).\r\n\r\nI'd rather keep this frequency much lower than that even, as this seems like a significant potential cost for free tier operations: It'd be 7% of the monthly cost of a tiny general-purpose compute instance (t4g.micro) at AWS.\r\n\r\nI'm also a bit concerned about dumping LFC state every so often. Dumping that state is not free, and while it isn't all _that_ expensive it does block all IO operations to the LFC for a short while. Doing the dump regularly will likely cause some (small?) amount of degraded performance.\r\n\r\nIf instead of frequently dumping this state, we should dump the state only when needed (i.e. with pending shutdown or pending restart), so that we'll only degrade performance if and when needed."
      ],
      "neon-optimize-data-structures": [
        "Suggestion: Why don't we store subxact depth in DdlHashTable, and maintain a SubtransLevel counter that we can check the DdlHashTable against?\r\n\r\nWe know we won't have to create a new DdlHashTable for level=1 when CurrentLevel=2, unless level 2's subxact is rolled back to level 1.\r\n\r\nSo the stack (of sorts) would look like `SubXactLevel=130; CurrentDHT = DHT (level=100) -> DHT (level=99) -> DHT (level=40) -> DHT (level=21) -> DHT (RootTable)` instead of `SubXactLevel=30; DHT 100 -> DHT 99 -> .. -> DHT 1 -> DHT RootTable`.\r\n\r\nWDYT?\r\n\r\nAs you can see, that will reduce the memory usage required for 1000 subxacts followed by one with DDL from 1000 DdlHashTables to just 1.",
        "Does this not have off-by-one issues? I'd prefer keeping a single counter, rather than one reset every time you push a table: you can't have more than 2^31 subxacts in one transaction, as you'd run out of XIDs.\r\n\r\nAnd in that case, you can change the table `if (CurrentDdlTable->subtrans_level > --SubtransDdlLevel)`"
      ],
      "neon-limit-concurrent-access-slots": [
        "True, but I wouldn't expect them to be working on the LFC _concurrently_. Similar to the shared buffers partition locks and the WAL writers locks, I think we only need a limited amount of concurrent slots to satisfy the workload (e.g. 128), as we only really need the lock once we're ready to start loading data.",
        "Yes, see https://github.com/neondatabase/neon/pull/10312#discussion_r1922225273 - we can limit this to a limited number of concurrent backends.\r\n\r\nNote that we'll only need a prewarm state once a backend has the prefetched data in memory (no need to hold an LFC entry for a not-yet-received prefetch entry; actually, holding that is dangerous for other backends that want to prefetch a set of pages from that chunk), and the turnaround time for prewarming is expected to be very small: it's quite unlikely you're waiting for a long time on concurrent backends to finish their work, as reasonably there are at most BLOCKS_PER_CHUNK backends that are doing IO on the LFC chunk, and each such IO probably won't take very long."
      ],
      "neon-mind-transaction-boundaries": [
        "Could you please do better connection lifetime management here? It's quite bad to see the connections get leaked.",
        "Both `CHECKPOINT` and `COMMIT` should be sufficient to flush current WAL.\r\n\r\nAdditionally, the `last_flush_lsn` is quite out-of-date by the time `pg_switch_wal()` has succeeded. Better use `SELECT pg_current_wal_insert_lsn()` and then run `CHECKPOINT`."
      ],
      "neon-escape-sql-parameters": [
        "Please make sure you correctly escape the database and user parameters of this connection string."
      ],
      "neon-database-replica-promotion-safeguards": [
        "Why does it matter whether this is a promoted replica or not? I see you use it in `walproposer.c`, but that seems like a workaround and bypass of a valid check, not a correct solution to an issue."
      ],
      "neon-minimize-unnecessary-allocations": [
        "True, but that'd add a `format!()` overhead, and I'd rather prevent that. |\r\n\r\nThis allows 'static Strings, also further reducing alloc overhead."
      ],
      "neon-document-connection-transitions": [
        "This is missing the important element where the Primary that has to be shut down first, but the (old) primary does not show up in this diagram.",
        "Oh, it didn't have the same actor labeling as those of the secondary, so I'd failed to notice this.\r\n\r\nEither way, that needs additional details, as there are some compute_ctl-postgres interactions which we need to detail on the primary as well for this to work correctly and consistently.",
        "Not \"missing\" per se, but I do find it confusing that the RFC does go into detail for one side (the promoting replica) but not the other (the primary), while both sides are critial for correct functioning of the system. I'd expected both sides to have the same amount of detail."
      ],
      "neon-configuration-context-alignment": [
        "```suggestion\r\n\t\t\t\t\t\t!AmPrewarmWorker && /* do not configure the timeout-based pumping system in prewarm workers */\r\n```"
      ],
      "neon-adaptive-cache-expiration-strategy": [
        "> or evicted from LFC after receiving the corresponding WAL record (in case of replica).\r\n\r\nThis is incorrect: WAL will be replayed for every page that's currently in the LFC or shared buffers.\r\n\r\n> Wondering if there's any interactions between the user workload and prewarm that are worth considering.\r\n> if it's available then the LFC cache might have become stale over time.\r\n\r\nThe LFC can become stale, but only if the main page is still in shared buffers. The (modified) page from buffers will at some point be written, which will make the LFC lose its stale-ness.\r\n\r\nIn any case, the stale-ness of a page in LFC doesn't (shouldn't) matter for this RFC.",
        "I'm a bit concerned about issues that would arise from deletion on a weekly schedule while using the endpoint exclusively for monthly tasks - you'd want that endpoint to have good performance."
      ],
      "neon-prefer-opt-in-security": [
        "Why is anon suddenly a default preloaded library?",
        "OK, but why does it need to be a default-on setting?\r\n\r\nI, as a user, don't want `anon` anywhere close to my data, due to its inherent nature to return the wrong data (i.e. anything but the actual stored data)."
      ],
      "neon-cache-performance-preservation": [
        "Make primary report its flushLSN to CPlane on shutdown, and have CPlane pass that on with the `/promote` call so that compute_ctl can ensure that it has received that WAL before promotion.",
        "> If I'm reading this right, there's a an availability gap between the termination and promotion\r\n\r\nThe secondary *could* be made available for read-only queries before the primary shuts down. However, we can *not* allow write queries to the secondary before the primary has shut down, so there will always be a gap where there is no writer available.\r\n\r\nThe system should be fast enough, however, that this gap is no longer than a second at most; and probably much less.",
        "> It seems like we can swap promotion and termination around while maintaining this property.\r\n\r\nNo, Vlad, we can not do that.\r\n\r\nTwo computes can Never, NEVER, _never_ both be Primary on the same timeline at the same time.  CPlane is supposed to make sure of that, and Compute will fail if it doesn't. Promotion of the replica before the original Primary shut down will cause errors, panics, data loss, shutdowns, and/or nasal deamons on the Primary, this promoting Secondary, or both.\r\n\r\nWe don't want to test that theory.\r\n\r\n\r\n> What would happen to the client in this case? Let's say we have an in-progress query when the old primary terminates.\r\n\r\nTheir session would and should get disconnected. How that client handles disconnections is not something we care about here.",
        "> More specifically, by idle I mean it would have to not write any WAL and be in some sort of consensus observer role.\r\n\r\nThat's what a hot standby is for PostgreSQL - the hot standby is not allowed to write WAL, but could stop reading and applying WAL from the primary and start writing its own WAL at a moment's notice.\r\n\r\nThe issue is that you can't promote until you've replayed all acknowledged WAL from safekeepers, because otherwise you'd fail to replay commits that the primary may have already sent acknowledgements for to its clients, essentially losing commits."
      ],
      "neon-proper-metrics-design": [
        "If a table is dropped, its count is presumably removed from the total, too."
      ],
      "neon-clear-consistent-identifier-names": [
        "I don't like using plain numbers for something that is more than just that plain number, unless context makes it abundantly clear. As Display doesn't guarantee context, the value of each type was clarified by adding the full enum tag to Display output.\r\n\r\nInstead of neutering the Display implementation, I think it's better to fix the clap usage so that it doesn't rely on `Display` for serializing strings - we implement Serialize and Deserialize for those purposes.",
        "Please rename this to `WalSegmentSize`. \r\n\r\nPostgreSQL stores data files in Segments of 1GB increments, which makes this value confusing."
      ],
      "neon-proper-option-type-usage": [
        "Shouldn't this be Option<Lsn>, given that it's only set during shutdown?"
      ],
      "neon-flexible-documented-configurations": [
        "Did you consider providing a single function for this, like the following?\r\n\r\n```suggestion\r\n+CREATE OR REPLACE FUNCTION anon.enable_transparent_masking_superuser(\r\n+  dbname TEXT,\r\n+  toggle BOOL DEFAULT = true,\r\n+)\r\n+RETURNS VOID AS\r\n+$$\r\n+BEGIN\r\n+  EXECUTE format('ALTER DATABASE %I SET anon.transparent_dynamic_masking TO %L', dbname, toggle::text);\r\n+END;\r\n+$$\r\n+  LANGUAGE plpgsql\r\n+  VOLATILE\r\n+  SECURITY DEFINER\r\n+  SET search_path=''\r\n+;\r\n```"
      ],
      "neon-handle-all-error-paths": [
        "Please add an Assert(slot->response->tag == T_NeonGetPageResponse) below the `continue` code block, so that any misplaced response in slot->response is caught, at least in debug builds.",
        "Yes, but I'd rather have that check in place here as well, in case anything changes in other places that touch slot->response.",
        "So, is this the fix, or is there another component that's been fixed with this PR?",
        "Hmm, I see. How come we're not handling termination -related errors in PG_CATCH? Because we can't recover from them?"
      ],
      "neon-scope-jwt-authentication-tokens": [
        "The RFC for this storage service https://github.com/neondatabase/neon/pull/9661 implies that compute's tokens won't contain information about which endpoint it is, so the \"S3 proxy\" (which isn't just that, and thus probably shouldn't be called that) **can't** validate that the request came from a compute with the right endpoint_id.",
        "I'm not quite sure yet about the S3 design. \r\n\r\nYes, it'll have to be tenant-prefixed, and probably Endpoint-prefixed too, but I'm not yet 100% sure if it'll also be timeline-prefixed.",
        "> I think that from the security standpoint, the tenant should be enough as the tenant is our level of multi-tenancy,\r\n\r\nI don't think that's good enough. Compute's tokens should be bound to (Tenant, Timeline, Lsn >/=), so it can't ask for data created on completely disjoint timelines in the same tenant (e.g. `a` branches into `b` and `c`; compute on `b` shouldn't be able to query data in `c`).\r\n\r\n> and we use it for storage auth already\r\n\r\nIMV that's a bad argument. Having a bad practice doesn't mean we should adapt it in new projects if we can prevent it."
      ],
      "neon-document-api-specs-completely": [
        "CPlane would be hitting this service.\r\n\r\n(Note that we're not necessarily using S3; any blob storage will do)"
      ],
      "neon-guard-against-race-conditions": [
        "Shouldn't `IsUnderPostmaster` be a better approach than these rather arbitrary conditions?\r\n\r\nI.e. \r\n```suggestion\r\n\tif (newval && *newval != '\\0' && IsUnderPostmaster && RecoveryInProgress())\r\n```",
        "Yeah, it's weird that we're checking for the availability of shmem when we know that shmem should be available exactly when we run code with `IsUnderPostmaster`. AFAIK there is no valid reason why we should be unable to access shared memory in `IsUnderPostmaster` processes.",
        "Fixed.",
        "Fixed."
      ],
      "neon-handle-network-interrupts-safely": [
        "Note that moving this call to _before_ `page_server->send() || page_server->flush()` will cause requests sent by `page_server_request` to not be pipelined with getpage requests already in the connection, and so this may have 2 RTT latency, instead of 1 RTT: 1 RTT to finish all open GetPage requests, and 1 RTT for the actual NeonRequest.\r\n\r\nIt doesn't look like the behaviour here is very different, so why not put it immediately before the `page_server->receive()` call?",
        "OK, I think I got it. \r\nIf a connection is dropped during consume_prefetch_responses, then that signal is not carried on to the synchronous request path that requested `consume_prefetch_responses`, so putting consume_prefetch_responses between `page_server->sync()` and `page_server->receive()` could cause the connection to get stuck waiting on a newly created empty connection.\r\n\r\nIf we update `consume_prefetch_responses` to return the output of wait_for(), then we can use that to determine connection state, and continue handling everything correctly.\r\n\r\n```\r\nstatic bool\r\nconsume_prefetch_responses(void)\r\n{\r\n\tif (MyPState->ring_receive < MyPState->ring_unused)\r\n\t\treturn prefetch_wait_for(MyPState->ring_unused - 1);\r\n\treturn true;\r\n}\r\n```\r\n\r\n\r\nand then, in sync request paths, you'd do the following, inside a PG_TRY block to make sure to drop connections when the request gets cancelled:\r\n\r\n```\r\n\t\twhile (!page_server->send(shard_no, &request.hdr)\r\n\t\t\t\t|| !page_server->flush(shard_no)\r\n\t\t\t\t|| !consume_prefetch_responses())\r\n\t\t{\r\n\t\t\t/*\r\n\t\t\t * Loop until we've successfully\r\n\t\t\t *  1.) Written the request into the shard's connection,\r\n\t\t\t *  2.) Flushed that request to the network, and\r\n\t\t\t *  3.) Consumed all open prefetch requests still on the line.\r\n\t\t\t */\r\n\t\t};\r\n```\r\n\r\n(the sync request path in page_server_request and communicator_read_slru_segment)",
        "Hmm, ok."
      ]
    }
  },
  "lgrammel": {
    "repos": [
      "vercel/ai"
    ],
    "entries": [
      {
        "slug": "ai-async-error-callbacks",
        "title": "Async error callbacks"
      },
      {
        "slug": "ai-consistent-camelcase-naming",
        "title": "Consistent camelCase naming"
      },
      {
        "slug": "ai-consistent-provider-options",
        "title": "Consistent provider options"
      },
      {
        "slug": "ai-consistent-semantic-naming",
        "title": "Consistent semantic naming"
      },
      {
        "slug": "ai-document-api-schemas",
        "title": "Document API schemas"
      },
      {
        "slug": "ai-document-configuration-decisions",
        "title": "Document configuration decisions"
      },
      {
        "slug": "ai-explicit-code-organization-patterns",
        "title": "Explicit code organization patterns"
      },
      {
        "slug": "ai-flexible-api-inputs",
        "title": "Flexible API inputs"
      },
      {
        "slug": "ai-format-for-rendering-compatibility",
        "title": "Format for rendering compatibility"
      },
      {
        "slug": "ai-keep-tests-simple",
        "title": "Keep tests simple"
      },
      {
        "slug": "ai-maintain-api-naming-consistency",
        "title": "Maintain API naming consistency"
      },
      {
        "slug": "ai-optimize-ci-type-checking",
        "title": "Optimize CI type checking"
      },
      {
        "slug": "ai-place-configurations-appropriately",
        "title": "Place configurations appropriately"
      },
      {
        "slug": "ai-provide-actionable-examples",
        "title": "Provide actionable examples"
      },
      {
        "slug": "ai-test-before-documenting",
        "title": "Test before documenting"
      },
      {
        "slug": "ai-type-safe-null-handling",
        "title": "Type-safe null handling"
      },
      {
        "slug": "ai-validate-pattern-matching",
        "title": "Validate pattern matching"
      },
      {
        "slug": "ai-versioning-for-migrations",
        "title": "Versioning for migrations"
      }
    ],
    "comments": {
      "ai-flexible-api-inputs": [
        "replace tool schemas with json schemas\n\nthis affects structured outputs as well",
        "out of curiosity, what happens if the url is entered as a string? (should ideally be supported)",
        "(no need for PR updates re this comment)",
        "we have special conversion mechanisms that we should be using, no need to this build one-off, and we can also delay and do it on v5 once all providers are there to reduce # of ports.",
        "(the mechanism has been introduced / updated on v5 recently)"
      ],
      "ai-place-configurations-appropriately": [
        "can you make it a constructor parameter (in the options) for the model that is then defined in the providers? (which would have that knowledge)",
        "include the default object generation mode in `config`",
        "This seems like something we could turn into a top-level option eventually?",
        "voice and response_format are setting we have standardized. we usually do not duplicate standardized settings in the providerOptions"
      ],
      "ai-validate-pattern-matching": [
        "this can lead to issues where the wrong enum value is selected as a partial result, leading to changing enum values. i'd prefer a solution where that cannot happen (either by only returning fully matched enum values, or (more complicated) by returning enum values for which only one value is possible per prefix)",
        "What is this for? Can we do this more elegantly?",
        "interesting - looks like this is not going to work for base64? we need tests around this and make it work for base 64 as well. Also it might be good to have some way of opting (or flagging on the signatures) since this removal won't be needed for images presumably.",
        "btw, is this a separate bugfix that should be extracted into a standalone PR?"
      ],
      "ai-explicit-code-organization-patterns": [
        "The value is set in the constructor. Would prefer `private strictMode: boolean` to prevent the impression that the `false` setting here matters.",
        "avoid export `*` - we need to control what we re-export",
        "this is strange - can we directly import (from provider utils or the file)?",
        "can't you replace it with:\r\n\r\n```\r\nimport { convertArrayToReadableStream } from '@ai-sdk/provider-utils/test';\r\n```",
        "ideally for imports we avoid barrel files as much as possible and go for the original source"
      ],
      "ai-consistent-provider-options": [
        "make provider-specific. different providers have different structures. have default that matches openai",
        "this is going to be brittle as they change their APIs. Can we expose the raw body from the final result and strip this to a minimum?",
        "Please keep specs fully separate for now.",
        "I've extracted `SharedV2ProviderOptions` and `SharedV2ProviderMetadata` that we can use in all model specs: https://github.com/vercel/ai/pull/5733",
        "Please use the validation approach for provider options that we use elsewhere (so we get a type for type checking and can throw errors before the request). See https://github.com/vercel/ai/blob/main/packages/anthropic/src/anthropic-messages-language-model.ts#L122 and https://github.com/vercel/ai/blob/main/packages/anthropic/src/anthropic-messages-language-model.ts#L724 for an example"
      ],
      "ai-optimize-ci-type-checking": [
        "which one will run in CI? important that we keep full checks there",
        "where will the js files go?"
      ],
      "ai-type-safe-null-handling": [
        "`undefined` or `never`? (i have used `never` in other places for something similar)",
        "`never` is actually better ts wise imo",
        "and you might want it to be optional ",
        "most stream parts are defined in stream parts afaik. do we need to expose this? otherwise would prefer if it's consistent with the other stream parts. also, `any` is risky. is `unknown` possible?",
        "shouldnt this be automatically typed?",
        "can you add a zod schema and validate instead?",
        "avoid any",
        "also return type should be narrowed via is",
        "you can remove indent via \r\n\r\nfor (const citation of response.message.citations ?? [])",
        "can leave undefined (no `|| ''`) and then use `?.` before `startsWith`"
      ],
      "ai-document-configuration-decisions": [
        "this is a downgrade - why?",
        "we can make zod a normal dependency since this is not a lib we publish",
        "is there a risk that this breaks common js usage of the ai sdk?",
        "needed for the rsc move?"
      ],
      "ai-async-error-callbacks": [
        "Wondering if that's the desired behavior - might be good to think through use cases",
        "wonder if it's worth pushing the onError object into `consumeStream` to keep the behavior between `consumeStream` here and the general helper aligned?",
        "the helper is afaik only used internally so we can change the main `stream` object to become a parameter object \r\n\r\n```\r\n{\r\n  stream,\r\n  onError?\r\n}\r\n```"
      ],
      "ai-provide-actionable-examples": [
        "need to set expectations:\r\n\r\n- only for green-field prototypes (no migrations yet)\r\n- not for production use\r\n- please provide feedback\r\n- expect large breaking changes while in alpha"
      ],
      "ai-format-for-rendering-compatibility": [
        "this is somewhat specific. we also have a `contributing/` folder where we can have more detailed guides, architecture docs, etc. What do you think about having a `contributing/how-to-create-a-codemode.md` and then referring to the contributing directory here?"
      ],
      "ai-keep-tests-simple": [
        "instead of flags, just set up custom prepare methods or if it's a single test define the test input in the test\r\n\r\nprefer less magic / thinking in tests, often it is very helpful to be able to look at the raw input without any indirections / logic",
        "just inline the parts. unit tests should be as straightforward as possible. if you want to check that includeRawChunks is passed correctly, add a separate test for just that.",
        "an inline snapshot on content is prob easier, also checks order. in general such logic in tests is usually a code smell",
        "is this needed? might make sense to just call `new ChatStore` in the tests to be explicit.",
        "(then you can also inline some variables in the tests to see how the store would look like)",
        "this test can be split up into several tests for the individual callbacks. should also test that they are invoked with the expected objects. testing `store.getmessages` should be a separate test as well.\r\n\r\n(one `it` test should ideally only test 1 \"thing\" so they can fail individually)",
        "please stub the dates instead. see e.g. \r\n\r\nhttps://github.com/vercel/ai/blob/main/packages/ai/core/generate-text/generate-text.test.ts#L1280\r\n\r\n```ts\r\n  _internal: {\r\n    generateId = originalGenerateId,\r\n    currentDate = () => new Date(),\r\n  } = {},\r\n  ```\r\n  \r\nTests should never have any variable aspects such as dates. With stubbed values we can exactly test the passthrough."
      ],
      "ai-document-api-schemas": [
        "add model id and jsdoc?"
      ],
      "ai-test-before-documenting": [
        "While this is generally true, it is really a misleading explanation.\r\n\r\nThis happens when a provider API returns with an error. Most of the time, this means that there API cannot be used in this way, e.g. invalid message structure. The user needs to carefully read the actual error message from the provider and figure out what to do.",
        "@Und3rf10w have you tested this with the gemini api? couldn't find it in their docs",
        "I guess we can add the fs.readFile from the executable example"
      ],
      "ai-consistent-camelcase-naming": [
        "why was this changed to snake case? if that is the case in the code we should fix the code instead",
        "we usually use `camelCase` in the provider options to match conventions (even if the provider uses `snake_case` in their apis)"
      ],
      "ai-maintain-api-naming-consistency": [
        "We need to consider renaming this on the `embed` method returns as well, and potentially for `embedMany`"
      ],
      "ai-versioning-for-migrations": [
        "on v5 it does not matter, the release will be `major` regardless. ",
        "Lets have them always use a `patch` changeset - only we should be doing minor/major"
      ],
      "ai-consistent-semantic-naming": [
        "rn to `create...` (start function w/ verb)",
        "Prefer full words for generic in upper case, e.g. `CONTEXT`",
        "convention: start non-classes/types (ie regular vars like schemas) with lowercase letter",
        "wonder how far we should to here. usually we do this but it is also overhead",
        "rename to `maxParallelCalls` or `maxParallelRequests` or `maxConcurrentRequests`. If we rename to `Concurrent` here it would be good to do the same in the model spec",
        "We should return `GenerateObjectResult<TYPE>` (not the default, that's internal)",
        "also can you change `TYPE` to `RESULT` (which is more specific, a generic will always be a type)",
        "rename to `mediaType` and refer to IANA media types (see other examples esp. on v5 branch)",
        "let's fix that in `v5` and move to a pattern similar to groq where we parse all options first",
        "what is the unit? can we include it in the var name to avoid confusion, e.g. `durationInSeconds`"
      ]
    }
  },
  "DimasKovas": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-clear-consistent-identifier-names",
        "title": "Clear consistent identifier names"
      },
      {
        "slug": "neon-configurable-cache-parameters",
        "title": "Configurable cache parameters"
      },
      {
        "slug": "neon-configuration-context-alignment",
        "title": "Configuration context alignment"
      },
      {
        "slug": "neon-database-before-memory",
        "title": "Database before memory"
      },
      {
        "slug": "neon-design-domain-specific-error-types",
        "title": "Design domain-specific error types"
      },
      {
        "slug": "neon-ensure-algorithm-robustness",
        "title": "Ensure algorithm robustness"
      },
      {
        "slug": "neon-environment-specific-config-defaults",
        "title": "Environment-specific config defaults"
      },
      {
        "slug": "neon-extract-and-reuse",
        "title": "Extract and reuse"
      },
      {
        "slug": "neon-handle-all-error-paths",
        "title": "Handle all error paths"
      },
      {
        "slug": "neon-hierarchical-semantic-naming",
        "title": "Hierarchical semantic naming"
      },
      {
        "slug": "neon-secure-authentication-handling",
        "title": "Secure authentication handling"
      },
      {
        "slug": "neon-stage-configuration-changes-gradually",
        "title": "Stage configuration changes gradually"
      },
      {
        "slug": "neon-structure-endpoints-for-rest",
        "title": "Structure endpoints for REST"
      },
      {
        "slug": "neon-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      }
    ],
    "comments": {
      "neon-stage-configuration-changes-gradually": [
        "Some compat tests are broken because I think it runs old computes (without `neon.safekeeper_connstrings`) with new control plane.\r\nThis kind of changes are usually done with several steps (several PRs):\r\n1. First, add new option to compute, but do not use it in CP (or use under a feature flag).\r\n2. Wait for the new option to reach the release\r\n3. Change the option in CP\r\n4. Delete old option\r\n\r\nIt's quite complicated and not fast, that's why I liked the idea of a separate `extra_conn_options`",
        "Do I understand correctly that we need to restart all computes to switch this options in control plane in production?"
      ],
      "neon-database-before-memory": [
        "Can we have a tenant only with read-only timelines?\r\nIn this case similar logic is needed in `tenant_delete_safekeepers`"
      ],
      "neon-configurable-cache-parameters": [
        "What if it already exists?\r\n\r\nIn that case we will forget that we have another entry and will never remove it.\r\n\r\nI guess we may want to store an array as an value,\r\n\r\nor, as an alternative, if we have `tokio::sync::mpsc::unbounded_channel` for deleting entries (and drain it in `select!`), then we may insert a new entry into the `data_cache` and add previous LSN (if existed) to `deletion_queue`, so the background task will delete it\r\n\r\nThe second option is probably simpler (don't need to keep an array)"
      ],
      "neon-environment-specific-config-defaults": [
        "Now I see why you didn't like \"enabled\" field :)\r\n\r\nLet's put `#[serde(default)]` on the whole `DiskUsageEvictionTaskConfig`, so all fields will be defaulted to values from `DiskUsageEvictionTaskConfig::default()`, and remove it from fields. Then `{enabled=false}` will be a valid config"
      ],
      "neon-ensure-algorithm-robustness": [
        "Well, I guess it's very unlikely to happen, but working with floating numbers, I wouldn't assume that some numbers add up to 100. There is always a precision error. Maybe returning the last value would be a better option?"
      ],
      "neon-secure-authentication-handling": [
        "Need to protect those methods with `check_permissions(&request, None)` to block these API calls with tenant-scoped tokens.",
        "It will panic if authorization header is shorter than 7 bytes, need to check. I think `.strip_prefix(\"Bearer\")` does the job in the rust-way\r\n\r\nAnd may be `trim_start()` before it also makes sense\r\n\r\n",
        "My initial comment was slightly incorrect - having a space after `Bearer` as it was in your code before ( `.strip_prefix(\"Bearer \")`) is probably more correct because otherwise, it will match header value `BearerSmth <token>`. "
      ],
      "neon-configuration-context-alignment": [
        "I think `PGC_POSTMASTER` fits better here, because we don't have code to reconnect to safekeepers when this option changes right now.\r\nOr need to implement `assign_neon_safekeeper_extra_conninfo_options` hook to restart the walproposer process if the option changes similarly to `assign_neon_safekeepers`"
      ],
      "neon-extract-and-reuse": [
        "nit: Maybe reuse `split_off_safekeepers_generation` here? Looks exactly like it"
      ],
      "neon-clear-consistent-identifier-names": [
        "nit: it's not HTTP client anymore. just `client` sounds fine to me",
        "nit: technically, we already have `posthog` in lib name, probably it's not worth it to add `PostHog` to every class name. E.g. for the client I'm OK with `PostHogClient`, because there are too many `Client`s already. But for those structs the name is already pretty long and unique."
      ],
      "neon-use-descriptive-identifiers": [
        "nit: it's more difficult to understand inverted boolean options (like \"disable\")\r\nAnd later this option converts to `kick_secondary_downloads`, which is even more frustrating\r\nLet's call it the same `kick_secondary_downloads` everywhere"
      ],
      "neon-handle-all-error-paths": [
        "If we set `safekeepers` guc to`g#123`, endptr will point to the `\\0` terminator. Here you skip the terminator and return a pointer past the string data. It will probably cause a segfault later in the code\r\n\r\nIt is an incorrect value, and cplane should never specify it, but the best practice for handling incorrect input is an error, not SIGSEGV :)\r\n\r\nNeed to check that `*endptr == ':'` "
      ],
      "neon-structure-endpoints-for-rest": [
        "nit: `/v1/feature_flag/:flag_name` looks more \"RESTful\", but I'm fine with current `?flag=my-flag` too",
        "nit: probably we can move these to `/debug/v1`, how it's done in storcon, to make it more clear that it's debug handlers"
      ],
      "neon-design-domain-specific-error-types": [
        "Is it ok that we return `None` and not real errors here and there?\r\nProbably we want to log if there is some error in evaluation because usually it means bugs",
        "nit: it's already fine (and much better than just `return None`), but adding some context to error messages would increase debugability (e.g. the name of the missing property).\r\nIt's applies to all other places as well"
      ],
      "neon-hierarchical-semantic-naming": [
        "nit: google style guide suggests using verbs in method names. `GetDbSize`, `CheckRelExists`, `GetRelSize`",
        "nit: I would rather call it `LsnInfo` or smth more specific than \"common\". I don't think it's going to be extended"
      ]
    }
  },
  "pierrejeambrun": {
    "repos": [
      "apache/airflow"
    ],
    "entries": [
      {
        "slug": "airflow-avoid-code-duplication",
        "title": "Avoid code duplication"
      },
      {
        "slug": "airflow-component-reuse-first",
        "title": "Component reuse first"
      },
      {
        "slug": "airflow-document-public-api-boundaries",
        "title": "Document public API boundaries"
      },
      {
        "slug": "airflow-document-security-exceptions",
        "title": "Document security exceptions"
      },
      {
        "slug": "airflow-ensure-deterministic-queries",
        "title": "Ensure deterministic queries"
      },
      {
        "slug": "airflow-internationalize-ui-text",
        "title": "Internationalize ui text"
      },
      {
        "slug": "airflow-leverage-backend-api-capabilities",
        "title": "Leverage backend API capabilities"
      },
      {
        "slug": "airflow-maintain-code-consistency",
        "title": "Maintain code consistency"
      },
      {
        "slug": "airflow-parameterize-similar-tests",
        "title": "Parameterize similar tests"
      },
      {
        "slug": "airflow-standardize-api-parameter-handling",
        "title": "Standardize API parameter handling"
      },
      {
        "slug": "airflow-use-descriptive-action-names",
        "title": "Use descriptive action names"
      },
      {
        "slug": "airflow-use-guards-over-assertions",
        "title": "Use guards over assertions"
      },
      {
        "slug": "airflow-validate-configuration-source-changes",
        "title": "Validate configuration source changes"
      },
      {
        "slug": "airflow-validate-nulls-explicitly",
        "title": "Validate nulls explicitly"
      }
    ],
    "comments": {
      "airflow-document-security-exceptions": [
        "Maybe just disable the eslint rule around the iframe component. \r\n\r\nAnd also add a comment to explain why this is safe. (We are only framing trusted sources, coming from the AuthManager extra menu items. Auth Manager is part of the deployment, as per our security policy Deployment Managers are considered safe). A link to our security policy too would be great. https://airflow.apache.org/docs/apache-airflow/stable/security/security_model.html"
      ],
      "airflow-leverage-backend-api-capabilities": [
        "You don't need to do that slicing in the front-end. Endpoint is paginated, just request the first 5.",
        "I think this logic can holds a few problems, first of the endpoint `useAssetServiceGetAssets` is paginated. It means that you'll only retrieve the first 50 items, and you need a mechanism to display the remaining items (cf tables pagination component).\r\n\r\nIn addition, there is no guarantee at this point that all assets for a specified group will be on the same page, you could end up with a group missing some assets because those were not returned yet.\r\n\r\nI think the easiest approach would be to add a backend ui endpoints to list/retrieve asset groups directly. (I group would list all of it's child asset and we would paginate on groups directly). Maybe there are other approaches possible but I would need to think about it.",
        "We shouldn't do that manually in the UI. (call the endpoints with two different parameters and then merge results in the UI).\r\n\r\nThe backend should support this. (You can write a custom filter that will do the search accross both 'asset.name` and `asset.group`, or expand the `search_param_factory` and `_SearchParam` to be able to take a `list[ColumnElement]` as attribute. And perform a search across multiple columns."
      ],
      "airflow-use-guards-over-assertions": [
        "We shouldn't need a hard casting like that. \"as\"\r\n",
        "You'll be able to simplify this check once the backend always returns a list for `menuItems`"
      ],
      "airflow-validate-configuration-source-changes": [
        "You can't do that, response from `usePluginServiceGetPlugins` and `useConfig(\"plugins_extra_menu_items\")` are completely different this needs more work.\r\n\r\n\r\nYou should do the same as you did above. Check if `plugin` views is in the permissions, if it's not don't render the `<PuginMenus />` in the nav.",
        "Maybe keep `appbuilder_menu_items`, I have a PR that that will deprecte it and replace it with `external_views_items`",
        "As an admin I should see plugins if I have some defined:\r\n\r\nYour PR\r\n![Screenshot 2025-06-20 at 17 19 45](https://github.com/user-attachments/assets/3bfb5b17-da2e-4ea4-b141-a7fd7ff7341b)\r\n\r\nMain:\r\n![Screenshot 2025-06-20 at 17 22 49](https://github.com/user-attachments/assets/873492d2-0423-4faa-9dfc-04df13b83b6a)\r\n\r\n"
      ],
      "airflow-maintain-code-consistency": [
        "Here we should probably show text.\r\n\r\nThis is what we are doing for all other buttons in the header. (so we see a clear \"* Favorite\" and \"* Unfavorite\" button. Also it should probably be on the right align with the 'reparse' button.",
        "Maybe extract this into a really small fn. \"getPluginIframeRoute\" or something, in the same file.\r\n\r\nJust so the comment, and how to construct the iframe component remains in one place and not copy pasted everywhere. I'm scared of someone updating in one place and not in other occurrences. (That doens't cost much to do and would help preventing mistakes in the future)"
      ],
      "airflow-validate-nulls-explicitly": [
        "That type checking assert feels really weird. Maybe just handle the None case `if ... is None` -> `raise dag version not found`, while we wait for the follow up PR to clean that up."
      ],
      "airflow-component-reuse-first": [
        "We already do this at different places in the code. (We use chevron icons from react-icon lib instead), you can do something similar."
      ],
      "airflow-document-public-api-boundaries": [
        "I thought about that, and I think this is already handled by the plugin system. If someone wants to host the bundle in airflow, they can simply add a `fastapi_app` to extend the api-server capabilities and serve static content from anywhere they'd like. \r\n\r\n\r\n\r\nSomething similar to what we are doing for the Simple Auth Manager. That would look like this in your plugin:\r\n\r\n```python\r\n# In your plugins folder\r\napp = FastAPI()\r\n\r\ndirectory = Path(__file__).parent.joinpath(\"static\") # Folder holding static assets\r\n\r\napp.mount(\r\n    \"/static\",\r\n    StaticFiles(\r\n        directory=directory,\r\n        html=True,\r\n    ),\r\n    name=\"static_file_plugin\",\r\n)\r\n\r\n    \r\nclass AirflowServeStaticFilesPlugin(AirflowPlugin):\r\n    fastapi_apps = [\r\n        {\r\n            \"app\": app,\r\n            \"url_prefix\": \"/static-plugin\",\r\n            \"name\": \"Static plugin\",\r\n        }\r\n    ]\r\n```",
        "Maybe I can add a note in the doc for this. ",
        "![Screenshot 2025-06-26 at 15 02 09](https://github.com/user-attachments/assets/650e64de-6db0-404c-acdf-39bbb71db40a)\r\n\r\n"
      ],
      "airflow-avoid-code-duplication": [
        "for loop (because if MAX_SORT_PARAMS == 5, we don't want to do copy/past this 5 times)"
      ],
      "airflow-standardize-api-parameter-handling": [
        "No, FastAPI handles `node_ids: list[str] | None = None` natively. FastAPI chose exploded form query params, i.e `?node_ids=1&node_ids=2` you will directly receive `node_ids=[\"1\",\"2\"] in your function.",
        "Please stay consistent with the rest of the API, we don't want some endpoints doing that and some other doing something else, also this will remove the need for you to manually parse that string.",
        "The multiple query param orders will not be passed like this `order_by=-criteria1,criteria2`, but `order_by=-criteria1&order_by=criteria2` to be consistent with the `exploded` way passing query parameters list that FastAPI is defaulting too.",
        "`two` shouldn't be hardcoded but come from the `MAX_SORT_PARAMS` value.\r\n\r\nWe need a test for that.",
        "I'm not sure it's exactly equivalent. If there is no `order_by` query param specified, we shouldn't add any by default and let the query default order operate:\r\n- order_by = None => don't update the query at all and let the default ordering of the query operate.\r\n- order_by = [] or [\"\"] -> fill with [primary_key_string] (or raise validation error)"
      ],
      "airflow-parameterize-similar-tests": [
        "Can you reorganize this and put tests that sort on the same criteria next to each other. For instance `criteria1` is 'last_run_state', then `criteria2` is \"display_name\" so we can more easily compare.\r\n\r\nAlso for the second sort to take effect you need data where the first criteria is equal. I'm not sure we have this at the moment.\r\n\r\nBasically we need to tests where:\r\n`{\"order_by\": [\"criteria1\", \"criteria2\"]}` will yield a different result than `{\"order_by\": [\"criteria1\", \"-criteria2\"]}` to highlight that `criteria2` sorting is actually doing something and taken into account.",
        "1 class per endpoint. Multiple method for different test cases. We shouldn't  have two classes there."
      ],
      "airflow-use-descriptive-action-names": [
        "Good point, updated!"
      ],
      "airflow-ensure-deterministic-queries": [
        "I think this problem will be here for any state that has a `start_date` None. (no_status, scheduled, queued, etc....)",
        "Maybe we should completely remove that `where` clause. The run does not have to be 'started' to actually be considered, WDYT?",
        "Also we most likely want to add a test case for this scenario.",
        "> This function is currently only used in the /dashboard stats endpoint, so initially I handled it by allowing only QUEUED runs (which are the only valid state with start_date=None).\r\n\r\nWhat function are you talking about `generate_dag_with_latest_run_query` ? `generate_dag_with_latest_run_query` is not used in the stats endpoint but in many other dag listing endpoints.",
        "Please don't use bitwise operator, sqlalchemy exposes \"not\" \"and\" \"or\" which are clearer."
      ],
      "airflow-internationalize-ui-text": [
        "> Additionally, for languages that I am not familiar with, what is the recommended approach for handling those translations?\r\n\r\nDo not update other languages, they will fallback to english for the time being and other translation owners will be in charge to fill the missing translation before the next release."
      ]
    }
  },
  "shaedrich": {
    "repos": [
      "laravel/framework"
    ],
    "entries": [
      {
        "slug": "framework-cache-expensive-operations",
        "title": "Cache expensive operations"
      },
      {
        "slug": "framework-descriptive-configuration-keys",
        "title": "Descriptive configuration keys"
      },
      {
        "slug": "framework-design-flexible-apis",
        "title": "Design flexible APIs"
      },
      {
        "slug": "framework-escape-column-names-properly",
        "title": "Escape column names properly"
      },
      {
        "slug": "framework-explicit-null-handling",
        "title": "Explicit null handling"
      },
      {
        "slug": "framework-name-indicates-clear-purpose",
        "title": "Name indicates clear purpose"
      },
      {
        "slug": "framework-optimize-for-code-readability",
        "title": "Optimize for code readability"
      },
      {
        "slug": "framework-optimize-loop-operations",
        "title": "Optimize loop operations"
      },
      {
        "slug": "framework-optimize-migration-code",
        "title": "Optimize migration code"
      },
      {
        "slug": "framework-precise-type-annotations",
        "title": "Precise type annotations"
      },
      {
        "slug": "framework-use-modern-phpunit-attributes",
        "title": "Use modern PHPUnit attributes"
      },
      {
        "slug": "framework-use-semantic-exceptions",
        "title": "Use semantic exceptions"
      }
    ],
    "comments": {
      "framework-escape-column-names-properly": [
        "One possible solution for https://github.com/laravel/framework/pull/54025#discussion_r1898460480\r\n```suggestion\r\n\r\n    protected static function isFunctionalExpression(string $column): bool\r\n    {\r\n        return preg_match('/\\(.+\\)/', $column);\r\n    }\r\n```",
        "https://github.com/laravel/framework/pull/54025#discussion_r1898688984 can be applied here, then:\r\n```suggestion\r\n            return self::isFunctionalExpression($column) ? $column : $this->wrap($column);\r\n```",
        "https://github.com/laravel/framework/pull/54025#discussion_r1898675719 got outdatedâ€”not sure if you still want to change that:\r\n```suggestion\r\n        $columns = collect($command->columns)\r\n            ->map(fn (string $column) => self::isFunctionalExpression($column) ? $column : $this->wrap($column))\r\n            ->implode(', ');\r\n```"
      ],
      "framework-optimize-loop-operations": [
        "You could prevent trailing iterations when calculating the result is done, speeding the process slightly more up:\r\n```suggestion\r\n        foreach ($map as $roman => $value) {\r\n            while ($number >= $value) {\r\n                $result .= $roman;\r\n                $number -= $value;\r\n            }\r\n            \r\n            if ($number === 0) {\r\n            \treturn $result;\r\n            }\r\n        }\r\n```"
      ],
      "framework-use-modern-phpunit-attributes": [
        "You could alternatively use [PHPUnit's attribute annotations](https://docs.phpunit.de/en/10.5/attributes.html#requiresphp) to achieve this if you like:\r\n```suggestion\r\n#[RequiresPhp('8.4.0')]\r\nclass DatabaseEloquentWithPropertyHooksTest extends TestCase\r\n{\r\n    protected function setUp(): void\r\n    {\r\n        parent::setUp();\r\n```",
        "Alternatively, you could use with attributes:\r\n```suggestion\r\n    #[RequiresPhp('^8.4')]\r\n    public function testHasCastsOnBcmathNumber()\r\n    {\r\n```\r\nor\r\n```suggestion\r\n    #[RequiresPhpExtension('bcmath')]\r\n    public function testHasCastsOnBcmathNumber()\r\n    {\r\n```\r\nor\r\n```suggestion\r\n    #[RequiresMethod(Number::class, '__construct')]\r\n    public function testHasCastsOnBcmathNumber()\r\n    {\r\n```",
        "You're welcomeâ€”always happy to introduce someone to something new ðŸ‘ðŸ» ",
        "You can use the [`#[DataProvider]` attribute](https://docs.phpunit.de/en/11.5/attributes.html#data-provider) here\r\n```suggestion\r\n    #[DataProvider('provideStrSanatizeTestStrings')\r\n    public function testSanitize(?string $subject, ?string $expected, ?HtmlSanitizerConfig $config)\r\n    {\r\n        $this->assertSame($expected, Str::sanitize($subject, $config));\r\n    }\r\n    \r\n    public static function provideStrSanatizeTestStrings()\r\n    {\r\n        return [\r\n            'non-empty string is returned as is' => ['Hello', 'Hello', null],\r\n            'stringified number' => [123, '123', null],\r\n            'null is returned as is' => [null, null, null],\r\n            'empty string is returned as is' => ['', '', null],\r\n            'XSS attack in string is sanitized' => ['Hello<script>alert(\"XSS\")</script>', 'Hello', null],\r\n            'XSS attack is sanitized to empty string' => ['<script>alert(\"XSS\")</script>', '', null],\r\n            'data attribute in HTML element is sanitized' => ['<span data-attr=\"foo\"></span>', '<span></span>', null],\r\n            'event handler in HTML element is sanitized' => ['<img src=\"https://laravel.com/does-not-exist.jpg\" onerror=\"alert(1)\" />', '<img src=\"https://laravel.com/does-not-exist.jpg\" />', null],\r\n            'inline JavaScript in HTML hyperlink element is sanitized' => ['<a href=\"javascript:alert(1)\">Click me</a>', '<a>Click me</a>', null],\r\n            'HTML image element is returned as is' => ['<img src=\"/does-not-exist.jpg\" />', '<img src=\"/does-not-exist.jpg\" />', (new HtmlSanitizerConfig)->allowElement('img', 'src')->allowRelativeMedias()],\r\n            'HTML image element src URL is rewritten to use TLS' => ['<img src=\"http://laravel.com/safe.jpg\" />', '<img src=\"https://laravel.com/safe.jpg\" />', (new HtmlSanitizerConfig)->allowElement('img', 'src')->forceHttpsUrls()],\r\n            'data attribute in HTML element is not sanitized' => ['<span data-attr=\"foo\"></span>', '<span data-attr=\"foo\"></span>', (new HtmlSanitizerConfig)->allowElement('span')->allowAttribute('data-attr', '*')],\r\n        ];\r\n    }\r\n```",
        "Alternatively, you can use the [`[#TestWith]` attribute](https://docs.phpunit.de/en/11.5/attributes.html#testwith):\r\n```suggestion\r\n    #[TestWith(['Hello', 'Hello', null])\r\n    #[TestWith([123, '123', null])\r\n    #[TestWith([null, null, null])\r\n    #[TestWith(['', '', null])\r\n    #[TestWith(['Hello<script>alert(\"XSS\")</script>', 'Hello', null])\r\n    #[TestWith(['<script>alert(\"XSS\")</script>', '', null])\r\n    #[TestWith(['<span data-attr=\"foo\"></span>', '<span></span>', null])\r\n    #[TestWith(['<img src=\"https://laravel.com/does-not-exist.jpg\" onerror=\"alert(1)\" />', '<img src=\"https://laravel.com/does-not-exist.jpg\" />', null])\r\n    #[TestWith(['<a href=\"javascript:alert(1)\">Click me</a>', '<a>Click me</a>', null])\r\n    #[TestWith(['<img src=\"/does-not-exist.jpg\" />', '<img src=\"/does-not-exist.jpg\" />', (new HtmlSanitizerConfig)->allowElement('img', 'src')->allowRelativeMedias()])\r\n    #[TestWith(['<img src=\"http://laravel.com/safe.jpg\" />', '<img src=\"https://laravel.com/safe.jpg\" />', (new HtmlSanitizerConfig)->allowElement('img', 'src')->forceHttpsUrls()])\r\n    #[TestWith(['<span data-attr=\"foo\"></span>', '<span data-attr=\"foo\"></span>', (new HtmlSanitizerConfig)->allowElement('span')->allowAttribute('data-attr', '*')])\r\n    public function testSanitize(?string $subject, ?string $expected, ?HtmlSanitizerConfig $config)\r\n    {\r\n        this->assertSame($expected, Str::sanitize($subject, $config));\r\n    }\r\n```",
        "You might want to use a [`#[DataProvider]`](https://docs.phpunit.de/en/10.5/attributes.html#data-provider) for this",
        "Nice, never used it but looks neat ðŸ‘ðŸ» ",
        "same here",
        "All of these arrays could alternatively be put into fixture files ðŸ¤” "
      ],
      "framework-cache-expensive-operations": [
        "Good point ðŸ‘ðŸ» I had a similar ideaâ€”looks like there's an appetite for this ðŸš€",
        "Thanks for the hintâ€”I'll adjust that ðŸ‘ðŸ»",
        "While it's not possible with `class_uses_recursive`, this could be taken one step further, by implementing something similar to `class_uses_recursive`: Caching all classes in the recursive chain ðŸ¤” "
      ],
      "framework-optimize-for-code-readability": [
        "Shouldn't this be handled _inside_ the `concatPathToUrl()` method?",
        "You could even shorten that, if you want:\r\n```suggestion\r\n        return match (true) {\r\n            empty($url) => trim($path, '/'),\r\n            empty($path) => trim($url, '/'),\r\n            default => rtrim($url, '/').'/'.ltrim($path, '/'),\r\n        };\r\n```",
        "Yeah, no problem ðŸ˜ƒ :+1:",
        "match statements with only two cases kinda defeat the purpose, I'd say\r\n```suggestion\r\n        $originalAttribute = $this->replacePlaceholderInString($attribute);\r\n\r\n        $attribute = $rule instanceof Rules\\File\r\n            ? $attribute\r\n            : $this->replacePlaceholderInString($attribute);\r\n```",
        "This can be just\r\n```suggestion\r\n                ->map('base_path')\r\n```\r\nor\r\n```suggestion\r\n                ->map(base_path(...))\r\n```",
        "This can just be\r\n```suggestion\r\n            ->map([ $this, 'qualifyColumn' ])\r\n```\r\n\r\nor\r\n```suggestion\r\n            ->map($this->qualifyColumn(...))\r\n```",
        "This can be simplified to just\r\n```suggestion\r\n            ->map([ $this, 'parseJsonPathArrayKeys' ])\r\n```\r\n\r\nor\r\n```suggestion\r\n            ->map($this->parseJsonPathArrayKeys(...))\r\n```",
        "You might be able to use first-class callable syntax here:\r\n```suggestion\r\n            return collect($value)->map(deepCollect(...));\r\n```",
        "You might want to use early returns here:\r\n```suggestion\r\n    public function scopeFilter(Builder $builder, array $params): void\r\n    {\r\n        if (!is_array($params) || $params === []) {\r\n        \treturn;\r\n        }\r\n\r\n        foreach ($params as $class => $methodOrValue) {\r\n            $className = $this->filterNamespace . ucfirst($class);\r\n\r\n            if (!class_exists($className)) {\r\n\t\t\t\tcontinue;\r\n            }\r\n\r\n            if (method_exists($className, $methodOrValue)) {\r\n                $className::{$methodOrValue}($builder);\r\n            }\r\n\r\n            if (method_exists($className, 'apply')) {\r\n                $className::apply($builder, $methodOrValue);\r\n            }\r\n        }\r\n    }\r\n```",
        "You can avoid intermediate variables:\r\n```suggestion\r\n        $result = Str::of($expression)\r\n        \t->replace('(', '')\r\n        \t->replace(')', '')\r\n        \t->explode(',');\r\n```",
        "You could use first-class callable syntax here:\r\n```suggestion\r\n            $this->prepareRule(...),\r\n```"
      ],
      "framework-design-flexible-apis": [
        "What about just wrapping the callback? This would be following the original train of thought of this PR:\r\n```diff\r\npublic function stubUrl($url, $callback)\r\n{\r\n    return $this->fake(function ($request, $options) use ($url, $callback) {\r\n        if (! Str::is(Str::start($url, '*'), $request->url())) {\r\n            return;\r\n        }\r\n\r\n+        if (is_numeric($callback) || is_string($callback) || is_array($callback) {\r\n+            $callback = static::response($callback);\r\n+        }\r\n\r\n        if ($callback instanceof Closure || $callback instanceof ResponseSequence) {\r\n            return $callback($request, $options);\r\n        }\r\n-\r\n-        if (is_numeric($callback) || is_string($callback) || is_array($callback)) { // here\r\n-            return static::response($callback);\r\n-        }\r\n\r\n        return $callback;\r\n    });\r\n}\r\n```",
        "Oh, I misread `ResponseSequence` as `Response` ðŸ¤¦ðŸ»â€â™‚ï¸ Your suggestion makes more sense then ðŸ˜… ",
        "> I've added all the changes except your enum idea because I merged one in early by accident and I can't merge that in now due to it being classed as outdated. Could you suggest that change again and I'll pull that in\r\n\r\nRe-adding the enum suggestion from https://github.com/laravel/framework/pull/54464#discussion_r1941450711 as requested per https://github.com/laravel/framework/pull/54464#issuecomment-2634468189\r\n```suggestion\r\n     * @param  int-mask-of<PHP_QUERY_*>|HttpQueryEncoding $encodingType (optional) Query encoding type.\r\n     * @return string\r\n     */\r\n    public static function query($array, $encodingType = HttpQueryEncoding::Rfc3986)\r\n    {\r\n        return http_build_query($array, '', '&', enum_value($encodingType))\r\n```",
        "Please keep in mind, that you have to manually\r\n* import `enum_value` via `use function Illuminate\\Support\\enum_value;`\r\n* create said enum as\r\n  ```php\r\n  <?php\r\n\r\n  namespace Illuminate\\Http\\Request\\Enums;\r\n\r\n  enum HttpQueryEncoding: int\r\n  {\r\n      case Rfc3986 = PHP_QUERY_RFC3986;\r\n      case Rfc1738 = PHP_QUERY_RFC1738;\r\n  }\r\n  ```"
      ],
      "framework-use-semantic-exceptions": [
        "Is `0` helpful here? If I can't generate a _random_ integer, I'd expect to get an exception thrown (maybe [`Random\\RandomException`](https://www.php.net/manual/en/class.random-randomexception.php) or [`Random\\RandomError`](https://www.php.net/manual/en/class.random-randomerror.php) ðŸ¤”)",
        "Also, doesn't `random_int()` handle validation itself?",
        "If a function doesn't throw an exception, I'd at least expect it to return `null` or `false`, since `0` is a number but it's not random, so this can be quite confusing",
        "Wouldn't it make more sense to throw a different exception (like [`\\Ramsey\\Uuid\\Exception\\InvalidUuidStringException`](https://github.com/ramsey/uuid/blob/4.x/src/Exception/InvalidUuidStringException.php)) as you suggested?\r\n\r\n> Create an entirely new exception for this case. I'd be happy to pivot to this if it makes more sense for the framework.\r\n\r\nI mean, the whole point is, that it's not about that the model isn't found.\r\n\r\n> Throw a ValidationException instead. Something like ValidationException::withMessages([$field => '???']). I'm not sure what the message should be in that case.\r\n\r\nI don't think, we should mix the validation functionality of the framework with the rest of the framework. A dedicated exception like in your first suggestion sounds better",
        "> Went with a new exception, `InvalidIdFormatException`. The problem with using `InvalidUuidStringException` is that any time a user requests an endpoint with an invalid UUID, it would be reported (to Sentry, DataDog, et cetera) which isn't desirable.\r\n\r\nAgreed. A new exception sounds good ðŸ‘ðŸ»  Also, this way, we are less dependent on `ramsey/uuid` (I'm remembering what chaos the introduction of the lazy classes into `ramsey/uuid` caused for some who used the package).\r\n\r\n> By creating a more generic exception, it saves users from having to follow the additional steps to avoid eating up their exception reporting limits.\r\n\r\nMy only problem now with this is that the naming is too generic for the implementation. For the implementation to match the level of genericness the name has, it would have to have some kind of property holding the format or a description of it."
      ],
      "framework-descriptive-configuration-keys": [
        "According to your config doc block,\r\n> number of minutes\r\n\r\nthis is now minutes that are added\r\n```suggestion\r\n        $expiresAt = Carbon::now()->addMinutes(config('session.maintenance_bypass_cookie_lifetime'));\r\n```",
        "Accoding to your PHPDoc block,\r\n> number of minutes\r\n\r\nbut 7720 minutes is not 12 hours\r\n```suggestion\r\n    /*\r\n    |--------------------------------------------------------------------------\r\n    | Maintenance Bypass Cookie Lifetime\r\n    |--------------------------------------------------------------------------\r\n    |\r\n    | Here you may specify the number of minutes the maintenance mode\r\n    | bypass cookie should remain valid. This cookie is issued when a\r\n    | user accesses the application using the secret bypass URL. Once\r\n    | set, it allows temporary access to the application while it is\r\n    | in maintenance mode. The default duration is 12 hours.\r\n    |\r\n    */\r\n\r\n    'maintenance_bypass_cookie_lifetime' => (int) env('SESSION_MAINTENANCE_BYPASS_COOKIE_LIFETIME', 720),\r\n```",
        "Maybe, to avoid confusion when using, the unit should be included in the key:\r\n```suggestion\r\n    /*\r\n    |--------------------------------------------------------------------------\r\n    | Maintenance Bypass Cookie Lifetime\r\n    |--------------------------------------------------------------------------\r\n    |\r\n    | Here you may specify the number of minutes the maintenance mode\r\n    | bypass cookie should remain valid. This cookie is issued when a\r\n    | user accesses the application using the secret bypass URL. Once\r\n    | set, it allows temporary access to the application while it is\r\n    | in maintenance mode. The default duration is 12 hours.\r\n    |\r\n    */\r\n\r\n    'maintenance_bypass_cookie_lifetime_minutes' => (int) env('SESSION_MAINTENANCE_BYPASS_COOKIE_LIFETIME_MINUTES', 720),\r\n```"
      ],
      "framework-explicit-null-handling": [
        "You could just directly check this\r\n```suggestion\r\n        if ($reflector->getAttributes(Lazy::class) !== []) {\r\n```",
        "Not much. Technically, a function call is more expensive than a comparison, but this should mostly be optimized away in practice. One _slight_ advantage would be a clear communication that we are working with an array here and don't have to check _any_ structure's emptiness.",
        "Wouldn't this suffice?\r\n```suggestion\r\n        $name = (string) ($this->argument('name') ?? $choice);\r\n```",
        "You can avoid the function call like this:\r\n```suggestion\r\n        if ($alias === null) {\r\n```",
        "According to your `is_null()` call, your string can be `null`. This should be reflected in the PHPDoc comment type as well:\r\n```suggestion\r\n     * @param  string|null  $string  The input string to sanitize.\r\n     * @param  HtmlSanitizerConfig|null  $config  Custom configuration to use for sanitizing.\r\n     * @return string The sanitized string.\r\n     */\r\n    public static function sanitize($string, ?HtmlSanitizerConfig $config = null)\r\n    {\r\n        if ($string === null) {\r\n```",
        "According to your `is_null()` call, the method can return `null`. This should be reflected in the PHPDoc comment type as well:\r\n```suggestion\r\n     * @return string|null The sanitized string.\r\n     */\r\n    public static function sanitize($string, ?HtmlSanitizerConfig $config = null)\r\n    {\r\n        if (is_null($string)) {\r\n            return null;\r\n```"
      ],
      "framework-precise-type-annotations": [
        "Since `prefix()` expects a `string`, so this can be reflected here:\r\n```suggestion\r\n     * @param  array<string, mixed>  $values\r\n```",
        "The `array` will either consist of strings or be empty, so we can narrow this further as well:\r\n```suggestion\r\n     * @return string[]\r\n```",
        "Ah, damn, sorry, hadn't submitted it and didn't see that it was closed when I hit the 'submit' button an hour later ðŸ˜…",
        "Shouldn't this stay\r\n```suggestion\r\n     * @return \\Illuminate\\Support\\Collection<int, TPivotModel>\r\n```\r\n? If there is trouble with static analysis, the [PHPDoc comment in `InteractsWithPivotTable`](https://github.com/laravel/framework/blob/12.x/src/Illuminate/Database/Eloquent/Relations/Concerns/InteractsWithPivotTable.php#L496) should be adjusted accordingly",
        "This is not primarily to be read by humans but by tools and for them `mixed` and your proposed alternative make a **huge** difference. If you want to make it more bite-sized, you can always resort to `@phpstan-type` or the like",
        "Therefore, you can narrow the return here as well:\r\n```suggestion\r\n     * @return int-mask-of<JSON_*>\r\n```",
        "Wouldn't this have sufficed?\r\n```suggestion\r\n     * @return (\\Illuminate\\Routing\\Controllers\\Middleware|\\Closure|string)[]\r\n```",
        "Also, is `string` the closest we can get? Have you tried `callable`?\r\n```suggestion\r\n     * @return array<int,\\Illuminate\\Routing\\Controllers\\Middleware|callable>\r\n```",
        "That would actually be great for such cases ðŸ˜‚ðŸ‘ðŸ» ",
        "This could be improved to read:\r\n```suggestion\r\n     * @param  string|string[]  $columns  The column(s) to include in the index.\r\n```"
      ],
      "framework-name-indicates-clear-purpose": [
        "Wouldn't it make sense to use a prefix here to indicate the return value? Because, I would expect the method to convert it from array-like values to `Arrayable` instances\r\n```suggestion\r\n    public static function isArrayable($value)\r\n```",
        "Ah, thanks for the explanation ðŸ‘ðŸ» ",
        "\"number\" might be misleading here:\r\n```suggestion\r\n     * Generate a random integer of the given length.\r\n```",
        "Even though, you are using `strlen()` here, I'd say, \"digits\" is the more correct term when dealing with numbers:\r\n```suggestion\r\n     * Generate a random number with the given amount of digits.\r\n     *\r\n     * @param  int  $digits\r\n     * @return int\r\n     */\r\n    public static function random(int $digits = 6): int\r\n    {\r\n        $maxDigits = strlen((string) PHP_INT_MAX);\r\n\r\n        if ($digits < 1 || $digits > $maxDigits) {\r\n            return 0;\r\n        }\r\n\r\n        $min = 10 ** ($digits - 1);\r\n        $max = (10 ** $digits) - 1;\r\n\r\n        if ($digits == $maxDigits) {\r\n```"
      ],
      "framework-optimize-migration-code": [
        "Function call can be avoided like this:\r\n```suggestion\r\n        $migrations = isset($options['selected']) && $options['selected'] !== [] ? $options['selected'] : $this->getMigrationsForRollback($options);\r\n```",
        "Same here:\r\n```suggestion\r\n        if ($selected !== []) {\r\n```",
        "You might be able to replace it with this:\r\n```suggestion\r\n            ->keyBy($this->getMigrationName(...))\r\n```"
      ]
    }
  },
  "ololobus": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-adaptive-cache-expiration-strategy",
        "title": "Adaptive cache expiration strategy"
      },
      {
        "slug": "neon-avoid-flaky-tests",
        "title": "Avoid flaky tests"
      },
      {
        "slug": "neon-cache-performance-preservation",
        "title": "Cache performance preservation"
      },
      {
        "slug": "neon-clear-consistent-identifier-names",
        "title": "Clear consistent identifier names"
      },
      {
        "slug": "neon-comprehensive-code-documentation",
        "title": "Comprehensive code documentation"
      },
      {
        "slug": "neon-configuration-context-alignment",
        "title": "Configuration context alignment"
      },
      {
        "slug": "neon-design-metrics-for-insights",
        "title": "Design metrics for insights"
      },
      {
        "slug": "neon-document-api-specs-completely",
        "title": "Document API specs completely"
      },
      {
        "slug": "neon-document-connection-transitions",
        "title": "Document connection transitions"
      },
      {
        "slug": "neon-document-parameter-choices",
        "title": "Document parameter choices"
      },
      {
        "slug": "neon-flexible-documented-configurations",
        "title": "Flexible documented configurations"
      },
      {
        "slug": "neon-hierarchical-semantic-naming",
        "title": "Hierarchical semantic naming"
      },
      {
        "slug": "neon-keep-files-focused-small",
        "title": "Keep files focused small"
      },
      {
        "slug": "neon-log-level-appropriately",
        "title": "Log level appropriately"
      },
      {
        "slug": "neon-proactive-cache-warming",
        "title": "Proactive cache warming"
      },
      {
        "slug": "neon-proper-metrics-design",
        "title": "Proper metrics design"
      },
      {
        "slug": "neon-scope-jwt-authentication-tokens",
        "title": "Scope JWT authentication tokens"
      },
      {
        "slug": "neon-stage-configuration-changes-gradually",
        "title": "Stage configuration changes gradually"
      },
      {
        "slug": "neon-use-descriptive-identifiers",
        "title": "Use descriptive identifiers"
      }
    ],
    "comments": {
      "neon-stage-configuration-changes-gradually": [
        "I think it shouldn't be a part of feature flags, which are meant to be temporary. Instead, it should be a part of the main spec body because it's a permanent feature/mode\r\n\r\n(This `ActivityMonitorExperimental` is a bit special, I left it for longer, because statistics-based monitoring is tricky, but we should also remove it already)"
      ],
      "neon-design-metrics-for-insights": [
        "It usually makes sense to track 2 parameters out of 3: total, failed, success, so that you can always reconstruct all 3. Because just number of requests doesn't tell us much, we care more about success/error rates. Could be a separate PR, up to you"
      ],
      "neon-keep-files-focused-small": [
        "NIT: this `compute.rs` is really huge already. Should we move all new method implementations and structs into `compute_prewarm.rs`? It will be pretty well-scoped and should improve navigation and readability. WDYT?\r\n\r\nPersonally, I was already struggling with the `compute.rs` size",
        "> It seems like we have a good pattern going where the ComputeNode methods become small wrappers around functions in other files.\r\n\r\nThis could be the way to go as well, but I was more thinking about just having a second `impl` block like\r\n```rust\r\nimpl ComputeNode {\r\n    pub async fn prewarm_status(&self) -> PrewarmStatus {...}\r\n\r\n    // The rest of prewarm methods\r\n}\r\n\r\n// The rest of prewarm structs\r\n```\r\nin a separate file `compute_prewarm.rs`. If that's possible in Rust (afaik, it's)"
      ],
      "neon-proactive-cache-warming": [
        "Makes sense, @MMeent can you suggest what the raw numbers should be? Like `target_lfc_size_pages` and `processed_lfc_pages`? Will it work?",
        "Added, thanks",
        "I mean that we just start primary from scratch with empty caches, the only option to improve the situation is to do async auto-prewarm, while already accepting new connections. So from cplane perspective, it's just a normal start from with auto-prewarm enabled",
        "Well, that's debatable whether we need auto-prewarm or not at all. It exists in vanilla Postgres. The idea is that we can prewarm caches faster when we do it intentionally vs. when user tries to prewarm by just doing their normal workload\r\n\r\nImagine cplane, it eventually accesses all non-deleted projects/endpoint/branches. If we just restart it at Neon, it will take some time to visit all objects. Yet, if we actively prewarm caches in the brackground, the chance that next project read will hit the cache will be higher, as it will be already there, even though cplane hasn't read it explicitly\r\n\r\nIn practice, it may not suite all workloads, but we cannot answer for sure until we implement it and battle-test, but imo it exists in vanilla Postgres for a reason, so there are use-cases",
        "Well, seconds is just a unit, it can be set to 5, 15 minutes. For cplane-orchestrated there is a separate API, I imagined periodic dumping to be useful for\r\ni) auto-prewarm, i.e. compute periodically dumps LFC content, so later it can be used at restart. In theory, we can only dump at graceful shutdown, but then it won't help with accidental compute crash/restart, as there might be no LFC state to warm up from\r\nii) later for having a hot standby, i.e. it can periodically fetch fresh content from S3 and do prewarming\r\n\r\nI don't want to wire too complex logic via cplane, so TBH, I don't see other options to have a robust auto-prewarm without periodic dumping of the LFC state, pg_prewarm does the same via `pg_prewarm.autoprewarm_interval`\r\n\r\n@MMeent @knizhnik do you have any specific suggestions of how we can implement it without periodic dumping?",
        "Thanks for the comment about hot standby\r\n\r\n> We can make it part of endpoint shutdown procedures?\r\n\r\nYes, this is what I meant by 'In theory, we can only dump at graceful shutdown'. That'd work in most of the cases, but what I don't like is that it doesn't cover any abnormal termination like OOM, VM restarts, etc.\r\n\r\nWith your cost estimation, dumping every 5 minutes is completely reasonable",
        "> Note that \"i.e. it can periodically fetch fresh content from S3 and do prewarming\" won't work as you seem to expect it to, as prewarming is explicitly designed to never evict existing pages from LFC, and thus won't do much if the size of LFC doesn't change for the hot standby. It's prewarming by replacing unused pages with potentially useful pages, and explicitly not LFC state synchronization.\r\n\r\nRe-reading it after a long time and now it still looks like it should work. Like\r\n\r\n1. We did prewarm once\r\n2. After some time, we fetch LFC content again and iterate over blocks to check if they are present in the LFC\r\n2.1. If block is in LFC -- good, WAL replay should keep it up-to-date\r\n2.2. If it's not in LFC -- we will fetch it from pageserver\r\n\r\nThat way, we do not need any eviction explicitly, and it will help with keeping the LFC relatively warm. Not saying that we need to do it exactly like that, I like you suggestion with switching the replay mode and replaying all pages, it's just this could be a viable alternative\r\n\r\nOr do I miss something?",
        "I was actually thinking about using the default that pg_prewarm uses -- 300s. I think it's frequent enough for this purpose. This will lower it it to ~$40 per 1k computes per month, which is good enough, imo",
        "Mentioned this default explicitly, thanks for the estimation"
      ],
      "neon-avoid-flaky-tests": [
        "Any sleep-based waiting in tests almost certainly causes flakiness. Please, rewrite it into waiting for the LFC content to appear in the remote storage. There is a generic helper in python tests for that -- `wait_until()`, see usage in other tests"
      ],
      "neon-log-level-appropriately": [
        "NIT: I'd reverse this and instead log 'Skipping pgbouncer and local_proxy termination because in dev mode' when dev_mode is true. Otherwise, we log this in real envs, but it doesn't make any sense as we log separate line when we actually send signals"
      ],
      "neon-document-connection-transitions": [
        "Primary in the diagram is the old primary, and it's shut down first, it's also mentioned in text. Or what do you mean?",
        "My intent was to keep it reasonably high-level. Do you see some important interactions missing here?",
        "This step is right after we terminate the primary, so yes, during normal termination, we can expect that at this moment primary will be already terminated and all connections to it will be closed.\r\n\r\nI added this item after talking to Stas, as he had a fair point that the old primary could be unresponsive during this promotion flow, so we will send termination and k8s resources deletion requests, but we cannot generally guarantee that it will be dead by this time. So this step is more to protect from the situation, when old connections will still be connected to the old primary\r\n\r\nSee also item 7 in failure modes. I'm not quite sure how big the problem is. Safekeepers will guarantee that there is only one running writer at a time, so it's more like a nice-to-have, than must-have feature, just to prevent unnecessary interference and side effects (I worried about some stale reads from the old primary and failing writes because safekeepers should reject them)"
      ],
      "neon-configuration-context-alignment": [
        "It should default to false",
        "@knizhnik I still see that it defaults to true"
      ],
      "neon-document-parameter-choices": [
        "Can you please add test comments clarifying what parameters mean and what are the different test modes? After quickly eyeballing the test, I cannot easily grasp the `with_compute_ctl` and why we pass `ids` as a test parameter"
      ],
      "neon-adaptive-cache-expiration-strategy": [
        "Replied here https://github.com/neondatabase/neon/pull/11294#discussion_r2006276800 and here https://github.com/neondatabase/neon/pull/11294#discussion_r2006283261 as well\r\n\r\n> Wondering if there's any interactions between the user workload and prewarm that are worth considering.\r\n> if it's available then the LFC cache might have become stale over time.\r\n\r\nThis PR https://github.com/neondatabase/neon/pull/10442 introduced additional locking when accessing LFC, so it's now considered safe to write there concurrently, so that's the base for all this work.\r\n\r\nDuring prefetch, we always request the latest pages from the pageserver. If, after loading page gets modified, then it will be either updated in LFC (in case of primary) or evicted from LFC after receiving the corresponding WAL record (in case of replica). In other words, if pages is not present in the LFC, we will fetch it from the pageserver; if someone (backend, normal client workload) tries to write it concurrently, then the access will be synchronized, and we should still get a freshness guarantee. @knizhnik or @MMeent can correct me, as I'm not fluent in the underlying mechanism, I consider it as given here\r\n\r\nThus, it should be safe to prewarm LFC concurrently with user load. The only problem is performance, I wrote about it in other comments, but anyway. Yes, if it's highly intensive workload, then prewarm can compete for storage resources with user workload, so we can consider auto-prewarm to be user-togglable feature, I wrote about it in the section about auto-prewarm concerns\r\n\r\n@VladLazar @mtyazici let me know if it makes it clearer",
        "> Logical Replication\r\n\r\nI recall that Konstantin did a POC like that. We discarded that because it only helps with keeping a warm replica, and it's not possible to implement autoprewarm with that + it bloats the WAL on safekeepers and eats the network bandwidth. It's not a big problem, as Pageservers should discard such records during ingestion, so it wont bloat the data files, but it's still nice to avoid\r\n\r\n> The idea here is to expose some LFC primitives at the SQL level on the primary. The API should allow for fetching the current state of the LFC in a way that it can be reproduced\r\n\r\nI don't remember that we considered any diffing, but otherwise it's pretty much how it works -- we have SQL funcs to dump/load caches state",
        "If we consider this compute data as non-critical, could we avoid explicit deletion completely? I was thinking about setting a TTL for perfix/bucket https://docs.aws.amazon.com/AmazonS3/latest/userguide/how-to-set-lifecycle-configuration-intro.html (never used it personally, though)\r\n\r\nThat should most likely work for prewarm/caches content. Assuming we set it to a high enough value (like 7d or 30d), if one doesn't start endpoint for that long, they likely don't care about prewarming much. For `pg_stat_statements` it's pretty much the same -- well, your perf data will expire after N days -- sounds fair. For stats it could be a bit more annoying, but again should be not critical at all\r\n\r\nAt the same time, with TTL we avoid implementing a huge piece of deletion orchestration.\r\n\r\nWhat do you think?",
        "Explicit deletion would work as well, I think, just more work on the control plane side"
      ],
      "neon-cache-performance-preservation": [
        "Not a fan of wiring even more stuff via cplane. The flushLSN should be just the last 'consensus' LSN from sefekeepers, right? Cannot compute figure out it on its own before/during promotion (kinda incomplete sync-safekeepers, just without data copying, or we can even do a normal sync-safekeepers on compute before promotion)?\r\n\r\nThat'd be much more robust and less bug-prone because it doesn't put any implicit assumptions that someone passes the right LSN to us"
      ],
      "neon-proper-metrics-design": [
        "I think it's not considered a best-practice, in the docs it's formulated like\r\n\r\n> As a rule of thumb, either the sum() or the avg() over all dimensions of a given metric should be meaningful (though not necessarily useful). If it is not meaningful, split the data up into multiple metrics.\r\n\r\nhttps://prometheus.io/docs/practices/naming/ (see other suggestions there)\r\n\r\nI suggest you split it into two separate metrics with a clear meaning"
      ],
      "neon-clear-consistent-identifier-names": [
        "NIT, but it impacts readability a lot -- `state.state` -- what state of what state? I suggest making it more clear\r\n\r\n```suggestion\r\n        compute: &Arc<ComputeNode>,\r\n```"
      ],
      "neon-use-descriptive-identifiers": [
        "Am I right that instead of these cryptic `prewarm_info[n]` you can use here `total, prewarmed, skipped` defined above?"
      ],
      "neon-flexible-documented-configurations": [
        "I wonder, should we do the same for compute_ctl connections? Especially activity monitor, it runs a bunch of queries pretty often. It probably should be enough to put this option here https://github.com/neondatabase/neon/blob/24d7c37e6ee7b730f983487351721f40922a9745/compute_tools/src/compute.rs#L362",
        "There is a TODO two lines above the place I've linked\r\nhttps://github.com/neondatabase/neon/blob/24d7c37e6ee7b730f983487351721f40922a9745/compute_tools/src/compute.rs#L358-L360\r\n\r\nI was thinking about passing all essential parameters from the compute_ctl without relying on control plane.\r\n\r\nWe can probably still keep an option for control plane to override, not sure if reversing the order here\r\n\r\nSome(options) => format!(\"{} {}\", options, EXTRA_OPTIONS)\r\n\r\nto\r\n\r\nSome(options) => format!(\"{} {}\", EXTRA_OPTIONS, options)\r\n\r\nwill work\r\n",
        "> And then the pseudocode that you wrote actually already exists at\r\n\r\nYeah, but I meant that we should swap `options, EXTRA_OPTIONS` if we want cplane values to take precedence"
      ],
      "neon-scope-jwt-authentication-tokens": [
        "I think we need to add endpoint_id to the token. It won't hurt to have this extra protection to ensure that endpoints cannot write to each other sub-paths. Any problems with adding it?",
        "@myrrc looks mostly good to me, thanks, I only have minor comments. I suggest we put it into other PR -- https://github.com/neondatabase/neon/pull/9661 as it belongs to the unlogged storage/S3 proxy RFC, not just to prewarm flow specifically\r\n\r\ncc @MMeent ",
        "If we make prefix like `/epufs/tenants/{tenant_id}/{endpoint_id|any_other_lower_level_key}/...`, we could decide whether to use tenant or tenant+endpoint pair. I think that from the security standpoint, the tenant should be enough as the tenant is our level of multi-tenancy, and we use it for storage auth already",
        "This info was added in another section, so resolving",
        "I think we could elaborate on that, i.e. that we will use JWTs with tenant+timeline IDs, which both provides good tenants isolation and adds an additional protection layer for different timelines to do not mess with each other data "
      ],
      "neon-document-api-specs-completely": [
        "Let's provide a brief API spec for this EPUFS service, i.e. what are the methods and parameters we are going to have:\r\n- PUT: tenant, timeline, endpoint, relative path, data -> json response\r\n- GET: tenant, timeline, endpoint, relative path -> file content response\r\n- DELETE: tenant [ timeline [ endpoint ] ] -> json response"
      ],
      "neon-comprehensive-code-documentation": [
        "> endpoint_id is set to None while prewarming from other endpoint, see replica promotion\r\n\r\nThis doesn't sound right. When we promote we should prewarm from another endpoint, so endpoint_id should **not** be None, right?",
        "@myrrc, please, do not merge incorrect code (including comments) into `main` with the hope of fixing it in another PR. Another PR may never happen, it might be delayed for an arbitrary amount of time, you can forget, etc.",
        "My understanding is that the difference between /// and !// is that the former applies to the following block, while the latter applies to the upper, which is frequently used for the top-level comments for the crate/module. Here, the comment applies to the struct, so /// seems applicable, or do I miss something?"
      ],
      "neon-hierarchical-semantic-naming": [
        "As discussed, I'd use `compute_pg_` prefix to indicate that it comes from Postgres. Same for both metrics",
        "I think we should drop `min` from the metric. Yes, in the view it's min_mxid across all tables in this DB (same about `frozen_xid`, note PG naming consistency), but when you take age() and sort DESC you actually get oldest as it's properly mentioned in the description.\r\n\r\nSo I guess at the end metrics could be named like\r\n- compute_pg_oldest_frozen_xid_age\r\n- compute_pg_oldest_mxid_age\r\n\r\nor something"
      ]
    }
  },
  "dnr": {
    "repos": [
      "temporalio/temporal"
    ],
    "entries": [
      {
        "slug": "temporal-consistent-naming-patterns",
        "title": "Consistent naming patterns"
      },
      {
        "slug": "temporal-context-aware-network-calls",
        "title": "Context-aware network calls"
      },
      {
        "slug": "temporal-design-stable-apis",
        "title": "Design stable APIs"
      },
      {
        "slug": "temporal-dynamic-config-usage-principles",
        "title": "Dynamic config usage principles"
      },
      {
        "slug": "temporal-ensure-deterministic-execution",
        "title": "Ensure deterministic execution"
      },
      {
        "slug": "temporal-minimize-code-nesting-depth",
        "title": "Minimize code nesting depth"
      },
      {
        "slug": "temporal-minimize-credential-data-exposure",
        "title": "Minimize credential data exposure"
      },
      {
        "slug": "temporal-names-reflect-precise-behavior",
        "title": "Names reflect precise behavior"
      },
      {
        "slug": "temporal-optimize-api-consumption",
        "title": "Optimize API consumption"
      },
      {
        "slug": "temporal-precompute-and-cache",
        "title": "Precompute and cache"
      },
      {
        "slug": "temporal-protocol-buffer-organization",
        "title": "Protocol buffer organization"
      },
      {
        "slug": "temporal-safe-lock-usage-patterns",
        "title": "Safe lock usage patterns"
      },
      {
        "slug": "temporal-specific-assertion-methods",
        "title": "Specific assertion methods"
      },
      {
        "slug": "temporal-structured-contextual-logging",
        "title": "Structured contextual logging"
      },
      {
        "slug": "temporal-trust-getx-accessors",
        "title": "Trust GetX accessors"
      },
      {
        "slug": "temporal-use-dedicated-configuration-files",
        "title": "Use dedicated configuration files"
      }
    ],
    "comments": {
      "temporal-design-stable-apis": [
        "actually nevermind.. after reading more I think this should return a slice of counts, one per subqueue. taskQueueDB should work in terms of subqueues only, it shouldn't know anything about their semantics, like priority"
      ],
      "temporal-minimize-credential-data-exposure": [
        "The TLS fields in here seem like they could get pretty big. Does it really make sense to pass full client cert chains (keeping in mind that they've already been validated here)? If not, should we maybe copy this struct and clear some of the larger and less useful fields?\r\n\r\n`TLSSubject.Names` is probably not useful since the common ones are parsed into the fields above it (and of course everyone only uses CommonName)\r\n\r\n`TLSConnection.State.PeerCertificates`, `VerifiedChain`, etc. could be quite large and probably were not intended to be encoded as JSON\r\n\r\nDo you have an example of what a typical value looks like in JSON for TLS and non-TLS?\r\n"
      ],
      "temporal-minimize-code-nesting-depth": [
        "sure"
      ],
      "temporal-consistent-naming-patterns": [
        "oh, I meant that rename comment as a joke, I don't think we should rename it, it's better (more readable, less confusing) to be consistent with the name in workflowservice.",
        "Yeah.. I did it this way since several of the \"id\" fields are already there and I wanted to reuse them instead of having a duplicate id field. I think with a few wrapper functions we can minimize the risk of mistakes. Also it's more efficient to inline it instead of another message with just two fields.\r\n\r\nI'm like 80% sure I want to do it this way. Maybe let's reconsider after the whole first batch of PRs?",
        "I didn't want to rename the existing fields because of compatibility, but actually that only matters for proto-json which we're not using internally, so I'll rename them.",
        "On the idea of a submessage: messages with the new fields will be persisted in a different table, so it's not about compatibility per se. Though I would like to be able to use the same messages for both tables.\r\n\r\nFor AllocatedTaskInfo: we have a lot of those in memory and in db so I want to pay attention to efficiency there. I think it makes sense to just inline the two fields.\r\n\r\nFor SubqueueInfo: this is persisted, but not that many of them, so we could use a new field.\r\n\r\nFor InternalTaskQueueStatus: yes, could use a new field.\r\n\r\nI really hate having to do an extra allocation and chase another pointer just for these values. If we were using gogo it could be non-nullable and embedded, or we could just write custom getters and setters to avoid misuse :(\r\n\r\nLet me try it and see how it looks.",
        "I tried it. Mostly it's about the same readability since we still need wrappers to convert to/from structs. Some parts are slightly clearer. Overall I'd say slight improvement for readability. Adapting the existing code took `6 files changed, 74 insertions(+), 73 deletions(-)`  But it reduces the diff from main slightly.\r\n\r\nOverall I'm unhappy about the tradeoff but I'll go with it.",
        "```suggestion\r\n    temporal.api.workflowservice.v1.RecordWorkerHeartbeatRequest api_request = 2;\r\n```\r\n?\r\ndoesn't matter, just wondering",
        "also usually it's just called `request`"
      ],
      "temporal-use-dedicated-configuration-files": [
        "Can we put a comment here that says to not remove this, and add ignores to `proto/internal/buf.yaml` instead?",
        "don't do this, add lines here instead:\r\nhttps://github.com/temporalio/temporal/blob/main/proto/internal/buf.yaml#L14\r\n",
        "I know, but using that file is a better way to disable the check temporarily:\r\n- it's scoped to specific files instead of everything\r\n- if you forget to re-enable it, someone will notice eventually\r\n- keeps git history of the makefile cleaner"
      ],
      "temporal-precompute-and-cache": [
        "this is parsing the query on every evaluation? shouldn't that happen upfront in newWorkerQueryEngine?",
        "Yes, `&cv` would allocate a new ConstrainedValue, which would defeat the caching. This returns a pointer into the given slice, which comes directly from the Client. I'll add a comment",
        "Ideally we would do the exclusion before the call to get the dynamic config, to avoid that call if we're excluding anyway",
        "It'll be less expensive after https://github.com/temporalio/temporal/pull/7052 that caches conversions"
      ],
      "temporal-context-aware-network-calls": [
        "Would it be possible to do all the headers in one call to `metadata.AppendToOutgoingContext`?",
        "Can you use NewRequestWithContext or WithContext to make the http call context-aware? "
      ],
      "temporal-dynamic-config-usage-principles": [
        "we don't actually need a dynamic config here since we're not going to modify it in a real environment, just unit tests, so we can do it like:\r\n```suggestion\r\n    RateLimiterRefreshInterval: time.Minute,\r\n```\r\nand get rid of the dynamic config setting. then just set it to another value in the test.",
        "Maybe say a little more here about what happens when the percentage is above or below this threshold?",
        "But the setting has a single meaning and a specific effect.. the docstring should describe those. And the name should too, ideally. There shouldn't be multiple consumers with different semantics for the same setting.\r\n\r\nAnyway, 0.05 isn't \"the percentage of hosts that are not yet ready to serve traffic\", the percentage of hosts not ready to serve traffic is usually 0%, sometimes 10%, sometimes 50%, etc. The setting is a threshold for some behavior to change based on that value.",
        "should be dynamic config? (also to enable/disable)",
        "don't take the whole Collection here, take a dynamicconfig.DurationPropertyFn (just `func () time.Duration`). that makes it easier to test and also more efficient (by a tiny bit).\r\n\r\nyou can use dynamicconfig.GetDurationPropertyFn to create a static one for tests"
      ],
      "temporal-optimize-api-consumption": [
        "let's factor out:\r\n```\r\ncall() {\r\n  curl -fL -X POST -H \"Accept: application/vnd.github.v3+json\" -H \"Authorization: token $PAT\" \"$@\"\r\n}\r\n```\r\nthen use it like\r\n```\r\ncall \"https://api.github.com/repos/$PARENT_REPO/actions/workflows/$WORKFLOW_ID/dispatches\" -d '{\"ref\":'\"$PARENT_BRANCH\"', \"inputs\": { \"repo\":'\"$REPO\"', \"branch\":'\"$BRANCH\"' }}'\r\n```",
        "get both of these with one call, like\r\n```suggestion\r\n  result=$(call \"https://api.github.com/repos/$PARENT_REPO/actions/runs/$run_id\")\r\n  status=$(echo \"$result\" | jq -r .status)\r\n  conclusion=$(echo \"$result\" | jq -r .conclusion)\r\n```"
      ],
      "temporal-structured-contextual-logging": [
        "use log tags, not string manipulation. or just remove this before merging",
        "this one seems like it needs namespace [and/or id] tags to be helpful"
      ],
      "temporal-trust-getx-accessors": [
        "The GetX() wrappers check for nil receivers, so you can just write `if userData.GetData() == nil`",
        "this may be nil so you have to be careful:\r\n```suggestion\r\n\t\ttaskQueueType := req.GetTaskQueueType()\r\n\t\ttaskQueueUserData := userData.GetData().GetPerType()[int32(taskQueueType)]\r\n\t\tdescrResp.DescResponse.Configs = taskQueueUserData.GetTaskQueueConfig()\r\n```",
        "I don't think we need to deepcopy the key, you can't use reference types as keys, and for things like structs or arrays, you can't mutate a field in the key.\r\n\r\nI'll add a test.",
        "You don't actually need these two if statements, nils are handled correctly everywhere, you can just do\r\n```suggestion\r\n\t\t\ttypedUserData := userData.GetData().GetPerType()[int32(pm.Partition().TaskType())]\r\n\t\t\tfor _, v := range typedUserData.GetDeploymentData().GetVersions() {\r\n```",
        "The map lookup would return nil.. `nil.GetDeploymentData().GetVersions()` is fine, it just returns a nil map and the range over it does nothing. If you did something like assign to a field that would npe though."
      ],
      "temporal-ensure-deterministic-execution": [
        "Changes to this condition can cause nondeterminism errors so if we do need to do this (and I don't think we do) then it should be versioned.\r\n\r\nAs far as I know proto.Equal is totally fine and correct here. If you disagree could you point to some evidence otherwise?\r\n\r\n(Yes there are replay tests but they can't catch all edge cases)",
        "Just because it's flagged by workflowcheck doesn't mean that it's actually nondeterministic. And just because it's not flagged doesn't mean that it is determistic (how did proto.Marshal get by? that one actually is documented to be nondetermistic)",
        "I was only suggesting reverting the changes to the schedule wf (at least the proto.Equal) and adding ignore comments, not the whole PR"
      ],
      "temporal-safe-lock-usage-patterns": [
        "we usually use \"lock\"\n```suggestion\n\t\ttaskTrackerLock            sync.RWMutex\n```"
      ],
      "temporal-protocol-buffer-organization": [
        "I'm having second thoughts here... duplicating all of this structure seems like a waste of effort and opportunity for bugs. as you said, we have an HLC value available at the top level, and that's the one we're using to merge anyway... what do you think about basically reusing the public api message types here?",
        "put this field on line 380 above min_task_id, so we always have <pass, id> next to each other and in that order wherever they appear"
      ],
      "temporal-names-reflect-precise-behavior": [
        "elsewhere in this file I see a lot of checks against `wh.config.MaxIDLengthLimit()`, so might as well use that one for identity to be consistent",
        "Oh that's much better!",
        "\"attachRateLimiter\" doesn't \"attach\" anything, it just returns a new RateLimiter. also it doesn't use anything from the taskQueueConfig or the MatcherTestSuite. so this should just be `func newDefaultRateLimiter() quotasRateLimiter`",
        "I'd agree with that rename.. \"drainBuffer\" sounds like it'll be empty when it returns"
      ],
      "temporal-specific-assertion-methods": [
        "```suggestion\r\n\t\trequire.Empty(t, description.GetPendingActivities())\r\n```",
        "```suggestion\r\n\t\trequire.Empty(t, description.GetPendingActivities())\r\n```",
        "```suggestion\r\n\t\trequire.NotZero(t, startedActivityCount.Load())\r\n```"
      ]
    }
  },
  "jkotas": {
    "repos": [
      "dotnet/runtime"
    ],
    "entries": [
      {
        "slug": "runtime-avoid-busy-waiting",
        "title": "Avoid busy waiting"
      },
      {
        "slug": "runtime-cache-expensive-computations",
        "title": "Cache expensive computations"
      },
      {
        "slug": "runtime-centralize-platform-configurations",
        "title": "Centralize platform configurations"
      },
      {
        "slug": "runtime-choose-appropriate-error-mechanisms",
        "title": "Choose appropriate error mechanisms"
      },
      {
        "slug": "runtime-choose-descriptive-names",
        "title": "Choose descriptive names"
      },
      {
        "slug": "runtime-document-code-meaningfully",
        "title": "Document code meaningfully"
      },
      {
        "slug": "runtime-document-configuration-intent",
        "title": "Document configuration intent"
      },
      {
        "slug": "runtime-explicit-api-versioning",
        "title": "Explicit API versioning"
      },
      {
        "slug": "runtime-follow-naming-patterns",
        "title": "Follow naming patterns"
      },
      {
        "slug": "runtime-maintain-configuration-compatibility",
        "title": "Maintain configuration compatibility"
      },
      {
        "slug": "runtime-memory-barrier-pairing",
        "title": "Memory barrier pairing"
      },
      {
        "slug": "runtime-memory-ordering-matters",
        "title": "Memory ordering matters"
      },
      {
        "slug": "runtime-model-actual-hardware-costs",
        "title": "Model actual hardware costs"
      },
      {
        "slug": "runtime-names-reflect-actual-purpose",
        "title": "Names reflect actual purpose"
      },
      {
        "slug": "runtime-optimize-aligned-simd-operations",
        "title": "Optimize aligned SIMD operations"
      },
      {
        "slug": "runtime-optimize-common-paths",
        "title": "Optimize common paths"
      },
      {
        "slug": "runtime-optimize-for-readability",
        "title": "Optimize for readability"
      },
      {
        "slug": "runtime-optimize-memory-access",
        "title": "Optimize memory access"
      },
      {
        "slug": "runtime-preserve-pointer-authentication",
        "title": "Preserve pointer authentication"
      },
      {
        "slug": "runtime-simplify-code-expressions",
        "title": "Simplify code expressions"
      },
      {
        "slug": "runtime-specific-exceptions-with-context",
        "title": "Specific exceptions with context"
      },
      {
        "slug": "runtime-structure-configuration-options",
        "title": "Structure configuration options"
      }
    ],
    "comments": {
      "runtime-simplify-code-expressions": [
        "```suggestion\r\n    Volatile<bool> g_GCBridgeActive = false;\r\n```\r\nNit: It is fine to use native C/C++ bool for internal details."
      ],
      "runtime-names-reflect-actual-purpose": [
        "FWIW, the user facing property to enable event pipe (and related features) is called `EventSourceSupport` in NativeAOT (https://learn.microsoft.com/en-us/dotnet/core/deploying/native-aot/diagnostics#observability-and-telemetry).",
        "> Yeah, I find EventSourceSupport confusing for all that Event Pipe / Diagnostic Server can do.\r\n\r\nI agree that it is not the most descriptive name. On the other hand, it gets complicated to explain what works and does not work in the matrix of fine-grained combinations. It is why we have opted to have one config switch that controls it all."
      ],
      "runtime-maintain-configuration-compatibility": [
        "> The two big things are that we have some unused gaps now that could be filled in and I don't see why we need to use unique IDs across X86 and Arm64 (that is X86Base and ArmBase can't currently both be R2R ID 1).\r\n\r\nThis would make r2rdump more complicated. Right now, r2rdump just does Enum.ToString() to dump the instruction set. \r\n\r\nAlso, r2rdump is designed to be able to dump older versions of r2r images. Removing the definitions of the existing enum members breaks that. These removals should be reverted.",
        "> we should be able to do much like we do in NAOT\r\n\r\nNAOT does not have the versioning problem like R2R. There is no mixing and matching of versions, etc.",
        "> Right now we're using 60 bits for R2R, when we should only need 34 total. If we split it by platform then it's 11 for Arm64 and 23 for x64. Just given the pending work around SVE and AVX, we will likely go over 64-bits in the next release and need to take a break anyways.\r\n\r\nReadyToRunInstructionSet enum is not used to form a bit mask. Going above 64 should work just fine."
      ],
      "runtime-choose-appropriate-error-mechanisms": [
        "It is unusual to return error code via out argument, and only allow that error code to be OOM.\r\n\r\nCan we return proper error code, and return the pointer via `[out]` argument instead?\r\n\r\nThe error code can be an HRESULT or an enum specific to this call. (I know HRESULTs are Windows-specific, but they are part of BCL Exception so they are not going away x-plat, so there is not much value in trying hard to avoid them.)",
        "I think it is fine add back what you need to get this to compile. "
      ],
      "runtime-follow-naming-patterns": [
        "This method is not returning anything after this change, so `Get...` name does not fit what it does anymore. \r\n\r\nI would rename it to `PrintHelp`, `DisplayHelp` or something similar.\r\n\r\n",
        "(I have not commented on all places with this name. All of them should be fixed.)"
      ],
      "runtime-document-code-meaningfully": [
        "What's the license on the file that this was copied from?",
        "We prefer to err on the side of doing more attribution.\r\n\r\nThis is more than 100 lines copied nearly verbatim, including docs. I think it is above the threshold."
      ],
      "runtime-explicit-api-versioning": [
        "Do we need bump `EE_INTERFACE_MAJOR_VERSION` for this this?",
        "I am worried about the existing standalone GC scenarios that the GC team cares about being broken. As implemented in the PR currently, I do not think you can use new GC with old runtime, and vice versa."
      ],
      "runtime-optimize-aligned-simd-operations": [
        ">  I don't believe trying to polyfill via malloc/free is reliable as there is no guarantee that free works with arbitrary pointers:\r\n\r\nThe polyfill would have to be for both aligned alloc and free, something like this: https://github.com/dotnet/runtime/issues/33244#issuecomment-595848832"
      ],
      "runtime-optimize-for-readability": [
        "> RuntimeMethodInfo doesn't overload those operators so it doesn't matter\r\n\r\nMethodInfo does overload those operators. MethodInfo operators are going to be selected to compare RuntimeMethodInfo [quick test](https://sharplab.io/#v2:EYLgtghglgdgNAFxFANgHwAIAYAEGCMAdAEoCmAZiqQMYJQD2MA3ALABQ7EwAzggE4RaeAEw5iAVxh0wpALKkEAC3oATAJIxy9HCBzylqjVvYBvdjgt4AzHnwA2HMHr0UOACqleACglSoM/WV1TW0wAEpzSzM2S1i8AHYcMBwAXhScGHEUFFYYywBfdnygA=)"
      ],
      "runtime-optimize-common-paths": [
        "We build with both llvm and gcc successfully. What's the compiler used by SunOS that this needs fixing?",
        "Why does this compile fine with gcc on Linux?",
        "> it would be nice if NAOT could target this as the default\r\n\r\nI do not think we want to have the baseline supported piecemeal (it is unlikely we would be able to it correctly since it is impossible to test). If we want to raise the baseline, we should do it for the whole product. I would not be opposed to raising the product baseline to x86-64-v2.\r\n\r\n>  its an 18 year old baseline\r\n\r\nWhen did Intel/Amd stop selling the last processor without x86-64-v2? It is the more interesting date for this discussion.\r\n\r\n> Win11 prior to 24H2 is documented as requiring SSE4.1 otherwise\r\n\r\nWhere is it documented?"
      ],
      "runtime-centralize-platform-configurations": [
        "So this will need ifdefs controlled by configure script based on what's available on different Unix flavors. You can always do polyfill using malloc when there is no native API.",
        "Here is the existing place where all similar adjustment are concentrated today:\r\n\r\nhttps://github.com/dotnet/runtime/blob/9d771a26f058a9fa4a49850d4778bbab7aa79a22/src/libraries/Native/Unix/configure.cmake#L532-L558\r\n\r\nWe may want to move this one to this central place too."
      ],
      "runtime-cache-expensive-computations": [
        "This is an expensive way to compute a bool. Should we have a virtual property for this?",
        "Pass the result of `type.GetClassLayout()` to the worked methods to avoid recomputing it? (All except `ComputeCStructFieldLayout` needs it on some path.) It can be passed around as `in` since it is likely going grow into a large struct over time.",
        "Pre-allocate this HashSet and Dictionary to avoid re-hashing?",
        "```suggestion\r\n                symbolRemapping = new Dictionary<ISymbolNode, ISymbolNode>((int)(1.05 * (previousSymbolRemapping?.Count ?? 0)));\r\n```",
        "Otherwise, we are guaranteed to rehash the whole thing on most iterations just to add a few more elements."
      ],
      "runtime-document-configuration-intent": [
        "Are any of these documented in official docs? If they are not, I do not think we need to keep them.",
        "`IlcInstructionSet` is explicitly documented in the repo docs as [subject to change without a breaking change notice](https://github.com/dotnet/runtime/blob/main/src/coreclr/nativeaot/docs/optimizing.md).",
        "Command line arguments for direct invocation of crossgen2/ilc are not officially documented/supported."
      ],
      "runtime-memory-barrier-pairing": [
        "Do we need a counter-part barrier on the writer sides (e.g. in DynamicHelperFixup)?"
      ],
      "runtime-preserve-pointer-authentication": [
        "I see that you have `Sign with SP` at the end of the list. I am not sure whether you want to wait with tackling it as the last item. I expect that you will need to revisit and retest number of changes in this PR to make signing with SP work. ",
        "Signing return addresses with `SP` is required to deliver on security promise of PAC and it changes how things like hijacking need to be handled. I expect that you will need to redo a large part of change to make it work and then retest everything again. You can certainly do that, but I do not think it is the most efficient way to deliver this feature.",
        "Would it make sense to keep the bits on `m_pvHJRetAddr` unstripped so that the original return address is protected at all times? I think stripping the bits here creates a hole in the protection. Should we strip the bits later when we read `m_pvHJRetAddr` for stackwalking, but not when we use it to store/restore the original return address so that the return address is protected for the whole time?",
        "Once the code actually returns to the restored return address, it is going to authenticate. It should not be necessary to do an extra authentication during hijacking.",
        "> the original address is preserved so that after GC we can return to it, and the updated address would take the execution flow to desired new address\r\n\r\nRight. My point is that it would be better to keep the signed address intact through the whole process:\r\n- Delete PacStripPtr in Thread::HijackThread and store the original signed address in m_pvHJRetAddr instead \r\n- Delete PacSignPtr in Thread::UnhijackThread\r\n- Add PacStripPtr as necessary to places that read m_pvHJRetAddr and do not expect the signature in the upper bits\r\n\r\nIt will make the original return address protected while it is stored in m_pvHJRetAddr. As implemented currently, the return address is not protected while it is stored in m_pvHJRetAddr."
      ],
      "runtime-structure-configuration-options": [
        "```suggestion\r\n#if !defined(DACCESS_COMPILE) && !defined(FEATURE_CDAC_UNWINDER)`\r\n```\r\nWe have two builds of out-of-proc unwinders: DACCESS and CDAC. I think we want to take the offline path for both out-of-proc unwinders. `TARGET_ARM64` should not be needed in the condition once you do enable to the offline path for all out-of-proc unwinders.\r\n\r\n(All similar ifdefs in this file should be changed like this.)",
        "We want to be able to test interpreter-only mode on Windows and Linux even with support JIT compiled in. This should be based on some config switch - like `DOTNET_Interpter=*` that should make us to use the interpreter for all methods.",
        "@janvorli Do you have thoughts how we want to test interpreter-only mode?\r\n\r\nI am thinking we may want to introduce a new dedicated environment variable: `DOTNET_InterpreterMode`:\r\n\r\n`DOTNET_InterpreterMode=0`: default, do not use interpreter except explicit opt-in via `DOTNET_Interpreter`\r\n`DOTNET_InterpreterMode=1`: use interpreter for everything except (1) methods that have R2R compiled code and (2) all code in System.Private.CoreLib. All code in System.Private.CoreLib falls back to JIT if there is no R2R available for it. This can replace the testing mode introduced in https://github.com/dotnet/runtime/pull/116570/files#diff-3e5a329159ca5b2268e62be8a0d776b6092681e9b241210cb4d57e3454816abcR403 since it will cover code in non-entrypoint assemblies too and thus will be more comprehensive. This mode should have good balance between speed and coverage. We may want to use it for running libraries tests with interpreter eventually.\r\n`DOTNET_InterpreterMode=2`: use interpreter for everything except intrinsics. All intrinsics fallback to JIT. Implies `DOTNET_ReadyToRun=0`. I am not sure how much this will be useful in practice, but it sounds like interesting mode to have.\r\n`DOTNET_InterpreterMode=3`: use interpreter for everything, the full interpreter-only mode, no fallbacks to R2R or JIT whatsoever. Implies `DOTNET_ReadyToRun=0`, `DOTNET_EnableHWIntrinsic=0`, \r\n\r\nAn alternative is to piece together solutions from existing environment variables (`DOTNET_Interpreter`, `DOTNET_ReadyToRun`, `DOTNET_EnableHWIntrinsic`, ...), but it may be hard to make it do what we want exactly.\r\n\r\n> Should I introduce that flag as part of this PR?\r\n\r\nI do not have an opinion - it is fine with me to introduce it as part of this PR once we agree on how it should work.",
        "> I am not sure if the mode 1 would replace the testing mode for coreclr tests though, as it would also interpret the xunit stuff which I assume would be quite slow.\r\n\r\ncoreclr tests should have the xunit stuff source generated. It should not be a lot of code - it does not run reflection and other heavy lifting like regular xunit."
      ],
      "runtime-avoid-busy-waiting": [
        "Yes, somebody tried really hard to make `Volatile.Read` for 64-bit values atomic on x86, and this is the price that was paid for it."
      ],
      "runtime-choose-descriptive-names": [
        "```suggestion\r\n                            trace::error(_X(\"The application '%s' is not a managed .dll.\"), app_candidate.c_str());\r\n```\r\nI do not think managed .exes are a thing in modern .NET. None of the tooling will produce them. Should we drop `or .exe` here to avoid suggesting that managed .exes are a thing?\r\n\r\nAlternatively, we can say `is not a .NET binary.\". \"binary\" looks less Windows-specific, but also less descriptive. It won't give you a hint that you need to pass in .dll."
      ],
      "runtime-memory-ordering-matters": [
        "```suggestion\r\n        void* pvHIjackAddr = (void*)pfnHijackFunction;\r\n#if defined(TARGET_ARM64)\r\n        pvHIjackAddr = PacSignPtr(pvHIjackAddr);\r\n#endif // TARGET_ARM64\r\n        *ppvRetAddrLocation = pvHIjackAddr;\r\n```\r\nWe should avoid writing the wrong value first, and then overwriting it with the correct value. It can cause interesting race conditions."
      ],
      "runtime-specific-exceptions-with-context": [
        "\"Dynamic entrypoint allocation is not supported in the current environment.\"?",
        "Can we throw the exception immediately here instead of propagating it manually? The manual error propagation looks like a left-over from the .NET Native MRT/app split.",
        "This should be `PlatformNotSupportedException`. From .NET Framework design guidelines:\r\n\r\nDO throw PlatformNotSupportedException to indicate the operation cannot complete in the current runtime environment but could on a different runtime or operating system."
      ],
      "runtime-optimize-memory-access": [
        "Should this be done as unsigned division to match CoreCLR? \r\n\r\nSigned division is extra instructions: https://godbolt.org/z/cxxWP67n6",
        ">  Also validating that byteCount % alignment == 0 would be fairly expensive.\r\n\r\nYou do not need to use `%` once you know that the alignment is power of 2.",
        "Does the formula for the second argument need to use the alignment after it has been bumped up to `sizeof(void*)`?",
        "> basically, all MemoryMarshal.Read/Write/Cast, Unsafe.Read/ReadUnligned/Write/WriteUnligned, Unsafe.As\r\n\r\nMemoryMarshal.Cast and Unsafe.As are the only ones with the potential alignment problem, there are not that many, and we have been accepting fixes to make the core more portable (e.g. https://github.com/dotnet/runtime/pull/98812).\r\n\r\n> The only tricky cases\r\n\r\nAnd the platforms/architecture that we never heard of that Unity may run on.",
        "> it still is going to fail on a platform where such reads aren't fixed by the OS. At least on CoreCLR/NAOT\r\n\r\nIf we end up targeting a platform like that, the JIT will need to be fixed up to make `Unsafe.ReadUnaligned` work as appropriate.",
        "> I guess it's one of those cases when safe code doesn't read better than unsafe\r\n\r\nIt is because of we have not designed safe APIs that make this kind of code look good.\r\n\r\nI think that that best you can do using existing .NET APIs is approximation of \"eat the span\" pattern that's idiomatic in golang:\r\n```csharp\r\nwhile (span.Length >= 8)\r\n{\r\n   ulong v = BitConverter.ToInt64(span);\r\n\r\n   ...\r\n\r\n   span = span.Slice(8);\r\n}\r\n```\r\n\r\nNew APIs can make it look better:\r\n```\r\nwhile (BitConverter.TryRead(span, out long v))\r\n{\r\n   ...\r\n\r\n   span = span.Slice(8);\r\n}\r\n```\r\n\r\nAnother alternative is to introduce iterators that are idiomatic in Rust:\r\n```\r\n// Rust equivalent is for chunk in data.array_chunks::<8>()\r\nforeach (long v in span.IterateAsInt64())\r\n{\r\n...\r\n}\r\n```",
        "> `=> Aggregate<T, IdentityOperator<T>, Crc32Operator<T>>(x)`\r\n\r\nI get that this pattern is centralizing the unsafe code, but I do not think that it reads well."
      ],
      "runtime-model-actual-hardware-costs": [
        "This should be enabled for crossgen too.",
        "Yep, as @AndyAyersMS said. PREJIT means AOT compilation in general. R2R means specific ABI for AOT compiled code, e.g. code sequence to access generic dictionaries, etc."
      ]
    }
  },
  "lzchen": {
    "repos": [
      "open-telemetry/opentelemetry-python"
    ],
    "entries": [
      {
        "slug": "opentelemetry-python-adapt-for-linter-compatibility",
        "title": "Adapt for linter compatibility"
      },
      {
        "slug": "opentelemetry-python-choose-data-structures-wisely",
        "title": "Choose data structures wisely"
      },
      {
        "slug": "opentelemetry-python-configuration-source-precedence",
        "title": "Configuration source precedence"
      },
      {
        "slug": "opentelemetry-python-follow-python-naming-conventions",
        "title": "Follow Python naming conventions"
      },
      {
        "slug": "opentelemetry-python-future-proof-api-design",
        "title": "Future-proof API design"
      },
      {
        "slug": "opentelemetry-python-handle-exceptions-appropriately",
        "title": "Handle exceptions appropriately"
      },
      {
        "slug": "opentelemetry-python-maintain-consistent-naming",
        "title": "Maintain consistent naming"
      },
      {
        "slug": "opentelemetry-python-optimize-code-location-scope",
        "title": "Optimize code location scope"
      },
      {
        "slug": "opentelemetry-python-optimize-configuration-structure",
        "title": "Optimize configuration structure"
      },
      {
        "slug": "opentelemetry-python-precise-configuration-specifications",
        "title": "Precise configuration specifications"
      },
      {
        "slug": "opentelemetry-python-prevent-recursive-logging-calls",
        "title": "Prevent recursive logging calls"
      },
      {
        "slug": "opentelemetry-python-sanitize-observability-data-exports",
        "title": "Sanitize observability data exports"
      },
      {
        "slug": "opentelemetry-python-structured-changelog-documentation",
        "title": "Structured changelog documentation"
      },
      {
        "slug": "opentelemetry-python-telemetry-version-pinning",
        "title": "Telemetry version pinning"
      },
      {
        "slug": "opentelemetry-python-write-purposeful-comments",
        "title": "Write purposeful comments"
      }
    ],
    "comments": {
      "opentelemetry-python-configuration-source-precedence": [
        "Would this be as a large of a change as let's say...http semantic convention from old to new? Given that this is similarly a beta component (like the instrumentations), I am open to simply making this change and treating it as a specification change (and breaking customers) but if we feel the amount of usage also warrants an opt-in mechanism I am also open to that (although leaning towards just making the change).",
        "I think we covered this a bit in the SIG. It would be great to have this behavior be configurable with custom implementations of the distro. So instead of override a private `_initialize_components` function, we expose this through `configure` so distros can have dictate their own behavior. It also makes sense then to use `kwargs` in this case.",
        "Should this be additive or should we have the in-code configuration take priority and replace entrypoints similar to how we override env var? Would users be surprised by the \"default\" behavior?"
      ],
      "opentelemetry-python-structured-changelog-documentation": [
        "Are we leaving this blank?",
        "Nit: Include description of why we are adding `Final`"
      ],
      "opentelemetry-python-choose-data-structures-wisely": [
        "Should the increment to `measurements_seen` occur AFTER finding the index? Otherwise we tend to skip the first bucket?"
      ],
      "opentelemetry-python-prevent-recursive-logging-calls": [
        "+ 1 to this. Wdyt about using `warnings` instead of logger for other places in which are common to have recursions?"
      ],
      "opentelemetry-python-write-purposeful-comments": [
        "Could you clarify in the docstring here that `reset` is used for resetting any stateful logic after a collection cycle?",
        "Nit: Maybe put a comment here to explain this logic. We are iterating through `self.__attributes` which contains the entire set of original attributes from the recorded `Measurement` and saving only the remaining attributes after filtering out the keys from the post-view filtered attributes.",
        "Nit: Might be nice to add a comment to link to [this](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md#type-any) in the specs."
      ],
      "opentelemetry-python-future-proof-api-design": [
        "Does this mean that users must explicitly \"convert\" a histogram to a timing histogram to use it as a context manager?"
      ],
      "opentelemetry-python-telemetry-version-pinning": [
        "Maybe let's use `opentelemetry-api~=1.25`?",
        "Probably want to do `opentelemetry-api~=1.25` not `opentelemetry-api~=1.25.0`. This will help catch any bugs we release in minor versions."
      ],
      "opentelemetry-python-follow-python-naming-conventions": [
        "@vivek378521 \r\n\r\nHave you made the change for this? It still says `Optional[dict]` instead of `Attributes` for typing.",
        "Should we stick with `trace_exporter_names` instead?",
        "We call them \"trace_exporter\" because user is configuring what exporter they want under the \"trace\" flag. As well, the entry points are using `opentelemetry_traces_exporter`.",
        "As discussed in the 5/30 SIG, `traces` makes more sense because we are passing it in as a flag in `opentelemetry-instrument`, we use the term `traces_exporter` for our exporter [entrypoints](https://github.com/open-telemetry/opentelemetry-python/blob/main/exporter/opentelemetry-exporter-zipkin-json/pyproject.toml#L35) and I believe the idea was to use the signal types (traces, metrics, logs) for names."
      ],
      "opentelemetry-python-sanitize-observability-data-exports": [
        "> The resulting unit SHOULD be added to the metric as [UNIT metadata](https://github.com/OpenObservability/OpenMetrics/blob/main/specification/OpenMetrics.md#metricfamily) and as a suffix to the metric name unless the metric name already contains the unit, or the unit MUST be omitted.\r\n\r\nDoesn't the name need to have the unit appended as a suffix if it doesn't exist already in the name?",
        "> The resulting unit SHOULD be added to the metric as [UNIT metadata](https://github.com/OpenObservability/OpenMetrics/blob/main/specification/OpenMetrics.md#metricfamily) and as a suffix to the metric name unless the metric name already contains the unit, or the unit MUST be omitted.\r\n\r\nJust to clarify, we will be checking the existence of a SANITIZED and valid unit name in a sanitized and valid metric name to determine whether or not to omit the appending of the suffix correct?\r\n\r\nThe current logic makes sense to me, just wondering if there is ever a case where the unit can be found already in the metric name in which case we DON'T have to strip leading `_` from the unit name? Not a blocker, just a small question.",
        "> This may cause ambiguity in scenarios where multiple similar-named attributes share invalid characters at the same location. In such unlikely cases, if multiple key-value pairs are converted to have the same Prometheus key, the values MUST be concatenated together, separated by ;, and ordered by the lexicographical order of the original keys.\r\n\r\nAre we going to be addressing this use case in the future?",
        "Forgive me for my lack of understanding of how Prometheus backend works but it looks like they accept a list of keys and values and they expect the indices of the pairs (key, value) to line up to represent a label. I'm not sure how sorting the original dictionary solves this issue? Isn't the logic simply pairing off each key/value?"
      ],
      "opentelemetry-python-optimize-configuration-structure": [
        "Any reason why we are renaming this envs?",
        "@xrmx \r\n\r\nHas this been resolved offline? What was the verdict?"
      ],
      "opentelemetry-python-optimize-code-location-scope": [
        "I changed this so it follows more like how API modules are structured.\r\nThere is a `util` folder with now with `instrumentation.py` as a separate module."
      ],
      "opentelemetry-python-handle-exceptions-appropriately": [
        "Also perhaps link the issue in the comments.",
        "Nit: Shouldn't the exception handling occur within the `_readfile` function instead of the below?",
        "@sandy2008 \r\n\r\nI'm curious as to why we have to reraise the error at the higher level? Can't we remove the try except in `_load_credentials` just let `_read_file` handle the exception + return `None`?",
        "Could you keep the exception handling as is?",
        "Design-wise that might be the case but changing this would be a breaking change. I would suggest keeping it as is and maybe down the road we can explore user experience options via configuration."
      ],
      "opentelemetry-python-adapt-for-linter-compatibility": [
        "I'm not too familiar with jinja. What does this change do?"
      ],
      "opentelemetry-python-maintain-consistent-naming": [
        "```suggestion\r\n- Rename metric handle to bound metric instrument\r\n```"
      ],
      "opentelemetry-python-precise-configuration-specifications": [
        "Should we also ignore the previous files that we ignored like `.tox` and `.venv`?"
      ]
    }
  },
  "kimwnasptd": {
    "repos": [
      "kubeflow/kubeflow"
    ],
    "entries": [
      {
        "slug": "kubeflow-api-structure-balance",
        "title": "API structure balance"
      },
      {
        "slug": "kubeflow-automate-style-enforcement",
        "title": "Automate style enforcement"
      },
      {
        "slug": "kubeflow-centralize-configuration-constants",
        "title": "Centralize configuration constants"
      },
      {
        "slug": "kubeflow-centralize-configuration-values",
        "title": "Centralize configuration values"
      },
      {
        "slug": "kubeflow-centralize-dependency-configurations",
        "title": "Centralize dependency configurations"
      },
      {
        "slug": "kubeflow-check-before-use",
        "title": "Check before use"
      },
      {
        "slug": "kubeflow-configurable-security-with-defaults",
        "title": "Configurable security with defaults"
      },
      {
        "slug": "kubeflow-consistent-descriptive-naming",
        "title": "Consistent descriptive naming"
      },
      {
        "slug": "kubeflow-control-header-modification",
        "title": "Control header modification"
      },
      {
        "slug": "kubeflow-descriptive-consistent-naming",
        "title": "Descriptive consistent naming"
      },
      {
        "slug": "kubeflow-document-code-thoroughly",
        "title": "Document code thoroughly"
      },
      {
        "slug": "kubeflow-document-networking-annotations",
        "title": "Document networking annotations"
      },
      {
        "slug": "kubeflow-enforce-https-protocol",
        "title": "Enforce HTTPS protocol"
      },
      {
        "slug": "kubeflow-enforce-least-privilege",
        "title": "Enforce least privilege"
      },
      {
        "slug": "kubeflow-environment-aware-configuration-design",
        "title": "Environment-aware configuration design"
      },
      {
        "slug": "kubeflow-externalize-configuration-parameters",
        "title": "Externalize configuration parameters"
      },
      {
        "slug": "kubeflow-follow-api-conventions",
        "title": "Follow API conventions"
      },
      {
        "slug": "kubeflow-go-export-naming-conventions",
        "title": "Go export naming conventions"
      },
      {
        "slug": "kubeflow-manage-configuration-changes",
        "title": "Manage configuration changes"
      },
      {
        "slug": "kubeflow-mark-ui-text-i18n",
        "title": "Mark UI text i18n"
      },
      {
        "slug": "kubeflow-match-algorithms-to-purpose",
        "title": "Match algorithms to purpose"
      },
      {
        "slug": "kubeflow-normalize-url-paths",
        "title": "Normalize URL paths"
      },
      {
        "slug": "kubeflow-optimize-container-build-configurations",
        "title": "Optimize container build configurations"
      },
      {
        "slug": "kubeflow-pin-version-dependencies",
        "title": "Pin version dependencies"
      },
      {
        "slug": "kubeflow-precise-workflow-triggers",
        "title": "Precise workflow triggers"
      },
      {
        "slug": "kubeflow-prefer-external-configuration",
        "title": "Prefer external configuration"
      },
      {
        "slug": "kubeflow-prioritize-readability-over-brevity",
        "title": "Prioritize readability over brevity"
      },
      {
        "slug": "kubeflow-private-variable-naming-convention",
        "title": "Private variable naming convention"
      },
      {
        "slug": "kubeflow-simplify-code-structure",
        "title": "Simplify code structure"
      },
      {
        "slug": "kubeflow-specific-network-access-documentation",
        "title": "Specific network access documentation"
      },
      {
        "slug": "kubeflow-standardize-build-configurations",
        "title": "Standardize build configurations"
      },
      {
        "slug": "kubeflow-standardize-makefile-patterns",
        "title": "Standardize makefile patterns"
      },
      {
        "slug": "kubeflow-standardize-network-tools",
        "title": "Standardize network tools"
      },
      {
        "slug": "kubeflow-standardize-style-scripts",
        "title": "Standardize style scripts"
      },
      {
        "slug": "kubeflow-structured-owners-files",
        "title": "Structured OWNERS files"
      },
      {
        "slug": "kubeflow-type-appropriate-default-values",
        "title": "Type-appropriate default values"
      },
      {
        "slug": "kubeflow-unique-workflow-step-names",
        "title": "Unique workflow step names"
      },
      {
        "slug": "kubeflow-use-appropriate-log-levels",
        "title": "Use appropriate log levels"
      },
      {
        "slug": "kubeflow-use-css-classes-properly",
        "title": "Use CSS classes properly"
      },
      {
        "slug": "kubeflow-use-enums-for-state",
        "title": "Use enums for state"
      },
      {
        "slug": "kubeflow-use-modern-javascript-idioms",
        "title": "Use modern JavaScript idioms"
      },
      {
        "slug": "kubeflow-use-snake-case-in-python",
        "title": "Use snake_case in Python"
      },
      {
        "slug": "kubeflow-use-table-driven-tests",
        "title": "Use table-driven tests"
      },
      {
        "slug": "kubeflow-validate-inputs-explicitly",
        "title": "Validate inputs explicitly"
      }
    ],
    "comments": {
      "kubeflow-follow-api-conventions": [
        "@DavidSpek show your comment below about the error with Unmarshal https://github.com/kubeflow/kubeflow/pull/5660#issuecomment-797378906. Writing it here to keep it in one place.\r\n\r\nI'm not sure how you used the Unmarshal function but I believe you could also try and catch the error and not let it panic. Here's a very similar code I've seen https://github.com/kubeflow/katib/blob/master/pkg/new-ui/v1beta1/backend.go#L73"
      ],
      "kubeflow-descriptive-consistent-naming": [
        "nit: Let's use `centraldashboard-angular` here as well, to make sure we use the same value everywhere.\r\n\r\nWe can revert to `centraldashboard` once we are confident with switching the web apps"
      ],
      "kubeflow-centralize-configuration-values": [
        "Could you add the urls for the icons into the environment files instead? https://github.com/kubeflow/kubeflow/tree/master/components/crud-web-apps/jupyter/frontend/src/environments\r\n\r\nLets keep these links in a central place, where we might also add other links in the future like the svgs for the main page",
        "Thanks for the ping! \r\n\r\nGood catch on the CORS issue. Lets dump the SVGs in the `static/assets` folder. And also define the links that the frontend will use in the `environment` files as mentioned above."
      ],
      "kubeflow-optimize-container-build-configurations": [
        "I understand that by omitting the GOARCH env var we let golang decide on the architecture based on the node environment (machine). \r\n\r\nI'm just trying to wrap my mind around the next step. Is it going to be the process described in https://docs.docker.com/build/building/multi-platform/ to build images that have manifests for different platforms?",
        "@lehrig this was a *very* thorough explanation of the suggested approach! I'd also suggest to cross post it in the umbrella issue, so that it's readily available for anyone wondering how to e2e approach is going to be.\r\n\r\nAlso LGTM"
      ],
      "kubeflow-validate-inputs-explicitly": [
        "let's instead make a check if `NaN` is included in the value of either `cpu` or `cpu_limit`.\r\n\r\nAnd if that's the case then the backend should raise a `BadRequest` werkzeug exception (400)",
        "Lets also add some logic here that would handle incorrect values.\r\n\r\nThe expected values here, from the request, would be:\r\n* `None`, which means `\"jupyter\"`\r\n* `\"jupyter\"`\r\n* `\"rstudio\"`\r\n* `\"vscode\"`\r\n\r\nIf this field is present in the request but has a different value from the expected ones then the backend should raise a `400` error",
        "Indeed, the frontend sends only specific values but it's a good practice for the backend to make it explicit to the user/frontend that the data it received was not formatted as expected.",
        "And also you could look at this snippet for how to raise this error\r\n\r\nhttps://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/jupyter/backend/apps/common/form.py#L31-L34\r\n\r\nEDIT: was typing it before I saw your comment above ",
        "You are not wasting anyone's time. Your efforts are as important as mine and no releases were delayed because of a question :) "
      ],
      "kubeflow-externalize-configuration-parameters": [
        "Also, ultimately we could use these CRs then to list who the contributors in the namespace are from the CentralDashboard?"
      ],
      "kubeflow-manage-configuration-changes": [
        "I'm not sure I understand what the question or suggested change is here. Could you rephrase this?",
        "I see what you mean. The `start` script should be completely removed, since the users should run the UI locally with `build:watch` for now.\r\n\r\nI'll push a commit to remove this script",
        "Added a commit that removes the unused npm script",
        "Let's exclude this change from this PR and keep the touched lines to the minimum. This will also remove the 15.000 modified lines to the `package-lock.json` because of this change.\r\n\r\nAlthough I'd like to progressively replace the use of `lodash` with `lodash-es`, but until I get to it we can remove this from the `package.json`, but lets just do it in another PR."
      ],
      "kubeflow-consistent-descriptive-naming": [
        "nit: can we make this `pvcviewer-` instead? In the labels as well we do `pvcviewers` so let's be uniform",
        "Let's rename this to `CentralDashboard-angular Frontend Tests` and also the file to `centraldb_angular_frontend_test.yaml`"
      ],
      "kubeflow-use-css-classes-properly": [
        "Don't modify html elements directly with styles, always use a class instead that wraps what the style changes are. Also try to avoid as much as possible `!important` in CSS. Instead try to use more specific css selectors.\r\n\r\nHere you could introduce and use the following CSS class for each `<mav-icon>`\r\n```css\r\n.server-type {\r\n  height: 32px;\r\n  width: 150px;\r\n}\r\n```",
        "Add a wrapper class here to give this group a bottom margin\r\n\r\n```css\r\n.server-type-wrapper {\r\n  margin-bottom: 1rem;\r\n}"
      ],
      "kubeflow-normalize-url-paths": [
        "Let's rephrase this to something like the following:\r\n\r\nWhen Istio exports Services it always expects a `/` at the end. SO we'll need to make sure the links propagated to the iframe end with a `/`",
        "Let's add a comment here to explain why we need this function, which is to handle the sometimes missing `/` from some urls"
      ],
      "kubeflow-enforce-least-privilege": [
        "Should be good to go. It would also be a bug from the current code if it couldn't operate without listing namespaces, since it shouldn't need these permissions in the first place.\r\n\r\nNote that we might need to re-introduce them though, in a future iteration when we would want the apps to be self-standing. But we need to discuss other items for this effort. But I'm also mentioning this context to keep in mind"
      ],
      "kubeflow-match-algorithms-to-purpose": [
        "Why does this need to be a `while` loop and not an `if`? \r\n\r\nAlso, shouldn't we have the condition also check if the status is \"empty\"? Since we want to have this message only when the controller hasn't set the status? \r\n\r\nElse won't it always expose the `Waiting for StatefulSet to create the underlying Pod.` message for the first 10 seconds independently of whether the status is set?",
        "We can simplify this and remove the `for` loop for the `item` var.\r\n\r\nThe `coditions` are a list of dict objects. We only need to iterate over each condition dict, from the list, and check if that condition object contains the `reason` key"
      ],
      "kubeflow-centralize-dependency-configurations": [
        "Should we drop this package and install the corresponding kserve package instead?",
        "Could you instead make this change into the common code, so that it takes effect for all the web apps?\r\n\r\nhttps://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/common/backend/setup.py#L6"
      ],
      "kubeflow-centralize-configuration-constants": [
        "That's a good idea, it will help us have all the names organized in one place.\r\nAdding it",
        "@PatrickXYS I added another commit that introduces this `config.py` file.\r\nPTAL and if you are OK I can resolve this conversation",
        "OK, I'll just omit this field entirely. There no issue by completely omitting the field and the configmap's value will be used right?"
      ],
      "kubeflow-standardize-network-tools": [
        "@nrchakradhar thank you for your comments. I'm planning on tackling the IPv6 support for all the web apps after 1.3.\r\n\r\nThe first step is to bind the IPv6 equivalent addresses as well, as you've pointed out in your links. But I want to do a good deep dive on this and test some edge cases for which I don't have the cycles right now. "
      ],
      "kubeflow-precise-workflow-triggers": [
        "Let's re-use the makefile rules here, to avoid duplication of code in makefiles and GH Actions. The layers are already cached so rebuilding the image should be very fast\r\n```bash\r\nexport TAG=$(cat releasing/version/VERSION)\r\ncd components/centraldashboard-angular\r\nmake docker-build docker-push\r\n```",
        "@apo-ger let's leave the notebook images completely out for this effort.\r\n\r\nI believe we would be able to use the Makefiles if we'd use a `REGISTRY` env var in each Makefile. But let's discuss this in a follow up PR, since this one is already getting big",
        "just the changes from this PR, and we can send another PR for the latest tag in a separate PR that:\r\n1. Uses a `REGISTRY` env var in all makefiles (let's see if we need to do more)\r\n2. Use the Makefiles to build each notebook, but with a different TAG each time like we do for the rest of the components",
        "Since we are testing manifests this time we don't want to trigger the workflows when code changes, but rather when the **manifests** change.\r\n\r\nSo in this case it will be `components/centraldashboard/manifests/**`",
        "I think you missed this one. It should be `components/profile-controller/config/**`",
        "Lets add another path here for `components/crud-web-apps/common/**`. We want the web app to be rebuild whenever we also touch the common code.",
        "Lets add another path here for `components/crud-web-apps/common/**`. We want the web app to be rebuild whenever we also touch the common code.",
        "Lets add another path here for `components/crud-web-apps/common/**`. We want the web app to be rebuild whenever we also touch the common code.",
        "Let's also trigger this workflow for the `/components/common/**` directory, since it's using common code from there"
      ],
      "kubeflow-use-enums-for-state": [
        "Thank you for the feedback @tasos-ale! \r\n\r\nThe reason I used `true` as a default value here is because if it starts as `null` then the app will think that the dashboard is not present and try to make a request to fetch the namespaces. \r\n\r\nSo in this case I take for granted the opposite, which is expect the dashboard to be present and if after the check it's missing then the apps will make the request to fetch the namespaces",
        "but indeed the enumeration is a better way to express this. I'll use this enum instead of a boolean value and the apps will fetch the namespaces only if the state is `Disconnected`",
        "pushed a commit for the described changes"
      ],
      "kubeflow-use-snake-case-in-python": [
        "Let's rename this var to `creation_timestamp`. Python is using snake_case for var names \r\nhttps://peps.python.org/pep-0008/#descriptive-naming-styles",
        "let's not use `camelCase` for Python variables. This should be `cpu_limit`",
        "same with `camelCase` as above. This should be `memory_limit`"
      ],
      "kubeflow-mark-ui-text-i18n": [
        "Pushed a commit",
        "pushed a commit",
        "Pushed a commit"
      ],
      "kubeflow-standardize-style-scripts": [
        "Why do we introduce rules for `tslint`, which is getting deprecated?\r\n\r\nLet's instead have the following 2 rules:\r\n```json\r\n    \"lint-check\": \"ng lint\",\r\n    \"lint\": \"ng lint --fix\",\r\n```\r\nsimilarly to KWA https://github.com/kubeflow/katib/blob/master/pkg/new-ui/v1beta1/frontend/package.json#L12-L13\r\n\r\nWe might also need to make some small changes to the `angular.json`. Take a look at the previous effort for this in https://github.com/kubeflow/kubeflow/pull/6464",
        "I see that we don't have this script in TWA, like we do in Katib\r\nhttps://github.com/kubeflow/katib/tree/master/pkg/new-ui/v1beta1/frontend/scripts\r\n\r\nYet the action didn't fail. Why is that?",
        "OK I see what happened. `||` means that the second command (node script in this case) will NOT be run if the first part was successful. But the node script will run if the prettier check fails.\r\n\r\nThat script is just there to instruct users to run the formatting. Let's include it in this PR"
      ],
      "kubeflow-control-header-modification": [
        "I think we should not let users arbitrarily add headers to the requests that pass from the ingress gateway. For example they could even override the ISTIO `userid-header`, the `X-Forwarded-Proto` so that the server assumes http instead of https etc.\r\n\r\nI think we should reconsider the design we take here\r\nalso cc @yanniszark who has a lot of experience on security",
        "I took a more extensive look at the docs and I propose the following solution:\r\n\r\nThe headers that are used to configure parts of R-Studio are all non-standard headers, starting with `X-*`. Such example headers are\r\n* `X-RStudio-Request`\r\n* `X-RStudio-Root-Path`\r\n* `X-Forwarded-Host`\r\n* `X-Forwarded-Proto`\r\n* `X-Auth-Token`\r\n\r\nSo I propose to go the other way around from blacklisting specific headers. Let's only allow the users to edit/set non-standard headers that start with `X-*`. This way we provide all the necessary configuration for R-Studio and at the same time don't allow the users to edit any header arbitrarily.\r\n\r\nAnd if in the future we find users that need to specifically set well defined headers for a Notebook then we can discuss based on the specific use case on how to proceed and allow users to add more headers.\r\n\r\nThis is the case for the docs I looked up to now\r\nhttps://docs.rstudio.com/ide/server-pro/access-and-security.htmlhttps://docs.rstudio.com/connect/admin/authentication/proxied/\r\nhttps://docs.rstudio.com/connect/admin/appendix/configuration/#ProxyAuth.Settings\r\n",
        "And lastly, the controller will set accordingly the `X-RStudio-Root-Path` header if the `server-type` is `rstudio`. But the user could still be able to override this by manually specifying a different value.\r\n\r\nThis way in order for a CR to support RStuido it will only need the `notebooks.kubeflow.org/server-type` annotation, which provides a good default mechanism and extensibility to the users.",
        "> Also, using X-* for headers seems to have been deprecated int 2012: see https://tools.ietf.org/html/rfc6648\r\n\r\nThis is a very good argument to make to the folks maintaining the R-Studio code :) \r\n\r\n> However, by backing this into the controller administrators cannot easily customize these values.\r\n\r\nWhat extra headers do you think they would need to set aside, from the `X-*`  headers R-Studio knows/expects?\r\n\r\n> Regarding your last comment, are you suggesting that the current mechanisms added to the controller stay as they are, but the controller itself adds the annotations to the CR when notebooks.kubeflow.org/server-type: rstudio is present?\r\n\r\nNo, the controller will not add any extra annotations if the `server-type` is present. It will only configure the vsvc accordingly.\r\nLet me try to clarify with two examples:\r\n\r\nCR 1:\r\n```yaml\r\nmetadata:\r\n  annotations:\r\n    notebooks.kubeflow.org/server-type: rstudio\r\n```\r\n\r\nIn this case the controller will see that the Notebook is for R-Studio so it will set the `/` url rewrite in the vsvc and also set the `X-RStudio-Root-Path` header in the vsvc for the prefix\r\n\r\nCR 2:\r\n```yaml\r\nmetadata:\r\n  annotations:\r\n    notebooks.kubeflow.org/server-type: rstudio\r\n    notebooks.kubeflow.org/http-rewrite-uri: /some/path\r\n```\r\n\r\nIn this case the controller will first do exactly what it would for CR 1. But then it would also detect the `notebooks.kubeflow.org/http-rewrite-uri` and use this value for the Istio rewrite, instead of the default it previously put. \r\n\r\nAlso the same approach will be used with the `notebooks.kubeflow.org/http-headers-request-set`. If this annotation is present then the controller will use the `X-*` headers mentioned in this annotations instead",
        "> Regarding the server-type thing, we can do that in the future but its not necessary right now.\r\n\r\n@thesuperzapper I can understand your urge to have this effort merged as soon as possible, but this is not a big feature so lets get it right from the beginning and not worry about it in the future. The functionality I described is almost there.\r\n\r\n> Adding a whitelist would only act to reduce the usefulness of this feature for users why might have their own container images.\r\n\r\nCould you elaborate on this? What use cases do you think we limit? With the mention mechanism, of allowing the user to set any `X-*` header they want, we still support all of the configuration options from R-Studio."
      ],
      "kubeflow-document-networking-annotations": [
        "Can you add a comment here that for images in this group the backend will be:\r\n1. Adding the `notebooks.kubeflow.org/http-rewrite-uri` annotation to the CR, to rewrite the path that ends up in the running container to be `/`\r\n\r\nThis will help exposing to the users the constraints and logic that will be applied to images that belong to that group",
        "LGTM",
        "Can you add a comment here that for images in this group the backend will be:\r\n1. Adding the `notebooks.kubeflow.org/http-headers-request-set` annotation to the CR, for setting the `X-RStudio-Root-Path: ${ISTIO_PREFIX}`  header to each request to the running container\r\n2. Adding the `notebooks.kubeflow.org/http-rewrite-uri` annotation to the CR, to rewrite the path that ends up in the running container to be `/`\r\n\r\nThis will help exposing to the users the constraints and logic that will be applied to images that belong to that group",
        "LGTM"
      ],
      "kubeflow-standardize-build-configurations": [
        "I don't like the phrasing of this sentence, as it implies that for development purposes we need to run the command directly and can't use the Makefifle.\r\n\r\nWe can use the Makefile to build Jupyter app's image by running\r\n```bash\r\nREGISTRY_PROJECT=my-repo make docker-build\r\n```\r\n\r\n[ Tensorboard's makefile uses slightly different vars but we should iron them out and have the same template when we setup the CI/CD ]"
      ],
      "kubeflow-prioritize-readability-over-brevity": [
        "I was thinking that If in the future we want to handle other cases in the PATCH request, then each one of them should be its own function and not have all the logic unfolded in the PATCH handler",
        "I was thinking that If in the future we want to handle other cases in the PATCH request, then each one of them should be its own function and not have all the logic unfolded in the PATCH handler",
        "We should not ignore the E501 warnings.\r\nIf the line is too long you could put each argument in a distinct line.",
        "nit: @lalithvaka could you add this logic into a distinct function on its own?"
      ],
      "kubeflow-prefer-external-configuration": [
        "@tasos-ale let's have this as an empty list initially, since we don't have an app that supports it at this point in time",
        "Ideally I'd like this to info (which web app supports all namespaces) to be transmitted via the common library, so that the dashboard can dynamically know if an app supports this once it loads it.\r\n\r\nBut we can do with hardcoding the urls for now. If in the future we see a bigger need for this we can look into it",
        "Do we need all of these new imports or only a subset of them?",
        "Makes sense, lets keep them then"
      ],
      "kubeflow-api-structure-balance": [
        "I'd propose to avoid introducing a handler for the InferenceServices in the common backend code.\r\n\r\nThis would mean we'd need to add more handlers for list/delete/get/logs as well, which as a result will make the MWA depend even more in this common code. I'd propose to instead keep on using the `custom_resource.py` file's handlers for the ISVC logic in the MWA",
        "Could you add a `get_age(k8s_object)` in our common [helpers.py](https://github.com/kubeflow/kubeflow/blob/e99b5e18697a15088abea12543bd4e3f180ff984/components/crud-web-apps/common/backend/kubeflow/kubeflow/crud_backend/helpers.py) file that would return a dictionary with the `uptime` and `timestamp` values? Since other web apps will want to use this convention, for returning age information to their frontends, it would be a good idea to add this logic into a common place.\r\n\r\nFor now it could only work with dict objects, like CRs, and later one we could extend it to work with class objects as well, such as PVCs for example."
      ],
      "kubeflow-pin-version-dependencies": [
        "let's remove the dependency on the `test` rule. This would require users to have a configured Go environment to build the controller. \r\n\r\nIf in the future we'd like to run these tests then we should do this inside of the Dockerfile, just like we do for the CentralDashboard",
        "When I tried applying the new CRD on a cluster with the previous CRD I got the following errors back:\r\n```\r\nThe CustomResourceDefinition \"notebooks.kubeflow.org\" is invalid:\r\n* metadata.annotations: Too long: must have at most 262144 bytes\r\n* spec.preserveUnknownFields: Invalid value: true: must be false in order to use defaults in the schema\r\n```\r\n\r\nRegarding the `preserveUnknownFIelds`, could you explicitly set it to `false`?\r\n\r\nThis is also the recommended way in the docs to have compatibility: \r\nhttps://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#field-pruning\r\nhttps://github.com/kubernetes-sigs/controller-tools/issues/476#issuecomment-691519936\r\n\r\nAlthough with a quick look at the kubebuilder docs I'm not 100% sure whether we should do this with a [marker](https://book.kubebuilder.io/reference/markers/crd-processing.html) or via the [makefile](https://book.kubebuilder.io/reference/generating-crd.html#multiple-versions)",
        "After looking at this again I propose that we manually \r\n1. add the `spec.preserveUnknownFields: false` just for this release\r\n2. remove this field in KF 1.7, since it's only needed for the `apiextensions.k8s.io/v1beta1` to `apiextensions.k8s.io/v1` transition.\r\n\r\nThe `controller-gen` does not allow us to explicitly set this to false https://book.kubebuilder.io/reference/markers/crd-processing.html\r\n\r\n",
        "This is awesome @samuelvl!\r\n\r\nOne final nit, I see that you've commented out the `crd` part in the manifests, which results in the CRD to not be included when generating `base` or `overlays/kubeflow` \r\n\r\nhttps://github.com/kubeflow/kubeflow/pull/6374/files#diff-28481732533c7c75d2d2ba504e9670d90e72ed98f999115fd92a0f8e8a5aace2R20\r\n\r\nCould you revert it back, so that the CRD is included as well? We should be ready to merge afterwards",
        "Hmm, indeed in this PR we made the CRD quite bigger since we now actually include the full PodSpec for validation.\r\n\r\nBut the `maxDescLen=0` seems like a good way to cut down significantly the size of it. The spec is currently only the PodSpec, for which descriptions are widely available so we are OK.\r\n\r\nGood job @samuelvl!",
        "I tried to `kustomize build config/crd` with a 3.2 version, since this was the one [used from manifests](https://github.com/kubeflow/manifests#prerequisites) but I got the following errors:\r\n```\r\nError: no matches for OriginalId apiextensions.k8s.io_v1beta1_CustomResourceDefinition|~X|notebooks.kubeflow.org; no matches for CurrentId apiextensions.k8s.io_v1beta1_CustomResourceDefinition|~X|notebooks.kubeflow.org; failed to find unique target for patch apiextensions.k8s.io_v1beta1_CustomResourceDefinition|notebooks.kubeflow.org\r\n``` \r\n\r\nDo you know why these occur? They were not shows in the previous iteration of the controller, so maybe something slightly changed in the autogenerated manifests?\r\n\r\nIf I use the 3.8 version from the makefile this error goes away. Also it's not shown even with 3.2 when I build the `base`, so it's not that critical. But let's try to understand why this happens. I'll try to look into it within the next days as well",
        "I agree to remove the patch altogether. The patch was making the validation more lax, by removing validation from the CPU/Memory fields.\r\n\r\n@samuelvl can you double check that creating a Notebook with the Jupyter web app also works as expected after this change? To make sure the backend is submitting objects that are valid.",
        "Perfect!"
      ],
      "kubeflow-standardize-makefile-patterns": [
        "Can you also remove this rule? We don't use `build-gcb` anywhere in the project.\r\n\r\nIt's a good chance to clean up the project",
        "Can you add a `TAG ?= $(shell git describe --tags --always)` here? This is the one we use in the web apps as well. Let's standardize on this one",
        "nit: Could you also create an `image` rule, which builds and pushes the image?\r\n\r\nWe have this for the web apps as well and would be nice to keep this convention across all components\r\nhttps://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/jupyter/Makefile#L11",
        "@elikatsis ACK, this is a very good suggestion!\r\n\r\nI'll add a commit for Volumes web app in this PR, since it's fixing the formatting for this component. I'll then open up a new issue to keep track of the uniformity in the dockerfiles for the other web apps as well."
      ],
      "kubeflow-private-variable-naming-convention": [
        "let's make this `private`, to comply with the var's name"
      ],
      "kubeflow-use-modern-javascript-idioms": [
        "Could you restructure the if statements in order to reduce the nesting levels?\r\n\r\nSomething like:\r\n```javascript\r\nif (!queryParams || !queryParams[\"ns\"]) {\r\n  return this.buildHref(href, this.queryParams);\r\n}\r\n\r\nreturn this.buildHref(href.replace('{ns}', queryParams[\"ns\"]), queryParams);\r\n```",
        "LGTM!"
      ],
      "kubeflow-enforce-https-protocol": [
        "nit: `Invalid HOST url provided, must be like https*://*` (the protocol should be https in the end)"
      ],
      "kubeflow-use-appropriate-log-levels": [
        "I agree with @yanniszark.\r\nAlthough AFAIK `logr` does not have a [Warning level](https://github.com/go-logr/logr#why-not-more-named-levels-like-warning). So we should do this an Error instead.\r\n\r\n@gilbeckers could you also make the error message more explicit as to why it couldn't find the Notebook Container? Something like: `Could not find the Notebook container. No container has the same name with the CR '{notebook-name}'. Will not update the status of the CR.`"
      ],
      "kubeflow-automate-style-enforcement": [
        "We should look into moving away from `tslint` in the backend as well, now that we've decoupled the frontend and backend code"
      ],
      "kubeflow-structured-owners-files": [
        "@TobiasGoerke why are you not an approver? :) \r\n\r\nWe want the OWNERS file to depict the folks that are driving the component. Considering that you, @apo-ger and I have worked on the proposal of this I'd create the OWNERS file like this:\r\n\r\n```yaml\r\napprovers:\r\n  - apo-ger\r\n  - kimwnasptd\r\n  - TobiasGoerke\r\n```",
        "My bad, I just copied the reviewers from the jupyter web app.\r\nLets start with an empty list of reviewers for now",
        "I think I'm missing something here, where should approvers be listed?",
        "I see. Which people should I assign as reviewers?"
      ],
      "kubeflow-configurable-security-with-defaults": [
        "Lets make it configurable then, it should be a small change.\r\n\r\nI'll make it look for a `CSRF_SAMESITE` env var with a default value to `Strict`. If the value provided by the user is not `Strict`, `Lax` or `None` then again it will default to `Strict`\r\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie#SameSite",
        "force pushed. The PR should only contain one commit responsible for the CSRF functionality.\r\nI also made the `SameSite` attribute of the cookie configurable via the `CSRF_SAMESITE` variable.\r\n\r\nI also think a good next step would be to document the ENV Vars that each web app uses in the respective READMEs. This will make clear to the users how they can configure each app.",
        "ACK! I modified the README of the common code.\r\nI also created #5483 as an umbrella issue for the web apps"
      ],
      "kubeflow-go-export-naming-conventions": [
        "small nit, shouldn't this function start from lowercase since it's not meant to be used from outside this go module?"
      ],
      "kubeflow-unique-workflow-step-names": [
        "Could you provide some more details as to why this change is needed?",
        "Makes sense",
        "@PatrickXYS to make sure I'm synced with the proposed changes; the argument is to create a common base class for building a workflow that always has a step for kaniko-build, with `--no-push`, since we essentially use it in all of the component CI files?",
        "Regarding the changes for using a different class I agree with @DavidSpek that this isn't a blocker, so let's go with this implementation for now and if in the future we find ourselves repeating too much code then lets go over this argument.\r\nDoes this sound OK to you @PatrickXYS ?\r\n",
        "> Should the registry be called access-management or should it be kfam?\r\n\r\n@yanniszark what are your thoughts on this? I guess we should stick to `access-management` for now?\r\n\r\nI'll also give a heads up to our tracking issue to add this registry as well ",
        "> @kimwnasptd I actually just ran into an issue where using the common kaniko_builder.py actually complicates things slightly. For this situation specifically it is easily fixed\r\n\r\nCould you provide some more details for this? Just so that we can identify the pattern/pain-point once we see it reoccurring "
      ],
      "kubeflow-type-appropriate-default-values": [
        "In this case the default value for the conditions should be a `[]`, instead of `\"\"` since this var is a list"
      ],
      "kubeflow-simplify-code-structure": [
        "Let's remove this function. We only need this initialization in one place and can simplify with something like:\r\n\r\n```golang\r\nstatus := v1beta1.NotebookStatus{\r\n    Conditions:     make([]v1beta1.NotebookCondition, 0),\r\n    ReadyReplicas:  sts.Status.ReadyReplicas,\r\n    ContainerState: corev1.ContainerState{},\r\n}\r\n```",
        "Could you break this if statement to the following ones:\r\n\r\n```golang\r\nif pod.Status.ContainerStatuses[i].Name != instance.Name {\r\n    continue\r\n}\r\n\r\nif pod.Status.ContainerStatuses[i].State == instance.Status.ContainerState {\r\n    continue\r\n}\r\n\r\nlog.Info(\"Updating Notebook CR state: \", \"namespace\", instance.Namespace, \"name\", instance.Name)\r\ncs := pod.Status.ContainerStatuses[i].State\r\n...\r\n```",
        "nit: You could have an `if nodename == \"\" {` here and return nil if that holds.\r\n\r\nThen you could move the code below one tab to the left, outside of an if clause, to make it a little bit more simple to read"
      ],
      "kubeflow-environment-aware-configuration-design": [
        "It makes sure the image build, and stored in docker, will use the TAG defined in the `env` and not the default one which is\r\nhttps://github.com/kubeflow/kubeflow/blob/master/components/centraldashboard/Makefile#L2",
        "> @kimwnasptd I don't like that this applies the manifests and then patches them after, this will result in a generation of the pod that fails to run, quickly followed by one with the correct image.\r\n\r\nWhy is this a hard problem? The deployment will be patched immediately, and we wait for the Deployment object either way\r\n\r\n> I don't understand what was not working with the old sed approach, which modified the manifests before applying them?\r\n\r\nPreviously the flow was:\r\n1. build the manifests\r\n    1. These manifests in `master` would have `latest` tag in images\r\n    2. In a release branch we update the tag, so it would be something like `v1.8.0-rc.0`\r\n3. `sed` was used change the image with tag `latest`, to `${{env.TAG}}`\r\n4. The above worked for `master`, which has tag `latest` but doesn't for release branches\r\n\r\nSo the script works for `master`, because `latest` image tag is used in manifests, but fails in release branches which use a different tag in manifests.",
        "Lets remove the `logo` sections in the ConfigMap. As mentioned above they add a lot of boilerplate code that most of the users won't need to configure.",
        "@DavidSpek taking your comment https://github.com/kubeflow/kubeflow/pull/5646#issuecomment-801308346 here to have everything coordinated in this place\r\n\r\n> @kimwnasptd Could we do some sort of combination of the things you describe and what is being done now? More specifically, allow the user to set the URLs for the icons in the config map. \r\n\r\nThis is the initial idea I had https://github.com/kubeflow/kubeflow/pull/5646#issuecomment-797521345, although thinking about it more I believe it will be more involved to get it working. In this case we will need to first fetch the yaml [ and the request could fail ] and then inject the urls and use them.\r\n\r\n> Due to the use of trademarks and the applicable guidelines I think it is import to make it very easy for people to remove the logos (preferably without needing to build their own image).\r\n\r\nCould you elaborate a little bit more on this? How does the trademarks and applicable guidelines fit into the picture?\r\n\r\nI agree that we should make it easy for people to swap the logos they prefer, but these users are the minority and most probably won't have a hard time to rebuild the image.",
        "From what I understand the issue is not how the icon will appear to the UI [ via the backend directly, fetched from somewhere public etc ] but the fact that we use this logo in the software. With that I mean that if someone would use the app he would end up seeing the logo, which could imply that this is endorsed from RStudio etc.\r\n\r\nAlso an issue with fetching resources in the UI directly from the public internet is that the app unable will not work in air-gaped environments, where the users would only have access to specific IPs.\r\n\r\n",
        "So judging from our current situation, where we are tied by time constraints and the trademark situation would need some communication and tip toeing into what it's acceptable I would propose the following solution to unblock ourselves:\r\n\r\n1. Don't use the logos in this PR. We could use the [group options](https://v8.material.angular.io/components/select/overview#creating-groups-of-options) from Angular material, or just use buttons with text [ would prefer the first approach tbh ] to show the images for the different servers\r\n2. Open an issue about it and involve @castrojo in this. And once we figure out what we are allowed to do with the rstudio trademark we can open a new PR and add this functionality. We've done it once here so this won't be difficult to do in the future\r\n\r\nWDYT?",
        "Let's go with using the svgs. We'll use links to fetch them from where they publicly host them. Distributions could make a small change and add their svgs to the app and simply change these links, which is something that we were OK with from the beginning. Lets just add a small section for this in the app's README.\r\n\r\nRegarding the responsibility part, I'm not a layer on this so not an expert but we are a team and all of us move forward together. We are not going to point fingers to people if things go south. And also since I'm a reviewer and agreed to move forward with this so I'm also responsible.\r\n\r\nBut if indeed people complain about the usage then we will sincerely apologize and just change this to something like we discussed above. "
      ],
      "kubeflow-check-before-use": [
        "Let's move this part completely outside of this function and treat the Pod as the StatefulSet. \r\n\r\nThe main reconciliation logic is responsible for finding the objects (Pod, StatefulSet) and our function will need to check if they are `nil` or not",
        "This nesting level can be simplified by checking the incoming `pod` object:\r\n\r\n```golang\r\nif pod == nil {\r\n    log.Info(\"No pod found. Won't update notebook conditions and containerState\")\r\n    return status, nil\r\n}\r\n\r\n```"
      ],
      "kubeflow-use-table-driven-tests": [
        "Let's change the name of this function to `createNotebookStatus` and update it's signature to:\r\n```golang\r\nfunc createNotebookStatus(r *NotebookReconciler, nb *v1beta1.Notebook,\r\n\tsts *appsv1.StatefulSet, pod *corev1.Pod, req ctrl.Request) (v1beta1.NotebookStatus, error) {\r\n```\r\n\r\nThis means that:\r\n1. We will be passing all necessary objects, statefulset, pod, as arguments\r\n2. The function will be just calculating the status\r\n\r\nThis will also allow us to write some unit tests to ensure we calculate the status as expected.",
        "Nice! Let's also include another unit test for the case where the Notebook's Pod is unschedulable"
      ],
      "kubeflow-specific-network-access-documentation": [
        "Could you instead have a list for this numbered step and expose each `kubectl port-forward` command that a user needs to run?\r\n\r\nWe can just have commands for the Services that we proxy in the webpack config",
        "Let's add this explanation at the numbered step, since it explains how the proxying works. And, as mentioned above, have the bullet list for the different proxying commands",
        ">This continuous failure to mutate the target pod may block other target pods from mutation\r\nuntil the failing process ends.\r\n\r\nCould you clarify a little bit more what you mean here? By other Pods you are not referring to other unrelated Pods in the cluster right?\r\n\r\nIIUC, as you described, the Deployment/StatefulSet will be retrying to create the Pod which will keep failing. Are there other side effects to other pods?"
      ],
      "kubeflow-document-code-thoroughly": [
        "Could you add a comment here to document that the status of the CR will be updated based on the `ContainerState` of the container that has the same name with the CR?"
      ]
    }
  },
  "vicferpoy": {
    "repos": [
      "prowler-cloud/prowler"
    ],
    "entries": [
      {
        "slug": "prowler-configure-observability-variables",
        "title": "Configure observability variables"
      },
      {
        "slug": "prowler-document-dependency-versioning",
        "title": "Document dependency versioning"
      },
      {
        "slug": "prowler-ensure-migration-compatibility",
        "title": "Ensure migration compatibility"
      },
      {
        "slug": "prowler-flexible-ai-model-versions",
        "title": "Flexible AI model versions"
      },
      {
        "slug": "prowler-least-privilege-principle",
        "title": "Least privilege principle"
      },
      {
        "slug": "prowler-meaningful-consistent-naming",
        "title": "Meaningful consistent naming"
      },
      {
        "slug": "prowler-parameterize-configuration-values",
        "title": "Parameterize configuration values"
      },
      {
        "slug": "prowler-parameterize-similar-tests",
        "title": "Parameterize similar tests"
      },
      {
        "slug": "prowler-prioritize-code-readability",
        "title": "Prioritize code readability"
      },
      {
        "slug": "prowler-service-layer-abstraction",
        "title": "Service layer abstraction"
      },
      {
        "slug": "prowler-specific-exception-handling",
        "title": "Specific exception handling"
      },
      {
        "slug": "prowler-tenant-aware-query-optimization",
        "title": "Tenant-aware query optimization"
      },
      {
        "slug": "prowler-test-authentication-dependencies",
        "title": "Test authentication dependencies"
      },
      {
        "slug": "prowler-use-configurable-default-values",
        "title": "Use configurable default values"
      }
    ],
    "comments": {
      "prowler-document-dependency-versioning": [
        "Why is this required? I don't think this is an API dependency.",
        "Same here. We should not update the root pyproject if this PR is API specific."
      ],
      "prowler-least-privilege-principle": [
        "We don't. Thank you for noticing."
      ],
      "prowler-parameterize-configuration-values": [
        "Should we control this version through environment variable?",
        "We could define this version as the default value in case the environment variable is not present. Does this make sense to you?",
        "cc: @jfagoagas since you asked Adri to store the version staticly."
      ],
      "prowler-parameterize-similar-tests": [
        "Use pytest parametrization here, please. Easier to read, maintain and add new tests if required.",
        "Use pytest parametrization here, please. Easier to read, maintain and add new tests if required.",
        "This is not a unit test if you depend on a POST for a retrieval. Use pytest fixtures instead so you already have items to work with in the database when the test begins.",
        "Apply this principle on every other test, please."
      ],
      "prowler-configure-observability-variables": [
        "Does Sentry work if this variable is not set?"
      ],
      "prowler-service-layer-abstraction": [
        "I don't think this belongs in here. It also makes unit testing way more difficult. Please create a service layer to encapsulate all the logic related to the s3 integration."
      ],
      "prowler-prioritize-code-readability": [
        "I believe Adri already changed this. I agree it's better this way. We're getting older and I would have had issues trying to understand that piece of code in a few months ðŸ¤£ ",
        "I think for consistency with other serializers, you should define all the expected fields as in the read serializer and then using `extra_kwargs` to define them as `read_only`.",
        "I believe this is not correct. This function should return something, not update a internal attribute. Why did you modify the former logic? Since you are setting the base required permissions as `required_permissions = [Permissions.MANAGE_USERS]`, I believe this function should be something like:\r\n\r\n```python\r\n    def get_required_permissions(self):\r\n        \"\"\"\r\n        Returns the required permissions based on the request method.\r\n        \"\"\"\r\n        if self.action == \"me\":\r\n            # No permissions required for me request\r\n            return []\r\n        else:\r\n            # Require permission for the rest of the requests\r\n            return self.required_permissions\r\n```",
        "I thought this was one of the Django viewset functions. If it is auxiliary and our own, it's fine. Maybe consider renaming it as you suggested in other comment. "
      ],
      "prowler-use-configurable-default-values": [
        "I think this should come from an environment variable that takes `AUTH_URL` as base. But it shouldn't be hardcoded in the backend since this is a URL from the UI."
      ],
      "prowler-ensure-migration-compatibility": [
        "You didn't update the migration file after changes in `models.py`.",
        "What happens when we perform this migration on an already existing database with findings? What value do the older findings take? I believe we are missing, at least, `null=True` here and in the migration.",
        "> Sorry, this comment was meant to be added with the last review but I missed it somehow.",
        "Every time we add new `RunSQL` or `RunPython` migrations, we need to define the reverse operation and test it."
      ],
      "prowler-meaningful-consistent-naming": [
        "Consistency. Every task had that suffix, except this one.",
        "Does `NoName` come from any convention or something?",
        "Use `N/A` if possible then. For consistency with other wildcards.",
        "```suggestion\r\nclass SAMLConfiguration(RowLevelSecurityProtectedModel):\r\n```",
        "https://www.django-antipatterns.com/antipattern/plural-model-class-names.html",
        "We talk about `reports` everywhere. Since it is a `RLSTask`, it means this task can be checked by the user using the `/tasks` endpoints. Shouldn't we call it `scan-report`?\r\n\r\nAlso, the queue concerns me. Regarding Prowler Cloud, we do not have active workers for the `scans` queue. This means that we will have to adapt more logic to deploy another ECS for this kind of tasks.",
        "There are a few viewsets that do not implement this method. How is it working?",
        "Yes, I agree. Thanks for clarifying."
      ],
      "prowler-test-authentication-dependencies": [
        "We need to make sure the social login still works as intended because we had hard dependencies between the `dj-rest-auth` library and `django-allauth`.",
        "I will assume this was tested again and everything was correct. Thank you."
      ],
      "prowler-specific-exception-handling": [
        "Please, update regexes and detail messages, but don't modify the current logic. Use `ModelValidationError` with all the required fields. This breaks this exception."
      ],
      "prowler-flexible-ai-model-versions": [
        "This requires its own Postgres enum. Check `db_utils.py` for reference.",
        "Okay, fair. But in any case, this should be used for a ChoicesField, not a Char.",
        "Change it to use the enum after you create it. It should be a choice.",
        "Updated related comment: https://github.com/prowler-cloud/prowler/pull/7848#discussion_r2120913217"
      ],
      "prowler-tenant-aware-query-optimization": [
        "```suggestion\r\n        return LighthouseConfig.objects.filter(tenant_id=self.request.tenant_id)\r\n```\r\n\r\nEvery tenant would have its own config, isn't it? This would improve performance.",
        "Can't we move this logic inside the `_create_finding_delta` (consider renaming it if necessary) to avoid making extra queries when not needed? In that function, we retrieve the latest finding with the same `uid` to get the `delta` value. If that finding's `first_seen` is not `None`, we can just copy it from there without having to query again."
      ]
    }
  },
  "ggerganov": {
    "repos": [
      "ggml-org/llama.cpp"
    ],
    "entries": [
      {
        "slug": "llama.cpp-ai-parameter-organization",
        "title": "AI parameter organization"
      },
      {
        "slug": "llama.cpp-api-minimalism-principle",
        "title": "API minimalism principle"
      },
      {
        "slug": "llama.cpp-choose-appropriate-error-mechanism",
        "title": "Choose appropriate error mechanism"
      },
      {
        "slug": "llama.cpp-eliminate-code-duplication",
        "title": "eliminate code duplication"
      },
      {
        "slug": "llama.cpp-enable-callback-chaining",
        "title": "Enable callback chaining"
      },
      {
        "slug": "llama.cpp-explicit-control-flow-logic",
        "title": "explicit control flow logic"
      },
      {
        "slug": "llama.cpp-follow-naming-conventions",
        "title": "Follow naming conventions"
      },
      {
        "slug": "llama.cpp-maintain-code-consistency",
        "title": "maintain code consistency"
      },
      {
        "slug": "llama.cpp-maintain-consistent-naming-patterns",
        "title": "Maintain consistent naming patterns"
      },
      {
        "slug": "llama.cpp-maintain-naming-consistency",
        "title": "maintain naming consistency"
      },
      {
        "slug": "llama.cpp-measure-algorithm-performance-impact",
        "title": "measure algorithm performance impact"
      },
      {
        "slug": "llama.cpp-measure-before-optimizing",
        "title": "measure before optimizing"
      },
      {
        "slug": "llama.cpp-metal-shared-memory-sizing",
        "title": "Metal shared memory sizing"
      },
      {
        "slug": "llama.cpp-optimize-algorithmic-complexity",
        "title": "optimize algorithmic complexity"
      },
      {
        "slug": "llama.cpp-use-environment-variables",
        "title": "Use environment variables"
      },
      {
        "slug": "llama.cpp-use-model-metadata",
        "title": "use model metadata"
      },
      {
        "slug": "llama.cpp-validate-bounds-before-access",
        "title": "validate bounds before access"
      }
    ],
    "comments": {
      "llama.cpp-use-model-metadata": [
        "This should be handled by the meta data in the GGUF model. There is a boolean field for when BOS is needed or not."
      ],
      "llama.cpp-eliminate-code-duplication": [
        "After adding the broadcast support to `ggml_set_rows()` this is not really needed anymore, but I think it's nice to have either way.",
        "Ok, will add template in a follow-up PR. For now, removed the i64 support and added TODO."
      ],
      "llama.cpp-use-environment-variables": [
        "Avoid this by checking an environment variable instead. See `GGML_SCHED_DEBUG` for an example."
      ],
      "llama.cpp-maintain-code-consistency": [
        "For long argument lists, list them on new lines to improve readibility.",
        "This method is analogous to the `build_attn_inp_` methods, so we have to model it in a similar way.\r\n\r\nReplace this method with:\r\n\r\n```c++\r\n    // similar to build_attn_inp_kv_unified()\r\n    llm_graph_input_rs * build_rs_inp() const;\r\n```\r\n\r\nIntroduce new input class:\r\n\r\n```c++\r\n// similar to llm_graph_input_attn_kv_unified\r\n// put the `s_copy` tensor in this class (similar to `kq_mask`)\r\nclass llm_graph_input_rs : public llm_graph_input_i;\r\n```\r\n\r\nIn the future, this input class could be extended with additional input tensors that are needed by the recurrent cache if necessary (similar to the attention input classes).\r\n\r\nReplace `build_recurrent_state()` and `build_rwkv_shift_load()` with overloads:\r\n\r\n```c++\r\n    // similar to build_attn()\r\n    ggml_tensor * build_rs(\r\n        llm_graph_input_rs * inp,\r\n             ggml_cgraph * gf,\r\n             ggml_tensor * s,             \r\n                 int32_t   state_size,\r\n                 int32_t   n_seqs,\r\n                    bool   avoid_copies = false) const;\r\n\r\n    // similar to build_attn()\r\n    ggml_tensor * build_rwkv_token_shift_load(\r\n        llm_graph_input_rs * inp,\r\n             ggml_cgraph * gf,\r\n      const llama_ubatch & ubatch,\r\n                     int   il) const;\r\n```\r\n\r\n",
        "When this change is applied, we have to do a similar addition for the hybrid implementation. The basic pattern is that you need to introduce a new input class similar to `llm_graph_input_attn_kv_unified` and `llm_graph_input_rs`, but this one will contain inputs for both the attention and for the recurrent state.\r\n\r\nSo probably something like:\r\n\r\n```c++\r\n// this input class will have both the input tensors needed for the attention and for\r\n// the recurrent state. see llm_graph_input_attn_kv_unified_iswa for example\r\nclass llm_graph_input_mem_hybrid : public llm_graph_input_i;\r\n```\r\n\r\nWe then add overloads for `build_attn()`, `build_rs()` and `build_rwkv_token_shift_load()`.",
        "For now it's important to follow the existing patterns even if there is some extra code duplication. We can rework the implementation if needed in separate refactor PRs.\r\n\r\nThe recommended way to avoid large duplications is how the `build_attn()` is implemented to use a helper, memory-agnostic method `build_attn_mha()`. This way, the memory-specific logic is implemented in the `build_attn()` overloads and the bulk of the remaining logic is reused by calling `build_attn_mha()`. You can apply this pattern both for `build_rs()` and `build_mambaX_layer()`.",
        "Looking at the `build_mamba_layer()` function, it appears complex, but actually it boils down to something like this:\r\n\r\n```c++\r\nbuild_mamba_layer(inp) {\r\n    rec_state = static_cast<...>(mstate);\r\n\r\n    conv = rec_state->update_conv(inp); // analogous to unified get_k() + cpy_k()\r\n    ssm  = rec_state->update_ssm (inp); // analogous to unified get_v() + cpy_v()\r\n\r\n    common = get_common(model);\r\n\r\n    do_something_with_conv(model, common, conv);\r\n    do_something_with_ssm (model, common, ssm);\r\n}\r\n```\r\n\r\nThe `get_common(model)` and `do_something_with_...(model, ...)` parts have to be extracted out of this function and remain implemented in `llama-model.cpp` because they use model tensors and do not depend on the memory module. The `update_conv()` and `update_ssm()` function have to be implemented only once in the `class llama_kv_cache_recurrent`. Internally, they can use a helper `build_recurrent_state()` function to deduplicate the code. Note that the hybrid cache will call these methods from the recurrent cache instance and won't have to implement them a second time.\r\n\r\nWhen you decompose the function like this, the implementation becomes much simpler:\r\n\r\n```c++\r\nbuild_mamba_layer(inp) {\r\n    rec_state = static_cast<...based_on_inp...>(m_state);\r\n\r\n    conv = rec_state->update_conv(inp);\r\n    ssm  = rec_state->update_ssm (inp);\r\n\r\n    return { conv, ssm };\r\n}\r\n```\r\n\r\nThis is much easier to overload multiple times for different `inp` types. I would even split it one more time:\r\n\r\n```c++\r\n// llama-graph:\r\n\r\n// one for recurrent and one for hybrid\r\nbuild_mamba_conv(inp) {\r\n    rec_state = static_cast<...based on inp...>(m_state);\r\n\r\n    conv = rec_state->update_conv(inp->get_s_copy());\r\n    \r\n    return conv;\r\n}\r\n\r\n// one for recurrent and one for hybrid\r\nbuild_mamba_ssm(inp) {\r\n    rec_state = static_cast<...based on inp...>(m_state);\r\n\r\n    ssm = rec_state->update_ssm(inp->get_s_copy());\r\n    \r\n    return ssm;\r\n}\r\n```\r\n\r\nI hope I'm not missing something, but I think this should work and should be clean.\r\n\r\nAlternatively, we can also just bring the PR to a working state any way you think makes sense and then I will try to do a refactoring pass and see how it goes. Would just need some sample test commands to experiment with."
      ],
      "llama.cpp-measure-algorithm-performance-impact": [
        "I am not convinced that we have to add this new implementation.\r\n\r\nCan you provide a reference for the PlaMo-2 tokenizer so we can understand how it differs from the existing tokenizer algorithms?",
        "Regarding the implementation here - it has to follow the existing pattern for all other tokenizers. No need to create a new `class llama_vocab_plamo2`.",
        "> ... handle multiple stop words efficiently - with grammar trigger words we may have many\r\n\r\nCan you clarify why is that and how many stop words we could have in typical use cases?",
        "I see. I don't have a good sense of the computation complexity for finding the trigger words in typical use cases. If we can show that the algorithm improves the performance in a measurable way, then it's ok. If not, we might want to fallback to some simpler brute-force approach."
      ],
      "llama.cpp-optimize-algorithmic-complexity": [
        "```suggestion\r\n#ifdef __ARM_FEATURE_MATMUL_INT8\r\n    assert((nrc == 2) || (nrc == 1));\r\n#else\r\n    assert(nrc == 1);\r\n#endif\r\n```"
      ],
      "llama.cpp-follow-naming-conventions": [
        "The convention is to match the enum name with the prefix of the values:\r\n\r\n```suggestion\r\nenum diffusion_alg {\r\n    DIFFUSION_ALG_ORIGIN       = 0,\r\n    DIFFUSION_ALG_MASKGIT_PLUS = 1,\r\n    DIFFUSION_ALG_TOPK_MARGIN  = 2,\r\n    DIFFUSION_ALG_ENTROPY      = 3,\r\n};\r\n```",
        "Add a `DEPRECATED` overload of this call to make it easier for developers to adjust to the change.",
        "Ah, we break the `llama_sampler_init_...` pattern if we do it like this. Maybe like this instead:\r\n\r\n```c\r\nllama_sampler_init_grammar(...); // same as master\r\nllama_sampler_init_grammar_lazy(...); // same as PR but without `bool lazy`?\r\n```\r\n\r\np.s. huh, naming things is difficult ðŸ˜„ ",
        "Can this be simply `LLM_KV_MAMBA_RMS_NORM`? If there isn't anything very specific for Falcon H1, it's better to keep the names generic.",
        "Should this be just `ssm_d_ssm`?\r\n\r\nGenerally, I think the `mamba` prefix is not needed here. For example:\r\n\r\n```\r\nmamba_rms_norm -> ssm_rms_norm\r\nmamba_expand -> ssm_expand\r\n```\r\n\r\n@compilade Do you agree?",
        "For consistency, rename the var:\r\n\r\n```suggestion\r\n    std::string api_prefix = \"\";                                                                         // NOLINT\r\n```"
      ],
      "llama.cpp-metal-shared-memory-sizing": [
        "You can always allocate `32*sizeof(float)` bytes of shared memory for simplicity:\r\n\r\n```suggestion\r\n                    const int64_t shmem_size = 32;\r\n```\r\n\r\nNote that shared memory buffers in metal require to have size multiple of 16:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/e2b7621e7c265a6739225125cf9c534f471b3472/ggml/src/ggml-metal/ggml-metal.m#L4912-L4916\r\n\r\nBy forcing `shmem_size = 32;` you handle this requirement."
      ],
      "llama.cpp-explicit-control-flow-logic": [
        "> For example, a sequence set containing multiple seq_ids cannot be mixed with one having a seq_id in the multi-sequence set.\r\n\r\nYes, this logic here at the beginning of the function determines the unique non-overlapping sequence sets that will be contained in this ubatch:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/034b0557987c31d624854e923b505d4541399c7b/src/llama-batch.cpp#L421-L446"
      ],
      "llama.cpp-ai-parameter-organization": [
        "This is already available from the `hparams` - no need to duplicate it here.",
        "`max_length` should be removed and the existing `n_ubatch` parameter should be used instead.",
        "Normally, we don't put the `vocab_size` as `hparam`. Instead, we pick it from the `llama_vocab`. So this is likely not needed.",
        "This parameter should can be avoided.\r\n\r\nSee the logic here:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/a3403ae3502b5f4d562b727bca8c810a3b212198/src/llama-model.cpp#L529-L538\r\n\r\nAnd as an example how we apply it for the Gemma model which can have a custom attention head size like in your case:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/a3403ae3502b5f4d562b727bca8c810a3b212198/src/llama-model.cpp#L1021-L1025"
      ],
      "llama.cpp-api-minimalism-principle": [
        "What is the reason to implement this inside the `llama_context` as opposed to in the diffusion example itself? If we do so, the user would have much more control over the inference and we won't have to design around this new API.",
        "It should be moved to the example. The `libllama` API should be as minimal as possible without redundant interfaces. Wrappers are implemented in user code (e.g. `libcommon`/`examples`/`tools`).",
        "If this is the only use case of `llama_sampler_accept_str()` maybe we can simply extend `llama_sampler_init_grammar()` to accept an initial string (i.e. the trigger) and avoid extending the API.",
        "> Oh, you mean to move the trigger logic in the grammar itself?\r\n\r\nNo, I had in mind something more like this:\r\n\r\n```c\r\n// llama.h\r\n    LLAMA_API struct llama_sampler * llama_sampler_init_grammar(\r\n            const struct llama_model * model,\r\n                          const char * grammar_str,\r\n                          const char * grammar_root,\r\n                          const char * init_str); // optionally used to feed initial data to the grammar state\r\n\r\n// common/sampling.cpp\r\n    gsmpl->grmr  = llama_sampler_init_grammar(model, gsmpl->params.grammar.c_str(), \"root\", trigger.c_str());\r\n    \r\n    return true;\r\n```\r\n\r\nThe logic for triggering remains in user code."
      ],
      "llama.cpp-maintain-consistent-naming-patterns": [
        "Using `shortconv` prefix would be more consistent with the existing naming pattern:\n\n```suggestion\n    MODEL_TENSOR.SHORTCONV_CONV:            \"shortconv.{bid}.shortconv.conv\",\n    MODEL_TENSOR.SHORTCONV_INPROJ:          \"shortconv.{bid}.shortconv.in_proj\",\n    MODEL_TENSOR.SHORTCONV_OUTPROJ:         \"shortconv.{bid}.shortconv.out_proj\",\n```"
      ],
      "llama.cpp-choose-appropriate-error-mechanism": [
        "I don't understand the change - if `repack()` returns -1, we will immediately assert on line 405:\r\n\r\n```c++\r\n    auto OK            = tensor_traits->repack(tensor, data, size);\r\n\r\n    GGML_ASSERT(OK == 0);\r\n```",
        "In the compute calls, it's better to keep the `GGML_ASSERT`s. These would never get called if `repack()` has already bailed."
      ],
      "llama.cpp-measure-before-optimizing": [
        "I think it's OK to enable by default. It requires the `LLAMA_SET_ROWS` anyway for the majority of models (except cacheless embedding models such as BERT), so it will take some time before it gets actually enabled. Should be enough to spot any potential problems until then.",
        "Yes, the time for reset is microscopic. I will simplify as suggested.",
        "It's done once per completion request, at the beginning, upon processing the input json parameters.",
        "I fixed this anyway: https://github.com/ggml-org/llama.cpp/pull/14721"
      ],
      "llama.cpp-validate-bounds-before-access": [
        "I used the `.at()` method: [11ee725](https://github.com/ggml-org/llama.cpp/pull/14363/commits/11ee725a373f8a3ec8f9c8bd94cdd99e72fcd501)",
        "I missed that. Fixing.",
        "This is slightly more future-proof version:\r\n\r\n```suggestion\r\n            GGML_ASSERT(src1->type == GGML_TYPE_I32);\r\n            int64_t row_idx = ((const int32_t *)src1->data)[i];\r\n            GGML_ASSERT(row_idx >= 0 && row_idx < src0->ne[1]);\r\n```\r\n\r\nAt some point in the future we might consider changing the indices of `ggml_get_rows()` to become I64 so this assert will be helpful."
      ],
      "llama.cpp-maintain-naming-consistency": [
        "```suggestion\r\n        params.send_progress = json_value(data, \"send_progress\", false);\r\n```",
        "For consistency, all of these should either be byte offsets or element offsets, but not both.",
        "For consistency with the other arch names, use a dash instead of underscore:\r\n\r\n```suggestion\r\n    { LLM_ARCH_FALCON_H1,        \"falcon-h1\"        },\r\n```"
      ],
      "llama.cpp-enable-callback-chaining": [
        "I wonder if instead of extending the `llama_sampler` API, it would be better to pass necessary callbacks (such as `is_empty`, `accept_str`, etc.) through the `llama_sampler_init_grammar()` call."
      ]
    }
  },
  "ritchie46": {
    "repos": [
      "pola-rs/polars"
    ],
    "entries": [
      {
        "slug": "polars-appropriate-error-handling",
        "title": "Appropriate error handling"
      },
      {
        "slug": "polars-ci-workflow-configuration-best",
        "title": "CI workflow configuration best"
      },
      {
        "slug": "polars-consistent-naming-standards",
        "title": "Consistent naming standards"
      },
      {
        "slug": "polars-defer-expensive-operations",
        "title": "Defer expensive operations"
      },
      {
        "slug": "polars-evaluate-algorithmic-complexity-tradeoffs",
        "title": "Evaluate algorithmic complexity tradeoffs"
      },
      {
        "slug": "polars-explicit-configuration-precedence",
        "title": "Explicit configuration precedence"
      },
      {
        "slug": "polars-explicit-null-handling",
        "title": "Explicit null handling"
      },
      {
        "slug": "polars-extract-duplicated-code",
        "title": "Extract duplicated code"
      },
      {
        "slug": "polars-favor-clarity-over-brevity",
        "title": "Favor clarity over brevity"
      },
      {
        "slug": "polars-feature-flag-compatibility",
        "title": "Feature flag compatibility"
      },
      {
        "slug": "polars-hide-implementation-details",
        "title": "Hide implementation details"
      },
      {
        "slug": "polars-names-reveal-clear-intent",
        "title": "Names reveal clear intent"
      },
      {
        "slug": "polars-optimize-data-transformations",
        "title": "Optimize data transformations"
      },
      {
        "slug": "polars-optimize-memory-allocation-patterns",
        "title": "Optimize memory allocation patterns"
      },
      {
        "slug": "polars-organize-tests-efficiently",
        "title": "Organize tests efficiently"
      },
      {
        "slug": "polars-prevent-cryptic-errors",
        "title": "Prevent cryptic errors"
      },
      {
        "slug": "polars-prevent-deadlock-conditions",
        "title": "Prevent deadlock conditions"
      },
      {
        "slug": "polars-safe-null-handling",
        "title": "Safe null handling"
      }
    ],
    "comments": {
      "polars-organize-tests-efficiently": [
        "Can this be done in-memory with `io.BytesIO()`. We prefer in-memory tests when possible as this is faster."
      ],
      "polars-consistent-naming-standards": [
        "`use_abs_path`.\r\n\r\nWe have a convention that separate words are `_` separated. ",
        "Can we use python `None`  directly (so not part of this set.\r\n\r\nThe argument then becomes `arg: MaintainOrder | None`\r\n\r\nPS. I also think we should name it `MaintainOrderJoin`"
      ],
      "polars-defer-expensive-operations": [
        "I don't think we need an extra benchmark for this.",
        "No, but the goal isn't to hit everything with benchmarks. ",
        "I understand what benchmarks do. :) I think the change is good, but I want to get rid of the in-repo benchmarks, I am not happy with them on the shared runners. We have on-premise benchmarks running. \r\n\r\nCan you remove this?"
      ],
      "polars-ci-workflow-configuration-best": [
        "Don't publish to pypi if this fails.",
        "Don't we need to include all variants then? (E.g. is the exclude set not smaller than the include set?)",
        "Right, I think you're right. I also do need to think harder with the include set. :laughing: "
      ],
      "polars-feature-flag-compatibility": [
        "The feature is `bigidx`, not u64 idx in Rust."
      ],
      "polars-optimize-data-transformations": [
        "You don't have to:\r\n\r\n\r\n```rust\r\n matches!(\r\n        function,\r\n        FunctionExpr::Range(RangeFunction::IntRange { .. })\r\n    );\r\n```\r\n",
        "I think we should do this check during the IR::conversion, in `resolve_groupby` here:\r\n\r\nhttps://github.com/pola-rs/polars/blob/05f2abbf1b7f76f0b34c3c552fc33aa6da186561/crates/polars-plan/src/plans/conversion/dsl_to_ir.rs#L1013",
        "Ah, right. I see that we first do a `with_columns` here. We should store the `index` column on line 1155 and do the `with_column` rewrite during IR conversion.\r\n\r\nI understand it's a bit more than you anticipated. I can do the pre-work for that later if you like?",
        "Don't create `tmp_df` as what it does now.\r\n\r\nLet's say we have `pivot_df: A, B, C` and we create `value_col: A` from `pivot_df`. By concatting/hstacking we create:\r\n\r\n`tmp_df: A, A, B, C`. Where we just want to pass the context of `pivot_df` to the aggregation. So we can pass  `pivot_df` directly to `expr.evaluate`.",
        "Could you clarify how you distinct \"scanned\" from \"read\"? \r\n"
      ],
      "polars-optimize-memory-allocation-patterns": [
        "Maybe we can hoist the writer out of the while loop?"
      ],
      "polars-safe-null-handling": [
        "We can check `is_nan` on the floats.\r\n\r\nWe can make our own `NonNan` wrapper type in `polars-utils`, which does this check on entry.",
        "No need to use unsafe for a single value.",
        "No need to use unsafe for a single value.",
        "No need to use unsafe for a single value.",
        "No need to use unsafe for a single value.",
        "No need to use unsafe for a single value.",
        "Can this return `Option<bool>`? On `None` we don't know and we must continue. But on `Some<false>` we could already error early."
      ],
      "polars-explicit-null-handling": [
        "This is wrong. If our sum doesn't ignore nulls, it doesn't propagate them, but replaces them with the identity: 0.\r\n\r\nThe horizontal semantics should be the same as the vertical semantics.",
        "I mean that our `sum` is agnostic to nulls. I think we made a mistake exposing this to `sum_horizontal` as our vertical sum is agnostic to nulls.",
        "Yeah, I think you're right. Consider it an observation. ;) Will take a look a bit later. "
      ],
      "polars-explicit-configuration-precedence": [
        "We should soften this guarantee. We will try to run on this engine, but don't guarantee it. Both streaming and gpu have cpu fallbacks."
      ],
      "polars-appropriate-error-handling": [
        "If it is an implementation error on our end, we can panic.",
        "Instead of panicking, this should just raise an unsupported error. I don't want to put PR links in error messages either."
      ],
      "polars-prevent-cryptic-errors": [
        "The guard position should be at the generic location. I believe that is in `_parse_inputs_as_iterable`.",
        "Yes, that's good. In `select` we also should not accept dictionaries."
      ],
      "polars-extract-duplicated-code": [
        "Can we move this to a separate function so that implementation is separate from dispatch?",
        "Can we add a `schema: Option<Schema>` argument to `prepare_expression_for_context` to reduce duplication.",
        "This is also converted in `IR::Scan` can we factor this out into a function?",
        "Can we do that in this PR? \r\n\r\nMake a function and factor out the shared arguments?",
        "I believe this code is exactly the same as in collect. Can we put it in a function?"
      ],
      "polars-names-reveal-clear-intent": [
        "Given that our schema conflicts with the catalog schema definition. Shall we name it `create_namespace` and mention in the docstrings that we mean catalog schema's for that?\r\n"
      ],
      "polars-evaluate-algorithmic-complexity-tradeoffs": [
        "I do wonder though if it is worth the extra binary bloat. The `is_nan` will be correctly predicted on every non-nan, until we hit it. On that mis prediction we are done, so I think it doesn't really matter and we can save a monomorphized function.",
        "This allocates a new vec. We should be able to gather without reallocing.",
        "I think this should be supported. For decimals we should extract the `Int128` in both the haystack and the needle. \r\n\r\nAnd for the nested types we should convert both to the row-encoding."
      ],
      "polars-favor-clarity-over-brevity": [
        "Let's make this required keyword arguments:\r\n\r\n`self, * , dtype: PolarsDataType | type[Any], endianness: Endianness = \"little\"`"
      ],
      "polars-hide-implementation-details": [
        "We call that the shape in public API of Polars:\r\n\r\nhttps://docs.pola.rs/api/python/stable/reference/api/polars.datatypes.Array.html\r\n\r\nNit:\r\n\r\nI also would not mention that array's are sequentially nested. I want users to think of NDArrays. That we sequentially nest them is an implementation detail.",
        "The Polars public API strongly prefers full names. So this should be `\"ir\", \"physical\"`. (I think IR is common enough to not write it out. :) )"
      ],
      "polars-prevent-deadlock-conditions": [
        "We should use the `enter_polars` which handles the `allow_threads`."
      ]
    }
  },
  "JohannesGaessler": {
    "repos": [
      "ggml-org/llama.cpp"
    ],
    "entries": [
      {
        "slug": "llama.cpp-ai-parameter-organization",
        "title": "AI parameter organization"
      },
      {
        "slug": "llama.cpp-api-minimalism-principle",
        "title": "API minimalism principle"
      },
      {
        "slug": "llama.cpp-consistent-clear-naming",
        "title": "consistent clear naming"
      },
      {
        "slug": "llama.cpp-consolidate-algorithmic-patterns",
        "title": "consolidate algorithmic patterns"
      },
      {
        "slug": "llama.cpp-explicit-control-flow-logic",
        "title": "explicit control flow logic"
      },
      {
        "slug": "llama.cpp-explicit-performance-handling",
        "title": "explicit performance handling"
      },
      {
        "slug": "llama.cpp-maintain-code-consistency",
        "title": "maintain code consistency"
      },
      {
        "slug": "llama.cpp-maintain-naming-consistency",
        "title": "maintain naming consistency"
      },
      {
        "slug": "llama.cpp-measure-before-optimizing",
        "title": "measure before optimizing"
      },
      {
        "slug": "llama.cpp-optimize-memory-access-patterns",
        "title": "optimize memory access patterns"
      },
      {
        "slug": "llama.cpp-prefer-const-variables",
        "title": "prefer const variables"
      },
      {
        "slug": "llama.cpp-specify-naming-formats-explicitly",
        "title": "Specify naming formats explicitly"
      },
      {
        "slug": "llama.cpp-systematic-test-coverage",
        "title": "systematic test coverage"
      },
      {
        "slug": "llama.cpp-use-strongly-typed-configurations",
        "title": "Use strongly typed configurations"
      },
      {
        "slug": "llama.cpp-validate-configuration-options-explicitly",
        "title": "validate configuration options explicitly"
      }
    ],
    "comments": {
      "llama.cpp-maintain-code-consistency": [
        "```suggestion\r\n    enum ggml_opt_optimizer {\r\n```\r\n\r\nFor consistency with the surrounding code.",
        "Move this declaration upwards so that it's in the same place as the other getters for `ggml_opt_context` (remember to also move the implementation)."
      ],
      "llama.cpp-use-strongly-typed-configurations": [
        "Can't this be determined automatically?",
        "I think an enum is better than a string.\r\n\r\nThe way I would implement the automatic detection is to try loading GGUF and Parquet first. The order shouldn't matter since the loading will fail if there is a mismatch. If both fail, load as plain text. I don't think fallback logic beyond that is needed."
      ],
      "llama.cpp-consolidate-algorithmic-patterns": [
        "```suggestion\r\n#if defined(AMD_MMA_AVAILABLE)\r\n        int64_t * xi = (int64_t *) t.x;\r\n        const int64_t * xs = (int64_t *) ((const int *) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\r\n        xi[0] = xs[0];\r\n#elif defined(NEW_MMA_AVAILABLE)\r\n```\r\n\r\nSee contributing guidelines.\r\n\r\nMore generally, if I interpret this code correctly it seems to me like it is basically the same as `load_generic` except the data is being loaded as a single 64 bit value. So I would suggest modifying that function instead.",
        "With NVIDIA the limitation for the source data in `ldmatrix` is that it needs to be a pointer to shared memory with 16 byte alignment. I was thinking about it more in terms of the memory space. In practice I think we would want to have the pointers be aligned anyways so I think it's fine to add memory alignment as a precondition for `load_generic` (the generic is more about not needing the `ldmatrix` instruction).",
        "The resulting value of 32 is correct but the way it's being calculated is wrong. The idea is that each thread moves 4 bytes so `threads_per_row` should always be equal to `WARP_SIZE` or with these changes `MMQ_TILE_NE_K`.",
        "Sorry I didn't read the PR properly. I thought `MMQ_TILE_NE_K` was the constant number of 32 bit values per row for all data types. Quite frankly I don't like the current state of things though. If the number of elements per row in a tile is a multiple of the physical warp size that is to me a sensible implementation, I don't like it being a seemingly arbitrary multiple of 32. I'm not 100% sure what would be the best solution here, maybe move `mmq_type_traits` up and define the tile shapes that way?",
        "Ultimately I currently can't think of a solution that really makes me happy. I think for this PR we can just merge it as-is and if I can later think of a better way to do it we can refactor it then (provided you'll be around for testing since I don't have access to CDNA3 hardware).",
        "I think all of these cases can be covered with `*dst = float(*src)`.",
        "Add a check for whether `src_t` and `dst_t` are the same, don't do a cast to float in that case (also covers FP16 -> FP16).",
        "Conversion between floats should not all be covered so this check can also be deduplicated by checking whether both types are either FP32, FP16, or BF16.",
        "This is fine for now but long-term I think it will be simpler (for floating-point data types) to define `__device__` functions that map from and to float rather than explicit mappings between 2 types.",
        "I think it would be better to turn `ggml_cuda_mul_mat_batched_cublas` into a template with the `ggml_type` as a template parameter.",
        "Actually, maybe turn the current function into something like `ggml_cuda_mul_mat_batched_cublas_impl` with a template parameter for the type and create a new function `ggml_cuda_mul_mat_batched_cublas` that just calls the impl function with the correct type.",
        ">ggml_get_to_fp16_nc_cuda\r\n\r\nWork with the `to_t_nc_cuda_t` template instead?\r\n\r\n>Perhaps just 3 different functions and ggml_cuda_mul_mat_batched_cublas does the dispatch based on type?\r\n\r\nI would really like to avoid this if at all possible. The cuBLAS interface should in principle be the same for FP32, FP16, and BF16. It should be possible to call cuBLAS concisely, one solution would I think be to define something like a `cublas_type_traits` template struct to get e.g. the right cuBLAS compute types (take a look at `mmq_type_traits` in `mmq.cuh` for an example of what I mean).",
        "The pointers are being cast to `char *` anyways so I think a simpler solution would be to just pass `void *` pointers.",
        "I would suggest adding something like `const bool XXX = ...` on the line above. As it is it's not clear to me what the correct interpretation of the check is. (Also it prevents the line from becoming too long.)",
        "This is not quite correct. For NVIDIA FP16 is supported on Pascal or newer (but CC 6.1 has terrible FP16 performance). So I would changing this to `supports_fast_fp16` and use that as one of the inputs for `use_fp16` (since they are only used together).\r\n\r\nAlso, I don't have a P100 available for testing, but the correct logic for CUDA and AMD should be `fast_fp16_hardware_available(cc)`. Consider extending that function for MUSA since it is also used to determine the optimal code paths for e.g. MoE models.",
        ">Would it be better to replace this with fast_fp16_hardware_available(cc) since that function is already updated in this PR?\r\n\r\nYes, that's what I meant."
      ],
      "llama.cpp-specify-naming-formats-explicitly": [
        "Preferably use an integer for the version. If you want to specify minor versions my suggestion is to either specify multiple integers or to do what NVIDIA does and specify e.g. v12.34 as something like `12034`.",
        "```suggestion\r\n    *   **Naming**: `training.tensor.{index}` (e.g., `training.tensor.0`, `training.tensor.1`, ...). No leading zeros.\r\n```\r\n\r\nWould also be fine to add leading zeros but it should be specified."
      ],
      "llama.cpp-systematic-test-coverage": [
        "You can remove this logic if the Vulkan backend simply returns that `OPT_STEP_SGD` is unsupported, the corresponding test will then simply be skipped."
      ],
      "llama.cpp-prefer-const-variables": [
        "```suggestion\r\n            const int j = j0 + tile_C::get_j(0);\r\n            const float2 dsB = __half22float2(y_dm[j*MMQ_TILE_Y_K + k01/QI8_1]);\r\n```\r\n\r\nPreferably use `const` if applicable.",
        "```suggestion\r\n    const int global_idx     = blockIdx.x * blockDim.x + threadIdx.x;\r\n    const int total_elements = batches * channels * out_h * out_w;\r\n```\r\n\r\nPreferably add const to variables unless they are actually intended to be changed later."
      ],
      "llama.cpp-optimize-memory-access-patterns": [
        "```suggestion\r\n        if constexpr (I != 64 || J != 2) {\r\n            int64_t * xi = (int64_t *) t.x;\r\n            const int64_t * xs = (int64_t *) ((const int *) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\r\n            xi[0] = xs[0];\r\n            return;\r\n        }\r\n```\r\n\r\nI think this would be simpler.",
        "I basically meant to have the instructions for loading data as 64 bit encapsulated in an `ifdef AMD_MFMA_AVAILABLE ... #endif` and to use the generic implementation if the preconditions aren't met. But if this is going to be refactored anyways it doesn't matter.",
        "```suggestion\r\n    const void * __restrict__ src0,\r\n    void * __restrict__ dst,\r\n```\r\n\r\nOn older GPUs especially you can gain some performance by telling the compiler that the pointers aren't aliased.",
        "I didn't look at the generated PTX code for this specific CUDA code but in my experience the CPU code pattern with explicit byte offsets performs comparatively poorly on GPUs. My recommendation would be to have the input and output types as template parameters and to compute the strides in units of the types (e.g. `nb01/ggml_element_size(src0)`) in host code. This is of course assuming that this kernel has a non-negligible contribution to the end-to-end performance in the first place.",
        "I didn't compare the two versions but I think it would be better to use a static block size of e.g. 256 here and to map the thread indices to tensor data indices as if the tensor was flattened. Running CUDA code with fractional warps will always result in wasted GPU resources.",
        "```suggestion\r\n    const int i01 = blockIdx.x;\r\n    const int i00 = blockIdx.y*blockDim.x + threadIdx.x;\r\n```\r\n\r\nThis is how you should organize the threads, otherwise the memory accesses are uncoalesced for `nb00 == ggml_element_size(src0)`. If `blockIdx` had the same size in all dimensions we could just do:\r\n\r\n```\r\n    const int i01 = blockIdx.y;\r\n    const int i00 = blockIdx.x*blockDim.x + threadIdx.x;\r\n```\r\n\r\nBut then we run into issues for `i01 >= 65536`. So the solution is to just swap `blockIdx.x` and `blockIdx.y` but without touching `blockdim` and `threadIdx` (since they are conceptually different)."
      ],
      "llama.cpp-explicit-performance-handling": [
        "```suggestion\r\n# Properties that are boolean and are converted to Yes/No for the table:\r\nLLAMA_BENCH_BOOL_PROPERTIES = [\"embeddings\", \"cpu_strict\", \"use_mmap\", \"no_kv_offload\", \"flash_attn\"]\r\nTEST_BACKEND_OPS_BOOL_PROPERTIES = [\"supported\", \"passed\"]\r\n```",
        "```suggestion\r\n    elif unit == \"GFLOPS\":\r\n        gflops = value\r\n    else:\r\n        assert False\r\n```"
      ],
      "llama.cpp-explicit-control-flow-logic": [
        "This logic is incorrect. Whether or not the optimizer needs buffers for the momenta is independent of whether or not buffers are needed to accumulate the gradients.",
        "```suggestion\r\n            struct ggml_tensor * opt_step;\r\n            switch (opt_ctx->optimizer_type) {\r\n                case GGML_OPT_OPTIMIZER_ADAMW:\r\n                    opt_step = ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, adamw_params);\r\n                    break;\r\n                case GGML_OPT_OPTIMIZER_SGD:\r\n                    opt_step = ggml_opt_step_sgd(opt_ctx->ctx_compute, node, grad, adamw_params);\r\n                    break;\r\n                default:\r\n                    GGML_ABORT(\"fatal error\");\r\n                    break;\r\n            }\r\n```\r\n\r\nThis is what I meant regarding separating the logic for allocation and setting `opt_step`. I don't have a problem with whether or not the code produces the correct logic, I have a problem with the implicit reasoning behind it. Whether or not the momenta should be allocated is derived from the type of optimizer, not the other way around.",
        "The optimizer type should be constant for the lifetime of `ggml_opt_context` so it's enough to just check whether the constant optimizer type needs the momenta.",
        "I think that if someone wants to change the optimizer type they should simply free the current `ggml_opt_context` and create a new one. The model weights will be unaffected and the overhead should be negligible vs. the runtime of one epoch.",
        "Add the optimizer type to `ggml_opt_params` and `ggml_opt_context`, set the value of the context during initialization, do not change it afterwards, use it for the `need_moment` logic. Does that make it clear enough?",
        "This is inconsistent with the docstring in `ggml.h`. As I outlined before for AdamW, the interface in `ggml.h` should be using the human-readable parameters. Please simply pass `alpha` and `wd` here. A derived parameter `keep` should be calculated in the backend-specific implementations for `OPT_STEP_SGD`."
      ],
      "llama.cpp-ai-parameter-organization": [
        "The number of epochs is not a property of the optimizer.",
        "It should be in \"user code\". The concept of an epoch only makes sense in conjunction with a dataset, so it should be added in functions like `ggml_opt_epoch` or `llama_opt_epoch` where there is explicit iteration over a dataset."
      ],
      "llama.cpp-api-minimalism-principle": [
        "I would say it's preferable to keep the interface simple: add the `--preview` flag, if set it always shows a fixed number of sequences, both as tokens and as text."
      ],
      "llama.cpp-measure-before-optimizing": [
        "Optimizer steps are going to be I/O bound and optimizing compute is not going to make a meaningful difference for the runtime of the steps, for the runtime of the total probram it's completely negligible. So please revert this change, I think the other variant is easier to understand.",
        "My biggest concern with the code is the amount of effort needed to maintain it, particularly when it comes to debugging and asserting that the code on master works correctly. It is quite likely that I will at some point be in a situation where a user reports bad training results and I will not know whether that is the due to a bug in ggml or due to bad hyperparamters or something similar. So it is very important to me that the data layout is consistent across multiple levels. \r\n\r\nThe correct way to implement the micro-optimization of pre-computing a parameter derived from the human-interpretable parameters is as follows:\r\n\r\n1. Pass the human-interpretable parameters to `ggml_opt_step_adamw` / `ggml_opt_step_sdg`.\r\n2. In the CUDA host code, pre-compute some derived parameters from the human-interpretable parameters.\r\n3. Change the CUDA device code to accept the derived parameters instead.\r\n\r\nThe way CUDA works is that the CPU schedules the GPU kernels in a CUDA stream and then waits for said stream to finish all kernels. Scheduling the kernels is of course much faster and it doesn't matter how fast you are as long as you are fast enough to keep the GPU busy. So adding a bit of overhead to the scheduling has essentially no impact on the runtime of a CUDA program even if you do it once per CUDA kernel launch instead of once per epoch.",
        "If performance is a concern we could maybe provide a list of EoG tokens to iterate over instead of iterating over all tokens and checking whether each one is EoG. Although I think iterating over all tokens once per request is going to be negligible vs. iterating over all tokens once per generated token as is being done for sampling."
      ],
      "llama.cpp-maintain-naming-consistency": [
        "```suggestion\r\n                    snprintf(buf, sizeof(buf), \"%6.2f kFLOP\", flops / 1e3);\r\n```\r\n\r\nNot that it was added with this PR, but the correct capitalization for a kilo FLOP is with a k.",
        "I don't understand the purpose of this line.",
        "Ah okay, I didn't see the inline `+=` in the line below. I think the code could be made easier to understand if you change the name of `step_prefix_len` to something like `step_prefix_len_orig` and move the `+=` to a separate line."
      ],
      "llama.cpp-consistent-clear-naming": [
        "```suggestion\r\n    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);\r\n```\r\n\r\nYou were right that it's fine to use either `tile_B::I` or `tile_C::J`. We should consistently use one of them.",
        "Either way is fine, it should just be consistent. I have a slight preference towards `tile_C::J` since that is what is used for the iteration over  `ntx`.",
        "Rename the arguments to e.g. `s01` to avoid confusion with the offsets in bytes. Preferably use `int64_t` since that is the data type used in host code."
      ],
      "llama.cpp-validate-configuration-options-explicitly": [
        "```suggestion\r\n        if self.tool == \"llama-bench\":\r\n            self.check_keys = set(LLAMA_BENCH_KEY_PROPERTIES + [\"build_commit\", \"test_time\", \"avg_ts\"])\r\n        elif self.tool == \"test-backend-ops\":\r\n            self.check_keys = set(TEST_BACKEND_OPS_KEY_PROPERTIES + [\"build_commit\", \"test_time\"])\r\n        else:\r\n            assert False\r\n```\r\n\r\nPreferably assert exact matches in case we ever add more tools.",
        "I would suggest setting the `--tool` CLI argument to `None` by default. If it is `None` here, use either table if it exists.",
        "I don't understand what you mean by that.",
        "It should be the same behavior by default. The default value for `--tool` is `None`. If pointed at an SQLite file with a table produced by `llama-bench`, check for the existence of table `test` and use it if it exists. Otherwise use table `test_backend_ops` if it exists. If the user explicitly specifies `--tool`, try to use the corresponding table, raise an error if it doesn't exist."
      ]
    }
  },
  "sydney-runkle": {
    "repos": [
      "pydantic/pydantic"
    ],
    "entries": [
      {
        "slug": "pydantic-avoid-shared-structure-mutation",
        "title": "Avoid shared structure mutation"
      },
      {
        "slug": "pydantic-avoid-unnecessary-operations",
        "title": "Avoid unnecessary operations"
      },
      {
        "slug": "pydantic-balance-documentation-thoroughness",
        "title": "Balance documentation thoroughness"
      },
      {
        "slug": "pydantic-cache-expensive-computations",
        "title": "Cache expensive computations"
      },
      {
        "slug": "pydantic-categorize-error-types",
        "title": "Categorize error types"
      },
      {
        "slug": "pydantic-consistent-configuration-patterns",
        "title": "Consistent configuration patterns"
      },
      {
        "slug": "pydantic-document-code-rationale",
        "title": "Document code rationale"
      },
      {
        "slug": "pydantic-document-configuration-relationships",
        "title": "Document configuration relationships"
      },
      {
        "slug": "pydantic-eliminate-redundant-computation",
        "title": "Eliminate redundant computation"
      },
      {
        "slug": "pydantic-enforce-style-with-linters",
        "title": "Enforce style with linters"
      },
      {
        "slug": "pydantic-explicit-over-implicit",
        "title": "Explicit over implicit"
      },
      {
        "slug": "pydantic-maintain-code-consistency",
        "title": "Maintain code consistency"
      },
      {
        "slug": "pydantic-preserve-language-conventions",
        "title": "Preserve language conventions"
      },
      {
        "slug": "pydantic-robust-error-messaging",
        "title": "Robust error messaging"
      },
      {
        "slug": "pydantic-safe-attribute-access-pattern",
        "title": "Safe attribute access pattern"
      },
      {
        "slug": "pydantic-semantic-over-syntactic",
        "title": "Semantic over syntactic"
      },
      {
        "slug": "pydantic-simple-defaults-flexible-overrides",
        "title": "Simple defaults, flexible overrides"
      },
      {
        "slug": "pydantic-specific-types-for-performance",
        "title": "Specific types for performance"
      },
      {
        "slug": "pydantic-standardize-dependency-management",
        "title": "Standardize dependency management"
      },
      {
        "slug": "pydantic-structured-configuration-management",
        "title": "Structured configuration management"
      },
      {
        "slug": "pydantic-write-targeted-specific-tests",
        "title": "Write targeted, specific tests"
      }
    ],
    "comments": {
      "pydantic-avoid-shared-structure-mutation": [
        "Seems like we lost this logic - is this needed anywhere?",
        "Would it make sense to break this change out into a different PR?",
        "Just curious, why do we need to add this here now?"
      ],
      "pydantic-standardize-dependency-management": [
        "```suggestion\r\n          enable-cache: true\r\n          python-version: '3.12'\r\n```"
      ],
      "pydantic-consistent-configuration-patterns": [
        "Should we use `ConfigDict(str_max_length=10)`?",
        "Should we recommend one over the other?",
        "The default does not override the config, this only overrides config if set. Good question!",
        "I believe in the migration guide we talk about priority here (sometimes it's not intuitive). Can we make a note on that here, and add a dupe key to the merged configs and show which one is preserved in the result?"
      ],
      "pydantic-write-targeted-specific-tests": [
        "Perhaps this could be a bit more verbose:\r\n\r\n1. Can we include the `SchemaError` information in this result?\r\n2. Could we add a note about the fact that this is purely a testing feature, not a runtime `pydantic` check (at this point)?\r\n\r\nPerhaps this is a bit excessive, but can we test this test? As in, run a test within a test, and check that this is raised for invalid schema? If not, no worries, but could you document an example of a failing test on this PR just to have as a reference for the blame later?",
        "Yeah, I think having their own tests would be good so that we can avoid the conditional warning checks.",
        "Why remove this?"
      ],
      "pydantic-preserve-language-conventions": [
        "`validate_by_name` logic still applies to `__init__` :)",
        "@stevapple, feel free to open an issue with a summary of this discussion!",
        "```suggestion\r\nimport json\r\nfrom pydantic import BaseModel, computed_field\r\n\r\n\r\nclass Box(BaseModel):\r\n    width: float\r\n    height: float\r\n    depth: float\r\n\r\n    @computed_field\r\n    @property  # (1)!\r\n    def volume(self) -> float:\r\n        return self.width * self.height * self.depth\r\n\r\n\r\nprint(json.dumps(Box.model_json_schema(mode='serialization'), indent=2))\r\n\"\"\"\r\n{\r\n  \"properties\": {\r\n    \"width\": {\r\n      \"title\": \"Width\",\r\n      \"type\": \"number\"\r\n    },\r\n    \"height\": {\r\n      \"title\": \"Height\",\r\n      \"type\": \"number\"\r\n    },\r\n    \"depth\": {\r\n      \"title\": \"Depth\",\r\n      \"type\": \"number\"\r\n    },\r\n    \"volume\": {\r\n      \"readOnly\": true,\r\n      \"title\": \"Volume\",\r\n      \"type\": \"number\"\r\n    }\r\n  },\r\n  \"required\": [\r\n    \"width\",\r\n    \"height\",\r\n    \"depth\",\r\n    \"volume\"\r\n  ],\r\n  \"title\": \"Box\",\r\n  \"type\": \"object\"\r\n}\r\n\"\"\"\r\n```\r\n\r\nLooking great, just a formatting fix here."
      ],
      "pydantic-balance-documentation-thoroughness": [
        "Agreed, good call.",
        "I've opened an issue and linked this comment. See https://github.com/pydantic/pydantic/issues/11491",
        "Isn't this something worth including in the conceptual docs?",
        "Agreed.",
        "I'm fine with this as long as we link to model docs. Maybe we could add a few other notes as well like json schema, etc.",
        "I feel like the explicit specifications here were helpful - people often ask about `init_typed`..."
      ],
      "pydantic-specific-types-for-performance": [
        "I like this example overall.\r\n\r\nI think we should:\r\n* Link to the related [performance section](https://docs.pydantic.dev/latest/concepts/performance/#sequence-vs-list-or-tuple-mapping-vs-dict)\r\n* Give a bit more context re efficiency here - the more specific you can be with a type, the better (sort of in a philosophical sense)",
        "Sure - basically, in one line or two, you can explain that generally, the more specific a type, the faster validation will be, as we don't have to perform as many checks / coercion attempts."
      ],
      "pydantic-document-configuration-relationships": [
        "> Also, what should happen if you set validate_by_alias=False, but explicitly set by_alias=True or by_name=True during validation?\r\n\r\nValidation time settings always take priority, when set. This is the same with `strict`.",
        "I'm sympathetic to the literal pattern argument. If we were starting fully from scratch, I think it might make more sense. Specifically, the boolean traps can be a bit confusing. In particular, the fact that you have to set `validate_by_name=True` if `validate_by_alias=False` explicitly is a bit confusing, especially for new users.\r\n\r\nOne thing we could do to mitigate this challenge is automatically set `validate_by_name=True` if a user sets `validate_By_alias=False`.\r\n\r\nMy thoughts re why we should stick with the 2 boolean flags:\r\n\r\n* It represents less change to this setting compared to a switch to literals - there's already a lot of change going on here, and I'm hesitant to introduce a setting `type` change as well.\r\n* 2 boolean flags provide greater configurability for interaction between config and runtime settings, as you can override one behavior and not the other. It's also helpful to have unset markers for each thing. For example:\r\n\r\n```\r\nM1: validate_by_alias = True, validate_by_name = False\r\nM2: validate_by_alias = False, validate_by_name = True\r\n\r\nruntime setting: by_name = True\r\n\r\n==>\r\n\r\nM1: alias and name validation\r\nM2: name only validation\r\n```\r\n\r\nThis can't be achieved with the literal approach. Either you'd use:\r\n* `validate_by='name'`, and M1 would lose alias validation\r\n* `validate_by='name and alias'` and M2 would no longer avoid validating with alias\r\n\r\n* Autocomplete is easier with boolean flags, and the behavior is relatively intuitive\r\n\r\nAliases are one of the most common (if not the most commonly used) field tool, so I do think this decision is quite important. I also understand that if we go with bools here, we're stuck with that until at least V4. ",
        "Agreed, definitely important. I've pushed a [commit](https://github.com/pydantic/pydantic/pull/11468/commits/2341a58997683df514a7dcd27a615a5d05678f55) to make this backwards compatible.\r\n\r\nThe new behavior:\r\n\r\n```py\r\nfrom pydantic import BaseModel, ConfigDict, Field\r\n\r\nclass Model1(BaseModel):\r\n    model_config = ConfigDict(validate_by_alias=False, validate_by_name=True)\r\n\r\n    my_field: str = Field(alias='my_alias')\r\n\r\nm1 = Model1(my_field='foo')\r\n\r\nclass Model2(BaseModel):\r\n    model_config = ConfigDict(validate_by_alias=True, validate_by_name=False)\r\n\r\n    my_field: str = Field(alias='my_alias')\r\n\r\nm2 = Model2(my_alias='foo')\r\n\r\nclass Model3(BaseModel):\r\n    model_config = ConfigDict(validate_by_alias=True, validate_by_name=True)\r\n\r\n    my_field: str = Field(alias='my_alias')\r\n\r\n# for this case, we prefer the field name over the alias, as we did in the past\r\n# if populate_by_name was True\r\nm31 = Model3(my_field='foo')\r\nm32 = Model3(my_alias='foo')\r\n#> test.py:23: error: Missing named argument \"my_field\" for \"Model3\"  [call-arg]\r\n\r\nclass Model4(BaseModel):\r\n    my_field: str = Field(alias='my_alias')\r\n\r\nm4 = Model4(my_alias='foo')\r\n\r\nclass Model5(BaseModel):\r\n    model_config = ConfigDict(populate_by_name=True)\r\n\r\n    my_field: str = Field(alias='my_alias')\r\n\r\n# for this case, we prefer the field name over the alias\r\nm51 = Model5(my_field='foo')\r\nm52 = Model5(my_alias='foo')\r\n#> test.py:39: error: Missing named argument \"my_field\" for \"Model5\"  [call-arg]\r\n```",
        "Fixed this problem by consolidating the settings as you had previously :)"
      ],
      "pydantic-explicit-over-implicit": [
        "This feels cleaner to me than the way we had it before, I wasn't a fan of having `tzdata` in `dev`. Thanks!",
        "It does seem redundant..."
      ],
      "pydantic-structured-configuration-management": [
        "Will do, this is going to take a while.",
        "Just curious, why did we move this up?",
        "I think we can only remove this bound if we change the lower bound for sqlalchemy?",
        "Or just add a conditional one for 3.13"
      ],
      "pydantic-semantic-over-syntactic": [
        "Would `is_type_alias_type` make more sense?",
        "I'd prefer `is_type_alias_type` just bc it's easier to read, but it's up to you, doesn't really matter!",
        "Maybe add `Schema` to the name of this? Or `CoreSchema`?",
        "Can we make this typevar more clear, maybe like `PropertyReturnType` or `ReturnType` or something like that?",
        "Good call"
      ],
      "pydantic-robust-error-messaging": [
        "Is this true? Doesn't it only need to be not `None` when `_fields.takes_validated_data_argument(self.default_factory)` is True?",
        "Fine by me, feel free to merge then :)."
      ],
      "pydantic-simple-defaults-flexible-overrides": [
        "See changes - I've sort of gone the other way here - wrap val is the same, but `__init__` is greatly simplified ðŸ‘ "
      ],
      "pydantic-cache-expensive-computations": [
        "@MarkusSintonen did a good job at intuitively extracting some of this logic as follows:\r\n\r\n```py\r\ndef get_existing_core_schema(obj: Any) -> core_schema.CoreSchema | None:\r\n    # Only use the cached value from this _exact_ class; we don't want one from a parent class\r\n    # This is why we check `cls.__dict__` and don't use `cls.__pydantic_core_schema__` or similar.\r\n    if (\r\n        hasattr(obj, '__dict__')\r\n        and (existing_schema := obj.__dict__.get('__pydantic_core_schema__')) is not None\r\n        and not isinstance(existing_schema, MockCoreSchema)\r\n    ):\r\n        return existing_schema\r\n    return None\r\n```\r\n\r\nMaybe we could eagerly pull changes like that into this PR, given that https://github.com/pydantic/pydantic/pull/10655 isn't quite ready to merge yet?",
        "I'm ok with leaving this dupe for now",
        "Why the removal of the `defer_build` check?",
        "```suggestion\r\n                # Deleting the validator/serializer is necessary as otherwise they can get reused in\r\n                # pydantic-core. Same applies for the core schema that can be reused in schema generation.\r\n```",
        "I wouldn't expect to have a `slow_memo_handler`, given that the point of memoization is to make things fast. Maybe we could use `setattr_handler` instead?"
      ],
      "pydantic-safe-attribute-access-pattern": [
        "Do we not even need to try this?",
        "I like the version @Viicos suggested.",
        "Can you avoid the ignore with a `cast` like `metadata = cast(schema.setdefault('metadata', {})`?",
        "Can we leave this as is? What `AttributeError` might occur here?",
        "Makes sense, thanks for the improvement!"
      ],
      "pydantic-maintain-code-consistency": [
        "```suggestion\r\n            model_frozen = cls.model_config.get('frozen')\r\n            field_frozen = getattr(cls.__pydantic_fields__.get(name), 'frozen', False)\r\n            if model_frozen or field_frozen:\r\n```",
        "We should make this consistent with the above pattern.\r\n\r\nIt's hard - we're not at the 3 repetition rule here that necessitates abstraction. I'm ok with keeping this duplicated code as long as it's as consistent as possible."
      ],
      "pydantic-enforce-style-with-linters": [
        "Did you want to add a lint step to precommit that helps enforce standardization here?"
      ],
      "pydantic-document-code-rationale": [
        "As discussed in person, let's add a comment that this warning is only omitted when calling super. I think we need to make it more clear what the consequences of this deprecation are.",
        "Maybe add a note about what successfully collected means - specifically, that annotations were successfully evaluated?",
        "Let's be even more explicit here.\r\n\r\nCan we:\r\n* Explain subsituting definition refs\r\n* Explain why / how we add tagged info to discriminators?\r\n\r\nThis isn't your doing at all, but our internal `GenerateSchema` logic is poorly documented, so I think it's good to have thorough improvements as a standard for PRs here going forward.",
        "Yeah, that probably makes more sense, given that's where we do the namespace fetching anyways. Happy to move there.",
        "Might put this under a closed by default block as to not overwhelm users not using this complex functionality.",
        "Maybe the class itself, honestly. Bc it applies to both `__init__` and `rebuild`",
        "Can we add a docstring to this function and the one above? If you're switching contexts into the world of typing and types management, a little description here could really help."
      ],
      "pydantic-eliminate-redundant-computation": [
        "Why is `is_generic_alias` not included in `typing_inspection`?",
        "Good question, looking into this.",
        "So, this happens when we have a model that deferred (or failed) schema building.\r\n\r\nI think this logic needs to be more complex - should we reassign `__pydantic_core_schema__`? What about `__pydantic_validator__` and `__pydantic_serializer__`?",
        "I've updated this - we no longer do the recursive call :).",
        "Good call, done.",
        "Hmm, I don't think there's an easy way to do this - you can't easily create a `TypeAdapter` with just a core schema. Even if we could hack that together, should we?",
        "Found a relatively simple way with `TypeAdapter`. That's easiest for now, as we already have the `isinstance` conditional in the wrap validator in the custom core schema.\r\n\r\nDown the line, maybe it does make sense to just use `SchemaValidator` inside these types to perform internal validation. I'm going to write up an issue with next steps based on the feedback here, and I'll include this ðŸ‘ "
      ],
      "pydantic-avoid-unnecessary-operations": [
        "@MarkusSintonen did a good job at intuitively extracting some of this logic as follows:\r\n\r\n```py\r\ndef get_existing_core_schema(obj: Any) -> core_schema.CoreSchema | None:\r\n    # Only use the cached value from this _exact_ class; we don't want one from a parent class\r\n    # This is why we check `cls.__dict__` and don't use `cls.__pydantic_core_schema__` or similar.\r\n    if (\r\n        hasattr(obj, '__dict__')\r\n        and (existing_schema := obj.__dict__.get('__pydantic_core_schema__')) is not None\r\n        and not isinstance(existing_schema, MockCoreSchema)\r\n    ):\r\n        return existing_schema\r\n    return None\r\n```\r\n\r\nMaybe we could eagerly pull changes like that into this PR, given that https://github.com/pydantic/pydantic/pull/10655 isn't quite ready to merge yet?",
        "I'm ok with leaving this dupe for now"
      ],
      "pydantic-categorize-error-types": [
        "Maybe we could offer more clarity on usage errors, like providing an example or two"
      ]
    }
  },
  "mwilsnd": {
    "repos": [
      "maplibre/maplibre-native"
    ],
    "entries": [
      {
        "slug": "maplibre-native-buffer-bounds-validation",
        "title": "Buffer bounds validation"
      },
      {
        "slug": "maplibre-native-conditional-observability-instrumentation",
        "title": "Conditional observability instrumentation"
      },
      {
        "slug": "maplibre-native-cross-platform-ci-validation",
        "title": "Cross-platform CI validation"
      },
      {
        "slug": "maplibre-native-descriptive-named-constants",
        "title": "Descriptive named constants"
      },
      {
        "slug": "maplibre-native-document-platform-requirements",
        "title": "Document platform requirements"
      },
      {
        "slug": "maplibre-native-enforce-clear-data-ownership",
        "title": "Enforce clear data ownership"
      },
      {
        "slug": "maplibre-native-externalize-config-values",
        "title": "Externalize config values"
      },
      {
        "slug": "maplibre-native-follow-modern-c-guidelines",
        "title": "Follow modern C++ guidelines"
      },
      {
        "slug": "maplibre-native-group-related-properties",
        "title": "Group related properties"
      },
      {
        "slug": "maplibre-native-handle-errors-by-severity",
        "title": "Handle errors by severity"
      },
      {
        "slug": "maplibre-native-lock-responsibly-always",
        "title": "Lock responsibly, always"
      },
      {
        "slug": "maplibre-native-modern-c-style-practices",
        "title": "Modern C++ style practices"
      },
      {
        "slug": "maplibre-native-numerical-precision-considerations",
        "title": "Numerical precision considerations"
      },
      {
        "slug": "maplibre-native-optimize-compilation-flags",
        "title": "Optimize compilation flags"
      },
      {
        "slug": "maplibre-native-preallocate-collection-capacity",
        "title": "Preallocate collection capacity"
      },
      {
        "slug": "maplibre-native-prefer-safe-null-handling",
        "title": "Prefer safe null handling"
      },
      {
        "slug": "maplibre-native-template-instantiation-trade-offs",
        "title": "Template instantiation trade-offs"
      },
      {
        "slug": "maplibre-native-use-proper-logging",
        "title": "Use proper logging"
      }
    ],
    "comments": {
      "maplibre-native-use-proper-logging": [
        "This was changed from Error to Debug intentionally, some of the tests use a [log observer](https://github.com/maplibre/maplibre-native/blob/88917b18065f6c7ee5d11ab2e109e0fab7af6edf/src/mbgl/util/logging.cpp#L53-L56) and only [Debug is omitted](https://github.com/maplibre/maplibre-native/blob/88917b18065f6c7ee5d11ab2e109e0fab7af6edf/src/mbgl/util/logging.cpp#L79). If XOpenDisplay fails during a test using the [FixtureLogObserver](https://github.com/maplibre/maplibre-native/blob/main/test/src/mbgl/test/fixture_log_observer.hpp), the test will fail because of the error level used in this retry message."
      ],
      "maplibre-native-cross-platform-ci-validation": [
        "A couple things:\r\n- bwb is really slow, it isn't building in parallel. There might be a way to fix that though.\r\n- bwb fails to locate a provisioning profile which is why we switched to bwx. I tested building the demo app and it still fails here in bwb mode."
      ],
      "maplibre-native-buffer-bounds-validation": [
        "Do we want to check that `size + offset` is within the buffer bounds here?"
      ],
      "maplibre-native-conditional-observability-instrumentation": [
        "Could this be put behind a preprocessor macro so it only happens when built with Tracy support?"
      ],
      "maplibre-native-preallocate-collection-capacity": [
        "How often do we actually see reallocation happening here?"
      ],
      "maplibre-native-modern-c-style-practices": [
        "Yup, looks like I don't even need to specify it with Clang 16.",
        "Use braces on multi-line conditionals",
        "Prefer `static_cast<GLint>`"
      ],
      "maplibre-native-descriptive-named-constants": [
        "Could you assign these indices to names to help clarify? ie `constexpr size_t LinePropUpdateFlags = 0;` (or with an enum),",
        "Can you store this in a named variable to add context, ex `constexpr uint32_t maximumVertexBindingCount = 16;`"
      ],
      "maplibre-native-lock-responsibly-always": [
        "Can we add an option to wait forever by passing 0? Would it also make sense to reset the timeout if some events in the queue are processed but more are enqueued as a result?"
      ],
      "maplibre-native-template-instantiation-trade-offs": [
        "I think in this case (and for `addCurrentVertex`), it's probably preferable to stick with `std::function`. What I expect to happen here is each unique lambda instantiates a new instance of this template. Since there is a lot of code in this template function, each new lambda given duplicates all this code. That quickly sends the trade-off in the other direction towards `std::function` the moment more than one lambda is passed to this template. Since each lambda is unique and the template approach is an almost certain inline, that prevents the compiler from de-duping the 95+% of the function body that is otherwise identical.",
        "I'm not super happy with this, but I can't specialize a template member function that also accepts a lambda without using `std::function` to erase the lambda type.",
        "The problem stems from the polymorphism of LayerGroupBase - a template virtual function isn't possible, so we need to know what kind of layer group we're dealing with to then invoke the right instance's visit template.",
        "I don't think so. There are two aspects to overhead with `std::function`, allocation and indirection. We probably weren't incurring allocation overhead in any of these changes, but we were paying the indirection cost.\r\n\r\n`std::function`'s type erasure works by using virtual dispatch. The compiler can't inline these virtual function calls as the functions provided could vary and *possibly* carry an allocated closure.",
        "+1 extract this to a function. In particular, comment the usage of `somePrime` as the FNV prime (assuming this is where it came from).",
        "Looks good."
      ],
      "maplibre-native-optimize-compilation-flags": [
        "Build with `-Oz` here to prevent the inliner behavior from distorting the optimal size configuration. Otherwise things may get aggressively inlined which distorts the metric we're interested in.\r\n\r\nAdditionally, if we can, build only for armv8/AArch64. By default I think the xcframework should have both binaries in it (x86-64 and armv8). Bloaty gets confused by that and tends to pick one, usually x86-64 in my testing. Arm binaries are going to be bigger just by virtue of the instruction set, so make sure we're comparing arm."
      ],
      "maplibre-native-prefer-safe-null-handling": [
        "They ultimately mean the same thing, and at least in MSVC/Windows land, NULL is still used in a lot of places. I do agree though a chance to modernize while we're in here is good."
      ],
      "maplibre-native-follow-modern-c-guidelines": [
        "nit: `virtual` isn't required when the `override` keyword is present, high compiler warning levels can complain if they follow the core guidelines on [virtual, override and final](http://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines.html#c128-virtual-functions-should-specify-exactly-one-of-virtual-override-or-final)",
        "Prefer `using` over typedef",
        "syntax: `using CollisionBoxUBO = CollisionUBO;`"
      ],
      "maplibre-native-document-platform-requirements": [
        "This header won't be found for bazel builds, can you move it inside the `#if MLN_RENDER_BACKEND_OPENGL` block and additionally guard it with `defined(MLN_USE_TRACY )`?"
      ],
      "maplibre-native-handle-errors-by-severity": [
        "Should this throw? We need an allocator if we're going to render stuff.",
        "We should probably throw since assert will get dropped in release builds. Calling stubs should always terminate."
      ],
      "maplibre-native-numerical-precision-considerations": [
        "I think it's fine for now, if we start having to do this in other places we should add some test constants though."
      ],
      "maplibre-native-externalize-config-values": [
        "Should these limits be configurable at all? They could be set via build flags/preprocessor values.",
        "Can this be guarded by `#ifdef MLN_USE_TRACY `? We shouldn't try and load this extension unless we're building for instrumentation with Tracy"
      ],
      "maplibre-native-enforce-clear-data-ownership": [
        "I had to look up how to do this actually, haha. It extracts a value from the container and move it to a shared pointer inside the lambda's capture group. Doing it this way means we can keep using the iterator for the rest of the container.",
        "From when I worked on it, I believe `extract().mapped()` was my best option to prevent a copy of the element while also removing it from the container. Other options didn't look like they'd work.\r\n\r\nIt may have that lambda issue, I never noticed the other instance in `TileCache`. I also haven't seen the destructor running on the main thread. To be safe, I'd say do the same here as in `TileCache`.",
        "> Is `tiles.extract((tilesIt++)->first)` important here, vs. `tiles.extract(tilesIt++)`? It's doing an extra key search.\r\n\r\nIf it compiles and works correctly then no, I suppose it isn't.",
        ">It seems to be called somewhat rarely.\r\n\r\nIf it gets called at all then I say we should fix it, there is always a possibility of threading issues if that is allowed to happen.",
        "Have you considered what might happen if the caller holds on to the returned `string_view` while another thread (or even the same one later on) triggers a reallocation of `buffer`?\r\n\r\nIt might be prudent to couple the read lock with the returned string_view to at least reduce such a possibility.",
        "Yes, a copy would be the safest option if you feel that is acceptable. I would consider the frequency at which strings are fetched from the buffer to determine if constructing `std::string`s is an acceptable performance trade-off.",
        "We can just take a read lock here. If we don't find our string in the cache, we release the read lock and take a write lock to do our insertion."
      ],
      "maplibre-native-group-related-properties": [
        "Is the intention here to only do polylines? I think this could be made a bit more clear what \"mode\" we're in, perhaps break these parameters out to a struct and pass it to `addPolyline` instead of having them here."
      ]
    }
  },
  "Jarred-Sumner": {
    "repos": [
      "oven-sh/bun"
    ],
    "entries": [
      {
        "slug": "bun-always-await-promises",
        "title": "Always await promises"
      },
      {
        "slug": "bun-cache-repeated-accesses",
        "title": "Cache repeated accesses"
      },
      {
        "slug": "bun-check-exceptions-consistently",
        "title": "Check exceptions consistently"
      },
      {
        "slug": "bun-consistent-descriptive-identifiers",
        "title": "Consistent descriptive identifiers"
      },
      {
        "slug": "bun-maintain-consistent-style",
        "title": "Maintain consistent style"
      },
      {
        "slug": "bun-match-api-conventions",
        "title": "Match API conventions"
      },
      {
        "slug": "bun-match-filenames-to-contents",
        "title": "Match filenames to contents"
      },
      {
        "slug": "bun-network-api-compatibility",
        "title": "Network API compatibility"
      },
      {
        "slug": "bun-never-swallow-errors",
        "title": "Never swallow errors"
      },
      {
        "slug": "bun-optimize-database-interactions",
        "title": "Optimize database interactions"
      },
      {
        "slug": "bun-thread-safe-state-transitions",
        "title": "Thread-safe state transitions"
      },
      {
        "slug": "bun-use-memory-pools",
        "title": "Use memory pools"
      },
      {
        "slug": "bun-validate-network-inputs",
        "title": "Validate network inputs"
      },
      {
        "slug": "bun-validate-nullability-explicitly",
        "title": "Validate nullability explicitly"
      }
    ],
    "comments": {
      "bun-maintain-consistent-style": [
        "```suggestion\nconst Installer = @This();\n```",
        "This should be a top-level struct with \r\n```zig\r\nconst AsyncHTTP = @This();\r\n```",
        "This should be a top-level struct.\r\n```suggestion\r\nconst Headers = @This();\r\n```"
      ],
      "bun-consistent-descriptive-identifiers": [
        "Tests may have json in the name. This should just be a strip ansi IMO"
      ],
      "bun-match-api-conventions": [
        "@claude make it return .invalid_arg\r\n"
      ],
      "bun-never-swallow-errors": [
        "This assertion shouldn't be removed"
      ],
      "bun-optimize-database-interactions": [
        "One thing I'm not sure about: do we need to call sqlite3_step repeatedly or can we get away iwth only stepping once?"
      ],
      "bun-match-filenames-to-contents": [
        "The file name for this should be `InternalState.zig` and InternalState should be a top-level struct.",
        "The file should be called ThreadSafeStreamBuffer.zig and this should be a top-level struct\r\n\r\n```suggestion\r\nconst ThreadSafeStreamBuffer = @This();\r\n```",
        "The file name should be `Signals.zig` and this should be a top-level struct\r\n\r\n```suggestion\r\nconst Signals = @This();\r\n```",
        "The file name should be `HTTPThread.zig`, and this should be a top-level struct\r\n```suggestion\r\nconst HTTPThread = @This();\r\n```",
        "This file should be `processDependencyList.zig` instead of `process.zig` as proces could mean spawning a process, processing data, processing tasks, etc"
      ],
      "bun-network-api-compatibility": [
        "does this descriptor match node?"
      ],
      "bun-always-await-promises": [
        "This should check if it returned a Promise and if that Promise is not resolved, and only then use await.",
        "sink.end can return a promise, so we should await it."
      ],
      "bun-validate-network-inputs": [
        "```suggestion\r\n    llhttp_type_t type = static_cast<llhttp_type_t>(typeValue.toNumber(globalObject));\r\n    RETURN_IF_EXCEPTION(scope, {});\r\n```"
      ],
      "bun-check-exceptions-consistently": [
        "This would crash on exception\r\n```suggestion\r\n            auto value = params->get(globalObject, property);\r\n            RETURN_IF_EXCEPTION(scope, nullptr);\r\n```"
      ],
      "bun-cache-repeated-accesses": [
        "Since sqlite3_step can potentially take awhile, let's use a `MarkedArgumentBuffer args` so the GC doesn't see the JSArray until it's about to be returned."
      ],
      "bun-use-memory-pools": [
        "```suggestion\n        const link_target_buf = bun.PathBufferPool.get();\n        defer bun.PathBufferPool.put(link_target_buf);\n```",
        "```suggestion\n        const link_dest_buf = bun.PathBufferPool.get();\n        defer bun.PathBufferPool.put(link_dest_buf);\n```",
        "```suggestion\n        const link_rel_buf = bun.PathBufferPool.get();\n        defer bun.PathBufferPool.put(link_rel_buf);\n```",
        "```suggestion\r\n                const normalized_buf = bun.PathBufferPool.get();\r\n                defer Bun.PathBufferPool.put(normalized_buf);\r\n```",
        "Use `bun.PathBufferPool`"
      ],
      "bun-thread-safe-state-transitions": [
        "This isn't threadsafe. Fortunately, the `fetch_or` call returns the previous value, so you can do the check there. ANd since it's an or, it won't unset the terminated flag."
      ],
      "bun-validate-nullability-explicitly": [
        "Null-terminated slices must always be freed, and `ptr: [:0]const u8` implicitly casts to `ptr: []const u8`. So this is not correct."
      ]
    }
  },
  "oliviertassinari": {
    "repos": [
      "mui/material-ui"
    ],
    "entries": [
      {
        "slug": "material-ui-consistent-component-api-patterns",
        "title": "Consistent component API patterns"
      },
      {
        "slug": "material-ui-consistent-package-naming",
        "title": "Consistent package naming"
      },
      {
        "slug": "material-ui-document-compatibility-boundaries",
        "title": "Document compatibility boundaries"
      },
      {
        "slug": "material-ui-document-implementation-decisions",
        "title": "Document implementation decisions"
      },
      {
        "slug": "material-ui-effect-hook-best-practices",
        "title": "Effect hook best practices"
      },
      {
        "slug": "material-ui-event-triggered-network-requests",
        "title": "Event-triggered network requests"
      },
      {
        "slug": "material-ui-explicit-configuration-resolution",
        "title": "Explicit configuration resolution"
      },
      {
        "slug": "material-ui-explicit-configuration-specifications",
        "title": "Explicit configuration specifications"
      },
      {
        "slug": "material-ui-follow-library-recommendations",
        "title": "Follow library recommendations"
      },
      {
        "slug": "material-ui-maintain-configuration-accuracy",
        "title": "Maintain configuration accuracy"
      },
      {
        "slug": "material-ui-meaningful-and-consistent-names",
        "title": "Meaningful and consistent names"
      },
      {
        "slug": "material-ui-nextjs-integration-patterns",
        "title": "Next.js integration patterns"
      },
      {
        "slug": "material-ui-use-design-system-tokens",
        "title": "Use design system tokens"
      },
      {
        "slug": "material-ui-use-screen-queries",
        "title": "Use screen queries"
      },
      {
        "slug": "material-ui-write-timeless-documentation",
        "title": "Write timeless documentation"
      }
    ],
    "comments": {
      "material-ui-meaningful-and-consistent-names": [
        "Usually called accumulator https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/reduce#examples, shorten `acc` in most of the codebase:\r\n\r\n```suggestion\r\n        (acc, curr) => ({\r\n          ...acc,\r\n```"
      ],
      "material-ui-effect-hook-best-practices": [
        "Moved this to wrap the maximum possible scope.",
        "> Is this operating on the same key as useLocalStorageState?\r\n\r\nIt does, but the TODO comments I have left in the code are to move to a point where the mode is to be handled by `useCurrentColorScheme` and not `useLocalStorageState`.",
        "@Janpot Yes but what I see is that the mode is a lot more integrated with MUI System once you use CSSÂ Variables and Zero runtime than it was with Emotion (what Toolpad uses).\r\n\r\nMUI System almost needs to completely own that logic. If it's possible to separate things then I'm ðŸ’¯ for it. I had this feeling that hard to understand how things work because of how closely things are linked together but I don't really see how to simplify things. We have effectively built https://github.com/pacocoursey/next-themes.\r\ncc @siriwatknp.\r\n\r\n"
      ],
      "material-ui-consistent-package-naming": [
        "Per https://www.notion.so/mui-org/engineering-mui-utils-purpose-9a9fc9da3a004864b6c4e1f4d1f24f95?pvs=4#101cbfe7b660800f9d59c9204ece4ec7?\r\n\r\n```suggestion\r\n  \"name\": \"@mui/internal-bundle-size\",\r\n```",
        "> https://www.notion.so/mui-org/engineering-mui-utils-purpose-9a9fc9da3a004864b6c4e1f4d1f24f95?pvs=4#d1699bb48f0f4ef58ca831d1ee9772ec\r\n\r\nAh right, ok, my bad, I agree, it's a different class. I propose this then: \r\n\r\n<img width=\"703\" alt=\"SCR-20250508-uatk\" src=\"https://github.com/user-attachments/assets/e49b37d8-bdf9-436b-a1bb-0dadb1b819cc\" />\r\n\r\nhttps://www.notion.so/mui-org/engineering-mui-utils-purpose-9a9fc9da3a004864b6c4e1f4d1f24f95?pvs=4#d1699bb48f0f4ef58ca831d1ee9772ec\r\n\r\nFor example, if we apply this in https://github.com/mui/mui-private/pull/867/files#diff-cdf978f995619066e7c5a60f61cb6d14debaa5f59943afdffe682a9ea53b091aR16\r\n\r\n<img width=\"520\" alt=\"SCR-20250508-ubib\" src=\"https://github.com/user-attachments/assets/02a5c4a0-4c2e-4b30-8aa2-3862d263268f\" />\r\n\r\nthis would become `@mui/private-chat-uikit`. cc @hasdfa.\r\n\r\n> I'm just removing the name altogether, problem solved \r\n\r\nOh yeah, if it has no dependents, even better ðŸ‘ "
      ],
      "material-ui-use-screen-queries": [
        "Prefer using the `screen`. We are moving tests as much as possible to rely on global queries. This is purely to keep the test environment simple. Most of the time, we don't need the notion of a container. We render one element at once on the screen.\r\n\r\n```suggestion\r\n    render(<TextareaAutosize style={{ backgroundColor: 'yellow' }} />);\r\n    const input = document.querySelector<HTMLTextAreaElement>('textarea')!;\r\n```"
      ],
      "material-ui-explicit-configuration-specifications": [
        "Because we set \"latest\" for the dependencies version, and most people who create reproduction never pin the npm packages version, having the versions set at the root should help us reproduce 2+ years old bugs when we try to iterate on some logic.",
        "> That's not very useful to recreate the installed environment\r\n\r\n@Janpot Oh yeah, agree, it only helps to know it's v4 and not v5 or v6 ðŸ˜†. \r\n\r\n> There will be a lockfile in the sandbox of the reproduction which should be sufficient to recreate and inspect the conditions of the bug.\r\n\r\nIn the reproduction provided in https://github.com/mui/material-ui/issues, none seems to have this.\r\n\r\n(_A side note, they all seem to prefer CodeSandbox even when we push to StackBlitz by default in the instructions, maybe some SEO lag, or something else._)",
        "Yeah, I think it would be better, but this is more time-consuming to implement. I think we can narrow the scope  down, make the PR simpler. I'm reverting the version.",
        "It looks like @jedwards1211 handled Material UI v7 support in https://github.com/jcoreio/material-ui-popup-state/issues/153, so we can make the change.\r\n\r\nIn the future, we might find surprises with breaking changes, and have to temporarily comment those lines.",
        "@jedwards1211 was there major changes that requires to update those examples? For example https://deploy-preview-46051--material-ui.netlify.app/material-ui/react-menu/#material-ui-popup-state",
        "OK, thanks. So I think that we can assume that we are good."
      ],
      "material-ui-document-implementation-decisions": [
        "Why remove codesandbox?\r\n\r\n- https://pro.similarweb.com/#/digitalsuite/websiteanalysis/overview/website-performance/*/999/1m?webSource=Total&key=codesandbox.io,stackblitz.com\r\n- https://analytics.google.com/analytics/web/?authuser=1#/analysis/p353089763/edit/KctZ977QSou7VoMyBQMaqw\r\n\r\nIt looks like the majority of our users go with codesandbox. "
      ],
      "material-ui-explicit-configuration-resolution": [
        "```suggestion\r\n  // must be loaded last: https://github.com/tailwindlabs/prettier-plugin-tailwindcss?tab=readme-ov-file#compatibility-with-other-prettier-plugins\r\n  plugins.push('prettier-plugin-tailwindcss');\r\n```"
      ],
      "material-ui-event-triggered-network-requests": [
        "To check what should be the correct behavior with the middle click",
        "How about?\r\n\r\n```suggestion\r\n    if ((inputValue === '' || !open) && event.button === 0) {\r\n```\r\n\r\n<img width=\"797\" alt=\"Screenshot 2023-03-20 at 01 51 17\" src=\"https://user-images.githubusercontent.com/3165635/226222148-a910ba0c-8a90-4436-ace1-c7b1817bde73.png\">\r\n\r\nhttps://developer.mozilla.org/en-US/docs/Web/API/MouseEvent/buttons\r\n\r\n\r\nWe use `event.button` for the select and the slider: https://github.com/mui/material-ui/pull/22557#discussion_r486698221",
        "@michaldudak In the case of `event.buttons`, I imagine that the logic would need to be `event.buttons === 1` while it's `event.button === 0` in our case. But overall, I think that the issue is:\r\n\r\n- button is active: which button was pressed on the mouse **to** trigger the event\r\n- buttons is passive: which buttons are pressed on the mouse **when** a mouse event is triggered",
        "Maybe\r\n\r\n```suggestion\r\n    // Only handle event when the main button is pressed (left click).\r\n    if ((inputValue === '' || !open) && event.button === 0) {\r\n```"
      ],
      "material-ui-document-compatibility-boundaries": [
        "That was not true, it has never been allowed, per https://mui.com/material-ui/guides/minimizing-bundle-size/#option-one-use-path-imports."
      ],
      "material-ui-use-design-system-tokens": [
        "Why does it use a custom value? Meaning, why is this not using the theme breakpoints? I think deviations from the standard deserve code comments.",
        "Rely on the design token, not hard-coded values."
      ],
      "material-ui-maintain-configuration-accuracy": [
        "Is there a singleton on this one? I would imagine that the system can be a direct dependency\r\n\r\nAlso, per https://github.com/mui/material-ui/pull/32623 we should add a peer dependent on emotion."
      ],
      "material-ui-consistent-component-api-patterns": [
        "I would imagine that SvgIcon will eventually go inside `@mui/system` because once we add a new set of icons for Joy UI 1. Will we duplicate `SvgIcon` again? 2. Won't we want to have Material UI users be able to import these new icons without loading Joy UI?",
        "But then, maybe it means that we should solve #21251 and let the developers import `SvgIcon` from the right package?\r\n\r\n```jsx\r\nimport SvgIconMd from '@mui/material/SvgIcon';\r\nimport SvgIconJoy from '@mui/joy/SvgIcon';\r\nimport EditIcon from '@mui/icons-material/Edit';\r\n\r\n<SvgIconMd>\r\n  <EditIcon />\r\n</SvgIconMd>\r\n```\r\n\r\nIt's a fallback method of FontAwesome: https://fontawesome.com/docs/web/use-with/react/add-icons#add-individual-icons-explicitly.",
        "> It is tailored specifically for Material Design icons (sizes, viewport)\r\n\r\nI don't think that these are fundamentally specific to Material Design or Joy UI. We document how it can be used with random SVG path in https://mui.com/material-ui/icons/#font-awesome and are using it for the same use case as well. IMHO, the value of this component is to apply a few better default: attributes, a11y resets, CSS resets, that developer would want. It's also to provide the MUI System style helper. ",
        "The second argument was meant to be called with a value, not an option. It's related to #23708 and #31192. Unclear what is right here."
      ],
      "material-ui-follow-library-recommendations": [
        "I'm not sure we need this e2e test in the first place. It doesn't harm but I think that the origin of the problem is that `<TextField onClick` should be registered on the root element of the text field, we can't, move it to a lower element as we did in https://github.com/mui/material-ui/pull/36892."
      ],
      "material-ui-write-timeless-documentation": [
        "@mui-org/x heads up, this is a breaking change, you need to apply the same diff once you upgrade the mono-repo. We improve the support for the translation of the docs."
      ],
      "material-ui-nextjs-integration-patterns": [
        "It looks like we can remove `passHref` now per https://nextjs.org/docs/app/api-reference/components/link and the source: https://github.com/vercel/next.js/blob/8e0acd90341d59f53447189f265c5c8f9b1e3c28/packages/next/src/client/link.tsx. \r\n\r\nI'm fixing this in https://github.com/mui/material-ui/pull/44871."
      ]
    }
  },
  "patak-dev": {
    "repos": [
      "vitejs/vite"
    ],
    "entries": [
      {
        "slug": "vite-break-down-complex-functions",
        "title": "Break down complex functions"
      },
      {
        "slug": "vite-configure-ssr-environments",
        "title": "Configure SSR environments"
      },
      {
        "slug": "vite-descriptive-consistent-naming",
        "title": "Descriptive consistent naming"
      },
      {
        "slug": "vite-document-protocol-configurations-clearly",
        "title": "Document protocol configurations clearly"
      },
      {
        "slug": "vite-environment-variable-management",
        "title": "Environment variable management"
      },
      {
        "slug": "vite-evolve-apis-with-compatibility",
        "title": "Evolve APIs with compatibility"
      },
      {
        "slug": "vite-explicit-version-requirements",
        "title": "Explicit version requirements"
      },
      {
        "slug": "vite-leverage-native-tooling",
        "title": "Leverage native tooling"
      },
      {
        "slug": "vite-manage-configuration-inheritance-carefully",
        "title": "Manage configuration inheritance carefully"
      },
      {
        "slug": "vite-optimize-glob-operations",
        "title": "Optimize glob operations"
      },
      {
        "slug": "vite-restrict-server-access",
        "title": "Restrict server access"
      },
      {
        "slug": "vite-secure-workflow-permissions",
        "title": "Secure workflow permissions"
      },
      {
        "slug": "vite-use-optional-patterns",
        "title": "Use optional patterns"
      }
    ],
    "comments": {
      "vite-use-optional-patterns": [
        "Conceptually, I would prefer this one to be:\r\n```suggestion\r\n  isSelfAccepting?: boolean\r\n```\r\nAs the value is undefined until we compute it. In the case of `transformResult`, I think is different because the value is reset to null.",
        "Conceptually, I would prefer this one to be:\r\n```suggestion\r\n  isSelfAccepting?: boolean\r\n```\r\nAs the value is undefined until we compute it. In the case of `transformResult`, I think is different because the value is reset to null.",
        "Nice, better to avoid the function. We could let it be undefined, no?\r\n```suggestion\r\n  const rebase = pattern.match(/.*\\/?node_modules\\//)?.[0]\r\n  if (rebase) {\r\n```"
      ],
      "vite-configure-ssr-environments": [
        "Backward compatibility. `server.moduleGraph` has nodes from the `client` and `ssr` environment. And `server.ssrLoadModule(url)` transform the code in the `ssr` environment.",
        "```suggestion\r\nThe implicit `ssr` environment and other non-client environments use a `RunnableDevEnvironment` by default during dev. While this requires the runtime to be the same with the one the Vite server is running in, this works similarly with `ssrLoadModule` and allows frameworks to migrate and enable HMR for their SSR dev story. You can guard any runnable environment with an `isRunnableDevEnvironment` function.\r\n```"
      ],
      "vite-leverage-native-tooling": [
        "I think this paragraph can be simplified. Something like this would be easier for me to read:\r\n\r\n```suggestion\r\n## Performance\r\n\r\n`rolldown-vite` is focused on ensuring compatibility with the existing ecosystem, so defaults are geared towards a smooth transition. You can get further performance gains by switching over to faster Rust-based transforms and other customizations.\r\n\r\n### Enabling Native Plugins\r\n```",
        "should dev also be mentioned here to avoid users assuming it is build only?\r\nmaybe:\r\n\r\n```suggestion\r\n- Use [Rolldown instead of Rollup and esbuild](./rolldown) to speed up Vite and opt into a more aligned dev and build story.\r\n```"
      ],
      "vite-optimize-glob-operations": [
        "Maybe we could use a regex here?\r\n\r\n```suggestion\r\nconst nodeModulesRE = /.*\\/?node_modules\\//\r\nfunction rebaseGlobPattern(pattern: string) {\r\n  return pattern.match(nodeModulesRE)?.[0] ?? ''\r\n}\r\n```"
      ],
      "vite-environment-variable-management": [
        "The \"Consequently\" is a bit confusing to me here. Maybe: \r\n```suggestion\r\n`NODE_ENV=...` can be set in the command, and also in your `.env` file. If `NODE_ENV` is specified in a `.env.[mode]` file, the mode can be used to control its value. However, both `NODE_ENV` and modes remain as two different concepts.\r\n```"
      ],
      "vite-break-down-complex-functions": [
        "Maybe this would be more clear with a condition out of the for loop:\r\n```js\r\n  function setupEnvironment(environmentName, environmentConfig) {\r\n    const environment = await environmentConfig.build.createEnvironment(\r\n      environmentName,\r\n      environmentConfig,\r\n    )\r\n    await environment.init()\r\n    environments[environmentName] = environment\r\n  }\r\n\r\n  if (selectedEnvironmentName) {\r\n    setupEnvironment(selectedEnvironmentName, config)\r\n  }\r\n  else {\r\n    // ... current for loop using setupEnvironment(environmentName, environmentConfig)\r\n  }\r\n```"
      ],
      "vite-secure-workflow-permissions": [
        "Ya, I think the Vitest setup may work well for Vite too."
      ],
      "vite-evolve-apis-with-compatibility": [
        "Agreed. The idea is to avoid deprecations warnings for current methods in Vite 6, and then add them in a future 6.x minor (at the same time we remove the experimental label from Environment API).",
        "The idea for the new signature is:\r\n1. we use this in `transformRequest` where we no longer have access to the `server`. The server was not used at all, only the `config` is needed here.\r\n2. we have the `filePath` when we use it, we don't need to call `fsPathFromUrl`\r\n\r\nAt least internally, we needed this new function. We don't need to expose this new signature in this PR though. And we certainly don't want to deprecate the old one at this point. We can leave the new one as internal, and then decide later on if we want to improve on this API (I think in general it is better to pass a config instead of the whole server if that's enough)",
        "Ah! Ya, now I remember you exposed it. About the order, I think is better to pass the config first (same if you need to pass a server or environment). If we later need to add an extra param, this form avoids ending up with config in the middle.",
        "I was thinking about deprecating at one point `isFileServingAllowed` and only keep the new one later on, so we won't need to align then",
        "We had a discussions with Pedro about knowing if you are being called during the scan phase [here](https://discord.com/channels/804011606160703521/831456449632534538/1278763329116049450). I think we need to expose an API to know this. The `ScanEnvironment` isn't the same as a `DevEnvironment` because it doesn't expose a `moduleGraph`. I think we can make it part of the API.\r\n\r\nAbout the `FutureCompatEnvironment` and the typing, I think the last one to touch this part was @ArnaudBarre. The idea is to allow plugins to do\r\n```js\r\nif (this.environment.mode === 'dev') {\r\n  this.environment.moduleGraph // this works without a cast\r\n}\r\n```\r\nWe could play with a different design though. Maybe if you have an idea here we could check it in a PR to see how it affects `this.environment` usage?",
        "`UnknownEnvironment` sounds a lot better to me. I like it. About scan, what would be the type for it, thought?",
        "I'm not 100% bought into the current `ScanEnvironment`, mainly because if we ever want to have a scan environment during build, we would need to name it `BuildScanEnvironment` (I find the possibility remote given that we tried this direction and backtracked and rolldown is in the picture now, but anyways). So I'm ok making this internal for now.",
        "Thanks for implementing this Bjorn! https://github.com/vitejs/vite/pull/18008"
      ],
      "vite-explicit-version-requirements": [
        "I think we should require Node 20.19.0+, 22.12.0+. To keep things cleaner, maybe we could merge this PR as \"remove node 18 support\" as it removes several branches we needed for it, and then propose the `require(esm)` by default range in another PR?"
      ],
      "vite-manage-configuration-inheritance-carefully": [
        "That would be a fantastic refactoring ðŸ’¯ ",
        "I'm fine with omitting the inferred ones, maybe adding a comment about what how that inference works. My vote goes for the current implementation in the PR. I think once the \"merge\" helper is in, it is going to be easier to work with.",
        "I was thinking on the `mergeWithDefaults` that Sapphi implemented. We could prevent `vite.build(configDefaults)` by checking the instance. But you're right that they may do something like: `vite.build({ ...configDefaults, foo: 'bar' })`.\r\n\r\nI don't think that current users would ever try to do something like this, but depending how we document `configDefaults`, I see your point.\r\n\r\nIt feels a bit cumbersome to use if we expose a ton of variables like `CONFIG_DEFAULTS_RESOLVE_CONDITIONS`. Imagine having to import 3 or 4 of these.\r\n\r\nMaybe we could make all the properties in the public `configDefaults` non-enumerable so they can not iterate over the keys and the user needs to access them one by one. We should get the same properties as the individual variables but with better typing and easier to import. We would still have a `internalConfigDefaults` that we can iterate to be used in `mergeWithDefaults`",
        "Maybe it is a good idea to merge only the refactoring that makes things cleaner internally for now, and then we can discuss how to expose the defaults in another PR. Maybe we don't need to do this for Vite 6, people can still copy and past the defaults from the docs (it isn't the same though, that will fix the values, instead of adding an extra to the defaults that is what we have now but it may be ok)",
        "I think it would be better to let server environments explicitly opt-in here setting `noDiscovery: false` to avoid making the default more complex."
      ],
      "vite-descriptive-consistent-naming": [
        "better to call `cwd` as `base` here? So we avoid possible confusion with `cwd` not being `cwd` when modified."
      ],
      "vite-restrict-server-access": [
        "```suggestion\r\nSetting `server.allowedHosts` to `true` allows any website to send requests to your dev server through DNS rebinding attacks, allowing them to download your source code and content. We recommend always using an explicit list of allowed hosts. See [GHSA-vg6x-rcgg-rjx6](https://github.com/vitejs/vite/security/advisories/GHSA-vg6x-rcgg-rjx6) for more details.\r\n```",
        "```suggestion\r\nSetting `server.cors` to `true` allows any website to send requests to your dev server and download your source code and content. We recommend always using an explicit list of allowed origins.\r\n```"
      ],
      "vite-document-protocol-configurations-clearly": [
        "```suggestion\r\n- **Default:** `{ origin: /^https?:\\/\\/(?:(?:[^:]+\\.)?(?:localhost|test)|127\\.0\\.0\\.1|\\[::1\\])(?::\\d+)?$/ }` (allows localhost, `127.0.0.1` and `::1`)\r\n```"
      ]
    }
  },
  "ollevche": {
    "repos": [
      "opentofu/opentofu"
    ],
    "entries": [
      {
        "slug": "opentofu-clear-concise-documentation",
        "title": "Clear concise documentation"
      },
      {
        "slug": "opentofu-craft-actionable-errors",
        "title": "Craft actionable errors"
      },
      {
        "slug": "opentofu-defensive-null-handling",
        "title": "Defensive null handling"
      },
      {
        "slug": "opentofu-document-intent-and-limitations",
        "title": "Document intent and limitations"
      },
      {
        "slug": "opentofu-document-phased-migration-paths",
        "title": "Document phased migration paths"
      },
      {
        "slug": "opentofu-document-with-examples",
        "title": "Document with examples"
      },
      {
        "slug": "opentofu-names-preserve-cognitive-context",
        "title": "Names preserve cognitive context"
      },
      {
        "slug": "opentofu-preserve-sensitive-data-marks",
        "title": "Preserve sensitive data marks"
      },
      {
        "slug": "opentofu-prevent-backing-array-surprises",
        "title": "Prevent backing array surprises"
      },
      {
        "slug": "opentofu-reduce-code-nesting",
        "title": "Reduce code nesting"
      },
      {
        "slug": "opentofu-review-consistency-assumptions",
        "title": "Review consistency assumptions"
      },
      {
        "slug": "opentofu-separate-configuration-lifecycles",
        "title": "Separate configuration lifecycles"
      },
      {
        "slug": "opentofu-specify-configuration-behaviors",
        "title": "Specify configuration behaviors"
      },
      {
        "slug": "opentofu-structure-tests-thoroughly",
        "title": "Structure tests thoroughly"
      },
      {
        "slug": "opentofu-use-relative-documentation-links",
        "title": "Use relative documentation links"
      }
    ],
    "comments": {
      "opentofu-use-relative-documentation-links": [
        "```suggestion\r\nUnlike other [custom conditions](../expressions/custom-conditions.mdx), assertions do not affect OpenTofu's execution of an operation. A failed assertion reports a warning without halting the ongoing operation. This contrasts with other custom conditions, such as a postcondition, where OpenTofu produces an error immediately, halting the operation and blocking the application or planning of future resources.\r\n```",
        "```suggestion\r\nPreconditions are unique amongst the custom conditions in that they execute _before_ a resource change is applied or planned. [Choosing Between Preconditions and Postconditions](../expressions/custom-conditions.mdx#choosing-between-preconditions-and-postconditions) offers guidance on choosing between a precondition and a postcondition, and the same topics also apply to choosing between a precondition and a check block.\r\n```"
      ],
      "opentofu-document-with-examples": [
        "We are not consistent with those, so it is more of nit. To follow other updates tone of voice:\r\n\r\n```suggestion\r\n* \"force-unlock\" option is now supported by the HTTP backend. ([#2381](https://github.com/opentofu/opentofu/pull/2381))\r\n```"
      ],
      "opentofu-review-consistency-assumptions": [
        "One more thing to keep in mind is that `s3` backend is being used with other non-AWS s3 compatible storage implementations. Those services may or may not support the conditional locking feature. Ideally, we would check somehow if that feature is supported and error otherwise. Not sure if there's an easy way to check that, but we need to at least document this pitfall.",
        "We need not to forget to update the user-facing docs as well.",
        "I may be missing something, but do we need to update the digest? If I understand correctly, we could go with a `IfNoneMatch` lock creation so it would fail for any other tofu run, while the file exists. On Unlock, we could just delete it altogether. As far as I can tell, this matches the dynamodb implementation and eliminates data races via `IfNoneMatch` option. \r\n\r\nWhat do you think?",
        "Got you, thanks! I missed that part, I will re-read it and leave another comment.",
        "After some digging, I think this option is unnecessary even for dynamodb locking. This feature was introduced to workaround the eventual consistency nature of s3 (before 2020). It is checking if the read state file is the latest written one and we no longer need to check this since s3 become strongly consistent ([blogpost](https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/)) in 2020.",
        "This is something only slightly relevant for this RFC, so I think we can tackle it separately. Alternatively, we can leave it as is if we plan to deprecate dynamodb locking completely. Otherwise, removing md5 write / read from dynamodb locking will positively affect the performance."
      ],
      "opentofu-structure-tests-thoroughly": [
        "Makes sense, I added names for test cases in [9d345f3](https://github.com/opentofu/opentofu/pull/2673/commits/9d345f3385a809b204485d667b1c7b5b0d77a370)",
        "Thanks, added in [a4f25c2](https://github.com/opentofu/opentofu/pull/1728/commits/a4f25c288cf40005fb55b8f6be479d540716acc2)",
        "I would prefer a more clear separation between test logic and test data (inputs and outputs). I think the following structure covers test needs in a slightly simpler way (you would also need to change test run logic inside a loop):\r\n\r\n```suggestion\r\n\tinputLockInfo := statemgr.LockInfo{\r\n\t\tID:        \"ada-lovelace-state-lock-id\",\r\n\t\tWho:       \"AdaLovelace\",\r\n\t\tOperation: \"TestTypePlan\",\r\n\t}\r\n\r\n\ttestCases := []struct {\r\n\t\tname\t\t\t\tstring\r\n\t\tlockResponseStatus\tint\r\n\t\tlockResponseBody\t\t[]byte\r\n\t\texpectedLockInfo\t\t*statemgr.LockInfo // or just an ID\r\n\t\texpectedErrorMsg\t\tstring\r\n\t}{\r\n\t...\r\n```"
      ],
      "opentofu-reduce-code-nesting": [
        "we can return early here since we will return anyway right after the switch-case",
        "let's reduce nesting here:\r\n\r\n```suggestion\r\n\t\tif pv, ok := v.(GraphNodeProviderConsumer); !ok {\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\t...\r\n```",
        "Thanks! Made it more idiomatic in [8075c3f](https://github.com/opentofu/opentofu/pull/1959/commits/8075c3f26301c29062c5e2568f6bba9a762b71c5)",
        "[354964b](https://github.com/opentofu/opentofu/pull/1750/commits/354964b836ed72308b6fc8e48f5b7d42cda7c460)",
        "nit: just a bit clearer with reduced nesting\r\n\r\n```suggestion\r\n                ext := tfFileExt(p)\r\n\t\tif ext == \"\" {\r\n\t\t\trelevantPaths = append(relevantPaths, p)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t        parallelTofuExt := strings.ReplaceAll(ext, \".tf\", \".tofu\")\r\n\t        pathWithoutExt, _ := strings.CutSuffix(p, ext)\r\n\t        parallelTofuPath := pathWithoutExt + parallelTofuExt\r\n        \r\n\t        // If the .tf file has a parallel .tofu file in the directory,\r\n\t        // we'll ignore the .tf file and only use the .tofu file\r\n\t        if slices.Contains(paths, parallelTofuPath) {\r\n\t\t        ignoredPaths = append(ignoredPaths, p)\r\n\t        } else {\r\n\t\t        relevantPaths = append(relevantPaths, p)\r\n\t        }\r\n```"
      ],
      "opentofu-preserve-sensitive-data-marks": [
        "This is actually irrelevant for the deprecation marks. However, we don't need to remove sensitive marks if there are none.",
        "I removed that change. The code below is not only removing sensitive marks, but also reordering existing ones (from nested to top level). I believe this was not an issue for most cases, but now that we actually care about other marks, it could be a problem. I will review usages.",
        "Found a few more cases, where sensitive marks handling needs to be refactored and put the changes under a separate PR - https://github.com/opentofu/opentofu/pull/2673"
      ],
      "opentofu-craft-actionable-errors": [
        "Makes sense, changed tone of voice in d44bf936584acd2e90273774b29711082cbb7377"
      ],
      "opentofu-defensive-null-handling": [
        "`m.Aliases` must be allocated before the loop.",
        "`AsString()` panics if the value is marked so we need to call it only after the sensitivity check.",
        "`AliasExpr` could be nil here and lead to a panic.\r\n\r\nnit: it may be also useful to split the condition into multiple IFs / switch to make it easier to read and provide a more personalized error message.",
        "It is nit, you can just ignore that part of the comment!",
        "Yep, I completely agree with you. I moved it higher and changed `buildAllVariables` to write into an existing map so we use the same map everywhere.\r\n\r\nbadc0309785d7139b9e9b66cdd5d41714814d142"
      ],
      "opentofu-document-intent-and-limitations": [
        "nit: it is useful to have a comment here describing when/how `Provider` struct is built",
        "I extended the comment to describe a little more and also named a few examples: [920b9ab](https://github.com/opentofu/opentofu/pull/1728/commits/920b9ab7ad00e3ec97644ed62bcb724cc900d2fa). ",
        "Do you mind leaving a note on why do we multiply by 2?",
        "Yes, sure. I mean let's add a comment to the code to make it totally clear."
      ],
      "opentofu-names-preserve-cognitive-context": [
        "`psuedo` should be renamed like `decodedExpectedVar` to be more readable."
      ],
      "opentofu-prevent-backing-array-surprises": [
        "I extended the comment to `ComposeBySchema` function to elaborate more on seed and randomization sequence: [6447b38](https://github.com/opentofu/opentofu/pull/1772/commits/6447b38ff4b9824809a67c80b098174f96fee3df).\r\n\r\nAlso, I put more comments where iteration order is important: [ea83b47](https://github.com/opentofu/opentofu/pull/1772/commits/ea83b47d87067a31601c1c4f389f0d7598318363).",
        "`reflect.DeepEqual` feels a bit of an overkill here. Also, it could fail with wrapped diags.\r\n\r\nWould you mind adapting the logic in [ConsolidateWarnings](https://github.com/opentofu/opentofu/blob/main/internal/tfdiags/consolidate_warnings.go#L25) here?\r\n\r\nI think we can treat two diags as duplicates if:\r\n* their `Severity`s are equal\r\n* their `Source.Subject`s are present and the whole `Source`s are equal\r\n* their `Description`s are equal\r\n\r\nI would also check if `DoNotConsolidateDiagnostic` function allows merging the diag.",
        "Yep, I agree `DoNotConsolidateDiagnostic` check is optional here.\r\n\r\n> perhaps with the Ranges added as well.\r\n\r\n`Source`s should be equal and `Source.Subject`s (ranges) should be non-zero. "
      ],
      "opentofu-clear-concise-documentation": [
        "nit:\r\n```suggestion\r\n`terraform_remote_state` does not respect the sensitivity of source state snapshots. Use with caution when referencing state with sensitive root module output values.\r\n```",
        "nit: not sure if we need an example here, IMO it increases page complexity more than adds relevant context (for this particular page).",
        "Yep, makes sense. Moved note to usage block for most of the commands: [3980be3](https://github.com/opentofu/opentofu/pull/1843/commits/3980be38e72560ca2a97aea35f51c0f0ea66b9cf)",
        "that's a miss, thank you!\r\n\r\n[e27f67c](https://github.com/opentofu/opentofu/pull/1843/commits/e27f67c2a6ed5485312283c102c84e11accd337b)"
      ],
      "opentofu-specify-configuration-behaviors": [
        "```suggestion\r\n> When OpenTofu S3 backend is used with an S3 compatible provider, it needs to be checked that the provider supports conditional writes in the same way AWS S3 is offering.\r\n```"
      ],
      "opentofu-document-phased-migration-paths": [
        "Another comment about the migration path - what users actually need to do? I assume this is the following flow:\r\n1. Change the configuration to enable native locking\r\n2. `tofu apply`\r\n3. Change the configuration to remove dynamodb usage\r\n\r\nHowever, I am not sure if changes to the backend configuration would trigger state write. We need to test it additionally and document properly. "
      ],
      "opentofu-separate-configuration-lifecycles": [
        "We already track some of the modernization/refactoring issues and I would suggest we document that down to address later. Both @Evi1Pumpkin and @cam72cam arguments sound pretty reasonable.\r\n\r\nBreaking down config package structures into different stages such as loading (raw) and evaluation (ready for graph processing) makes perfect sense to me.",
        "IMO we have to ensure other tools **fail** correctly if this field is something they rely on. I think it should be omit completely.",
        "We treat it as an error because the provider could be set in both resource and import blocks. If that is the case, providers (and aliases) should match. If the provider alias contains `each` / `count`, they will be different for sure so we error. Does it answer your question?",
        "Extended the comments in [82308c0](https://github.com/opentofu/opentofu/pull/1960/commits/82308c030a18c975a39138db95a8bd4adaf81945)",
        "```suggestion\r\n\t\t\tDetail:   \"count was declared as a provider attribute in a test file\",\r\n```",
        "I checked the usage of `Sensitive` field, and I believe we have a few more places where we could use `Deprecated` field.\r\n\r\nThe first one is the interactive input. There are few options: let the user know the field is deprecated beforehand or even skip the interactive input if the field is deprecated. Not sure about the last one, however deprecation would mean we have another field, which should be used instead, and thus it makes sense to just skip the deprecated one. The functions I am referring to are [Local.interactivelyCollectVariables](https://github.com/opentofu/opentofu/blob/main/internal/backend/local/backend_local.go#L417) and [Meta.getInput](https://github.com/opentofu/opentofu/blob/main/internal/command/meta_config.go#L162).\r\n\r\nThe second one is JSON output mode. It includes the configuration part, which IMO should reflect the deprecation as well as other configured options. For example, here is a part of configuration planned into a file:\r\n\r\n```terraform\r\nmodule \"mod\" {\r\n  source = \"./mod\"\r\n  a = \"b\"\r\n}\r\n```\r\n\r\nAnd this is a part of `tofu show --json planfile` command:\r\n\r\n```json\r\n    \"root_module\": {\r\n      \"module_calls\": {\r\n        \"mod\": {\r\n          \"source\": \"./mod\",\r\n          \"expressions\": {\r\n            \"a\": {\r\n              \"constant_value\": \"b\"\r\n            }\r\n          },\r\n          \"module\": {\r\n            \"variables\": {\r\n              \"a\": {\r\n                \"default\": \"b\"\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  ```\r\n  \r\n  What do you think?",
        "Thanks! JSON outputs look good to me. However, I am not sure if I understand how the deprecated message is shown on interactive input. I would use similar approach as in the warning, where we say \"Variable is marked as deprecated with the following message: <MESSAGE>\". ",
        "Nice catch, thanks!\r\n\r\n[5fa601a](https://github.com/opentofu/opentofu/pull/1772/commits/5fa601a8bae2f435f88d08e2b390d95febabbb6d)"
      ]
    }
  },
  "knizhnik": {
    "repos": [
      "neondatabase/neon"
    ],
    "entries": [
      {
        "slug": "neon-cache-performance-preservation",
        "title": "Cache performance preservation"
      },
      {
        "slug": "neon-comprehensive-code-documentation",
        "title": "Comprehensive code documentation"
      },
      {
        "slug": "neon-configurable-cache-parameters",
        "title": "Configurable cache parameters"
      },
      {
        "slug": "neon-configuration-context-alignment",
        "title": "Configuration context alignment"
      },
      {
        "slug": "neon-database-replica-promotion-safeguards",
        "title": "Database replica promotion safeguards"
      },
      {
        "slug": "neon-extract-and-reuse",
        "title": "Extract and reuse"
      },
      {
        "slug": "neon-guard-against-race-conditions",
        "title": "Guard against race conditions"
      },
      {
        "slug": "neon-handle-all-error-paths",
        "title": "Handle all error paths"
      },
      {
        "slug": "neon-handle-network-interrupts-safely",
        "title": "Handle network interrupts safely"
      },
      {
        "slug": "neon-limit-concurrent-access-slots",
        "title": "Limit concurrent access slots"
      },
      {
        "slug": "neon-mind-transaction-boundaries",
        "title": "Mind transaction boundaries"
      },
      {
        "slug": "neon-optimize-data-structures",
        "title": "Optimize data structures"
      },
      {
        "slug": "neon-optimize-what-matters",
        "title": "Optimize what matters"
      },
      {
        "slug": "neon-performance-test-pragmatism",
        "title": "Performance test pragmatism"
      },
      {
        "slug": "neon-proactive-cache-warming",
        "title": "Proactive cache warming"
      }
    ],
    "comments": {
      "neon-optimize-what-matters": [
        "I do not think that this code is performance critical.\r\nSo it is better to keep it simpler and straightforward rather than try to optimise it.\r\nThis cold assignment can be easily avoided by doing `metrics[NUM_QT_BUCKETS].bucket_le = INFINITY` after the loop.  But pleas notice thatI have not written this function: it is just cut&paste of existed `histogram_to_metrics` function. If it needs to be optimised, then it should be done in separate PR.\r\n"
      ],
      "neon-proactive-cache-warming": [
        "+1 for  CPlane-triggered writes.\r\nAlternatively it can done as part of checkpoint. ",
        "I think dumping it in checkpoint is the best choice."
      ],
      "neon-configurable-cache-parameters": [
        "Will do, as I wrote - this constant is just temporary thing to illustrate approach. Definitely it needs to be replaced with parameter. I just didn't want to cut&paste a lot of boilerplate code before there is some consensus of how to solve this issue (if my proposal with two cache is accepted).",
        "Done",
        "Done"
      ],
      "neon-optimize-data-structures": [
        "I thought about it but preferred simpler implementation.\r\nBut after tbhinging for a while, I come to the conclusion that storing counter in `DdlHashTable` can even simplify things:\r\n1. We do not need to create all chain of `DdlHashTable`.\r\n2. We can allocate `DdlHashTable` in CurTransactionContext and do not worry about it's deallocation.\r\nSo thank you for the advice - I implemented this approach.",
        "> Does this not have off-by-one issues? I\r\n\r\nOne is subtracted when `subtrans_level` is assigned:\r\n```\r\nnew_table->subtrans_level = SubtransDdlLevel - 1;\r\n```\r\n\r\n> I'd prefer keeping a single counter, rather than one reset every time you push a table:\r\n\r\nOk, it seems to be more clear and straightforward. Will change.",
        "it can be done simpler using sscanf:\r\n```\r\nint pos;\r\nif (sscanf(safekeepers_list, \"g#%u:%n\", generation, &pos) == 1) { \r\n     return safekeepers_list + pos;\r\n} else {\r\n     return safekeepers_list;\r\n}\r\n```"
      ],
      "neon-limit-concurrent-access-slots": [
        "It is true for prewarm, but if we are going to use the same mechanism for storing prefetch results in LFC as soon as they arrived, number of writer ca. be as larger as number of backends (`max_connections`) which can be thousands.",
        "Once again, if this mechanism is used not only for prewarm, but for normal backends, then maintaining per-bakend state is non-disarable."
      ],
      "neon-mind-transaction-boundaries": [
        "I am absolutely against using `with` construction in tests.\r\nIt is really very convenient and safe in real python database applications.\r\nBut in case of test connection lifetime management is not critical - leaking connection is not a problem, it is in any case dropped when endpoint is restarted or testis finished.\r\n\r\nThe main problem with `with` construction which cause many problems: it implicitly start new transaction and even in auto commit all code in `with` body is executed in one transaction.\r\n\r\nBut please notice that I have also added your version of the test which uses this `with` constructions for connections.\r\n"
      ],
      "neon-database-replica-promotion-safeguards": [
        "The `replica_promote` flag is used only to disarm this check:\r\n```\r\n\t\t/*\r\n\t\t * Basebackup LSN always points to the beginning of the record (not\r\n\t\t * the page), as StartupXLOG most probably wants it this way.\r\n\t\t * Safekeepers don't skip header as they need continious stream of\r\n\t\t * data, so correct LSN for comparison.\r\n\t\t */\r\n\t\tif (SkipXLogPageHeader(wp, wp->propTermStartLsn) != wp->api.get_redo_start_lsn(wp))\r\n```\r\n\r\nIn our case redo_restart corresponds to old redo_restart LSN of replica.\r\nMy first attempt to fix the problem was to explicitly set this restart LSN using `SetRedoStartLsn()`\r\nBut it is not available in PG14.\r\nSo I decided just to disarm this check.\r\nI will be pleased if you can propose better solution.\r\n",
        "May be this check is failed because I disabled recovery:\r\n```\r\n\t/*\r\n\t * Consider whether we need to assign a new timeline ID.\r\n\t *\r\n\t * If we did archive recovery, we always assign a new ID.  This handles a\r\n\t * couple of issues.  If we stopped short of the end of WAL during\r\n\t * recovery, then we are clearly generating a new timeline and must assign\r\n\t * it a unique new ID.  Even if we ran to the end, modifying the current\r\n\t * last segment is problematic because it may result in trying to\r\n\t * overwrite an already-archived copy of that segment, and we encourage\r\n\t * DBAs to make their archive_commands reject that.  We can dodge the\r\n\t * problem by making the new active segment have a new timeline ID.\r\n\t *\r\n\t * In a normal crash recovery, we can just extend the timeline we were in.\r\n\t */\r\n\tnewTLI = endOfRecoveryInfo->lastRecTLI;\r\n-\tif (ArchiveRecoveryRequested)\r\n+\tif (ArchiveRecoveryRequested && !ZenithRecoveryRequested)\r\n```\r\nbut then we should somehow address the problem with timelines.\r\nI am not fan of this hack with adding ` !ZenithRecoveryRequested` - if there some better solution I will be glad to implement it.  But still not sure that bumping timeline on replica promotion=node restart is good idea.\r\n"
      ],
      "neon-configuration-context-alignment": [
        "Done",
        "Sorry, forget to save file."
      ],
      "neon-performance-test-pragmatism": [
        "Linux is not real time OS. And we are running tests in cloud. \r\nSo, especially if query execution time is relatively small (as in this case < 1 second), it is not possible to expect that execution time with optimization always be smaller than without it. Adding such assert is just a direct road to yet another fluky test. Certainly we can repeat query several times and calculate average time.  But IMHO it is overkill.\r\nBut may be such flukyness for performance tests is not so critical as for regression tests...\r\n",
        "Ok, done"
      ],
      "neon-extract-and-reuse": [
        "Done"
      ],
      "neon-cache-performance-preservation": [
        "Postgres can write WAL not only as result of some query execution.\r\nThere many background activities which cause writing WAL: checkpoint, vacuum,..."
      ],
      "neon-handle-all-error-paths": [
        "Please notice that right now this check is performed at the moment when response is assigned to slot by `check_getpage_response` function.\r\nThis is first thing that it does and panics if it is not true.",
        "Ok, will add assertion here.",
        "There are several cases which can cause incorrect handling of errors or interrupts: when connection is not dropped or it is dropped put prefetch ring state is not reset.\r\nActually what I have done:\r\n1. Move `consume_prefetch_responses` prior to sending new (non-getpage request)\r\n2. Block interrupts while preforming disconnect (CHECK_FOR_INTERRUPTS can be called from any logging function and can cause exit from this function without actually dripping connection)\r\n3. Register `before_shmem_exit` to handle terminate backend requests which are not handled by `PG_TRY/PG_CATCH`.",
        "FATAL errors are not matched by TRY/CATCH blocks. In this case proc_exit is called directly.\r\nSo we have not closed connection but still can communicate with PS in backend cleanup code (i.e. removing temp relations)"
      ],
      "neon-guard-against-race-conditions": [
        "Done",
        "This really strange check is used to ensure that shared memory is initialized and not detached (as done by PG14 for stat process).\r\nSo checking it with `IsUnderPostmaster` is not correct.\r\nSimilar check is done in several other places in our codeL:\r\n```\r\nfile_cache-inmem.c\u0000205:\tif (!lfc_ctl || !UsedShmemSegAddr || IsParallelWorker())\r\nfile_cache.c\u0000395:\t * SIGHUP and it has access to shared memory (UsedShmemSegAddr != NULL),\r\nfile_cache.c\u0000398:\treturn lfc_ctl && MyProc && UsedShmemSegAddr && !IsParallelWorker();\r\nlibpagestore.c\u0000183:\treturn pagestore_shared && UsedShmemSegAddr;\r\nwalproposer_pg.c\u0000314:\tif (newval && *newval != '\\0' && UsedShmemSegAddr && walprop_shared && RecoveryInProgress())\r\n```\r\nMay be it will be good to somehow refactor it and use single function for it.\r\n",
        "In stat process in PG14 `IsUnderPostmaster` is true, but shared memory is detached."
      ],
      "neon-handle-network-interrupts-safely": [
        "I do not think that pipelining of non-get page requests and get page request is really important. There are only two other frequent requests: exists and nblocks, but both should be eliminated in most cases  by resize cache.\r\n\r\nIs it absolutely necessary to move `consume_prefetch_responses` before sending new non-get page request?\r\nNo, it is not. If we can enforce that in case of error:\r\n1. Connection is dropped\r\n2. Prefetch ring state it reset\r\nthan it is not needed. The original error I have found is that this code calls `page_server->disconnect` which is not resetting prefetch state. My first attempt to fix it was just to add call of `prefetch_on_ps_disconnect()` here.\r\nBut then I thought that it will be safer to consume prefetch responses before sending new request.",
        "Just to clarify: I wanted to separate prefetch state (when there are some inflight `getpage` requests) and non-prefetch state (when we do classical server request-response call).\r\n",
        "Sorry, I do not fully agree with your analysis (or at least do not completely understand it).\r\nHow `page_server->receive() ` can got stuck waiting for new connection?\r\n`pageserver_receive` is not waiting for connection. It immediately returns NULL id state is not `PS_Connected`.\r\n\r\nThe problem I have reported is different. Assume that there is some pending interrupt - i.e. statement timeout.\r\nAnd it is invoked from `CHECK_FOR_INTERRUPTS` places in one of the functions called in TRY block in `page_server_request`. Inn this case CATCH block calls `page_server->disconnect(shard_no);`. But it actually calls `pageserver_disconnect_shard` which drop connection but doesn't reset prefetch state (doesn't call `prefetch_on_ps_disconnect`).\r\n"
      ],
      "neon-comprehensive-code-documentation": [
        "Done"
      ]
    }
  }
}